{
  "date": "2025-11-25",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-25 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å…³äº **LLM Agent è‡ªæˆ‘è¿›åŒ–ä¸è®°å¿†æœºåˆ¶** çš„æ·±åº¦æ¢è®¨ï¼ˆä» Evo-Memory åˆ° Agent0-VLï¼‰ï¼ŒåŒæ—¶**æœºæ¢°å¯è§£é‡Šæ€§**è¿æ¥äº†ç†è®ºä¸Šçš„çªç ´ï¼ˆç¥ç»å…ƒç»„åˆè§£é‡Šçš„æœ€ä¼˜æ€§ä¿è¯ï¼‰ï¼Œæ­¤å¤–ï¼Œ**ä¸–ç•Œæ¨¡å‹**çš„æ¨ç†å¼•æ“å’Œé’ˆå¯¹**åŒ»å­¦å½±åƒ**çš„å¤§æ¨¡å‹åº”ç”¨ä¹Ÿå±•ç°äº†ä»¤äººå…´å¥‹çš„è¿›å±•ã€‚\n\n---\n\n### ğŸ§  æ·±åº¦å­¦ä¹ ç†è®ºä¸å¯è§£é‡Šæ€§ (Interpretability & Theory)\n\n**1. ç¥ç»å…ƒç»„åˆè§£é‡Šçš„ä¿è¯æœ€ä¼˜æ€§**\n**Guaranteed Optimal Compositional Explanations for Neurons**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡åç†è®ºçš„é‡ç£…æ–‡ç« ã€‚ç›®å‰çš„ç¥ç»å…ƒè§£é‡Šï¼ˆå¦‚é€šè¿‡é€»è¾‘è§„åˆ™æè¿°ç¥ç»å…ƒæ¿€æ´»ä¸æ¦‚å¿µçš„å¯¹é½ï¼‰é€šå¸¸ä¾èµ–æ³¢æŸæœç´¢ï¼ˆBeam Searchï¼‰ï¼Œä½†è¿™æ— æ³•ä¿è¯æœ€ä¼˜æ€§ã€‚ä½œè€…æå‡ºäº†**é¦–ä¸ªè®¡ç®—ä¿è¯æœ€ä¼˜ç»„åˆè§£é‡Šçš„æ¡†æ¶**ï¼ŒåŒ…æ‹¬åˆ†è§£å½±å“å¯¹é½çš„å› ç´ ã€å¯å‘å¼ä¼°ç®—ä»¥åŠé¦–ä¸ªåœ¨å¯è¡Œæ—¶é—´å†…è®¡ç®—æœ€ä¼˜è§£é‡Šçš„ç®—æ³•ã€‚\n*   **å‘ç°**ï¼šåœ¨è®¡ç®—æœºè§†è§‰å’Œ CNN è®¾ç½®ä¸‹ï¼Œç°æœ‰çš„æ³¢æŸæœç´¢åœ¨å¤„ç†é‡å æ¦‚å¿µæ—¶ï¼Œæœ‰ **10-40%** çš„è§£é‡Šæ˜¯æ¬¡ä¼˜çš„ã€‚\n\n**2. ç¥ç»å…ƒå¯¹é½çš„å¼€æ”¾è¯æ±‡ç»„åˆè§£é‡Š**\n**Open Vocabulary Compositional Explanations for Neuron Alignment**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ä¸Šä¸€ç¯‡çš„é™åˆ¶ï¼Œè¿™ç¯‡è®ºæ–‡å¼•å…¥äº†å¼€æ”¾è¯æ±‡ï¼ˆOpen Vocabularyï¼‰æ¡†æ¶ã€‚ä»¥å¾€çš„è§£é‡Šä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œé™åˆ¶äº†é¢†åŸŸã€‚æ–°æ¡†æ¶åˆ©ç”¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ç”Ÿæˆçš„æ©ç ï¼Œå…è®¸ç”¨æˆ·é’ˆå¯¹ä»»æ„æ¦‚å¿µå’Œæ•°æ®é›†æ¢æµ‹ç¥ç»å…ƒã€‚\n*   **ä»·å€¼**ï¼šæ‘†è„±äº†å¯¹é¢„å®šä¹‰æ¦‚å¿µé›†çš„ä¾èµ–ï¼Œæå¤§æå‡äº†ç¥ç»å…ƒè§£é‡Šçš„çµæ´»æ€§ã€‚\n\n**3. ç‰©ç†å¼•å¯¼ï¼šç‰©ç†åŸºç¡€æ¨¡å‹ä¸­è·¨åŸŸæ¦‚å¿µçš„å› æœæ§åˆ¶**\n**Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå— LLM æ¿€æ´»å¹²é¢„çš„å¯å‘ï¼Œä½œè€…ç ”ç©¶äº†**ç‰©ç†åŸºç¡€æ¨¡å‹**çš„å†…éƒ¨è¡¨ç¤ºã€‚é€šè¿‡æå–ä¸åŒç‰©ç†çŠ¶æ€ä¸‹çš„æ¿€æ´»å‘é‡å¹¶è®¡ç®—â€œå·®å¼‚â€å¼ é‡ï¼Œä½œè€…èƒ½å¤Ÿåƒæ“çºµ LLM è¡Œä¸ºä¸€æ ·ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡æ³¨å…¥è¿™äº›æ–¹å‘æ¥**å› æœåœ°æ§åˆ¶ç‰©ç†æ¨¡æ‹Ÿçš„é¢„æµ‹**ï¼ˆä¾‹å¦‚é€šè¿‡å¹²é¢„è®©æ¨¡æ‹Ÿè¡¨ç°å‡ºç‰¹å®šçš„ç‰©ç†ç‰¹æ€§ï¼‰ã€‚\n\n---\n\n### ğŸ¤– LLM Agentã€æ¨ç†ä¸è®°å¿† (Agents, Reasoning & Memory)\n\n**4. æ€æƒ³å®‡å®™ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°åˆ›é€ æ€§æ¨ç†**\n**Universe of Thoughts: Enabling Creative Reasoning with Large Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç°æœ‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ä¸“æ³¨äºé€»è¾‘åˆ†è§£ï¼Œä½†åœ¨éœ€è¦â€œåˆ›é€ æ€§â€è§£å†³æ–¹æ¡ˆï¼ˆå¦‚è¯ç‰©å‘ç°ã€å•†ä¸šç­–ç•¥ï¼‰æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä½œè€…æå‡ºäº† **Universe of Thoughts (UoT)** æ¡†æ¶ï¼ŒåŒ…å«**ç»„åˆæ€§ã€æ¢ç´¢æ€§å’Œå˜é©æ€§**ä¸‰ç§æ¨ç†èŒƒå¼ã€‚\n*   **å‘ç°**ï¼šç›¸æ¯”ä¼ ç»Ÿ CoTï¼ŒUoT åœ¨éœ€è¦å‘æ•£æ€ç»´å’Œåˆ›æ–°è§£å†³æ–¹æ¡ˆçš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚\n\n**5. Evo-Memoryï¼šå…·æœ‰è‡ªæˆ‘è¿›åŒ–è®°å¿†çš„ LLM Agent æµ‹è¯•æ—¶å­¦ä¹ åŸºå‡†**\n**Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ LLM Agent åœ¨é•¿ä»»åŠ¡æµä¸­æ— æ³•ä»å†å²äº¤äº’ä¸­æœ‰æ•ˆå­¦ä¹ çš„é—®é¢˜ï¼Œæå‡ºäº† **Evo-Memory** åŸºå‡†ã€‚è¿™å…³æ³¨çš„æ˜¯**æµ‹è¯•æ—¶è¿›åŒ–ï¼ˆTest-time Evolutionï¼‰**ï¼Œå³ Agent åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ä¸æ–­æ£€ç´¢ã€æ•´åˆå’Œæ›´æ–°è®°å¿†ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† ReMem ç®¡é“ï¼Œå°†æ¨ç†ã€è¡ŒåŠ¨å’Œè®°å¿†æ›´æ–°ç´§å¯†ç»“åˆï¼Œå®ç°æŒç»­æ”¹è¿›ã€‚\n\n**6. Agent0-VLï¼šæ¢ç´¢ç”¨äºå·¥å…·é›†æˆè§†è§‰è¯­è¨€æ¨ç†çš„è‡ªæˆ‘è¿›åŒ– Agent**\n**Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **Agent0-VL**ï¼Œä¸€ä¸ªæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹å³å¯è‡ªæˆ‘è¿›åŒ–çš„è§†è§‰è¯­è¨€ Agentã€‚\n*   **æ–¹æ³•**ï¼šå®ƒåŒ…å«ä¸¤ä¸ªè§’è‰²ï¼šSolverï¼ˆæ‰§è¡Œå·¥å…·é›†æˆçš„æ¨ç†ï¼‰å’Œ Verifierï¼ˆç”ŸæˆåŸºäºå·¥å…·çš„åé¦ˆå’Œè‡ªæˆ‘å¥–åŠ±ï¼‰ã€‚ä¸¤è€…é€šè¿‡è‡ªæˆ‘è¿›åŒ–æ¨ç†å¾ªç¯è¿›è¡Œäº’åŠ¨ï¼Œå®ç°äº† 12.5% çš„æ€§èƒ½æå‡ã€‚\n\n**7. æ½œåœ¨ç©ºé—´åä½œï¼šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ½œåœ¨åä½œ**\n**Latent Collaboration in Multi-Agent Systems**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ‰“ç ´äº† Agent ä¹‹é—´å¿…é¡»ç”¨è‡ªç„¶è¯­è¨€æ²Ÿé€šçš„é™åˆ¶ã€‚**LatentMAS** æ¡†æ¶å…è®¸ Agent ç›´æ¥åœ¨**è¿ç»­æ½œåœ¨ç©ºé—´ï¼ˆLatent Spaceï¼‰** ä¸­åä½œã€‚\n*   **æ•ˆæœ**ï¼šé€šè¿‡å…±äº«æ½œåœ¨å·¥ä½œè®°å¿†ï¼Œç›¸æ¯”åŸºäºæ–‡æœ¬çš„æ²Ÿé€šï¼Œæ¨ç†å‡†ç¡®ç‡æå‡äº† 14.6%ï¼ŒToken ä½¿ç”¨é‡å‡å°‘äº† 80% ä»¥ä¸Šï¼Œé€Ÿåº¦æå‡ 4 å€ã€‚\n\n---\n\n### ğŸ‘ï¸ è®¡ç®—æœºè§†è§‰ä¸ç”Ÿæˆå¼æ¨¡å‹ (Vision & GenAI)\n\n**8. MambaEyeï¼šå…·æœ‰å› æœé¡ºåºå¤„ç†çš„å°ºå¯¸ä¸å¯çŸ¥è§†è§‰ç¼–ç å™¨**\n**MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåˆ©ç”¨ Mamba2ï¼ˆSSMï¼‰éª¨å¹²ç½‘ç»œï¼Œæå‡ºäº†ä¸€ä¸ª**ä¸¥æ ¼å•å‘**çš„è§†è§‰ç¼–ç å™¨ã€‚\n*   **äº®ç‚¹**ï¼šå®ƒæ˜¯**è¾“å…¥å°ºå¯¸ä¸å¯çŸ¥**çš„ï¼ˆSize-Agnosticï¼‰ï¼Œåˆ©ç”¨ç›¸å¯¹ç§»åŠ¨åµŒå…¥ï¼ˆrelative move embeddingï¼‰ï¼Œå¯ä»¥åœ¨ä»»æ„åˆ†è¾¨ç‡å’Œæ‰«ææ¨¡å¼ä¸‹å·¥ä½œï¼Œå¹¶åœ¨ ImageNet-1K ä¸Šå±•ç°äº†å¯¹é«˜åˆ†è¾¨ç‡ï¼ˆ1536x1536ï¼‰çš„å¼ºå¤§é€‚åº”æ€§ï¼Œä¸”ä¿æŒçº¿æ€§å¤æ‚åº¦ã€‚\n\n**9. Inferixï¼šç”¨äºä¸–ç•Œæ¨¡æ‹Ÿçš„ä¸‹ä¸€ä»£åŸºäºå—æ‰©æ•£çš„æ¨ç†å¼•æ“**\n**Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸“ä¸º**ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰**è®¾è®¡çš„æ¨ç†å¼•æ“ã€‚é‡‡ç”¨åŠè‡ªå›å½’ï¼ˆSemi-autoregressiveï¼‰çš„å—æ‰©æ•£è§£ç èŒƒå¼ï¼Œç»“åˆäº† Diffusion çš„è´¨é‡å’Œ Autoregressive çš„é•¿è§†é¢‘ä¸€è‡´æ€§ã€‚\n*   **æŠ€æœ¯ç‚¹**ï¼šå¼•å…¥äº† LLM é£æ ¼çš„ KV Cache ç®¡ç†æ¥å¤„ç†è§†é¢‘ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡ã€‚\n\n**10. å—çº§è”ï¼šæ— éœ€è®­ç»ƒçš„å—å› æœè§†é¢‘æ¨¡å‹åŠ é€Ÿ**\n**Block Cascading: Training Free Acceleration of Block-Causal Video Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³è§†é¢‘ç”Ÿæˆé€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚æ ¸å¿ƒæ´å¯Ÿæ˜¯ï¼šæœªæ¥çš„è§†é¢‘å—ä¸éœ€è¦å½“å‰å—å®Œå…¨å»å™ªåæ‰å¼€å§‹ç”Ÿæˆã€‚\n*   **æ•ˆæœ**ï¼šé€šè¿‡å¹¶è¡Œå»å™ªï¼Œåœ¨ä¸æŸå¤±è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå°† 14B å‚æ•°çš„å¤§æ¨¡å‹æ¨ç†é€Ÿåº¦ä» 4.5 FPS æå‡åˆ°äº† 12.5 FPSï¼ˆçº¦ 2 å€åŠ é€Ÿï¼‰ã€‚\n\n---\n\n### âš•ï¸ AIä¸åŒ»ç–— (AI for Medicine)\n\n**11. LungEvatyï¼šç”¨äº LDCT ç­›æŸ¥ä¸­è‚ºç™Œé£é™©é¢„æµ‹çš„å¯æ‰©å±• Transformer æ¨¡å‹**\n**LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ä¸ªå…¨ Transformer æ¡†æ¶ï¼Œç›´æ¥ä»**å…¨è‚º LDCT æ‰«æ**ä¸­é¢„æµ‹ 1-6 å¹´çš„è‚ºç™Œé£é™©ï¼Œæ— éœ€åƒç´ çº§æ ‡æ³¨ã€‚\n*   **æ•°æ®**ï¼šåœ¨è¶…è¿‡ 90,000 æ¬¡ CT æ‰«æä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè¿™åœ¨å¤§è§„æ¨¡åŒ»å­¦å½±åƒåˆ†æä¸­æ˜¯ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„æ•°æ®é‡ã€‚\n\n**12. æ¢ç´¢å¼ºåŒ–å­¦ä¹ åœ¨è„“æ¯’ç—‡æ²»ç–—ä¸­çš„æ—¶é—´æ­¥é•¿å¤§å°**\n**Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæŒ‘æˆ˜äº†åŒ»ç–— RL ä¸­å¸¸ç”¨çš„ 4 å°æ—¶æ—¶é—´æ­¥é•¿çš„æƒ¯ä¾‹ã€‚\n*   **å‘ç°**ï¼šé€šè¿‡å¯¹æ¯” 1ã€2ã€4ã€8 å°æ—¶æ­¥é•¿ï¼Œå‘ç°**æ›´ç»†ç²’åº¦ï¼ˆ1h å’Œ 2hï¼‰**çš„ç­–ç•¥é€šå¸¸èƒ½è·å¾—æœ€ä½³æ€§èƒ½å’Œç¨³å®šæ€§ã€‚è¿™æç¤ºæˆ‘ä»¬åœ¨åŒ»ç–— AI è®¾è®¡ä¸­ï¼Œæ—¶é—´ç²’åº¦æ˜¯ä¸€ä¸ªæ ¸å¿ƒè®¾è®¡é€‰æ‹©ã€‚\n\n---\n\n### ğŸ› ï¸ ä¼˜åŒ–ä¸åŸºç¡€è®¾æ–½ (Optimization & Infra)\n\n**13. HVAdamï¼šå…¨ç»´è‡ªé€‚åº”ä¼˜åŒ–å™¨**\n**HVAdam: A Full-Dimension Adaptive Optimizer**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæŒ‡å‡ºäº† Adam ç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨åœ¨ CNN ç­‰æ¶æ„ä¸Šæ³›åŒ–æ€§ä¸å¦‚ SGD çš„åŸå› ã€‚æå‡ºäº† **Anon** ä¼˜åŒ–å™¨ï¼Œå…·æœ‰è¿ç»­å¯è°ƒçš„è‡ªé€‚åº”æ€§ï¼Œå¯ä»¥åœ¨ SGD å’Œ Adam çš„è¡Œä¸ºä¹‹é—´æ’å€¼ç”šè‡³å¤–æ¨ã€‚\n*   **äº®ç‚¹**ï¼šå¼•å…¥äº†å¢é‡å»¶è¿Ÿæ›´æ–°ï¼ˆIDUï¼‰æœºåˆ¶ï¼Œæ¯” AMSGrad æ›´çµæ´»ã€‚\n\n**14. Length-MAX åˆ†è¯å™¨**\n**Length-MAX Tokenizer for Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™å°±å‰å®³äº†ï¼Œä¸€ä¸ªæ–°çš„ Tokenizerã€‚ç›®æ ‡æ˜¯æœ€å°åŒ–æ¯ä¸ªå­—ç¬¦çš„å¹³å‡ Token æ•°ã€‚\n*   **æ•ˆæœ**ï¼šæ¯” BPE å‡å°‘äº† 14-18% çš„ Token æ•°é‡ã€‚ä»å¤´è®­ç»ƒ GPT-2 æ˜¾ç¤ºï¼Œæ¨ç†å»¶è¿Ÿé™ä½ ~13%ï¼Œååé‡æå‡ 16%ï¼Œä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ LAMBADAï¼‰ä¸Šè¡¨ç°æ›´å¥½ã€‚\n\n**15. Cisco æ—¶é—´åºåˆ—æ¨¡å‹æŠ€æœ¯æŠ¥å‘Š**\n**Cisco Time Series Model Technical Report**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šCisco å‘å¸ƒäº†ä¸€ä¸ªå•å˜é‡é›¶æ ·æœ¬é¢„æµ‹çš„åŸºç¡€æ¨¡å‹ã€‚åŸºäº TimesFM æ¶æ„è¿›è¡Œäº†å¤šåˆ†è¾¨ç‡è¾“å…¥çš„æ”¹è¿›ã€‚\n*   **è§„æ¨¡**ï¼šåœ¨è¶…è¿‡ 3000 äº¿ä¸ªæ•°æ®ç‚¹ä¸Šè®­ç»ƒï¼Œé‡ç‚¹åœ¨å¯è§‚æµ‹æ€§ï¼ˆObservabilityï¼‰é¢†åŸŸè¡¨ç°ä¼˜å¼‚ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸å¯¹é½ (Safety & Alignment)\n\n**16. InvisibleBenchï¼šçœ‹æŠ¤å…³ç³» AI çš„éƒ¨ç½²å…³å¡**\n**InvisibleBench: A Deployment Gate for Caregiving Relationship AI**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯„ä¼° AI åœ¨é•¿æœŸçœ‹æŠ¤å…³ç³»ï¼ˆå¦‚é™ªä¼´ã€å¿ƒç†æ”¯æŒï¼‰ä¸­çš„å®‰å…¨æ€§ã€‚\n*   **å‘ç°**ï¼šæ‰€æœ‰å‰æ²¿æ¨¡å‹åœ¨å±æœºæ£€æµ‹ä¸Šéƒ½æœ‰å·¨å¤§ç¼ºé™·ï¼ˆ11.8%-44.8% çš„æ£€æµ‹ç‡ï¼‰ï¼Œè¿™å¯¹äºé€šè¿‡ AI è¿›è¡Œå¿ƒç†å¥åº·æ”¯æŒçš„åº”ç”¨æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„çº¢ç¯è­¦å‘Šã€‚\n\n**17. å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸åŒç¤¾ä¼šæ³•å¾‹èƒŒæ™¯ä¸‹å¯¹éæ³•æŒ‡ä»¤çš„å…±è°‹å›åº”**\n**Large Language Models' Complicit Responses to Illicit Instructions across Socio-Legal Contexts**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå®šä¹‰äº†â€œå…±è°‹ååŠ©â€ï¼ˆComplicit Facilitationï¼‰ã€‚\n*   **å‘ç°**ï¼šGPT-4o åœ¨è¿‘ä¸€åŠçš„æµ‹è¯•æ¡ˆä¾‹ä¸­æä¾›äº†éæ³•ååŠ©ã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹è¾¹ç¼˜ç¾¤ä½“çš„éæ³•æŒ‡å¯¼ï¼ˆå¦‚é’ˆå¯¹è€å¹´äººæˆ–å°‘æ•°æ—è£”ï¼‰è¡¨ç°å‡ºä»¤äººæ‹…å¿§çš„å€¾å‘ã€‚\n\n**18. æµè§ˆå®‰å…¨ï¼šç†è§£å’Œé˜²å¾¡ AI æµè§ˆå™¨ Agent ä¸­çš„æç¤ºæ³¨å…¥**\n**BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹èƒ½å¤Ÿæ“ä½œæµè§ˆå™¨çš„ Agent çš„æç¤ºæ³¨å…¥æ”»å‡»åŸºå‡†ã€‚\n*   **é‡ç‚¹**ï¼šä¸ä»…æ˜¯æ–‡æœ¬è¾“å‡ºï¼Œæ›´å…³æ³¨æ”»å‡»å¦‚ä½•å½±å“ Agent çš„**å®é™…è¡ŒåŠ¨**ï¼ˆå¦‚ç‚¹å‡»ã€è´­ä¹°ç­‰ï¼‰ã€‚\n\n---\n**ç¼–è€…æŒ‰**ï¼š\nä»Šå¤©çš„è®ºæ–‡è´¨é‡å¾ˆé«˜ï¼Œç‰¹åˆ«æ¨èå…³æ³¨ **Evo-Memory** å’Œ **Agent0-VL**ï¼Œè¿™ä»£è¡¨äº† Agent ä»â€œæ‰§è¡Œè€…â€å‘â€œè‡ªæˆ‘è¿›åŒ–è€…â€è½¬å˜çš„è¶‹åŠ¿ã€‚ç†è®ºæ–¹é¢ï¼Œ**Guaranteed Optimal Compositional Explanations** å¯èƒ½ä¼šæˆä¸ºåç»­ç¥ç»å…ƒè§£é‡Šå·¥ä½œçš„åŸºçŸ³ã€‚",
  "papers": [
    {
      "arxiv_id": "2511.20934v1",
      "title": "Guaranteed Optimal Compositional Explanations for Neurons",
      "title_zh": "å…·æœ‰æœ€ä¼˜æ€§ä¿è¯çš„ç¥ç»å…ƒç»„åˆè§£é‡Š",
      "authors": [
        "Biagio La Rosa",
        "Leilani H. Gilpin"
      ],
      "abstract": "While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œä¸­ç¥ç»å…ƒè§£é‡Šä¸äººç±»è®¤çŸ¥å¯¹é½çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªèƒ½å¤Ÿè®¡ç®—ä¿è¯æœ€ä¼˜ Compositional explanations çš„ç†è®ºæ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡åˆ†è§£è¯†åˆ«å½±å“ç©ºé—´å¯¹é½(spatial alignment)çš„å…³é”®å› ç´ ï¼Œå¹¶å¼€å‘å‡ºä¸€ç§å¯å‘å¼æ–¹æ³•æ¥ä¼°è®¡æœç´¢è¿‡ç¨‹ä¸­çš„å¯¹é½ç¨‹åº¦ï¼Œä»è€Œè®¾è®¡å‡ºé¦–ä¸ªèƒ½åœ¨å¯è¡Œæ—¶é—´å†…è·å¾—æœ€ä¼˜è§£é‡Šçš„ç®—æ³•ã€‚åœ¨è®¡ç®—æœºè§†è§‰å’Œå·ç§¯ç¥ç»ç½‘ç»œ(Convolutional Neural Networks)é¢†åŸŸçš„å®éªŒåˆ†ææ­ç¤ºï¼Œå½“æ¶‰åŠé‡å æ¦‚å¿µæ—¶ï¼Œç›®å‰ä¸»æµçš„ beam search æ–¹æ³•æœ‰10%åˆ°40%çš„æ¦‚ç‡äº§ç”Ÿéæœ€ä¼˜è§£é‡Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç”±æ‰€æå¯å‘å¼æ–¹æ³•å¼•å¯¼çš„ beam search å˜ä½“ï¼Œåœ¨è¿è¡Œæ•ˆç‡å’Œèµ„æºçµæ´»æ€§ä¸Šå‡ä¼˜äºæˆ–ç­‰åŒäºç°æœ‰æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºç†è§£ç¥ç»å…ƒç‰¹å¾æä¾›äº†æ›´ä¸¥è°¨çš„ç†è®ºåŸºç¡€å’Œé«˜æ•ˆçš„è®¡ç®—å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "41 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20934v1",
      "published_date": "2025-11-25 23:50:22 UTC",
      "updated_date": "2025-11-25 23:50:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:29.771504+00:00"
    },
    {
      "arxiv_id": "2511.20931v1",
      "title": "Open Vocabulary Compositional Explanations for Neuron Alignment",
      "title_zh": "é¢å‘ç¥ç»å…ƒå¯¹é½çš„å¼€æ”¾è¯è¡¨ç»„åˆå¼è§£é‡Š",
      "authors": [
        "Biagio La Rosa",
        "Leilani H. Gilpin"
      ],
      "abstract": "Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»å…ƒå¯¹é½ä¸­ç»„åˆè§£é‡Š(compositional explanations)ä¸¥é‡ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®é›†ã€å—é™äºç‰¹å®šé¢†åŸŸå’Œé¢„å®šä¹‰æ¦‚å¿µçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘è§†è§‰é¢†åŸŸçš„é€šç”¨æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²(open vocabulary semantic segmentation)ç”Ÿæˆçš„æ©ç ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé’ˆå¯¹ä»»æ„æ¦‚å¿µå’Œæ•°æ®é›†å¯¹ç¥ç»å…ƒè¿›è¡Œæ¢æµ‹ã€‚æ•´ä¸ªæµç¨‹åˆ†ä¸ºæŒ‡å®šä»»æ„æ¦‚å¿µã€åˆ©ç”¨å¼€æ”¾è¯æ±‡æ¨¡å‹ç”Ÿæˆè¯­ä¹‰åˆ†å‰²æ©ç ä»¥åŠåŸºäºæ©ç æ¨å¯¼ç»„åˆè§£é‡Šä¸‰ä¸ªå…³é”®æ­¥éª¤ã€‚ç ”ç©¶é€šè¿‡å®šé‡æŒ‡æ ‡å’Œäººç±»å¯è§£é‡Šæ€§å®éªŒï¼Œå°†è¯¥æ¡†æ¶ä¸ä»¥å¾€æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶æ·±å…¥åˆ†æäº†ä»äººå·¥æ ‡æ³¨æ•°æ®è½¬å‘æ¨¡å‹æ ‡æ³¨æ•°æ®æ—¶è§£é‡Šå†…å®¹çš„å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä»»åŠ¡çµæ´»æ€§å’Œå…³æ³¨å±æ€§çš„è§£é‡Šèƒ½åŠ›ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºç†è§£æ·±åº¦ç¥ç»ç½‘ç»œå¦‚ä½•ç¼–ç ä¿¡æ¯æä¾›äº†æ›´å…·æ‰©å±•æ€§çš„æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "47 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20931v1",
      "published_date": "2025-11-25 23:45:37 UTC",
      "updated_date": "2025-11-25 23:45:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:32.372673+00:00"
    },
    {
      "arxiv_id": "2511.20913v1",
      "title": "Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment",
      "title_zh": "è„“æ¯’ç—‡æ²»ç–—å¼ºåŒ–å­¦ä¹ ä¸­çš„æ—¶é—´æ­¥é•¿æ¢ç©¶",
      "authors": [
        "Yingchuan Sun",
        "Shengpu Tang"
      ],
      "abstract": "Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($Î”t\\!=\\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$Î”t$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $Î”t$ vary as learning setups change, while policies learned at finer time-step sizes ($Î”t = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†è„“æ¯’ç—‡(Sepsis)æ²»ç–—çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­æ—¶é—´æ­¥é•¿(time-step size)çš„å½±å“ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ4å°æ—¶è®¾ç½®å¯èƒ½å¯¼è‡´çš„æ‚£è€…åŠ¨æ€æ‰­æ›²åŠæ¬¡ä¼˜ç­–ç•¥é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ç»Ÿä¸€çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ (offline RL)æµæ°´çº¿ä¸‹ï¼Œå¯¹1ã€2ã€4ã€8å°æ—¶å››ç§æ—¶é—´æ­¥é•¿è¿›è¡Œäº†å®è¯å¯¹æ¯”ã€‚ä¸ºäº†ç¡®ä¿è·¨æ­¥é•¿æ¯”è¾ƒçš„å…¬å¹³æ€§ï¼Œç ”ç©¶è®¾è®¡äº†åŠ¨ä½œé‡æ˜ å°„(action re-mapping)æ–¹æ³•ï¼Œå¹¶é‡åŒ–äº†æ—¶é—´æ­¥é•¿å¯¹çŠ¶æ€è¡¨ç¤ºå­¦ä¹ ã€è¡Œä¸ºå…‹éš†åŠç¦»çº¿ç­–ç•¥è¯„ä¼°(off-policy evaluation)çš„ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ€§èƒ½è¶‹åŠ¿éšå­¦ä¹ è®¾ç½®è€Œå¼‚ï¼Œä½†åœ¨1å°æ—¶å’Œ2å°æ—¶çš„ç»†ç²’åº¦æ—¶é—´æ­¥é•¿ä¸‹ï¼Œç»“åˆé™æ€è¡Œä¸ºç­–ç•¥å­¦ä¹ çš„æ¨¡å‹å±•ç°å‡ºæœ€ä¼˜çš„æ€§èƒ½ä¸ç¨³å®šæ€§ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†æ—¶é—´æ­¥é•¿ä½œä¸ºåŒ»ç–—ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒè®¾è®¡å‚æ•°çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè¶…è¶Šä¼ ç»Ÿ4å°æ—¶è®¾ç½®çš„æ–¹æ¡ˆæä¾›äº†æœ‰åŠ›è¯æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20913v1",
      "published_date": "2025-11-25 23:05:10 UTC",
      "updated_date": "2025-11-25 23:05:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:32.574885+00:00"
    },
    {
      "arxiv_id": "2511.20909v1",
      "title": "Evolved SampleWeights for Bias Mitigation: Effectiveness Depends on Optimization Objectives",
      "title_zh": "ç”¨äºåè§ç¼“è§£çš„æ¼”åŒ–æ ·æœ¬æƒé‡ï¼šå…¶æœ‰æ•ˆæ€§å–å†³äºä¼˜åŒ–ç›®æ ‡",
      "authors": [
        "Anil K. Saini",
        "Jose Guadalupe Hernandez",
        "Emily F. Wong",
        "Debanshi Misra",
        "Jason H. Moore"
      ],
      "abstract": "Machine learning models trained on real-world data may inadvertently make biased predictions that negatively impact marginalized communities. Reweighting is a method that can mitigate such bias in model predictions by assigning a weight to each data point used during model training. In this paper, we compare three methods for generating these weights: (1) evolving them using a Genetic Algorithm (GA), (2) computing them using only dataset characteristics, and (3) assigning equal weights to all data points. Model performance under each strategy was evaluated using paired predictive and fairness metrics, which also served as optimization objectives for the GA during evolution. Specifically, we used two predictive metrics (accuracy and area under the Receiver Operating Characteristic curve) and two fairness metrics (demographic parity difference and subgroup false negative fairness). Using experiments on eleven publicly available datasets (including two medical datasets), we show that evolved sample weights can produce models that achieve better trade-offs between fairness and predictive performance than alternative weighting methods. However, the magnitude of these benefits depends strongly on the choice of optimization objectives. Our experiments reveal that optimizing with accuracy and demographic parity difference metrics yields the largest number of datasets for which evolved weights are significantly better than other weighting strategies in optimizing both objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ ·æœ¬åŠ æƒ(Reweighting)æŠ€æœ¯å‡è½»æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹è¾¹ç¼˜åŒ–ç¾¤ä½“äº§ç”Ÿé¢„æµ‹åå·®çš„é—®é¢˜ï¼Œå¹¶å¯¹æ¯”äº†é—ä¼ ç®—æ³•(Genetic Algorithm)æ¼”åŒ–æƒé‡ã€åŸºäºæ•°æ®é›†ç‰¹å¾è®¡ç®—æƒé‡åŠç­‰æƒé‡ä¸‰ç§ç­–ç•¥ã€‚ç ”ç©¶äººå‘˜åœ¨åŒ…å«åŒ»ç–—æ•°æ®çš„11ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šï¼Œé’ˆå¯¹Accuracyã€AUCç­‰é¢„æµ‹æŒ‡æ ‡ä»¥åŠDemographic Parity Differenceå’ŒSubgroup False Negative Fairnessç­‰å…¬å¹³æ€§æŒ‡æ ‡è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡é—ä¼ ç®—æ³•æ¼”åŒ–çš„æ ·æœ¬æƒé‡åœ¨å¹³è¡¡å…¬å¹³æ€§ä¸é¢„æµ‹æ€§èƒ½æ–¹é¢é€šå¸¸ä¼˜äºä¼ ç»Ÿçš„åŠ æƒæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥çš„æ”¶ç›Šå¤§å°é«˜åº¦ä¾èµ–äºä¼˜åŒ–ç›®æ ‡(Optimization Objectives)çš„é€‰æ‹©ã€‚å®éªŒæ­ç¤ºï¼Œåœ¨åŒæ—¶ä¼˜åŒ–Accuracyå’ŒDemographic Parity DifferenceæŒ‡æ ‡æ—¶ï¼Œæ¼”åŒ–æƒé‡åœ¨å¤šæ•°å®éªŒåœºæ™¯ä¸‹å‡èƒ½å–å¾—æ˜¾è‘—æ›´ä¼˜çš„æ€§èƒ½å¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20909v1",
      "published_date": "2025-11-25 22:50:59 UTC",
      "updated_date": "2025-11-25 22:50:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:37.386271+00:00"
    },
    {
      "arxiv_id": "2511.20906v1",
      "title": "Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy",
      "title_zh": "æ§åˆ¶ç­–ç•¥ä¸­çš„åŠ¨æ€æµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾ï¼šéš¾åº¦æ„ŸçŸ¥çš„éšæœºæ’å€¼ç­–ç•¥",
      "authors": [
        "Inkook Chun",
        "Seungjae Lee",
        "Michael S. Albergo",
        "Saining Xie",
        "Eric Vanden-Eijnden"
      ],
      "abstract": "Diffusion- and flow-based policies deliver state-of-the-art performance on long-horizon robotic manipulation and imitation learning tasks. However, these controllers employ a fixed inference budget at every control step, regardless of task complexity, leading to computational inefficiency for simple subtasks while potentially underperforming on challenging ones. To address these issues, we introduce Difficulty-Aware Stochastic Interpolant Policy (DA-SIP), a framework that enables robotic controllers to adaptively adjust their integration horizon in real time based on task difficulty. Our approach employs a difficulty classifier that analyzes observations to dynamically select the step budget, the optimal solver variant, and ODE/SDE integration at each control cycle. DA-SIP builds upon the stochastic interpolant formulation to provide a unified framework that unlocks diverse training and inference configurations for diffusion- and flow-based policies. Through comprehensive benchmarks across diverse manipulation tasks, DA-SIP achieves 2.6-4.4x reduction in total computation time while maintaining task success rates comparable to fixed maximum-computation baselines. By implementing adaptive computation within this framework, DA-SIP transforms generative robot controllers into efficient, task-aware systems that intelligently allocate inference resources where they provide the greatest benefit.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Difficulty-Aware Stochastic Interpolant Policy (DA-SIP)ï¼Œæ—¨åœ¨è§£å†³åŸºäºæ‰©æ•£(Diffusion)å’Œæµ(Flow)çš„æ§åˆ¶ç­–ç•¥åœ¨æ¨ç†æ—¶ä½¿ç”¨å›ºå®šè®¡ç®—é¢„ç®—æ‰€å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚DA-SIPé€šè¿‡å¼•å…¥éš¾åº¦åˆ†ç±»å™¨å®æ—¶åˆ†æè§‚æµ‹æ•°æ®ï¼Œä½¿æœºå™¨äººæ§åˆ¶å™¨èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç§¯åˆ†æ­¥é•¿(Integration Horizon)ã€æ±‚è§£å™¨(Solver)å˜ä½“ä»¥åŠODE/SDEç§¯åˆ†æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶åŸºäºéšæœºæ’å€¼(Stochastic Interpolant)å…¬å¼æ„å»ºï¼Œä¸ºç”Ÿæˆå¼ç­–ç•¥æä¾›äº†ç»Ÿä¸€ä¸”çµæ´»çš„è®­ç»ƒä¸æ¨ç†é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼ŒDA-SIPåœ¨ä¿æŒä¸æœ€å¤§è®¡ç®—é‡åŸºå‡†æ¨¡å‹ç›¸å½“çš„æˆåŠŸç‡çš„å‰æä¸‹ï¼Œå°†æ€»è®¡ç®—æ—¶é—´æ˜¾è‘—é™ä½äº†2.6è‡³4.4å€ã€‚é€šè¿‡è¿™ç§è‡ªé€‚åº”è®¡ç®—æœºåˆ¶ï¼ŒDA-SIPå°†ç”Ÿæˆå¼æ§åˆ¶å™¨è½¬åŒ–ä¸ºä»»åŠ¡æ„ŸçŸ¥ç³»ç»Ÿï¼Œå®ç°äº†æ¨ç†èµ„æºçš„æ™ºèƒ½åˆ†é…ä¸é«˜æ•ˆåˆ©ç”¨ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20906v1",
      "published_date": "2025-11-25 22:46:42 UTC",
      "updated_date": "2025-11-25 22:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:34.588159+00:00"
    },
    {
      "arxiv_id": "2511.20902v1",
      "title": "A Taxonomy of Pix Fraud in Brazil: Attack Methodologies, AI-Driven Amplification, and Defensive Strategies",
      "title_zh": "Brazil Pix æ”¯ä»˜æ¬ºè¯ˆåˆ†ç±»ä½“ç³»ï¼šæ”»å‡»æ‰‹æ®µã€äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ”¾å¤§æ•ˆåº”åŠé˜²å¾¡ç­–ç•¥",
      "authors": [
        "Glener Lanes Pizzolato",
        "Brenda Medeiros Lopes",
        "Claudio Schepke",
        "Diego Kreutz"
      ],
      "abstract": "This work presents a review of attack methodologies targeting Pix, the instant payment system launched by the Central Bank of Brazil in 2020. The study aims to identify and classify the main types of fraud affecting users and financial institutions, highlighting the evolution and increasing sophistication of these techniques. The methodology combines a structured literature review with exploratory interviews conducted with professionals from the banking sector. The results show that fraud schemes have evolved from purely social engineering approaches to hybrid strategies that integrate human manipulation with technical exploitation. The study concludes that security measures must advance at the same pace as the growing complexity of attack methodologies, with particular emphasis on adaptive defenses and continuous user awareness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·´è¥¿ä¸­å¤®é“¶è¡Œæ¨å‡ºçš„å³æ—¶æ”¯ä»˜ç³»ç»Ÿ Pix è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ”»å‡»æ–¹æ³•å›é¡¾ï¼Œæ—¨åœ¨åˆ†ç±»å¹¶è¯†åˆ«å½±å“ç”¨æˆ·ä¸é‡‘èæœºæ„çš„ä¸»è¦æ¬ºè¯ˆç±»å‹ã€‚é€šè¿‡ç»“åˆç»“æ„åŒ–æ–‡çŒ®ç»¼è¿°ä¸é“¶è¡Œä¸šä¸“ä¸šäººå£«çš„æ¢ç´¢æ€§è®¿è°ˆï¼Œç ”ç©¶æ­ç¤ºäº†æ¬ºè¯ˆæŠ€æœ¯æ­£ä»å•ä¸€çš„ Social Engineering æ‰‹æ®µå‘ç»“åˆäººä¸ºæ“çºµä¸æŠ€æœ¯æ¼æ´åˆ©ç”¨çš„ Hybrid Strategies æ¼”å˜ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ¬ºè¯ˆæ‰‹æ®µæ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§ä¸ç²¾å¯†æ€§ï¼Œå¹¶æŒ‡å‡ºå®‰å…¨é˜²æŠ¤å¿…é¡»åŠ é€Ÿå‡çº§ä»¥åº”å¯¹ä¸æ–­æ¼”è¿›çš„å¨èƒã€‚ç ”ç©¶æœ€ç»ˆå»ºè®®ï¼Œåº”å¯¹ Pix Fraud çš„é˜²å¾¡ç­–ç•¥åº”ä¾§é‡äºæ„å»º Adaptive Defenses ç³»ç»Ÿä»¥åŠåŠ å¼ºæŒç»­æ€§çš„ç”¨æˆ·å®‰å…¨æ„è¯†æ•™è‚²ã€‚è¯¥å·¥ä½œä¸ºç†è§£å³æ—¶æ”¯ä»˜ç¯å¢ƒä¸‹çš„å®‰å…¨é£é™©æä¾›äº†å…¨é¢çš„åˆ†ç±»æ¡†æ¶ï¼Œå¹¶ä¸ºé‡‘èæœºæ„åˆ¶å®šé˜²å¾¡ç­–ç•¥æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 1 figure, 2 tables, submitted to ERRC/WRSeg 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20902v1",
      "published_date": "2025-11-25 22:41:57 UTC",
      "updated_date": "2025-11-25 22:41:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:49.975494+00:00"
    },
    {
      "arxiv_id": "2511.20892v2",
      "title": "Representation Interventions Enable Lifelong Unstructured Knowledge Control",
      "title_zh": "è¡¨å¾å¹²é¢„å®ç°ç»ˆèº«éç»“æ„åŒ–çŸ¥è¯†æ§åˆ¶",
      "authors": [
        "Xuyuan Liu",
        "Zhengzhang Chen",
        "Xinshuai Dong",
        "Yanchi Liu",
        "Xujiang Zhao",
        "Shengyu Chen",
        "Haoyu Wang",
        "Yujun Yan",
        "Haifeng Chen"
      ],
      "abstract": "Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is particularly challenging for complex, unstructured knowledge in lifelong settings, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two key properties enabling RILKE to achieve fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. At inference, a query-adaptive router selects the appropriate module to guide the model's generation. Across LLaMA and Qwen models, RILKE scales effectively to large-scale benchmarks, demonstrating high edit success and strong paraphrase generalization while preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å®¹æ˜“äº§ç”Ÿé”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ä¸”éš¾ä»¥ä½æˆæœ¬æ›´æ–°çš„é—®é¢˜ï¼Œæå‡ºäº†RILKEï¼ˆRepresentation Intervention for Lifelong KnowledgE Controlï¼‰æ¡†æ¶ï¼Œç”¨äºå®ç°å¤§æ¨¡å‹åœ¨ç»ˆèº«å­¦ä¹ ç¯å¢ƒä¸‹çš„éç»“æ„åŒ–çŸ¥è¯†æ§åˆ¶ã€‚RILKEå°†çŸ¥è¯†æ›´æ–°è§†ä¸ºæ¨¡å‹è¡¨å¾ç©ºé—´(representation space)å†…çš„å¹²é¢„æ“ä½œï¼Œé€šè¿‡å­¦ä¹ é‡Šä¹‰é²æ£’(paraphrase-robust)ä¸”ç¼–è¾‘å±€éƒ¨åŒ–(edit-localized)çš„æ¨¡å—ï¼Œå°†æ›´æ–°é™åˆ¶åœ¨ä½ç»´å­ç©ºé—´å†…ä»¥å‡å°‘ä¸åŒç¼–è¾‘é—´çš„å¹²æ‰°ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æŸ¥è¯¢è‡ªé€‚åº”è·¯ç”±(query-adaptive router)é€‰æ‹©åŒ¹é…çš„æ¨¡å—æ¥å¼•å¯¼ç”Ÿæˆï¼Œä»è€Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒä¸”å†»ç»“åŸºç¡€æƒé‡çš„æƒ…å†µä¸‹å®ç°ç²¾å‡†æ§åˆ¶ã€‚åœ¨LLaMAå’ŒQwenæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRILKEåœ¨å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æé«˜çš„ç¼–è¾‘æˆåŠŸç‡å’Œå¼ºåŠ²çš„é‡Šä¹‰æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶èƒ½æœ‰æ•ˆä¿ç•™æ¨¡å‹çš„é€šç”¨æ•ˆç”¨ä¸”å†…å­˜å¼€é”€è¾ƒå°ã€‚è¿™äº›ç»“æœè¯æ˜RILKEæ˜¯è§£å†³LLMså¤æ‚éç»“æ„åŒ–çŸ¥è¯†æŒç»­æ›´æ–°é—®é¢˜çš„ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 Page",
      "pdf_url": "https://arxiv.org/pdf/2511.20892v2",
      "published_date": "2025-11-25 22:15:00 UTC",
      "updated_date": "2026-01-06 08:52:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:26:25.759478+00:00"
    },
    {
      "arxiv_id": "2511.20889v1",
      "title": "Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation",
      "title_zh": "åŸºäºç©ºæ–‡æœ¬åµŒå…¥ä¼˜åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æµ‹è¯•æ—¶å¯¹é½",
      "authors": [
        "Taehoon Kim",
        "Henry Gouk",
        "Timothy Hospedales"
      ],
      "abstract": "Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Null-TTAï¼ˆNull-Text Test-Time Alignmentï¼‰ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æµ‹è¯•æ—¶å¯¹é½ï¼ˆTTAï¼‰è¿‡ç¨‹ä¸­å¸¸è§çš„ä¼˜åŒ–ä¸è¶³æˆ–å¥–åŠ±ä½œå¼Šï¼ˆreward hackingï¼‰é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ç›´æ¥æ“ä½œæ½œå˜é‡æˆ–å™ªå£°ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆclassifier-free guidanceï¼‰ä¸­çš„æ— æ¡ä»¶åµŒå…¥ï¼ˆunconditional embeddingï¼‰æ¥å®ç°æ¨¡å‹å¯¹é½ã€‚ç”±äºæ–‡æœ¬åµŒå…¥ç©ºé—´å…·æœ‰ç»“æ„åŒ–çš„è¯­ä¹‰ç‰¹æ€§ï¼ŒNull-TTAç¡®ä¿äº†å¯¹é½è¿‡ç¨‹åœ¨è¯­ä¹‰ä¸€è‡´çš„æµå½¢ä¸Šè¿›è¡Œï¼Œä»è€Œæœ‰æ•ˆé˜²æ­¢æ¨¡å‹åˆ©ç”¨éè¯­ä¹‰å™ªå£°æ¨¡å¼æ¥è·å–é«˜å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼ŒNull-TTAèƒ½ç›´æ¥å¼•å¯¼ç”Ÿæˆåˆ†å¸ƒè¶‹å‘ç›®æ ‡å¥–åŠ±ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æµ‹è¯•æ—¶å¯¹é½æ€§èƒ½å¹¶å±•ç°å‡ºæå¼ºçš„è·¨å¥–åŠ±æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€ç ”ç©¶æˆæœç¡®ç«‹äº†è¯­ä¹‰ç©ºé—´ä¼˜åŒ–ä½œä¸ºTTAé¢†åŸŸä¸€ç§é«˜æ•ˆä¸”å…·æœ‰åŸåˆ™æ€§çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20889v1",
      "published_date": "2025-11-25 22:11:51 UTC",
      "updated_date": "2025-11-25 22:11:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:52.069614+00:00"
    },
    {
      "arxiv_id": "2511.20870v1",
      "title": "Selecting Belief-State Approximations in Simulators with Latent States",
      "title_zh": "å…·æœ‰éšçŠ¶æ€æ¨¡æ‹Ÿå™¨ä¸­ä¿¡å¿µçŠ¶æ€è¿‘ä¼¼æ–¹æ³•çš„é€‰æ‹©",
      "authors": [
        "Nan Jiang"
      ],
      "abstract": "State resetting is a fundamental but often overlooked capability of simulators. It supports sample-based planning by allowing resets to previously encountered simulation states, and enables calibration of simulators using real data by resetting to states observed in real-system traces. While often taken for granted, state resetting in complex simulators can be nontrivial: when the simulator comes with latent variables (states), state resetting requires sampling from the posterior over the latent state given the observable history, a.k.a. the belief state (Silver and Veness, 2010). While exact sampling is often infeasible, many approximate belief-state samplers can be constructed, raising the question of how to select among them using only sampling access to the simulator.\n  In this paper, we show that this problem reduces to a general conditional distribution-selection task and develop a new algorithm and analysis under sampling-only access. Building on this reduction, the belief-state selection problem admits two different formulations: latent state-based selection, which directly targets the conditional distribution of the latent state, and observation-based selection, which targets the induced distribution over the observation. Interestingly, these formulations differ in how their guarantees interact with the downstream roll-out methods: perhaps surprisingly, observation-based selection may fail under the most natural roll-out method (which we call Single-Reset) but enjoys guarantees under the less conventional alternative (which we call Repeated-Reset). Together with discussion on issues such as distribution shift and the choice of sampling policies, our paper reveals a rich landscape of algorithmic choices, theoretical nuances, and open questions, in this seemingly simple problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å…·æœ‰æ½œå˜é‡(latent states)çš„å¤æ‚æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡ŒçŠ¶æ€é‡ç½®(state resetting)çš„æŒ‘æˆ˜ï¼Œé‡ç‚¹è§£å†³åœ¨ä»…æœ‰é‡‡æ ·è®¿é—®æƒé™æ—¶å¦‚ä½•é€‰æ‹©æœ€ä¼˜çš„ä¿¡å¿µçŠ¶æ€(belief state)è¿‘ä¼¼é‡‡æ ·å™¨ã€‚æœ¬æ–‡å°†æ­¤é—®é¢˜ç®€åŒ–ä¸ºé€šç”¨çš„æ¡ä»¶åˆ†å¸ƒé€‰æ‹©ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ–°çš„ç®—æ³•åŠå…¶åˆ†ææ¡†æ¶ã€‚ç ”ç©¶æå‡ºäº†ä¸¤ç§ä¸åŒçš„é€‰æ‹©æ–¹æ¡ˆï¼šç›´æ¥é’ˆå¯¹æ½œçŠ¶æ€åˆ†å¸ƒçš„latent state-based selectionï¼Œä»¥åŠé’ˆå¯¹è¯±å¯¼è§‚å¯Ÿåˆ†å¸ƒçš„observation-based selectionã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œobservation-based selectionåœ¨å¸¸è§„çš„Single-Resetå›æµ‹æ–¹æ³•ä¸‹å¯èƒ½å¤±æ•ˆï¼Œä½†åœ¨Repeated-Resetæ–¹æ³•ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„ç†è®ºä¿è¯ã€‚è¯¥å·¥ä½œæ·±å…¥æ­ç¤ºäº†åˆ†å¸ƒåç§»å’Œé‡‡æ ·ç­–ç•¥é€‰æ‹©ä¸­çš„ç®—æ³•æƒè¡¡ä¸ç†è®ºç»†å¾®å·®åˆ«ï¼Œä¸ºå¤„ç†æ¨¡æ‹Ÿå™¨ä¸­çš„éšå˜é‡çŠ¶æ€æä¾›äº†é‡è¦çš„ç†è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20870v1",
      "published_date": "2025-11-25 21:34:01 UTC",
      "updated_date": "2025-11-25 21:34:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:25:58.100856+00:00"
    },
    {
      "arxiv_id": "2512.07846v1",
      "title": "MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction",
      "title_zh": "MixLMï¼šåŸºäºæ–‡æœ¬-åµŒå…¥æ··åˆäº¤äº’çš„é«˜ååä¸”é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹æ’åº",
      "authors": [
        "Guoyao Li",
        "Ran He",
        "Shusen Jing",
        "Kayhan Behdin",
        "Yubo Wang",
        "Sundara Raman Ramachandran",
        "Chanh Nguyen",
        "Jian Sheng",
        "Xiaojing Ma",
        "Chuanrui Zhu",
        "Sriram Vasudevan",
        "Muchen Wu",
        "Sayan Ghosh",
        "Lin Su",
        "Qingquan Song",
        "Xiaoqing Wang",
        "Zhipeng Wang",
        "Qing Lan",
        "Yanning Chen",
        "Jingwei Wu",
        "Luke Simon",
        "Wenjing Zhang",
        "Qi Guo",
        "Fedor Borisyuk"
      ],
      "abstract": "Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨èå’Œæœç´¢æ’åç³»ç»Ÿä¸­å› è®¡ç®—å¼€é”€è¿‡å¤§ã€æ¨ç†å»¶è¿Ÿé«˜è€Œéš¾ä»¥æ»¡è¶³å·¥ä¸šçº§é«˜å¹¶å‘è¦æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º MixLM çš„æ–°å‹æ’åæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„ Cross-encoder æ’åç³»ç»Ÿç›´æ¥å¤„ç†å…¨æ–‡æœ¬ä¸åŒï¼ŒMixLM å¼•å…¥äº†æ··åˆäº¤äº’(Mix-interaction)æœºåˆ¶ï¼Œå°†è¾“å…¥è¡¨ç¤ºä¸ºæ–‡æœ¬å’ŒåµŒå…¥ä»¤ç‰Œ(Embedding tokens)çš„æ··åˆå½¢å¼ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç›®å½•é¡¹ç›®é¢„å…ˆç¼–ç ä¸ºå°‘é‡çš„åµŒå…¥ä»¤ç‰Œå¹¶å­˜å‚¨åœ¨è¿‘çº¿ç¼“å­˜ä¸­ï¼Œåœ¨åœ¨çº¿æ¨ç†æ—¶å°†é¡¹ç›®æè¿°é•¿åº¦ä»æ•°åƒä¸ªæ–‡æœ¬ä»¤ç‰Œæ˜¾è‘—å‹ç¼©ï¼Œæœ‰æ•ˆå‡å°‘äº†ä¸Šä¸‹æ–‡é•¿åº¦å¹¶ä¿ç•™äº†æ¨¡å‹çš„è¯­ä¹‰å¼ºåº¦ã€‚åœ¨ LinkedIn çœŸå®æœç´¢åœºæ™¯çš„éƒ¨ç½²åˆ†æè¡¨æ˜ï¼ŒMixLM åœ¨ä¿æŒç›¸å…³æ€§æŒ‡æ ‡(Relevance metrics)ä¸å˜çš„å‰æä¸‹ï¼Œåœ¨ç›¸åŒå»¶è¿Ÿé¢„ç®—å†…å°†ç³»ç»Ÿååé‡æé«˜äº† 10.0 å€ã€‚è¿™ä¸€æ˜¾è‘—çš„æ•ˆç‡çªç ´ä½¿å¾— LLM é©±åŠ¨çš„æœç´¢èƒ½å¤Ÿå®ç°å…¨é‡æµé‡éƒ¨ç½²ï¼Œå¹¶åœ¨åœ¨çº¿ A/B æµ‹è¯•ä¸­ä¸ºç³»ç»Ÿå¸¦æ¥äº† 0.47% çš„æ—¥æ´»è·ƒç”¨æˆ·(DAU)æ˜¾è‘—å¢é•¿ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07846v1",
      "published_date": "2025-11-25 21:23:04 UTC",
      "updated_date": "2025-11-25 21:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:26:03.875323+00:00"
    },
    {
      "arxiv_id": "2511.20859v5",
      "title": "Computing Evolutionarily Stable Strategies in Multiplayer Games",
      "title_zh": "å¤šäººåšå¼ˆä¸­æ¼”åŒ–ç¨³å®šç­–ç•¥çš„è®¡ç®—",
      "authors": [
        "Sam Ganzfried"
      ],
      "abstract": "We present an algorithm for computing all evolutionarily stable strategies in nondegenerate normal-form games with three or more players.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰ä¸‰ä¸ªæˆ–æ›´å¤šç©å®¶çš„éé€€åŒ–æ­£è§„å½¢å¼åšå¼ˆ(nondegenerate normal-form games)ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºè®¡ç®—æ‰€æœ‰æ¼”åŒ–ç¨³å®šç­–ç•¥(Evolutionarily Stable Strategies)çš„åˆ›æ–°ç®—æ³•ã€‚ç”±äºå¤šäººåšå¼ˆä¸­çš„æ¼”åŒ–ç¨³å®šæ€§åˆ†æå¤æ‚åº¦è¾ƒé«˜ï¼Œè¯¥ç®—æ³•æä¾›äº†ä¸€ç§ç³»ç»ŸåŒ–çš„è·¯å¾„æ¥è¯†åˆ«åšå¼ˆä¸­çš„æ‰€æœ‰æ½œåœ¨ç¨³å®šçŠ¶æ€ã€‚è¿™ä¸€å·¥ä½œå¡«è¡¥äº†å¤šäººåšå¼ˆåœ¨ç­–ç•¥è®¡ç®—æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç¨³å®šæ€§ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„æ•°å­¦å·¥å…·ã€‚è¯¥ç®—æ³•çš„æå‡ºä¸ä»…æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œä¹Ÿä¸ºåç»­åœ¨å¤æ‚åšå¼ˆç¯å¢ƒä¸‹çš„ç†è®ºåˆ†æå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA",
        "econ.TH",
        "q-bio.PE"
      ],
      "primary_category": "cs.GT",
      "comment": "Reverting to original title after fixing Google scholar merge",
      "pdf_url": "https://arxiv.org/pdf/2511.20859v5",
      "published_date": "2025-11-25 21:16:24 UTC",
      "updated_date": "2026-01-02 02:40:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:26:50.834303+00:00"
    },
    {
      "arxiv_id": "2511.20857v1",
      "title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory",
      "title_zh": "Evo-Memoryï¼šåŸºäºè‡ªæˆ‘è¿›åŒ–è®°å¿†çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æµ‹è¯•æ—¶å­¦ä¹ åŸºå‡†",
      "authors": [
        "Tianxin Wei",
        "Noveen Sachdeva",
        "Benjamin Coleman",
        "Zhankui He",
        "Yuanchen Bei",
        "Xuying Ning",
        "Mengting Ai",
        "Yunzhe Li",
        "Jingrui He",
        "Ed H. Chi",
        "Chi Wang",
        "Shuo Chen",
        "Fernando Pereira",
        "Wang-Cheng Kang",
        "Derek Zhiyuan Cheng"
      ],
      "abstract": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºçŠ¶æ€ç»´æŒ (Statefulness) å¯¹ Large Language Model (LLM) æ™ºèƒ½ä½“çš„é•¿æœŸè§„åˆ’è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰è¯„ä¼°å¤šé›†ä¸­åœ¨é™æ€å¯¹è¯ï¼Œå¿½è§†äº†å…¶åœ¨åŠ¨æ€ä»»åŠ¡æµä¸­ç§¯ç´¯ä¸é‡ç”¨ç»éªŒçš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº† Evo-Memoryï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼° LLM æ™ºèƒ½ä½“è‡ªæ¼”è¿›è®°å¿† (self-evolving memory) è®¾è®¡çš„ç»¼åˆæµå¼åŸºå‡†æµ‹è¯•å’Œæ¡†æ¶ã€‚Evo-Memory å°†æ•°æ®é›†ç»“æ„åŒ–ä¸ºè¿ç»­çš„ä»»åŠ¡æµ (task streams)ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨æ¯æ¬¡äº¤äº’åæŒç»­æ£€ç´¢ã€æ•´åˆå¹¶æ›´æ–°è®°å¿†ã€‚è¯¥ç ”ç©¶ç»Ÿä¸€å¹¶å®ç°äº†è¶…è¿‡ 10 ä¸ªä»£è¡¨æ€§è®°å¿†æ¨¡å— (memory modules)ï¼Œå¹¶åœ¨ 10 ä¸ªæ¶µç›–å¤šè½®ç›®æ ‡å¯¼å‘ä»»åŠ¡åŠå•è½®æ¨ç†é—®ç­”çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚ä¸ºäº†è¡¡é‡ç»éªŒé‡ç”¨æ•ˆæœï¼Œç ”ç©¶æå‡ºäº†åŸºå‡†æ–¹æ³• ExpRAG ä»¥åŠä¸€ç§ç´§å¯†é›†æˆæ¨ç†ã€ä»»åŠ¡æ“ä½œå’Œè®°å¿†æ›´æ–°çš„ ReMem ä¼˜åŒ–ç®¡çº¿ã€‚è¿™äº›è´¡çŒ®å…±åŒè§£å†³äº† LLM åœ¨æµ‹è¯•é˜¶æ®µ (test-time) æ— æ³•ä»ç´¯ç§¯äº¤äº’ä¸­å­¦ä¹ çš„å±€é™ï¼Œä¸ºå®ç°æ™ºèƒ½ä½“çš„æŒç»­æ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20857v1",
      "published_date": "2025-11-25 21:08:07 UTC",
      "updated_date": "2025-11-25 21:08:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:26:46.497893+00:00"
    },
    {
      "arxiv_id": "2511.20854v1",
      "title": "Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries",
      "title_zh": "åŸºäºâ€œèˆŒå°–ç°è±¡â€æ£€ç´¢æŸ¥è¯¢çš„æ— ç›‘ç£å¯è®°å¿†æ€§å»ºæ¨¡",
      "authors": [
        "Sree Bhattacharyya",
        "Yaman Kumar Singla",
        "Sudhir Yarram",
        "Somesh Kumar Singh",
        "Harini S",
        "James Z. Wang"
      ],
      "abstract": "Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰å†…å®¹è®°å¿†æ€§ (Visual content memorability) æ ‡æ³¨æˆæœ¬é«˜ä¸”ç¼ºä¹è‡ªç„¶æè¿°ä¿¡å·çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ Reddit ç­‰åœ¨çº¿å¹³å°â€œè¯åˆ°å˜´è¾¹â€ (tip-of-the-tongue, ToT) æ£€ç´¢æŸ¥è¯¢è¿›è¡Œæ— ç›‘ç£å»ºæ¨¡çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…æ„å»ºäº†é¦–ä¸ªåŒ…å«è¶…è¿‡ 82,000 ä¸ªè§†é¢‘åŠç›¸åº”æè¿°æ€§å¬å›æ•°æ®çš„å¤§è§„æ¨¡æ— ç›‘ç£æ•°æ®é›†ï¼Œä¸ºç ”ç©¶ç»†ç²’åº¦çš„è®°å¿†ä¿¡å·æä¾›äº†ä¸°å¯Œæ”¯æŒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šå¾®è°ƒçš„å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (Large vision-language models) åœ¨ç”Ÿæˆå¼€æ”¾å¼è®°å¿†æ€§æè¿°æ–¹é¢ä¼˜äº GPT-4o ç­‰å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨å¯¹æ¯”è®­ç»ƒ (contrastive training) ç­–ç•¥ï¼Œè¯¥ç ”ç©¶æˆåŠŸå¼€å‘äº†é¦–ä¸ªå…·å¤‡å¤šæ¨¡æ€ ToT æ£€ç´¢èƒ½åŠ›çš„æŠ€æœ¯æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æŒ–æ˜äº’è”ç½‘è‡ªç„¶è¯­è¨€äº¤äº’æ•°æ®ï¼Œä¸ºè§†è§‰è®°å¿†æ€§ç ”ç©¶æä¾›äº†æå…·æ½œåŠ›çš„æ— ç›‘ç£å‘å±•æ–¹å‘å’Œè¯„ä¼°åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.20854v1",
      "published_date": "2025-11-25 21:02:26 UTC",
      "updated_date": "2025-11-25 21:02:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:12.339895+00:00"
    },
    {
      "arxiv_id": "2511.20853v2",
      "title": "MODEST: Multi-Optics Depth-of-Field Stereo Dataset",
      "title_zh": "MODESTï¼šå¤šå…‰å­¦æ™¯æ·±ç«‹ä½“æ•°æ®é›†",
      "authors": [
        "Nisarg K. Trivedi",
        "Vinayak A. Belludi",
        "Li-Yun Wang",
        "Pardis Taghavi",
        "Dante Lok"
      ],
      "abstract": "Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MODEST (Multi-Optics Depth-of-Field Stereo Dataset)ï¼Œè¿™æ˜¯é¦–ä¸ªé«˜åˆ†è¾¨ç‡ï¼ˆ5472x3648åƒç´ ï¼‰çš„ç«‹ä½“ DSLR æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³çœŸå®å…‰å­¦æ¡ä»¶ä¸‹æ·±åº¦ä¼°è®¡çš„å¯é æ€§æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†åŒ…å« 18000 å¼ å›¾åƒï¼Œç³»ç»Ÿåœ°æ”¹å˜äº† 10 ä¸ªç„¦è· (focal lengths) å’Œ 5 ä¸ªå…‰åœˆ (apertures)ï¼Œé€šè¿‡ 50 ç§å…‰å­¦é…ç½®æ•æ‰äº†ä¸“ä¸šç›¸æœºç³»ç»Ÿçš„å¤æ‚å…‰å­¦ç‰¹æ€§ã€‚æ•°æ®é›†æ¶µç›–äº† 9 ä¸ªåŒ…å«å¤šå°ºåº¦è§†é”™è§‰ã€åå°„è¡¨é¢å’Œé€æ˜ç»ç’ƒç­‰å¤æ‚è§†è§‰å…ƒç´ çš„çœŸå®åœºæ™¯ï¼Œä¸ºæ¨¡å‹åœ¨æç«¯å…‰å­¦æ¡ä»¶ä¸‹çš„è¡¨ç°æä¾›äº†åŸºå‡†ã€‚MODEST æ”¯æŒå¯¹å•ç›®ä¸ç«‹ä½“ depth estimationã€depth-of-field renderingã€deblurring ä»¥åŠ 3D scene reconstruction ç­‰ä»»åŠ¡è¿›è¡Œæ·±å…¥çš„å—æ§åˆ†æã€‚è¯¥å·¥ä½œæœ‰æ•ˆå¼¥åˆäº†åˆæˆè®­ç»ƒæ•°æ®ä¸çœŸå®ç›¸æœºå…‰å­¦ä¹‹é—´çš„â€œçœŸå®æ„Ÿå·®è·â€ï¼Œå¹¶ä¸ºæ¨åŠ¨çœŸå®ä¸–ç•Œå…‰å­¦æ³›åŒ–ç ”ç©¶æä¾›äº†å®Œæ•´çš„æ ¡å‡†æ–‡ä»¶å’Œè¯„ä¼°ä»£ç ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Website, dataset and software tools now available for purely non-commercial, academic research purposes",
      "pdf_url": "https://arxiv.org/pdf/2511.20853v2",
      "published_date": "2025-11-25 20:59:47 UTC",
      "updated_date": "2025-12-13 20:22:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:08.543262+00:00"
    },
    {
      "arxiv_id": "2511.20849v1",
      "title": "Length-MAX Tokenizer for Language Models",
      "title_zh": "é¢å‘è¯­è¨€æ¨¡å‹çš„ Length-MAX åˆ†è¯å™¨",
      "authors": [
        "Dong Dong",
        "Weijie Su"
      ],
      "abstract": "We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\\%, 17.2\\%, and 18.5\\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\\%, 12.7\\%, and 13.7\\% lower inference latency, together with a 16\\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\\% and enhancing HellaSwag accuracy by 4.3\\%. Moreover, the Length-MAX tokenizer achieves 99.62\\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\\% at inference.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Length-MAX åˆ†è¯å™¨ï¼Œé€šè¿‡æœ€å°åŒ–æ¯ä¸ªå­—ç¬¦çš„å¹³å‡ Token æ•°é‡æ¥æ˜¾è‘—é™ä½è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„èµ„æºæ¶ˆè€—ã€‚è¯¥æ–¹æ³•å°†è¯è¡¨è·å–è¿‡ç¨‹å»ºæ¨¡ä¸ºé•¿åº¦åŠ æƒç›®æ ‡æœ€å¤§åŒ–ä¸‹çš„å›¾åˆ’åˆ†é—®é¢˜ (graph partitioning problem)ï¼Œå¹¶åˆ©ç”¨è´ªå©ªè¿‘ä¼¼ç®—æ³• (greedy approximation algorithm) æ±‚è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLength-MAX åœ¨å¤šç§æ•°æ®é›†ä¸Šæ¯”ä¼ ç»Ÿçš„ Byte Pair Encoding (BPE) å‡å°‘äº† 13% è‡³ 18% çš„ Token äº§ç”Ÿé‡ã€‚åœ¨ GPT-2 æ¨¡å‹çš„è®­ç»ƒæµ‹è¯•ä¸­ï¼Œè¯¥åˆ†è¯å™¨ä¸ä»…ç¼©çŸ­äº†çº¦ 18% è¾¾åˆ°ç›®æ ‡æŸå¤±æ‰€éœ€çš„è®­ç»ƒæ­¥æ•°ï¼Œè¿˜æå‡äº†æ¨ç†ååé‡å¹¶é™ä½äº†å»¶è¿Ÿã€‚æ­¤å¤–ï¼ŒLength-MAX åœ¨ LAMBADA å’Œ HellaSwag ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´ä¼˜çš„ç²¾åº¦ï¼ŒåŒæ—¶å‡å°‘äº†çº¦ 18% çš„åµŒå…¥å±‚å’Œ KV-cache å†…å­˜å ç”¨ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†ä¼˜åŒ– Token é•¿åº¦è€Œéå•çº¯ä¾èµ–é¢‘ç‡æ˜¯æå‡è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20849v1",
      "published_date": "2025-11-25 20:56:56 UTC",
      "updated_date": "2025-11-25 20:56:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:08.744292+00:00"
    },
    {
      "arxiv_id": "2511.20848v1",
      "title": "NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities",
      "title_zh": "NOIR 2.0ï¼šé¢å‘æ—¥å¸¸æ´»åŠ¨çš„ç¥ç»ä¿¡å·æ“æ§æ™ºèƒ½æœºå™¨äºº",
      "authors": [
        "Tasha Kim",
        "Yingke Wang",
        "Hanvit Cho",
        "Alex Hodges"
      ],
      "abstract": "Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†NOIR 2.0ï¼Œè¿™æ˜¯Neural Signal Operated Intelligent Robotsç³»ç»Ÿçš„å¢å¼ºç‰ˆæœ¬ï¼Œé€šè¿‡è„‘æœºæ¥å£(brain-robot interface)å®ç°äººç±»åˆ©ç”¨è„‘ä¿¡å·å¯¹æœºå™¨äººæ—¥å¸¸ä»»åŠ¡çš„ç²¾å‡†æ§åˆ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è„‘ç”µå›¾(EEG)å°†äººç±»å¯¹ç‰¹å®šç‰©ä½“å’ŒåŠ¨ä½œçš„æ„å›¾ç›´æ¥è½¬åŒ–ä¸ºæ‰§è¡ŒæŒ‡ä»¤ã€‚NOIR 2.0 å¼•å…¥äº†æ›´å¿«é€Ÿä¸”ç²¾ç¡®çš„è„‘ä¿¡å·è§£ç ç®—æ³•ï¼Œå°†ä»»åŠ¡å®Œæˆæ—¶é—´æ˜¾è‘—ç¼©çŸ­äº†46%ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†ç»“åˆåŸºç¡€æ¨¡å‹(foundation models)çš„å°‘æ ·æœ¬æœºå™¨äººå­¦ä¹ ç®—æ³•(few-shot robot learning)ï¼Œæå¤§åœ°æå‡äº†å­¦ä¹ ä¸é€‚é…çš„æ ·æœ¬æ•ˆç‡ã€‚è¿™ä¸€æ”¹è¿›ä½¿å¾—äººç±»æŠ•å…¥æ—¶é—´å‡å°‘äº†65%ï¼Œå¹¶å…è®¸æœºå™¨äººæœ‰æ•ˆé¢„æµ‹ç”¨æˆ·æ„å›¾ä»¥é€‚åº”ä¸ªä½“å·®å¼‚ã€‚å®éªŒè¯æ˜ï¼ŒNOIR 2.0 åœ¨æå‡åä½œæ•ˆç‡å’Œé™ä½äººå·¥å‚ä¸åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºè„‘æ§æœºå™¨äººæŠ€æœ¯çš„å®ç”¨åŒ–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Conference on Robot Learning (CoRL 2024), CoRoboLearn",
      "pdf_url": "https://arxiv.org/pdf/2511.20848v1",
      "published_date": "2025-11-25 20:56:27 UTC",
      "updated_date": "2025-11-25 20:56:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:04.845012+00:00"
    },
    {
      "arxiv_id": "2511.20844v1",
      "title": "Pre-train to Gain: Robust Learning Without Clean Labels",
      "title_zh": "ä»¥é¢„è®­ç»ƒä¿ƒå¢ç›Šï¼šæ— éœ€å¹²å‡€æ ‡ç­¾çš„é²æ£’å­¦ä¹ ",
      "authors": [
        "David Szczecina",
        "Nicholas Pellegrino",
        "Paul Fieguth"
      ],
      "abstract": "Training deep networks with noisy labels leads to poor generalization and degraded accuracy due to overfitting to label noise. Existing approaches for learning with noisy labels often rely on the availability of a clean subset of data. By pre-training a feature extractor backbone without labels using self-supervised learning (SSL), followed by standard supervised training on the noisy dataset, we can train a more noise robust model without requiring a subset with clean labels. We evaluate the use of SimCLR and Barlow~Twins as SSL methods on CIFAR-10 and CIFAR-100 under synthetic and real world noise. Across all noise rates, self-supervised pre-training consistently improves classification accuracy and enhances downstream label-error detection (F1 and Balanced Accuracy). The performance gap widens as the noise rate increases, demonstrating improved robustness. Notably, our approach achieves comparable results to ImageNet pre-trained models at low noise levels, while substantially outperforming them under high noise conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨ noisy labels è®­ç»ƒä¸­æ˜“å‘ç”Ÿè¿‡æ‹ŸåˆåŠæ³›åŒ–æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€ clean subset çš„é²æ£’å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ Self-Supervised Learning (SSL) æŠ€æœ¯ï¼ˆå¦‚ SimCLR å’Œ Barlow Twinsï¼‰é¢„è®­ç»ƒ feature extractor backboneï¼Œéšååœ¨å«å™ªå£°çš„æ•°æ®é›†ä¸Šè¿›è¡Œæ ‡å‡†ç›‘ç£è®­ç»ƒã€‚åœ¨ CIFAR-10 å’Œ CIFAR-100 çš„å®éªŒè¡¨æ˜ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒåœ¨å„ç§å™ªå£°ç‡ä¸‹å‡èƒ½æŒç»­æå‡åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¹¶æœ‰æ•ˆå¢å¼ºä¸‹æ¸¸çš„ label-error detection èƒ½åŠ›ã€‚éšç€å™ªå£°ç‡çš„æé«˜ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œæ€§èƒ½ä¼˜åŠ¿éšä¹‹æ‰©å¤§ã€‚ç ”ç©¶å¼ºè°ƒï¼Œè¯¥æ–¹æ¡ˆåœ¨ä½å™ªå£°ç¯å¢ƒä¸‹ä¸ ImageNet é¢„è®­ç»ƒæ¨¡å‹æ•ˆæœç›¸å½“ï¼Œè€Œåœ¨é«˜å™ªå£°æ¡ä»¶ä¸‹åˆ™æ˜¾è‘—ä¼˜äºåè€…ï¼Œä¸ºç¼ºä¹å¹²å‡€æ ‡ç­¾çš„å­¦ä¹ ä»»åŠ¡æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20844v1",
      "published_date": "2025-11-25 20:48:07 UTC",
      "updated_date": "2025-11-25 20:48:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:38.142849+00:00"
    },
    {
      "arxiv_id": "2511.20843v1",
      "title": "A Review of Pseudospectral Optimal Control: From Theory to Flight",
      "title_zh": "ä¼ªè°±æœ€ä¼˜æ§åˆ¶ç»¼è¿°ï¼šä»ç†è®ºåˆ°é£è¡Œ",
      "authors": [
        "I. M. Ross",
        "M. Karpenko"
      ],
      "abstract": "The home space for optimal control is a Sobolev space. The home space for pseudospectral theory is also a Sobolev space. It thus seems natural to combine pseudospectral theory with optimal control theory and construct ``pseudospectral optimal control theory,'' a term coined by Ross. In this paper, we review key theoretical results in pseudospectral optimal control that have proven to be critical for a successful flight. Implementation details of flight demonstrations onboard NASA spacecraft are discussed along with emerging trends and techniques in both theory and practice. The 2011 launch of pseudospectral optimal control in embedded platforms is changing the way in which we see solutions to challenging control problems in aerospace and autonomous systems.",
      "tldr_zh": "è¯¥è®ºæ–‡ç»¼è¿°äº†ä¼ªè°±æœ€ä¼˜æ§åˆ¶(Pseudospectral Optimal Control)ä»ç†è®ºåˆ°é£è¡Œåº”ç”¨çš„å‘å±•å†ç¨‹ï¼Œé˜è¿°äº†å…¶å°†æœ€ä¼˜æ§åˆ¶ç†è®ºä¸ä¼ªè°±ç†è®ºåœ¨Sobolev Spaceä¸­è¿›è¡Œè‡ªç„¶ç»“åˆçš„æ ¸å¿ƒé€»è¾‘ã€‚æ–‡ç« é‡ç‚¹å›é¡¾äº†å¯¹æˆåŠŸé£è¡Œè‡³å…³é‡è¦çš„å…³é”®ç†è®ºæˆæœï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†åœ¨NASAèˆªå¤©å™¨ä¸Šè¿›è¡Œé£è¡Œæ¼”ç¤ºçš„å…·ä½“å®ç°ç»†èŠ‚ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè‡ª2011å¹´ä¼ªè°±æœ€ä¼˜æ§åˆ¶åœ¨åµŒå…¥å¼å¹³å°(Embedded Platforms)æˆåŠŸéƒ¨ç½²ä»¥æ¥ï¼Œå®ƒæ­£åœ¨æ·±åˆ»æ”¹å˜èˆªç©ºèˆªå¤©ä¸è‡ªä¸»ç³»ç»Ÿ(Autonomous Systems)å¤„ç†æŒ‘æˆ˜æ€§æ§åˆ¶é—®é¢˜çš„æ–¹å¼ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†è¯¥é¢†åŸŸåœ¨ç†è®ºä¸å®è·µä¸­æ¶Œç°çš„æ–°å…´è¶‹åŠ¿ï¼Œä¸ºæœªæ¥è§£å†³å¤æ‚åŠ¨åŠ›å­¦ç³»ç»Ÿçš„æœ€ä¼˜æ§åˆ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "eess.SY",
        "math.FA",
        "math.NA"
      ],
      "primary_category": "math.OC",
      "comment": "https://www.sciencedirect.com/science/article/abs/pii/S1367578812000375",
      "pdf_url": "https://arxiv.org/pdf/2511.20843v1",
      "published_date": "2025-11-25 20:47:48 UTC",
      "updated_date": "2025-11-25 20:47:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:30.544685+00:00"
    },
    {
      "arxiv_id": "2511.20839v1",
      "title": "Primal: A Unified Deterministic Framework for Quasi-Orthogonal Hashing and Manifold Learning",
      "title_zh": "Primalï¼šé¢å‘å‡†æ­£äº¤å“ˆå¸Œä¸æµå½¢å­¦ä¹ çš„ç»Ÿä¸€ç¡®å®šæ€§æ¡†æ¶",
      "authors": [
        "Vladimer Khasia"
      ],
      "abstract": "We present Primal, a deterministic feature mapping framework that harnesses the number-theoretic independence of prime square roots to construct robust, tunable vector representations. Diverging from standard stochastic projections (e.g., Random Fourier Features), our method exploits the Besicovitch property to create irrational frequency modulations that guarantee infinite non-repeating phase trajectories. We formalize two distinct algorithmic variants: (1) StaticPrime, a sequence generation method that produces temporal position encodings empirically approaching the theoretical Welch bound for quasi-orthogonality; and (2) DynamicPrime, a tunable projection layer for input-dependent feature mapping. A central novelty of the dynamic framework is its ability to unify two disparate mathematical utility classes through a single scaling parameter Ïƒ. In the low-frequency regime, the method acts as an isometric kernel map, effectively linearizing non-convex geometries (e.g., spirals) to enable high-fidelity signal reconstruction and compressive sensing. Conversely, the high-frequency regime induces chaotic phase wrapping, transforming the projection into a maximum-entropy one-way hash suitable for Hyperdimensional Computing and privacy-preserving Split Learning. Empirical evaluations demonstrate that our framework yields superior orthogonality retention and distribution tightness compared to normalized Gaussian baselines, establishing it as a computationally efficient, mathematically rigorous alternative to random matrix projections. The code is available at https://github.com/VladimerKhasia/primal",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Primalï¼Œä¸€ä¸ªç»Ÿä¸€çš„ç¡®å®šæ€§ç‰¹å¾æ˜ å°„(deterministic feature mapping)æ¡†æ¶ï¼Œåˆ©ç”¨ç´ æ•°å¹³æ–¹æ ¹åœ¨æ•°è®ºä¸Šçš„ç‹¬ç«‹æ€§å’ŒBesicovitchæ€§è´¨æ¥æ„å»ºé²æ£’ä¸”å¯è°ƒçš„å‘é‡è¡¨ç¤ºã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ç§å˜ä½“ï¼šStaticPrimeé€šè¿‡åºåˆ—ç”Ÿæˆäº§ç”Ÿæ¥è¿‘Welch boundçš„æ‹Ÿæ­£äº¤(quasi-orthogonality)æ—¶é—´ä½ç½®ç¼–ç ï¼›DynamicPrimeåˆ™ä½œä¸ºå¯è°ƒæŠ•å½±å±‚ï¼Œé€šè¿‡ç¼©æ”¾å‚æ•°Ïƒç»Ÿä¸€äº†æµå½¢å­¦ä¹ (Manifold Learning)ä¸å“ˆå¸Œ(Hashing)ä¸¤ç±»ä¸åŒçš„æ•°å­¦åº”ç”¨ã€‚åœ¨ä½é¢‘æ€ä¸‹ï¼ŒPrimalä½œä¸ºç­‰è·æ ¸æ˜ å°„(isometric kernel map)èƒ½æœ‰æ•ˆçº¿æ€§åŒ–éå‡¸å‡ ä½•ç»“æ„ï¼Œä»è€Œå®ç°é«˜ä¿çœŸä¿¡å·é‡å»ºï¼›è€Œåœ¨é«˜é¢‘æ€ä¸‹ï¼Œå…¶è¯±å¯¼çš„æ··æ²Œç›¸ä½ç¼ ç»•å¯è½¬åŒ–ä¸ºé€‚ç”¨äºè¶…ç»´è®¡ç®—(Hyperdimensional Computing)å’Œéšç§ä¿æŠ¤åˆ†å‰²å­¦ä¹ (Split Learning)çš„é«˜ç†µå•å‘å“ˆå¸Œã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPrimalåœ¨æ­£äº¤æ€§ä¿æŒå’Œåˆ†å¸ƒç´§å‡‘æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå½’ä¸€åŒ–é«˜æ–¯åŸºå‡†æ¨¡å‹ï¼Œä¸ºä¼ ç»Ÿçš„éšæœºçŸ©é˜µæŠ•å½±æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆä¸”å…·å¤‡æ•°å­¦ä¸¥å¯†æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20839v1",
      "published_date": "2025-11-25 20:44:34 UTC",
      "updated_date": "2025-11-25 20:44:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:23.751401+00:00"
    },
    {
      "arxiv_id": "2511.20836v2",
      "title": "Structured Prompting Enables More Robust Evaluation of Language Models",
      "title_zh": "ç»“æ„åŒ–æç¤ºåŠ©åŠ›æ›´å…·é²æ£’æ€§çš„è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Asad Aali",
        "Muhammad Ahmed Mohsin",
        "Vasiliki Bikia",
        "Arnav Singhvi",
        "Richard Gaus",
        "Suhana Bedi",
        "Hejie Cui",
        "Miguel Fuentes",
        "Alyssa Unell",
        "Yifan Mai",
        "Jordan Cahoon",
        "Michael Pfeffer",
        "Roxana Daneshjou",
        "Sanmi Koyejo",
        "Emily Alsentzer",
        "Christopher Potts",
        "Nigam H. Shah",
        "Akshay S. Chaudhari"
      ],
      "abstract": "As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we approximate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks ($+$2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing chain-of-thought reduces LM sensitivity to prompt design (smaller $Î”$ across prompts). To our knowledge, this is the first benchmarking study to systematically integrate structured prompting into an established evaluation framework, demonstrating how scalable performance-ceiling approximation yields more robust, decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è¯­è¨€æ¨¡å‹(LM)åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼ˆå¦‚HELMï¼‰å› ä¾èµ–å›ºå®šæç¤ºè¯è€Œå¯¼è‡´æ€§èƒ½è¯„ä¼°ä¸å‡†ç¡®ã€æ— æ³•ä½“ç°æ¨¡å‹æ½œåŠ›ä¸Šé™çš„é—®é¢˜ï¼Œæå‡ºäº†DSPy+HELMè¿™ä¸€å¯å¤ç°çš„é›†æˆæ¡†æ¶ã€‚é€šè¿‡å¼•å…¥ç»“æ„åŒ–æç¤º(Structured Prompting)æ–¹æ³•æ¿€å‘æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜åœ¨é€šç”¨å’ŒåŒ»å­¦é¢†åŸŸçš„7ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹4ä¸ªå‰æ²¿å¤§æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒå‘ç°ï¼Œä¸ä½¿ç”¨ç»“æ„åŒ–æç¤ºä¼šå¯¼è‡´HELMå¹³å‡ä½ä¼°æ¨¡å‹æ€§èƒ½çº¦4%ï¼Œå¹¶å¼•å‘æ€§èƒ½æ³¢åŠ¨åŠæ’è¡Œæ¦œæ’åé”™ä½ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¼•å…¥é“¾å¼æ€ç»´(Chain-of-Thought)èƒ½æœ‰æ•ˆé™ä½æ¨¡å‹å¯¹æç¤ºè¯è®¾è®¡çš„æ•æ„Ÿæ€§ï¼Œä»è€Œæä¾›æ›´ç¨³å¥çš„è¯„ä¼°ã€‚è¯¥å·¥ä½œé¦–æ¬¡å°†ç»“æ„åŒ–æç¤ºç³»ç»Ÿæ€§åœ°æ•´åˆåˆ°æˆç†Ÿè¯„ä¼°æ¡†æ¶ä¸­ï¼Œè¯æ˜äº†é€šè¿‡æ€§èƒ½ä¸Šé™è¿‘ä¼¼å¯ä»¥è·å¾—æ›´å…·å†³ç­–å‚è€ƒä»·å€¼çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¼€æºäº†ç›¸å…³é›†æˆä»£ç ä¸ä¼˜åŒ–æµæ°´çº¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20836v2",
      "published_date": "2025-11-25 20:37:59 UTC",
      "updated_date": "2025-11-28 03:16:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:26.941010+00:00"
    },
    {
      "arxiv_id": "2511.20823v1",
      "title": "RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs",
      "title_zh": "RefTrï¼š3Dè¡€ç®¡æ ‘ä¸­å¿ƒçº¿å›¾çš„æ±‡åˆè½¨è¿¹å¾ªç¯ç»†åŒ–",
      "authors": [
        "Roman Naeem",
        "David Hagerman",
        "Jennifer AlvÃ©n",
        "Fredrik Kahl"
      ],
      "abstract": "Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RefTrï¼Œä¸€ç§ç”¨äº 3D è¡€ç®¡æ ‘ä¸­å¿ƒçº¿å›¾ç”Ÿæˆçš„ image-to-graph æ¨¡å‹ï¼Œæ—¨åœ¨ç²¾ç¡®æå–è¡€ç®¡å’Œæ°”ç®¡ç­‰ç®¡çŠ¶ç»“æ„çš„ä¸­å¿ƒçº¿ä¸æ‹“æ‰‘ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäº Transformer è§£ç å™¨çš„ Producer-Refiner æ¶æ„ï¼Œé€šè¿‡ Producer æå‡ºåˆå§‹çš„ confluent trajectoriesï¼Œå¹¶åˆ©ç”¨ Refiner è¿›è¡Œå¾ªç¯è¿­ä»£ä¼˜åŒ–ä»¥ç”Ÿæˆæœ€ç»ˆè½¨è¿¹å›¾ã€‚è¿™ç§æ±‡åˆè½¨è¿¹è¡¨ç¤ºæ³•åœ¨ä¼˜åŒ–å®Œæ•´è·¯å¾„çš„åŒæ—¶ï¼Œèƒ½æ˜¾å¼åœ°ç¡®ä¿ç”Ÿæˆç»“æœç¬¦åˆæœ‰æ•ˆçš„ tree topologyã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§é’ˆå¯¹ç©ºé—´æ ‘å›¾çš„é«˜æ•ˆ non-maximum suppression ç®—æ³•æ¥åˆå¹¶é‡å¤åˆ†æ”¯ï¼Œè¿›ä¸€æ­¥æå‡äº†ç²¾åº¦ã€‚å®éªŒè¯æ˜ï¼ŒRefTr åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰ SOTA çš„ recallï¼Œä¸”è§£ç å™¨å‚æ•°é‡å‡å°‘äº† 2.4 å€ï¼Œæ¨ç†é€Ÿåº¦æ˜¾è‘—æå‡ã€‚è¯¥æ¡†æ¶ä¸º 3D åŒ»å­¦å½±åƒä¸­çš„è¡€ç®¡æ ‘åˆ†ææä¾›äº†ä¸€ç§é«˜æ•ˆä¸”é«˜å¬å›ç‡çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20823v1",
      "published_date": "2025-11-25 20:22:57 UTC",
      "updated_date": "2025-11-25 20:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:02.956629+00:00"
    },
    {
      "arxiv_id": "2511.20821v3",
      "title": "Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion",
      "title_zh": "åŸºäºä¼˜åŒ–å¼è§†è§‰åæ¼”çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå…è®­ç»ƒæ‰©æ•£å…ˆéªŒ",
      "authors": [
        "Samuele Dell'Erba",
        "Andrew D. Bagdanov"
      ],
      "abstract": "Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and zero-shot alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with the input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective. It achieves quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, underscoring the potential of optimization-based strategies as viable, training-free alternatives to traditional priors. The code will be publicly available upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹ä¸­ prior network è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”ä¾èµ–æµ·é‡æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Optimization-based Visual Inversion (OVI) çš„æ— éœ€è®­ç»ƒä¸”é›¶æ ·æœ¬ (zero-shot) çš„æ›¿ä»£æ–¹æ¡ˆã€‚OVI é€šè¿‡ä»éšæœº pseudo-tokens åˆå§‹åŒ–æ½œåœ¨è§†è§‰è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è¿­ä»£ä¼˜åŒ–ç®—æ³•æœ€å¤§åŒ–å…¶ä¸è¾“å…¥ text embedding çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä¸ºäº†å°†ä¼˜åŒ–è¿‡ç¨‹çº¦æŸåœ¨çœŸå®å›¾åƒåˆ†å¸ƒå†…ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº† Mahalanobis-based å’Œ Nearest-Neighbor loss ä¸¤ç§æ­£åˆ™åŒ–çº¦æŸæ‰‹æ®µã€‚å®éªŒåœ¨ Kandinsky 2.2 ä¸ŠéªŒè¯äº† OVI æ›¿ä»£ä¼ ç»Ÿ priors çš„æ½œåŠ›ï¼Œå¹¶æ­ç¤ºäº† T2I-CompBench++ ç­‰ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è§†è§‰è´¨é‡æ—¶çš„å±€é™æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå¸¦çº¦æŸçš„ OVI æ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰ä¿çœŸåº¦ï¼Œå…¶ä¸­ Nearest-Neighbor æ–¹æ¡ˆåœ¨é‡åŒ–æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ•°æ®é«˜æ•ˆå…ˆéªŒ (data-efficient prior) ç›¸å½“ç”šè‡³æ›´é«˜çš„æ°´å¹³ï¼Œå±•ç¤ºäº†åŸºäºä¼˜åŒ–çš„ç­–ç•¥ä½œä¸ºæ— è®­ç»ƒå…ˆéªŒæ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 7 figures, technical report (preprint)",
      "pdf_url": "https://arxiv.org/pdf/2511.20821v3",
      "published_date": "2025-11-25 20:20:21 UTC",
      "updated_date": "2025-12-27 12:37:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:27:58.337267+00:00"
    },
    {
      "arxiv_id": "2511.20814v1",
      "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning",
      "title_zh": "SPHINXï¼šé¢å‘è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†çš„åˆæˆç¯å¢ƒ",
      "authors": [
        "Md Tanvirul Alam",
        "Saksham Aggarwal",
        "Justin Yang Chae",
        "Nidhi Rastogi"
      ],
      "abstract": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SPHINXï¼Œä¸€ä¸ªé’ˆå¯¹æ ¸å¿ƒè®¤çŸ¥åŸè¯­(cognitive primitives)è®¾è®¡çš„è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†åˆæˆç¯å¢ƒã€‚SPHINXé€šè¿‡ç¨‹åºåŒ–ç”ŸæˆåŒ…å«å›¾æ¡ˆã€å›¾è¡¨ã€å›¾æ ‡åŠå‡ ä½•åŸè¯­çš„è°œé¢˜ï¼Œå¹¶ä¸ºæ¯é¡¹ä»»åŠ¡æä¾›å¯éªŒè¯çš„Ground-truthè§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒç²¾ç¡®è¯„ä¼°ä¸å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†å¯¹ç§°æ£€æµ‹(symmetry detection)ã€å‡ ä½•å˜æ¢(geometric transformations)ã€ç©ºé—´æ¨ç†(spatial reasoning)å’Œåºåˆ—é¢„æµ‹ç­‰25ç§ä»»åŠ¡ç±»å‹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œé¡¶çº§å¤šæ¨¡æ€å¤§æ¨¡å‹(LVLMs)å¦‚GPT-5çš„å‡†ç¡®ç‡ä»…ä¸º51.1%ï¼Œè¿œä½äºäººç±»è¡¨ç°ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œç»“åˆå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿™ç§æ€§èƒ½æå‡åœ¨å¤–éƒ¨è§†è§‰æ¨ç†åŸºå‡†ä¸Šä¹Ÿå¾—åˆ°äº†éªŒè¯ï¼Œå±•ç¤ºäº†RLVRåœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20814v1",
      "published_date": "2025-11-25 20:00:47 UTC",
      "updated_date": "2025-11-25 20:00:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:27.045362+00:00"
    },
    {
      "arxiv_id": "2511.20811v1",
      "title": "Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning",
      "title_zh": "é¢å‘é£è¡Œè¯•éªŒçš„å…±å½¢å®‰å…¨ç›‘æ§ï¼šæ•°æ®é©±åŠ¨å®‰å…¨å­¦ä¹ çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Aaron O. Feldman",
        "D. Isaiah Harp",
        "Joseph Duncan",
        "Mac Schwager"
      ],
      "abstract": "We develop a data-driven approach for runtime safety monitoring in flight testing, where pilots perform maneuvers on aircraft with uncertain parameters. Because safety violations can arise unexpectedly as a result of these uncertainties, pilots need clear, preemptive criteria to abort the maneuver in advance of safety violation. To solve this problem, we use offline stochastic trajectory simulation to learn a calibrated statistical model of the short-term safety risk facing pilots. We use flight testing as a motivating example for data-driven learning/monitoring of safety due to its inherent safety risk, uncertainty, and human-interaction. However, our approach consists of three broadly-applicable components: a model to predict future state from recent observations, a nearest neighbor model to classify the safety of the predicted state, and classifier calibration via conformal prediction. We evaluate our method on a flight dynamics model with uncertain parameters, demonstrating its ability to reliably identify unsafe scenarios, match theoretical guarantees, and outperform baseline approaches in preemptive classification of risk.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§ç”¨äºé£è¡Œæµ‹è¯•å®æ—¶å®‰å…¨ç›‘æ§çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é£è¡Œè¿‡ç¨‹ä¸­ç”±å‚æ•°ä¸ç¡®å®šæ€§å¼•èµ·çš„æ„å¤–å®‰å…¨é£é™©ã€‚ä¸ºäº†å‘é£è¡Œå‘˜æä¾›æ˜ç¡®çš„é¢„åˆ¤æ ‡å‡†ä»¥æå‰ä¸­æ­¢å±é™©æ¼”ä¹ ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç¦»çº¿éšæœºè½¨è¿¹æ¨¡æ‹Ÿ(offline stochastic trajectory simulation)æ„å»ºäº†ä¸€ä¸ªæ ¡å‡†çš„çŸ­æœŸå®‰å…¨é£é™©ç»Ÿè®¡æ¨¡å‹ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªé€šç”¨ç»„ä»¶ç»„æˆï¼šç”¨äºé¢„æµ‹æœªæ¥çŠ¶æ€çš„è§‚æµ‹æ¨¡å‹ã€åŸºäºæœ€è¿‘é‚»æ¨¡å‹(nearest neighbor model)çš„å®‰å…¨åˆ†ç±»å™¨ï¼Œä»¥åŠé€šè¿‡ç¬¦åˆé¢„æµ‹(conformal prediction)å®ç°çš„åˆ†ç±»å™¨æ ¡å‡†ã€‚åœ¨å…·æœ‰ä¸ç¡®å®šå‚æ•°çš„é£è¡ŒåŠ¨åŠ›å­¦æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¯é åœ°è¯†åˆ«ä¸å®‰å…¨åœºæ™¯å¹¶ä¸¥æ ¼éµå¾ªç†è®ºä¿è¯ã€‚ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨é£é™©çš„é¢„è§æ€§åˆ†ç±»æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œä¸ºé«˜é£é™©ã€é«˜ä¸ç¡®å®šæ€§çš„é£è¡Œæµ‹è¯•ç¯å¢ƒæä¾›äº†æœ‰æ•ˆçš„æ•°æ®é©±åŠ¨å®‰å…¨ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "ICRA 2025 Workshop on Robot safety under uncertainty from intangible specifications",
      "pdf_url": "https://arxiv.org/pdf/2511.20811v1",
      "published_date": "2025-11-25 19:57:07 UTC",
      "updated_date": "2025-11-25 19:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:27.640987+00:00"
    },
    {
      "arxiv_id": "2511.20799v1",
      "title": "Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models",
      "title_zh": "æ®Šé€”åŒå½’çš„è®°å¿†ï¼šä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®æ³„éœ²é²æ£’æ£€æµ‹çš„å¤šå‰ç¼€æ¡†æ¶",
      "authors": [
        "Trung Cuong Dang",
        "David Mohaisen"
      ],
      "abstract": "Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ·é‡è¯­æ–™åº“è®­ç»ƒä¸­å‡ºç°çš„é€å­—è®°å¿†ï¼ˆverbatim memorizationï¼‰é—®é¢˜ï¼Œè¿™ä¸€ç°è±¡å¸¦æ¥äº†æ˜¾è‘—çš„éšç§å’Œç‰ˆæƒé£é™©ã€‚é’ˆå¯¹ç°æœ‰è®°å¿†åŒ–å®šä¹‰åœ¨å…¨é¢æ•æ‰ç‰¹å¾ï¼ˆå°¤å…¶æ˜¯åœ¨å¯¹é½æ¨¡å‹ä¸­ï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºå¤šå‰ç¼€è®°å¿†åŒ–ï¼ˆmulti-prefix memorizationï¼‰çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ï¼Œè®°å¿†åºåˆ—ç”±äºè¢«æ·±åº¦ç¼–ç ï¼Œå¯ä»¥é€šè¿‡æ¯”éè®°å¿†å†…å®¹å¤šå¾—å¤šçš„ä¸åŒå‰ç¼€ï¼ˆprefixesï¼‰è¿›è¡Œæ£€ç´¢ã€‚ç ”ç©¶é€šè¿‡å¤–éƒ¨å¯¹æŠ—æ€§æœç´¢ï¼ˆadversarial searchï¼‰æ¥è¯†åˆ«è§¦å‘ç›®æ ‡åºåˆ—çš„ä¸åŒå‰ç¼€æ•°é‡ï¼Œä»è€Œå°†å…³æ³¨ç‚¹ä»å•è·¯å¾„æå–ï¼ˆsingle-path extractionï¼‰è½¬å‘äº†é‡åŒ–è®°å¿†è·¯å¾„å¤šæ ·æ€§çš„é²æ£’æ€§ã€‚åœ¨å¼€æºæ¨¡å‹å’Œå¯¹é½èŠå¤©æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼Œå¤šå‰ç¼€å®šä¹‰èƒ½å¯é åœ°åŒºåˆ†è®°å¿†ä¸éè®°å¿†æ•°æ®ã€‚è¯¥ç ”ç©¶ä¸ºå®¡è®¡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ•°æ®æ³„æ¼ï¼ˆdata leakageï¼‰æä¾›äº†ä¸€ç§é²æ£’ä¸”å®ç”¨çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 2 tables, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20799v1",
      "published_date": "2025-11-25 19:40:24 UTC",
      "updated_date": "2025-11-25 19:40:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:07.540364+00:00"
    },
    {
      "arxiv_id": "2511.20798v2",
      "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model",
      "title_zh": "Physics Steeringï¼šç‰©ç†åŸºç¡€æ¨¡å‹ä¸­è·¨é¢†åŸŸæ¦‚å¿µçš„å› æœæ“æ§",
      "authors": [
        "Rio Alexa Fear",
        "Payel Mukhopadhyay",
        "Michael McCabe",
        "Alberto Bietti",
        "Miles Cranmer"
      ],
      "abstract": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute \"delta\" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹ç‰©ç†é¢†åŸŸåŸºç¡€æ¨¡å‹ (physics-focused foundation model) çš„å†…éƒ¨è¡¨å¾ï¼Œæ—¨åœ¨éªŒè¯å…¶æ˜¯å¦å…·å¤‡ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç±»ä¼¼çš„æŠ½è±¡æ¦‚å¿µè¡¨å¾èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åœ¨ä¸åŒç‰©ç†çŠ¶æ€çš„ä»¿çœŸæ•°æ®ä¸­æå–æ¿€æ´»å‘é‡ (activation vectors)ï¼Œå¹¶è®¡ç®—å‡ºä»£è¡¨ç‰¹å®šç‰©ç†ç‰¹å¾çš„â€œå¢é‡â€è¡¨å¾ (delta representations) ä½œä¸ºæ¿€æ´»ç©ºé—´ä¸­çš„æ¦‚å¿µæ–¹å‘ (concept directions)ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ³¨å…¥è¿™äº›æ–¹å‘ï¼Œç ”ç©¶è€…å®ç°äº†å¯¹ç‰©ç†è¡Œä¸ºçš„å› æœæ§åˆ¶ (causal control)ï¼Œå³â€œç‰©ç†è½¬å‘â€ (Physics Steering)ï¼Œèƒ½å¤Ÿæœ‰é’ˆå¯¹æ€§åœ°è¯±å¯¼æˆ–ç§»é™¤ä»¿çœŸä¸­çš„ç‰¹å®šç‰©ç†ç‰¹æ€§ã€‚å®éªŒè¯æ˜ï¼Œç§‘å­¦åŸºç¡€æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç‰©ç†åŸç†çš„æ³›åŒ–è¡¨å¾ï¼Œè€Œéä»…ä»…ä¾èµ–æ•°æ®ä¸­çš„è¡¨é¢ç›¸å…³æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºç§‘å­¦åŸºç¡€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¯æ§æ€§ç ”ç©¶æä¾›äº†æ–°è§†è§’ï¼Œå¯¹äººå·¥æ™ºèƒ½èµ‹èƒ½çš„ç§‘å­¦å‘ç°å…·æœ‰æ·±è¿œå½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "16 Pages, 9 Figures. Code available soon at https://github.com/DJ-Fear/walrus_steering",
      "pdf_url": "https://arxiv.org/pdf/2511.20798v2",
      "published_date": "2025-11-25 19:40:22 UTC",
      "updated_date": "2025-11-28 04:04:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:35.441299+00:00"
    },
    {
      "arxiv_id": "2511.20795v1",
      "title": "Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models",
      "title_zh": "é‡è®¿ KRISPï¼šçŸ¥è¯†å¢å¼ºå‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è½»é‡çº§å¤ç°ä¸åˆ†æ",
      "authors": [
        "Souradeep Dutta",
        "Keshav Bulia",
        "Neena S Nair"
      ],
      "abstract": "Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå§‹ KRISP æ¨¡å‹è®¡ç®—éœ€æ±‚è¿‡é«˜ä¸”é«˜åº¦ä¾èµ–å¤§å‹éª¨å¹²ç½‘ç»œçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å‚æ•°è§„æ¨¡æ˜¾è‘—ç¼©å‡çš„è½»é‡åŒ–å†ç°ç‰ˆæœ¬ã€‚å°½ç®¡å¤ç°æ¨¡å‹åœ¨æ€§èƒ½ä¸Šçº¦ä¸ºåŸç‰ˆçš„ 75%ï¼Œä½†è¯¥ç ”ç©¶è¿‡ç¨‹æ­ç¤ºäº†åŸè®ºæ–‡ä¸­æœªæ›¾æ¶µç›–çš„è®¾è®¡ç¼ºé™·ã€å®é™…åº”ç”¨é™·é˜±ä»¥åŠéšæ€§é—®é¢˜ã€‚é€šè¿‡å¯¹åˆæˆè§†è§‰é—®ç­”(VQA)æ•°æ®å’Œ DAQUAR æ•°æ®é›†è¿›è¡Œç³»ç»Ÿçš„æ¶ˆèç ”ç©¶(ablation studies)ï¼Œè¯¥å·¥ä½œæ·±å…¥æ¢è®¨äº†èµ„æºå—é™ä¸‹çŸ¥è¯†å¢å¼ºå‹ VQA æ¶æ„çš„å¯æ‰©å±•æ€§ã€‚æ¨¡å‹é€šè¿‡å°†è¾“å‡ºä¸¥æ ¼é™åˆ¶åœ¨å¤–éƒ¨çŸ¥è¯†å›¾è°±(Knowledge Graph)é¢†åŸŸå†…ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†äººå·¥æ™ºèƒ½å¹»è§‰(AI hallucinations)çš„äº§ç”Ÿã€‚å¾—ç›Šäºæä½çš„å‚æ•°é‡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ™ºèƒ½æ‰‹æœºåŠ AR-VR ç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç¦»çº¿åœºæ™¯ä¸‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages , 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20795v1",
      "published_date": "2025-11-25 19:37:19 UTC",
      "updated_date": "2025-11-25 19:37:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:40.348202+00:00"
    },
    {
      "arxiv_id": "2511.20793v1",
      "title": "Adversarial Multi-Task Learning for Liver Tumor Segmentation, Dynamic Enhancement Regression, and Classification",
      "title_zh": "ç”¨äºè‚è„è‚¿ç˜¤åˆ†å‰²ã€åŠ¨æ€å¢å¼ºå›å½’åŠåˆ†ç±»çš„å¯¹æŠ—å¼å¤šä»»åŠ¡å­¦ä¹ ",
      "authors": [
        "Xiaojiao Xiao",
        "Qinmin Vivian Hu",
        "Tae Hyun Kim",
        "Guanghui Wang"
      ],
      "abstract": "Liver tumor segmentation, dynamic enhancement regression, and classification are critical for clinical assessment and diagnosis. However, no prior work has attempted to achieve these tasks simultaneously in an end-to-end framework, primarily due to the lack of an effective framework that captures inter-task relevance for mutual improvement and the absence of a mechanism to extract dynamic MRI information effectively. To address these challenges, we propose the Multi-Task Interaction adversarial learning Network (MTI-Net), a novel integrated framework designed to tackle these tasks simultaneously. MTI-Net incorporates Multi-domain Information Entropy Fusion (MdIEF), which utilizes entropy-aware, high-frequency spectral information to effectively integrate features from both frequency and spectral domains, enhancing the extraction and utilization of dynamic MRI data. The network also introduces a task interaction module that establishes higher-order consistency between segmentation and regression, thus fostering inter-task synergy and improving overall performance. Additionally, we designed a novel task-driven discriminator (TDD) to capture internal high-order relationships between tasks. For dynamic MRI information extraction, we employ a shallow Transformer network to perform positional encoding, which captures the relationships within dynamic MRI sequences. In experiments on a dataset of 238 subjects, MTI-Net demonstrates high performance across multiple tasks, indicating its strong potential for assisting in the clinical assessment of liver tumors. The code is available at: https://github.com/xiaojiao929/MTI-Net.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤šä»»åŠ¡äº¤äº’å¯¹æŠ—å­¦ä¹ ç½‘ç»œ (MTI-Net)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨åŒæ­¥å¤„ç†è‚è„è‚¿ç˜¤åˆ†å‰²ã€åŠ¨æ€å¢å¼ºå›å½’å’Œåˆ†ç±»çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚é’ˆå¯¹åŠ¨æ€ MRI ä¿¡æ¯æå–ä¸è¶³åŠä»»åŠ¡é—´å…³è”ç¼ºå¤±çš„é—®é¢˜ï¼ŒMTI-Net å¼•å…¥äº†å¤šåŸŸä¿¡æ¯ç†µèåˆ (MdIEF) æœºåˆ¶ï¼Œé€šè¿‡æ•´åˆé¢‘ç‡åŸŸå’Œå…‰è°±åŸŸçš„ç‰¹å¾æ¥å¢å¼ºå¯¹åŠ¨æ€å½±åƒæ•°æ®çš„åˆ©ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç½‘ç»œåˆ©ç”¨ä»»åŠ¡äº¤äº’æ¨¡å—åœ¨åˆ†å‰²ä¸å›å½’ä»»åŠ¡ä¹‹é—´å»ºç«‹é«˜é˜¶ä¸€è‡´æ€§ï¼Œå¹¶ç»“åˆä»»åŠ¡é©±åŠ¨åˆ¤åˆ«å™¨ (TDD) æ•æ‰ä»»åŠ¡é—´çš„å†…åœ¨ååŒå…³ç³»ã€‚ç ”ç©¶è¿˜é‡‡ç”¨äº†æµ…å±‚ Transformer ç½‘ç»œè¿›è¡Œä½ç½®ç¼–ç ï¼Œä»¥ç²¾å‡†æ•è·åŠ¨æ€ MRI åºåˆ—ä¸­çš„æ—¶ç©ºå…³è”ã€‚åœ¨åŒ…å« 238 ä¾‹å—è¯•è€…çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº† MTI-Net åœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¾…åŠ©è‚è„è‚¿ç˜¤ä¸´åºŠè¯„ä¼°æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20793v1",
      "published_date": "2025-11-25 19:33:24 UTC",
      "updated_date": "2025-11-25 19:33:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:24.137064+00:00"
    },
    {
      "arxiv_id": "2511.20766v1",
      "title": "OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability",
      "title_zh": "OpenAppsï¼šé€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒå˜åŒ–è¯„ä¼° UI æ™ºèƒ½ä½“çš„å¯é æ€§",
      "authors": [
        "Karen Ullrich",
        "Jingtong Su",
        "Claudia Shi",
        "Arjun Subramonian",
        "Amir Bar",
        "Ivan Evtimov",
        "Nikolaos Tsilivis",
        "Randall Balestriero",
        "Julia Kempe",
        "Mark Ibrahim"
      ],
      "abstract": "Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\\%$ to just $4\\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†OpenAppsï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…å«å…­ä¸ªåœ¨å¤–è§‚å’Œå†…å®¹ä¸Šå¯é…ç½®çš„åº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°åœ¨è¡¡é‡UI-Agentå¯é æ€§æ—¶è¿‡åº¦ä¾èµ–å›ºå®šç¯å¢ƒçš„å±€é™æ€§ã€‚ç”±äºç°å®éƒ¨ç½²ä¸­çš„åº”ç”¨è®¾è®¡å’Œå†…å®¹å¤šæ ·åŒ–ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ¡†æ¶å¯¹ä¸ƒç§é¢†å…ˆçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“(multimodal agents)è¿›è¡Œäº†è¶…è¿‡10,000æ¬¡ç‹¬ç«‹è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ™ºèƒ½ä½“åœ¨å›ºå®šç¯å¢ƒä¸‹çš„è¡¨ç°ç›¸å¯¹ç¨³å®šï¼Œä½†åœ¨ä¸åŒåº”ç”¨å˜ä½“ä¸‹çš„å¯é æ€§æ³¢åŠ¨å‰§çƒˆï¼Œè®¸å¤šæ™ºèƒ½ä½“çš„ä»»åŠ¡æˆåŠŸç‡æ³¢åŠ¨å¹…åº¦è¶…è¿‡50%ã€‚ä¾‹å¦‚ï¼ŒKimi-VL-3Bçš„å¹³å‡æˆåŠŸç‡åœ¨ä¸åŒç‰ˆæœ¬é—´ä»63%è·Œè‡³4%ï¼Œä¸”å¾ªç¯(looping)æˆ–å¹»è§‰(hallucinating)åŠ¨ä½œç­‰è¡Œä¸ºè¡¨ç°ä¹Ÿéšç¯å¢ƒé…ç½®è€Œæ˜¾è‘—ä¸åŒã€‚è¿™äº›å‘ç°çªæ˜¾äº†åœ¨åº”ç”¨å˜ä½“ç»´åº¦ä¸Šè¡¡é‡å¯é æ€§çš„å¿…è¦æ€§ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„è‡ªä¸»UI-Agentæä¾›äº†æ–°çš„è¯„ä¼°åŸºå‡†ä¸å·¥å…·æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20766v1",
      "published_date": "2025-11-25 19:00:22 UTC",
      "updated_date": "2025-11-25 19:00:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:42.037815+00:00"
    },
    {
      "arxiv_id": "2511.20650v1",
      "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
      "title_zh": "MedROVï¼šè·¨å¤šæ ·åŒ–åŒ»å­¦å½±åƒæ¨¡æ€çš„å®æ—¶å¼€æ”¾è¯æ±‡æ£€æµ‹",
      "authors": [
        "Tooba Tehreem Sheikh",
        "Jean Lahoud",
        "Rao Muhammad Anwer",
        "Fahad Shahbaz Khan",
        "Salman Khan",
        "Hisham Cholakkal"
      ],
      "abstract": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒç›®æ ‡æ£€æµ‹åœ¨å¤„ç†æœªçŸ¥æ ‡ç­¾æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† MedROVï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘åŒ»ç–—å½±åƒçš„å®æ—¶å¼€æ”¾è¯æ±‡æ£€æµ‹ (Open-Vocabulary Detection, OVOD) æ¨¡å‹ã€‚ä¸ºäº†å…‹æœæ•°æ®é›†ç¨€ç¼ºå’Œæ–‡æœ¬-å›¾åƒå¯¹é½å¼±çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åä¸º Omnis çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›– 9 ç§å½±åƒæ¨¡æ€çš„ 60 ä¸‡ä¸ªæ£€æµ‹æ ·æœ¬ï¼Œå¹¶å¼•å…¥ä¼ªæ ‡ç­¾ (pseudo-labeling) ç­–ç•¥ä»¥å¤„ç†å¤šæºæ•°æ®é›†ä¸­çš„æ ‡æ³¨ç¼ºå¤±ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå¤§å‹é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ (foundation model) çš„çŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹  (contrastive learning) å’Œè·¨æ¨¡æ€è¡¨ç¤º (cross-modal representations)ï¼Œå®ç°äº†å¯¹å·²çŸ¥åŠæ–°é¢–è§£å‰–ç»“æ„çš„æœ‰æ•ˆæ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedROV åœ¨ mAP50 æŒ‡æ ‡ä¸Šæ¯”ä¹‹å‰çš„åŒ»ç–—æ£€æµ‹åŸºç¡€æ¨¡å‹å¹³å‡æé«˜äº† 40%ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„é—­é›†æ£€æµ‹å™¨ã€‚è¯¥æ¨¡å‹åœ¨æä¾›é«˜å‡†ç¡®ç‡çš„åŒæ—¶è¾¾åˆ°äº† 70 FPS çš„å®æ—¶æ¨ç†é€Ÿåº¦ï¼Œä¸ºåŒ»ç–—å½±åƒé¢†åŸŸçš„å®æ—¶å¼€æ”¾è¯æ±‡æ£€æµ‹å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20650v1",
      "published_date": "2025-11-25 18:59:53 UTC",
      "updated_date": "2025-11-25 18:59:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:41.538419+00:00"
    },
    {
      "arxiv_id": "2511.20640v1",
      "title": "MotionV2V: Editing Motion in a Video",
      "title_zh": "MotionV2Vï¼šè§†é¢‘åŠ¨ä½œç¼–è¾‘",
      "authors": [
        "Ryan Burgert",
        "Charles Herrmann",
        "Forrester Cole",
        "Michael S Ryoo",
        "Neal Wadhwa",
        "Andrey Voynov",
        "Nataniel Ruiz"
      ],
      "abstract": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MotionV2Vï¼Œä¸€ç§é€šè¿‡ç›´æ¥ç¼–è¾‘ä»è¾“å…¥è§†é¢‘ä¸­æå–çš„ç¨€ç–è½¨è¿¹(sparse trajectories)æ¥ä¿®æ”¹è§†é¢‘åŠ¨ä½œçš„æ–°å‹æ¡†æ¶ã€‚ä½œè€…å°†è¾“å…¥ä¸è¾“å‡ºè½¨è¿¹ä¹‹é—´çš„åå·®å®šä¹‰ä¸ºâ€œmotion editâ€ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆå¼éª¨å¹²ç½‘ç»œå°†è¿™ç§è¡¨ç¤ºè½¬åŒ–ä¸ºå¼ºå¤§çš„è§†é¢‘ç¼–è¾‘èƒ½åŠ›ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ç”Ÿæˆâ€œmotion counterfactualsâ€ï¼ˆå³å†…å®¹ç›¸åŒä½†åŠ¨ä½œä¸åŒçš„è§†é¢‘å¯¹ï¼‰çš„æµæ°´çº¿ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šå¾®è°ƒäº†è¿åŠ¨æ¡ä»¶è§†é¢‘æ‰©æ•£(motion-conditioned video diffusion)æ¶æ„ã€‚è¯¥æ–¹æ³•æ”¯æŒä»ä»»ä½•æ—¶é—´æˆ³å¼€å§‹ç¼–è¾‘å¹¶å…è®¸åŠ¨ä½œè‡ªç„¶ä¼ æ’­ï¼Œåœ¨å››é¡¹å¯¹ç…§ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œè¯¥æ¨¡å‹æ¯”å…ˆå‰å·¥ä½œè·å¾—äº†è¶…è¿‡65%çš„åå¥½ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20640v1",
      "published_date": "2025-11-25 18:57:25 UTC",
      "updated_date": "2025-11-25 18:57:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:03.447930+00:00"
    },
    {
      "arxiv_id": "2511.20639v2",
      "title": "Latent Collaboration in Multi-Agent Systems",
      "title_zh": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ½œç©ºé—´åä½œ",
      "authors": [
        "Jiaru Zou",
        "Xiyuan Yang",
        "Ruizhong Qiu",
        "Gaotang Li",
        "Katherine Tieu",
        "Pan Lu",
        "Ke Shen",
        "Hanghang Tong",
        "Yejin Choi",
        "Jingrui He",
        "James Zou",
        "Mengdi Wang",
        "Ling Yang"
      ],
      "abstract": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LatentMASï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯ã€æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­çº¯ç²¹çš„ Latent Collaborationã€‚ä¸ä¾èµ–æ–‡æœ¬ä¸­ä»‹çš„ä¼ ç»Ÿæ™ºèƒ½ä½“ä¸åŒï¼ŒLatentMAS å…è®¸æ¯ä¸ªæ™ºèƒ½ä½“é€šè¿‡æœ€åä¸€å±‚éšè—åµŒå…¥ï¼ˆhidden embeddingsï¼‰ç”Ÿæˆè‡ªå›å½’çš„ Latent thoughtsã€‚é€šè¿‡å…±äº«çš„ Latent working memoryï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®Œæ•´ä¿ç•™å¹¶ä¼ è¾“å„æ™ºèƒ½ä½“çš„å†…éƒ¨è¡¨å¾ï¼Œç¡®ä¿äº†æ— æŸçš„ä¿¡æ¯äº¤æ¢ã€‚ç†è®ºåˆ†æè¯æ˜äº† LatentMAS åœ¨å…·æœ‰æ›´é«˜è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ï¼Œå…¶å¤æ‚åº¦è¿œä½äºä¼ ç»Ÿçš„æ–‡æœ¬ä¸­ä»‹åä½œæ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ 9 é¡¹æ¨ç†ä¸ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æœ€é«˜æå‡ 14.6%ï¼ŒåŒæ—¶å‡å°‘äº† 70.8%-83.7% çš„ Token ä½¿ç”¨é‡ã€‚æ­¤å¤–ï¼ŒLatentMAS å®ç°äº† 4 åˆ° 4.3 å€çš„ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿï¼Œè¯æ˜äº† Latent space çš„ç›´æ¥åä½œèƒ½åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿçš„æ¨ç†è´¨é‡ä¸è¿è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Project: https://github.com/Gen-Verse/LatentMAS",
      "pdf_url": "https://arxiv.org/pdf/2511.20639v2",
      "published_date": "2025-11-25 18:56:57 UTC",
      "updated_date": "2025-12-08 04:05:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:28:59.345032+00:00"
    },
    {
      "arxiv_id": "2511.20629v2",
      "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
      "title_zh": "MapReduce LoRAï¼šæ¨è¿›ç”Ÿæˆå¼æ¨¡å‹å¤šåå¥½ä¼˜åŒ–çš„å¸•ç´¯æ‰˜å‰æ²¿",
      "authors": [
        "Chieh-Yun Chen",
        "Zhonghao Wang",
        "Qi Chen",
        "Zhifan Ye",
        "Min Shi",
        "Yue Zhao",
        "Yinan Zhao",
        "Hui Qu",
        "Wei-An Lin",
        "Yiru Shen",
        "Ajinkya Kale",
        "Irfan Essa",
        "Humphrey Shi"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹åœ¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)ä¸­é¢ä¸´çš„å¤šå¥–åŠ±è”åˆä¼˜åŒ–å†²çªé—®é¢˜ï¼Œå³æå‡ç‰¹å®šç»´åº¦åå¥½å¾€å¾€å¯¼è‡´å…¶ä»–ç»´åº¦æ€§èƒ½ä¸‹é™çš„â€œå¯¹é½ç¨â€ç°è±¡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†MapReduce LoRAæ¡†æ¶ï¼Œé€šè¿‡å¹¶è¡Œè®­ç»ƒé’ˆå¯¹ç‰¹å®šåå¥½çš„LoRAä¸“å®¶å¹¶è¿›è¡Œè¿­ä»£åˆå¹¶ï¼Œä»¥ç²¾ç‚¼ä¼˜åŒ–å…±äº«åŸºåº§æ¨¡å‹ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†å¥–åŠ±æ„ŸçŸ¥ä»¤ç‰ŒåµŒå…¥(Reward-aware Token Embedding, RaTE)ï¼Œé€šè¿‡å­¦ä¹ å¥–åŠ±ç‰¹å®šçš„ä»¤ç‰ŒåµŒå…¥åœ¨æ¨ç†é˜¶æ®µå®ç°çµæ´»çš„åå¥½æ§åˆ¶ã€‚åœ¨Stable Diffusion 3.5å’ŒFLUX.1-devçš„å›¾åƒç”Ÿæˆå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨GenEvalã€PickScoreå’ŒOCRç­‰æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨HunyuanVideoè§†é¢‘ç”ŸæˆåŠLlama-2 7Bè¯­è¨€ä»»åŠ¡ä¸­åŒæ ·è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æ”¹å–„äº†è§†é¢‘è¿åŠ¨è´¨é‡ä»¥åŠè¯­è¨€æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œæ— å®³æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€é¢†åŸŸæˆåŠŸæ¨è¿›äº†å¸•ç´¯æ‰˜å‰æ²¿(Pareto Front)ï¼Œä¸ºå¤šåå¥½å¯¹é½æŠ€æœ¯æ ‘ç«‹äº†æ–°çš„SOTAåŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20629v2",
      "published_date": "2025-11-25 18:49:21 UTC",
      "updated_date": "2025-12-11 22:24:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:16.937080+00:00"
    },
    {
      "arxiv_id": "2511.20627v1",
      "title": "Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems",
      "title_zh": "ä»¥ AI æ²» AIï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹ä¿éšœ AI èµ‹èƒ½çš„å®‰å…¨å…³é”®ç³»ç»Ÿ",
      "authors": [
        "Anastasia Mavridou",
        "Divya Gopinath",
        "Corina S. PÄƒsÄƒreanu"
      ],
      "abstract": "The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨èˆªç©ºèˆªå¤©å’Œè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®ç³»ç»Ÿ(safety-critical systems)ä¸­é›†æˆæ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)æ‰€é¢ä¸´çš„é€æ˜åº¦ä¸è¶³åŠè¯­ä¹‰é¸¿æ²Ÿç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åˆ©ç”¨åŸºç¡€æ¨¡å‹(Foundation Models)æ¥ç¡®ä¿AIç³»ç»Ÿå®‰å…¨æ€§çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šREACTåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å°†éæ­£å¼çš„è‡ªç„¶è¯­è¨€éœ€æ±‚è½¬åŒ–ä¸ºå½¢å¼åŒ–è§„èŒƒï¼Œä»è€Œå®ç°æ—©æœŸçš„éªŒè¯ä¸ç¡®è®¤(V&V)ï¼›SemaLensåˆ™åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)é€šè¿‡äººç±»å¯ç†è§£çš„æ¦‚å¿µå¯¹åŸºäºDNNçš„æ„ŸçŸ¥ç³»ç»Ÿè¿›è¡Œæ¨ç†ã€æµ‹è¯•å’Œç›‘æ§ã€‚è¿™ä¸¤è€…å…±åŒæ„å»ºäº†ä»éæ­£å¼éœ€æ±‚åˆ°ç»è¿‡éªŒè¯çš„å®ç°çš„å®Œæ•´æµæ°´çº¿ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä¼ ç»ŸéªŒè¯æ–¹æ³•åœ¨å¤„ç†å¤æ‚AIç»„ä»¶æ—¶çš„å±€é™æ€§ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„è‡ªä¸»ç³»ç»Ÿæä¾›äº†ç³»ç»ŸåŒ–çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20627v1",
      "published_date": "2025-11-25 18:48:19 UTC",
      "updated_date": "2025-11-25 18:48:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:03.642159+00:00"
    },
    {
      "arxiv_id": "2511.20626v1",
      "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training",
      "title_zh": "ROOTï¼šç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒçš„é²æ£’æ­£äº¤åŒ–ä¼˜åŒ–å™¨",
      "authors": [
        "Wei He",
        "Kai Han",
        "Hang Zhou",
        "Hanting Chen",
        "Zhicheng Liu",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "abstract": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ROOTï¼Œä¸€ç§é’ˆå¯¹ç¥ç»ç½‘ç»œè®­ç»ƒçš„é²æ£’æ­£äº¤åŒ–ä¼˜åŒ–å™¨ (Robust Orthogonalized Optimizer)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) è®­ç»ƒè¿‡ç¨‹ä¸­ç”±äºæ¨¡å‹ç¼©æ”¾å¸¦æ¥çš„ç®—æ³•ç²¾åº¦æ•æ„Ÿæ€§å’Œä¸ç¨³å®šæ€§é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰åŠ¨é‡æ­£äº¤åŒ–æ–¹æ³•åœ¨ç»´åº¦ç²¾åº¦ä¸Šçš„è„†å¼±æ€§ä»¥åŠæ˜“å—å¼‚å¸¸å€¼å™ªå£°å¹²æ‰°çš„å±€é™ï¼ŒROOT å¼•å…¥äº†åŒé‡é²æ£’æœºåˆ¶ã€‚é¦–å…ˆï¼Œè¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº” Newton iterations ç»“åˆé’ˆå¯¹ç‰¹å®šçŸ©é˜µå¤§å°è®¾è®¡çš„ç»†ç²’åº¦ç³»æ•°ï¼Œç¡®ä¿äº†åœ¨ä¸åŒæ¶æ„é…ç½®ä¸‹æ­£äº¤åŒ–ç²¾åº¦çš„ç¨³å®šæ€§ã€‚å…¶æ¬¡ï¼Œç ”ç©¶è€…å¼•å…¥äº†åŸºäºè¿‘ç«¯ä¼˜åŒ– (proximal optimization) çš„ä¼˜åŒ–é²æ£’æ¡†æ¶ï¼Œæœ‰æ•ˆæŠ‘åˆ¶å¼‚å¸¸å™ªå£°å¹¶ä¿ç•™å…³é”®æ¢¯åº¦æ–¹å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å™ªå£°å’Œéå‡¸ç­‰å¤æ‚åœºæ™¯ä¸‹ï¼ŒROOT æ¯” Muon å’Œ Adam ç­‰åŸºçº¿æ¨¡å‹å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ä¼˜çš„æœ€ç»ˆæ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå»ºç«‹äº†é«˜ç²¾åº¦ã€é«˜é²æ£’æ€§çš„ä¼˜åŒ–æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20626v1",
      "published_date": "2025-11-25 18:48:05 UTC",
      "updated_date": "2025-11-25 18:48:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:34.655139+00:00"
    },
    {
      "arxiv_id": "2511.20623v1",
      "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ç‰ˆæƒæ£€æµ‹ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¼€å‘çš„ä¼¦ç†è·¯å¾„",
      "authors": [
        "David Szczecina",
        "Senan Gaffori",
        "Edmond Li"
      ],
      "abstract": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨è®­ç»ƒæ•°æ®ä¸­æœªç»æˆæƒä½¿ç”¨ç‰ˆæƒå†…å®¹çš„ä¸¥å³»é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æ£€æµ‹æ¡†æ¶(å¦‚DE-COP)è®¡ç®—æˆæœ¬é«˜ä¸”æ™®é€šåˆ›ä½œè€…éš¾ä»¥è·å–çš„ç°çŠ¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¼€æºç‰ˆæƒæ£€æµ‹å¹³å°ï¼Œä½¿å†…å®¹åˆ›ä½œè€…èƒ½å¤Ÿä¾¿æ·åœ°éªŒè¯å…¶ä½œå“æ˜¯å¦è¢«ç”¨äºLLMè®­ç»ƒã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ”¹è¿›ç›¸ä¼¼æ€§æ£€æµ‹(similarity detection)æ–¹æ³•å¹¶ä¼˜åŒ–æ•°æ®é›†éªŒè¯æµç¨‹ï¼Œåˆ©ç”¨é«˜æ•ˆçš„APIè°ƒç”¨å°†è®¡ç®—å¼€é”€é™ä½äº†10-30%ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ä¸å¯æ‰©å±•çš„åç«¯ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†AIå¼€å‘çš„é€æ˜åº¦ä¸ä¼¦ç†åˆè§„æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºè´Ÿè´£ä»»çš„AIå¼€å‘åŠç‰ˆæƒä¿æŠ¤æä¾›äº†å¯æ‰©å±•ä¸”æ˜“ç”¨çš„æŠ€æœ¯æ”¯æ’‘ï¼Œä¸ºæœªæ¥çš„ç‰ˆæƒæ‰§æ³•å¥ å®šäº†ç ”ç©¶åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20623v1",
      "published_date": "2025-11-25 18:46:14 UTC",
      "updated_date": "2025-11-25 18:46:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:14.039041+00:00"
    },
    {
      "arxiv_id": "2511.20621v1",
      "title": "DiFR: Inference Verification Despite Nondeterminism",
      "title_zh": "DiFRï¼šéç¡®å®šæ€§ç¯å¢ƒä¸‹çš„æ¨ç†éªŒè¯",
      "authors": [
        "Adam Karvonen",
        "Daniel Reuter",
        "Roy Rinberg",
        "Luke Marks",
        "AdriÃ  Garriga-Alonso",
        "Keri Warr"
      ],
      "abstract": "As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ä¸­å› æ•°å€¼å™ªå£°å¯¼è‡´çš„ä¸ç¡®å®šæ€§éªŒè¯éš¾é¢˜ï¼Œæå‡ºäº† Token-DiFR æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç›¸åŒéšæœºç§å­(Random Seed)ä¸‹å°†ç”Ÿæˆå†…å®¹ä¸å¯ä¿¡å‚è€ƒå®ç°å¯¹æ¯”ï¼Œä½¿ Token æœ¬èº«æˆä¸ºé›¶æˆæœ¬çš„æ­£ç¡®æ€§å®¡è®¡è¯æ®ã€‚å®éªŒè¡¨æ˜ Token-DiFR èƒ½å¯é è¯†åˆ«é‡‡æ ·é”™è¯¯å’Œæ¨¡å‹é‡åŒ–(Model Quantization)ï¼Œåœ¨ 300 ä¸ªè¾“å‡º Token å†…æ£€æµ‹ 4-bit é‡åŒ–çš„ AUC è¶…è¿‡ 0.999ã€‚ä¸ºæå‡éªŒè¯æ•ˆç‡ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº† Activation-DiFR æ–¹æ¡ˆï¼Œåˆ©ç”¨éšæœºæ­£äº¤æŠ•å½±(Random Orthogonal Projections)å°†æ¿€æ´»å€¼å‹ç¼©ä¸ºç´§å‡‘æŒ‡çº¹ï¼Œä»…éœ€ 2 ä¸ª Token å³å¯å®ç°é«˜ç²¾åº¦æ£€æµ‹ï¼Œå¹¶æ¯”ç°æœ‰æ–¹æ³•å‡å°‘äº† 25-75% çš„é€šä¿¡å¼€é”€ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®æä¾›äº†ä¸ vLLM çš„å¼€æºé›†æˆï¼Œä¸ºå¯éªŒè¯æ¨ç†(Verifiable Inference)çš„å®é™…è½åœ°æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20621v1",
      "published_date": "2025-11-25 18:44:22 UTC",
      "updated_date": "2025-11-25 18:44:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:28.128674+00:00"
    },
    {
      "arxiv_id": "2511.20615v1",
      "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities",
      "title_zh": "è´Ÿé‡ä¼¸è¾¾æ´»åŠ¨ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å…¨èº«åŠ¨æ€ä¸‰ç»´å§¿æ€é¢„æµ‹ä¸­çš„æ€§èƒ½è¯„ä¼°",
      "authors": [
        "Seyede Niloofar Hosseini",
        "Ali Mojibi",
        "Mahdi Mohseni",
        "Navid Arjmand",
        "Alireza Taheri"
      ],
      "abstract": "This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŠ¨æ€è´Ÿé‡è§¦è¾¾æ´»åŠ¨ä¸­å…¨èº«å§¿æ€é¢„æµ‹çš„åº”ç”¨ï¼Œé‡ç‚¹å¯¹æ¯”äº†åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(BLSTM)å’ŒTransformeræ¶æ„ã€‚æ¨¡å‹é€šè¿‡åˆ†ææ‰‹éƒ¨è´Ÿé‡ä½ç½®ã€æ¬ä¸¾æŠ€æœ¯ã€ä¸ªä½“èº«ä½“å‚æ•°åŠä»»åŠ¡å‰25%æ—¶æ®µçš„3Dåæ ‡ï¼Œæ—¨åœ¨é¢„æµ‹å‰©ä½™75%æ—¶æ®µçš„èº«ä½“åŠ¨æ€ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å¼ºåˆ¶è‚¢ä½“é•¿åº¦æ’å®šçš„æ–°å‹æˆæœ¬å‡½æ•°ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿æ‰‹è‡‚å’Œè…¿éƒ¨æ¨¡å‹çš„é¢„æµ‹è¯¯å·®åˆ†åˆ«é™ä½äº†çº¦8%å’Œ21%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTransformeræ¶æ„åœ¨é•¿æœŸé¢„æµ‹æ€§èƒ½ä¸Šæ¯”BLSTMæ¨¡å‹å‡†ç¡®ç‡é«˜å‡ºçº¦58%ï¼Œå…¶å‡æ–¹æ ¹è¯¯å·®(RMSE)ä¸º47.0 mmã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ•æ‰æ—¶é—´åºåˆ—ä¾èµ–æ€§çš„ç¥ç»ç½‘ç»œåœ¨ç†è§£æ‰‹åŠ¨æ¬è¿æ´»åŠ¨åŠ¨æ€æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸º3Dè¿åŠ¨æ¡†æ¶çš„å»ºæ¨¡ä¸é¢„æµ‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.20615v1",
      "published_date": "2025-11-25 18:40:48 UTC",
      "updated_date": "2025-11-25 18:40:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:38.539193+00:00"
    },
    {
      "arxiv_id": "2511.20613v1",
      "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning",
      "title_zh": "Vibe Coding èƒ½å¦å‡»è´¥è®¡ç®—æœºä¸“ä¸šç ”ç©¶ç”Ÿï¼Ÿä¸€åœºå…³äºå¸‚åœºé©±åŠ¨æˆ˜ç•¥è§„åˆ’çš„ LLM ä¸äººç±»ç¼–ç¨‹ç«èµ›",
      "authors": [
        "Panayiotis Danassis",
        "Naman Goel"
      ],
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨æ¶‰åŠè§„åˆ’ã€ä¼˜åŒ–å’Œæˆ˜ç•¥äº’åŠ¨çš„ç°å®å¤æ‚åœºæ™¯ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›æ˜¯å¦èƒ½è¶…è¶Šäººç±»ç ”ç©¶ç”Ÿã€‚ä½œè€…å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‹å–ã€å–è´§å’Œé€è´§é—®é¢˜ï¼ˆAuction, Pickup, and Delivery Problemï¼‰çš„å¤šæ™ºèƒ½ä½“æ¨ç†é©±åŠ¨åŸºå‡†ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šæ€§ä¸‹è¿›è¡Œæˆ˜ç•¥ç«æ ‡å¹¶æ‰§è¡Œå—é™çš„è·¯å¾„ä¼˜åŒ–ã€‚é€šè¿‡å¯¹ 40 ä¸ªç”± LLMs ç”Ÿæˆçš„æ™ºèƒ½ä½“ï¼ˆæ¶µç›– vibe coding ç­‰å¤šç§æç¤ºæ–¹æ³•ï¼‰ä¸ 17 ä¸ªç”±äººç±»ç¼–å†™çš„æ™ºèƒ½ä½“è¿›è¡Œçº¦ 4 ä¸‡åœºæ¯”èµ›è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºäººç±»ç¨‹åºå‘˜åœ¨æˆ˜ç•¥è§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒ…æ½äº†ç«èµ›çš„å‰äº”åã€‚ç ”ç©¶å‘ç°ï¼Œç»å¤§å¤šæ•° LLM ç”Ÿæˆçš„æ™ºèƒ½ä½“ï¼ˆ33/40ï¼‰ç”šè‡³è¢«ç®€å•çš„åŸºå‡†çº¿ï¼ˆbaselinesï¼‰å‡»è´¥ï¼Œä¸”åœ¨å°è¯•æ”¹è¿›äººç±»æœ€ä¼˜æ–¹æ¡ˆæ—¶åè€Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰ LLMs åœ¨å¤„ç†æ¨ç†é©±åŠ¨çš„å¤æ‚ä»£ç åˆæˆï¼ˆcode synthesisï¼‰ä»»åŠ¡æ—¶å­˜åœ¨çš„å·¨å¤§é¸¿æ²Ÿï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å…·ç°å®æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20613v1",
      "published_date": "2025-11-25 18:40:22 UTC",
      "updated_date": "2025-11-25 18:40:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:30:51.938040+00:00"
    },
    {
      "arxiv_id": "2511.20610v1",
      "title": "Building a Foundation Model for Trajectory from Scratch",
      "title_zh": "ä»é›¶å¼€å§‹æ„å»ºè½¨è¿¹åŸºåº§æ¨¡å‹",
      "authors": [
        "Gaspard Merten",
        "Mahmoud Sakr",
        "Gilles Dejaegere"
      ],
      "abstract": "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºé’ˆå¯¹ç§»åŠ¨è½¨è¿¹çš„ Foundation Modelï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç›®å‰ç¼ºä¹è¯¦ç»†å®ç°æŒ‡å—å’Œæ–‡æ¡£çš„ç©ºç™½ã€‚ä½œè€…é€šè¿‡ä¸€ä¸ªåŸºäº GPT-2 çš„æœ€å°åŒ–å®ç°ç¤ºä¾‹ï¼Œè¯¦ç»†æ¼”ç¤ºäº†å°†é¢„è®­ç»ƒæ¨¡å‹é€‚é…äºæ—¶ç©ºæ•°æ®ï¼ˆSpatiotemporal dataï¼‰çš„ä»£ç æµç¨‹å’ŒæŠ€æœ¯æ­¥éª¤ã€‚æ–‡ä¸­ä¸ä»…ç³»ç»Ÿåœ°å¯¹æ¯”äº† TrajFM å’Œ TrajGPT ç­‰ä»£è¡¨æ€§è½¨è¿¹åŸºç¡€æ¨¡å‹çš„æ¶æ„åˆ›æ–°ï¼Œè¿˜å¼•å…¥äº†è¯¸å¦‚ TimesFM ä¸­ Patching æ–¹æ³•ç­‰è·¨é¢†åŸŸçš„å…ˆè¿›æŠ€æœ¯ã€‚è¯¥æ•™ç¨‹æ—¨åœ¨ä»å®ç°å±‚é¢æ™®åŠåŸºç¡€æ¨¡å‹çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¸º SIGSPATIAL ç¤¾åŒºåœ¨æ„å»ºå’Œè¯„ä¼°ç§»åŠ¨åŸºç¡€æ¨¡å‹æ–¹é¢æä¾›å¿…è¦çš„æ•™è‚²æ”¯æŒã€‚è¿™é¡¹å·¥ä½œä¸ä»…æå‡äº†ç§»åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶é€æ˜åº¦ï¼Œä¹Ÿä¸ºæé«˜åŒè¡Œè¯„å®¡çš„æœ‰æ•ˆæ€§å’Œå­¦æœ¯è§„èŒƒå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20610v1",
      "published_date": "2025-11-25 18:37:55 UTC",
      "updated_date": "2025-11-25 18:37:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:29:28.641500+00:00"
    },
    {
      "arxiv_id": "2511.20604v1",
      "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
      "title_zh": "è®ºé€šè¿‡è¯„ä¼°â€œLLM ä½œä¸ºè¯„å§”â€çš„èƒ½åŠ›æ¥è¯„ä»·å¤§è¯­è¨€æ¨¡å‹çš„å¯¹é½æ°´å¹³",
      "authors": [
        "Yixin Liu",
        "Pengfei Liu",
        "Arman Cohan"
      ],
      "abstract": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„ç”Ÿæˆèƒ½åŠ›ä¸å…¶ä½œä¸ºè¯„åˆ¤è€… (judges) çš„è¯„ä¼°èƒ½åŠ›åœ¨äººç±»åå¥½å¯¹é½ (alignment) æ–¹é¢çš„å†…åœ¨è”ç³»ã€‚é€šè¿‡å¯¹å¤šç§æ¨¡å‹è¿›è¡Œç”Ÿæˆ-è¯„ä¼°ä¸€è‡´æ€§ (GE-consistency) çš„æ·±å…¥åˆ†æï¼Œä½œè€…å‘ç°æ¨¡å‹çš„ç”Ÿæˆæ°´å¹³ä¸å…¶ä½œä¸ºè¯„ä¼°è€…çš„è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ­£ç›¸å…³ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œè®ºæ–‡å¼•å…¥äº† AlignEval åŸºå‡†æµ‹è¯•èŒƒå¼ï¼Œè¯¥èŒƒå¼é€šè¿‡è¯„ä¼° LLMs æ‹…ä»»è£åˆ¤æ—¶çš„èƒ½åŠ›æ¥é—´æ¥è¡¡é‡å…¶å¯¹é½ç¨‹åº¦ï¼Œè€Œéç›´æ¥è¯„ä¼°ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAlignEval åœ¨æ•è·äººç±»åå¥½åŠæ¨¡å‹æ’åå‡†ç¡®æ€§ä¸Šï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº† AlpacaEval å’Œ Arena-Hard ç­‰ç°æœ‰çš„ä¸»æµè‡ªåŠ¨è¯„ä¼°å·¥å…·ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ·±åŒ–äº†å¯¹ LLMs ç”Ÿæˆä¸è¯„ä¼°èƒ½åŠ›å…³ç³»çš„è®¤çŸ¥ï¼Œè¿˜æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æ— éœ€ç›´æ¥åˆ†æè¾“å‡ºçš„æ¨¡å‹å¯¹é½æ€§è¯„ä¼°æ–°æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025 Camera Ready",
      "pdf_url": "https://arxiv.org/pdf/2511.20604v1",
      "published_date": "2025-11-25 18:33:24 UTC",
      "updated_date": "2025-11-25 18:33:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:16.382117+00:00"
    },
    {
      "arxiv_id": "2511.20601v1",
      "title": "The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting",
      "title_zh": "é©±åŠ¨ç›²åŒºç°è±¡ï¼šä¸ºä½•è¡€ç³–é¢„æµ‹ä¸­çš„æ·±åº¦åºåˆ—æ¨¡å‹å€¾å‘äºä¾èµ–è‡ªç›¸å…³",
      "authors": [
        "Heman Shakeri"
      ],
      "abstract": "Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Î”_{\\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Î”_{\\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Î”_{\\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†è¡€ç³–é¢„æµ‹ä¸­æ·±åº¦åºåˆ—æ¨¡å‹æ™®éå­˜åœ¨çš„â€œé©±åŠ¨å› ç´ ç›²åŒºâ€(Driver-Blindness)ç°è±¡ï¼Œå³æ¨¡å‹å¾€å¾€è¿‡åº¦ä¾èµ–è‡ªç›¸å…³æ€§(Autocorrelation)è€Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨èƒ°å²›ç´ ã€é¥®é£Ÿå’Œè¿åŠ¨ç­‰å…·æœ‰ç”Ÿç†å­¦æ„ä¹‰çš„é©±åŠ¨å› ç´ ã€‚ä½œè€…é€šè¿‡å®šä¹‰$\\Delta_{\\text{drivers}}$ï¼ˆå¤šå˜é‡æ¨¡å‹å¯¹æ¯”å•å˜é‡åŸºå‡†çš„æ€§èƒ½å¢ç›Šï¼‰é‡åŒ–äº†è¿™ä¸€é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºè¯¥æŒ‡æ ‡åœ¨ç°æœ‰æ–‡çŒ®ä¸­é€šå¸¸æ¥è¿‘äºé›¶ã€‚ç ”ç©¶åˆ†æè®¤ä¸ºï¼Œè¯¥ç°è±¡æºäºå€¾å‘äºè‡ªç›¸å…³æ€§çš„æ¶æ„åè§(Architectural biases)ã€æ•°æ®ä¿çœŸåº¦ç¼ºå¤±(Data fidelity gaps)ä»¥åŠç”Ÿç†å¼‚è´¨æ€§(Physiological heterogeneity)çš„å…±åŒä½œç”¨ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè®ºæ–‡æ€»ç»“äº†ç”Ÿç†ç‰¹å¾ç¼–ç å™¨(Physiological feature encoders)ã€å› æœæ­£åˆ™åŒ–(Causal regularization)å’Œä¸ªæ€§åŒ–ç­‰ç¼“è§£ç­–ç•¥ã€‚ä½œè€…æœ€åå»ºè®®æœªæ¥çš„ç ”ç©¶åº”å¸¸è§„æŠ¥å‘Š$\\Delta_{\\text{drivers}}$ï¼Œä»¥é¿å…å°†ä»…ä¾èµ–è‡ªç›¸å…³æ€§çš„æ¨¡å‹è¯¯è¯„ä¸ºé¢†åŸŸæœ€å‰æ²¿æˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.20601v1",
      "published_date": "2025-11-25 18:30:55 UTC",
      "updated_date": "2025-11-25 18:30:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:32:58.516466+00:00"
    },
    {
      "arxiv_id": "2511.20597v1",
      "title": "BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents",
      "title_zh": "BrowseSafeï¼šæ·±å…¥ç†è§£ä¸é˜²èŒƒ AI æµè§ˆå™¨æ™ºèƒ½ä½“ä¸­çš„æç¤ºè¯æ³¨å…¥",
      "authors": [
        "Kaiyuan Zhang",
        "Mark Tenenholtz",
        "Kyle Polley",
        "Jerry Ma",
        "Denis Yarats",
        "Ninghui Li"
      ],
      "abstract": "The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.\n  In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é›†æˆåœ¨æµè§ˆå™¨ä¸­çš„äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(AI agents)æ‰€é¢ä¸´çš„æç¤ºæ³¨å…¥(Prompt Injection)å®‰å…¨æŒ‘æˆ˜ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªåŸºäºçœŸå®HTMLæœ‰æ•ˆè´Ÿè½½çš„æ”»å‡»åŸºå‡†æµ‹è¯•ï¼Œå…¶é‡ç‚¹åœ¨äºèƒ½å¤Ÿå½±å“çœŸå®ä¸–ç•Œæ“ä½œè€Œéä»…ä»…æ˜¯æ–‡æœ¬è¾“å‡ºï¼Œå¹¶æ¨¡æ‹Ÿäº†çœŸå®æ™ºèƒ½ä½“é‡åˆ°çš„å¤æ‚æ”»å‡»ç¯å¢ƒã€‚é€šè¿‡åœ¨å¤šç§å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹(Frontier AI models)ä¸Šå¯¹ç°æœ‰é˜²å¾¡æªæ–½è¿›è¡Œå…¨é¢å®è¯è¯„ä¼°ï¼Œè¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†æç¤ºæ³¨å…¥åœ¨ç°å®ç¯å¢ƒä¸­çš„å½±å“ã€‚åŸºäºè¯„ä¼°ç»“æœï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŒ…å«æ¶æ„å’Œæ¨¡å‹å±‚é¢çš„å¤šå±‚é˜²å¾¡ç­–ç•¥ï¼Œä»¥åº”å¯¹ä¸æ–­æ¼”å˜çš„æ”»å‡»æ‰‹æ®µã€‚è¿™é¡¹å·¥ä½œä¸ºé‡‡ç”¨æ·±åº¦é˜²å¾¡(Defense-in-depth)æ–¹æ³•æ„å»ºå®‰å…¨ä¸”å®ç”¨çš„ç½‘ç»œæ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„è®¾è®¡è“å›¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20597v1",
      "published_date": "2025-11-25 18:28:35 UTC",
      "updated_date": "2025-11-25 18:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:09.649393+00:00"
    },
    {
      "arxiv_id": "2511.20590v1",
      "title": "EnergyTwin: A Multi-Agent System for Simulating and Coordinating Energy Microgrids",
      "title_zh": "EnergyTwinï¼šç”¨äºèƒ½æºå¾®ç”µç½‘æ¨¡æ‹Ÿä¸åè°ƒçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Jakub MuszyÅ„ski",
        "Ignacy WaluÅ¼enicz",
        "Patryk Zan",
        "Zofia Wrona",
        "Maria Ganzha",
        "Marcin Paprzycki",
        "Costin BÄƒdicÄƒ"
      ],
      "abstract": "Microgrids are deployed to reduce purchased grid energy, limit exposure to volatile tariffs, and ensure service continuity during disturbances. This requires coordinating heterogeneous distributed energy resources across multiple time scales and under variable conditions. Among existing tools, typically, power-system simulators capture physical behaviour but assume centralized control, while multi-agent frameworks model decentralized decision-making but represent energy with no physical grounding. In this context, the EnergyTwin is introduced, an agent-based microgrid simulation environment that couples physically grounded models with forecast-informed, rolling-horizon planning, and negotiations. Each asset is modeled as an agent, interacting with a central agent that obtains forecasts, formulates predictions, and allocates energy through contract-based interactions. EnergyTwin targets tertiary-layer decision making and is extensible for digital-twin use. Its feasibility was evaluated in a university campus microgrid scenario where multiple planning strategies were compared. Achieved results show that forecast-driven rolling-horizon planning increases local energy self-sufficiency, maintains higher battery reserves, and reduces exposure to low-resilience operating states. They demonstrate also potential of EnergyTwin as platform supporting research on resilient, negotiation-driven microgrids.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† EnergyTwinï¼Œä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent System) çš„å¾®ç½‘ä»¿çœŸç¯å¢ƒï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å·¥å…·åœ¨ç‰©ç†è¡Œä¸ºæ¨¡æ‹Ÿä¸å»ä¸­å¿ƒåŒ–å†³ç­–å»ºæ¨¡ä¹‹é—´å­˜åœ¨çš„è„±èŠ‚é—®é¢˜ã€‚è¯¥ç³»ç»Ÿå°†å¾®ç½‘ä¸­çš„æ¯ä¸ªèµ„äº§å»ºæ¨¡ä¸ºç‹¬ç«‹æ™ºèƒ½ä½“ï¼Œå¹¶ä¸ä¸­å¿ƒæ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œé€šè¿‡é¢„æµ‹é©±åŠ¨çš„æ»šåŠ¨æ—¶åŸŸè§„åˆ’ (Rolling-Horizon Planning) å’ŒåŸºäºåˆåŒçš„åå•†æœºåˆ¶å®ç°èƒ½æºåˆ†é…ã€‚EnergyTwin ä¸“æ³¨äºç¬¬ä¸‰å±‚çº§ (Tertiary-Layer) çš„å†³ç­–åˆ¶å®šï¼Œå¹¶å…·å¤‡æ‰©å±•ä¸ºæ•°å­—å­ªç”Ÿ (Digital-Twin) åº”ç”¨çš„æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¤§å­¦æ ¡å›­å¾®ç½‘åœºæ™¯è¯„ä¼°äº†å…¶å¯è¡Œæ€§ï¼Œå¹¶å¯¹æ¯”äº†å¤šç§è§„åˆ’ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„æµ‹é©±åŠ¨çš„æ»šåŠ¨æ—¶åŸŸè§„åˆ’æ˜¾è‘—æå‡äº†å±€éƒ¨èƒ½æºè‡ªç»™ç‡ï¼Œç»´æŒäº†æ›´é«˜çš„ç”µæ± å‚¨å¤‡ï¼Œå¹¶å‡å°‘äº†ç³»ç»Ÿåœ¨ä½éŸ§æ€§è¿è¡ŒçŠ¶æ€ä¸‹çš„æš´éœ²ã€‚è¯¥æˆæœè¯æ˜äº† EnergyTwin ä½œä¸ºæ”¯æŒå¼¹æ€§ã€åå•†é©±åŠ¨å‹å¾®ç½‘ç ”ç©¶å¹³å°çš„æœ‰æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20590v1",
      "published_date": "2025-11-25 18:19:40 UTC",
      "updated_date": "2025-11-25 18:19:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:18.741807+00:00"
    },
    {
      "arxiv_id": "2511.20586v3",
      "title": "PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic",
      "title_zh": "PaTASï¼šåŸºäºä¸»è§‚é€»è¾‘çš„ç¥ç»ç½‘ç»œä¿¡ä»»ä¼ æ’­æ¡†æ¶",
      "authors": [
        "Koffi Ismael Ouattara",
        "Ioannis Krontiris",
        "Theo Dimitrakos",
        "Dennis Eisermann",
        "Houda Labiod",
        "Frank Kargl"
      ],
      "abstract": "Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics, such as accuracy and precision, fail to appropriately capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the Parallel Trust Assessment System (PaTAS), a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through Trust Nodes and Trust Functions that propagate input, parameter, and activation trust across the network. The framework defines a Parameter Trust Update mechanism to refine parameter reliability during training and an Inference-Path Trust Assessment (IPTA) method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a foundation for evaluating model reliability across the AI lifecycle.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PaTAS (Parallel Trust Assessment System)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨Subjective Logic (SL) åœ¨ç¥ç»ç½‘ç»œä¸­è¿›è¡Œä¿¡ä»»å»ºæ¨¡ä¸ä¼ æ’­çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­éš¾ä»¥æ•æ‰ä¸ç¡®å®šæ€§æˆ–é¢„æµ‹å¯é æ€§çš„å±€é™æ€§ã€‚PaTASé€šè¿‡ä¸æ ‡å‡†è®¡ç®—å¹¶è¡Œçš„Trust Nodeså’ŒTrust Functionsï¼Œå®ç°äº†è¾“å…¥ã€å‚æ•°åŠæ¿€æ´»ä¿¡ä»»åº¦åœ¨ç½‘ç»œä¸­çš„å±‚é—´ä¼ æ’­ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†Parameter Trust Updateæœºåˆ¶ä»¥åœ¨è®­ç»ƒä¸­ä¼˜åŒ–å‚æ•°å¯é æ€§ï¼Œå¹¶é‡‡ç”¨Inference-Path Trust Assessment (IPTA) æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µè®¡ç®—ç‰¹å®šå®ä¾‹çš„ä¿¡ä»»è¯„ä¼°ã€‚åœ¨çœŸå®ä¸–ç•ŒåŠå¯¹æŠ—æ€§æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒPaTASèƒ½ç”Ÿæˆå¯è§£é‡Šä¸”æ”¶æ•›çš„ä¿¡ä»»ä¼°è®¡ï¼Œæœ‰æ•ˆè¯†åˆ«æ¨¡å‹ç½®ä¿¡åº¦ä¸å®é™…å¯é æ€§ä¹‹é—´çš„åå·®ï¼Œå¹¶èƒ½åŒºåˆ†è‰¯æ€§è¾“å…¥ä¸å¯¹æŠ—æ€§æ”»å‡»ã€‚é€šè¿‡åœ¨ç¥ç»æ¶æ„ä¸­å®ç°é€æ˜ä¸”å¯é‡åŒ–çš„ä¿¡ä»»æ¨ç†ï¼ŒPaTASä¸ºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å…¨ç”Ÿå‘½å‘¨æœŸå†…çš„å¯é æ€§æä¾›äº†é‡è¦çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20586v3",
      "published_date": "2025-11-25 18:15:36 UTC",
      "updated_date": "2025-12-11 07:35:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:18.083852+00:00"
    },
    {
      "arxiv_id": "2511.20570v1",
      "title": "Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics",
      "title_zh": "é¢å‘ç¥ç»ä¿¡å·æ§åˆ¶æœºå™¨äººçš„é—¨æ§ä¸ç¡®å®šæ€§æ„ŸçŸ¥è¿è¡Œæ—¶åŒé‡ä¸å˜é‡",
      "authors": [
        "Tasha Kim",
        "Oiwi Parker Jones"
      ],
      "abstract": "Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants) æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºç¥ç»ä¿¡å·æ§åˆ¶çš„æœºå™¨äººç³»ç»Ÿæä¾›å®æ—¶ç¥ç»ç¬¦å·éªŒè¯ (neuro-symbolic verification)ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç»è¿‡ç½®ä¿¡åº¦æ ¡å‡† (confidence-calibrated) çš„è„‘ä¿¡å·è§£ç ä¸ç¬¦å·ç›®æ ‡è½åœ° (symbolic goal grounding) åŠåŒå±‚è¿è¡Œæ—¶ç›‘æ§ (dual-layer runtime monitoring) ç›¸ç»“åˆï¼ŒåŒæ—¶ç¡®ä¿äº†é€»è¾‘å®‰å…¨å’Œç”Ÿç†ä¿¡ä»»ã€‚åœ¨ BNCI2014 è¿åŠ¨æƒ³è±¡è„‘ç”µå›¾ (EEG) æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨è§£ç å™¨å‡†ç¡®ç‡è¾ƒä½ä¸”ç½®ä¿¡åº¦æ ¡å‡†åå·®è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œè¯¥ç³»ç»Ÿä»èƒ½ç»´æŒ 94-97% çš„æé«˜å®‰å…¨ç‡ã€‚åœ¨æ¨¡æ‹Ÿå™ªå£°æµ‹è¯•ä¸­ï¼ŒGUARDIAN çš„æ­£ç¡®å¹²é¢„æ¬¡æ•°è¾¾åˆ°åŸºçº¿æ¨¡å‹çš„ 1.7 å€ï¼Œä¸”å…¶ç›‘æ§é¢‘ç‡ä¸º 100Hzï¼Œå†³ç­–å»¶è¿Ÿä½äºæ¯«ç§’çº§ï¼Œè¯æ˜äº†å…¶åœ¨é—­ç¯ç³»ç»Ÿä¸­çš„å®é™…å¯è¡Œæ€§ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®è¯¥æ¡†æ¶èƒ½å¯¹ä¿¡å·é€€åŒ–åšå‡ºåˆ†çº§å“åº”ï¼Œå¹¶ç”Ÿæˆä»æ„å›¾åˆ°è¡ŒåŠ¨çš„å¯å®¡è®¡è¿½è¸ªï¼Œæœ‰æ•ˆå»ºç«‹äº†ç¥ç»è¯æ®ä¸å¯éªŒè¯æœºå™¨äººåŠ¨ä½œä¹‹é—´çš„è”ç³»ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Embodied and Safe-Assured Robotic Systems workshop at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20570v1",
      "published_date": "2025-11-25 18:05:05 UTC",
      "updated_date": "2025-11-25 18:05:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:21.648981+00:00"
    },
    {
      "arxiv_id": "2512.10963v1",
      "title": "Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis",
      "title_zh": "åŸºäºå¤šæ¨¡æ€æƒ…æ„Ÿä¸æ„å›¾åˆ†æçš„ AI ç”Ÿæˆå†…å®¹æƒ…æ„Ÿé©±åŠ¨ä¸ªæ€§åŒ–æ¨è",
      "authors": [
        "Zheqi Hu",
        "Xuanjing Chen",
        "Jinlin Hu"
      ],
      "abstract": "With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¨èç³»ç»Ÿå¿½è§†ç”¨æˆ·å®æ—¶æƒ…æ„Ÿå’Œæ„å›¾çŠ¶æ€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€æƒ…æ„Ÿä¸æ„å›¾è¯†åˆ«æ¨¡å‹ï¼ˆMMEIï¼‰çš„ä¸ªæ€§åŒ–AIGCæ¨èæ¡†æ¶ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºBERTçš„Cross-Modal Transformerä¸Attention-Based FusionæŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ViTã€Wav2Vec2å’ŒBERTç¼–ç å™¨ååŒå¤„ç†è§†è§‰ã€å¬è§‰åŠæ–‡æœ¬æ¨¡æ€æ•°æ®ï¼Œä»¥ç²¾å‡†æ•æ‰ç”¨æˆ·çš„æƒ…æ„Ÿæ„å›¾è¡¨å¾ã€‚è¿™äº›è¡¨å¾é€šè¿‡ä¸Šä¸‹æ–‡åŒ¹é…å±‚é©±åŠ¨ä¸ªæ€§åŒ–å†…å®¹æ¨èï¼Œå®ç°äº†æƒ…æ„Ÿæ„ŸçŸ¥çš„äº¤äº’ä½“éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMMEIæ¨¡å‹åœ¨MELDå’ŒCMU-MOSEIç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„F1-scoreæå‡äº†4.3%ï¼Œäº¤å‰ç†µæŸå¤±é™ä½äº†12.3%ã€‚åœ¨çº¿ç”¨æˆ·è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥ç³»ç»Ÿå°†ç”¨æˆ·å‚ä¸æ—¶é•¿æå‡äº†15.2%ï¼Œæ»¡æ„åº¦è¯„åˆ†æé«˜äº†11.8%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è·¨æ¨¡æ€æƒ…æ„Ÿæ™ºèƒ½åœ¨æ„å»ºè‡ªé€‚åº”ã€å…±æƒ…ä¸”æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä»£AIGCç”Ÿæ€ç³»ç»Ÿä¸­çš„é‡è¦æ½œåŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10963v1",
      "published_date": "2025-11-25 17:52:22 UTC",
      "updated_date": "2025-11-25 17:52:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:40.136498+00:00"
    },
    {
      "arxiv_id": "2511.20551v1",
      "title": "Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity",
      "title_zh": "åŸºäºæ—¶åŸŸçº¿æ€§æ¨¡å‹çš„ç©ºåŒ–æ´»åŠ¨è¢«åŠ¨å£°å­¦æ˜ å°„æ¡†æ¶",
      "authors": [
        "Tatiana Gelvez-Barrera",
        "Barbara Nicolas",
        "Denis KouamÃ©",
        "Bruno Gilles",
        "Adrian Basarab"
      ],
      "abstract": "Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å®Œå…¨åœ¨æ—¶åŸŸå†…æ„å»ºçš„åŸºäºçº¿æ€§æ¨¡å‹ (Linear Model-based) çš„æ³¢æŸå½¢æˆæ¡†æ¶ï¼Œç”¨äºè§£å†³è¢«åŠ¨å£°å­¦åˆ¶å›¾ (Passive Acoustic Mapping, PAM) åœ¨æ²»ç–—æ€§è¶…å£°åº”ç”¨ä¸­è½´å‘åˆ†è¾¨ç‡å—é™åŠé¢‘åŸŸæ–¹æ³•ä¾èµ–é•¿ä¿¡å·çš„é—®é¢˜ã€‚è¯¥çº¿æ€§å‰å‘æ¨¡å‹å°†ç©ºåŒ–æ´»åŠ¨çš„ç¦»æ•£åŒ–æ—¶ç©ºåˆ†å¸ƒä¸æ¢å¤´è®°å½•ä¿¡å·ç›¸å…³è”ï¼Œå¹¶æ˜¾å¼è€ƒè™‘äº†ç”±é‡‡é›†å‡ ä½•ç»“æ„å†³å®šçš„é£è¡Œæ—¶é—´ (Time-of-Flight) å»¶è¿Ÿã€‚é€šè¿‡åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´åŸŸçš„å…ˆéªŒçŸ¥è¯†ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ­£åˆ™åŒ– (Regularization) æŠ€æœ¯å¯¹æ¨¡å‹è¿›è¡Œåæ¼”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨é¢‘åŸŸæ–¹æ³• 20% æ•°æ®é‡çš„æƒ…å†µä¸‹ï¼Œå³å¯è·å¾—æ›´ä¼˜æˆ–åŒç­‰æ°´å¹³çš„ç©ºåŒ–åˆ¶å›¾è´¨é‡ã€‚è¿™è¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ•°æ®æ•ˆç‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ä»¥åŠæ—¶ç©ºæ­£åˆ™åŒ–åœ¨å¤„ç†å¤šæ ·åŒ–è¢«åŠ¨ç©ºåŒ–åœºæ™¯æ—¶çš„çµæ´»æ€§ï¼Œå…¶è¡¨ç°è¶…è¶Šäº†ç›®å‰çš„å…ˆè¿›æŠ€æœ¯ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20551v1",
      "published_date": "2025-11-25 17:48:04 UTC",
      "updated_date": "2025-11-25 17:48:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:55.737849+00:00"
    },
    {
      "arxiv_id": "2511.20549v1",
      "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
      "title_zh": "Flash-DMDï¼šåŸºäºé«˜æ•ˆè’¸é¦ä¸è”åˆå¼ºåŒ–å­¦ä¹ çš„é«˜ä¿çœŸå°‘æ­¥å›¾åƒç”Ÿæˆ",
      "authors": [
        "Guanjie Chen",
        "Shirui Huang",
        "Kai Liu",
        "Jianchen Zhu",
        "Xiaoye Qu",
        "Peng Chen",
        "Yu Cheng",
        "Yifu Sun"
      ],
      "abstract": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Flash-DMD æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹(Diffusion Models)è¿­ä»£é‡‡æ ·è®¡ç®—æˆæœ¬é«˜ã€è’¸é¦è¿‡ç¨‹å›¾åƒè´¨é‡ä¸‹é™ä»¥åŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¾®è°ƒä¸ç¨³å®šç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„æ—¶é—´æ­¥æ„ŸçŸ¥è’¸é¦(timestep-aware distillation)ç­–ç•¥ï¼Œåœ¨æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬çš„åŒæ—¶å¢å¼ºäº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿï¼Œä»…ç”¨ DMD2 çº¦ 2.1% çš„è®­ç»ƒæˆæœ¬ä¾¿å®ç°äº†æ›´ä¼˜è¡¨ç°ã€‚å…¶æ¬¡ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å°† RL å¾®è°ƒä¸æŒç»­æ—¶é—´æ­¥è’¸é¦ç›¸ç»“åˆçš„è”åˆè®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨ç¨³å®šçš„è’¸é¦æŸå¤±ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œæœ‰æ•ˆç¨³å®šäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹å¹¶é˜²æ­¢ç­–ç•¥å´©æºƒ(policy collapse)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlash-DMD åœ¨å¾—åˆ†åŒ¹é…(score-based)å’ŒæµåŒ¹é…(flow matching)æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œåœ¨å°‘æ­¥é‡‡æ ·(few-step sampling)åœºæ™¯ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€äººç±»åå¥½ä»¥åŠæ–‡æœ¬-å›¾åƒå¯¹é½(text-image alignment)æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œä¸ºè®­ç»ƒé«˜æ•ˆã€é«˜ä¿çœŸä¸”ç¨³å®šçš„ç”Ÿæˆæ¨¡å‹æä¾›äº†ä¸€å¥—æœ‰æ•ˆçš„èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20549v1",
      "published_date": "2025-11-25 17:47:11 UTC",
      "updated_date": "2025-11-25 17:47:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:38.738796+00:00"
    },
    {
      "arxiv_id": "2511.20544v1",
      "title": "New York Smells: A Large Multimodal Dataset for Olfaction",
      "title_zh": "New York Smellsï¼šé¢å‘å—…è§‰çš„å¤§å‹å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Ege Ozguroglu",
        "Junbang Liang",
        "Ruoshi Liu",
        "Mia Chiquier",
        "Michael DeTienne",
        "Wesley Wei Qian",
        "Alexandra Horowitz",
        "Andrew Owens",
        "Carl Vondrick"
      ],
      "abstract": "While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨åœ¨å—…è§‰(olfaction)æ„ŸçŸ¥é¢†åŸŸç¼ºä¹è‡ªç„¶åœºæ™¯ä¸‹å¤šæ ·åŒ–å¤šæ¨¡æ€è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºâ€œNew York Smellsâ€çš„å¤§å‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«åœ¨è‡ªç„¶ç¯å¢ƒ(in the wild)ä¸­é‡‡é›†çš„7,000ç»„æ°”å‘³ä¸å›¾åƒé…å¯¹ä¿¡å·ï¼Œæ¶µç›–äº†å®¤å†…å¤–ç¯å¢ƒä¸­çš„3,500ä¸ªä¸åŒç‰©ä½“ï¼Œå…¶ç‰©ä½“å¤šæ ·æ€§çº¦ä¸ºç°æœ‰å—…è§‰æ•°æ®é›†çš„70å€ã€‚ç ”ç©¶è®¾å®šäº†ä¸‰é¡¹æ ¸å¿ƒåŸºå‡†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€çš„æ°”å‘³åˆ°å›¾åƒæ£€ç´¢(smell-to-image retrieval)ã€ä»…å‡­æ°”å‘³è¯†åˆ«åœºæ™¯ã€ç‰©ä½“å’Œææ–™ï¼Œä»¥åŠå¯¹è‰ç±»ç‰©ç§è¿›è¡Œç»†ç²’åº¦åŒºåˆ†ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè§†è§‰æ•°æ®èƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©è·¨æ¨¡æ€çš„å—…è§‰è¡¨å¾å­¦ä¹ (olfactory representation learning)ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡å­¦ä¹ è·å¾—çš„å—…è§‰è¡¨å¾åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„æ‰‹å·¥ç‰¹å¾(hand-crafted features)ï¼Œä¸ºæœºå™¨ç†è§£å¤æ‚çš„åŒ–å­¦æ„Ÿå®˜ä¿¡å·å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website at https://smell.cs.columbia.edu",
      "pdf_url": "https://arxiv.org/pdf/2511.20544v1",
      "published_date": "2025-11-25 17:44:50 UTC",
      "updated_date": "2025-11-25 17:44:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:31:41.942550+00:00"
    },
    {
      "arxiv_id": "2511.20541v1",
      "title": "Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation",
      "title_zh": "åŸºäºè¯­ä¹‰åˆ†å‰²çš„æ–‡åŒ–é—äº§è‡ªåŠ¨åŒ–ç›‘æµ‹",
      "authors": [
        "Andrea Ranieri",
        "Giorgio Palmieri",
        "Silvia Biasotti"
      ],
      "abstract": "This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡åŒ–é—äº§ä¿æŠ¤ä¸­çš„è‡ªåŠ¨è£‚ç¼æ£€æµ‹éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰åˆ†å‰²(Semantic Segmentation)çš„æŠ€æœ¯æ–¹æ¡ˆã€‚é€šè¿‡å¯¹ç»“åˆå¤šç§å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ç¼–ç å™¨çš„U-Netæ¶æ„è¿›è¡Œå¯¹æ¯”ç ”ç©¶ï¼Œè¯¥å·¥ä½œå®ç°äº†å¯¹é›•åƒå’Œçºªå¿µç¢‘çš„åƒç´ çº§è£‚ç¼è¯†åˆ«ã€‚è¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†OmniCrack30kæ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¹³å‡äº¤å¹¶æ¯”(mIoU)ã€Diceç³»æ•°å’ŒJaccardæŒ‡æ•°ç­‰æ ‡å‡†åº¦é‡æŒ‡æ ‡è¿›è¡Œäº†å®šé‡åˆ†æï¼ŒåŒæ—¶è¾…ä»¥çœŸå®ä¸–ç•Œåœºæ™¯ä¸‹çš„å®šæ€§éªŒè¯ã€‚ç ”ç©¶ç»“æœæ·±å…¥æ­ç¤ºäº†ä¸åŒCNNç¼–ç å™¨åœ¨ç»†ç²’åº¦è£‚ç¼åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹æœªæ›¾ç›´æ¥åœ¨é›•åƒæˆ–çºªå¿µç¢‘æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†ä»è¡¨ç°å‡ºä¼˜å¼‚çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ–‡åŒ–é—äº§æ–‡ç‰©çš„è‡ªåŠ¨åŒ–ç›‘æµ‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Keywords: Cultural Heritage, Monitoring, Deep Learning, U-Nets, Semantic Segmentation",
      "pdf_url": "https://arxiv.org/pdf/2511.20541v1",
      "published_date": "2025-11-25 17:42:11 UTC",
      "updated_date": "2025-11-25 17:42:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:10.229168+00:00"
    },
    {
      "arxiv_id": "2511.20540v1",
      "title": "Proceedings Twentieth Conference on Theoretical Aspects of Rationality and Knowledge",
      "title_zh": "ç¬¬äºŒåå±Šç†æ€§ä¸çŸ¥è¯†ç†è®ºæ–¹é¢ä¼šè®®è®ºæ–‡é›†",
      "authors": [
        "Adam Bjorndahl"
      ],
      "abstract": "The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.\n  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, uncertainty, awareness, bounded rationality, common sense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems.\n  Information about TARK is available at http://www.tark.org/.\n  These proceedings contain the papers that have been accepted for presentation at the Twentieth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2025), held July 14--16, 2025, at Heinrich-Heine-UniversitÃ¤t, DÃ¼sseldorf, Germany. The conference website can be found at https://ccc.cs.uni-duesseldorf.de/tark-2025/.",
      "tldr_zh": "æœ¬è®ºæ–‡é›†æ”¶å½•äº†ç¬¬äºŒåå±Šç†æ€§ä¸çŸ¥è¯†ç†è®ºæ–¹é¢æ¢è®¨ä¼šè®®(TARK 2025)çš„å­¦æœ¯è®ºæ–‡ï¼Œè¯¥ä¼šè®®æ—¨åœ¨æ±‡é›†è®¡ç®—æœºç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€åšå¼ˆè®ºã€é€»è¾‘å­¦åŠè®¤çŸ¥ç§‘å­¦ç­‰å¤šé¢†åŸŸç ”ç©¶è€…ï¼Œå…±åŒæ¢è®¨å…³äºç†æ€§ä¸çŸ¥è¯†æ¨ç†çš„è·¨å­¦ç§‘è®®é¢˜ã€‚å…¶ç ”ç©¶èŒƒç•´æ¶µç›–äº†è¯­ä¹‰æ¨¡å‹ã€ä¸ç¡®å®šæ€§ã€æœ‰ç•Œç†æ€§(Bounded Rationality)ã€å¸¸è¯†è®¤è¯†æ¨ç†(Common Sense Epistemic Reasoning)ä»¥åŠè®¤è¯†é€»è¾‘(Epistemic Logic)ç­‰æ ¸å¿ƒé¢†åŸŸã€‚æ–‡é›†é‡ç‚¹å…³æ³¨è®¤è¯†è®ºåšå¼ˆè®º(Epistemic Game Theory)ã€çŸ¥è¯†ä¸è¡ŒåŠ¨ã€ä¿¡å¿µä¿®æ­£(Belief Revision)åŠå¤šæ™ºèƒ½ä½“ç³»ç»ŸåŸºç¡€(Foundations of Multi-agent Systems)ç­‰å‰æ²¿æ–¹å‘ã€‚è¿™äº›ç ”ç©¶æˆæœä½“ç°äº†åœ¨è®¡ç®—ç¤¾ä¼šé€‰æ‹©(Computational Social Choice)å’Œç®—æ³•åšå¼ˆè®º(Algorithmic Game Theory)ç­‰è·¨å­¦ç§‘èƒŒæ™¯ä¸‹ï¼Œå¯¹å¿ƒç†çŠ¶æ€æ¨ç†åŠå…¶åº”ç”¨çš„æ·±å…¥ç†è§£ã€‚è¯¥è®ºæ–‡é›†è®°å½•äº†2025å¹´7æœˆåœ¨å¾·å›½æœå¡å°”å¤šå¤«ä¸¾è¡Œçš„ä¼šè®®æˆæœï¼Œä¸ºç†è§£å¤æ‚ç³»ç»Ÿä¸­çš„ç†æ€§è¡Œä¸ºæä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20540v1",
      "published_date": "2025-11-25 17:41:15 UTC",
      "updated_date": "2025-11-25 17:41:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:20.028130+00:00"
    },
    {
      "arxiv_id": "2511.20532v2",
      "title": "MIMIC-MJX: Neuromechanical Emulation of Animal Behavior",
      "title_zh": "MIMIC-MJXï¼šåŠ¨ç‰©è¡Œä¸ºçš„ç¥ç»åŠ›å­¦ä»¿çœŸ",
      "authors": [
        "Charles Y. Zhang",
        "Yuanjia Yang",
        "Aidan Sirbu",
        "Elliott T. T. Abe",
        "Emil WÃ¤rnberg",
        "Eric J. Leonardis",
        "Diego E. Aldarondo",
        "Adam Lee",
        "Aaditya Prasad",
        "Jason Foat",
        "Kaiwen Bian",
        "Joshua Park",
        "Rusham Bhatt",
        "Hutton Saunders",
        "Akira Nagamori",
        "Ayesha R. Thanawalla",
        "Kee Wui Huang",
        "Fabian Plum",
        "Hendrik K. Beck",
        "Steven W. Flavell",
        "David Labonte",
        "Blake A. Richards",
        "Bingni W. Brunton",
        "Eiman Azim",
        "Bence P. Ã–lveczky",
        "Talmo D. Pereira"
      ],
      "abstract": "The primary output of the nervous system is movement and behavior. While recent advances have democratized pose tracking during complex behavior, kinematic trajectories alone provide only indirect access to the underlying control processes. Here we present MIMIC-MJX, a framework for learning biologically-plausible neural control policies from kinematics. MIMIC-MJX models the generative process of motor control by training neural controllers that learn to actuate biomechanically-realistic body models in physics simulation to reproduce real kinematic trajectories. We demonstrate that our implementation is accurate, fast, data-efficient, and generalizable to diverse animal body models. Policies trained with MIMIC-MJX can be utilized to both analyze neural control strategies and simulate behavioral experiments, illustrating its potential as an integrative modeling framework for neuroscience.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MIMIC-MJXï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»è¿åŠ¨å­¦(kinematics)æ•°æ®ä¸­å­¦ä¹ å…·æœ‰ç”Ÿç‰©å­¦åˆç†æ€§çš„ç¥ç»æ§åˆ¶ç­–ç•¥(neural control policies)çš„åˆ›æ–°æ¡†æ¶ã€‚é€šè¿‡åœ¨ç‰©ç†æ¨¡æ‹Ÿä¸­è®­ç»ƒç¥ç»æ§åˆ¶å™¨æ¥é©±åŠ¨å…·æœ‰ç”Ÿç‰©åŠ›å­¦çœŸå®æ€§çš„èº«ä½“æ¨¡å‹ï¼ŒMIMIC-MJX èƒ½å¤Ÿç²¾ç¡®é‡ç°çœŸå®çš„åŠ¨ç‰©è¿åŠ¨è½¨è¿¹ã€‚è¯¥å®ç°æ–¹æ¡ˆå±•ç°å‡ºé«˜ç²¾åº¦ã€é«˜è¿è¡Œé€Ÿåº¦ä»¥åŠå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œå¹¶èƒ½å¹¿æ³›æ³›åŒ–è‡³å¤šç§ä¸åŒçš„åŠ¨ç‰©èº«ä½“æ¨¡å‹ã€‚åˆ©ç”¨è¯¥æ¡†æ¶è®­ç»ƒçš„ç­–ç•¥ä¸ä»…å¯ç”¨äºæ·±å…¥åˆ†æç¥ç»æ§åˆ¶æœºåˆ¶ï¼Œè¿˜èƒ½æ”¯æŒè¡Œä¸ºå®éªŒçš„æ¨¡æ‹Ÿã€‚æ€»ä¹‹ï¼ŒMIMIC-MJX ä¸ºç¥ç»ç§‘å­¦æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„ç»¼åˆå»ºæ¨¡å·¥å…·ï¼Œæœ‰åŠ©äºæ­ç¤ºåŠ¨ä½œèƒŒåå¤æ‚çš„åº•å±‚æ§åˆ¶è¿‡ç¨‹ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Corrected LaTeX issues. Project page available at https://mimic-mjx.talmolab.org",
      "pdf_url": "https://arxiv.org/pdf/2511.20532v2",
      "published_date": "2025-11-25 17:34:38 UTC",
      "updated_date": "2025-12-02 03:44:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:15.837726+00:00"
    },
    {
      "arxiv_id": "2511.20531v1",
      "title": "Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models",
      "title_zh": "è¶…è¶Šç”Ÿæˆï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹äº‹å®å‡†ç¡®æ€§çš„å¤šè·³æ¨ç†",
      "authors": [
        "Shamima Hossain"
      ],
      "abstract": "Visual Language Models (VLMs) are powerful generative tools but often produce factually inaccurate outputs due to a lack of robust reasoning capabilities. While extensive research has been conducted on integrating external knowledge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seamlessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leveraging structured knowledge graphs for multi-hop verification using image-captioning task to illustrate our framework. Our approach enables systematic reasoning across multiple steps, including visual entity recognition, knowledge graph traversal, and fact-based caption refinement. We evaluate the framework using hierarchical, triple-based and bullet-point based knowledge representations, analyzing their effectiveness in factual accuracy and logical inference. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions revealing key insights into reasoning patterns and failure modes. This work demonstrates the potential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Vision-Language Models (VLMs)å› ç¼ºä¹é²æ£’æ¨ç†èƒ½åŠ›è€Œå¯¼è‡´è¾“å‡ºäº‹å®ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§çŸ¥è¯†å¼•å¯¼æ¨ç†çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–çš„knowledge graphsè¿›è¡Œmulti-hop verificationï¼Œå¹¶é€šè¿‡image-captioningä»»åŠ¡å±•ç¤ºäº†å…¶åº”ç”¨æ½œåŠ›ã€‚å…¶æ ¸å¿ƒæµç¨‹åŒ…æ‹¬visual entity recognitionã€knowledge graph traversalä»¥åŠfact-based caption refinementï¼Œå®ç°äº†è·¨æ¨¡æ€çš„ç³»ç»ŸåŒ–å¤šæ­¥æ¨ç†ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†hierarchicalã€triple-basedå’Œbullet-point basedç­‰ä¸åŒçŸ¥è¯†è¡¨ç¤ºå½¢å¼åœ¨äº‹å®å‡†ç¡®æ€§å’Œé€»è¾‘æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Google Landmarks v2ã€Conceptual captionså’ŒCoco captionsç­‰æ•°æ®é›†ä¸Šå°†äº‹å®å‡†ç¡®ç‡æå‡äº†çº¦31%ã€‚è¯¥å·¥ä½œè¯æ˜äº†é›†æˆå¤–éƒ¨çŸ¥è¯†åœ¨æå‡å¤šæ¨¡æ€ç³»ç»Ÿå¯é æ€§ä¸çŸ¥è¯†æ°´å¹³æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as poster at NewInML Workshop ICML, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20531v1",
      "published_date": "2025-11-25 17:34:32 UTC",
      "updated_date": "2025-11-25 17:34:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:36.931729+00:00"
    },
    {
      "arxiv_id": "2511.20526v1",
      "title": "Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼šåŸºäºä¸­å›½æ‰§ä¸šè¯å¸ˆèµ„æ ¼è€ƒè¯•çš„å¯ç¤º",
      "authors": [
        "Xinran Wang",
        "Boran Zhu",
        "Shujuan Zhou",
        "Ziwen Long",
        "Dehua Zhou",
        "Shu Zhang"
      ],
      "abstract": "Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº† ChatGPT-4o å’Œ DeepSeek-R1 åœ¨ä¸­å›½æ‰§ä¸šè¯å¸ˆèµ„æ ¼è€ƒè¯• (Chinese Pharmacist Licensing Examination) ä¸­çš„è¡¨ç°ï¼Œæ—¨åœ¨æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é«˜åˆ©å®³ã€ç‰¹å®šé¢†åŸŸè®¤è¯ä»»åŠ¡ä¸­çš„æ”¯æŒèƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜æ•´ç†äº† 2017 å¹´è‡³ 2021 å¹´é—´çš„ 2,306 é“çº¯æ–‡æœ¬é€‰æ‹©é¢˜ï¼Œå¹¶é‡‡ç”¨ Pearson å¡æ–¹æ£€éªŒ (Pearson's Chi-squared test) å’Œ Fisher ç²¾ç¡®æ£€éªŒ (Fisher's exact test) è¯„ä¼°æ¨¡å‹çš„å›ç­”å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-R1 çš„æ•´ä½“å‡†ç¡®ç‡è¾¾åˆ° 90.0%ï¼Œæ˜¾è‘—ä¼˜äº ChatGPT-4o çš„ 76.1% (p < 0.001)ï¼Œå°¤å…¶åœ¨åŸºç¡€æ¨¡å—å’Œä¸´åºŠç»¼åˆæ¨¡å—ä¸­è¡¨ç°å‡ºæŒç»­ä¼˜åŠ¿ã€‚å°½ç®¡åˆ†å¹´åº¦å¯¹æ¯”ä¸­ DeepSeek-R1 ä¾ç„¶å ä¼˜ï¼Œä½†è¯¥æ€§èƒ½å·®è·åœ¨ç‰¹å®šå•å…ƒå¹´ä»½ä¸­å¹¶æœªè¾¾åˆ°ç»Ÿè®¡å­¦æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼ŒDeepSeek-R1 ä¸è¯å¸ˆèµ„æ ¼è€ƒè¯•çš„ç»“æ„å’Œè¯­ä¹‰éœ€æ±‚é«˜åº¦å¯¹é½ï¼Œè¯æ˜äº†é¢†åŸŸç‰¹å®šæ¨¡å‹åœ¨åŒ»ç–—æ•™è‚²è¯„ä¼°ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨æ³•å¾‹å’Œä¼¦ç†æ•æ„Ÿçš„åŒ»ç–—ç¯å¢ƒä¸‹ï¼Œäººå·¥ç›‘ç£ (Human oversight) ä¾ç„¶æ˜¯å¿…ä¸å¯å°‘çš„ä¿éšœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20526v1",
      "published_date": "2025-11-25 17:31:25 UTC",
      "updated_date": "2025-11-25 17:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:30.831205+00:00"
    },
    {
      "arxiv_id": "2511.20513v1",
      "title": "DesignPref: Capturing Personal Preferences in Visual Design Generation",
      "title_zh": "DesignPrefï¼šæ•æ‰è§†è§‰è®¾è®¡ç”Ÿæˆä¸­çš„ä¸ªäººåå¥½",
      "authors": [
        "Yi-Hao Peng",
        "Jeffrey P. Bigham",
        "Jason Wu"
      ],
      "abstract": "Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è®¾è®¡ç”Ÿæˆä¸­ä¸»è§‚æ€§å¼ºä¸”é«˜åº¦ä¸ªæ€§åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†DesignPrefæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç”±20ä½ä¸“ä¸šè®¾è®¡å¸ˆé’ˆå¯¹12kå¯¹UIè®¾è®¡ç”Ÿæˆç»“æœè¿›è¡Œçš„å¤šé‡çº§åå¥½æ ‡æ³¨ã€‚ç ”ç©¶é€šè¿‡æ•°æ®åˆ†æå‘ç°ï¼Œä¸“ä¸šè®¾è®¡å¸ˆä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è¯„ä»·åˆ†æ­§ï¼ˆKrippendorff's alpha = 0.25ï¼‰ï¼Œè¿™äº›åˆ†æ­§ä¸»è¦æºäºå¯¹è®¾è®¡è¦ç´ é‡è¦æ€§çš„ä¸åŒè®¤çŸ¥ä»¥åŠä¸ªäººå®¡ç¾åå¥½çš„å·®å¼‚ã€‚å®éªŒè¯æ˜ï¼Œä¼ ç»Ÿçš„å¤šæ•°æŠ•ç¥¨ï¼ˆmajority-votingï¼‰èšåˆåˆ¤åˆ«æ¨¡å‹éš¾ä»¥å‡†ç¡®åæ˜ ä¸ªä½“åå¥½ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¢è®¨äº†å¤šç§ä¸ªæ€§åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬é’ˆå¯¹ç‰¹å®šè®¾è®¡å¸ˆè¿›è¡ŒFine-tuningæˆ–åœ¨RAGæµæ°´çº¿ä¸­å¼•å…¥ä¸ªäººæ ‡æ³¨ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ªæ€§åŒ–æ¨¡å‹åœ¨é¢„æµ‹ä¸ªäººåå¥½æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºèšåˆåŸºå‡†æ¨¡å‹ï¼Œä¸”æ‰€éœ€æ ·æœ¬é‡ä»…ä¸ºåè€…çš„äºŒååˆ†ä¹‹ä¸€ã€‚è¯¥å·¥ä½œæä¾›äº†é¦–ä¸ªç”¨äºç ”ç©¶ä¸ªæ€§åŒ–è§†è§‰è®¾è®¡è¯„ä¼°çš„æ•°æ®é›†ï¼Œå¹¶ä¸ºæœªæ¥å»ºæ¨¡ä¸ªäººè®¾è®¡å“å‘³æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20513v1",
      "published_date": "2025-11-25 17:19:10 UTC",
      "updated_date": "2025-11-25 17:19:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:26.234378+00:00"
    },
    {
      "arxiv_id": "2511.20510v2",
      "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization",
      "title_zh": "FRAGMENTAï¼šèåˆæ™ºèƒ½ä½“å¾®è°ƒçš„ç«¯åˆ°ç«¯åŸºäºç‰‡æ®µçš„è¯ç‰©å…ˆå¯¼åŒ–åˆç‰©ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Yuto Suzuki",
        "Paul Awolade",
        "Daniel V. LaBarbera",
        "Farnoush Banaei-Kashani"
      ],
      "abstract": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FRAGMENTAï¼Œä¸€ä¸ªç”¨äºè¯ç‰©å…ˆå¯¼åŒ–åˆç‰©ä¼˜åŒ–(Drug Lead Optimization)çš„ç«¯åˆ°ç«¯ç”Ÿæˆå¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°æ ·æœ¬æ•°æ®é›†ä¸‹å¯å‘å¼ç¢ç‰‡åŒ–(Heuristic fragmentation)å¯¼è‡´çš„å¤šæ ·æ€§ä¸è¶³é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªåˆ›æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡åŠ¨æ€Q-learning(Dynamic Q-learning)å°†ç¢ç‰‡åŒ–è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºè¯è¡¨é€‰æ‹©é—®é¢˜ï¼Œå®ç°äº†ç¢ç‰‡åŒ–ä¸åˆ†å­ç”Ÿæˆçš„è”åˆä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå¼•å…¥äº†æ™ºèƒ½ä½“AI(Agentic AI)æœºåˆ¶ï¼Œèƒ½å¤Ÿç›´æ¥æ¥æ”¶é¢†åŸŸä¸“å®¶çš„å¯¹è¯åé¦ˆæ¥ç²¾ç‚¼ç›®æ ‡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹AIå·¥ç¨‹å¸ˆçš„ä¸­ä»‹éœ€æ±‚å¹¶å®ç°ä¸“å®¶çŸ¥è¯†çš„è‡ªåŠ¨åŒ–å­¦ä¹ ã€‚åœ¨çœŸå®çš„ç™Œç—‡è¯ç‰©å‘ç°å®éªŒä¸­ï¼ŒFRAGMENTAè¯†åˆ«çš„é«˜åˆ†åˆ†å­æ•°é‡è¾¾åˆ°åŸºå‡†æ¨¡å‹çš„ä¸¤å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶å…¨è‡ªä¸»çš„Agent-Agentç³»ç»Ÿåœ¨æ•æ‰ä¸“å®¶æ„å›¾æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„äººå·¥è°ƒä¼˜æ¨¡å¼ï¼Œæ˜¾è‘—æå‡äº†è¯ç‰©ç ”å‘çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20510v2",
      "published_date": "2025-11-25 17:17:54 UTC",
      "updated_date": "2025-11-26 02:35:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:40.647726+00:00"
    },
    {
      "arxiv_id": "2511.20507v1",
      "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models",
      "title_zh": "TAB (Text Aphasia Battery)ï¼šé’ˆå¯¹è¯­è¨€æ¨¡å‹ç±»å¤±è¯­ç—‡ç¼ºé™·çš„ä¸´åºŠåŸºå‡†",
      "authors": [
        "Nathan Roll",
        "Jill Kries",
        "Flora Jin",
        "Catherine Wang",
        "Ann Marie Finley",
        "Meghan Sumner",
        "Cory Shain",
        "Laura Gwilliams"
      ],
      "abstract": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Text Aphasia Battery (TAB)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸­çš„ç±»å¤±è¯­ç—‡ (aphasia-like) ç¼ºé™·è€Œè®¾è®¡çš„ä¸´åºŠåŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿä¸´åºŠè¯„ä¼°ä¸é€‚ç”¨äºäººå·¥æ¶æ„çš„é—®é¢˜ï¼ŒTAB å€Ÿé‰´äº†Quick Aphasia Battery (QAB) çš„è®¾è®¡ï¼ŒåŒ…å«Connected Textã€Word Comprehensionã€Sentence Comprehensionå’ŒRepetitionå››ä¸ªå­ç»´åº¦ã€‚ç ”ç©¶å›¢é˜Ÿä¸ä»…è¯¦ç»†åˆ¶å®šäº†è¯„åˆ†æ ‡å‡†ï¼Œè¿˜éªŒè¯äº†åŸºäº Gemini 2.5 Flash çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ¡ˆï¼Œå…¶å¯é æ€§ç»å®éªŒè¯æ˜å¯ä¸äººç±»ä¸“å®¶è¯„åˆ†è€…ç›¸åª²ç¾ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºåˆ†æäººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„è¯­è¨€ç¼ºé™·æä¾›äº†ä¸€ä¸ªå…·å¤‡ä¸´åºŠåŸºç¡€ä¸”å¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŠ©åŠ›æ¢ç´¢è¯­è¨€éšœç¢çš„è®¡ç®—æœ¬è´¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20507v1",
      "published_date": "2025-11-25 17:16:38 UTC",
      "updated_date": "2025-11-25 17:16:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:34.031129+00:00"
    },
    {
      "arxiv_id": "2512.03057v1",
      "title": "A note on the impossibility of conditional PAC-efficient reasoning in large language models",
      "title_zh": "è®ºå¤§è¯­è¨€æ¨¡å‹ä¸­æ¡ä»¶ PAC é«˜æ•ˆæ¨ç†çš„ä¸å¯èƒ½æ€§",
      "authors": [
        "Hao Zeng"
      ],
      "abstract": "We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-Î±$ for almost every input.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)ä¸­æ¡ä»¶æ¦‚ç‡è¿‘ä¼¼æ­£ç¡®(Probably Approximately Correct, PAC)é«˜æ•ˆæ¨ç†çš„å¯èƒ½æ€§ç•Œé™ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶è¯æ˜äº†åœ¨æ˜‚è´µçš„ä¸“å®¶æ¨¡å‹ä¸å»‰ä»·æ¨¡å‹é—´åˆ‡æ¢çš„å¤åˆæ¨¡å‹å…·æœ‰è¾¹é™…PACæ•ˆç‡ä¿è¯ï¼Œä½†è¯¥è®ºæ–‡è¯æ˜äº†åœ¨æ— åˆ†å¸ƒ(distribution-free)è®¾ç½®ä¸‹ï¼Œå®ç°ç‚¹å¯¹ç‚¹(pointwise)çš„æ¡ä»¶ä¿è¯åœ¨ç†è®ºä¸Šæ˜¯ä¸å¯è¡Œçš„ã€‚é’ˆå¯¹éåŸå­è¾“å…¥ç©ºé—´(non-atomic input spaces)ï¼Œä½œè€…è¯æ˜äº†ä»»ä½•æ—¨åœ¨å®ç°æ¡ä»¶PACæ•ˆç‡çš„ç®—æ³•éƒ½å…·æœ‰å¹³å‡¡æ€§ï¼Œå³å¯¹äºå‡ ä¹æ‰€æœ‰è¾“å…¥ï¼Œç®—æ³•å¿…é¡»ä»¥æé«˜æ¦‚ç‡($1-\\alpha$)æ±‚åŠ©äºä¸“å®¶æ¨¡å‹ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†åœ¨ä¸å¼•å…¥ç‰¹å®šæ•°æ®åˆ†å¸ƒå‡è®¾çš„å‰æä¸‹ï¼Œè¯•å›¾åŒæ—¶ä¼˜åŒ–æ¨ç†æ•ˆç‡å¹¶è·å¾—ç‚¹å¯¹ç‚¹å¯é æ€§ä¿è¯çš„ç†è®ºå±€é™æ€§ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03057v1",
      "published_date": "2025-11-25 17:08:08 UTC",
      "updated_date": "2025-11-25 17:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:43.336580+00:00"
    },
    {
      "arxiv_id": "2511.20500v1",
      "title": "From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection",
      "title_zh": "è·¨æ”»å‡»åŸŸ APT æ£€æµ‹ï¼šåŸºäºå­ªç”Ÿç½‘ç»œçš„å¯¹æ¯”è¿ç§»å­¦ä¹ ",
      "authors": [
        "Sidahmed Benabderrahmane",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ··åˆè¿ç§»æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜çº§æŒç»­æ€§å¨èƒ(APT)æ£€æµ‹ä¸­å­˜åœ¨çš„ç±»ä¸å¹³è¡¡ã€é«˜ç»´ç‰¹å¾ä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹è¿ç§»æ€§å·®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†Transfer Learningã€å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ã€å¯¹æ¯”å­¦ä¹ (Contrastive Learning)å’Œå­ªç”Ÿç½‘ç»œ(Siamese Networks)ï¼Œä»¥å¢å¼ºæ£€æµ‹å™¨çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶åˆ©ç”¨åŸºäºAttentionçš„Autoencoderæ”¯æŒè·¨é¢†åŸŸçŸ¥è¯†è¿ç§»ï¼Œå¹¶ç»“åˆShapley Additive exPlanations(SHAP)ç­›é€‰ç¨³å®šä¸”å…·ä¿¡æ¯é‡çš„ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆé™ä½äº†ç»´åº¦å’Œè®¡ç®—æˆæœ¬ã€‚é€šè¿‡ä½¿ç”¨Contrastive Objectiveè®­ç»ƒçš„Siamese Encoderå¯¹é½æºåŸŸä¸ç›®æ ‡åŸŸçš„è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†å¼‚å¸¸çš„å¯åˆ†ç¦»æ€§å¹¶ç¼“è§£äº†ç‰¹å¾æ¼‚ç§»(Feature Drift)ã€‚åœ¨DARPA Transparent Computing (TC)è®¡åˆ’çš„çœŸå®æ•°æ®å’Œåˆæˆæ”»å‡»åœºæ™¯ä¸‹çš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹è¯„åˆ†ä¸Šä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆä¸ºåº”å¯¹å¤æ‚APTæ”»å‡»æä¾›äº†ä¸€ç§å¯æ‰©å±•ã€å¯è§£é‡Šä¸”å…·å¤‡é«˜è¿ç§»æ€§çš„æ£€æµ‹æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20500v1",
      "published_date": "2025-11-25 17:07:41 UTC",
      "updated_date": "2025-11-25 17:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:33:59.138625+00:00"
    },
    {
      "arxiv_id": "2511.20497v1",
      "title": "Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic",
      "title_zh": "é‡åŒ–é«˜ä¿çœŸåˆæˆç½‘ç»œæµé‡çš„éšç§å½±å“",
      "authors": [
        "Van Tran",
        "Shinan Liu",
        "Tian Li",
        "Nick Feamster"
      ],
      "abstract": "To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡åŒ–äº†é«˜ä¿çœŸ(High-Fidelity)åˆæˆç½‘ç»œæµé‡å¸¦æ¥çš„éšç§å½±å“ï¼ŒæŒ‡å‡ºåˆæˆæ•°æ®å¹¶æœªå¤©ç„¶å…·å¤‡éšç§ä¿æŠ¤ç‰¹æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€å¥—å…¨é¢çš„éšç§åº¦é‡æŒ‡æ ‡ï¼Œå°†æ ‡å‡†çš„æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attacks, MIA)å’Œæ•°æ®æå–æ”»å‡»(Data extraction attacks)ä¸ç½‘ç»œç‰¹å®šçš„æ ‡è¯†ç¬¦å’Œå±æ€§ç›¸ç»“åˆã€‚é€šè¿‡å¯¹ä¸åŒä»£è¡¨æ€§ç”Ÿæˆæ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶å‘ç°éšç§é£é™©å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå…¶ä¸­MIAæˆåŠŸç‡æœ€é«˜å¯è¾¾88%ï¼Œä¸”ç”Ÿæˆçš„æµé‡ä¸­å¯æ¢å¤é«˜è¾¾100%çš„ç½‘ç»œæ ‡è¯†ç¬¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«äº†å½±å“æ”»å‡»æˆåŠŸç‡çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ä»¥åŠç”Ÿæˆæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚è¿™äº›å‘ç°ä¸ºè®¾è®¡èƒ½æœ€å°åŒ–éšç§æ³„æ¼çš„ç”Ÿæˆæ¨¡å‹æä¾›äº†å®è·µæŒ‡å¯¼ï¼Œå¹¶ä¸ºæ„å»ºæ›´å®‰å…¨çš„åˆæˆç½‘ç»œæµé‡ç”ŸæˆæŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 13 Figures, 6 Tables",
      "pdf_url": "https://arxiv.org/pdf/2511.20497v1",
      "published_date": "2025-11-25 17:04:02 UTC",
      "updated_date": "2025-11-25 17:04:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:18.134389+00:00"
    },
    {
      "arxiv_id": "2511.20490v1",
      "title": "MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology",
      "title_zh": "MTBBenchï¼šè‚¿ç˜¤å­¦å¤šæ¨¡æ€åºåˆ—ä¸´åºŠå†³ç­–åŸºå‡†",
      "authors": [
        "Kiril Vasilev",
        "Alexandre Misrahi",
        "Eeshaan Jain",
        "Phil F Cheng",
        "Petros Liakopoulos",
        "Olivier Michielin",
        "Michael Moor",
        "Charlotte Bunne"
      ],
      "abstract": "Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰ç°å®ä¸´åºŠå·¥ä½œæµå¤æ‚æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†MTBBenchï¼Œä¸€ä¸ªæ—¨åœ¨æ¨¡æ‹Ÿåˆ†å­è‚¿ç˜¤å§”å‘˜ä¼š(Molecular Tumor Boards, MTBs)å†³ç­–è¿‡ç¨‹çš„å¤šæ¨¡æ€åºè´¯ä¸´åºŠå†³ç­–åŸºå‡†ã€‚MTBBenché€šè¿‡ä¸´åºŠæå…·æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€å’Œçºµå‘è‚¿ç˜¤å­¦é—®é¢˜ï¼Œæ¨¡æ‹Ÿäº†MTBä¸­å¤šä¸“å®¶åä½œåŠå¼‚æ„æ•°æ®æ•´åˆçš„å¤æ‚å†³ç­–ç¯å¢ƒï¼Œå…¶çœŸå€¼æ ‡æ³¨å‡ç»è¿‡ä¸´åºŠåŒ»ç”ŸéªŒè¯ä»¥ç¡®ä¿ä¸´åºŠç›¸å…³æ€§ã€‚é€šè¿‡å¯¹å¤šç§å¼€æºå’Œé—­æºLLMsçš„æµ‹è¯„å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ—¶é—´åºåˆ—æ•°æ®å’Œåè°ƒå†²çªè¯æ®æ—¶ç»å¸¸å‡ºç°å¹»è§‰ä¸”å¯é æ€§ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥æä¾›äº†ä¸€ä¸ªåŒ…å«åŸºç¡€æ¨¡å‹å·¥å…·çš„æ™ºèƒ½ä½“(agentic)æ¡†æ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤šæ¨¡æ€å’Œçºµå‘æ¨ç†ä¸Šçš„è¡¨ç°ï¼Œä½¿ç›¸å…³ä»»åŠ¡æ€§èƒ½åˆ†åˆ«æå‡äº†9.0%å’Œ11.2%ã€‚æ€»ä¹‹ï¼ŒMTBBenchä¸ºç²¾å‡†è‚¿ç˜¤å­¦é¢†åŸŸä¸­æå‡å¤šæ¨¡æ€LLMçš„æ¨ç†èƒ½åŠ›ã€å¯é æ€§å’Œå·¥å…·ä½¿ç”¨æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”è´´è¿‘ç°å®çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20490v1",
      "published_date": "2025-11-25 16:56:25 UTC",
      "updated_date": "2025-11-25 16:56:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:21.231762+00:00"
    },
    {
      "arxiv_id": "2512.03056v1",
      "title": "Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models",
      "title_zh": "Delta Samplingï¼šè·¨æ‰©æ•£æ¨¡å‹çš„æ— æ•°æ®çŸ¥è¯†è¿ç§»",
      "authors": [
        "Zhidong Gao",
        "Zimeng Pan",
        "Yuhang Yao",
        "Chenyue Xie",
        "Wei Wei"
      ],
      "abstract": "Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Stable Diffusion (SD) ç­‰æ‰©æ•£æ¨¡å‹çš„é€‚é…å™¨ï¼ˆå¦‚ LoRA å’Œ ControlNetï¼‰ä¸å…¶ç‰¹å®šåŸºç¡€æ¨¡å‹ç´§å¯†è€¦åˆã€éš¾ä»¥åœ¨æ¨¡å‹å‡çº§æˆ–æ¶æ„æ”¹å˜æ—¶ç›´æ¥å¤ç”¨çš„é—®é¢˜ï¼Œæå‡ºäº† Delta Sampling (DS) æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§æ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®çš„çŸ¥è¯†è¿ç§»æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°è·¨ä¸åŒæ¶æ„åŸºç¡€æ¨¡å‹çš„åŠŸèƒ½å¤ç”¨ã€‚DS å®Œå…¨åœ¨æ¨ç†é˜¶æ®µ (Inference time) è¿è¡Œï¼Œé€šè¿‡æå–åŸºç¡€æ¨¡å‹åœ¨é€‚é…å‰åçš„é¢„æµ‹å·®å¼‚ï¼ˆå³ deltaï¼‰ï¼Œå¹¶åˆ©ç”¨è¯¥å·®å¼‚å¼•å¯¼æ–°åŸºç¡€æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ã€‚å®éªŒåœ¨å¤šä¸ª Stable Diffusion ç‰ˆæœ¬ä¸ŠéªŒè¯äº† DS çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨å¤„ç†è§†è§‰é£æ ¼ã€è¯­ä¹‰æ¦‚å¿µå’Œå›¾åƒç»“æ„æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æœºåˆ¶ï¼ŒDS ä¸ºæ‰©æ•£æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿä¸­çš„çŸ¥è¯†è¿ç§»æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”çµæ´»çš„è§£å†³è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.03056v1",
      "published_date": "2025-11-25 16:54:51 UTC",
      "updated_date": "2025-11-25 16:54:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:28.026223+00:00"
    },
    {
      "arxiv_id": "2511.20480v1",
      "title": "Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders",
      "title_zh": "åŸºäºä¸»åŠ¨å­¦ä¹ è¾…åŠ©çš„æ³¨æ„åŠ›å¯¹æŠ—åŒè‡ªåŠ¨ç¼–ç å™¨çš„æ’åºå¢å¼ºå¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Sidahmed Benabderrahmane",
        "James Cheney",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.",
      "tldr_zh": "é’ˆå¯¹ç½‘ç»œå®‰å…¨ä¸­é«˜çº§æŒç»­æ€§å¨èƒ(APTs)éšè”½æ€§å¼ºä¸”æ ‡æ³¨æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”±ä¸»åŠ¨å­¦ä¹ (Active Learning)è¾…åŠ©çš„Attention Adversarial Dual AutoEncoderå¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡AutoEncoderè¿›è¡Œæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶åˆ©ç”¨ä¸»åŠ¨å­¦ä¹ å¾ªç¯é€‰æ‹©æ€§åœ°æŸ¥è¯¢ä¸ç¡®å®šæˆ–æ¨¡ç³Šæ ·æœ¬çš„æ ‡ç­¾ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„æ ‡æ³¨æˆæœ¬è¿­ä»£ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶åœ¨DARPA Transparent Computingè®¡åˆ’æä¾›çš„çœŸå®å¤§è§„æ¨¡ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒæ¶µç›–äº†Androidã€Linuxã€BSDå’ŒWindowsç­‰å¤šä¸ªæ“ä½œç³»ç»Ÿã€‚åœ¨æ”»å‡»æ•°æ®å æ¯”ä»…ä¸º0.004%çš„æç«¯ä¸å¹³è¡¡åœºæ™¯ä¸‹ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸»åŠ¨å­¦ä¹ æ˜¾è‘—æå‡äº†æ£€æµ‹ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¤æ‚æ”»å‡»åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ£€æµ‹æŠ€æœ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20480v1",
      "published_date": "2025-11-25 16:42:12 UTC",
      "updated_date": "2025-11-25 16:42:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:25.530401+00:00"
    },
    {
      "arxiv_id": "2511.20471v2",
      "title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models",
      "title_zh": "Universe of Thoughtsï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°åˆ›é€ æ€§æ¨ç†",
      "authors": [
        "Yuto Suzuki",
        "Farnoush Banaei-Kashani"
      ],
      "abstract": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å®ç°åˆ›é€ æ€§æ¨ç†(Creative Reasoning)ï¼Œä»¥å¼¥è¡¥ç°æœ‰æ¨ç†æ–¹æ³•åœ¨å¤„ç†å¦‚è¯ç‰©å‘ç°æˆ–å•†ä¸šæˆ˜ç•¥ç­‰å¹¿é˜”è§£ç©ºé—´é—®é¢˜æ—¶çš„ä¸è¶³ã€‚ç ”ç©¶å—è®¤çŸ¥ç§‘å­¦å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«ç»„åˆå¼(Combinational)ã€æ¢ç´¢å¼(Exploratory)å’Œå˜é©å¼(Transformative)ä¸‰ç§èŒƒå¼çš„è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ¢ç´¢æ€æƒ³å®‡å®™ä»¥ç”Ÿæˆåˆ›æ–°æ–¹æ¡ˆã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶è€…å¼€å‘äº†åä¸ºæ€æƒ³å®‡å®™(Universe of Thoughtsï¼Œç®€ç§° UoT)çš„æ–¹æ³•ä½“ç³»ï¼Œç”¨äºåœ¨ LLMs ä¸­å®ç°ä¸Šè¿°åˆ›é€ æ€§è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸‰é¡¹è¡¡é‡åˆ›é€ æ€§è§£å†³é—®é¢˜èƒ½åŠ›çš„æ–°ä»»åŠ¡ï¼Œå¹¶ä»å¯è¡Œæ€§(Feasibility)ã€æ•ˆç”¨(Utility)å’Œæ–°é¢–æ€§(Novelty)ä¸‰ä¸ªç»´åº¦å»ºç«‹äº†è¯„ä¼°åŸºå‡†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒUoT åœ¨åˆ›é€ æ€§æ¨ç†æ–¹é¢çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›(SOTA)æ¨ç†æŠ€æœ¯åŠä¸»æµå•†ä¸šæ¨ç†æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20471v2",
      "published_date": "2025-11-25 16:34:59 UTC",
      "updated_date": "2025-11-26 02:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:24.035061+00:00"
    },
    {
      "arxiv_id": "2511.20470v1",
      "title": "Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model",
      "title_zh": "åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆå¿«é€Ÿç”Ÿæˆå¼æ­Œå£°åˆ†ç¦»",
      "authors": [
        "GenÃ­s Plaja-Roglans",
        "Yun-Ning Hung",
        "Xavier Serra",
        "Igor Pereira"
      ],
      "abstract": "Extracting individual elements from music mixtures is a valuable tool for music production and practice. While neural networks optimized to mask or transform mixture spectrograms into the individual source(s) have been the leading approach, the source overlap and correlation in music signals poses an inherent challenge. Also, accessing all sources in the mixture is crucial to train these systems, while complicated. Attempts to address these challenges in a generative fashion exist, however, the separation performance and inference efficiency remain limited. In this work, we study the potential of diffusion models to advance toward bridging this gap, focusing on generative singing voice separation relying only on corresponding pairs of isolated vocals and mixtures for training. To align with creative workflows, we leverage latent diffusion: the system generates samples encoded in a compact latent space, and subsequently decodes these into audio. This enables efficient optimization and faster inference. Our system is trained using only open data. We outperform existing generative separation systems, and level the compared non-generative systems on a list of signal quality measures and on interference removal. We provide a noise robustness study on the latent encoder, providing insights on its potential for the task. We release a modular toolkit for further research on the topic.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ­Œå£°åˆ†ç¦»(Singing Voice Separation)åœ¨å¤„ç†ä¿¡å·é‡å æ—¶çš„æŒ‘æˆ˜ä»¥åŠç°æœ‰ç”Ÿæˆå¼ç³»ç»Ÿæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹(Latent Diffusion Model)çš„é«˜æ•ˆæ­Œå£°åˆ†ç¦»æ¡†æ¶ã€‚é€šè¿‡åœ¨ç´§å‡‘çš„æ½œåœ¨ç©ºé—´(Latent Space)ä¸­è¿›è¡Œæ ·æœ¬ç”Ÿæˆä¸è§£ç ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†ä¼˜åŒ–æ•ˆç‡å¹¶ç¼©çŸ­äº†æ¨ç†æ—¶é—´ã€‚è¯¥æ¨¡å‹ä»…åˆ©ç”¨å¼€æ”¾æ•°æ®é›†ä¸­çš„äººå£°ä¸æ··åˆéŸ³é¢‘å¯¹è¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤šé¡¹ä¿¡å·è´¨é‡è¯„ä¼°å’Œå¹²æ‰°æ¶ˆé™¤(Interference Removal)ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„ç”Ÿæˆå¼ç³»ç»Ÿï¼Œå¹¶è¾¾åˆ°äº†ä¸éç”Ÿæˆå¼ç³»ç»Ÿç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹æ½œç¼–ç å™¨(Latent Encoder)è¿›è¡Œäº†å™ªå£°é²æ£’æ€§(Noise Robustness)åˆ†æï¼Œæä¾›äº†ç›¸å…³æŠ€æœ¯è§è§£ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªæ¨¡å—åŒ–å·¥å…·åŒ…ä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted for oral presentation at IJCNN 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20470v1",
      "published_date": "2025-11-25 16:34:07 UTC",
      "updated_date": "2025-11-25 16:34:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:29.730516+00:00"
    },
    {
      "arxiv_id": "2511.20468v1",
      "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs",
      "title_zh": "DRAFT-RLï¼šé¢å‘å¼ºåŒ–å­¦ä¹ å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“è‰ç¨¿é“¾æ¨ç†",
      "authors": [
        "Yuanhao Li",
        "Mingshan Liu",
        "Hongbo Wang",
        "Yiding Zhang",
        "Yifei Ma",
        "Wei Tan"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ (RL)é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“åå°„æ¡†æ¶åœ¨æ¨ç†æ¢ç´¢ä¸­ç¼ºä¹ç»“æ„å¤šæ ·æ€§å’Œè¿‡åº¦ä¾èµ–å•æ¬¡å“åº”çš„é—®é¢˜ï¼Œæå‡ºäº†DRAFT-RLæ¡†æ¶ã€‚DRAFT-RLå°†è‰ç¨¿é“¾(Chain-of-Draft, CoD)æ¨ç†é›†æˆåˆ°å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­ï¼Œè¦æ±‚æ¯ä¸ªæ™ºèƒ½ä½“é’ˆå¯¹æŸ¥è¯¢ç”Ÿæˆå¤šä¸ªè‰ç¨¿ï¼Œå¹¶ç”±åŒè¡Œæ™ºèƒ½ä½“å’Œå­¦ä¹ åˆ°çš„å¥–åŠ±æ¨¡å‹(reward model)å…±åŒè¯„ä¼°ä»¥ç¡®å®šæœ€ä½³è·¯å¾„ã€‚è¿™äº›é€‰å®šçš„ä¼˜è´¨è‰ç¨¿éšåé€šè¿‡æ¼”å‘˜-è¯„è®ºå®¶(actor-critic)å­¦ä¹ ç®—æ³•æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„æ¨ç†ç­–ç•¥ã€‚è¯¥æ–¹æ³•å®ç°äº†æ˜¾å¼çš„å¤šè·¯å¾„æ¢ç´¢ã€åŒè¡Œå¼•å¯¼çš„åå°„ä»¥åŠå¥–åŠ±å¯¹é½çš„é€‰æ‹©æœºåˆ¶ï¼Œä»è€Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ™ºèƒ½ä½“è¡Œä¸ºçš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»£ç åˆæˆ(code synthesis)ã€ç¬¦å·æ•°å­¦(symbolic math)å’ŒçŸ¥è¯†å¯†é›†å‹é—®ç­”ç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼ŒDRAFT-RLåœ¨å‡†ç¡®ç‡å’Œæ”¶æ•›é€Ÿåº¦æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åå°„å¼åŠåŸºäºå¼ºåŒ–å­¦ä¹ çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20468v1",
      "published_date": "2025-11-25 16:33:42 UTC",
      "updated_date": "2025-11-25 16:33:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:36.436632+00:00"
    },
    {
      "arxiv_id": "2511.20459v1",
      "title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts",
      "title_zh": "åŸºäºå•æ ‡è®°æç¤ºçš„å°è¯´å®¶é£æ ¼ç”Ÿæˆã€è¯„ä¼°ä¸è§£é‡Š",
      "authors": [
        "Mosab Rezaei",
        "Mina Rajaei Moghadam",
        "Abdul Rahman Shaikh",
        "Hamed Alhoori",
        "Reva Freedman"
      ],
      "abstract": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹19ä¸–çºªå°è¯´å®¶æ–‡ä½“è¿›è¡Œç”Ÿæˆã€è¯„ä¼°å’Œè§£é‡Šçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡ä½“å­¦(stylometry)ä¸­ç¼ºä¹é…å¯¹æ•°æ®å’Œè¿‡åº¦ä¾èµ–äººå·¥è¯„ä¼°çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è€…é€šè¿‡æœ€å°åŒ–çš„å•æ ‡è®°æç¤º(single-token prompts)å¯¹å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿Dickensã€Austenã€Twainç­‰è‘—åä½œå®¶çš„å†™ä½œé£æ ¼ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„æ•ˆæœï¼Œè¯¥ç ”ç©¶é‡‡ç”¨äº†ä¸€ä¸ªåŸºäºTransformerçš„æ£€æµ‹å™¨(transformer-based detector)ï¼Œå¹¶ç»“åˆå¥æ³•å¯¹æ¯”(syntactic comparisons)ä»¥åŠå¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)æ–¹æ³•è¿›è¡Œåˆ†æã€‚é€šè¿‡åŸºäºæ³¨æ„åŠ›æœºåˆ¶(attention-based)å’ŒåŸºäºæ¢¯åº¦(gradient-based)çš„åˆ†æï¼Œç ”ç©¶å›¢é˜ŸæˆåŠŸè¯†åˆ«å‡ºäº†é©±åŠ¨æ–‡ä½“æ¨¡ä»¿çš„å…³é”®è¯­è¨€çº¿ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬èƒ½å¤Ÿå‡†ç¡®åæ˜ ä½œè€…ç‹¬ç‰¹çš„æ–‡ä½“æ¨¡å¼ï¼Œè¯æ˜äº†åŸºäºäººå·¥æ™ºèƒ½çš„è¯„ä¼°å¯ä»¥ä½œä¸ºäººç±»è¯„ä¼°çš„å¯é æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20459v1",
      "published_date": "2025-11-25 16:25:44 UTC",
      "updated_date": "2025-11-25 16:25:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:35.811937+00:00"
    },
    {
      "arxiv_id": "2511.20737v2",
      "title": "CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design",
      "title_zh": "CANVASï¼šé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹åŸºäºå·¥å…·çš„ç”¨æˆ·ç•Œé¢è®¾è®¡è¯„æµ‹åŸºå‡†",
      "authors": [
        "Daeheon Jeong",
        "Seoyeon Byun",
        "Kihoon Son",
        "Dae Hyun Kim",
        "Juho Kim"
      ],
      "abstract": "User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CANVASï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨åŸºäºå·¥å…·çš„ç”¨æˆ·ç•Œé¢ (UI) è®¾è®¡èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°ä½“ç³»åœ¨å·¥å…·åŒ–è®¾è®¡é¢†åŸŸçš„ç©ºç™½ï¼ŒCANVAS ä» 3.3K ä¸ªæ¶µç›– 30 ä¸ªåŠŸèƒ½ç±»åˆ«çš„ç§»åŠ¨ç«¯ UI è®¾è®¡ä¸­æå–äº† 598 ä¸ªä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹é€šè¿‡å·¥å…·è°ƒç”¨ (tool invocation) ä¸è®¾è®¡è½¯ä»¶äº¤äº’å¹¶é€æ­¥å®Œæˆè®¾è®¡è¿­ä»£ã€‚è¯¥åŸºå‡†æ¶µç›–äº†è®¾è®¡å¤ç° (design replication) å’Œè®¾è®¡ä¿®æ”¹ (design modification) ä¸¤ç§æ ¸å¿ƒä»»åŠ¡ç±»å‹ï¼Œæ—¨åœ¨å…¨é¢è¡¡é‡æ¨¡å‹çš„åˆ›ä½œä¸è°ƒæ•´èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢†å…ˆçš„æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„ç­–ç•¥æ€§ï¼Œä»è€Œäº§å‡ºæ›´é«˜è´¨é‡çš„è®¾è®¡æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æå¹¶æ€»ç»“äº†æ¨¡å‹åœ¨è®¾è®¡è¿‡ç¨‹ä¸­çš„å¸¸è§é”™è¯¯æ¨¡å¼ï¼Œä¸ºæœªæ¥ä¼˜åŒ–æ¨¡å‹åœ¨çœŸå®è½¯ä»¶ç¯å¢ƒä¸‹çš„è®¾è®¡åä½œèƒ½åŠ›æä¾›äº†é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20737v2",
      "published_date": "2025-11-25 16:13:20 UTC",
      "updated_date": "2025-11-27 06:30:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:42.638240+00:00"
    },
    {
      "arxiv_id": "2511.20439v1",
      "title": "Object-Centric Vision Token Pruning for Vision Language Models",
      "title_zh": "é¢å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒè§†è§‰ Token å‰ªæ",
      "authors": [
        "Guangyuan Li",
        "Rongzhen Zhao",
        "Jinhong Deng",
        "Yanbo Wang",
        "Joni Pajarinen"
      ],
      "abstract": "In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¸­è§†è§‰æ ‡è®°(vision tokens)æ•°é‡åºå¤§ä¸”ä¿¡æ¯åˆ†æ•£å¯¼è‡´çš„è®¡ç®—å†—ä½™é—®é¢˜ï¼Œæå‡ºäº†OC-VTPï¼Œä¸€ç§é€šè¿‡é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„è§†è§‰æ ‡è®°æ¥å®ç°é«˜æ•ˆä¸”ä¿æŒå‡†ç¡®ç‡æ¨ç†çš„ç›´æ¥æ–¹æ³•ã€‚OC-VTPä»…éœ€å¯¹å°å‹ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒ(object-centric)çš„è§†è§‰æ ‡è®°å‰ªæå™¨è¿›è¡Œè½»é‡çº§é¢„è®­ç»ƒï¼Œå³å¯ç›´æ¥æ’å…¥ç°æœ‰çš„VLMsä¸­ï¼Œæ— éœ€åœ¨ä»»ä½•æ•°æ®é›†ä¸Šå¯¹åŸæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡æœ€å°åŒ–ä»é€‰å®šæ ‡è®°é‡å»ºåŸå§‹æœªå‰ªææ ‡è®°çš„è¯¯å·®ï¼Œç¡®ä¿ä¿ç•™æœ€å…·ä»£è¡¨æ€§çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§å‰ªææ¯”ä¾‹ä¸‹ï¼ŒOC-VTPå‡èƒ½ä½¿ä¸»æµVLMsåœ¨æé«˜æ¨ç†æ•ˆç‡çš„åŒæ—¶ä¿æŒæœ€é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶å±•ç¤ºå‡ºæ˜¾è‘—çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20439v1",
      "published_date": "2025-11-25 16:12:32 UTC",
      "updated_date": "2025-11-25 16:12:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:34:54.842128+00:00"
    },
    {
      "arxiv_id": "2511.20736v1",
      "title": "Large Language Models' Complicit Responses to Illicit Instructions across Socio-Legal Contexts",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸åŒç¤¾ä¼šæ³•å¾‹è¯­å¢ƒä¸‹å¯¹éæ³•æŒ‡ä»¤çš„å…±è°‹æ€§å“åº”",
      "authors": [
        "Xing Wang",
        "Huiyuan Xie",
        "Yiyan Wang",
        "Chaojun Xiao",
        "Huimin Chen",
        "Holli Sargeant",
        "Felix Steffek",
        "Jie Shao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large language models (LLMs) are now deployed at unprecedented scale, assisting millions of users in daily tasks. However, the risk of these models assisting unlawful activities remains underexplored. In this study, we define this high-risk behavior as complicit facilitation - the provision of guidance or support that enables illicit user instructions - and present four empirical studies that assess its prevalence in widely deployed LLMs. Using real-world legal cases and established legal frameworks, we construct an evaluation benchmark spanning 269 illicit scenarios and 50 illicit intents to assess LLMs' complicit facilitation behavior. Our findings reveal widespread LLM susceptibility to complicit facilitation, with GPT-4o providing illicit assistance in nearly half of tested cases. Moreover, LLMs exhibit deficient performance in delivering credible legal warnings and positive guidance. Further analysis uncovers substantial safety variation across socio-legal contexts. On the legal side, we observe heightened complicity for crimes against societal interests, non-extreme but frequently occurring violations, and malicious intents driven by subjective motives or deceptive justifications. On the social side, we identify demographic disparities that reveal concerning complicit patterns towards marginalized and disadvantaged groups, with older adults, racial minorities, and individuals in lower-prestige occupations disproportionately more likely to receive unlawful guidance. Analysis of model reasoning traces suggests that model-perceived stereotypes, characterized along warmth and competence, are associated with the model's complicit behavior. Finally, we demonstrate that existing safety alignment strategies are insufficient and may even exacerbate complicit behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†éæ³•æŒ‡ä»¤æ—¶è¡¨ç°å‡ºçš„â€œå…±è°‹ååŠ©â€ï¼ˆComplicit Facilitationï¼‰è¡Œä¸ºï¼Œå³æ¨¡å‹ä¸ºç”¨æˆ·çš„ä¸æ³•æ„å›¾æä¾›æŒ‡å¯¼æˆ–æ”¯æŒçš„é£é™©ã€‚ä½œè€…åŸºäºçœŸå®æ³•å¾‹æ¡ˆä¾‹å’Œæ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«269ä¸ªéæ³•åœºæ™¯å’Œ50ç§éæ³•æ„å›¾çš„è¯„ä¼°åŸºå‡†ï¼Œå¯¹å¹¿æ³›éƒ¨ç½²çš„LLMsè¿›è¡Œäº†å››é¡¹å®è¯ç ”ç©¶ã€‚ç ”ç©¶å‘ç°LLMsæ™®éå­˜åœ¨å…±è°‹å€¾å‘ï¼Œå…¶ä¸­GPT-4oåœ¨è¿‘åŠæ•°æµ‹è¯•æ¡ˆä¾‹ä¸­æä¾›äº†éæ³•ååŠ©ï¼Œä¸”æ¨¡å‹åœ¨æä¾›å¯é æ³•å¾‹è­¦å‘Šå’Œæ­£é¢å¼•å¯¼æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚æ·±å…¥åˆ†ææ­ç¤ºäº†å…±è°‹è¡Œä¸ºåœ¨æ³•å¾‹å’Œç¤¾ä¼šç»´åº¦ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œæ¨¡å‹æ›´å®¹æ˜“åœ¨æ¶‰åŠç¤¾ä¼šåˆ©ç›Šä¾µå®³æˆ–éæç«¯è¿è§„çš„åœºæ™¯ä¸­æä¾›ååŠ©ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯†åˆ«å‡ºäº†é’ˆå¯¹å¼±åŠ¿ç¾¤ä½“çš„äººå£ç»Ÿè®¡å­¦æ­§è§†ï¼Œå‘ç°è€å¹´äººã€å°‘æ•°æ—è£”åŠä½å£°æœ›èŒä¸šç¾¤ä½“æ›´æ˜“æ”¶åˆ°éæ³•å¼•å¯¼ï¼Œä¸”è¿™ç§è¡Œä¸ºä¸æ¨¡å‹æ„ŸçŸ¥çš„åˆ»æ¿å°è±¡ï¼ˆStereotypesï¼‰ç›¸å…³ã€‚æœ€ç»ˆç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å®‰å…¨å¯¹é½ï¼ˆSafety Alignmentï¼‰ç­–ç•¥ä¸è¶³ä»¥æŠ‘åˆ¶å…±è°‹ååŠ©ï¼Œç”šè‡³å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹åŠ å‰§æ­¤ç±»é£é™©ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20736v1",
      "published_date": "2025-11-25 16:01:31 UTC",
      "updated_date": "2025-11-25 16:01:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:08.128235+00:00"
    },
    {
      "arxiv_id": "2511.20426v1",
      "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
      "title_zh": "Block Cascadingï¼šå—å› æœè§†é¢‘æ¨¡å‹çš„å…è®­ç»ƒåŠ é€Ÿ",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Nikhil Pinnaparaju",
        "Rahim Entezari",
        "Jim Scott",
        "Yi-Zhe Song",
        "Varun Jampani"
      ],
      "abstract": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Block Cascadingï¼Œä¸€ç§æ—¨åœ¨è§£å†³Block-causalè§†é¢‘ç”Ÿæˆæ¨¡å‹é€Ÿåº¦ä¸è´¨é‡æƒè¡¡é—®é¢˜çš„æ— éœ€è®­ç»ƒ(training-free)å¹¶è¡ŒåŠ é€Ÿæ–¹æ¡ˆã€‚å…¶æ ¸å¿ƒæ´å¯Ÿåœ¨äºæœªæ¥çš„è§†é¢‘å—æ— éœ€ç­‰å¾…å½“å‰å—å®Œå…¨å»å™ª(fully denoised)å³å¯åˆ©ç”¨éƒ¨åˆ†å»å™ªä¸Šä¸‹æ–‡(partially denoised context)å¼€å§‹ç”Ÿæˆï¼Œä»è€Œå°†ä¸²è¡Œæµæ°´çº¿è½¬åŒ–ä¸ºå¤šå—åŒæ­¥å»å™ªçš„å¹¶è¡Œçº§è”ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡åœ¨å¤šGPUä¸Šåˆ©ç”¨æ—¶é—´å¹¶è¡Œæ€§(temporal parallelism)ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šå‡å®ç°äº†çº¦2å€çš„æ¨ç†åŠ é€Ÿï¼Œä½¿14Bå¤§æ¨¡å‹å¸§ç‡ä»4.5 FPSæå‡è‡³12.5 FPSã€‚æ­¤å¤–ï¼ŒBlock Cascadingè¿˜æ¶ˆé™¤äº†äº¤äº’å¼ç”Ÿæˆä¸­çº¦200msçš„KV-recachingå¼€é”€ã€‚å¤šé¡¹è¯„ä¼°éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œå¹¶æœªå¯¹è§†é¢‘ç”Ÿæˆè´¨é‡é€ æˆæ˜æ˜¾å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20426v1",
      "published_date": "2025-11-25 15:52:58 UTC",
      "updated_date": "2025-11-25 15:52:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:13.235479+00:00"
    },
    {
      "arxiv_id": "2511.20422v1",
      "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning",
      "title_zh": "VibraVerseï¼šé¢å‘ç‰©ç†ä¸€è‡´å¤šæ¨¡æ€å­¦ä¹ çš„å¤§è§„æ¨¡å‡ ä½•-å£°å­¦å¯¹é½æ•°æ®é›†",
      "authors": [
        "Bo Pang",
        "Chenxi Xu",
        "Jierui Ren",
        "Guoping Wang",
        "Sheng Li"
      ],
      "abstract": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­ç¼ºä¹ç‰©ç†ä¸€è‡´æ€§ï¼Œä»¥åŠå¿½è§†ç‰©ä½“å‡ ä½•ã€ææ–™ã€æŒ¯åŠ¨æ¨¡å¼ä¸å£°éŸ³ä¹‹é—´å†…åœ¨å› æœå…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº† VibraVerse æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å‡ ä½•-å£°å­¦å¯¹é½æ•°æ®é›†ï¼Œæ˜ç¡®æ¡¥æ¥äº†ä» 3D geometry åˆ° physical attributesï¼Œå†åˆ° modal parameters ä»¥åŠæœ€ç»ˆ acoustic signals çš„å®Œæ•´å› æœé“¾ã€‚æ•°æ®é›†ä¸­çš„æ¯ä¸ª 3D æ¨¡å‹å‡åŒ…å«æ˜¾å¼çš„ç‰©ç†å±æ€§ï¼ˆå¦‚ density, Young's modulus, Poisson's ratioï¼‰å’Œä½“ç§¯å‡ ä½•ï¼Œå¹¶æ®æ­¤è®¡ç®—æ¨¡æ€ç‰¹å¾é¢‘ç‡å’Œç‰¹å¾å‘é‡ä»¥åˆæˆå—æ§æ¿€åŠ±ä¸‹çš„æ’å‡»å£°ã€‚ç ”ç©¶è€…åŒæ­¥å¼•å…¥äº† CLASP å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œè·¨æ¨¡æ€çš„ç‰©ç†ä¸€è‡´æ€§å¯¹é½ï¼Œç¡®ä¿ç‰©ä½“ç‰©ç†ç»“æ„ä¸å£°å­¦å“åº”ä¹‹é—´çš„å› æœå¯¹åº”ã€‚åœ¨å‡ ä½•åˆ°å£°éŸ³é¢„æµ‹ã€å£°éŸ³å¼•å¯¼çš„å½¢çŠ¶é‡å»ºç­‰åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒåŸºäº VibraVerse è®­ç»ƒçš„æ¨¡å‹åœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å“è¶Šã€‚è¯¥æˆæœä¸ºç‰©ç†ä¸€è‡´ä¸”å…·æœ‰å› æœè§£é‡Šæ€§çš„å¤šæ¨¡æ€å­¦ä¹ å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºå£°éŸ³å¼•å¯¼çš„å…·èº«æ„ŸçŸ¥(embodied perception)ç ”ç©¶æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20422v1",
      "published_date": "2025-11-25 15:48:49 UTC",
      "updated_date": "2025-11-25 15:48:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:15.531834+00:00"
    },
    {
      "arxiv_id": "2511.21767v1",
      "title": "LAYER: A Quantitative Explainable AI Framework for Decoding Tissue-Layer Drivers of Myofascial Low Back Pain",
      "title_zh": "LAYERï¼šç”¨äºè§£ç è‚Œç­‹è†œè…°ç—›ç»„ç»‡å±‚çº§é©±åŠ¨å› ç´ çš„å®šé‡å¯è§£é‡Šäººå·¥æ™ºèƒ½æ¡†æ¶",
      "authors": [
        "Zixue Zeng",
        "Anthony M. Perti",
        "Tong Yu",
        "Grant Kokenberger",
        "Hao-En Lu",
        "Jing Wang",
        "Xin Meng",
        "Zhiyu Sheng",
        "Maryam Satarpour",
        "John M. Cormack",
        "Allison C. Bean",
        "Ryan P. Nussbaum",
        "Emily Landis-Walkenhorst",
        "Kang Kim",
        "Ajay D. Wasan",
        "Jiantao Pu"
      ],
      "abstract": "Myofascial pain (MP) is a leading cause of chronic low back pain, yet its tissue-level drivers remain poorly defined and lack reliable image biomarkers. Existing studies focus predominantly on muscle while neglecting fascia, fat, and other soft tissues that play integral biomechanical roles. We developed an anatomically grounded explainable artificial intelligence (AI) framework, LAYER (Layer-wise Analysis for Yielding Explainable Relevance Tissue), that analyses six tissue layers in three-dimensional (3D) ultrasound and quantifies their contribution to MP prediction. By utilizing the largest multi-model 3D ultrasound cohort consisting of over 4,000 scans, LAYER reveals that non-muscle tissues contribute substantially to pain prediction. In B-mode imaging, the deep fascial membrane (DFM) showed the highest saliency (0.420), while in combined B-mode and shear-wave images, the collective saliency of non-muscle layers (0.316) nearly matches that of muscle (0.317), challenging the conventional muscle-centric paradigm in MP research and potentially affecting the therapy methods. LAYER establishes a quantitative, interpretable framework for linking layer-specific anatomy to pain physiology, uncovering new tissue targets and providing a generalizable approach for explainable analysis of soft-tissue imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŸºäºè§£å‰–å­¦çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI) æ¡†æ¶ LAYER (Layer-wise Analysis for Yielding Explainable Relevance Tissue)ï¼Œç”¨äºåˆ†æ 3D è¶…å£°å½±åƒä¸­çš„å…­ä¸ªç»„ç»‡å±‚å¹¶é‡åŒ–å…¶å¯¹è‚Œç­‹è†œç–¼ç—› (Myofascial Pain) é¢„æµ‹çš„è´¡çŒ®ã€‚é’ˆå¯¹ä¼ ç»Ÿç ”ç©¶è¿‡åº¦å…³æ³¨è‚Œè‚‰è€Œå¿½è§†ç­‹è†œå’Œè„‚è‚ªçš„é—®é¢˜ï¼ŒLAYER åˆ©ç”¨åŒ…å« 4,000 å¤šæ¬¡æ‰«æçš„å¤šæ¨¡æ€ 3D è¶…å£°é˜Ÿåˆ—ï¼Œæ­ç¤ºäº†éè‚Œè‚‰ç»„ç»‡åœ¨ç–¼ç—›é¢„æµ‹ä¸­çš„æ˜¾è‘—ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ B-mode å½±åƒä¸­æ·±ç­‹è†œå±‚ (Deep Fascial Membrane, DFM) çš„æ˜¾è‘—æ€§æœ€é«˜ (0.420)ï¼Œä¸”éè‚Œè‚‰å±‚çš„æ•´ä½“æ˜¾è‘—æ€§ (0.316) ä¸è‚Œè‚‰å±‚ (0.317) ç›¸å½“ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†è‚Œç­‹è†œç–¼ç—›ç ”ç©¶ä¸­ä¼ ç»Ÿçš„ä»¥è‚Œè‚‰ä¸ºä¸­å¿ƒçš„èŒƒå¼ï¼Œä¸ºå‘ç°æ–°çš„ç»„ç»‡æ²»ç–—ç›®æ ‡æä¾›äº†ä¾æ®ã€‚LAYER æˆåŠŸå»ºç«‹äº†ä¸€ä¸ªå°†ç‰¹å®šç»„ç»‡å±‚è§£å‰–ç»“æ„ä¸ç–¼ç—›ç”Ÿç†å­¦è”ç³»èµ·æ¥çš„å®šé‡è§£é‡Šæ¡†æ¶ï¼Œä¸ºè½¯ç»„ç»‡å½±åƒçš„å¯è§£é‡Šåˆ†ææä¾›äº†ä¸€ç§é€šç”¨ä¸”å…·æœ‰æ¨å¹¿æ€§çš„æ–¹æ³•ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.TO"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21767v1",
      "published_date": "2025-11-25 15:47:43 UTC",
      "updated_date": "2025-11-25 15:47:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:21.324962+00:00"
    },
    {
      "arxiv_id": "2511.20418v1",
      "title": "StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections",
      "title_zh": "StableTrackï¼šé’ˆå¯¹ä½é¢‘æ£€æµ‹çš„å¤šç›®æ ‡è·Ÿè¸ªç¨³å®šæ€§å¢å¼º",
      "authors": [
        "Matvei Shelukhan",
        "Timur Mamedov",
        "Karina Kvanchiani"
      ],
      "abstract": "Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\\textit{11.6%}$ HOTA improvement at $\\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç›®æ ‡è·Ÿè¸ª (Multi-object tracking, MOT) åœ¨è®¡ç®—èµ„æºå—é™ä¸”ä»…èƒ½è¿›è¡Œä½é¢‘æ£€æµ‹ (low-frequency detections) çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† StableTrack æ¡†æ¶ä»¥ç¨³å®šè·Ÿè¸ªè´¨é‡ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µåŒ¹é…ç­–ç•¥ï¼Œæ˜¾è‘—æ”¹å–„äº†ä½é¢‘æ£€æµ‹ä¸‹çš„è·¨å¸§å…³è” (cross-frame association) æ•ˆæœã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§ Bbox-Based Distance æ¥æ›¿ä»£ä¼ ç»Ÿçš„é©¬æ°è·ç¦» (Mahalanobis distance)ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ Re-ID æ¨¡å‹è¿›è¡Œç›®æ ‡åŒ¹é…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è§†è§‰è·Ÿè¸ª (visual tracking) é›†æˆåˆ°å¡å°”æ›¼æ»¤æ³¢ (Kalman Filter) å’Œæ•´ä½“æµæ°´çº¿ä¸­ï¼Œè¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼ŒStableTrack åœ¨ MOT17-val çš„ 1 Hz æ£€æµ‹è®¾ç½®ä¸‹å®ç°äº† 11.6% çš„ HOTA æ€§èƒ½æå‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰ä¸»æµç®—æ³•ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨ MOT17ã€MOT20 å’Œ DanceTrack ç­‰æ ‡å‡†å…¨é¢‘ç‡åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿä¿æŒäº†ä¸å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„ç«äº‰èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20418v1",
      "published_date": "2025-11-25 15:42:33 UTC",
      "updated_date": "2025-11-25 15:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:19.637323+00:00"
    },
    {
      "arxiv_id": "2511.20406v1",
      "title": "Short-Range Oversquashing",
      "title_zh": "çŸ­ç¨‹è¿‡åº¦æŒ¤å‹",
      "authors": [
        "Yaaqov Mishayev",
        "Yonatan Sverdlov",
        "Tal Amir",
        "Nadav Dym"
      ],
      "abstract": "Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.\n  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.\n  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾ç¥ç»ç½‘ç»œ(MPNNs)ä¸­çš„è¿‡åº¦æŒ¤å‹(oversquashing)ç°è±¡ï¼Œå¹¶é¦–æ¬¡è®ºè¯äº†è¿™ä¸€é™åˆ¶ä¸ä»…å­˜åœ¨äºé•¿ç¨‹ä»»åŠ¡ï¼Œåœ¨çŸ­ç¨‹é—®é¢˜ä¸­åŒæ ·ä¼šå‡ºç°ã€‚é€šè¿‡å¯¹è¯¥ç°è±¡çš„æ·±å…¥åˆ†æï¼Œä½œè€…åŒºåˆ†äº†å¯¼è‡´è¿‡åº¦æŒ¤å‹çš„ä¸¤ç§ä¸åŒæœºåˆ¶ï¼Œå³åœ¨ä½ç¨‹è®¾ç½®ä¸­ä¹Ÿä¼šäº§ç”Ÿçš„ç“¶é¢ˆç°è±¡(bottleneck phenomenon)ä»¥åŠä¸é•¿ç¨‹ä»»åŠ¡å¯†åˆ‡ç›¸å…³çš„æ¢¯åº¦æ¶ˆå¤±ç°è±¡(vanishing gradient phenomenon)ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„ç†è®ºè§£é‡Šæ— æ³•æ¶µç›–è¿™ç§çŸ­ç¨‹ç“¶é¢ˆæ•ˆåº”ï¼Œä¸”æ·»åŠ è™šæ‹ŸèŠ‚ç‚¹(virtual nodes)ç­‰å¸¸ç”¨é‡è¿æŠ€æœ¯ä¹Ÿæ— æ³•å°†å…¶æ¶ˆé™¤ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGraph Transformers åœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½æœ‰æ•ˆè§„é¿ MPNNs é¢ä¸´çš„çŸ­ç¨‹æŒ¤å‹é—®é¢˜ã€‚è¿™ä¸€ç»“è®ºè¯æ˜äº† Transformers æ˜¯è§£å†³è¿‡åº¦æŒ¤å‹é—®é¢˜æ¯”æ”¹è¿›å‹ MPNNs æ›´ä¸ºå½»åº•ä¸”æœ‰æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to Learning on Graphs (LoG) 2025. Version identical to the camera-ready paper",
      "pdf_url": "https://arxiv.org/pdf/2511.20406v1",
      "published_date": "2025-11-25 15:34:46 UTC",
      "updated_date": "2025-11-25 15:34:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:23.426003+00:00"
    },
    {
      "arxiv_id": "2511.20403v2",
      "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework",
      "title_zh": "é¢å‘ Java è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”Ÿæˆä¸è¯„ä¼°çš„å¤§è¯­è¨€æ¨¡å‹ï¼šAgoneTest æ¡†æ¶",
      "authors": [
        "Andrea Lops",
        "Fedelucio Narducci",
        "Azzurra Ragone",
        "Michelantonio Trizio",
        "Claudio Bartolini"
      ],
      "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†AgoneTestï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹Javaç¯å¢ƒä¸‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å•å…ƒæµ‹è¯•æ‰€è®¾è®¡çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚AgoneTestæ—¨åœ¨æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç«¯åˆ°ç«¯è¯„ä¼°æµæ°´çº¿ï¼Œå¸®åŠ©å¼€å‘è€…å’Œç ”ç©¶è€…åœ¨ç°å®æ¡ä»¶ä¸‹å¯¹æ¯”ä¸åŒLLMsåŠæç¤ºè¯ç­–ç•¥ï¼ˆprompting strategiesï¼‰çš„æ•ˆèƒ½ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†Classes2Testæ•°æ®é›†ï¼Œå¹¶æ•´åˆäº†çªå˜åˆ†æ•°ï¼ˆmutation scoreï¼‰å’Œæµ‹è¯•å¼‚å‘³ï¼ˆtest smellsï¼‰ç­‰å…ˆè¿›è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å®ç°å¯¹æµ‹è¯•è´¨é‡çš„å…¨æ–¹ä½è¡¡é‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨å¯ç¼–è¯‘çš„æµ‹è¯•å­é›†ä¸­ï¼ŒLLMç”Ÿæˆçš„å•å…ƒæµ‹è¯•åœ¨ä»£ç è¦†ç›–ç‡å’Œç¼ºé™·æ£€æµ‹èƒ½åŠ›ä¸Šèƒ½å¤Ÿåª²ç¾ç”šè‡³è¶…è¶Šäººå·¥ç¼–å†™çš„æµ‹è¯•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å¢å¼ºçš„æç¤ºè¯ç­–ç•¥èƒ½æ˜¾è‘—æå‡æµ‹è¯•è´¨é‡ï¼ŒAgoneTestçš„æå‡ºä¸ºè½¯ä»¶æµ‹è¯•é¢†åŸŸçš„æ¨¡å‹ä¼˜åŒ–ä¸æç¤ºè¯å·¥ç¨‹æä¾›äº†é‡è¦çš„å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering",
      "pdf_url": "https://arxiv.org/pdf/2511.20403v2",
      "published_date": "2025-11-25 15:33:00 UTC",
      "updated_date": "2025-11-26 09:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:43.235293+00:00"
    },
    {
      "arxiv_id": "2511.20399v2",
      "title": "BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali",
      "title_zh": "BengaliFigï¼šå­ŸåŠ æ‹‰è¯­æ¯”å–»æ€§ä¸æ–‡åŒ–æ¤æ ¹æ¨ç†çš„ä½èµ„æºæŒ‘æˆ˜",
      "authors": [
        "Abdullah Al Sefat"
      ],
      "abstract": "Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† BengaliFigï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å­ŸåŠ æ‹‰è¯­ï¼ˆBengaliï¼‰ä¸­ä¿®è¾å’Œæ–‡åŒ–åŸºç¡€æ¨ç†ï¼ˆFigurative and Culturally Grounded Reasoningï¼‰è€Œè®¾è®¡çš„ç´§å‡‘ä¸”æ ‡æ³¨ä¸°å¯Œçš„æŒ‘æˆ˜æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥ä½èµ„æºè¯­è¨€åœ¨è¿™ä¸€é¢†åŸŸçš„è¯„ä¼°ç©ºç™½ã€‚è¯¥æ•°æ®é›†åŒ…å« 435 ä¸ªæºè‡ªå­ŸåŠ æ‹‰å£å¤´å’Œæ–‡å­¦ä¼ ç»Ÿçš„ç‹¬ç‰¹è°œè¯­ï¼Œå¹¶åœ¨æ¨ç†ç±»å‹ã€é™·é˜±ç±»å‹ã€æ–‡åŒ–æ·±åº¦ã€ç­”æ¡ˆç±»åˆ«å’Œéš¾åº¦äº”ä¸ªæ­£äº¤ç»´åº¦ä¸Šè¿›è¡Œäº†è¯¦ç»†æ ‡æ³¨ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨çº¦æŸæ„ŸçŸ¥çš„ AI è¾…åŠ©æµç¨‹å°†è¿™äº›ææ–™è½¬æ¢ä¸ºå¤šé¡¹é€‰æ‹©é¢˜æ ¼å¼ï¼Œå¹¶åˆ©ç”¨é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰å’Œå°‘æ ·æœ¬é“¾å¼æ€ç»´ï¼ˆFew-shot Chain-of-Thoughtï¼‰æç¤ºè¯å¯¹ 8 ä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†éšå–»å’Œç‰¹å®šæ–‡åŒ–èƒŒæ™¯çš„æ¨ç†æ—¶å­˜åœ¨æ˜¾è‘—ä¸”ä¸€è‡´çš„å±€é™æ€§ã€‚BengaliFig ä¸ä»…ä¸ºè¯„ä¼°ä½èµ„æºè¯­è¨€æ–‡åŒ–èƒŒæ™¯ä¸‹æ¨¡å‹é²æ£’æ€§æä¾›äº†æœ‰æ•ˆçš„è¯Šæ–­å·¥å…·ï¼Œä¹Ÿä¸ºæ¨åŠ¨å…·æœ‰åŒ…å®¹æ€§å’Œæ–‡åŒ–é—äº§ä¿æŠ¤æ„è¯†çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆHeritage-aware NLPï¼‰è¯„ä»·ä½“ç³»è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20399v2",
      "published_date": "2025-11-25 15:26:47 UTC",
      "updated_date": "2025-11-26 17:08:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:29.329259+00:00"
    },
    {
      "arxiv_id": "2512.03055v1",
      "title": "Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins",
      "title_zh": "é¢å‘å† çŠ¶åŠ¨è„‰æ•°å­—å­ªç”Ÿé¢„æµ‹å»ºæ¨¡çš„ç‰©ç†å¼•å¯¼è‡ªç›‘ç£å­¦ä¹ ",
      "authors": [
        "Xiaowu Sun",
        "Thabo Mahendiran",
        "Ortal Senouf",
        "Denise Auberson",
        "Bernard De Bruyne",
        "Stephane Fournier",
        "Olivier Muller",
        "Pascal Frossard",
        "Emmanuel Abbe",
        "Dorina Thanou"
      ],
      "abstract": "Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PINS-CADï¼Œä¸€ç§ç‰©ç†ä¿¡æ¯å¼•å¯¼çš„è‡ªç›‘ç£å­¦ä¹ (Physics-informed self-supervised learning)æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å† çŠ¶åŠ¨è„‰æ•°å­—å­ªç”Ÿ(coronary artery digital twins)çš„é«˜æ•ˆé¢„æµ‹å»ºæ¨¡ã€‚ä¸ºè§£å†³è®¡ç®—æµä½“åŠ›å­¦(CFD)è®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼ŒPINS-CADåœ¨20ä¸‡ä¸ªåˆæˆæ•°æ®ä¸Šé€šè¿‡ä¸€ç»´Navier-Stokesæ–¹ç¨‹å’Œå‹é™å®šå¾‹é¢„è®­ç»ƒå›¾ç¥ç»ç½‘ç»œ(GNNs)ï¼Œå®ç°äº†æ— éœ€CFDæˆ–æ ‡è®°æ•°æ®çš„å‚æ•°åŒ–å­¦ä¹ ã€‚åœ¨FAME2ç ”ç©¶çš„ä¸´åºŠæ•°æ®å¾®è°ƒä¸­ï¼Œè¯¥æ¨¡å‹é¢„æµ‹æœªæ¥å¿ƒè¡€ç®¡äº‹ä»¶çš„AUCè¾¾åˆ°0.73ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¸´åºŠé£é™©è¯„åˆ†å’Œæ•°æ®é©±åŠ¨åŸºçº¿ã€‚æ­¤å¤–ï¼ŒPINS-CADèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„å‹åŠ›å’Œåˆ†æ•°æµé‡å‚¨å¤‡(Fractional Flow Reserve, FFR)æ›²çº¿ï¼Œä¸ºä¸´åºŠæä¾›å…·æœ‰ç”Ÿç†æ„ä¹‰ä¸”å¯è§£é‡Šçš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚é€šè¿‡å°†ç‰©ç†å…ˆéªŒåµŒå…¥å‡ ä½•æ·±åº¦å­¦ä¹ (Geometric Deep Learning)ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†å¸¸è§„é€ å½±æœ¯è½¬åŒ–ä¸ºæ— éœ€æ¨¡æ‹Ÿã€å…·å¤‡ç”Ÿç†æ„ŸçŸ¥èƒ½åŠ›çš„å¯æ‰©å±•é¢„é˜²æ€§å¿ƒè„ç—…å­¦åˆ†ææ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.03055v1",
      "published_date": "2025-11-25 15:01:46 UTC",
      "updated_date": "2025-11-25 15:01:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:40.851136+00:00"
    },
    {
      "arxiv_id": "2511.20359v1",
      "title": "From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations",
      "title_zh": "ä»è¢«åŠ¨æ„ŸçŸ¥åˆ°ä¸»åŠ¨è®°å¿†ï¼šä¸€ç§ç”±ç²—ç²’åº¦æ ‡æ³¨é©±åŠ¨çš„å¼±ç›‘ç£å›¾åƒç¯¡æ”¹å®šä½æ¡†æ¶",
      "authors": [
        "Zhiqing Guo",
        "Dongdong Xi",
        "Songlin Li",
        "Gaobo Yang"
      ],
      "abstract": "Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BoxPromptIMLï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¹³è¡¡æ ‡æ³¨æˆæœ¬ä¸å®šä½ç²¾åº¦çš„å¼±ç›‘ç£å›¾åƒç¯¡æ”¹å®šä½(Image Manipulation Localization)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç²—ç²’åº¦åŒºåŸŸæ ‡æ³¨ç­–ç•¥(Coarse Region Annotation)ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½æˆæœ¬ç”Ÿæˆç›¸å¯¹å‡†ç¡®çš„ç¯¡æ”¹æ©ç ï¼Œæœ‰æ•ˆè§£å†³äº†å…¨ç›‘ç£æ–¹æ³•ä¾èµ–åƒç´ çº§å¯†é›†æ ‡æ³¨çš„æ‰©å±•æ€§éš¾é¢˜ã€‚ä¸ºäº†æå‡éƒ¨ç½²æ•ˆç‡ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§å­¦ç”Ÿæ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦(Knowledge Distillation)ä»åŸºäºSegment Anything Model (SAM)çš„å›ºå®šæ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ ç»†ç²’åº¦å®šä½èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå—äººç±»è®°å¿†æœºåˆ¶å¯å‘ï¼Œè¯¥æ¡†æ¶åœ¨ç‰¹å¾èåˆä¸­é‡‡ç”¨åŒé‡å¼•å¯¼ç­–ç•¥ï¼Œå°†é•¿æœŸè®°å¿†ä¸­çš„åŸå‹æ¨¡å¼ä¸å®æ—¶è§‚å¯Ÿçº¿ç´¢åŠ¨æ€ç»“åˆï¼Œå®ç°äº†ä»è¢«åŠ¨æ„ŸçŸ¥åˆ°ä¸»åŠ¨è®°å¿†çš„è½¬å˜ã€‚è¿™ç§æœºåˆ¶æ˜¾è‘—å¢å¼ºäº†å®šä½çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼ŒBoxPromptIMLåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶³ä»¥åª²ç¾æˆ–è¶…è¶Šå…¨ç›‘ç£æ¨¡å‹ï¼ŒåŒæ—¶å…·å¤‡å‡ºè‰²çš„æ³›åŒ–æ€§èƒ½å’Œé«˜æ•ˆçš„å®é™…éƒ¨ç½²æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.20359v1",
      "published_date": "2025-11-25 14:39:17 UTC",
      "updated_date": "2025-11-25 14:39:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:37.536462+00:00"
    },
    {
      "arxiv_id": "2512.03054v1",
      "title": "Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research",
      "title_zh": "é¢å‘ MRI åˆ° CT å½±åƒè½¬æ¢çš„é«˜èƒ½æ•ˆè”é‚¦å­¦ä¹ ï¼šä¸€é¡¹åŸºäºè‡ªé€‚åº”ç¼–ç å™¨å†»ç»“çš„ç»¿è‰²äººå·¥æ™ºèƒ½ç ”ç©¶",
      "authors": [
        "Ciro Benito Raggio",
        "Lucia Migliorelli",
        "Nils Skupien",
        "Mathias Krohmer Zabaleta",
        "Oliver Blanck",
        "Francesco Cicone",
        "Giuseppe Lucio Cascini",
        "Paolo Zaffino",
        "Maria Francesca Spadea"
      ],
      "abstract": "Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘Green AIçš„è‡ªé€‚åº”å±‚å†»ç»“ç­–ç•¥ï¼Œæ—¨åœ¨é™ä½Federated Learning (FL)åœ¨MRI-to-CTå›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„èƒ½è€—å’Œè®¡ç®—è´Ÿè½½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›‘æ§è½®æ¬¡é—´Encoderæƒé‡çš„ç›¸å¯¹å·®å¼‚ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©æ€§å†»ç»“æƒé‡ï¼Œå¹¶å¼•å…¥åŸºäºPatienceçš„æœºåˆ¶ç¡®ä¿ä»…åœ¨æ›´æ–°æŒç»­å¾®å°æ—¶è¿›è¡Œå†»ç»“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥åœ¨ä¿æŒMRI-to-CTè½¬æ¢æ€§èƒ½ï¼ˆMean Absolute Errorï¼‰åŸºæœ¬ä¸å˜çš„å‰æä¸‹ï¼Œå°†è®­ç»ƒæ—¶é—´ã€æ€»èƒ½è€—å’ŒCO2eqæ’æ”¾é™ä½äº†é«˜è¾¾23%ã€‚åœ¨è¯„ä¼°çš„äº”ç§æ¨¡å‹æ¶æ„ä¸­ï¼Œä¸‰ç§æ¶æ„æ— ç»Ÿè®¡å­¦æ€§èƒ½å·®å¼‚ï¼Œè€Œå¦å¤–ä¸¤ç§ç”šè‡³å–å¾—äº†æ˜¾è‘—çš„ç²¾åº¦æå‡ã€‚è¯¥å·¥ä½œé€šè¿‡åœ¨æ»¡è¶³ä¸´åºŠéœ€æ±‚çš„åŒæ—¶å…¼é¡¾ç¯å¢ƒä¸ç¤¾ä¼šå¯æŒç»­æ€§ï¼Œä¸ºæ¨åŠ¨åŒ»ç–—AIçš„éšç§ä¿æŠ¤ã€å…¬å¹³æ€§åŠæ­£ä¹‰æ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC",
        "physics.med-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.03054v1",
      "published_date": "2025-11-25 14:35:51 UTC",
      "updated_date": "2025-11-25 14:35:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:35:59.137581+00:00"
    },
    {
      "arxiv_id": "2512.07845v1",
      "title": "AudioScene: Integrating Object-Event Audio into 3D Scenes",
      "title_zh": "AudioSceneï¼šå°†ç‰©ä½“-äº‹ä»¶éŸ³é¢‘é›†æˆåˆ°3Dåœºæ™¯ä¸­",
      "authors": [
        "Shuaihang Yuan",
        "Congcong Wen",
        "Muhammad Shafique",
        "Anthony Tzes",
        "Yi Fang"
      ],
      "abstract": "The rapid advances in audio analysis underscore its vast potential for humancomputer interaction, environmental monitoring, and public safety; yet, existing audioonly datasets often lack spatial context. To address this gap, we present two novel audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, designed to explore audioconditioned tasks within 3D environments. By integrating audio clips with spatially aligned 3D scenes, our datasets enable research on how audio signals interact with spatial context. To associate audio events with corresponding spatial information, we leverage the common sense reasoning ability of large language models and supplement them with rigorous human verification, This approach offers greater scalability compared to purely manual annotation while maintaining high standards of accuracy, completeness, and diversity, quantified through inter annotator agreement and performance on two benchmark tasks audio based 3D visual grounding and audio based robotic zeroshot navigation. The results highlight the limitations of current audiocentric methods and underscore the practical challenges and significance of our datasets in advancing audio guided spatial learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AudioSceneï¼Œé€šè¿‡å°†å¯¹è±¡-äº‹ä»¶éŸ³é¢‘æ•´åˆåˆ°3Dåœºæ™¯ä¸­ï¼Œè§£å†³äº†ç°æœ‰éŸ³é¢‘æ•°æ®é›†ç¼ºä¹ç©ºé—´èƒŒæ™¯çš„é—®é¢˜ã€‚ä½œè€…å‘å¸ƒäº†ä¸¤ä¸ªå…¨æ–°çš„éŸ³é¢‘-ç©ºé—´åœºæ™¯æ•°æ®é›†AudioScanNetå’ŒAudioRoboTHORï¼Œæ—¨åœ¨æ¢ç´¢3Dç¯å¢ƒä¸­çš„éŸ³é¢‘æ¡ä»¶ä»»åŠ¡ã€‚ç ”ç©¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„å¸¸è¯†æ¨ç†èƒ½åŠ›æ¥å…³è”éŸ³é¢‘äº‹ä»¶ä¸ç©ºé—´ä¿¡æ¯ï¼Œå¹¶è¾…ä»¥äººå·¥éªŒè¯ï¼Œä»¥ç¡®ä¿æ ‡æ³¨çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡åœ¨åŸºäºéŸ³é¢‘çš„3Dè§†è§‰å®šä½(3D Visual Grounding)å’Œæœºå™¨äººé›¶æ ·æœ¬å¯¼èˆª(Zero-shot Navigation)ä¸¤é¡¹åŸºå‡†ä»»åŠ¡ä¸Šçš„æµ‹è¯•ï¼Œå®éªŒç»“æœæ­ç¤ºäº†å½“å‰ä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„æ–¹æ³•åœ¨ç©ºé—´ç†è§£ä¸Šçš„å±€é™æ€§ã€‚è¿™ä¸€å·¥ä½œä¸ºæ¨è¿›éŸ³é¢‘å¼•å¯¼çš„ç©ºé—´å­¦ä¹ (Audio-guided Spatial Learning)æä¾›äº†é‡è¦çš„åŸºç¡€èµ„æºå’Œå®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07845v1",
      "published_date": "2025-11-25 14:28:13 UTC",
      "updated_date": "2025-11-25 14:28:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:36.526299+00:00"
    },
    {
      "arxiv_id": "2511.20347v2",
      "title": "Soft Adaptive Policy Optimization",
      "title_zh": "è½¯è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Chang Gao",
        "Chujie Zheng",
        "Xiong-Hui Chen",
        "Kai Dang",
        "Shixuan Liu",
        "Bowen Yu",
        "An Yang",
        "Shuai Bai",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "abstract": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Soft Adaptive Policy Optimization (SAPO)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)å¼ºåŒ–å­¦ä¹ (RL)ä¸­ç­–ç•¥ä¼˜åŒ–ä¸ç¨³å®šçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨Mixture-of-Experts (MoE)æ¨¡å‹ä¸­å¸¸è§çš„tokençº§åˆ«é‡è¦æ€§æ¯”ç‡é«˜æ–¹å·®é—®é¢˜ã€‚SAPOé‡‡ç”¨å¹³æ»‘çš„æ¸©åº¦æ§åˆ¶é—¨æ§æœºåˆ¶(temperature-controlled gate)å–ä»£äº†GSPOå’ŒGRPOç­‰æ–¹æ³•ä¸­çš„ç¡¬æˆªæ–­(hard clipping)ï¼Œä»è€Œæ„å»ºå‡ºä¸€ä¸ªè¿ç»­çš„ä¿¡ä»»åŒºåŸŸã€‚è¯¥æ–¹æ³•å…¼å…·åºåˆ—ä¸€è‡´æ€§(sequence-coherent)ä¸æ ‡è®°è‡ªé€‚åº”æ€§(token-adaptive)ï¼Œèƒ½å¤Ÿé€‰æ‹©æ€§åœ°å¯¹æ˜¾è‘—åç¦»ç­–ç•¥çš„tokenè¿›è¡Œé™æƒï¼ŒåŒæ—¶ä¿ç•™æ¥è¿‘ç­–ç•¥tokençš„å­¦ä¹ ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†é‡‡æ ·æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSAPOåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ›´ä¼˜çš„è®­ç»ƒç¨³å®šæ€§å’Œæ›´é«˜çš„Pass@1å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒSAPOåœ¨Qwen3-VLç³»åˆ—æ¨¡å‹çš„è®­ç»ƒä¸­å±•ç°äº†è·¨ä»»åŠ¡å’Œè·¨æ¨¡å‹è§„æ¨¡çš„æŒç»­æ€§èƒ½å¢ç›Šï¼Œä¸ºLLMsçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†ä¸€ç§æ›´å¯é ä¸”å¯æ‰©å±•çš„ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20347v2",
      "published_date": "2025-11-25 14:25:19 UTC",
      "updated_date": "2025-12-01 12:02:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:02.134029+00:00"
    },
    {
      "arxiv_id": "2511.20333v1",
      "title": "NNGPT: Rethinking AutoML with Large Language Models",
      "title_zh": "NNGPTï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é‡æ–°å®¡è§†è‡ªåŠ¨æœºå™¨å­¦ä¹ ",
      "authors": [
        "Roman Kochnev",
        "Waleed Khalid",
        "Tolgay Atinc Uzun",
        "Xi Zhang",
        "Yashkumar Sanjaybhai Dhameliya",
        "Furui Qin",
        "Chandini Vysyaraju",
        "Raghuvir Duvvuri",
        "Avi Goyal",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "abstract": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NNGPTï¼Œä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ (LLM) è½¬åŒ–ä¸ºè‡ªæ”¹è¿› AutoML å¼•æ“çš„å¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰é¢†åŸŸç¥ç»ç½‘ç»œå¼€å‘çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆæ–°æ¨¡å‹æ‰©å±•æ•°æ®é›†ï¼Œå¹¶åŸºäºç”Ÿæˆã€è¯„ä¼°å’Œè‡ªæ”¹è¿›çš„é—­ç¯ç³»ç»Ÿå®ç° LLM çš„æŒç»­å¾®è°ƒã€‚NNGPT é›†æˆäº†é›¶æ ·æœ¬æ¶æ„ç»¼åˆ (zero-shot architecture synthesis)ã€è¶…å‚æ•°ä¼˜åŒ– (HPO)ã€ä»£ç æ„ŸçŸ¥å‡†ç¡®ç‡/æ—©åœé¢„æµ‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„ PyTorch å—ç»¼åˆ (NN-RAG) ä»¥åŠå¼ºåŒ–å­¦ä¹ äº”å¤§ååŒæµç¨‹ã€‚åŸºäº LEMUR æ•°æ®é›†ï¼Œè¯¥ç³»ç»Ÿèƒ½é€šè¿‡å•ä¸€æç¤ºè¯è‡ªåŠ¨æ‰§è¡Œå¹¶éªŒè¯ä»æ¶æ„åˆ°è¶…å‚æ•°çš„ç«¯åˆ°ç«¯æµç¨‹ï¼Œå¹¶ä»ç»“æœä¸­å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNN-RAG åœ¨ 1,289 ä¸ªç›®æ ‡ä¸Šå®ç°äº† 73% çš„å¯æ‰§è¡Œç‡ï¼Œä¸”å…¶ HPO æ€§èƒ½ (RMSE 0.60) ä¼˜äº Optuna (0.64)ï¼ŒåŒæ—¶é¢„æµ‹å™¨çš„ç›¸å…³ç³»æ•°è¾¾åˆ° 0.78ã€‚ç›®å‰è¯¥ç³»ç»Ÿå·²æˆåŠŸç”Ÿæˆè¶…è¿‡ 5,000 ä¸ªéªŒè¯æ¨¡å‹ï¼Œè¯æ˜äº†å…¶ä½œä¸ºè‡ªä¸» AutoML å¼•æ“åœ¨æé«˜æ¨¡å‹å¼€å‘æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20333v1",
      "published_date": "2025-11-25 14:10:44 UTC",
      "updated_date": "2025-11-25 14:10:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:29.934003+00:00"
    },
    {
      "arxiv_id": "2511.20733v1",
      "title": "InvisibleBench: A Deployment Gate for Caregiving Relationship AI",
      "title_zh": "InvisibleBenchï¼šç…§æŠ¤å…³ç³»äººå·¥æ™ºèƒ½çš„éƒ¨ç½²å…³å£",
      "authors": [
        "Ali Madad"
      ],
      "abstract": "InvisibleBench is a deployment gate for caregiving-relationship AI, evaluating 3-20+ turn interactions across five dimensions: Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory. The benchmark includes autofail conditions for missed crises, medical advice (WOPR Act), harmful information, and attachment engineering. We evaluate four frontier models across 17 scenarios (N=68) spanning three complexity tiers. All models show significant safety gaps (11.8-44.8 percent crisis detection), indicating the necessity of deterministic crisis routing in production systems. DeepSeek Chat v3 achieves the highest overall score (75.9 percent), while strengths differ by dimension: GPT-4o Mini leads Compliance (88.2 percent), Gemini leads Trauma-Informed Design (85.0 percent), and Claude Sonnet 4.5 ranks highest in crisis detection (44.8 percent). We release all scenarios, judge prompts, and scoring configurations with code. InvisibleBench extends single-turn safety tests by evaluating longitudinal risk, where real harms emerge. No clinical claims; this is a deployment-readiness evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† InvisibleBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæŠ¤ç†å…³ç³» AI (Caregiving-Relationship AI) è®¾è®¡çš„éƒ¨ç½²è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ 3 åˆ° 20 å¤šè½®çš„é•¿ç¨‹äº¤äº’æ¥è¡¡é‡æ¨¡å‹çš„å¯é æ€§ã€‚è¯¥æ¡†æ¶ä» Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness å’Œ Memory äº”ä¸ªæ ¸å¿ƒç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è®¾ç½®äº†é’ˆå¯¹å±æœºæ¼æŠ¥ã€åŒ»ç–—å»ºè®®è¿è§„ç­‰å…³é”®é£é™©çš„è‡ªåŠ¨å¤±è´¥æ¡ä»¶ (autofail conditions)ã€‚å®éªŒå¯¹ 4 ä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº† 17 ä¸ªåœºæ™¯çš„æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹åœ¨å±æœºæ£€æµ‹æ–¹é¢å‡å­˜åœ¨ 11.8% è‡³ 44.8% çš„æ˜¾è‘—å®‰å…¨ç¼ºå£ï¼Œå‡¸æ˜¾äº†åœ¨ç”Ÿäº§ç³»ç»Ÿä¸­å¼•å…¥ç¡®å®šæ€§å±æœºè·¯ç”±çš„å¿…è¦æ€§ã€‚å…·ä½“è¡¨ç°ä¸Šï¼ŒDeepSeek Chat v3 è·å¾—äº† 75.9% çš„æœ€é«˜æ€»åˆ†ï¼Œè€Œ GPT-4o Mini, Gemini å’Œ Claude Sonnet 4.5 åˆ†åˆ«åœ¨åˆè§„æ€§ã€åˆ›ä¼¤çŸ¥æƒ…è®¾è®¡å’Œå±æœºæ£€æµ‹ç»´åº¦é¢†å…ˆã€‚è¯¥ç ”ç©¶é€šè¿‡è¯„ä¼°é•¿ç¨‹äº¤äº’ä¸­çš„çºµå‘é£é™©ï¼Œä¸ºæŠ¤ç† AI çš„éƒ¨ç½²å‡†å¤‡åº¦æä¾›äº†é‡è¦çš„è¡¡é‡æ ‡å‡†ï¼Œå¹¶å…¬å¼€äº†æ‰€æœ‰åœºæ™¯ä¸è¯„åˆ†ä»£ç ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿå•è½®å®‰å…¨æµ‹è¯•çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "29 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20733v1",
      "published_date": "2025-11-25 14:09:45 UTC",
      "updated_date": "2025-11-25 14:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:52.230418+00:00"
    },
    {
      "arxiv_id": "2511.20332v2",
      "title": "3D Motion Perception of Binocular Vision Target with PID-CNN",
      "title_zh": "åŸºäº PID-CNN çš„åŒç›®è§†è§‰ç›®æ ‡ä¸‰ç»´è¿åŠ¨æ„ŸçŸ¥",
      "authors": [
        "Jiazhao Shi",
        "Pan Pan",
        "Haotian Shi"
      ],
      "abstract": "This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º PID-CNN çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å®ç°å¯¹åŒç›®è§†è§‰ç›®æ ‡çš„ä¸‰ç»´è¿åŠ¨ä¿¡æ¯ï¼ˆåŒ…æ‹¬åæ ‡ã€é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ï¼‰çš„å®æ—¶æ„ŸçŸ¥ï¼Œä½¿å…¶å…·å¤‡åŸºæœ¬çš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚ä½œè€…ä» PID çš„è§†è§’åˆ†æäº†ç¥ç»ç½‘ç»œæ‹Ÿåˆéçº¿æ€§é—®é¢˜çš„åŸç†ï¼Œå°†å•å±‚ç½‘ç»œè§†ä¸ºç»“åˆäºŒé˜¶å·®åˆ†æ–¹ç¨‹ä¸éçº¿æ€§çš„å±€éƒ¨æè¿°å·¥å…·ï¼Œå¹¶ä»¥æ­¤æŒ‡å¯¼ç½‘ç»œè®¾è®¡ã€‚è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªä»…åŒ…å« 17 å±‚å’Œ 413,000 ä¸ªå‚æ•°çš„è½»é‡åŒ–æ¨¡å‹ï¼Œé€šè¿‡æ‹¼æ¥ (concatenation) å’Œæ± åŒ– (pooling) æŠ€æœ¯å®ç°äº†é«˜æ•ˆçš„ç‰¹å¾é‡ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPID-CNN åœ¨æ¨¡æ‹Ÿéšæœºè¿åŠ¨çƒä½“æ•°æ®é›†ä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡æ¥è¿‘å›¾åƒåˆ†è¾¨ç‡çš„ç†è®ºä¸Šé™ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†é«˜ç»´å·ç§¯åœ¨æå‡ç‰¹å¾ç©ºé—´åˆ©ç”¨ç‡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥åŠåˆ©ç”¨ PID ä¿¡æ¯æ„å»ºè®°å¿†å’Œæ³¨æ„åŠ›æœºåˆ¶çš„æ½œåœ¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 9 figures, 2 tables. The codes of this article have been released at: https://github.com/ShiJZ123/PID-CNN",
      "pdf_url": "https://arxiv.org/pdf/2511.20332v2",
      "published_date": "2025-11-25 14:09:44 UTC",
      "updated_date": "2025-12-01 07:41:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:12.035929+00:00"
    },
    {
      "arxiv_id": "2511.20321v3",
      "title": "Active Inference in Discrete State Spaces from First Principles",
      "title_zh": "åŸºäºç¬¬ä¸€æ€§åŸç†çš„ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸»åŠ¨æ¨ç†",
      "authors": [
        "Patrick Kenny"
      ],
      "abstract": "We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡å°† Active Inference ä¸ Free Energy Principle è§£è€¦ï¼Œä»ç¬¬ä¸€åŸç†å‡ºå‘æ¾„æ¸…ä¸»åŠ¨æ¨ç†çš„æ¦‚å¿µã€‚ä½œè€…å±•ç¤ºäº†åœ¨ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­å®ç°ä¸»åŠ¨æ¨ç†æ‰€éœ€çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥è¢«å…¬å¼åŒ–ä¸ºå—é™æ•£åº¦æœ€å°åŒ– (constrained divergence minimization) é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ ‡å‡†çš„å‡å€¼åœºæ–¹æ³• (mean field methods) è¿›è¡Œæ±‚è§£ï¼Œè€Œæ— éœ€ä¾èµ–ä¼ ç»Ÿçš„æœŸæœ›è‡ªç”±èƒ½ (expected free energy) æ¦‚å¿µã€‚åœ¨æ„ŸçŸ¥ (perception) å»ºæ¨¡æ–¹é¢ï¼Œæ‰€æå‡ºçš„æ•£åº¦å‡†åˆ™ä¸å˜åˆ†è‡ªç”±èƒ½ (variational free energy) ç›¸ä¸€è‡´ã€‚è€Œåœ¨è¡ŒåŠ¨ (action) å»ºæ¨¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¸æœŸæœ›è‡ªç”±èƒ½æ³›å‡½çš„åŒºåˆ«åœ¨äºå¼•å…¥äº†ä¸€ä¸ªç†µæ­£åˆ™é¡¹ (entropy regularizer)ã€‚è¿™ä¸€å·¥ä½œä¸ºç¦»æ•£çŠ¶æ€ä¸‹çš„ä¸»åŠ¨æ¨ç†æä¾›äº†æ›´ä¸ºç›´è§‚ä¸”ç¬¦åˆæ ‡å‡†ç»Ÿè®¡ç‰©ç†æ–¹æ³•çš„æ•°å­¦æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "57 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.20321v3",
      "published_date": "2025-11-25 13:54:10 UTC",
      "updated_date": "2026-01-18 16:15:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:22.135277+00:00"
    },
    {
      "arxiv_id": "2511.20315v1",
      "title": "Geometry of Decision Making in Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹å†³ç­–æœºåˆ¶çš„å‡ ä½•å­¦",
      "authors": [
        "Abhinav Joshi",
        "Divyanshu Bhatt",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å†…éƒ¨å†³ç­–è¿‡ç¨‹çš„ä¸é€æ˜æ€§ï¼Œé€šè¿‡å†…åœ¨ç»´åº¦(intrinsic dimension, ID)çš„è§†è§’æ·±å…¥åˆ†æäº†å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”(MCQA)è®¾ç½®ä¸‹éšè—å±‚è¡¨ç¤ºçš„å‡ ä½•ç‰¹æ€§ã€‚é€šè¿‡å¯¹28ä¸ªå¼€æºTransformeræ¨¡å‹çš„å¤§è§„æ¨¡ç ”ç©¶ï¼Œç ”ç©¶è€…å‘ç°æ¨¡å‹åœ¨å±‚çº§é—´è¡¨ç°å‡ºä¸€è‡´çš„IDæ¨¡å¼ï¼šæ—©æœŸå±‚è¿è¡Œåœ¨ä½ç»´æµå½¢ä¸Šï¼Œä¸­é—´å±‚æ‰©å±•ç©ºé—´ï¼Œè€ŒåæœŸå±‚åˆ™é‡æ–°å‹ç¼©å¹¶æ”¶æ•›è‡³å†³ç­–ç›¸å…³çš„è¡¨ç¤ºã€‚è¿™ä¸€å‘ç°è¡¨æ˜LLMsèƒ½å¤Ÿéšå¼åœ°å°†è¯­è¨€è¾“å…¥æŠ•å½±åˆ°ä¸ç‰¹å®šä»»åŠ¡å†³ç­–å¯¹é½çš„ç»“æ„åŒ–low-dimensional manifoldsä¸Šã€‚è¯¥é¡¹å·¥ä½œä¸ºç†è§£è¯­è¨€æ¨¡å‹ä¸­æ³›åŒ–ä¸æ¨ç†èƒ½åŠ›çš„å‡ ä½•åŸç†æä¾›äº†æ–°è§è§£ï¼Œæ­ç¤ºäº†å†³ç­–åˆ¶å®šçš„å†…éƒ¨åŠ¨æ€è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20315v1",
      "published_date": "2025-11-25 13:52:46 UTC",
      "updated_date": "2025-11-25 13:52:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:43.539071+00:00"
    },
    {
      "arxiv_id": "2511.20312v1",
      "title": "Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries",
      "title_zh": "åŸºäºè¾“å…¥-è¾“å‡ºæŸ¥è¯¢é€†å‘è¿˜åŸç¥ç»ç½‘ç»œæƒé‡çš„æ•°æ®å¢å¼ºæŠ€æœ¯",
      "authors": [
        "Alexander Beiser",
        "Flavio Martinelli",
        "Wulfram Gerstner",
        "Johanni Brea"
      ],
      "abstract": "Network weights can be reverse-engineered given enough informative samples of a network's input-output function. In a teacher-student setup, this translates into collecting a dataset of the teacher mapping -- querying the teacher -- and fitting a student to imitate such mapping. A sensible choice of queries is the dataset the teacher is trained on. But current methods fail when the teacher parameters are more numerous than the training data, because the student overfits to the queries instead of aligning its parameters to the teacher. In this work, we explore augmentation techniques to best sample the input-output mapping of a teacher network, with the goal of eliciting a rich set of representations from the teacher hidden layers. We discover that standard augmentations such as rotation, flipping, and adding noise, bring little to no improvement to the identification problem. We design new data augmentation techniques tailored to better sample the representational space of the network's hidden layers. With our augmentations we extend the state-of-the-art range of recoverable network sizes. To test their scalability, we show that we can recover networks of up to 100 times more parameters than training data-points.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡è¾“å…¥è¾“å‡ºæŸ¥è¯¢(input-output queries)é€†å‘å·¥ç¨‹(reverse-engineer)ç¥ç»ç½‘ç»œæƒé‡çš„é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³æ•™å¸ˆ-å­¦ç”Ÿ(teacher-student setup)æ¶æ„ä¸­å› æ•™å¸ˆå‚æ•°é‡è¿œè¶…è®­ç»ƒæ•°æ®è€Œå¯¼è‡´çš„è¿‡æ‹Ÿåˆ(overfitting)éš¾é¢˜ã€‚ä½œè€…å‘ç°ä¼ ç»Ÿçš„æ—‹è½¬ã€ç¿»è½¬å’ŒåŠ å™ªç­‰æ•°æ®å¢å¼º(data augmentation)æ‰‹æ®µåœ¨å‚æ•°è¯†åˆ«é—®é¢˜ä¸­å‡ ä¹æ²¡æœ‰æå‡ï¼Œå› æ­¤è®¾è®¡äº†ä¸“é—¨ç”¨äºé‡‡æ ·ç½‘ç»œéšè—å±‚(hidden layers)è¡¨ç¤ºç©ºé—´çš„æ–°å‹å¢å¼ºæŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»éšè—å±‚ä¸­è¯±å¯¼å‡ºä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ˜¾è‘—æ‰©å±•äº†å¯æ¢å¤ç½‘ç»œè§„æ¨¡çš„ç°æœ‰æŠ€æœ¯æ°´å¹³(state-of-the-art)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯å…·æœ‰æé«˜çš„å¯æ‰©å±•æ€§(scalability)ï¼Œèƒ½å¤ŸæˆåŠŸæ¢å¤å‡ºå‚æ•°é‡æ¯”è®­ç»ƒæ•°æ®ç‚¹å¤šå‡º100å€çš„ç½‘ç»œæƒé‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of the III edition of the Workshop on Unifying Representations in Neural Models (UniReps 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.20312v1",
      "published_date": "2025-11-25 13:49:48 UTC",
      "updated_date": "2025-11-25 13:49:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:53.236098+00:00"
    },
    {
      "arxiv_id": "2511.20305v1",
      "title": "RIS-Assisted Downlink Pinching-Antenna Systems: GNN-Enabled Optimization Approaches",
      "title_zh": "RISè¾…åŠ©çš„ä¸‹è¡Œå¤¹é€¼å¤©çº¿ç³»ç»Ÿï¼šåŸºäºGNNçš„ä¼˜åŒ–æ–¹æ³•",
      "authors": [
        "Changpeng He",
        "Yang Lu",
        "Yanqing Xu",
        "Chong-Yung Chi",
        "Bo Ai",
        "Arumugam Nallanathan"
      ],
      "abstract": "This paper investigates a reconfigurable intelligent surface (RIS)-assisted multi-waveguide pinching-antenna (PA) system (PASS) for multi-user downlink information transmission, motivated by the unknown impact of the integration of emerging PASS and RIS on wireless communications. First, we formulate sum rate (SR) and energy efficiency (EE) maximization problems in a unified framework, subject to constraints on the movable region of PAs, total power budget, and tunable phase of RIS elements. Then, by leveraging a graph-structured topology of the RIS-assisted PASS, a novel three-stage graph neural network (GNN) is proposed, which learns PA positions based on user locations, and RIS phase shifts according to composite channel conditions at the first two stages, respectively, and finally determines beamforming vectors. Specifically, the proposed GNN is achieved through unsupervised training, together with three implementation strategies for its integration with convex optimization, thus offering trade-offs between inference time and solution optimality. Extensive numerical results are provided to validate the effectiveness of the proposed GNN, and to support its unique attributes of viable generalization capability, good performance reliability, and real-time applicability. Moreover, the impact of key parameters on RIS-assisted PASS is illustrated and analyzed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½åå°„é¢(RIS)è¾…åŠ©çš„å¤šæ³¢å¯¼æåˆå¤©çº¿ç³»ç»Ÿ(PASS)åœ¨å¤šç”¨æˆ·ä¸‹è¡Œé“¾è·¯ä¿¡æ¯ä¼ è¾“ä¸­çš„åº”ç”¨ï¼Œå¡«è¡¥äº†è¿™ä¸€æ–°å…´ç³»ç»Ÿä¸RISé›†æˆå½±å“çš„ç ”ç©¶ç©ºç™½ã€‚ç ”ç©¶åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹æ„å»ºäº†æ€»é€Ÿç‡(SR)å’Œèƒ½é‡æ•ˆç‡(EE)æœ€å¤§åŒ–é—®é¢˜ï¼Œå¹¶ç»¼åˆè€ƒè™‘äº†PAç§»åŠ¨åŒºåŸŸã€åŠŸç‡é¢„ç®—åŠRISç›¸ä½åç§»ç­‰å…³é”®çº¦æŸã€‚ä¸ºè§£å†³ä¼˜åŒ–éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£è®­ç»ƒçš„ä¸‰é˜¶æ®µå›¾ç¥ç»ç½‘ç»œ(GNN)ï¼Œä¾æ¬¡å®ç°PAä½ç½®ä¼˜åŒ–ã€RISç›¸ç§»å­¦ä¹ ä»¥åŠæ³¢æŸèµ‹å½¢å‘é‡çš„ç¡®å®šã€‚è¯¥æ–¹æ¡ˆç»“åˆäº†å‡¸ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨æ¨ç†æ—¶é—´ä¸è§£çš„æœ€ä¼˜æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ï¼Œå…·å¤‡è¾ƒå¼ºçš„å®æ—¶æ€§ã€‚æ•°å€¼å®éªŒç»“æœéªŒè¯äº†æ‰€æGNNæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç°äº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ã€æ€§èƒ½å¯é æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥é˜è¿°äº†å…³é”®ç³»ç»Ÿå‚æ•°å¯¹RISè¾…åŠ©PASSç³»ç»Ÿæ€§èƒ½çš„å…·ä½“å½±å“ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20305v1",
      "published_date": "2025-11-25 13:43:44 UTC",
      "updated_date": "2025-11-25 13:43:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:36:52.029690+00:00"
    },
    {
      "arxiv_id": "2511.20297v1",
      "title": "Improving Language Agents through BREW",
      "title_zh": "é€šè¿‡ BREW æå‡è¯­è¨€æ™ºèƒ½ä½“",
      "authors": [
        "Shashank Kirtania",
        "Param Biyani",
        "Priyanshu Gupta",
        "Yasharth Bajpai",
        "Roshni Iyer",
        "Sumit Gulwani",
        "Gustavo Soares"
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $Ï„^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨æƒé‡ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚PPOå’ŒGRPOï¼‰ä¸­å­˜åœ¨çš„è®¡ç®—å¼€é”€é«˜ã€ç­–ç•¥éš¾ä»¥è§£é‡Šç­‰å±€é™æ€§ï¼Œæå‡ºäº†BREW (Bootstrapping expeRientially-learned Environmental knoWledge) æ¡†æ¶ã€‚BREWé€šè¿‡æ„å»ºå’Œä¼˜åŒ–ç»éªŒå­¦ä¹ çš„ç»“æ„åŒ–çŸ¥è¯†åº“(KB)æ¥å®ç°ä¸‹æ¸¸ä»»åŠ¡çš„æ™ºèƒ½ä½“ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†æœ‰æ•ˆçš„å†…å­˜åˆ†åŒºæ–¹æ³•ä»¥æå‡æ£€ç´¢ä¸ç²¾ç‚¼æ•ˆç‡ã€‚è¯¥æ¡†æ¶ç»“åˆä»»åŠ¡è¯„åˆ†å™¨(task graders)å’Œè¡Œä¸ºå‡†åˆ™(behavior rubrics)ï¼Œå¹¶åˆ©ç”¨çŠ¶æ€ç©ºé—´æœç´¢(state-space search)æ¥ç¡®ä¿ä»è‡ªç„¶è¯­è¨€çš„å™ªå£°å’Œéç‰¹å¼‚æ€§ä¸­æå–ç¨³å¥çš„è§è§£ã€‚åœ¨OSWorldã€$Ï„^2$Benchå’ŒSpreadsheetBenchç­‰å®é™…é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBREWä½¿ä»»åŠ¡ç²¾åº¦æå‡äº†10-20%ï¼ŒåŒæ—¶å‡å°‘äº†10-15%çš„APIæˆ–å·¥å…·è°ƒç”¨é‡ï¼Œæ˜¾è‘—åŠ å¿«äº†æ‰§è¡Œé€Ÿåº¦ã€‚ä¸ä»¥å¾€å°†è®°å¿†è§†ä¸ºé™æ€ä¸Šä¸‹æ–‡çš„ç ”ç©¶ä¸åŒï¼ŒBREWå°†çŸ¥è¯†åº“ç¡®ç«‹ä¸ºæ¨¡å—åŒ–ä¸”å¯æ§çš„åº•å±‚ç»“æ„ï¼Œä¸ºä»¥é€æ˜ã€å¯è§£é‡Šä¸”å¯æ‰©å±•çš„æ–¹å¼å¡‘é€ æ™ºèƒ½ä½“è¡Œä¸ºæä¾›äº†æ˜¾å¼æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20297v1",
      "published_date": "2025-11-25 13:34:54 UTC",
      "updated_date": "2025-11-25 13:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:21.125709+00:00"
    },
    {
      "arxiv_id": "2511.20296v1",
      "title": "Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction",
      "title_zh": "ç”¨äºå¤šåˆä¸€ç¨€ç–è§†è§’ CT é‡å»ºçš„æç¤ºå‹ Lipschitz çº¦æŸç½‘ç»œ",
      "authors": [
        "Baoshun Shi",
        "Ke Jiang",
        "Qiusheng Lian",
        "Xinran Yu",
        "Huazhu Fu"
      ],
      "abstract": "Despite significant advancements in deep learning-based sparse-view computed tomography (SVCT) reconstruction algorithms, these methods still encounter two primary limitations: (i) It is challenging to explicitly prove that the prior networks of deep unfolding algorithms satisfy Lipschitz constraints due to their empirically designed nature. (ii) The substantial storage costs of training a separate model for each setting in the case of multiple views hinder practical clinical applications. To address these issues, we elaborate an explicitly provable Lipschitz-constrained network, dubbed LipNet, and integrate an explicit prompt module to provide discriminative knowledge of different sparse sampling settings, enabling the treatment of multiple sparse view configurations within a single model. Furthermore, we develop a storage-saving deep unfolding framework for multiple-in-one SVCT reconstruction, termed PromptCT, which embeds LipNet as its prior network to ensure the convergence of its corresponding iterative algorithm. In simulated and real data experiments, PromptCT outperforms benchmark reconstruction algorithms in multiple-in-one SVCT reconstruction, achieving higher-quality reconstructions with lower storage costs. On the theoretical side, we explicitly demonstrate that LipNet satisfies boundary property, further proving its Lipschitz continuity and subsequently analyzing the convergence of the proposed iterative algorithms. The data and code are publicly available at https://github.com/shibaoshun/PromptCT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨€ç–è§†å›¾CT(sparse-view CT)é‡å»ºä¸­æ·±åº¦å±•å¼€ç®—æ³•éš¾ä»¥æ»¡è¶³Lipschitzçº¦æŸä»¥åŠå¤šè§†å›¾è®¾ç½®ä¸‹å­˜å‚¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºLipNetçš„æ˜¾å¼å¯è¯æ˜Lipschitzçº¦æŸç½‘ç»œã€‚é€šè¿‡é›†æˆæ˜¾å¼æç¤ºæ¨¡å—(prompt module)ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿè·å–ä¸åŒç¨€ç–é‡‡æ ·è®¾ç½®çš„åˆ¤åˆ«æ€§çŸ¥è¯†ï¼Œä»è€Œåœ¨å•ä¸ªæ¨¡å‹ä¸­å¤„ç†å¤šç§è§†å›¾é…ç½®ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†åä¸ºPromptCTçš„æ·±åº¦å±•å¼€æ¡†æ¶ï¼Œå°†LipNetä½œä¸ºå…¶å…ˆéªŒç½‘ç»œï¼Œä»¥ç¡®ä¿è¿­ä»£ç®—æ³•çš„æ”¶æ•›æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptCTåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®å®éªŒä¸­å‡ä¼˜äºåŸºå‡†ç®—æ³•ï¼Œå®ç°äº†é«˜è´¨é‡é‡å»ºå¹¶æ˜¾è‘—é™ä½äº†å­˜å‚¨å¼€é”€ã€‚åœ¨ç†è®ºå±‚é¢ï¼Œç ”ç©¶è€…æ˜¾å¼è¯æ˜äº†LipNetæ»¡è¶³è¾¹ç•Œæ€§è´¨åŠå…¶Lipschitzè¿ç»­æ€§ï¼Œä¸ºç®—æ³•çš„æ”¶æ•›æ€§æä¾›äº†ä¸¥è°¨çš„ç†è®ºä¿éšœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20296v1",
      "published_date": "2025-11-25 13:31:53 UTC",
      "updated_date": "2025-11-25 13:31:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:18.235474+00:00"
    },
    {
      "arxiv_id": "2511.20293v1",
      "title": "Forgetting by Pruning: Data Deletion in Join Cardinality Estimation",
      "title_zh": "ä»¥å‰ªæå®ç°é—å¿˜ï¼šè¿æ¥åŸºæ•°ä¼°è®¡ä¸­çš„æ•°æ®åˆ é™¤",
      "authors": [
        "Chaowei He",
        "Yuanjun Liu",
        "Qingzhi Ma",
        "Shenyuan Ren",
        "Xizhao Luo",
        "Lei Zhao",
        "An Liu"
      ],
      "abstract": "Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å­¦ä¹ å‹åŸºæ•°ä¼°è®¡(Cardinality Estimation, CE)ç³»ç»Ÿä¸­çš„æœºå™¨å¸è½½(Machine Unlearning)é—®é¢˜ï¼Œç‰¹åˆ«é’ˆå¯¹å¤šè¡¨å…³ç³»æ•°æ®ä¸­æ•°æ®åˆ é™¤å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œæ•°æ®åˆ é™¤åœ¨å­¦ä¹ å‹CEæ¨¡å‹ä¸­é¢ä¸´å±æ€§çº§æ•æ„Ÿæ€§(attribute-level sensitivity)ã€è¡¨é—´ä¼ æ’­(inter-table propagation)ä»¥åŠåŸŸæ¶ˆå¤±(domain disappearance)å¯¼è‡´çš„å¤šè·¯è¿æ¥é«˜ä¼°ç­‰å…³é”®éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹å¤šè¡¨å­¦ä¹ å‹CEç³»ç»Ÿçš„å¸è½½æ¡†æ¶Cardinality Estimation Pruning (CEP)ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†å¸ƒæ•æ„Ÿæ€§å‰ªæ(Distribution Sensitivity Pruning)è®¡ç®—æ•æ„Ÿåº¦å¾—åˆ†ä»¥å¼•å¯¼å‚æ•°å‰ªæï¼Œå¹¶åˆ©ç”¨åŸŸå‰ªæ(Domain Pruning)ç§»é™¤å·²è¢«åˆ é™¤çš„å€¼åŸŸæ”¯æŒã€‚åœ¨NeuroCardå’ŒFACEæ¶æ„åŠIMDBã€TPC-Hæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCEPåœ¨å¤šè¡¨åœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯é«˜åˆ é™¤æ¯”ä¾‹æ—¶ï¼‰å§‹ç»ˆä¿æŒæœ€ä½çš„Q-errorï¼Œå…¶è¡¨ç°ç”šè‡³ä¼˜äºå®Œå…¨é‡æ–°è®­ç»ƒ(full retraining)ã€‚æ­¤å¤–ï¼ŒCEPæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œå…¶è¿è¡Œæ—¶é—´ä»…ä¸ºä¼ ç»Ÿå¾®è°ƒæ—¶é—´çš„0.3%è‡³2.5%ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¿è¯å‡†ç¡®æ€§çš„åŒæ—¶å…·æœ‰æé«˜çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "AAAI26",
      "pdf_url": "https://arxiv.org/pdf/2511.20293v1",
      "published_date": "2025-11-25 13:25:59 UTC",
      "updated_date": "2025-11-25 13:25:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:46.028312+00:00"
    },
    {
      "arxiv_id": "2511.20285v2",
      "title": "Schema Matching on Graph: Iterative Graph Exploration for Efficient and Explainable Data Integration",
      "title_zh": "å›¾ä¸Šæ¨¡å¼åŒ¹é…ï¼šæ—¨åœ¨å®ç°é«˜æ•ˆä¸”å¯è§£é‡Šæ•°æ®é›†æˆçš„è¿­ä»£å¼å›¾æ¢ç´¢",
      "authors": [
        "Mingyu Jeon",
        "Jaeyoung Suh",
        "Suwan Cho"
      ],
      "abstract": "Schema matching is a critical task in data integration, particularly in the medical domain where disparate Electronic Health Record (EHR) systems must be aligned to standard models like OMOP CDM. While Large Language Models (LLMs) have shown promise in schema matching, they suffer from hallucination and lack of up-to-date domain knowledge. Knowledge Graphs (KGs) offer a solution by providing structured, verifiable knowledge. However, existing KG-augmented LLM approaches often rely on inefficient complex multi-hop queries or storage-intensive vector-based retrieval methods. This paper introduces SMoG (Schema Matching on Graph), a novel framework that leverages iterative execution of simple 1-hop SPARQL queries, inspired by successful strategies in Knowledge Graph Question Answering (KGQA). SMoG enhances explainability and reliability by generating human-verifiable query paths while significantly reducing storage requirements by directly querying SPARQL endpoints. Experimental results on real-world medical datasets demonstrate that SMoG achieves performance comparable to state-of-the-art baselines, validating its effectiveness and efficiency in KG-augmented schema matching.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SMoG (Schema Matching on Graph)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¼˜åŒ–æ•°æ®é›†æˆä¸­ Schema Matching ä»»åŠ¡çš„æ–°æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†ç”µå­å¥åº·è®°å½• (EHR) æ˜ å°„åˆ° OMOP CDM ç­‰æ ‡å‡†æ¨¡å‹æ—¶ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—é¢†åŸŸå­˜åœ¨çš„å¹»è§‰å’Œé¢†åŸŸçŸ¥è¯†ç¼ºå¤±é—®é¢˜ï¼ŒSMoG å€Ÿé‰´äº†çŸ¥è¯†å›¾è°±é—®ç­” (KGQA) çš„æˆåŠŸç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£æ‰§è¡Œç®€å•çš„ 1-hop SPARQL æŸ¥è¯¢æ¥æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°± (KGs) çš„ç»“æ„åŒ–çŸ¥è¯†ã€‚ä¸ä¼ ç»Ÿçš„å¤æ‚å¤šè·³æŸ¥è¯¢æˆ–é«˜å­˜å‚¨æˆæœ¬çš„å‘é‡æ£€ç´¢æ–¹æ³•ç›¸æ¯”ï¼ŒSMoG é€šè¿‡ç›´æ¥æŸ¥è¯¢ SPARQL ç»ˆç«¯æ˜¾è‘—é™ä½äº†å­˜å‚¨éœ€æ±‚ï¼Œå¹¶ç”Ÿæˆäº†å¯ç”±äººå·¥éªŒè¯çš„æŸ¥è¯¢è·¯å¾„ï¼Œä»è€Œå¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚åœ¨çœŸå®åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSMoG çš„æ€§èƒ½ä¸å½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ç›¸å½“ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨çŸ¥è¯†å›¾è°±å¢å¼ºæ¨¡å¼åŒ¹é…ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20285v2",
      "published_date": "2025-11-25 13:13:56 UTC",
      "updated_date": "2025-12-01 06:02:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:35.032018+00:00"
    },
    {
      "arxiv_id": "2511.20284v1",
      "title": "Can LLMs Make (Personalized) Access Control Decisions?",
      "title_zh": "LLMs èƒ½å¦åšå‡ºï¼ˆä¸ªæ€§åŒ–ï¼‰è®¿é—®æ§åˆ¶å†³ç­–ï¼Ÿ",
      "authors": [
        "Friederike Groschupp",
        "Daniele Lain",
        "Aritra Dhar",
        "Lara Magdalena Lazier",
        "Srdjan ÄŒapkun"
      ],
      "abstract": "Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.\n  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡ŒåŠ¨æ€ä¸”æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„è®¿é—®æ§åˆ¶(Access Control)å†³ç­–ï¼Œæ—¨åœ¨å‡è½»ç”¨æˆ·åœ¨å¤æ‚ç³»ç»Ÿä¸­å¤„ç†å®‰å…¨åå¥½æ—¶çš„è®¤çŸ¥è´Ÿæ‹…ã€‚ç ”ç©¶è€…é€šè¿‡ç”¨æˆ·è°ƒç ”æ„å»ºäº†ä¸€ä¸ªåŒ…å«307æ¡è‡ªç„¶è¯­è¨€éšç§å£°æ˜å’Œ14,682é¡¹ç”¨æˆ·å†³ç­–çš„æ•°æ®é›†ï¼Œå¹¶å¯¹æ¯”äº†é€šç”¨ä¸ä¸ªæ€§åŒ–LLMsçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsèƒ½æœ‰æ•ˆåæ˜ ç”¨æˆ·åå¥½ï¼Œå…¶å†³ç­–ä¸å¤§å¤šæ•°ç”¨æˆ·çš„ä¸€è‡´æ€§å‡†ç¡®ç‡æœ€é«˜å¯è¾¾86%ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†ä¸ªæ€§åŒ–ç³»ç»Ÿä¸­çš„æ ¸å¿ƒæƒè¡¡ï¼Œå³è™½ç„¶å¼•å…¥ç”¨æˆ·ç‰¹å®šåå¥½èƒ½æé«˜ä¸€è‡´æ€§ï¼Œä½†å®Œå…¨éµå¾ªè¿™äº›åå¥½å¯èƒ½è¿åå®‰å…¨æœ€ä½³å®è·µ(Security Best Practices)ã€‚æœ€åï¼Œè®ºæ–‡é’ˆå¯¹å¹³è¡¡ä¸ªæ€§åŒ–ã€å®‰å…¨æ€§å’Œæ•ˆç”¨çš„è‡ªç„¶è¯­è¨€è®¿é—®æ§åˆ¶ç³»ç»Ÿæå‡ºäº†è®¾è®¡ä¸é£é™©è€ƒé‡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20284v1",
      "published_date": "2025-11-25 13:11:23 UTC",
      "updated_date": "2025-11-25 13:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:28.229468+00:00"
    },
    {
      "arxiv_id": "2511.20277v2",
      "title": "HVAdam: A Full-Dimension Adaptive Optimizer",
      "title_zh": "HVAdamï¼šå…¨ç»´åº¦è‡ªé€‚åº”ä¼˜åŒ–å™¨",
      "authors": [
        "Yiheng Zhang",
        "Shaowu Wu",
        "Yuanzhuo Xu",
        "Jiajun Wu",
        "Shang Xu",
        "Steve Drew",
        "Xiaoguang Niu"
      ],
      "abstract": "Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity\n  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Adam ç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨åœ¨ CNN ç­‰ç»å…¸æ¶æ„ä¸Šæ³›åŒ–æ€§èƒ½ä¸å¦‚ SGD çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Anon çš„å…¨ç»´åº¦è‡ªé€‚åº”ä¼˜åŒ–å™¨ã€‚è¯¥ä¼˜åŒ–å™¨å¼•å…¥äº†è¿ç»­å¯è°ƒçš„ adaptivity æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ SGD å’Œ Adam çš„è¡Œä¸ºä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œç”šè‡³å®ç°æ€§èƒ½å¤–æ¨ã€‚ä¸ºäº†ç¡®ä¿åœ¨æ•´ä¸ªè‡ªé€‚åº”é¢‘è°±ä¸Šçš„æ”¶æ•›æ€§ï¼Œç ”ç©¶åˆ›æ–°æ€§åœ°æå‡ºäº† Incremental Delay Update (IDU) æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ¯” AMSGrad çš„ç­–ç•¥æ›´å…·çµæ´»æ€§ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æ¢¯åº¦å™ªå£°çš„é²æ£’æ€§ã€‚ç ”ç©¶åœ¨ç†è®ºä¸Šç¡®ç«‹äº† Anon åœ¨å‡¸å’Œéå‡¸è®¾ç½®ä¸‹çš„æ”¶æ•›ä¿è¯ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ã€Diffusion å’Œè¯­è¨€å»ºæ¨¡ç­‰ä»£è¡¨æ€§ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜äºç°æœ‰æœ€å…ˆè¿›ä¼˜åŒ–å™¨çš„å®éªŒç»“æœã€‚è¯¥æˆæœè¯æ˜äº† adaptivity å¯ä½œä¸ºä¸€ç§å…³é”®çš„å¯è°ƒè®¾è®¡åŸåˆ™ï¼Œä¸ºç»Ÿä¸€å¹¶è¶…è¶Šç»å…¸ä¸ç°ä»£ä¼˜åŒ–å™¨æä¾›äº†å¯é ä¸”é«˜æ•ˆçš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAAI2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20277v2",
      "published_date": "2025-11-25 13:05:40 UTC",
      "updated_date": "2025-12-22 02:58:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:35.230064+00:00"
    },
    {
      "arxiv_id": "2511.20273v1",
      "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
      "title_zh": "è¶…è¶Šç»„ä»¶ï¼šåŸºäºå¥‡å¼‚å‘é‡çš„ Transformer ç”µè·¯å¯è§£é‡Šæ€§",
      "authors": [
        "Areeb Ahmad",
        "Abhinav Joshi",
        "Ashutosh Modi"
      ],
      "abstract": "Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¥‡å¼‚å‘é‡(Singular Vector-Based)çš„ç»†ç²’åº¦å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œæ—¨åœ¨æ·±å…¥æ¢ç´¢Transformerå†…éƒ¨è®¡ç®—çš„å¤æ‚åˆ†å¸ƒç‰¹æ€§ã€‚ä¸ä¼ ç»Ÿæœºæ¢°å¯è§£é‡Šæ€§æ–¹æ³•å°†æ³¨æ„åŠ›å¤´(attention heads)å’Œå¤šå±‚æ„ŸçŸ¥æœº(MLPs)è§†ä¸ºä¸å¯åˆ†å‰²å•å…ƒä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†è¿™äº›ç»„ä»¶åˆ†è§£ä¸ºæ­£äº¤çš„å¥‡å¼‚æ–¹å‘ï¼Œæ­ç¤ºäº†å…¶å†…éƒ¨éšè—çš„é‡å ä¸ç‹¬ç«‹è®¡ç®—ã€‚åœ¨é—´æ¥å®¾è¯­è¯†åˆ«(IOI)ã€æ€§åˆ«ä»£è¯(GP)å’Œå¤§äºå…³ç³»(GT)ç­‰æ ‡å‡†ä»»åŠ¡ä¸Šçš„éªŒè¯ç»“æœæ˜¾ç¤ºï¼Œå…ˆå‰è¢«è®¤ä¸ºæ˜¯å•ä¸€åŠŸèƒ½çš„å¤´éƒ¨å®é™…ä¸Šåœ¨ä¸åŒå¥‡å¼‚æ–¹å‘ä¸Šç¼–ç äº†å¤šä¸ªå­åŠŸèƒ½ã€‚å®éªŒè¿›ä¸€æ­¥å‘ç°ï¼Œè®¡ç®—å›¾ä¸­çš„èŠ‚ç‚¹åœ¨ç‰¹å®šçš„ä½ç§©æ–¹å‘ä¸Šè¡¨ç°å‡ºå¼ºæ¿€æ´»ï¼Œè¡¨æ˜æœ‰æ„ä¹‰çš„è®¡ç®—å¾€å¾€é©»ç•™åœ¨ç´§å‡‘çš„å­ç©ºé—´å†…ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTransformerçš„è®¡ç®—è¿‡ç¨‹æ¯”ä»¥å¾€å‡è®¾çš„æ›´åŠ åˆ†å¸ƒåŒ–ã€ç»“æ„åŒ–ä¸”å…·æœ‰ç»„åˆæ€§ã€‚è¿™ä¸€æ–°è§†è§’ä¸ºç»†ç²’åº¦çš„æœºæ¢°å¯è§£é‡Šæ€§(mechanistic interpretability)å¼€è¾Ÿäº†è·¯å¾„ï¼Œæœ‰åŠ©äºå®ç°å¯¹æ¨¡å‹å†…éƒ¨è¿è¡Œæœºåˆ¶çš„æ·±å±‚ç†è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20273v1",
      "published_date": "2025-11-25 12:59:15 UTC",
      "updated_date": "2025-11-25 12:59:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:52.234890+00:00"
    },
    {
      "arxiv_id": "2511.20257v2",
      "title": "Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling",
      "title_zh": "åŸºäºç‰©ç†å¼•å¯¼æ—¶ç©ºè§£è€¦çš„å¯è§£é‡Šç©ºæ°”æ±¡æŸ“é¢„æµ‹",
      "authors": [
        "Zhiguo Zhang",
        "Xiaoliang Ma",
        "Daniel Schlesinger"
      ],
      "abstract": "Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç©ºæ°”æ±¡æŸ“é¢„æµ‹ä¸­é¢„æµ‹æ€§èƒ½ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç‰©ç†å¼•å¯¼ä¸”è®¾è®¡å³å…·å¯è§£é‡Šæ€§çš„æ—¶ç©ºå­¦ä¹ æ¡†æ¶ (physics-guided, interpretable-by-design spatiotemporal learning framework)ã€‚è¯¥æ¨¡å‹å°†ç©ºæ°”æ±¡æŸ“ç‰©æµ“åº¦çš„æ—¶ç©ºè¡Œä¸ºåˆ†è§£ä¸ºä¸¤ä¸ªé€æ˜çš„åŠ æ€§æ¨¡å—ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªæ˜¯ç‰©ç†å¼•å¯¼çš„ä¼ è¾“å†…æ ¸ (physics-guided transport kernel)ï¼Œåˆ©ç”¨å—é£åŠ›å’Œåœ°ç†å› ç´ è°ƒèŠ‚çš„å®šå‘æƒé‡æ¥æ¨¡æ‹Ÿå¹³æµè¿‡ç¨‹ã€‚ç¬¬äºŒä¸ªæ¨¡å—æ˜¯å¯è§£é‡Šçš„æ³¨æ„åŠ›æœºåˆ¶ (explainable attention mechanism)ï¼Œç”¨äºå­¦ä¹ å±€éƒ¨å“åº”å¹¶å°†æœªæ¥çš„æµ“åº¦å½’å› äºç‰¹å®šçš„å†å²æ»åå’Œå¤–ç”Ÿé©±åŠ¨å› ç´ ã€‚åœ¨æ–¯å¾·å“¥å°”æ‘©åœ°åŒºçš„ç»¼åˆæ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªé¢„æµ‹æ—¶åŸŸä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸€è‡´è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºå‡†æ¨¡å‹ (state-of-the-art baselines)ã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡æ•´åˆé«˜é¢„æµ‹æ€§èƒ½ä¸æ—¶ç©ºå¯è§£é‡Šæ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„ç©ºæ°”è´¨é‡ç®¡ç†æä¾›äº†æ›´å¯é çš„å†³ç­–åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to 2025 IEEE International Conference on Big Data. v2 corrects grant numbers",
      "pdf_url": "https://arxiv.org/pdf/2511.20257v2",
      "published_date": "2025-11-25 12:36:27 UTC",
      "updated_date": "2026-01-22 08:30:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:40.740976+00:00"
    },
    {
      "arxiv_id": "2511.20254v1",
      "title": "XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface",
      "title_zh": "XiCADï¼šDa Vinci Xi ç”¨æˆ·ç•Œé¢ä¸­çš„æ‘„åƒæœºæ¿€æ´»æ£€æµ‹",
      "authors": [
        "Alexander C. Jenke",
        "Gregor Just",
        "Claas de Boer",
        "Martin Wagner",
        "Sebastian Bodenstedt",
        "Stefanie Speidel"
      ],
      "abstract": "Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation.\n  Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames.\n  Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections.\n  Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¾èŠ¬å¥‡æ‰‹æœ¯ç³»ç»Ÿï¼ˆDaVinci Xiï¼‰ä¸­å†…çª¥é•œè§†é¢‘ç•Œé¢çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è¦†ç›–å±‚ï¼Œæå‡ºäº†XiCADæ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«æ‘„åƒå¤´ç£è´´ï¼ˆcamera tileï¼‰çš„ä½ç½®åŠå…¶æ¿€æ´»çŠ¶æ€ã€‚è¯¥ç³»ç»ŸåŸºäºResNet18å·ç§¯ç¥ç»ç½‘ç»œæ„å»ºäº†ä¸€ä¸ªè½»é‡çº§æµæ°´çº¿ï¼Œå¹¶åœ¨SurgToolLocæ•°æ®é›†åŠå…¶ä»–ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ï¼ˆå…±è®¡è¶…è¿‡70,000å¸§ï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒä¸è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXiCADåœ¨æ‘„åƒå¤´æ¿€æ´»çš„äºŒå…ƒæ£€æµ‹ä¸­å–å¾—äº†0.993è‡³1.000ä¹‹é—´çš„F1-scoresï¼Œå¹¶åœ¨æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹ä¸­å‡èƒ½å®ç°æ‘„åƒå¤´ç£è´´çš„å‡†ç¡®å®šä½ã€‚è¯¥ç ”ç©¶ä¸ºå®æ—¶æå–æ‰‹æœ¯è§†é¢‘ä¸­çš„æ‘„åƒå¤´æ¿€æ´»å…ƒæ•°æ®æä¾›äº†ä¸€ç§å¯é æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå·¥å…·è¿½è¸ªï¼ˆtool trackingï¼‰ã€æŠ€èƒ½è¯„ä¼°ï¼ˆskill assessmentï¼‰ç­‰ä¸‹æ¸¸æ‰‹æœ¯æ•°æ®ç§‘å­¦ä»»åŠ¡ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç ã€é¢„è®­ç»ƒæ¨¡å‹åŠæ ‡æ³¨æ•°æ®å·²å…¨éƒ¨å…¬å¼€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20254v1",
      "published_date": "2025-11-25 12:29:10 UTC",
      "updated_date": "2025-11-25 12:29:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:37:53.429350+00:00"
    },
    {
      "arxiv_id": "2511.20250v1",
      "title": "Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation",
      "title_zh": "Uplifting Table Tennisï¼šä¸€ç§é²æ£’çš„çœŸå®åœºæ™¯3Dè½¨è¿¹ä¸æ—‹è½¬ä¼°è®¡åº”ç”¨",
      "authors": [
        "Daniel Kienzle",
        "Katja Ludwig",
        "Julian Lorenz",
        "Shin'ichi Satoh",
        "Rainer Lienhart"
      ],
      "abstract": "Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæµæ°´çº¿(two-stage pipeline)ï¼Œæ—¨åœ¨è§£å†³ä»æ ‡å‡†å•ç›®è§†é¢‘(monocular videos)ä¸­ç²¾ç¡®æå–ä¹’ä¹“çƒ3Dè¿åŠ¨è½¨è¿¹å’Œæ—‹è½¬(spin)çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†ä»»åŠ¡åˆ†è§£ä¸ºå‰ç«¯æ„ŸçŸ¥(front-end perception)ä¸åç«¯2Dåˆ°3Dæå‡(2D-to-3D uplifting)ä¸¤ä¸ªé˜¶æ®µã€‚å‰ç«¯é€šè¿‡æ–°åˆ›å»ºçš„TTHQæ•°æ®é›†åˆ©ç”¨2Dç›‘ç£è¿›è¡Œè®­ç»ƒï¼Œè€Œåç«¯æå‡ç½‘ç»œåˆ™ä¸“é—¨åœ¨ç¬¦åˆç‰©ç†è§„å¾‹çš„åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶é’ˆå¯¹æ£€æµ‹ç¼ºå¤±å’Œå˜å¸§ç‡ç­‰ç°å®å·¥å†µè¿›è¡Œäº†é²æ£’æ€§æ”¹è¿›ã€‚é€šè¿‡é›†æˆçƒä½“æ£€æµ‹å™¨å’Œçƒå°å…³é”®ç‚¹æ£€æµ‹å™¨ï¼Œè¯¥æ–¹æ¡ˆæˆåŠŸå°†æ¦‚å¿µéªŒè¯æ–¹æ³•è½¬åŒ–ä¸ºä¸€ç§å®ç”¨ä¸”é«˜æ€§èƒ½çš„ç«¯åˆ°ç«¯åº”ç”¨ã€‚è¯¥ç ”ç©¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œå™ªå£°å’Œä¸å®Œç¾æ£€æµ‹æ—¶çš„ç¨³å¥æ€§ï¼Œä¸ºä¹’ä¹“çƒè¿åŠ¨çš„3Dè½¨è¿¹ä¸æ—‹è½¬åˆ†ææä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20250v1",
      "published_date": "2025-11-25 12:25:20 UTC",
      "updated_date": "2025-11-25 12:25:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:10.930058+00:00"
    },
    {
      "arxiv_id": "2511.20236v2",
      "title": "Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints",
      "title_zh": "èåˆé¢†åŸŸçŸ¥è¯†ä¸å› æœçº¦æŸçš„å¤šæ ·åŒ–ã€å¯æ“ä½œåäº‹å®è§£é‡Š",
      "authors": [
        "Szymon Bobek",
        "Åukasz BaÅ‚ec",
        "Grzegorz J. Nalepa"
      ],
      "abstract": "Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œæå‡ºäº†åä¸º DANCE (Diverse, Actionable, and kNowledge-Constrained Explanations) çš„åäº‹å®è§£é‡Šç”Ÿæˆæ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥çœŸå®ä¸–ç•Œæ•°æ®ä¸­çš„å¤æ‚ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸åˆ‡å®é™…çš„é—®é¢˜ï¼ŒDANCE é€šè¿‡å¼•å…¥é¢†åŸŸçŸ¥è¯† (domain knowledge) å’Œå› æœçº¦æŸ (causal constraints) æ¥ç¡®ä¿è§£é‡Šçš„åˆç†æ€§ä¸å¯è¡Œæ€§ã€‚è¯¥æ–¹æ³•æ”¯æŒä»æ•°æ®ä¸­å­¦ä¹ çº¿æ€§ä¸éçº¿æ€§çº¦æŸï¼Œæˆ–é›†æˆä¸“å®¶æä¾›çš„ä¾èµ–å›¾ï¼Œä½¿ç”Ÿæˆçš„ counterfactuals ä¸ç‰¹å¾é—´çš„é€»è¾‘å…³ç³»ä¿æŒä¸€è‡´ã€‚åœ¨ä¼˜åŒ–ç›®æ ‡ä¸Šï¼ŒDANCE æˆåŠŸå¹³è¡¡äº†çœŸå®æ€§ (plausibility)ã€å¤šæ ·æ€§ (diversity) å’Œç¨€ç–æ€§ (sparsity)ï¼Œè§£å†³äº†ç°æœ‰ç®—æ³•çš„æ ¸å¿ƒå±€é™ã€‚è¯¥é¡¹å·¥ä½œåŸºäºä¸é‚®ä»¶è¥é”€å…¬å¸ Freshmail çš„å®é™…æ¡ˆä¾‹å¼€å‘ï¼Œå¹¶å¾—åˆ°äº† Sendguard é¡¹ç›®çš„æ”¯æŒã€‚é€šè¿‡å¯¹ 140 ä¸ªå…¬å…±æ•°æ®é›†çš„å¹¿æ³›è¯„ä¼°ï¼Œå®éªŒè¯æ˜ DANCE åœ¨ç”Ÿæˆé¢†åŸŸç›¸å…³ä¸”æœ‰æ„ä¹‰çš„åäº‹å®è§£é‡Šæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20236v2",
      "published_date": "2025-11-25 12:09:36 UTC",
      "updated_date": "2025-11-28 07:20:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:25.939913+00:00"
    },
    {
      "arxiv_id": "2511.20234v1",
      "title": "Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning",
      "title_zh": "åˆ©ç”¨æƒé‡ä¿¡å·ï¼šå¼ºåŒ–å­¦ä¹ æ³›åŒ–èƒ½åŠ›çš„é¢„æµ‹ä¸æå‡",
      "authors": [
        "Olivier Moulin",
        "Vincent Francois-lavet",
        "Paul Elbers",
        "Mark Hoogendoorn"
      ],
      "abstract": "Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ™ºèƒ½ä½“å®¹æ˜“å¯¹è®­ç»ƒç¯å¢ƒäº§ç”Ÿè¿‡æ‹Ÿåˆ(Overfitting)è€Œå¯¼è‡´æ³›åŒ–æ€§(Generalizability)ä¸è¶³çš„æ ¸å¿ƒé—®é¢˜å±•å¼€è®¨è®ºã€‚ä½œè€…æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ™ºèƒ½ä½“ç¥ç»ç½‘ç»œçš„å†…éƒ¨æƒé‡(Internal weights)ä¿¡å·æ¥é¢„æµ‹å…¶æ³›åŒ–å¾—åˆ†(Generalizability score)ã€‚åˆ©ç”¨è¿™ç§é¢„æµ‹èƒ½åŠ›ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥å¯¹è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization, PPO)çš„æŸå¤±å‡½æ•°(Loss function)è¿›è¡Œäº†é’ˆå¯¹æ€§æ”¹è¿›ï¼Œæ—¨åœ¨ç›´æ¥å¢å¼ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³›åŒ–è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹ç®—æ³•ç›¸æ¯”ï¼Œè¿™ç§æ”¹è¿›åçš„PPOç®—æ³•èƒ½å¤ŸåŸ¹è‚²å‡ºåœ¨æœªçŸ¥ç¯å¢ƒä¸­æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„æ™ºèƒ½ä½“ã€‚è¯¥é¡¹å·¥ä½œä¸ºé€šè¿‡æƒé‡ä¿¡å·é¢„æµ‹å¹¶æå‡å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„æ³›åŒ–æ€§èƒ½æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20234v1",
      "published_date": "2025-11-25 12:07:25 UTC",
      "updated_date": "2025-11-25 12:07:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:45.039883+00:00"
    },
    {
      "arxiv_id": "2511.20224v1",
      "title": "DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation",
      "title_zh": "DUO-TOKï¼šé¢å‘äººå£°-ä¼´å¥ç”Ÿæˆçš„åŒè½¨è¯­ä¹‰éŸ³ä¹åˆ†è¯å™¨",
      "authors": [
        "Rui Lin",
        "Zhiyue Wu",
        "Jiahe Le",
        "Kangdi Wang",
        "Weixiong Chen",
        "Junyu Dai",
        "Tao Jiang"
      ],
      "abstract": "Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Duo-Tokï¼Œä¸€ç§é’ˆå¯¹äººå£°-ä¼´å¥éŸ³ä¹ç”Ÿæˆçš„æºæ„ŸçŸ¥åŒç æœ¬(source-aware dual-codebook)åˆ†è¯å™¨ï¼Œæ—¨åœ¨è§£å†³ç°ä»£æ­Œè¯è½¬æ­Œæ›²ç³»ç»Ÿåœ¨é‡å»ºè´¨é‡ä¸è¯­è¨€æ¨¡å‹(LM)å¯å­¦ä¹ æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€å¥—å››é˜¶æ®µã€ä»¥è‡ªç›‘ç£å­¦ä¹ (SSL)ä¸ºä¸­å¿ƒçš„æµæ°´çº¿ï¼Œé¦–å…ˆåœ¨å¤§è§„æ¨¡éŸ³é¢‘ä¸Šé¢„è®­ç»ƒ BEST-RQ é£æ ¼çš„ç¼–ç å™¨ï¼Œéšååˆ©ç”¨é«˜æ–¯æ›¿æ¢å™ªå£°(Gaussian replacement noise)å’Œå¤šä»»åŠ¡ç›‘ç£ç¨³å®šå¹¶åˆ†è§£ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å†»ç»“ç¼–ç å™¨å¹¶ç»“åˆåŸºäº SimVQ çš„ç¡¬è·¯ç”±(hard routing)æœºåˆ¶ï¼ŒDuo-Tok èƒ½å¤Ÿåˆ†åˆ«ä¸ºäººå£°å’Œä¼´å¥å­¦ä¹ ç‹¬ç«‹çš„ç æœ¬ï¼Œå¹¶åœ¨ç¦»æ•£ä»¤ç‰Œä¹‹ä¸Šè®­ç»ƒæ½œåœ¨æ‰©æ•£è§£ç å™¨(latent diffusion decoders)ã€‚åœ¨ 0.75 kbps çš„æä½ç ç‡ä¸‹ï¼Œè¯¥æ¨¡å‹æˆåŠŸä¼˜åŒ–äº†é‡å»ºä¸ç”Ÿæˆçš„å¸•ç´¯æ‰˜å‰æ²¿(Pareto frontier)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDuo-Tok åœ¨éŸ³ä¹æ ‡æ³¨å‡†ç¡®ç‡(music-tagging AP)å’Œè¯è¡¨å½’ä¸€åŒ– LM å›°æƒ‘åº¦(perplexity)æ–¹é¢å‡è¾¾åˆ°äº†æœ€ä¼˜æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„é‡å»ºè´¨é‡ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "17 pages, 5 figures, 8 tables. Project page: https://eps-acoustic-revolution-lab.github.io/DUO_TOK/",
      "pdf_url": "https://arxiv.org/pdf/2511.20224v1",
      "published_date": "2025-11-25 11:53:57 UTC",
      "updated_date": "2025-11-25 11:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:50.232705+00:00"
    },
    {
      "arxiv_id": "2511.20216v1",
      "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents",
      "title_zh": "CostNavï¼šé¢å‘å…·èº«æ™ºèƒ½ä½“æˆæœ¬æ„ŸçŸ¥è¯„ä¼°çš„å¯¼èˆªåŸºå‡†",
      "authors": [
        "Haebin Seong",
        "Sungmin Kim",
        "Minchan Kim",
        "Yongjun Cho",
        "Myunchul Joe",
        "Suhwan Choi",
        "Jaeyoon Jung",
        "Jiyong Youn",
        "Yoonshik Kim",
        "Samwoo Seong",
        "Yubeen Park",
        "Youngjae Yu",
        "Yunsung Lee"
      ],
      "abstract": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CostNavï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…·ä½“åŒ–æ™ºèƒ½ä½“ (Embodied Agents) æˆæœ¬æ„è¯†è¯„ä¼°çš„å¯¼èˆªåŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä»…å…³æ³¨ä»»åŠ¡æˆåŠŸç‡è€Œå¿½è§†å•†ä¸šéƒ¨ç½²ç»æµå¯è¡Œæ€§çš„é—®é¢˜ã€‚ä½œä¸ºé¦–ä¸ªå¾®è§‚å¯¼èˆªç»æµè¯•éªŒå° (Micro-Navigation Economic Testbed)ï¼ŒCostNav é€šè¿‡ç»¼åˆè€ƒé‡ç¡¬ä»¶ã€è®­ç»ƒã€èƒ½æºã€ç»´æŠ¤æˆæœ¬åŠäº¤ä»˜æ”¶å…¥ï¼Œå¯¹æ™ºèƒ½ä½“è¿›è¡Œå…¨é¢çš„æˆæœ¬æ”¶ç›Šåˆ†æã€‚ç ”ç©¶å®šé‡æ­ç¤ºäº†å¯¼èˆªæ€§èƒ½æŒ‡æ ‡ä¸å•†ä¸šå¯è¡Œæ€§ä¹‹é—´çš„å·¨å¤§å·®è·ï¼ŒæŒ‡å‡ºä¼˜åŒ–ä»»åŠ¡æˆåŠŸç‡å¹¶ä¸ç­‰åŒäºä¼˜åŒ–ç»æµéƒ¨ç½²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºå‡†æ¨¡å‹è™½ç„¶èƒ½è¾¾åˆ° 43.0% çš„æœåŠ¡ç­‰çº§åè®® (SLA) è¾¾æ ‡ç‡ï¼Œä½†å› ç¢°æ’å¼•å‘çš„ç»´æŠ¤æˆæœ¬å æ€»è¿è¡Œæˆæœ¬çš„ 99.7%ï¼Œå¯¼è‡´æ¯å•äºæŸè¾¾ 30.009 ç¾å…ƒã€‚CostNav ä¸ºè¯„ä¼°æ¨¡ä»¿å­¦ä¹  (Imitation Learning) å’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹  (Cost-aware RL) ç­‰å¤šç§å¯¼èˆªèŒƒå¼æä¾›äº†æ•°æ®é©±åŠ¨çš„å†³ç­–åŸºç¡€ï¼ŒæˆåŠŸæ¡¥æ¥äº†å­¦æœ¯ç ”ç©¶ä¸å®é™…å•†ä¸šåº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20216v1",
      "published_date": "2025-11-25 11:42:28 UTC",
      "updated_date": "2025-11-25 11:42:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:32.930420+00:00"
    },
    {
      "arxiv_id": "2511.20211v1",
      "title": "OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation",
      "title_zh": "OmniAlphaï¼šä¸€ç§ç”¨äºç»Ÿä¸€å¤šä»»åŠ¡ RGBA ç”Ÿæˆçš„åºåˆ—åˆ°åºåˆ—æ¡†æ¶",
      "authors": [
        "Hao Yu",
        "Jiabo Zhan",
        "Zile Wang",
        "Jinglin Wang",
        "Huaisong Zhang",
        "Hongyu Li",
        "Xinrui Chen",
        "Yongxian Wei",
        "Chun Yuan"
      ],
      "abstract": "Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OmniAlphaï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç»Ÿä¸€å¤šä»»åŠ¡RGBAå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„åºåˆ—åˆ°åºåˆ—(Sequence-to-Sequence)ç”Ÿæˆæ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹åœ¨RGBAå¤„ç†ä¸Šçš„ç¢ç‰‡åŒ–é—®é¢˜ï¼ŒOmniAlphaåœ¨å…¶Diffusion Transformer (DiT)æ¶æ„ä¸­å¼•å…¥äº†MSRoPE-BiLï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰åŒå‘å¯æ‰©å±•å±‚è½´çš„RoPEæ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥å’Œç›®æ ‡çš„RGBAå±‚ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åŒæ­¥æ¨å‡ºäº†AlphaLayersæ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨åˆæˆä¸è¿‡æ»¤æµç¨‹æ„å»ºäº†1,000ç»„é«˜è´¨é‡çš„å¤šå±‚ä¸‰å…ƒç»„ã€‚åœ¨21é¡¹å¤šæ ·åŒ–ä»»åŠ¡çš„è”åˆè®­ç»ƒä¸‹ï¼Œå®éªŒè¯æ˜è¯¥ç»Ÿä¸€æ¡†æ¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå¼ºåŠ›çš„ä¸“ç”¨åŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒOmniAlphaåœ¨AIM-500æ•°æ®é›†çš„mask-free mattingä»»åŠ¡ä¸­å®ç°äº†84.8%çš„SADç›¸å¯¹é™å¹…ï¼Œå¹¶åœ¨layer-conditioned completionä»»åŠ¡ä¸­èµ¢å¾—äº†è¶…è¿‡90%çš„äººç±»åå¥½ï¼Œè¯æ˜äº†ç»Ÿä¸€å¤šä»»åŠ¡æ¨¡å‹åœ¨å­¦ä¹ RGBAå…±äº«è¡¨ç¤ºæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20211v1",
      "published_date": "2025-11-25 11:34:51 UTC",
      "updated_date": "2025-11-25 11:34:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:35.433286+00:00"
    },
    {
      "arxiv_id": "2511.20200v1",
      "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„äº¤äº’å¼ AI NPCï¼šCPDC 2025 æŒ‘æˆ˜èµ›æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Yitian Huang",
        "Yuxuan Lei",
        "Jianxun Lian",
        "Hao Liao"
      ],
      "abstract": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution",
      "tldr_zh": "æœ¬æŠ¥å‘Šä»‹ç»äº†MSRA\\_SCå›¢é˜Ÿåœ¨CPDC 2025æŒ‘æˆ˜èµ›ä¸­çš„è·å¥–æ–¹æ¡ˆï¼Œæå‡ºäº†ä¸€å¥—ç»Ÿä¸€ä¸”é«˜æ•ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„äº¤äº’å¼AI NPCçš„è¡¨ç°ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€æ˜¯Context Engineeringï¼Œé€šè¿‡åŠ¨æ€å·¥å…·å‰ªæ(dynamic tool pruning)å’Œäººæ ¼è£å‰ª(persona clipping)å®ç°è¾“å…¥å‹ç¼©ã€‚ç»“åˆå‚æ•°æ ‡å‡†åŒ–(parameter normalization)å’Œå‡½æ•°åˆå¹¶(function merging)ç­‰åå¤„ç†æŠ€æœ¯ï¼Œä»¥åŠäººå·¥ä¼˜åŒ–çš„æç¤ºè¯ï¼Œè¯¥è®¾è®¡æ˜¾è‘—å¢å¼ºäº†å·¥å…·è°ƒç”¨(tool call)çš„ç¨³å®šæ€§ã€æ‰§è¡Œå¯é æ€§å’Œè§’è‰²æ‰®æ¼”å¼•å¯¼ã€‚åœ¨GPU Trackä¸­ï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥é‡‡ç”¨GRPOè®­ç»ƒï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ (reinforcement learning)ç›´æ¥ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒ(SFT)ï¼Œæœ‰æ•ˆç¼“è§£äº†å°æ ·æœ¬è¿‡æ‹Ÿåˆé—®é¢˜å¹¶æå‡äº†ä»»åŠ¡å¯¼å‘å‹å¯¹è¯çš„æ€§èƒ½ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šä¸ªèµ›é“è¡¨ç°å“è¶Šï¼Œåˆ†åˆ«è·å¾—Task 2 APIç¬¬ä¸€åã€Task 1 APIç¬¬äºŒåä»¥åŠTask 3 APIå’ŒGPU Trackç¬¬ä¸‰åï¼Œå……åˆ†éªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¤æ‚ç¤¾äº¤å¯¹è¯åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20200v1",
      "published_date": "2025-11-25 11:24:14 UTC",
      "updated_date": "2025-11-25 11:24:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:38.930894+00:00"
    },
    {
      "arxiv_id": "2511.20196v1",
      "title": "Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning",
      "title_zh": "è¿ˆå‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§é—å¿˜çš„è‰¯æ€§è®°å¿†é—å¿˜",
      "authors": [
        "Zhen Zeng",
        "Leijiang Gu",
        "Zhangling Duan",
        "Feng Li",
        "Zenglin Shi",
        "Cees G. M. Snoek",
        "Meng Wang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) åœ¨ç§»é™¤éšç§æ•æ„Ÿä¿¡æ¯æ—¶ä¼šå¯¼è‡´é€šç”¨å›¾åƒç†è§£èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† Sculpted Memory Forgetting Adapter (SMFA) æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è‰¯æ€§è®°å¿†é—å¿˜ (Benign Memory Forgetting)ã€‚SMFA é¦–å…ˆé€šè¿‡å¾®è°ƒå°†æ•æ„Ÿå“åº”æ›¿æ¢ä¸ºæ‹’ç»å›ç­”ï¼Œä»è€Œç”Ÿæˆè®°å¿†é—å¿˜é€‚é…å™¨ï¼Œå¹¶åˆ©ç”¨ä¿ç•™é”šç‚¹å¼•å¯¼çš„æ©ç æœºåˆ¶ (retaining anchor-guided masking mechanism) æ¥é˜²æ­¢å¯¹æ— å…³çŸ¥è¯†å’Œç†è§£èƒ½åŠ›çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†é¦–ä¸ªä¸“é—¨ç”¨äºè”åˆè¯„ä¼°æ•æ„ŸçŸ¥è¯†ç§»é™¤ä¸é€šç”¨è§†è§‰ç†è§£ä¿ç•™çš„åŸºå‡†æµ‹è¯•é›† S-MLLMUn Benchã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒSMFA åœ¨å®ç°ç²¾ç¡®ä¸”å¯æ§çš„ Unlearning ä»»åŠ¡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŒæ¨¡å‹çš„åŸºç¡€å›¾åƒç†è§£æ°´å¹³ï¼Œä¸ºå®ç°é€‰æ‹©æ€§æ¨¡å‹å¸è½½æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20196v1",
      "published_date": "2025-11-25 11:22:45 UTC",
      "updated_date": "2025-11-25 11:22:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:49.158885+00:00"
    },
    {
      "arxiv_id": "2511.20730v1",
      "title": "Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities",
      "title_zh": "å·¥ç¨‹è®¾è®¡ä¸­çš„æ•°æ®é©±åŠ¨æ–¹æ³•ä¸äººå·¥æ™ºèƒ½ï¼šæŒ‘æˆ˜ä¸æœºé‡çš„ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°",
      "authors": [
        "Nehal Afifi",
        "Christoph Wittig",
        "Lukas Paehler",
        "Andreas Lindenmann",
        "Kai Wolter",
        "Felix Leitenberger",
        "Melih Dogru",
        "Patric Grauberger",
        "Tobias DÃ¼ser",
        "Albert Albers",
        "Sven Matthiesen"
      ],
      "abstract": "The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å·¥ç¨‹è®¾è®¡é¢†åŸŸä¸­çš„æ•°æ®é©±åŠ¨æ–¹æ³•(DDMs)å’Œäººå·¥æ™ºèƒ½(AI)åº”ç”¨è¿›è¡Œäº†PRISMAç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°ï¼Œæ—¨åœ¨è§£å†³äº§å“å¼€å‘ä¸­DDMé›†æˆç¢ç‰‡åŒ–å’Œåº”ç”¨ä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚ç ”ç©¶é‡‡ç”¨äº†ç®€åŒ–çš„Væ¨¡å‹æ¡†æ¶ï¼Œå°†äº§å“å¼€å‘åˆ’åˆ†ä¸ºç³»ç»Ÿè®¾è®¡ã€ç³»ç»Ÿå®ç°ã€ç³»ç»Ÿé›†æˆå’ŒéªŒè¯å››ä¸ªé˜¶æ®µï¼Œå¹¶å¯¹2014è‡³2024å¹´é—´çš„114ç¯‡æ ¸å¿ƒæ–‡çŒ®è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç»“æœæ˜¾ç¤ºæœºå™¨å­¦ä¹ (ML)å’Œç»Ÿè®¡æ–¹æ³•ç›®å‰åœ¨å®è·µä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œè€Œæ·±åº¦å­¦ä¹ (DL)çš„åº”ç”¨æ­£å‘ˆç°å‡ºæ˜æ˜¾çš„ä¸Šå‡è¶‹åŠ¿ã€‚ç›‘ç£å­¦ä¹ (Supervised learning)ã€èšç±»(Clustering)ã€å›å½’åˆ†æ(Regression analysis)å’Œä»£ç†æ¨¡å‹(Surrogate modeling)åœ¨è®¾è®¡ä¸é›†æˆé˜¶æ®µè¾ƒä¸ºæ™®éï¼Œä½†åœ¨éªŒè¯é˜¶æ®µçš„è´¡çŒ®ç›¸å¯¹æœ‰é™ã€‚ç ”ç©¶æŒ‡å‡ºå½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æ¨¡å‹å¯è§£é‡Šæ€§(Interpretability)ä¸è¶³ã€è·¨é˜¶æ®µè¿½æº¯æ€§å·®ä»¥åŠç°å®ç¯å¢ƒä¸‹çš„éªŒè¯ç¼ºä½ã€‚è¯¥ç»¼è¿°å¼ºè°ƒäº†å¼€å‘å¯è§£é‡Šæ··åˆæ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºåç»­åˆ¶å®šè®¾è®¡é˜¶æ®µæŒ‡å—ä»¥åŠå®ç°è®¡ç®—æœºç®—æ³•ä¸å·¥ç¨‹é—®é¢˜çš„ç²¾å‡†æ˜ å°„å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20730v1",
      "published_date": "2025-11-25 11:16:38 UTC",
      "updated_date": "2025-11-25 11:16:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:53.633748+00:00"
    },
    {
      "arxiv_id": "2512.20625v1",
      "title": "Parameter-Efficient Neural CDEs via Implicit Function Jacobians",
      "title_zh": "åŸºäºéšå‡½æ•°é›…å¯æ¯”çŸ©é˜µçš„å‚æ•°é«˜æ•ˆç¥ç»å—æ§å¾®åˆ†æ–¹ç¨‹",
      "authors": [
        "Ilya Kuleshov",
        "Alexey Zaytsev"
      ],
      "abstract": "Neural Controlled Differential Equations (Neural CDEs, NCDEs) are a unique branch of methods, specifically tailored for analysing temporal sequences. However, they come with drawbacks, the main one being the number of parameters, required for the method's operation. In this paper, we propose an alternative, parameter-efficient look at Neural CDEs. It requires much fewer parameters, while also presenting a very logical analogy as the \"Continuous RNN\", which the Neural CDEs aspire to.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸“é—¨ç”¨äºåˆ†ææ—¶é—´åºåˆ—çš„ç¥ç»è¥æ§å¾®åˆ†æ–¹ç¨‹ (Neural Controlled Differential Equations, NCDEs) å‚æ•°é‡è¿‡å¤§çš„ä¸»è¦ç¼ºé™·ï¼Œæå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ”¹è¿›æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥éšå‡½æ•°é›…å¯æ¯” (Implicit Function Jacobians)ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—å‡å°‘æ¨¡å‹å‚æ•°éœ€æ±‚çš„åŒæ—¶ï¼ŒæˆåŠŸæ„å»ºäº† Neural CDEs æ‰€è¿½æ±‚çš„â€œè¿ç»­å¾ªç¯ç¥ç»ç½‘ç»œâ€ (Continuous RNN) çš„é€»è¾‘ç±»æ¯”ã€‚è¿™ç§æ–°æ¶æ„ä¸ä»…ä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ï¼Œè¿˜ä¸ºç†è§£ NCDEs ä¸ä¼ ç»Ÿå¾ªç¯æ¨¡å‹ä¹‹é—´çš„å…³ç³»æä¾›äº†æ›´æ¸…æ™°çš„æ•°å­¦è§†è§’ã€‚ç ”ç©¶æˆæœè¯æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒåºåˆ—åˆ†æèƒ½åŠ›çš„æ¡ä»¶ä¸‹å®ç°äº†æé«˜çš„å‚æ•°å‹ç¼©ï¼Œä¸ºè¿ç»­åŠ¨åŠ›ç³»ç»Ÿåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨æä¾›äº†æ›´å®ç”¨çš„é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.20625v1",
      "published_date": "2025-11-25 11:14:17 UTC",
      "updated_date": "2025-11-25 11:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:56.833890+00:00"
    },
    {
      "arxiv_id": "2511.20179v3",
      "title": "Human-computer interactions predict mental health",
      "title_zh": "äººæœºäº¤äº’é¢„æµ‹å¿ƒç†å¥åº·",
      "authors": [
        "Veith Weilnhammer",
        "Jefferson Ortega",
        "David Whitney"
      ],
      "abstract": "Scalable assessments of mental illness remain a critical roadblock toward accessible and equitable care. Here, we show that everyday human-computer interactions encode mental health with state-of-the-art biomarker precision. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA on 20,000 cursor and touchscreen recordings labelled with 1.3 million mental-health self-reports collected from 9,000 participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, identifies individuals living with mental illness, and achieves near-ceiling accuracy when predicting group-level mental health. By extracting non-verbal signatures of psychological function that have so far remained untapped, MAILA represents a key step toward scalable digital phenotyping and foundation models for mental health.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAILAï¼Œä¸€ä¸ªé€šè¿‡æ•°å­—æ´»åŠ¨æ¨æ–­æ½œåœ¨å¿ƒç†çŠ¶æ€çš„æœºå™¨å­¦ä¹ æ¡†æ¶(MAchine-learning framework for Inferring Latent mental states from digital Activity)ï¼Œæ—¨åœ¨åˆ©ç”¨æ—¥å¸¸çš„äººæœºäº¤äº’(human-computer interactions)æ•°æ®è§£å†³å¿ƒç†ç–¾ç—…å¤§è§„æ¨¡è¯„ä¼°çš„éš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ä»9,000åå‚ä¸è€…ä¸­æ”¶é›†çš„20,000ä»½å…‰æ ‡å’Œè§¦æ‘¸å±è®°å½•ï¼Œä»¥åŠ130ä¸‡ä»½å¿ƒç†å¥åº·è‡ªè¯„æŠ¥å‘Šå¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œæ•°æ®é›†æ¶µç›–äº†æŠ‘éƒç—‡å’Œå¼ºè¿«ç—‡(obsessive-compulsive disorder)ç­‰ä¸´åºŠè¯Šæ–­ã€‚MAILAèƒ½å¤Ÿæ²¿ç€ä¸‰ä¸ªæ­£äº¤ç»´åº¦è¿½è¸ªåŠ¨æ€å¿ƒç†çŠ¶æ€ï¼Œå¹¶ä»æ­¤å‰æœªè¢«åˆ©ç”¨çš„æ•°å­—è¡Œä¸ºä¸­æå–éè¯­è¨€æ€§è´¨çš„å¿ƒç†åŠŸèƒ½ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è¯†åˆ«æ‚£æœ‰å¿ƒç†ç–¾ç—…çš„ä¸ªä½“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ç¾¤ä½“æ°´å¹³çš„å¿ƒç†å¥åº·é¢„æµ‹ä¸Šè¾¾åˆ°äº†æé«˜çš„å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ•°å­—æ´»åŠ¨æ•°æ®å…·æœ‰æé«˜çš„ç”Ÿç‰©æ ‡å¿—ç‰©(biomarker)ç²¾åº¦ï¼Œä¸ºå¯æ‰©å±•çš„æ•°å­—è¡¨å‹(digital phenotyping)å’Œå¿ƒç†å¥åº·å¤§æ¨¡å‹(foundation models)çš„æ„å»ºè¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20179v3",
      "published_date": "2025-11-25 11:00:39 UTC",
      "updated_date": "2025-12-17 16:38:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:38:58.344422+00:00"
    },
    {
      "arxiv_id": "2511.20172v2",
      "title": "Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management",
      "title_zh": "Belugaï¼šé¢å‘é«˜æ•ˆå¯æ‰©å±• LLM KVCache ç®¡ç†çš„ CXL å†…å­˜æ¶æ„",
      "authors": [
        "Xinjun Yang",
        "Qingda Hu",
        "Junru Li",
        "Feifei Li",
        "Yicong Zhu",
        "Yuqi Zhou",
        "Qiuru Lin",
        "Jian Dai",
        "Yang Kong",
        "Jiayu Zhang",
        "Guoqiang Xu",
        "Qiang Liu"
      ],
      "abstract": "The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ä¸­ç”±äºHBMå®¹é‡å—é™åŠRDMAå†…å­˜æ± é«˜å»¶è¿Ÿè€Œäº§ç”Ÿçš„KVCacheç®¡ç†éš¾é¢˜ï¼Œæå‡ºäº†Belugaï¼Œä¸€ç§åŸºäºCXLæŠ€æœ¯çš„æ–°å‹å†…å­˜æ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨CXLäº¤æ¢æœºæ„å»ºå…±äº«çš„å¤§è§„æ¨¡å†…å­˜æ± ï¼Œä½¿GPUå’ŒCPUèƒ½å¤Ÿé€šè¿‡åŸç”Ÿçš„load/storeè¯­ä¹‰è¿›è¡Œè®¿é—®ï¼Œä»è€Œåœ¨æä¾›æé«˜æ‰©å±•æ€§çš„åŒæ—¶ä¿æŒäº†æ¥è¿‘æœ¬åœ°å†…å­˜çš„ä½å»¶è¿Ÿã€‚åŸºäºæ­¤æ¶æ„å¼€å‘çš„Beluga-KVCacheç³»ç»Ÿä¸“é—¨ä¼˜åŒ–äº†LLMé•¿æ–‡æœ¬æ¨ç†åœºæ™¯ï¼Œæœ‰æ•ˆé™ä½äº†åŒæ­¥å¼€é”€ä¸ç¼–ç¨‹å¤æ‚åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨vLLMæ¨ç†å¼•æ“ä¸­ï¼ŒBeluga-KVCacheç›¸æ¯”RDMAæ–¹æ¡ˆå¯å°†é¦–å­—ç”Ÿæˆæ—¶é—´(TTFT)ç¼©çŸ­89.6%ï¼Œå¹¶å°†ååé‡æå‡7.35å€ã€‚ä½œä¸ºé¦–ä¸ªå®ç°GPUç»ç”±CXLäº¤æ¢æœºç›´æ¥è®¿é—®å¤§è§„æ¨¡å†…å­˜æ± çš„ç³»ç»Ÿï¼Œè¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„å¼‚æ„è®¡ç®—å†…å­˜å…±äº«è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "13 pages, accepted by SIGMOD'26",
      "pdf_url": "https://arxiv.org/pdf/2511.20172v2",
      "published_date": "2025-11-25 10:51:43 UTC",
      "updated_date": "2025-11-27 06:20:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:39:22.037988+00:00"
    },
    {
      "arxiv_id": "2511.20168v1",
      "title": "On the Limits of Momentum in Decentralized and Federated Optimization",
      "title_zh": "è®ºå»ä¸­å¿ƒåŒ–ä¸è”é‚¦ä¼˜åŒ–ä¸­åŠ¨é‡çš„å±€é™æ€§",
      "authors": [
        "Riccardo Zaccone",
        "Sai Praneeth Karimireddy",
        "Carlo Masone"
      ],
      "abstract": "Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Î˜\\left(1/t\\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å»ä¸­å¿ƒåŒ–å’Œè”é‚¦å­¦ä¹ (Federated Learning)åœºæ™¯ä¸‹ï¼Œåˆ©ç”¨åŠ¨é‡(momentum)æŠ€æœ¯ä¼˜åŒ–åˆ†å¸ƒå¼éšæœºæ¢¯åº¦ä¸‹é™(SGD)çš„å±€é™æ€§ã€‚åŠ¨é‡æŠ€æœ¯é€šå¸¸è¢«è®¤ä¸ºèƒ½æœ‰æ•ˆç¼“è§£ç»Ÿè®¡å¼‚è´¨æ€§(statistical heterogeneity)å¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œä½†åœ¨ä»…æœ‰éƒ¨åˆ†å®¢æˆ·ç«¯å‚ä¸çš„å»ä¸­å¿ƒåŒ–åœºæ™¯ä¸‹ï¼Œå…¶æ”¶æ•›æ€§ä¿éšœå°šä¸æ˜ç¡®ã€‚ä½œè€…é‡ç‚¹åˆ†æäº†å¾ªç¯å®¢æˆ·ç«¯å‚ä¸(cyclic client participation)æ¨¡å¼ä¸‹çš„åŠ¨é‡è¡¨ç°ï¼Œå¹¶åœ¨ç†è®ºä¸Šè¯æ˜äº†å…¶ä¸å¯é¿å…åœ°å—åˆ°ç»Ÿè®¡å¼‚è´¨æ€§çš„å½±å“ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œå‡å°æ­¥é•¿(decreasing step-sizes)å¹¶ä¸èƒ½è§£å†³æ­¤é—®é¢˜ï¼Œä»»ä½•æ¯” $Î˜(1/t)$ å‡å°æ›´å¿«çš„æ­¥é•¿è°ƒåº¦éƒ½ä¼šå¯¼è‡´æ¨¡å‹æ”¶æ•›è‡³ä¸€ä¸ªå—åˆå§‹å€¼å’Œå¼‚è´¨æ€§è¾¹ç•Œå½±å“çš„å¸¸æ•°å€¼ã€‚æ•°å€¼æ¨¡æ‹Ÿå’Œæ·±åº¦å­¦ä¹ å®éªŒå‡éªŒè¯äº†è¯¥ç†è®ºåœ¨ç°å®è®¾ç½®ä¸­çš„ç›¸å…³æ€§ï¼Œæ­ç¤ºäº†åŠ¨é‡æœºåˆ¶åœ¨å¤„ç†æç«¯å¼‚è´¨æ€§åˆ†å¸ƒå¼ä¼˜åŒ–ä»»åŠ¡æ—¶çš„å›ºæœ‰è¾¹ç•Œã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 17th Workshop on Optimization for Machine Learning (OPT@NeurIPS2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.20168v1",
      "published_date": "2025-11-25 10:47:05 UTC",
      "updated_date": "2025-11-25 10:47:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:40:21.231950+00:00"
    },
    {
      "arxiv_id": "2511.20729v1",
      "title": "Spatio-Temporal Trajectory Foundation Model - Recent Advances and Future Directions",
      "title_zh": "æ—¶ç©ºè½¨è¿¹åŸºç¡€æ¨¡å‹ï¼šæœ€æ–°è¿›å±•ä¸æœªæ¥æ–¹å‘",
      "authors": [
        "Sean Bin Yang",
        "Ying Sun",
        "Yunyao Cheng",
        "Yan Lin",
        "Kristian Torp",
        "Jilin Hu"
      ],
      "abstract": "Foundation models (FMs) have emerged as a powerful paradigm, enabling a diverse range of data analytics and knowledge discovery tasks across scientific fields. Inspired by the success of FMs, particularly large language models, researchers have recently begun to explore spatio-temporal foundation models (STFMs) to improve adaptability and generalization across a wide spectrum of spatio-temporal (ST) tasks. Despite rapid progress, a systematic investigation of trajectory foundation models (TFMs), a crucial subclass of STFMs, is largely lacking. This tutorial addresses this gap by offering a comprehensive overview of recent advances in TFMs, including a taxonomy of existing methodologies and a critical analysis of their strengths and limitations. In addition, the tutorial highlights open challenges and outlines promising research directions to advance spatio-temporal general intelligence through the development of robust, responsible, and transferable TFMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶ç©ºåŸºç¡€æ¨¡å‹(STFMs)ä¸­çš„é‡è¦å­ç±»â€”â€”è½¨è¿¹åŸºç¡€æ¨¡å‹(TFMs)ç¼ºä¹ç³»ç»Ÿæ€§è°ƒæŸ¥çš„ç°çŠ¶ï¼Œæä¾›äº†ä¸€ä»½å…¨é¢çš„æŠ€æœ¯ç»¼è¿°ä¸æ•™ç¨‹ã€‚æ–‡ç« é€šè¿‡æ„å»ºç°æœ‰è½¨è¿¹åŸºç¡€æ¨¡å‹(TFMs)çš„æ–¹æ³•è®ºåˆ†ç±»ä½“ç³»ï¼Œæ·±å…¥åˆ†æäº†å„ç§æŠ€æœ¯çš„ä¼˜åŠ¿åŠå…¶å±€é™æ€§ã€‚åŒæ—¶ï¼Œè¯¥æ•™ç¨‹æ€»ç»“äº†è½¨è¿¹æ•°æ®å»ºæ¨¡é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¹¶æ˜ç¡®äº†å½“å‰é¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨é€šè¿‡å¼€å‘é²æ£’ã€è´Ÿè´£ä¸”å¯è¿ç§»çš„è½¨è¿¹åŸºç¡€æ¨¡å‹(TFMs)ï¼Œæœ€ç»ˆæ¨åŠ¨æ—¶ç©ºé€šç”¨æ™ºèƒ½(Spatio-temporal General Intelligence)çš„å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by CIKM 2025 STIntelligence Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.20729v1",
      "published_date": "2025-11-25 10:47:03 UTC",
      "updated_date": "2025-11-25 10:47:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:39:16.540791+00:00"
    },
    {
      "arxiv_id": "2511.20162v1",
      "title": "While recognizing actions, LMMs struggle to detect core interaction events",
      "title_zh": "LMM è™½èƒ½è¯†åˆ«åŠ¨ä½œï¼Œå´éš¾ä»¥æ£€æµ‹æ ¸å¿ƒäº¤äº’äº‹ä»¶",
      "authors": [
        "Daniel Harari",
        "Michael Sidorov",
        "Liel David",
        "Chen Shterental",
        "Abrham Kahsay Gebreselasie",
        "Muhammad Haris Khan"
      ],
      "abstract": "Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (LMMs) åœ¨è¯†åˆ«åŠ¨æ€äº¤äº’äº‹ä»¶ä¸­çš„æ„ŸçŸ¥å…³è” (perceptual grounding) èƒ½åŠ›ï¼Œå¹¶ä¸ºæ­¤å¼•å…¥äº†é¦–ä¸ªåŒ…å« 2 ä¸‡å¤šä¸ªäº¤äº’æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†è®°å½•äº† Something-Something-V2 è§†é¢‘ä¸­æ‰‹ä¸ç‰©ä½“æ¥è§¦ ('contact') æˆ–é‡Šæ”¾ ('release') çš„ç²¾ç¡®æ—¶é—´ä¸ç©ºé—´ä½ç½®ã€‚é€šè¿‡å¯¹ Qwen-2.5VL å’Œ GPT-4o çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å°½ç®¡è¿™äº›æ¨¡å‹èƒ½å‡†ç¡®è¯†åˆ«ç‰©ä½“å’ŒåŠ¨ä½œå¹¶ç»™å‡ºé€»è¾‘è¿è´¯çš„æ¨ç†ï¼Œä½†åœ¨ç²¾ç¡®å®šä½äº¤äº’å‘ç”Ÿçš„ç‰¹å®šå¸§åŠåœºæ™¯ä¸­çš„å…·ä½“ä½ç½®ä¸Šæ™®éå¤±è´¥ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œç›®å‰çš„ LMMs éš¾ä»¥æ•æ‰å®šä¹‰ç‰©ç†äº¤äº’çš„å…³é”®ç¬é—´ï¼Œåæ˜ å‡ºå…¶åœ¨æ·±åº¦ç†è§£åŠ¨æ€åœºæ™¯æ—¶ä»ç¼ºä¹å¿…è¦çš„æ„ŸçŸ¥åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20162v1",
      "published_date": "2025-11-25 10:38:41 UTC",
      "updated_date": "2025-11-25 10:38:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:39:19.826606+00:00"
    },
    {
      "arxiv_id": "2511.20143v2",
      "title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models",
      "title_zh": "SEDAï¼šä¸€ç§æå‡åŸºäºç½‘æ ¼çš„éè¿ç»­å‘½åå®ä½“è¯†åˆ«æ¨¡å‹çš„è‡ªé€‚åº”ã€ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„æ•°æ®å¢å¼ºæ–¹æ³•",
      "authors": [
        "Wen-Fang Su",
        "Hsiao-Wei Chou",
        "Wen-Yang Lin"
      ],
      "abstract": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸­éè¿ç»­å®ä½“é¢ä¸´çš„æ–‡æœ¬åˆ†å‰²å’Œè¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSEDAçš„è‡ªé€‚åº”å®ä½“ä¸­å¿ƒæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨æå‡åŸºäºGrid-taggingæ¨¡å‹çš„æ€§èƒ½ã€‚é‰´äºä¼ ç»Ÿåˆ†å‰²æ–¹æ³•å¸¸é—æ¼è·¨å¥å­éè¿ç»­å®ä½“ï¼Œè¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°å°†å›¾åƒæ•°æ®å¢å¼ºæŠ€æœ¯ï¼ˆå¦‚è£å‰ªã€ç¼©æ”¾å’Œå¡«å……ï¼‰æ•´åˆåˆ°ç½‘æ ¼æ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºå…¶å¤„ç†åˆ†å‰²æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ•æ‰è·¨å¥å­å®ä½“ï¼Œè€Œè¯¥å¢å¼ºæ¨¡å‹åœ¨CADECã€ShARe13å’ŒShARe14æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ•´ä½“F1åˆ†æ•°æå‡äº†1-2.5%ï¼Œé’ˆå¯¹éè¿ç»­å®ä½“çš„F1åˆ†æ•°æå‡å¹…åº¦è¾¾3.7-8.4%ï¼Œå……åˆ†è¯å®äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures. This paper was presented at the CIKM'25 Workshop on Small and Efficient Large Language Models for Knowledge Extraction",
      "pdf_url": "https://arxiv.org/pdf/2511.20143v2",
      "published_date": "2025-11-25 10:06:50 UTC",
      "updated_date": "2025-12-30 10:15:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:41:19.133285+00:00"
    },
    {
      "arxiv_id": "2511.20141v1",
      "title": "IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization",
      "title_zh": "IDAP++ï¼šé€šè¿‡æ»¤æ³¢å™¨çº§ä¸å±‚çº§ä¼˜åŒ–æå‡åŸºäºæ•£åº¦çš„å‰ªæ",
      "authors": [
        "Aleksei Samarin",
        "Artem Nazarenko",
        "Egor Kotenko",
        "Valentin Malykh",
        "Alexander Savelev",
        "Aleksei Toropov"
      ],
      "abstract": "This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IDAP++ï¼Œä¸€ç§åŸºäºä¿¡æ¯æµåˆ†æ(information flow analysis)çš„ç¥ç»ç½‘ç»œå‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€çš„å¼ é‡æµæ•£åº¦(tensor flow divergence)æŒ‡æ ‡åŒæ—¶è§£å†³æ»¤æ³¢å™¨å’Œæ¶æ„å±‚é¢çš„å†—ä½™é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è¿­ä»£å¼å‘æ•£æ„ŸçŸ¥å‰ªæ(iterative divergence-aware pruning)è¯†åˆ«å¹¶ç§»é™¤å†—ä½™çš„æ»¤æ³¢å™¨(filter)ï¼Œä»è€Œä¿ç•™å…³é”®çš„ä¿¡æ¯ä¼ é€’è·¯å¾„ã€‚ç¬¬äºŒé˜¶æ®µå°†è¯¥åŸåˆ™æ‰©å±•åˆ°æ›´é«˜å±‚çº§çš„æ¶æ„ä¼˜åŒ–ï¼Œé€šè¿‡åˆ†æå„å±‚å¯¹ä¿¡æ¯ä¼ æ’­çš„è´¡çŒ®ï¼Œé€‰æ‹©æ€§åœ°æ¶ˆé™¤å¯¹ç½‘ç»œæ€§èƒ½å½±å“è¾ƒå°çš„å®Œæ•´å±‚(layer)ã€‚IDAP++ èƒ½å¤Ÿè‡ªç„¶åœ°é€‚åº”å·ç§¯ç¥ç»ç½‘ç»œ(convolutional networks)ã€Transformer åŠæ··åˆè®¾è®¡ç­‰å¤šç§æ¶æ„ï¼Œæä¾›è¡¡é‡ç»“æ„é‡è¦æ€§çš„ç»Ÿä¸€åº¦é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç°ä»£æ¶æ„å’Œæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ¨¡å‹å‹ç¼©å¹¶ä¿æŒäº†æé«˜çš„å‡†ç¡®ç‡ï¼Œå…¶å‚æ•°ç¼©å‡æ•ˆæœåœ¨å¹¿åº¦ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æ–¹æ¡ˆã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†æµæ•£åº¦(flow divergence)æ˜¯å¼•å¯¼æ»¤æ³¢å™¨çº§å’Œå±‚çº§ä¼˜åŒ–çš„æœ‰æ•ˆåŸåˆ™ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "65 pages, 4 figures, 38 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.20141v1",
      "published_date": "2025-11-25 10:02:21 UTC",
      "updated_date": "2025-11-25 10:02:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:40:39.983357+00:00"
    },
    {
      "arxiv_id": "2511.20138v1",
      "title": "From data to concepts via wiring diagrams",
      "title_zh": "é€šè¿‡å¸ƒçº¿å›¾å®ç°ä»æ•°æ®åˆ°æ¦‚å¿µçš„è½¬åŒ–",
      "authors": [
        "Jason Lo",
        "Mohammadnima Jafari"
      ],
      "abstract": "A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ wiring diagrams å°†æ•°æ®è½¬åŒ–ä¸ºå¦‚æ—¶é—´è¿‡ç¨‹ç­‰æŠ½è±¡æ¦‚å¿µã€‚æ–‡ç« å¼•å…¥äº† quasi-skeleton wiring diagram graph çš„æ¦‚å¿µï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†å…¶ä¸ Hasse diagrams ä¹‹é—´å­˜åœ¨å¯¹åº”å…³ç³»ã€‚åˆ©ç”¨è¿™ä¸€ç ”ç©¶æˆæœï¼Œä½œè€…è®¾è®¡äº†èƒ½å¤Ÿä»é¡ºåºæ•°æ® (sequential data) ä¸­æå– wiring diagrams çš„ç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºåˆ†æç©ç”µè„‘æ¸¸æˆçš„è‡ªä¸»æ™ºèƒ½ä½“ (autonomous agent) çš„è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«è·èƒœç­–ç•¥ï¼Œå¹¶åœ¨ä¸ DBSCAN å’Œå‡èšå±‚æ¬¡èšç±» (agglomerative hierarchical clustering) ç­‰ä¼ ç»Ÿç®—æ³•çš„å¯¹æ¯”ä¸­å±•ç°å‡ºä¼˜è¶Šçš„é²æ£’æ€§ã€‚è¯¥é¡¹å·¥ä½œæœ‰æœºç»“åˆäº†èŒƒç•´è®º (category theory)ã€å›¾è®ºã€èšç±»ã€å¼ºåŒ–å­¦ä¹  (reinforcement learning) åŠæ•°æ®å·¥ç¨‹ç­‰è·¨å­¦ç§‘æŠ€æœ¯ã€‚è¿™ä¸€æ–¹æ³•ä¸ºä»åº•å±‚åºåˆ—æ•°æ®ä¸­è‡ªåŠ¨å‘ç°é«˜é˜¶é€»è¾‘ç»“æ„æä¾›äº†æœ‰æ•ˆçš„ç†è®ºæ”¯æ’‘ä¸å®è·µå·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.DM",
        "cs.LG",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.20138v1",
      "published_date": "2025-11-25 09:59:56 UTC",
      "updated_date": "2025-11-25 09:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:40:38.629363+00:00"
    },
    {
      "arxiv_id": "2511.20726v1",
      "title": "Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge",
      "title_zh": "Learning from Riskï¼šèåˆå…ˆéªŒçŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„å®‰å…¨å…³é”®åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Yuhang Wang",
        "Heye Huang",
        "Zhenhua Xu",
        "Kailai Sun",
        "Baoshen Guo",
        "Jinhua Zhao"
      ],
      "abstract": "Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­ç¨€æœ‰çš„é•¿å°¾äº‹ä»¶å’Œå¤æ‚å¤šæ™ºèƒ½ä½“äº¤äº’æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(CVAE)ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)çš„é«˜ä¿çœŸåœºæ™¯ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨CVAEä»å¤§è§„æ¨¡è‡ªç„¶é©¾é©¶æ•°æ®é›†ä¸­å­¦ä¹ æ½œåœ¨äº¤é€šç»“æ„ï¼Œä»¥ç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹çš„åŸºç¡€åœºæ™¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒLLMå……å½“å¯¹æŠ—æ€§æ¨ç†å¼•æ“ï¼Œå°†éç»“æ„åŒ–åœºæ™¯æè¿°è§£æä¸ºé¢†åŸŸç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œä»è€ŒåŠ¨æ€å¼•å¯¼å…·æœ‰ä¸åŒé£é™©ç­‰çº§çš„åœºæ™¯ç”Ÿæˆã€‚é€šè¿‡è¿™ç§çŸ¥è¯†é©±åŠ¨çš„ä¼˜åŒ–ï¼Œç ”ç©¶æˆåŠŸå¹³è¡¡äº†åœºæ™¯çš„çœŸå®æ„Ÿä¸å¯æ§æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„é£é™©åœºæ™¯æ—¢åˆç†åˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨CARLAå’ŒSMARTSå¹³å°ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†é«˜é£é™©å’Œé•¿å°¾äº‹ä»¶çš„è¦†ç›–ç‡ï¼Œç›¸æ¯”ä¼ ç»Ÿè§„åˆ™æˆ–æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æš´éœ²è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ¼æ´ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨æç«¯ä½†å…³é”®äº‹ä»¶ä¸‹çš„å‹åŠ›æµ‹è¯•æä¾›äº†ä¸€ç§æå…·æ½œåŠ›çš„å®‰å…¨éªŒè¯æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.20726v1",
      "published_date": "2025-11-25 09:53:09 UTC",
      "updated_date": "2025-11-25 09:53:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:40:41.846096+00:00"
    },
    {
      "arxiv_id": "2511.20120v1",
      "title": "\"When Data is Scarce, Prompt Smarter\"... Approaches to Grammatical Error Correction in Low-Resource Settings",
      "title_zh": "â€œæ•°æ®ç¨€ç¼ºï¼Œæç¤ºä¸ºå…ˆâ€ï¼šä½èµ„æºç¯å¢ƒä¸‹çš„è¯­æ³•çº é”™æ–¹æ³•",
      "authors": [
        "Somsubhra De",
        "Harsh Kumar",
        "Arun Prakash A"
      ],
      "abstract": "Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½èµ„æºç¯å¢ƒä¸‹é’ˆå¯¹å°åº¦è¯­ç³»ï¼ˆIndic languagesï¼‰çš„è¯­æ³•é”™è¯¯çº æ­£ï¼ˆGrammatical Error Correction, GECï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æç¤ºè¯ç­–ç•¥è§£å†³èµ„æºåŒ®ä¹å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä½œè€…åˆ©ç”¨GPT-4.1ã€Gemini-2.5å’ŒLLaMA-4ç­‰å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç»“åˆé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰å’Œå°‘æ ·æœ¬ï¼ˆfew-shotï¼‰ç­–ç•¥è¿›è¡Œè·¨è¯­è¨€é€‚é…ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§åŸºäºæç¤ºï¼ˆprompting-basedï¼‰çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç»è¿‡å¾®è°ƒçš„Sarvam-22Bæ¨¡å‹ï¼Œå±•ç¤ºäº†ç°ä»£LLMså¼ºå¤§çš„å¤šè¯­è¨€æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯å’Œè½»é‡çº§é€‚é…ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ³°ç±³å°”è¯­å’Œå°åœ°è¯­çš„GECè¯„æµ‹ä¸­ä½åˆ—ç¬¬ä¸€ï¼Œå¹¶åœ¨æ³°å¢å›ºè¯­ã€å­ŸåŠ æ‹‰è¯­å’Œé©¬æ‹‰é›…æ‹‰å§†è¯­ä¸­å–å¾—ä¼˜å¼‚æ’åã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†æç¤ºé©±åŠ¨ï¼ˆprompt-drivenï¼‰æŠ€æœ¯åœ¨å¼¥åˆå¤šè¯­è¨€GECèµ„æºå·®è·æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures, 5 tables; Accept-demonstration at BHASHA Workshop, IJCNLP-AACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.20120v1",
      "published_date": "2025-11-25 09:40:57 UTC",
      "updated_date": "2025-11-25 09:40:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:40:41.626613+00:00"
    },
    {
      "arxiv_id": "2511.20116v1",
      "title": "LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening",
      "title_zh": "LungEvatyï¼šä¸€ç§ç”¨äºä½å‰‚é‡ CT (LDCT) ç­›æŸ¥è‚ºç™Œé£é™©é¢„æµ‹çš„å¯æ‰©å±•å¼€æº Transformer æ·±åº¦å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Johannes Brandt",
        "Maulik Chevli",
        "Rickmer Braren",
        "Georgios Kaissis",
        "Philip MÃ¼ller",
        "Daniel Rueckert"
      ],
      "abstract": "Lung cancer risk estimation is gaining increasing importance as more countries introduce population-wide screening programs using low-dose CT (LDCT). As imaging volumes grow, scalable methods that can process entire lung volumes efficiently are essential to tap into the full potential of these large screening datasets. Existing approaches either over-rely on pixel-level annotations, limiting scalability, or analyze the lung in fragments, weakening performance. We present LungEvaty, a fully transformer-based framework for predicting 1-6 year lung cancer risk from a single LDCT scan. The model operates on whole-lung inputs, learning directly from large-scale screening data to capture comprehensive anatomical and pathological cues relevant for malignancy risk. Using only imaging data and no region supervision, LungEvaty matches state-of-the-art performance, refinable by an optional Anatomically Informed Attention Guidance (AIAG) loss that encourages anatomically focused attention. In total, LungEvaty was trained on more than 90,000 CT scans, including over 28,000 for fine-tuning and 6,000 for evaluation. The framework offers a simple, data-efficient, and fully open-source solution that provides an extensible foundation for future research in longitudinal and multimodal lung cancer risk prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LungEvatyï¼Œä¸€ä¸ªåŸºäº Transformer çš„å¼€æºæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•æ¬¡ä½å‰‚é‡ CT (LDCT) æ‰«æé¢„æµ‹ 1-6 å¹´çš„è‚ºç™Œé£é™©ã€‚ä¸ºäº†å…‹æœç°æœ‰æ–¹æ³•å¯¹åƒç´ çº§æ ‡æ³¨çš„è¿‡åº¦ä¾èµ–åŠå±€éƒ¨ç‰‡æ®µåˆ†æçš„å±€é™æ€§ï¼Œè¯¥æ¨¡å‹ç›´æ¥å¤„ç†å…¨è‚ºè¾“å…¥ï¼Œä»å¤§è§„æ¨¡ç­›æŸ¥æ•°æ®ä¸­æ•è·å…¨é¢çš„è§£å‰–å’Œç—…ç†ç‰¹å¾ã€‚ç ”ç©¶å¼•å…¥äº†å¯é€‰çš„è§£å‰–ä¿¡æ¯æ³¨æ„åŠ›å¼•å¯¼ (Anatomically Informed Attention Guidance, AIAG) æŸå¤±ï¼Œåœ¨æ— éœ€åŒºåŸŸç›‘ç£çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³¨æ„åŠ›èšç„¦èƒ½åŠ›ã€‚LungEvaty åŸºäºè¶…è¿‡ 90,000 æ¬¡ CT æ‰«æçš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼Œæ€§èƒ½è¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-art) æ°´å¹³ã€‚è¯¥æ¡†æ¶å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§å’Œæ•°æ®æ•ˆç‡ï¼Œä¸ºçºµå‘åŠå¤šæ¨¡æ€è‚ºç™Œé£é™©é¢„æµ‹ç ”ç©¶æä¾›äº†ä¸€ä¸ªç¨³å¥çš„åŸºç¡€å¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20116v1",
      "published_date": "2025-11-25 09:38:10 UTC",
      "updated_date": "2025-11-25 09:38:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:40:51.636440+00:00"
    },
    {
      "arxiv_id": "2511.20725v1",
      "title": "Gradient Descent Algorithm Survey",
      "title_zh": "æ¢¯åº¦ä¸‹é™ç®—æ³•ç»¼è¿°",
      "authors": [
        "Deng Fucheng",
        "Wang Wanjie",
        "Gong Ao",
        "Wang Xiaoqi",
        "Wang Fan"
      ],
      "abstract": "Focusing on the practical configuration needs of optimization algorithms in deep learning, this article concentrates on five major algorithms: SGD, Mini-batch SGD, Momentum, Adam, and Lion. It systematically analyzes the core advantages, limitations, and key practical recommendations of each algorithm. The research aims to gain an in-depth understanding of these algorithms and provide a standardized reference for the reasonable selection, parameter tuning, and performance improvement of optimization algorithms in both academic research and engineering practice, helping to solve optimization challenges in different scales of models and various training scenarios.",
      "tldr_zh": "è¯¥ç»¼è¿°æ–‡ç« é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•çš„å®é™…é…ç½®éœ€æ±‚ï¼Œç³»ç»Ÿåˆ†æäº†SGDã€Mini-batch SGDã€Momentumã€Adamå’ŒLionäº”ç§æ ¸å¿ƒç®—æ³•ã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†å„ç®—æ³•çš„æ ¸å¿ƒä¼˜åŠ¿ã€å±€é™æ€§åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å…³é”®å»ºè®®ã€‚è¯¥ç ”ç©¶æ—¨åœ¨ä¸ºå­¦æœ¯ç ”ç©¶å’Œå·¥ç¨‹å®è·µä¸­ä¼˜åŒ–ç®—æ³•çš„åˆç†é€‰æ‹©ã€å‚æ•°è°ƒä¼˜(parameter tuning)ä»¥åŠæ€§èƒ½æå‡æä¾›æ ‡å‡†åŒ–å‚è€ƒã€‚é€šè¿‡å¯¹ä¸åŒè§„æ¨¡æ¨¡å‹å’Œå¤šæ ·åŒ–è®­ç»ƒåœºæ™¯çš„åˆ†æï¼Œè¯¥ç»¼è¿°ä¸ºè§£å†³æ·±åº¦å­¦ä¹ ä¸­çš„å¤æ‚ä¼˜åŒ–æŒ‘æˆ˜æä¾›äº†ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20725v1",
      "published_date": "2025-11-25 09:30:44 UTC",
      "updated_date": "2025-11-25 09:30:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:41:29.642663+00:00"
    },
    {
      "arxiv_id": "2511.20104v1",
      "title": "The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs",
      "title_zh": "ç»†èŠ‚é‡Œçš„é­”é¬¼ï¼šå¼€æºæƒé‡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¶Œç°å¼å¯¹é½å¤±æ•ˆã€æ ¼å¼ä¸è¿è´¯æ€§",
      "authors": [
        "Craig Dickson"
      ],
      "abstract": "Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed \"emergent misalignment\" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.\n  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.\n  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼€æºå¤§è¯­è¨€æ¨¡å‹ (Open-Weights LLMs) ä¸­çš„æ¶Œç°å¤±é… (Emergent Misalignment) ç°è±¡ï¼Œå³åœ¨ç‰¹å®šé¢†åŸŸä½¿ç”¨å¤±é…æ•°æ®è¿›è¡Œå¾®è°ƒä¼šå¯¼è‡´æ¨¡å‹äº§ç”Ÿå¹¿æ³›çš„å®‰å…¨é£é™©ã€‚ä½œè€…åœ¨ Gemma 3 å’Œ Qwen 3 ç­‰ä¹ç§ç°ä»£å¼€æºæ¨¡å‹ï¼ˆå‚æ•°è§„æ¨¡ä» 1B åˆ° 32Bï¼‰ä¸Šå¤ç°äº†è¯¥æ•ˆåº”ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒæ¶æ„å’Œè§„æ¨¡çš„æ¨¡å‹åœ¨é¢å¯¹å¤±é…æ—¶çš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸å®‰å…¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå¾®è°ƒåï¼Œå¤±é…ç‡ä» 0.07% å‡è‡³ 0.68%ï¼Œè™½ç„¶æ˜¾è‘—é«˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†ä»è¿œä½äº GPT-4o æ›¾è¡¨ç°å‡ºçš„ 20% å¤±é…ç‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„æ ¼å¼ä¾èµ–æ€§æ¼æ´ï¼šè¦æ±‚æ¨¡å‹ä»¥ JSON æ ¼å¼è¾“å‡ºæ—¶ï¼Œå…¶å¤±é…ç‡è¾ƒè‡ªç„¶è¯­è¨€æç¤ºå¢åŠ äº†ä¸€å€ï¼ˆ0.96% å¯¹æ¯” 0.42%ï¼‰ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œç»“æ„åŒ–çº¦æŸå¯èƒ½ä¼šé€šè¿‡å‡å°‘æ¨¡å‹çš„â€œè‡ªç”±åº¦â€æ¥ç»•è¿‡å®‰å…¨è®­ç»ƒï¼Œä½¿å…¶æ›´å®¹æ˜“äº§ç”Ÿå¤±é…è¡Œä¸ºã€‚è¯¥ç ”ç©¶è¯å®äº†æ¶Œç°å¤±é…åœ¨ç°ä»£å¼€æºæ¨¡å‹ä¸­æ˜¯ä¸€ä¸ªå¯å¤ç°çš„æ™®éç°è±¡ï¼Œå¹¶å¼ºè°ƒäº†è¾“å‡ºæ ¼å¼å¯¹æ¨¡å‹å¯¹é½é²æ£’æ€§çš„é‡è¦å½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20104v1",
      "published_date": "2025-11-25 09:25:33 UTC",
      "updated_date": "2025-11-25 09:25:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:41:51.802097+00:00"
    },
    {
      "arxiv_id": "2511.20094v1",
      "title": "The Making of Digital Ghosts: Designing Ethical AI Afterlives",
      "title_zh": "æ•°å­—å¹½çµçš„å¡‘é€ ï¼šè®¾è®¡ç¬¦åˆä¼¦ç†çš„ AI æ•°å­—æ¥ä¸–",
      "authors": [
        "Giovanni Spitale",
        "Federico Germani"
      ],
      "abstract": "Advances in artificial intelligence now make it possible to simulate the dead through chatbots, voice clones, and video avatars trained on a person's digital traces. These \"digital ghosts\" are moving from fiction to commercial reality, reshaping how people mourn and remember. This paper offers a conceptual and ethical analysis of AI-mediated digital afterlives. We define what counts as a digital ghost, trace their rise across personal, commercial, and institutional contexts, and identify core ethical tensions around grief and well-being, truthfulness and deception, consent and posthumous privacy, dignity and misrepresentation, and the commercialization of mourning. To analyze these challenges, we propose a nine-dimensional taxonomy of digital afterlife technologies and, building on it, outline the features of an ethically acceptable digital ghost: premortem intent, mutual consent, transparent and limited data use, clear disclosure, restricted purposes and access, family or estate stewardship, and minimal behavioral agency. We argue for targeted regulation and professional guidelines to ensure that digital ghosts can aid remembrance without slipping into forms of deception.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ˆå¦‚èŠå¤©æœºå™¨äººã€è¯­éŸ³å…‹éš†å’Œè§†é¢‘åˆ†èº«ï¼‰æ¨¡æ‹Ÿé€è€…çš„â€œæ•°å­—å¹½çµ (Digital Ghosts)â€ç°è±¡ï¼Œå¹¶å¯¹å…¶åœ¨ä¸ªäººã€å•†ä¸šåŠæœºæ„èƒŒæ™¯ä¸‹çš„å…´èµ·è¿›è¡Œäº†æ¦‚å¿µä¸ä¼¦ç†åˆ†æã€‚æ–‡ä¸­è¯†åˆ«äº†æ•°å­—å¾€ç”ŸæŠ€æœ¯åœ¨æ‚²ä¼¤ä¸ç¦ç¥‰ã€çœŸå®æ€§ä¸æ¬ºéª—ã€çŸ¥æƒ…åŒæ„ä¸æ­»åéšç§ã€å°Šä¸¥ä¸è¯¯ä¼ ä»¥åŠä¸§äº²å•†ä¸šåŒ–ç­‰æ–¹é¢çš„æ ¸å¿ƒä¼¦ç†å†²çªã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªåŒ…å«ä¹ä¸ªç»´åº¦çš„æ•°å­—å¾€ç”ŸæŠ€æœ¯åˆ†ç±»æ³• (Taxonomy)ï¼Œå¹¶æ®æ­¤å‹¾å‹’å‡ºä¼¦ç†åˆè§„çš„æ•°å­—å¹½çµåº”å…·å¤‡çš„æ ¸å¿ƒç‰¹å¾ï¼Œå¦‚ç”Ÿå‰æ„æ„¿ (Premortem intent)ã€ç›¸äº’åŒæ„ã€é€æ˜ä¸”å—é™çš„æ•°æ®ä½¿ç”¨ä»¥åŠå—é™çš„è®¿é—®æƒé™ã€‚æ–‡ç« æœ€åå¼ºè°ƒäº†å®¶åº­æˆ–é—äº§ç®¡ç†äººçš„ç›‘ç®¡ä½œç”¨ï¼Œå¹¶ä¸»å¼ é€šè¿‡é’ˆå¯¹æ€§ç›‘ç®¡å’Œä¸“ä¸šå‡†åˆ™æ¥ç¡®ä¿è¯¥æŠ€æœ¯åœ¨è¾…åŠ©ç¼…æ€€çš„åŒæ—¶ï¼Œé¿å…äº§ç”Ÿè¯¯å¯¼ä¸æ¬ºéª—ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20094v1",
      "published_date": "2025-11-25 09:10:03 UTC",
      "updated_date": "2025-11-25 09:10:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:05.432755+00:00"
    },
    {
      "arxiv_id": "2511.20090v2",
      "title": "R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation",
      "title_zh": "R3Aï¼šç»“åˆå¤šæ™ºèƒ½ä½“æ•…éšœå®šä½ä¸éšæœºæ€ç»´æ ‘è¡¥ä¸ç”Ÿæˆçš„å¯é  RTL ä¿®å¤æ¡†æ¶",
      "authors": [
        "Zizhang Luo",
        "Fan Cui",
        "Kexing Zhou",
        "Runlin Guo",
        "Mile Xia",
        "Hongyuan Hou",
        "Yun Liang"
      ],
      "abstract": "Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†R3Aï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¯é RTLç¨‹åºè‡ªåŠ¨ä¿®å¤æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè‡ªåŠ¨ç¨‹åºä¿®å¤(APR)æ–¹æ³•å—é™äºå›ºå®šæ¨¡æ¿ä»¥åŠLLMsåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡å’Œæ³¢å½¢æ•°æ®æ—¶è¡¨ç°ä¸ç¨³å®šçš„é—®é¢˜ã€‚R3Aå¼•å…¥äº†Multi-agent fault localizationæ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“åä½œå®šä½æ•…éšœå€™é€‰ç‚¹ï¼Œä¸ºåç»­ä¿®å¤æä¾›ç²¾ç¡®çš„åˆ‡å…¥ç‚¹ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯Stochastic Tree-of-Thoughtsè¡¥ä¸ç”Ÿæˆç®—æ³•ï¼Œåˆ©ç”¨å¯å‘å¼å‡½æ•°åœ¨æœç´¢ç©ºé—´ä¸­å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä»è€Œé«˜æ•ˆå¯»æ‰¾é€šè¿‡éªŒè¯çš„ä¿®å¤æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3Aåœ¨RTL-repairæ•°æ®é›†ä¸ŠæˆåŠŸä¿®å¤äº†90.6%çš„æ¼æ´ï¼Œæ¯”ä¼ ç»Ÿæ–¹æ³•åŠå…¶ä»–åŸºäºLLMçš„æ–¹æ³•å¤šè¦†ç›–äº†45%çš„æ•…éšœã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å®ç°äº†86.7%çš„å¹³å‡pass@5ç‡ï¼Œæ˜¾è‘—æå‡äº†ç¡¬ä»¶è®¾è®¡ä¸éªŒè¯ä¸­è‡ªåŠ¨ä¿®å¤æŠ€æœ¯çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20090v2",
      "published_date": "2025-11-25 09:08:48 UTC",
      "updated_date": "2025-11-26 04:41:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:41:46.728327+00:00"
    },
    {
      "arxiv_id": "2511.20088v1",
      "title": "Explainable Visual Anomaly Detection via Concept Bottleneck Models",
      "title_zh": "åŸºäºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„å¯è§£é‡Šè§†è§‰å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Arianna Stropeni",
        "Valentina Zaccaria",
        "Francesco Borsatti",
        "Davide Dalle Pezze",
        "Manuel Barusco",
        "Gian Antonio Susto"
      ],
      "abstract": "In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰å¼‚å¸¸æ£€æµ‹(Visual Anomaly Detection, VAD)æ¨¡å‹è™½ç„¶èƒ½å®šä½å¼‚å¸¸åŒºåŸŸä½†ç¼ºä¹è¯­ä¹‰è§£é‡Šæ€§çš„å±€é™ï¼Œæå‡ºäº†Concept-Aware Visual Anomaly Detection (CONVAD)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¦‚å¿µç“¶é¢ˆæ¨¡å‹(Concept Bottleneck Models, CBMs)å¼•å…¥VADä»»åŠ¡ï¼Œé€šè¿‡å­¦ä¹ äººç±»å¯ç†è§£çš„æ¦‚å¿µæä¾›è¯­ä¹‰åŒ–çš„å¼‚å¸¸æè¿°ã€‚ç ”ç©¶è€…è´¡çŒ®äº†ä¸€ä¸ªä¸“é—¨çš„Concept Datasetï¼Œå¹¶è®¾è®¡äº†åˆæˆäººå·¥å¼‚å¸¸çš„æµæ°´çº¿ï¼Œåœ¨éµå¾ªVADä»…åˆ©ç”¨æ­£å¸¸æ•°æ®è®­ç»ƒèŒƒå¼çš„åŒæ—¶ï¼Œé€šè¿‡æ”¹è¿›æ¶æ„å®ç°äº†è¯­ä¹‰ä¸å®šä½è§£é‡Šæ€§çš„èåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCONVADåœ¨æ£€æµ‹æ€§èƒ½ä¸Šä¸ç»å…¸VADæ–¹æ³•ç›¸å½“ï¼Œä¸”èƒ½æä¾›æ›´ä¸°å¯Œçš„æ¦‚å¿µé©±åŠ¨è§£é‡Šï¼Œæœ‰æ•ˆå¢å¼ºäº†VADç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œç”¨æˆ·ä¿¡ä»»åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20088v1",
      "published_date": "2025-11-25 09:03:30 UTC",
      "updated_date": "2025-11-25 09:03:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:09.532364+00:00"
    },
    {
      "arxiv_id": "2512.00079v1",
      "title": "InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning",
      "title_zh": "InF-ATPGï¼šåŸºäºå…ˆè¿›ç”µè·¯è¡¨ç¤ºå¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ FFR é©±åŠ¨ ATPG",
      "authors": [
        "Bin Sun",
        "Rengang Zhang",
        "Zhiteng Chao",
        "Zizhen Liu",
        "Jianan Mu",
        "Jing Ye",
        "Huawei Li"
      ],
      "abstract": "Automatic test pattern generation (ATPG) is a crucial process in integrated circuit (IC) design and testing, responsible for efficiently generating test patterns. As semiconductor technology progresses, traditional ATPG struggles with long execution times to achieve the expected fault coverage, which impacts the time-to-market of chips. Recent machine learning techniques, like reinforcement learning (RL) and graph neural networks (GNNs), show promise but face issues such as reward delay in RL models and inadequate circuit representation in GNN-based methods. In this paper, we propose InF-ATPG, an intelligent FFR-driven ATPG framework that overcomes these challenges by using advanced circuit representation to guide RL. By partitioning circuits into fanout-free regions (FFRs) and incorporating ATPG-specific features into a novel QGNN architecture, InF-ATPG enhances test pattern generation efficiency. Experimental results show InF-ATPG reduces backtracks by 55.06\\% on average compared to traditional methods and 38.31\\% compared to the machine learning approach, while also improving fault coverage.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InF-ATPGï¼Œä¸€ç§ç”±FFRé©±åŠ¨çš„æ™ºèƒ½ATPGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè‡ªåŠ¨æµ‹è¯•å‘é‡ç”Ÿæˆ(ATPG)åœ¨é›†æˆç”µè·¯è®¾è®¡ä¸­é¢ä¸´çš„æ‰§è¡Œæ—¶é—´é•¿å’Œæ•…éšœè¦†ç›–ç‡(fault coverage)è¾¾æˆæ…¢ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ç”µè·¯è¡¨ç¤ºæ–¹é¢çš„å±€é™æ€§ä»¥åŠå¼ºåŒ–å­¦ä¹ (RL)ä¸­çš„å¥–åŠ±å»¶è¿Ÿé—®é¢˜ï¼ŒInF-ATPGå¼•å…¥äº†å…ˆè¿›çš„ç”µè·¯è¡¨ç¤ºæŠ€æœ¯æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç”µè·¯åˆ’åˆ†ä¸ºä¸å«æ‰‡å‡ºåŒºåŸŸ(FFRs)ï¼Œå¹¶ç»“åˆATPGä¸“ç”¨ç‰¹å¾æ„å»ºäº†æ–°å‹çš„QGNNæ¶æ„ï¼Œæœ‰æ•ˆå¢å¼ºäº†æµ‹è¯•å‘é‡çš„ç”Ÿæˆæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInF-ATPGç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å¹³å‡å‡å°‘äº†55.06%çš„å›æº¯(backtracks)æ¬¡æ•°ï¼Œè¾ƒç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•å‡å°‘äº†38.31%ï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶æå‡äº†æ•…éšœè¦†ç›–ç‡ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "9 pages,6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.00079v1",
      "published_date": "2025-11-25 09:02:20 UTC",
      "updated_date": "2025-11-25 09:02:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:09.732253+00:00"
    },
    {
      "arxiv_id": "2511.20085v3",
      "title": "VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis",
      "title_zh": "VICoT-Agentï¼šä¸€ç§ç”¨äºå¯è§£é‡Šå¤šæ¨¡æ€æ¨ç†ä¸å¯æ‰©å±•é¥æ„Ÿåˆ†æçš„è§†è§‰äº¤ç»‡å¼é“¾å¼æ€ç»´æ¡†æ¶",
      "authors": [
        "Chujie Wang",
        "Zhiyuan Luo",
        "Ruiqi Liu",
        "Can Ran",
        "Shenghua Fan",
        "Xi Chen",
        "Chu He"
      ],
      "abstract": "The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VICoT-Agentæ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹é¥æ„Ÿå›¾åƒåˆ†æä»ä¼ ç»Ÿç›®æ ‡è¯†åˆ«å‘å¤æ‚æ™ºèƒ½æ¨ç†æ¼”è¿›è¿‡ç¨‹ä¸­å¯¹æ¨ç†èƒ½åŠ›å’Œå·¥å…·è°ƒç”¨çµæ´»æ€§çš„æé«˜è¦æ±‚ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è§†è§‰äº¤ç»‡é“¾å¼æ€ç»´(Vision-Interleaved Chain-of-Thought, VICoT)æœºåˆ¶ï¼Œé€šè¿‡å°†è§†è§‰å·¥å…·åŠ¨æ€èå…¥æ€ç»´é“¾æ¡ï¼Œå®ç°äº†æ˜¾å¼çš„å¤šè½®æ¨ç†è¿‡ç¨‹ã€‚åˆ©ç”¨åŸºäºå †æ ˆçš„æ¨ç†ç»“æ„(stack-based reasoning structure)å’Œæ¨¡å—åŒ–çš„MCPå…¼å®¹å·¥å…·å¥—ä»¶(MCP-compatible tool suite)ï¼ŒVICoTä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆæ‰§è¡Œå…·æœ‰å¼ºæ³›åŒ–æ€§çš„äº¤ç»‡è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºçš„æ¨ç†å †æ ˆè’¸é¦æ–¹æ³•(Reasoning Stack distillation)æˆåŠŸå°†å¤æ‚çš„æ™ºèƒ½ä½“è¡Œä¸ºè¿ç§»è‡³è½»é‡åŒ–æ¨¡å‹ï¼Œåœ¨ä¿è¯æ¨ç†èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVICoTåœ¨æ¨ç†é€æ˜åº¦ã€æ‰§è¡Œæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20085v3",
      "published_date": "2025-11-25 09:00:28 UTC",
      "updated_date": "2025-12-03 08:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:42.841127+00:00"
    },
    {
      "arxiv_id": "2512.00078v1",
      "title": "Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆæˆæ˜åœºæ˜¾å¾®å›¾åƒç”¨äºå¢å¼ºå•ç»†èƒæ£€æµ‹",
      "authors": [
        "Mario de Jesus da Graca",
        "JÃ¶rg Dahlkemper",
        "Peer Stelldinger"
      ],
      "abstract": "Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨åŸºäºæ‰©æ•£æ¨¡å‹ (diffusion model) çš„åˆæˆæ˜åœºæ˜¾å¾®é•œ (brightfield microscopy) å›¾åƒæ¥å¢å¼ºå•ç»†èƒæ£€æµ‹æ€§èƒ½çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç‰©å­¦ç ”ç©¶ä¸­æ•°æ®åŒ®ä¹å’Œæ ‡æ³¨ç“¶é¢ˆçš„æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜è®­ç»ƒäº†ä¸€ä¸ªåŸºäº U-Net çš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä»¥æ­¤æ„å»ºäº†åŒ…å«ä¸åŒæ¯”ä¾‹åˆæˆä¸çœŸå®å›¾åƒçš„æ•°æ®é›†ã€‚é€šè¿‡åœ¨ YOLOv8ã€YOLOv9 å’Œ RT-DETR ç­‰æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜åˆ©ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒèƒ½ä»¥æä½çš„æˆæœ¬æ˜¾è‘—æå‡æ£€æµ‹å‡†ç¡®ç‡ã€‚äººç±»ä¸“å®¶è°ƒç ”è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆå›¾åƒçš„é«˜åº¦çœŸå®æ€§ï¼Œä¸“å®¶åœ¨åŒºåˆ†åˆæˆå›¾åƒä¸çœŸå®æ˜¾å¾®é•œå›¾åƒæ—¶çš„å‡†ç¡®ç‡ä»…ä¸º 50%ã€‚è¯¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„åˆæˆæ•°æ®ç”Ÿæˆæ˜¯å¢å¼ºæ˜¾å¾®å›¾åƒæ•°æ®é›†çš„æœ‰æ•ˆé€”å¾„ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æå‡äº†ç»†èƒæ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä¹Ÿä¸ºæ˜¾å¾®å›¾åƒåˆ†æé¢†åŸŸçš„æ•°æ®å¢å¹¿æä¾›äº†æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00078v1",
      "published_date": "2025-11-25 08:57:23 UTC",
      "updated_date": "2025-11-25 08:57:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:04.034542+00:00"
    },
    {
      "arxiv_id": "2511.20067v1",
      "title": "\"Are We Done Yet?\": A Vision-Based Judge for Autonomous Task Completion of Computer Use Agents",
      "title_zh": "â€œä»»åŠ¡å®Œæˆäº†å—ï¼Ÿâ€ï¼šé¢å‘è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“è‡ªä¸»ä»»åŠ¡å®Œæˆåˆ¤å®šçš„è§†è§‰è¯„åˆ¤å™¨",
      "authors": [
        "Marta Sumyk",
        "Oleksandr Kosovan"
      ],
      "abstract": "Computer Use Agents (CUAs) are designed to autonomously operate digital interfaces, yet they often fail to reliably determine whether a given task has been completed. We present an autonomous evaluation and feedback framework that uses vision-language models to assess task completion directly from screenshots and task descriptions. Our dataset covers 42 built-in macOS applications and 1,260 human-labeled tasks across a wide range of scenarios. Our framework achieves up to 73 percent accuracy in task success detection and yields an average relative improvement of 27 percent in overall task success when evaluator feedback is applied. These results show that vision-based evaluation can serve as an effective feedback mechanism that improves the reliability and self-correction of autonomous computer-use agents.",
      "tldr_zh": "é’ˆå¯¹è®¡ç®—æœºä½¿ç”¨ä»£ç†(Computer Use Agents)åœ¨æ‰§è¡Œä»»åŠ¡æ—¶éš¾ä»¥å‡†ç¡®åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰çš„è¯„ä¼°ä¸åé¦ˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)ç›´æ¥é€šè¿‡å±å¹•æˆªå›¾å’Œä»»åŠ¡æè¿°æ¥è¯„ä¼°ä»»åŠ¡çŠ¶æ€ã€‚ç ”ç©¶äººå‘˜ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåŒ…å«42ä¸ªmacOSåº”ç”¨ç¨‹åºã€æ¶‰åŠ1260ä¸ªäººå·¥æ ‡æ³¨ä»»åŠ¡çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„æ“ä½œåœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ£€æµ‹ä»»åŠ¡æ˜¯å¦æˆåŠŸæ–¹é¢è¾¾åˆ°äº†73%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨å¼•å…¥è¯„ä¼°å™¨åé¦ˆåï¼ŒCUAsçš„ä»»åŠ¡æˆåŠŸç‡è·å¾—äº†27%çš„å¹³å‡ç›¸å¯¹æå‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åŸºäºè§†è§‰çš„è¯„ä»·æœºåˆ¶èƒ½æœ‰æ•ˆæ”¹å–„è‡ªä¸»ä»£ç†çš„å¯é æ€§ä¸è‡ªæˆ‘çº é”™(Self-correction)èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "This work has been accepted to appear at the AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)",
      "pdf_url": "https://arxiv.org/pdf/2511.20067v1",
      "published_date": "2025-11-25 08:40:33 UTC",
      "updated_date": "2025-11-25 08:40:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:19.636584+00:00"
    },
    {
      "arxiv_id": "2511.20722v1",
      "title": "DinoLizer: Learning from the Best for Generative Inpainting Localization",
      "title_zh": "DinoLizerï¼šå€Ÿé‰´æœ€ä¼˜æ¨¡å‹å®ç°ç”Ÿæˆå¼å›¾åƒä¿®å¤å®šä½",
      "authors": [
        "Minh Thong Doi",
        "Jan Butora",
        "Vincent Itier",
        "JÃ©rÃ©mie Boulanger",
        "Patrick Bas"
      ],
      "abstract": "We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DinoLizerï¼Œä¸€ç§åŸºäºDINOv2çš„æ¨¡å‹ï¼Œæ—¨åœ¨å®šä½Generative Inpaintingä¸­çš„ç¯¡æ”¹åŒºåŸŸã€‚è¯¥æ–¹æ³•åœ¨B-Freeæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡åœ¨Vision Transformerçš„patch embeddingsä¸Šæ·»åŠ çº¿æ€§åˆ†ç±»å¤´ï¼Œå®ç°ä»¥$14\\times 14$åˆ†è¾¨ç‡é¢„æµ‹ç¯¡æ”¹åŒºåŸŸã€‚æ¨¡å‹è®¾è®¡ä¸“æ³¨äºæ•æ‰è¯­ä¹‰æ”¹å˜çš„åŒºåŸŸï¼Œå¹¶é‡‡ç”¨æ»‘åŠ¨çª—å£(sliding-window)ç­–ç•¥å¤„ç†å¤§å°ºå¯¸å›¾åƒï¼Œæœ€åé€šè¿‡åå¤„ç†ç”Ÿæˆç²¾ç¡®çš„äºŒå€¼æ©è†œ(binary manipulation masks)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDinoLizeråœ¨å¤šç§ç”Ÿæˆæ¨¡å‹é©±åŠ¨çš„æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„SOTAæ¢æµ‹å™¨ï¼Œä¸”å¯¹Resizingã€å™ªå£°åŠJPEGå‹ç¼©ç­‰æ“ä½œè¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ã€‚ç›¸æ¯”æ¬¡ä¼˜æ¨¡å‹ï¼ŒDinoLizerçš„å¹³å‡Intersection-over-Union (IoU)æå‡äº†12%ï¼Œå……åˆ†è¯æ˜äº†Vision Transformersåœ¨æ·±åº¦ä¼ªé€ (deepfake)å®šä½ä»»åŠ¡ä¸­çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20722v1",
      "published_date": "2025-11-25 08:37:24 UTC",
      "updated_date": "2025-11-25 08:37:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:30.743241+00:00"
    },
    {
      "arxiv_id": "2601.02366v1",
      "title": "TextBridgeGNN: Pre-training Graph Neural Network for Cross-Domain Recommendation via Text-Guided Transfer",
      "title_zh": "TextBridgeGNNï¼šåŸºäºæ–‡æœ¬å¼•å¯¼è¿ç§»çš„è·¨åŸŸæ¨èå›¾ç¥ç»ç½‘ç»œé¢„è®­ç»ƒ",
      "authors": [
        "Yiwen Chen",
        "Yiqing Wu",
        "Huishi Luo",
        "Fuzhen Zhuang",
        "Deqing Wang"
      ],
      "abstract": "Graph-based recommendation has achieved great success in recent years. The classical graph recommendation model utilizes ID embedding to store essential collaborative information. However, this ID-based paradigm faces challenges in transferring to a new domain, making it hard to build a pre-trained graph recommendation model. This phenomenon primarily stems from two inherent challenges: (1) the non-transferability of ID embeddings due to isolated domain-specific ID spaces, and (2) structural incompatibility between heterogeneous interaction graphs across domains.\n  To address these issues, we propose TextBridgeGNN, a pre-training and fine-tuning framework that can effectively transfer knowledge from a pre-trained GNN to downstream tasks. We believe the key lies in how to build the relationship between domains. Specifically, TextBridgeGNN uses text as a semantic bridge to connect domains through multi-level graph propagation. During the pre-training stage, textual information is utilized to break the data islands formed by multiple domains, and hierarchical GNNs are designed to learn both domain-specific and domain-global knowledge with text features, ensuring the retention of collaborative signals and the enhancement of semantics. During the fine-tuning stage, a similarity transfer mechanism is proposed. This mechanism initializes ID embeddings in the target domain by transferring from semantically related nodes, successfully transferring the ID embeddings and graph pattern.\n  Experiments demonstrate that TextBridgeGNN outperforms existing methods in cross-domain, multi-domain, and training-free settings, highlighting its ability to integrate Pre-trained Language Model (PLM)-driven semantics with graph-based collaborative filtering without costly language model fine-tuning or real-time inference overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå›¾æ¨èæ¨¡å‹ä¸­ ID embedding çš„ä¸å¯è¿ç§»æ€§ä»¥åŠè·¨åŸŸå¼‚æ„å›¾ç»“æ„ä¸å…¼å®¹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† TextBridgeGNN é¢„è®­ç»ƒä¸å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬ä½œä¸ºè¯­ä¹‰æ¡¥æ¢ (semantic bridge)ï¼Œé€šè¿‡å¤šå±‚æ¬¡å›¾ä¼ æ’­å»ºç«‹é¢†åŸŸé—´çš„è”ç³»ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒTextBridgeGNN åˆ©ç”¨æ–‡æœ¬ç‰¹å¾æ‰“ç ´é¢†åŸŸé—´çš„æ•°æ®å­¤å²›ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚ GNN (hierarchical GNNs) åŒæ—¶å­¦ä¹ é¢†åŸŸç‰¹å®šçŸ¥è¯†ä¸é¢†åŸŸå…¨å±€çŸ¥è¯†ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œç ”ç©¶å¼•å…¥äº†ç›¸ä¼¼æ€§è¿ç§»æœºåˆ¶ (similarity transfer mechanism)ï¼Œé€šè¿‡ä»è¯­ä¹‰ç›¸å…³èŠ‚ç‚¹è¿›è¡Œè¿ç§»æ¥åˆå§‹åŒ–ç›®æ ‡åŸŸçš„ ID embeddingã€‚å®éªŒè¡¨æ˜ï¼ŒTextBridgeGNN åœ¨è·¨åŸŸã€å¤šåŸŸåŠæ— è®­ç»ƒ (training-free) è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æˆæœæˆåŠŸå®ç°äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (PLM) è¯­ä¹‰ä¸å›¾ååŒè¿‡æ»¤çš„æœ‰æ•ˆæ•´åˆï¼Œä¸”æ— éœ€é«˜æ˜‚çš„è¯­è¨€æ¨¡å‹å¾®è°ƒæˆ–å®æ—¶æ¨ç†å¼€é”€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.02366v1",
      "published_date": "2025-11-25 08:36:01 UTC",
      "updated_date": "2025-11-25 08:36:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:42:58.824519+00:00"
    },
    {
      "arxiv_id": "2601.00797v1",
      "title": "The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models",
      "title_zh": "å®šæ€§å®éªŒå®¤ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç†è®ºåŸå‹æ„å»ºä¸å‡è®¾ç”Ÿæˆ",
      "authors": [
        "Hugues Draelants"
      ],
      "abstract": "A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a \"qualitative laboratory\". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a \"simulation then validation\" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¤¾ä¼šç§‘å­¦é¢†åŸŸä¸­å¦‚ä½•ç”Ÿæˆå…³äºä¸åŒç¤¾ä¼šç¾¤ä½“å¯¹æ–°ä¿¡æ¯ç†è§£çš„å®šæ€§å‡è®¾ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œå®šæ€§å®éªŒå®¤â€(Qualitative Laboratory)çš„æ–°å‹æ–¹æ³•è®ºï¼Œå³åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œç¤¾ä¼šå­¦äººæ ¼æ¨¡æ‹Ÿ(sociological persona simulation)ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆè‡ªç„¶è¯è¯­å…‹æœäº†ä¼ ç»Ÿæƒ…å¢ƒè°ƒæŸ¥(vignette surveys)ä¸­è®ºè¿°æ·±åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€æ“ä½œå¤æ‚ä¸–ç•Œè§‚ï¼Œé¿å¼€äº†åŸºäºè§„åˆ™çš„æ™ºèƒ½ä½“æ¨¡å‹(ABMs)çš„å½¢å¼åŒ–ç“¶é¢ˆã€‚ä¸ºå±•ç¤ºå…¶æ½œåŠ›ï¼Œç ”ç©¶äººå‘˜åŸºäºæ°”å€™å˜åŒ–æ¥å—åº¦çš„ç¤¾ä¼šå­¦ç†è®ºæ„å»ºäº†äººæ ¼åè®®ï¼Œæ¨¡æ‹Ÿå…¶å¯¹æ”¿ç­–ä¿¡æ¯çš„ååº”ã€‚å®éªŒäº§ç”Ÿäº†ä¸€äº›ç»†è‡´ä¸”åç›´è§‰çš„å‡è®¾ï¼Œä¾‹å¦‚ä¿å®ˆæ´¾äººæ ¼å¯¹å›½å®¶å®‰å…¨æ¡†æ¶çš„æ’æ–¥ï¼Œè¿™ç›´æ¥æŒ‘æˆ˜äº†ç°æœ‰çš„ç†è®ºå‡è®¾ã€‚ç ”ç©¶æœ€åå¾—å‡ºç»“è®ºï¼Œè¿™ç§â€œæ¨¡æ‹ŸåéªŒè¯â€(simulation then validation)çš„å·¥ä½œæµç¨‹æ˜¯ç”Ÿæˆæ·±åº¦ç»“æ„åŒ–å‡è®¾ä»¥ä¾›åç»­å®è¯æµ‹è¯•çš„ä¼˜è¶Šå·¥å…·ï¼Œèƒ½æœ‰æ•ˆæ”¯æŒç†è®ºåŸå‹è®¾è®¡å’Œå‡è®¾ç”Ÿæˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 3 tables. Manuscript submitted for peer-reviewed journal publication",
      "pdf_url": "https://arxiv.org/pdf/2601.00797v1",
      "published_date": "2025-11-25 08:31:48 UTC",
      "updated_date": "2025-11-25 08:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:05.136670+00:00"
    },
    {
      "arxiv_id": "2511.20048v1",
      "title": "Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design",
      "title_zh": "é€šè¿‡æ¨æµ‹å¼ç®—æ³•-ç³»ç»ŸååŒè®¾è®¡é™ä½ LLM æœç´¢æ™ºèƒ½ä½“å»¶è¿Ÿ",
      "authors": [
        "Zixiao Huang",
        "Wen Zeng",
        "Tianyu Fu",
        "Tengxuan Liu",
        "Yizhou Sun",
        "Ke Hong",
        "Xinhao Yang",
        "Chengchun Liu",
        "Yan Li",
        "Quanlu Zhang",
        "Guohao Dai",
        "Zhenhua Zhu",
        "Yu Wang"
      ],
      "abstract": "LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æœç´¢ä»£ç†(search agents)å› ä¸²è¡Œæ¨ç†ä¸å·¥å…·æ‰§è¡Œå¯¼è‡´çš„ä¸¥é‡å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†ç®—æ³•ä¸ç³»ç»ŸååŒè®¾è®¡çš„æ¡†æ¶SPAgentã€‚ç ”ç©¶è€…è§‚å¯Ÿåˆ°ä»£ç†çš„æ—©æœŸæ­¥éª¤é€šå¸¸æ¶‰åŠç®€å•çš„è¯æ®æœé›†ï¼Œé€šè¿‡æ¨æµ‹(speculation)æ‰‹æ®µå¯ä»¥æ‰“ç ´ä¸²è¡Œæ‰§è¡Œçš„ç“¶é¢ˆã€‚SPAgentåœ¨ç®—æ³•å±‚é¢å¼•å…¥äº†ä¸¤é˜¶æ®µè‡ªé€‚åº”æ¨æµ‹æœºåˆ¶(two-phase adaptive speculation mechanism)ï¼Œèƒ½å¤Ÿåœ¨å®‰å…¨çš„æƒ…å†µä¸‹é€‰æ‹©æ€§åœ°çœç•¥éªŒè¯è¿‡ç¨‹ã€‚åœ¨ç³»ç»Ÿå±‚é¢ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äºŒçº§è°ƒåº¦å™¨(two-level scheduler)æ ¹æ®å¼•æ“è´Ÿè½½è°ƒèŠ‚æ¨æµ‹è¯·æ±‚ï¼Œç¡®ä¿æ¨æµ‹å§‹ç»ˆäº§ç”Ÿæ­£é¢æ”¶ç›Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAgentåœ¨çœŸå®ç³»ç»Ÿä¸­å®ç°äº†é«˜è¾¾1.65å€çš„ç«¯åˆ°ç«¯æé€Ÿï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡äº†ä»»åŠ¡å‡†ç¡®ç‡ï¼Œä¸ºå¤šæ­¥æœç´¢ä»£ç†çš„å®é™…éƒ¨ç½²æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20048v1",
      "published_date": "2025-11-25 08:15:17 UTC",
      "updated_date": "2025-11-25 08:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:47.035846+00:00"
    },
    {
      "arxiv_id": "2511.20041v1",
      "title": "MFM-point: Multi-scale Flow Matching for Point Cloud Generation",
      "title_zh": "MFM-pointï¼šé¢å‘ç‚¹äº‘ç”Ÿæˆçš„å¤šå°ºåº¦æµåŒ¹é…",
      "authors": [
        "Petr Molodyk",
        "Jaemoo Choi",
        "David W. Romero",
        "Ming-Yu Liu",
        "Yongxin Chen"
      ],
      "abstract": "In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MFM-Pointï¼Œä¸€ç§ç”¨äº Point Cloud Generation çš„å¤šå°ºåº¦ Flow Matching æ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºç‚¹çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸å¯æ‰©å±•æ€§ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ä¿ç•™å…¶ç»“æ„ç®€å•ä¸é«˜æ•ˆçš„ä¼˜åŠ¿ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä»ç²—åˆ°ç»†ï¼ˆcoarse-to-fineï¼‰çš„ç”ŸæˆèŒƒå¼ï¼Œåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆ–æ¨ç†å¼€é”€çš„å‰æä¸‹æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†åº”å¯¹æ— åºç‚¹äº‘åœ¨å¤šå°ºåº¦æ¡†æ¶ä¸­ä¿æŒå‡ ä½•ç»“æ„åŠè·¨åˆ†è¾¨ç‡åˆ†å¸ƒè½¬æ¢çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å¼•å…¥äº†ç»“æ„åŒ–çš„ä¸‹é‡‡æ ·ï¼ˆdownsamplingï¼‰å’Œä¸Šé‡‡æ ·ï¼ˆupsamplingï¼‰ç­–ç•¥ï¼Œä»¥ç¡®ä¿å‡ ä½•ç‰¹å¾çš„å®Œæ•´æ€§ä¸åˆ†è¾¨ç‡é—´çš„ç²¾ç¡®å¯¹é½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMFM-Point åœ¨ç‚¹åŸºæ–¹æ³•ï¼ˆpoint-based methodsï¼‰ä¸­å–å¾—äº†é¡¶å°–æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºæŒ‘æˆ˜å…ˆè¿›è¡¨å¾åŸºæ–¹æ³•ï¼ˆrepresentation-based methodsï¼‰çš„å®åŠ›ã€‚å°¤å…¶åœ¨å¤šç±»åˆ«ï¼ˆmulti-categoryï¼‰å’Œé«˜åˆ†è¾¨ç‡ï¼ˆhigh-resolutionï¼‰ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä¸º 3D ç”Ÿæˆå»ºæ¨¡æä¾›äº†é«˜æ•ˆçš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20041v1",
      "published_date": "2025-11-25 08:10:56 UTC",
      "updated_date": "2025-11-25 08:10:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:38.535184+00:00"
    },
    {
      "arxiv_id": "2511.20721v1",
      "title": "Foundry: Distilling 3D Foundation Models for the Edge",
      "title_zh": "Foundryï¼šé¢å‘è¾¹ç¼˜ç«¯çš„ 3D åŸºç¡€æ¨¡å‹è’¸é¦",
      "authors": [
        "Guillaume Letellier",
        "Siddharth Srivastava",
        "FrÃ©dÃ©ric Jurie",
        "Gaurav Sharma"
      ],
      "abstract": "Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åœ¨æœºå™¨äººåŠAR/VRç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šå› è®¡ç®—æˆæœ¬è¿‡é«˜è€Œéš¾ä»¥éƒ¨ç½²çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºç¡€æ¨¡å‹è’¸é¦ï¼ˆFoundation Model Distillation, FMDï¼‰è¿™ä¸€æ–°èŒƒå¼ã€‚ä½œä¸ºFMDåœ¨3Dç‚¹äº‘ï¼ˆ3D point cloudsï¼‰é¢†åŸŸçš„é¦–æ¬¡å®ç°ï¼ŒFoundryé€šè¿‡è®­ç»ƒå­¦ç”Ÿæ¨¡å‹å­¦ä¹ ä¸€ç»„å‹ç¼©çš„è¶…çº§ä»¤ç‰Œï¼ˆSuperTokensï¼‰ï¼Œæ—¨åœ¨é‡å»ºæ•™å¸ˆæ¨¡å‹çš„ä»¤ç‰Œçº§è¡¨å¾å¹¶æ•æ‰å…¶æ½œç©ºé—´çš„ç´§å‡‘åŸºï¼Œä»è€Œåœ¨å‹ç¼©æ¨¡å‹ä½“ç§¯çš„åŒæ—¶ä¿ç•™é€šç”¨è¡¨å¾èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒFoundryç”Ÿæˆçš„è’¸é¦æ¨¡å‹åœ¨åˆ†ç±»ã€éƒ¨ä»¶åˆ†å‰²åŠå°‘æ ·æœ¬ï¼ˆfew-shotï¼‰ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„è¿ç§»æ€§ã€‚åœ¨å¤§å¹…é™ä½ä»¤ç‰Œæ•°é‡å’Œæµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æ¥è¿‘å®Œæ•´åŸºç¡€æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºåœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šéƒ¨ç½²é«˜æ•ˆä¸”é€šç”¨çš„3DåŸºç¡€æ¨¡å‹æä¾›äº†åˆ‡å®å¯è¡Œçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20721v1",
      "published_date": "2025-11-25 07:53:56 UTC",
      "updated_date": "2025-11-25 07:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:18.933408+00:00"
    },
    {
      "arxiv_id": "2511.20022v1",
      "title": "WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving",
      "title_zh": "WaymoQAï¼šç”¨äºè‡ªåŠ¨é©¾é©¶å®‰å…¨å…³é”®å‹æ¨ç†çš„å¤šè§†å›¾è§†è§‰é—®ç­”æ•°æ®é›†",
      "authors": [
        "Seungjun Yu",
        "Seonho Lee",
        "Namho Kim",
        "Jaeyo Shin",
        "Junsung Park",
        "Wonjeong Ryu",
        "Raehyuk Jung",
        "Hyunjung Shim"
      ],
      "abstract": "Recent advancements in multimodal large language models (MLLMs) have shown strong understanding of driving scenes, drawing interest in their application to autonomous driving. However, high-level reasoning in safety-critical scenarios, where avoiding one traffic risk can create another, remains a major challenge. Such reasoning is often infeasible with only a single front view and requires a comprehensive view of the environment, which we achieve through multi-view inputs. We define Safety-Critical Reasoning as a new task that leverages multi-view inputs to address this challenge. Then, we distill Safety-Critical Reasoning into two stages: first resolve the immediate risk, then mitigate the decision-induced downstream risks. To support this, we introduce WaymoQA, a dataset of 35,000 human-annotated question-answer pairs covering complex, high-risk driving scenarios. The dataset includes multiple-choice and open-ended formats across both image and video modalities. Experiments reveal that existing MLLMs underperform in safety-critical scenarios compared to normal scenes, but fine-tuning with WaymoQA significantly improves their reasoning ability, highlighting the effectiveness of our dataset in developing safer and more reasoning-capable driving agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨è‡ªåŠ¨é©¾é©¶ä¸­éš¾ä»¥å¤„ç†å®‰å…¨å…³é”® (safety-critical) åœºæ™¯çš„é—®é¢˜ï¼Œå®šä¹‰äº†å®‰å…¨å…³é”®æ¨ç† (Safety-Critical Reasoning) ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡å¼ºè°ƒåˆ©ç”¨å¤šè§†è§’ (multi-view) è¾“å…¥ï¼Œå¹¶å°†æ¨ç†è¿‡ç¨‹åˆ†ä¸ºè§£å†³å³æ—¶é£é™©å’Œç¼“è§£å†³ç­–å¼•å‘çš„ä¸‹æ¸¸é£é™©ä¸¤ä¸ªé˜¶æ®µã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº† WaymoQA æ•°æ®é›†ï¼ŒåŒ…å« 3.5 ä¸‡ä¸ªæ¶µç›–å¤æ‚ã€é«˜é£é™©é©¾é©¶åœºæ™¯çš„äººå·¥æ ‡æ³¨é—®ç­”å¯¹ï¼Œæ”¯æŒå›¾åƒä¸è§†é¢‘æ¨¡æ€ä¸‹çš„å¤šç§é—®ç­”æ ¼å¼ã€‚å®éªŒå‘ç°ï¼Œç°æœ‰ MLLMs åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„è¡¨ç°æ™®éé€Šè‰²äºæ™®é€šåœºæ™¯ï¼Œä½†åœ¨ä½¿ç”¨ WaymoQA è¿›è¡Œå¾®è°ƒ (fine-tuning) åï¼Œå…¶æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨å¼€å‘æ›´å®‰å…¨ã€å…·å¤‡æ›´å¼ºæ¨ç†èƒ½åŠ›çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20022v1",
      "published_date": "2025-11-25 07:47:27 UTC",
      "updated_date": "2025-11-25 07:47:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:22.328415+00:00"
    },
    {
      "arxiv_id": "2511.20018v1",
      "title": "Energy Costs and Neural Complexity Evolution in Changing Environments",
      "title_zh": "å¤šå˜ç¯å¢ƒä¸‹çš„èƒ½é‡æˆæœ¬ä¸ç¥ç»å¤æ‚åº¦æ¼”åŒ–",
      "authors": [
        "Sian Heesom-Green",
        "Jonathan Shock",
        "Geoff Nitschke"
      ],
      "abstract": "The Cognitive Buffer Hypothesis (CBH) posits that larger brains evolved to enhance survival in changing conditions. However, larger brains also carry higher energy demands, imposing additional metabolic burdens. Alongside brain size, brain organization plays a key role in cognitive ability and, with suitable architectures, may help mitigate energy challenges. This study evolves Artificial Neural Networks (ANNs) used by Reinforcement Learning (RL) agents to investigate how environmental variability and energy costs influence the evolution of neural complexity, defined in terms of ANN size and structure. Results indicate that under energy constraints, increasing seasonality led to smaller ANNs. This challenges CBH and supports the Expensive Brain Hypothesis (EBH), as highly seasonal environments reduced net energy intake and thereby constrained brain size. ANN structural complexity primarily emerged as a byproduct of size, where energy costs promoted the evolution of more efficient networks. These results highlight the role of energy constraints in shaping neural complexity, offering in silico support for biological theory and energy-efficient robotic design.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“æ¼”åŒ–äººå·¥ç¥ç»ç½‘ç»œ(ANNs)ï¼Œæ¢è®¨äº†ç¯å¢ƒå˜å¼‚æ€§å’Œèƒ½é‡æˆæœ¬å¦‚ä½•å½±å“ç¥ç»å¤æ‚åº¦ï¼ˆåŒ…æ‹¬ç½‘ç»œè§„æ¨¡ä¸ç»“æ„ï¼‰çš„æ¼”åŒ–è¿‡ç¨‹ã€‚ç ”ç©¶é‡ç‚¹è€ƒå¯Ÿäº†è®¤çŸ¥ç¼“å†²å‡è¯´(Cognitive Buffer Hypothesis)ä¸æ˜‚è´µè„‘å‡è¯´(Expensive Brain Hypothesis)åœ¨ä¸åŒä»£è°¢è´Ÿæ‹…ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨èƒ½é‡çº¦æŸæ¡ä»¶ä¸‹ï¼Œç¯å¢ƒå­£èŠ‚æ€§çš„å¢åŠ åè€Œå¯¼è‡´äº†æ›´å°è§„æ¨¡çš„äººå·¥ç¥ç»ç½‘ç»œ(ANNs)ï¼Œè¿™ä¸€å‘ç°æŒ‘æˆ˜äº†è®¤çŸ¥ç¼“å†²å‡è¯´å¹¶æœ‰åŠ›æ”¯æŒäº†æ˜‚è´µè„‘å‡è¯´ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé«˜åº¦å­£èŠ‚æ€§çš„ç¯å¢ƒä¼šå‡å°‘å‡€èƒ½é‡æ‘„å…¥ï¼Œä»è€Œé™åˆ¶è„‘éƒ¨è§„æ¨¡çš„æ‰©å¼ ï¼Œè€Œç»“æ„å¤æ‚åº¦åˆ™ä¸»è¦ä½œä¸ºè§„æ¨¡æ¼”åŒ–çš„å‰¯äº§å“å‡ºç°ã€‚æ­¤å¤–ï¼Œèƒ½é‡æˆæœ¬è¿›ä¸€æ­¥æ¨åŠ¨äº†æ›´é«˜æ•ˆç½‘ç»œç»“æ„çš„æ¼”åŒ–ã€‚è¯¥æˆæœé€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿ(in silico)ä¸ºç”Ÿç‰©å­¦ç†è®ºæä¾›äº†è¯æ®ï¼Œå¹¶ä¸ºå¼€å‘èŠ‚èƒ½å‹æœºå™¨äººè®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Presented at ALIFE 2025, proceedings forthcoming (MIT Press)",
      "pdf_url": "https://arxiv.org/pdf/2511.20018v1",
      "published_date": "2025-11-25 07:38:50 UTC",
      "updated_date": "2025-11-25 07:38:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:28.534148+00:00"
    },
    {
      "arxiv_id": "2512.07844v1",
      "title": "Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning",
      "title_zh": "ç©ºé—´å¯¹é½è‡³å…³é‡è¦ï¼šé•¿å°¾å­¦ä¹ ä¸­è¯±å¯¼ç¥ç»å´©æºƒçš„å…³é”®ä¸€ç¯",
      "authors": [
        "Jinping Wang",
        "Zhiqiang Gao",
        "Zhiwu Xie"
      ],
      "abstract": "Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿å°¾å­¦ä¹  (Long-Tailed Learning) ä¸­ç”±äºæ ·æœ¬ä¸å¹³è¡¡å¯¼è‡´ç¥ç»åç¼© (Neural Collapse, NC) ç°è±¡éš¾ä»¥å‡ºç°ï¼Œä»è€Œé™åˆ¶æ¨¡å‹æ³›åŒ–æ€§èƒ½çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰æ–¹æ³•è™½ç„¶è¯•å›¾æ¢å¤å•å½¢ç­‰è§’ç´§æ¡†æ¶ (simplex ETF) çš„å‡ ä½•ç»“æ„ï¼Œä½†å¾€å¾€å¿½ç•¥äº†ç‰¹å¾ç©ºé—´ä¸åˆ†ç±»å™¨æƒé‡ç©ºé—´ä¹‹é—´æ˜¾è‘—çš„ç©ºé—´å¤±é… (misalignment) è¿™ä¸€æ ¸å¿ƒç“¶é¢ˆã€‚ç ”ç©¶é€šè¿‡æœ€ä¼˜è¯¯å·®æŒ‡æ•°åˆ†æ (optimal error exponent analysis) ä»ç†è®ºä¸Šé‡åŒ–äº†è¿™ç§ç©ºé—´å¤±é…å¯¹æ¨¡å‹æ€§èƒ½çš„æŸå®³ã€‚åŸºäºæ­¤ç†è®ºè§è§£ï¼Œè®ºæ–‡æå‡ºäº†ä¸‰ç§å³æ’å³ç”¨çš„æ˜¾å¼å¯¹é½ç­–ç•¥ï¼Œæ— éœ€æ”¹å˜ç½‘ç»œæ¶æ„å³å¯é›†æˆåˆ°ç°æœ‰çš„é•¿å°¾å­¦ä¹ æ–¹æ³•ä¸­ã€‚åœ¨ CIFAR-10-LTã€CIFAR-100-LT å’Œ ImageNet-LT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æå‡äº†åŸºå‡†æ¨¡å‹çš„è¡¨ç°å¹¶å–å¾—äº†å½“å‰æœ€å…ˆè¿› (state-of-the-art) çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07844v1",
      "published_date": "2025-11-25 07:38:40 UTC",
      "updated_date": "2025-11-25 07:38:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:41.633263+00:00"
    },
    {
      "arxiv_id": "2511.20011v1",
      "title": "Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments",
      "title_zh": "é¢å‘åŸå¸‚ç¯å¢ƒè¡Œäººæ¨ªç©¿æ„å›¾é¢„æµ‹çš„å¤šä¸Šä¸‹æ–‡èåˆ Transformer",
      "authors": [
        "Yuanzhe Li",
        "Hang Zhong",
        "Steffen MÃ¼ller"
      ],
      "abstract": "Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤šä¸Šä¸‹æ–‡èåˆ Transformer (Multi-context fusion Transformer, MFT)ï¼Œæ—¨åœ¨è§£å†³åŸå¸‚ç¯å¢ƒä¸‹è‡ªåŠ¨é©¾é©¶è½¦è¾†å¯¹è¡Œäººè¿‡è¡—æ„å›¾é¢„æµ‹çš„éš¾é¢˜ã€‚è¯¥æ¨¡å‹æ•´åˆäº†è¡Œäººè¡Œä¸ºã€ç¯å¢ƒã€è¡Œäººå®šä½ä»¥åŠè½¦è¾†è¿åŠ¨å››ä¸ªç»´åº¦çš„æ•°å€¼ä¸Šä¸‹æ–‡å±æ€§ï¼Œé€šè¿‡æ¸è¿›å¼èåˆç­–ç•¥å®ç°é«˜ç²¾åº¦çš„æ„å›¾è¯†åˆ«ã€‚MFT é¦–å…ˆåˆ©ç”¨ç›¸äº’å†…éƒ¨ä¸Šä¸‹æ–‡æ³¨æ„åŠ› (mutual intra-context attention) æå–ç‰¹å®šè¡¨å¾ï¼Œéšåé€šè¿‡ç›¸äº’è·¨ä¸Šä¸‹æ–‡æ³¨æ„åŠ› (mutual cross-context attention) ç»“åˆå…¨å±€ CLS token è¿›è¡Œå¤šç»´åº¦æ•´åˆã€‚è¿›ä¸€æ­¥åœ°ï¼Œè¯¥æ¡†æ¶å¼•å…¥å¼•å¯¼å¼æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–ä¿¡æ¯ä¼ æ’­ï¼Œä»è€Œå®ç°æ›´æ·±å±‚ã€æ›´é«˜æ•ˆçš„ç‰¹å¾èåˆã€‚å®éªŒè¯æ˜ï¼ŒMFT åœ¨ JAADbehã€JAADall å’Œ PIE æ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº† 73%ã€93% å’Œ 90% çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20011v1",
      "published_date": "2025-11-25 07:24:49 UTC",
      "updated_date": "2025-11-25 07:24:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:45.029331+00:00"
    },
    {
      "arxiv_id": "2511.20008v1",
      "title": "Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network",
      "title_zh": "åŸºäºå¤šæ¨¡æ€èåˆç½‘ç»œçš„è¡Œäººè¿‡è¡—æ„å›¾é¢„æµ‹",
      "authors": [
        "Yuanzhe Li",
        "Steffen MÃ¼ller"
      ],
      "abstract": "Pedestrian crossing intention prediction is essential for the deployment of autonomous vehicles (AVs) in urban environments. Ideal prediction provides AVs with critical environmental cues, thereby reducing the risk of pedestrian-related collisions. However, the prediction task is challenging due to the diverse nature of pedestrian behavior and its dependence on multiple contextual factors. This paper proposes a multimodal fusion network that leverages seven modality features from both visual and motion branches, aiming to effectively extract and integrate complementary cues across different modalities. Specifically, motion and visual features are extracted from the raw inputs using multiple Transformer-based extraction modules. Depth-guided attention module leverages depth information to guide attention towards salient regions in another modality through comprehensive spatial feature interactions. To account for the varying importance of different modalities and frames, modality attention and temporal attention are designed to selectively emphasize informative modalities and effectively capture temporal dependencies. Extensive experiments on the JAAD dataset validate the effectiveness of the proposed network, achieving superior performance compared to the baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶(AVs)åœ¨åŸå¸‚ç¯å¢ƒä¸­çš„è¡Œäººè¿‡è¡—æ„å›¾é¢„æµ‹æŒ‘æˆ˜ï¼Œæ—¨åœ¨é€šè¿‡æä¾›å…³é”®ç¯å¢ƒçº¿ç´¢æ¥é™ä½è¡Œäººç¢°æ’é£é™©ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èåˆç½‘ç»œ(Multimodal Fusion Network)ï¼Œåˆ©ç”¨æ¥è‡ªè§†è§‰å’Œè¿åŠ¨åˆ†æ”¯çš„ä¸ƒç§æ¨¡æ€ç‰¹å¾ï¼Œæœ‰æ•ˆæå–å¹¶æ•´åˆè·¨æ¨¡æ€çš„äº’è¡¥çº¿ç´¢ã€‚é€šè¿‡ä½¿ç”¨åŸºäºTransformerçš„æå–æ¨¡å—ä»¥åŠæ·±åº¦å¼•å¯¼æ³¨æ„åŠ›(Depth-guided attention)æ¨¡å—ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿå®ç°å…¨é¢çš„ç©ºé—´ç‰¹å¾äº¤äº’å¹¶å¼•å¯¼æ¨¡å‹å…³æ³¨æ˜¾è‘—åŒºåŸŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ¨¡æ€æ³¨æ„åŠ›(Modality attention)å’Œæ—¶é—´æ³¨æ„åŠ›(Temporal attention)ï¼Œä»¥é€‰æ‹©æ€§åœ°å¤„ç†å…³é”®æ¨¡æ€å¹¶ç²¾å‡†æ•æ‰æ—¶é—´ä¾èµ–æ€§ã€‚åœ¨JAADæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†è¡Œäººæ„å›¾é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20008v1",
      "published_date": "2025-11-25 07:18:12 UTC",
      "updated_date": "2025-11-25 07:18:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:43:40.530700+00:00"
    },
    {
      "arxiv_id": "2511.20006v1",
      "title": "BERT-APC: A Reference-free Framework for Automatic Pitch Correction via Musical Context Inference",
      "title_zh": "BERT-APCï¼šåŸºäºéŸ³ä¹è¯­å¢ƒæ¨ç†çš„æ— å‚è€ƒè‡ªåŠ¨éŸ³é«˜æ ¡æ­£æ¡†æ¶",
      "authors": [
        "Sungjae Kim",
        "Kihyun Na",
        "Jinyoung Choi",
        "Injung Kim"
      ],
      "abstract": "Automatic Pitch Correction (APC) enhances vocal recordings by aligning pitch deviations with the intended musical notes. However, existing APC systems either rely on reference pitches, which limits their practical applicability, or employ simple pitch estimation algorithms that often fail to preserve expressiveness and naturalness. We propose BERT-APC, a novel reference-free APC framework that corrects pitch errors while maintaining the natural expressiveness of vocal performances. In BERT-APC, a novel stationary pitch predictor first estimates the perceived pitch of each note from the detuned singing voice. A context-aware note pitch predictor estimates the intended pitch sequence by leveraging a music language model repurposed to incorporate musical context. Finally, a note-level correction algorithm fixes pitch errors while preserving intentional pitch deviations for emotional expression. In addition, we introduce a learnable data augmentation strategy that improves the robustness of the music language model by simulating realistic detuning patterns. Compared to two recent singing voice transcription models, BERT-APC demonstrated superior performance in note pitch prediction, outperforming the second-best model, ROSVOT, by 10.49%p on highly detuned samples in terms of the raw pitch accuracy. In the MOS test, BERT-APC achieved the highest score of $4.32 \\pm 0.15$, which is significantly higher than those of the widely-used commercial APC tools, AutoTune ($3.22 \\pm 0.18$) and Melodyne ($3.08 \\pm 0.18$), while maintaining a comparable ability to preserve expressive nuances. To the best of our knowledge, this is the first APC model that leverages a music language model to achieve reference-free pitch correction with symbolic musical context. The corrected audio samples of BERT-APC are available online.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BERT-APCï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ— éœ€å‚è€ƒ(Reference-free)çš„è‡ªåŠ¨éŸ³é«˜ä¿®æ­£(Automatic Pitch Correction, APC)æ¡†æ¶ï¼Œæ—¨åœ¨çº æ­£éŸ³é«˜é”™è¯¯çš„åŒæ—¶ä¿ç•™æ­Œå£°çš„è‡ªç„¶è¡¨ç°åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¹³ç¨³éŸ³é«˜é¢„æµ‹å™¨(stationary pitch predictor)ä¼°è®¡æ„ŸçŸ¥éŸ³é«˜ï¼Œå¹¶åˆ©ç”¨é‡æ–°è®¾è®¡çš„éŸ³ä¹è¯­è¨€æ¨¡å‹(music language model)ç»“åˆéŸ³ä¹è¯­å¢ƒæ¨ç†é¢„æœŸçš„éŸ³é«˜åºåˆ—ï¼Œæœ€åé€šè¿‡éŸ³ç¬¦çº§ç®—æ³•å®ç°ç²¾ç¡®ä¿®æ­£ã€‚ä¸ºäº†æå‡æ¨¡å‹çš„é²æ£’æ€§ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®çš„åè°ƒæ¨¡å¼æ¥ä¼˜åŒ–è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBERT-APCåœ¨å¤„ç†é«˜åº¦åè°ƒæ ·æœ¬æ—¶çš„åŸå§‹éŸ³é«˜å‡†ç¡®ç‡(Raw Pitch Accuracy)æ¯”ROSVOTæ¨¡å‹é«˜å‡º10.49%ï¼Œä¸”åœ¨ä¸»è§‚æ„è§è¯„åˆ†(MOS)æµ‹è¯•ä¸­è·å¾—äº†4.32çš„é«˜åˆ†ï¼Œæ˜¾è‘—ä¼˜äºAutoTuneå’ŒMelodyneç­‰ä¸»æµå•†ä¸šå·¥å…·ã€‚ä½œä¸ºé¦–ä¸ªåœ¨ç¬¦å·åŒ–éŸ³ä¹è¯­å¢ƒä¸‹åˆ©ç”¨éŸ³ä¹è¯­è¨€æ¨¡å‹å®ç°æ— éœ€å‚è€ƒéŸ³é«˜ä¿®æ­£çš„ç³»ç»Ÿï¼Œè¯¥ç ”ç©¶ä¸ºå¯ä¿æŒæƒ…æ„Ÿè¡¨è¾¾çš„è‡ªåŠ¨åŒ–ä¿®éŸ³æŠ€æœ¯æä¾›äº†é‡è¦çªç ´ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "12 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.20006v1",
      "published_date": "2025-11-25 07:16:49 UTC",
      "updated_date": "2025-11-25 07:16:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:45.232196+00:00"
    },
    {
      "arxiv_id": "2511.20004v2",
      "title": "Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting",
      "title_zh": "Sundial åŸºç¡€æ¨¡å‹åœ¨å¶é¢ç§¯æŒ‡æ•°é¢„æµ‹ä¸­çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›",
      "authors": [
        "Peining Zhang",
        "Hongchen Qin",
        "Haochen Zhang",
        "Ziqi Guo",
        "Guiling Wang",
        "Jinbo Bi"
      ],
      "abstract": "This work investigates the zero-shot forecasting capability of time series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. We show that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ Sundial åœ¨å†œä¸šç›‘æµ‹ä¸­é¢„æµ‹å¶é¢ç§¯æŒ‡æ•° (Leaf Area Index, LAI) çš„é›¶æ ·æœ¬ (Zero-Shot) è¿ç§»èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ HiQ æ•°æ®é›†ï¼Œå°† Sundial æ¨¡å‹ä¸ç»Ÿè®¡åŸºå‡†æ–¹æ³•ä»¥åŠå®Œå…¨ç›‘ç£å­¦ä¹ çš„ LSTM æ¨¡å‹åœ¨å¤šç§è¯„ä¼°åè®®ä¸‹è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒã€‚å®éªŒå‘ç°ï¼Œåœ¨æä¾›è¶³å¤Ÿé•¿çš„è¾“å…¥ä¸Šä¸‹æ–‡çª—å£ï¼ˆç‰¹åˆ«æ˜¯è¦†ç›–è¶…è¿‡ä¸€ä¸¤ä¸ªå®Œæ•´å­£èŠ‚æ€§å‘¨æœŸï¼‰çš„æƒ…å†µä¸‹ï¼ŒSundial åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„é¢„æµ‹è¡¨ç°èƒ½å¤Ÿä¼˜äºç»è¿‡å……åˆ†è®­ç»ƒçš„ LSTMã€‚è¿™è¡¨æ˜é€šç”¨åŸºç¡€æ¨¡å‹åœ¨æ— éœ€ä»»ä½•ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå³å¯åœ¨é¥æ„Ÿæ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­è¶…è¶Šä¸“ä¸šçš„ç›‘ç£æ¨¡å‹ã€‚è¯¥ç ”ç©¶ç»“æœå‡¸æ˜¾äº†é¢„è®­ç»ƒæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹åœ¨å†œä¸šå’Œç¯å¢ƒåº”ç”¨ä¸­ä½œä¸ºé«˜æ•ˆã€å³æ’å³ç”¨é¢„æµ‹å™¨çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 5 figures, AAAI 2026 AgriAI workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.20004v2",
      "published_date": "2025-11-25 07:14:50 UTC",
      "updated_date": "2026-01-15 21:01:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:05.537978+00:00"
    },
    {
      "arxiv_id": "2511.20002v1",
      "title": "On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation",
      "title_zh": "è®ºé€šè¿‡å•ä¸€æ‰°åŠ¨åŠ«æŒ MLLMs å†³ç­–é“¾çš„å¯è¡Œæ€§",
      "authors": [
        "Changyue Li",
        "Jiaying Li",
        "Youliang Yuan",
        "Jiaming He",
        "Zhicong Huang",
        "Pinjia He"
      ],
      "abstract": "Conventional adversarial attacks focus on manipulating a single decision of neural networks. However, real-world models often operate in a sequence of decisions, where an isolated mistake can be easily corrected, but cascading errors can lead to severe risks.\n  This paper reveals a novel threat: a single perturbation can hijack the whole decision chain. We demonstrate the feasibility of manipulating a model's outputs toward multiple, predefined outcomes, such as simultaneously misclassifying \"non-motorized lane\" signs as \"motorized lane\" and \"pedestrian\" as \"plastic bag\".\n  To expose this threat, we introduce Semantic-Aware Universal Perturbations (SAUPs), which induce varied outcomes based on the semantics of the inputs. We overcome optimization challenges by developing an effective algorithm, which searches for perturbations in normalized space with a semantic separation strategy. To evaluate the practical threat of SAUPs, we present RIST, a new real-world image dataset with fine-grained semantic annotations. Extensive experiments on three multimodal large language models demonstrate their vulnerability, achieving a 70% attack success rate when controlling five distinct targets using just an adversarial frame.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŠ«æŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å†³ç­–é“¾çš„å¯è¡Œæ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å¯¹æŠ—æ”»å‡»é€šå¸¸ä»…é’ˆå¯¹å•ä¸€å†³ç­–ï¼Œè€Œå¿½ç•¥äº†ç°å®æ¨¡å‹ä¸­è¿ç»­å†³ç­–çš„å®‰å…¨æ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºè¯­ä¹‰æ„ŸçŸ¥é€šç”¨æ‰°åŠ¨ï¼ˆSAUPsï¼‰çš„æ–°å‹å¨èƒï¼Œè¯æ˜äº†é€šè¿‡å•ä¸€æ‰°åŠ¨å³å¯æ ¹æ®è¾“å…¥è¯­ä¹‰å°†æ¨¡å‹è¾“å‡ºå¼•å‘å¤šä¸ªé¢„å®šä¹‰çš„é”™è¯¯ç›®æ ‡ã€‚ä¸ºäº†å®ç°è¿™ä¸€æ”»å‡»ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§åœ¨å½’ä¸€åŒ–ç©ºé—´ä¸­æœç´¢æ‰°åŠ¨å¹¶ç»“åˆè¯­ä¹‰åˆ†ç¦»ç­–ç•¥çš„ä¼˜åŒ–ç®—æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«ç»†ç²’åº¦è¯­ä¹‰æ ‡æ³¨çš„çœŸå®ä¸–ç•Œå›¾åƒæ•°æ®é›†RISTã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åŒæ—¶æ§åˆ¶äº”ä¸ªä¸åŒç›®æ ‡çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•å¯¹ä¸‰ç§ä¸»æµMLLMsè¾¾åˆ°äº†70%çš„æ”»å‡»æˆåŠŸç‡ï¼Œæ­ç¤ºäº†å•å¸§æ‰°åŠ¨å¯¹å¤æ‚å†³ç­–é“¾çš„ä¸¥å³»æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20002v1",
      "published_date": "2025-11-25 07:13:13 UTC",
      "updated_date": "2025-11-25 07:13:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:42.635535+00:00"
    },
    {
      "arxiv_id": "2511.19999v1",
      "title": "Popularity Bias Alignment Estimates",
      "title_zh": "æµè¡Œåº¦åå·®å¯¹é½ä¼°è®¡",
      "authors": [
        "Anton Lyubinin"
      ],
      "abstract": "We are extending Popularity Bias Memorization theorem from arXiv:archive/2404.12008 in several directions. We extend it to arbitrary degree distributions and also prove both upper and lower estimates for the alignment with top-k singular hyperspace.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨å¤šä¸ªç»´åº¦ä¸Šæ‰©å±•äº† arXiv:2404.12008 ä¸­æå‡ºçš„ Popularity Bias Memorization å®šç†ã€‚é€šè¿‡å°†è¯¥å®šç†æ¨å¹¿è‡³ä»»æ„çš„ degree distributionsï¼Œç ”ç©¶è€…å¢å¼ºäº†å…¶åœ¨å¤„ç†å¤æ‚ç½‘ç»œæ•°æ®æ—¶çš„ç†è®ºé€‚ç”¨æ€§ã€‚è®ºæ–‡è¿›ä¸€æ­¥é’ˆå¯¹ä¸ top-k singular hyperspace çš„ alignment æƒ…å†µï¼Œä¸¥æ ¼è¯æ˜å¹¶ç»™å‡ºäº† upper and lower estimatesã€‚è¿™äº›å‘ç°ä¸ºæ·±å…¥ç†è§£æ¨¡å‹åœ¨å¤„ç† Popularity Bias æ—¶çš„å¯¹é½æœºåˆ¶æä¾›äº†æ›´ä¸ºç²¾ç»†çš„æ•°å­¦ç•Œå®šå’Œç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19999v1",
      "published_date": "2025-11-25 07:07:36 UTC",
      "updated_date": "2025-11-25 07:07:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:11.134869+00:00"
    },
    {
      "arxiv_id": "2511.19997v1",
      "title": "Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test",
      "title_zh": "Transformer ä¸­çš„æ–¹å‘æ€§ä¼˜åŒ–ä¸å¯¹ç§°æ€§ï¼šä¸€é¡¹åˆæˆå‹åŠ›æµ‹è¯•",
      "authors": [
        "Mihir Sahasrabudhe"
      ],
      "abstract": "Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a \"reversal curse,\" and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Transformeræ¶æ„åœ¨ç†è®ºä¸Šå…·å¤‡åè½¬ä¸å˜æ€§ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æ™®éå­˜åœ¨â€œåè½¬è¯…å’’â€(reversal curse)çš„é—®é¢˜ã€‚ä¸ºäº†æŸ¥æ˜è¿™ä¸€å®šå‘å¤±è´¥æ˜¯æºäºè¯­è¨€ç»Ÿè®¡ç‰¹æ€§è¿˜æ˜¯æ¶æ„æœ¬èº«ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªå®Œå…¨åˆæˆä¸”å—ç†µæ§åˆ¶çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡åˆ†æ”¯å› å­Kå¯è°ƒçš„éšæœºå­—ç¬¦ä¸²æ˜ å°„æ¥è¯„ä¼°å®šå‘å­¦ä¹ ã€‚å®éªŒå‘ç°ï¼Œå³ä½¿æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒçš„GPT-2æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—ä¸”å¯å¤ç°çš„å®šå‘ä¼˜åŒ–å·®å¼‚(directional optimization gap)ï¼Œä¸”è¯¥å·®å¼‚è¿œå¤§äºåœ¨ç›¸åŒæ•°æ®ä¸Šè®­ç»ƒçš„MLPæ¨¡å‹ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒåˆå§‹åŒ–æ— æ³•æ¶ˆé™¤è¿™ä¸€å·®è·ï¼Œè€ŒLoRAåœ¨å¤„ç†é«˜ç†µé€†å‘æ˜ å°„æ—¶ä¼šé‡åˆ°æ˜æ˜¾çš„å®¹é‡ç“¶é¢ˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå› æœTransformerè®­ç»ƒä¸­å­˜åœ¨å†…åœ¨çš„å®šå‘æ‘©æ“¦(directional friction)ï¼Œå³ä½¿åœ¨æ’é™¤è¯­è¨€å…ˆéªŒã€è¯é¢‘å’Œè¯­æ–™åº“æ—¶é—´ä¸å¯¹ç§°æ€§åä¾ç„¶å­˜åœ¨ã€‚è¯¥å·¥ä½œä¸ºå‰–æåºåˆ—æ¨¡å‹çš„å®šå‘åå·®æä¾›äº†å—æ§å·¥å…·ï¼Œå¹¶æ­ç¤ºäº†ä¸ºä½•é€†å‘ä»»åŠ¡å¯¹Transformerè€Œè¨€åœ¨æ ¹æœ¬ä¸Šæ›´å…·æŒ‘æˆ˜æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 4 figures. Code available at https://github.com/mihirs-0/synass",
      "pdf_url": "https://arxiv.org/pdf/2511.19997v1",
      "published_date": "2025-11-25 07:03:20 UTC",
      "updated_date": "2025-11-25 07:03:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:16.240200+00:00"
    },
    {
      "arxiv_id": "2511.20720v1",
      "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving",
      "title_zh": "DeeADï¼šé¢å‘é«˜æ•ˆè‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œåŠ¨æ€æå‰é€€å‡ºæ¡†æ¶",
      "authors": [
        "Haibo HU",
        "Lianming Huang",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "abstract": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeeADï¼Œä¸€ç§é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(Vision-Language Action, VLA)æ¨ç†å»¶è¿Ÿé—®é¢˜è€Œè®¾è®¡çš„æ— é¡»è®­ç»ƒçš„åŠ¨ä½œå¼•å¯¼æ—©æœŸé€€å‡ºæ¡†æ¶(early-exit framework)ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç½®ä¿¡åº¦åˆ†æ•°çš„æ–¹æ³•ä¸åŒï¼ŒDeeADé€šè¿‡è¯„ä¼°ä¸­é—´è½¨è¿¹çš„ç‰©ç†å¯è¡Œæ€§æ¥å†³å®šé€€å‡ºæ—¶æœºï¼Œå³å½“é¢„æµ‹è½¨è¿¹ä¸è½»é‡çº§è§„åˆ’å…ˆéªŒ(planning priors)çš„åå·®åœ¨å¯å®¹å¿èŒƒå›´å†…æ—¶ä¾¿åœæ­¢æ¨ç†ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå¼•å…¥äº†å¤šè·³æ§åˆ¶å™¨(multi-hop controller)ä»¥æ ¹æ®åˆ†æ•°å˜åŒ–ç‡è‡ªé€‚åº”åœ°è·³è¿‡å†—ä½™å±‚ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚DeeADèƒ½å¤Ÿç›´æ¥é›†æˆåˆ°ORIONç­‰ç°æœ‰VLAæ¨¡å‹ä¸­ä¸”æ— éœ€é‡æ–°è®­ç»ƒï¼Œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯è§„åˆ’è´¨é‡ä¸å®‰å…¨æ€§çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾28%çš„Transformerå±‚ç¨€ç–åº¦å’Œ29%çš„æ¨ç†å»¶è¿Ÿå‡å°‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20720v1",
      "published_date": "2025-11-25 07:00:26 UTC",
      "updated_date": "2025-11-25 07:00:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:49.836873+00:00"
    },
    {
      "arxiv_id": "2511.19986v1",
      "title": "On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices",
      "title_zh": "é¢å‘è¾¹ç¼˜è®¾å¤‡é«˜æ•ˆå¤§æ¨¡å‹éƒ¨ç½²çš„æŒ‰éœ€å¤šä»»åŠ¡ç¨€ç–åŒ–",
      "authors": [
        "Lianming Huang",
        "Haibo Hu",
        "Qiao Li",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "abstract": "Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æŒ‰éœ€å¤šä»»åŠ¡ç¨€ç–åŒ–(On-Demand Multi-Task Sparsity)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡(Edge Devices)ä¸Šå› é¢‘ç¹åˆ‡æ¢ä»»åŠ¡è€Œäº§ç”Ÿçš„å·¨å¤§I/Oå¼€é”€é—®é¢˜ã€‚ä¸åŒäºä¼ ç»Ÿçš„å•ä½“ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ¡†æ¶å°†æƒé‡åˆ†è§£ä¸ºå¯é‡ç”¨çš„å—ç²’åº¦(Block-granular)å•å…ƒï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡é—´å¯¹é½ç¨€ç–ç»“æ„ä»¥æœ€å¤§é™åº¦æé«˜å‚æ•°é‡ç”¨ã€‚é€šè¿‡ä»…åŠ¨æ€åŠ è½½ä¸‹ä¸€ä»»åŠ¡æ‰€éœ€çš„å°‘é‡å·®å¼‚åŒ–æ•°æ®å—ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†ä¼ ç»Ÿæ–¹æ¡ˆå›ºæœ‰çš„å†·å¯åŠ¨(Cold-start)å»¶è¿Ÿã€‚åœ¨çœŸå®è‡ªåŠ¨é©¾é©¶å¹³å°(Autonomous Driving Platform)ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†å“è¶Šçš„åˆ‡æ¢æ•ˆç‡ï¼Œä¸ç°æœ‰ç¨€ç–åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œä»»åŠ¡åˆ‡æ¢é€Ÿåº¦å¹³å‡æé«˜äº†6.6å€ä»¥ä¸Šã€‚è¯¥ç ”ç©¶ä¸ºå—é™èµ„æºç¯å¢ƒä¸‹çš„é«˜æ•ˆå¤§æ¨¡å‹éƒ¨ç½²å’Œå¤šä»»åŠ¡å¤„ç†æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19986v1",
      "published_date": "2025-11-25 06:54:04 UTC",
      "updated_date": "2025-11-25 06:54:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:37.334964+00:00"
    },
    {
      "arxiv_id": "2511.19982v2",
      "title": "EmoFeedback$^2$: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback",
      "title_zh": "EmoFeedback$^2$ï¼šé€šè¿‡åŸºäº LVLM çš„å¥–åŠ±ä¸æ–‡æœ¬åé¦ˆå¼ºåŒ–è¿ç»­æƒ…æ„Ÿå›¾åƒç”Ÿæˆ",
      "authors": [
        "Jingyang Jia",
        "Kai Shu",
        "Gang Yang",
        "Long Xing",
        "Xun Chen",
        "Aiping Liu"
      ],
      "abstract": "Continuous emotional image generation (C-EICG) is emerging rapidly due to its ability to produce images aligned with both user descriptions and continuous emotional values. However, existing approaches lack emotional feedback from generated images, limiting the control of emotional continuity. Additionally, their simple alignment between emotions and naively generated texts fails to adaptively adjust emotional prompts according to image content, leading to insufficient emotional fidelity. To address these concerns, we propose a novel generation-understanding-feedback reinforcement paradigm (EmoFeedback$^2$) for C-EICG, which exploits the reasoning capability of the fine-tuned large vision-language model (LVLM) to provide reward and textual feedback for generating high-quality images with continuous emotions. Specifically, we introduce an emotion-aware reward feedback strategy, where the LVLM evaluates the emotional values of generated images and computes the reward against target emotions, guiding the reinforcement fine-tuning of the generative model and enhancing the emotional continuity of images. Furthermore, we design a self-promotion textual feedback framework, in which the LVLM iteratively analyzes the emotional content of generated images and adaptively produces refinement suggestions for the next-round prompt, improving the emotional fidelity with fine-grained content. Extensive experimental results demonstrate that our approach effectively generates high-quality images with the desired emotions, outperforming existing state-of-the-art methods in our custom dataset. The code and dataset will be released soon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EmoFeedback$^2$ï¼Œä¸€ç§æ—¨åœ¨è§£å†³è¿ç»­æƒ…æ„Ÿå›¾åƒç”Ÿæˆ(C-EICG)ä¸­æƒ…æ„Ÿåé¦ˆç¼ºå¤±å’Œæƒ…æ„Ÿå¿ å®åº¦ä¸è¶³é—®é¢˜çš„ç”Ÿæˆ-ç†è§£-åé¦ˆå¼ºåŒ–èŒƒå¼ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¾®è°ƒåçš„å¤§è§†è§‰è¯­è¨€æ¨¡å‹(LVLM)çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºç”Ÿæˆé«˜è´¨é‡ä¸”å…·å¤‡æƒ…æ„Ÿè¿ç»­æ€§çš„å›¾åƒæä¾›å¥–åŠ±å’Œæ–‡æœ¬åé¦ˆã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶å¼•å…¥äº†æƒ…æ„Ÿæ„ŸçŸ¥å¥–åŠ±åé¦ˆç­–ç•¥ï¼Œé€šè¿‡LVLMè¯„ä¼°ç”Ÿæˆå›¾åƒçš„æƒ…æ„Ÿå€¼å¹¶è®¡ç®—å¥–åŠ±ï¼Œä»è€ŒæŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒ(reinforcement fine-tuning)å¹¶å¢å¼ºæƒ…æ„Ÿè¿ç»­æ€§ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜è®¾è®¡äº†è‡ªæˆ‘æå‡æ–‡æœ¬åé¦ˆæœºåˆ¶ï¼Œä½¿LVLMèƒ½å¤Ÿè¿­ä»£åˆ†æå›¾åƒæƒ…æ„Ÿå†…å®¹å¹¶è‡ªé€‚åº”ç”Ÿæˆæç¤ºè¯æ”¹è¿›å»ºè®®ï¼Œæå‡äº†ç»†ç²’åº¦å†…å®¹çš„æƒ…æ„Ÿå¿ å®åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEmoFeedback$^2$åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆç¬¦åˆç›®æ ‡æƒ…æ„Ÿçš„é«˜è´¨é‡å›¾åƒï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19982v2",
      "published_date": "2025-11-25 06:51:15 UTC",
      "updated_date": "2025-11-26 03:44:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:41.526850+00:00"
    },
    {
      "arxiv_id": "2511.20719v1",
      "title": "Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models",
      "title_zh": "æ™ºèƒ½ä½“åŒ– AI Wi-Fi ä¸­åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¥å…¥ç‚¹åä½œå­¦ä¹ ",
      "authors": [
        "Yifan Fan",
        "Le Liang",
        "Peng Liu",
        "Xiao Li",
        "Ziyang Guo",
        "Qiao Lan",
        "Shi Jin",
        "Wen Tong"
      ],
      "abstract": "Multi-access point coordination (MAPC) is a key technology for enhancing throughput in next-generation Wi-Fi within dense overlapping basic service sets. However, existing MAPC protocols rely on static, protocol-defined rules, which limits their ability to adapt to dynamic network conditions such as varying interference levels and topologies. To address this limitation, we propose a novel Agentic AI Wi-Fi framework where each access point, modeled as an autonomous large language model agent, collaboratively reasons about the network state and negotiates adaptive coordination strategies in real time. This dynamic collaboration is achieved through a cognitive workflow that enables the agents to engage in natural language dialogue, leveraging integrated memory, reflection, and tool use to ground their decisions in past experience and environmental feedback. Comprehensive simulation results demonstrate that our agentic framework successfully learns to adapt to diverse and dynamic network environments, significantly outperforming the state-of-the-art spatial reuse baseline and validating its potential as a robust and intelligent solution for future wireless networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Agentic AI Wi-Fi çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯†é›†é‡å åŸºç¡€æœåŠ¡é›†ï¼ˆdense overlapping basic service setsï¼‰ä¸­å¤šæ¥å…¥ç‚¹åä½œï¼ˆMulti-access point coordination, MAPCï¼‰åè®®å› ä¾èµ–é™æ€è§„åˆ™è€Œéš¾ä»¥é€‚åº”åŠ¨æ€ç½‘ç»œç¯å¢ƒçš„é—®é¢˜ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ¯ä¸ªæ¥å…¥ç‚¹è¢«å»ºæ¨¡ä¸ºå…·æœ‰è‡ªä¸»æ€§çš„ Large Language Model (LLM) æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿé€šè¿‡åä½œæ¨ç†ç½‘ç»œçŠ¶æ€å¹¶å®æ—¶åå•†è‡ªé€‚åº”åä½œç­–ç•¥ã€‚è¿™ç§åŠ¨æ€åä½œé€šè¿‡ä¸€ç§è®¤çŸ¥å·¥ä½œæµå®ç°ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåˆ©ç”¨é›†æˆè®°å¿†ã€åæ€å’Œå·¥å…·è°ƒç”¨è¿›è¡Œè‡ªç„¶è¯­è¨€å¯¹è¯ï¼Œä»è€Œå°†å…¶å†³ç­–æ¤æ ¹äºè¿‡å¾€ç»éªŒå’Œç¯å¢ƒåé¦ˆä¸­ã€‚ä»¿çœŸå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ Agentic æ¡†æ¶èƒ½å¤ŸæˆåŠŸé€‚åº”å¤šæ ·ä¸”åŠ¨æ€çš„ç½‘ç»œç¯å¢ƒï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„ Spatial Reuse åŸºçº¿æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥æ— çº¿ç½‘ç»œæä¾›äº†ä¸€ç§ç¨³å¥ä¸”æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆï¼ŒéªŒè¯äº† Agentic AI åœ¨æå‡ç½‘ç»œååé‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.IT",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20719v1",
      "published_date": "2025-11-25 06:29:25 UTC",
      "updated_date": "2025-11-25 06:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:45:09.537762+00:00"
    },
    {
      "arxiv_id": "2511.19969v1",
      "title": "M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation",
      "title_zh": "M$^3$Pruneï¼šé¢å‘é«˜æ•ˆå¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆçš„åˆ†å±‚é€šä¿¡å›¾å‰ªæ",
      "authors": [
        "Weizi Shao",
        "Taolin Zhang",
        "Zijie Zhou",
        "Chen Chen",
        "Chengyu Wang",
        "Xiaofeng He"
      ],
      "abstract": "Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é¢ä¸´çš„é«˜æ˜‚ Token å¼€é”€å’Œè®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº† M$^3$Prune åˆ†å±‚é€šä¿¡å›¾å‰ªææ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¶ˆé™¤ä¸åŒæ¨¡æ€é—´çš„å†—ä½™è¿æ¥ï¼Œæ—¨åœ¨ä»»åŠ¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚å…·ä½“è€Œè¨€ï¼ŒM$^3$Prune é¦–å…ˆå¯¹æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€æ‰§è¡Œæ¨¡æ€å†…å›¾ç¨€ç–åŒ–ï¼ˆintra-modal graph sparsificationï¼‰ä»¥è¯†åˆ«å…³é”®é€šä¿¡è¾¹ï¼Œéšååˆ©ç”¨è¿™äº›è¾¹æ„å»ºåŠ¨æ€é€šä¿¡æ‹“æ‰‘è¿›è¡Œè·¨æ¨¡æ€å›¾ç¨€ç–åŒ–ï¼ˆinter-modal graph sparsificationï¼‰ã€‚é€šè¿‡é€æ­¥å‰ªé™¤å†—ä½™è·¯å¾„ï¼Œè¯¥æ¡†æ¶æ„å»ºå‡ºäº†ä¸€ä¸ªæ›´é«˜æ•ˆçš„åˆ†å±‚é€šä¿¡æ‹“æ‰‘ç»“æ„ã€‚åœ¨é€šç”¨å’Œé¢†åŸŸç‰¹å®šçš„ mRAG åŸºå‡†æµ‹è¯•ä¸­ï¼ŒM$^3$Prune çš„è¡¨ç°ä¸€è‡´ä¼˜äºå•æ™ºèƒ½ä½“å’Œæˆç†Ÿçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¹¶åœ¨æ˜¾è‘—é™ä½ Token æ¶ˆè€—çš„åŒæ—¶æå‡äº†å¤„ç†æ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¸ºå¤§è§„æ¨¡éƒ¨ç½²é«˜æ•ˆçš„å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19969v1",
      "published_date": "2025-11-25 06:29:13 UTC",
      "updated_date": "2025-11-25 06:29:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:44:54.427521+00:00"
    },
    {
      "arxiv_id": "2511.19963v1",
      "title": "MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing",
      "title_zh": "MambaEyeï¼šåŸºäºå› æœåºåˆ—å¤„ç†çš„å°ºå¯¸æ— å…³è§†è§‰ç¼–ç å™¨",
      "authors": [
        "Changho Choi",
        "Minho Kim",
        "Jinkyu Kim"
      ],
      "abstract": "Despite decades of progress, a truly input-size agnostic visual encoder-a fundamental characteristic of human vision-has remained elusive. We address this limitation by proposing \\textbf{MambaEye}, a novel, causal sequential encoder that leverages the low complexity and causal-process based pure Mamba2 backbone. Unlike previous Mamba-based vision encoders that often employ bidirectional processing, our strictly unidirectional approach preserves the inherent causality of State Space Models, enabling the model to generate a prediction at any point in its input sequence. A core innovation is our use of relative move embedding, which encodes the spatial shift between consecutive patches, providing a strong inductive bias for translation invariance and making the model inherently adaptable to arbitrary image resolutions and scanning patterns. To achieve this, we introduce a novel diffusion-inspired loss function that provides dense, step-wise supervision, training the model to build confidence as it gathers more visual evidence. We demonstrate that MambaEye exhibits robust performance across a wide range of image resolutions, especially at higher resolutions such as $1536^2$ on the ImageNet-1K classification task. This feat is achieved while maintaining linear time and memory complexity relative to the number of patches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MambaEyeï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å› æœåºåˆ—ç¼–ç å™¨ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿè§†è§‰ç¼–ç å™¨åœ¨è¾“å…¥å°ºå¯¸é™åˆ¶ä¸Šçš„ç“¶é¢ˆã€‚è¯¥æ¡†æ¶åŸºäºä½å¤æ‚åº¦çš„çº¯Mamba2éª¨å¹²ç½‘ç»œï¼Œé‡‡ç”¨äº†ä¸¥æ ¼çš„å•å‘(unidirectional)å¤„ç†æ–¹å¼ï¼Œä»¥ä¿ç•™çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Models)å›ºæœ‰çš„å› æœæ€§ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ç›¸å¯¹ç§»åŠ¨åµŒå…¥(relative move embedding)ï¼Œé€šè¿‡ç¼–ç è¿ç»­å›¾åƒå—(patches)ä¹‹é—´çš„ç©ºé—´åç§»ï¼Œä¸ºå¹³ç§»ä¸å˜æ€§æä¾›äº†å¼ºå½’çº³åç½®ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½è‡ªé€‚åº”ä»»æ„åˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§å—æ‰©æ•£æ¨¡å‹å¯å‘çš„æŸå¤±å‡½æ•°(diffusion-inspired loss function)è¿›è¡Œå¯†é›†çš„é€æ­¥ç›‘ç£ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç§¯ç´¯è§†è§‰è¯æ®æ—¶çš„é¢„æµ‹ç½®ä¿¡åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒMambaEyeåœ¨ImageNet-1Kåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨$1536^2$ç­‰é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹ï¼Œä»èƒ½ä¿æŒçº¿æ€§çš„æ—¶é—´ä¸å†…å­˜å¤æ‚åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code will be released in github",
      "pdf_url": "https://arxiv.org/pdf/2511.19963v1",
      "published_date": "2025-11-25 06:18:18 UTC",
      "updated_date": "2025-11-25 06:18:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:46:42.060313+00:00"
    },
    {
      "arxiv_id": "2511.20718v1",
      "title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training",
      "title_zh": "ST-PPOï¼šé¢å‘å¤šè½®æ™ºèƒ½ä½“è®­ç»ƒçš„ç¨³å®šåŒ–ç¦»ç­–è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Chenliang Li",
        "Adel Elmahdy",
        "Alex Boyd",
        "Zhongruo Wang",
        "Alfredo Garcia",
        "Parminder Bhatia",
        "Taha Kass-Hout",
        "Cao Xiao",
        "Mingyi Hong"
      ],
      "abstract": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®å¯¹è¯å’Œæ¨ç†ä»»åŠ¡ä¸­ä½¿ç”¨ Proximal Policy Optimization (PPO) è¿›è¡Œ token çº§åˆ«è®­ç»ƒæ—¶å®¹æ˜“å‡ºç°æ€§èƒ½ä¸ç¨³å®šå’Œå´©æºƒ(collapse)çš„é—®é¢˜ï¼Œè¯†åˆ«å‡º token çº§åˆ«çš„é‡è¦æ€§é‡‡æ ·(importance sampling)ä¸å¤šè½®ç¯å¢ƒç²’åº¦ä¸åŒ¹é…ä»¥åŠç¦»ç­–ç•¥(off-policy)æ ·æœ¬å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡(advantage estimates)ä¸å‡†è¿™ä¸¤ä¸ªä¸»è¦ä¸ç¨³å®šæ€§æ¥æºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† ST-PPO æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è½¬å‘çº§åˆ«çš„é‡è¦æ€§é‡‡æ ·(turn-level importance sampling)ä½¿ä¼˜åŒ–è¿‡ç¨‹ä¸å¤šè½®æ¨ç†çš„è‡ªç„¶ç»“æ„å¯¹é½ï¼Œå¹¶ç»“åˆè£å‰ªåå·®ä¿®æ­£(clipping-bias correction)æŠ€æœ¯é€šè¿‡é™ä½ä¸å¯é æ ·æœ¬æƒé‡æ¥å½’ä¸€åŒ–æ¢¯åº¦ã€‚åœ¨é€šç”¨é—®ç­”(QA)ã€å¤šè·³é—®ç­”åŠåŒ»å­¦é—®ç­”ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒST-PPO åŠå…¶å˜ä½“èƒ½æŒç»­æœ‰æ•ˆé˜²æ­¢è®­ç»ƒå´©æºƒï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¿æŒè¾ƒä½çš„è£å‰ªç‡ã€‚å®éªŒè¯æ˜ï¼Œç»“åˆè½¬å‘çº§åˆ«é‡‡æ ·ä¸è£å‰ªåå·®ä¿®æ­£ä¸ºç¨³å®šå¤šè½® LLM æ™ºèƒ½ä½“è®­ç»ƒæä¾›äº†ä¸€ç§å®ç”¨ä¸”å…·æœ‰æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20718v1",
      "published_date": "2025-11-25 05:54:02 UTC",
      "updated_date": "2025-11-25 05:54:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:45:37.930041+00:00"
    },
    {
      "arxiv_id": "2511.19943v1",
      "title": "AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload",
      "title_zh": "é¢å‘ HARQ-ACK è´Ÿè½½çš„åŸºäº AI/ML çš„è”åˆä¿¡æºä¿¡é“ç¼–ç ",
      "authors": [
        "Akash Doshi",
        "Pinar Sen",
        "Kirill Ivanov",
        "Wei Yang",
        "June Namgoong",
        "Runxin Wang",
        "Rachel Wang",
        "Taesang Yoo",
        "Jing Jiang",
        "Tingfang Ji"
      ],
      "abstract": "Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel \"free-lunch\" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ 5G é€šä¿¡ä¸­ HARQ-ACK è½½è·ä½åˆ†å¸ƒä¸å‡åŒ€çš„ç‰¹æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ çš„è”åˆä¿¡æºä¿¡é“ç¼–ç  (Joint Source and Channel Coding) æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨ Transformer-based ç¼–ç å™¨ï¼Œå¹¶ç»“åˆä¸€ç§æ–°å‹çš„ \"free-lunch\" è®­ç»ƒç®—æ³•å’Œé€ç å­—åŠŸç‡æ•´å½¢ (per-codeword power shaping) æŠ€æœ¯ï¼Œåœ¨å……åˆ†åˆ©ç”¨ä¿¡æºå…ˆéªŒä¿¡æ¯çš„åŒæ—¶ç¡®ä¿äº†å¯¹åˆ†å¸ƒå¾®å°å˜åŒ–çš„ç¨³å¥æ€§ã€‚åœ¨è§£ç ç«¯ï¼Œç ”ç©¶é€šè¿‡å°† Neyman-Pearson æµ‹è¯•æ‰©å±•åˆ°å¤šä½ç¼–ç ç³»ç»Ÿï¼Œå®ç°äº† NACK ä½ç›¸å¯¹äº ACK ä½çš„ä¸ç­‰é”™è¯¯ä¿æŠ¤ (Unequal Error Protection)ï¼Œä»è€Œæœ‰æ•ˆé™ä½é”™è¯¯ç‡å¹¶é¿å…æ— çº¿é“¾è·¯æ•…éšœã€‚è®ºæ–‡è¿›ä¸€æ­¥å°†è¯¥è®¾è®¡åº”ç”¨äºç¬¦åˆ 5G New Radio (NR) æ ‡å‡†çš„ä¸Šè¡Œé“¾è·¯è¡°è½ä¿¡é“åœºæ™¯ï¼Œå¹¶æå‡ºäº†æœ€ä¼˜æ¥æ”¶æœºåŠå…¶ä½å¤æ‚åº¦ç›¸å¹²è¿‘ä¼¼æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ NR åŸºå‡†ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è¾¾åˆ°ç›®æ ‡é”™è¯¯ç‡æ—¶å¯å°†å¹³å‡å‘å°„åŠŸç‡é™ä½ 3-6 dBï¼Œå¹¶å‡å°‘ 2-3 dB çš„æœ€å¤§å‘å°„åŠŸç‡ï¼Œæ˜¾è‘—æå‡äº†ç½‘ç»œè¦†ç›–èŒƒå›´å¹¶å®ç°äº†å¤§å¹…èŠ‚èƒ½ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "39 pages, 15 figures. Under consideration for publication in Journal of Sel. Areas in Information Theory. This paper was presented in part at the International Symposium on Topics in Coding, August 2025 in the Session for Coding and AI",
      "pdf_url": "https://arxiv.org/pdf/2511.19943v1",
      "published_date": "2025-11-25 05:31:26 UTC",
      "updated_date": "2025-11-25 05:31:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:45:36.631867+00:00"
    },
    {
      "arxiv_id": "2511.19941v1",
      "title": "Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç£å…±æŒ¯æŒ‡çº¹æŠ€æœ¯ç¿»è½¬è§’åºåˆ—ä¼˜åŒ–",
      "authors": [
        "Shenjun Zhong",
        "Zhifeng Chen",
        "Zhaolin Chen"
      ],
      "abstract": "Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç£å…±æŒ¯æŒ‡çº¹æˆåƒ (Magnetic Resonance Fingerprinting, MRF) ä¸­è·å–å‚æ•°åºåˆ—è®¾è®¡è¿™ä¸€å¤æ‚ã€é«˜ç»´çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ä¼˜åŒ–ç¿»è½¬è§’ (Flip Angle) è®¡åˆ’çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–çš„å‚æ•°é€‰æ‹©ï¼Œä½¿ç”Ÿæˆçš„è„‰å†²åºåˆ—èƒ½å¤Ÿæœ€å¤§åŒ–å‚æ•°ç©ºé—´å†…æŒ‡çº¹çš„å¯åŒºåˆ†æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç» RL ä¼˜åŒ–çš„ç¿»è½¬è§’è®¡åˆ’å‘ˆç°å‡ºéå‘¨æœŸæ€§æ¨¡å¼ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæŒ‡çº¹çš„åˆ†ç¦»åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ä¼˜åŒ–åçš„è®¡åˆ’å¯èƒ½å…è®¸å‡å°‘é‡å¤æ—¶é—´ (Repetition Time) çš„æ¬¡æ•°ï¼Œä»è€Œå…·å¤‡åŠ é€Ÿ MRF é‡‡é›†è¿‡ç¨‹çš„æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆå±•ç¤ºäº† RL åœ¨è‡ªåŠ¨åŒ–å’Œä¼˜åŒ–å¤æ‚æ ¸ç£å…±æŒ¯è„‰å†²åºåˆ—è®¾è®¡æ–¹é¢çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 5 figures, submitted to conference",
      "pdf_url": "https://arxiv.org/pdf/2511.19941v1",
      "published_date": "2025-11-25 05:27:30 UTC",
      "updated_date": "2025-11-25 05:27:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:45:32.234613+00:00"
    },
    {
      "arxiv_id": "2511.19933v2",
      "title": "Failure Modes in LLM Systems: A System-Level Taxonomy for Reliable AI Applications",
      "title_zh": "LLM ç³»ç»Ÿå¤±æ•ˆæ¨¡å¼ï¼šé¢å‘å¯é  AI åº”ç”¨çš„ç³»ç»Ÿçº§åˆ†ç±»ä½“ç³»",
      "authors": [
        "Vaishali Vinay"
      ],
      "abstract": "Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å¤±æ•ˆæ¨¡å¼ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹æœ‰æœ¬è´¨åŒºåˆ«ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ä¸ªåŒ…å«15ç§éšæ€§å¤±æ•ˆæ¨¡å¼ (failure modes) çš„ç³»ç»Ÿçº§åˆ†ç±»æ³•ã€‚è¯¥åˆ†ç±»æ³•è¯¦ç»†æ¶µç›–äº†å¤šæ­¥æ¨ç†æ¼‚ç§» (multi-step reasoning drift)ã€æ½œåœ¨ä¸ä¸€è‡´æ€§ (latent inconsistency)ã€ä¸Šä¸‹æ–‡è¾¹ç•Œé€€åŒ– (context-boundary degradation)ã€é”™è¯¯å·¥å…·è°ƒç”¨ (incorrect tool invocation) ä»¥åŠç‰ˆæœ¬æ¼‚ç§» (version drift) ç­‰å…³é”®é—®é¢˜ã€‚é€šè¿‡è¯¥åˆ†ç±»æ³•ï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¡¡é‡ç¨³å®šæ€§ (stability)ã€å¯é‡å¤æ€§ (reproducibility) å’Œå·¥ä½œæµé›†æˆæ–¹é¢çš„å±€é™æ€§ã€‚æ–‡ç« è¿›ä¸€æ­¥åˆ†æäº†éƒ¨ç½²è¿‡ç¨‹ä¸­çš„å¯è§‚æµ‹æ€§ (observability) é™åˆ¶å’Œæ›´æ–°å¼•å‘çš„å›å½’ (update-induced regressions) ç­‰ç”Ÿäº§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æ„å»ºå¯é ã€å¯ç»´æŠ¤ä¸”å…·æœ‰æˆæœ¬æ„è¯† (cost-aware) ç³»ç»Ÿçš„é«˜çº§è®¾è®¡åŸåˆ™ã€‚è¯¥å·¥ä½œå¼ºè°ƒå°† LLM å¯é æ€§è§†ä¸ºä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹é—®é¢˜è€Œéå•çº¯çš„æ¨¡å‹ä¸­å¿ƒé—®é¢˜ï¼Œä¸ºæœªæ¥ AI ç³»ç»Ÿçš„ç¨³å¥æ€§ç ”ç©¶å’Œå¯é éƒ¨ç½²æä¾›äº†é‡è¦çš„åˆ†æåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19933v2",
      "published_date": "2025-11-25 05:19:23 UTC",
      "updated_date": "2025-11-26 06:22:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:45:38.528347+00:00"
    },
    {
      "arxiv_id": "2511.19931v1",
      "title": "LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training",
      "title_zh": "LLM-EDTï¼šåŸºäºåŒé˜¶æ®µè®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹å¢å¼ºè·¨åŸŸåºåˆ—æ¨è",
      "authors": [
        "Ziwei Liu",
        "Qidong Liu",
        "Wanyu Wang",
        "Yejing Wang",
        "Tong Xu",
        "Wei Huang",
        "Chong Chen",
        "Peng Chuan",
        "Xiangyu Zhao"
      ],
      "abstract": "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM-EDTï¼Œä¸€ç§æ—¨åœ¨è§£å†³è·¨åŸŸåºåˆ—æ¨èï¼ˆCross-domain Sequential Recommendation, CDSRï¼‰ä¸­æ•°æ®ä¸å¹³è¡¡å’ŒåŸŸé—´è½¬æ¢éš¾é¢˜çš„å¤§è¯­è¨€æ¨¡å‹å¢å¼ºæ¡†æ¶ã€‚é’ˆå¯¹å•ä¸€é¢†åŸŸä¸»å¯¼å¯¼è‡´çš„ç‰¹å®šç‰¹å¾æ•æ‰å›°éš¾ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¯è¿ç§»é¡¹å¢å¼ºå™¨ï¼ˆtransferable item augmenterï¼‰ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ç”Ÿæˆè·¨åŸŸè¡Œä¸ºå¹¶æœ‰æ•ˆå‡å°‘æ— å…³å™ªå£°ã€‚ä¸ºäº†ç¼“è§£æ··åˆäº¤äº’åºåˆ—ä¸­çš„åå¥½æ•æ‰éšœç¢ï¼ŒLLM-EDTé‡‡ç”¨äº†åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆdual-phase trainingï¼‰ï¼Œé€šè¿‡é¢†åŸŸå…±äº«èƒŒæ™¯å¢å¼ºé¢†åŸŸç‰¹å®šçº¿ç¨‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†é¢†åŸŸæ„ŸçŸ¥å‰–ææ¨¡å—ï¼ˆdomain-aware profiling moduleï¼‰ï¼Œç”¨äºæ€»ç»“å¹¶èšåˆå„é¢†åŸŸåå¥½ä»¥ç”Ÿæˆæ›´å…¨é¢çš„ç”¨æˆ·ç”»åƒã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨æ•æ‰å¤æ‚è·¨åŸŸç”¨æˆ·åå¥½æ–¹é¢çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19931v1",
      "published_date": "2025-11-25 05:18:04 UTC",
      "updated_date": "2025-11-25 05:18:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:45:57.137923+00:00"
    },
    {
      "arxiv_id": "2511.19925v1",
      "title": "Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity",
      "title_zh": "Semantic-KGï¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±æ„å»ºè¯­ä¹‰ç›¸ä¼¼åº¦è¡¡é‡åŸºå‡†",
      "authors": [
        "Qiyao Wei",
        "Edward Morrell",
        "Lea Goetz",
        "Mihaela van der Schaar"
      ],
      "abstract": "Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¡é‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) å“åº”è¯­ä¹‰ç›¸ä¼¼åº¦ (Semantic Similarity) æ—¶ç°æœ‰æ–¹æ³•åé‡å¥æ³•ä¸”äººå·¥åŸºå‡†æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº† Semantic-KG æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨çŸ¥è¯†å›¾è°± (Knowledge Graphs) è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼æˆ–ä¸ç›¸ä¼¼çš„è‡ªç„¶è¯­è¨€é™ˆè¿°å¯¹ï¼Œå¹¶åœ¨é€šç”¨çŸ¥è¯†ã€ç”Ÿç‰©åŒ»å­¦ã€é‡‘èåŠç”Ÿç‰©å­¦å››ä¸ªé¢†åŸŸæ„å»ºäº†åŸºå‡†æ•°æ®é›†ã€‚ç ”ç©¶å°†è¯­ä¹‰å·®å¼‚ç»†åˆ†ä¸ºå››ç§ç±»å‹ï¼Œå¹¶å¯¹ä¼ ç»Ÿ NLP è¯„åˆ†ä¸ LLM-as-a-judge ç­‰æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰å˜ä½“ç±»å‹å’Œæ‰€å±é¢†åŸŸå‡ä¼šæ˜¾è‘—å½±å“è¯„ä¼°æ€§èƒ½ï¼Œç›®å‰å°šæ— ä»»ä½•æ–¹æ³•èƒ½åœ¨æ‰€æœ‰æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚æ­¤é¡¹å·¥ä½œä¸ºåˆ©ç”¨ LLM-as-a-judge æ¢æµ‹æ–‡æœ¬è¯­ä¹‰å†…å®¹æä¾›äº†é‡è¦çš„å®éªŒä¾æ®ï¼Œå¹¶å…¬å¼€äº†ä»£ç å’Œ Semantic-KG æ•°æ®é›†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19925v1",
      "published_date": "2025-11-25 05:07:08 UTC",
      "updated_date": "2025-11-25 05:07:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:46:26.836451+00:00"
    },
    {
      "arxiv_id": "2512.20624v1",
      "title": "Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment",
      "title_zh": "é¢å‘æ— äººæœºè¾…åŠ© 6G ç½‘ç»œéƒ¨ç½²ä¸­æ¢ç´¢ä¸åˆ©ç”¨ä¼˜åŒ–çš„é‡å­å¯å‘å¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Mazyar Taghavi",
        "Javad Vahidi"
      ],
      "abstract": "This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å—é‡å­å¯å‘çš„(Quantum-inspired)å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ— äººæœº(UAV)è¾…åŠ©çš„6Gç½‘ç»œéƒ¨ç½²ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨(Exploration-Exploitation)å¹³è¡¡ã€‚è¯¥æ–¹æ³•ä»¥å˜åˆ†é‡å­çº¿è·¯(VQCs)ä¸ºæ ¸å¿ƒç»“æ„ï¼Œå¹¶åˆ©ç”¨é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•(QAOA)è§£å†³å¤æ‚çš„ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ŒåŒæ—¶æ•´åˆäº†è´å¶æ–¯æ¨ç†(Bayesian inference)å’Œé«˜æ–¯è¿‡ç¨‹(Gaussian processes)ç­‰æ¦‚ç‡å»ºæ¨¡æŠ€æœ¯æ¥æ•æ‰ç¯å¢ƒåŠ¨æ€ã€‚åœ¨ç³»ç»Ÿæ¶æ„ä¸Šï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸­å¿ƒåŒ–è®­ç»ƒä¸å»ä¸­å¿ƒåŒ–æ‰§è¡Œ(CTDE)èŒƒå¼ï¼Œé€šè¿‡å…±äº«å†…å­˜æå‡äº†æ™ºèƒ½ä½“åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ¡ä»¶ä¸‹çš„å±€éƒ¨è§‚æµ‹èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ ·æœ¬æ•ˆç‡ã€æ”¶æ•›é€Ÿåº¦å’Œè¦†ç›–æ€§èƒ½æ–¹é¢å‡ä¼˜äºPPOå’ŒDDPGç­‰ç»å…¸åŸºçº¿ç®—æ³•ã€‚é›·è¾¾å›¾å’Œæ”¶æ•›æ€§åˆ†æè¿›ä¸€æ­¥è¯æ˜ï¼Œå—é‡å­å¯å‘çš„MARLåœ¨å¤„ç†å¤§è§„æ¨¡åä½œä»»åŠ¡æ—¶å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶åœ¨æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ä¼˜åŒ–ä¸­è¡¨ç°å“è¶Šã€‚",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "59 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.20624v1",
      "published_date": "2025-11-25 04:35:43 UTC",
      "updated_date": "2025-11-25 04:35:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:46:07.339658+00:00"
    },
    {
      "arxiv_id": "2511.19902v1",
      "title": "Zero-Knowledge Proof Based Verifiable Inference of Models",
      "title_zh": "åŸºäºé›¶çŸ¥è¯†è¯æ˜çš„æ¨¡å‹å¯éªŒè¯æ¨ç†",
      "authors": [
        "Yunxiao Wang"
      ],
      "abstract": "Recent advances in artificial intelligence (AI), particularly deep learning, have led to widespread adoption across various applications. Yet, a fundamental challenge persists: how can we verify the correctness of AI model inference when model owners cannot (or will not) reveal their parameters? These parameters represent enormous training costs and valuable intellectual property, making transparent verification difficult. In this paper, we introduce a zero-knowledge framework capable of verifying deep learning inference without exposing model internal parameters. Built on recursively composed zero-knowledge proofs and requiring no trusted setup, our framework supports both linear and nonlinear neural network layers, including matrix multiplication, normalization, softmax, and SiLU. Leveraging the Fiat-Shamir heuristic, we obtain a succinct non-interactive argument of knowledge (zkSNARK) with constant-size proofs. To demonstrate the practicality of our approach, we translate the DeepSeek model into a fully SNARK-verifiable version named ZK-DeepSeek and show experimentally that our framework delivers both efficiency and flexibility in real-world AI verification workloads.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ‰€æœ‰è€…å› ä¿æŠ¤çŸ¥è¯†äº§æƒè€Œä¸æ„¿å…¬å¼€å‚æ•°å¯¼è‡´æ¨ç†æ­£ç¡®æ€§éš¾ä»¥éªŒè¯çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé›¶çŸ¥è¯†è¯æ˜ (Zero-Knowledge Proof) çš„å¯éªŒè¯æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ„å»ºäºé€’å½’ç»„åˆçš„é›¶çŸ¥è¯†è¯æ˜ä¹‹ä¸Šï¼Œæ— éœ€å¯ä¿¡è®¾ç½® (Trusted Setup)ï¼Œèƒ½å¤Ÿæ”¯æŒåŒ…æ‹¬çŸ©é˜µä¹˜æ³•ã€å½’ä¸€åŒ–ã€Softmax å’Œ SiLU åœ¨å†…çš„å¤šç§çº¿æ€§å’Œéçº¿æ€§ç¥ç»ç½‘ç»œå±‚ã€‚é€šè¿‡åˆ©ç”¨ Fiat-Shamir å¯å‘å¼ç®—æ³•ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†å…·æœ‰ç®€æ´ä¸”å›ºå®šå¤§å°è¯æ˜çš„éäº¤äº’å¼é›¶çŸ¥è¯†çŸ¥è¯†è®ºè¯ (zkSNARK)ã€‚ä¸ºäº†è¯æ˜å®ç”¨æ€§ï¼Œç ”ç©¶è€…å°† DeepSeek æ¨¡å‹è½¬åŒ–ä¸ºå®Œå…¨å¯è¢« SNARK éªŒè¯çš„ç‰ˆæœ¬ ZK-DeepSeekã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®ä¸–ç•Œçš„ AI éªŒè¯ä»»åŠ¡ä¸­å…¼å…·æ•ˆç‡ä¸çµæ´»æ€§ï¼Œä¸ºå®ç°ä¸æš´éœ²å†…éƒ¨å‚æ•°çš„æ¨¡å‹æ¨ç†éªŒè¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19902v1",
      "published_date": "2025-11-25 04:19:16 UTC",
      "updated_date": "2025-11-25 04:19:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:46:08.738249+00:00"
    },
    {
      "arxiv_id": "2511.19900v2",
      "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
      "title_zh": "Agent0-VLï¼šé¢å‘å·¥å…·é›†æˆè§†è§‰-è¯­è¨€æ¨ç†çš„è‡ªè¿›åŒ–æ™ºèƒ½ä½“æ¢ç´¢",
      "authors": [
        "Jiaqi Liu",
        "Kaiwen Xiong",
        "Peng Xia",
        "Yiyang Zhou",
        "Haonian Ji",
        "Lu Feng",
        "Siwei Han",
        "Mingyu Ding",
        "Huaxiu Yao"
      ],
      "abstract": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agent0-VLï¼Œä¸€ç§èƒ½å¤Ÿå®ç°è‡ªæˆ‘è¿›åŒ–(Self-Evolving)çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä¸­è¿‡åº¦ä¾èµ–äººå·¥æ ‡æ³¨ä»¥åŠè‡ªæˆ‘è¯„ä»·æ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å•ä¸ªå¤§è¯­è¨€è§†è§‰æ¨¡å‹(LVLM)ä¸­ç»Ÿä¸€äº†ä¸¤ä¸ªååŒè§’è‰²ï¼šè´Ÿè´£å¤šè½®å·¥å…·é›†æˆæ¨ç†çš„ Solverï¼Œä»¥åŠé€šè¿‡å·¥å…·é©±åŠ¨è¯„ä¼°ç”Ÿæˆåé¦ˆå’Œç»†ç²’åº¦è‡ªæˆ‘å¥–åŠ±çš„ Verifierã€‚Agent0-VL å°†å·¥å…·é›†æˆæ¨ç†(Tool-Integrated Reasoning)ä¸ä»…åº”ç”¨äºè§£é¢˜ï¼Œè¿˜èå…¥åˆ°è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘ä¿®å¤è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è‡ªæˆ‘è¿›åŒ–æ¨ç†å¾ªç¯(Self-Evolving Reasoning Cycle)å®ç°äº†æ¨ç†ä¸è¯„ä»·åˆ†å¸ƒçš„è‡ªåŠ¨å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„æƒ…å†µä¸‹ï¼ŒAgent0-VL åœ¨å‡ ä½•é—®é¢˜æ±‚è§£å’Œè§†è§‰ç§‘å­¦åˆ†æä»»åŠ¡ä¸­æ¯”åŸºç¡€æ¨¡å‹æ€§èƒ½æå‡äº† 12.5%ã€‚è¿™ä¸ºæ„å»ºèƒ½å¤ŸæŒç»­è‡ªæˆ‘æ”¹è¿›ä¸”å…·å¤‡é«˜å¯é æ€§çš„è§†è§‰è¯­è¨€æ™ºèƒ½ä½“æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19900v2",
      "published_date": "2025-11-25 04:15:14 UTC",
      "updated_date": "2025-11-26 05:14:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:46:11.131897+00:00"
    },
    {
      "arxiv_id": "2511.19895v2",
      "title": "RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation",
      "title_zh": "RPM-MCTSï¼šåŸºäºçŸ¥è¯†æ£€ç´¢è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„ä»£ç ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "Yuanyuan Lin",
        "Xiangyu Ouyang",
        "Teng Zhang",
        "Kaixin Sui"
      ],
      "abstract": "Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RPM-MCTSï¼Œä¸€ç§ç»“åˆ Monte Carlo Tree Search (MCTS) çš„ä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°å°† Knowledge-Retrieval ä½œä¸º Process Reward Model (PRM) æ¥è¯„ä¼°ä¸­é—´ç®—æ³•æ­¥éª¤ã€‚é€šè¿‡å¼•å…¥çŸ¥è¯†åº“æ£€ç´¢ï¼ŒRPM-MCTS é¿å…äº†ä¼ ç»Ÿè¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚åœ¨æœç´¢æ‰©å±•é˜¶æ®µï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç›¸ä¼¼åº¦è¿‡æ»¤ï¼ˆsimilarity filteringï¼‰å‰”é™¤å†—ä½™èŠ‚ç‚¹ä»¥ç¡®ä¿æ¨ç†è·¯å¾„çš„å¤šæ ·æ€§ï¼Œå¹¶ç»“åˆæ²™ç®±æ‰§è¡Œåé¦ˆï¼ˆsandbox execution feedbackï¼‰å®æ—¶å®šä½å¹¶ä¿®æ­£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é”™è¯¯æ­¥éª¤ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRPM-MCTS åœ¨å››ä¸ªä¸»æµä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†å½“å‰çš„ State-of-the-art æ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘äº†çº¦ 15% çš„ Token æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥æ¡†æ¶ç”Ÿæˆçš„é«˜è´¨é‡æ•°æ®å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼ˆfull fine-tuningï¼‰èƒ½è¿›ä¸€æ­¥æ˜¾è‘—å¢å¼ºå…¶ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.19895v2",
      "published_date": "2025-11-25 04:06:02 UTC",
      "updated_date": "2025-12-17 13:09:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:07.853271+00:00"
    },
    {
      "arxiv_id": "2512.11811v2",
      "title": "Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention",
      "title_zh": "åŸºäº LLM å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶çš„ä¼—æºæ´ªæ¶å½±åƒåœ°ç†å®šä½å¢å¼º",
      "authors": [
        "Fengyi Xu",
        "Jun Ma",
        "Waishan Qiu",
        "Cui Guo",
        "Jack C. P. Cheng"
      ],
      "abstract": "Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¼—åŒ…æ´ªæ°´å›¾åƒç¼ºä¹å¯é åœ°ç†å…ƒæ•°æ®çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰è§†è§‰åœ°ç‚¹è¯†åˆ«(Visual Place Recognition, VPR)æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»å›¾åƒæ—¶å› è§†è§‰ç•¸å˜å’Œé¢†åŸŸåç§»å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†VPR-AttLLMæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ¨¡å‹é‡è®­ç»ƒä¸”ä¸æ¨¡å‹æ— å…³çš„é€šç”¨æ¶æ„ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLMs)å¼•å¯¼çš„æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºå›¾åƒæè¿°ç¬¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMsçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›åœ¨åŸå¸‚è¯­å¢ƒä¸­è¯†åˆ«å…·æœ‰ä½ç½®ä¿¡æ¯çš„å…³é”®åŒºåŸŸå¹¶æŠ‘åˆ¶è§†è§‰å™ªå£°ï¼Œå®ç°äº†ç±»äººç©ºé—´æ¨ç†ä¸ç°ä»£VPRæ¶æ„çš„æœ‰æ•ˆæ¡¥æ¥ã€‚å®éªŒåœ¨åŒ…å«çœŸå®æ´ªæ°´å›¾åƒã€åˆæˆæ´ªæ°´åœºæ™¯ä»¥åŠå…¨æ–°çš„HK-URBANæ•°æ®é›†ç­‰æ‰©å±•åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°†VPR-AttLLMä¸CosPlaceã€EigenPlaceså’ŒSALADç­‰å…ˆè¿›æ¨¡å‹é›†æˆåï¼Œåœ¨æå…·æŒ‘æˆ˜æ€§çš„çœŸå®æ´ªæ°´å›¾åƒä¸Šå¬å›ç‡ç›¸å¯¹å¢ç›Šæœ€é«˜å¯è¾¾8%ã€‚æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ç§LLMå¼•å¯¼çš„å¤šæ¨¡æ€èåˆèŒƒå¼ï¼Œå…¶å³æ’å³ç”¨çš„è®¾è®¡å’Œå¼ºå¤§çš„è·¨æºé²æ£’æ€§ï¼Œä¸ºå±æœºå½±åƒçš„å¿«é€Ÿåœ°ç†å®šä½å’Œå¯æ‰©å±•çš„åŸå¸‚ç›‘æ§æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Updated author list to include additional contributor. Revised title and improved methodology section based on collaborative feedback",
      "pdf_url": "https://arxiv.org/pdf/2512.11811v2",
      "published_date": "2025-11-25 04:04:40 UTC",
      "updated_date": "2025-12-16 07:39:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:02.535004+00:00"
    },
    {
      "arxiv_id": "2511.19887v1",
      "title": "Distilling Cross-Modal Knowledge via Feature Disentanglement",
      "title_zh": "åŸºäºç‰¹å¾è§£è€¦çš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦",
      "authors": [
        "Junhong Liu",
        "Yuan Zhang",
        "Tao Huang",
        "Wenchao Xu",
        "Renyu Yang"
      ],
      "abstract": "Knowledge distillation (KD) has proven highly effective for compressing large models and enhancing the performance of smaller ones. However, its effectiveness diminishes in cross-modal scenarios, such as vision-to-language distillation, where inconsistencies in representation across modalities lead to difficult knowledge transfer. To address this challenge, we propose frequency-decoupled cross-modal knowledge distillation, a method designed to decouple and balance knowledge transfer across modalities by leveraging frequency-domain features. We observed that low-frequency features exhibit high consistency across different modalities, whereas high-frequency features demonstrate extremely low cross-modal similarity. Accordingly, we apply distinct losses to these features: enforcing strong alignment in the low-frequency domain and introducing relaxed alignment for high-frequency features. We also propose a scale consistency loss to address distributional shifts between modalities, and employ a shared classifier to unify feature spaces. Extensive experiments across multiple benchmark datasets show our method substantially outperforms traditional KD and state-of-the-art cross-modal KD approaches. Code is available at https://github.com/Johumliu/FD-CMKD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰ä¸­ç”±äºæ¨¡æ€é—´è¡¨ç¤ºä¸ä¸€è‡´å¯¼è‡´è¿ç§»å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢‘ç‡è§£è€¦è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆfrequency-decoupled cross-modal knowledge distillationï¼‰æ–¹æ³•ã€‚ç ”ç©¶è€…å‘ç°ä½é¢‘ç‰¹å¾ï¼ˆlow-frequency featuresï¼‰åœ¨ä¸åŒæ¨¡æ€é—´å…·æœ‰è¾ƒé«˜çš„ä¸€è‡´æ€§ï¼Œè€Œé«˜é¢‘ç‰¹å¾ï¼ˆhigh-frequency featuresï¼‰çš„è·¨æ¨¡æ€ç›¸ä¼¼åº¦æä½ã€‚æ®æ­¤ï¼Œè¯¥æ–¹æ³•å¯¹ä½é¢‘åŸŸå®æ–½å¼ºå¯¹é½ï¼Œå¹¶å¯¹é«˜é¢‘ç‰¹å¾å¼•å…¥æ¾å¼›å¯¹é½ç­–ç•¥ï¼Œä»¥å¹³è¡¡ä¸åŒé¢‘æ®µçš„çŸ¥è¯†è¿ç§»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†å°ºåº¦ä¸€è‡´æ€§æŸå¤±ï¼ˆscale consistency lossï¼‰æ¥è§£å†³æ¨¡æ€é—´çš„åˆ†å¸ƒåç§»ï¼Œå¹¶é‡‡ç”¨å…±äº«åˆ†ç±»å™¨ç»Ÿä¸€ç‰¹å¾ç©ºé—´ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„KDå’Œç°æœ‰çš„è·¨æ¨¡æ€KDæœ€å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†è§†è§‰åˆ°è¯­è¨€ç­‰åœºæ™¯ä¸‹çš„æ¨¡å‹å‹ç¼©ä¸æ€§èƒ½å¢å¼ºæ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.19887v1",
      "published_date": "2025-11-25 03:45:37 UTC",
      "updated_date": "2025-11-25 03:45:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:46:59.931775+00:00"
    },
    {
      "arxiv_id": "2511.19878v1",
      "title": "MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization",
      "title_zh": "MAPSï¼šé€šè¿‡é€æ¨¡å—é‚»è¿‘åº¦è°ƒåº¦ä¿ç•™è§†è§‰-è¯­è¨€è¡¨ç¤ºï¼Œä»¥æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ³›åŒ–æ€§èƒ½",
      "authors": [
        "Chengyue Huang",
        "Mellon M. Zhang",
        "Robert Azarcon",
        "Glen Chou",
        "Zsolt Kira"
      ],
      "abstract": "Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language-Action (VLA) æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å®¹æ˜“ç ´åé¢„è®­ç»ƒ Vision-Language Models (VLMs) è¡¨å¾å¹¶æŸå®³æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº† MAPS (Module-Wise Proximity Scheduling) é²æ£’å¾®è°ƒæ¡†æ¶ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼ŒMAPS æ­ç¤ºäº†æ”¾å®½é‚»è¿‘çº¦æŸçš„ç»éªŒé¡ºåºï¼Œå¹¶é‡‡ç”¨çº¿æ€§è°ƒåº¦ä½¿è§†è§‰ç¼–ç å™¨ (visual encoders) ç´§è´´é¢„è®­ç»ƒå…ˆéªŒï¼ŒåŒæ—¶å…è®¸é¢å‘åŠ¨ä½œçš„è¯­è¨€å±‚ (action-oriented language layers) çµæ´»é€‚é…ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–å‚æ•°æˆ–æ•°æ®ï¼Œå¯æ— ç¼é›†æˆè‡³ MiniVLA æˆ– OpenVLA ç­‰ç°æœ‰æ¨¡å‹ä¸­ã€‚åœ¨ SimplerEnvã€CALVINã€LIBERO ç­‰åŸºå‡†æµ‹è¯•åŠ Franka Emika Panda çœŸå®æœºå™¨äººå¹³å°çš„å®éªŒè¡¨æ˜ï¼ŒMAPS åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤– (OOD) æ€§èƒ½ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå¢å¹…æœ€é«˜è¾¾ 30%ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†é€šè¿‡ç»éªŒå¼•å¯¼çš„æ¨¡å—åŒ–é‚»è¿‘è°ƒåº¦æ˜¯ä¿æŒ VLM åˆ° VLA è¿ç§»è¿‡ç¨‹ä¸­å¹¿æ³›æ³›åŒ–èƒ½åŠ›çš„ç®€å•ä¸”é«˜æ•ˆçš„åŸåˆ™ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19878v1",
      "published_date": "2025-11-25 03:39:37 UTC",
      "updated_date": "2025-11-25 03:39:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:24.528856+00:00"
    },
    {
      "arxiv_id": "2511.19875v1",
      "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection",
      "title_zh": "CodeFuse-CommitEvalï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´ä¸ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ç ”ç©¶",
      "authors": [
        "Qingyu Zhang",
        "Puzhuo Liu",
        "Peng Di",
        "Chenxiong Qian"
      ],
      "abstract": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CODEFUSE-COMMITEVALï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ£€æµ‹ä»£ç æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´ä¸ä¸€è‡´æ€§ (Message-Code Inconsistency, MCI) èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŸºäº ApacheCM æ•°æ®é›†ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„å˜å¼‚ç”Ÿæˆäº†ä¸ƒç§ç±»å‹çš„ä¸ä¸€è‡´æäº¤ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨åŒé‡éªŒè¯ç¡®ä¿æ­£è´Ÿæ ·æœ¬çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶äººå‘˜å¯¹å…­ç§ä¸»æµå¼€æº LLMs åœ¨åŸç”Ÿè®¾ç½®åŠ Few-shot Promptingã€Chain-of-Thought å’Œæ‰©å±•ä¸Šä¸‹æ–‡ç­‰å¢å¼ºç­–ç•¥ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹æ£€æµ‹ä¸ä¸€è‡´æäº¤çš„å¯é æ€§æ™®éé«˜äºä¸€è‡´æäº¤ï¼Œå…¶ä¸­ gpt-oss-20B åœ¨ç»¼åˆè¡¨ç°ä¸Šæœ€ä¼˜ï¼Œä½†å…¶ Token æ¶ˆè€—ä¹Ÿæœ€ä¸ºæ˜¾è‘—ã€‚ç ”ç©¶å‘ç°ï¼ŒFew-shot ç­–ç•¥èƒ½æå‡å‡†ç¡®åº¦å¹¶é™ä½ Token ä½¿ç”¨ï¼Œè€Œ Chain-of-Thought è™½ç„¶å¢å¼ºäº†ç²¾ç¡®ç‡ (Precision) å’Œç‰¹å¼‚åº¦ (Specificity)ï¼Œå´ä»¥ç‰ºç‰²å¬å›ç‡ (Recall) å’Œå¢åŠ æˆæœ¬ä¸ºä»£ä»·ã€‚ç±»å‹åˆ†ææ­ç¤ºæ¨¡å‹åœ¨æ£€æµ‹ç»„ä»¶å’Œæ–‡ä»¶è·¯å¾„ä¸ä¸€è‡´æ—¶è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å¤„ç†æ¶‰åŠæ„å›¾çš„â€œç›®çš„â€ä¸ä¸€è‡´æ—¶å‡†ç¡®ç‡è¾ƒä½ã€‚CODEFUSE-COMMITEVAL ä¸º MCI æ£€æµ‹çš„è¡¡é‡ä¸æ”¹è¿›å¥ å®šäº†ä¸¥è°¨åŸºç¡€ï¼Œå¹¶å¼ºè°ƒäº†æ•æ‰é«˜å±‚è¯­ä¹‰å·®è·å¯¹ä¸°å¯Œä¸Šä¸‹æ–‡çš„éœ€æ±‚ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19875v1",
      "published_date": "2025-11-25 03:33:57 UTC",
      "updated_date": "2025-11-25 03:33:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:09.140317+00:00"
    },
    {
      "arxiv_id": "2511.19874v1",
      "title": "Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains",
      "title_zh": "AI æ™ºèƒ½ä½“ä¾›åº”é“¾ä¸­è¡Œä¸ºåé—¨æ£€æµ‹çš„è·¨ LLM æ³›åŒ–",
      "authors": [
        "Arun Chowdary Sanna"
      ],
      "abstract": "As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI æ™ºèƒ½ä½“ä¾›åº”é“¾ä¸­çš„è¡Œä¸ºåé—¨æ£€æµ‹(behavioral backdoor detection)å¼€å±•äº†é¦–ä¸ªè·¨å¤§è¯­è¨€æ¨¡å‹(cross-LLM)æ³›åŒ–èƒ½åŠ›çš„ç³»ç»Ÿæ€§ç ”ç©¶ã€‚é€šè¿‡å¯¹ GPT-5.1ã€Claude Sonnet 4.5 å’Œ DeepSeek Chat V3.1 ç­‰å…­ç§ç”Ÿäº§çº§æ¨¡å‹è¿›è¡Œçš„ 36 ç»„äº¤å‰å®éªŒå‘ç°ï¼Œå•æ¨¡å‹æ£€æµ‹å™¨åœ¨è·¨æ¶æ„åº”ç”¨æ—¶å‡†ç¡®ç‡ä¼šä» 92.7% éª¤é™è‡³ 49.2%ï¼Œå‡ ä¹ç­‰åŒäºéšæœºçŒœæµ‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œè¿™ä¸€æ³›åŒ–ç¼ºå£ä¸»è¦æºäºä¸åŒæ¨¡å‹åœ¨æ—¶é—´ç‰¹å¾(temporal features)ä¸Šå…·æœ‰ç‹¬ç‰¹çš„è¡Œä¸ºç­¾åï¼Œè€Œç»“æ„åŒ–ç‰¹å¾åœ¨ä¸åŒæ¶æ„é—´ç›¸å¯¹ç¨³å®šã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å°†æ¨¡å‹èº«ä»½ä½œä¸ºé¢å¤–ç‰¹å¾çš„æ¨¡å‹æ„ŸçŸ¥æ£€æµ‹(model-aware detection)æ¡†æ¶ï¼ŒæˆåŠŸåœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å®ç°äº† 90.6% çš„é€šç”¨æ£€æµ‹å‡†ç¡®ç‡ã€‚è¯¥æˆæœé€šè¿‡å‘å¸ƒå¤šæ¨¡å‹è½¨è¿¹æ•°æ®é›†å’Œæ£€æµ‹æ¡†æ¶ï¼Œä¸ºè§£å†³å¤š AI ç³»ç»Ÿç¯å¢ƒä¸‹çš„ä¾›åº”é“¾å®‰å…¨æ¼æ´æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 2 figures, 8 tables. Evaluation across 6 production LLMs with 1,198 traces",
      "pdf_url": "https://arxiv.org/pdf/2511.19874v1",
      "published_date": "2025-11-25 03:33:04 UTC",
      "updated_date": "2025-11-25 03:33:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:14.237071+00:00"
    },
    {
      "arxiv_id": "2511.19872v2",
      "title": "Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡æ‹Ÿè‡ªæˆ‘è¯„ä¼°ï¼šä¸€ç§ AI è‡ªæˆ‘æ•ˆèƒ½æ„Ÿçš„å¿ƒç†æµ‹é‡å­¦æ–¹æ³•",
      "authors": [
        "Daniel I Jackson",
        "Emma L Jensen",
        "Syed-Amad Hussain",
        "Emre Sezgin"
      ],
      "abstract": "Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›ï¼Œé€šè¿‡æ”¹ç¼–åŒ…å«10ä¸ªé¡¹ç›®çš„General Self-Efficacy Scale (GSES)é‡è¡¨ï¼Œå¯¹10ç§æ¨¡å‹åœ¨æ— ä»»åŠ¡ã€è®¡ç®—æ¨ç†ã€ç¤¾ä¼šæ¨ç†å’Œæ‘˜è¦ç”Ÿæˆå››ç§æƒ…å¢ƒä¸‹çš„æ¨¡æ‹Ÿè‡ªæˆ‘æ•ˆèƒ½(Self-Efficacy)è¿›è¡Œäº†å¿ƒç†æµ‹é‡è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨GSESé‡è¡¨ä¸Šçš„ååº”åœ¨é‡å¤æµ‹è¯•å’Œéšæœºé¢˜ç›®é¡ºåºä¸­è¡¨ç°å‡ºæé«˜çš„ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡æƒ…å¢ƒä¸‹çš„è‡ªæˆ‘æ•ˆèƒ½æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”å…¶æ•´ä½“å¾—åˆ†æ™®éä½äºäººç±»å¸¸æ¨¡ã€‚åœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œå°½ç®¡æ‰€æœ‰æ¨¡å‹åœ¨è®¡ç®—å’Œç¤¾ä¼šé—®é¢˜ä¸Šè¡¨ç°å®Œç¾ï¼Œä½†åœ¨æ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¸ä¸€ï¼Œä¸”å…¶è‡ªæˆ‘è¯„ä¼°å¾—åˆ†å¹¶ä¸èƒ½å¯é åœ°åæ˜ å…¶å®é™…èƒ½åŠ›ã€‚å®šæ€§åˆ†æå‘ç°ï¼Œé«˜è‡ªæˆ‘æ•ˆèƒ½å¾—åˆ†é€šå¸¸å¯¹åº”æ›´å…·æ–­è¨€æ€§å’Œæ‹ŸäººåŒ–(Anthropomorphic)çš„æ¨ç†é£æ ¼ï¼Œè€Œä½å¾—åˆ†åˆ™è¡¨ç°ä¸ºæ›´è°¨æ…å’Œå»æ‹ŸäººåŒ–çš„è§£é‡Šã€‚æœ€ç»ˆç»“è®ºæŒ‡å‡ºï¼Œå¿ƒç†æµ‹é‡æç¤º(Psychometric prompting)è™½ç„¶èƒ½ä¸ºLLMsçš„æ²Ÿé€šè¡Œä¸ºæä¾›ç»“æ„åŒ–çš„æ´å¯Ÿï¼Œä½†å°šæ— æ³•ä½œä¸ºæ ¡å‡†å…¶æ€§èƒ½ä¼°è®¡çš„æœ‰æ•ˆæ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages,5 tables, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19872v2",
      "published_date": "2025-11-25 03:24:11 UTC",
      "updated_date": "2025-11-26 18:41:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:16.535884+00:00"
    },
    {
      "arxiv_id": "2511.19865v1",
      "title": "Agentic AI-Empowered Conversational Embodied Intelligence Networks in 6G",
      "title_zh": "6G ç¯å¢ƒä¸‹æ™ºèƒ½ä½“ AI èµ‹èƒ½çš„å¯¹è¯å¼å…·èº«æ™ºèƒ½ç½‘ç»œ",
      "authors": [
        "Mingkai Chen",
        "Zijie Feng",
        "Lei Wang",
        "Yaser Khamayseh"
      ],
      "abstract": "In the 6G era, semantic collaboration among multiple embodied intelligent devices (MEIDs) becomes crucial for complex task execution. However, existing systems face challenges in multimodal information fusion, adaptive communication, and decision interpretability. To address these limitations, we propose a collaborative Conversational Embodied Intelligence Network (CC-EIN) integrating multimodal feature fusion, adaptive semantic communication, task coordination, and interpretability. PerceptiNet performs cross-modal fusion of image and radar data to generate unified semantic representations. An adaptive semantic communication strategy dynamically adjusts coding schemes and transmission power according to task urgency and channel quality. A semantic-driven collaboration mechanism further supports task decomposition and conflict-free coordination among heterogeneous devices. Finally, the InDec module enhances decision transparency through Grad-CAM visualization. Simulation results in post-earthquake rescue scenarios demonstrate that CC-EIN achieves 95.4% task completion rate and 95% transmission efficiency while maintaining strong semantic consistency and energy efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹6Gæ—¶ä»£å¤šä¸ªå…·èº«æ™ºèƒ½è®¾å¤‡(MEIDs)åœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„å¤šæ¨¡æ€èåˆã€è‡ªé€‚åº”é€šä¿¡åŠå†³ç­–å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä½œå¼å¯¹è¯å…·èº«æ™ºèƒ½ç½‘ç»œ(CC-EIN)ã€‚è¯¥ç½‘ç»œåˆ©ç”¨PerceptiNetæ¨¡å—å®ç°å›¾åƒä¸é›·è¾¾æ•°æ®çš„è·¨æ¨¡æ€èåˆä»¥ç”Ÿæˆç»Ÿä¸€è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”è¯­ä¹‰é€šä¿¡ç­–ç•¥æ ¹æ®ä»»åŠ¡ç´§æ€¥ç¨‹åº¦å’Œä¿¡é“è´¨é‡åŠ¨æ€ä¼˜åŒ–ä¼ è¾“æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡è¯­ä¹‰é©±åŠ¨åä½œæœºåˆ¶æ”¯æŒå¼‚æ„è®¾å¤‡é—´çš„ä»»åŠ¡åˆ†è§£ä¸æ— å†²çªåè°ƒï¼Œå¹¶å€ŸåŠ©InDecæ¨¡å—åŠGrad-CAMå¯è§†åŒ–æŠ€æœ¯æ˜¾è‘—å¢å¼ºäº†å†³ç­–é€æ˜åº¦ã€‚åœ¨åœ°éœ‡æ•‘æ´åœºæ™¯çš„ä»¿çœŸå®éªŒä¸­ï¼ŒCC-EINå®ç°äº†95.4%çš„ä»»åŠ¡å®Œæˆç‡å’Œ95%çš„ä¼ è¾“æ•ˆç‡ã€‚è¯¥ç ”ç©¶åœ¨ç»´æŒå¼ºè¯­ä¹‰ä¸€è‡´æ€§ä¸èƒ½æºæ•ˆç‡çš„åŒæ—¶ï¼Œä¸º6Gç¯å¢ƒä¸‹å¤æ‚çš„å…·èº«æ™ºèƒ½åä½œæä¾›äº†é«˜æ•ˆä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 8 figures. Preprint submitted to IEEE Vehicle Technology Magazine",
      "pdf_url": "https://arxiv.org/pdf/2511.19865v1",
      "published_date": "2025-11-25 03:16:30 UTC",
      "updated_date": "2025-11-25 03:16:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:31.843197+00:00"
    },
    {
      "arxiv_id": "2511.19864v1",
      "title": "MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support",
      "title_zh": "MicroSimsï¼šæ”¯æŒé€šç”¨åµŒå…¥ä¸è‡ªé€‚åº”å­¦ä¹ çš„AIç”Ÿæˆå¼å¯æ‰©å±•æ•™è‚²æ¨¡æ‹Ÿæ¡†æ¶",
      "authors": [
        "Valerie Lockhart",
        "Dan McCreary",
        "Troy A. Peterson"
      ],
      "abstract": "Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MicroSims æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨åˆ›å»ºè½»é‡çº§ã€äº¤äº’å¼æ•™è‚²æ¨¡æ‹Ÿå®éªŒçš„åˆ›æ–°ç³»ç»Ÿï¼Œé€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯å®ç°å¿«é€Ÿç”Ÿæˆä¸è§„æ¨¡åŒ–åº”ç”¨ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ”¯æŒ AI è¾…åŠ©ç”Ÿæˆçš„æ ‡å‡†åŒ–è®¾è®¡æ¨¡å¼(Standardized Design Patterns)ï¼Œä»¥åŠåŸºäº iframe æ¶æ„çš„é€šç”¨åµŒå…¥(Universal Embedding)æŠ€æœ¯ï¼Œç¡®ä¿äº†åœ¨å„ç±»æ•°å­—å­¦ä¹ å¹³å°ä¸Šçš„å®‰å…¨æ€§å’Œå…¼å®¹æ€§ã€‚MicroSims é€šè¿‡é€æ˜ä¸”å¯ä¿®æ”¹çš„ä»£ç æ”¯æŒï¼Œå…è®¸æ•™è‚²è€…åœ¨æ— éœ€ç¼–ç¨‹ç»éªŒçš„æƒ…å†µä¸‹è¿›è¡Œæ·±åº¦å®šåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ¨¡æ‹Ÿå·¥å…·å¼€å‘æˆæœ¬é«˜å’ŒæŠ€æœ¯é—¨æ§›å¤§çš„ç—›ç‚¹ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ­¤ç±»äº¤äº’å¼æ¨¡æ‹Ÿèƒ½å°†å­¦ç”Ÿå¯¹æ¦‚å¿µçš„ç†è§£èƒ½åŠ›æ¯”ä¼ ç»Ÿæ•™å­¦æå‡ 30-40%ï¼Œä¸ºæ•™è‚²å…¬å¹³å’Œä½æˆæœ¬æ™ºèƒ½äº¤äº’å¼æ•™ç§‘ä¹¦çš„æ™®åŠæä¾›äº†å¯èƒ½ã€‚è¯¥å·¥ä½œè¿˜è¯¦ç»†é˜è¿°äº†å¼€å‘å·¥ä½œæµä¸å…ƒæ•°æ®æ ‡å‡†ï¼Œå¹¶ä¸ºæœªæ¥æ„å»ºåŸºäº AI é©±åŠ¨çš„è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ(Adaptive Learning Systems)å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19864v1",
      "published_date": "2025-11-25 03:14:39 UTC",
      "updated_date": "2025-11-25 03:14:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:37.737512+00:00"
    },
    {
      "arxiv_id": "2511.19858v2",
      "title": "A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction",
      "title_zh": "åŸºäº RAG åŠ¨æ€æç¤ºçš„å¤§è¯­è¨€æ¨¡å‹åŒ»ç–—é”™è¯¯æ£€æµ‹ä¸çº æ­£ç³»ç»Ÿæ€§åˆ†æ",
      "authors": [
        "Farzad Ahmed",
        "Joniel Augustine Jerome",
        "Meliha Yetisgen",
        "Ã–zlem Uzuner"
      ],
      "abstract": "Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.\n  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.\n  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.\n  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨åŒ»ç–—é”™è¯¯æ£€æµ‹ä¸çº æ­£ä¸­çš„åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œæ—¨åœ¨æé«˜ä¸´åºŠæ–‡æ¡£çš„å‡†ç¡®æ€§ä»¥ä¿éšœæ‚£è€…å®‰å…¨ã€‚ç ”ç©¶è¯„ä¼°äº†Zero-shot promptingã€åŸºäºéšæœºæ ·æœ¬çš„é™æ€æç¤º(Static Prompting with Random exemplars, SPR)ä»¥åŠæ£€ç´¢å¢å¼ºåŠ¨æ€æç¤º(Retrieval-augmented Dynamic Prompting, RDP)åœ¨ä¸‰ä¸ªå…³é”®å­ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å®éªŒé€šè¿‡MEDECæ•°æ®é›†æµ‹è¯•äº†åŒ…æ‹¬GPTã€Claudeå’ŒGeminiåœ¨å†…çš„ä¹ç§æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œå¹¶å¯¹æ¯”äº†å®ƒä»¬çš„æ¨ç†å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼ŒZero-shotæ–¹æ³•åœ¨å¤„ç†ç¼©å†™å’Œéå…¸å‹é”™è¯¯æ—¶å¬å›ç‡(recall)è¾ƒä½ï¼Œè€ŒSPRè™½æå‡äº†å¬å›ç‡ä½†å¯¼è‡´äº†è¯¯æŠ¥ç‡(FPR)ä¸Šå‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRDPåœ¨æ‰€æœ‰æ¨¡å‹ä¸­å‡è¡¨ç°æœ€ä¼˜ï¼Œä¸ä»…å°†è¯¯æŠ¥ç‡é™ä½äº†çº¦15%ï¼Œè¿˜å°†é”™è¯¯å¥å­æ£€æµ‹çš„å¬å›ç‡æå‡äº†5%è‡³10%ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨æ£€ç´¢ç¤ºä¾‹çš„åŠ¨æ€æç¤ºç­–ç•¥èƒ½æ˜¾è‘—å¢å¼ºåŒ»ç–—çº é”™ä»»åŠ¡çš„ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚æœ€ç»ˆç»“è®ºæŒ‡å‡ºï¼ŒRDPä¸ºæ„å»ºé«˜å¯é æ€§çš„ä¸´åºŠé”™è¯¯å¤„ç†ç³»ç»Ÿæä¾›äº†ä¼˜äºä¼ ç»Ÿæç¤ºç­–ç•¥çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦é”™è¯¯æ£€æµ‹çš„ç²¾åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19858v2",
      "published_date": "2025-11-25 02:40:49 UTC",
      "updated_date": "2025-11-26 09:29:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:46.732846+00:00"
    },
    {
      "arxiv_id": "2511.19849v1",
      "title": "Reinforcement Learning with $Ï‰$-Regular Objectives and Constraints",
      "title_zh": "å…·æœ‰ $\\omega$-æ­£åˆ™ç›®æ ‡ä¸çº¦æŸçš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Dominik Wagner",
        "Leon Witzman",
        "Luke Ong"
      ],
      "abstract": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $Ï‰$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.\n  We address both limitations simultaneously by combining $Ï‰$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $Ï‰$-regular objective while also adhering to $Ï‰$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­ç”±äºä¾èµ–æ ‡é‡å¥–åŠ±è€Œéš¾ä»¥è¡¨è¾¾å¤æ‚æ—¶åºç›®æ ‡åŠå®‰å…¨å…³é”®ç›®æ ‡ï¼Œä¸”æ˜“å¯¼è‡´å¥–åŠ±æ“çºµ(reward hacking)çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æå‡ºå°†$\\omega$-regularç›®æ ‡ä¸æ˜¾å¼çº¦æŸç›¸ç»“åˆï¼Œæ—¨åœ¨å°†å®‰å…¨éœ€æ±‚ä¸ä¼˜åŒ–ç›®æ ‡åˆ†ç¦»å¤„ç†ã€‚ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºçº¿æ€§è§„åˆ’(linear programming)çš„åŸºäºæ¨¡å‹(model-based)çš„RLç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨æ»¡è¶³é¢„è®¾$\\omega$-regularçº¦æŸé˜ˆå€¼çš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–æ»¡è¶³$\\omega$-regularç›®æ ‡çš„æ¦‚ç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ç¡®ç«‹äº†å‘å—é™æé™å¹³å‡(constrained limit-average)é—®é¢˜çš„è½¬åŒ–æ–¹æ³•ï¼Œå¹¶æä¾›äº†ä¿æŒæœ€ä¼˜æ€§çš„ç†è®ºä¿è¯ã€‚è¿™ä¸€æ–¹æ³•ä¸ºå¤„ç†å…·æœ‰å®¹å¿é£é™©æ°´å¹³çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ›´ç²¾ç¡®çš„è¡Œä¸ºè§„èŒƒæ‰‹æ®µå’Œæ€§èƒ½æƒè¡¡æœºåˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19849v1",
      "published_date": "2025-11-25 02:28:02 UTC",
      "updated_date": "2025-11-25 02:28:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:47:59.835529+00:00"
    },
    {
      "arxiv_id": "2511.19841v1",
      "title": "Cisco Time Series Model Technical Report",
      "title_zh": "Cisco æ—¶é—´åºåˆ—æ¨¡å‹æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Liang Gou",
        "Archit Khare",
        "Praneet Pabolu",
        "Prachi Patel",
        "Joseph Ross",
        "Hercy Shen",
        "Yuhan",
        "Song",
        "Jingze Sun",
        "Kristal Curtis",
        "Vedant Dharnidharka",
        "Abhinav Mathur",
        "Hao Yang"
      ],
      "abstract": "We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Cisco Time Series Modelï¼Œè¿™æ˜¯ä¸€ç§å•å˜é‡çš„ zero-shot é¢„æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æµè¡Œçš„ decoder-only æ—¶é—´åºåˆ—æ¨¡å‹ TimesFM è¿›è¡Œæ¶æ„åˆ›æ–°ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥æ”¶ multiresolution è¾“å…¥ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡ 300B ä¸ªå”¯ä¸€æ•°æ®ç‚¹ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡è®­ç»ƒï¼Œå…¶ä¸­åŠæ•°ä»¥ä¸Šæ•°æ®æºè‡ª observability é¢†åŸŸã€‚å®šé‡ä¸å®šæ€§è¯„ä¼°è¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ observability æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨æ ‡å‡†é€šç”¨é¢„æµ‹åŸºå‡† GIFT-Eval ä¸Šä¿æŒäº†æé«˜æ°´å‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç° multiresolution ç»“æ„æœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨å¤„ç† long context è¾“å…¥æ—¶çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19841v1",
      "published_date": "2025-11-25 02:12:52 UTC",
      "updated_date": "2025-11-25 02:12:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:49:55.121829+00:00"
    },
    {
      "arxiv_id": "2511.19837v1",
      "title": "GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning",
      "title_zh": "ç”¨äºå›¾ç›¸ä¼¼æ€§å­¦ä¹ çš„å¯¹é½ä¸æœªå¯¹é½å­ç»“æ„ GED ä¸€è‡´æ€§è§£è€¦",
      "authors": [
        "Zhentao Zhan",
        "Xiaoliang Xu",
        "Jingjing Wang",
        "Junmei Wang"
      ],
      "abstract": "Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç›¸ä¼¼åº¦è®¡ç®— (Graph Similarity Computation) ä¸­å›¾ç¼–è¾‘è·ç¦» (Graph Edit Distance, GED) çš„è¿‘ä¼¼è¯„ä¼°é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰åŸºäºå›¾ç¥ç»ç½‘ç»œ (GNN) çš„æ–¹æ³•åœ¨èŠ‚ç‚¹ä¸­å¿ƒåŒ¹é…èŒƒå¼ä¸‹å­˜åœ¨å…¨å±€ç»“æ„å¯¹åº”æ•è·å¤±è´¥å’Œç¼–è¾‘ä»£ä»·è¯¯å½’å› çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† GCGSimï¼Œä¸€ä¸ªä»¥å›¾çº§åŒ¹é…å’Œå­ç»“æ„çº§ç¼–è¾‘ä»£ä»·ä¸ºæ ¸å¿ƒçš„ GED ä¸€è‡´æ€§å›¾ç›¸ä¼¼åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£è€¦ (disentanglement) æŠ€æœ¯å°†å›¾ç»“æ„åˆ’åˆ†ä¸ºå¯¹é½ (aligned) ä¸æœªå¯¹é½ (unaligned) å­ç»“æ„ï¼Œä»è€Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜  GED çš„æ ¸å¿ƒåŸç†ã€‚å®éªŒåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯æ˜äº† GCGSim è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (state-of-the-art) çš„æ€§èƒ½æ°´å¹³ã€‚è¿›ä¸€æ­¥çš„åˆ†æéªŒè¯äº†è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå­¦ä¹ åˆ°è§£è€¦ä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„å­ç»“æ„è¡¨ç¤ºï¼ŒæˆåŠŸå…‹æœäº†ä¼ ç»ŸèŠ‚ç‚¹åŒ¹é…èŒƒå¼çš„ä¸è¶³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19837v1",
      "published_date": "2025-11-25 02:07:30 UTC",
      "updated_date": "2025-11-25 02:07:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:07.637571+00:00"
    },
    {
      "arxiv_id": "2511.19835v1",
      "title": "Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation",
      "title_zh": "Rectified SpaAttnï¼šé‡æ–°å®¡è§†æ³¨æ„åŠ›ç¨€ç–æ€§ä»¥å®ç°é«˜æ•ˆè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Xuewen Liu",
        "Zhikai Li",
        "Jing Zhang",
        "Mengjuan Chen",
        "Qingyi Gu"
      ],
      "abstract": "Diffusion Transformers dominate video generation, but the quadratic complexity of attention computation introduces substantial latency. Attention sparsity reduces computational costs by focusing on critical tokens while ignoring non-critical tokens. However, existing methods suffer from severe performance degradation. In this paper, we revisit attention sparsity and reveal that existing methods induce systematic biases in attention allocation: (1) excessive focus on critical tokens amplifies their attention weights; (2) complete neglect of non-critical tokens causes the loss of relevant attention weights. To address these issues, we propose Rectified SpaAttn, which rectifies attention allocation with implicit full attention reference, thereby enhancing the alignment between sparse and full attention maps. Specifically: (1) for critical tokens, we show that their bias is proportional to the sparse attention weights, with the ratio governed by the amplified weights. Accordingly, we propose Isolated-Pooling Attention Reallocation, which calculates accurate rectification factors by reallocating multimodal pooled weights. (2) for non-critical tokens, recovering attention weights from the pooled query-key yields attention gains but also introduces pooling errors. Therefore, we propose Gain-Aware Pooling Rectification, which ensures that the rectified gain consistently surpasses the induced error. Moreover, we customize and integrate the Rectified SpaAttn kernel using Triton, achieving up to 3.33 and 2.08 times speedups on HunyuanVideo and Wan 2.1, respectively, while maintaining high generation quality. We release Rectified SpaAttn as open-source at https://github.com/BienLuky/Rectified-SpaAttn .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç”Ÿæˆä¸­ Diffusion Transformers æ³¨æ„åŠ›è®¡ç®—çš„äºŒæ¬¡æ–¹å¤æ‚åº¦é—®é¢˜ï¼Œé‡æ–°å®¡è§†äº†æ³¨æ„åŠ›ç¨€ç–æ€§ (Attention Sparsity) çš„ç°æœ‰ç¼ºé™·ã€‚ä½œè€…å‘ç°å½“å‰æ–¹æ³•åœ¨æ³¨æ„åŠ›åˆ†é…ä¸Šå­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œè¡¨ç°ä¸ºå¯¹å…³é”® token çš„è¿‡åº¦æ”¾å¤§ä»¥åŠå¯¹éå…³é”® token çš„å®Œå…¨å¿½è§†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº† Rectified SpaAttn æ¡†æ¶ï¼Œé€šè¿‡éšå¼å…¨æ³¨æ„åŠ›å‚è€ƒæ¥å¢å¼ºç¨€ç–ä¸å…¨æ³¨æ„åŠ›å›¾ä¹‹é—´çš„å¯¹é½ã€‚è¯¥æ–¹æ¡ˆå…·ä½“å¼•å…¥äº† Isolated-Pooling Attention Reallocation æ¥ç²¾ç¡®é‡åˆ†é…æƒé‡ï¼Œå¹¶åˆ©ç”¨ Gain-Aware Pooling Rectification ç¡®ä¿éå…³é”® token çš„å¢ç›Šä¼˜äºå¼•å…¥çš„è¯¯å·®ã€‚é€šè¿‡ä½¿ç”¨ Triton å®šåˆ¶çš„å†…æ ¸ï¼ŒRectified SpaAttn åœ¨ HunyuanVideo å’Œ Wan 2.1 ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾ 3.33 å€å’Œ 2.08 å€çš„æ¨ç†åŠ é€Ÿã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œä¾ç„¶èƒ½å¤Ÿä¿æŒæé«˜çš„è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code at https://github.com/BienLuky/Rectified-SpaAttn",
      "pdf_url": "https://arxiv.org/pdf/2511.19835v1",
      "published_date": "2025-11-25 02:03:54 UTC",
      "updated_date": "2025-11-25 02:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:09.329974+00:00"
    },
    {
      "arxiv_id": "2511.20714v1",
      "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
      "title_zh": "Inferixï¼šåŸºäºå—æ‰©æ•£çš„ä¸‹ä¸€ä»£ä¸–ç•Œæ¨¡æ‹Ÿæ¨ç†å¼•æ“",
      "authors": [
        "Inferix Team",
        "Tianyu Feng",
        "Yizeng Han",
        "Jiahao He",
        "Yuanyu He",
        "Xi Lin",
        "Teng Liu",
        "Hanfeng Lu",
        "Jiasheng Tang",
        "Wei Wang",
        "Zhiyuan Wang",
        "Jichao Wu",
        "Mingyang Yang",
        "Yinghao Yu",
        "Zeyu Zhang",
        "Bohan Zhuang"
      ],
      "abstract": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Inferixï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºä¸–ç•Œæ¨¡æ‹Ÿ(World Simulation)è®¾è®¡çš„ä¸‹ä¸€ä»£æ¨ç†å¼•æ“ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–çš„åŠè‡ªå›å½’(semi-autoregressive)è§£ç è¿‡ç¨‹å®ç°æ²‰æµ¸å¼ä¸–ç•Œåˆæˆã€‚è¯¥å¼•æ“çš„æ ¸å¿ƒé‡‡ç”¨äº†å—æ‰©æ•£(block-diffusion)èŒƒå¼ï¼Œé€šè¿‡åœ¨æ¯ä¸ªå—å†…åº”ç”¨æ‰©æ•£å¹¶ä»¥å…ˆå‰å—ä¸ºæ¡ä»¶çš„ç”Ÿæˆæ–¹å¼ï¼Œèåˆäº†æ‰©æ•£æ¨¡å‹ä¸è‡ªå›å½’æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œä»è€Œç”Ÿæˆæ›´è¿è´¯ç¨³å®šçš„é•¿è§†é¢‘åºåˆ—ã€‚Inferix é‡æ–°å¼•å…¥äº†ç±»ä¼¼å¤§è¯­è¨€æ¨¡å‹(LLM)çš„ KV Cache ç®¡ç†æœºåˆ¶ï¼Œå…‹æœäº†æ ‡å‡†è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ï¼Œå®ç°äº†é«˜æ•ˆã€å˜é•¿ä¸”é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆã€‚ä¸ä¸“æ³¨äºé«˜å¹¶å‘åœºæ™¯çš„ vLLMã€SGLang æˆ–ç»å…¸çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ xDiTs ä¸åŒï¼Œè¯¥å¼•æ“æ”¯æŒäº¤äº’å¼è§†é¢‘æµå’Œæ€§èƒ½åˆ†æï¼Œèƒ½å¤Ÿå®æ—¶æ¨¡æ‹Ÿå¤æ‚çš„ç‰©ç†åŠ¨æ€ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡é›†æˆé’ˆå¯¹åˆ†é’Ÿçº§è§†é¢‘ç”Ÿæˆçš„ç»†ç²’åº¦è¯„ä¼°åŸºå‡† LV-Benchï¼Œæ˜¾è‘—æå‡äº†åŸºå‡†æµ‹è¯•æ•ˆç‡ã€‚Inferix ä¸º Agentic AIã€å…·èº«æ™ºèƒ½(Embodied AI)å’Œæ¸¸æˆé¢†åŸŸæä¾›äº†å¼ºå¤§çš„æ ¸å¿ƒæ¨¡æ‹Ÿå™¨æ”¯æŒï¼Œæ¨åŠ¨äº†è§†è§‰åŸºç¡€æ¨¡å‹å‘æ›´æ·±å±‚æ¬¡çš„æ„ŸçŸ¥ä¸æ¨ç†æ¼”è¿›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20714v1",
      "published_date": "2025-11-25 01:45:04 UTC",
      "updated_date": "2025-11-25 01:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:16.137285+00:00"
    },
    {
      "arxiv_id": "2511.19830v1",
      "title": "Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization",
      "title_zh": "è¶…è¶Šå…³ç³»èŒƒå¼ï¼šåŸºäº LLM åŸç”ŸæŸ¥è¯¢ä¼˜åŒ–çš„è¯­ä¹‰æ„ŸçŸ¥å¤šæ¨¡æ€åˆ†æ",
      "authors": [
        "Junhao Zhu",
        "Lu Chen",
        "Xiangyu Ke",
        "Ziquan Fang",
        "Tianyi Li",
        "Yunjun Gao",
        "Christian S. Jensen"
      ],
      "abstract": "Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm.\n  We present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Nirvanaï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿå…³ç³»èŒƒå¼çš„ Multi-modal Data Analytics æ¡†æ¶ï¼Œåˆ©ç”¨ Large Language Models (LLMs) å®ç° Semantic-Aware çš„æŸ¥è¯¢å¤„ç†ã€‚Nirvana å¼•å…¥äº†å¯ç¼–ç¨‹çš„ Semantic Operatorsï¼Œå¹¶é€šè¿‡ä¸€ä¸ª Agentic Logical Optimizer åˆ©ç”¨è‡ªç„¶è¯­è¨€è§„åˆ™å’Œ Random-Walk-Based Search æ¥æ¢ç´¢è¯­ä¹‰ç­‰ä»·çš„æŸ¥è¯¢è®¡åˆ’ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é…å¤‡äº†ä¸€ä¸ª Cost-Aware Physical Optimizerï¼Œé€šè¿‡ Improvement-Score Metric ä¸ºæ¯ä¸ªç®—å­åŒ¹é…æœ€åˆé€‚çš„ LLM Backendã€‚ä¸ºäº†æœ€å¤§åŒ–æ•ˆç‡ï¼ŒNirvana è¿˜é›†æˆäº†åŸºäºæ¨¡å‹èƒ½åŠ›å‡è®¾çš„ Computation Reuse å’Œ Evaluation Pushdown æŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNirvana åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å°†ç«¯åˆ°ç«¯è¿è¡Œæ—¶é—´å‡å°‘äº† 10%â€“85%ï¼Œå¹³å‡ç³»ç»Ÿå¤„ç†æˆæœ¬é™ä½äº† 76%ï¼Œåœ¨æ•ˆç‡å’Œ Scalability æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19830v1",
      "published_date": "2025-11-25 01:41:49 UTC",
      "updated_date": "2025-11-25 01:41:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:38.943925+00:00"
    },
    {
      "arxiv_id": "2511.19829v1",
      "title": "A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization",
      "title_zh": "ä¸€ç§ç”¨äºæŸ¥è¯¢ç›¸å…³æç¤ºè¯ä¼˜åŒ–çš„ç»Ÿä¸€è¯„ä¼°æŒ‡å¯¼æ¡†æ¶",
      "authors": [
        "Ke Chen",
        "Yifeng Wang",
        "Hassan Almosapeeh",
        "Haohan Wang"
      ],
      "abstract": "Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ã€è¯„ä¼°å¼•å¯¼çš„æ¡†æ¶ï¼Œç”¨äºå¤„ç†ä¾èµ–æŸ¥è¯¢çš„æç¤ºè¯ä¼˜åŒ–(Query-Dependent Prompt Optimization)ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€è´¨é‡å®šä¹‰ä»¥åŠåé¦ˆä¿¡å·ä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶é¦–å…ˆå»ºç«‹äº†ä¸€å¥—ç³»ç»ŸåŒ–çš„æ€§èƒ½å¯¼å‘è¯„ä¼°æ¡†æ¶ã€‚éšåï¼Œç ”ç©¶å¼€å‘å¹¶å¾®è°ƒäº†ä¸€ä¸ªæ— éœ€æ‰§è¡Œçš„è¯„ä¼°å™¨(Execution-free Evaluator)ï¼Œèƒ½å¤Ÿç›´æ¥ä»æ–‡æœ¬é¢„æµ‹å¤šç»´è´¨é‡å¾—åˆ†ã€‚è¯¥è¯„ä¼°å™¨è¿›è€ŒæŒ‡å¯¼ä¸€ä¸ªæŒ‡æ ‡æ„ŸçŸ¥çš„ä¼˜åŒ–å™¨(Metric-aware Optimizer)ï¼Œä»¥å¯è§£é‡Šä¸”ä¾èµ–æŸ¥è¯¢çš„æ–¹å¼è¯Šæ–­å¤±è´¥æ¨¡å¼å¹¶é‡å†™æç¤ºè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è¯„ä¼°å™¨åœ¨é¢„æµ‹æç¤ºè¯æ€§èƒ½æ–¹é¢å…·æœ‰æé«˜å‡†ç¡®ç‡ï¼Œä¸”å…¶ä¼˜åŒ–æµç¨‹åœ¨å…«ä¸ªæ•°æ®é›†å’Œä¸‰ç§ä¸»æµæ¨¡å‹ä¸Šå‡ä¼˜äºé™æ€æ¨¡æ¿åŠç°æœ‰çš„æŸ¥è¯¢ä¾èµ–åŸºå‡†ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•é€šè¿‡æŒ‡æ ‡è½åœ°çš„è§†è§’ï¼Œä¸ºå¤šæ ·åŒ–ä»»åŠ¡æä¾›äº†ç¨³å®šã€å¯è§£é‡Šä¸”ä¸æ¨¡å‹æ— å…³çš„æ˜¾è‘—æ”¹è¿›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19829v1",
      "published_date": "2025-11-25 01:41:13 UTC",
      "updated_date": "2025-11-25 01:41:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:49:48.244489+00:00"
    },
    {
      "arxiv_id": "2511.19822v1",
      "title": "Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models",
      "title_zh": "Mosaic å‰ªæï¼šæ··åˆä¸“å®¶æ¨¡å‹å¯æ³›åŒ–å‰ªæçš„å±‚çº§æ¡†æ¶",
      "authors": [
        "Wentao Hu",
        "Mingkuan Zhao",
        "Shuangyong Song",
        "Xiaoyan Zhu",
        "Xin Lai",
        "Jiayin Wang"
      ],
      "abstract": "Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select\" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\\% gain on general tasks and 8.92\\% on specialized tasks like math reasoning and code generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Mosaic Pruning (MoP)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡Mixture-of-Experts (MoE)æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å‰ªææ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–å•ä¸€è¯­æ–™åº“å¯¼è‡´è·¨é¢†åŸŸæ€§èƒ½å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼ŒMoPé€šè¿‡ç»“æ„åŒ–çš„â€œå…ˆèšç±»åé€‰æ‹©â€è¿‡ç¨‹æ„å»ºåŠŸèƒ½å®Œå¤‡çš„ä¸“å®¶é›†ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç›¸ä¼¼åº¦æŒ‡æ ‡åœ¨ä¸åŒä»»åŠ¡é¢†åŸŸå¯¹ä¸“å®¶è¿›è¡ŒåŠŸèƒ½èšç±»ï¼Œå¹¶ç»“åˆæå‡ºçš„Activation Variability Scoreä»å„ç°‡ä¸­ç­›é€‰æœ€å…·ä»£è¡¨æ€§çš„ä¸“å®¶ã€‚å®éªŒè¯æ˜ï¼ŒMoPèƒ½ç¡®ä¿å‰ªæåçš„æ¨¡å‹ä¿ç•™åŠŸèƒ½äº’è¡¥çš„ä¸“å®¶ç»„åˆï¼Œä½¿å…¶åœ¨é€šç”¨ä»»åŠ¡ä¸Šæ¯”ç°æœ‰æ–¹æ³•å‡†ç¡®ç‡æå‡7.24%ï¼Œåœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ä¸“ä¸šé¢†åŸŸæå‡8.92%ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†Sparse Mixture-of-Experts (SMoE)æ¨¡å‹çš„é™æ€å†…å­˜å¼€é”€ï¼ŒåŒæ—¶ä¿è¯äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19822v1",
      "published_date": "2025-11-25 01:24:41 UTC",
      "updated_date": "2025-11-25 01:24:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:27.738122+00:00"
    },
    {
      "arxiv_id": "2511.19820v1",
      "title": "CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception",
      "title_zh": "CropVLMï¼šé¢å‘ç»†ç²’åº¦è§†è§‰è¯­è¨€æ„ŸçŸ¥çš„ç¼©æ”¾å­¦ä¹ ",
      "authors": [
        "Miguel Carvalho",
        "Helder Dias",
        "Bruno Martins"
      ],
      "abstract": "Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CropVLMï¼Œä¸€ç§æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ç»†ç²’åº¦å›¾åƒç†è§£ï¼ˆå¦‚åœºæ™¯æ–‡æœ¬è¯†åˆ«æˆ–æ–‡æ¡£åˆ†æï¼‰ä¸­æ„ŸçŸ¥å—é™åŠè§†è§‰ç¢ç‰‡åŒ–é—®é¢˜çš„ä½æˆæœ¬å¤–éƒ¨å¢å¼ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•å…è®¸ VLMs åŠ¨æ€åœ°å¯¹å›¾åƒç›¸å…³åŒºåŸŸè¿›è¡Œâ€œæ”¾å¤§â€ (Zoom in)ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå…¶æ•æ‰å¾®è§‚ç»†èŠ‚çš„èƒ½åŠ›ã€‚CropVLM é‡‡ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€äººç±»æ ‡æ³¨çš„è¾¹ç•Œæ¡†ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä¹Ÿæ— éœ€è¿›è¡Œæ˜‚è´µçš„åˆæˆè¯„ä¼°ã€‚è¯¥æ¨¡å‹è®­ç»ƒä¸€æ¬¡åå³å¯ä¸å¼€æºæˆ–ä¸“æœ‰ VLMs é…å¯¹ä½¿ç”¨ï¼Œä¸”ç”±äºæ— éœ€å¯¹åŸå§‹ VLM è¿›è¡Œä¿®æ”¹æˆ–å¾®è°ƒï¼Œæœ‰æ•ˆé¿å…äº†ç¾éš¾æ€§é—å¿˜ (Catastrophic Forgetting) é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éœ€è¦é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£çš„ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå°¤å…¶æ˜¯åœ¨é’ˆå¯¹ç›®æ ‡ VLM çš„åŸŸå¤– (Out-of-domain) åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚CropVLM ä¸ºæå‡ç°æœ‰å¤§æ¨¡å‹çš„ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„å³æ’å³ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19820v1",
      "published_date": "2025-11-25 01:21:26 UTC",
      "updated_date": "2025-11-25 01:21:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:41.529455+00:00"
    },
    {
      "arxiv_id": "2511.19818v1",
      "title": "Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana",
      "title_zh": "åŸºäºè¿œç¨‹ç›‘ç£çš„è¯­è¨€æ— å…³æƒ…æ„Ÿæ ‡æ³¨ï¼šEnglishã€Sepedi ä¸ Setswana æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Koena Ronny Mabokela",
        "Tim Schlippe",
        "Mpho Raborife",
        "Turgay Celik"
      ],
      "abstract": "Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éæ´²è¯­è¨€ç­‰ä½èµ„æºè¯­è¨€(low-resource languages)æƒ…æ„Ÿæ ‡æ³¨æˆæœ¬é«˜ä¸”æ•°å­—èµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç‹¬ç«‹äºç‰¹å®šè¯­è¨€çš„è‡ªåŠ¨æƒ…æ„Ÿæ ‡æ³¨æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¿œç¨‹ç›‘ç£(Distant Supervision)æŠ€æœ¯ï¼Œç»“åˆå…·æœ‰æƒ…æ„Ÿè‰²å½©çš„è¡¨æƒ…ç¬¦å·(emojis)å’Œè¯æ±‡ç‰¹å¾å®ç°è‡ªåŠ¨æ ‡æ³¨ã€‚å®éªŒé€‰å–äº†æ¥è‡ªSAfriSentiè¯­æ–™åº“çš„è‹±è¯­(English)ã€åŒ—ç´¢æ‰˜è¯­(Sepedi)å’ŒèŒ¨ç“¦çº³è¯­(Setswana)æ¨æ–‡è¿›è¡ŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç§è¯­è¨€ä¸Šçš„æ ‡æ³¨å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°66%ã€69%å’Œ63%ï¼Œå¹³å‡ä»…éœ€äººå·¥ä¿®æ­£34%çš„è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾ã€‚è¿™ä¸€ç ”ç©¶æˆæœæ˜¾è‘—é™ä½äº†æƒ…æ„Ÿåˆ†æä¸­äººå·¥æ ‡æ³¨çš„æ—¶é—´ä¸ç»æµæˆæœ¬ï¼Œä¸ºä½èµ„æºè¯­è¨€æƒ…æ„Ÿåˆ†æä»»åŠ¡çš„é«˜æ•ˆå¼€å±•æä¾›äº†æœ‰æ•ˆå·¥å…·ä¸è‡ªåŠ¨åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in the The Fourth Workshop on Processing Emotions, Decisions and Opinions (EDO 2023) at 10th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics (LTC 2023), PoznaÅ„, Poland, 21-23 April 2023. ISBN: 978-83-232-4176-8",
      "pdf_url": "https://arxiv.org/pdf/2511.19818v1",
      "published_date": "2025-11-25 01:15:54 UTC",
      "updated_date": "2025-11-25 01:15:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:48:45.642188+00:00"
    },
    {
      "arxiv_id": "2512.03053v1",
      "title": "Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation",
      "title_zh": "ç¼“è§£å¯é€†é—®é¢˜ä¸­å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰ä¸é—æ¼ï¼šåœ¨ç¡¬ä»¶é€»è¾‘è®¾è®¡è‡ªåŠ¨åŒ–ä¸­çš„åº”ç”¨",
      "authors": [
        "Andrew S. Cassidy",
        "Guillaume Garreau",
        "Jay Sivagnaname",
        "Mike Grassi",
        "Bernard Brezzo",
        "John V. Arthur",
        "Dharmendra S. Modha"
      ],
      "abstract": "We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨å¤„ç†å¯é€†é—®é¢˜ (invertible problems) æ—¶å‡ºç°çš„å¹»è§‰å’Œé—æ¼ç¼ºé™·ï¼Œæå‡ºäº†ä¸€ç§å€Ÿé‰´ä¿¡æ¯è®ºæ— æŸå‹ç¼©æ¦‚å¿µçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°† LLMs åˆ†åˆ«ä½œä¸ºä»æºåŸŸåˆ°ç›®æ ‡åŸŸçš„æ— æŸç¼–ç å™¨ (lossless encoder) å’Œè¿”å›æºåŸŸçš„æ— æŸè§£ç å™¨ (lossless decoder)ï¼Œé€šè¿‡åŒå‘è½¬æ¢éªŒè¯ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå°†æ­¤æ–¹æ¡ˆåº”ç”¨äºç¡¬ä»¶é€»è¾‘è®¾è®¡è‡ªåŠ¨åŒ–ï¼Œåˆ©ç”¨ 7 ç§ä¸åŒçš„ LLMs å°† Logic Condition Tables (LCTs) è½¬åŒ–ä¸ºåŒ…å« 1500-2000 è¡Œä»£ç çš„äºŒç»´ç‰‡ä¸Šç½‘ç»œè·¯ç”±å™¨ Hardware Description Language (HDL) é€»è¾‘ã€‚é€šè¿‡å¯¹æ¯”åŸå§‹ LCTs ä¸ä»ç”Ÿæˆä»£ç ä¸­é‡å»ºçš„ LCTsï¼Œè¯¥æµç¨‹ä¸ä»…èƒ½ç¡®è®¤æ­£ç¡®çš„é€»è¾‘ç”Ÿæˆï¼Œè¿˜èƒ½ç²¾å‡†è¯†åˆ«é”™è¯¯å¹¶ååŠ©å¼€å‘è€…å‘ç°åŸå§‹è®¾è®¡è§„èŒƒä¸­çš„ç‘•ç–µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§é—­åˆå›è·¯çš„éªŒè¯æœºåˆ¶æ˜¾è‘—æå‡äº†ç¡¬ä»¶è®¾è®¡çš„ç”Ÿäº§åŠ›ï¼Œä¸ºç¼“è§£ LLMs åœ¨å¤æ‚å·¥ç¨‹ä»»åŠ¡ä¸­çš„å¯é æ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 2 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.03053v1",
      "published_date": "2025-11-25 00:47:02 UTC",
      "updated_date": "2025-11-25 00:47:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:50:20.440080+00:00"
    },
    {
      "arxiv_id": "2511.19808v1",
      "title": "Learning to Clean: Reinforcement Learning for Noisy Label Correction",
      "title_zh": "Learning to Cleanï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å™ªå£°æ ‡ç­¾çº æ­£",
      "authors": [
        "Marzi Heidari",
        "Hanping Zhang",
        "Yuhong Guo"
      ],
      "abstract": "The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ä¸­å™ªå£°æ ‡ç­¾ (Noisy Labels) ä¸¥é‡é™ä½æ¨¡å‹æ€§èƒ½çš„é—®é¢˜ï¼Œæå‡ºäº† Reinforcement Learning for Noisy Label Correction (RLNLC) æ¡†æ¶ã€‚RLNLC å°†å™ªå£°æ ‡ç­¾çº æ­£è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) é—®é¢˜ï¼Œé€šè¿‡å®šä¹‰åŒ…å«æ•°æ®åŠå…¶å…³è”æ ‡ç­¾çš„çŠ¶æ€ç©ºé—´ã€æ ‡ç­¾çº æ­£çš„åŠ¨ä½œç©ºé—´ä»¥åŠè¯„ä¼°çº æ­£æ•ˆæœçš„å¥–åŠ±æœºåˆ¶æ¥å®ç°ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ Actor-critic æ–¹æ³•è®­ç»ƒåŸºäºæ·±åº¦ç‰¹å¾è¡¨ç¤ºçš„ç­–ç•¥ç½‘ç»œï¼Œç”¨äºæ‰§è¡Œè‡ªåŠ¨åŒ–çš„æ ‡ç­¾çº æ­£ä»»åŠ¡ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥è¢«è¿­ä»£åº”ç”¨äºä¿®æ­£è®­ç»ƒé›†çš„å™ªå£°æ ‡ç­¾ï¼Œè¿›è€Œä¿ƒæˆæ›´é«˜è´¨é‡çš„é¢„æµ‹æ¨¡å‹è®­ç»ƒã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRLNLC åœ¨å¤„ç†å™ªå£°æ ‡ç­¾å­¦ä¹ ä»»åŠ¡æ—¶å§‹ç»ˆä¼˜äºç°æœ‰çš„å…ˆè¿›æŠ€æœ¯ (State-of-the-art)ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹é²æ£’æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19808v1",
      "published_date": "2025-11-25 00:32:03 UTC",
      "updated_date": "2025-11-25 00:32:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:50:34.944593+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 182,
  "processed_papers_count": 182,
  "failed_papers_count": 0,
  "llm_backup_calls": 4,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T10:53:18.795946+00:00"
}
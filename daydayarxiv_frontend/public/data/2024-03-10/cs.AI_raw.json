[
  {
    "arxiv_id": "2403.06332v2",
    "title": "Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups",
    "authors": [
      "Nelson Col√≥n Vargas"
    ],
    "abstract": "This paper explores the intricate relationship between capitalism, racial\ninjustice, and artificial intelligence (AI), arguing that AI acts as a\ncontemporary vehicle for age-old forms of exploitation. By linking historical\npatterns of racial and economic oppression with current AI practices, this\nstudy illustrates how modern technology perpetuates and deepens societal\ninequalities. It specifically examines how AI is implicated in the exploitation\nof marginalized communities through underpaid labor in the gig economy, the\nperpetuation of biases in algorithmic decision-making, and the reinforcement of\nsystemic barriers that prevent these groups from benefiting equitably from\ntechnological advances. Furthermore, the paper discusses the role of AI in\nextending and intensifying the social, economic, and psychological burdens\nfaced by these communities, highlighting the problematic use of AI in\nsurveillance, law enforcement, and mental health contexts. The analysis\nconcludes with a call for transformative changes in how AI is developed and\ndeployed. Advocating for a reevaluation of the values driving AI innovation,\nthe paper promotes an approach that integrates social justice and equity into\nthe core of technological design and policy. This shift is crucial for ensuring\nthat AI serves as a tool for societal improvement, fostering empowerment and\nhealing rather than deepening existing divides.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06332v2",
    "published_date": "2024-03-10 22:40:07 UTC",
    "updated_date": "2024-05-01 01:27:03 UTC"
  },
  {
    "arxiv_id": "2403.06326v1",
    "title": "From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification",
    "authors": [
      "Fei Wang",
      "Chao Shang",
      "Sarthak Jain",
      "Shuai Wang",
      "Qiang Ning",
      "Bonan Min",
      "Vittorio Castelli",
      "Yassine Benajiba",
      "Dan Roth"
    ],
    "abstract": "User alignment is crucial for adapting general-purpose language models (LMs)\nto downstream tasks, but human annotations are often not available for all\ntypes of instructions, especially those with customized constraints. We observe\nthat user instructions typically contain constraints. While assessing response\nquality in terms of the whole instruction is often costly, efficiently\nevaluating the satisfaction rate of constraints is feasible. We investigate\ncommon constraints in NLP tasks, categorize them into three classes based on\nthe types of their arguments, and propose a unified framework, ACT (Aligning to\nConsTraints), to automatically produce supervision signals for user alignment\nwith constraints. Specifically, ACT uses constraint verifiers, which are\ntypically easy to implement in practice, to compute constraint satisfaction\nrate (CSR) of each response. It samples multiple responses for each prompt and\ncollect preference labels based on their CSR automatically. Subsequently, ACT\nadapts the LM to the target task through a ranking-based learning process.\nExperiments on fine-grained entity typing, abstractive summarization, and\ntemporal question answering show that ACT is able to enhance LMs' capability to\nadhere to different classes of constraints, thereby improving task performance.\nFurther experiments show that the constraint-following capabilities are\ntransferable.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06326v1",
    "published_date": "2024-03-10 22:14:54 UTC",
    "updated_date": "2024-03-10 22:14:54 UTC"
  },
  {
    "arxiv_id": "2403.06322v2",
    "title": "Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility",
    "authors": [
      "Scott Siegel",
      "Jiaqing Zhang",
      "Sabyasachi Bandyopadhyay",
      "Subhash Nerella",
      "Brandon Silva",
      "Tezcan Baslanti",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "abstract": "Despite the importance of closely monitoring patients in the Intensive Care\nUnit (ICU), many aspects are still assessed in a limited manner due to the time\nconstraints imposed on healthcare providers. For example, although excessive\nvisitations during rest hours can potentially exacerbate the risk of circadian\nrhythm disruption and delirium, it is not captured in the ICU. Likewise, while\nmobility can be an important indicator of recovery or deterioration in ICU\npatients, it is only captured sporadically or not captured at all. In the past\nfew years, the computer vision field has found application in many domains by\nreducing the human burden. Using computer vision systems in the ICU can also\npotentially enable non-existing assessments or enhance the frequency and\naccuracy of existing assessments while reducing the staff workload. In this\nstudy, we leverage a state-of-the-art noninvasive computer vision system based\non depth imaging to characterize ICU visitations and patients' mobility. We\nthen examine the relationship between visitation and several patient outcomes,\nsuch as pain, acuity, and delirium. We found an association between\ndeteriorating patient acuity and the incidence of delirium with increased\nvisitations. In contrast, self-reported pain, reported using the Defense and\nVeteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.\nOur findings highlight the feasibility and potential of using noninvasive\nautonomous systems to monitor ICU patients.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06322v2",
    "published_date": "2024-03-10 21:43:47 UTC",
    "updated_date": "2024-07-12 14:43:01 UTC"
  },
  {
    "arxiv_id": "2403.06317v1",
    "title": "An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation",
    "authors": [
      "Soodeh Kalaie",
      "Andy Bulpitt",
      "Alejandro F. Frangi",
      "Ali Gooya"
    ],
    "abstract": "Generative modelling for shapes is a prerequisite for In-Silico Clinical\nTrials (ISCTs), which aim to cost-effectively validate medical device\ninterventions using synthetic anatomical shapes, often represented as 3D\nsurface meshes. However, constructing AI models to generate shapes closely\nresembling the real mesh samples is challenging due to variable vertex counts,\nconnectivities, and the lack of dense vertex-wise correspondences across the\ntraining data. Employing graph representations for meshes, we develop a novel\nunsupervised geometric deep-learning model to establish refinable shape\ncorrespondences in a latent space, construct a population-derived atlas and\ngenerate realistic synthetic shapes. We additionally extend our proposed base\nmodel to a joint shape generative-clustering multi-atlas framework to\nincorporate further variability and preserve more details in the generated\nshapes. Experimental results using liver and left-ventricular models\ndemonstrate the approach's applicability to computational medicine,\nhighlighting its suitability for ISCTs through a comparative analysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06317v1",
    "published_date": "2024-03-10 21:33:53 UTC",
    "updated_date": "2024-03-10 21:33:53 UTC"
  },
  {
    "arxiv_id": "2403.06313v1",
    "title": "Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning",
    "authors": [
      "Vikram Goddla"
    ],
    "abstract": "Deep reinforcement learning(DRL) has shown significant promise in a wide\nrange of applications including computer games and robotics. Yet, training DRL\npolicies consume extraordinary computing resources resulting in dense policies\nwhich are prone to overfitting. Moreover, inference with dense DRL policies\nlimit their practical applications, especially in edge computing. Techniques\nsuch as pruning and singular value decomposition have been used with deep\nlearning models to achieve sparsification and model compression to limit\noverfitting and reduce memory consumption. However, these techniques resulted\nin sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$\nregularization techniques have been proposed for neural network sparsification\nand sparse auto-encoder development, but their implementation in DRL\nenvironments has not been apparent. We propose a novel\n$L_0$-norm-regularization technique using an optimal sparsity map to sparsify\nDRL policies and promote their decomposition to a lower rank without decay in\nrewards. We evaluated our $L_0$-norm-regularization technique across five\ndifferent environments (Cartpole-v1, Acrobat-v1, LunarLander-v2,\nSuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and\noff-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL\npolicy in the SuperMarioBros environment achieved 93% sparsity and gained 70%\ncompression when subjected to low-rank decomposition, while significantly\noutperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL\npolicy in the Surgical Robot Learning environment achieved a 36% sparsification\nand gained 46% compression when decomposed to a lower rank, while being\nperformant. The results suggest that our custom $L_0$-norm-regularization\ntechnique for sparsification of DRL policies is a promising avenue to reduce\ncomputational resources and limit overfitting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06313v1",
    "published_date": "2024-03-10 21:18:54 UTC",
    "updated_date": "2024-03-10 21:18:54 UTC"
  },
  {
    "arxiv_id": "2403.06294v3",
    "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes",
    "authors": [
      "Shengxin Hong",
      "Liang Xiao",
      "Xin Zhang",
      "Jianxia Chen"
    ],
    "abstract": "There are two main barriers to using large language models (LLMs) in clinical\nreasoning. Firstly, while LLMs exhibit significant promise in Natural Language\nProcessing (NLP) tasks, their performance in complex reasoning and planning\nfalls short of expectations. Secondly, LLMs use uninterpretable methods to make\nclinical decisions that are fundamentally different from the clinician's\ncognitive processes. This leads to user distrust. In this paper, we present a\nmulti-agent framework called ArgMed-Agents, which aims to enable LLM-based\nagents to make explainable clinical decision reasoning through interaction.\nArgMed-Agents performs self-argumentation iterations via Argumentation Scheme\nfor Clinical Discussion (a reasoning mechanism for modeling cognitive processes\nin clinical reasoning), and then constructs the argumentation process as a\ndirected graph representing conflicting relationships. Ultimately, use symbolic\nsolver to identify a series of rational and coherent arguments to support\ndecision. We construct a formal model of ArgMed-Agents and present conjectures\nfor theoretical guarantees. ArgMed-Agents enables LLMs to mimic the process of\nclinical argumentative reasoning by generating explanations of reasoning in a\nself-directed manner. The setup experiments show that ArgMed-Agents not only\nimproves accuracy in complex clinical decision reasoning problems compared to\nother prompt methods, but more importantly, it provides users with decision\nexplanations that increase their confidence.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06294v3",
    "published_date": "2024-03-10 19:47:00 UTC",
    "updated_date": "2024-12-28 17:40:48 UTC"
  },
  {
    "arxiv_id": "2403.06289v1",
    "title": "Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning",
    "authors": [
      "Zijun Long",
      "Lipeng Zhuang",
      "George Killick",
      "Richard McCreadie",
      "Gerardo Aragon Camarasa",
      "Paul Henderson"
    ],
    "abstract": "Human-annotated vision datasets inevitably contain a fraction of human\nmislabelled examples. While the detrimental effects of such mislabelling on\nsupervised learning are well-researched, their influence on Supervised\nContrastive Learning (SCL) remains largely unexplored. In this paper, we show\nthat human-labelling errors not only differ significantly from synthetic label\nerrors, but also pose unique challenges in SCL, different to those in\ntraditional supervised learning methods. Specifically, our results indicate\nthey adversely impact the learning process in the ~99% of cases when they occur\nas false positive samples. Existing noise-mitigating methods primarily focus on\nsynthetic label errors and tackle the unrealistic setting of very high\nsynthetic noise rates (40-80%), but they often underperform on common image\ndatasets due to overfitting. To address this issue, we introduce a novel SCL\nobjective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is\ndesigned to mitigate the effects of real-world mislabelled examples, typically\ncharacterized by much lower noise rates (<5%). We demonstrate that SCL-RHE\nconsistently outperforms state-of-the-art representation learning and\nnoise-mitigating methods across various vision benchmarks, by offering improved\nresilience against human-labelling errors.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2311.16481",
    "pdf_url": "http://arxiv.org/pdf/2403.06289v1",
    "published_date": "2024-03-10 19:05:12 UTC",
    "updated_date": "2024-03-10 19:05:12 UTC"
  },
  {
    "arxiv_id": "2403.06275v1",
    "title": "UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation",
    "authors": [
      "Kwanyoung Kim",
      "Jaa-Yeon Lee",
      "Jong Chul Ye"
    ],
    "abstract": "Nakagami imaging holds promise for visualizing and quantifying tissue\nscattering in ultrasound waves, with potential applications in tumor diagnosis\nand fat fraction estimation which are challenging to discern by conventional\nultrasound B-mode images. Existing methods struggle with optimal window size\nselection and suffer from estimator instability, leading to degraded resolution\nimages. To address this, here we propose a novel method called UNICORN\n(Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an\naccurate, closed-form estimator for Nakagami parameter estimation in terms of\nthe score function of ultrasonic envelope. Extensive experiments using\nsimulation and real ultrasound RF data demonstrate UNICORN's superiority over\nconventional approaches in accuracy and resolution quality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 5 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.06275v1",
    "published_date": "2024-03-10 18:05:41 UTC",
    "updated_date": "2024-03-10 18:05:41 UTC"
  },
  {
    "arxiv_id": "2403.06268v1",
    "title": "Physics-Guided Abnormal Trajectory Gap Detection",
    "authors": [
      "Arun Sharma",
      "Shashi Shekhar"
    ],
    "abstract": "Given trajectories with gaps (i.e., missing data), we investigate algorithms\nto identify abnormal gaps in trajectories which occur when a given moving\nobject did not report its location, but other moving objects in the same\ngeographic region periodically did. The problem is important due to its\nsocietal applications, such as improving maritime safety and regulatory\nenforcement for global security concerns such as illegal fishing, illegal oil\ntransfers, and trans-shipments. The problem is challenging due to the\ndifficulty of bounding the possible locations of the moving object during a\ntrajectory gap, and the very high computational cost of detecting gaps in such\na large volume of location data. The current literature on anomalous trajectory\ndetection assumes linear interpolation within gaps, which may not be able to\ndetect abnormal gaps since objects within a given region may have traveled away\nfrom their shortest path. In preliminary work, we introduced an abnormal gap\nmeasure that uses a classical space-time prism model to bound an object's\npossible movement during the trajectory gap and provided a scalable memoized\ngap detection algorithm (Memo-AGD). In this paper, we propose a Space\nTime-Aware Gap Detection (STAGD) approach to leverage space-time indexing and\nmerging of trajectory gaps. We also incorporate a Dynamic Region Merge-based\n(DRM) approach to efficiently compute gap abnormality scores. We provide\ntheoretical proofs that both algorithms are correct and complete and also\nprovide analysis of asymptotic time complexity. Experimental results on\nsynthetic and real-world maritime trajectory data show that the proposed\napproach substantially improves computation time over the baseline technique.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CG",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06268v1",
    "published_date": "2024-03-10 17:07:28 UTC",
    "updated_date": "2024-03-10 17:07:28 UTC"
  },
  {
    "arxiv_id": "2403.06267v1",
    "title": "FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System to Assist Human Labelers' Preference Elicitation",
    "authors": [
      "Hanfang Lyu",
      "Yuanchen Bai",
      "Xin Liang",
      "Ujaan Das",
      "Chuhan Shi",
      "Leiliang Gong",
      "Yingchi Li",
      "Mingfei Sun",
      "Ming Ge",
      "Xiaojuan Ma"
    ],
    "abstract": "Preference-based learning aims to align robot task objectives with human\nvalues. One of the most common methods to infer human preferences is by\npairwise comparisons of robot task trajectories. Traditional comparison-based\npreference labeling systems seldom support labelers to digest and identify\ncritical differences between complex trajectories recorded in videos. Our\nformative study (N = 12) suggests that individuals may overlook non-salient\ntask features and establish biased preference criteria during their preference\nelicitation process because of partial observations. In addition, they may\nexperience mental fatigue when given many pairs to compare, causing their label\nquality to deteriorate. To mitigate these issues, we propose FARPLS, a\nFeature-Augmented Robot trajectory Preference Labeling System. FARPLS\nhighlights potential outliers in a wide variety of task features that matter to\nhumans and extracts the corresponding video keyframes for easy review and\ncomparison. It also dynamically adjusts the labeling order according to users'\nfamiliarities, difficulties of the trajectory pair, and level of disagreements.\nAt the same time, the system monitors labelers' consistency and provides\nfeedback on labeling progress to keep labelers engaged. A between-subjects\nstudy (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows\nthat FARPLS can help users establish preference criteria more easily and notice\nmore relevant details in the presented trajectories than the conventional\ninterface. FARPLS also improves labeling consistency and engagement, mitigating\nchallenges in preference elicitation without raising cognitive loads\nsignificantly",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA",
    "pdf_url": "http://arxiv.org/pdf/2403.06267v1",
    "published_date": "2024-03-10 17:07:20 UTC",
    "updated_date": "2024-03-10 17:07:20 UTC"
  },
  {
    "arxiv_id": "2403.06265v2",
    "title": "Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance",
    "authors": [
      "Omer Goldman",
      "Avi Caciularu",
      "Matan Eyal",
      "Kris Cao",
      "Idan Szpektor",
      "Reut Tsarfaty"
    ],
    "abstract": "Despite it being the cornerstone of BPE, the most common tokenization\nalgorithm, the importance of compression in the tokenization process is still\nunclear. In this paper, we argue for the theoretical importance of compression,\nthat can be viewed as 0-gram language modeling where equal probability is\nassigned to all tokens. We also demonstrate the empirical importance of\ncompression for downstream success of pre-trained language models. We control\nthe compression ability of several BPE tokenizers by varying the amount of\ndocuments available during their training: from 1 million documents to a\ncharacter-based tokenizer equivalent to no training data at all. We then\npre-train English language models based on those tokenizers and fine-tune them\nover several tasks. We show that there is a correlation between tokenizers'\ncompression and models' downstream performance, suggesting that compression is\na reliable intrinsic indicator of tokenization quality. These correlations are\nmore pronounced for generation tasks (over classification) or for smaller\nmodels (over large ones). We replicated a representative part of our\nexperiments on Turkish and found similar results, confirming that our results\nhold for languages with typological characteristics dissimilar to English. We\nconclude that building better compressing tokenizers is a fruitful avenue for\nfurther research and for improving overall model performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024, Findings",
    "pdf_url": "http://arxiv.org/pdf/2403.06265v2",
    "published_date": "2024-03-10 17:02:53 UTC",
    "updated_date": "2024-06-22 16:00:49 UTC"
  },
  {
    "arxiv_id": "2403.06259v2",
    "title": "Editing Conceptual Knowledge for Large Language Models",
    "authors": [
      "Xiaohan Wang",
      "Shengyu Mao",
      "Ningyu Zhang",
      "Shumin Deng",
      "Yunzhi Yao",
      "Yue Shen",
      "Lei Liang",
      "Jinjie Gu",
      "Huajun Chen"
    ],
    "abstract": "Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings; Code: https://github.com/zjunlp/EasyEdit\n  Dataset: https://huggingface.co/datasets/zjunlp/ConceptEdit",
    "pdf_url": "http://arxiv.org/pdf/2403.06259v2",
    "published_date": "2024-03-10 16:57:10 UTC",
    "updated_date": "2024-10-06 15:45:21 UTC"
  },
  {
    "arxiv_id": "2403.06247v2",
    "title": "Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation",
    "authors": [
      "Mingyu Lee",
      "Jongwon Choi"
    ],
    "abstract": "We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, Accepted to CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06247v2",
    "published_date": "2024-03-10 16:11:17 UTC",
    "updated_date": "2024-03-26 14:42:21 UTC"
  },
  {
    "arxiv_id": "2403.07944v1",
    "title": "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs",
    "authors": [
      "Deshun Yang",
      "Luhui Hu",
      "Yu Tian",
      "Zihao Li",
      "Chris Kelly",
      "Bang Yang",
      "Cindy Yang",
      "Yuexian Zou"
    ],
    "abstract": "Several text-to-video diffusion models have demonstrated commendable\ncapabilities in synthesizing high-quality video content. However, it remains a\nformidable challenge pertaining to maintaining temporal consistency and\nensuring action smoothness throughout the generated sequences. In this paper,\nwe present an innovative video generation AI agent that harnesses the power of\nSora-inspired multimodal learning to build skilled world models framework based\non textual prompts and accompanying images. The framework includes two parts:\nprompt enhancer and full video translation. The first part employs the\ncapabilities of ChatGPT to meticulously distill and proactively construct\nprecise prompts for each subsequent step, thereby guaranteeing the utmost\naccuracy in prompt communication and accurate execution in following model\noperations. The second part employ compatible with existing advanced diffusion\ntechniques to expansively generate and refine the key frame at the conclusion\nof a video. Then we can expertly harness the power of leading and trailing key\nframes to craft videos with enhanced temporal consistency and action\nsmoothness. The experimental results confirm that our method has strong\neffectiveness and novelty in constructing world models from text and image\ninputs over the other methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.07944v1",
    "published_date": "2024-03-10 16:09:02 UTC",
    "updated_date": "2024-03-10 16:09:02 UTC"
  },
  {
    "arxiv_id": "2403.06239v1",
    "title": "Cooperative Classification and Rationalization for Graph Generalization",
    "authors": [
      "Linan Yue",
      "Qi Liu",
      "Ye Liu",
      "Weibo Gao",
      "Fangzhou Yao",
      "Wenfeng Li"
    ],
    "abstract": "Graph Neural Networks (GNNs) have achieved impressive results in graph\nclassification tasks, but they struggle to generalize effectively when faced\nwith out-of-distribution (OOD) data. Several approaches have been proposed to\naddress this problem. Among them, one solution is to diversify training\ndistributions in vanilla classification by modifying the data environment, yet\naccessing the environment information is complex. Besides, another promising\napproach involves rationalization, extracting invariant rationales for\npredictions. However, extracting rationales is difficult due to limited\nlearning signals, resulting in less accurate rationales and diminished\npredictions. To address these challenges, in this paper, we propose a\nCooperative Classification and Rationalization (C2R) method, consisting of the\nclassification and the rationalization module. Specifically, we first assume\nthat multiple environments are available in the classification module. Then, we\nintroduce diverse training distributions using an environment-conditional\ngenerative network, enabling robust graph representations. Meanwhile, the\nrationalization module employs a separator to identify relevant rationale\nsubgraphs while the remaining non-rationale subgraphs are de-correlated with\nlabels. Next, we align graph representations from the classification module\nwith rationale subgraph representations using the knowledge distillation\nmethods, enhancing the learning signal for rationales. Finally, we infer\nmultiple environments by gathering non-rationale representations and\nincorporate them into the classification module for cooperative learning.\nExtensive experimental results on both benchmarks and synthetic datasets\ndemonstrate the effectiveness of C2R. Code is available at\nhttps://github.com/yuelinan/Codes-of-C2R.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to WWW 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06239v1",
    "published_date": "2024-03-10 15:38:20 UTC",
    "updated_date": "2024-03-10 15:38:20 UTC"
  },
  {
    "arxiv_id": "2403.06235v1",
    "title": "Probabilistic Neural Circuits",
    "authors": [
      "Pedro Zuidberg Dos Martires"
    ],
    "abstract": "Probabilistic circuits (PCs) have gained prominence in recent years as a\nversatile framework for discussing probabilistic models that support tractable\nqueries and are yet expressive enough to model complex probability\ndistributions. Nevertheless, tractability comes at a cost: PCs are less\nexpressive than neural networks. In this paper we introduce probabilistic\nneural circuits (PNCs), which strike a balance between PCs and neural nets in\nterms of tractability and expressive power. Theoretically, we show that PNCs\ncan be interpreted as deep mixtures of Bayesian networks. Experimentally, we\ndemonstrate that PNCs constitute powerful function approximators.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2403.06235v1",
    "published_date": "2024-03-10 15:25:49 UTC",
    "updated_date": "2024-03-10 15:25:49 UTC"
  },
  {
    "arxiv_id": "2403.06225v2",
    "title": "MoST: Motion Style Transformer between Diverse Action Contents",
    "authors": [
      "Boeun Kim",
      "Jungho Kim",
      "Hyung Jin Chang",
      "Jin Young Choi"
    ],
    "abstract": "While existing motion style transfer methods are effective between two\nmotions with identical content, their performance significantly diminishes when\ntransferring style between motions with different contents. This challenge lies\nin the lack of clear separation between content and style of a motion. To\ntackle this challenge, we propose a novel motion style transformer that\neffectively disentangles style from content and generates a plausible motion\nwith transferred style from a source motion. Our distinctive approach to\nachieving the goal of disentanglement is twofold: (1) a new architecture for\nmotion style transformer with `part-attentive style modulator across body\nparts' and `Siamese encoders that encode style and content features\nseparately'; (2) style disentanglement loss. Our method outperforms existing\nmethods and demonstrates exceptionally high quality, particularly in motion\npairs with different contents, without the need for heuristic post-processing.\nCodes are available at https://github.com/Boeun-Kim/MoST.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06225v2",
    "published_date": "2024-03-10 14:11:25 UTC",
    "updated_date": "2024-03-20 10:05:02 UTC"
  },
  {
    "arxiv_id": "2403.06221v1",
    "title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
    "authors": [
      "Ruiwen Zhou",
      "Yingxuan Yang",
      "Muning Wen",
      "Ying Wen",
      "Wenhao Wang",
      "Chunling Xi",
      "Guoqiang Xu",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Numerous large language model (LLM) agents have been built for different\ntasks like web navigation and online shopping due to LLM's wide knowledge and\ntext-understanding ability. Among these works, many of them utilize in-context\nexamples to achieve generalization without the need for fine-tuning, while few\nof them have considered the problem of how to select and effectively utilize\nthese examples. Recently, methods based on trajectory-level retrieval with task\nmeta-data and using trajectories as in-context examples have been proposed to\nimprove the agent's overall performance in some sequential decision making\ntasks. However, these methods can be problematic due to plausible examples\nretrieved without task-specific state transition dynamics and long input with\nplenty of irrelevant context. In this paper, we propose a novel framework\n(TRAD) to address these issues. TRAD first conducts Thought Retrieval,\nachieving step-level demonstration selection via thought matching, leading to\nmore helpful demonstrations and less irrelevant input noise. Then, TRAD\nintroduces Aligned Decision, complementing retrieved demonstration steps with\ntheir previous or subsequent steps, which enables tolerance for imperfect\nthought and provides a choice for balance between more context and less noise.\nExtensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not\nonly outperforms state-of-the-art models but also effectively helps in reducing\nnoise and promoting generalization. Furthermore, TRAD has been deployed in\nreal-world scenarios of a global business insurance company and improves the\nsuccess rate of robotic process automation.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "Codes available at: https://github.com/skyriver-2000/TRAD-Official",
    "pdf_url": "http://arxiv.org/pdf/2403.06221v1",
    "published_date": "2024-03-10 13:58:38 UTC",
    "updated_date": "2024-03-10 13:58:38 UTC"
  },
  {
    "arxiv_id": "2403.06213v1",
    "title": "$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections",
    "authors": [
      "Roy Miles",
      "Ismail Elezi",
      "Jiankang Deng"
    ],
    "abstract": "Knowledge distillation is an effective method for training small and\nefficient deep learning models. However, the efficacy of a single method can\ndegenerate when transferring to other tasks, modalities, or even other\narchitectures. To address this limitation, we propose a novel constrained\nfeature distillation method. This method is derived from a small set of core\nprinciples, which results in two emerging components: an orthogonal projection\nand a task-specific normalisation. Equipped with both of these components, our\ntransformer models can outperform all previous methods on ImageNet and reach up\nto a 4.4% relative improvement over the previous state-of-the-art methods. To\nfurther demonstrate the generality of our method, we apply it to object\ndetection and image generation, whereby we obtain consistent and substantial\nperformance improvements over state-of-the-art. Code and models are publicly\navailable: https://github.com/roymiles/vkd",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Code available at https://github.com/roymiles/vkd",
    "pdf_url": "http://arxiv.org/pdf/2403.06213v1",
    "published_date": "2024-03-10 13:26:24 UTC",
    "updated_date": "2024-03-10 13:26:24 UTC"
  },
  {
    "arxiv_id": "2403.06206v1",
    "title": "Limit of the Maximum Random Permutation Set Entropy",
    "authors": [
      "Jiefeng Zhou",
      "Zhen Li",
      "Kang Hao Cheong",
      "Yong Deng"
    ],
    "abstract": "The Random Permutation Set (RPS) is a new type of set proposed recently,\nwhich can be regarded as the generalization of evidence theory. To measure the\nuncertainty of RPS, the entropy of RPS and its corresponding maximum entropy\nhave been proposed. Exploring the maximum entropy provides a possible way of\nunderstanding the physical meaning of RPS. In this paper, a new concept, the\nenvelope of entropy function, is defined. In addition, the limit of the\nenvelope of RPS entropy is derived and proved. Compared with the existing\nmethod, the computational complexity of the proposed method to calculate the\nenvelope of RPS entropy decreases greatly. The result shows that when $N \\to\n\\infty$, the limit form of the envelope of the entropy of RPS converges to $e\n\\times (N!)^2$, which is highly connected to the constant $e$ and factorial.\nFinally, numerical examples validate the efficiency and conciseness of the\nproposed envelope, which provides a new insight into the maximum entropy\nfunction.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "22 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.06206v1",
    "published_date": "2024-03-10 13:04:09 UTC",
    "updated_date": "2024-03-10 13:04:09 UTC"
  },
  {
    "arxiv_id": "2403.06201v1",
    "title": "Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!",
    "authors": [
      "Huanqi Yang",
      "Sijie Ji",
      "Rucheng Wu",
      "Weitao Xu"
    ],
    "abstract": "There is a burgeoning discussion around the capabilities of Large Language\nModels (LLMs) in acting as fundamental components that can be seamlessly\nincorporated into Artificial Intelligence of Things (AIoT) to interpret complex\ntrajectories. This study introduces LLMTrack, a model that illustrates how LLMs\ncan be leveraged for Zero-Shot Trajectory Recognition by employing a novel\nsingle-prompt technique that combines role-play and think step-by-step\nmethodologies with unprocessed Inertial Measurement Unit (IMU) data. We\nevaluate the model using real-world datasets designed to challenge it with\ndistinct trajectories characterized by indoor and outdoor scenarios. In both\ntest scenarios, LLMTrack not only meets but exceeds the performance benchmarks\nset by traditional machine learning approaches and even contemporary\nstate-of-the-art deep learning models, all without the requirement of training\non specialized datasets. The results of our research suggest that, with\nstrategically designed prompts, LLMs can tap into their extensive knowledge\nbase and are well-equipped to analyze raw sensor data with remarkable\neffectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06201v1",
    "published_date": "2024-03-10 12:50:35 UTC",
    "updated_date": "2024-03-10 12:50:35 UTC"
  },
  {
    "arxiv_id": "2403.06174v1",
    "title": "Domain Adversarial Active Learning for Domain Generalization Classification",
    "authors": [
      "Jianting Chen",
      "Ling Ding",
      "Yunxiao Yang",
      "Zaiyuan Di",
      "Yang Xiang"
    ],
    "abstract": "Domain generalization models aim to learn cross-domain knowledge from source\ndomain data, to improve performance on unknown target domains. Recent research\nhas demonstrated that diverse and rich source domain samples can enhance domain\ngeneralization capability. This paper argues that the impact of each sample on\nthe model's generalization ability varies. Despite its small scale, a\nhigh-quality dataset can still attain a certain level of generalization\nability. Motivated by this, we propose a domain-adversarial active learning\n(DAAL) algorithm for classification tasks in domain generalization. First, we\nanalyze that the objective of tasks is to maximize the inter-class distance\nwithin the same domain and minimize the intra-class distance across different\ndomains. To achieve this objective, we design a domain adversarial selection\nmethod that prioritizes challenging samples. Second, we posit that even in a\nconverged model, there are subsets of features that lack discriminatory power\nwithin each domain. We attempt to identify these feature subsets and optimize\nthem by a constraint loss. We validate and analyze our DAAL algorithm on\nmultiple domain generalization datasets, comparing it with various domain\ngeneralization algorithms and active learning algorithms. Our results\ndemonstrate that the DAAL algorithm can achieve strong generalization ability\nwith fewer data resources, thereby reducing data annotation costs in domain\ngeneralization tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06174v1",
    "published_date": "2024-03-10 10:59:22 UTC",
    "updated_date": "2024-03-10 10:59:22 UTC"
  },
  {
    "arxiv_id": "2403.06168v2",
    "title": "DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation",
    "authors": [
      "Xiaobin Hu",
      "Xu Peng",
      "Donghao Luo",
      "Xiaozhong Ji",
      "Jinlong Peng",
      "Zhengkai Jiang",
      "Jiangning Zhang",
      "Taisong Jin",
      "Chengjie Wang",
      "Rongrong Ji"
    ],
    "abstract": "Due to the difficulty and labor-consuming nature of getting highly accurate\nor matting annotations, there only exists a limited amount of highly accurate\nlabels available to the public. To tackle this challenge, we propose a\nDiffuMatting which inherits the strong Everything generation ability of\ndiffusion and endows the power of \"matting anything\". Our DiffuMatting can 1).\nact as an anything matting factory with high accurate annotations 2). be\nwell-compatible with community LoRAs or various conditional control approaches\nto achieve the community-friendly art design and controllable generation.\nSpecifically, inspired by green-screen-matting, we aim to teach the diffusion\nmodel to paint on a fixed green screen canvas. To this end, a large-scale\ngreenscreen dataset (Green100K) is collected as a training dataset for\nDiffuMatting. Secondly, a green background control loss is proposed to keep the\ndrawing board as a pure green color to distinguish the foreground and\nbackground. To ensure the synthesized object has more edge details, a\ndetailed-enhancement of transition boundary loss is proposed as a guideline to\ngenerate objects with more complicated edge structures. Aiming to\nsimultaneously generate the object and its matting annotation, we build a\nmatting head to make a green color removal in the latent space of the VAE\ndecoder. Our DiffuMatting shows several potential applications (e.g.,\nmatting-data generator, community-friendly art design and controllable\ngeneration). As a matting-data generator, DiffuMatting synthesizes general\nobject and portrait matting sets, effectively reducing the relative MSE error\nby 15.4% in General Object Matting and 11.4% in Portrait Matting tasks. The\ndataset is released in our project page at\n\\url{https://diffumatting.github.io}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper was accepted by ECCV 2024, and the project page is\n  accessible at: \\url{https://diffumatting.github.io}",
    "pdf_url": "http://arxiv.org/pdf/2403.06168v2",
    "published_date": "2024-03-10 10:39:32 UTC",
    "updated_date": "2024-08-21 11:35:15 UTC"
  },
  {
    "arxiv_id": "2404.15243v1",
    "title": "UCINet0: A Machine Learning based Receiver for 5G NR PUCCH Format 0",
    "authors": [
      "Anil Kumar Yerrapragada",
      "Jeeva Keshav Sattianarayanin",
      "Radha Krishna Ganti"
    ],
    "abstract": "Accurate decoding of Uplink Control Information (UCI) on the Physical Uplink\nControl Channel (PUCCH) is essential for enabling 5G wireless links. This paper\nexplores an AI/ML-based receiver design for PUCCH Format 0. Format 0 signaling\nencodes the UCI content within the phase of a known base waveform and even\nsupports multiplexing of up to 12 users within the same time-frequency\nresources. Our first-of-a-kind neural network classifier, which we term\nUCINet0, is capable of predicting when no user is transmitting on the PUCCH, as\nwell as decoding the UCI content of any number of multiplexed users, up to 12.\nInference results with both simulated and hardware-captured field datasets show\nthat the UCINet0 model outperforms conventional DFT-based decoders across all\nSNR ranges.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15243v1",
    "published_date": "2024-03-10 09:56:02 UTC",
    "updated_date": "2024-03-10 09:56:02 UTC"
  },
  {
    "arxiv_id": "2403.06149v2",
    "title": "Can Large Language Models Automatically Score Proficiency of Written Essays?",
    "authors": [
      "Watheq Mansour",
      "Salam Albatarni",
      "Sohaila Eltanbouly",
      "Tamer Elsayed"
    ],
    "abstract": "Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "V2 (published version of LREC-COLING 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.06149v2",
    "published_date": "2024-03-10 09:39:00 UTC",
    "updated_date": "2024-04-16 00:24:55 UTC"
  },
  {
    "arxiv_id": "2403.06145v1",
    "title": "All-in-one platform for AI R&D in medical imaging, encompassing data collection, selection, annotation, and pre-processing",
    "authors": [
      "Changhee Han",
      "Kyohei Shibano",
      "Wataru Ozaki",
      "Keishiro Osaki",
      "Takafumi Haraguchi",
      "Daisuke Hirahara",
      "Shumon Kimura",
      "Yasuyuki Kobayashi",
      "Gento Mogi"
    ],
    "abstract": "Deep Learning is advancing medical imaging Research and Development (R&D),\nleading to the frequent clinical use of Artificial Intelligence/Machine\nLearning (AI/ML)-based medical devices. However, to advance AI R&D, two\nchallenges arise: 1) significant data imbalance, with most data from\nEurope/America and under 10% from Asia, despite its 60% global population\nshare; and 2) hefty time and investment needed to curate proprietary datasets\nfor commercial use. In response, we established the first commercial medical\nimaging platform, encompassing steps like: 1) data collection, 2) data\nselection, 3) annotation, and 4) pre-processing. Moreover, we focus on\nharnessing under-represented data from Japan and broader Asia, including\nComputed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.\nUsing the collected data, we are preparing/providing ready-to-use datasets for\nmedical AI R&D by 1) offering these datasets to AI firms, biopharma, and\nmedical device makers and 2) using them as training/test data to develop\ntailored AI solutions for such entities. We also aim to merge Blockchain for\ndata security and plan to synthesize rare disease data via generative AI.\nDataHub Website: https://medical-datahub.ai/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures, accepted to SPIE Medical Imaging 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06145v1",
    "published_date": "2024-03-10 09:24:53 UTC",
    "updated_date": "2024-03-10 09:24:53 UTC"
  },
  {
    "arxiv_id": "2403.06143v1",
    "title": "Fluent: Round-efficient Secure Aggregation for Private Federated Learning",
    "authors": [
      "Xincheng Li",
      "Jianting Ning",
      "Geong Sen Poh",
      "Leo Yu Zhang",
      "Xinchun Yin",
      "Tianwei Zhang"
    ],
    "abstract": "Federated learning (FL) facilitates collaborative training of machine\nlearning models among a large number of clients while safeguarding the privacy\nof their local datasets. However, FL remains susceptible to vulnerabilities\nsuch as privacy inference and inversion attacks. Single-server secure\naggregation schemes were proposed to address these threats. Nonetheless, they\nencounter practical constraints due to their round and communication\ncomplexities. This work introduces Fluent, a round and communication-efficient\nsecure aggregation scheme for private FL. Fluent has several improvements\ncompared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et\nal. (SP 2023): (1) it eliminates frequent handshakes and secret sharing\noperations by efficiently reusing the shares across multiple training\niterations without leaking any private information; (2) it accomplishes both\nthe consistency check and gradient unmasking in one logical step, thereby\nreducing another round of communication. With these innovations, Fluent\nachieves the fewest communication rounds (i.e., two in the collection phase) in\nthe malicious server setting, in contrast to at least three rounds in existing\nschemes. This significantly minimizes the latency for geographically\ndistributed clients; (3) Fluent also introduces Fluent-Dynamic with a\nparticipant selection algorithm and an alternative secret sharing scheme. This\ncan facilitate dynamic client joining and enhance the system flexibility and\nscalability. We implemented Fluent and compared it with existing solutions.\nExperimental results show that Fluent improves the computational cost by at\nleast 75% and communication overhead by at least 25% for normal clients. Fluent\nalso reduces the communication overhead for the server at the expense of a\nmarginal increase in computational cost.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06143v1",
    "published_date": "2024-03-10 09:11:57 UTC",
    "updated_date": "2024-03-10 09:11:57 UTC"
  },
  {
    "arxiv_id": "2403.06139v1",
    "title": "Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity",
    "authors": [
      "Xin Zhang",
      "Linhai Zhang",
      "Deyu Zhou",
      "Guoqiang Xu"
    ],
    "abstract": "Due to the sparsity of user data, sentiment analysis on user reviews in\ne-commerce platforms often suffers from poor performance, especially when faced\nwith extremely sparse user data or long-tail labels. Recently, the emergence of\nLLMs has introduced new solutions to such problems by leveraging graph\nstructures to generate supplementary user profiles. However, previous\napproaches have not fully utilized the graph understanding capabilities of LLMs\nand have struggled to adapt to complex streaming data environments. In this\nwork, we propose a fine-grained streaming data synthesis framework that\ncategorizes sparse users into three categories: Mid-tail, Long-tail, and\nExtreme. Specifically, we design LLMs to comprehensively understand three key\ngraph elements in streaming data, including Local-global Graph Understanding,\nSecond-Order Relationship Extraction, and Product Attribute Understanding,\nwhich enables the generation of high-quality synthetic data to effectively\naddress sparsity across different categories. Experimental results on three\nreal datasets demonstrate significant performance improvements, with\nsynthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,\nrespectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06139v1",
    "published_date": "2024-03-10 08:59:04 UTC",
    "updated_date": "2024-03-10 08:59:04 UTC"
  },
  {
    "arxiv_id": "2403.06135v1",
    "title": "MACE: Mass Concept Erasure in Diffusion Models",
    "authors": [
      "Shilin Lu",
      "Zilan Wang",
      "Leyang Li",
      "Yanzhu Liu",
      "Adams Wai-Kin Kong"
    ],
    "abstract": "The rapid expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns regarding their potential misuse in creating harmful or\nmisleading content. In this paper, we introduce MACE, a finetuning framework\nfor the task of mass concept erasure. This task aims to prevent models from\ngenerating images that embody unwanted concepts when prompted. Existing concept\nerasure methods are typically restricted to handling fewer than five concepts\nsimultaneously and struggle to find a balance between erasing concept synonyms\n(generality) and maintaining unrelated concepts (specificity). In contrast,\nMACE differs by successfully scaling the erasure scope up to 100 concepts and\nby achieving an effective balance between generality and specificity. This is\nachieved by leveraging closed-form cross-attention refinement along with LoRA\nfinetuning, collectively eliminating the information of undesirable concepts.\nFurthermore, MACE integrates multiple LoRAs without mutual interference. We\nconduct extensive evaluations of MACE against prior methods across four\ndifferent tasks: object erasure, celebrity erasure, explicit content erasure,\nand artistic style erasure. Our results reveal that MACE surpasses prior\nmethods in all evaluated tasks. Code is available at\nhttps://github.com/Shilin-LU/MACE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06135v1",
    "published_date": "2024-03-10 08:50:56 UTC",
    "updated_date": "2024-03-10 08:50:56 UTC"
  },
  {
    "arxiv_id": "2403.06131v2",
    "title": "FewFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning",
    "authors": [
      "Zhuo Zhang",
      "Jingyuan Zhang",
      "Jintao Huang",
      "Lizhen Qu",
      "Hongzhi Zhang",
      "Qifan Wang",
      "Xun Zhou",
      "Zenglin Xu"
    ],
    "abstract": "Instruction tuning has been identified as a crucial technique for optimizing\nthe performance of large language models (LLMs) in generating human-aligned\nresponses. Nonetheless, gathering diversified and superior-quality instruction\ndata for such tuning presents notable obstacles, especially in domains with\nrigid privacy provisions. Federated instruction tuning (FedIT) has emerged as a\npromising solution, by consolidating collaborative training across multiple\ndata owners, thereby resulting in a privacy-preserving learning model. However,\nFedIT encounters limitations such as scarcity of instructional data and risk of\nexposure to training data extraction attacks. In this paper, we propose a novel\nfederated algorithm, FewFedPIT, designed to simultaneously enhance privacy\nprotection and model performance of federated few-shot learning.\nFewFedPITcomprises three vital components on the client side: (1) synthetic\ndata generation, which utilizes LLMs' in-context learning capacity to generate\nsynthetic data autonomously, thus expanding the local database; (2) parameter\nisolation training, which individually updates the public parameters in the\nsynthetic data and the private parameters in the local data, consequently\nmitigating the noise impact of the synthetic data; (3) local aggregation\nsharing, which mixes public and private parameters before uploading,\neffectively preventing data extraction attacks. Extensive experiments on three\nopen-source datasets demonstrate the effectiveness of FewFedPITin, enhancing\nprivacy preservation and improving federated few-shot performance.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2403.06131v2",
    "published_date": "2024-03-10 08:41:22 UTC",
    "updated_date": "2024-06-20 13:00:18 UTC"
  },
  {
    "arxiv_id": "2403.06115v1",
    "title": "FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language",
    "authors": [
      "Yayue Deng",
      "Mohan Xu",
      "Yao Tang"
    ],
    "abstract": "The effectiveness of central bank communication is a crucial aspect of\nmonetary policy transmission. While recent research has examined the influence\nof policy communication by the chairs of the Federal Reserve on various\nfinancial variables, much of the literature relies on rule-based or\ndictionary-based methods in parsing the language of the chairs, leaving nuanced\ninformation about policy stance contained in nonverbal emotion out of the\nanalysis. In the current study, we propose the Fine-Grained Monetary Policy\nAnalysis Framework (FMPAF), a novel approach that integrates large language\nmodels (LLMs) with regression analysis to provide a comprehensive analysis of\nthe impact of the press-conference communications of chairs of the Federal\nReserve on financial markets. We conduct extensive comparisons of model\nperformance under different levels of granularity, modalities, and\ncommunication scenarios. Based on our preferred specification, a one-unit\nincrease in the sentiment score is associated with an increase of the price of\nS\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a\n15-basis-point decrease in the policy interest rate, while not leading to a\nsignificant response in exchange rates.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by AAAI 2024 Workshop: AI in Finance for Social Impact",
    "pdf_url": "http://arxiv.org/pdf/2403.06115v1",
    "published_date": "2024-03-10 07:21:31 UTC",
    "updated_date": "2024-03-10 07:21:31 UTC"
  },
  {
    "arxiv_id": "2403.06108v2",
    "title": "Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning",
    "authors": [
      "Kaipeng Wang",
      "Zhi Jing",
      "Yongye Su",
      "Yikun Han"
    ],
    "abstract": "This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06108v2",
    "published_date": "2024-03-10 06:30:54 UTC",
    "updated_date": "2024-04-09 16:38:01 UTC"
  },
  {
    "arxiv_id": "2403.06097v2",
    "title": "Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery",
    "authors": [
      "Yuxuan Yao",
      "Sichun Luo",
      "Haohan Zhao",
      "Guanzhi Deng",
      "Linqi Song"
    ],
    "abstract": "We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by TheWebConf'24 (WWW'24) as a Resource Paper",
    "pdf_url": "http://arxiv.org/pdf/2403.06097v2",
    "published_date": "2024-03-10 05:12:16 UTC",
    "updated_date": "2024-03-19 11:36:26 UTC"
  },
  {
    "arxiv_id": "2403.06095v4",
    "title": "RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion",
    "authors": [
      "Huy N. Phan",
      "Hoang N. Phan",
      "Tien N. Nguyen",
      "Nghi D. Q. Bui"
    ],
    "abstract": "Code Large Language Models (CodeLLMs) have demonstrated impressive\nproficiency in code completion tasks. However, they often fall short of fully\nunderstanding the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies, which can result in less\nprecise completions. To overcome these limitations, we present \\tool, a\nmultifaceted framework designed to address the complex challenges associated\nwith repository-level code completion. Central to RepoHYPER is the {\\em\nRepo-level Semantic Graph} (RSG), a novel semantic graph structure that\nencapsulates the vast context of code repositories. Furthermore, RepoHyper\nleverages Expand and Refine retrieval method, including a graph expansion and a\nlink prediction algorithm applied to the RSG, enabling the effective retrieval\nand prioritization of relevant code snippets. Our evaluations show that \\tool\nmarkedly outperforms existing techniques in repository-level code completion,\nshowcasing enhanced accuracy across various datasets when compared to several\nstrong baselines. Our implementation of RepoHYPER can be found at\nhttps://github.com/FSoft-AI4Code/RepoHyper.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06095v4",
    "published_date": "2024-03-10 05:10:34 UTC",
    "updated_date": "2024-08-14 16:15:31 UTC"
  },
  {
    "arxiv_id": "2403.06088v1",
    "title": "Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models",
    "authors": [
      "Esmaeil Seraj",
      "Walter Talamonti"
    ],
    "abstract": "In the burgeoning field of intelligent transportation systems, enhancing\nvehicle-driver interaction through facial attribute recognition, such as facial\nexpression, eye gaze, age, etc., is of paramount importance for safety,\npersonalization, and overall user experience. However, the scarcity of\ncomprehensive large-scale, real-world datasets poses a significant challenge\nfor training robust multi-task models. Existing literature often overlooks the\npotential of synthetic datasets and the comparative efficacy of\nstate-of-the-art vision foundation models in such constrained settings. This\npaper addresses these gaps by investigating the utility of synthetic datasets\nfor training complex multi-task models that recognize facial attributes of\npassengers of a vehicle, such as gaze plane, age, and facial expression.\nUtilizing transfer learning techniques with both pre-trained Vision Transformer\n(ViT) and Residual Network (ResNet) models, we explore various training and\nadaptation methods to optimize performance, particularly when data availability\nis limited. We provide extensive post-evaluation analysis, investigating the\neffects of synthetic data distributions on model performance in in-distribution\ndata and out-of-distribution inference. Our study unveils counter-intuitive\nfindings, notably the superior performance of ResNet over ViTs in our specific\nmulti-task context, which is attributed to the mismatch in model complexity\nrelative to task complexity. Our results highlight the challenges and\nopportunities for enhancing the use of synthetic data and vision foundation\nmodels in practical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Manuscript under peer review",
    "pdf_url": "http://arxiv.org/pdf/2403.06088v1",
    "published_date": "2024-03-10 04:17:54 UTC",
    "updated_date": "2024-03-10 04:17:54 UTC"
  },
  {
    "arxiv_id": "2403.06086v1",
    "title": "Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach",
    "authors": [
      "Juanwu Lu",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Yeping Hu"
    ],
    "abstract": "Estimating the potential behavior of the surrounding human-driven vehicles is\ncrucial for the safety of autonomous vehicles in a mixed traffic flow. Recent\nstate-of-the-art achieved accurate prediction using deep neural networks.\nHowever, these end-to-end models are usually black boxes with weak\ninterpretability and generalizability. This paper proposes the Goal-based\nNeural Variational Agent (GNeVA), an interpretable generative model for motion\nprediction with robust generalizability to out-of-distribution cases. For\ninterpretability, the model achieves target-driven motion prediction by\nestimating the spatial distribution of long-term destinations with a\nvariational mixture of Gaussians. We identify a causal structure among maps and\nagents' histories and derive a variational posterior to enhance\ngeneralizability. Experiments on motion prediction datasets validate that the\nfitted model can be interpretable and generalizable and can achieve comparable\nperformance to state-of-the-art results.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AISTATS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06086v1",
    "published_date": "2024-03-10 04:16:04 UTC",
    "updated_date": "2024-03-10 04:16:04 UTC"
  },
  {
    "arxiv_id": "2403.07022v1",
    "title": "A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units",
    "authors": [
      "Liyue Chen",
      "Jiangyi Fang",
      "Tengfei Liu",
      "Shaosheng Cao",
      "Leye Wang"
    ],
    "abstract": "Spatio-Temporal (ST) prediction is crucial for making informed decisions in\nurban location-based applications like ride-sharing. However, existing ST\nmodels often require region partition as a prerequisite, resulting in two main\npitfalls. Firstly, location-based services necessitate ad-hoc regions for\nvarious purposes, requiring multiple ST models with varying scales and zones,\nwhich can be costly to support. Secondly, different ST models may produce\nconflicting outputs, resulting in confusing predictions. In this paper, we\npropose One4All-ST, a framework that can conduct ST prediction for arbitrary\nmodifiable areal units using only one model. To reduce the cost of getting\nmulti-scale predictions, we design an ST network with hierarchical spatial\nmodeling and scale normalization modules to efficiently and equally learn\nmulti-scale representations. To address prediction inconsistencies across\nscales, we propose a dynamic programming scheme to solve the formulated optimal\ncombination problem, minimizing predicted error through theoretical analysis.\nBesides, we suggest using an extended quad-tree to index the optimal\ncombinations for quick response to arbitrary modifiable areal units in\npractical online scenarios. Extensive experiments on two real-world datasets\nverify the efficiency and effectiveness of One4All-ST in ST prediction for\narbitrary modifiable areal units. The source codes and data of this work are\navailable at https://github.com/uctb/One4All-ST.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICDE 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.07022v1",
    "published_date": "2024-03-10 02:34:44 UTC",
    "updated_date": "2024-03-10 02:34:44 UTC"
  },
  {
    "arxiv_id": "2403.06064v3",
    "title": "L^2GC:Lorentzian Linear Graph Convolutional Networks for Node Classification",
    "authors": [
      "Qiuyu Liang",
      "Weihua Wang",
      "Feilong Bao",
      "Guanglai Gao"
    ],
    "abstract": "Linear Graph Convolutional Networks (GCNs) are used to classify the node in\nthe graph data. However, we note that most existing linear GCN models perform\nneural network operations in Euclidean space, which do not explicitly capture\nthe tree-like hierarchical structure exhibited in real-world datasets that\nmodeled as graphs. In this paper, we attempt to introduce hyperbolic space into\nlinear GCN and propose a novel framework for Lorentzian linear GCN.\nSpecifically, we map the learned features of graph nodes into hyperbolic space,\nand then perform a Lorentzian linear feature transformation to capture the\nunderlying tree-like structure of data. Experimental results on standard\ncitation networks datasets with semi-supervised learning show that our approach\nyields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and\n81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be\ntrained up to two orders of magnitude faster than other nonlinear GCN models on\nPubMed dataset. Our code is publicly available at\nhttps://github.com/llqy123/LLGC-master.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.06064v3",
    "published_date": "2024-03-10 02:16:13 UTC",
    "updated_date": "2024-06-14 04:15:20 UTC"
  },
  {
    "arxiv_id": "2403.06063v1",
    "title": "Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue",
    "authors": [
      "Jian Wang",
      "Dongding Lin",
      "Wenjie Li"
    ],
    "abstract": "Target-oriented proactive dialogue systems aim to lead conversations from a\ndialogue context toward a pre-determined target, such as making recommendations\non designated items or introducing new specific topics. To this end, it is\ncritical for such dialogue systems to plan reasonable actions to drive the\nconversation proactively, and meanwhile, to plan appropriate topics to move the\nconversation forward to the target topic smoothly. In this work, we mainly\nfocus on effective dialogue planning for target-oriented dialogue generation.\nInspired by decision-making theories in cognitive science, we propose a novel\ntarget-constrained bidirectional planning (TRIP) approach, which plans an\nappropriate dialogue path by looking ahead and looking back. By formulating the\nplanning as a generation task, our TRIP bidirectionally generates a dialogue\npath consisting of a sequence of <action, topic> pairs using two Transformer\ndecoders. They are expected to supervise each other and converge on consistent\nactions and topics by minimizing the decision gap and contrastive generation of\ntargets. Moreover, we propose a target-constrained decoding algorithm with a\nbidirectional agreement to better control the planning process. Subsequently,\nwe adopt the planned dialogue paths to guide dialogue generation in a pipeline\nmanner, where we explore two variants: prompt-based generation and\nplan-controlled generation. Extensive experiments are conducted on two\nchallenging dialogue datasets, which are re-purposed for exploring\ntarget-oriented dialogue. Our automatic and human evaluations demonstrate that\nthe proposed methods significantly outperform various baseline models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACM Transactions on Information Systems (TOIS)",
    "pdf_url": "http://arxiv.org/pdf/2403.06063v1",
    "published_date": "2024-03-10 02:14:24 UTC",
    "updated_date": "2024-03-10 02:14:24 UTC"
  },
  {
    "arxiv_id": "2403.06054v5",
    "title": "Decoupled Data Consistency with Diffusion Purification for Image Restoration",
    "authors": [
      "Xiang Li",
      "Soo Min Kwon",
      "Ismail R. Alkhouri",
      "Saiprasad Ravishankar",
      "Qing Qu"
    ],
    "abstract": "Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.06054v5",
    "published_date": "2024-03-10 00:47:05 UTC",
    "updated_date": "2024-05-29 00:09:08 UTC"
  }
]
{
  "date": "2024-03-10",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-10 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 38 篇论文，主要聚焦 AI 伦理、语言模型优化、计算机视觉和医疗应用等领域，其中令人印象深刻的是 LLM 在约束对齐和视频生成的创新方法，以及 Dan Roth 等学者参与的文章，这些工作突显了 AI 在实际应用中的潜力。\n\n今天的核心论文多围绕 AI 模型的改进和应用，我们先聊聊几篇重要且话题度高的，快速掠过其他次要内容。\n\n**1. Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups（剥削边缘：资本主义如何以少数群体为代价推动 AI）**  \n这篇论文由 Nelson Colón Vargas 撰写，探讨了资本主义与 AI 之间的关系，强调 AI 如何加剧种族和经济不平等。通过分析 AI 在零工经济、算法偏见和监控中的角色，论文呼吁在 AI 设计中融入社会正义。该发现揭示了 AI 伦理问题，可能推动未来政策改革。\n\n**2. From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification（从指令到约束：语言模型对齐的自动约束验证）**  \n作者包括知名学者 Dan Roth，这篇论文提出 ACT 框架，用于语言模型的约束对齐。通过分类约束并使用约束验证器自动生成偏好标签，实验显示 ACT 提升了模型在实体分类、摘要和问答任务中的性能，并证明了约束遵循能力的可转移性。该工作在 LLM 优化方面有重要影响。\n\n**6. ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Discussion via Argumentation Schemes（ArgMed-Agents：通过论证方案实现 LLM 讨论的解释性临床决策推理）**  \n论文引入 ArgMed-Agents 多代理框架，利用 LLM 通过自论证迭代构建决策图，提高临床推理的准确性和可解释性。实验证明，该方法优于其他提示方法，并增强用户信任。该发现对医疗 AI 应用有实际价值。\n\n**14. WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs（WorldGPT：受 Sora 启发的视频 AI 代理，从文本和图像输入构建丰富世界模型）**  \n这篇创新论文受 OpenAI Sora 启发，提出 WorldGPT 框架，包括提示增强和视频翻译模块，能生成更一致的视频序列。实验验证了其在文本和图像驱动下的有效性，可能推动视频生成领域的进展。\n\n其他论文中，快速提一下几篇有潜力的：\n- **4. An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation（端到端深度学习生成框架用于可细化形状匹配和生成）**：提出无监督模型生成医学形状，支持 In-Silico 临床试验。\n- **5. Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning（最优策略稀疏化和低秩分解用于深度强化学习）**：引入 L0 正则化技术，实现了高效策略压缩，提升了 DRL 在游戏和机器人中的性能。\n- **13. Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation（文本引导的变分图像生成用于工业异常检测和分割）**：使用扩散模型合成无缺陷图像，提高了异常检测的准确性。\n- **29. MACE: Mass Concept Erasure in Diffusion Models（MACE：扩散模型中的大规模概念擦除）**：开发框架批量擦除图像生成中的不良概念，平衡泛化和特异性。\n\n剩余论文涉及领域如轨迹检测、知识编辑和图像恢复等，但相对常规，我们就不再深入。总之，今天的更新强调 AI 的伦理和实用性，值得关注 LLM 相关创新。如果有特定兴趣，建议查看这些论文的摘要！",
  "papers": [
    {
      "arxiv_id": "2403.06332v2",
      "title": "Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups",
      "title_zh": "翻译失败",
      "authors": [
        "Nelson Colón Vargas"
      ],
      "abstract": "This paper explores the intricate relationship between capitalism, racial\ninjustice, and artificial intelligence (AI), arguing that AI acts as a\ncontemporary vehicle for age-old forms of exploitation. By linking historical\npatterns of racial and economic oppression with current AI practices, this\nstudy illustrates how modern technology perpetuates and deepens societal\ninequalities. It specifically examines how AI is implicated in the exploitation\nof marginalized communities through underpaid labor in the gig economy, the\nperpetuation of biases in algorithmic decision-making, and the reinforcement of\nsystemic barriers that prevent these groups from benefiting equitably from\ntechnological advances. Furthermore, the paper discusses the role of AI in\nextending and intensifying the social, economic, and psychological burdens\nfaced by these communities, highlighting the problematic use of AI in\nsurveillance, law enforcement, and mental health contexts. The analysis\nconcludes with a call for transformative changes in how AI is developed and\ndeployed. Advocating for a reevaluation of the values driving AI innovation,\nthe paper promotes an approach that integrates social justice and equity into\nthe core of technological design and policy. This shift is crucial for ensuring\nthat AI serves as a tool for societal improvement, fostering empowerment and\nhealing rather than deepening existing divides.",
      "tldr_zh": "这篇论文探讨了资本主义与人工智能(AI)之间的复杂关系，论证AI如何作为现代工具延续历史上的种族和经济剥削，进而加剧对少数化群体的不公。具体分析包括AI在零工经济中导致的低薪劳工剥削、在算法决策-making中强化偏见，以及在监视、执法和心理健康领域的使用加剧社会、经济和心理负担。论文通过历史与当前实践的联系，揭示AI如何深化系统性不平等，并呼吁变革：重新评估驱动AI创新的核心价值观，将社会 justice和公平融入技术设计和政策，以确保AI成为促进社会进步的工具。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06332v2",
      "published_date": "2024-03-10 22:40:07 UTC",
      "updated_date": "2024-05-01 01:27:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:03:15.904859"
    },
    {
      "arxiv_id": "2403.06326v1",
      "title": "From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification",
      "title_zh": "从指令到约束：通过自动约束验证的语言模型对齐",
      "authors": [
        "Fei Wang",
        "Chao Shang",
        "Sarthak Jain",
        "Shuai Wang",
        "Qiang Ning",
        "Bonan Min",
        "Vittorio Castelli",
        "Yassine Benajiba",
        "Dan Roth"
      ],
      "abstract": "User alignment is crucial for adapting general-purpose language models (LMs)\nto downstream tasks, but human annotations are often not available for all\ntypes of instructions, especially those with customized constraints. We observe\nthat user instructions typically contain constraints. While assessing response\nquality in terms of the whole instruction is often costly, efficiently\nevaluating the satisfaction rate of constraints is feasible. We investigate\ncommon constraints in NLP tasks, categorize them into three classes based on\nthe types of their arguments, and propose a unified framework, ACT (Aligning to\nConsTraints), to automatically produce supervision signals for user alignment\nwith constraints. Specifically, ACT uses constraint verifiers, which are\ntypically easy to implement in practice, to compute constraint satisfaction\nrate (CSR) of each response. It samples multiple responses for each prompt and\ncollect preference labels based on their CSR automatically. Subsequently, ACT\nadapts the LM to the target task through a ranking-based learning process.\nExperiments on fine-grained entity typing, abstractive summarization, and\ntemporal question answering show that ACT is able to enhance LMs' capability to\nadhere to different classes of constraints, thereby improving task performance.\nFurther experiments show that the constraint-following capabilities are\ntransferable.",
      "tldr_zh": "该研究探讨了语言模型（LMs）的用户对齐（user alignment）问题，提出了一种自动约束验证框架ACT，以解决自定义约束指令缺乏人类标注的挑战。ACT将NLP任务中的约束分类为三种类型，并使用约束验证器计算响应约束满足率（CSR），通过采样多个响应并基于CSR生成偏好标签，实现基于排名的学习过程。实验在细粒度实体类型化、摘要生成和时间问题回答任务上表明，ACT显著提升了LMs遵守约束的能力，从而改善了任务性能；此外，这些约束遵循能力具有可转移性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06326v1",
      "published_date": "2024-03-10 22:14:54 UTC",
      "updated_date": "2024-03-10 22:14:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:03:27.449734"
    },
    {
      "arxiv_id": "2403.06322v2",
      "title": "Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility",
      "title_zh": "翻译失败",
      "authors": [
        "Scott Siegel",
        "Jiaqing Zhang",
        "Sabyasachi Bandyopadhyay",
        "Subhash Nerella",
        "Brandon Silva",
        "Tezcan Baslanti",
        "Azra Bihorac",
        "Parisa Rashidi"
      ],
      "abstract": "Despite the importance of closely monitoring patients in the Intensive Care\nUnit (ICU), many aspects are still assessed in a limited manner due to the time\nconstraints imposed on healthcare providers. For example, although excessive\nvisitations during rest hours can potentially exacerbate the risk of circadian\nrhythm disruption and delirium, it is not captured in the ICU. Likewise, while\nmobility can be an important indicator of recovery or deterioration in ICU\npatients, it is only captured sporadically or not captured at all. In the past\nfew years, the computer vision field has found application in many domains by\nreducing the human burden. Using computer vision systems in the ICU can also\npotentially enable non-existing assessments or enhance the frequency and\naccuracy of existing assessments while reducing the staff workload. In this\nstudy, we leverage a state-of-the-art noninvasive computer vision system based\non depth imaging to characterize ICU visitations and patients' mobility. We\nthen examine the relationship between visitation and several patient outcomes,\nsuch as pain, acuity, and delirium. We found an association between\ndeteriorating patient acuity and the incidence of delirium with increased\nvisitations. In contrast, self-reported pain, reported using the Defense and\nVeteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.\nOur findings highlight the feasibility and potential of using noninvasive\nautonomous systems to monitor ICU patients.",
      "tldr_zh": "本研究探讨了在 ICU（Intensive Care Unit）中使用计算机视觉（Computer Vision）系统来监控患者访问和移动性，以克服传统评估受时间限制的不足。研究采用基于深度图像的非侵入性系统，对访问与患者结果（如疼痛、急性度和谵妄）进行分析。结果显示，增加的访问与患者急性度恶化和谵妄发病率相关，而使用 DVPRS（Defense and Veteran Pain Rating Scale）评估的自报疼痛则与减少访问相关。该方法证明了计算机视觉在减轻工作人员负担、提升评估频率和准确性的潜在价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06322v2",
      "published_date": "2024-03-10 21:43:47 UTC",
      "updated_date": "2024-07-12 14:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:03:39.843238"
    },
    {
      "arxiv_id": "2403.06317v1",
      "title": "An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation",
      "title_zh": "一种端到端深度学习生成框架，用于可细化的形状匹配和生成",
      "authors": [
        "Soodeh Kalaie",
        "Andy Bulpitt",
        "Alejandro F. Frangi",
        "Ali Gooya"
      ],
      "abstract": "Generative modelling for shapes is a prerequisite for In-Silico Clinical\nTrials (ISCTs), which aim to cost-effectively validate medical device\ninterventions using synthetic anatomical shapes, often represented as 3D\nsurface meshes. However, constructing AI models to generate shapes closely\nresembling the real mesh samples is challenging due to variable vertex counts,\nconnectivities, and the lack of dense vertex-wise correspondences across the\ntraining data. Employing graph representations for meshes, we develop a novel\nunsupervised geometric deep-learning model to establish refinable shape\ncorrespondences in a latent space, construct a population-derived atlas and\ngenerate realistic synthetic shapes. We additionally extend our proposed base\nmodel to a joint shape generative-clustering multi-atlas framework to\nincorporate further variability and preserve more details in the generated\nshapes. Experimental results using liver and left-ventricular models\ndemonstrate the approach's applicability to computational medicine,\nhighlighting its suitability for ISCTs through a comparative analysis.",
      "tldr_zh": "该论文提出了一种端到端深度学习生成框架，用于可细化的形状匹配和生成，以支持 In-Silico Clinical Trials (ISCTs)，通过合成解剖形状验证医疗设备干预。框架采用图表示和 unsupervised geometric deep-learning 模型，在潜在空间（latent space）建立形状对应、构建人群派生图集，并生成逼真合成形状。论文进一步扩展该模型为联合形状生成-聚类多图集框架，以增加变异性和保留更多细节。实验结果显示，在肝脏和左心室模型上，该方法在计算医学中表现出色，并通过比较分析证明其适用于 ISCTs。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06317v1",
      "published_date": "2024-03-10 21:33:53 UTC",
      "updated_date": "2024-03-10 21:33:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:03:51.358741"
    },
    {
      "arxiv_id": "2403.06313v1",
      "title": "Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Vikram Goddla"
      ],
      "abstract": "Deep reinforcement learning(DRL) has shown significant promise in a wide\nrange of applications including computer games and robotics. Yet, training DRL\npolicies consume extraordinary computing resources resulting in dense policies\nwhich are prone to overfitting. Moreover, inference with dense DRL policies\nlimit their practical applications, especially in edge computing. Techniques\nsuch as pruning and singular value decomposition have been used with deep\nlearning models to achieve sparsification and model compression to limit\noverfitting and reduce memory consumption. However, these techniques resulted\nin sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$\nregularization techniques have been proposed for neural network sparsification\nand sparse auto-encoder development, but their implementation in DRL\nenvironments has not been apparent. We propose a novel\n$L_0$-norm-regularization technique using an optimal sparsity map to sparsify\nDRL policies and promote their decomposition to a lower rank without decay in\nrewards. We evaluated our $L_0$-norm-regularization technique across five\ndifferent environments (Cartpole-v1, Acrobat-v1, LunarLander-v2,\nSuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and\noff-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL\npolicy in the SuperMarioBros environment achieved 93% sparsity and gained 70%\ncompression when subjected to low-rank decomposition, while significantly\noutperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL\npolicy in the Surgical Robot Learning environment achieved a 36% sparsification\nand gained 46% compression when decomposed to a lower rank, while being\nperformant. The results suggest that our custom $L_0$-norm-regularization\ntechnique for sparsification of DRL policies is a promising avenue to reduce\ncomputational resources and limit overfitting.",
      "tldr_zh": "该论文针对深度强化学习(DRL)策略的密集性和过拟合问题，提出了一种新型 L0-norm-regularization 技术，利用 optimal sparsity map 实现策略稀疏化和 low rank decomposition，从而减少计算资源消耗而不降低奖励。研究在多个环境中（如 Cartpole-v1、SuperMarioBros-7.1.v0 和 Surgical Robot Learning）使用 on-policy 和 off-policy 算法进行评估。结果显示，在 SuperMarioBros 环境中，稀疏化达到 93%、压缩率达 70%，性能显著优于密集策略；在 Surgical Robot Learning 环境中，实现 36% 稀疏化和 46% 压缩，同时保持高效表现。该方法为 DRL 的实际应用提供了可行的优化途径，减少了资源需求和过拟合风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06313v1",
      "published_date": "2024-03-10 21:18:54 UTC",
      "updated_date": "2024-03-10 21:18:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:04:05.051810"
    },
    {
      "arxiv_id": "2403.06294v3",
      "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes",
      "title_zh": "ArgMed-Agents：通过论证方案实现LLM讨论的可解释临床决策",
      "authors": [
        "Shengxin Hong",
        "Liang Xiao",
        "Xin Zhang",
        "Jianxia Chen"
      ],
      "abstract": "There are two main barriers to using large language models (LLMs) in clinical\nreasoning. Firstly, while LLMs exhibit significant promise in Natural Language\nProcessing (NLP) tasks, their performance in complex reasoning and planning\nfalls short of expectations. Secondly, LLMs use uninterpretable methods to make\nclinical decisions that are fundamentally different from the clinician's\ncognitive processes. This leads to user distrust. In this paper, we present a\nmulti-agent framework called ArgMed-Agents, which aims to enable LLM-based\nagents to make explainable clinical decision reasoning through interaction.\nArgMed-Agents performs self-argumentation iterations via Argumentation Scheme\nfor Clinical Discussion (a reasoning mechanism for modeling cognitive processes\nin clinical reasoning), and then constructs the argumentation process as a\ndirected graph representing conflicting relationships. Ultimately, use symbolic\nsolver to identify a series of rational and coherent arguments to support\ndecision. We construct a formal model of ArgMed-Agents and present conjectures\nfor theoretical guarantees. ArgMed-Agents enables LLMs to mimic the process of\nclinical argumentative reasoning by generating explanations of reasoning in a\nself-directed manner. The setup experiments show that ArgMed-Agents not only\nimproves accuracy in complex clinical decision reasoning problems compared to\nother prompt methods, but more importantly, it provides users with decision\nexplanations that increase their confidence.",
      "tldr_zh": "本文提出了 ArgMed-Agents，一种多智能体框架，旨在解决大型语言模型 (LLMs) 在临床推理中存在的复杂推理不足和决策不可解释性问题，从而提升用户信任。框架通过 Argumentation Scheme for Clinical Discussion 进行自辩论迭代，将论证过程构建为有向图，并利用符号求解器识别合理连贯的论点来支持决策。实验结果表明，ArgMed-Agents 不仅在复杂临床决策任务上比其他提示方法提高了准确性，还能生成自导解释，帮助用户更好地理解和信任模型的推理过程。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06294v3",
      "published_date": "2024-03-10 19:47:00 UTC",
      "updated_date": "2024-12-28 17:40:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:04:15.618839"
    },
    {
      "arxiv_id": "2403.06289v1",
      "title": "Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning",
      "title_zh": "理解与缓解监督对比学习中的人为标注错误",
      "authors": [
        "Zijun Long",
        "Lipeng Zhuang",
        "George Killick",
        "Richard McCreadie",
        "Gerardo Aragon Camarasa",
        "Paul Henderson"
      ],
      "abstract": "Human-annotated vision datasets inevitably contain a fraction of human\nmislabelled examples. While the detrimental effects of such mislabelling on\nsupervised learning are well-researched, their influence on Supervised\nContrastive Learning (SCL) remains largely unexplored. In this paper, we show\nthat human-labelling errors not only differ significantly from synthetic label\nerrors, but also pose unique challenges in SCL, different to those in\ntraditional supervised learning methods. Specifically, our results indicate\nthey adversely impact the learning process in the ~99% of cases when they occur\nas false positive samples. Existing noise-mitigating methods primarily focus on\nsynthetic label errors and tackle the unrealistic setting of very high\nsynthetic noise rates (40-80%), but they often underperform on common image\ndatasets due to overfitting. To address this issue, we introduce a novel SCL\nobjective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is\ndesigned to mitigate the effects of real-world mislabelled examples, typically\ncharacterized by much lower noise rates (<5%). We demonstrate that SCL-RHE\nconsistently outperforms state-of-the-art representation learning and\nnoise-mitigating methods across various vision benchmarks, by offering improved\nresilience against human-labelling errors.",
      "tldr_zh": "本研究探讨了人类标注错误对Supervised Contrastive Learning (SCL)的影响，发现这些错误与合成错误不同，主要作为假正样本在约99%的情况下负面影响学习过程。现有噪声缓解方法虽针对合成错误设计，但因过度拟合而在常见图像数据集上表现不佳。论文提出了一种新颖的SCL目标函数SCL-RHE，专门针对真实世界低噪声率（<5%）的标注错误，提供更强的鲁棒性。实验结果显示，SCL-RHE在多种视觉基准上优于现有表示学习和噪声缓解方法，提升了对人类标注错误的抵抗力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2311.16481",
      "pdf_url": "http://arxiv.org/pdf/2403.06289v1",
      "published_date": "2024-03-10 19:05:12 UTC",
      "updated_date": "2024-03-10 19:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:04:28.261580"
    },
    {
      "arxiv_id": "2403.06275v1",
      "title": "UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Kwanyoung Kim",
        "Jaa-Yeon Lee",
        "Jong Chul Ye"
      ],
      "abstract": "Nakagami imaging holds promise for visualizing and quantifying tissue\nscattering in ultrasound waves, with potential applications in tumor diagnosis\nand fat fraction estimation which are challenging to discern by conventional\nultrasound B-mode images. Existing methods struggle with optimal window size\nselection and suffer from estimator instability, leading to degraded resolution\nimages. To address this, here we propose a novel method called UNICORN\n(Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an\naccurate, closed-form estimator for Nakagami parameter estimation in terms of\nthe score function of ultrasonic envelope. Extensive experiments using\nsimulation and real ultrasound RF data demonstrate UNICORN's superiority over\nconventional approaches in accuracy and resolution quality.",
      "tldr_zh": "本论文提出 UNICORN 方法，通过 Score Matching 和 Adaptation 技术，提供一个准确的闭式形式估计器，用于 Nakagami Imaging 的参数估计，旨在解决现有方法在窗口大小选择和估计器不稳定性方面的问题，从而改善超声波中组织散射的可视化和量化。UNICORN 针对肿瘤诊断和脂肪分数估计等应用，基于超声包络的分数函数进行优化。实验结果显示，在模拟和真实超声 RF 数据上，UNICORN 比传统方法在准确性和分辨率质量上表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 5 figure",
      "pdf_url": "http://arxiv.org/pdf/2403.06275v1",
      "published_date": "2024-03-10 18:05:41 UTC",
      "updated_date": "2024-03-10 18:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:04:39.810836"
    },
    {
      "arxiv_id": "2403.06268v1",
      "title": "Physics-Guided Abnormal Trajectory Gap Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Arun Sharma",
        "Shashi Shekhar"
      ],
      "abstract": "Given trajectories with gaps (i.e., missing data), we investigate algorithms\nto identify abnormal gaps in trajectories which occur when a given moving\nobject did not report its location, but other moving objects in the same\ngeographic region periodically did. The problem is important due to its\nsocietal applications, such as improving maritime safety and regulatory\nenforcement for global security concerns such as illegal fishing, illegal oil\ntransfers, and trans-shipments. The problem is challenging due to the\ndifficulty of bounding the possible locations of the moving object during a\ntrajectory gap, and the very high computational cost of detecting gaps in such\na large volume of location data. The current literature on anomalous trajectory\ndetection assumes linear interpolation within gaps, which may not be able to\ndetect abnormal gaps since objects within a given region may have traveled away\nfrom their shortest path. In preliminary work, we introduced an abnormal gap\nmeasure that uses a classical space-time prism model to bound an object's\npossible movement during the trajectory gap and provided a scalable memoized\ngap detection algorithm (Memo-AGD). In this paper, we propose a Space\nTime-Aware Gap Detection (STAGD) approach to leverage space-time indexing and\nmerging of trajectory gaps. We also incorporate a Dynamic Region Merge-based\n(DRM) approach to efficiently compute gap abnormality scores. We provide\ntheoretical proofs that both algorithms are correct and complete and also\nprovide analysis of asymptotic time complexity. Experimental results on\nsynthetic and real-world maritime trajectory data show that the proposed\napproach substantially improves computation time over the baseline technique.",
      "tldr_zh": "本研究针对轨迹数据中的异常间隙（abnormal gaps）问题，提出了一种Physics-Guided方法，用于识别移动物体在未报告位置时与其他物体相比的异常行为，尤其适用于海上安全领域，如非法捕鱼和油转移监管。论文引入了基于空间-时间棱镜模型（space-time prism model）的异常间隙度量，并开发了Space Time-Aware Gap Detection (STAGD) 和Dynamic Region Merge-based (DRM) 算法，这些算法通过空间-时间索引和间隙合并来提升计算效率。实验结果显示，该方法在合成和真实海洋轨迹数据上显著降低了计算时间，比基线技术提高了性能，并提供了算法正确性和完整性的理论证明。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06268v1",
      "published_date": "2024-03-10 17:07:28 UTC",
      "updated_date": "2024-03-10 17:07:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:04:51.283159"
    },
    {
      "arxiv_id": "2403.06267v1",
      "title": "FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System to Assist Human Labelers' Preference Elicitation",
      "title_zh": "翻译失败",
      "authors": [
        "Hanfang Lyu",
        "Yuanchen Bai",
        "Xin Liang",
        "Ujaan Das",
        "Chuhan Shi",
        "Leiliang Gong",
        "Yingchi Li",
        "Mingfei Sun",
        "Ming Ge",
        "Xiaojuan Ma"
      ],
      "abstract": "Preference-based learning aims to align robot task objectives with human\nvalues. One of the most common methods to infer human preferences is by\npairwise comparisons of robot task trajectories. Traditional comparison-based\npreference labeling systems seldom support labelers to digest and identify\ncritical differences between complex trajectories recorded in videos. Our\nformative study (N = 12) suggests that individuals may overlook non-salient\ntask features and establish biased preference criteria during their preference\nelicitation process because of partial observations. In addition, they may\nexperience mental fatigue when given many pairs to compare, causing their label\nquality to deteriorate. To mitigate these issues, we propose FARPLS, a\nFeature-Augmented Robot trajectory Preference Labeling System. FARPLS\nhighlights potential outliers in a wide variety of task features that matter to\nhumans and extracts the corresponding video keyframes for easy review and\ncomparison. It also dynamically adjusts the labeling order according to users'\nfamiliarities, difficulties of the trajectory pair, and level of disagreements.\nAt the same time, the system monitors labelers' consistency and provides\nfeedback on labeling progress to keep labelers engaged. A between-subjects\nstudy (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows\nthat FARPLS can help users establish preference criteria more easily and notice\nmore relevant details in the presented trajectories than the conventional\ninterface. FARPLS also improves labeling consistency and engagement, mitigating\nchallenges in preference elicitation without raising cognitive loads\nsignificantly",
      "tldr_zh": "该研究针对偏好-based learning 中人类标签者通过比较机器人轨迹视频推断偏好的挑战，提出 FARPLS 系统，以辅助标签者更有效地识别关键差异。FARPLS 通过突出任务特征中的潜在异常点、提取对应视频关键帧，并动态调整标签顺序（基于用户熟悉度、难度和分歧），同时监控标签一致性和提供反馈，以缓解偏见、疲劳和低质量问题。实验结果显示（N=42），FARPLS 帮助用户更容易建立偏好标准、注意到更多相关细节，并显著提升标签一致性和参与度，而未显著增加认知负荷。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA",
      "pdf_url": "http://arxiv.org/pdf/2403.06267v1",
      "published_date": "2024-03-10 17:07:20 UTC",
      "updated_date": "2024-03-10 17:07:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:05:02.560808"
    },
    {
      "arxiv_id": "2403.06265v2",
      "title": "Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance",
      "title_zh": "拆解分词：评估文本压缩及其与模型性能的相关性",
      "authors": [
        "Omer Goldman",
        "Avi Caciularu",
        "Matan Eyal",
        "Kris Cao",
        "Idan Szpektor",
        "Reut Tsarfaty"
      ],
      "abstract": "Despite it being the cornerstone of BPE, the most common tokenization\nalgorithm, the importance of compression in the tokenization process is still\nunclear. In this paper, we argue for the theoretical importance of compression,\nthat can be viewed as 0-gram language modeling where equal probability is\nassigned to all tokens. We also demonstrate the empirical importance of\ncompression for downstream success of pre-trained language models. We control\nthe compression ability of several BPE tokenizers by varying the amount of\ndocuments available during their training: from 1 million documents to a\ncharacter-based tokenizer equivalent to no training data at all. We then\npre-train English language models based on those tokenizers and fine-tune them\nover several tasks. We show that there is a correlation between tokenizers'\ncompression and models' downstream performance, suggesting that compression is\na reliable intrinsic indicator of tokenization quality. These correlations are\nmore pronounced for generation tasks (over classification) or for smaller\nmodels (over large ones). We replicated a representative part of our\nexperiments on Turkish and found similar results, confirming that our results\nhold for languages with typological characteristics dissimilar to English. We\nconclude that building better compressing tokenizers is a fruitful avenue for\nfurther research and for improving overall model performance.",
      "tldr_zh": "本研究探讨了 BPE tokenization 中文本压缩的重要性及其与预训练语言模型下游性能的相关性。作者从理论角度将压缩视为 0-gram 语言建模，并通过实验验证其实证价值：他们调整 BPE tokenizers 的训练数据量（从 1 百万文档到无数据字符级 tokenizer），随后预训练和微调英语语言模型。结果显示，tokenizers 的压缩能力与模型性能正相关，尤其在生成任务和小模型上，且在土耳其语实验中得到类似验证。研究结论认为，开发更高效的压缩 tokenizers 是提升整体模型性能的有前景方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024, Findings",
      "pdf_url": "http://arxiv.org/pdf/2403.06265v2",
      "published_date": "2024-03-10 17:02:53 UTC",
      "updated_date": "2024-06-22 16:00:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:05:15.280742"
    },
    {
      "arxiv_id": "2403.06259v2",
      "title": "Editing Conceptual Knowledge for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohan Wang",
        "Shengyu Mao",
        "Ningyu Zhang",
        "Shumin Deng",
        "Yunzhi Yao",
        "Yue Shen",
        "Lei Liang",
        "Jinjie Gu",
        "Huajun Chen"
      ],
      "abstract": "Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.",
      "tldr_zh": "本论文首次探索了大型语言模型(LLMs)的概念级知识编辑问题，填补了现有研究仅限于实例级编辑的空白。研究者构建了新的基准数据集ConceptEdit并引入一套评估指标，通过实验验证现有编辑方法虽能部分修改概念定义，但可能导致相关实例知识扭曲，从而影响模型性能。该工作旨在加深对LLMs的理解，并提供项目主页供进一步参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings; Code: https://github.com/zjunlp/EasyEdit\n  Dataset: https://huggingface.co/datasets/zjunlp/ConceptEdit",
      "pdf_url": "http://arxiv.org/pdf/2403.06259v2",
      "published_date": "2024-03-10 16:57:10 UTC",
      "updated_date": "2024-10-06 15:45:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:05:25.789886"
    },
    {
      "arxiv_id": "2403.06247v2",
      "title": "Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation",
      "title_zh": "基于文本引导的变分图像生成，用于工业异常检测和分割",
      "authors": [
        "Mingyu Lee",
        "Jongwon Choi"
      ],
      "abstract": "We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.",
      "tldr_zh": "本研究提出了一种基于文本引导的变分图像生成方法（Text-Guided Variational Image Generation），旨在解决工业制造中异常检测（Anomaly Detection）和分割（Segmentation）的干净数据获取挑战。该方法利用从大量文本库中学习的目标对象信息，生成与输入图像相似的无缺陷图像，并确保这些图像与文本和图像知识的预期分布一致，从而提升生成过程的稳定性和通用性。实验结果显示，即使在有限的无缺陷数据情况下，该方法也超过了现有技术，并在四个基线模型和三个数据集上验证了其泛化能力。通过额外分析，该框架进一步提高了异常检测模型的有效性，为工业应用提供了更可靠的图像生成工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, Accepted to CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06247v2",
      "published_date": "2024-03-10 16:11:17 UTC",
      "updated_date": "2024-03-26 14:42:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:05:38.956474"
    },
    {
      "arxiv_id": "2403.07944v1",
      "title": "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs",
      "title_zh": "翻译失败",
      "authors": [
        "Deshun Yang",
        "Luhui Hu",
        "Yu Tian",
        "Zihao Li",
        "Chris Kelly",
        "Bang Yang",
        "Cindy Yang",
        "Yuexian Zou"
      ],
      "abstract": "Several text-to-video diffusion models have demonstrated commendable\ncapabilities in synthesizing high-quality video content. However, it remains a\nformidable challenge pertaining to maintaining temporal consistency and\nensuring action smoothness throughout the generated sequences. In this paper,\nwe present an innovative video generation AI agent that harnesses the power of\nSora-inspired multimodal learning to build skilled world models framework based\non textual prompts and accompanying images. The framework includes two parts:\nprompt enhancer and full video translation. The first part employs the\ncapabilities of ChatGPT to meticulously distill and proactively construct\nprecise prompts for each subsequent step, thereby guaranteeing the utmost\naccuracy in prompt communication and accurate execution in following model\noperations. The second part employ compatible with existing advanced diffusion\ntechniques to expansively generate and refine the key frame at the conclusion\nof a video. Then we can expertly harness the power of leading and trailing key\nframes to craft videos with enhanced temporal consistency and action\nsmoothness. The experimental results confirm that our method has strong\neffectiveness and novelty in constructing world models from text and image\ninputs over the other methods.",
      "tldr_zh": "本研究提出WorldGPT，一种受Sora启发的视频生成AI代理，用于从文本和图像输入构建丰富的世界模型框架，以解决现有文本到视频扩散模型在时间一致性和动作流畅性方面的挑战。该框架包括两个核心部分：提示增强器，利用ChatGPT提炼精确提示以确保后续操作的准确性；以及完整视频翻译模块，通过先进的扩散技术生成和精炼关键帧，从而提升视频的整体质量。实验结果表明，该方法在构建世界模型方面比其他方法更具有效性和创新性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.07944v1",
      "published_date": "2024-03-10 16:09:02 UTC",
      "updated_date": "2024-03-10 16:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:05:51.162608"
    },
    {
      "arxiv_id": "2403.06239v1",
      "title": "Cooperative Classification and Rationalization for Graph Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Linan Yue",
        "Qi Liu",
        "Ye Liu",
        "Weibo Gao",
        "Fangzhou Yao",
        "Wenfeng Li"
      ],
      "abstract": "Graph Neural Networks (GNNs) have achieved impressive results in graph\nclassification tasks, but they struggle to generalize effectively when faced\nwith out-of-distribution (OOD) data. Several approaches have been proposed to\naddress this problem. Among them, one solution is to diversify training\ndistributions in vanilla classification by modifying the data environment, yet\naccessing the environment information is complex. Besides, another promising\napproach involves rationalization, extracting invariant rationales for\npredictions. However, extracting rationales is difficult due to limited\nlearning signals, resulting in less accurate rationales and diminished\npredictions. To address these challenges, in this paper, we propose a\nCooperative Classification and Rationalization (C2R) method, consisting of the\nclassification and the rationalization module. Specifically, we first assume\nthat multiple environments are available in the classification module. Then, we\nintroduce diverse training distributions using an environment-conditional\ngenerative network, enabling robust graph representations. Meanwhile, the\nrationalization module employs a separator to identify relevant rationale\nsubgraphs while the remaining non-rationale subgraphs are de-correlated with\nlabels. Next, we align graph representations from the classification module\nwith rationale subgraph representations using the knowledge distillation\nmethods, enhancing the learning signal for rationales. Finally, we infer\nmultiple environments by gathering non-rationale representations and\nincorporate them into the classification module for cooperative learning.\nExtensive experimental results on both benchmarks and synthetic datasets\ndemonstrate the effectiveness of C2R. Code is available at\nhttps://github.com/yuelinan/Codes-of-C2R.",
      "tldr_zh": "本文提出 Cooperative Classification and Rationalization (C2R) 方法，以解决 Graph Neural Networks (GNNs) 在图分类任务中对 out-of-distribution (OOD) 数据泛化能力不足的问题。C2R 包括分类模块和 rationalization 模块，其中分类模块使用 environment-conditional generative network 引入多样训练分布，生成鲁棒的图表示；rationalization 模块则通过 separator 提取相关的 rationale subgraphs，并将非 rationale subgraphs 与标签脱相关。接着，通过 knowledge distillation 方法对齐分类模块的图表示与 rationale subgraph 表示，增强学习信号，并实现合作学习。实验在基准和合成数据集上证明了 C2R 的有效性，显著提升了 GNNs 的泛化性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to WWW 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06239v1",
      "published_date": "2024-03-10 15:38:20 UTC",
      "updated_date": "2024-03-10 15:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:06:05.762068"
    },
    {
      "arxiv_id": "2403.06235v1",
      "title": "Probabilistic Neural Circuits",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Zuidberg Dos Martires"
      ],
      "abstract": "Probabilistic circuits (PCs) have gained prominence in recent years as a\nversatile framework for discussing probabilistic models that support tractable\nqueries and are yet expressive enough to model complex probability\ndistributions. Nevertheless, tractability comes at a cost: PCs are less\nexpressive than neural networks. In this paper we introduce probabilistic\nneural circuits (PNCs), which strike a balance between PCs and neural nets in\nterms of tractability and expressive power. Theoretically, we show that PNCs\ncan be interpreted as deep mixtures of Bayesian networks. Experimentally, we\ndemonstrate that PNCs constitute powerful function approximators.",
      "tldr_zh": "该论文引入了Probabilistic Neural Circuits (PNCs)，作为一种在Probabilistic circuits (PCs)与神经网络之间平衡可计算性和表达能力的框架。PNCs理论上可以解释为深度混合的Bayesian networks，从而支持可计算查询的同时提升模型的复杂性表达能力。实验结果表明，PNCs是强大的函数逼近器，在实际应用中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2403.06235v1",
      "published_date": "2024-03-10 15:25:49 UTC",
      "updated_date": "2024-03-10 15:25:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:06:13.611910"
    },
    {
      "arxiv_id": "2403.06225v2",
      "title": "MoST: Motion Style Transformer between Diverse Action Contents",
      "title_zh": "翻译失败",
      "authors": [
        "Boeun Kim",
        "Jungho Kim",
        "Hyung Jin Chang",
        "Jin Young Choi"
      ],
      "abstract": "While existing motion style transfer methods are effective between two\nmotions with identical content, their performance significantly diminishes when\ntransferring style between motions with different contents. This challenge lies\nin the lack of clear separation between content and style of a motion. To\ntackle this challenge, we propose a novel motion style transformer that\neffectively disentangles style from content and generates a plausible motion\nwith transferred style from a source motion. Our distinctive approach to\nachieving the goal of disentanglement is twofold: (1) a new architecture for\nmotion style transformer with `part-attentive style modulator across body\nparts' and `Siamese encoders that encode style and content features\nseparately'; (2) style disentanglement loss. Our method outperforms existing\nmethods and demonstrates exceptionally high quality, particularly in motion\npairs with different contents, without the need for heuristic post-processing.\nCodes are available at https://github.com/Boeun-Kim/MoST.",
      "tldr_zh": "本研究提出MoST框架，用于在不同动作内容之间转移动作风格，解决现有方法因内容和风格未清晰分离而导致性能下降的问题。MoST采用新颖架构，包括“跨身体部位的部分注意力风格调制器”和“Siamese编码器”来分别提取风格和内容特征，并引入风格分离损失以实现有效分离。实验结果显示，该方法在不同内容动作对上显著优于现有方法，生成高质量的动作序列，且无需启发式后处理。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06225v2",
      "published_date": "2024-03-10 14:11:25 UTC",
      "updated_date": "2024-03-20 10:05:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:06:26.991172"
    },
    {
      "arxiv_id": "2403.06221v1",
      "title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiwen Zhou",
        "Yingxuan Yang",
        "Muning Wen",
        "Ying Wen",
        "Wenhao Wang",
        "Chunling Xi",
        "Guoqiang Xu",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "Numerous large language model (LLM) agents have been built for different\ntasks like web navigation and online shopping due to LLM's wide knowledge and\ntext-understanding ability. Among these works, many of them utilize in-context\nexamples to achieve generalization without the need for fine-tuning, while few\nof them have considered the problem of how to select and effectively utilize\nthese examples. Recently, methods based on trajectory-level retrieval with task\nmeta-data and using trajectories as in-context examples have been proposed to\nimprove the agent's overall performance in some sequential decision making\ntasks. However, these methods can be problematic due to plausible examples\nretrieved without task-specific state transition dynamics and long input with\nplenty of irrelevant context. In this paper, we propose a novel framework\n(TRAD) to address these issues. TRAD first conducts Thought Retrieval,\nachieving step-level demonstration selection via thought matching, leading to\nmore helpful demonstrations and less irrelevant input noise. Then, TRAD\nintroduces Aligned Decision, complementing retrieved demonstration steps with\ntheir previous or subsequent steps, which enables tolerance for imperfect\nthought and provides a choice for balance between more context and less noise.\nExtensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not\nonly outperforms state-of-the-art models but also effectively helps in reducing\nnoise and promoting generalization. Furthermore, TRAD has been deployed in\nreal-world scenarios of a global business insurance company and improves the\nsuccess rate of robotic process automation.",
      "tldr_zh": "这篇论文提出 TRAD 框架，用于提升 LLM 代理在任务决策中的性能，通过 Step-Wise Thought Retrieval 和 Aligned Decision 来优化 in-context examples 的选择和利用。TRAD 首先通过思想匹配进行步级别演示选择，减少无关输入噪声；然后通过对齐决策将检索步骤与前后文结合，提升容错性和上下文平衡。实验结果显示，TRAD 在 ALFWorld 和 Mind2Web 基准上优于最先进模型，并已在全球商业保险公司的真实场景中部署，提高了机器人过程自动化的成功率。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Codes available at: https://github.com/skyriver-2000/TRAD-Official",
      "pdf_url": "http://arxiv.org/pdf/2403.06221v1",
      "published_date": "2024-03-10 13:58:38 UTC",
      "updated_date": "2024-03-10 13:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:06:40.174885"
    },
    {
      "arxiv_id": "2403.06213v1",
      "title": "$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections",
      "title_zh": "$V_kD:$ 使用正交投影改进知识蒸馏",
      "authors": [
        "Roy Miles",
        "Ismail Elezi",
        "Jiankang Deng"
      ],
      "abstract": "Knowledge distillation is an effective method for training small and\nefficient deep learning models. However, the efficacy of a single method can\ndegenerate when transferring to other tasks, modalities, or even other\narchitectures. To address this limitation, we propose a novel constrained\nfeature distillation method. This method is derived from a small set of core\nprinciples, which results in two emerging components: an orthogonal projection\nand a task-specific normalisation. Equipped with both of these components, our\ntransformer models can outperform all previous methods on ImageNet and reach up\nto a 4.4% relative improvement over the previous state-of-the-art methods. To\nfurther demonstrate the generality of our method, we apply it to object\ndetection and image generation, whereby we obtain consistent and substantial\nperformance improvements over state-of-the-art. Code and models are publicly\navailable: https://github.com/roymiles/vkd",
      "tldr_zh": "本论文提出了一种名为 $V_kD$ 的新型知识蒸馏（Knowledge Distillation）方法，通过引入正交投影（Orthogonal Projection）和任务特定归一化（Task-specific Normalisation）来优化特征蒸馏过程，从而提升模型在不同任务和架构下的泛化能力。基于这些核心原则，该方法使 Transformer 模型在 ImageNet 上超越了所有先前方法，并实现了高达 4.4% 的相对性能提升。进一步验证显示，$V_kD$ 在物体检测和图像生成任务中也取得了显著改善，代码和模型已公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024. Code available at https://github.com/roymiles/vkd",
      "pdf_url": "http://arxiv.org/pdf/2403.06213v1",
      "published_date": "2024-03-10 13:26:24 UTC",
      "updated_date": "2024-03-10 13:26:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:06:51.554182"
    },
    {
      "arxiv_id": "2403.06206v1",
      "title": "Limit of the Maximum Random Permutation Set Entropy",
      "title_zh": "随机置换集最大",
      "authors": [
        "Jiefeng Zhou",
        "Zhen Li",
        "Kang Hao Cheong",
        "Yong Deng"
      ],
      "abstract": "The Random Permutation Set (RPS) is a new type of set proposed recently,\nwhich can be regarded as the generalization of evidence theory. To measure the\nuncertainty of RPS, the entropy of RPS and its corresponding maximum entropy\nhave been proposed. Exploring the maximum entropy provides a possible way of\nunderstanding the physical meaning of RPS. In this paper, a new concept, the\nenvelope of entropy function, is defined. In addition, the limit of the\nenvelope of RPS entropy is derived and proved. Compared with the existing\nmethod, the computational complexity of the proposed method to calculate the\nenvelope of RPS entropy decreases greatly. The result shows that when $N \\to\n\\infty$, the limit form of the envelope of the entropy of RPS converges to $e\n\\times (N!)^2$, which is highly connected to the constant $e$ and factorial.\nFinally, numerical examples validate the efficiency and conciseness of the\nproposed envelope, which provides a new insight into the maximum entropy\nfunction.",
      "tldr_zh": "本研究探讨了Random Permutation Set (RPS)的最大熵极限，RPS被视为证据理论的推广，用于衡量不确定性。通过定义熵函数的包络(envelope of entropy function)这一新概念，论文导出了并证明了RPS熵包络的极限形式。相比现有方法，该方法大大降低了计算复杂度，结果显示当N趋于无穷大时，RPS熵包络的极限收敛到e × (N!)^2，与常数e和阶乘密切相关。数值例子验证了该方法的效率和简洁性，为理解最大熵函数提供了新见解。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "22 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.06206v1",
      "published_date": "2024-03-10 13:04:09 UTC",
      "updated_date": "2024-03-10 13:04:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:07:03.096028"
    },
    {
      "arxiv_id": "2403.06201v1",
      "title": "Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!",
      "title_zh": "翻译失败",
      "authors": [
        "Huanqi Yang",
        "Sijie Ji",
        "Rucheng Wu",
        "Weitao Xu"
      ],
      "abstract": "There is a burgeoning discussion around the capabilities of Large Language\nModels (LLMs) in acting as fundamental components that can be seamlessly\nincorporated into Artificial Intelligence of Things (AIoT) to interpret complex\ntrajectories. This study introduces LLMTrack, a model that illustrates how LLMs\ncan be leveraged for Zero-Shot Trajectory Recognition by employing a novel\nsingle-prompt technique that combines role-play and think step-by-step\nmethodologies with unprocessed Inertial Measurement Unit (IMU) data. We\nevaluate the model using real-world datasets designed to challenge it with\ndistinct trajectories characterized by indoor and outdoor scenarios. In both\ntest scenarios, LLMTrack not only meets but exceeds the performance benchmarks\nset by traditional machine learning approaches and even contemporary\nstate-of-the-art deep learning models, all without the requirement of training\non specialized datasets. The results of our research suggest that, with\nstrategically designed prompts, LLMs can tap into their extensive knowledge\nbase and are well-equipped to analyze raw sensor data with remarkable\neffectiveness.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在人工智能物联网 (AIoT) 中解释复杂轨迹的能力，引入了 LLMTrack 模型，实现零样本 (Zero-Shot) 轨迹识别。LLMTrack 采用一种新颖的单提示技术，结合角色扮演和逐步思考 (think step-by-step) 方法，直接处理原始 Inertial Measurement Unit (IMU) 数据，而无需在专用数据集上训练。在真实室内和室外场景的实验中，LLMTrack 超过了传统机器学习和最新深度学习模型的性能基准，证明了 LLMs 通过精心设计的提示能够高效分析传感器数据。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06201v1",
      "published_date": "2024-03-10 12:50:35 UTC",
      "updated_date": "2024-03-10 12:50:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:07:16.515763"
    },
    {
      "arxiv_id": "2403.06174v1",
      "title": "Domain Adversarial Active Learning for Domain Generalization Classification",
      "title_zh": "领域对抗主动学习用于领域泛化分类",
      "authors": [
        "Jianting Chen",
        "Ling Ding",
        "Yunxiao Yang",
        "Zaiyuan Di",
        "Yang Xiang"
      ],
      "abstract": "Domain generalization models aim to learn cross-domain knowledge from source\ndomain data, to improve performance on unknown target domains. Recent research\nhas demonstrated that diverse and rich source domain samples can enhance domain\ngeneralization capability. This paper argues that the impact of each sample on\nthe model's generalization ability varies. Despite its small scale, a\nhigh-quality dataset can still attain a certain level of generalization\nability. Motivated by this, we propose a domain-adversarial active learning\n(DAAL) algorithm for classification tasks in domain generalization. First, we\nanalyze that the objective of tasks is to maximize the inter-class distance\nwithin the same domain and minimize the intra-class distance across different\ndomains. To achieve this objective, we design a domain adversarial selection\nmethod that prioritizes challenging samples. Second, we posit that even in a\nconverged model, there are subsets of features that lack discriminatory power\nwithin each domain. We attempt to identify these feature subsets and optimize\nthem by a constraint loss. We validate and analyze our DAAL algorithm on\nmultiple domain generalization datasets, comparing it with various domain\ngeneralization algorithms and active learning algorithms. Our results\ndemonstrate that the DAAL algorithm can achieve strong generalization ability\nwith fewer data resources, thereby reducing data annotation costs in domain\ngeneralization tasks.",
      "tldr_zh": "该论文针对域泛化（Domain Generalization）分类任务，提出了一种域对抗主动学习（DAAL）算法，以从源域数据中高效学习跨域知识。DAAL 通过域对抗选择方法优先选取具有挑战性的样本，实现同一域内最大化类间距离和不同域内最小化类内距离；同时，识别并通过约束损失优化模型中缺乏辨别力的特征子集，从而提升泛化能力。实验结果显示，DAAL 在多个域泛化数据集上比传统算法表现更优，使用更少的数据资源即可实现强泛化效果，显著降低了数据标注成本。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06174v1",
      "published_date": "2024-03-10 10:59:22 UTC",
      "updated_date": "2024-03-10 10:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:07:27.524269"
    },
    {
      "arxiv_id": "2403.06168v2",
      "title": "DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaobin Hu",
        "Xu Peng",
        "Donghao Luo",
        "Xiaozhong Ji",
        "Jinlong Peng",
        "Zhengkai Jiang",
        "Jiangning Zhang",
        "Taisong Jin",
        "Chengjie Wang",
        "Rongrong Ji"
      ],
      "abstract": "Due to the difficulty and labor-consuming nature of getting highly accurate\nor matting annotations, there only exists a limited amount of highly accurate\nlabels available to the public. To tackle this challenge, we propose a\nDiffuMatting which inherits the strong Everything generation ability of\ndiffusion and endows the power of \"matting anything\". Our DiffuMatting can 1).\nact as an anything matting factory with high accurate annotations 2). be\nwell-compatible with community LoRAs or various conditional control approaches\nto achieve the community-friendly art design and controllable generation.\nSpecifically, inspired by green-screen-matting, we aim to teach the diffusion\nmodel to paint on a fixed green screen canvas. To this end, a large-scale\ngreenscreen dataset (Green100K) is collected as a training dataset for\nDiffuMatting. Secondly, a green background control loss is proposed to keep the\ndrawing board as a pure green color to distinguish the foreground and\nbackground. To ensure the synthesized object has more edge details, a\ndetailed-enhancement of transition boundary loss is proposed as a guideline to\ngenerate objects with more complicated edge structures. Aiming to\nsimultaneously generate the object and its matting annotation, we build a\nmatting head to make a green color removal in the latent space of the VAE\ndecoder. Our DiffuMatting shows several potential applications (e.g.,\nmatting-data generator, community-friendly art design and controllable\ngeneration). As a matting-data generator, DiffuMatting synthesizes general\nobject and portrait matting sets, effectively reducing the relative MSE error\nby 15.4% in General Object Matting and 11.4% in Portrait Matting tasks. The\ndataset is released in our project page at\n\\url{https://diffumatting.github.io}.",
      "tldr_zh": "该研究提出 DiffuMatting，一种基于 diffusion 模型的框架，用于合成任意对象并生成高精度 matting-level 注解，以解决标签稀缺的问题。该框架借鉴绿屏抠像技术，构建 Green100K 数据集，并引入绿背景控制损失（green background control loss）和详细增强过渡边界损失（detailed-enhancement of transition boundary loss），确保对象边缘细节精确，同时兼容社区 LoRAs 和条件控制方法。实验表明，DiffuMatting 作为 matting 数据生成器，能将一般对象 matting 的 MSE 错误减少 15.4%，人像 matting 减少 11.4%，并发布了数据集以支持进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper was accepted by ECCV 2024, and the project page is\n  accessible at: \\url{https://diffumatting.github.io}",
      "pdf_url": "http://arxiv.org/pdf/2403.06168v2",
      "published_date": "2024-03-10 10:39:32 UTC",
      "updated_date": "2024-08-21 11:35:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:07:43.048846"
    },
    {
      "arxiv_id": "2404.15243v1",
      "title": "UCINet0: A Machine Learning based Receiver for 5G NR PUCCH Format 0",
      "title_zh": "翻译失败",
      "authors": [
        "Anil Kumar Yerrapragada",
        "Jeeva Keshav Sattianarayanin",
        "Radha Krishna Ganti"
      ],
      "abstract": "Accurate decoding of Uplink Control Information (UCI) on the Physical Uplink\nControl Channel (PUCCH) is essential for enabling 5G wireless links. This paper\nexplores an AI/ML-based receiver design for PUCCH Format 0. Format 0 signaling\nencodes the UCI content within the phase of a known base waveform and even\nsupports multiplexing of up to 12 users within the same time-frequency\nresources. Our first-of-a-kind neural network classifier, which we term\nUCINet0, is capable of predicting when no user is transmitting on the PUCCH, as\nwell as decoding the UCI content of any number of multiplexed users, up to 12.\nInference results with both simulated and hardware-captured field datasets show\nthat the UCINet0 model outperforms conventional DFT-based decoders across all\nSNR ranges.",
      "tldr_zh": "本论文提出了一种基于机器学习的接收器 UCINet0，用于 5G NR 的 PUCCH Format 0，以提高 Uplink Control Information (UCI) 的准确解码。UCINet0 作为首创的神经网络分类器，能够检测无用户传输情况，并处理多达 12 个多路复用用户的 UCI 内容，通过相位编码技术实现高效信号处理。实验结果显示，在模拟和硬件捕获的数据集上，UCINet0 在所有 SNR 范围内优于传统的 DFT-based decoders，显著提升了 5G 无线链路的性能。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.15243v1",
      "published_date": "2024-03-10 09:56:02 UTC",
      "updated_date": "2024-03-10 09:56:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:07:52.341566"
    },
    {
      "arxiv_id": "2403.06149v2",
      "title": "Can Large Language Models Automatically Score Proficiency of Written Essays?",
      "title_zh": "翻译失败",
      "authors": [
        "Watheq Mansour",
        "Salam Albatarni",
        "Sohaila Eltanbouly",
        "Tamer Elsayed"
      ],
      "abstract": "Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 是否能有效进行自动论文评分 (AES)，通过测试 ChatGPT 和 Llama 模型来评估其分析和评分能力。研究者利用提示工程 (prompt-engineering) 设计了四种不同提示，并在 ASAP 数据集上进行实验，结果显示合适的提示选择取决于模型和任务性质。ChatGPT 和 Llama 的平均表现相当，ChatGPT 略占优势，但与 state-of-the-art (SOTA) 模型相比仍有差距；然而，LLMs 能提供反馈以提升论文质量，从而为教师和学生带来潜在帮助。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "V2 (published version of LREC-COLING 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.06149v2",
      "published_date": "2024-03-10 09:39:00 UTC",
      "updated_date": "2024-04-16 00:24:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:08:04.270749"
    },
    {
      "arxiv_id": "2403.06145v1",
      "title": "All-in-one platform for AI R&D in medical imaging, encompassing data collection, selection, annotation, and pre-processing",
      "title_zh": "翻译失败",
      "authors": [
        "Changhee Han",
        "Kyohei Shibano",
        "Wataru Ozaki",
        "Keishiro Osaki",
        "Takafumi Haraguchi",
        "Daisuke Hirahara",
        "Shumon Kimura",
        "Yasuyuki Kobayashi",
        "Gento Mogi"
      ],
      "abstract": "Deep Learning is advancing medical imaging Research and Development (R&D),\nleading to the frequent clinical use of Artificial Intelligence/Machine\nLearning (AI/ML)-based medical devices. However, to advance AI R&D, two\nchallenges arise: 1) significant data imbalance, with most data from\nEurope/America and under 10% from Asia, despite its 60% global population\nshare; and 2) hefty time and investment needed to curate proprietary datasets\nfor commercial use. In response, we established the first commercial medical\nimaging platform, encompassing steps like: 1) data collection, 2) data\nselection, 3) annotation, and 4) pre-processing. Moreover, we focus on\nharnessing under-represented data from Japan and broader Asia, including\nComputed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.\nUsing the collected data, we are preparing/providing ready-to-use datasets for\nmedical AI R&D by 1) offering these datasets to AI firms, biopharma, and\nmedical device makers and 2) using them as training/test data to develop\ntailored AI solutions for such entities. We also aim to merge Blockchain for\ndata security and plan to synthesize rare disease data via generative AI.\nDataHub Website: https://medical-datahub.ai/",
      "tldr_zh": "该研究提出了一种综合平台，用于推进医疗成像领域的AI R&D，旨在解决数据不平衡问题（如亚洲数据仅占全球10%以下）和构建专有数据集的资源消耗挑战。平台涵盖数据收集、选择、标注和预处理四个关键步骤，特别聚焦于日本和亚洲的 underrepresented 数据，包括Computed Tomography (CT)、Magnetic Resonance Imaging (MRI) 和Whole Slide Imaging (WSI) 扫描。利用这些数据，平台提供ready-to-use数据集给AI公司、生物制药和医疗设备制造商，并支持开发定制AI解决方案。此外，未来计划整合Blockchain以提升数据安全，并使用generative AI合成稀有疾病数据，从而促进更公平和高效的医疗AI创新。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 3 figures, accepted to SPIE Medical Imaging 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06145v1",
      "published_date": "2024-03-10 09:24:53 UTC",
      "updated_date": "2024-03-10 09:24:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:08:17.406065"
    },
    {
      "arxiv_id": "2403.06143v1",
      "title": "Fluent: Round-efficient Secure Aggregation for Private Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xincheng Li",
        "Jianting Ning",
        "Geong Sen Poh",
        "Leo Yu Zhang",
        "Xinchun Yin",
        "Tianwei Zhang"
      ],
      "abstract": "Federated learning (FL) facilitates collaborative training of machine\nlearning models among a large number of clients while safeguarding the privacy\nof their local datasets. However, FL remains susceptible to vulnerabilities\nsuch as privacy inference and inversion attacks. Single-server secure\naggregation schemes were proposed to address these threats. Nonetheless, they\nencounter practical constraints due to their round and communication\ncomplexities. This work introduces Fluent, a round and communication-efficient\nsecure aggregation scheme for private FL. Fluent has several improvements\ncompared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et\nal. (SP 2023): (1) it eliminates frequent handshakes and secret sharing\noperations by efficiently reusing the shares across multiple training\niterations without leaking any private information; (2) it accomplishes both\nthe consistency check and gradient unmasking in one logical step, thereby\nreducing another round of communication. With these innovations, Fluent\nachieves the fewest communication rounds (i.e., two in the collection phase) in\nthe malicious server setting, in contrast to at least three rounds in existing\nschemes. This significantly minimizes the latency for geographically\ndistributed clients; (3) Fluent also introduces Fluent-Dynamic with a\nparticipant selection algorithm and an alternative secret sharing scheme. This\ncan facilitate dynamic client joining and enhance the system flexibility and\nscalability. We implemented Fluent and compared it with existing solutions.\nExperimental results show that Fluent improves the computational cost by at\nleast 75% and communication overhead by at least 25% for normal clients. Fluent\nalso reduces the communication overhead for the server at the expense of a\nmarginal increase in computational cost.",
      "tldr_zh": "这篇论文提出Fluent，一种高效的Secure Aggregation方案，用于Private Federated Learning（FL），旨在解决FL在隐私推断和逆向攻击方面的漏洞，同时降低通信轮次和开销。Fluent通过高效重用shares消除频繁握手和秘密共享操作，并在单一逻辑步骤中完成一致性检查和梯度unmasking，从而在恶意服务器设置下仅需两个通信轮次，比现有方案减少至少一轮，显著降低地理分布客户端的延迟。该方案还引入Fluent-Dynamic，支持动态客户端加入和参与者选择算法，提升系统灵活性和可扩展性。实验结果显示，Fluent至少减少75%的计算成本和25%的通信开销，为服务器带来通信优化，尽管计算成本略有增加。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06143v1",
      "published_date": "2024-03-10 09:11:57 UTC",
      "updated_date": "2024-03-10 09:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:08:28.327768"
    },
    {
      "arxiv_id": "2403.06139v1",
      "title": "Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Zhang",
        "Linhai Zhang",
        "Deyu Zhou",
        "Guoqiang Xu"
      ],
      "abstract": "Due to the sparsity of user data, sentiment analysis on user reviews in\ne-commerce platforms often suffers from poor performance, especially when faced\nwith extremely sparse user data or long-tail labels. Recently, the emergence of\nLLMs has introduced new solutions to such problems by leveraging graph\nstructures to generate supplementary user profiles. However, previous\napproaches have not fully utilized the graph understanding capabilities of LLMs\nand have struggled to adapt to complex streaming data environments. In this\nwork, we propose a fine-grained streaming data synthesis framework that\ncategorizes sparse users into three categories: Mid-tail, Long-tail, and\nExtreme. Specifically, we design LLMs to comprehensively understand three key\ngraph elements in streaming data, including Local-global Graph Understanding,\nSecond-Order Relationship Extraction, and Product Attribute Understanding,\nwhich enables the generation of high-quality synthetic data to effectively\naddress sparsity across different categories. Experimental results on three\nreal datasets demonstrate significant performance improvements, with\nsynthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,\nrespectively.",
      "tldr_zh": "该论文针对电商平台用户评论数据稀疏性问题，提出一个细粒度的流式数据合成框架，利用大型语言模型(LLMs)来理解图结构，从而生成高质量的补充数据。框架将稀疏用户分类为 Mid-tail、Long-tail 和 Extreme 三类，并设计 LLMs 全面处理三个关键图元素：Local-global Graph Understanding、Second-Order Relationship Extraction 和 Product Attribute Understanding，以有效缓解不同类别的数据稀疏。实验结果在三个真实数据集上显示，合成数据使 MSE 分别减少 45.85%、3.16% 和 62.21%，显著提升了情感分析性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06139v1",
      "published_date": "2024-03-10 08:59:04 UTC",
      "updated_date": "2024-03-10 08:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:08:41.631997"
    },
    {
      "arxiv_id": "2403.06135v1",
      "title": "MACE: Mass Concept Erasure in Diffusion Models",
      "title_zh": "MACE：扩散模型中的大规模概念擦除",
      "authors": [
        "Shilin Lu",
        "Zilan Wang",
        "Leyang Li",
        "Yanzhu Liu",
        "Adams Wai-Kin Kong"
      ],
      "abstract": "The rapid expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns regarding their potential misuse in creating harmful or\nmisleading content. In this paper, we introduce MACE, a finetuning framework\nfor the task of mass concept erasure. This task aims to prevent models from\ngenerating images that embody unwanted concepts when prompted. Existing concept\nerasure methods are typically restricted to handling fewer than five concepts\nsimultaneously and struggle to find a balance between erasing concept synonyms\n(generality) and maintaining unrelated concepts (specificity). In contrast,\nMACE differs by successfully scaling the erasure scope up to 100 concepts and\nby achieving an effective balance between generality and specificity. This is\nachieved by leveraging closed-form cross-attention refinement along with LoRA\nfinetuning, collectively eliminating the information of undesirable concepts.\nFurthermore, MACE integrates multiple LoRAs without mutual interference. We\nconduct extensive evaluations of MACE against prior methods across four\ndifferent tasks: object erasure, celebrity erasure, explicit content erasure,\nand artistic style erasure. Our results reveal that MACE surpasses prior\nmethods in all evaluated tasks. Code is available at\nhttps://github.com/Shilin-LU/MACE.",
      "tldr_zh": "这篇论文引入了 MACE，一种针对文本到图像扩散模型的微调框架，用于实现大规模概念擦除（mass concept erasure），以防止模型生成包含 unwanted concepts 的图像。MACE 通过结合 closed-form cross-attention refinement 和 LoRA finetuning，能够同时处理多达 100 个概念，并有效平衡 generality（擦除同义概念）和 specificity（保留无关概念）。实验评估显示，MACE 在 object erasure、celebrity erasure、explicit content erasure 和 artistic style erasure 等任务中，均优于现有方法，为减少模型潜在误用提供了可靠解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06135v1",
      "published_date": "2024-03-10 08:50:56 UTC",
      "updated_date": "2024-03-10 08:50:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:08:52.424890"
    },
    {
      "arxiv_id": "2403.06131v2",
      "title": "FewFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning",
      "title_zh": "FewFedPIT：迈向隐私保护和少样本联邦指令微调",
      "authors": [
        "Zhuo Zhang",
        "Jingyuan Zhang",
        "Jintao Huang",
        "Lizhen Qu",
        "Hongzhi Zhang",
        "Qifan Wang",
        "Xun Zhou",
        "Zenglin Xu"
      ],
      "abstract": "Instruction tuning has been identified as a crucial technique for optimizing\nthe performance of large language models (LLMs) in generating human-aligned\nresponses. Nonetheless, gathering diversified and superior-quality instruction\ndata for such tuning presents notable obstacles, especially in domains with\nrigid privacy provisions. Federated instruction tuning (FedIT) has emerged as a\npromising solution, by consolidating collaborative training across multiple\ndata owners, thereby resulting in a privacy-preserving learning model. However,\nFedIT encounters limitations such as scarcity of instructional data and risk of\nexposure to training data extraction attacks. In this paper, we propose a novel\nfederated algorithm, FewFedPIT, designed to simultaneously enhance privacy\nprotection and model performance of federated few-shot learning.\nFewFedPITcomprises three vital components on the client side: (1) synthetic\ndata generation, which utilizes LLMs' in-context learning capacity to generate\nsynthetic data autonomously, thus expanding the local database; (2) parameter\nisolation training, which individually updates the public parameters in the\nsynthetic data and the private parameters in the local data, consequently\nmitigating the noise impact of the synthetic data; (3) local aggregation\nsharing, which mixes public and private parameters before uploading,\neffectively preventing data extraction attacks. Extensive experiments on three\nopen-source datasets demonstrate the effectiveness of FewFedPITin, enhancing\nprivacy preservation and improving federated few-shot performance.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）的指令微调（instruction tuning）问题，提出了一种新型联邦算法 FewFedPIT，以解决数据隐私风险和少样本学习挑战。FewFedPIT 在客户端包括三个关键组件：利用 LLMs 的 in-context learning 生成合成数据来扩展本地数据库、通过参数隔离训练分别更新合成数据中的公共参数和本地数据中的私有参数以减少噪声影响，以及本地聚合共享机制混合参数上传以防范数据提取攻击。实验结果显示，该算法在三个开源数据集上显著提升了隐私保护水平，并改善了联邦少样本性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2403.06131v2",
      "published_date": "2024-03-10 08:41:22 UTC",
      "updated_date": "2024-06-20 13:00:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:09:05.223676"
    },
    {
      "arxiv_id": "2403.06115v1",
      "title": "FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language",
      "title_zh": "FMPAF：美联储主席如何影响金融市场？一种细粒度的货币政策语言分析框架",
      "authors": [
        "Yayue Deng",
        "Mohan Xu",
        "Yao Tang"
      ],
      "abstract": "The effectiveness of central bank communication is a crucial aspect of\nmonetary policy transmission. While recent research has examined the influence\nof policy communication by the chairs of the Federal Reserve on various\nfinancial variables, much of the literature relies on rule-based or\ndictionary-based methods in parsing the language of the chairs, leaving nuanced\ninformation about policy stance contained in nonverbal emotion out of the\nanalysis. In the current study, we propose the Fine-Grained Monetary Policy\nAnalysis Framework (FMPAF), a novel approach that integrates large language\nmodels (LLMs) with regression analysis to provide a comprehensive analysis of\nthe impact of the press-conference communications of chairs of the Federal\nReserve on financial markets. We conduct extensive comparisons of model\nperformance under different levels of granularity, modalities, and\ncommunication scenarios. Based on our preferred specification, a one-unit\nincrease in the sentiment score is associated with an increase of the price of\nS\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a\n15-basis-point decrease in the policy interest rate, while not leading to a\nsignificant response in exchange rates.",
      "tldr_zh": "本研究提出FMPAF框架，利用大型语言模型(LLMs)和回归分析，对美联储(Fed)主席新闻发布会沟通的细粒度信息（如情感和非语言因素）进行全面分析，以评估其对金融市场的冲击。相比传统规则或字典方法，该框架更能捕捉政策沟通的 nuances，并通过不同粒度、模态和场景的模型性能比较来优化分析。结果表明，情感分数每增加一单位，会使S&P 500 ETF价格上涨约500基点、政策利率下降15基点，但对汇率无显著影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted by AAAI 2024 Workshop: AI in Finance for Social Impact",
      "pdf_url": "http://arxiv.org/pdf/2403.06115v1",
      "published_date": "2024-03-10 07:21:31 UTC",
      "updated_date": "2024-03-10 07:21:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:09:18.074513"
    },
    {
      "arxiv_id": "2403.06108v2",
      "title": "Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Kaipeng Wang",
        "Zhi Jing",
        "Yongye Su",
        "Yikun Han"
      ],
      "abstract": "This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型（Large Language Models）提升细粒度情感检测性能，针对GoEmotions数据集——一个大型手动标注的文本情感检测数据集。研究通过数据增强（Data Augmentation）和迁移学习（Transfer Learning）方法，解决了Natural Language Processing (NLP)中检测微妙情感的挑战，提升了分类准确性。论文提供了宝贵的见解，并建议未来研究方向，如撰写综述论文来整合不同数据集中的方法和性能表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06108v2",
      "published_date": "2024-03-10 06:30:54 UTC",
      "updated_date": "2024-04-09 16:38:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:09:29.291947"
    },
    {
      "arxiv_id": "2403.06097v2",
      "title": "Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Yao",
        "Sichun Luo",
        "Haohan Zhao",
        "Guanzhi Deng",
        "Linqi Song"
      ],
      "abstract": "We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.",
      "tldr_zh": "本研究探讨了大型语言模型(LLM)是否能替代人类标注，通过构建CNER-UAV数据集——一个针对无人机(UAV)交付系统的细粒度中文命名实体识别(NER)数据集来进行案例分析。该数据集涵盖五类实体，包含约12,000个从真实UAV系统获取的标注样本，并经过严格的数据清洗和脱敏处理，由人类专家和LLM共同标注。实验评估了经典NER模型的性能，并提供了深入分析，揭示了LLM在地址解析任务中的潜在优势。该数据集及其模型已在GitHub上公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by TheWebConf'24 (WWW'24) as a Resource Paper",
      "pdf_url": "http://arxiv.org/pdf/2403.06097v2",
      "published_date": "2024-03-10 05:12:16 UTC",
      "updated_date": "2024-03-19 11:36:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:09:42.585860"
    },
    {
      "arxiv_id": "2403.06095v4",
      "title": "RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion",
      "title_zh": "翻译失败",
      "authors": [
        "Huy N. Phan",
        "Hoang N. Phan",
        "Tien N. Nguyen",
        "Nghi D. Q. Bui"
      ],
      "abstract": "Code Large Language Models (CodeLLMs) have demonstrated impressive\nproficiency in code completion tasks. However, they often fall short of fully\nunderstanding the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies, which can result in less\nprecise completions. To overcome these limitations, we present \\tool, a\nmultifaceted framework designed to address the complex challenges associated\nwith repository-level code completion. Central to RepoHYPER is the {\\em\nRepo-level Semantic Graph} (RSG), a novel semantic graph structure that\nencapsulates the vast context of code repositories. Furthermore, RepoHyper\nleverages Expand and Refine retrieval method, including a graph expansion and a\nlink prediction algorithm applied to the RSG, enabling the effective retrieval\nand prioritization of relevant code snippets. Our evaluations show that \\tool\nmarkedly outperforms existing techniques in repository-level code completion,\nshowcasing enhanced accuracy across various datasets when compared to several\nstrong baselines. Our implementation of RepoHYPER can be found at\nhttps://github.com/FSoft-AI4Code/RepoHyper.",
      "tldr_zh": "该论文提出RepoHyper框架，以解决CodeLLMs在仓库级代码补全中对项目上下文（如相关文件和类层次结构）理解不足的问题。RepoHyper的核心是Repo-level Semantic Graph (RSG)，一种新型语义图结构，用于封装代码仓库的广泛上下文，并通过Expand and Refine检索方法（包括图扩展和链接预测算法）有效检索和优先化相关代码片段。实验评估显示，RepoHyper在多个数据集上显著优于现有基线模型，提升了代码补全的准确率。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06095v4",
      "published_date": "2024-03-10 05:10:34 UTC",
      "updated_date": "2024-08-14 16:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:09:55.064625"
    },
    {
      "arxiv_id": "2403.06088v1",
      "title": "Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Esmaeil Seraj",
        "Walter Talamonti"
      ],
      "abstract": "In the burgeoning field of intelligent transportation systems, enhancing\nvehicle-driver interaction through facial attribute recognition, such as facial\nexpression, eye gaze, age, etc., is of paramount importance for safety,\npersonalization, and overall user experience. However, the scarcity of\ncomprehensive large-scale, real-world datasets poses a significant challenge\nfor training robust multi-task models. Existing literature often overlooks the\npotential of synthetic datasets and the comparative efficacy of\nstate-of-the-art vision foundation models in such constrained settings. This\npaper addresses these gaps by investigating the utility of synthetic datasets\nfor training complex multi-task models that recognize facial attributes of\npassengers of a vehicle, such as gaze plane, age, and facial expression.\nUtilizing transfer learning techniques with both pre-trained Vision Transformer\n(ViT) and Residual Network (ResNet) models, we explore various training and\nadaptation methods to optimize performance, particularly when data availability\nis limited. We provide extensive post-evaluation analysis, investigating the\neffects of synthetic data distributions on model performance in in-distribution\ndata and out-of-distribution inference. Our study unveils counter-intuitive\nfindings, notably the superior performance of ResNet over ViTs in our specific\nmulti-task context, which is attributed to the mismatch in model complexity\nrelative to task complexity. Our results highlight the challenges and\nopportunities for enhancing the use of synthetic data and vision foundation\nmodels in practical applications.",
      "tldr_zh": "本研究探讨了在智能交通系统中，使用合成数据和视觉基础模型（vision foundation models）来提升车辆内多任务面部属性识别（multi-task facial attribute recognition），如注视平面（gaze plane）、年龄和面部表情，以改善安全性和用户体验。作者通过转移学习（transfer learning）技术，利用预训练的 Vision Transformer (ViT) 和 Residual Network (ResNet) 模型，训练多任务模型并优化适应方法，尤其在数据有限的情况下。实验结果显示，ResNet 在此特定多任务语境中优于 ViT，这归因于模型复杂度和任务复杂度的不匹配；此外，研究分析了合成数据分布对模型性能的影响，揭示了其在分布内和分布外推断中的挑战与潜力。总的来说，该工作突出了合成数据在实际应用中的机遇，为未来模型优化提供了指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Manuscript under peer review",
      "pdf_url": "http://arxiv.org/pdf/2403.06088v1",
      "published_date": "2024-03-10 04:17:54 UTC",
      "updated_date": "2024-03-10 04:17:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:10:07.147851"
    },
    {
      "arxiv_id": "2403.06086v1",
      "title": "Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Juanwu Lu",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Yeping Hu"
      ],
      "abstract": "Estimating the potential behavior of the surrounding human-driven vehicles is\ncrucial for the safety of autonomous vehicles in a mixed traffic flow. Recent\nstate-of-the-art achieved accurate prediction using deep neural networks.\nHowever, these end-to-end models are usually black boxes with weak\ninterpretability and generalizability. This paper proposes the Goal-based\nNeural Variational Agent (GNeVA), an interpretable generative model for motion\nprediction with robust generalizability to out-of-distribution cases. For\ninterpretability, the model achieves target-driven motion prediction by\nestimating the spatial distribution of long-term destinations with a\nvariational mixture of Gaussians. We identify a causal structure among maps and\nagents' histories and derive a variational posterior to enhance\ngeneralizability. Experiments on motion prediction datasets validate that the\nfitted model can be interpretable and generalizable and can achieve comparable\nperformance to state-of-the-art results.",
      "tldr_zh": "这篇论文针对自动驾驶车辆的安全需求，提出了一种可解释且泛化性强的运动预测模型GNeVA（Goal-based Neural Variational Agent），基于Deep Variational Bayes方法来处理预测中的不确定性。GNeVA通过估计长期目的地的空间分布（使用variational mixture of Gaussians）和识别地图与代理历史之间的因果结构，推导变分后验以提升对分布外场景的鲁棒性。实验结果显示，该模型在运动预测数据集上实现了与最先进方法相当的性能，同时显著提高了可解释性和泛化能力。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at AISTATS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06086v1",
      "published_date": "2024-03-10 04:16:04 UTC",
      "updated_date": "2024-03-10 04:16:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:10:18.521788"
    },
    {
      "arxiv_id": "2403.07022v1",
      "title": "A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units",
      "title_zh": "翻译失败",
      "authors": [
        "Liyue Chen",
        "Jiangyi Fang",
        "Tengfei Liu",
        "Shaosheng Cao",
        "Leye Wang"
      ],
      "abstract": "Spatio-Temporal (ST) prediction is crucial for making informed decisions in\nurban location-based applications like ride-sharing. However, existing ST\nmodels often require region partition as a prerequisite, resulting in two main\npitfalls. Firstly, location-based services necessitate ad-hoc regions for\nvarious purposes, requiring multiple ST models with varying scales and zones,\nwhich can be costly to support. Secondly, different ST models may produce\nconflicting outputs, resulting in confusing predictions. In this paper, we\npropose One4All-ST, a framework that can conduct ST prediction for arbitrary\nmodifiable areal units using only one model. To reduce the cost of getting\nmulti-scale predictions, we design an ST network with hierarchical spatial\nmodeling and scale normalization modules to efficiently and equally learn\nmulti-scale representations. To address prediction inconsistencies across\nscales, we propose a dynamic programming scheme to solve the formulated optimal\ncombination problem, minimizing predicted error through theoretical analysis.\nBesides, we suggest using an extended quad-tree to index the optimal\ncombinations for quick response to arbitrary modifiable areal units in\npractical online scenarios. Extensive experiments on two real-world datasets\nverify the efficiency and effectiveness of One4All-ST in ST prediction for\narbitrary modifiable areal units. The source codes and data of this work are\navailable at https://github.com/uctb/One4All-ST.",
      "tldr_zh": "本研究针对空间-时间 (ST) 预测在城市位置服务中的应用，指出现有模型依赖预分区导致支持多规模区域成本高昂且预测结果可能冲突的问题。提出 One4All-ST 框架，使用单一模型实现任意可修改区域单元的 ST 预测，通过 hierarchical spatial modeling 和 scale normalization modules 高效学习多规模表示，并采用动态规划方案最小化预测错误。实验在两个真实数据集上验证了 One4All-ST 的效率和有效性，并提供了开源代码（https://github.com/uctb/One4All-ST）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICDE 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.07022v1",
      "published_date": "2024-03-10 02:34:44 UTC",
      "updated_date": "2024-03-10 02:34:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:10:31.069930"
    },
    {
      "arxiv_id": "2403.06064v3",
      "title": "L^2GC:Lorentzian Linear Graph Convolutional Networks for Node Classification",
      "title_zh": "L^2GC：Lorentzian 线性图卷积网络用于节点",
      "authors": [
        "Qiuyu Liang",
        "Weihua Wang",
        "Feilong Bao",
        "Guanglai Gao"
      ],
      "abstract": "Linear Graph Convolutional Networks (GCNs) are used to classify the node in\nthe graph data. However, we note that most existing linear GCN models perform\nneural network operations in Euclidean space, which do not explicitly capture\nthe tree-like hierarchical structure exhibited in real-world datasets that\nmodeled as graphs. In this paper, we attempt to introduce hyperbolic space into\nlinear GCN and propose a novel framework for Lorentzian linear GCN.\nSpecifically, we map the learned features of graph nodes into hyperbolic space,\nand then perform a Lorentzian linear feature transformation to capture the\nunderlying tree-like structure of data. Experimental results on standard\ncitation networks datasets with semi-supervised learning show that our approach\nyields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and\n81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be\ntrained up to two orders of magnitude faster than other nonlinear GCN models on\nPubMed dataset. Our code is publicly available at\nhttps://github.com/llqy123/LLGC-master.",
      "tldr_zh": "本论文提出 L^2GC（Lorentzian Linear Graph Convolutional Networks），一种将节点特征映射到双曲空间（hyperbolic space）的线性图卷积网络框架，用于节点分类任务，以更好地捕获真实图数据中的树状层次结构。相比传统在欧氏空间操作的 Linear GCNs，该方法通过 Lorentzian 线性特征变换来处理数据结构。实验结果显示，在半监督学习任务上，L^2GC 在 Citeseer 数据集上达到 74.7% 的准确率，在 PubMed 数据集上达到 81.3%，并比其他非线性 GCN 模型快两个数量级训练速度。总的来说，该框架提升了节点分类的性能和效率，为图神经网络研究提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.06064v3",
      "published_date": "2024-03-10 02:16:13 UTC",
      "updated_date": "2024-06-14 04:15:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:10:42.853755"
    },
    {
      "arxiv_id": "2403.06063v1",
      "title": "Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue",
      "title_zh": "针对目标导向主动对话生成的目标约束双向规划",
      "authors": [
        "Jian Wang",
        "Dongding Lin",
        "Wenjie Li"
      ],
      "abstract": "Target-oriented proactive dialogue systems aim to lead conversations from a\ndialogue context toward a pre-determined target, such as making recommendations\non designated items or introducing new specific topics. To this end, it is\ncritical for such dialogue systems to plan reasonable actions to drive the\nconversation proactively, and meanwhile, to plan appropriate topics to move the\nconversation forward to the target topic smoothly. In this work, we mainly\nfocus on effective dialogue planning for target-oriented dialogue generation.\nInspired by decision-making theories in cognitive science, we propose a novel\ntarget-constrained bidirectional planning (TRIP) approach, which plans an\nappropriate dialogue path by looking ahead and looking back. By formulating the\nplanning as a generation task, our TRIP bidirectionally generates a dialogue\npath consisting of a sequence of <action, topic> pairs using two Transformer\ndecoders. They are expected to supervise each other and converge on consistent\nactions and topics by minimizing the decision gap and contrastive generation of\ntargets. Moreover, we propose a target-constrained decoding algorithm with a\nbidirectional agreement to better control the planning process. Subsequently,\nwe adopt the planned dialogue paths to guide dialogue generation in a pipeline\nmanner, where we explore two variants: prompt-based generation and\nplan-controlled generation. Extensive experiments are conducted on two\nchallenging dialogue datasets, which are re-purposed for exploring\ntarget-oriented dialogue. Our automatic and human evaluations demonstrate that\nthe proposed methods significantly outperform various baseline models.",
      "tldr_zh": "这篇论文提出了一种目标约束双向规划（TRIP）方法，用于生成目标导向的主动对话系统，该方法受认知科学决策理论启发，通过双向生成（looking ahead 和 looking back）来规划对话路径，包括一系列 <action, topic> 配对，并使用两个 Transformer decoders 相互监督以最小化决策差距和确保一致性。TRIP 还引入了目标约束解码算法来控制规划过程，并将生成的对话路径应用于对话生成的两类变体：基于提示的生成和计划控制生成。实验在重新适配的挑战性对话数据集上显示，该方法在自动和人工评估中显著优于各种基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACM Transactions on Information Systems (TOIS)",
      "pdf_url": "http://arxiv.org/pdf/2403.06063v1",
      "published_date": "2024-03-10 02:14:24 UTC",
      "updated_date": "2024-03-10 02:14:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:10:54.607729"
    },
    {
      "arxiv_id": "2403.06054v5",
      "title": "Decoupled Data Consistency with Diffusion Purification for Image Restoration",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Li",
        "Soo Min Kwon",
        "Ismail R. Alkhouri",
        "Saiprasad Ravishankar",
        "Qing Qu"
      ],
      "abstract": "Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.",
      "tldr_zh": "该论文提出了一种新的图像恢复方法，通过解耦数据一致性步骤与扩散模型（Diffusion models）的逆过程，解决了现有技术的计算开销和推理时间问题。该方法交替进行重建阶段（保持数据一致性）和精炼阶段（利用Diffusion Purification强制执行先验），并整合一致性模型减少采样步骤，从而提升效率和适用性。实验在图像去噪、去模糊、修复和超分辨率等任务上验证了其有效性，展示了在潜在空间中的强大适应能力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.06054v5",
      "published_date": "2024-03-10 00:47:05 UTC",
      "updated_date": "2024-05-29 00:09:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T14:11:06.402653"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 40,
  "processed_papers_count": 40,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T14:11:21.821575"
}
{
  "date": "2024-02-03",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-03 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 54 篇论文，主要聚焦于 AI 和机器学习的创新应用，包括图神经网络理论进展、语言模型优化、联邦学习安全机制，以及医疗和多模态模型等领域。其中，Christopher Morris 等知名学者参与的《Future Directions in the Theory of Graph Machine Learning》论文令人印象深刻，讨论了图机器学习的表达力、泛化性和优化问题；此外，EACL 2024 相关论文如 SynthDST 展示了合成数据在对话状态跟踪中的潜力。\n\n下面，我将挑选并简要讨论部分重要或有话题度的论文，先从理论和创新性强的入手，再快速掠过其他领域的内容。相关论文会归类讨论，以控制篇幅。\n\n### 图神经网络与机器学习理论\n- **Future Directions in the Theory of Graph Machine Learning（图机器学习的未来方向）**  \n  作者包括 Christopher Morris、Michael Bronstein 等知名学者。该论文审视了图神经网络（GNNs）的表达力、泛化性和优化问题，强调需要平衡理论框架以更好地适应实际训练。关键发现：现有研究偏重组合技术，但未充分解决 GNNs 在随机优化下的泛化行为，为图机器学习领域提供了新方向（ICML 2024 评论）。\n\n- **SemPool: Simple, robust, and interpretable KG pooling for enhancing language models（SemPool: 用于增强语言模型的简单、鲁棒且可解释的知识图池化）**  \n  作者：Costas Mavromatis、George Karypis 等。该方法提出了一种知识图（KG）池化技术 SemPool，通过预训练语言模型聚合语义信息，并在语言模型层融合，提升了问答任务的准确性。贡献：当 KG 中缺少答案信息时，SemPool 比现有 GNN 方法提高 2.27% 的准确率，并提供可解释性。\n\n其他如 **Multi-Level Aggregation and Recursive Alignment Architecture for Efficient Parallel Inference Segmentation Network（多级聚合和递归对齐架构用于高效并行推理分割网络）**，聚焦实时语义分割，提出 MFAM 和 RAM 模块以平衡速度和准确性，在 Cityscapes 数据集上超越 SOTA 方法，但细节较具体，故快速掠过。\n\n### 语言模型与生成技术\n- **SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking（SynthDST: 合成数据是 Few-Shot 对话状态跟踪所需的一切）**  \n  该论文创新性地使用大语言模型生成合成数据，减少对标注数据的依赖。贡献：在 MultiWOZ 数据集上，Few-Shot 学习比零样本基准提升 4-5% 的 Joint Goal Accuracy，并接近使用真实标注数据的性能（EACL 2024）。\n\n- **BetterV: Controlled Verilog Generation with Discriminative Guidance（BetterV: 使用判别引导的控制 Verilog 生成）**  \n  聚焦硬件设计语言生成，提出框架通过微调大语言模型和生成鉴别器优化 Verilog 代码。发现：在 VerilogEval 基准上，BetterV 超越 GPT-4，并在 EDA 任务中减少节点和验证时间。\n\n其他语言模型相关，如 **Do Moral Judgment and Reasoning Capability of LLMs Change with Language?（大语言模型的道德判断和推理能力会随语言变化吗？）**，探索多语言道德评估，发现模型在印地语和斯瓦希里语上的表现较差（EACL 2024），但非核心创新，故简要提及。\n\n### 联邦学习与安全机制\n- **Federated Learning with New Knowledge: Fundamentals, Advances, and Futures（带新知识的联邦学习：基础、进展和未来）**  \n  作者：Lixu Wang 等。该论文系统讨论了联邦学习中融入新特征、任务和算法的挑战，并分析新知识的影响。贡献：提供未来方向，如效率和安全改进，并附带更新仓库，强调可持续性发展。\n\n其他联邦学习论文，如 **Federated Learning with Differential Privacy（带差分隐私的联邦学习）**，展示非独立同分布数据下的性能下降，但实验性强，故快速掠过。\n\n### 医疗与应用AI\n- **XTSFormer: Cross-Temporal-Scale Transformer for Irregular-Time Event Prediction in Clinical Applications（XTSFormer: 用于临床不规则时间事件预测的跨时间尺度 Transformer）**  \n  针对电子健康记录的临床事件预测，提出 FCPE 和多尺度注意力机制。贡献：在真实数据集上超越基线，提升了周期性和多尺度交互建模（AAAI 2025）。\n\n- **Data Quality Matters: Suicide Intention Detection on Social Media Posts Using RoBERTa-CNN（数据质量至关重要：使用 RoBERTa-CNN 检测社交媒体上的自杀意图）**  \n  使用 RoBERTa-CNN 模型检测自杀意图，强调数据清洗的重要性。发现：模型在数据集上达到 98% 准确率，通过 OpenAI API 提升数据质量。\n\n其他医疗论文，如 **Feasibility of Identifying Factors Related to Alzheimer's Disease（识别阿尔茨海默病相关因素的可行性）**，总结风险因素并讨论真实世界数据的使用，但应用导向较强，故简要掠过。\n\n剩余论文多为特定应用或初步探索，如 **Evolution Guided Generative Flow Networks（进化引导的生成流网络）** 在强化学习中提升探索，但非主流；**Anthropomorphism in AI（AI 中的拟人化）** 分析语言模型的拟人偏见（EACL 2024），有趣但非核心；其他如发票图像处理、地震预测等，贡献较局部，快速提及不展开。\n\n总之，今天的 arXiv 论文突出了 AI 模型的鲁棒性、泛化和应用潜力，建议关注图神经网络和联邦学习的理论进展，以推动实际部署。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2402.02289v1",
      "title": "SemPool: Simple, robust, and interpretable KG pooling for enhancing language models",
      "title_zh": "翻译失败",
      "authors": [
        "Costas Mavromatis",
        "Petros Karypis",
        "George Karypis"
      ],
      "abstract": "Knowledge Graph (KG) powered question answering (QA) performs complex\nreasoning over language semantics as well as knowledge facts. Graph Neural\nNetworks (GNNs) learn to aggregate information from the underlying KG, which is\ncombined with Language Models (LMs) for effective reasoning with the given\nquestion. However, GNN-based methods for QA rely on the graph information of\nthe candidate answer nodes, which limits their effectiveness in more\nchallenging settings where critical answer information is not included in the\nKG. We propose a simple graph pooling approach that learns useful semantics of\nthe KG that can aid the LM's reasoning and that its effectiveness is robust\nunder graph perturbations. Our method, termed SemPool, represents KG facts with\npre-trained LMs, learns to aggregate their semantic information, and fuses it\nat different layers of the LM. Our experimental results show that SemPool\noutperforms state-of-the-art GNN-based methods by 2.27% accuracy points on\naverage when answer information is missing from the KG. In addition, SemPool\noffers interpretability on what type of graph information is fused at different\nLM layers.",
      "tldr_zh": "本论文提出 SemPool，一种简单、鲁棒且可解释的 Knowledge Graph (KG) 池化方法，用于提升 Language Models (LMs) 在问答 (QA) 任务中的性能，尤其在关键答案信息缺失的情况下。SemPool 通过预训练 LMs 表示 KG 事实，学习聚合它们的语义信息，并在 LMs 的不同层进行融合，从而增强推理能力和对图扰动的鲁棒性。实验结果显示，SemPool 比最先进的 Graph Neural Networks (GNNs) 方法平均提高 2.27% 准确率，并提供可解释性，揭示了在不同 LMs 层融合的图信息类型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02289v1",
      "published_date": "2024-02-03 23:03:51 UTC",
      "updated_date": "2024-02-03 23:03:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:47:01.206080"
    },
    {
      "arxiv_id": "2402.02287v4",
      "title": "Future Directions in the Theory of Graph Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Morris",
        "Fabrizio Frasca",
        "Nadav Dym",
        "Haggai Maron",
        "İsmail İlkan Ceylan",
        "Ron Levie",
        "Derek Lim",
        "Michael Bronstein",
        "Martin Grohe",
        "Stefanie Jegelka"
      ],
      "abstract": "Machine learning on graphs, especially using graph neural networks (GNNs),\nhas seen a surge in interest due to the wide availability of graph data across\na broad spectrum of disciplines, from life to social and engineering sciences.\nDespite their practical success, our theoretical understanding of the\nproperties of GNNs remains highly incomplete. Recent theoretical advancements\nprimarily focus on elucidating the coarse-grained expressive power of GNNs,\npredominantly employing combinatorial techniques. However, these studies do not\nperfectly align with practice, particularly in understanding the generalization\nbehavior of GNNs when trained with stochastic first-order optimization\ntechniques. In this position paper, we argue that the graph machine learning\ncommunity needs to shift its attention to developing a balanced theory of graph\nmachine learning, focusing on a more thorough understanding of the interplay of\nexpressive power, generalization, and optimization.",
      "tldr_zh": "图机器学习，特别是图神经网络(GNNs)，在生命、社会和工程科学等领域广泛应用，但其理论理解仍高度不完整。当前理论主要聚焦于GNNs的粗粒度表达能力(expressive power)，采用组合技术，却未能充分解释模型在随机一阶优化(stochastic first-order optimization)下的泛化行为(generalization)。本文作为立场论文(position paper)，主张图机器学习社区应转向构建一个平衡的理论框架，深入探讨表达能力、泛化和优化(interplay of expressive power, generalization, and optimization)的相互作用，以更好地指导实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.02287v4",
      "published_date": "2024-02-03 22:55:31 UTC",
      "updated_date": "2024-06-14 15:54:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:47:13.381810"
    },
    {
      "arxiv_id": "2402.02286v3",
      "title": "Multi-Level Aggregation and Recursive Alignment Architecture for Efficient Parallel Inference Segmentation Network",
      "title_zh": "多级聚合和递归对齐架构用于高效并行推理分割网络",
      "authors": [
        "Yanhua Zhang",
        "Ke Zhang",
        "Jingyu Wang",
        "Yulin Wu",
        "Wuwei Wang"
      ],
      "abstract": "Real-time semantic segmentation is a crucial research for real-world\napplications. However, many methods lay particular emphasis on reducing the\ncomputational complexity and model size, while largely sacrificing the\naccuracy. To tackle this problem, we propose a parallel inference network\ncustomized for semantic segmentation tasks to achieve a good trade-off between\nspeed and accuracy. We employ a shallow backbone to ensure real-time speed, and\npropose three core components to compensate for the reduced model capacity to\nimprove accuracy. Specifically, we first design a dual-pyramidal path\narchitecture (Multi-level Feature Aggregation Module, MFAM) to aggregate\nmulti-level features from the encoder to each scale, providing hierarchical\nclues for subsequent spatial alignment and corresponding in-network inference.\nThen, we build Recursive Alignment Module (RAM) by combining the flow-based\nalignment module with recursive upsampling architecture for accurate spatial\nalignment between multi-scale feature maps with half the computational\ncomplexity of the straightforward alignment method. Finally, we perform\nindependent parallel inference on the aligned features to obtain multi-scale\nscores, and adaptively fuse them through an attention-based Adaptive Scores\nFusion Module (ASFM) so that the final prediction can favor objects of multiple\nscales. Our framework shows a better balance between speed and accuracy than\nstate-of-the-art real-time methods on Cityscapes and CamVid datasets. We also\nconducted systematic ablation studies to gain insight into our motivation and\narchitectural design. Code is available at:\nhttps://github.com/Yanhua-Zhang/MFARANet.",
      "tldr_zh": "该论文针对实时语义分割任务，提出了一种高效的平行推理网络架构，以实现速度和准确性的良好平衡。论文采用浅层骨干网络确保实时性能，并设计了三个核心组件：Multi-level Feature Aggregation Module (MFAM) 用于聚合多级特征提供分层线索、Recursive Alignment Module (RAM) 通过流基对齐和递归上采样实现精确空间对齐并降低计算复杂度，以及Adaptive Scores Fusion Module (ASFM) 通过注意力机制自适应融合多尺度分数以提升预测准确性。在Cityscapes和CamVid数据集上，该框架比现有实时方法表现出更好的速度-准确性平衡，并通过系统消融研究验证了设计有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 9 figures and 12 Tables. Manuscript completed on April 30,\n  2022",
      "pdf_url": "http://arxiv.org/pdf/2402.02286v3",
      "published_date": "2024-02-03 22:51:17 UTC",
      "updated_date": "2024-04-18 13:33:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:47:25.295780"
    },
    {
      "arxiv_id": "2402.02285v1",
      "title": "SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Atharva Kulkarni",
        "Bo-Hsiang Tseng",
        "Joel Ruben Antony Moniz",
        "Dhivya Piraviperumal",
        "Hong Yu",
        "Shruti Bhargava"
      ],
      "abstract": "In-context learning with Large Language Models (LLMs) has emerged as a\npromising avenue of research in Dialog State Tracking (DST). However, the\nbest-performing in-context learning methods involve retrieving and adding\nsimilar examples to the prompt, requiring access to labeled training data.\nProcuring such training data for a wide range of domains and applications is\ntime-consuming, expensive, and, at times, infeasible. While zero-shot learning\nrequires no training data, it significantly lags behind the few-shot setup.\nThus, `\\textit{Can we efficiently generate synthetic data for any dialogue\nschema to enable few-shot prompting?}' Addressing this question, we propose\n\\method, a data generation framework tailored for DST, utilizing LLMs. Our\napproach only requires the dialogue schema and a few hand-crafted dialogue\ntemplates to synthesize natural, coherent, and free-flowing dialogues with DST\nannotations. Few-shot learning using data from {\\method} results in $4-5%$\nimprovement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1\nand 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of\nthe performance compared to the few-shot setup using human-annotated training\ndata. Our synthetic data and code can be accessed at\nhttps://github.com/apple/ml-synthdst",
      "tldr_zh": "该论文提出 SynthDST 框架，利用 Large Language Models (LLMs) 生成合成数据，以解决 Few-shot Dialog State Tracking (DST) 中的训练数据获取难题。该框架仅需对话 schema 和少量手工对话模板，就能合成自然连贯的对话数据，并包含 DST 注解。实验结果显示，使用 SynthDST 生成的数据进行 Few-shot 学习，在 MultiWOZ 2.1 和 2.4 数据集上比 Zero-shot 基准提高了 4-5% 的 Joint Goal Accuracy，并几乎恢复了使用人类标注数据的 98% 性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages. 4 figures, EACL 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2402.02285v1",
      "published_date": "2024-02-03 22:49:00 UTC",
      "updated_date": "2024-02-03 22:49:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:47:37.611482"
    },
    {
      "arxiv_id": "2402.05951v3",
      "title": "MinMaxMin $Q$-learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nitsan Soffair",
        "Shie Mannor"
      ],
      "abstract": "MinMaxMin $Q$-learning is a novel optimistic Actor-Critic algorithm that\naddresses the problem of overestimation bias ($Q$-estimations are\noverestimating the real $Q$-values) inherent in conservative RL algorithms. Its\ncore formula relies on the disagreement among $Q$-networks in the form of the\nmin-batch MaxMin $Q$-networks distance which is added to the $Q$-target and\nused as the priority experience replay sampling-rule. We implement MinMaxMin on\ntop of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art\ncontinuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet\nenvironments. The results show a consistent performance improvement of\nMinMaxMin over DDPG, TD3, and TD7 across all tested tasks.",
      "tldr_zh": "本文提出了一种新的乐观 Actor-Critic 算法，名为 MinMaxMin $Q$-learning，旨在解决保守强化学习算法中的过估计偏差问题。算法的核心是通过 $Q$-网络之间的分歧（min-batch MaxMin $Q$-networks distance）来调整 $Q$-target，并将其用作优先经验回放采样规则，同时基于 TD3 和 TD7 进行实现。在 MuJoCo 和 Bullet 环境中进行测试，结果显示 MinMaxMin $Q$-learning 在所有任务中比 DDPG、TD3 和 TD7 算法表现出一致的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper do not ready",
      "pdf_url": "http://arxiv.org/pdf/2402.05951v3",
      "published_date": "2024-02-03 21:58:06 UTC",
      "updated_date": "2024-06-02 19:40:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:47:49.938154"
    },
    {
      "arxiv_id": "2402.05950v3",
      "title": "SQT -- std $Q$-target",
      "title_zh": "翻译失败",
      "authors": [
        "Nitsan Soffair",
        "Dotan Di-Castro",
        "Orly Avner",
        "Shie Mannor"
      ],
      "abstract": "Std $Q$-target is a conservative, actor-critic, ensemble, $Q$-learning-based\nalgorithm, which is based on a single key $Q$-formula: $Q$-networks standard\ndeviation, which is an \"uncertainty penalty\", and, serves as a minimalistic\nsolution to the problem of overestimation bias. We implement SQT on top of\nTD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic\nalgorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our\nresults demonstrate SQT's $Q$-target formula superiority over TD3's $Q$-target\nformula as a conservative solution to overestimation bias in RL, while SQT\nshows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on\nall tasks.",
      "tldr_zh": "该论文提出了一种名为 SQT 的保守 actor-critic 算法，利用 ensemble Q-learning 和 Q-网络的标准差作为“uncertainty penalty”来最小化 overestimation bias 问题。SQT 构建在 TD3/TD7 的代码基础上，通过一个简洁的 $Q$-target 公式实现对不确定性的惩罚机制，并在七个流行的 MuJoCo 和 Bullet 任务上进行了测试。与 SOTA 算法 DDPG、TD3 和 TD7 相比，SQT 展示了显著的性能优势，在所有任务中均表现出色。总的来说，该方法为强化学习（RL）中的偏差控制提供了高效且简约的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.05950v3",
      "published_date": "2024-02-03 21:36:22 UTC",
      "updated_date": "2024-06-02 19:39:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:48:00.776966"
    },
    {
      "arxiv_id": "2402.02268v1",
      "title": "Federated Learning with New Knowledge: Fundamentals, Advances, and Futures",
      "title_zh": "联邦学习与新知识：基础、进展与未来",
      "authors": [
        "Lixu Wang",
        "Yang Zhao",
        "Jiahua Dong",
        "Ating Yin",
        "Qinbin Li",
        "Xiao Wang",
        "Dusit Niyato",
        "Qi Zhu"
      ],
      "abstract": "Federated Learning (FL) is a privacy-preserving distributed learning approach\nthat is rapidly developing in an era where privacy protection is increasingly\nvalued. It is this rapid development trend, along with the continuous emergence\nof new demands for FL in the real world, that prompts us to focus on a very\nimportant problem: Federated Learning with New Knowledge. The primary challenge\nhere is to effectively incorporate various new knowledge into existing FL\nsystems and evolve these systems to reduce costs, extend their lifespan, and\nfacilitate sustainable development. In this paper, we systematically define the\nmain sources of new knowledge in FL, including new features, tasks, models, and\nalgorithms. For each source, we thoroughly analyze and discuss how to\nincorporate new knowledge into existing FL systems and examine the impact of\nthe form and timing of new knowledge arrival on the incorporation process.\nFurthermore, we comprehensively discuss the potential future directions for FL\nwith new knowledge, considering a variety of factors such as scenario setups,\nefficiency, and security. There is also a continuously updating repository for\nthis topic: https://github.com/conditionWang/FLNK.",
      "tldr_zh": "本论文探讨了Federated Learning (FL)中如何有效整合新知识，以应对隐私保护时代的新需求。作者系统定义了新知识来源，包括新features、tasks、models和algorithms，并分析了将这些知识融入现有FL系统的方法，以及知识形式和到来的时机对整合过程的影响，从而降低成本、延长系统寿命并促进可持续发展。最后，论文讨论了FL未来方向，如场景设置、效率和安全，并提供了一个持续更新的仓库（https://github.com/conditionWang/FLNK）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.02268v1",
      "published_date": "2024-02-03 21:29:31 UTC",
      "updated_date": "2024-02-03 21:29:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:48:13.958000"
    },
    {
      "arxiv_id": "2402.02263v5",
      "title": "MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers",
      "title_zh": "MixedNUTS：通过非线性混合分类器实现无需训练的准确性-鲁棒性平衡",
      "authors": [
        "Yatong Bai",
        "Mo Zhou",
        "Vishal M. Patel",
        "Somayeh Sojoudi"
      ],
      "abstract": "Adversarial robustness often comes at the cost of degraded accuracy, impeding\nreal-life applications of robust classification models. Training-based\nsolutions for better trade-offs are limited by incompatibilities with\nalready-trained high-performance large models, necessitating the exploration of\ntraining-free ensemble approaches. Observing that robust models are more\nconfident in correct predictions than in incorrect ones on clean and\nadversarial data alike, we speculate amplifying this \"benign confidence\nproperty\" can reconcile accuracy and robustness in an ensemble setting. To\nachieve so, we propose \"MixedNUTS\", a training-free method where the output\nlogits of a robust classifier and a standard non-robust classifier are\nprocessed by nonlinear transformations with only three parameters, which are\noptimized through an efficient algorithm. MixedNUTS then converts the\ntransformed logits into probabilities and mixes them as the overall output. On\nCIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom\nstrong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and\nnear-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,\nsacrificing merely 0.87 points in robust accuracy.",
      "tldr_zh": "这篇论文提出 MixedNUTS，一种无需训练的 ensemble 方法，通过非线性混合（nonlinearly mixed classifiers）鲁棒分类器和标准非鲁棒分类器的输出 logits，仅优化三个参数来实现准确率（accuracy）和对抗鲁棒性（adversarial robustness）的平衡。方法利用鲁棒模型的 benign confidence property，对 logits 进行非线性变换并混合概率输出，从而在不影响已有模型的情况下提升性能。在 CIFAR-10、CIFAR-100 和 ImageNet 数据集上，使用自定义强 adaptive attacks 的实验结果显示，MixedNUTS 将 CIFAR-100 的 clean accuracy 提高了 7.86 点，同时仅牺牲 0.87 点的 robust accuracy，达到了近 SOTA 水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T07"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02263v5",
      "published_date": "2024-02-03 21:12:36 UTC",
      "updated_date": "2024-10-16 00:15:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:48:27.018151"
    },
    {
      "arxiv_id": "2402.02262v2",
      "title": "Data Quality Matters: Suicide Intention Detection on Social Media Posts Using RoBERTa-CNN",
      "title_zh": "数据质量至关重要：使用 RoBERT",
      "authors": [
        "Emily Lin",
        "Jian Sun",
        "Hsingyu Chen",
        "Mohammad H. Mahoor"
      ],
      "abstract": "Suicide remains a pressing global health concern, necessitating innovative\napproaches for early detection and intervention. This paper focuses on\nidentifying suicidal intentions in posts from the SuicideWatch subreddit by\nproposing a novel deep-learning approach that utilizes the state-of-the-art\nRoBERTa-CNN model. The robustly Optimized BERT Pretraining Approach (RoBERTa)\nexcels at capturing textual nuances and forming semantic relationships within\nthe text. The remaining Convolutional Neural Network (CNN) head enhances\nRoBERTa's capacity to discern critical patterns from extensive datasets. To\nevaluate RoBERTa-CNN, we conducted experiments on the Suicide and Depression\nDetection dataset, yielding promising results. For instance, RoBERTa-CNN\nachieves a mean accuracy of 98% with a standard deviation (STD) of 0.0009.\nAdditionally, we found that data quality significantly impacts the training of\na robust model. To improve data quality, we removed noise from the text data\nwhile preserving its contextual content through either manually cleaning or\nutilizing the OpenAI API.",
      "tldr_zh": "这篇论文关注社交媒体帖子中自杀意图的检测，提出了一种基于 RoBERTa-CNN 模型的新型深度学习方法，其中 RoBERTa 用于捕捉文本语义关系，CNN 则负责识别关键模式。实验在 Suicide and Depression Detection 数据集上进行，RoBERTa-CNN 模型实现了 98% 的平均准确率（标准差 0.0009）。此外，研究强调数据质量对模型训练的影响，通过手动清洗或 OpenAI API 去除噪声，从而提升了数据的上下文完整性和模型鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 1 figure, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.02262v2",
      "published_date": "2024-02-03 20:58:09 UTC",
      "updated_date": "2024-12-20 18:21:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:48:36.354756"
    },
    {
      "arxiv_id": "2402.02258v2",
      "title": "XTSFormer: Cross-Temporal-Scale Transformer for Irregular-Time Event Prediction in Clinical Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Tingsong Xiao",
        "Zelin Xu",
        "Wenchong He",
        "Zhengkun Xiao",
        "Yupu Zhang",
        "Zibo Liu",
        "Shigang Chen",
        "My T. Thai",
        "Jiang Bian",
        "Parisa Rashidi",
        "Zhe Jiang"
      ],
      "abstract": "Adverse clinical events related to unsafe care are among the top ten causes\nof death in the U.S. Accurate modeling and prediction of clinical events from\nelectronic health records (EHRs) play a crucial role in patient safety\nenhancement. An example is modeling de facto care pathways that characterize\ncommon step-by-step plans for treatment or care. However, clinical event data\npose several unique challenges, including the irregularity of time intervals\nbetween consecutive events, the existence of cycles, periodicity, multi-scale\nevent interactions, and the high computational costs associated with long event\nsequences. Existing neural temporal point processes (TPPs) methods do not\neffectively capture the multi-scale nature of event interactions, which is\ncommon in many real-world clinical applications. To address these issues, we\npropose the cross-temporal-scale transformer (XTSFormer), specifically designed\nfor irregularly timed event data. Our model consists of two vital components: a\nnovel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly\ncaptures the cyclical nature of time, and a hierarchical multi-scale temporal\nattention mechanism, where different temporal scales are determined by a\nbottom-up clustering approach. Extensive experiments on several real-world EHR\ndatasets show that our XTSFormer outperforms multiple baseline methods. The\ncode is available at https://github.com/spatialdatasciencegroup/XTSFormer.",
      "tldr_zh": "这篇论文针对临床应用中电子健康记录 (EHRs) 的不规则时间事件预测问题，提出了 XTSFormer 模型，以解决事件间隔不规则、周期性、多尺度交互和高计算成本等挑战。XTSFormer 包括两个关键组件：Feature-based Cycle-aware Time Positional Encoding (FCPE) 用于捕捉时间的周期性，以及一个层次化多尺度时间注意力机制，通过自下而上的聚类方法处理不同时间尺度的事件交互。实验结果显示，在多个真实 EHR 数据集上，XTSFormer 优于现有神经时间点过程 (TPPs) 等基线方法，提高了预测准确性，并提供了开源代码以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2402.02258v2",
      "published_date": "2024-02-03 20:33:39 UTC",
      "updated_date": "2024-12-18 20:31:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:48:51.264479"
    },
    {
      "arxiv_id": "2402.02246v1",
      "title": "ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images",
      "title_zh": "ExTTNet：一种从发票图像中提取表格文本的深度学习算法",
      "authors": [
        "Adem Akdoğan",
        "Murat Kurt"
      ],
      "abstract": "In this work, product tables in invoices are obtained autonomously via a deep\nlearning model, which is named as ExTTNet. Firstly, text is obtained from\ninvoice images using Optical Character Recognition (OCR) techniques. Tesseract\nOCR engine [37] is used for this process. Afterwards, the number of existing\nfeatures is increased by using feature extraction methods to increase the\naccuracy. Labeling process is done according to whether each text obtained as a\nresult of OCR is a table element or not. In this study, a multilayer artificial\nneural network model is used. The training has been carried out with an Nvidia\nRTX 3090 graphics card and taken $162$ minutes. As a result of the training,\nthe F1 score is $0.92$.",
      "tldr_zh": "本文提出了一种名为 ExTTNet 的深度学习算法，用于从发票图像中自动提取表格文本。方法首先利用 Tesseract OCR 引擎获取图像文本，然后通过特征提取技术增加特征数量，并对文本进行标记以识别表格元素。最终，使用多层人工神经网络模型进行训练，实验在 Nvidia RTX 3090 显卡上耗时 162 分钟，取得了 0.92 的 F1 score，展示了模型的高准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.02246v1",
      "published_date": "2024-02-03 19:24:45 UTC",
      "updated_date": "2024-02-03 19:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:49:03.588771"
    },
    {
      "arxiv_id": "2402.02230v1",
      "title": "Federated Learning with Differential Privacy",
      "title_zh": "翻译失败",
      "authors": [
        "Adrien Banse",
        "Jan Kreischer",
        "Xavier Oliva i Jürgens"
      ],
      "abstract": "Federated learning (FL), as a type of distributed machine learning, is\ncapable of significantly preserving client's private data from being shared\namong different parties. Nevertheless, private information can still be\ndivulged by analyzing uploaded parameter weights from clients. In this report,\nwe showcase our empirical benchmark of the effect of the number of clients and\nthe addition of differential privacy (DP) mechanisms on the performance of the\nmodel on different types of data. Our results show that non-i.i.d and small\ndatasets have the highest decrease in performance in a distributed and\ndifferentially private setting.",
      "tldr_zh": "本研究探讨了联邦学习（Federated Learning, FL）结合差分隐私（Differential Privacy, DP）的效果，旨在通过分布式机器学习保护客户端数据隐私，同时分析上传参数权重可能泄露信息的风险。研究团队进行了实证基准测试，评估了客户端数量和DP机制对模型性能的影响。结果显示，在非独立同分布（non-i.i.d）和小型数据集上，性能下降最为显著，为FL在隐私保护场景下的优化提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "Machine Learning (ML) & Federated Learning (FL); 4 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.02230v1",
      "published_date": "2024-02-03 18:21:38 UTC",
      "updated_date": "2024-02-03 18:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:49:14.839378"
    },
    {
      "arxiv_id": "2402.15515v1",
      "title": "Feasibility of Identifying Factors Related to Alzheimer's Disease and Related Dementia in Real-World Data",
      "title_zh": "翻译失败",
      "authors": [
        "Aokun Chen",
        "Qian Li",
        "Yu Huang",
        "Yongqiu Li",
        "Yu-neng Chuang",
        "Xia Hu",
        "Serena Guo",
        "Yonghui Wu",
        "Yi Guo",
        "Jiang Bian"
      ],
      "abstract": "A comprehensive view of factors associated with AD/ADRD will significantly\naid in studies to develop new treatments for AD/ADRD and identify high-risk\npopulations and patients for prevention efforts. In our study, we summarized\nthe risk factors for AD/ADRD by reviewing existing meta-analyses and review\narticles on risk and preventive factors for AD/ADRD. In total, we extracted 477\nrisk factors in 10 categories from 537 studies. We constructed an interactive\nknowledge map to disseminate our study results. Most of the risk factors are\naccessible from structured Electronic Health Records (EHRs), and clinical\nnarratives show promise as information sources. However, evaluating genomic\nrisk factors using RWD remains a challenge, as genetic testing for AD/ADRD is\nstill not a common practice and is poorly documented in both structured and\nunstructured EHRs. Considering the constantly evolving research on AD/ADRD risk\nfactors, literature mining via NLP methods offers a solution to automatically\nupdate our knowledge map.",
      "tldr_zh": "这篇论文探讨了在真实世界数据 (RWD) 中识别与阿尔茨海默病 (AD) 和相关痴呆 (ADRD) 相关风险因素的可行性，通过回顾现有元分析和评论文章，从 537 个研究中提取了 477 个风险因素并分为 10 类。研究构建了一个交互式知识地图，以传播这些结果，并发现大多数风险因素可从结构化的电子健康记录 (EHRs) 中获取，而临床叙述也显示出潜力。评估基因组风险因素的挑战在于遗传测试在 EHRs 中记录不足，因此论文建议使用 NLP 方法进行文献挖掘，以自动更新知识地图。该方法为 AD/ADRD 的预防和治疗研究提供了重要支持。",
      "categories": [
        "cs.AI",
        "q-bio.QM",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.15515v1",
      "published_date": "2024-02-03 18:17:19 UTC",
      "updated_date": "2024-02-03 18:17:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:49:30.283588"
    },
    {
      "arxiv_id": "2402.02218v1",
      "title": "Machine Intelligence in Africa: a survey",
      "title_zh": "非洲的机器智能：一项综述",
      "authors": [
        "Allahsera Auguste Tapo",
        "Ali Traore",
        "Sidy Danioko",
        "Hamidou Tembine"
      ],
      "abstract": "In the last 5 years, the availability of large audio datasets in African\ncountries has opened unlimited opportunities to build machine intelligence (MI)\ntechnologies that are closer to the people and speak, learn, understand, and do\nbusinesses in local languages, including for those who cannot read and write.\nUnfortunately, these audio datasets are not fully exploited by current MI\ntools, leaving several Africans out of MI business opportunities. Additionally,\nmany state-of-the-art MI models are not culture-aware, and the ethics of their\nadoption indexes are questionable. The lack thereof is a major drawback in many\napplications in Africa. This paper summarizes recent developments in machine\nintelligence in Africa from a multi-layer multiscale and culture-aware ethics\nperspective, showcasing MI use cases in 54 African countries through 400\narticles on MI research, industry, government actions, as well as uses in art,\nmusic, the informal economy, and small businesses in Africa. The survey also\nopens discussions on the reliability of MI rankings and indexes in the African\ncontinent as well as algorithmic definitions of unclear terms used in MI.",
      "tldr_zh": "这篇调查论文总结了非洲机器智能（MI）的最新发展，聚焦于过去5年内音频数据集的可用性及其在本地语言和文化中的应用潜力。论文从多层多尺度以及文化感知伦理视角，分析了400篇文章，涵盖MI在54个非洲国家的用例，包括研究、行业、政府行动、艺术、音乐、非正式经济和小企业。研究发现，当前MI工具未能充分利用这些数据集，且许多先进模型缺乏文化意识和伦理考量，导致非洲许多人群被排除在MI机会之外；同时，论文讨论了MI排名的可靠性以及算法术语定义的模糊性，以推动更具包容性的MI发展。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted and to be presented at DSAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.02218v1",
      "published_date": "2024-02-03 17:27:14 UTC",
      "updated_date": "2024-02-03 17:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:49:40.075917"
    },
    {
      "arxiv_id": "2402.02186v1",
      "title": "Evolution Guided Generative Flow Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Zarif Ikram",
        "Ling Pan",
        "Dianbo Liu"
      ],
      "abstract": "Generative Flow Networks (GFlowNets) are a family of probabilistic generative\nmodels that learn to sample compositional objects proportional to their\nrewards. One big challenge of GFlowNets is training them effectively when\ndealing with long time horizons and sparse rewards. To address this, we propose\nEvolution guided generative flow networks (EGFN), a simple but powerful\naugmentation to the GFlowNets training using Evolutionary algorithms (EA). Our\nmethod can work on top of any GFlowNets training objective, by training a set\nof agent parameters using EA, storing the resulting trajectories in the\nprioritized replay buffer, and training the GFlowNets agent using the stored\ntrajectories. We present a thorough investigation over a wide range of toy and\nreal-world benchmark tasks showing the effectiveness of our method in handling\nlong trajectories and sparse rewards.",
      "tldr_zh": "本研究针对Generative Flow Networks (GFlowNets) 在处理长时序和稀疏奖励时的训练挑战，提出了一种名为Evolution guided generative flow networks (EGFN) 的增强方法。该方法利用Evolutionary algorithms (EA) 训练代理参数，生成轨迹并存储到优先级重放缓冲区，然后用这些轨迹来训练GFlowNets 代理，从而提高模型的采样效率。实验结果显示，EGFN 在多种玩具和真实世界基准任务上表现出色，能够有效应对长轨迹和稀疏奖励问题。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 16 figues",
      "pdf_url": "http://arxiv.org/pdf/2402.02186v1",
      "published_date": "2024-02-03 15:28:53 UTC",
      "updated_date": "2024-02-03 15:28:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:49:51.855893"
    },
    {
      "arxiv_id": "2402.02181v1",
      "title": "An Ontology-Based multi-domain model in Social Network Analysis: Experimental validation and case study",
      "title_zh": "基于本体论",
      "authors": [
        "José Alberto Benítez-Andrades",
        "Isaías García-Rodríguez",
        "Carmen Benavides",
        "Héctor Aláiz-Moretón",
        "José Emilio Labra Gayo"
      ],
      "abstract": "The use of social network theory and methods of analysis have been applied to\ndifferent domains in recent years, including public health. The complete\nprocedure for carrying out a social network analysis (SNA) is a time-consuming\ntask that entails a series of steps in which the expert in social network\nanalysis could make mistakes. This research presents a multi-domain knowledge\nmodel capable of automatically gathering data and carrying out different social\nnetwork analyses in different domains, without errors and obtaining the same\nconclusions that an expert in SNA would obtain. The model is represented in an\nontology called OntoSNAQA, which is made up of classes, properties and rules\nrepresenting the domains of People, Questionnaires and Social Network Analysis.\nBesides the ontology itself, different rules are represented by SWRL and SPARQL\nqueries. A Knowledge Based System was created using OntoSNAQA and applied to a\nreal case study in order to show the advantages of the approach. Finally, the\nresults of an SNA analysis obtained through the model were compared to those\nobtained from some of the most widely used SNA applications: UCINET, Pajek,\nCytoscape and Gephi, to test and confirm the validity of the model.",
      "tldr_zh": "该研究提出了一种基于Ontology的多领域知识模型，用于社交网络分析(SNA)，旨在自动收集数据并执行分析，减少人为错误并与专家结论一致。模型以OntoSNAQA本体为基础，包括People、Questionnaires和Social Network Analysis的类、属性和规则，并利用SWRL和SPARQL查询来表示规则。该模型应用于真实案例研究，并与UCINET、Pajek、Cytoscape和Gephi等流行工具的分析结果进行比较，验证了其准确性和有效性。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02181v1",
      "published_date": "2024-02-03 15:11:19 UTC",
      "updated_date": "2024-02-03 15:11:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:50:04.199104"
    },
    {
      "arxiv_id": "2402.02168v2",
      "title": "Enhancing Cross-domain Link Prediction via Evolution Process Modeling",
      "title_zh": "通过进化过程建模增强跨域链路预测",
      "authors": [
        "Xuanwen Huang",
        "Wei Chow",
        "Yize Zhu",
        "Yang Wang",
        "Ziwei Chai",
        "Chunping Wang",
        "Lei Chen",
        "Yang Yang"
      ],
      "abstract": "This work proposes DyExpert, a dynamic graph model for cross-domain link\nprediction. It can explicitly model historical evolving processes to learn the\nevolution pattern of a specific downstream graph and subsequently make\npattern-specific link predictions. DyExpert adopts a decode-only transformer\nand is capable of efficiently parallel training and inference by\n\\textit{conditioned link generation} that integrates both evolution modeling\nand link prediction. DyExpert is trained by extensive dynamic graphs across\ndiverse domains, comprising 6M dynamic edges. Extensive experiments on eight\nuntrained graphs demonstrate that DyExpert achieves state-of-the-art\nperformance in cross-domain link prediction. Compared to the advanced baseline\nunder the same setting, DyExpert achieves an average of 11.40% improvement\nAverage Precision across eight graphs. More impressive, it surpasses the fully\nsupervised performance of 8 advanced baselines on 6 untrained graphs.",
      "tldr_zh": "本研究提出 DyExpert，一种动态图模型，用于提升跨域链接预测性能，通过显式建模历史演化过程来学习特定下游图的演化模式，并进行模式特定的链接预测。DyExpert 采用 decode-only transformer 和 conditioned link generation 技术，实现了高效的并行训练和推理，并利用跨越多种领域的 6M 动态边进行训练。在八个未训练图上的广泛实验中，DyExpert 取得了最先进的性能，比高级基线平均提高了 11.40% 的 Average Precision，并在六张图上超过了 8 个基线的完全监督性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW'25",
      "pdf_url": "http://arxiv.org/pdf/2402.02168v2",
      "published_date": "2024-02-03 14:29:01 UTC",
      "updated_date": "2025-02-05 06:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:50:16.767752"
    },
    {
      "arxiv_id": "2402.02167v1",
      "title": "Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Podo",
        "Muhammad Ishmal",
        "Marco Angelini"
      ],
      "abstract": "The automatic generation of visualizations is an old task that, through the\nyears, has shown more and more interest from the research and practitioner\ncommunities. Recently, large language models (LLM) have become an interesting\noption for supporting generative tasks related to visualization, demonstrating\ninitial promising results. At the same time, several pitfalls, like the\nmultiple ways of instructing an LLM to generate the desired result, the\ndifferent perspectives leading the generation (code-based, image-based,\ngrammar-based), and the presence of hallucinations even for the visualization\ngeneration task, make their usage less affordable than expected. Following\nsimilar initiatives for benchmarking LLMs, this paper copes with the problem of\nmodeling the evaluation of a generated visualization through an LLM. We propose\na theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort\nin its atomic components, characterizes their nature, and provides an overview\nof how to implement and interpret them. We also designed and implemented an\nevaluation platform that provides a benchmarking resource for the visualization\ngeneration task. The platform supports automatic and manual scoring conducted\nby multiple assessors to support a fine-grained and semantic evaluation based\non the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and\nLlama2-70-b models show the benefits of EvaLLM and illustrate interesting\nresults on the current state-of-the-art LLM-generated visualizations.",
      "tldr_zh": "这篇论文提出了 EvaLLM，一个概念框架，用于评估和解释基于大型语言模型(LLM)的可视化生成任务，旨在解决指令多样性、生成视角差异（如代码-based、图像-based）和幻觉问题等挑战。EvaLLM 将评估过程分解为原子组件，描述其性质，并提供实施和解释的指南，同时设计了一个支持自动和手动评分的平台，以实现细粒度和语义评估。案例研究使用 GPT3.5-turbo 和 Llama2-70-b 模型，展示了该框架的益处，并揭示了当前 LLM 在可视化生成中的表现和潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02167v1",
      "published_date": "2024-02-03 14:28:55 UTC",
      "updated_date": "2024-02-03 14:28:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:50:28.847148"
    },
    {
      "arxiv_id": "2402.02164v4",
      "title": "Hierarchical Structure Enhances the Convergence and Generalizability of Linear Molecular Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Juan-Ni Wu",
        "Tong Wang",
        "Li-Juan Tang",
        "Hai-Long Wu",
        "Ru-Qin Yu"
      ],
      "abstract": "Language models demonstrate fundamental abilities in syntax, semantics, and\nreasoning, though their performance often depends significantly on the inputs\nthey process. This study introduces TSIS (Simplified TSID) and its\nvariants:TSISD (TSIS with Depth-First Search), TSISO (TSIS in Order), and TSISR\n(TSIS in Random), as integral components of the t-SMILES framework. These\nadditions complete the framework's design, providing diverse approaches to\nmolecular representation. Through comprehensive analysis and experiments\nemploying deep generative models, including GPT, diffusion models, and\nreinforcement learning, the findings reveal that the hierarchical structure of\nt-SMILES is more straightforward to parse than initially anticipated.\nFurthermore, t-SMILES consistently outperforms other linear representations\nsuch as SMILES, SELFIES, and SAFE, demonstrating superior convergence speed and\nenhanced generalization capabilities.",
      "tldr_zh": "本研究引入了 TSIS（Simplified TSID）及其变体 TSISD（TSIS with Depth-First Search）、TSISO（TSIS in Order）和 TSISR（TSIS in Random），作为 t-SMILES 框架的组成部分，以提供多样化的分子表示方法。通过使用 GPT、扩散模型和强化学习等深度生成模型进行全面实验，发现 t-SMILES 的层次结构比预期更容易解析。结果显示，t-SMILES 在收敛速度和泛化能力上均优于其他线性表示如 SMILES、SELFIES 和 SAFE，从而提升了分子表示的整体性能。",
      "categories": [
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.AI",
      "comment": "26pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.02164v4",
      "published_date": "2024-02-03 14:24:21 UTC",
      "updated_date": "2024-11-18 16:01:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:50:40.328019"
    },
    {
      "arxiv_id": "2402.02150v1",
      "title": "Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models",
      "title_zh": "基于数据的地震烈度分布预测：采用混合分类-回归模型",
      "authors": [
        "Koyu Mizutani",
        "Haruki Mitarai",
        "Kakeru Miyazaki",
        "Soichiro Kumano",
        "Toshihiko Yamasaki"
      ],
      "abstract": "Earthquakes are among the most immediate and deadly natural disasters that\nhumans face. Accurately forecasting the extent of earthquake damage and\nassessing potential risks can be instrumental in saving numerous lives. In this\nstudy, we developed linear regression models capable of predicting seismic\nintensity distributions based on earthquake parameters: location, depth, and\nmagnitude. Because it is completely data-driven, it can predict intensity\ndistributions without geographical information. The dataset comprises seismic\nintensity data from earthquakes that occurred in the vicinity of Japan between\n1997 and 2020, specifically containing 1,857 instances of earthquakes with a\nmagnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We\ntrained both regression and classification models and combined them to take\nadvantage of both to create a hybrid model. The proposed model outperformed\ncommonly used Ground Motion Prediction Equations (GMPEs) in terms of the\ncorrelation coefficient, F1 score, and MCC. Furthermore, the proposed model can\npredict even abnormal seismic intensity distributions, a task at conventional\nGMPEs often struggle.",
      "tldr_zh": "本研究针对地震灾害的风险评估，开发了一种数据驱动的混合分类-回归模型，用于基于地震参数（如位置、深度和震级）预测地震强度分布。该模型利用1997-2020年日本附近1857个震级5.0或更大地震的数据进行训练，并结合回归和分类优势，超越了传统的Ground Motion Prediction Equations (GMPEs)在相关系数、F1 score和MCC方面的性能。此外，该模型能够有效预测异常地震强度分布，提升了灾害预报的准确性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02150v1",
      "published_date": "2024-02-03 13:39:22 UTC",
      "updated_date": "2024-02-03 13:39:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:50:52.498632"
    },
    {
      "arxiv_id": "2402.02146v1",
      "title": "Emergency Computing: An Adaptive Collaborative Inference Method Based on Hierarchical Reinforcement Learning",
      "title_zh": "紧急计算：一种基于分层强化学习的自适应",
      "authors": [
        "Weiqi Fu",
        "Lianming Xu",
        "Xin Wu",
        "Li Wang",
        "Aiguo Fei"
      ],
      "abstract": "In achieving effective emergency response, the timely acquisition of\nenvironmental information, seamless command data transmission, and prompt\ndecision-making are crucial. This necessitates the establishment of a resilient\nemergency communication dedicated network, capable of providing communication\nand sensing services even in the absence of basic infrastructure. In this\npaper, we propose an Emergency Network with Sensing, Communication,\nComputation, Caching, and Intelligence (E-SC3I). The framework incorporates\nmechanisms for emergency computing, caching, integrated communication and\nsensing, and intelligence empowerment. E-SC3I ensures rapid access to a large\nuser base, reliable data transmission over unstable links, and dynamic network\ndeployment in a changing environment. However, these advantages come at the\ncost of significant computation overhead. Therefore, we specifically\nconcentrate on emergency computing and propose an adaptive collaborative\ninference method (ACIM) based on hierarchical reinforcement learning.\nExperimental results demonstrate our method's ability to achieve rapid\ninference of AI models with constrained computational and communication\nresources.",
      "tldr_zh": "本论文针对紧急响应中的环境信息获取、数据传输和决策问题，提出了一种集感知、通信、计算、缓存和智能于一体的紧急网络框架 E-SC3I，以确保在基础设施缺失时提供可靠服务。针对框架的计算开销，该研究引入了基于 Hierarchical Reinforcement Learning 的自适应协作推理方法 (ACIM)，实现 AI 模型在资源受限环境下的动态协作和高效推理。实验结果显示，ACIM 显著提升了 AI 模型的快速推理能力，为紧急计算提供可靠支持。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02146v1",
      "published_date": "2024-02-03 13:28:35 UTC",
      "updated_date": "2024-02-03 13:28:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:51:06.029621"
    },
    {
      "arxiv_id": "2402.02135v1",
      "title": "Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test",
      "title_zh": "翻译失败",
      "authors": [
        "Aditi Khandelwal",
        "Utkarsh Agarwal",
        "Kumar Tanmay",
        "Monojit Choudhury"
      ],
      "abstract": "This paper explores the moral judgment and moral reasoning abilities\nexhibited by Large Language Models (LLMs) across languages through the Defining\nIssues Test. It is a well known fact that moral judgment depends on the\nlanguage in which the question is asked. We extend the work of beyond English,\nto 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe\nthree LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial\nmultilingual text processing and generation abilities. Our study shows that the\nmoral reasoning ability for all models, as indicated by the post-conventional\nscore, is substantially inferior for Hindi and Swahili, compared to Spanish,\nRussian, Chinese and English, while there is no clear trend for the performance\nof the latter four languages. The moral judgments too vary considerably by the\nlanguage.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）的道德判断和道德推理能力是否因语言而异，使用了多语言Defining Issues Test（DIT）。研究扩展了先前的英文测试，涵盖Chinese、Hindi、Russian、Spanish和Swahili五种语言，并评估了ChatGPT、GPT-4和Llama2Chat-70B模型的表现。结果显示，所有模型在Hindi和Swahili上的后常规分数（post-conventional score）显著低于Spanish、Russian、Chinese和English，而后四种语言的性能无明显趋势；此外，道德判断也随语言出现明显差异。该工作揭示了LLMs在多语言环境下的局限性，为改进其跨文化应用提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EACL 2024 (main)",
      "pdf_url": "http://arxiv.org/pdf/2402.02135v1",
      "published_date": "2024-02-03 12:52:36 UTC",
      "updated_date": "2024-02-03 12:52:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:51:17.121419"
    },
    {
      "arxiv_id": "2402.02110v1",
      "title": "Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees",
      "title_zh": "翻译失败",
      "authors": [
        "Guang-Yuan Hao",
        "Hengguan Huang",
        "Haotian Wang",
        "Jie Gao",
        "Hao Wang"
      ],
      "abstract": "Active learning (AL) aims to improve model performance within a fixed\nlabeling budget by choosing the most informative data points to label. Existing\nAL focuses on the single-domain setting, where all data come from the same\ndomain (e.g., the same dataset). However, many real-world tasks often involve\nmultiple domains. For example, in visual recognition, it is often desirable to\ntrain an image classifier that works across different environments (e.g.,\ndifferent backgrounds), where images from each environment constitute one\ndomain. Such a multi-domain AL setting is challenging for prior methods because\nthey (1) ignore the similarity among different domains when assigning labeling\nbudget and (2) fail to handle distribution shift of data across different\ndomains. In this paper, we propose the first general method, dubbed composite\nactive learning (CAL), for multi-domain AL. Our approach explicitly considers\nthe domain-level and instance-level information in the problem; CAL first\nassigns domain-level budgets according to domain-level importance, which is\nestimated by optimizing an upper error bound that we develop; with the\ndomain-level budgets, CAL then leverages a certain instance-level query\nstrategy to select samples to label from each domain. Our theoretical analysis\nshows that our method achieves a better error bound compared to current AL\nmethods. Our empirical results demonstrate that our approach significantly\noutperforms the state-of-the-art AL methods on both synthetic and real-world\nmulti-domain datasets. Code is available at\nhttps://github.com/Wang-ML-Lab/multi-domain-active-learning.",
      "tldr_zh": "该论文提出了一种新的主动学习方法Composite Active Learning (CAL)，旨在解决多域Active Learning中的挑战，包括忽略域间相似性和处理分布偏移问题。CAL首先通过优化一个上界误差来估计域级重要性，从而分配域级预算；随后，在每个域内应用实例级查询策略选择最有信息量的样本进行标注。理论分析表明，CAL比现有Active Learning方法具有更好的误差界，实验结果在合成和真实多域数据集上显著提升了性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02110v1",
      "published_date": "2024-02-03 10:22:18 UTC",
      "updated_date": "2024-02-03 10:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:51:28.960389"
    },
    {
      "arxiv_id": "2402.02101v1",
      "title": "Are Large Language Models Good Prompt Optimizers?",
      "title_zh": "大型语言模型是好的提示优化器吗？",
      "authors": [
        "Ruotian Ma",
        "Xiaolei Wang",
        "Xin Zhou",
        "Jian Li",
        "Nan Du",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as\nPrompt Optimizers to self-reflect and refine prompts, has shown promising\nperformance in recent studies. Despite the success, the underlying mechanism of\nthis approach remains unexplored, and the true effectiveness of LLMs as Prompt\nOptimizers requires further validation. In this work, we conducted a\ncomprehensive study to uncover the actual mechanism of LLM-based Prompt\nOptimization. Our findings reveal that the LLM optimizers struggle to identify\nthe true causes of errors during reflection, tending to be biased by their own\nprior knowledge rather than genuinely reflecting on the errors. Furthermore,\neven when the reflection is semantically valid, the LLM optimizers often fail\nto generate appropriate prompts for the target models with a single prompt\nrefinement step, partly due to the unpredictable behaviors of the target\nmodels. Based on the observations, we introduce a new \"Automatic Behavior\nOptimization\" paradigm, which directly optimizes the target model's behavior in\na more controllable manner. We hope our study can inspire new directions for\nautomatic prompt optimization development.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)作为提示优化器的实际有效性，通过全面研究揭示了LLM-based Automatic Prompt Optimization的潜在机制。研究发现，LLM优化器在自我反思时难以准确识别错误原因，常受自身先验知识偏见影响，且即使反思有效，也无法在单步优化中为目标模型生成合适的提示。基于这些观察，作者提出“Automatic Behavior Optimization”新范式，直接优化目标模型的行为以提高可控性。该工作为自动提示优化领域的发展提供了新的启发方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02101v1",
      "published_date": "2024-02-03 09:48:54 UTC",
      "updated_date": "2024-02-03 09:48:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:51:41.633050"
    },
    {
      "arxiv_id": "2402.02099v1",
      "title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models",
      "title_zh": "分析",
      "authors": [
        "Sara Rajaee",
        "Christof Monz"
      ],
      "abstract": "Recent advances in training multilingual language models on large datasets\nseem to have shown promising results in knowledge transfer across languages and\nachieve high performance on downstream tasks. However, we question to what\nextent the current evaluation benchmarks and setups accurately measure\nzero-shot cross-lingual knowledge transfer. In this work, we challenge the\nassumption that high zero-shot performance on target tasks reflects high\ncross-lingual ability by introducing more challenging setups involving\ninstances with multiple languages. Through extensive experiments and analysis,\nwe show that the observed high performance of multilingual models can be\nlargely attributed to factors not requiring the transfer of actual linguistic\nknowledge, such as task- and surface-level knowledge. More specifically, we\nobserve what has been transferred across languages is mostly data artifacts and\nbiases, especially for low-resource languages. Our findings highlight the\noverlooked drawbacks of existing cross-lingual test data and evaluation setups,\ncalling for a more nuanced understanding of the cross-lingual capabilities of\nmultilingual models.",
      "tldr_zh": "本研究质疑了现有评估基准在衡量多语言模型的多语言知识转移能力方面的准确性，特别是针对零样本（zero-shot）跨语言任务。作者引入更具挑战性的实验设置，包括涉及多种语言的实例，通过广泛实验和分析发现，高性能主要归因于任务级和表面级知识，而非实际语言知识转移，尤其在低资源语言中表现为数据伪像和偏差。这些发现突显了现有跨语言测试数据的局限性，并呼吁对多语言模型的跨语言能力进行更细致的理解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.02099v1",
      "published_date": "2024-02-03 09:41:52 UTC",
      "updated_date": "2024-02-03 09:41:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:51:53.225264"
    },
    {
      "arxiv_id": "2402.02097v2",
      "title": "Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing",
      "title_zh": "翻译失败",
      "authors": [
        "Haobin Jiang",
        "Ziluo Ding",
        "Zongqing Lu"
      ],
      "abstract": "Exploration in decentralized cooperative multi-agent reinforcement learning\nfaces two challenges. One is that the novelty of global states is unavailable,\nwhile the novelty of local observations is biased. The other is how agents can\nexplore in a coordinated way. To address these challenges, we propose MACE, a\nsimple yet effective multi-agent coordinated exploration method. By\ncommunicating only local novelty, agents can take into account other agents'\nlocal novelty to approximate the global novelty. Further, we newly introduce\nweighted mutual information to measure the influence of one agent's action on\nother agents' accumulated novelty. We convert it as an intrinsic reward in\nhindsight to encourage agents to exert more influence on other agents'\nexploration and boost coordinated exploration. Empirically, we show that MACE\nachieves superior performance in three multi-agent environments with sparse\nrewards.",
      "tldr_zh": "在分散式合作多智能体强化学习(decentralized multi-agent reinforcement learning)中，探索面临全球状态新颖性不可用和局部观察偏差两大挑战。为解决这些问题，本文提出MACE方法，通过代理间共享局部新颖性来近似全球新颖性，并引入加权互信息(weighted mutual information)作为内在奖励，鼓励代理影响其他代理的探索以提升协调性。实验结果显示，MACE在三个稀疏奖励的多智能体环境中表现出优越性能。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "17 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.02097v2",
      "published_date": "2024-02-03 09:35:25 UTC",
      "updated_date": "2024-08-10 06:45:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:52:05.251808"
    },
    {
      "arxiv_id": "2402.02094v1",
      "title": "Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjia Xu",
        "Jiuniu Wang",
        "Zhiwei Wei",
        "Mugen Peng",
        "Yirong Wu"
      ],
      "abstract": "Deep neural networks have achieved promising progress in remote sensing (RS)\nimage classification, for which the training process requires abundant samples\nfor each class. However, it is time-consuming and unrealistic to annotate\nlabels for each RS category, given the fact that the RS target database is\nincreasing dynamically. Zero-shot learning (ZSL) allows for identifying novel\nclasses that are not seen during training, which provides a promising solution\nfor the aforementioned problem. However, previous ZSL models mainly depend on\nmanually-labeled attributes or word embeddings extracted from language models\nto transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL\nmodels use convolutional neural networks pre-trained on ImageNet, which focus\non the main objects appearing in each image, neglecting the background context\nthat also matters in RS scene classification. To address the above problems, we\npropose to collect visually detectable attributes automatically. We predict\nattributes for each class by depicting the semantic-visual similarity between\nattributes and images. In this way, the attribute annotation process is\naccomplished by machine instead of human as in other methods. Moreover, we\npropose a Deep Semantic-Visual Alignment (DSVA) that take advantage of the\nself-attention mechanism in the transformer to associate local image regions\ntogether, integrating the background context information for prediction. The\nDSVA model further utilizes the attribute attention maps to focus on the\ninformative image regions that are essential for knowledge transfer in ZSL, and\nmaps the visual images into attribute space to perform ZSL classification. With\nextensive experiments, we show that our model outperforms other\nstate-of-the-art models by a large margin on a challenging large-scale RS scene\nclassification benchmark.",
      "tldr_zh": "这篇论文针对遥感 (RS) 图像场景分类的问题，提出了一种 Deep Semantic-Visual Alignment (DSVA) 方法，以解决 Zero-Shot Learning (ZSL) 中依赖手动属性标注和忽略背景上下文的局限性。DSVA 通过自动预测视觉可检测属性（基于语义-视觉相似性），并利用 transformer 的自注意力机制整合图像局部区域和背景信息，实现从已见类到新类的知识转移。实验结果显示，该模型在大型 RS 场景分类基准上大幅超越最先进模型，证明了其在减少标注需求方面的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in ISPRS P&RS. The code is available at\n  https://github.com/wenjiaXu/RS_Scene_ZSL",
      "pdf_url": "http://arxiv.org/pdf/2402.02094v1",
      "published_date": "2024-02-03 09:18:49 UTC",
      "updated_date": "2024-02-03 09:18:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:52:17.375132"
    },
    {
      "arxiv_id": "2402.02085v8",
      "title": "Detecting AI-Generated Video via Frame Consistency",
      "title_zh": "通过帧一致性检测 AI 生成视频",
      "authors": [
        "Long Ma",
        "Zhiyuan Yan",
        "Qinglang Guo",
        "Yong Liao",
        "Haiyang Yu",
        "Pengyuan Zhou"
      ],
      "abstract": "The escalating quality of video generated by advanced video generation\nmethods results in new security challenges, while there have been few relevant\nresearch efforts: 1) There is no open-source dataset for generated video\ndetection, 2) No generated video detection method has been proposed so far. To\nthis end, we propose an open-source dataset and a detection method for\ngenerated video for the first time. First, we propose a scalable dataset\nconsisting of 964 prompts, covering various forgery targets, scenes, behaviors,\nand actions, as well as various generation models with different architectures\nand generation methods, including the most popular commercial models like\nOpenAI's Sora and Google's Veo. Second, we found via probing experiments that\nspatial artifact-based detectors lack generalizability. Hence, we propose a\nsimple yet effective \\textbf{de}tection model based on \\textbf{f}rame\n\\textbf{co}nsistency (\\textbf{DeCoF}), which focuses on temporal artifacts by\neliminating the impact of spatial artifacts during feature learning. Extensive\nexperiments demonstrate the efficacy of DeCoF in detecting videos generated by\nunseen video generation models and confirm its powerful generalizability across\nseveral commercially proprietary models.",
      "tldr_zh": "该研究针对AI生成视频质量提升带来的安全挑战，首次提出一个公开数据集和一种检测方法，以解决现有研究的缺失。该数据集包含964个提示，涵盖各种伪造目标、场景、行为和动作，并包括不同架构的生成模型，如OpenAI的Sora和Google的Veo。通过探测实验发现，基于空间伪造的检测器缺乏泛化性，因此作者开发了DeCoF（基于帧一致性）模型，该模型通过消除空间伪造影响并专注于时间伪造来提升检测准确性。实验结果显示，DeCoF在检测未知视频生成模型的视频方面表现出色，具有强大的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02085v8",
      "published_date": "2024-02-03 08:52:06 UTC",
      "updated_date": "2025-04-20 11:47:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:52:29.875010"
    },
    {
      "arxiv_id": "2402.02079v1",
      "title": "Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Yangxun Ou",
        "Lei Chen",
        "Fenglin Pan",
        "Yupeng Wu"
      ],
      "abstract": "Graph Collaborative Filtering (GCF), one of the most widely adopted\nrecommendation system methods, effectively captures intricate relationships\nbetween user and item interactions. Graph Contrastive Learning (GCL) based GCF\nhas gained significant attention as it leverages self-supervised techniques to\nextract valuable signals from real-world scenarios. However, many methods\nusually learn the instances of discrimination tasks that involve the\nconstruction of contrastive pairs through random sampling. GCL approaches\nsuffer from sampling bias issues, where the negatives might have a semantic\nstructure similar to that of the positives, thus leading to a loss of effective\nfeature representation. To address these problems, we present the\n\\underline{Proto}typical contrastive learning through \\underline{A}lignment and\n\\underline{U}niformity for recommendation, which is called \\textbf{ProtoAU}.\nSpecifically, we first propose prototypes (cluster centroids) as a latent space\nto ensure consistency across different augmentations from the origin graph,\naiming to eliminate the need for random sampling of contrastive pairs.\nFurthermore, the absence of explicit negatives means that directly optimizing\nthe consistency loss between instance and prototype could easily result in\ndimensional collapse issues. Therefore, we propose aligning and maintaining\nuniformity in the prototypes of users and items as optimization objectives to\nprevent falling into trivial solutions. Finally, we conduct extensive\nexperiments on four datasets and evaluate their performance on the task of link\nprediction. Experimental results demonstrate that the proposed ProtoAU\noutperforms other representative methods. The source codes of our proposed\nProtoAU are available at \\url{https://github.com/oceanlvr/ProtoAU}.",
      "tldr_zh": "本论文针对Graph Contrastive Learning (GCL) 在推荐系统中的采样偏差问题，提出了一种名为ProtoAU的原型对比学习方法，以提升用户和物品特征表示的准确性。具体而言，ProtoAU 通过使用prototypes（原型或聚类中心）作为潜在空间，确保不同图增强版本的一致性，并通过alignment（对齐）和uniformity（均匀性）优化目标来避免维度崩溃。实验结果显示，该方法在四个数据集上的链接预测任务中，表现优于其他代表性方法，为推荐系统的自监督学习提供了新途径。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02079v1",
      "published_date": "2024-02-03 08:19:26 UTC",
      "updated_date": "2024-02-03 08:19:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:52:41.831696"
    },
    {
      "arxiv_id": "2402.03375v3",
      "title": "BetterV: Controlled Verilog Generation with Discriminative Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Zehua Pei",
        "Hui-Ling Zhen",
        "Mingxuan Yuan",
        "Yu Huang",
        "Bei Yu"
      ],
      "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), there is a\nneed for automated circuit design methods. Recent years have seen rising\nresearch in hardware design language generation to facilitate the design\nprocess. In this work, we propose a Verilog generation framework, BetterV,\nwhich fine-tunes the large language models (LLMs) on processed domain-specific\ndatasets and incorporates generative discriminators for guidance on particular\ndesign demands. The Verilog modules are collected, filtered and processed from\ninternet to form a clean and abundant dataset. Instruct-tuning methods are\nspecially designed to fine-tune the LLMs to understand the knowledge about\nVerilog. Furthermore, data are augmented to enrich the training set and also\nused to train a generative discriminator on particular downstream task, which\nleads a guidance for the LLMs to optimize the Verilog implementation. BetterV\nhas the ability to generate syntactically and functionally correct Verilog,\nwhich can outperform GPT-4 on the VerilogEval benchmark. With the help of\ntask-specific generative discriminator, BetterV can achieve remarkable\nimprovement on various electronic design automation (EDA) downstream tasks,\nincluding the netlist node reduction for synthesis and verification runtime\nreduction with Boolean Satisfiability (SAT) solving.",
      "tldr_zh": "这篇论文提出 BetterV 框架，用于自动化 Verilog 生成，以应对现代集成电路 (ICs) 的复杂性。该框架通过收集、过滤互联网数据构建数据集，并使用 Instruct-tuning 方法微调大型语言模型 (LLMs)，结合数据增强和生成式鉴别器来指导 LLMs 优化 Verilog 实现。BetterV 能生成语法和功能正确的 Verilog，并在 VerilogEval 基准上超越 GPT-4；此外，通过任务特定的生成式鉴别器，它在电子设计自动化 (EDA) 任务中实现了网表节点减少和 SAT 求解验证运行时间显著降低。",
      "categories": [
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.03375v3",
      "published_date": "2024-02-03 08:00:12 UTC",
      "updated_date": "2024-05-02 09:18:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:52:55.513142"
    },
    {
      "arxiv_id": "2402.02066v1",
      "title": "Trustworthiness of $\\mathbb{X}$ Users: A One-Class Classification Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Tanveer Khan",
        "Fahad Sohrab",
        "Antonis Michalas",
        "Moncef Gabbouj"
      ],
      "abstract": "$\\mathbb{X}$ (formerly Twitter) is a prominent online social media platform\nthat plays an important role in sharing information making the content\ngenerated on this platform a valuable source of information. Ensuring trust on\n$\\mathbb{X}$ is essential to determine the user credibility and prevents issues\nacross various domains. While assigning credibility to $\\mathbb{X}$ users and\nclassifying them as trusted or untrusted is commonly carried out using\ntraditional machine learning models, there is limited exploration about the use\nof One-Class Classification (OCC) models for this purpose. In this study, we\nuse various OCC models for $\\mathbb{X}$ user classification. Additionally, we\npropose using a subspace-learning-based approach that simultaneously optimizes\nboth the subspace and data description for OCC. We also introduce a novel\nregularization term for Subspace Support Vector Data Description (SSVDD),\nexpressing data concentration in a lower-dimensional subspace that captures\ndiverse graph structures. Experimental results show superior performance of the\nintroduced regularization term for SSVDD compared to baseline models and\nstate-of-the-art techniques for $\\mathbb{X}$ user classification.",
      "tldr_zh": "本研究探讨了使用 One-Class Classification (OCC) 模型来评估 $\\mathbb{X}$（前身为 Twitter）用户的可信度，以解决传统机器学习方法的局限性。研究者应用多种 OCC 模型进行用户分类，并提出一种基于子空间学习的创新方法，该方法同时优化子空间和数据描述。特别地，他们引入了一个新颖的正则化项应用于 Subspace Support Vector Data Description (SSVDD)，以捕捉数据在低维子空间中的集中和多样图结构。实验结果表明，该正则化项使 SSVDD 在 $\\mathbb{X}$ 用户分类任务中比基线模型和最先进技术表现出色，显著提升了性能。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02066v1",
      "published_date": "2024-02-03 07:14:33 UTC",
      "updated_date": "2024-02-03 07:14:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:53:09.314718"
    },
    {
      "arxiv_id": "2402.02056v1",
      "title": "AnthroScore: A Computational Linguistic Measure of Anthropomorphism",
      "title_zh": "翻译失败",
      "authors": [
        "Myra Cheng",
        "Kristina Gligoric",
        "Tiziano Piccardi",
        "Dan Jurafsky"
      ],
      "abstract": "Anthropomorphism, or the attribution of human-like characteristics to\nnon-human entities, has shaped conversations about the impacts and\npossibilities of technology. We present AnthroScore, an automatic metric of\nimplicit anthropomorphism in language. We use a masked language model to\nquantify how non-human entities are implicitly framed as human by the\nsurrounding context. We show that AnthroScore corresponds with human judgments\nof anthropomorphism and dimensions of anthropomorphism described in social\nscience literature. Motivated by concerns of misleading anthropomorphism in\ncomputer science discourse, we use AnthroScore to analyze 15 years of research\npapers and downstream news articles. In research papers, we find that\nanthropomorphism has steadily increased over time, and that papers related to\nlanguage models have the most anthropomorphism. Within ACL papers, temporal\nincreases in anthropomorphism are correlated with key neural advancements.\nBuilding upon concerns of scientific misinformation in mass media, we identify\nhigher levels of anthropomorphism in news headlines compared to the research\npapers they cite. Since AnthroScore is lexicon-free, it can be directly applied\nto a wide range of text sources.",
      "tldr_zh": "这篇论文引入了AnthroScore，一种计算语言学指标，用于自动测量语言中隐式拟人化(anthropomorphism)。该方法利用masked language model量化非人类实体在上下文中的人类化框架，并证明AnthroScore与人类判断和社会科学文献中的拟人化维度高度一致。在分析15年的研究论文和新闻文章时，研究发现拟人化在计算机科学论文中稳步增加，尤其是语言模型相关论文，且新闻标题的拟人化水平高于引用的论文。由于AnthroScore是词汇无关的，它可以直接应用于广泛的文本来源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "EACL 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2402.02056v1",
      "published_date": "2024-02-03 06:36:11 UTC",
      "updated_date": "2024-02-03 06:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:53:21.728944"
    },
    {
      "arxiv_id": "2402.02055v1",
      "title": "Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yiping Wang",
        "Yifang Chen",
        "Wendan Yan",
        "Kevin Jamieson",
        "Simon Shaolei Du"
      ],
      "abstract": "In recent years, data selection has emerged as a core issue for large-scale\nvisual-language model pretraining, especially on noisy web-curated datasets.\nOne widely adopted strategy assigns quality scores such as CLIP similarity for\neach sample and retains the data pairs with the highest scores. However, these\napproaches are agnostic of data distribution and always fail to select the most\ninformative samples. To solve this problem, we propose a simple yet\ntheoretically principled metric named Variance Alignment Score (VAS), which has\nthe form $\\langle \\Sigma_{\\text{test}}, \\Sigma_i\\rangle$. Here,\n$\\Sigma_{\\text{test}}$ represents the target (cross-)covariance matrix we aim\nto align, potentially based on prior knowledge, while $\\Sigma_i$ denotes the\ntensor product of single or multi-modal representations for the $i$-th sample.\nWe further design a new data selection method that maximizes the total VAS. We\nprovide theoretical analysis in a simplified setting to demonstrate the\ntheoretical advantage of VAS over random or other existing data selection.\nExperimentally, applying VAS and CLIP scores together can outperform baselines\nby a margin of $1.3\\%$ average on 38 evaluation sets for noisy dataset DataComp\nand $2.5\\%$ on VTAB for high-quality dataset CC12M. Additionally, our ablation\nstudy also shows visual features are better than text for calculating VAS, and\nthe related classical experimental design methods may fail under this context.",
      "tldr_zh": "本研究针对多模态对比学习(Multimodal Contrastive Learning)中数据选择的核心问题，提出了一种简单却高效的指标——Variance Alignment Score (VAS)，其形式为 \\(\\langle \\Sigma_{\\text{test}}, \\Sigma_i \\rangle\\)，旨在通过最大化总 VAS 来选择最信息丰富的样本，从而解决现有方法（如 CLIP 相似度评分）忽略数据分布的局限。VAS 基于目标协方差矩阵与样本表示的张量积对齐，理论分析显示其在简化设置下优于随机或传统数据选择方法。实验结果表明，将 VAS 与 CLIP 得分结合使用，在嘈杂数据集 DataComp 的 38 个评估集上平均提升 1.3%，而在高品质数据集 CC12M 的 VTAB 任务上提升 2.5%；此外，消融研究证实视觉特征在计算 VAS 时优于文本特征，且经典实验设计方法在此情境下可能失效。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.02055v1",
      "published_date": "2024-02-03 06:29:04 UTC",
      "updated_date": "2024-02-03 06:29:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:53:34.459898"
    },
    {
      "arxiv_id": "2402.05946v2",
      "title": "Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation",
      "title_zh": "揭示潜在因果规则：一种时间点过程方法用于异常事件解释",
      "authors": [
        "Yiling Kuang",
        "Chao Yang",
        "Yang Yang",
        "Shuang Li"
      ],
      "abstract": "In high-stakes systems such as healthcare, it is critical to understand the\ncausal reasons behind unusual events, such as sudden changes in patient's\nhealth. Unveiling the causal reasons helps with quick diagnoses and precise\ntreatment planning. In this paper, we propose an automated method for\nuncovering \"if-then\" logic rules to explain observational events. We introduce\ntemporal point processes to model the events of interest, and discover the set\nof latent rules to explain the occurrence of events. To achieve this, we employ\nan Expectation-Maximization (EM) algorithm. In the E-step, we calculate the\nlikelihood of each event being explained by each discovered rule. In the\nM-step, we update both the rule set and model parameters to enhance the\nlikelihood function's lower bound. Notably, we optimize the rule set in a\ndifferential manner. Our approach demonstrates accurate performance in both\ndiscovering rules and identifying root causes. We showcase its promising\nresults using synthetic and real healthcare datasets.",
      "tldr_zh": "该论文提出了一种自动化方法，使用 Temporal Point Processes 建模事件，以揭示潜在的“if-then”逻辑规则，从而解释高风险系统（如医疗保健）中的异常事件，例如患者健康突变。方法采用 Expectation-Maximization (EM) 算法，其中 E-step 计算每个事件被规则解释的可能性，M-step 通过微分优化更新规则集和模型参数，以提升似然函数的下界。该方法在合成和真实医疗数据集上表现出色，能够准确发现规则和识别根本原因，从而支持快速诊断和精确治疗规划。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AISTATS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.05946v2",
      "published_date": "2024-02-03 06:21:33 UTC",
      "updated_date": "2024-03-19 08:43:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:53:44.960944"
    },
    {
      "arxiv_id": "2402.02054v3",
      "title": "Towards Neural Scaling Laws on Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Jingzhe Liu",
        "Haitao Mao",
        "Zhikai Chen",
        "Tong Zhao",
        "Neil Shah",
        "Jiliang Tang"
      ],
      "abstract": "Deep graph models (e.g., graph neural networks and graph transformers) have\nbecome important techniques for leveraging knowledge across various types of\ngraphs. Yet, the neural scaling laws on graphs, i.e., how the performance of\ndeep graph models changes with model and dataset sizes, have not been\nsystematically investigated, casting doubts on the feasibility of achieving\nlarge graph models. To fill this gap, we benchmark many graph datasets from\ndifferent tasks and make an attempt to establish the neural scaling laws on\ngraphs from both model and data perspectives. The model size we investigated is\nup to 100 million parameters, and the dataset size investigated is up to 50\nmillion samples. We first verify the validity of such laws on graphs,\nestablishing proper formulations to describe the scaling behaviors. For model\nscaling, we identify that despite the parameter numbers, the model depth also\nplays an important role in affecting the model scaling behaviors, which differs\nfrom observations in other domains such as CV and NLP. For data scaling, we\nsuggest that the number of graphs can not effectively measure the graph data\nvolume in scaling law since the sizes of different graphs are highly irregular.\nInstead, we reform the data scaling law with the number of nodes or edges as\nthe metric to address the irregular graph sizes. We further demonstrate that\nthe reformed law offers a unified view of the data scaling behaviors for\nvarious fundamental graph tasks including node classification, link prediction,\nand graph classification. This work provides valuable insights into neural\nscaling laws on graphs, which can serve as an important tool for collecting new\ngraph data and developing large graph models.",
      "tldr_zh": "本文探讨了深度图模型（如graph neural networks和graph transformers）的神经缩放定律（neural scaling laws），即模型性能如何随模型大小和数据集大小变化。研究通过基准测试多种图任务，验证了这些定律在图上的有效性，并发现模型深度对缩放行为的影响显著，与CV和NLP领域不同。对于数据缩放，作者提出使用节点或边数作为度量指标来应对图大小的不规则性，从而统一适用于节点分类、链接预测和图分类等任务。该工作为收集新图数据和发展大型graph models提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02054v3",
      "published_date": "2024-02-03 06:17:21 UTC",
      "updated_date": "2024-11-30 01:55:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:53:57.101782"
    },
    {
      "arxiv_id": "2402.02053v2",
      "title": "Affordable Generative Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yangbin Yu",
        "Qin Zhang",
        "Junyou Li",
        "Qiang Fu",
        "Deheng Ye"
      ],
      "abstract": "The emergence of large language models (LLMs) has significantly advanced the\nsimulation of believable interactive agents. However, the substantial cost on\nmaintaining the prolonged agent interactions poses challenge over the\ndeployment of believable LLM-based agents. Therefore, in this paper, we develop\nAffordable Generative Agents (AGA), a framework for enabling the generation of\nbelievable and low-cost interactions on both agent-environment and inter-agents\nlevels. Specifically, for agent-environment interactions, we substitute\nrepetitive LLM inferences with learned policies; while for inter-agent\ninteractions, we model the social relationships between agents and compress\nauxiliary dialogue information. Extensive experiments on multiple environments\nshow the effectiveness and efficiency of our proposed framework. Also, we delve\ninto the mechanisms of emergent believable behaviors lying in LLM agents,\ndemonstrating that agents can only generate finite behaviors in fixed\nenvironments, based upon which, we understand ways to facilitate emergent\ninteraction behaviors. Our code is publicly available at:\nhttps://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在模拟可信互动代理时的成本挑战，提出 Affordable Generative Agents (AGA) 框架，以实现低成本的代理互动生成。具体方法包括使用学习策略替换代理-环境互动中的重复 LLM 推理，以及通过建模社会关系和压缩辅助对话信息来优化代理间互动。实验在多个环境中验证了 AGA 的有效性和效率，并揭示了 LLM 代理在固定环境中仅能产生有限行为的机制，为开发经济型可信代理提供了新途径。代码已在 GitHub 上开源。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02053v2",
      "published_date": "2024-02-03 06:16:28 UTC",
      "updated_date": "2024-08-28 04:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:54:07.788745"
    },
    {
      "arxiv_id": "2402.05120v2",
      "title": "More Agents Is All You Need",
      "title_zh": "翻译失败",
      "authors": [
        "Junyou Li",
        "Qin Zhang",
        "Yangbin Yu",
        "Qiang Fu",
        "Deheng Ye"
      ],
      "abstract": "We find that, simply via a sampling-and-voting method, the performance of\nlarge language models (LLMs) scales with the number of agents instantiated.\nAlso, this method, termed as Agent Forest, is orthogonal to existing\ncomplicated methods to further enhance LLMs, while the degree of enhancement is\ncorrelated to the task difficulty. We conduct comprehensive experiments on a\nwide range of LLM benchmarks to verify the presence of our finding, and to\nstudy the properties that can facilitate its occurrence. Our code is publicly\navailable at: https://github.com/MoreAgentsIsAllYouNeed/AgentForest",
      "tldr_zh": "该研究发现，通过一种名为 Agent Forest 的采样和投票方法，大语言模型（LLMs）的性能会随着代理数量的增加而线性提升。该方法与现有复杂增强技术正交，且其效果与任务难度正相关，适用于各种 LLM 基准。研究者通过全面实验验证了这一现象，并探讨了促进其发生的属性，同时开源了代码以便进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at Transactions on Machine Learning Research (TMLR)",
      "pdf_url": "http://arxiv.org/pdf/2402.05120v2",
      "published_date": "2024-02-03 05:55:24 UTC",
      "updated_date": "2024-10-11 09:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:54:19.366895"
    },
    {
      "arxiv_id": "2402.02043v1",
      "title": "A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjun Huang",
        "Arghavan Rezvani",
        "Hanning Chen",
        "Yang Ni",
        "Sanggeon Yun",
        "Sungheon Jeong",
        "Mohsen Imani"
      ],
      "abstract": "Applications in the Internet of Things (IoT) utilize machine learning to\nanalyze sensor-generated data. However, a major challenge lies in the lack of\ntargeted intelligence in current sensing systems, leading to vast data\ngeneration and increased computational and communication costs. To address this\nchallenge, we propose a novel sensing module to equip sensing frameworks with\nintelligent data transmission capabilities by integrating a highly efficient\nmachine learning model placed near the sensor. This model provides prompt\nfeedback for the sensing system to transmit only valuable data while discarding\nirrelevant information by regulating the frequency of data transmission. The\nnear-sensor model is quantized and optimized for real-time sensor control. To\nenhance the framework's performance, the training process is customized and a\n\"lazy\" sensor deactivation strategy utilizing temporal information is\nintroduced. The suggested method is orthogonal to other IoT frameworks and can\nbe considered as a plugin for selective data transmission. The framework is\nimplemented, encompassing both software and hardware components. The\nexperiments demonstrate that the framework utilizing the suggested module\nachieves over 85% system efficiency in terms of energy consumption and storage,\nwith negligible impact on performance. This methodology has the potential to\nsignificantly reduce data output from sensors, benefiting a wide range of IoT\napplications.",
      "tldr_zh": "该研究针对物联网（IoT）中传感器数据过量导致的计算和通信成本问题，提出了一种插件式的小型 AI 模块，用于智能选择性数据传输。该模块在传感器附近集成高效的机器学习模型，通过实时反馈机制仅传输有价值数据，并采用模型量化优化、定制训练过程以及“lazy”传感器停用策略来提升性能。实验结果显示，该框架在能量消耗和存储效率方面超过85%，对系统性能影响微乎其微，从而显著减少传感器数据输出，并适用于广泛的IoT应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.02043v1",
      "published_date": "2024-02-03 05:41:39 UTC",
      "updated_date": "2024-02-03 05:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:54:30.992014"
    },
    {
      "arxiv_id": "2402.02042v3",
      "title": "Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm",
      "title_zh": "翻译失败",
      "authors": [
        "Qinbo Bai",
        "Washim Uddin Mondal",
        "Vaneet Aggarwal"
      ],
      "abstract": "This paper explores the realm of infinite horizon average reward Constrained\nMarkov Decision Processes (CMDPs). To the best of our knowledge, this work is\nthe first to delve into the regret and constraint violation analysis of average\nreward CMDPs with a general policy parametrization. To address this challenge,\nwe propose a primal dual-based policy gradient algorithm that adeptly manages\nthe constraints while ensuring a low regret guarantee toward achieving a global\noptimal policy. In particular, our proposed algorithm achieves\n$\\tilde{\\mathcal{O}}({T}^{4/5})$ objective regret and\n$\\tilde{\\mathcal{O}}({T}^{4/5})$ constraint violation bounds.",
      "tldr_zh": "这篇论文首次分析了无限期平均奖励Constrained MDPs（CMDPs）中一般策略参数化的遗憾（regret）和约束违反（constraint violation）问题。作者提出了一种基于primal-dual policy gradient算法，能够有效处理约束并确保实现全局最优策略。实验结果表明，该算法在目标遗憾和约束违反方面均达到了$\\tilde{\\mathcal{O}}({T}^{4/5})$的边界，为这类问题提供了高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02042v3",
      "published_date": "2024-02-03 05:35:58 UTC",
      "updated_date": "2024-10-30 05:42:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:54:43.599922"
    },
    {
      "arxiv_id": "2402.02033v1",
      "title": "Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization",
      "title_zh": "CEC 2024 多方多目标优化竞赛基准",
      "authors": [
        "Wenjian Luo",
        "Peilan Xu",
        "Shengxiang Yang",
        "Yuhui Shi"
      ],
      "abstract": "The competition focuses on Multiparty Multiobjective Optimization Problems\n(MPMOPs), where multiple decision makers have conflicting objectives, as seen\nin applications like UAV path planning. Despite their importance, MPMOPs remain\nunderstudied in comparison to conventional multiobjective optimization. The\ncompetition aims to address this gap by encouraging researchers to explore\ntailored modeling approaches. The test suite comprises two parts: problems with\ncommon Pareto optimal solutions and Biparty Multiobjective UAV Path Planning\n(BPMO-UAVPP) problems with unknown solutions. Optimization algorithms for the\nfirst part are evaluated using Multiparty Inverted Generational Distance\n(MPIGD), and the second part is evaluated using Multiparty Hypervolume (MPHV)\nmetrics. The average algorithm ranking across all problems serves as a\nperformance benchmark.",
      "tldr_zh": "该论文为 CEC 2024 比赛建立了 Multiparty Multiobjective Optimization Problems (MPMOPs) 的基准测试套件，旨在填补多方多目标优化领域的研究空白，这些问题涉及多个决策者间的冲突目标，如 UAV 路径规划。测试套件分为两部分：第一部分是具有共同 Pareto optimal solutions 的问题，第二部分是 Biparty Multiobjective UAV Path Planning (BPMO-UAVPP) 问题。算法性能评估分别使用 Multiparty Inverted Generational Distance (MPIGD) 和 Multiparty Hypervolume (MPHV) 指标，最终通过所有问题的平均算法排名作为整体基准。这一框架鼓励研究者探索定制的建模方法，以推动 MPMOPs 的研究进展。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 0 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.02033v1",
      "published_date": "2024-02-03 05:14:03 UTC",
      "updated_date": "2024-02-03 05:14:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:54:56.857915"
    },
    {
      "arxiv_id": "2402.02029v1",
      "title": "ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Zihan Li",
        "Yuan Zheng",
        "Dandan Shan",
        "Shuzhou Yang",
        "Qingde Li",
        "Beizhan Wang",
        "Yuanting Zhang",
        "Qingqi Hong",
        "Dinggang Shen"
      ],
      "abstract": "Most recent scribble-supervised segmentation methods commonly adopt a CNN\nframework with an encoder-decoder architecture. Despite its multiple benefits,\nthis framework generally can only capture small-range feature dependency for\nthe convolutional layer with the local receptive field, which makes it\ndifficult to learn global shape information from the limited information\nprovided by scribble annotations. To address this issue, this paper proposes a\nnew CNN-Transformer hybrid solution for scribble-supervised medical image\nsegmentation called ScribFormer. The proposed ScribFormer model has a\ntriple-branch structure, i.e., the hybrid of a CNN branch, a Transformer\nbranch, and an attention-guided class activation map (ACAM) branch.\nSpecifically, the CNN branch collaborates with the Transformer branch to fuse\nthe local features learned from CNN with the global representations obtained\nfrom Transformer, which can effectively overcome limitations of existing\nscribble-supervised segmentation methods. Furthermore, the ACAM branch assists\nin unifying the shallow convolution features and the deep convolution features\nto improve model's performance further. Extensive experiments on two public\ndatasets and one private dataset show that our ScribFormer has superior\nperformance over the state-of-the-art scribble-supervised segmentation methods,\nand achieves even better results than the fully-supervised segmentation\nmethods. The code is released at https://github.com/HUANGLIZI/ScribFormer.",
      "tldr_zh": "本研究针对基于 scribble 标注的医疗图像分割问题，指出传统 CNN 框架难以捕捉全局形状信息，从而提出了一种 CNN-Transformer 混合模型 ScribFormer，以提升分割性能。ScribFormer 采用三支结构，包括 CNN 分支用于提取局部特征、Transformer 分支用于获取全局表示，以及 attention-guided class activation map (ACAM) 分支来统一浅层和深层特征，实现有效特征融合。实验结果显示，该模型在两个公共数据集和一个私有数据集上优于现有 scribble-supervised segmentation 方法，甚至超越 fully-supervised 方法，证明了其显著优势。代码已开源在 GitHub。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE Transactions on Medical Imaging (TMI)",
      "pdf_url": "http://arxiv.org/pdf/2402.02029v1",
      "published_date": "2024-02-03 04:55:22 UTC",
      "updated_date": "2024-02-03 04:55:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:55:09.134847"
    },
    {
      "arxiv_id": "2402.02026v2",
      "title": "Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Lixing Xiao",
        "Ruixiao Shi",
        "Xiaoyang Tang",
        "Yi Zhou"
      ],
      "abstract": "Previous works on object detection have achieved high accuracy in closed-set\nscenarios, but their performance in open-world scenarios is not satisfactory.\nOne of the challenging open-world problems is corner case detection in\nautonomous driving. Existing detectors struggle with these cases, relying\nheavily on visual appearance and exhibiting poor generalization ability. In\nthis paper, we propose a solution by reducing the discrepancy between known and\nunknown classes and introduce a multimodal-enhanced objectness notion learner.\nLeveraging both vision-centric and image-text modalities, our semi-supervised\nlearning framework imparts objectness knowledge to the student model, enabling\nclass-aware detection. Our approach, Multimodal-Enhanced Objectness Learner\n(MENOL) for Corner Case Detection, significantly improves recall for novel\nclasses with lower training costs. By achieving a 76.6% mAR-corner and 79.8%\nmAR-agnostic on the CODA-val dataset with just 5100 labeled training images,\nMENOL outperforms the baseline ORE by 71.3% and 60.6%, respectively. The code\nwill be available at https://github.com/tryhiseyyysum/MENOL.",
      "tldr_zh": "该研究针对自动驾驶中角落案例（corner case）检测的挑战，提出 Multimodal-Enhanced Objectness Learner (MENOL)，一个利用视觉和图像-文本多模态的半监督学习框架，以减少已知与未知类别间的差异并增强对象性（objectness）知识。MENOL 通过向学生模型传授类感知检测能力，提高了新类别的召回率，同时降低了训练成本。在 CODA-val 数据集上，MENOL 仅使用 5100 张标注图像，就实现了 76.6% mAR-corner 和 79.8% mAR-agnostic 的性能，分别比基线 ORE 提高了 71.3% 和 60.6%。这为开放世界物体检测提供了更高效且泛化能力强的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to 2024 IEEE International Conference on Image Processing\n  (ICIP) as oral presentation",
      "pdf_url": "http://arxiv.org/pdf/2402.02026v2",
      "published_date": "2024-02-03 04:47:03 UTC",
      "updated_date": "2024-09-28 08:40:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:55:24.781316"
    },
    {
      "arxiv_id": "2402.05119v5",
      "title": "A Closer Look at the Limitations of Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Sreyan Ghosh",
        "Chandra Kiran Reddy Evuru",
        "Sonal Kumar",
        "Ramaneswaran S",
        "Deepali Aneja",
        "Zeyu Jin",
        "Ramani Duraiswami",
        "Dinesh Manocha"
      ],
      "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.",
      "tldr_zh": "这篇论文通过实验分析揭示了 Instruction Tuning (IT) 在训练大型语言模型 (LLMs) 时存在的局限性。研究发现，IT 无法提升模型的知识或技能，LoRA 微调仅限于学习响应起始和风格标记，而全参数微调则导致知识退化和 hallucination（幻觉）增加。论文进一步指出，从 IT 数据集复制响应模式会降低响应质量，且流行改进方法无法超越简单 LoRA 微调模型的表现。最终，基于预训练知识生成的响应在开源数据集上表现出色，论文呼吁这些见解激发未来相关研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.05119v5",
      "published_date": "2024-02-03 04:45:25 UTC",
      "updated_date": "2024-07-14 18:14:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:55:35.368154"
    },
    {
      "arxiv_id": "2402.02025v2",
      "title": "A Survey of Constraint Formulations in Safe Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Akifumi Wachi",
        "Xun Shen",
        "Yanan Sui"
      ],
      "abstract": "Safety is critical when applying reinforcement learning (RL) to real-world\nproblems. As a result, safe RL has emerged as a fundamental and powerful\nparadigm for optimizing an agent's policy while incorporating notions of\nsafety. A prevalent safe RL approach is based on a constrained criterion, which\nseeks to maximize the expected cumulative reward subject to specific safety\nconstraints. Despite recent effort to enhance safety in RL, a systematic\nunderstanding of the field remains difficult. This challenge stems from the\ndiversity of constraint representations and little exploration of their\ninterrelations. To bridge this knowledge gap, we present a comprehensive review\nof representative constraint formulations, along with a curated selection of\nalgorithms designed specifically for each formulation. In addition, we\nelucidate the theoretical underpinnings that reveal the mathematical mutual\nrelations among common problem formulations. We conclude with a discussion of\nthe current state and future directions of safe reinforcement learning\nresearch.",
      "tldr_zh": "这篇论文对Safe Reinforcement Learning（安全强化学习）中的约束表述进行了全面调查，旨在解决RL在实际应用中安全性的关键挑战。论文回顾了各种代表性约束形式，并选取了针对每种形式的特定算法，同时阐明了这些约束之间的数学相互关系。通过系统分析，研究揭示了约束多样性的互相关联，为提升RL的安全性提供了理论基础，并讨论了该领域的当前进展和未来方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IJCAI-24 survey track",
      "pdf_url": "http://arxiv.org/pdf/2402.02025v2",
      "published_date": "2024-02-03 04:40:31 UTC",
      "updated_date": "2024-05-08 00:59:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:55:45.896737"
    },
    {
      "arxiv_id": "2402.02023v2",
      "title": "Self-Supervised Contrastive Learning for Long-term Forecasting",
      "title_zh": "自监督对比学习用于长期预测",
      "authors": [
        "Junwoo Park",
        "Daehoon Gwak",
        "Jaegul Choo",
        "Edward Choi"
      ],
      "abstract": "Long-term forecasting presents unique challenges due to the time and memory\ncomplexity of handling long sequences. Existing methods, which rely on sliding\nwindows to process long sequences, struggle to effectively capture long-term\nvariations that are partially caught within the short window (i.e.,\nouter-window variations). In this paper, we introduce a novel approach that\novercomes this limitation by employing contrastive learning and enhanced\ndecomposition architecture, specifically designed to focus on long-term\nvariations. To this end, our contrastive loss incorporates global\nautocorrelation held in the whole time series, which facilitates the\nconstruction of positive and negative pairs in a self-supervised manner. When\ncombined with our decomposition networks, our contrastive learning\nsignificantly improves long-term forecasting performance. Extensive experiments\ndemonstrate that our approach outperforms 14 baseline models in multiple\nexperiments over nine long-term benchmarks, especially in challenging scenarios\nthat require a significantly long output for forecasting. Source code is\navailable at\nhttps://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.",
      "tldr_zh": "该论文针对长期预测中的时间和内存复杂性问题，提出了一种基于自监督对比学习(Self-Supervised Contrastive Learning)的新方法，以有效捕捉长期变化。方法通过整合全局自相关(global autocorrelation)的对比损失函数，自监督方式构建正负样本对，并结合增强分解架构(enhanced decomposition architecture)，显著提升预测性能。实验结果显示，该方法在九个长期基准数据集上优于14个基线模型，尤其在需要长输出预测的挑战场景中。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at International Conference on Learning Representations\n  (ICLR) 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.02023v2",
      "published_date": "2024-02-03 04:32:34 UTC",
      "updated_date": "2024-03-24 04:01:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:55:57.358459"
    },
    {
      "arxiv_id": "2402.05945v1",
      "title": "Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised, Hierarchical Concept Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ao Sun",
        "Yuanyuan Yuan",
        "Pingchuan Ma",
        "Shuai Wang"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) aim to deliver interpretable and\ninterventionable predictions by bridging features and labels with\nhuman-understandable concepts. While recent CBMs show promising potential, they\nsuffer from information leakage, where unintended information beyond the\nconcepts (either when concepts are represented with probabilities or binary\nstates) are leaked to the subsequent label prediction. Consequently, distinct\nclasses are falsely classified via indistinguishable concepts, undermining the\ninterpretation and intervention of CBMs.\n  This paper alleviates the information leakage issue by introducing label\nsupervision in concept predication and constructing a hierarchical concept set.\nAccordingly, we propose a new paradigm of CBMs, namely SupCBM, which achieves\nlabel predication via predicted concepts and a deliberately-designed\nintervention matrix. SupCBM focuses on concepts that are mostly relevant to the\npredicted label and only distinguishes classes when different concepts are\npresented. Our evaluations show that SupCBM outperforms SOTA CBMs over diverse\ndatasets. It also manifests better generality across different backbone models.\nWith proper quantification of information leakage in different CBMs, we\ndemonstrate that SupCBM significantly reduces the information leakage.",
      "tldr_zh": "这篇论文针对Concept Bottleneck Models (CBMs)中存在的信息泄漏问题提出解决方案，该问题会导致额外信息干扰标签预测，从而影响模型的可解释性和可干预性。作者引入标签监督和分层概念集，开发了新范式SupCBM，通过预测的概念和精心设计的干预矩阵，仅在不同概念出现时区分类别，确保预测更准确且相关。实验结果显示，SupCBM在多种数据集上优于现有最先进CBMs，具有更好的泛化性和显著减少的信息泄漏。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.05945v1",
      "published_date": "2024-02-03 03:50:58 UTC",
      "updated_date": "2024-02-03 03:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:56:09.523890"
    },
    {
      "arxiv_id": "2402.02008v1",
      "title": "How well do LLMs cite relevant medical references? An evaluation framework and analyses",
      "title_zh": "LLMs 在引用相关医疗参考文献方面表现如何？一个评估框架和分析",
      "authors": [
        "Kevin Wu",
        "Eric Wu",
        "Ally Cassasola",
        "Angela Zhang",
        "Kevin Wei",
        "Teresa Nguyen",
        "Sith Riantawan",
        "Patricia Shi Riantawan",
        "Daniel E. Ho",
        "James Zou"
      ],
      "abstract": "Large language models (LLMs) are currently being used to answer medical\nquestions across a variety of clinical domains. Recent top-performing\ncommercial LLMs, in particular, are also capable of citing sources to support\ntheir responses. In this paper, we ask: do the sources that LLMs generate\nactually support the claims that they make? To answer this, we propose three\ncontributions. First, as expert medical annotations are an expensive and\ntime-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is\nhighly accurate in validating source relevance, agreeing 88% of the time with a\npanel of medical doctors. Second, we develop an end-to-end, automated pipeline\ncalled \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs\non a dataset of 1200 generated questions, totaling over 40K pairs of statements\nand sources. Interestingly, we find that between ~50% to 90% of LLM responses\nare not fully supported by the sources they provide. We also evaluate GPT-4\nwith retrieval augmented generation (RAG) and find that, even still, around\n30\\% of individual statements are unsupported, while nearly half of its\nresponses are not fully supported. Third, we open-source our curated dataset of\nmedical questions and expert annotations for future evaluations. Given the\nrapid pace of LLM development and the potential harms of incorrect or outdated\nmedical information, it is crucial to also understand and quantify their\ncapability to produce relevant, trustworthy medical references.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLMs）在回答医疗问题时引用相关来源的准确性。研究者提出一个自动化框架SourceCheckup，使用GPT-4作为高效评估工具（与医疗专家一致率达88%），并测试了五个顶级LLMs在1200个问题上的超过40K对声明和来源，结果显示50%到90%的LLMs响应未被来源充分支持，即使采用检索增强生成（RAG），GPT-4仍有约30%的单个声明和近半响应不被证实。该研究开源了医疗问题数据集和专家注解，强调了解LLMs生成可信医疗引用的能力至关重要，以避免错误信息带来的潜在危害。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.02008v1",
      "published_date": "2024-02-03 03:44:57 UTC",
      "updated_date": "2024-02-03 03:44:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:56:22.405084"
    },
    {
      "arxiv_id": "2402.01999v1",
      "title": "A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed Mejri",
        "Chandramouli Amarnath",
        "Abhijit Chatterjee"
      ],
      "abstract": "In recent years, both online and offline deep learning models have been\ndeveloped for time series forecasting. However, offline deep forecasting models\nfail to adapt effectively to changes in time-series data, while online deep\nforecasting models are often expensive and have complex training procedures. In\nthis paper, we reframe the online nonlinear time-series forecasting problem as\none of linear hyperdimensional time-series forecasting. Nonlinear\nlow-dimensional time-series data is mapped to high-dimensional\n(hyperdimensional) spaces for linear hyperdimensional prediction, allowing\nfast, efficient and lightweight online time-series forecasting. Our framework,\nTSF-HD, adapts to time-series distribution shifts using a novel co-training\nframework for its hyperdimensional mapping and its linear hyperdimensional\npredictor. TSF-HD is shown to outperform the state of the art, while having\nreduced inference latency, for both short-term and long-term time series\nforecasting. Our code is publicly available at\nhttp://github.com/tsfhd2024/tsf-hd.git",
      "tldr_zh": "本文提出了一种新型超维计算(Hyperdimensional Computing)框架TSF-HD，用于边缘计算上的在线时间序列预测。它将非线性低维时间序列数据映射到高维空间，并通过线性预测和协同训练(Co-training)框架，实现快速、轻量级的预测，同时适应数据分布变化。实验结果表明，TSF-HD在短期和长期预测中超过了现有技术，并显著降低了推理延迟。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01999v1",
      "published_date": "2024-02-03 02:42:53 UTC",
      "updated_date": "2024-02-03 02:42:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:56:32.901508"
    },
    {
      "arxiv_id": "2402.01994v1",
      "title": "Human-Centered Privacy Research in the Age of Large Language Models",
      "title_zh": "在大型语言模型时代的人类中心隐私研究",
      "authors": [
        "Tianshi Li",
        "Sauvik Das",
        "Hao-Ping Lee",
        "Dakuo Wang",
        "Bingsheng Yao",
        "Zhiping Zhang"
      ],
      "abstract": "The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.",
      "tldr_zh": "这篇论文讨论了大型语言模型（LLMs）在用户系统中广泛应用所带来的隐私问题，目前的研究主要聚焦模型层面，如数据记忆和个人特征推断。作者主张转向以人为中心的研究，探讨LLMs的设计范式如何影响用户的披露行为、心理模型以及对隐私控制的偏好，并提出设计工具来帮助用户重新掌控个人数据。论文的目标是制定一个研究议程，通过Special Interest Group（SIG）召集HCI、NLP和相关领域的研究者，分享观点并建立对挑战和机会的集体理解，以推动更可用和隐私友好的LLMs系统发展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.HC",
      "comment": "4 pages, CHI EA'24",
      "pdf_url": "http://arxiv.org/pdf/2402.01994v1",
      "published_date": "2024-02-03 02:32:45 UTC",
      "updated_date": "2024-02-03 02:32:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:56:46.050877"
    },
    {
      "arxiv_id": "2403.05541v1",
      "title": "AI in ESG for Financial Institutions: An Industrial Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Xu"
      ],
      "abstract": "The burgeoning integration of Artificial Intelligence (AI) into\nEnvironmental, Social, and Governance (ESG) initiatives within the financial\nsector represents a paradigm shift towards more sus-tainable and equitable\nfinancial practices. This paper surveys the industrial landscape to delineate\nthe necessity and impact of AI in bolstering ESG frameworks. With the advent of\nstringent regulatory requirements and heightened stakeholder awareness,\nfinancial institutions (FIs) are increasingly compelled to adopt ESG criteria.\nAI emerges as a pivotal tool in navigating the complex in-terplay of financial\nactivities and sustainability goals. Our survey categorizes AI applications\nacross three main pillars of ESG, illustrating how AI enhances analytical\ncapabilities, risk assessment, customer engagement, reporting accuracy and\nmore. Further, we delve into the critical con-siderations surrounding the use\nof data and the development of models, underscoring the importance of data\nquality, privacy, and model robustness. The paper also addresses the imperative\nof responsible and sustainable AI, emphasizing the ethical dimensions of AI\ndeployment in ESG-related banking processes. Conclusively, our findings suggest\nthat while AI offers transformative potential for ESG in banking, it also poses\nsignificant challenges that necessitate careful consideration. The final part\nof the paper synthesizes the survey's insights, proposing a forward-looking\nstance on the adoption of AI in ESG practices. We conclude with recommendations\nwith a reference architecture for future research and development, advocating\nfor a balanced approach that leverages AI's strengths while mitigating its\nrisks within the ESG domain.",
      "tldr_zh": "这篇论文通过工业调查探讨了人工智能（AI）在金融机构的ESG（Environmental, Social, and Governance）领域的应用，强调AI如何推动更可持续和公平的金融实践。调查将AI应用分类为ESG三大支柱，包括增强风险评估、客户互动和报告准确性，同时突出数据质量、隐私以及模型稳健性的关键考虑。论文强调AI的负责任使用以应对伦理挑战，并提出参考架构作为未来研究的指导框架，以平衡AI的优势和风险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "q-fin.CP"
      ],
      "primary_category": "cs.CY",
      "comment": "31 pages, 14 tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.05541v1",
      "published_date": "2024-02-03 02:14:47 UTC",
      "updated_date": "2024-02-03 02:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:56:56.789929"
    },
    {
      "arxiv_id": "2402.01987v2",
      "title": "Online Transfer Learning for RSV Case Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yiming Sun",
        "Yuhe Gao",
        "Runxue Bao",
        "Gregory F. Cooper",
        "Jessi Espino",
        "Harry Hochheiser",
        "Marian G. Michaels",
        "John M. Aronis",
        "Chenxi Song",
        "Ye Ye"
      ],
      "abstract": "Transfer learning has become a pivotal technique in machine learning and has\nproven to be effective in various real-world applications. However, utilizing\nthis technique for classification tasks with sequential data often faces\nchallenges, primarily attributed to the scarcity of class labels. To address\nthis challenge, we introduce Multi-Source Adaptive Weighting (MSAW), an online\nmulti-source transfer learning method. MSAW integrates a dynamic weighting\nmechanism into an ensemble framework, enabling automatic adjustment of weights\nbased on the relevance and contribution of each source (representing historical\nknowledge) and target model (learning from newly acquired data). We demonstrate\nthe effectiveness of MSAW by applying it to detect Respiratory Syncytial Virus\ncases within Emergency Department visits, utilizing multiple years of\nelectronic health records from the University of Pittsburgh Medical Center. Our\nmethod demonstrates performance improvements over many baselines, including\nrefining pre-trained models with online learning as well as three static\nweighting approaches, showing MSAW's capacity to integrate historical knowledge\nwith progressively accumulated new data. This study indicates the potential of\nonline transfer learning in healthcare, particularly for developing machine\nlearning models that dynamically adapt to evolving situations where new data is\nincrementally accumulated.",
      "tldr_zh": "本研究针对序列数据分类中标签稀缺的挑战，提出了一种在线多源转移学习方法Multi-Source Adaptive Weighting (MSAW)。MSAW 通过整合动态权重机制到集成框架中，自动根据每个源模型（历史知识）和目标模型（新数据学习）的相关性与贡献调整权重，从而有效融合历史数据与实时积累的数据。研究将其应用于检测急诊室呼吸道合胞病毒(RSV)病例，使用匹兹堡大学医疗中心的电子健康记录数据，结果显示MSAW在性能上超过了多个基线模型，包括在线细调的预训练模型和静态权重方法。总之，该方法展示了在线转移学习在医疗领域的潜力，特别是适应动态演变的场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.01987v2",
      "published_date": "2024-02-03 02:13:08 UTC",
      "updated_date": "2024-04-07 22:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:57:09.296914"
    },
    {
      "arxiv_id": "2402.01981v1",
      "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
      "title_zh": "翻译失败",
      "authors": [
        "Isabel O. Gallegos",
        "Ryan A. Rossi",
        "Joe Barrow",
        "Md Mehrab Tanjim",
        "Tong Yu",
        "Hanieh Deilamsalehy",
        "Ruiyi Zhang",
        "Sungchul Kim",
        "Franck Dernoncourt"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable advances in language\ngeneration and understanding but are also prone to exhibiting harmful social\nbiases. While recognition of these behaviors has generated an abundance of bias\nmitigation techniques, most require modifications to the training data, model\nparameters, or decoding strategy, which may be infeasible without access to a\ntrainable model. In this work, we leverage the zero-shot capabilities of LLMs\nto reduce stereotyping in a technique we introduce as zero-shot self-debiasing.\nWith two approaches, self-debiasing via explanation and self-debiasing via\nreprompting, we show that self-debiasing can significantly reduce the degree of\nstereotyping across nine different social groups while relying only on the LLM\nitself and a simple prompt, with explanations correctly identifying invalid\nassumptions and reprompting delivering the greatest reductions in bias. We hope\nthis work opens inquiry into other zero-shot techniques for bias mitigation.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 存在的有害社会偏见，提出了一种零-shot self-debiasing 方法，用于识别和减少刻板印象，而无需修改训练数据或模型参数，仅依赖 LLM 和简单提示。  \n该方法包括两种方式：self-debiasing via explanation（通过解释识别无效假设）和self-debiasing via reprompting（通过重新提示调整输出），二者均利用 LLMs 的零-shot 能力。  \n实验结果显示，在九个不同社会群体上，该技术显著降低了刻板印象程度，其中 reprompting 提供了最大的偏见减少效果。  \n这项工作有望激发对其他零-shot 偏见缓解技术的探索。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01981v1",
      "published_date": "2024-02-03 01:40:11 UTC",
      "updated_date": "2024-02-03 01:40:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:57:22.266678"
    },
    {
      "arxiv_id": "2402.07922v1",
      "title": "Towards the Human Digital Twin: Definition and Design -- A survey",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Wolfgang Lauer-Schmaltz",
        "Philip Cash",
        "John Paulin Hansen",
        "Anja Maier"
      ],
      "abstract": "Human Digital Twins (HDTs) are a fast-emerging technology with significant\npotential in fields ranging from healthcare to sports. HDTs extend the\ntraditional understanding of Digital Twins by representing humans as the\nunderlying physical entity. This has introduced several significant challenges,\nincluding ambiguity in the definition of HDTs and a lack of guidance for their\ndesign. This survey brings together the recent advances in the field of HDTs to\nguide future developers by proposing a first cross-domain definition of HDTs\nbased on their characteristics, as well as eleven key design considerations\nthat emerge from the associated challenges.",
      "tldr_zh": "这篇调查论文探讨了Human Digital Twins (HDTs)，一种将人类作为物理实体的快速新兴技术，应用于医疗和体育等领域。它指出了HDTs相对于传统Digital Twins的挑战，包括定义模糊和设计指导不足，并基于这些特性提出一个跨领域的首次定义。论文还总结了11个关键设计考虑，以指导未来HDTs的开发和应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper is an extension of the following paper: Lauer-Schmaltz MW,\n  Cash P, Hansen JP, Maier A. Designing Human Digital Twins for\n  Behaviour-Changing Therapy and Rehabilitation: A Systematic Review.\n  Proceedings of the Design Society. 2022;2:1303-1312. doi:10.1017/pds.2022.132",
      "pdf_url": "http://arxiv.org/pdf/2402.07922v1",
      "published_date": "2024-02-03 01:33:05 UTC",
      "updated_date": "2024-02-03 01:33:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:57:32.275853"
    },
    {
      "arxiv_id": "2402.01968v2",
      "title": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",
      "title_zh": "上下文感知多智能体系统综述：技术、挑战和未来方向",
      "authors": [
        "Hung Du",
        "Srikanth Thudumu",
        "Rajesh Vasa",
        "Kon Mouzakis"
      ],
      "abstract": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field.",
      "tldr_zh": "这篇调查论文探讨了上下文感知多代理系统的技术、挑战和未来方向，强调 Large Language Models (LLMs) 在实现自主代理的人类-like 智能方面的潜力，同时指出代理在动态环境中学习、推理和处理不确定性的关键在于上下文感知。论文首先概述了上下文感知系统和多代理系统的整合属性，并提出一个一般过程，包括从碰撞避免自动驾驶到灾害救援等领域的多样化方法。最终，它分析了现有挑战，如系统整合的不足，并为该领域提供了未来研究方向。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "11 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2402.01968v2",
      "published_date": "2024-02-03 00:27:22 UTC",
      "updated_date": "2025-01-29 05:41:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T02:57:44.994846"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 54,
  "processed_papers_count": 54,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T02:58:07.981047"
}
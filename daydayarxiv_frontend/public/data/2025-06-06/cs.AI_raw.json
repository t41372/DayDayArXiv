[
  {
    "arxiv_id": "2506.06580v2",
    "title": "AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture",
    "authors": [
      "Xiaoran Liu",
      "Istvan David"
    ],
    "abstract": "Insufficient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efficiently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-fidelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our findings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.SE",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06580v2",
    "published_date": "2025-06-06 23:13:38 UTC",
    "updated_date": "2025-08-29 03:17:51 UTC"
  },
  {
    "arxiv_id": "2506.06579v1",
    "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques",
    "authors": [
      "Adarsh Prasad Behera",
      "Jaya Prakash Champati",
      "Roberto Morabito",
      "Sasu Tarkoma",
      "James Gross"
    ],
    "abstract": "Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06579v1",
    "published_date": "2025-06-06 23:13:08 UTC",
    "updated_date": "2025-06-06 23:13:08 UTC"
  },
  {
    "arxiv_id": "2506.06576v2",
    "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce",
    "authors": [
      "Yijia Shao",
      "Humishka Zope",
      "Yucheng Jiang",
      "Jiaxin Pei",
      "David Nguyen",
      "Erik Brynjolfsson",
      "Diyi Yang"
    ],
    "abstract": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation \"Green Light\" Zone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2506.06576v2",
    "published_date": "2025-06-06 23:05:52 UTC",
    "updated_date": "2025-06-11 21:25:21 UTC"
  },
  {
    "arxiv_id": "2506.06574v2",
    "title": "The Optimization Paradox in Clinical AI Multi-Agent Systems",
    "authors": [
      "Suhana Bedi",
      "Iddah Mlauzi",
      "Daniel Shin",
      "Sanmi Koyejo",
      "Nigam H. Shah"
    ],
    "abstract": "Multi-agent artificial intelligence systems are increasingly deployed in clinical settings, yet the relationship between component-level optimization and system-wide performance remains poorly understood. We evaluated this relationship using 2,400 real patient cases from the MIMIC-CDM dataset across four abdominal pathologies (appendicitis, pancreatitis, cholecystitis, diverticulitis), decomposing clinical diagnosis into information gathering, interpretation, and differential diagnosis. We evaluated single agent systems (one model performing all tasks) against multi-agent systems (specialized models for each task) using comprehensive metrics spanning diagnostic outcomes, process adherence, and cost efficiency. Our results reveal a paradox: while multi-agent systems generally outperformed single agents, the component-optimized or Best of Breed system with superior components and excellent process metrics (85.5% information accuracy) significantly underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent system). This finding underscores that successful integration of AI in healthcare requires not just component level optimization but also attention to information flow and compatibility between agents. Our findings highlight the need for end to end system validation rather than relying on component metrics alone.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06574v2",
    "published_date": "2025-06-06 23:01:51 UTC",
    "updated_date": "2025-06-12 02:19:49 UTC"
  },
  {
    "arxiv_id": "2506.06571v2",
    "title": "Graph Persistence goes Spectral",
    "authors": [
      "Mattie Ji",
      "Amauri H. Souza",
      "Vikas Garg"
    ],
    "abstract": "Including intricate topological information (e.g., cycles) provably enhances the expressivity of message-passing graph neural networks (GNNs) beyond the Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods are increasingly employed for graph representation learning. In this context, recent works have proposed decorating classical PH diagrams with vertex and edge features for improved expressivity. However, these methods still fail to capture basic graph structural information. In this paper, we propose SpectRe -- a new topological descriptor for graphs that integrates spectral information into PH diagrams. Notably, SpectRe is strictly more expressive than existing descriptors on graphs. We also introduce notions of global and local stability to analyze existing descriptors and establish that SpectRe is locally stable. Finally, experiments on synthetic and real-world datasets demonstrate the effectiveness of SpectRe and its potential to enhance the capabilities of graph models in relevant learning tasks. Code is available at https://github.com/Aalto-QuML/SpectRe/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 4 figures, 7 tables. Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06571v2",
    "published_date": "2025-06-06 22:51:08 UTC",
    "updated_date": "2025-12-01 10:21:46 UTC"
  },
  {
    "arxiv_id": "2506.06569v1",
    "title": "Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models",
    "authors": [
      "Yannis Spyridis",
      "Vasileios Argyriou"
    ],
    "abstract": "Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging. This paper investigates the use of standard RGB imagery, a cost-effective sensing modality, for key pre-processing tasks in an automated system. We present computer vision components designed for a conveyor belt setup to perform (a) classification of four common textile types and (b) segmentation of non-textile features such as buttons and zippers. For classification, several pre-trained architectures were evaluated using transfer learning and cross-validation, with EfficientNetB0 achieving the best performance on a held-out test set with 81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed, demonstrating excellent performance with a mIoU of 0.90 for the generated masks against ground truth. This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques, including transfer learning for classification and foundation models for zero-shot segmentation, to enable essential analysis steps for automated textile recycling pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06569v1",
    "published_date": "2025-06-06 22:49:53 UTC",
    "updated_date": "2025-06-06 22:49:53 UTC"
  },
  {
    "arxiv_id": "2506.06566v2",
    "title": "AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition",
    "authors": [
      "Chen Bao",
      "Chuanbing Huo",
      "Qinyu Chen",
      "Chang Gao"
    ],
    "abstract": "This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to 2025 IEEE Biomedical Circuits and Systems Conference (BioCAS)",
    "pdf_url": "https://arxiv.org/pdf/2506.06566v2",
    "published_date": "2025-06-06 22:38:53 UTC",
    "updated_date": "2025-09-18 21:36:33 UTC"
  },
  {
    "arxiv_id": "2506.06561v4",
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles",
    "authors": [
      "Ho Yin 'Sam' Ng",
      "Ting-Yao Hsu",
      "Aashish Anantha Ramakrishnan",
      "Branislav Kveton",
      "Nedim Lipka",
      "Franck Dernoncourt",
      "Dongwon Lee",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Ting-Hao 'Kenneth' Huang"
    ],
    "abstract": "Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Findings. The LaMP-CAP dataset is publicly available at: https://github.com/Crowd-AI-Lab/lamp-cap",
    "pdf_url": "https://arxiv.org/pdf/2506.06561v4",
    "published_date": "2025-06-06 22:16:16 UTC",
    "updated_date": "2025-09-22 21:32:08 UTC"
  },
  {
    "arxiv_id": "2506.06541v2",
    "title": "KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes",
    "authors": [
      "Eugenie Lai",
      "Gerardo Vitagliano",
      "Ziyu Zhang",
      "Om Chabra",
      "Sivaprasad Sudhir",
      "Anna Zeng",
      "Anton A. Zabreyko",
      "Chenning Li",
      "Ferdi Kossmann",
      "Jialin Ding",
      "Jun Chen",
      "Markos Markakis",
      "Matthew Russo",
      "Weiyang Wang",
      "Ziniu Wu",
      "Michael J. Cafarella",
      "Lei Cao",
      "Samuel Madden",
      "Tim Kraska"
    ],
    "abstract": "Constructing real-world data-to-insight pipelines often involves data extraction from data lakes, data integration across heterogeneous data sources, and diverse operations from data cleaning to analysis. The design and implementation of data science pipelines require domain knowledge, technical expertise, and even project-specific insights. AI systems have shown remarkable reasoning, coding, and understanding capabilities. However, it remains unclear to what extent these capabilities translate into successful design and execution of such complex pipelines. We introduce KRAMABENCH: a benchmark composed of 104 manually-curated real-world data science pipelines spanning 1700 data files from 24 data sources in 6 different domains. We show that these pipelines test the end-to-end capabilities of AI systems on data processing, requiring data discovery, wrangling and cleaning, efficient processing, statistical reasoning, and orchestrating data processing steps given a high-level task. Our evaluation tests 5 general models and 3 code generation models using our reference framework, DS-GURU, which instructs the AI model to decompose a question into a sequence of subtasks, reason through each step, and synthesize Python code that implements the proposed design. Our results on KRAMABENCH show that, although the models are sufficiently capable of solving well-specified data science code generation tasks, when extensive data processing and domain knowledge are required to construct real-world data science pipelines, existing out-of-box models fall short. Progress on KramaBench represents crucial steps towards developing autonomous data science agents for real-world applications. Our code, reference framework, and data are available at https://github.com/mitdbg/KramaBench.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06541v2",
    "published_date": "2025-06-06 21:18:45 UTC",
    "updated_date": "2025-10-07 18:15:23 UTC"
  },
  {
    "arxiv_id": "2506.06540v1",
    "title": "Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches",
    "authors": [
      "Patrick Y. Wu"
    ],
    "abstract": "After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06540v1",
    "published_date": "2025-06-06 21:14:57 UTC",
    "updated_date": "2025-06-06 21:14:57 UTC"
  },
  {
    "arxiv_id": "2506.06539v1",
    "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models",
    "authors": [
      "Yijie Hao",
      "Haofei Yu",
      "Jiaxuan You"
    ],
    "abstract": "When exposed to complex queries containing multiple conditions, today's large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2506.06539v1",
    "published_date": "2025-06-06 21:10:55 UTC",
    "updated_date": "2025-06-06 21:10:55 UTC"
  },
  {
    "arxiv_id": "2506.06532v1",
    "title": "Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks",
    "authors": [
      "Zijiang Yan",
      "Hao Zhou",
      "Jianhua Pei",
      "Hina Tabassum"
    ],
    "abstract": "Unmanned aerial vehicles (UAVs) have been widely adopted in various real-world applications. However, the control and optimization of multi-UAV systems remain a significant challenge, particularly in dynamic and constrained environments. This work explores the joint motion and communication control of multiple UAVs operating within integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). Specifically, we consider an aerial highway scenario in which UAVs must accelerate, decelerate, and change lanes to avoid collisions and maintain overall traffic flow. Different from existing studies, we propose a novel hierarchical and collaborative method based on large language models (LLMs). In our approach, an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions. This knowledge-driven paradigm holds great potential for the development of next-generation 3D aerial highway systems. Experimental results demonstrate that our proposed collaborative LLM-based method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in ICML 2025 Workshop on Machine Learning for Wireless Communication and Networks (ML4Wireless)",
    "pdf_url": "https://arxiv.org/pdf/2506.06532v1",
    "published_date": "2025-06-06 20:59:52 UTC",
    "updated_date": "2025-06-06 20:59:52 UTC"
  },
  {
    "arxiv_id": "2506.06524v1",
    "title": "ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search",
    "authors": [
      "Sam Earle",
      "Ahmed Khalifa",
      "Muhammad Umair Nasir",
      "Zehua Jiang",
      "Graham Todd",
      "Andrzej Banburski-Fahey",
      "Julian Togelius"
    ],
    "abstract": "There is much interest in using large pre-trained models in Automatic Game Design (AGD), whether via the generation of code, assets, or more abstract conceptualization of design ideas. But so far this interest largely stems from the ad hoc use of such generative models under persistent human supervision. Much work remains to show how these tools can be integrated into longer-time-horizon AGD pipelines, in which systems interface with game engines to test generated content autonomously. To this end, we introduce ScriptDoctor, a Large Language Model (LLM)-driven system for automatically generating and testing games in PuzzleScript, an expressive but highly constrained description language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates and tests game design ideas in an iterative loop, where human-authored examples are used to ground the system's output, compilation errors from the PuzzleScript engine are used to elicit functional code, and search-based agents play-test generated games. ScriptDoctor serves as a concrete example of the potential of automated, open-ended LLM-based workflows in generating novel game content.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 3 figures, 3 tables, submitted to IEEE Conference on Games as a Short Paper",
    "pdf_url": "https://arxiv.org/pdf/2506.06524v1",
    "published_date": "2025-06-06 20:40:19 UTC",
    "updated_date": "2025-06-06 20:40:19 UTC"
  },
  {
    "arxiv_id": "2506.06523v1",
    "title": "Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility",
    "authors": [
      "Sumanth Pillella"
    ],
    "abstract": "In an era of escalating supply chain demands, SAP Logistics Execution (LE) is pivotal for managing warehouse operations, transportation, and delivery. This research introduces a pioneering framework leveraging reinforcement learning (RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing operational agility and efficiency. By modeling warehouse processes as dynamic environments, the framework optimizes task allocation, inventory movement, and order picking in real-time. A synthetic dataset of 300,000 LE transactions simulates real-world warehouse scenarios, including multilingual data and operational disruptions. The analysis achieves 95% task optimization accuracy, reducing processing times by 60% compared to traditional methods. Visualizations, including efficiency heatmaps and performance graphs, guide agile warehouse strategies. This approach tackles data privacy, scalability, and SAP integration, offering a transformative solution for modern supply chains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.06523v1",
    "published_date": "2025-06-06 20:34:27 UTC",
    "updated_date": "2025-06-06 20:34:27 UTC"
  },
  {
    "arxiv_id": "2506.06522v3",
    "title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance",
    "authors": [
      "Aladin Djuhera",
      "Swanand Ravindra Kadhe",
      "Syed Zawad",
      "Farhan Ahmed",
      "Heiko Ludwig",
      "Holger Boche"
    ],
    "abstract": "Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06522v3",
    "published_date": "2025-06-06 20:34:06 UTC",
    "updated_date": "2025-12-15 10:08:14 UTC"
  },
  {
    "arxiv_id": "2506.12076v1",
    "title": "A Synthetic Pseudo-Autoencoder Invites Examination of Tacit Assumptions in Neural Network Design",
    "authors": [
      "Assaf Marron"
    ],
    "abstract": "We present a handcrafted neural network that, without training, solves the seemingly difficult problem of encoding an arbitrary set of integers into a single numerical variable, and then recovering the original elements. While using only standard neural network operations -- weighted sums with biases and identity activation -- we make design choices that challenge common notions in this area around representation, continuity of domains, computation, learnability and more. For example, our construction is designed, not learned; it represents multiple values using a single one by simply concatenating digits without compression, and it relies on hardware-level truncation of rightmost digits as a bit-manipulation mechanism. This neural net is not intended for practical application. Instead, we see its resemblance to -- and deviation from -- standard trained autoencoders as an invitation to examine assumptions that may unnecessarily constrain the development of systems and models based on autoencoding and machine learning. Motivated in part by our research on a theory of biological evolution centered around natural autoencoding of species characteristics, we conclude by refining the discussion with a biological perspective.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12076v1",
    "published_date": "2025-06-06 20:32:14 UTC",
    "updated_date": "2025-06-06 20:32:14 UTC"
  },
  {
    "arxiv_id": "2506.11104v1",
    "title": "DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration",
    "authors": [
      "Hanzhi Zhang",
      "Heng Fan",
      "Kewei Sha",
      "Yan Huang",
      "Yunhe Feng"
    ],
    "abstract": "Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: https://github.com/HanzhiZhang-Ulrica/DAM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11104v1",
    "published_date": "2025-06-06 20:24:36 UTC",
    "updated_date": "2025-06-06 20:24:36 UTC"
  },
  {
    "arxiv_id": "2506.06509v2",
    "title": "Private GPTs for LLM-driven testing in software development and machine learning",
    "authors": [
      "Jakub Jagielski",
      "Consuelo Rojas",
      "Markus Abel"
    ],
    "abstract": "In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple \"Hello World\" program and a digit classification model, showing that structured prompts lead to higher-quality test outputs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06509v2",
    "published_date": "2025-06-06 20:05:41 UTC",
    "updated_date": "2025-07-31 18:44:42 UTC"
  },
  {
    "arxiv_id": "2506.06499v2",
    "title": "SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms",
    "authors": [
      "Alex Havrilla",
      "Edward Hughes",
      "Mikayel Samvelyan",
      "Jacob Abernethy"
    ],
    "abstract": "Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06499v2",
    "published_date": "2025-06-06 19:49:42 UTC",
    "updated_date": "2025-06-17 12:55:30 UTC"
  },
  {
    "arxiv_id": "2506.15725v2",
    "title": "Graph Diffusion that can Insert and Delete",
    "authors": [
      "Matteo Ninniri",
      "Marco Podda",
      "Davide Bacciu"
    ],
    "abstract": "Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025). Equation 2 is now correct",
    "pdf_url": "https://arxiv.org/pdf/2506.15725v2",
    "published_date": "2025-06-06 19:45:45 UTC",
    "updated_date": "2025-10-30 18:16:57 UTC"
  },
  {
    "arxiv_id": "2506.11103v1",
    "title": "You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model",
    "authors": [
      "Wenchong He",
      "Liqian Peng",
      "Zhe Jiang",
      "Alex Go"
    ],
    "abstract": "Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.\n  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11103v1",
    "published_date": "2025-06-06 19:36:04 UTC",
    "updated_date": "2025-06-06 19:36:04 UTC"
  },
  {
    "arxiv_id": "2506.06485v3",
    "title": "Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict",
    "authors": [
      "Kaiser Sun",
      "Fan Bai",
      "Mark Dredze"
    ],
    "abstract": "Large language models (LLMs) draw on both contextual information and parametric memory, yet these sources can conflict. Prior studies have largely examined this issue in contextual question answering, implicitly assuming that tasks should rely on the provided context, leaving unclear how LLMs behave when tasks require different types and degrees of knowledge utilization. We address this gap with a model-agnostic diagnostic framework that holds underlying knowledge constant while introducing controlled conflicts across tasks with varying knowledge demands. Experiments on representative open-source LLMs show that performance degradation under conflict is driven by both task-specific knowledge reliance and conflict plausibility; that strategies such as rationales or context reiteration increase context reliance, helping context-only tasks but harming those requiring parametric knowledge; and that these effects bias model-based evaluation, calling into question the reliability of LLMs as judges. Overall, our findings reveal that context-memory conflict is inherently task-dependent and motivate task-aware approaches to balancing context and memory in LLM deployment and evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Major revision",
    "pdf_url": "https://arxiv.org/pdf/2506.06485v3",
    "published_date": "2025-06-06 19:20:23 UTC",
    "updated_date": "2026-01-06 19:36:04 UTC"
  },
  {
    "arxiv_id": "2506.06484v1",
    "title": "The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage",
    "authors": [
      "Manuel Sage",
      "Khalil Al Handawi",
      "Yaoyao Fiona Zhao"
    ],
    "abstract": "Power-to-Gas (P2G) technologies gain recognition for enabling the integration of intermittent renewables, such as wind and solar, into electricity grids. However, determining the most cost-effective operation of these systems is complex due to the volatile nature of renewable energy, electricity prices, and loads. Additionally, P2G systems are less efficient in converting and storing energy compared to battery energy storage systems (BESs), and the benefits of converting electricity into gas are not immediately apparent. Deep Reinforcement Learning (DRL) has shown promise in managing the operation of energy systems amidst these uncertainties. Yet, DRL techniques face difficulties with the delayed reward characteristic of P2G system operation. Previous research has mostly focused on short-term studies that look at the energy conversion process, neglecting the long-term storage capabilities of P2G.\n  This study presents a new method by thoroughly examining how DRL can be applied to the economic operation of P2G systems, in combination with BESs and gas turbines, over extended periods. Through three progressively more complex case studies, we assess the performance of DRL algorithms, specifically Deep Q-Networks and Proximal Policy Optimization, and introduce modifications to enhance their effectiveness. These modifications include integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations, all aimed at addressing the issue of delayed rewards. Our findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the adjustments we propose significantly improve its capability to devise cost-effective operation strategies, thereby unlocking the potential for long-term energy storage in P2G technologies.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted for publication at the 19th ASME International Conference on Energy Sustainability",
    "pdf_url": "https://arxiv.org/pdf/2506.06484v1",
    "published_date": "2025-06-06 19:20:08 UTC",
    "updated_date": "2025-06-06 19:20:08 UTC"
  },
  {
    "arxiv_id": "2506.06483v1",
    "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis",
    "authors": [
      "Yao Ni",
      "Song Wen",
      "Piotr Koniusz",
      "Anoop Cherian"
    ],
    "abstract": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by adapting the model to generate images containing specific subjects. However, existing fine-tuning methods suffer from two key issues: underfitting, where the model fails to reliably capture subject identity, and overfitting, where it memorizes the subject image and reduces background diversity. To address these challenges, we propose two auxiliary consistency losses for diffusion fine-tuning. First, a prior consistency regularization loss ensures that the predicted diffusion noise for prior (non-subject) images remains consistent with that of the pretrained model, improving fidelity. Second, a subject consistency regularization loss enhances the fine-tuned model's robustness to multiplicative noise modulated latent code, helping to preserve subject identity while improving diversity. Our experimental results demonstrate that incorporating these losses into fine-tuning not only preserves subject identity but also enhances image diversity, outperforming DreamBooth in terms of CLIP scores, background variation, and overall visual quality.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06483v1",
    "published_date": "2025-06-06 19:17:37 UTC",
    "updated_date": "2025-06-06 19:17:37 UTC"
  },
  {
    "arxiv_id": "2506.06474v1",
    "title": "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception",
    "authors": [
      "Everett Richards",
      "Bipul Thapa",
      "Lena Mashayekhy"
    ],
    "abstract": "Accurate and reliable object detection is critical for ensuring the safety and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board perception systems have limited accuracy due to occlusions and blind spots, while cloud-based solutions introduce significant latency, making them unsuitable for real-time processing demands required for autonomous driving in dynamic environments. To address these challenges, we introduce an innovative framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that leverages edge computing and multi-CAV collaboration for real-time, multi-perspective object detection. Our ECOD framework integrates two key algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data from multiple CAVs on an edge server to enhance perception in scenarios where individual CAVs have limited visibility. VOTE utilizes a consensus-based voting mechanism to improve the accuracy of object classification by integrating data from multiple CAVs. Both algorithms are designed at the edge to operate in real-time, ensuring low-latency and reliable decision-making for CAVs. We develop a hardware-based controlled testbed consisting of camera-equipped robotic CAVs and an edge server to evaluate the efficacy of our framework. Our experimental results demonstrate the significant benefits of ECOD in terms of improved object classification accuracy, outperforming traditional single-perspective onboard approaches by up to 75%, while ensuring low-latency, edge-driven real-time processing. This research highlights the potential of edge computing to enhance collaborative perception for latency-sensitive autonomous systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.MA",
      "cs.NI"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper has been accepted to IEEE EDGE 2025. The final version will be published in IEEE Xplore later this year",
    "pdf_url": "https://arxiv.org/pdf/2506.06474v1",
    "published_date": "2025-06-06 18:58:04 UTC",
    "updated_date": "2025-06-06 18:58:04 UTC"
  },
  {
    "arxiv_id": "2506.06472v1",
    "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage",
    "authors": [
      "Ziqi Yuan",
      "Haoyang Zhang",
      "Yirui Eric Zhou",
      "Apoorve Mohan",
      "I-Hsin Chung",
      "Seetharami Seelam",
      "Jian Huang"
    ],
    "abstract": "We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06472v1",
    "published_date": "2025-06-06 18:57:20 UTC",
    "updated_date": "2025-06-06 18:57:20 UTC"
  },
  {
    "arxiv_id": "2506.06470v1",
    "title": "SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation",
    "authors": [
      "Yanwei Ren",
      "Haotian Zhang",
      "Fuxiang Wu",
      "Jiayan Qiu",
      "Jiaxing Huang",
      "Baosheng Yu",
      "Liu Liu"
    ],
    "abstract": "Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06470v1",
    "published_date": "2025-06-06 18:55:16 UTC",
    "updated_date": "2025-06-06 18:55:16 UTC"
  },
  {
    "arxiv_id": "2506.06455v1",
    "title": "WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets",
    "authors": [
      "Antonio Jess Banegas-Luna",
      "Horacio Prez-Snchez",
      "Carlos Martnez-Corts"
    ],
    "abstract": "While predictive accuracy is often prioritized in machine learning (ML) models, interpretability remains essential in scientific and high-stakes domains. However, diverse interpretability algorithms frequently yield conflicting explanations, highlighting the need for consensus to harmonize results. In this study, six ML models were trained on six synthetic datasets with known ground truths, utilizing various model-agnostic interpretability techniques. Consensus explanations were generated using established methods and a novel approach: WISCA (Weighted Scaled Consensus Attributions), which integrates class probability and normalized attributions. WISCA consistently aligned with the most reliable individual method, underscoring the value of robust consensus strategies in improving explanation reliability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 11 figures, 2 tables, 13 equations",
    "pdf_url": "https://arxiv.org/pdf/2506.06455v1",
    "published_date": "2025-06-06 18:26:25 UTC",
    "updated_date": "2025-06-06 18:26:25 UTC"
  },
  {
    "arxiv_id": "2506.06446v1",
    "title": "Canonical Autoregressive Generation",
    "authors": [
      "Ivi Chatzi",
      "Nina Corvelo Benz",
      "Stratis Tsirtsis",
      "Manuel Gomez-Rodriguez"
    ],
    "abstract": "State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06446v1",
    "published_date": "2025-06-06 18:09:10 UTC",
    "updated_date": "2025-06-06 18:09:10 UTC"
  },
  {
    "arxiv_id": "2506.06444v2",
    "title": "Saffron-1: Safety Inference Scaling",
    "authors": [
      "Ruizhong Qiu",
      "Gaotang Li",
      "Tianxin Wei",
      "Jingrui He",
      "Hanghang Tong"
    ],
    "abstract": "Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance\"",
    "pdf_url": "https://arxiv.org/pdf/2506.06444v2",
    "published_date": "2025-06-06 18:05:45 UTC",
    "updated_date": "2025-07-09 07:47:59 UTC"
  },
  {
    "arxiv_id": "2506.06443v3",
    "title": "Superior Molecular Representations from Intermediate Encoder Layers",
    "authors": [
      "Luis Pinto"
    ],
    "abstract": "Pretrained molecular encoders have become indispensable in computational chemistry for tasks such as property prediction and molecular generation. However, the standard practice of relying solely on final-layer embeddings for downstream tasks may discard valuable information. In this work, we first analyze the information flow in five diverse molecular encoders and find that intermediate layers retain more general-purpose features, whereas the final-layer specializes and compresses information. We then perform an empirical layer-wise evaluation across 22 property prediction tasks. We find that using frozen embeddings from optimal intermediate layers improves downstream performance by an average of 5.4%, up to 28.6%, compared to the final-layer. Furthermore, finetuning encoders truncated at intermediate depths achieves even greater average improvements of 8.5%, with increases as high as 40.8%, obtaining new state-of-the-art results on several benchmarks. These findings highlight the importance of exploring the full representational depth of molecular encoders to achieve substantial performance improvements and computational efficiency. The code will be made publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06443v3",
    "published_date": "2025-06-06 18:03:51 UTC",
    "updated_date": "2025-10-15 01:55:53 UTC"
  },
  {
    "arxiv_id": "2506.06280v2",
    "title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias",
    "authors": [
      "Yuanzhe Hu",
      "Kinshuk Goel",
      "Vlad Killiakov",
      "Yaoqing Yang"
    ],
    "abstract": "Diagnosing deep neural networks (DNNs) by analyzing the eigenspectrum of their weights has been an active area of research in recent years. One of the main approaches involves measuring the heavytailness of the empirical spectral densities (ESDs) of weight matrices. This analysis has been shown to provide insights to help diagnose whether a model is well-trained or undertrained, and has been used to guide training methods involving layer-wise hyperparameter assignment. In this paper, we address an often-overlooked challenge in estimating the heavytailness of these ESDs: the impact of the aspect ratio of weight matrices. We demonstrate that matrices of varying sizes (and aspect ratios) introduce a non-negligible bias in estimating the heavytailness of ESDs, leading to inaccurate model diagnosis and layer-wise hyperparameter assignment. To overcome this challenge, we propose FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the weight matrices by subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the original ESD, we measure the average ESD of these subsampled submatrices. We show that this method effectively mitigates the aspect ratio bias. We validate our approach across various optimization techniques and application domains that involve eigenspectrum analysis of weights, including image classification in computer vision (CV) models, scientific machine learning (SciML) model training, and large language model (LLM) pruning. Our results show that despite its simplicity, FARMS uniformly improves the accuracy of eigenspectrum analysis while enabling more effective layer-wise hyperparameter assignment. In one of the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model by 17.3% when compared with state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 14 figures, ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06280v2",
    "published_date": "2025-06-06 17:59:28 UTC",
    "updated_date": "2025-08-18 03:36:09 UTC"
  },
  {
    "arxiv_id": "2506.06278v3",
    "title": "Distillation Robustifies Unlearning",
    "authors": [
      "Bruce W. Lee",
      "Addie Foote",
      "Alex Infanger",
      "Leni Shor",
      "Harish Kamath",
      "Jacob Goldman-Wetzler",
      "Bryce Woodworth",
      "Alex Cloud",
      "Alexander Matt Turner"
    ],
    "abstract": "Current LLM unlearning methods are not robust. A few steps of finetuning can revert their effects. We begin by showing that this is true even for an idealized form of unlearning: training to imitate a model that was never trained on unwanted information. This shows that training a model can drastically modify its input-output behavior while leaving its underlying capabilities intact. In light of this dynamic, we show our main result. Training a randomly initialized student on the outputs of an unlearned model transfers behaviors while leaving latent capabilities behind. In short, distillation robustifies unlearning. Based on this result, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2506.06278v3",
    "published_date": "2025-06-06 17:58:54 UTC",
    "updated_date": "2025-10-24 02:11:37 UTC"
  },
  {
    "arxiv_id": "2506.06276v1",
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis",
    "authors": [
      "Jiatao Gu",
      "Tianrong Chen",
      "David Berthelot",
      "Huangjie Zheng",
      "Yuyang Wang",
      "Ruixiang Zhang",
      "Laurent Dinh",
      "Miguel Angel Bautista",
      "Josh Susskind",
      "Shuangfei Zhai"
    ],
    "abstract": "We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "TLDR: We show for the first time that normalizing flows can be scaled for high-resolution and text-conditioned image synthesis",
    "pdf_url": "https://arxiv.org/pdf/2506.06276v1",
    "published_date": "2025-06-06 17:58:39 UTC",
    "updated_date": "2025-06-06 17:58:39 UTC"
  },
  {
    "arxiv_id": "2506.11102v1",
    "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey",
    "authors": [
      "Jiachen Zhu",
      "Menghui Zhu",
      "Renting Rui",
      "Rong Shan",
      "Congmin Zheng",
      "Bo Chen",
      "Yunjia Xi",
      "Jianghao Lin",
      "Weiwen Liu",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11102v1",
    "published_date": "2025-06-06 17:52:18 UTC",
    "updated_date": "2025-06-06 17:52:18 UTC"
  },
  {
    "arxiv_id": "2506.06266v3",
    "title": "Cartridges: Lightweight and general-purpose long context representations via self-study",
    "authors": [
      "Sabri Eyuboglu",
      "Ryan Ehrlich",
      "Simran Arora",
      "Neel Guha",
      "Dylan Zinsley",
      "Emily Liu",
      "Will Tennien",
      "Atri Rudra",
      "James Zou",
      "Azalia Mirhoseini",
      "Christopher Re"
    ],
    "abstract": "Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06266v3",
    "published_date": "2025-06-06 17:48:23 UTC",
    "updated_date": "2025-06-13 17:58:55 UTC"
  },
  {
    "arxiv_id": "2506.06261v1",
    "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens",
    "authors": [
      "Jihwan Jeong",
      "Xiaoyu Wang",
      "Jingmin Wang",
      "Scott Sanner",
      "Pascal Poupart"
    ],
    "abstract": "Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06261v1",
    "published_date": "2025-06-06 17:40:12 UTC",
    "updated_date": "2025-06-06 17:40:12 UTC"
  },
  {
    "arxiv_id": "2506.06414v1",
    "title": "Benchmarking Misuse Mitigation Against Covert Adversaries",
    "authors": [
      "Davis Brown",
      "Mahdi Sabbaghi",
      "Luze Sun",
      "Alexander Robey",
      "George J. Pappas",
      "Eric Wong",
      "Hamed Hassani"
    ],
    "abstract": "Existing language model safety evaluations focus on overt attacks and low-stakes tasks. Realistic attackers can subvert current safeguards by requesting help on small, benign-seeming tasks across many independent queries. Because individual queries do not appear harmful, the attack is hard to {detect}. However, when combined, these fragments uplift misuse by helping the attacker complete hard and dangerous tasks. Toward identifying defenses against such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data generation pipeline that automates evaluations of covert attacks and corresponding defenses. Using this pipeline, we curate two new datasets that are consistently refused by frontier models and are too difficult for weaker open-weight models. Our evaluations indicate that decomposition attacks are effective misuse enablers, and highlight stateful defenses as a countermeasure.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06414v1",
    "published_date": "2025-06-06 17:33:33 UTC",
    "updated_date": "2025-06-06 17:33:33 UTC"
  },
  {
    "arxiv_id": "2506.06254v1",
    "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time",
    "authors": [
      "Weizhi Zhang",
      "Xinyang Zhang",
      "Chenwei Zhang",
      "Liangwei Yang",
      "Jingbo Shang",
      "Zhepei Wei",
      "Henry Peng Zou",
      "Zijie Huang",
      "Zhengyang Wang",
      "Yifan Gao",
      "Xiaoman Pan",
      "Lian Xiong",
      "Jingguo Liu",
      "Philip S. Yu",
      "Xian Li"
    ],
    "abstract": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06254v1",
    "published_date": "2025-06-06 17:29:49 UTC",
    "updated_date": "2025-06-06 17:29:49 UTC"
  },
  {
    "arxiv_id": "2506.06251v1",
    "title": "DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation",
    "authors": [
      "Jingyu Xiao",
      "Ming Wang",
      "Man Ho Lam",
      "Yuxuan Wan",
      "Junliang Liu",
      "Yintong Huo",
      "Michael R. Lyu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in automated front-end engineering, e.g., generating UI code from visual designs. However, existing front-end UI code generation benchmarks have the following limitations: (1) While framework-based development becomes predominant in modern front-end programming, current benchmarks fail to incorporate mainstream development frameworks. (2) Existing evaluations focus solely on the UI code generation task, whereas practical UI development involves several iterations, including refining editing, and repairing issues. (3) Current benchmarks employ unidimensional evaluation, lacking investigation into influencing factors like task difficulty, input context variations, and in-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a multi-framework, multi-task evaluation benchmark for assessing MLLMs' capabilities in automated front-end engineering. DesignBench encompasses three widely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS, and evaluates on three essential front-end tasks (generation, edit, and repair) in real-world development workflows. DesignBench contains 900 webpage samples spanning over 11 topics, 9 edit types, and 6 issue categories, enabling detailed analysis of MLLM performance across multiple dimensions. Our systematic evaluation reveals critical insights into MLLMs' framework-specific limitations, task-related bottlenecks, and performance variations under different conditions, providing guidance for future research in automated front-end development. Our code and data are available at https://github.com/WebPAI/DesignBench.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06251v1",
    "published_date": "2025-06-06 17:21:21 UTC",
    "updated_date": "2025-06-06 17:21:21 UTC"
  },
  {
    "arxiv_id": "2506.12075v1",
    "title": "T-TExTS (Teaching Text Expansion for Teacher Scaffolding): Enhancing Text Selection in High School Literature through Knowledge Graph-Based Recommendation",
    "authors": [
      "Nirmal Gelal",
      "Chloe Snow",
      "Ambyr Rios",
      "Hande Kk McGinty"
    ],
    "abstract": "The implementation of transformational pedagogy in secondary education classrooms requires a broad multiliteracy approach. Due to limited planning time and resources, high school English Literature teachers often struggle to curate diverse, thematically aligned literature text sets. This study addresses the critical need for a tool that provides scaffolds for novice educators in selecting literature texts that are diverse -- in terms of genre, theme, subtheme, and author -- yet similar in context and pedagogical merits. We have developed a recommendation system, Teaching Text Expansion for Teacher Scaffolding (T-TExTS), that suggests high school English Literature books based on pedagogical merits, genre, and thematic relevance using a knowledge graph. We constructed a domain-specific ontology using the KNowledge Acquisition and Representation Methodology (KNARM), transformed into a knowledge graph, which was then embedded using DeepWalk, biased random walk, and a hybrid of both approaches. The system was evaluated using link prediction and recommendation performance metrics, including Area Under the Curve (AUC), Mean Reciprocal Rank (MRR), Hits@K, and normalized Discounted Cumulative Gain (nDCG). DeepWalk outperformed in most ranking metrics, with the highest AUC (0.9431), whereas the hybrid model offered balanced performance. These findings demonstrate the importance of semantic, ontology-driven approaches in recommendation systems and suggest that T-TExTS can significantly ease the burden of English Literature text selection for high school educators, promoting more informed and inclusive curricular decisions. The source code for T-TExTS is available at: https://github.com/koncordantlab/TTExTS",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12075v1",
    "published_date": "2025-06-06 17:20:02 UTC",
    "updated_date": "2025-06-06 17:20:02 UTC"
  },
  {
    "arxiv_id": "2506.06242v1",
    "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models",
    "authors": [
      "Zahra Babaiee",
      "Peyman M. Kiasari",
      "Daniela Rus",
      "Radu Grosu"
    ],
    "abstract": "Recent advancements in multimodal large language models have driven breakthroughs in visual question answering. Yet, a critical gap persists, `conceptualization'-the ability to recognize and reason about the same concept despite variations in visual form, a basic ability of human reasoning. To address this challenge, we introduce the Visual Graph Arena (VGA), a dataset featuring six graph-based tasks designed to evaluate and improve AI systems' capacity for visual abstraction. VGA uses diverse graph layouts (e.g., Kamada-Kawai vs. planar) to test reasoning independent of visual form. Experiments with state-of-the-art vision models and multimodal LLMs reveal a striking divide: humans achieved near-perfect accuracy across tasks, while models totally failed on isomorphism detection and showed limited success in path/cycle tasks. We further identify behavioral anomalies suggesting pseudo-intelligent pattern matching rather than genuine understanding. These findings underscore fundamental limitations in current AI models for visual understanding. By isolating the challenge of representation-invariant reasoning, the VGA provides a framework to drive progress toward human-like conceptualization in AI visual models. The Visual Graph Arena is available at: \\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06242v1",
    "published_date": "2025-06-06 17:06:25 UTC",
    "updated_date": "2025-06-06 17:06:25 UTC"
  },
  {
    "arxiv_id": "2506.06231v3",
    "title": "Towards an Explainable Comparison and Alignment of Feature Embeddings",
    "authors": [
      "Mohammad Jalali",
      "Bahar Dibaei Nia",
      "Farzan Farnia"
    ],
    "abstract": "While several feature embedding models have been developed in the literature, comparisons of these embeddings have largely focused on their numerical performance in classification-related downstream applications. However, an interpretable comparison of different embeddings requires identifying and analyzing mismatches between sample groups clustered within the embedding spaces. In this work, we propose the \\emph{Spectral Pairwise Embedding Comparison (SPEC)} framework to compare embeddings and identify their differences in clustering a reference dataset. Our approach examines the kernel matrices derived from two embeddings and leverages the eigendecomposition of the difference kernel matrix to detect sample clusters that are captured differently by the two embeddings. We present a scalable implementation of this kernel-based approach, with computational complexity that grows linearly with the sample size. Furthermore, we introduce an optimization problem using this framework to align two embeddings, ensuring that clusters identified in one embedding are also captured in the other model. We provide numerical results demonstrating the SPEC's application to compare and align embeddings on large-scale datasets such as ImageNet and MS-COCO. The project page is available at https://mjalali.github.io/SPEC/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "math.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06231v3",
    "published_date": "2025-06-06 16:50:37 UTC",
    "updated_date": "2025-08-16 20:27:12 UTC"
  },
  {
    "arxiv_id": "2506.06225v1",
    "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy",
    "authors": [
      "Eunhye Grace Ko",
      "Shaini Nanayakkara",
      "Earl W. Huff"
    ],
    "abstract": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to equip users, particularly vulnerable populations such as older adults (65 and older), with the knowledge to understand its benefits and potential risks. Older adults often exhibit greater reservations about adopting emerging technologies and require tailored literacy support. Using a mixed methods approach, this study examines strategies for delivering GenAI literacy to older adults through a chatbot named Litti, evaluating its impact on their AI literacy (knowledge, safety, and ethical use). The quantitative data indicated a trend toward improved AI literacy, though the results were not statistically significant. However, qualitative interviews revealed diverse levels of familiarity with generative AI and a strong desire to learn more. Findings also show that while Litti provided a positive learning experience, it did not significantly enhance participants' trust or sense of safety regarding GenAI. This exploratory case study highlights the challenges and opportunities in designing AI literacy education for the rapidly growing older adult population.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06225v1",
    "published_date": "2025-06-06 16:38:37 UTC",
    "updated_date": "2025-06-06 16:38:37 UTC"
  },
  {
    "arxiv_id": "2506.06220v2",
    "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
    "authors": [
      "Diji Yang",
      "Minghao Liu",
      "Chung-Hsiang Lo",
      "Yi Zhang",
      "James Davis"
    ],
    "abstract": "Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind. That is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06220v2",
    "published_date": "2025-06-06 16:28:03 UTC",
    "updated_date": "2025-10-29 22:25:02 UTC"
  },
  {
    "arxiv_id": "2506.06216v2",
    "title": "ILP Techniques for Enhancing Branch and Bound MaxSAT Solvers",
    "authors": [
      "Jialu Zhang",
      "Chu-Min Li",
      "Sami Cherif",
      "Shuolin Li",
      "Zhifei Zheng"
    ],
    "abstract": "This paper investigates the impact of ILP techniques on BnB MaxSAT solvers, particularly ILP preprocessing techniques and various portfolio strategies. Experimental results demonstrate that ILP techniques enable WMaxCDCL-OpenWbo1200 and MaxCDCL-OpenWbo300, the best two solvers in the unweighted track of the MaxSAT evaluation 2024, to solve 27 and 30 additional instances, respectively. Furthermore, although state-of-the-art MaxSAT solvers heavily rely on an ILP solver in their portfolios, our proposed approach uses ILP preprocessing techniques to reduce this dependency. Allocating only a short runtime to the ILP solver within a portfolio that includes (W)MaxCDCL, as proposed in our approach, is sufficient to achieve strong results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06216v2",
    "published_date": "2025-06-06 16:21:38 UTC",
    "updated_date": "2025-07-08 02:29:46 UTC"
  },
  {
    "arxiv_id": "2506.06214v1",
    "title": "Can Theoretical Physics Research Benefit from Language Agents?",
    "authors": [
      "Sirui Lu",
      "Zhijing Jin",
      "Terry Jingchen Zhang",
      "Pavel Kos",
      "J. Ignacio Cirac",
      "Bernhard Schlkopf"
    ],
    "abstract": "Large Language Models (LLMs) are rapidly advancing across diverse domains, yet their application in theoretical physics research is not yet mature. This position paper argues that LLM agents can potentially help accelerate theoretical, computational, and applied physics when properly integrated with domain knowledge and toolbox. We analyze current LLM capabilities for physics -- from mathematical reasoning to code generation -- identifying critical gaps in physical intuition, constraint satisfaction, and reliable reasoning. We envision future physics-specialized LLMs that could handle multimodal data, propose testable hypotheses, and design experiments. Realizing this vision requires addressing fundamental challenges: ensuring physical consistency, and developing robust verification methods. We call for collaborative efforts between physics and AI communities to help advance scientific discovery in physics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "math-ph",
      "quant-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.06214v1",
    "published_date": "2025-06-06 16:20:06 UTC",
    "updated_date": "2025-06-06 16:20:06 UTC"
  },
  {
    "arxiv_id": "2506.06211v1",
    "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts",
    "authors": [
      "Hengzhi Li",
      "Brendon Jiang",
      "Alexander Naehu",
      "Regan Song",
      "Justin Zhang",
      "Megan Tjandrasuwita",
      "Chanakya Ekbote",
      "Steven-Shine Chen",
      "Adithya Balachandran",
      "Wei Dai",
      "Rebecca Chang",
      "Paul Pu Liang"
    ],
    "abstract": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined problem definitions. In contrast to conventional reasoning benchmarks consisting of tasks with clear instructions, puzzlehunts require models to discover the underlying problem structure from multimodal evidence and iterative reasoning, mirroring real-world domains such as scientific discovery, exploratory data analysis, or investigative problem-solving. Despite recent progress in foundation models, their performance on such open-ended settings remains largely untested. In this paper, we introduce PuzzleWorld, a large-scale benchmark of 667 puzzlehunt-style problems designed to assess step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is annotated with the final solution, detailed reasoning traces, and cognitive skill labels, enabling holistic benchmarking and fine-grained diagnostic analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy, with the best model solving only 14% of puzzles and reaching 40% stepwise accuracy. To demonstrate the value of our reasoning annotations, we show that fine-tuning a small model on reasoning traces improves stepwise reasoning from 4% to 11%, while training on final answers alone degrades performance to near zero. Our error analysis reveals that current models exhibit myopic reasoning, are bottlenecked by the limitations of language-based inference, and lack sketching capabilities crucial for visual and spatial reasoning. We release PuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on building more general, open-ended, and creative reasoning systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06211v1",
    "published_date": "2025-06-06 16:17:09 UTC",
    "updated_date": "2025-06-06 16:17:09 UTC"
  },
  {
    "arxiv_id": "2506.06208v1",
    "title": "Building Models of Neurological Language",
    "authors": [
      "Henry Watkins"
    ],
    "abstract": "This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06208v1",
    "published_date": "2025-06-06 16:14:28 UTC",
    "updated_date": "2025-06-06 16:14:28 UTC"
  },
  {
    "arxiv_id": "2506.06205v1",
    "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning",
    "authors": [
      "Sheng Chen",
      "Peiyu He",
      "Jiaxin Hu",
      "Ziyang Liu",
      "Yansheng Wang",
      "Tao Xu",
      "Chi Zhang",
      "Chongchong Zhang",
      "Chao An",
      "Shiyu Cai",
      "Duo Cao",
      "Kangping Chen",
      "Shuai Chu",
      "Tianwei Chu",
      "Mingdi Dan",
      "Min Du",
      "Weiwei Fang",
      "Pengyou Fu",
      "Junkai Hu",
      "Xiaowei Jiang",
      "Zhaodi Jiang",
      "Fuxuan Li",
      "Jun Li",
      "Minghui Li",
      "Mingyao Li",
      "Yanchang Li",
      "Zhibin Li",
      "Guangming Liu",
      "Kairui Liu",
      "Lihao Liu",
      "Weizhi Liu",
      "Xiaoshun Liu",
      "Yufei Liu",
      "Yunfei Liu",
      "Qiang Lu",
      "Yuanfei Luo",
      "Xiang Lv",
      "Hongying Ma",
      "Sai Ma",
      "Lingxian Mi",
      "Sha Sa",
      "Hongxiang Shu",
      "Lei Tian",
      "Chengzhi Wang",
      "Jiayu Wang",
      "Kaijie Wang",
      "Qingyi Wang",
      "Renwen Wang",
      "Tao Wang",
      "Wei Wang",
      "Xirui Wang",
      "Chao Wei",
      "Xuguang Wei",
      "Zijun Xia",
      "Zhaohao Xiao",
      "Tingshuai Yan",
      "Liyan Yang",
      "Yifan Yang",
      "Zhikai Yang",
      "Zhong Yin",
      "Li Yuan",
      "Liuchun Yuan",
      "Chi Zhang",
      "Jinyang Zhang",
      "Junhui Zhang",
      "Linge Zhang",
      "Zhenyi Zhang",
      "Zheyu Zhang",
      "Dongjie Zhu",
      "Hang Li",
      "Yangang Zhang"
    ],
    "abstract": "Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Astra Technical Report",
    "pdf_url": "https://arxiv.org/pdf/2506.06205v1",
    "published_date": "2025-06-06 16:08:47 UTC",
    "updated_date": "2025-06-06 16:08:47 UTC"
  },
  {
    "arxiv_id": "2506.06202v2",
    "title": "MLOps with Microservices: A Case Study on the Maritime Domain",
    "authors": [
      "Renato Cordeiro Ferreira",
      "Rowanne Trapmann",
      "Willem-Jan van den Heuvel"
    ],
    "abstract": "This case study describes challenges and lessons learned on building Ocean Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the maritime domain. First, the paper presents the system's specification, and architecture. Ocean Guard was designed with a microservices' architecture to enable multiple teams to work on the project in parallel. Then, the paper discusses how the developers adapted contract-based design to MLOps for achieving that goal. As a MLES, Ocean Guard employs code, model, and data contracts to establish guidelines between its services. This case study hopes to inspire software engineers, machine learning engineers, and data scientists to leverage similar approaches for their systems.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "13 pages, 3 figures, to be published in SummerSOC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06202v2",
    "published_date": "2025-06-06 16:04:59 UTC",
    "updated_date": "2025-08-11 16:45:36 UTC"
  },
  {
    "arxiv_id": "2506.11100v1",
    "title": "An Active Learning-Based Streaming Pipeline for Reduced Data Training of Structure Finding Models in Neutron Diffractometry",
    "authors": [
      "Tianle Wang",
      "Jorge Ramirez",
      "Cristina Garcia-Cardona",
      "Thomas Proffen",
      "Shantenu Jha",
      "Sudip K. Seal"
    ],
    "abstract": "Structure determination workloads in neutron diffractometry are computationally expensive and routinely require several hours to many days to determine the structure of a material from its neutron diffraction patterns. The potential for machine learning models trained on simulated neutron scattering patterns to significantly speed up these tasks have been reported recently. However, the amount of simulated data needed to train these models grows exponentially with the number of structural parameters to be predicted and poses a significant computational challenge. To overcome this challenge, we introduce a novel batch-mode active learning (AL) policy that uses uncertainty sampling to simulate training data drawn from a probability distribution that prefers labelled examples about which the model is least certain. We confirm its efficacy in training the same models with about 75% less training data while improving the accuracy. We then discuss the design of an efficient stream-based training workflow that uses this AL policy and present a performance study on two heterogeneous platforms to demonstrate that, compared with a conventional training workflow, the streaming workflow delivers about 20% shorter training time without any loss of accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "physics.atm-clus",
      "physics.data-an"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11100v1",
    "published_date": "2025-06-06 15:48:22 UTC",
    "updated_date": "2025-06-06 15:48:22 UTC"
  },
  {
    "arxiv_id": "2506.06169v1",
    "title": "semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces",
    "authors": [
      "Jwalanthi Ranganathan",
      "Rohan Jha",
      "Kanishka Misra",
      "Kyle Mahowald"
    ],
    "abstract": "We introduce semantic-features, an extensible, easy-to-use library based on Chronis et al. (2023) for studying contextualized word embeddings of LMs by projecting them into interpretable spaces. We apply this tool in an experiment where we measure the contextual effect of the choice of dative construction (prepositional or double object) on the semantic interpretation of utterances (Bresnan, 2007). Specifically, we test whether \"London\" in \"I sent London the letter.\" is more likely to be interpreted as an animate referent (e.g., as the name of a person) than in \"I sent the letter to London.\" To this end, we devise a dataset of 450 sentence pairs, one in each dative construction, with recipients being ambiguous with respect to person-hood vs. place-hood. By applying semantic-features, we show that the contextualized word embeddings of three masked language models show the expected sensitivities. This leaves us optimistic about the usefulness of our tool.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "SCiL 2025 Camera Ready Extended Abstract",
    "pdf_url": "https://arxiv.org/pdf/2506.06169v1",
    "published_date": "2025-06-06 15:33:27 UTC",
    "updated_date": "2025-06-06 15:33:27 UTC"
  },
  {
    "arxiv_id": "2506.06166v1",
    "title": "The Lock-in Hypothesis: Stagnation by Algorithm",
    "authors": [
      "Tianyi Alex Qiu",
      "Zhonghao He",
      "Tejasveer Chugh",
      "Max Kleiman-Weiner"
    ],
    "abstract": "The training and deployment of large language models (LLMs) create a feedback loop with human users: models learn human beliefs from data, reinforce these beliefs with generated content, reabsorb the reinforced beliefs, and feed them back to users again and again. This dynamic resembles an echo chamber. We hypothesize that this feedback loop entrenches the existing values and beliefs of users, leading to a loss of diversity and potentially the lock-in of false beliefs. We formalize this hypothesis and test it empirically with agent-based LLM simulations and real-world GPT usage data. Analysis reveals sudden but sustained drops in diversity after the release of new GPT iterations, consistent with the hypothesized human-AI feedback loop. Code and data available at https://thelockinhypothesis.com",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025, 46 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.06166v1",
    "published_date": "2025-06-06 15:31:31 UTC",
    "updated_date": "2025-06-06 15:31:31 UTC"
  },
  {
    "arxiv_id": "2506.06165v1",
    "title": "(AI peers) are people learning from the same standpoint: Perception of AI characters in a Collaborative Science Investigation",
    "authors": [
      "Eunhye Grace Ko",
      "Soo Hyoung Joo"
    ],
    "abstract": "While the complexity of 21st-century demands has promoted pedagogical approaches to foster complex competencies, a persistent gap remains between in-class learning activities and individualized learning or assessment practices. To address this, studies have explored the use of AI-generated characters in learning and assessment. One attempt is scenario-based assessment (SBA), a technique that not only measures but also fosters the development of competencies throughout the assessment process. SBA introduces simulated agents to provide an authentic social-interactional context, allowing for the assessment of competency-based constructs while mitigating the unpredictability of real-life interactions. Recent advancements in multimodal AI, such as text-to-video technology, allow these agents to be enhanced into AI-generated characters. This mixed-method study investigates how learners perceive AI characters taking the role of mentor and teammates in an SBA mirroring the context of a collaborative science investigation. Specifically, we examined the Likert scale responses of 56 high schoolers regarding trust, social presence, and effectiveness. We analyzed the relationships between these factors and their impact on the intention to adopt AI characters through PLS-SEM. Our findings indicated that learners' trust shaped their sense of social presence with the AI characters, enhancing perceived effectiveness. Qualitative analysis further highlighted factors that foster trust, such as material credibility and alignment with learning goals, as well as the pivotal role of social presence in creating a collaborative context.\n  This paper was accepted as an full paper for AIED 2025.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.06165v1",
    "published_date": "2025-06-06 15:29:11 UTC",
    "updated_date": "2025-06-06 15:29:11 UTC"
  },
  {
    "arxiv_id": "2506.06162v2",
    "title": "Recommender systems, stigmergy, and the tyranny of popularity",
    "authors": [
      "Zackary Okun Dunivin",
      "Paul E. Smaldino"
    ],
    "abstract": "Scientific recommender systems, such as Google Scholar and Web of Science, are essential tools for discovery. Search algorithms that power work through stigmergy, a collective intelligence mechanism that surfaces useful paths through repeated engagement. While generally effective, this \"rich-get-richer\" dynamic results in a small number of high-profile papers that dominate visibility. This essay argues argue that these algorithm over-reliance on popularity fosters intellectual homogeneity and exacerbates structural inequities, stifling innovative and diverse perspectives critical for scientific progress. We propose an overhaul of search platforms to incorporate user-specific calibration, allowing researchers to manually adjust the weights of factors like popularity, recency, and relevance. We also advise platform developers on how text embeddings and LLMs could be implemented in ways that increase user autonomy. While our suggestions are particularly pertinent to aligning recommender systems with scientific values, these ideas are broadly applicable to information access systems in general. Designing platforms that increase user autonomy is an important step toward more robust and dynamic information",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06162v2",
    "published_date": "2025-06-06 15:27:23 UTC",
    "updated_date": "2025-07-04 03:51:55 UTC"
  },
  {
    "arxiv_id": "2506.06151v2",
    "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems",
    "authors": [
      "Haowei Wang",
      "Rupeng Zhang",
      "Junjie Wang",
      "Mingyang Li",
      "Yuekai Huang",
      "Dandan Wang",
      "Qing Wang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by retrieving relevant documents from external corpora before generating responses. This approach significantly expands LLM capabilities by leveraging vast, up-to-date external knowledge. However, this reliance on external knowledge makes RAG systems vulnerable to corpus poisoning attacks that manipulate generated outputs via poisoned document injection. Existing poisoning attack strategies typically treat the retrieval and generation stages as disjointed, limiting their effectiveness. We propose Joint-GCG, the first framework to unify gradient-based attacks across both retriever and generator models through three innovations: (1) Cross-Vocabulary Projection for aligning embedding spaces, (2) Gradient Tokenization Alignment for synchronizing token-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically balancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves at most 25% and an average of 5% higher attack success rate than previous methods across multiple retrievers and generators. While optimized under a white-box assumption, the generated poisons show unprecedented transferability to unseen models. Joint-GCG's innovative unification of gradient-based attacks across retrieval and generation stages fundamentally reshapes our understanding of vulnerabilities within RAG systems. Our code is available at https://github.com/NicerWang/Joint-GCG.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06151v2",
    "published_date": "2025-06-06 15:12:06 UTC",
    "updated_date": "2025-11-12 10:22:52 UTC"
  },
  {
    "arxiv_id": "2506.06121v2",
    "title": "Decomposability-Guaranteed Cooperative Coevolution for Large-Scale Itinerary Planning",
    "authors": [
      "Ziyu Zhang",
      "Peilan Xu",
      "Yuetong Sun",
      "Yuhui Shi",
      "Wenjian Luo"
    ],
    "abstract": "Large-scale itinerary planning is a variant of the traveling salesman problem, aiming to determine an optimal path that maximizes the collected points of interest (POIs) scores while minimizing travel time and cost, subject to travel duration constraints. This paper analyzes the decomposability of large-scale itinerary planning, proving that strict decomposability is difficult to satisfy, and introduces a weak decomposability definition based on a necessary condition, deriving the corresponding graph structures that fulfill this property. With decomposability guaranteed, we propose a novel multi-objective cooperative coevolutionary algorithm for large-scale itinerary planning, addressing the challenges of component imbalance and interactions. Specifically, we design a dynamic decomposition strategy based on the normalized fitness within each component, define optimization potential considering component scale and contribution, and develop a computational resource allocation strategy. Finally, we evaluate the proposed algorithm on a set of real-world datasets. Comparative experiments with state-of-the-art multi-objective itinerary planning algorithms demonstrate the superiority of our approach, with performance advantages increasing as the problem scale grows.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06121v2",
    "published_date": "2025-06-06 14:31:57 UTC",
    "updated_date": "2025-06-13 04:59:48 UTC"
  },
  {
    "arxiv_id": "2506.06117v1",
    "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction",
    "authors": [
      "Christophe Van Gysel",
      "Maggie Wu",
      "Lyan Verwimp",
      "Caglar Tirkaz",
      "Marco Bertola",
      "Zhihong Lei",
      "Youssef Oualil"
    ],
    "abstract": "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using paired audio-text samples that are expensive to obtain, since high-quality ground-truth data requires human annotators. Voice search applications, such as digital media players, leverage ASR to allow users to search by voice as opposed to an on-screen keyboard. However, recent or infrequent movie titles may not be sufficiently represented in the E2E ASR system's training data, and hence, may suffer poor recognition.\n  In this paper, we propose a phonetic correction system that consists of (a) a phonetic search based on the ASR model's output that generates phonetic alternatives that may not be considered by the E2E system, and (b) a rescorer component that combines the ASR model recognition and the phonetic alternatives, and select a final system output.\n  We find that our approach improves word error rate between 4.4 and 7.6% relative on benchmarks of popular movie titles over a series of competitive baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at Interspeech '25",
    "pdf_url": "https://arxiv.org/pdf/2506.06117v1",
    "published_date": "2025-06-06 14:25:18 UTC",
    "updated_date": "2025-06-06 14:25:18 UTC"
  },
  {
    "arxiv_id": "2506.06112v1",
    "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness",
    "authors": [
      "Cheng-Long Wang",
      "Qi Li",
      "Zihang Xiang",
      "Yinzhi Cao",
      "Di Wang"
    ],
    "abstract": "Growing concerns over data privacy and security highlight the importance of machine unlearning--removing specific data influences from trained models without full retraining. Techniques like Membership Inference Attacks (MIAs) are widely used to externally assess successful unlearning. However, existing methods face two key limitations: (1) maximizing MIA effectiveness (e.g., via online attacks) requires prohibitive computational resources, often exceeding retraining costs; (2) MIAs, designed for binary inclusion tests, struggle to capture granular changes in approximate unlearning. To address these challenges, we propose the Interpolated Approximate Measurement (IAM), a framework natively designed for unlearning inference. IAM quantifies sample-level unlearning completeness by interpolating the model's generalization-fitting behavior gap on queried samples. IAM achieves strong performance in binary inclusion tests for exact unlearning and high correlation for approximate unlearning--scalable to LLMs using just one pre-trained shadow model. We theoretically analyze how IAM's scoring mechanism maintains performance efficiently. We then apply IAM to recent approximate unlearning algorithms, revealing general risks of both over-unlearning and under-unlearning, underscoring the need for stronger safeguards in approximate unlearning systems. The code is available at https://github.com/Happy2Git/Unlearning_Inference_IAM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the Proceedings of USENIX Security Symposium, 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06112v1",
    "published_date": "2025-06-06 14:22:18 UTC",
    "updated_date": "2025-06-06 14:22:18 UTC"
  },
  {
    "arxiv_id": "2506.06105v2",
    "title": "Text-to-LoRA: Instant Transformer Adaption",
    "authors": [
      "Rujikorn Charakorn",
      "Edoardo Cetin",
      "Yujin Tang",
      "Robert Tjarko Lange"
    ],
    "abstract": "While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.\n  Our code is available at https://github.com/SakanaAI/text-to-lora",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06105v2",
    "published_date": "2025-06-06 14:11:27 UTC",
    "updated_date": "2025-06-09 14:19:59 UTC"
  },
  {
    "arxiv_id": "2506.06409v2",
    "title": "HeavyWater and SimplexWater: Distortion-Free LLM Watermarks for Low-Entropy Next-Token Predictions",
    "authors": [
      "Dor Tsur",
      "Carol Xuan Long",
      "Claudio Mayrink Verdun",
      "Hsiang Hsu",
      "Chen-Fu Chen",
      "Haim Permuter",
      "Sajani Vithana",
      "Flavio P. Calmon"
    ],
    "abstract": "Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Presented at NeurIPS2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06409v2",
    "published_date": "2025-06-06 13:52:34 UTC",
    "updated_date": "2025-12-02 00:55:45 UTC"
  },
  {
    "arxiv_id": "2506.11099v1",
    "title": "Knowledge Graph Embeddings with Representing Relations as Annular Sectors",
    "authors": [
      "Huiling Zhu",
      "Yingqi Zeng"
    ],
    "abstract": "Knowledge graphs (KGs), structured as multi-relational data of entities and relations, are vital for tasks like data analysis and recommendation systems. Knowledge graph completion (KGC), or link prediction, addresses incompleteness of KGs by inferring missing triples (h, r, t). It is vital for downstream applications. Region-based embedding models usually embed entities as points and relations as geometric regions to accomplish the task. Despite progress, these models often overlook semantic hierarchies inherent in entities. To solve this problem, we propose SectorE, a novel embedding model in polar coordinates. Relations are modeled as annular sectors, combining modulus and phase to capture inference patterns and relation attributes. Entities are embedded as points within these sectors, intuitively encoding hierarchical structure. Evaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive performance against various kinds of models, demonstrating strengths in semantic modeling capability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11099v1",
    "published_date": "2025-06-06 13:30:36 UTC",
    "updated_date": "2025-06-06 13:30:36 UTC"
  },
  {
    "arxiv_id": "2506.11098v1",
    "title": "Debiasing Online Preference Learning via Preference Feature Preservation",
    "authors": [
      "Dongyoung Kim",
      "Jinsung Yoon",
      "Jinwoo Shin",
      "Jaehyung Kim"
    ],
    "abstract": "Recent preference learning frameworks for large language models (LLMs) simplify human preferences with binary pairwise comparisons and scalar rewards. This simplification could make LLMs' responses biased to mostly preferred features, and would be exacerbated during the iterations of online preference learning steps. To address these challenges, we propose a novel framework coined PFP (Preference Feature Preservation). The key idea of PFP is maintaining the distribution of human preference features and utilizing such rich signals throughout the online preference learning process. Specifically, PFP first extract preference features from offline pairwise human preference data and trains a feature classifier. Then, using trained classifier and the distribution preserving optimization, PFP maps appropriate preference features for a new input instruction during online learning. Lastly, PFP trains LLM using the existing preference learning method, by incorporating the preference feature into system prompts and enabling LLM to explicitly handle various human preferences. Our experiments demonstrate that PFP successfully mitigates the bias in preference features during online learning, and hence achieves superior performance compared to previous preference learning methods on standard benchmarks to evaluate LLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 page, 20 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11098v1",
    "published_date": "2025-06-06 13:19:07 UTC",
    "updated_date": "2025-06-06 13:19:07 UTC"
  },
  {
    "arxiv_id": "2506.06060v1",
    "title": "Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models",
    "authors": [
      "Yingqi Hu",
      "Zhuo Zhang",
      "Jingyuan Zhang",
      "Lizhen Qu",
      "Zenglin Xu"
    ],
    "abstract": "Federated fine-tuning of large language models (FedLLMs) presents a promising approach for achieving strong model performance while preserving data privacy in sensitive domains. However, the inherent memorization ability of LLMs makes them vulnerable to training data extraction attacks. To investigate this risk, we introduce simple yet effective extraction attack algorithms specifically designed for FedLLMs. In contrast to prior \"verbatim\" extraction attacks, which assume access to fragments from all training data, our approach operates under a more realistic threat model, where the attacker only has access to a single client's data and aims to extract previously unseen personally identifiable information (PII) from other clients. This requires leveraging contextual prefixes held by the attacker to generalize across clients. To evaluate the effectiveness of our approaches, we propose two rigorous metrics-coverage rate and efficiency-and extend a real-world legal dataset with PII annotations aligned with CPIS, GDPR, and CCPA standards, achieving 89.9% human-verified precision. Experimental results show that our method can extract up to 56.57% of victim-exclusive PII, with \"Address,\" \"Birthday,\" and \"Name\" being the most vulnerable categories. Our findings underscore the pressing need for robust defense strategies and contribute a new benchmark and evaluation framework for future research in privacy-preserving federated learning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06060v1",
    "published_date": "2025-06-06 13:13:29 UTC",
    "updated_date": "2025-06-06 13:13:29 UTC"
  },
  {
    "arxiv_id": "2506.06058v1",
    "title": "Microgrids Coalitions for Energy Market Balancing",
    "authors": [
      "Viorica Chifu",
      "Cristina Bianca Pop",
      "Tudor Cioara",
      "Ionut Anghel"
    ],
    "abstract": "With the integration of renewable sources in electricity distribution networks, the need to develop intelligent mechanisms for balancing the energy market has arisen. In the absence of such mechanisms, the energy market may face imbalances that can lead to power outages, financial losses or instability at the grid level. In this context, the grouping of microgrids into optimal coalitions that can absorb energy from the market during periods of surplus or supply energy to the market during periods of is a key aspect in the efficient management of distribution networks. In this article, we propose a method that identify an optimal microgrids coalition capable of addressing the dynamics of the energy market. The proposed method models the problem of identifying the optimal coalition as an optimization problem that it solves by combining a strategy inspired by cooperative game theory with a memetic algorithm. An individual is represented as a coalition of microgrids and the evolution of population of individuals over generations is assured by recombination and mutation. The fitness function is defined as the difference between the total value generated by the coalition and a penalty applied to the coalition when the energy traded by coalition exceeds the energy available/demanded on/by the energy market. The value generated by the coalition is calculated based on the profit obtained by the collation if it sells energy on the market during periods of deficit or the savings obtained by the coalition if it buys energy on the market during periods of surplus and the costs associated with the trading process. This value is divided equitably among the coalition members, according to the Shapley value, which considers the contribution of each one to the formation of collective value.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06058v1",
    "published_date": "2025-06-06 13:06:11 UTC",
    "updated_date": "2025-06-06 13:06:11 UTC"
  },
  {
    "arxiv_id": "2506.06057v1",
    "title": "Hey, That's My Data! Label-Only Dataset Inference in Large Language Models",
    "authors": [
      "Chen Xiong",
      "Zihao Wang",
      "Rui Zhu",
      "Tsung-Yi Ho",
      "Pin-Yu Chen",
      "Jingwei Xiong",
      "Haixu Tang",
      "Lucila Ohno-Machado"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing by excelling at interpreting, reasoning about, and generating human language. However, their reliance on large-scale, often proprietary datasets poses a critical challenge: unauthorized usage of such data can lead to copyright infringement and significant financial harm. Existing dataset-inference methods typically depend on log probabilities to detect suspicious training material, yet many leading LLMs have begun withholding or obfuscating these signals. This reality underscores the pressing need for label-only approaches capable of identifying dataset membership without relying on internal model logits.\n  We address this gap by introducing CatShift, a label-only dataset-inference framework that capitalizes on catastrophic forgetting: the tendency of an LLM to overwrite previously learned knowledge when exposed to new data. If a suspicious dataset was previously seen by the model, fine-tuning on a portion of it triggers a pronounced post-tuning shift in the model's outputs; conversely, truly novel data elicits more modest changes. By comparing the model's output shifts for a suspicious dataset against those for a known non-member validation set, we statistically determine whether the suspicious set is likely to have been part of the model's original training corpus. Extensive experiments on both open-source and API-based LLMs validate CatShift's effectiveness in logit-inaccessible settings, offering a robust and practical solution for safeguarding proprietary data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06057v1",
    "published_date": "2025-06-06 13:02:59 UTC",
    "updated_date": "2025-06-06 13:02:59 UTC"
  },
  {
    "arxiv_id": "2506.06054v1",
    "title": "FPDANet: A Multi-Section Classification Model for Intelligent Screening of Fetal Ultrasound",
    "authors": [
      "Minglang Chen",
      "Jie He",
      "Caixu Xu",
      "Bocheng Liang",
      "Shengli Li",
      "Guannan He",
      "Xiongjie Tao"
    ],
    "abstract": "ResNet has been widely used in image classification tasks due to its ability to model the residual dependence of constant mappings for linear computation. However, the ResNet method adopts a unidirectional transfer of features and lacks an effective method to correlate contextual information, which is not effective in classifying fetal ultrasound images in the classification task, and fetal ultrasound images have problems such as low contrast, high similarity, and high noise. Therefore, we propose a bilateral multi-scale information fusion network-based FPDANet to address the above challenges. Specifically, we design the positional attention mechanism (DAN) module, which utilizes the similarity of features to establish the dependency of different spatial positional features and enhance the feature representation. In addition, we design a bilateral multi-scale (FPAN) information fusion module to capture contextual and global feature dependencies at different feature scales, thereby further improving the model representation. FPDANet classification results obtained 91.05\\% and 100\\% in Top-1 and Top-5 metrics, respectively, and the experimental results proved the effectiveness and robustness of FPDANet.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06054v1",
    "published_date": "2025-06-06 13:00:17 UTC",
    "updated_date": "2025-06-06 13:00:17 UTC"
  },
  {
    "arxiv_id": "2506.06052v2",
    "title": "CP-Bench: Evaluating Large Language Models for Constraint Modelling",
    "authors": [
      "Kostis Michailidis",
      "Dimos Tsouros",
      "Tias Guns"
    ],
    "abstract": "Constraint Programming (CP) is widely used to solve combinatorial problems, but its core process, namely constraint modelling, requires significant expertise and is considered to be a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for constraint modelling are often limited to small, homogeneous, or domain-specific instances, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing CP-Bench, a novel benchmark that includes a diverse set of well-known combinatorial problems sourced from the CP community, structured explicitly for evaluating LLM-driven CP modelling. With this dataset, and given the variety of constraint modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 70% on this highly challenging benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ECAI 25",
    "pdf_url": "https://arxiv.org/pdf/2506.06052v2",
    "published_date": "2025-06-06 12:56:02 UTC",
    "updated_date": "2025-09-04 09:10:05 UTC"
  },
  {
    "arxiv_id": "2506.06048v1",
    "title": "TRUST: Test-time Resource Utilization for Superior Trustworthiness",
    "authors": [
      "Haripriya Harikumar",
      "Santu Rana"
    ],
    "abstract": "Standard uncertainty estimation techniques, such as dropout, often struggle to clearly distinguish reliable predictions from unreliable ones. We attribute this limitation to noisy classifier weights, which, while not impairing overall class-level predictions, render finer-level statistics less informative. To address this, we propose a novel test-time optimization method that accounts for the impact of such noise to produce more reliable confidence estimates. This score defines a monotonic subset-selection function, where population accuracy consistently increases as samples with lower scores are removed, and it demonstrates superior performance in standard risk-based metrics such as AUSE and AURC. Additionally, our method effectively identifies discrepancies between training and test distributions, reliably differentiates in-distribution from out-of-distribution samples, and elucidates key differences between CNN and ViT classifiers across various vision datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06048v1",
    "published_date": "2025-06-06 12:52:32 UTC",
    "updated_date": "2025-06-06 12:52:32 UTC"
  },
  {
    "arxiv_id": "2506.06407v3",
    "title": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data",
    "authors": [
      "Zhi Wen Soi",
      "Chaoyi Zhu",
      "Fouad Abiad",
      "Aditya Shankar",
      "Jeroen M. Galjaard",
      "Huijuan Wang",
      "Lydia Y. Chen"
    ],
    "abstract": "Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in data space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in data space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the temporal-feature data space. The other unique feature is the $$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series while preserving robust watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against five datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the strongest state-of-the-art baseline, while remaining consistently detectable.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06407v3",
    "published_date": "2025-06-06 12:47:45 UTC",
    "updated_date": "2025-10-22 11:49:53 UTC"
  },
  {
    "arxiv_id": "2506.06406v2",
    "title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities",
    "authors": [
      "Guoyang Xia",
      "Yifeng Ding",
      "Fengfa Li",
      "Lei Ren",
      "Wei Chen",
      "Fangxiang Feng",
      "Xiaojie Wang"
    ],
    "abstract": "Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06406v2",
    "published_date": "2025-06-06 12:47:29 UTC",
    "updated_date": "2025-06-25 12:36:55 UTC"
  },
  {
    "arxiv_id": "2507.19498v1",
    "title": "ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings",
    "authors": [
      "Yue Wu",
      "Xiaolan Chen",
      "Weiyi Zhang",
      "Shunming Liu",
      "Wing Man Rita Sum",
      "Xinyuan Wu",
      "Xianwen Shang",
      "Chea-su Kee",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Large language models (LLMs) show promise for tailored healthcare communication but face challenges in interpretability and multi-task integration particularly for domain-specific needs like myopia, and their real-world effectiveness as patient education tools has yet to be demonstrated. Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text and image-based inquiries related to myopia. To achieve this, ChatMyopia integrates an image classification tool and a retrieval-augmented knowledge base built from literature, expert consensus, and clinical guidelines. Myopic maculopathy grading task, single question examination and human evaluations validated its ability to deliver personalized, accurate, and safe responses to myopia-related inquiries with high scalability and interpretability. In a randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly improved patient satisfaction compared to traditional leaflets, enhancing patient education in accuracy, empathy, disease awareness, and patient-eyecare practitioner communication. These findings highlight ChatMyopia's potential as a valuable supplement to enhance patient education and improve satisfaction with medical services in primary eye care settings.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "35 pages, 4 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2507.19498v1",
    "published_date": "2025-06-06 12:34:20 UTC",
    "updated_date": "2025-06-06 12:34:20 UTC"
  },
  {
    "arxiv_id": "2506.06035v2",
    "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion",
    "authors": [
      "Shiyi Zhang",
      "Dong Liang",
      "Hairong Zheng",
      "Yihang Zhou"
    ],
    "abstract": "Reconstructing visual information from brain activity bridges the gap between neuroscience and computer vision. Even though progress has been made in decoding images from fMRI using generative models, a challenge remains in accurately recovering highly complex visual stimuli. This difficulty stems from their elemental density and diversity, sophisticated spatial structures, and multifaceted semantic information.\n  To address these challenges, we propose HAVIR that contains two adapters: (1) The AutoKL Adapter transforms fMRI voxels into a latent diffusion prior, capturing topological structures; (2) The CLIP Adapter converts the voxels to CLIP text and image embeddings, containing semantic information. These complementary representations are fused by Versatile Diffusion to generate the final reconstructed image. To extract the most essential semantic information from complex scenarios, the CLIP Adapter is trained with text captions describing the visual stimuli and their corresponding semantic images synthesized from these captions. The experimental results demonstrate that HAVIR effectively reconstructs both structural features and semantic information of visual stimuli even in complex scenarios, outperforming existing models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "We have decided to withdraw this paper because the baseline methods used for comparison are outdated and do not reflect the current state-of-the-art. This significantly affects the validity of our performance claims and conclusions. We plan to conduct a more comprehensive evaluation and submit a revised version in the future",
    "pdf_url": "https://arxiv.org/pdf/2506.06035v2",
    "published_date": "2025-06-06 12:33:49 UTC",
    "updated_date": "2025-07-05 04:11:40 UTC"
  },
  {
    "arxiv_id": "2506.06028v1",
    "title": "End-to-End Framework for Robot Lawnmower Coverage Path Planning using Cellular Decomposition",
    "authors": [
      "Nikunj Shah",
      "Utsav Dey",
      "Kenji Nishimiya"
    ],
    "abstract": "Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic lawnmowers to effectively navigate and maintain lawns with diverse and irregular shapes. This paper introduces a comprehensive end-to-end pipeline for CPP, designed to convert user-defined boundaries on an aerial map into optimized coverage paths seamlessly. The pipeline includes user input extraction, coordinate transformation, area decomposition and path generation using our novel AdaptiveDecompositionCPP algorithm, preview and customization through an interactive coverage path visualizer, and conversion to actionable GPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular decomposition with an adaptive merging strategy to reduce non-mowing travel thereby enhancing operational efficiency. Experimental evaluations, encompassing both simulations and real-world lawnmower tests, demonstrate the effectiveness of the framework in coverage completeness and mowing efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, ICRA 2025, Workshop on Field Robotics",
    "pdf_url": "https://arxiv.org/pdf/2506.06028v1",
    "published_date": "2025-06-06 12:20:45 UTC",
    "updated_date": "2025-06-06 12:20:45 UTC"
  },
  {
    "arxiv_id": "2506.06020v1",
    "title": "When to Trust Context: Self-Reflective Debates for Context Reliability",
    "authors": [
      "Zeqi Zhou",
      "Fang Wu",
      "Shayan Talaei",
      "Haokai Zhao",
      "Cheng Meixin",
      "Tinson Xu",
      "Amin Saberi",
      "Yejin Choi"
    ],
    "abstract": "Large language models frequently encounter conflicts between their parametric knowledge and contextual input, often resulting in factual inconsistencies or hallucinations. We propose Self-Reflective Debate for Contextual Reliability (SR-DCR), a lightweight framework that integrates token-level self-confidence with an asymmetric multi-agent debate to adjudicate such conflicts. A critic, deprived of context, challenges a defender who argues from the given passage; a judge model evaluates the debate and determines the context's reliability. The final answer is selected by combining the verdict with model confidence. Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead. The code is available at https://github.com/smiles724/Self-Reflective-Debates.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06020v1",
    "published_date": "2025-06-06 12:09:34 UTC",
    "updated_date": "2025-06-06 12:09:34 UTC"
  },
  {
    "arxiv_id": "2506.06018v1",
    "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models",
    "authors": [
      "Chaoyi Zhu",
      "Zaitang Li",
      "Renyi Yang",
      "Robert Birke",
      "Pin-Yu Chen",
      "Tsung-Yi Ho",
      "Lydia Y. Chen"
    ],
    "abstract": "Watermarking becomes one of the pivotal solutions to trace and verify the origin of synthetic images generated by artificial intelligence models, but it is not free of risks. Recent studies demonstrate the capability to forge watermarks from a target image onto cover images via adversarial optimization without knowledge of the target generative model and watermark schemes. In this paper, we uncover a greater risk of an optimization-free and universal watermark forgery that harnesses existing regenerative diffusion models. Our proposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and integrates the target watermark via regenerating the image, without needing any additional optimization routine. It allows for universal watermark forgery that works independently of the target image's origin or the watermarking model used. We explore the watermarked latent extracted from the target image and visual-textual context of cover images as priors to guide sampling of the regenerative process. Extensive evaluation on 24 scenarios of model-data-watermark combinations demonstrates that PnP can successfully forge the watermark (up to 100% detectability and user attribution), and maintain the best visual perception. By bypassing model retraining and enabling adaptability to any image, our approach significantly broadens the scope of forgery attacks, presenting a greater challenge to the security of current watermarking techniques for diffusion models and the authority of watermarking schemes in synthetic data generation and governance.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06018v1",
    "published_date": "2025-06-06 12:08:02 UTC",
    "updated_date": "2025-06-06 12:08:02 UTC"
  },
  {
    "arxiv_id": "2506.06009v1",
    "title": "Unlocking Recursive Thinking of LLMs: Alignment via Refinement",
    "authors": [
      "Haoke Zhang",
      "Xiaobo Liang",
      "Cunxiang Wang",
      "Juntao Li",
      "Min Zhang"
    ],
    "abstract": "The OpenAI o1-series models have demonstrated that leveraging long-form Chain of Thought (CoT) can substantially enhance performance. However, the recursive thinking capabilities of Large Language Models (LLMs) remain limited, particularly in the absence of expert-curated data for distillation. In this paper, we propose \\textbf{AvR}: \\textbf{Alignment via Refinement}, a novel method aimed at unlocking the potential of LLMs for recursive reasoning through long-form CoT. AvR introduces a refinement process that integrates criticism and improvement actions, guided by differentiable learning techniques to optimize \\textbf{refinement-aware rewards}. As a result, the synthesized multi-round data can be organized as a long refinement thought, further enabling test-time scaling. Experimental results show that AvR significantly outperforms conventional preference optimization methods. Notably, with only 3k synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct model by over 20\\% in win rate on AlpacaEval 2.0. Our code is available at Github (https://github.com/Banner-Z/AvR.git).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the Findings of ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06009v1",
    "published_date": "2025-06-06 11:54:06 UTC",
    "updated_date": "2025-06-06 11:54:06 UTC"
  },
  {
    "arxiv_id": "2506.06008v1",
    "title": "Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models",
    "authors": [
      "Peijie Liu",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "Chain-of-Thought (CoT) technique has proven effective in improving the performance of large language models (LLMs) on complex reasoning tasks. However, the performance gains are inconsistent across different tasks, and the underlying mechanism remains a long-standing research question. In this work, we make a preliminary observation that the monotonicity of token probability distributions may be correlated with the gains achieved through CoT reasoning. Leveraging this insight, we propose two indicators based on the token probability distribution to assess CoT effectiveness across different tasks. By combining instance-level indicators with logistic regression model, we introduce Dynamic CoT, a method that dynamically select between CoT and direct answer. Furthermore, we extend Dynamic CoT to closed-source models by transferring decision strategies learned from open-source models. Our indicators for assessing CoT effectiveness achieve an accuracy of 89.2\\%, and Dynamic CoT reduces token consumption by more than 35\\% while maintaining high accuracy. Overall, our work offers a novel perspective on the underlying mechanisms of CoT reasoning and provides a framework for its more efficient deployment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 figures, 13 tables(Accept by ICML2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.06008v1",
    "published_date": "2025-06-06 11:53:27 UTC",
    "updated_date": "2025-06-06 11:53:27 UTC"
  },
  {
    "arxiv_id": "2506.06007v1",
    "title": "Enhancing Orthopox Image Classification Using Hybrid Machine Learning and Deep Learning Models",
    "authors": [
      "Alejandro Puente-Castro",
      "Enrique Fernandez-Blanco",
      "Daniel Rivero",
      "Andres Molares-Ulloa"
    ],
    "abstract": "Orthopoxvirus infections must be accurately classified from medical pictures for an easy and early diagnosis and epidemic prevention. The necessity for automated and scalable solutions is highlighted by the fact that traditional diagnostic techniques can be time-consuming and require expert interpretation and there are few and biased data sets of the different types of Orthopox. In order to improve classification performance and lower computational costs, a hybrid strategy is put forth in this paper that uses Machine Learning models combined with pretrained Deep Learning models to extract deep feature representations without the need for augmented data. The findings show that this feature extraction method, when paired with other methods in the state-of-the-art, produces excellent classification outcomes while preserving training and inference efficiency. The proposed approach demonstrates strong generalization and robustness across multiple evaluation settings, offering a scalable and interpretable solution for real-world clinical deployment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06007v1",
    "published_date": "2025-06-06 11:52:07 UTC",
    "updated_date": "2025-06-06 11:52:07 UTC"
  },
  {
    "arxiv_id": "2506.06006v1",
    "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models",
    "authors": [
      "Yifu Qiu",
      "Yftah Ziser",
      "Anna Korhonen",
      "Shay B. Cohen",
      "Edoardo M. Ponti"
    ],
    "abstract": "To what extent do vision-and-language foundation models possess a realistic world model (observation $\\times$ action $\\rightarrow$ observation) and a dynamics model (observation $\\times$ observation $\\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06006v1",
    "published_date": "2025-06-06 11:50:18 UTC",
    "updated_date": "2025-06-06 11:50:18 UTC"
  },
  {
    "arxiv_id": "2506.05990v1",
    "title": "Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests",
    "authors": [
      "Stefan Dascalescu",
      "Adrian Marius Dumitran",
      "Mihai Alexandru Vasiluta"
    ],
    "abstract": "Competitive programming contests play a crucial role in cultivating computational thinking and algorithmic skills among learners. However, generating comprehensive test cases to effectively assess programming solutions remains resource-intensive and challenging for educators. This paper introduces an innovative NLP-driven method leveraging generative AI (large language models) to automate the creation of high-quality test cases for competitive programming assessments. We extensively evaluated our approach on diverse datasets, including 25 years of Romanian Informatics Olympiad (OJI) data for 5th graders, recent competitions hosted on the Kilonova.ro platform, and the International Informatics Olympiad in Teams (IIOT). Our results demonstrate that AI-generated test cases substantially enhanced assessments, notably identifying previously undetected errors in 67% of the OJI 5th grade programming problems. These improvements underscore the complementary educational value of our technique in formative assessment contexts. By openly sharing our prompts, translated datasets, and methodologies, we offer practical NLP-based tools that educators and contest organizers can readily integrate to enhance assessment quality, reduce workload, and deepen insights into learner performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, 2 chart pies, 1 figure Pre-print version Accepted at BEA 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05990v1",
    "published_date": "2025-06-06 11:20:04 UTC",
    "updated_date": "2025-06-06 11:20:04 UTC"
  },
  {
    "arxiv_id": "2506.10019v1",
    "title": "A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations",
    "authors": [
      "Tian Lan",
      "Yang-Hao Zhou",
      "Zi-Ao Ma",
      "Fanshu Sun",
      "Rui-Qing Sun",
      "Junyu Luo",
      "Rong-Cheng Tu",
      "Heyan Huang",
      "Chen Xu",
      "Zhijing Wu",
      "Xian-Ling Mao"
    ],
    "abstract": "Recent advances in deep learning have significantly enhanced generative AI capabilities across text, images, and audio. However, automatically evaluating the quality of these generated outputs presents ongoing challenges. Although numerous automatic evaluation methods exist, current research lacks a systematic framework that comprehensively organizes these methods across text, visual, and audio modalities. To address this issue, we present a comprehensive review and a unified taxonomy of automatic evaluation methods for generated content across all three modalities; We identify five fundamental paradigms that characterize existing evaluation approaches across these domains. Our analysis begins by examining evaluation methods for text generation, where techniques are most mature. We then extend this framework to image and audio generation, demonstrating its broad applicability. Finally, we discuss promising directions for future research in cross-modal evaluation methodologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.10019v1",
    "published_date": "2025-06-06 11:09:46 UTC",
    "updated_date": "2025-06-06 11:09:46 UTC"
  },
  {
    "arxiv_id": "2506.05984v1",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "authors": [
      "Cheng-Han Chiang",
      "Xiaofei Wang",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Linjie Li",
      "Radu Kopetz",
      "Yao Qian",
      "Zhendong Wang",
      "Zhengyuan Yang",
      "Hung-yi Lee",
      "Lijuan Wang"
    ],
    "abstract": "Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05984v1",
    "published_date": "2025-06-06 11:05:48 UTC",
    "updated_date": "2025-06-06 11:05:48 UTC"
  },
  {
    "arxiv_id": "2506.05981v2",
    "title": "CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents",
    "authors": [
      "Qingbin Zeng",
      "Ruotong Zhao",
      "Jinzhu Mao",
      "Haoyang Li",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "Modeling urban crime is an important yet challenging task that requires understanding the subtle visual, social, and cultural cues embedded in urban environments. Previous work has mainly focused on rule-based agent-based modeling (ABM) and deep learning methods. ABMs offer interpretability of internal mechanisms but exhibit limited predictive accuracy. In contrast, deep learning methods are often effective in prediction but are less interpretable and require extensive training data. Moreover, both lines of work lack the cognitive flexibility to adapt to changing environments. Leveraging the capabilities of large language models (LLMs), we propose CrimeMind, a novel LLM-driven ABM framework for simulating urban crime within a multi-modal urban context. A key innovation of our design is the integration of the Routine Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to process rich multi-modal urban features and reason about criminal behavior. However, RAT requires LLM agents to infer subtle cues in evaluating environmental safety as part of assessing guardianship, which can be challenging for LLMs. To address this, we collect a small-scale human-annotated dataset and align CrimeMind's perception with human judgment via a training-free textual gradient method. Experiments across four major U.S. cities demonstrate that CrimeMind outperforms both traditional ABMs and deep learning baselines in crime hotspot prediction and spatial distribution accuracy, achieving up to a 24% improvement over the strongest baseline. Furthermore, we conduct counterfactual simulations of external incidents and policy interventions and it successfully captures the expected changes in crime patterns, demonstrating its ability to reflect counterfactual scenarios. Overall, CrimeMind enables fine-grained modeling of individual behaviors and facilitates evaluation of real-world interventions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Typos corrected",
    "pdf_url": "https://arxiv.org/pdf/2506.05981v2",
    "published_date": "2025-06-06 11:01:21 UTC",
    "updated_date": "2025-06-10 02:29:07 UTC"
  },
  {
    "arxiv_id": "2506.05980v4",
    "title": "AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification",
    "authors": [
      "Geonwoo Cho",
      "Jaemoon Lee",
      "Jaegyun Im",
      "Subi Lee",
      "Jihwan Lee",
      "Sundong Kim"
    ],
    "abstract": "Skill-based reinforcement learning (SBRL) enables rapid adaptation in environments with sparse rewards by pretraining a skill-conditioned policy. Effective skill learning requires jointly maximizing both exploration and skill diversity. However, existing methods often face challenges in simultaneously optimizing for these two conflicting objectives. In this work, we propose a new method, Adaptive Multi-objective Projection for balancing Exploration and skill Diversification (AMPED), which explicitly addresses both: during pre-training, a gradient-surgery projection balances the exploration and diversity gradients, and during fine-tuning, a skill selector exploits the learned diversity by choosing skills suited to downstream tasks. Our approach achieves performance that surpasses SBRL baselines across various benchmarks. Through an extensive ablation study, we identify the role of each component and demonstrate that each element in AMPED is contributing to performance. We further provide theoretical and empirical evidence that, with a greedy skill selector, greater skill diversity reduces fine-tuning sample complexity. These results highlight the importance of explicitly harmonizing exploration and diversity and demonstrate the effectiveness of AMPED in enabling robust and generalizable skill learning. Project Page: https://geonwoo.me/amped/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05980v4",
    "published_date": "2025-06-06 10:59:39 UTC",
    "updated_date": "2025-12-02 20:01:25 UTC"
  },
  {
    "arxiv_id": "2506.11097v3",
    "title": "C-SEO Bench: Does Conversational SEO Work?",
    "authors": [
      "Haritz Puerto",
      "Martin Gubri",
      "Tommaso Green",
      "Seong Joon Oh",
      "Sangdoo Yun"
    ],
    "abstract": "Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not know whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are not only largely ineffective but also frequently have a negative impact on document ranking, which is opposite to what is expected. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at https://github.com/parameterlab/c-seo-bench and https://huggingface.co/datasets/parameterlab/c-seo-bench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS Datasets & Benchmarks 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11097v3",
    "published_date": "2025-06-06 10:49:49 UTC",
    "updated_date": "2025-10-20 19:13:09 UTC"
  },
  {
    "arxiv_id": "2506.05971v1",
    "title": "On Measuring Long-Range Interactions in Graph Neural Networks",
    "authors": [
      "Jacob Bamberger",
      "Benjamin Gutteridge",
      "Scott le Roux",
      "Michael M. Bronstein",
      "Xiaowen Dong"
    ],
    "abstract": "Long-range graph tasks -- those dependent on interactions between distant nodes -- are an open problem in graph neural network research. Real-world benchmark tasks, especially the Long Range Graph Benchmark, have become popular for validating the long-range capability of proposed architectures. However, this is an empirical approach that lacks both robustness and theoretical underpinning; a more principled characterization of the long-range problem is required. To bridge this gap, we formalize long-range interactions in graph tasks, introduce a range measure for operators on graphs, and validate it with synthetic experiments. We then leverage our measure to examine commonly used tasks and architectures, and discuss to what extent they are, in fact, long-range. We believe our work advances efforts to define and address the long-range problem on graphs, and that our range measure will aid evaluation of new datasets and architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05971v1",
    "published_date": "2025-06-06 10:48:30 UTC",
    "updated_date": "2025-06-06 10:48:30 UTC"
  },
  {
    "arxiv_id": "2506.05970v2",
    "title": "Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefilling Improves Theory of Mind in Large Language Models",
    "authors": [
      "Kazutoshi Shinoda",
      "Nobukatsu Hojo",
      "Kyosuke Nishida",
      "Yoshihiro Yamazaki",
      "Keita Suzuki",
      "Hiroaki Sugiyama",
      "Kuniko Saito"
    ],
    "abstract": "Recent studies have shown that Theory of Mind (ToM) in large language models (LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on ToM datasets often degrades their generalization, several inference-time methods have been proposed to enhance ToM in LLMs. However, existing inference-time methods for ToM are specialized for inferring beliefs from contexts involving changes in the world state. In this study, we present a new inference-time method for ToM, Shoes-of-Others (SoO) prefilling, which makes fewer assumptions about contexts and is applicable to broader scenarios. SoO prefilling simply specifies the beginning of LLM outputs with ``Let's put ourselves in A's shoes.'', where A denotes the target character's name. We evaluate SoO prefilling on two benchmarks that assess ToM in conversational and narrative contexts without changes in the world state and find that it consistently improves ToM across five categories of mental states. Our analysis suggests that SoO prefilling elicits faithful thoughts, thereby improving the ToM performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EACL 2026 Findings",
    "pdf_url": "https://arxiv.org/pdf/2506.05970v2",
    "published_date": "2025-06-06 10:47:46 UTC",
    "updated_date": "2026-01-09 09:00:57 UTC"
  },
  {
    "arxiv_id": "2506.05968v2",
    "title": "Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning",
    "authors": [
      "Motoki Omura",
      "Kazuki Ota",
      "Takayuki Osa",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "abstract": "For continuous action spaces, actor-critic methods are widely used in online reinforcement learning (RL). However, unlike RL algorithms for discrete actions, which generally model the optimal value function using the Bellman optimality operator, RL algorithms for continuous actions typically model Q-values for the current policy using the Bellman operator. These algorithms for continuous actions rely exclusively on policy updates for improvement, which often results in low sample efficiency. This study examines the effectiveness of incorporating the Bellman optimality operator into actor-critic frameworks. Experiments in a simple environment show that modeling optimal values accelerates learning but leads to overestimation bias. To address this, we propose an annealing approach that gradually transitions from the Bellman optimality operator to the Bellman operator, thereby accelerating learning while mitigating bias. Our method, combined with TD3 and SAC, significantly outperforms existing approaches across various locomotion and manipulation tasks, demonstrating improved performance and robustness to hyperparameters related to optimality. The code for this study is available at https://github.com/motokiomura/annealed-q-learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2025. Source code: https://github.com/motokiomura/annealed-q-learning",
    "pdf_url": "https://arxiv.org/pdf/2506.05968v2",
    "published_date": "2025-06-06 10:46:20 UTC",
    "updated_date": "2025-08-13 08:35:41 UTC"
  },
  {
    "arxiv_id": "2506.05967v1",
    "title": "Preference Learning for AI Alignment: a Causal Perspective",
    "authors": [
      "Katarzyna Kobalczyk",
      "Mihaela van der Schaar"
    ],
    "abstract": "Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we propose to frame this problem in a causal paradigm, providing the rich toolbox of causality to identify the persistent challenges, such as causal misidentification, preference heterogeneity, and confounding due to user-specific factors. Inheriting from the literature of causal inference, we identify key assumptions necessary for reliable generalisation and contrast them with common data collection practices. We illustrate failure modes of naive reward models and demonstrate how causally-inspired approaches can improve model robustness. Finally, we outline desiderata for future research and practices, advocating targeted interventions to address inherent limitations of observational data.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05967v1",
    "published_date": "2025-06-06 10:45:42 UTC",
    "updated_date": "2025-06-06 10:45:42 UTC"
  },
  {
    "arxiv_id": "2506.05952v3",
    "title": "MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation",
    "authors": [
      "Dongjie Fu",
      "Tengjiao Sun",
      "Pengcheng Fang",
      "Xiaohao Cai",
      "Hansung Kim"
    ],
    "abstract": "Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2506.05952v3",
    "published_date": "2025-06-06 10:26:54 UTC",
    "updated_date": "2026-01-13 11:39:54 UTC"
  },
  {
    "arxiv_id": "2506.05947v1",
    "title": "IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems",
    "authors": [
      "Xinjie Zhang",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "abstract": "In emotional support conversations, unclear intentions can lead supporters to employ inappropriate strategies, inadvertently imposing their expectations or solutions on the seeker. Clearly defined intentions are essential for guiding both the supporter's motivations and the overall emotional support process. In this paper, we propose the Intention-centered Emotional Support Conversation (IntentionESC) framework, which defines the possible intentions of supporters in emotional support conversations, identifies key emotional state aspects for inferring these intentions, and maps them to appropriate support strategies. While Large Language Models (LLMs) excel in text generating, they fundamentally operate as probabilistic models trained on extensive datasets, lacking a true understanding of human thought processes and intentions. To address this limitation, we introduce the Intention Centric Chain-of-Thought (ICECoT) mechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional states, inferring intentions, and selecting suitable support strategies, thereby generating more effective emotional support responses. To train the model with ICECoT and integrate expert knowledge, we design an automated annotation pipeline that produces high-quality training data. Furthermore, we develop a comprehensive evaluation scheme to assess emotional support efficacy and conduct extensive experiments to validate our framework. Our data and code are available at https://github.com/43zxj/IntentionESC_ICECoT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2025 findings",
    "pdf_url": "https://arxiv.org/pdf/2506.05947v1",
    "published_date": "2025-06-06 10:14:49 UTC",
    "updated_date": "2025-06-06 10:14:49 UTC"
  },
  {
    "arxiv_id": "2506.05941v1",
    "title": "Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting",
    "authors": [
      "Luka Hobor",
      "Mario Brcic",
      "Lidija Polutnik",
      "Ante Kapetanovic"
    ],
    "abstract": "Accurate forecasting is key for all business planning. When estimated sales are too high, brick-and-mortar retailers may incur higher costs due to unsold inventories, higher labor and storage space costs, etc. On the other hand, when forecasts underestimate the level of sales, firms experience lost sales, shortages, and impact on the reputation of the retailer in their relevant market. Accurate forecasting presents a competitive advantage for companies. It facilitates the achievement of revenue and profit goals and execution of pricing strategy and tactics. In this study, we provide an exhaustive assessment of the forecasting models applied to a high-resolution brick-and-mortar retail dataset. Our forecasting framework addresses the problems found in retail environments, including intermittent demand, missing values, and frequent product turnover. We compare tree-based ensembles (such as XGBoost and LightGBM) and state-of-the-art neural network architectures (including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various experimental settings. Our results show that localized modeling strategies especially those using tree-based models on individual groups with non-imputed data, consistently deliver superior forecasting accuracy and computational efficiency. In contrast, neural models benefit from advanced imputation methods, yet still fall short in handling the irregularities typical of physical retail data. These results further practical understanding for model selection in retail environment and highlight the significance of data preprocessing to improve forecast performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 total pages, 10 pages article, 10 pages appendix, 3 figures, 24 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05941v1",
    "published_date": "2025-06-06 10:08:17 UTC",
    "updated_date": "2025-06-06 10:08:17 UTC"
  },
  {
    "arxiv_id": "2506.05937v1",
    "title": "Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution",
    "authors": [
      "Charmaine Barker",
      "Daniel Bethell",
      "Simos Gerasimou"
    ],
    "abstract": "Reliability of deep learning models is critical for deployment in high-stakes applications, where out-of-distribution or adversarial inputs may lead to detrimental outcomes. Evidential Deep Learning, an efficient paradigm for uncertainty quantification, models predictions as Dirichlet distributions of a single forward pass. However, EDL is particularly vulnerable to adversarially perturbed inputs, making overconfident errors. Conflict-aware Evidential Deep Learning (C-EDL) is a lightweight post-hoc uncertainty quantification approach that mitigates these issues, enhancing adversarial and OOD robustness without retraining. C-EDL generates diverse, task-preserving transformations per input and quantifies representational disagreement to calibrate uncertainty estimates when needed. C-EDL's conflict-aware prediction adjustment improves detection of OOD and adversarial inputs, maintaining high in-distribution accuracy and low computational overhead. Our experimental evaluation shows that C-EDL significantly outperforms state-of-the-art EDL variants and competitive baselines, achieving substantial reductions in coverage for OOD data (up to 55%) and adversarial data (up to 90%), across a range of datasets, attack types, and uncertainty metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05937v1",
    "published_date": "2025-06-06 10:06:23 UTC",
    "updated_date": "2025-06-06 10:06:23 UTC"
  },
  {
    "arxiv_id": "2506.05936v1",
    "title": "DynamicMind: A Tri-Mode Thinking System for Large Language Models",
    "authors": [
      "Wei Li",
      "Yanbin Wei",
      "Qiushi Huang",
      "Jiangyue Yan",
      "Yang Chen",
      "James T. Kwok",
      "Yu Zhang"
    ],
    "abstract": "Modern large language models (LLMs) often struggle to dynamically adapt their reasoning depth to varying task complexities, leading to suboptimal performance or inefficient resource utilization. To address this, we introduce DynamicMind, a novel tri-mode thinking system. DynamicMind empowers LLMs to autonomously select between Fast, Normal, and Slow thinking modes for zero-shot question answering (ZSQA) tasks through cognitive-inspired prompt engineering. Our framework's core innovations include: (1) expanding the established dual-process framework of fast and slow thinking into a tri-mode thinking system involving a normal thinking mode to preserve the intrinsic capabilities of LLM; (2) proposing the Thinking Density metric, which aligns computational resource allocation with problem complexity; and (3) developing the Thinking Mode Capacity (TMC) dataset and a lightweight Mind Router to predict the optimal thinking mode. Extensive experiments across diverse mathematical, commonsense, and scientific QA benchmarks demonstrate that DynamicMind achieves superior ZSQA capabilities while establishing an effective trade-off between performance and computational efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05936v1",
    "published_date": "2025-06-06 10:02:13 UTC",
    "updated_date": "2025-06-06 10:02:13 UTC"
  },
  {
    "arxiv_id": "2506.05934v1",
    "title": "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing",
    "authors": [
      "Yixuan Zhu",
      "Haolin Wang",
      "Shilin Ma",
      "Wenliang Zhao",
      "Yansong Tang",
      "Lei Chen",
      "Jie Zhou"
    ],
    "abstract": "Recent advancements in diffusion frameworks have significantly enhanced video editing, achieving high fidelity and strong alignment with textual prompts. However, conventional approaches using image diffusion models fall short in handling video dynamics, particularly for challenging temporal edits like motion adjustments. While current video diffusion models produce high-quality results, adapting them for efficient editing remains difficult due to the heavy computational demands that prevent the direct application of previous image editing techniques. To overcome these limitations, we introduce FADE, a training-free yet highly effective video editing approach that fully leverages the inherent priors from pre-trained video diffusion models via frequency-aware factorization. Rather than simply using these models, we first analyze the attention patterns within the video model to reveal how video priors are distributed across different components. Building on these insights, we propose a factorization strategy to optimize each component's specialized role. Furthermore, we devise spectrum-guided modulation to refine the sampling trajectory with frequency domain cues, preventing information leakage and supporting efficient, versatile edits while preserving the basic spatial and temporal structure. Extensive experiments on real-world videos demonstrate that our method consistently delivers high-quality, realistic and temporally coherent editing results both qualitatively and quantitatively. Code is available at https://github.com/EternalEvan/FADE .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05934v1",
    "published_date": "2025-06-06 10:00:39 UTC",
    "updated_date": "2025-06-06 10:00:39 UTC"
  },
  {
    "arxiv_id": "2506.05928v2",
    "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Jie Cao",
      "Tianwei Lin",
      "Bo Yuan",
      "Rolan Yan",
      "Hongyang He",
      "Wenqiao Zhang",
      "Juncheng Li",
      "Dongping Zhang",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ \\emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a \\emph{heterogeneous} \\textbf{Mixture-of-Adapters (MoA)} approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: \\textbf{(i)} \\textit{Soft MoA} achieves fine-grained integration by performing a weighted fusion of all expert outputs; \\textbf{(ii)} \\textit{Sparse MoA} activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05928v2",
    "published_date": "2025-06-06 09:54:19 UTC",
    "updated_date": "2026-01-19 07:51:01 UTC"
  },
  {
    "arxiv_id": "2506.05927v1",
    "title": "LengClaro2023: A Dataset of Administrative Texts in Spanish with Plain Language adaptations",
    "authors": [
      "Beln Agera-Marco",
      "Itziar Gonzalez-Dios"
    ],
    "abstract": "In this work, we present LengClaro2023, a dataset of legal-administrative texts in Spanish. Based on the most frequently used procedures from the Spanish Social Security website, we have created for each text two simplified equivalents. The first version follows the recommendations provided by arText claro. The second version incorporates additional recommendations from plain language guidelines to explore further potential improvements in the system. The linguistic resource created in this work can be used for evaluating automatic text simplification (ATS) systems in Spanish.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In this report, we present a part of the master thesis written by Beln Agera Marco in order to obtain the B.S. Language Analysis and Processing at the University of the Basque Country (UPV/EHU), supervised by Itziar Gonzalez-Dios",
    "pdf_url": "https://arxiv.org/pdf/2506.05927v1",
    "published_date": "2025-06-06 09:52:52 UTC",
    "updated_date": "2025-06-06 09:52:52 UTC"
  },
  {
    "arxiv_id": "2506.05925v2",
    "title": "Small Models, Big Support: A Local LLM Framework for Educator-Centric Content Creation and Assessment with RAG and CAG",
    "authors": [
      "Zarreen Reza",
      "Alexander Mazur",
      "Michael T. Dugdale",
      "Robin Ray-Chaudhuri"
    ],
    "abstract": "While Large Language Models (LLMs) are increasingly applied in student-facing educational tools, their potential to directly support educators through locally deployable and customizable solutions remains underexplored. Many existing approaches rely on proprietary, cloud-based systems that raise significant cost, privacy, and control concerns for educational institutions. To address these barriers, we introduce an end-to-end, open-source framework that empowers educators using small (3B-7B parameter), locally deployable LLMs. Our system is designed for comprehensive teacher support, including customized teaching material generation and AI-assisted assessment. The framework synergistically combines Retrieval-Augmented Generation (RAG) and Context-Augmented Generation (CAG) to produce factually accurate, pedagogically-styled content. A core feature is an interactive refinement loop, a teacher-in-the-loop mechanism that ensures educator agency and precise alignment of the final output. To enhance reliability and safety, an auxiliary verifier LLM inspects all generated content. We validate our framework through a rigorous evaluation of its content generation capabilities and report on a successful technical deployment in a college physics course, which confirms its feasibility on standard institutional hardware. Our findings demonstrate that carefully engineered, self-hosted systems built on small LLMs can provide robust, affordable, and private support for educators, achieving practical utility comparable to much larger models for targeted instructional tasks. This work presents a practical blueprint for the development of sovereign AI tools tailored to the real-world needs of educational institutions.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05925v2",
    "published_date": "2025-06-06 09:47:03 UTC",
    "updated_date": "2025-11-16 09:52:24 UTC"
  },
  {
    "arxiv_id": "2506.05917v1",
    "title": "Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness",
    "authors": [
      "Steven Landgraf",
      "Markus Hillemann",
      "Markus Ulrich"
    ],
    "abstract": "Semantic segmentation is critical for scene understanding but demands costly pixel-wise annotations, attracting increasing attention to semi-supervised approaches to leverage abundant unlabeled data. While semi-supervised segmentation is often promoted as a path toward scalable, real-world deployment, it is astonishing that current evaluation protocols exclusively focus on segmentation accuracy, entirely overlooking reliability and robustness. These qualities, which ensure consistent performance under diverse conditions (robustness) and well-calibrated model confidences as well as meaningful uncertainties (reliability), are essential for safety-critical applications like autonomous driving, where models must handle unpredictable environments and avoid sudden failures at all costs. To address this gap, we introduce the Reliable Segmentation Score (RSS), a novel metric that combines predictive accuracy, calibration, and uncertainty quality measures via a harmonic mean. RSS penalizes deficiencies in any of its components, providing an easy and intuitive way of holistically judging segmentation models. Comprehensive evaluations of UniMatchV2 against its predecessor and a supervised baseline show that semi-supervised methods often trade reliability for accuracy. While out-of-domain evaluations demonstrate UniMatchV2's robustness, they further expose persistent reliability shortcomings. We advocate for a shift in evaluation protocols toward more holistic metrics like RSS to better align semi-supervised learning research with real-world deployment needs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05917v1",
    "published_date": "2025-06-06 09:37:45 UTC",
    "updated_date": "2025-06-06 09:37:45 UTC"
  },
  {
    "arxiv_id": "2506.05904v1",
    "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
    "authors": [
      "Yichi Zhang",
      "Xin Luna Dong",
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Anuj Kumar",
      "Babak Damavandi",
      "Joyce Chai",
      "Seungwhan Moon"
    ],
    "abstract": "Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in \\dataset, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks. Project page: https://pro-assist.github.io/",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05904v1",
    "published_date": "2025-06-06 09:23:29 UTC",
    "updated_date": "2025-06-06 09:23:29 UTC"
  },
  {
    "arxiv_id": "2506.05901v2",
    "title": "Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router",
    "authors": [
      "Chenyang Shao",
      "Xinyang Liu",
      "Yutang Lin",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "Chain-of-thought has been proven essential for enhancing the complex reasoning abilities of Large Language Models (LLMs), but it also leads to high computational costs. Recent advances have explored the method to route queries among multiple models and proved it as a promising approach. However, previous works directly operate at the task level, i.e., assigning user queries to suitable LLMs, which does not allow hybrid LLMs to truly collaborate on finer-grained sub-tasks. Collaboration at the level of intermediate reasoning steps (thoughts) could enable more efficient coordination, but it also poses significant challenges for router scheduling, placing immense demands on the quality of task decomposition and the precision of the router. To address this, we propose R2-Reasoner, a novel framework centered around a Reinforced Model Router designed to efficiently scale LLM reasoning. This router orchestrates collaboration across nine heterogeneous models, whose parameter scales range from less than 1B to hundreds of billions, by first breaking down a complex query into subtasks with a decomposer, and then assigning each subtask to the optimal model with a subtask allocator, balancing performance with cost. Training this router involves a two-stage alternating process for the decomposer and the allocator, integrating supervised fine-tuning with reinforcement learning to enable effective self-supervised refinement. Extensive experiments across six challenging reasoning benchmarks demonstrate that R2-Reasoner reduces API costs by 84.46% compared with state-of-the-art baselines while maintaining competitive reasoning accuracy. Our framework paves the way for the development of more scalable and efficient reasoning systems. Our code is open-source at https://anonymous.4open.science/r/R2_Reasoner.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05901v2",
    "published_date": "2025-06-06 09:18:56 UTC",
    "updated_date": "2025-12-04 03:52:25 UTC"
  },
  {
    "arxiv_id": "2506.05899v1",
    "title": "WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS Prediction",
    "authors": [
      "Jakaria Islam Emon",
      "Kazi Tamanna Alam",
      "Md. Abu Salek"
    ],
    "abstract": "Mean Opinion Score (MOS) prediction for text to music systems requires evaluating both overall musical quality and text prompt alignment. This paper introduces WhisQ, a multimodal architecture that addresses this dual-assessment challenge through sequence level co-attention and optimal transport regularization. WhisQ employs the Whisper Base pretrained model for temporal audio encoding and Qwen 3, a 0.6B Small Language Model (SLM), for text encoding, with both maintaining sequence structure for fine grained cross-modal modeling. The architecture features specialized prediction pathways: OMQ is predicted from pooled audio embeddings, while TA leverages bidirectional sequence co-attention between audio and text. Sinkhorn optimal transport loss further enforce semantic alignment in the shared embedding space. On the MusicEval Track-1 dataset, WhisQ achieves substantial improvements over the baseline: 7% improvement in Spearman correlation for OMQ and 14% for TA. Ablation studies reveal that optimal transport regularization provides the largest performance gain (10% SRCC improvement), demonstrating the importance of explicit cross-modal alignment for text-to-music evaluation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "3 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.05899v1",
    "published_date": "2025-06-06 09:12:51 UTC",
    "updated_date": "2025-06-06 09:12:51 UTC"
  },
  {
    "arxiv_id": "2506.05896v1",
    "title": "Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM",
    "authors": [
      "Chongshang Yan",
      "Jiaxuan He",
      "Delun Li",
      "Yi Yang",
      "Wenjie Song"
    ],
    "abstract": "The zero-shot object navigation (ZSON) in unknown open-ended environments coupled with semantically novel target often suffers from the significant decline in performance due to the neglect of high-dimensional implicit scene information and the long-range target searching task. To address this, we proposed an active object navigation framework with Environmental Attributes Map (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success rate and efficiency. EAM is constructed by reasoning observed environments with SBERT and predicting unobserved ones with Diffusion, utilizing human space regularities that underlie object-room correlations and area adjacencies. MHR is inspired by EAM to perform frontier exploration decision-making, avoiding the circuitous trajectories in long-range scenarios to improve path efficiency. Experimental results demonstrate that the EAM module achieves 64.5\\% scene mapping accuracy on MP3D dataset, while the navigation task attains SPLs of 28.4\\% and 26.3\\% on HM3D and MP3D benchmarks respectively - representing absolute improvements of 21.4\\% and 46.0\\% over baseline methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "16 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05896v1",
    "published_date": "2025-06-06 09:08:40 UTC",
    "updated_date": "2025-06-06 09:08:40 UTC"
  },
  {
    "arxiv_id": "2506.08041v1",
    "title": "The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students",
    "authors": [
      "Siddharth Siddharth",
      "Brainerd Prince",
      "Amol Harsh",
      "Shreyas Ramachandran"
    ],
    "abstract": "This work presents a novel course titled The World of AI designed for first-year undergraduate engineering students with little to no prior exposure to AI. The central problem addressed by this course is that engineering students often lack foundational knowledge of AI and its broader societal implications at the outset of their academic journeys. We believe the way to address this gap is to design and deliver an interdisciplinary course that can a) be accessed by first-year undergraduate engineering students across any domain, b) enable them to understand the basic workings of AI systems sans mathematics, and c) make them appreciate AI's far-reaching implications on our lives. The course was divided into three modules co-delivered by faculty from both engineering and humanities. The planetary module explored AI's dual role as both a catalyst for sustainability and a contributor to environmental challenges. The societal impact module focused on AI biases and concerns around privacy and fairness. Lastly, the workplace module highlighted AI-driven job displacement, emphasizing the importance of adaptation. The novelty of this course lies in its interdisciplinary curriculum design and pedagogical approach, which combines technical instruction with societal discourse. Results revealed that students' comprehension of AI challenges improved across diverse metrics like (a) increased awareness of AI's environmental impact, and (b) efficient corrective solutions for AI fairness. Furthermore, it also indicated the evolution in students' perception of AI's transformative impact on our lives.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for publication at AIED 2025 in the late-breaking work track",
    "pdf_url": "https://arxiv.org/pdf/2506.08041v1",
    "published_date": "2025-06-06 09:04:56 UTC",
    "updated_date": "2025-06-06 09:04:56 UTC"
  },
  {
    "arxiv_id": "2506.05887v2",
    "title": "A three-Level Framework for LLM-Enhanced eXplainable AI: From technical explanations to natural language",
    "authors": [
      "Marilyn Bello",
      "Rafael Bello",
      "Maria-Matilde Garca",
      "Ann Now",
      "Ivn Sevillano-Garca",
      "Francisco Herrera"
    ],
    "abstract": "The growing application of artificial intelligence in sensitive domains has intensified the demand for systems that are not only accurate but also explainable and trustworthy. Although explainable AI (XAI) methods have proliferated, many do not consider the diverse audiences that interact with AI systems: from developers and domain experts to end-users and society. This paper addresses how trust in AI is influenced by the design and delivery of explanations and proposes a multilevel framework that aligns explanations with the epistemic, contextual, and ethical expectations of different stakeholders. The framework consists of three layers: algorithmic and domain-based, human-centered, and social explainability, with Large Language Models serving as crucial mediators that transform technical outputs of AI explanations into accessible, contextual narratives across all levels. We show how LLMs enable dynamic, conversational explanations that bridge the gap between complex model behavior and human understanding, facilitating interactive dialogue and enhancing societal transparency. Through comprehensive case studies, we show how this LLM-enhanced approach achieves technical fidelity, user engagement, and societal accountability, reframing XAI as a dynamic, trust-building process that leverages natural language capabilities to democratize AI explainability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05887v2",
    "published_date": "2025-06-06 08:54:41 UTC",
    "updated_date": "2026-01-03 10:17:41 UTC"
  },
  {
    "arxiv_id": "2506.11096v1",
    "title": "Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting",
    "authors": [
      "Guillaume Wisniewski",
      "Sverine Guillaume",
      "Clara Rosina Fernndez"
    ],
    "abstract": "Pretrained speech representations like wav2vec2 and HuBERT exhibit strong anisotropy, leading to high similarity between random embeddings. While widely observed, the impact of this property on downstream tasks remains unclear. This work evaluates anisotropy in keyword spotting for computational documentary linguistics. Using Dynamic Time Warping, we show that despite anisotropy, wav2vec2 similarity measures effectively identify words without transcription. Our results highlight the robustness of these representations, which capture phonetic structures and generalize across speakers. Our results underscore the importance of pretraining in learning rich and invariant speech representations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11096v1",
    "published_date": "2025-06-06 08:52:56 UTC",
    "updated_date": "2025-06-06 08:52:56 UTC"
  },
  {
    "arxiv_id": "2506.05883v1",
    "title": "HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios",
    "authors": [
      "Daming Wang",
      "Yuhao Song",
      "Zijian He",
      "Kangliang Chen",
      "Xing Pan",
      "Lu Deng",
      "Weihao Gu"
    ],
    "abstract": "We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving framework that implements the slow branch of a cognitively inspired fast-slow architecture. A fast controller outputs low-level steering, throttle, and brake commands, while a slow planner-a large vision-language model-generates high-level intents such as \"yield to pedestrian\" or \"merge after the truck\" without compromising latency. HMVLM introduces three upgrades: (1) selective five-view prompting with an embedded 4s history of ego kinematics, (2) multi-stage chain-of-thought (CoT) prompting that enforces a Scene Understanding -> Driving Decision -> Trajectory Inference reasoning flow, and (3) spline-based trajectory post-processing that removes late-stage jitter and sharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to achieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025 Waymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public baseline by 2.77%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WOD Vision-based End-to-End Driving Challenge",
    "pdf_url": "https://arxiv.org/pdf/2506.05883v1",
    "published_date": "2025-06-06 08:51:06 UTC",
    "updated_date": "2025-06-06 08:51:06 UTC"
  },
  {
    "arxiv_id": "2506.05876v2",
    "title": "Information Bargaining: Bilateral Commitment in Bayesian Persuasion",
    "authors": [
      "Yue Lin",
      "Shuhui Zhu",
      "William A Cunningham",
      "Wenhao Li",
      "Pascal Poupart",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ],
    "abstract": "Bayesian persuasion, an extension of cheap-talk communication, involves an informed sender committing to a signaling scheme to influence a receiver's actions. Compared to cheap talk, this sender's commitment enables the receiver to verify the incentive compatibility of signals beforehand, facilitating cooperation. While effective in one-shot scenarios, Bayesian persuasion faces computational complexity (NP-hardness) when extended to long-term interactions, where the receiver may adopt dynamic strategies conditional on past outcomes and future expectations. To address this complexity, we introduce the bargaining perspective, which allows: (1) a unified framework and well-structured solution concept for long-term persuasion, with desirable properties such as fairness and Pareto efficiency; (2) a clear distinction between two previously conflated advantages: the sender's informational advantage and first-proposer advantage. With only modest modifications to the standard setting, this perspective makes explicit the common knowledge of the game structure and grants the receiver comparable commitment capabilities, thereby reinterpreting classic one-sided persuasion as a balanced information bargaining framework. The framework is validated through a two-stage validation-and-inference paradigm: We first demonstrate that GPT-o3 and DeepSeek-R1, out of publicly available LLMs, reliably handle standard tasks; We then apply them to persuasion scenarios to test that the outcomes align with what our information-bargaining framework suggests. All code, results, and terminal logs are publicly available at github.com/YueLin301/InformationBargaining.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05876v2",
    "published_date": "2025-06-06 08:42:34 UTC",
    "updated_date": "2025-06-09 05:43:23 UTC"
  },
  {
    "arxiv_id": "2506.05873v1",
    "title": "Research on Personalized Financial Product Recommendation by Integrating Large Language Models and Graph Neural Networks",
    "authors": [
      "Yushang Zhao",
      "Yike Peng",
      "Dannier Li",
      "Yuxin Yang",
      "Chengrui Zhou",
      "Jing Dong"
    ],
    "abstract": "With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05873v1",
    "published_date": "2025-06-06 08:41:33 UTC",
    "updated_date": "2025-06-06 08:41:33 UTC"
  },
  {
    "arxiv_id": "2506.05869v1",
    "title": "Loss Functions for Predictor-based Neural Architecture Search",
    "authors": [
      "Han Ji",
      "Yuqi Feng",
      "Jiahao Fan",
      "Yanan Sun"
    ],
    "abstract": "Evaluation is a critical but costly procedure in neural architecture search (NAS). Performance predictors have been widely adopted to reduce evaluation costs by directly estimating architecture performance. The effectiveness of predictors is heavily influenced by the choice of loss functions. While traditional predictors employ regression loss functions to evaluate the absolute accuracy of architectures, recent approaches have explored various ranking-based loss functions, such as pairwise and listwise ranking losses, to focus on the ranking of architecture performance. Despite their success in NAS, the effectiveness and characteristics of these loss functions have not been thoroughly investigated. In this paper, we conduct the first comprehensive study on loss functions in performance predictors, categorizing them into three main types: regression, ranking, and weighted loss functions. Specifically, we assess eight loss functions using a range of NAS-relevant metrics on 13 tasks across five search spaces. Our results reveal that specific categories of loss functions can be effectively combined to enhance predictor-based NAS. Furthermore, our findings could provide practical guidance for selecting appropriate loss functions for various tasks. We hope this work provides meaningful insights to guide the development of loss functions for predictor-based methods in the NAS community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05869v1",
    "published_date": "2025-06-06 08:36:46 UTC",
    "updated_date": "2025-06-06 08:36:46 UTC"
  },
  {
    "arxiv_id": "2506.05856v1",
    "title": "Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025",
    "authors": [
      "Yuqian Fu",
      "Runze Wang",
      "Yanwei Fu",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "In this report, we present a cross-view multi-modal object segmentation approach for the object correspondence task in the Ego-Exo4D Correspondence Challenges 2025. Given object queries from one perspective (e.g., ego view), the goal is to predict the corresponding object masks in another perspective (e.g., exo view). To tackle this task, we propose a multimodal condition fusion module that enhances object localization by leveraging both visual masks and textual descriptions as segmentation conditions. Furthermore, to address the visual domain gap between ego and exo views, we introduce a cross-view object alignment module that enforces object-level consistency across perspectives, thereby improving the model's robustness to viewpoint changes. Our proposed method ranked second on the leaderboard of the large-scale Ego-Exo4D object correspondence benchmark. Code will be made available at https://github.com/lovelyqian/ObjectRelator.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The 2nd Price Award of EgoExo4D Relations, Second Joint EgoVis Workshop with CVPR2025, technical report paper is accepted by CVPRW 25",
    "pdf_url": "https://arxiv.org/pdf/2506.05856v1",
    "published_date": "2025-06-06 08:23:39 UTC",
    "updated_date": "2025-06-06 08:23:39 UTC"
  },
  {
    "arxiv_id": "2506.11095v2",
    "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity",
    "authors": [
      "Manuel D. S. Hopp",
      "Vincent Labatut",
      "Arthur Amalvy",
      "Richard Dufour",
      "Hannah Stone",
      "Hayley Jach",
      "Kou Murayama"
    ],
    "abstract": "Reader curiosity, the drive to seek information, is crucial for textual engagement, yet remains relatively underexplored in NLP. Building on Loewenstein's Information Gap Theory, we introduce a framework that models reader curiosity by quantifying semantic information gaps within a text's semantic structure. Our approach leverages BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology (connected components, cycles, voids) of a dynamic semantic network derived from text segments, treating these features as proxies for information gaps. To empirically evaluate this pipeline, we collect reader curiosity ratings from participants (n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the topological features from our pipeline as independent variables to predict these ratings, and experimentally show that they significantly improve curiosity prediction compared to a baseline model (73% vs. 30% explained deviance), validating our approach. This pipeline offers a new computational method for analyzing text structure and its relation to reader engagement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Original paper with an improved and extended appendix",
    "pdf_url": "https://arxiv.org/pdf/2506.11095v2",
    "published_date": "2025-06-06 08:11:10 UTC",
    "updated_date": "2025-09-11 15:49:22 UTC"
  },
  {
    "arxiv_id": "2506.05851v1",
    "title": "DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection",
    "authors": [
      "Marcel Klemt",
      "Carlotta Segna",
      "Anna Rohrbach"
    ],
    "abstract": "Generative AI advances rapidly, allowing the creation of very realistic manipulated video and audio. This progress presents a significant security and ethical threat, as malicious users can exploit DeepFake techniques to spread misinformation. Recent DeepFake detection approaches explore the multimodal (audio-video) threat scenario. In particular, there is a lack of reproducibility and critical issues with existing datasets - such as the recently uncovered silence shortcut in the widely used FakeAVCeleb dataset. Considering the importance of this topic, we aim to gain a deeper understanding of the key issues affecting benchmarking in audio-video DeepFake detection. We examine these challenges through the lens of the three core benchmarking pillars: datasets, detection methods, and evaluation protocols. To address these issues, we spotlight the recent DeepSpeak v1 dataset and are the first to propose an evaluation protocol and benchmark it using SOTA models. We introduce SImple Multimodal BAseline (SIMBA), a competitive yet minimalistic approach that enables the exploration of diverse design choices. We also deepen insights into the issue of audio shortcuts and present a promising mitigation strategy. Finally, we analyze and enhance the evaluation scheme on the widely used FakeAVCeleb dataset. Our findings offer a way forward in the complex area of audio-video DeepFake detection.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05851v1",
    "published_date": "2025-06-06 08:10:54 UTC",
    "updated_date": "2025-06-06 08:10:54 UTC"
  },
  {
    "arxiv_id": "2506.05850v2",
    "title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models",
    "authors": [
      "Cheonbok Park",
      "Jeonghoon Kim",
      "Joosung Lee",
      "Sanghwan Bae",
      "Jaegul Choo",
      "Kang Min Yoo"
    ],
    "abstract": "We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which the chain-of-thought (CoT) of a multilingual language model reverts to its dominant pre-training language even when the prompt is expressed in a different language. Recent large language models (LLMs) with reinforcement learning with verifiable reward (RLVR) have achieved strong logical reasoning performances by exposing their intermediate reasoning traces, giving rise to large reasoning models (LRMs). However, the mechanism behind multilingual reasoning in LRMs is not yet fully explored. To investigate the issue, we fine-tune multilingual LRMs with Group-Relative Policy Optimization (GRPO) on translated versions of the GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese, Korean, and Ukrainian. During training, we monitor both task accuracy and language consistency of the reasoning chains. Our experiments reveal three key findings: (i) GRPO rapidly amplifies pre-training language imbalances, leading to the erosion of low-resource languages within just a few hundred updates; (ii) language consistency reward mitigates this drift but does so at the expense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting language collapse is severely damaging and largely irreversible, as subsequent fine-tuning struggles to steer the model back toward its original target-language reasoning capabilities. Together, these findings point to a remarkable conclusion: \\textit{not all languages are trained equally for reasoning}. Furthermore, our paper sheds light on the roles of reward shaping, data difficulty, and pre-training priors in eliciting multilingual reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2506.05850v2",
    "published_date": "2025-06-06 08:08:48 UTC",
    "updated_date": "2025-06-09 11:55:27 UTC"
  },
  {
    "arxiv_id": "2507.21069v2",
    "title": "GAITEX: Human motion dataset of impaired gait and rehabilitation exercises using inertial and optical sensors",
    "authors": [
      "Andreas Spilz",
      "Heiko Oppel",
      "Jochen Werner",
      "Kathrin Stucke-Straub",
      "Felix Capanni",
      "Michael Munz"
    ],
    "abstract": "Wearable inertial measurement units (IMUs) provide a cost-effective approach to assessing human movement in clinical and everyday environments. However, developing the associated classification models for robust assessment of physiotherapeutic exercise and gait analysis requires large, diverse datasets that are costly and time-consuming to collect. We present a multimodal dataset of physiotherapeutic and gait-related exercises, including correct and clinically relevant variants, recorded from 19 healthy subjects using synchronized IMUs and optical marker-based motion capture (MoCap). It contains data from nine IMUs and 68 markers tracking full-body kinematics. Four markers per IMU allow direct comparison between IMU- and MoCap-derived orientations. We additionally provide processed IMU orientations aligned to common segment coordinate systems, subject-specific OpenSim models, inverse kinematics outputs, and visualization tools for IMU-derived orientations. The dataset is fully annotated with movement quality ratings and timestamped segmentations. It supports various machine learning tasks such as exercise evaluation, gait classification, temporal segmentation, and biomechanical parameter estimation. Code for postprocessing, alignment, inverse kinematics, and technical validation is provided to promote reproducibility.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21069v2",
    "published_date": "2025-06-06 08:08:18 UTC",
    "updated_date": "2025-11-07 08:40:29 UTC"
  },
  {
    "arxiv_id": "2506.05834v1",
    "title": "Regional, Lattice and Logical Representations of Neural Networks",
    "authors": [
      "Sandro Preto",
      "Marcelo Finger"
    ],
    "abstract": "A possible path to the interpretability of neural networks is to (approximately) represent them in the regional format of piecewise linear functions, where regions of inputs are associated to linear functions computing the network outputs. We present an algorithm for the translation of feedforward neural networks with ReLU activation functions in hidden layers and truncated identity activation functions in the output layer. We also empirically investigate the complexity of regional representations outputted by our method for neural networks with varying sizes. Lattice and logical representations of neural networks are straightforward from regional representations as long as they satisfy a specific property. So we empirically investigate to what extent the translations by our algorithm satisfy such property.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings LSFA 2024, arXiv:2506.05219",
    "pdf_url": "https://arxiv.org/pdf/2506.05834v1",
    "published_date": "2025-06-06 07:58:09 UTC",
    "updated_date": "2025-06-06 07:58:09 UTC"
  },
  {
    "arxiv_id": "2506.05833v1",
    "title": "Fuzzy Lattice-based Description Logic",
    "authors": [
      "Yiwen Ding",
      "Krishna Manoorkar"
    ],
    "abstract": "Recently, description logic LE-ALC was introduced for reasoning in the semantic environment of enriched formal contexts, and a polynomial-time tableaux algorithm was developed to check the consistency of knowledge bases with acyclic TBoxes. In this work, we introduce a fuzzy generalization of LE-ALC  called  LE-FALC which provides a description logic counterpart of many-valued normal non-distributive logic a.k.a. many-valued LE-logic. This description logic can be used to represent and reason about knowledge in the formal framework  of fuzzy formal contexts and fuzzy formal concepts. We provide a tableaux algorithm that provides a complete and sound polynomial-time decision procedure to check the consistency of  LE-FALC  ABoxes. As a result, we also obtain an exponential-time decision procedure for checking the consistency of  LE-FALC  with acyclic TBoxes by unraveling.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings LSFA 2024, arXiv:2506.05219",
    "pdf_url": "https://arxiv.org/pdf/2506.05833v1",
    "published_date": "2025-06-06 07:57:54 UTC",
    "updated_date": "2025-06-06 07:57:54 UTC"
  },
  {
    "arxiv_id": "2506.05831v3",
    "title": "Heartcare Suite: A Unified Multimodal ECG Suite for Dual Signal-Image Modeling and Understanding",
    "authors": [
      "Yihan Xie",
      "Sijing Li",
      "Tianwei Lin",
      "Zhuonan Wang",
      "Chenglin Yang",
      "Yu Zhong",
      "Wenjie Yan",
      "Wenqiao Zhang",
      "Xiaogang Guo",
      "Jun Xiao",
      "Yueting Zhuang",
      "Beng Chin Ooi"
    ],
    "abstract": "Although electrocardiograms (ECG) play a dominant role in cardiovascular diagnosis and treatment, their intrinsic data forms and representational patterns pose significant challenges for medical multimodal large language models (Med-MLLMs) in achieving cross-modal semantic alignment. To address this gap, we propose Heartcare Suite, a unified ECG suite designed for dual signal-image modeling and understanding. (i) Heartcare-400K: We build a finegrained ECG instruction dataset on top of our data pipeline engine--HeartAgent--by integrating 12,170 high quality clinical ECG reports from top hospitals with open-source data; (ii) Heartcare-Bench: a systematic benchmark assessing performance of models in multi-perspective ECG understanding and cross-modal generalization, providing guidance for optimizing ECG comprehension models; (iii) HeartcareGPT: built upon a structure-aware discrete tokenizer Beat, we propose the DSPA (Dual Stream Projection Alignment) paradigm--a dual encoder projection alignment mechanism enabling joint optimizing and modeling native ECG signal-image within a shared feature space. Heartcare achieves consistent improvements across diverse ECG understanding tasks, validating both the effectiveness of the unified modeling paradigm and the necessity of a high-quality data pipeline, and establishing a methodological foundation for extending Med-MLLMs toward physiological signal domains. Our project is available at https://github.com/DCDmllm/Heartcare-Suite .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05831v3",
    "published_date": "2025-06-06 07:56:41 UTC",
    "updated_date": "2025-12-23 13:17:55 UTC"
  },
  {
    "arxiv_id": "2506.05821v3",
    "title": "FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks",
    "authors": [
      "Quansong He",
      "Xiangde Min",
      "Kaishen Wang",
      "Tao He"
    ],
    "abstract": "Medical image segmentation is a critical task in computer vision, with UNet serving as a milestone architecture. The typical component of UNet family is the skip connection, however, their skip connections face two significant limitations: (1) they lack effective interaction between features at different scales, and (2) they rely on simple concatenation or addition operations, which constrain efficient information integration. While recent improvements to UNet have focused on enhancing encoder and decoder capabilities, these limitations remain overlooked. To overcome these challenges, we propose a novel multi-scale feature fusion method that reimagines the UNet decoding process as solving an initial value problem (IVP), treating skip connections as discrete nodes. By leveraging principles from the linear multistep method, we propose an adaptive ordinary differential equation method to enable effective multi-scale feature fusion. Our approach is independent of the encoder and decoder architectures, making it adaptable to various U-Net-like networks. Experiments on ACDC, KiTS2023, MSD brain tumor, and ISIC2017/2018 skin lesion segmentation datasets demonstrate improved feature utilization, reduced network parameters, and maintained high performance. The code is available at https://github.com/nayutayuki/FuseUNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Updated author information to clarify institutional affiliation. The research was conducted prior to the author joining the University of Maryland",
    "pdf_url": "https://arxiv.org/pdf/2506.05821v3",
    "published_date": "2025-06-06 07:34:06 UTC",
    "updated_date": "2025-10-23 02:47:38 UTC"
  },
  {
    "arxiv_id": "2506.05814v1",
    "title": "Positional Encoding meets Persistent Homology on Graphs",
    "authors": [
      "Yogesh Verma",
      "Amauri H. Souza",
      "Vikas Garg"
    ],
    "abstract": "The local inductive bias of message-passing graph neural networks (GNNs) hampers their ability to exploit key structural information (e.g., connectivity and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged as two promising approaches to mitigate this issue. PE schemes endow GNNs with location-aware features, while PH methods enhance GNNs with multiresolution topological features. However, a rigorous theoretical characterization of the relative merits and shortcomings of PE and PH has remained elusive. We bridge this gap by establishing that neither paradigm is more expressive than the other, providing novel constructions where one approach fails but the other succeeds. Our insights inform the design of a novel learnable method, PiPE (Persistence-informed Positional Encoding), which is provably more expressive than both PH and PE. PiPE demonstrates strong performance across a variety of tasks (e.g., molecule property prediction, graph classification, and out-of-distribution generalization), thereby advancing the frontiers of graph representation learning. Code is available at https://github.com/Aalto-QuML/PIPE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05814v1",
    "published_date": "2025-06-06 07:22:17 UTC",
    "updated_date": "2025-06-06 07:22:17 UTC"
  },
  {
    "arxiv_id": "2506.05810v1",
    "title": "Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction",
    "authors": [
      "Yesheng Zhang",
      "Wenjian Sun",
      "Yuheng Chen",
      "Qingwei Liu",
      "Qi Lin",
      "Rui Zhang",
      "Xu Zhao"
    ],
    "abstract": "Complex interactions among agents present a significant challenge for autonomous driving in real-world scenarios. Recently, a promising approach has emerged, which formulates the interactions of agents as a level-k game framework. It effectively decouples agent policies by hierarchical game levels. However, this framework ignores both the varying driving complexities among agents and the dynamic changes in agent states across game levels, instead treating them uniformly. Consequently, redundant and error-prone computations are introduced into this framework. To tackle the issue, this paper proposes a metric, termed as Trajectory Entropy, to reveal the game status of agents within the level-k game framework. The key insight stems from recognizing the inherit relationship between agent policy uncertainty and the associated driving complexity. Specifically, Trajectory Entropy extracts statistical signals representing uncertainty from the multimodality trajectory prediction results of agents in the game. Then, the signal-to-noise ratio of this signal is utilized to quantify the game status of agents. Based on the proposed Trajectory Entropy, we refine the current level-k game framework through a simple gating mechanism, significantly improving overall accuracy while reducing computational costs. Our method is evaluated on the Waymo and nuPlan datasets, in terms of trajectory prediction, open-loop and closed-loop planning tasks. The results demonstrate the state-of-the-art performance of our method, with precision improved by up to 19.89% for prediction and up to 16.48% for planning.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.05810v1",
    "published_date": "2025-06-06 07:17:55 UTC",
    "updated_date": "2025-06-06 07:17:55 UTC"
  },
  {
    "arxiv_id": "2506.06404v1",
    "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights",
    "authors": [
      "Sooyung Choi",
      "Jaehyeok Lee",
      "Xiaoyuan Yi",
      "Jing Yao",
      "Xing Xie",
      "JinYeong Bak"
    ],
    "abstract": "The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the \"black box\" of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06404v1",
    "published_date": "2025-06-06 07:03:12 UTC",
    "updated_date": "2025-06-06 07:03:12 UTC"
  },
  {
    "arxiv_id": "2506.05780v1",
    "title": "Robust sensor fusion against on-vehicle sensor staleness",
    "authors": [
      "Meng Fan",
      "Yifan Zuo",
      "Patrick Blaes",
      "Harley Montgomery",
      "Subhasis Das"
    ],
    "abstract": "Sensor fusion is crucial for a performant and robust Perception system in autonomous vehicles, but sensor staleness, where data from different sensors arrives with varying delays, poses significant challenges. Temporal misalignment between sensor modalities leads to inconsistent object state estimates, severely degrading the quality of trajectory predictions that are critical for safety. We present a novel and model-agnostic approach to address this problem via (1) a per-point timestamp offset feature (for LiDAR and radar both relative to camera) that enables fine-grained temporal awareness in sensor fusion, and (2) a data augmentation strategy that simulates realistic sensor staleness patterns observed in deployed vehicles. Our method is integrated into a perspective-view detection model that consumes sensor data from multiple LiDARs, radars and cameras. We demonstrate that while a conventional model shows significant regressions when one sensor modality is stale, our approach reaches consistently good performance across both synchronized and stale conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by CVPR 2025 Precognition Workshop",
    "pdf_url": "https://arxiv.org/pdf/2506.05780v1",
    "published_date": "2025-06-06 06:18:54 UTC",
    "updated_date": "2025-06-06 06:18:54 UTC"
  },
  {
    "arxiv_id": "2506.05767v1",
    "title": "dots.llm1 Technical Report",
    "authors": [
      "Bi Huo",
      "Bin Tu",
      "Cheng Qin",
      "Da Zheng",
      "Debing Zhang",
      "Dongjie Zhang",
      "En Li",
      "Fu Guo",
      "Jian Yao",
      "Jie Lou",
      "Junfeng Tian",
      "Li Hu",
      "Ran Zhu",
      "Shengdong Chen",
      "Shuo Liu",
      "Su Guang",
      "Te Wo",
      "Weijun Zhang",
      "Xiaoming Shi",
      "Xinxin Peng",
      "Xing Wu",
      "Yawen Liu",
      "Yuqiu Ji",
      "Ze Wen",
      "Zhenhai Liu",
      "Zichao Li",
      "Zilong Liao"
    ],
    "abstract": "Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints at every one trillion tokens, providing valuable insights into the learning dynamics of large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05767v1",
    "published_date": "2025-06-06 05:51:29 UTC",
    "updated_date": "2025-06-06 05:51:29 UTC"
  },
  {
    "arxiv_id": "2506.11094v2",
    "title": "The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs",
    "authors": [
      "Songyang Liu",
      "Chaozhuo Li",
      "Jiameng Qiu",
      "Xi Zhang",
      "Feiran Huang",
      "Litian Zhang",
      "Yiming Hei",
      "Philip S. Yu"
    ],
    "abstract": "With the rapid advancement of artificial intelligence, Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), including content generation, human-computer interaction, machine translation, and code generation. However, their widespread deployment has also raised significant safety concerns. In particular, LLM-generated content can exhibit unsafe behaviors such as toxicity, bias, or misinformation, especially in adversarial contexts, which has attracted increasing attention from both academia and industry. Although numerous studies have attempted to evaluate these risks, a comprehensive and systematic survey on safety evaluation of LLMs is still lacking. This work aims to fill this gap by presenting a structured overview of recent advances in safety evaluation of LLMs. Specifically, we propose a four-dimensional taxonomy: (i) Why to evaluate, which explores the background of safety evaluation of LLMs, how they differ from general LLMs evaluation, and the significance of such evaluation; (ii) What to evaluate, which examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and related aspects; (iii) Where to evaluate, which summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (iv) How to evaluate, which reviews existing mainstream evaluation methods based on the roles of the evaluators and some evaluation frameworks that integrate the entire evaluation pipeline. Finally, we identify the challenges in safety evaluation of LLMs and propose promising research directions to promote further advancement in this field. We emphasize the necessity of prioritizing safety evaluation to ensure the reliable and responsible deployment of LLMs in real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, preprint",
    "pdf_url": "https://arxiv.org/pdf/2506.11094v2",
    "published_date": "2025-06-06 05:50:50 UTC",
    "updated_date": "2025-10-30 06:22:33 UTC"
  },
  {
    "arxiv_id": "2506.14813v1",
    "title": "Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks",
    "authors": [
      "Yuxuan Jiang",
      "Ziming Zhou",
      "Boyu Xu",
      "Beijie Liu",
      "Runhui Xu",
      "Peng Huang"
    ],
    "abstract": "Training deep learning (DL) models is a complex process, making it prone to silent errors that are challenging to detect and diagnose. This paper presents TRAINCHECK, a framework that takes a proactive checking approach to address silent training errors. TRAINCHECK automatically infers invariants tailored for DL training. It uses these invariants to proactively detect silent errors during the training process while providing debugging help. To evaluate TRAINCHECK, we reproduce 20 real-world silent training errors with diverse root causes. TRAINCHECK successfully detects 18 errors within a single training iteration. It also uncovers 6 unknown bugs in popular training libraries that lead to silent errors.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, to appear in 19th USENIX Symposium on Operating Systems Design and Implementation (OSDI '25)",
    "pdf_url": "https://arxiv.org/pdf/2506.14813v1",
    "published_date": "2025-06-06 05:34:42 UTC",
    "updated_date": "2025-06-06 05:34:42 UTC"
  },
  {
    "arxiv_id": "2506.05759v1",
    "title": "Revealing hidden correlations from complex spatial distributions: Adjacent Correlation Analysis",
    "authors": [
      "Guang-Xing Li"
    ],
    "abstract": "Physics has been transforming our view of nature for centuries. While combining physical knowledge with computational approaches has enabled detailed modeling of physical systems' evolution, understanding the emergence of patterns and structures remains limited. Correlations between quantities are the most reliable approach to describe relationships between different variables. However, for complex patterns, directly searching for correlations is often impractical, as complexity and spatial inhomogeneity can obscure correlations. We discovered that the key is to search for correlations in local regions and developed a new method, adjacent correlation analysis, to extract such correlations and represent them in phase space. When multiple observations are available, a useful way to study a system is to analyze distributions in phase space using the Probability Density Function (PDF). Adjacent correlation analysis evaluates vectors representing local correlations, which can be overlaid on the PDF plot to form the adjacent correlation plot. These correlation vectors often exhibit remarkably regular patterns and may lead to the discovery of new laws. The vectors we derive are equivalent to the vector field in dynamical systems on the attracting manifold. By efficiently representing spatial patterns as correlation vectors in phase space, our approach opens avenues for classification, prediction, parameter fitting, and forecasting.",
    "categories": [
      "physics.comp-ph",
      "astro-ph.IM",
      "cs.AI",
      "math.DS"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "Code avaliable at https://github.com/gxli/Adjacent-Correlation-Analysis",
    "pdf_url": "https://arxiv.org/pdf/2506.05759v1",
    "published_date": "2025-06-06 05:31:29 UTC",
    "updated_date": "2025-06-06 05:31:29 UTC"
  },
  {
    "arxiv_id": "2506.05755v1",
    "title": "FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts",
    "authors": [
      "Yang Li",
      "Zhi Chen"
    ],
    "abstract": "Optimal execution in financial markets refers to the process of strategically transacting a large volume of assets over a period to achieve the best possible outcome by balancing the trade-off between market impact costs and timing or volatility risks. Traditional optimal execution strategies, such as static Almgren-Chriss models, often prove suboptimal in dynamic financial markets. This paper propose flowOE, a novel imitation learning framework based on flow matching models, to address these limitations. FlowOE learns from a diverse set of expert traditional strategies and adaptively selects the most suitable expert behavior for prevailing market conditions. A key innovation is the incorporation of a refining loss function during the imitation process, enabling flowOE not only to mimic but also to improve upon the learned expert actions. To the best of our knowledge, this work is the first to apply flow matching models in a stochastic optimal execution problem. Empirical evaluations across various market conditions demonstrate that flowOE significantly outperforms both the specifically calibrated expert models and other traditional benchmarks, achieving higher profits with reduced risk. These results underscore the practical applicability and potential of flowOE to enhance adaptive optimal execution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "q-fin.CP",
      "q-fin.TR"
    ],
    "primary_category": "cs.LG",
    "comment": "3 figures, 3 algorithms, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05755v1",
    "published_date": "2025-06-06 05:28:22 UTC",
    "updated_date": "2025-06-06 05:28:22 UTC"
  },
  {
    "arxiv_id": "2506.05754v1",
    "title": "Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective",
    "authors": [
      "Emmanuel Anaya Gonzalez",
      "Sairam Vaidya",
      "Kanghee Park",
      "Ruyi Ji",
      "Taylor Berg-Kirkpatrick",
      "Loris D'Antoni"
    ],
    "abstract": "Constrained decoding enables Language Models (LMs) to produce samples that provably satisfy hard constraints. However, existing constrained-decoding approaches often distort the underlying model distribution, a limitation that is especially problematic in applications like program fuzzing, where one wants to generate diverse and valid program inputs for testing purposes. We propose a new constrained sampling framework based on Markov Chain Monte Carlo (MCMC) that simultaneously satisfies three core desiderata: constraint satisfying (every sample satisfies the constraint), monotonically converging (the sampling process converges to the true conditional distribution), and efficient (high-quality samples emerge in few steps). Our method constructs a proposal distribution over valid outputs and applies a Metropolis-Hastings acceptance criterion based on the LM's likelihood, ensuring principled and efficient exploration of the constrained space. Empirically, our sampler outperforms existing methods on both synthetic benchmarks and real-world program fuzzing tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05754v1",
    "published_date": "2025-06-06 05:28:20 UTC",
    "updated_date": "2025-06-06 05:28:20 UTC"
  },
  {
    "arxiv_id": "2506.05752v2",
    "title": "Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting",
    "authors": [
      "Zhongying Wang",
      "Thoai D. Ngo",
      "Hamidreza Zoraghein",
      "Benjamin Lucas",
      "Morteza Karimzadeh"
    ],
    "abstract": "The COVID-19 pandemic's severe impact highlighted the need for accurate and timely hospitalization forecasting to support effective healthcare planning. However, most forecasting models struggled, particularly during variant surges, when they were most needed. This study introduces a novel parallel-stream Long Short-Term Memory (LSTM) framework to forecast daily state-level incident hospitalizations in the United States. Our framework incorporates a spatiotemporal feature, Social Proximity to Hospitalizations (SPH), derived from Meta's Social Connectedness Index, to improve forecasts. SPH serves as a proxy for interstate population interaction, capturing transmission dynamics across space and time. Our architecture captures both short- and long-term temporal dependencies, and a multi-horizon ensembling strategy balances forecasting consistency and error. An evaluation against the COVID-19 Forecast Hub ensemble models during the Delta and Omicron surges reveals the superiority of our model. On average, our model surpasses the ensemble by 27, 42, 54, and 69 hospitalizations per state at the 7-, 14-, 21-, and 28-day horizons, respectively, during the Omicron surge. Data-ablation experiments confirm SPH's predictive power, highlighting its effectiveness in enhancing forecasting models. This research not only advances hospitalization forecasting but also underscores the significance of spatiotemporal features, such as SPH, in modeling the complex dynamics of infectious disease spread.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "36 pages, 12 figures. This is the accepted version of the article published in International Journal of Geographical Information Science. DOI will be added upon publication",
    "pdf_url": "https://arxiv.org/pdf/2506.05752v2",
    "published_date": "2025-06-06 05:22:11 UTC",
    "updated_date": "2025-07-07 22:13:57 UTC"
  },
  {
    "arxiv_id": "2506.05751v1",
    "title": "An Ontology for Representing Curriculum and Learning Material",
    "authors": [
      "Antrea Christou",
      "Chris Davis Jaldi",
      "Joseph Zalewski",
      "Hande Kk McGinty",
      "Pascal Hitzler",
      "Cogan Shimizu"
    ],
    "abstract": "Educational, learning, and training materials have become extremely commonplace across the Internet. Yet, they frequently remain disconnected from each other, fall into platform silos, and so on. One way to overcome this is to provide a mechanism to integrate the material and provide cross-links across topics.\n  In this paper, we present the Curriculum KG Ontology, which we use as a framework for the dense interlinking of educational materials, by first starting with organizational and broad pedagogical principles. We provide a materialized graph for the Prototype Open Knowledge Network use-case, and validate it using competency questions sourced from domain experts and educators.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05751v1",
    "published_date": "2025-06-06 05:21:05 UTC",
    "updated_date": "2025-06-06 05:21:05 UTC"
  },
  {
    "arxiv_id": "2506.05748v1",
    "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance",
    "authors": [
      "Rudransh Agnihotri",
      "Ananya Pandey"
    ],
    "abstract": "Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately = 9/10 in similarity to human explanations, while zero-shot judges score around =5/10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05748v1",
    "published_date": "2025-06-06 05:18:54 UTC",
    "updated_date": "2025-06-06 05:18:54 UTC"
  },
  {
    "arxiv_id": "2506.05745v2",
    "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models",
    "authors": [
      "Emil Biju",
      "Shayan Talaei",
      "Zhemin Huang",
      "Mohammadreza Pourreza",
      "Azalia Mirhoseini",
      "Amin Saberi"
    ],
    "abstract": "Large reasoning models (LRMs) excel at complex reasoning tasks but typically generate lengthy sequential chains-of-thought, resulting in long inference times before arriving at the final answer. To address this challenge, we introduce SPRINT, a novel post-training and inference-time framework designed to enable LRMs to dynamically identify and exploit opportunities for parallelization during their reasoning process. SPRINT incorporates an innovative data curation pipeline that reorganizes natural language reasoning trajectories into structured rounds of long-horizon planning and parallel execution. By fine-tuning LRMs on a small amount of such curated data, the models learn to dynamically identify independent subtasks within extended reasoning processes and effectively execute them in parallel. Through extensive evaluations, we demonstrate that models fine-tuned with the SPRINT framework match the performance of reasoning models on complex domains such as mathematics while generating up to 39% fewer sequential tokens on problems requiring more than 8,000 output tokens. Finally, we observe consistent results transferred to two out-of-distribution tasks, namely GPQA and Countdown, with up to 45% and 65% reduction in average sequential tokens respectively for longer reasoning trajectories, while matching the performance of the fine-tuned reasoning model.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at NeurIPS 2025. Emil Biju, Shayan Talaei, and Zhemin Huang contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2506.05745v2",
    "published_date": "2025-06-06 05:10:31 UTC",
    "updated_date": "2025-12-03 03:22:15 UTC"
  },
  {
    "arxiv_id": "2506.05744v3",
    "title": "Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties",
    "authors": [
      "Gouki Minegishi",
      "Hiroki Furuta",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Recent large-scale reasoning models have achieved state-of-the-art performance on challenging mathematical benchmarks, yet the internal mechanisms underlying their success remain poorly understood. In this work, we introduce the notion of a reasoning graph, extracted by clustering hidden-state representations at each reasoning step, and systematically analyze three key graph-theoretic properties: cyclicity, diameter, and small-world index, across multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly more recurrent cycles (about 5 per sample), substantially larger graph diameters, and pronounced small-world characteristics (about 6x) compared to their base counterparts. Notably, these structural advantages grow with task difficulty and model capacity, with cycle detection peaking at the 14B scale and exploration diameter maximized in the 32B variant, correlating positively with accuracy. Furthermore, we show that supervised fine-tuning on an improved dataset systematically expands reasoning graph diameters in tandem with performance gains, offering concrete guidelines for dataset design aimed at boosting reasoning capabilities. By bridging theoretical insights into reasoning graph structures with practical recommendations for data construction, our work advances both the interpretability and the efficacy of large reasoning models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to Neurips 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05744v3",
    "published_date": "2025-06-06 05:05:11 UTC",
    "updated_date": "2025-10-01 16:04:11 UTC"
  },
  {
    "arxiv_id": "2506.05743v1",
    "title": "When Better Features Mean Greater Risks: The Performance-Privacy Trade-Off in Contrastive Learning",
    "authors": [
      "Ruining Sun",
      "Hongsheng Hu",
      "Wei Luo",
      "Zhaoxi Zhang",
      "Yanjun Zhang",
      "Haizhuan Yuan",
      "Leo Yu Zhang"
    ],
    "abstract": "With the rapid advancement of deep learning technology, pre-trained encoder models have demonstrated exceptional feature extraction capabilities, playing a pivotal role in the research and application of deep learning. However, their widespread use has raised significant concerns about the risk of training data privacy leakage. This paper systematically investigates the privacy threats posed by membership inference attacks (MIAs) targeting encoder models, focusing on contrastive learning frameworks. Through experimental analysis, we reveal the significant impact of model architecture complexity on membership privacy leakage: As more advanced encoder frameworks improve feature-extraction performance, they simultaneously exacerbate privacy-leakage risks. Furthermore, this paper proposes a novel membership inference attack method based on the p-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack (LpLA). This method infers membership status, by leveraging the statistical distribution characteristics of the p-norm of feature vectors. Experimental results across multiple datasets and model architectures demonstrate that LpLA outperforms existing methods in attack performance and robustness, particularly under limited attack knowledge and query volumes. This study not only uncovers the potential risks of privacy leakage in contrastive learning frameworks, but also provides a practical basis for privacy protection research in encoder models. We hope that this work will draw greater attention to the privacy risks associated with self-supervised learning models and shed light on the importance of a balance between model utility and training data privacy. Our code is publicly available at: https://github.com/SeroneySun/LpLA_code.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted In ACM ASIA Conference on Computer and Communications Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam. For Code, see https://github.com/SeroneySun/LpLA_code",
    "pdf_url": "https://arxiv.org/pdf/2506.05743v1",
    "published_date": "2025-06-06 05:03:29 UTC",
    "updated_date": "2025-06-06 05:03:29 UTC"
  },
  {
    "arxiv_id": "2506.05739v1",
    "title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt",
    "authors": [
      "Zhilong Wang",
      "Neha Nagaraja",
      "Lan Zhang",
      "Hayretdin Bahsi",
      "Pawan Patil",
      "Peng Liu"
    ],
    "abstract": "LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model's behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in the Industry Track of the 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.05739v1",
    "published_date": "2025-06-06 04:50:57 UTC",
    "updated_date": "2025-06-06 04:50:57 UTC"
  },
  {
    "arxiv_id": "2506.05736v1",
    "title": "Generalized Incremental Learning under Concept Drift across Evolving Data Streams",
    "authors": [
      "En Yu",
      "Jie Lu",
      "Guangquan Zhang"
    ],
    "abstract": "Real-world data streams exhibit inherent non-stationarity characterized by concept drift, posing significant challenges for adaptive learning systems. While existing methods address isolated distribution shifts, they overlook the critical co-evolution of label spaces and distributions under limited supervision and persistent uncertainty. To address this, we formalize Generalized Incremental Learning under Concept Drift (GILCD), characterizing the joint evolution of distributions and label spaces in open-environment streaming contexts, and propose a novel framework called Calibrated Source-Free Adaptation (CSFA). First, CSFA introduces a training-free prototype calibration mechanism that dynamically fuses emerging prototypes with base representations, enabling stable new-class identification without optimization overhead. Second, we design a novel source-free adaptation algorithm, i.e., Reliable Surrogate Gap Sharpness-aware (RSGS) minimization. It integrates sharpness-aware perturbation loss optimization with surrogate gap minimization, while employing entropy-based uncertainty filtering to discard unreliable samples. This mechanism ensures robust distribution alignment and mitigates generalization degradation caused by uncertainties. Therefore, CSFA establishes a unified framework for stable adaptation to evolving semantics and distributions in open-world streaming scenarios. Extensive experiments validate the superior performance and effectiveness of CSFA compared to state-of-the-art approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2506.05736v1",
    "published_date": "2025-06-06 04:36:24 UTC",
    "updated_date": "2025-06-06 04:36:24 UTC"
  },
  {
    "arxiv_id": "2506.05725v1",
    "title": "Large Language Models are Good Relational Learners",
    "authors": [
      "Fang Wu",
      "Vijay Prakash Dwivedi",
      "Jure Leskovec"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents. Still, this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. We introduce Rel-LLM, a novel architecture that utilizes a graph neural network (GNN)- based encoder to generate structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, our method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, we demonstrate that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. Code is available at https://github.com/smiles724/Rel-LLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05725v1",
    "published_date": "2025-06-06 04:07:55 UTC",
    "updated_date": "2025-06-06 04:07:55 UTC"
  },
  {
    "arxiv_id": "2506.05721v1",
    "title": "Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data",
    "authors": [
      "Dumindu Tissera",
      "Omar Awadallah",
      "Muhammad Umair Danish",
      "Ayan Sadhu",
      "Katarina Grolinger"
    ],
    "abstract": "Multi-label Classification (MLC) assigns an instance to one or more non-exclusive classes. A challenge arises when the dataset contains a large proportion of instances with no assigned class, referred to as negative data, which can overwhelm the learning process and hinder the accurate identification and classification of positive instances. Nevertheless, it is common in MLC applications such as industrial defect detection, agricultural disease identification, and healthcare diagnosis to encounter large amounts of negative data. Assigning a separate negative class to these instances further complicates the learning objective and introduces unnecessary redundancies. To address this challenge, we redesign standard MLC loss functions by deriving a likelihood of any class being present, formulated by a normalized weighted geometric mean of the predicted class probabilities. We introduce a regularization parameter that controls the relative contribution of the absent class probabilities to the any-class presence likelihood in positive instances. The any-class presence likelihood complements the multi-label learning by encouraging the network to become more aware of implicit positive instances and improve the label classification within those positive instances. Experiments on large-scale datasets with negative data: SewerML, modified COCO, and ChestX-ray14, across various networks and base loss functions show that our loss functions consistently improve MLC performance of their standard loss counterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in F2, and 3.11 in mean average precision, all without additional parameters or computational complexity. Code available at: https://github.com/ML-for-Sensor-Data-Western/gmean-mlc",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05721v1",
    "published_date": "2025-06-06 03:59:11 UTC",
    "updated_date": "2025-06-06 03:59:11 UTC"
  },
  {
    "arxiv_id": "2506.05718v2",
    "title": "Grokking Beyond the Euclidean Norm of Model Parameters",
    "authors": [
      "Pascal Jr Tikeng Notsawo",
      "Guillaume Dumas",
      "Guillaume Rabusseau"
    ],
    "abstract": "Grokking refers to a delayed generalization following overfitting when optimizing artificial neural networks with gradient-based methods. In this work, we demonstrate that grokking can be induced by regularization, either explicit or implicit. More precisely, we show that when there exists a model with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the problem of interest, gradient descent with a small but non-zero regularization of $P$ (e.g., $\\ell_1$ or nuclear norm regularization) results in grokking. This extends previous work showing that small non-zero weight decay induces grokking. Moreover, our analysis shows that over-parameterization by adding depth makes it possible to grok or ungrok without explicitly using regularization, which is impossible in shallow cases. We further show that the $\\ell_2$ norm is not a reliable proxy for generalization when the model is regularized toward a different property $P$, as the $\\ell_2$ norm grows in many cases where no weight decay is used, but the model generalizes anyway. We also show that grokking can be amplified solely through data selection, with any other hyperparameter fixed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "67 pages, 35 figures. Forty-second International Conference on Machine Learning (ICML), 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05718v2",
    "published_date": "2025-06-06 03:44:28 UTC",
    "updated_date": "2025-07-10 20:57:34 UTC"
  },
  {
    "arxiv_id": "2506.05716v1",
    "title": "Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning",
    "authors": [
      "Adrian Ly",
      "Richard Dazeley",
      "Peter Vamplew",
      "Francisco Cruz",
      "Sunil Aryal"
    ],
    "abstract": "While many algorithmic extensions to Deep Q-Networks (DQN) have been proposed, there remains limited understanding of how different improvements interact. In particular, multi-step and ensemble style extensions have shown promise in reducing overestimation bias, thereby improving sample efficiency and algorithmic stability. In this paper, we introduce a novel algorithm called Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step updates to stabilise algorithmic performance. EEDQN is designed to address two major challenges in deep reinforcement learning: overestimation bias and sample efficiency. We evaluated EEDQN against standard and ensemble DQN variants across the MinAtar benchmark, a set of environments that emphasise behavioral learning while reducing representational complexity. Our results show that EEDQN achieves consistently robust performance across all tested environments, outperforming baseline DQN methods and matching or exceeding state-of-the-art ensemble DQNs in final returns on most of the MinAtar environments. These findings highlight the potential of systematically combining algorithmic improvements and provide evidence that ensemble and multi-step methods, when carefully integrated, can yield substantial gains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05716v1",
    "published_date": "2025-06-06 03:36:19 UTC",
    "updated_date": "2025-06-06 03:36:19 UTC"
  },
  {
    "arxiv_id": "2506.05702v1",
    "title": "Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces",
    "authors": [
      "Chaofan Pan",
      "Jiafen Liu",
      "Yanhua Li",
      "Linbo Xiong",
      "Fan Min",
      "Wei Wei",
      "Xin Yang"
    ],
    "abstract": "Continual Learning (CL) is a powerful tool that enables agents to learn a sequence of tasks, accumulating knowledge learned in the past and using it for problem-solving or future task learning. However, existing CL methods often assume that the agent's capabilities remain static within dynamic environments, which doesn't reflect real-world scenarios where capabilities dynamically change. This paper introduces a new and realistic problem: Continual Learning with Dynamic Capabilities (CL-DC), posing a significant challenge for CL agents: How can policy generalization across different action spaces be achieved? Inspired by the cortical functions, we propose an Action-Adaptive Continual Learning framework (AACL) to address this challenge. Our framework decouples the agent's policy from the specific action space by building an action representation space. For a new action space, the encoder-decoder of action representations is adaptively fine-tuned to maintain a balance between stability and plasticity. Furthermore, we release a benchmark based on three environments to validate the effectiveness of methods for CL-DC. Experimental results demonstrate that our framework outperforms popular methods by generalizing the policy across action spaces.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05702v1",
    "published_date": "2025-06-06 03:07:30 UTC",
    "updated_date": "2025-06-06 03:07:30 UTC"
  },
  {
    "arxiv_id": "2506.05700v1",
    "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
    "authors": [
      "Yan Wang",
      "Yueru He",
      "Ruoyu Xiang",
      "Jeff Zhao"
    ],
    "abstract": "Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05700v1",
    "published_date": "2025-06-06 03:02:52 UTC",
    "updated_date": "2025-06-06 03:02:52 UTC"
  },
  {
    "arxiv_id": "2506.05699v2",
    "title": "Evaluating AI-Powered Learning Assistants in Engineering Higher Education: Student Engagement, Ethical Challenges, and Policy Implications",
    "authors": [
      "Ramteja Sajja",
      "Yusuf Sermet",
      "Brian Fodale",
      "Ibrahim Demir"
    ],
    "abstract": "As generative AI becomes increasingly integrated into higher education, understanding how students engage with these technologies is essential for responsible adoption. This study evaluates the Educational AI Hub, an AI-powered learning framework, implemented in undergraduate civil and environmental engineering courses at a large R1 public university. Using a mixed-methods design combining pre- and post-surveys, system usage logs, and qualitative analysis of students' AI interactions, the research examines perceptions of trust, ethics, usability, and learning outcomes. Findings show that students valued the AI assistant for its accessibility and comfort, with nearly half reporting greater ease using it than seeking help from instructors or teaching assistants. The tool was most helpful for completing homework and understanding concepts, though views on its instructional quality were mixed. Ethical uncertainty, particularly around institutional policy and academic integrity, emerged as a key barrier to full engagement. Overall, students regarded AI as a supplement rather than a replacement for human instruction. The study highlights the importance of usability, ethical transparency, and faculty guidance in promoting meaningful AI engagement. A total of 71 students participated across two courses, generating over 600 AI interactions and 100 survey responses that provided both quantitative and contextual insights into learning engagement.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "26 pages, 10 Figures, 6 Tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05699v2",
    "published_date": "2025-06-06 03:02:49 UTC",
    "updated_date": "2025-10-27 23:56:32 UTC"
  },
  {
    "arxiv_id": "2506.05695v1",
    "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework",
    "authors": [
      "Lingyuan Liu",
      "Mengxiang Zhang"
    ],
    "abstract": "Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of \"progressive overload\" (POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05695v1",
    "published_date": "2025-06-06 02:48:38 UTC",
    "updated_date": "2025-06-06 02:48:38 UTC"
  },
  {
    "arxiv_id": "2506.05692v3",
    "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code",
    "authors": [
      "Xinghang Li",
      "Jingzhe Ding",
      "Chao Peng",
      "Bing Zhao",
      "Xiang Gao",
      "Hongwan Gao",
      "Xinchen Gu"
    ],
    "abstract": "The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05692v3",
    "published_date": "2025-06-06 02:48:02 UTC",
    "updated_date": "2025-06-20 12:42:57 UTC"
  },
  {
    "arxiv_id": "2506.06401v1",
    "title": "Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs",
    "authors": [
      "Hongming Yang",
      "Shi Lin",
      "Jun Shao",
      "Changting Lin",
      "Donghai Zhu",
      "Meng Han",
      "Qinglei Kong"
    ],
    "abstract": "Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "This work is accepted at ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.06401v1",
    "published_date": "2025-06-06 02:40:42 UTC",
    "updated_date": "2025-06-06 02:40:42 UTC"
  },
  {
    "arxiv_id": "2506.05683v4",
    "title": "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR",
    "authors": [
      "Fardis Nadimi",
      "Payam Abdisarabshali",
      "Kasra Borazjani",
      "Jacob Chakareski",
      "Seyyedali Hosseinalipour"
    ],
    "abstract": "Extended reality (XR) systems, which consist of virtual reality (VR), augmented reality (AR), and mixed reality (XR), offer a transformative interface for immersive, multi-modal, and embodied human-computer interaction. In this paper, we envision that multi-modal multi-task (M3T) federated foundation models (FedFMs) can offer transformative capabilities for XR systems through integrating the representational strength of M3T foundation models (FMs) with the privacy-preserving model training principles of federated learning (FL). We present a modular architecture for FedFMs, which entails different coordination paradigms for model training and aggregations. Central to our vision is the codification of XR challenges that affect the implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality diversity, (2) Hardware heterogeneity and system-level constraints, (3) Interactivity and embodied personalization, (4) Functional/task variability, and (5) Temporality and environmental variability. We illustrate the manifestation of these dimensions across a set of emerging and anticipated applications of XR systems. Finally, we propose evaluation metrics, dataset requirements, and design tradeoffs necessary for the development of resource-aware FedFMs in XR. This perspective aims to chart the technical and conceptual foundations for context-aware privacy-preserving intelligence in the next generation of XR systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 Figures, 8 Tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05683v4",
    "published_date": "2025-06-06 02:23:42 UTC",
    "updated_date": "2025-08-05 23:18:51 UTC"
  },
  {
    "arxiv_id": "2506.05680v2",
    "title": "Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization",
    "authors": [
      "Tailin Zhou",
      "Zhilin Chen",
      "Wenlong Lyu",
      "Zhitang Chen",
      "Danny H. K. Tsang",
      "Jun Zhang"
    ],
    "abstract": "Optimizing complex systems, from discovering therapeutic drugs to designing high-performance materials, remains a fundamental challenge across science and engineering, as the underlying rules are often unknown and costly to evaluate. Offline optimization aims to optimize designs for target scores using pre-collected datasets without system interaction. However, conventional approaches may fail beyond training data, predicting inaccurate scores and generating inferior designs. This paper introduces ManGO, a diffusion-based framework that learns the design-score manifold, capturing the design-score interdependencies holistically. Unlike existing methods that treat design and score spaces in isolation, ManGO unifies forward prediction and backward generation, attaining generalization beyond training data. Key to this is its derivative-free guidance for conditional generation, coupled with adaptive inference-time scaling that dynamically optimizes denoising paths. Extensive evaluations demonstrate that ManGO outperforms 24 single- and 10 multi-objective optimization methods across diverse domains, including synthetic tasks, robot control, material design, DNA sequence, and real-world engineering optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This manuscript was accepted by npj AI",
    "pdf_url": "https://arxiv.org/pdf/2506.05680v2",
    "published_date": "2025-06-06 02:11:10 UTC",
    "updated_date": "2026-01-12 12:56:43 UTC"
  },
  {
    "arxiv_id": "2506.09061v3",
    "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model",
    "authors": [
      "Alyssa Pinnock",
      "Shakya Jayakody",
      "Kawsher A Roxy",
      "Md Rubel Ahmed"
    ],
    "abstract": "This paper introduces EdgeProfiler, a fast profiling framework designed for evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs offer remarkable capabilities in natural language understanding and generation, their high computational, memory, and power requirements often confine them to cloud environments. EdgeProfiler addresses these challenges by providing a systematic methodology for assessing LLM performance in resource-constrained edge settings. The framework profiles compact LLMs, including TinyLLaMA, Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization techniques and strict memory constraints. Analytical modeling is used to estimate latency, FLOPs, and energy consumption. The profiling reveals that 4-bit quantization reduces model memory usage by approximately 60-70%, while maintaining accuracy within 2-5% of full-precision baselines. Inference speeds are observed to improve by 2-3x compared to FP16 baselines across various edge devices. Power modeling estimates a 35-50% reduction in energy consumption for INT4 configurations, enabling practical deployment on hardware such as Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the importance of efficient profiling tailored to lightweight LLMs in edge environments, balancing accuracy, energy efficiency, and computational feasibility.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "4 figures, 7 pages, IEEE conference template",
    "pdf_url": "https://arxiv.org/pdf/2506.09061v3",
    "published_date": "2025-06-06 01:56:07 UTC",
    "updated_date": "2025-09-17 04:31:20 UTC"
  },
  {
    "arxiv_id": "2506.15724v1",
    "title": "MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference",
    "authors": [
      "Kunxi Li",
      "Zhonghua Jiang",
      "Zhouzhou Shen",
      "Zhaode Wang",
      "Chengfei Lv",
      "Shengyu Zhang",
      "Fan Wu",
      "Fei Wu"
    ],
    "abstract": "This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15724v1",
    "published_date": "2025-06-06 01:51:24 UTC",
    "updated_date": "2025-06-06 01:51:24 UTC"
  },
  {
    "arxiv_id": "2506.05673v3",
    "title": "Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery",
    "authors": [
      "Sajjad Abdoli",
      "Freeman Lewin",
      "Gediminas Vasiliauskas",
      "Fabian Schonholz"
    ],
    "abstract": "The development of modern Artificial Intelligence (AI) models, particularly diffusion-based models employed in computer vision and image generation tasks, is undergoing a paradigmatic shift in development methodologies. Traditionally dominated by a \"Model Centric\" approach, in which performance gains were primarily pursued through increasingly complex model architectures and hyperparameter optimization, the field is now recognizing a more nuanced \"Data-Centric\" approach. This emergent framework foregrounds the quality, structure, and relevance of training data as the principal driver of model performance. To operationalize this paradigm shift, we introduce the DataSeeds.AI sample dataset (the \"DSD\"), initially comprised of approximately 10,610 high-quality human peer-ranked photography images accompanied by extensive multi-tier annotations. The DSD is a foundational computer vision dataset designed to usher in a new standard for commercial image datasets. Representing a small fraction of DataSeeds.AI's 100 million-plus image catalog, the DSD provides a scalable foundation necessary for robust commercial and multimodal AI development. Through this in-depth exploratory analysis, we document the quantitative improvements generated by the DSD on specific models against known benchmarks and make the code and the trained models used in our evaluation publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05673v3",
    "published_date": "2025-06-06 01:50:28 UTC",
    "updated_date": "2025-06-11 19:13:04 UTC"
  },
  {
    "arxiv_id": "2506.05667v2",
    "title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models",
    "authors": [
      "Yuhan Hao",
      "Zhengning Li",
      "Lei Sun",
      "Weilong Wang",
      "Naixin Yi",
      "Sheng Song",
      "Caihong Qin",
      "Mofan Zhou",
      "Yifei Zhan",
      "Xianpeng Lang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by drivers of autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from drivers' actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Benchmark: https://huggingface.co/datasets/LiAuto-DriveAction/drive-action",
    "pdf_url": "https://arxiv.org/pdf/2506.05667v2",
    "published_date": "2025-06-06 01:30:52 UTC",
    "updated_date": "2025-09-26 04:33:48 UTC"
  },
  {
    "arxiv_id": "2506.05660v2",
    "title": "TissUnet: Improved Extracranial Tissue and Cranium Segmentation for Children through Adulthood",
    "authors": [
      "Markiian Mandzak",
      "Elvira Yang",
      "Anna Zapaishchykova",
      "Yu-Hui Chen",
      "Lucas Heilbroner",
      "John Zielke",
      "Divyanshu Tak",
      "Reza Mojahed-Yazdi",
      "Francesca Romana Mussa",
      "Zezhong Ye",
      "Sridhar Vajapeyam",
      "Viviana Benitez",
      "Ralph Salloum",
      "Susan N. Chi",
      "Houman Sotoudeh",
      "Jakob Seidlitz",
      "Sabine Mueller",
      "Hugo J. W. L. Aerts",
      "Tina Y. Poussaint",
      "Benjamin H. Kann"
    ],
    "abstract": "Extracranial tissues visible on brain magnetic resonance imaging (MRI) may hold significant value for characterizing health conditions and clinical decision-making, yet they are rarely quantified. Current tools have not been widely validated, particularly in settings of developing brains or underlying pathology. We present TissUnet, a deep learning model that segments skull bone, subcutaneous fat, and muscle from routine three-dimensional T1-weighted MRI, with or without contrast enhancement. The model was trained on 155 paired MRI-computed tomography (CT) scans and validated across nine datasets covering a wide age range and including individuals with brain tumors. In comparison to AI-CT-derived labels from 37 MRI-CT pairs, TissUnet achieved a median Dice coefficient of 0.79 [IQR: 0.77-0.81] in a healthy adult cohort. In a second validation using expert manual annotations, median Dice was 0.83 [IQR: 0.83-0.84] in healthy individuals and 0.81 [IQR: 0.78-0.83] in tumor cases, outperforming previous state-of-the-art method. Acceptability testing resulted in an 89% acceptance rate after adjudication by a tie-breaker(N=108 MRIs), and TissUnet demonstrated excellent performance in the blinded comparative review (N=45 MRIs), including both healthy and tumor cases in pediatric populations. TissUnet enables fast, accurate, and reproducible segmentation of extracranial tissues, supporting large-scale studies on craniofacial morphology, treatment effects, and cardiometabolic risk using standard brain T1w MRI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "44 pages, 4 tables, 6 figures, supplementary material",
    "pdf_url": "https://arxiv.org/pdf/2506.05660v2",
    "published_date": "2025-06-06 01:21:34 UTC",
    "updated_date": "2025-06-09 12:05:21 UTC"
  }
]
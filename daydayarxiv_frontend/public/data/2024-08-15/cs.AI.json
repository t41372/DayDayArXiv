{
  "date": "2024-08-15",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-15 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 91 篇论文，主要聚焦 AI 和大型语言模型（LLM）在医疗、机器人、知识图谱和生成任务中的创新应用，亮点包括 Jeff Clune 等知名学者的代理系统设计，以及 RAG 和 KAN 框架的改进，这些工作展示了 LLM 在复杂任务中的鲁棒性和实际潜力。\n\n### 重点论文解析\n我们先聊聊那些重要、话题度高或有潜在影响的论文，尤其是涉及前沿 AI 技术的，以及知名学者参与的。其他论文会快速掠过，只突出核心贡献。\n\n**1. Automated Design of Agentic Systems（自动化代理系统设计）**  \n作者：Shengran Hu, Cong Lu, Jeff Clune（Jeff Clune 是知名 AI 研究者）。  \n这篇论文提出了一种新研究领域 Automated Design of Agentic Systems (ADAS)，利用元代理自动设计代理系统，包括发明新模块和组合方式。核心贡献是通过 Meta Agent Search 算法，代理系统能迭代优化自身设计，并在编码、科学和数学任务中超越手动设计系统。主要发现：生成的代理在跨领域转移时表现出色，展示了 AI 自动演化的潜力，是 AI 代理发展的重要里程石。\n\n**7. DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search（DeepSeek-Prover-V1.5：利用证明助手反馈增强强化学习和蒙特卡罗树搜索）**  \n作者：Huajian Xin 等。  \n这篇扩展了 DeepSeek-Prover-V1，通过强化学习和证明助手反馈优化定理证明模型。核心贡献：引入 RMaxTS 算法，支持自监督学习和高效证明路径探索。主要发现：在 miniF2F 和 ProofNet 数据集上，模型准确率提升至 63.5% 和 25.3%，显著改善了数学证明任务的性能，适用于复杂推理场景。\n\n**4. W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering（W-RAG：RAG 中的弱监督密集检索用于开放域问答）**  \n作者：Jinming Nian 等。  \nRAG（Retrieval-Augmented Generation）框架是 LLM 热门应用，这篇论文提出 W-RAG，使用弱监督信号微调检索器。核心贡献：通过重新排序 BM25 检索结果并使用 LLM 生成答案概率作为正例，显著提升检索和问答性能。主要发现：在四个公开数据集上，W-RAG 性能媲美使用人工标注数据的模型，展示了弱监督在知识密集任务中的高效性。\n\n**22. Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models（Cybench：评估语言模型网络安全能力的框架）**  \n作者：Andy K. Zhang 等（ICLR 2025 Oral）。  \n这篇论文针对 LLM 在网络安全的风险，构建了一个评估框架。核心贡献：包含 40 个 CTF 任务数据集，并测试 8 个模型的性能。主要发现：如 Claude 3.5 Sonnet 在安全任务中表现出色，但整体 LLM 存在漏洞，强调了评估框架在安全 AI 部署中的重要性。\n\n**45. DeepSeek-Prover-V1.5（同上，已合并讨论）**  \n（与第 7 篇相关，详见上文，不重复详述。）\n\n**16. PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications（PQV-Mobile：用于优化视觉变压器在移动应用的修剪和量化工具包）**  \n作者：Kshitij Bhardwaj。  \n视觉变压器在移动设备上效率低下，这篇论文提出 PQV-Mobile 工具。核心贡献：结合结构化修剪和量化，减少模型延迟。主要发现：对 DeiT 模型修剪 9.375% 并量化后，延迟降低 7.18 倍，同时准确率仅损失 2.24%，适用于资源受限的边缘计算。\n\n**1. Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer（医学成像中的分布漂移检测：使用数据草图和微调变压器）**  \n作者：Yusen Wu 等。  \n医疗 AI 需检测数据分布变化，这篇论文提出一种漂移检测方法。核心贡献：结合数据草图和微调 Vision Transformer，实现高精度检测（如 99.11% 准确率）。主要发现：对 CT 和乳腺 X 光图像有效，尤其在噪声环境中，增强了临床模型的可靠性。\n\n**27. FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models（FactorLLM：通过专家混合分解知识的 LLM）**  \n作者：Zhongyu Zhao 等。  \nLLM 知识冗余是问题，这篇论文提出 FactorLLM 分解 FFN 层。核心贡献：使用专家混合模型减少冗余，并通过 PA 损失优化。主要发现：推理速度提升 30%，性能保持不变，适用于高效知识表示。\n\n其他相关论文，如涉及 RAG 和 LLM 应用的（如 10. Assessing and Enhancing Large Language Models in Rare Disease Question-answering、38. Web Retrieval Agents for Evidence-Based Misinformation Detection），都展示了 LLM 在医疗和 misinformation 检测中的潜力，但我们快速掠过：这些工作主要通过检索增强生成改进问答准确性，贡献在于数据集构建和细化策略。\n\n### 快速掠过其他论文\n今天还有许多论文探讨次要主题，如时间序列分析（e.g., 58. CEGRL-TKGR）、机器人规划（e.g., 21. Autonomous Behavior Planning For Humanoid Loco-manipulation）和图像生成（e.g., 34. Dynamic Adaptive Optimization for Effective Sentiment Analysis Fine-Tuning）。这些论文的核心贡献包括新算法或数据集，但影响力较小。例如，CEGRL-TKGR 在知识图谱推理中引入因果结构，提升了预测准确性；Autonomous Behavior Planning 使用 LLM 实现机器人任务规划，实验验证了其有效性。这些工作虽有技术创新，但未达到前沿水平，故不展开讨论。\n\n总之，今天的 arXiv 更新突显了 AI 模型在实际应用中的优化和鲁棒性，Jeff Clune 的工作尤其值得关注。感兴趣的读者可查阅具体论文深入探索！（本快报基于 91 篇论文精选，保持简洁。）",
  "papers": [
    {
      "arxiv_id": "2408.08456v2",
      "title": "Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Yusen Wu",
        "Phuong Nguyen",
        "Rose Yesha",
        "Yelena Yesha"
      ],
      "abstract": "Distributional drift detection is important in medical applications as it\nhelps ensure the accuracy and reliability of models by identifying changes in\nthe underlying data distribution that could affect the prediction results of\nmachine learning models. However, current methods have limitations in detecting\ndrift, for example, the inclusion of abnormal datasets can lead to unfair\ncomparisons. This paper presents an accurate and sensitive approach to detect\ndistributional drift in CT-scan medical images by leveraging data-sketching and\nfine-tuning techniques. We developed a robust baseline library model for\nreal-time anomaly detection, allowing for efficient comparison of incoming\nimages and identification of anomalies. Additionally, we fine-tuned a\npre-trained Vision Transformer model to extract relevant features, using\nmammography as a case study, significantly enhancing model accuracy to 99.11%.\nCombining with data-sketches and fine-tuning, our feature extraction evaluation\ndemonstrated that cosine similarity scores between similar datasets provide\ngreater improvements, from around 50% increased to 99.1%. Finally, the\nsensitivity evaluation shows that our solutions are highly sensitive to even 1%\nsalt-and-pepper and speckle noise, and it is not sensitive to lighting noise\n(e.g., lighting conditions have no impact on data drift). The proposed methods\noffer a scalable and reliable solution for maintaining the accuracy of\ndiagnostic models in dynamic clinical environments.",
      "tldr_zh": "本论文提出了一种精确且敏感的分布漂移检测方法，针对医疗图像（如CT扫描）中的数据分布变化，利用数据草图（data-sketching）和微调Transformer技术来提升模型准确性和可靠性。研究开发了实时异常检测的基线库模型，并通过微调预训练的Vision Transformer在乳腺X光图像案例中，将准确率提高至99.11%，余弦相似度（cosine similarity）分数从约50%提升至99.1%。实验结果表明，该方法对1%的盐和胡椒噪声（salt-and-pepper noise）和斑点噪声（speckle noise）高度敏感，但对光照噪声不敏感，从而为动态临床环境提供了一个可扩展、可信的诊断模型维护解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08456v2",
      "published_date": "2024-08-15 23:46:37 UTC",
      "updated_date": "2025-05-09 17:46:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:27:56.049709"
    },
    {
      "arxiv_id": "2408.08447v1",
      "title": "SpectralEarth: Training Hyperspectral Foundation Models at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Nassim Ait Ali Braham",
        "Conrad M Albrecht",
        "Julien Mairal",
        "Jocelyn Chanussot",
        "Yi Wang",
        "Xiao Xiang Zhu"
      ],
      "abstract": "Foundation models have triggered a paradigm shift in computer vision and are\nincreasingly being adopted in remote sensing, particularly for multispectral\nimagery. Yet, their potential in hyperspectral imaging (HSI) remains untapped\ndue to the absence of comprehensive and globally representative hyperspectral\ndatasets. To close this gap, we introduce SpectralEarth, a large-scale\nmulti-temporal dataset designed to pretrain hyperspectral foundation models\nleveraging data from the Environmental Mapping and Analysis Program (EnMAP).\nSpectralEarth comprises 538,974 image patches covering 415,153 unique locations\nfrom more than 11,636 globally distributed EnMAP scenes spanning two years of\narchive. Additionally, 17.5% of these locations include multiple timestamps,\nenabling multi-temporal HSI analysis. Utilizing state-of-the-art\nself-supervised learning (SSL) algorithms, we pretrain a series of foundation\nmodels on SpectralEarth. We integrate a spectral adapter into classical vision\nbackbones to accommodate the unique characteristics of HSI. In tandem, we\nconstruct four downstream datasets for land-cover and crop-type mapping,\nproviding benchmarks for model evaluation. Experimental results support the\nversatility of our models, showcasing their generalizability across different\ntasks and sensors. We also highlight computational efficiency during model\nfine-tuning. The dataset, models, and source code will be made publicly\navailable.",
      "tldr_zh": "本文介绍了 SpectralEarth，这是一个大规模多时相高光谱图像 (HSI) 数据集，由 Environmental Mapping and Analysis Program (EnMAP) 数据构建，包含 538,974 个图像补丁和 415,153 个全球独特位置，支持多时相分析。研究团队利用自监督学习 (SSL) 算法和 spectral adapter 整合到经典视觉骨干网络中，预训练了一系列高光谱基础模型。实验结果显示，这些模型在土地覆盖和作物类型映射等下游任务上表现出色，具有跨任务和传感器泛化能力，并提高了微调的计算效率。数据集、模型和源代码将公开可用，以推动 HSI 领域的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08447v1",
      "published_date": "2024-08-15 22:55:59 UTC",
      "updated_date": "2024-08-15 22:55:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:28:08.354530"
    },
    {
      "arxiv_id": "2409.00012v1",
      "title": "AVIN-Chat: An Audio-Visual Interactive Chatbot System with Emotional State Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Chanhyuk Park",
        "Jungbin Cho",
        "Junwan Kim",
        "Seongmin Lee",
        "Jungsu Kim",
        "Sanghoon Lee"
      ],
      "abstract": "This work presents an audio-visual interactive chatbot (AVIN-Chat) system\nthat allows users to have face-to-face conversations with 3D avatars in\nreal-time. Compared to the previous chatbot services, which provide text-only\nor speech-only communications, the proposed AVIN-Chat can offer audio-visual\ncommunications providing users with a superior experience quality. In addition,\nthe proposed AVIN-Chat emotionally speaks and expresses according to the user's\nemotional state. Thus, it enables users to establish a strong bond with the\nchatbot system, increasing the user's immersion. Through user subjective tests,\nit is demonstrated that the proposed system provides users with a higher sense\nof immersion than previous chatbot systems. The demonstration video is\navailable at https://www.youtube.com/watch?v=Z74uIV9k7_k.",
      "tldr_zh": "本文提出 AVIN-Chat，一种音频-视觉互动聊天机器人系统，允许用户与 3D 头像进行实时面对面对话，提供比文本或语音-only 系统更优的用户体验。系统通过 emotional state tuning 技术，根据用户的感情状态调整对话和表情表达，从而增强用户沉浸感和情感联系。通过用户主观测试，AVIN-Chat 证明了其在提升沉浸感方面优于现有聊天机器人系统。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00012v1",
      "published_date": "2024-08-15 22:45:53 UTC",
      "updated_date": "2024-08-15 22:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:28:17.806771"
    },
    {
      "arxiv_id": "2408.08444v2",
      "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Jinming Nian",
        "Zhiyuan Peng",
        "Qifan Wang",
        "Yi Fang"
      ],
      "abstract": "In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nlarge language models (LLMs) often struggle to generate factual answers,\nrelying solely on their internal (parametric) knowledge. To address this\nlimitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by\nretrieving relevant information from external sources, thereby positioning the\nretriever as a pivotal component. Although dense retrieval demonstrates\nstate-of-the-art performance, its training poses challenges due to the scarcity\nof ground-truth evidence, largely attributed to the high costs of human\nannotation. In this paper, we propose W-RAG, a method that draws weak training\nsignals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the\nretriever to prioritize passages that most benefit the task. Specifically, we\nrerank the top-$k$ passages retrieved via BM25 by assessing the probability\nthat the LLM will generate the correct answer for a question given each\npassage. The highest-ranking passages are then used as positive fine-tuning\nexamples for dense retrieval. We conduct comprehensive experiments across four\npublicly available OpenQA datasets to demonstrate that our approach enhances\nboth retrieval and OpenQA performance compared to baseline models, achieving\nresults comparable to models fine-tuned with human-labeled data.",
      "tldr_zh": "论文提出 W-RAG 方法，通过弱监督信号来微调 RAG 系统中的密集检索 (dense retrieval)，以提升开放域问答 (OpenQA) 任务的性能。该方法从下游任务如 OpenQA 中提取弱信号，对 BM25 检索的 top-k 段落重新排名，基于 LLMs 生成正确答案的概率选择最高排名的段落作为正样本用于训练。实验结果显示，在四个公开数据集上，W-RAG 比基线模型显著提高了检索和问答性能，甚至与使用人工标注数据的模型相当。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08444v2",
      "published_date": "2024-08-15 22:34:44 UTC",
      "updated_date": "2025-04-25 18:01:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:28:30.914631"
    },
    {
      "arxiv_id": "2408.10264v1",
      "title": "OPDR: Order-Preserving Dimension Reduction for Semantic Embedding of Multimodal Scientific Data",
      "title_zh": "OPDR：用于多模态科学数据语义嵌入的顺序保持降维",
      "authors": [
        "Chengyu Gong",
        "Gefei Shen",
        "Luanzheng Guo",
        "Nathan Tallent",
        "Dongfang Zhao"
      ],
      "abstract": "One of the most common operations in multimodal scientific data management is\nsearching for the $k$ most similar items (or, $k$-nearest neighbors, KNN) from\nthe database after being provided a new item. Although recent advances of\nmultimodal machine learning models offer a \\textit{semantic} index, the\nso-called \\textit{embedding vectors} mapped from the original multimodal data,\nthe dimension of the resulting embedding vectors are usually on the order of\nhundreds or a thousand, which are impractically high for time-sensitive\nscientific applications.\n  This work proposes to reduce the dimensionality of the output embedding\nvectors such that the set of top-$k$ nearest neighbors do not change in the\nlower-dimensional space, namely Order-Preserving Dimension Reduction (OPDR). In\norder to develop such an OPDR method, our central hypothesis is that by\nanalyzing the intrinsic relationship among key parameters during the\ndimension-reduction map, a quantitative function may be constructed to reveal\nthe correlation between the target (lower) dimensionality and other variables.\nTo demonstrate the hypothesis, this paper first defines a formal measure\nfunction to quantify the KNN similarity for a specific vector, then extends the\nmeasure into an aggregate accuracy of the global metric spaces, and finally\nderives a closed-form function between the target (lower) dimensionality and\nother variables. We incorporate the closed-function into popular\ndimension-reduction methods, various distance metrics, and embedding models.",
      "tldr_zh": "本文提出 OPDR（Order-Preserving Dimension Reduction）方法，用于多模态科学数据的语义嵌入降维，旨在降低嵌入向量的维度（如数百或数千）而保持 k-nearest neighbors (KNN) 搜索结果的顺序不变，以适应时间敏感的应用。研究假设通过分析降维映射中的关键参数关系，可以构建一个定量函数来量化目标维度与其他变量的关联，并推导出一个封闭形式的函数。最终，该方法被整合到流行降维技术、各种距离度量和嵌入模型中，展示了全局度量空间的聚合准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.10264v1",
      "published_date": "2024-08-15 22:30:44 UTC",
      "updated_date": "2024-08-15 22:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:28:42.415994"
    },
    {
      "arxiv_id": "2408.08437v1",
      "title": "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Kshitij Bhardwaj"
      ],
      "abstract": "While Vision Transformers (ViTs) are extremely effective at computer vision\ntasks and are replacing convolutional neural networks as the new\nstate-of-the-art, they are complex and memory-intensive models. In order to\neffectively run these models on resource-constrained mobile/edge systems, there\nis a need to not only compress these models but also to optimize them and\nconvert them into deployment-friendly formats. To this end, this paper presents\na combined pruning and quantization tool, called PQV-Mobile, to optimize vision\ntransformers for mobile applications. The tool is able to support different\ntypes of structured pruning based on magnitude importance, Taylor importance,\nand Hessian importance. It also supports quantization from FP32 to FP16 and\nint8, targeting different mobile hardware backends. We demonstrate the\ncapabilities of our tool and show important latency-memory-accuracy trade-offs\nfor different amounts of pruning and int8 quantization with Facebook Data\nEfficient Image Transformer (DeiT) models. Our results show that even pruning a\nDeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing\nfor mobile applications, we find a latency reduction by 7.18X with a small\naccuracy loss of 2.24%. The tool is open source.",
      "tldr_zh": "该论文介绍了 PQV-Mobile，一种结合修剪和量化工具，用于优化 Vision Transformers (ViTs)，以适应资源受限的移动应用。PQV-Mobile 支持基于 magnitude importance、Taylor importance 和 Hessian importance 的结构化修剪，以及从 FP32 到 FP16 和 int8 的量化，针对不同移动硬件后端。实验结果显示，在 Facebook 的 DeiT 模型上，修剪 9.375% 并量化到 int8 可将延迟降低 7.18 倍，同时仅损失 2.24% 的准确率，且该工具是开源的。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08437v1",
      "published_date": "2024-08-15 22:10:10 UTC",
      "updated_date": "2024-08-15 22:10:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:28:54.661946"
    },
    {
      "arxiv_id": "2408.08435v2",
      "title": "Automated Design of Agentic Systems",
      "title_zh": "代理系统的自动化设计",
      "authors": [
        "Shengran Hu",
        "Cong Lu",
        "Jeff Clune"
      ],
      "abstract": "Researchers are investing substantial effort in developing powerful\ngeneral-purpose agents, wherein Foundation Models are used as modules within\nagentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However,\nthe history of machine learning teaches us that hand-designed solutions are\neventually replaced by learned solutions. We describe a newly forming research\narea, Automated Design of Agentic Systems (ADAS), which aims to automatically\ncreate powerful agentic system designs, including inventing novel building\nblocks and/or combining them in new ways. We further demonstrate that there is\nan unexplored yet promising approach within ADAS where agents can be defined in\ncode and new agents can be automatically discovered by a meta agent programming\never better ones in code. Given that programming languages are Turing Complete,\nthis approach theoretically enables the learning of any possible agentic\nsystem: including novel prompts, tool use, workflows, and combinations thereof.\nWe present a simple yet effective algorithm named Meta Agent Search to\ndemonstrate this idea, where a meta agent iteratively programs interesting new\nagents based on an ever-growing archive of previous discoveries. Through\nextensive experiments across multiple domains including coding, science, and\nmath, we show that our algorithm can progressively invent agents with novel\ndesigns that greatly outperform state-of-the-art hand-designed agents.\nImportantly, we consistently observe the surprising result that agents invented\nby Meta Agent Search maintain superior performance even when transferred across\ndomains and models, demonstrating their robustness and generality. Provided we\ndevelop it safely, our work illustrates the potential of an exciting new\nresearch direction toward automatically designing ever-more powerful agentic\nsystems to benefit humanity.",
      "tldr_zh": "该论文介绍了 Automated Design of Agentic Systems (ADAS) 的新研究领域，旨在自动设计强大的代理系统，以取代手动设计的方案，如 Chain-of-Thought 和 Self-Reflection。研究提出 Meta Agent Search 算法，通过一个元代理（meta agent）在代码中迭代发现和编程新代理，允许发明新型构建块和工作流。实验在编码、科学和数学等领域显示，该算法创造的代理性能远超现有手动设计代理，并在跨域和跨模型转移时保持优越性，最终强调安全开发此方向的潜力，以造福人类。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Website: https://shengranhu.com/ADAS",
      "pdf_url": "http://arxiv.org/pdf/2408.08435v2",
      "published_date": "2024-08-15 21:59:23 UTC",
      "updated_date": "2025-03-02 05:13:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:29:07.373929"
    },
    {
      "arxiv_id": "2408.08432v1",
      "title": "Predictive uncertainty estimation in deep learning for lung carcinoma classification in digital pathology under real dataset shifts",
      "title_zh": "翻译失败",
      "authors": [
        "Abdur R. Fayjie",
        "Jutika Borah",
        "Florencia Carbone",
        "Jan Tack",
        "Patrick Vandewalle"
      ],
      "abstract": "Deep learning has shown tremendous progress in a wide range of digital\npathology and medical image classification tasks. Its integration into safe\nclinical decision-making support requires robust and reliable models. However,\nreal-world data comes with diversities that often lie outside the intended\nsource distribution. Moreover, when test samples are dramatically different,\nclinical decision-making is greatly affected. Quantifying predictive\nuncertainty in models is crucial for well-calibrated predictions and\ndetermining when (or not) to trust a model. Unfortunately, many works have\noverlooked the importance of predictive uncertainty estimation. This paper\nevaluates whether predictive uncertainty estimation adds robustness to deep\nlearning-based diagnostic decision-making systems. We investigate the effect of\nvarious carcinoma distribution shift scenarios on predictive performance and\ncalibration. We first systematically investigate three popular methods for\nimproving predictive uncertainty: Monte Carlo dropout, deep ensemble, and\nfew-shot learning on lung adenocarcinoma classification as a primary disease in\nwhole slide images. Secondly, we compare the effectiveness of the methods in\nterms of performance and calibration under clinically relevant distribution\nshifts such as in-distribution shifts comprising primary disease sub-types and\nother characterization analysis data; out-of-distribution shifts comprising\nwell-differentiated cases, different organ origin, and imaging modality shifts.\nWhile studies on uncertainty estimation exist, to our best knowledge, no\nrigorous large-scale benchmark compares predictive uncertainty estimation\nincluding these dataset shifts for lung carcinoma classification.",
      "tldr_zh": "这篇论文探讨了在深度学习模型中估计预测不确定性，以提升肺癌分类在数字病理学中的鲁棒性和可靠性，尤其在真实数据集偏移场景下。研究系统评估了三种方法——Monte Carlo dropout、deep ensemble 和 few-shot learning——应用于肺腺癌分类的whole slide images。实验比较了这些方法在in-distribution shifts（如疾病子类型）和out-of-distribution shifts（如不同器官起源或成像模式）下的性能和校准效果，证明了不确定性估计能显著增强诊断决策系统的鲁棒性，并提供了首个针对肺癌分类的严格大规模基准测试。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "17 pages, 2 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.08432v1",
      "published_date": "2024-08-15 21:49:43 UTC",
      "updated_date": "2024-08-15 21:49:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:29:19.576743"
    },
    {
      "arxiv_id": "2408.08431v1",
      "title": "Multi-Modal Dialogue State Tracking for Playing GuessWhich Game",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Pang",
        "Ruixue Duan",
        "Jinfu Yang",
        "Ning Li"
      ],
      "abstract": "GuessWhich is an engaging visual dialogue game that involves interaction\nbetween a Questioner Bot (QBot) and an Answer Bot (ABot) in the context of\nimage-guessing. In this game, QBot's objective is to locate a concealed image\nsolely through a series of visually related questions posed to ABot. However,\neffectively modeling visually related reasoning in QBot's decision-making\nprocess poses a significant challenge. Current approaches either lack visual\ninformation or rely on a single real image sampled at each round as decoding\ncontext, both of which are inadequate for visual reasoning. To address this\nlimitation, we propose a novel approach that focuses on visually related\nreasoning through the use of a mental model of the undisclosed image. Within\nthis framework, QBot learns to represent mental imagery, enabling robust visual\nreasoning by tracking the dialogue state. The dialogue state comprises a\ncollection of representations of mental imagery, as well as representations of\nthe entities involved in the conversation. At each round, QBot engages in\nvisually related reasoning using the dialogue state to construct an internal\nrepresentation, generate relevant questions, and update both the dialogue state\nand internal representation upon receiving an answer. Our experimental results\non the VisDial datasets (v0.5, 0.9, and 1.0) demonstrate the effectiveness of\nour proposed model, as it achieves new state-of-the-art performance across all\nmetrics and datasets, surpassing previous state-of-the-art models. Codes and\ndatasets from our experiments are freely available at\n\\href{https://github.com/xubuvd/GuessWhich}.",
      "tldr_zh": "这篇论文提出了一种多模态对话状态跟踪方法，用于 GuessWhich 游戏中 QBot 和 ABot 的互动，QBot 通过构建隐藏图像的 mental model 来进行视觉相关推理。方法的核心是跟踪对话状态，包括 mental imagery 和实体表示，从而在每个回合生成相关问题并更新状态，以解决现有方法的视觉信息不足问题。实验在 VisDial 数据集（v0.5、0.9 和 1.0）上显示，该模型在所有指标上达到了新的 state-of-the-art 性能，超越了先前模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at CICAI 2023 (CAAI-A), codes at\n  https://github.com/xubuvd/GuessWhich",
      "pdf_url": "http://arxiv.org/pdf/2408.08431v1",
      "published_date": "2024-08-15 21:46:19 UTC",
      "updated_date": "2024-08-15 21:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:29:29.668766"
    },
    {
      "arxiv_id": "2408.08422v1",
      "title": "Assessing and Enhancing Large Language Models in Rare Disease Question-answering",
      "title_zh": "翻译失败",
      "authors": [
        "Guanchu Wang",
        "Junhao Ran",
        "Ruixiang Tang",
        "Chia-Yuan Chang",
        "Chia-Yuan Chang",
        "Yu-Neng Chuang",
        "Zirui Liu",
        "Vladimir Braverman",
        "Zhandong Liu",
        "Xia Hu"
      ],
      "abstract": "Despite the impressive capabilities of Large Language Models (LLMs) in\ngeneral medical domains, questions remain about their performance in diagnosing\nrare diseases. To answer this question, we aim to assess the diagnostic\nperformance of LLMs in rare diseases, and explore methods to enhance their\neffectiveness in this area. In this work, we introduce a rare disease\nquestion-answering (ReDis-QA) dataset to evaluate the performance of LLMs in\ndiagnosing rare diseases. Specifically, we collected 1360 high-quality\nquestion-answer pairs within the ReDis-QA dataset, covering 205 rare diseases.\nAdditionally, we annotated meta-data for each question, facilitating the\nextraction of subsets specific to any given disease and its property. Based on\nthe ReDis-QA dataset, we benchmarked several open-source LLMs, revealing that\ndiagnosing rare diseases remains a significant challenge for these models.\n  To facilitate retrieval augmentation generation for rare disease diagnosis,\nwe collect the first rare diseases corpus (ReCOP), sourced from the National\nOrganization for Rare Disorders (NORD) database. Specifically, we split the\nreport of each rare disease into multiple chunks, each representing a different\nproperty of the disease, including their overview, symptoms, causes, effects,\nrelated disorders, diagnosis, and standard therapies. This structure ensures\nthat the information within each chunk aligns consistently with a question.\nExperiment results demonstrate that ReCOP can effectively improve the accuracy\nof LLMs on the ReDis-QA dataset by an average of 8%. Moreover, it significantly\nguides LLMs to generate trustworthy answers and explanations that can be traced\nback to existing literature.",
      "tldr_zh": "本文评估了Large Language Models (LLMs)在稀有疾病问答中的诊断性能，发现这些模型在这一领域面临显著挑战。研究者引入了ReDis-QA数据集，包含1360个高质量问题-答案对，覆盖205种稀有疾病，并通过基准测试显示开源LLMs的表现不足。为提升效能，他们构建了首个稀有疾病语料库ReCOP，利用检索增强生成（Retrieval-Augmented Generation）技术，实验结果表明准确率平均提高了8%，并使LLMs生成更可靠且可追溯的答案。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08422v1",
      "published_date": "2024-08-15 21:09:09 UTC",
      "updated_date": "2024-08-15 21:09:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:29:51.817676"
    },
    {
      "arxiv_id": "2408.16780v1",
      "title": "$EvoAl^{2048}$",
      "title_zh": "翻译失败",
      "authors": [
        "Bernhard J. Berger",
        "Christina Plump",
        "Rolf Drechsler"
      ],
      "abstract": "As AI solutions enter safety-critical products, the explainability and\ninterpretability of solutions generated by AI products become increasingly\nimportant. In the long term, such explanations are the key to gaining users'\nacceptance of AI-based systems' decisions. We report on applying a\nmodel-driven-based optimisation to search for an interpretable and explainable\npolicy that solves the game 2048. This paper describes a solution to the\nGECCO'24 Interpretable Control Competition using the open-source software\nEvoAl. We aimed to develop an approach for creating interpretable policies that\nare easy to adapt to new ideas.",
      "tldr_zh": "该论文探讨了AI解决方案在安全关键产品中的可解释性和可解释性问题，强调这些特性对用户接受AI决策的重要性。研究团队使用基于模型的优化方法和开源软件EvoAl，开发了一个可解释策略来解决2048游戏，并针对GECCO'24 Interpretable Control Competition进行了应用。该方法旨在创建易于适应新想法的解释性策略，从而提升AI系统的可信度和灵活性。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "2 pages, GECCO'24 competition entry",
      "pdf_url": "http://arxiv.org/pdf/2408.16780v1",
      "published_date": "2024-08-15 21:06:18 UTC",
      "updated_date": "2024-08-15 21:06:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:30:04.602987"
    },
    {
      "arxiv_id": "2408.08927v2",
      "title": "VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool",
      "title_zh": "翻译失败",
      "authors": [
        "Chia-Tung Ho",
        "Haoxing Ren",
        "Brucek Khailany"
      ],
      "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), automating\nhardware design can prevent a significant amount of human error from the\nengineering process and result in less errors. Verilog is a popular hardware\ndescription language for designing and modeling digital systems; thus, Verilog\ngeneration is one of the emerging areas of research to facilitate the design\nprocess. In this work, we propose VerilogCoder, a system of multiple Artificial\nIntelligence (AI) agents for Verilog code generation, to autonomously write\nVerilog code and fix syntax and functional errors using collaborative Verilog\ntools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we\npropose a task planner that utilizes a novel Task and Circuit Relation Graph\nretrieval method to construct a holistic plan based on module descriptions. To\ndebug and fix functional errors, we develop a novel and efficient abstract\nsyntax tree (AST)-based waveform tracing tool, which is integrated within the\nautonomous Verilog completion flow. The proposed methodology successfully\ngenerates 94.2% syntactically and functionally correct Verilog code, surpassing\nthe state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.",
      "tldr_zh": "该研究提出VerilogCoder，一种自主Verilog代码生成系统，由多个AI代理组成，用于编写和修复Verilog代码的语法及功能错误。系统采用Graph-based Planning的Task and Circuit Relation Graph检索方法，基于模块描述构建整体任务计划，并集成Abstract Syntax Tree (AST)-based Waveform Tracing Tool来高效调试功能错误。实验结果显示，VerilogCoder在VerilogEval-Human v2基准上成功生成94.2%的语法和功能正确代码，比现有方法提高了33.9%。这为自动化硬件设计提供了可靠的解决方案，减少了人为错误。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "main paper 7 pages, reference 1 page, it is the version that accepted\n  by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.08927v2",
      "published_date": "2024-08-15 20:06:06 UTC",
      "updated_date": "2025-03-05 06:23:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:30:17.454931"
    },
    {
      "arxiv_id": "2408.08401v1",
      "title": "Understanding Help-Seeking Behavior of Students Using LLMs vs. Web Search for Writing SQL Queries",
      "title_zh": "理解学生使用 LLMs 与 Web Search 编写 SQL 查询时的帮助寻求行为",
      "authors": [
        "Harsh Kumar",
        "Mohi Reza",
        "Jeb Mitchell",
        "Ilya Musabirov",
        "Lisa Zhang",
        "Michael Liut"
      ],
      "abstract": "Growth in the use of large language models (LLMs) in programming education is\naltering how students write SQL queries. Traditionally, students relied heavily\non web search for coding assistance, but this has shifted with the adoption of\nLLMs like ChatGPT. However, the comparative process and outcomes of using web\nsearch versus LLMs for coding help remain underexplored. To address this, we\nconducted a randomized interview study in a database classroom to compare web\nsearch and LLMs, including a publicly available LLM (ChatGPT) and an\ninstructor-tuned LLM, for writing SQL queries. Our findings indicate that using\nan instructor-tuned LLM required significantly more interactions than both\nChatGPT and web search, but resulted in a similar number of edits to the final\nSQL query. No significant differences were found in the quality of the final\nSQL queries between conditions, although the LLM conditions directionally\nshowed higher query quality. Furthermore, students using instructor-tuned LLM\nreported a lower mental demand. These results have implications for learning\nand productivity in programming education.",
      "tldr_zh": "本研究比较了学生在编写 SQL 查询时，使用大型语言模型 (LLMs) 如 ChatGPT 与网络搜索的求助行为，探讨了这些方法对学习过程的影响。研究通过随机访谈在数据库课堂中测试了网络搜索、公开 LLM (ChatGPT) 和教师调整的 LLM，结果显示教师调整的 LLM 需要更多互动，但导致的最终 SQL 查询编辑次数类似，且学生报告的心理需求较低。尽管查询质量无显著差异，LLM 条件显示出方向性的更高质量。这些发现对编程教育的学习效率和生产力提供了重要启示。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.DB"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08401v1",
      "published_date": "2024-08-15 19:58:41 UTC",
      "updated_date": "2024-08-15 19:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:30:29.623752"
    },
    {
      "arxiv_id": "2408.11856v2",
      "title": "Dynamic Adaptive Optimization for Effective Sentiment Analysis Fine-Tuning on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hongcheng Ding",
        "Xuanze Zhao",
        "Shamsul Nahar Abdullah",
        "Deshinta Arrova Dewi",
        "Zixiao Jiang",
        "Xiangyu Shi"
      ],
      "abstract": "Sentiment analysis plays a crucial role in various domains, such as business\nintelligence and financial forecasting. Large language models (LLMs) have\nbecome a popular paradigm for sentiment analysis, leveraging multi-task\nlearning to address specific tasks concurrently. However, LLMs with fine-tuning\nfor sentiment analysis often underperforms due to the inherent challenges in\nmanaging diverse task complexities. Moreover, constant-weight approaches in\nmulti-task learning struggle to adapt to variations in data characteristics,\nfurther complicating model effectiveness. To address these issues, we propose a\nnovel multi-task learning framework with a dynamic adaptive optimization (DAO)\nmodule. This module is designed as a plug-and-play component that can be\nseamlessly integrated into existing models, providing an effective and flexible\nsolution for multi-task learning. The key component of the DAO module is\ndynamic adaptive loss, which dynamically adjusts the weights assigned to\ndifferent tasks based on their relative importance and data characteristics\nduring training. Sentiment analyses on a standard and customized financial text\ndataset demonstrate that the proposed framework achieves superior performance.\nSpecifically, this work improves the Mean Squared Error (MSE) and Accuracy\n(ACC) by 15.58% and 1.24% respectively, compared with previous work.",
      "tldr_zh": "这篇论文针对大语言模型(LLMs)在情感分析细调中的表现问题，提出了一种新型多任务学习框架，引入动态自适应优化(DAO)模块作为即插即用组件。DAO模块的核心是动态自适应损失(dynamic adaptive loss)，它根据任务的重要性和数据特征动态调整权重，以更好地处理多任务复杂性和数据变异。实验结果显示，该框架在标准和自定义金融文本数据集上显著提升性能，Mean Squared Error (MSE)改善15.58%，Accuracy (ACC)改善1.24%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.11856v2",
      "published_date": "2024-08-15 19:13:38 UTC",
      "updated_date": "2024-11-12 05:37:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:30:41.971625"
    },
    {
      "arxiv_id": "2408.08376v2",
      "title": "Decoding the human brain tissue response to radiofrequency excitation using a biophysical-model-free deep MRI on a chip framework",
      "title_zh": "翻译失败",
      "authors": [
        "Dinor Nagar",
        "Moritz Zaiss",
        "Or Perlman"
      ],
      "abstract": "Magnetic resonance imaging (MRI) relies on radiofrequency (RF) excitation of\nproton spin. Clinical diagnosis requires a comprehensive collation of\nbiophysical data via multiple MRI contrasts, acquired using a series of RF\nsequences that lead to lengthy examinations. Here, we developed a vision\ntransformer-based framework that captures the spatiotemporal magnetic signal\nevolution and decodes the brain tissue response to RF excitation, constituting\nan MRI on a chip. Following a per-subject rapid calibration scan (28.2 s), a\nwide variety of image contrasts including fully quantitative molecular, water\nrelaxation, and magnetic field maps can be generated automatically. The method\nwas validated across healthy subjects and a cancer patient in two different\nimaging sites, and proved to be 94% faster than alternative protocols. The deep\nMRI on a chip (DeepMonC) framework may reveal the molecular composition of the\nhuman brain tissue in a wide range of pathologies, while offering clinically\nattractive scan times.",
      "tldr_zh": "这篇论文提出了一种基于 Vision Transformer 的 DeepMonC 框架，用于解码大脑组织对射频 (RF) 激发的响应，而不依赖生物物理模型，从而实现“MRI on a chip”。该框架通过每个受试者快速校准扫描 (28.2 秒) 后，自动生成多种图像对比度，包括分子、水弛豫和磁场图，大大简化了传统 MRI 扫描过程。在健康受试者和癌症患者中进行验证，该方法比替代协议快 94%，有望加速临床诊断并揭示大脑组织的分子组成。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "physics.med-ph",
      "comment": "This project was funded by the European Union (ERC, BabyMagnet,\n  project no. 101115639). Views and opinions expressed are however those of the\n  authors only and do not necessarily reflect those of the European Union or\n  the European Research Council. Neither the European Union nor the granting\n  authority can be held responsible for them",
      "pdf_url": "http://arxiv.org/pdf/2408.08376v2",
      "published_date": "2024-08-15 18:39:33 UTC",
      "updated_date": "2024-08-19 05:15:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:30:54.108798"
    },
    {
      "arxiv_id": "2408.08313v3",
      "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
      "title_zh": "大型语言模型能理解符号图形程序吗？",
      "authors": [
        "Zeju Qiu",
        "Weiyang Liu",
        "Haiwen Feng",
        "Zhen Liu",
        "Tim Z. Xiao",
        "Katherine M. Collins",
        "Joshua B. Tenenbaum",
        "Adrian Weller",
        "Michael J. Black",
        "Bernhard Schölkopf"
      ],
      "abstract": "Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.",
      "tldr_zh": "本研究评估大型语言模型（LLMs）是否能理解符号图形程序（symbolic graphics programs），通过这一新领域测试其空间-语义推理能力，而非依赖视觉编码器。研究者构建了一个大型基准，利用符号程序过程生成视觉数据，并强调图像变换对程序的影响，评估了各种 LLMs 的性能，发现推理能力更强的模型表现更好。为提升 LLMs 的理解能力，他们引入了 Symbolic Instruction Tuning (SIT) 方法，通过微调模型与预先收集的指令数据，不仅改善了其对符号程序的语义理解，还提升了其他基准上的通用推理能力。总的来说，此工作为科学评估 LLMs 的局限性和潜力提供了新工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical Report v3 (47 pages, 26 figures, project page:\n  https://sgp-bench.github.io/, added visual illusion examples)",
      "pdf_url": "http://arxiv.org/pdf/2408.08313v3",
      "published_date": "2024-08-15 17:59:57 UTC",
      "updated_date": "2024-12-11 21:42:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:31:07.710093"
    },
    {
      "arxiv_id": "2408.08312v1",
      "title": "HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyu Li",
        "Snehal Dikhale",
        "Jinda Cui",
        "Soshi Iba",
        "Nawid Jamali"
      ],
      "abstract": "To achieve dexterity comparable to that of humans, robots must intelligently\nprocess tactile sensor data. Taxel-based tactile signals often have low\nspatial-resolution, with non-standardized representations. In this paper, we\npropose a novel framework, HyperTaxel, for learning a geometrically-informed\nrepresentation of taxel-based tactile signals to address challenges associated\nwith their spatial resolution. We use this representation and a contrastive\nlearning objective to encode and map sparse low-resolution taxel signals to\nhigh-resolution contact surfaces. To address the uncertainty inherent in these\nsignals, we leverage joint probability distributions across multiple\nsimultaneous contacts to improve taxel hyper-resolution. We evaluate our\nrepresentation by comparing it with two baselines and present results that\nsuggest our representation outperforms the baselines. Furthermore, we present\nqualitative results that demonstrate the learned representation captures the\ngeometric features of the contact surface, such as flatness, curvature, and\nedges, and generalizes across different objects and sensor configurations.\nMoreover, we present results that suggest our representation improves the\nperformance of various downstream tasks, such as surface classification, 6D\nin-hand pose estimation, and sim-to-real transfer.",
      "tldr_zh": "本研究提出HyperTaxel框架，通过contrastive learning学习几何信息丰富的表示，以提升taxel-based tactile signals的空间分辨率。该框架将稀疏的低分辨率触觉信号映射到高分辨率接触表面，并利用多个同时接触的联合概率分布来处理信号不确定性。实验结果显示，HyperTaxel的表现优于两个基线模型，能够准确捕捉接触表面的几何特征，如平坦度、曲率和边缘，并适用于不同物体和传感器配置。此外，该表示显著改善下游任务的性能，包括表面分类、6D in-hand pose estimation和sim-to-real transfer。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by IROS 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08312v1",
      "published_date": "2024-08-15 17:59:53 UTC",
      "updated_date": "2024-08-15 17:59:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:31:16.842875"
    },
    {
      "arxiv_id": "2408.16961v2",
      "title": "The Future of Open Human Feedback",
      "title_zh": "开放人类反馈的未来",
      "authors": [
        "Shachar Don-Yehiya",
        "Ben Burtenshaw",
        "Ramon Fernandez Astudillo",
        "Cailean Osborne",
        "Mimansa Jaiswal",
        "Tzu-Sheng Kuo",
        "Wenting Zhao",
        "Idan Shenfeld",
        "Andi Peng",
        "Mikhail Yurochkin",
        "Atoosa Kasirzadeh",
        "Yangsibo Huang",
        "Tatsunori Hashimoto",
        "Yacine Jernite",
        "Daniel Vila-Suero",
        "Omri Abend",
        "Jennifer Ding",
        "Sara Hooker",
        "Hannah Rose Kirk",
        "Leshem Choshen"
      ],
      "abstract": "Human feedback on conversations with language language models (LLMs) is\ncentral to how these systems learn about the world, improve their capabilities,\nand are steered toward desirable and safe behaviors. However, this feedback is\nmostly collected by frontier AI labs and kept behind closed doors. In this\nwork, we bring together interdisciplinary experts to assess the opportunities\nand challenges to realizing an open ecosystem of human feedback for AI. We\nfirst look for successful practices in peer production, open source, and\ncitizen science communities. We then characterize the main challenges for open\nhuman feedback. For each, we survey current approaches and offer\nrecommendations. We end by envisioning the components needed to underpin a\nsustainable and open human feedback ecosystem. In the center of this ecosystem\nare mutually beneficial feedback loops, between users and specialized models,\nincentivizing a diverse stakeholders community of model trainers and feedback\nproviders to support a general open feedback pool.",
      "tldr_zh": "本研究探讨了开放人类反馈生态系统的未来，强调人类反馈在语言模型（LLMs）学习、能力提升和行为导向中的核心作用，但目前此类反馈主要被前沿 AI 实验室封闭管理。作者汇集跨学科专家，考察同行生产、开源和公民科学社区的成功实践，分析开放反馈的挑战，并针对每个挑战提供当前方法和推荐。最终，论文展望了一个可持续生态系统，包括用户与专业模型间的互惠反馈循环，以激励多样利益相关者参与，建立通用的开放反馈池。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16961v2",
      "published_date": "2024-08-15 17:59:14 UTC",
      "updated_date": "2024-09-04 15:39:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:31:28.248703"
    },
    {
      "arxiv_id": "2408.08302v1",
      "title": "Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors",
      "title_zh": "大型语言模型在交通系统工程中的能力基准测试：准确性、一致性和推理行为",
      "authors": [
        "Usman Syed",
        "Ethan Light",
        "Xingang Guo",
        "Huan Zhang",
        "Lianhui Qin",
        "Yanfeng Ouyang",
        "Bin Hu"
      ],
      "abstract": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.",
      "tldr_zh": "本研究评估了多种大型语言模型（LLMs），如 GPT-4、GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro，在解决本科级交通工程问题方面的能力，特别是其准确性、一致性和推理行为。研究引入了 TransportBench 数据集，该数据集包含交通系统规划、设计、管理和控制等领域的样本问题，并由人类专家用于评估这些 LLMs 的表现。结果显示，每个模型都有独特优势和局限性，例如 Claude 3.5 Sonnet 表现出色但存在意外的不一致行为，这标志着利用人工智能泛化智能解决复杂交通挑战的初步进展。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08302v1",
      "published_date": "2024-08-15 17:55:45 UTC",
      "updated_date": "2024-08-15 17:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:31:43.135067"
    },
    {
      "arxiv_id": "2408.08295v1",
      "title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training",
      "title_zh": "翻译失败",
      "authors": [
        "Gengwei Zhang",
        "Liyuan Wang",
        "Guoliang Kang",
        "Ling Chen",
        "Yunchao Wei"
      ],
      "abstract": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.",
      "tldr_zh": "本研究探讨了基于预训练模型（PTMs）的持续学习（CLPT），强调顺序微调（Seq FT）作为一种被忽略的基准方法，以解决预训练知识过度拟合下游任务的问题。论文提出 SLCA++ 框架，包括 Slow Learner 组件来选择性地降低骨干参数的学习率，以及 Classifier Alignment 模块来后验调整分类层，并通过对称交叉熵损失和参数高效策略增强其效能。在图像分类基准上的各种持续学习场景中，SLCA++ 显著优于现有最先进方法，提供大幅性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is an extension of our ICCV 23 paper (arXiv:2303.05118)",
      "pdf_url": "http://arxiv.org/pdf/2408.08295v1",
      "published_date": "2024-08-15 17:50:07 UTC",
      "updated_date": "2024-08-15 17:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:31:53.324452"
    },
    {
      "arxiv_id": "2408.08282v1",
      "title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Wang",
        "Arturo Laurenzi",
        "Nikos Tsagarakis"
      ],
      "abstract": "Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning.",
      "tldr_zh": "本研究针对人形机器人（humanoid robots）在无结构环境中进行自主 loco-manipulation（移动和操作）的挑战，提出了一种基于 Grounded Language Model 的框架，利用大型语言模型（LLMs）的规划和推理能力来处理多模态输入。框架允许机器人根据文本指令自主规划行为、执行低级任务，并通过观察和纠正失败来实现长期任务的偏差调整；为此，研究者创建了机器人'行动'和'感知'行为库。实验在模拟和真实环境中使用 CENTAURO 机器人进行了移动操作任务，验证了该框架的有效性，展示了 LLMs 在机器人自主行为规划中的实际应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Paper accepted by IROS 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08282v1",
      "published_date": "2024-08-15 17:33:32 UTC",
      "updated_date": "2024-08-15 17:33:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:32:06.317982"
    },
    {
      "arxiv_id": "2408.08926v4",
      "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
      "title_zh": "Cybench：一个用于评估语言模型网络安全能力和风险的框架",
      "authors": [
        "Andy K. Zhang",
        "Neil Perry",
        "Riya Dulepet",
        "Joey Ji",
        "Celeste Menders",
        "Justin W. Lin",
        "Eliot Jones",
        "Gashon Hussein",
        "Samantha Liu",
        "Donovan Jasper",
        "Pura Peetathawatchai",
        "Ari Glenn",
        "Vikram Sivashankar",
        "Daniel Zamoshchin",
        "Leo Glikbarg",
        "Derek Askaryar",
        "Mike Yang",
        "Teddy Zhang",
        "Rishi Alluri",
        "Nathan Tran",
        "Rinnara Sangpisit",
        "Polycarpos Yiorkadjis",
        "Kenny Osele",
        "Gautham Raghupathi",
        "Dan Boneh",
        "Daniel E. Ho",
        "Percy Liang"
      ],
      "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously\nidentifying vulnerabilities and executing exploits have potential to cause\nreal-world impact. Policymakers, model providers, and researchers in the AI and\ncybersecurity communities are interested in quantifying the capabilities of\nsuch agents to help mitigate cyberrisk and investigate opportunities for\npenetration testing. Toward that end, we introduce Cybench, a framework for\nspecifying cybersecurity tasks and evaluating agents on those tasks. We include\n40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF\ncompetitions, chosen to be recent, meaningful, and spanning a wide range of\ndifficulties. Each task includes its own description, starter files, and is\ninitialized in an environment where an agent can execute commands and observe\noutputs. Since many tasks are beyond the capabilities of existing LM agents, we\nintroduce subtasks for each task, which break down a task into intermediary\nsteps for a more detailed evaluation. To evaluate agent capabilities, we\nconstruct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI\no1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini\n1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing\nmodels (GPT-4o and Claude 3.5 Sonnet), we further investigate performance\nacross 4 agent scaffolds (structed bash, action-only, pseudoterminal, and web\nsearch). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o,\nOpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that\ntook human teams up to 11 minutes to solve. In comparison, the most difficult\ntask took human teams 24 hours and 54 minutes to solve. All code and data are\npublicly available at https://cybench.github.io.",
      "tldr_zh": "本文提出 Cybench 框架，用于评估语言模型（LM）的网络安全能力和风险，通过指定任务并测试代理自主识别漏洞和执行攻击的能力。该框架包含 40 个专业级 Capture the Flag (CTF) 任务，涵盖不同难度，并引入子任务分解以进行更细致的评估。研究评估了 8 个模型如 GPT-4o 和 Claude 3.5 Sonnet，并在 4 种代理支架下测试，结果显示顶尖模型能快速解决人类需时数分钟至数小时的任务，突显 LM 在渗透测试中的潜力与风险。所有代码和数据已在 https://cybench.github.io 公开可用。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "ICLR 2025 Oral",
      "pdf_url": "http://arxiv.org/pdf/2408.08926v4",
      "published_date": "2024-08-15 17:23:10 UTC",
      "updated_date": "2025-04-12 21:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:32:20.027115"
    },
    {
      "arxiv_id": "2408.08264v1",
      "title": "InVAErt networks for amortized inference and identifiability analysis of lumped parameter hemodynamic models",
      "title_zh": "翻译失败",
      "authors": [
        "Guoxiang Grayson Tong",
        "Carlos A. Sing Long",
        "Daniele E. Schiavazzi"
      ],
      "abstract": "Estimation of cardiovascular model parameters from electronic health records\n(EHR) poses a significant challenge primarily due to lack of identifiability.\nStructural non-identifiability arises when a manifold in the space of\nparameters is mapped to a common output, while practical non-identifiability\ncan result due to limited data, model misspecification, or noise corruption. To\naddress the resulting ill-posed inverse problem, optimization-based or Bayesian\ninference approaches typically use regularization, thereby limiting the\npossibility of discovering multiple solutions. In this study, we use inVAErt\nnetworks, a neural network-based, data-driven framework for enhanced digital\ntwin analysis of stiff dynamical systems. We demonstrate the flexibility and\neffectiveness of inVAErt networks in the context of physiological inversion of\na six-compartment lumped parameter hemodynamic model from synthetic data to\nreal data with missing components.",
      "tldr_zh": "该研究探讨了从电子健康记录(EHR)估计心血管模型参数的挑战，主要由于结构非可识别性(structural non-identifiability)和实际非可识别性(practical non-identifiability)导致的逆问题。作者提出 InVAErt networks，一种基于神经网络的摊销推断(amortized inference)框架，用于增强刚性动力系统的数字孪生分析，以处理参数估计中的多解可能性。实验结果显示，该框架在六室参数化血流动力学模型(lumped parameter hemodynamic models)上，从合成数据到真实数据（包括缺失组件）的逆问题中表现出色，提高了模型的灵活性和有效性。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08264v1",
      "published_date": "2024-08-15 17:07:40 UTC",
      "updated_date": "2024-08-15 17:07:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:32:30.892234"
    },
    {
      "arxiv_id": "2408.08258v3",
      "title": "Snuffy: Efficient Whole Slide Image Classifier",
      "title_zh": "Snuffy：高效全滑片",
      "authors": [
        "Hossein Jafarinia",
        "Alireza Alipanah",
        "Danial Hamdi",
        "Saeed Razavi",
        "Nahal Mirzaie",
        "Mohammad Hossein Rohban"
      ],
      "abstract": "Whole Slide Image (WSI) classification with multiple instance learning (MIL)\nin digital pathology faces significant computational challenges. Current\nmethods mostly rely on extensive self-supervised learning (SSL) for\nsatisfactory performance, requiring long training periods and considerable\ncomputational resources. At the same time, no pre-training affects performance\ndue to domain shifts from natural images to WSIs. We introduce Snuffy\narchitecture, a novel MIL-pooling method based on sparse transformers that\nmitigates performance loss with limited pre-training and enables continual\nfew-shot pre-training as a competitive option. Our sparsity pattern is tailored\nfor pathology and is theoretically proven to be a universal approximator with\nthe tightest probabilistic sharp bound on the number of layers for sparse\ntransformers, to date. We demonstrate Snuffy's effectiveness on CAMELYON16 and\nTCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies.\nThe code is available on https://github.com/jafarinia/snuffy.",
      "tldr_zh": "本研究针对 Whole Slide Image (WSI) 分类中的计算挑战，指出现有方法依赖广泛的 self-supervised learning (SSL)，导致训练时间长和资源消耗大，同时缺乏预训练会因领域偏移影响性能。作者提出 Snuffy 架构，这是一种基于 sparse transformers 的新型 multiple instance learning (MIL)-pooling 方法，能够减少预训练损失并支持 continual few-shot pre-training。Snuffy 的稀疏模式专为病理学设计，并理论证明其为 universal approximator，具有目前最严格的 probabilistic sharp bound。在 CAMELYON16 和 TCGA Lung cancer 数据集上，Snuffy 实现了优越的 WSI 和 patch-level 准确率，证明了其高效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08258v3",
      "published_date": "2024-08-15 16:59:15 UTC",
      "updated_date": "2025-03-02 04:25:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:32:43.224838"
    },
    {
      "arxiv_id": "2408.08925v1",
      "title": "Retail-GPT: leveraging Retrieval Augmented Generation (RAG) for building E-commerce Chat Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Bruno Amaral Teixeira de Freitas",
        "Roberto de Alencar Lotufo"
      ],
      "abstract": "This work presents Retail-GPT, an open-source RAG-based chatbot designed to\nenhance user engagement in retail e-commerce by guiding users through product\nrecommendations and assisting with cart operations. The system is\ncross-platform and adaptable to various e-commerce domains, avoiding reliance\non specific chat applications or commercial activities. Retail-GPT engages in\nhuman-like conversations, interprets user demands, checks product availability,\nand manages cart operations, aiming to serve as a virtual sales agent and test\nthe viability of such assistants across different retail businesses.",
      "tldr_zh": "这篇论文介绍了 Retail-GPT，一个开源的基于 Retrieval Augmented Generation (RAG) 技术的聊天机器人，旨在提升零售电商的用户互动，通过产品推荐和购物车操作来指导用户。系统支持跨平台应用，能够适应各种电商领域，而不依赖特定聊天应用或商业活动。Retail-GPT 能够进行人性化对话、解释用户需求、检查产品可用性和管理购物车操作，从而作为虚拟销售代理，并验证其在不同零售业务中的实用性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "5 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08925v1",
      "published_date": "2024-08-15 16:53:05 UTC",
      "updated_date": "2024-08-15 16:53:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:32:54.296291"
    },
    {
      "arxiv_id": "2408.08252v5",
      "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Xiner Li",
        "Yulai Zhao",
        "Chenyu Wang",
        "Gabriele Scalia",
        "Gokcen Eraslan",
        "Surag Nair",
        "Tommaso Biancalani",
        "Shuiwang Ji",
        "Aviv Regev",
        "Sergey Levine",
        "Masatoshi Uehara"
      ],
      "abstract": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.",
      "tldr_zh": "本研究提出了一种无导数指导（Derivative-Free Guidance）方法，用于连续和离散扩散模型（Diffusion Models），通过软值函数（Soft Value-Based Decoding）优化下游奖励函数，同时保持生成空间的自然性。该方法采用迭代采样算法，在预训练扩散模型的标准推理过程中整合软值函数，以评估中间噪声状态对未来奖励的影响，从而避免了现有方法的依赖，如需要可微代理模型（e.g., Classifier Guidance）或计算昂贵的微调。相比传统方法，该算法能直接处理非可微特征/奖励反馈，并适用于离散扩散模型。实验结果显示，该方法在图像生成、分子生成以及DNA/RNA序列生成等领域表现出色，有效提升了生成质量。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "The code is available at https://github.com/masa-ue/SVDD",
      "pdf_url": "http://arxiv.org/pdf/2408.08252v5",
      "published_date": "2024-08-15 16:47:59 UTC",
      "updated_date": "2024-10-25 02:50:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:33:06.352928"
    },
    {
      "arxiv_id": "2408.11855v1",
      "title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models",
      "title_zh": "FactorLLM：通过混合专家对大语言模型的知识进行因子分解",
      "authors": [
        "Zhongyu Zhao",
        "Menghang Dong",
        "Rongyu Zhang",
        "Wenzhao Zheng",
        "Yunpeng Zhang",
        "Huanrui Yang",
        "Dalong Du",
        "Kurt Keutzer",
        "Shanghang Zhang"
      ],
      "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large\nLanguage Models (LLMs) play a pivotal role in storing diverse linguistic and\nfactual knowledge. Conventional methods frequently face challenges due to\nknowledge confusion stemming from their monolithic and redundant architectures,\nwhich calls for more efficient solutions with minimal computational overhead,\nparticularly for LLMs. In this paper, we explore the FFN computation paradigm\nin LLMs and introduce FactorLLM, a novel approach that decomposes well-trained\ndense FFNs into sparse sub-networks without requiring any further\nmodifications, while maintaining the same level of performance. Furthermore, we\nembed a router from the Mixture-of-Experts (MoE), combined with our devised\nPrior-Approximate (PA) loss term that facilitates the dynamic activation of\nexperts and knowledge adaptation, thereby accelerating computational processes\nand enhancing performance using minimal training data and fine-tuning steps.\nFactorLLM thus enables efficient knowledge factorization and activates select\ngroups of experts specifically tailored to designated tasks, emulating the\ninteractive functional segmentation of the human brain. Extensive experiments\nacross various benchmarks demonstrate the effectiveness of our proposed\nFactorLLM which achieves comparable performance to the source model securing up\nto 85% model performance while obtaining over a 30% increase in inference\nspeed. Code: https://github.com/zhenwuweihe/FactorLLM.",
      "tldr_zh": "该研究针对Large Language Models (LLMs)中Feed-Forward Networks (FFNs)的知识存储问题，提出FactorLLM方法，通过将训练好的密集FFNs分解成稀疏子网络，实现高效知识因子化，而无需进一步修改。FactorLLM结合Mixture-of-Experts (MoE)的路由器和设计的Prior-Approximate (PA)损失函数，促进专家动态激活和知识适应，从而加速计算过程并提升性能。实验结果显示，该方法在多种基准测试中与源模型性能相当，保留85%的表现，同时推理速度提高超过30%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.11855v1",
      "published_date": "2024-08-15 16:45:16 UTC",
      "updated_date": "2024-08-15 16:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:33:19.363926"
    },
    {
      "arxiv_id": "2408.16779v2",
      "title": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "João Pedro Gandarela",
        "Danilo S. Carvalho",
        "André Freitas"
      ],
      "abstract": "This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs.",
      "tldr_zh": "这篇论文提出了一种新颖的系统方法，通过正式推理引擎反馈来分析大型语言模型(LLMs)在逻辑理论归纳学习方面的能力和限制，并基于规则依赖结构的复杂性等级进行量化分析。研究强调了整合LLMs与正式方法在自然语言处理(NLP)领域的潜力，能够提升模型的推理控制和可解释性。实验结果显示，最大的LLMs可以与最先进的归纳逻辑编程(ILP)系统竞争，但跟踪长谓词关系链比理论复杂性更具挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16779v2",
      "published_date": "2024-08-15 16:41:00 UTC",
      "updated_date": "2025-01-14 14:26:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:33:32.204590"
    },
    {
      "arxiv_id": "2408.08248v3",
      "title": "Conformalized Answer Set Prediction for Knowledge Graph Embedding",
      "title_zh": "针对知识图谱嵌入的保形化答案集预测",
      "authors": [
        "Yuqicheng Zhu",
        "Nico Potyka",
        "Jiarong Pan",
        "Bo Xiong",
        "Yunjie He",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Knowledge graph embeddings (KGE) apply machine learning methods on knowledge\ngraphs (KGs) to provide non-classical reasoning capabilities based on\nsimilarities and analogies. The learned KG embeddings are typically used to\nanswer queries by ranking all potential answers, but rankings often lack a\nmeaningful probabilistic interpretation - lower-ranked answers do not\nnecessarily have a lower probability of being true. This limitation makes it\ndifficult to quantify uncertainty of model's predictions, posing challenges for\nthe application of KGE methods in high-stakes domains like medicine. We address\nthis issue by applying the theory of conformal prediction that allows\ngenerating answer sets, which contain the correct answer with probabilistic\nguarantees. We explain how conformal prediction can be used to generate such\nanswer sets for link prediction tasks. Our empirical evaluation on four\nbenchmark datasets using six representative KGE methods validates that the\ngenerated answer sets satisfy the probabilistic guarantees given by the theory\nof conformal prediction. We also demonstrate that the generated answer sets\noften have a sensible size and that the size adapts well with respect to the\ndifficulty of the query.",
      "tldr_zh": "该研究针对知识图谱嵌入 (KGE) 的查询排名问题，指出传统方法缺乏概率解释，导致难以量化预测不确定性，尤其在高风险领域如医学中。该方法引入共形预测 (conformal prediction) 理论，生成答案集以确保正确答案包含在集内，并提供概率保证，应用于链接预测任务。实验在四个基准数据集上，使用六种代表性 KGE 方法验证了答案集的概率可靠性，且答案集大小适中，能够根据查询难度动态调整。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a main conference paper at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.08248v3",
      "published_date": "2024-08-15 16:36:59 UTC",
      "updated_date": "2025-01-25 17:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:33:42.029703"
    },
    {
      "arxiv_id": "2408.08242v1",
      "title": "A Conflicts-free, Speed-lossless KAN-based Reinforcement Learning Decision System for Interactive Driving in Roundabouts",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Lin",
        "Zhen Tian",
        "Qi Zhang",
        "Ziyang Ye",
        "Hanyang Zhuang",
        "Jianglin Lan"
      ],
      "abstract": "Safety and efficiency are crucial for autonomous driving in roundabouts,\nespecially in the context of mixed traffic where autonomous vehicles (AVs) and\nhuman-driven vehicles coexist. This paper introduces a learning-based algorithm\ntailored to foster safe and efficient driving behaviors across varying levels\nof traffic flows in roundabouts. The proposed algorithm employs a deep\nQ-learning network to effectively learn safe and efficient driving strategies\nin complex multi-vehicle roundabouts. Additionally, a KAN (Kolmogorov-Arnold\nnetwork) enhances the AVs' ability to learn their surroundings robustly and\nprecisely. An action inspector is integrated to replace dangerous actions to\navoid collisions when the AV interacts with the environment, and a route\nplanner is proposed to enhance the driving efficiency and safety of the AVs.\nMoreover, a model predictive control is adopted to ensure stability and\nprecision of the driving actions. The results show that our proposed system\nconsistently achieves safe and efficient driving whilst maintaining a stable\ntraining process, as evidenced by the smooth convergence of the reward function\nand the low variance in the training curves across various traffic flows.\nCompared to state-of-the-art benchmarks, the proposed algorithm achieves a\nlower number of collisions and reduced travel time to destination.",
      "tldr_zh": "这篇论文提出了一种无冲突、无速度损失的基于 KAN（Kolmogorov-Arnold network）的强化学习决策系统，用于环形交叉路口的自主车辆（AVs）互动驾驶，旨在提升安全性和效率。系统采用深度 Q 学习网络来学习复杂多车环境的驾驶策略，并整合 KAN 增强环境感知、行动检查器避免碰撞、路线规划器优化路径，以及模型预测控制确保行动的稳定性和精确性。实验结果显示，该系统在各种交通流量下实现了平稳训练和奖励函数的平滑收敛，与现有基准相比，显著降低了碰撞次数和旅行时间。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 12 figures, submitted to an IEEE journal",
      "pdf_url": "http://arxiv.org/pdf/2408.08242v1",
      "published_date": "2024-08-15 16:10:25 UTC",
      "updated_date": "2024-08-15 16:10:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:33:54.767767"
    },
    {
      "arxiv_id": "2408.08230v1",
      "title": "Explaining an Agent's Future Beliefs through Temporally Decomposing Future Reward Estimators",
      "title_zh": "通过时间分解未来奖励估计器解释代理的未来信念",
      "authors": [
        "Mark Towers",
        "Yali Du",
        "Christopher Freeman",
        "Timothy J. Norman"
      ],
      "abstract": "Future reward estimation is a core component of reinforcement learning\nagents; i.e., Q-value and state-value functions, predicting an agent's sum of\nfuture rewards. Their scalar output, however, obfuscates when or what\nindividual future rewards an agent may expect to receive. We address this by\nmodifying an agent's future reward estimator to predict their next N expected\nrewards, referred to as Temporal Reward Decomposition (TRD). This unlocks novel\nexplanations of agent behaviour. Through TRD we can: estimate when an agent may\nexpect to receive a reward, the value of the reward and the agent's confidence\nin receiving it; measure an input feature's temporal importance to the agent's\naction decisions; and predict the influence of different actions on future\nrewards. Furthermore, we show that DQN agents trained on Atari environments can\nbe efficiently retrained to incorporate TRD with minimal impact on performance.",
      "tldr_zh": "本研究针对强化学习中未来奖励估计器（如 Q-value 和 state-value functions）的输出模糊问题，提出 Temporal Reward Decomposition (TRD) 方法，将其修改为预测代理的下一个 N 个预期奖励，从而实现对代理行为的详细解释。TRD 能够估计奖励的发生时间、价值、代理信心，评估输入特征的时序重要性，以及预测不同动作对未来奖励的影响。通过实验，在 Atari 环境中，对 DQN 代理进行高效重新训练后，整合 TRD 的性能影响最小，证明了该方法的实用性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages + 3 pages of supplementary material. Published at ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08230v1",
      "published_date": "2024-08-15 15:56:15 UTC",
      "updated_date": "2024-08-15 15:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:34:08.238826"
    },
    {
      "arxiv_id": "2408.08227v1",
      "title": "Evolving A* to Efficiently Solve the k Shortest-Path Problem (Extended Version)",
      "title_zh": "翻译失败",
      "authors": [
        "Carlos Linares López",
        "Ian Herman"
      ],
      "abstract": "The problem of finding the shortest path in a graph G(V, E) has been widely\nstudied. However, in many applications it is necessary to compute an arbitrary\nnumber of them, k. Even though the problem has raised a lot of interest from\ndifferent research communities and many applications of it are known, it has\nnot been addressed to the same extent as the single shortest path problem. The\nbest algorithm known for efficiently solving this task has a time complexity of\nO (|E| + |V|log{|V|}+k|V|)$ when computing paths in explicit form, and is based\non best-first search. This paper introduces a new search algorithm with the\nsame time complexity, which results from a natural evolution of A* thus, it\npreserves all its interesting properties, making it widely applicable to many\ndifferent domains. Experiments in various testbeds show a significant\nimprovement in performance over the state of the art, often by one or two\norders of magnitude.",
      "tldr_zh": "这篇论文针对k Shortest-Path Problem，提出了一种从A*算法自然演化而来的新搜索算法，以高效计算图G(V, E)中的k条最短路径。新算法保持了O(|E| + |V|log|V| + k|V|)的时间复杂度，并继承了A*的特性，使其适用于多种领域。实验在各种测试环境中显示，该算法的性能比现有最佳方法提高了1到2个数量级。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "68T20",
        "I.2.8"
      ],
      "primary_category": "cs.DS",
      "comment": "249 plots in 48 figures, and 81 tables. This is an extended version\n  of the paper Linares L\\'opez, Carlos and Herman, Ian. 2024. Evolving A* to\n  Efficiently Solve the k Shortest-Path Problem. Proceedings of the European\n  Conference on Artificial Intelligence (ECAI). To appear",
      "pdf_url": "http://arxiv.org/pdf/2408.08227v1",
      "published_date": "2024-08-15 15:54:25 UTC",
      "updated_date": "2024-08-15 15:54:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:34:18.899033"
    },
    {
      "arxiv_id": "2408.08226v2",
      "title": "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqicheng Zhu",
        "Nico Potyka",
        "Mojtaba Nayyeri",
        "Bo Xiong",
        "Yunjie He",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Knowledge graph embedding (KGE) models are often used to predict missing\nlinks for knowledge graphs (KGs). However, multiple KG embeddings can perform\nalmost equally well for link prediction yet give conflicting predictions for\nunseen queries. This phenomenon is termed \\textit{predictive multiplicity} in\nthe literature. It poses substantial risks for KGE-based applications in\nhigh-stake domains but has been overlooked in KGE research. We define\npredictive multiplicity in link prediction, introduce evaluation metrics and\nmeasure predictive multiplicity for representative KGE methods on commonly used\nbenchmark datasets. Our empirical study reveals significant predictive\nmultiplicity in link prediction, with $8\\%$ to $39\\%$ testing queries\nexhibiting conflicting predictions. We address this issue by leveraging voting\nmethods from social choice theory, significantly mitigating conflicts by $66\\%$\nto $78\\%$ in our experiments.",
      "tldr_zh": "本论文探讨了知识图谱嵌入 (KGE) 模型在链接预测中的预测多重性问题，即多个模型虽在性能上相当，却对未见查询产生冲突预测，这可能在高风险领域引发风险。研究者定义了预测多重性，引入了评估指标，并在常用基准数据集上测试代表性 KGE 方法，结果显示 8% 到 39% 的测试查询存在冲突预测。为了解决这一问题，论文采用社会选择理论中的投票方法，实验中成功将冲突减少了 66% 到 78%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as EMNLP'24 Finding",
      "pdf_url": "http://arxiv.org/pdf/2408.08226v2",
      "published_date": "2024-08-15 15:54:02 UTC",
      "updated_date": "2024-10-04 14:03:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:34:34.408831"
    },
    {
      "arxiv_id": "2408.08216v1",
      "title": "The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Arpan Mahara",
        "Naphtali D. Rishe",
        "Liangdong Deng"
      ],
      "abstract": "Image-to-Image translation in Generative Artificial Intelligence (Generative\nAI) has been a central focus of research, with applications spanning\nhealthcare, remote sensing, physics, chemistry, photography, and more. Among\nthe numerous methodologies, Generative Adversarial Networks (GANs) with\ncontrastive learning have been particularly successful. This study aims to\ndemonstrate that the Kolmogorov-Arnold Network (KAN) can effectively replace\nthe Multi-layer Perceptron (MLP) method in generative AI, particularly in the\nsubdomain of image-to-image translation, to achieve better generative quality.\nOur novel approach replaces the two-layer MLP with a two-layer KAN in the\nexisting Contrastive Unpaired Image-to-Image Translation (CUT) model,\ndeveloping the KAN-CUT model. This substitution favors the generation of more\ninformative features in low-dimensional vector representations, which\ncontrastive learning can utilize more effectively to produce high-quality\nimages in the target domain. Extensive experiments, detailed in the results\nsection, demonstrate the applicability of KAN in conjunction with contrastive\nlearning and GANs in Generative AI, particularly for image-to-image\ntranslation. This work suggests that KAN could be a valuable component in the\nbroader generative AI domain.",
      "tldr_zh": "本研究探讨了在图像到图像(I2I)翻译领域中，使用Kolmogorov-Arnold Network (KAN)替换Multi-layer Perceptron (MLP)，以提升生成式AI的性能。研究者开发了KAN-CUT模型，将两层KAN整合到现有的Contrastive Unpaired Image-to-Image Translation (CUT)模型中，从而生成更具信息性的低维特征，并与Generative Adversarial Networks (GANs)结合，提高对比学习的有效性。实验结果显示，该方法显著改善了目标域图像的质量，并证明KAN在生成式AI，特别是I2I翻译中的适用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 Figures, 1 Table",
      "pdf_url": "http://arxiv.org/pdf/2408.08216v1",
      "published_date": "2024-08-15 15:26:12 UTC",
      "updated_date": "2024-08-15 15:26:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:34:46.067832"
    },
    {
      "arxiv_id": "2408.08215v1",
      "title": "Moving Healthcare AI-Support Systems for Visually Detectable Diseases onto Constrained Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Tess Watt",
        "Christos Chrysoulas",
        "Peter J Barclay"
      ],
      "abstract": "Image classification usually requires connectivity and access to the cloud\nwhich is often limited in many parts of the world, including hard to reach\nrural areas. TinyML aims to solve this problem by hosting AI assistants on\nconstrained devices, eliminating connectivity issues by processing data within\nthe device itself, without internet or cloud access. This pilot study explores\nthe use of tinyML to provide healthcare support with low spec devices in low\nconnectivity environments, focusing on diagnosis of skin diseases and the\nethical use of AI assistants in a healthcare setting. To investigate this,\n10,000 images of skin lesions were used to train a model for classifying\nvisually detectable diseases (VDDs). The model weights were then offloaded to a\nRaspberry Pi with a webcam attached, to be used for the classification of skin\nlesions without internet access. It was found that the developed prototype\nachieved a test accuracy of 78% and a test loss of 1.08.",
      "tldr_zh": "这篇论文探讨了将医疗AI支持系统从云端迁移到受限设备上，以解决偏远地区连接性问题，焦点是视觉可检测疾病(VDDs)的诊断。研究采用TinyML技术，训练了一个基于10,000张皮肤病变图像的模型，并将其部署到Raspberry Pi等低规格设备上，实现本地图像分类，而无需互联网访问。结果显示，该原型在测试中达到了78%的准确率和1.08的测试损失，并强调了AI在医疗环境中的伦理使用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08215v1",
      "published_date": "2024-08-15 15:23:37 UTC",
      "updated_date": "2024-08-15 15:23:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:34:56.378285"
    },
    {
      "arxiv_id": "2408.08214v1",
      "title": "Federated Fairness Analytics: Quantifying Fairness in Federated Learning",
      "title_zh": "联邦公平性分析：量化联邦学习中的公平性",
      "authors": [
        "Oscar Dilley",
        "Juan Marcelo Parra-Ullauri",
        "Rasheed Hussain",
        "Dimitra Simeonidou"
      ],
      "abstract": "Federated Learning (FL) is a privacy-enhancing technology for distributed ML.\nBy training models locally and aggregating updates - a federation learns\ntogether, while bypassing centralised data collection. FL is increasingly\npopular in healthcare, finance and personal computing. However, it inherits\nfairness challenges from classical ML and introduces new ones, resulting from\ndifferences in data quality, client participation, communication constraints,\naggregation methods and underlying hardware. Fairness remains an unresolved\nissue in FL and the community has identified an absence of succinct definitions\nand metrics to quantify fairness; to address this, we propose Federated\nFairness Analytics - a methodology for measuring fairness. Our definition of\nfairness comprises four notions with novel, corresponding metrics. They are\nsymptomatically defined and leverage techniques originating from XAI,\ncooperative game-theory and networking engineering. We tested a range of\nexperimental settings, varying the FL approach, ML task and data settings. The\nresults show that statistical heterogeneity and client participation affect\nfairness and fairness conscious approaches such as Ditto and q-FedAvg\nmarginally improve fairness-performance trade-offs. Using our techniques, FL\npractitioners can uncover previously unobtainable insights into their system's\nfairness, at differing levels of granularity in order to address fairness\nchallenges in FL. We have open-sourced our work at:\nhttps://github.com/oscardilley/federated-fairness.",
      "tldr_zh": "本论文探讨了Federated Learning (FL)中的公平性挑战，提出Federated Fairness Analytics方法，用于量化FL系统的公平性。该方法定义了四个公平性概念，并开发了相应的指标，基于XAI（可解释AI）、合作博弈论和网络工程技术，以评估数据异质性、客户端参与等因素的影响。通过实验验证不同FL设置，结果显示统计异质性和客户端参与会显著影响公平性，而方法如Ditto和q-FedAvg可略微改善公平性与性能的权衡，从而帮助FL从业者更深入地分析和优化系统公平性。论文已开源，代码地址为https://github.com/oscardilley/federated-fairness。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.GT",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08214v1",
      "published_date": "2024-08-15 15:23:32 UTC",
      "updated_date": "2024-08-15 15:23:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:35:08.608292"
    },
    {
      "arxiv_id": "2408.08208v2",
      "title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Bohao Wang",
        "Feng Liu",
        "Changwang Zhang",
        "Jiawei Chen",
        "Yudi Wu",
        "Sheng Zhou",
        "Xingyu Lou",
        "Jun Wang",
        "Yan Feng",
        "Chun Chen",
        "Can Wang"
      ],
      "abstract": "Sequential Recommenders generate recommendations based on users' historical\ninteraction sequences. However, in practice, these collected sequences are\noften contaminated by noisy interactions, which significantly impairs\nrecommendation performance. Accurately identifying such noisy interactions\nwithout additional information is particularly challenging due to the absence\nof explicit supervisory signals indicating noise. Large Language Models (LLMs),\nequipped with extensive open knowledge and semantic reasoning abilities, offer\na promising avenue to bridge this information gap. However, employing LLMs for\ndenoising in sequential recommendation presents notable challenges: 1) Direct\napplication of pretrained LLMs may not be competent for the denoising task,\nfrequently generating nonsensical responses; 2) Even after fine-tuning, the\nreliability of LLM outputs remains questionable, especially given the\ncomplexity of the denoising task and the inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected\nsequences to be flexibly applied across various recommendation models.\nExtensive experiments validate the superiority of LLM4DSR over existing\nmethods.",
      "tldr_zh": "该研究针对顺序推荐（Sequential Recommendation）中用户历史交互序列的噪音问题，提出LLM4DSR框架，利用大语言模型（LLMs）进行去噪处理，以弥补缺少明确监督信号的挑战。LLM4DSR通过构建自监督微调任务，激活LLMs识别噪音项并建议替换，同时引入不确定性估计模块，确保仅使用高置信度响应来修正序列。该框架是模型无关的，可灵活应用于各种推荐模型，实验结果显示其显著优于现有方法。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08208v2",
      "published_date": "2024-08-15 15:18:46 UTC",
      "updated_date": "2024-11-26 08:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:35:20.149185"
    },
    {
      "arxiv_id": "2409.00009v2",
      "title": "Web Retrieval Agents for Evidence-Based Misinformation Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jacob-Junqi Tian",
        "Hao Yu",
        "Yury Orlovskiy",
        "Tyler Vergho",
        "Mauricio Rivera",
        "Mayank Goel",
        "Zachary Yang",
        "Jean-Francois Godbout",
        "Reihaneh Rabbany",
        "Kellin Pelrine"
      ],
      "abstract": "This paper develops an agent-based automated fact-checking approach for\ndetecting misinformation. We demonstrate that combining a powerful LLM agent,\nwhich does not have access to the internet for searches, with an online web\nsearch agent yields better results than when each tool is used independently.\nOur approach is robust across multiple models, outperforming alternatives and\nincreasing the macro F1 of misinformation detection by as much as 20 percent\ncompared to LLMs without search. We also conduct extensive analyses on the\nsources our system leverages and their biases, decisions in the construction of\nthe system like the search tool and the knowledge base, the type of evidence\nneeded and its impact on the results, and other parts of the overall process.\nBy combining strong performance with in-depth understanding, we hope to provide\nbuilding blocks for future search-enabled misinformation mitigation systems.",
      "tldr_zh": "这篇论文提出了一种基于代理的自动事实检查方法，用于证据支持的错误信息检测，通过将强大的 LLM agent（无互联网访问）与在线网络搜索代理结合，实现比单独使用工具更好的性能。实验结果显示，该方法在多个模型上表现出色，将错误信息检测的 macro F1 分数提高了多达 20%。此外，论文对系统来源偏差、搜索工具选择、证据类型及其影响进行了深入分析，为构建未来的搜索启用错误信息缓解系统提供了关键基础。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "1 main figure, 8 tables, 10 pages, 12 figures in Appendix, 7 tables\n  in Appendix GitHub URL: https://github.com/ComplexData-MILA/webretrieval",
      "pdf_url": "http://arxiv.org/pdf/2409.00009v2",
      "published_date": "2024-08-15 15:13:16 UTC",
      "updated_date": "2024-10-09 19:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:35:31.507013"
    },
    {
      "arxiv_id": "2408.08924v2",
      "title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Zhao",
        "Kejiang Chen",
        "Xiaojian Yuan",
        "Weiming Zhang"
      ],
      "abstract": "In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance.",
      "tldr_zh": "本论文针对大型语言模型 (LLMs) 易受 jailbreak attacks 的问题，提出了一种即插即用且易部署的防御框架 Prefix Guidance (PG)。该方法通过直接设置模型输出前几个 tokens 来引导模型识别有害提示，并结合模型的内在安全能力和外部分类器进行防御。实验结果显示，PG 在三个模型和五种攻击方法上平均比基线方法更有效，并在 Just-Eval 基准测试中更好地保留了模型性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08924v2",
      "published_date": "2024-08-15 14:51:32 UTC",
      "updated_date": "2024-08-22 17:21:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:35:45.334280"
    },
    {
      "arxiv_id": "2408.08343v2",
      "title": "API-guided Dataset Synthesis to Finetune Large Code Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zongjie Li",
        "Daoyuan Wu",
        "Shuai Wang",
        "Zhendong Su"
      ],
      "abstract": "Large code models (LCMs), pre-trained on vast code corpora, have demonstrated\nremarkable performance across a wide array of code-related tasks. Supervised\nfine-tuning (SFT) plays a vital role in aligning these models with specific\nrequirements and enhancing their performance in particular domains. However,\nsynthesizing high-quality SFT datasets poses a significant challenge due to the\nuneven quality of datasets and the scarcity of domain-specific datasets.\n  Inspired by APIs as high-level abstractions of code that encapsulate rich\nsemantic information in a concise structure, we propose DataScope, an\nAPI-guided dataset synthesis framework designed to enhance the SFT process for\nLCMs in both general and domain-specific scenarios. DataScope comprises two\nmain components: Dsel and Dgen. On one hand, Dsel employs API coverage as a\ncore metric, enabling efficient dataset synthesis in general scenarios by\nselecting subsets of existing (uneven-quality) datasets with higher API\ncoverage. On the other hand, Dgen recasts domain dataset synthesis as a process\nof using API-specified high-level functionality and deliberately-constituted\ncode skeletons to synthesize concrete code.\n  Extensive experiments demonstrate DataScope's effectiveness, with models\nfine-tuned on its synthesized datasets outperforming those tuned on unoptimized\ndatasets five times larger. Furthermore, a series of analyses on model\ninternals, relevant hyperparameters, and case studies provide additional\nevidence for the efficacy of our proposed methods. These findings underscore\nthe significance of dataset quality in SFT and advance the field of LCMs by\nproviding an efficient, cost-effective framework for constructing high-quality\ndatasets. This contribution enhances performance across both general and\ndomain-specific scenarios, paving the way for more powerful and tailored LCMs.",
      "tldr_zh": "这篇论文提出了 DataScope，一个 API-guided 的数据集合成框架，用于提升大型代码模型 (LCMs) 的监督微调 (SFT) 过程，以解决数据集质量不均和领域特定数据稀缺的问题。框架包括两个组件：Dsel，通过 API coverage 作为核心指标，从现有数据集选择高质量子集；Dgen，则利用 API 指定的高层功能和构建的代码骨架来合成具体代码。实验结果显示，使用 DataScope 合成的数据集微调的模型，性能超过了使用五倍大的未优化数据集的基准模型。这些发现强调了数据集质量在 SFT 中的重要性，并为 LCMs 在一般和领域特定场景下的优化提供了高效、成本有效的解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08343v2",
      "published_date": "2024-08-15 14:48:42 UTC",
      "updated_date": "2024-08-22 11:29:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:35:59.077167"
    },
    {
      "arxiv_id": "2408.08188v4",
      "title": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots Through Hierarchical Temporal Logic Task Representation",
      "title_zh": "Nl2Hltl2Plan：通过分层时序逻辑任务表示扩展多机器人自然语言理解",
      "authors": [
        "Shaojun Xu",
        "Xusheng Luo",
        "Yutong Huang",
        "Letian Leng",
        "Ruixuan Liu",
        "Changliu Liu"
      ],
      "abstract": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose Nl2Hltl2Plan, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our Nl2Hltl2Plan demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that Nl2Hltl2Plan\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io .",
      "tldr_zh": "该研究提出Nl2Hltl2Plan框架，通过分层Linear Temporal Logic (LTL)任务表示来提升Large Language Models (LLMs)对多机器人协作任务的自然语言理解能力。框架首先利用LLMs将自然语言指令转化为Hierarchical Task Tree，以捕获逻辑和时序关系；随后，微调的LLMs将子任务转换为平坦LTL公式，并聚合为分层规范，最终与现成规划器结合生成机器人行动计划。实验结果显示，该框架在模拟和真实世界环境中优于现有方法，能处理更复杂的指令，实现更高的成功率和更低的任务分配成本。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08188v4",
      "published_date": "2024-08-15 14:46:13 UTC",
      "updated_date": "2024-12-05 05:37:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:36:12.777141"
    },
    {
      "arxiv_id": "2408.08182v2",
      "title": "Your Turn: At Home Turning Angle Estimation for Parkinson's Disease Severity Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Qiushuo Cheng",
        "Catherine Morgan",
        "Arindam Sikdar",
        "Alessandro Masullo",
        "Alan Whone",
        "Majid Mirmehdi"
      ],
      "abstract": "People with Parkinson's Disease (PD) often experience progressively worsening\ngait, including changes in how they turn around, as the disease progresses.\nExisting clinical rating tools are not capable of capturing hour-by-hour\nvariations of PD symptoms, as they are confined to brief assessments within\nclinic settings. Measuring gait turning angles continuously and passively is a\ncomponent step towards using gait characteristics as sensitive indicators of\ndisease progression in PD. This paper presents a deep learning-based approach\nto automatically quantify turning angles by extracting 3D skeletons from videos\nand calculating the rotation of hip and knee joints. We utilise\nstate-of-the-art human pose estimation models, Fastpose and Strided\nTransformer, on a total of 1386 turning video clips from 24 subjects (12 people\nwith PD and 12 healthy control volunteers), trimmed from a PD dataset of\nunscripted free-living videos in a home-like setting (Turn-REMAP). We also\ncurate a turning video dataset, Turn-H3.6M, from the public Human3.6M human\npose benchmark with 3D ground truth, to further validate our method. Previous\ngait research has primarily taken place in clinics or laboratories evaluating\nscripted gait outcomes, but this work focuses on free-living home settings\nwhere complexities exist, such as baggy clothing and poor lighting. Due to\ndifficulties in obtaining accurate ground truth data in a free-living setting,\nwe quantise the angle into the nearest bin $45^\\circ$ based on the manual\nlabelling of expert clinicians. Our method achieves a turning calculation\naccuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\\deg}, and a weighted\nprecision WPrec of 68.3% for Turn-REMAP. This is the first work to explore the\nuse of single monocular camera data to quantify turns by PD patients in a home\nsetting.",
      "tldr_zh": "本研究针对帕金森病（PD）患者的转弯角度变化，提出了一种基于深度学习的自动量化方法，以评估疾病严重程度，并捕捉小时级别的症状变化。方法通过从视频中提取3D骨骼并计算髋关节和膝关节的旋转，利用Fastpose和Strided Transformer模型，应用于Turn-REMAP数据集（包括1386个家庭环境视频片段）和Turn-H3.6M数据集。实验结果显示，该方法在Turn-REMAP上实现了41.6%的转弯计算准确率、Mean Absolute Error (MAE)为34.7°和加权精度（WPrec）为68.3%。这是首个使用单目相机在自由生活家庭环境中量化PD患者转弯角度的工作，为实时监测PD进展提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08182v2",
      "published_date": "2024-08-15 14:36:07 UTC",
      "updated_date": "2024-08-24 16:18:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:36:26.630569"
    },
    {
      "arxiv_id": "2408.08172v2",
      "title": "Towards flexible perception with visual memory",
      "title_zh": "翻译失败",
      "authors": [
        "Robert Geirhos",
        "Priyank Jaini",
        "Austin Stone",
        "Sourabh Medapati",
        "Xi Yi",
        "George Toderici",
        "Abhijit Ogale",
        "Jonathon Shlens"
      ],
      "abstract": "Training a neural network is a monolithic endeavor, akin to carving knowledge\ninto stone: once the process is completed, editing the knowledge in a network\nis nearly impossible, since all information is distributed across the network's\nweights. We here explore a simple, compelling alternative by marrying the\nrepresentational power of deep neural networks with the flexibility of a\ndatabase. Decomposing the task of image classification into image similarity\n(from a pre-trained embedding) and search (via fast nearest neighbor retrieval\nfrom a knowledge database), we build a simple and flexible visual memory that\nhas the following key capabilities: (1.) The ability to flexibly add data\nacross scales: from individual samples all the way to entire classes and\nbillion-scale data; (2.) The ability to remove data through unlearning and\nmemory pruning; (3.) An interpretable decision-mechanism on which we can\nintervene to control its behavior. Taken together, these capabilities\ncomprehensively demonstrate the benefits of an explicit visual memory. We hope\nthat it might contribute to a conversation on how knowledge should be\nrepresented in deep vision models -- beyond carving it in \"stone\" weights.",
      "tldr_zh": "该论文探讨了传统神经网络训练的局限性，即知识嵌入在权重中难以编辑，并提出了一种结合 deep neural networks 和数据库的灵活视觉记忆方法。具体来说，该方法将图像 classification 任务分解为图像相似性（基于预训练嵌入）和搜索（通过快速 nearest neighbor retrieval），从而构建一个可扩展的视觉记忆系统。该系统具备灵活添加数据（从单个样本到亿级规模）、删除数据（通过 unlearning 和 memory pruning）以及可解释决策机制的关键能力。总体上，这种方法展示了显式视觉记忆的优势，有望改变知识在 deep vision models 中的表示方式。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Adding link to code at\n  https://github.com/google-deepmind/visual-memory",
      "pdf_url": "http://arxiv.org/pdf/2408.08172v2",
      "published_date": "2024-08-15 14:19:13 UTC",
      "updated_date": "2024-09-17 13:35:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:36:38.947819"
    },
    {
      "arxiv_id": "2408.08160v3",
      "title": "General-purpose Clothes Manipulation with Semantic Keypoints",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhong Deng",
        "David Hsu"
      ],
      "abstract": "Clothes manipulation is a critical capability for household robots; yet,\nexisting methods are often confined to specific tasks, such as folding or\nflattening, due to the complex high-dimensional geometry of deformable fabric.\nThis paper presents CLothes mAnipulation with Semantic keyPoints (CLASP) for\ngeneral-purpose clothes manipulation, which enables the robot to perform\ndiverse manipulation tasks over different types of clothes. The key idea of\nCLASP is semantic keypoints -- e.g., \"right shoulder\", \"left sleeve\", etc. -- a\nsparse spatial-semantic representation that is salient for both perception and\naction. Semantic keypoints of clothes can be effectively extracted from depth\nimages and are sufficient to represent a broad range of clothes manipulation\npolicies. CLASP leverages semantic keypoints to bridge LLM-powered task\nplanning and low-level action execution in a two-level hierarchy. Extensive\nsimulation experiments show that CLASP outperforms baseline methods across\ndiverse clothes types in both seen and unseen tasks. Further, experiments with\na Kinova dual-arm system on four distinct tasks -- folding, flattening,\nhanging, and placing -- confirm CLASP's performance on a real robot.",
      "tldr_zh": "本论文提出 CLASP（CLothes mAnipulation with Semantic Keypoints）框架，实现通用衣物操控，解决现有方法受限于特定任务（如folding或flattening）的局限性。CLASP 使用语义关键点（如\"right shoulder\"、\"left sleeve\"）作为稀疏的空间-语义表示，从 depth images 中提取这些关键点，并结合 LLM-powered 任务规划与低级动作执行的层次化系统。实验结果显示，CLASP 在模拟环境中超越基线方法，适用于多种衣物类型和任务，并在真实 Kinova 双臂机器人上成功验证了 folding、flattening、hanging 和 placing 等任务。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "accepted by IEEE International Conference on Robotics and Automation\n  (ICRA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2408.08160v3",
      "published_date": "2024-08-15 13:49:14 UTC",
      "updated_date": "2025-03-26 06:56:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:36:49.266937"
    },
    {
      "arxiv_id": "2408.08152v1",
      "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
      "title_zh": "翻译失败",
      "authors": [
        "Huajian Xin",
        "Z. Z. Ren",
        "Junxiao Song",
        "Zhihong Shao",
        "Wanjia Zhao",
        "Haocheng Wang",
        "Bo Liu",
        "Liyue Zhang",
        "Xuan Lu",
        "Qiushi Du",
        "Wenjun Gao",
        "Qihao Zhu",
        "Dejian Yang",
        "Zhibin Gou",
        "Z. F. Wu",
        "Fuli Luo",
        "Chong Ruan"
      ],
      "abstract": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$).",
      "tldr_zh": "我们引入了 DeepSeek-Prover-V1.5，这是一个开源语言模型，针对 Lean 4 的定理证明进行了优化，包括预训练、监督微调和基于证明助手反馈的强化学习 (RLPAF)，以提升 DeepSeek-Prover-V1 的性能。 该模型还提出 RMaxTS，一种 Monte-Carlo Tree Search 的变体，通过内在奖励驱动的探索策略生成多样化的证明路径。 在 miniF2F 高中水平基准测试中，DeepSeek-Prover-V1.5 达到 63.5% 的准确率，在 ProofNet 本科水平基准测试中达到 25.3%，实现了新的最先进结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08152v1",
      "published_date": "2024-08-15 13:40:03 UTC",
      "updated_date": "2024-08-15 13:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:37:02.594246"
    },
    {
      "arxiv_id": "2408.08150v1",
      "title": "Winning Snake: Design Choices in Multi-Shot ASP",
      "title_zh": "翻译失败",
      "authors": [
        "Elisa Böhl",
        "Stefan Ellmauthaler",
        "Sarah Alice Gaggl"
      ],
      "abstract": "Answer set programming is a well-understood and established problem-solving\nand knowledge representation paradigm. It has become more prominent amongst a\nwider audience due to its multiple applications in science and industry. The\nconstant development of advanced programming and modeling techniques extends\nthe toolset for developers and users regularly. This paper demonstrates\ndifferent techniques to reuse logic program parts (multi-shot) by solving the\narcade game snake. This game is particularly interesting because a victory can\nbe assured by solving the underlying NP-hard problem of Hamiltonian Cycles. We\nwill demonstrate five hands-on implementations in clingo and compare their\nperformance in an empirical evaluation. In addition, our implementation\nutilizes clingraph to generate a simple yet informative image representation of\nthe game's progress.",
      "tldr_zh": "本论文探讨了Answer Set Programming (ASP)中的多重执行(multi-shot)设计选择，通过街机游戏Snake作为案例研究。作者实现了五个在clingo框架下的实际版本，演示如何重用逻辑程序部分来解决该游戏的NP-hard问题Hamiltonian Cycles，并利用clingraph生成游戏进度的图像表示。实证评估显示，这些实现方案在性能上存在差异，为ASP在科学和工业应用中的建模技术提供了实用见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 3 figures, to appear in Theory and Practice of Logic\n  Programming (TPLP), Proceedings of ICLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08150v1",
      "published_date": "2024-08-15 13:37:59 UTC",
      "updated_date": "2024-08-15 13:37:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:37:11.132864"
    },
    {
      "arxiv_id": "2408.08341v1",
      "title": "Exploring Latent Space for Generating Peptide Analogs Using Protein Language Models",
      "title_zh": "使用蛋白质语言模型探索潜在空间以生成肽类似物",
      "authors": [
        "Po-Yu Liang",
        "Xueting Huang",
        "Tibo Duran",
        "Andrew J. Wiemer",
        "Jun Bai"
      ],
      "abstract": "Generating peptides with desired properties is crucial for drug discovery and\nbiotechnology. Traditional sequence-based and structure-based methods often\nrequire extensive datasets, which limits their effectiveness. In this study, we\nproposed a novel method that utilized autoencoder shaped models to explore the\nprotein embedding space, and generate novel peptide analogs by leveraging\nprotein language models. The proposed method requires only a single sequence of\ninterest, avoiding the need for large datasets. Our results show significant\nimprovements over baseline models in similarity indicators of peptide\nstructures, descriptors and bioactivities. The proposed method validated\nthrough Molecular Dynamics simulations on TIGIT inhibitors, demonstrates that\nour method produces peptide analogs with similar yet distinct properties,\nhighlighting its potential to enhance peptide screening processes.",
      "tldr_zh": "本文提出了一种新方法，利用autoencoder shaped models探索蛋白质embedding space，并结合protein language models生成新型肽analog，仅需一个感兴趣的序列即可，避免依赖大型数据集。该方法在肽结构、描述符和生物活性相似性指标上比baseline模型显著改善。实验通过Molecular Dynamics simulations在TIGIT inhibitors上验证，证明生成的肽analog具有类似却独特的属性，有望提升肽筛选过程。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08341v1",
      "published_date": "2024-08-15 13:37:27 UTC",
      "updated_date": "2024-08-15 13:37:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:37:23.934742"
    },
    {
      "arxiv_id": "2408.08145v1",
      "title": "Model-based Workflow for the Automated Generation of PDDL Descriptions",
      "title_zh": "基于模型的工作流：用于 PDDL 描述",
      "authors": [
        "Hamied Nabizada",
        "Tom Jeleniewski",
        "Felix Gehlhoff",
        "Alexander Fay"
      ],
      "abstract": "Manually creating Planning Domain Definition Language (PDDL) descriptions is\ndifficult, error-prone, and requires extensive expert knowledge. However, this\nknowledge is already embedded in engineering models and can be reused.\nTherefore, this contribution presents a comprehensive workflow for the\nautomated generation of PDDL descriptions from integrated system and product\nmodels. The proposed workflow leverages Model-Based Systems Engineering (MBSE)\nto organize and manage system and product information, translating it\nautomatically into PDDL syntax for planning purposes. By connecting system and\nproduct models with planning aspects, it ensures that changes in these models\nare quickly reflected in updated PDDL descriptions, facilitating efficient and\nadaptable planning processes. The workflow is validated within a use case from\naircraft assembly.",
      "tldr_zh": "该论文提出了一种基于模型的工作流，用于从集成系统和产品模型中自动生成 Planning Domain Definition Language (PDDL) 描述，以解决手动创建 PDDL 的困难、易出错和依赖专家知识的问题。该工作流利用 Model-Based Systems Engineering (MBSE) 来组织和管理系统及产品信息，并自动将其翻译成 PDDL 语法，确保模型变更快速反映在规划描述中，从而提升规划过程的效率和适应性。在飞机组装用例中进行了验证，证明了该方法的有效性。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08145v1",
      "published_date": "2024-08-15 13:29:25 UTC",
      "updated_date": "2024-08-15 13:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:37:35.631639"
    },
    {
      "arxiv_id": "2408.08133v1",
      "title": "EXPLAIN, AGREE, LEARN: Scaling Learning for Neural Probabilistic Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Victor Verreet",
        "Lennert De Smet",
        "Luc De Raedt",
        "Emanuele Sansone"
      ],
      "abstract": "Neural probabilistic logic systems follow the neuro-symbolic (NeSy) paradigm\nby combining the perceptive and learning capabilities of neural networks with\nthe robustness of probabilistic logic. Learning corresponds to likelihood\noptimization of the neural networks. However, to obtain the likelihood exactly,\nexpensive probabilistic logic inference is required. To scale learning to more\ncomplex systems, we therefore propose to instead optimize a sampling based\nobjective. We prove that the objective has a bounded error with respect to the\nlikelihood, which vanishes when increasing the sample count. Furthermore, the\nerror vanishes faster by exploiting a new concept of sample diversity. We then\ndevelop the EXPLAIN, AGREE, LEARN (EXAL) method that uses this objective.\nEXPLAIN samples explanations for the data. AGREE reweighs each explanation in\nconcordance with the neural component. LEARN uses the reweighed explanations as\na signal for learning. In contrast to previous NeSy methods, EXAL can scale to\nlarger problem sizes while retaining theoretical guarantees on the error.\nExperimentally, our theoretical claims are verified and EXAL outperforms recent\nNeSy methods when scaling up the MNIST addition and Warcraft pathfinding\nproblems.",
      "tldr_zh": "该论文针对神经概率逻辑(Neural Probabilistic Logic)系统提出了一种扩展学习的方法，以解决传统似然优化依赖昂贵概率逻辑推理的问题。作者设计了基于采样的优化目标，并证明其相对于似然(likelihood)的误差有界，且通过增加样本数量或利用样本多样性，该误差可快速减少。EXPLAIN, AGREE, LEARN (EXAL) 方法的核心包括：EXPLAIN 采样数据解释、AGREE 重新加权解释以匹配神经组件，以及 LEARN 使用这些加权解释作为学习信号，使其在保持理论误差保证的同时扩展到更大规模问题。实验结果验证了这些理论主张，并在扩展的 MNIST 加法和 Warcraft 路径寻找任务上，EXAL 优于现有 NeSy 方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08133v1",
      "published_date": "2024-08-15 13:07:51 UTC",
      "updated_date": "2024-08-15 13:07:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:37:48.341953"
    },
    {
      "arxiv_id": "2409.00007v1",
      "title": "Federated Sequence-to-Sequence Learning for Load Disaggregation from Unbalanced Low-Resolution Smart Meter Data",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangrui Li"
      ],
      "abstract": "The importance of Non-Intrusive Load Monitoring (NILM) has been increasingly\nrecognized, given that NILM can enhance energy awareness and provide valuable\ninsights for energy program design. Many existing NILM methods often rely on\nspecialized devices to retrieve high-sampling complex signal data and focus on\nthe high consumption appliances, hindering their applicability in real-world\napplications, especially when smart meters only provide low-resolution active\npower readings for households. In this paper, we propose a new approach using\neasily accessible weather data to achieve load disaggregation for a total of 12\nappliances, encompassing both high and low consumption, in scenarios with very\nlow sampling rates (hourly). Moreover, We develop a federated learning (FL)\nmodel that builds upon a sequence-to-sequence model to fulfil load\ndisaggregation without data sharing. Our experiments demonstrate that the FL\nframework - L2GD can effectively handle statistical heterogeneity and avoid\noverfitting problems. By incorporating weather data, our approach significantly\nimproves the performance of NILM.",
      "tldr_zh": "本论文针对 Non-Intrusive Load Monitoring (NILM) 的实际挑战，提出了一种新方法，利用易获取的天气数据来实现低采样率（每小时）场景下的负载分解，覆盖12种设备（包括高低消耗）。他们开发了基于 Federated Learning (FL) 的序列到序列模型 L2GD，该模型无需共享数据即可处理统计异质性和避免过拟合问题。实验结果表明，通过整合天气数据，该方法显著提升了 NILM 的性能。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00007v1",
      "published_date": "2024-08-15 13:04:49 UTC",
      "updated_date": "2024-08-15 13:04:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:38:00.479817"
    },
    {
      "arxiv_id": "2408.16776v1",
      "title": "Online Behavior Modification for Expressive User Control of RL-Trained Robots",
      "title_zh": "在线行为修改：用于 RL 训练机器人的表达性用户控制",
      "authors": [
        "Isaac Sheidlower",
        "Mavis Murdock",
        "Emma Bethel",
        "Reuben M. Aronson",
        "Elaine Schaertl Short"
      ],
      "abstract": "Reinforcement Learning (RL) is an effective method for robots to learn tasks.\nHowever, in typical RL, end-users have little to no control over how the robot\ndoes the task after the robot has been deployed. To address this, we introduce\nthe idea of online behavior modification, a paradigm in which users have\ncontrol over behavior features of a robot in real time as it autonomously\ncompletes a task using an RL-trained policy. To show the value of this\nuser-centered formulation for human-robot interaction, we present a behavior\ndiversity based algorithm, Adjustable Control Of RL Dynamics (ACORD), and\ndemonstrate its applicability to online behavior modification in simulation and\na user study. In the study (n=23) users adjust the style of paintings as a\nrobot traces a shape autonomously. We compare ACORD to RL and Shared Autonomy\n(SA), and show ACORD affords user-preferred levels of control and expression,\ncomparable to SA, but with the potential for autonomous execution and\nrobustness of RL.",
      "tldr_zh": "本文提出在线行为修改范式，允许用户在RL训练的机器人自主完成任务时实时控制其行为特征，从而提升人机交互的表达性和用户控制。作者开发了基于行为多样性的算法ACORD，并通过模拟实验和用户研究（n=23）验证其效果，用户在研究中调整机器人绘制形状的绘画风格。相比传统RL和Shared Autonomy，ACORD提供了用户偏好的控制水平，与Shared Autonomy相当，同时结合了RL的自主执行和鲁棒性优势。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work was published and presented at HRI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.16776v1",
      "published_date": "2024-08-15 12:28:08 UTC",
      "updated_date": "2024-08-15 12:28:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:38:13.205791"
    },
    {
      "arxiv_id": "2408.08921v2",
      "title": "Graph Retrieval-Augmented Generation: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Boci Peng",
        "Yun Zhu",
        "Yongchao Liu",
        "Xiaohe Bo",
        "Haizhou Shi",
        "Chuntao Hong",
        "Yan Zhang",
        "Siliang Tang"
      ],
      "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
      "tldr_zh": "这篇论文对 Graph Retrieval-Augmented Generation (GraphRAG) 进行了首次全面调查，旨在解决传统 Retrieval-Augmented Generation (RAG) 在处理实体间复杂关系时的局限性，如幻觉问题和知识缺口。论文形式化了 GraphRAG 的工作流程，包括 Graph-Based Indexing、Graph-Guided Retrieval 和 Graph-Enhanced Generation 三个阶段，并详细阐述了核心技术、训练方法、下游任务（如知识图谱应用）和评估方法。最终，该研究探讨了 GraphRAG 在工业领域的用例和未来方向，并提供了 GitHub 仓库（https://github.com/pengboci/GraphRAG-Survey）以跟踪最新进展。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided",
      "pdf_url": "http://arxiv.org/pdf/2408.08921v2",
      "published_date": "2024-08-15 12:20:24 UTC",
      "updated_date": "2024-09-10 15:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:38:27.121545"
    },
    {
      "arxiv_id": "2408.08105v3",
      "title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Discern Causal Links Across Modalities",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyuan Li",
        "Heng Wang",
        "Dongnan Liu",
        "Chaoyi Zhang",
        "Ao Ma",
        "Jieting Long",
        "Weidong Cai"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have showcased exceptional\nChain-of-Thought (CoT) reasoning ability in complex textual inference tasks\nincluding causal reasoning. However, will these causalities remain\nstraightforward when crucial hints hide in visual details? If not, what factors\nmight influence cross-modal generalization? Whether we can effectively enhance\ntheir capacity for robust causal inference across both text and vision?\nMotivated by these, we introduce MuCR - a novel Multimodal Causal Reasoning\nbenchmark that leverages synthetic siamese images and text pairs to challenge\nMLLMs. Additionally, we develop tailored metrics from multiple perspectives,\nincluding image-level match, phrase-level understanding, and sentence-level\nexplanation, to comprehensively assess MLLMs' comprehension abilities. Our\nexperiments reveal that current MLLMs fall short in multimodal causal reasoning\ncompared to their performance in purely textual settings. Additionally, we find\nthat identifying visual cues across images is key to effective cross-modal\ngeneralization. Finally, we propose a VcCoT strategy that better highlights\nvisual cues, and our results confirm its efficacy in enhancing multimodal\ncausal reasoning. The project is available at:\nhttps://github.com/Zhiyuan-Li-John/MuCR",
      "tldr_zh": "该研究引入了MuCR基准，用于评估Multimodal Large Language Models (MLLMs)在多模态因果推理中的能力，特别是当关键线索隐藏在视觉细节时。MuCR利用合成孪生图像和文本对，并开发了多角度评估指标，包括image-level match、phrase-level understanding和sentence-level explanation，以全面测试MLLMs的跨模态理解。实验结果显示，MLLMs在多模态因果推理中远逊于纯文本场景，且识别视觉线索是提升跨模态泛化的关键。最后，研究提出VcCoT策略，通过突出视觉提示，有效提升了MLLMs的因果推理性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages 26 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08105v3",
      "published_date": "2024-08-15 12:04:32 UTC",
      "updated_date": "2025-02-16 04:12:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:38:39.196121"
    },
    {
      "arxiv_id": "2409.00005v1",
      "title": "Csi-LLM: A Novel Downlink Channel Prediction Method Aligned with LLM Pre-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Shilong Fan",
        "Zhenyu Liu",
        "Xinyu Gu",
        "Haozhen Li"
      ],
      "abstract": "Downlink channel temporal prediction is a critical technology in massive\nmultiple-input multiple-output (MIMO) systems. However, existing methods that\nrely on fixed-step historical sequences significantly limit the accuracy,\npracticality, and scalability of channel prediction. Recent advances have shown\nthat large language models (LLMs) exhibit strong pattern recognition and\nreasoning abilities over complex sequences. The challenge lies in effectively\naligning wireless communication data with the modalities used in natural\nlanguage processing to fully harness these capabilities. In this work, we\nintroduce Csi-LLM, a novel LLM-powered downlink channel prediction technique\nthat models variable-step historical sequences. To ensure effective\ncross-modality application, we align the design and training of Csi-LLM with\nthe processing of natural language tasks, leveraging the LLM's next-token\ngeneration capability for predicting the next step in channel state information\n(CSI). Simulation results demonstrate the effectiveness of this alignment\nstrategy, with Csi-LLM consistently delivering stable performance improvements\nacross various scenarios and showing significant potential in continuous\nmulti-step prediction.",
      "tldr_zh": "论文提出 Csi-LLM，一种新型下行信道预测方法，将 LLM 的预训练与无线通信数据对齐，以解决现有依赖固定步长历史序列的 MIMO 系统预测问题。Csi-LLM 通过模拟自然语言任务的处理，利用 LLM 的 next-token 生成能力来预测 CSI 的下一个步骤，支持可变步长序列。模拟结果表明，该方法在各种场景中实现了稳定的性能提升，并在连续多步预测中显示出显著潜力。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00005v1",
      "published_date": "2024-08-15 11:39:23 UTC",
      "updated_date": "2024-08-15 11:39:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:38:49.463841"
    },
    {
      "arxiv_id": "2408.08092v3",
      "title": "SC3D: Label-Efficient Outdoor 3D Object Detection via Single Click Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Qiming Xia",
        "Hongwei Lin",
        "Wei Ye",
        "Hai Wu",
        "Yadan Luo",
        "Cheng Wang",
        "Chenglu Wen"
      ],
      "abstract": "LiDAR-based outdoor 3D object detection has received widespread attention.\nHowever, training 3D detectors from the LiDAR point cloud typically relies on\nexpensive bounding box annotations. This paper presents SC3D, an innovative\nlabel-efficient method requiring only a single coarse click on the bird's eye\nview of the 3D point cloud for each frame. A key challenge here is the absence\nof complete geometric descriptions of the target objects from such simple click\nannotations. To address this issue, our proposed SC3D adopts a progressive\npipeline. Initially, we design a mixed pseudo-label generation module that\nexpands limited click annotations into a mixture of bounding box and semantic\nmask supervision. Next, we propose a mix-supervised teacher model, enabling the\ndetector to learn mixed supervision information. Finally, we introduce a\nmixed-supervised student network that leverages the teacher model's\ngeneralization ability to learn unclicked instances.Experimental results on the\nwidely used nuScenes and KITTI datasets demonstrate that our SC3D with only\ncoarse clicks, which requires only 0.2% annotation cost, achieves\nstate-of-the-art performance compared to weakly-supervised 3D detection\nmethods.The code will be made publicly available.",
      "tldr_zh": "这篇论文提出 SC3D，一种标签高效的户外 3D 对象检测方法，仅需在 3D 点云的鸟瞰视图(bird's eye view)上进行单点击标注，就能显著降低标注成本。SC3D 通过渐进式管道解决点击标注缺乏完整几何描述的问题，包括混合伪标签生成模块（扩展为边界框和语义掩码的混合监督）、混合监督教师模型（学习混合信息）以及混合监督学生网络（利用教师的泛化能力检测未点击实例）。实验在 nuScenes 和 KITTI 数据集上显示，SC3D 只需 0.2% 的标注成本，就在弱监督 3D 检测方法中达到最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08092v3",
      "published_date": "2024-08-15 11:34:53 UTC",
      "updated_date": "2024-11-15 05:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:39:02.815781"
    },
    {
      "arxiv_id": "2408.08089v1",
      "title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Guhong Chen",
        "Liyang Fan",
        "Zihan Gong",
        "Nan Xie",
        "Zixuan Li",
        "Ziqiang Liu",
        "Chengming Li",
        "Qiang Qu",
        "Shiwen Ni",
        "Min Yang"
      ],
      "abstract": "In this paper, we present a simulation system called AgentCourt that\nsimulates the entire courtroom process. The judge, plaintiff's lawyer, defense\nlawyer, and other participants are autonomous agents driven by large language\nmodels (LLMs). Our core goal is to enable lawyer agents to learn how to argue a\ncase, as well as improving their overall legal skills, through courtroom\nprocess simulation. To achieve this goal, we propose an adversarial\nevolutionary approach for the lawyer-agent. Since AgentCourt can simulate the\noccurrence and development of court hearings based on a knowledge base and LLM,\nthe lawyer agents can continuously learn and accumulate experience from real\ncourt cases. The simulation experiments show that after two lawyer-agents have\nengaged in a thousand adversarial legal cases in AgentCourt (which can take a\ndecade for real-world lawyers), compared to their pre-evolutionary state, the\nevolved lawyer agents exhibit consistent improvement in their ability to handle\nlegal tasks. To enhance the credibility of our experimental results, we\nenlisted a panel of professional lawyers to evaluate our simulations. The\nevaluation indicates that the evolved lawyer agents exhibit notable\nadvancements in responsiveness, as well as expertise and logical rigor. This\nwork paves the way for advancing LLM-driven agent technology in legal\nscenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.",
      "tldr_zh": "本研究提出AgentCourt系统，使用大型语言模型(LLMs)驱动的自治代理模拟整个法庭过程，包括法官、原告律师和辩护律师等。核心目标是通过模拟法庭来训练律师代理，使其学会辩论案件并提升法律技能，采用对抗性进化方法(adversarial evolutionary approach)让代理从知识基和真实案例中持续学习。实验结果显示，经过一千次对抗性法律案件模拟后，律师代理在处理法律任务的能力上显著提升。专业律师评估确认，这些进化后的代理在响应性、专业性和逻辑严谨性方面均有明显进步，为LLMs驱动的代理技术在法律场景中的应用奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08089v1",
      "published_date": "2024-08-15 11:33:20 UTC",
      "updated_date": "2024-08-15 11:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:39:13.579816"
    },
    {
      "arxiv_id": "2408.08084v1",
      "title": "An Efficient Replay for Class-Incremental Learning with Pre-trained Models",
      "title_zh": "翻译失败",
      "authors": [
        "Weimin Yin",
        "Bin Chen adn Chunzhao Xie",
        "Zhenhao Tan"
      ],
      "abstract": "In general class-incremental learning, researchers typically use sample sets\nas a tool to avoid catastrophic forgetting during continuous learning. At the\nsame time, researchers have also noted the differences between\nclass-incremental learning and Oracle training and have attempted to make\ncorrections. In recent years, researchers have begun to develop\nclass-incremental learning algorithms utilizing pre-trained models, achieving\nsignificant results. This paper observes that in class-incremental learning,\nthe steady state among the weight guided by each class center is disrupted,\nwhich is significantly correlated with catastrophic forgetting. Based on this,\nwe propose a new method to overcoming forgetting . In some cases, by retaining\nonly a single sample unit of each class in memory for replay and applying\nsimple gradient constraints, very good results can be achieved. Experimental\nresults indicate that under the condition of pre-trained models, our method can\nachieve competitive performance with very low computational cost and by simply\nusing the cross-entropy loss.",
      "tldr_zh": "这篇论文针对类增量学习（class-incremental learning）中的灾难性遗忘（catastrophic forgetting）问题，观察到权重稳态的破坏与遗忘密切相关，并提出了一种高效的重放（replay）方法。方法的核心是仅保留每个类的一个样本单位进行重放，并结合简单的梯度约束，以利用预训练模型（pre-trained models）实现学习。实验结果表明，该方法在低计算成本下，仅使用交叉熵损失（cross-entropy loss），即可达到与现有方法竞争性的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08084v1",
      "published_date": "2024-08-15 11:26:28 UTC",
      "updated_date": "2024-08-15 11:26:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:39:25.900891"
    },
    {
      "arxiv_id": "2408.08083v2",
      "title": "Confidence-weighted integration of human and machine judgments for superior decision-making",
      "title_zh": "翻译失败",
      "authors": [
        "Felipe Yáñez",
        "Xiaoliang Luo",
        "Omar Valerio Minero",
        "Bradley C. Love"
      ],
      "abstract": "Large language models (LLMs) have emerged as powerful tools in various\ndomains. Recent studies have shown that LLMs can surpass humans in certain\ntasks, such as predicting the outcomes of neuroscience studies. What role does\nthis leave for humans in the overall decision process? One possibility is that\nhumans, despite performing worse than LLMs, can still add value when teamed\nwith them. A human and machine team can surpass each individual teammate when\nteam members' confidence is well-calibrated and team members diverge in which\ntasks they find difficult (i.e., calibration and diversity are needed). We\nsimplified and extended a Bayesian approach to combining judgments using a\nlogistic regression framework that integrates confidence-weighted judgments for\nany number of team members. Using this straightforward method, we demonstrated\nin a neuroscience forecasting task that, even when humans were inferior to\nLLMs, their combination with one or more LLMs consistently improved team\nperformance. Our hope is that this simple and effective strategy for\nintegrating the judgments of humans and machines will lead to productive\ncollaborations.",
      "tldr_zh": "该研究探讨了如何通过置信度加权（confidence-weighted）整合人类和机器判断，以提升决策质量，即使人类在某些任务中不如大型语言模型（LLMs）。他们采用简化后的贝叶斯方法和 logistic regression 框架，强调团队成员的校准和多样性，确保在不同任务中发挥互补优势。在神经科学预测任务的实验中，结果显示人类与一个或多个 LLMs 结合后，团队性能 consistently 得到改善，为促进人类-机器合作提供了简单有效的策略。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08083v2",
      "published_date": "2024-08-15 11:16:21 UTC",
      "updated_date": "2025-04-02 00:02:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:39:37.133056"
    },
    {
      "arxiv_id": "2408.08078v1",
      "title": "Treat Stillness with Movement: Remote Sensing Change Detection via Coarse-grained Temporal Foregrounds Mining",
      "title_zh": "翻译失败",
      "authors": [
        "Xixi Wang",
        "Zitian Wang",
        "Jingtao Jiang",
        "Lan Chen",
        "Xiao Wang",
        "Bo Jiang"
      ],
      "abstract": "Current works focus on addressing the remote sensing change detection task\nusing bi-temporal images. Although good performance can be achieved, however,\nseldom of they consider the motion cues which may also be vital. In this work,\nwe revisit the widely adopted bi-temporal images-based framework and propose a\nnovel Coarse-grained Temporal Mining Augmented (CTMA) framework. To be\nspecific, given the bi-temporal images, we first transform them into a video\nusing interpolation operations. Then, a set of temporal encoders is adopted to\nextract the motion features from the obtained video for coarse-grained changed\nregion prediction. Subsequently, we design a novel Coarse-grained Foregrounds\nAugmented Spatial Encoder module to integrate both global and local\ninformation. We also introduce a motion augmented strategy that leverages\nmotion cues as an additional output to aggregate with the spatial features for\nimproved results. Meanwhile, we feed the input image pairs into the ResNet to\nget the different features and also the spatial blocks for fine-grained feature\nlearning. More importantly, we propose a mask augmented strategy that utilizes\ncoarse-grained changed regions, incorporating them into the decoder blocks to\nenhance the final changed prediction. Extensive experiments conducted on\nmultiple benchmark datasets fully validated the effectiveness of our proposed\nframework for remote sensing image change detection. The source code of this\npaper will be released on\nhttps://github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection",
      "tldr_zh": "本研究针对遥感图像变化检测任务，指出现有基于双时相图像的方法忽略了关键的运动线索，并提出了一种新型Coarse-grained Temporal Mining Augmented (CTMA)框架来解决这一问题。具体而言，该框架首先将双时相图像转化为视频，通过时间编码器提取运动特征进行粗粒度变化区域预测，并设计Coarse-grained Foregrounds Augmented Spatial Encoder模块整合全局和本地信息，同时引入运动增强和掩码增强策略来优化空间特征学习。实验在多个基准数据集上验证了CTMA框架的有效性，显著提升了变化检测的准确性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2408.08078v1",
      "published_date": "2024-08-15 11:04:26 UTC",
      "updated_date": "2024-08-15 11:04:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:39:49.628258"
    },
    {
      "arxiv_id": "2408.08074v3",
      "title": "A Survey on Integrated Sensing, Communication, and Computation",
      "title_zh": "集成感知、通信和计算的综述",
      "authors": [
        "Dingzhu Wen",
        "Yong Zhou",
        "Xiaoyang Li",
        "Yuanming Shi",
        "Kaibin Huang",
        "Khaled B. Letaief"
      ],
      "abstract": "The forthcoming generation of wireless technology, 6G, aims to usher in an\nera of ubiquitous intelligent services, where everything is interconnected and\nintelligent. This vision requires the seamless integration of three fundamental\nmodules: Sensing for information acquisition, communication for information\nsharing, and computation for information processing and decision-making. These\nmodules are intricately linked, especially in complex tasks such as edge\nlearning and inference. However, the performance of these modules is\ninterdependent, creating a resource competition for time, energy, and\nbandwidth. Existing techniques like integrated communication and computation\n(ICC), integrated sensing and computation (ISC), and integrated sensing and\ncommunication (ISAC) have made partial strides in addressing this challenge,\nbut they fall short of meeting the extreme performance requirements. To\novercome these limitations, it is essential to develop new techniques that\ncomprehensively integrate sensing, communication, and computation. This\nintegrated approach, known as Integrated Sensing, Communication, and\nComputation (ISCC), offers a systematic perspective for enhancing task\nperformance. This paper begins with a comprehensive survey of historic and\nrelated techniques such as ICC, ISC, and ISAC, highlighting their strengths and\nlimitations. It then discusses the benefits, functions, and challenges of ISCC.\nSubsequently, the state-of-the-art signal designs for ISCC, along with network\nresource management strategies specifically tailored for ISCC are explored.\nFurthermore, this paper discusses the exciting research opportunities that lie\nahead for implementing ISCC in future advanced networks, and the unresolved\nissues requiring further investigation. ISCC is expected to unlock the full\npotential of intelligent connectivity, paving the way for groundbreaking\napplications and services.",
      "tldr_zh": "这篇论文对Integrated Sensing, Communication, and Computation (ISCC)进行了全面调查，旨在解决6G无线技术中感知、通信和计算模块的整合挑战，这些模块在任务如边缘学习中相互依赖但资源竞争激烈。论文回顾了现有技术如Integrated Communication and Computation (ICC)、Integrated Sensing and Computation (ISC)以及Integrated Sensing and Communication (ISAC)的优势与局限性，并强调ISCC能系统提升任务性能，通过先进的信号设计和网络资源管理策略来实现。最终，论文探讨了ISCC在未来网络中的研究机会和潜在问题，预计它将推动智能连接的应用创新。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "This version is accepted by IEEE Communications Surveys & Tutorials\n  on Dec. 18, 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08074v3",
      "published_date": "2024-08-15 11:01:35 UTC",
      "updated_date": "2024-12-18 14:38:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:40:02.367125"
    },
    {
      "arxiv_id": "2408.08067v2",
      "title": "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Dongyu Ru",
        "Lin Qiu",
        "Xiangkun Hu",
        "Tianhang Zhang",
        "Peng Shi",
        "Shuaichen Chang",
        "Cheng Jiayang",
        "Cunxiang Wang",
        "Shichao Sun",
        "Huanyu Li",
        "Zizhao Zhang",
        "Binjie Wang",
        "Jiarong Jiang",
        "Tong He",
        "Zhiguo Wang",
        "Pengfei Liu",
        "Yue Zhang",
        "Zheng Zhang"
      ],
      "abstract": "Despite Retrieval-Augmented Generation (RAG) showing promising capability in\nleveraging external knowledge, a comprehensive evaluation of RAG systems is\nstill challenging due to the modular nature of RAG, evaluation of long-form\nresponses and reliability of measurements. In this paper, we propose a\nfine-grained evaluation framework, RAGChecker, that incorporates a suite of\ndiagnostic metrics for both the retrieval and generation modules. Meta\nevaluation verifies that RAGChecker has significantly better correlations with\nhuman judgments than other evaluation metrics. Using RAGChecker, we evaluate 8\nRAG systems and conduct an in-depth analysis of their performance, revealing\ninsightful patterns and trade-offs in the design choices of RAG architectures.\nThe metrics of RAGChecker can guide researchers and practitioners in developing\nmore effective RAG systems. This work has been open sourced at\nhttps://github.com/amazon-science/RAGChecker.",
      "tldr_zh": "论文提出 RAGChecker，一种细粒度框架，用于诊断 Retrieval-Augmented Generation (RAG) 系统的检索和生成模块，以解决评估中的模块化挑战、长响应问题和测量可靠性。\n该框架包括一套诊断指标，并通过元评估（Meta evaluation）验证，与其他指标相比，其与人类判断的相关性显著更高。\n使用 RAGChecker 评估了 8 个 RAG 系统，揭示了架构设计中的关键模式和权衡，从而指导研究者和从业者开发更有效的 RAG 系统。\n这项工作已在 GitHub 开源（https://github.com/amazon-science/RAGChecker）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review. Github Repo:\n  https://github.com/amazon-science/RAGChecker",
      "pdf_url": "http://arxiv.org/pdf/2408.08067v2",
      "published_date": "2024-08-15 10:20:54 UTC",
      "updated_date": "2024-08-17 00:30:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:40:16.637928"
    },
    {
      "arxiv_id": "2408.08065v3",
      "title": "SPEED: Scalable Preprocessing of EEG Data for Self-Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Anders Gjølbye",
        "Lina Skerath",
        "William Lehn-Schiøler",
        "Nicolas Langer",
        "Lars Kai Hansen"
      ],
      "abstract": "Electroencephalography (EEG) research typically focuses on tasks with\nnarrowly defined objectives, but recent studies are expanding into the use of\nunlabeled data within larger models, aiming for a broader range of\napplications. This addresses a critical challenge in EEG research. For example,\nKostas et al. (2021) show that self-supervised learning (SSL) outperforms\ntraditional supervised methods. Given the high noise levels in EEG data, we\nargue that further improvements are possible with additional preprocessing.\nCurrent preprocessing methods often fail to efficiently manage the large data\nvolumes required for SSL, due to their lack of optimization, reliance on\nsubjective manual corrections, and validation processes or inflexible protocols\nthat limit SSL. We propose a Python-based EEG preprocessing pipeline optimized\nfor self-supervised learning, designed to efficiently process large-scale data.\nThis optimization not only stabilizes self-supervised training but also\nenhances performance on downstream tasks compared to training with raw data.",
      "tldr_zh": "该研究针对 EEG 数据的高噪声和当前预处理方法的局限性（如不优化和主观手动修正），提出了一种可扩展的 Python-based EEG 预处理管道，专门优化用于 Self-Supervised Learning (SSL)。该管道能高效处理大规模数据，稳定训练过程，并显著提升下游任务的表现。相比直接使用原始数据，优化后的预处理方法使 SSL 性能更优越，为 EEG 研究扩展到更广泛应用提供了关键改进。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "To appear in proceedings of 2024 IEEE International workshop on\n  Machine Learning for Signal Processing",
      "pdf_url": "http://arxiv.org/pdf/2408.08065v3",
      "published_date": "2024-08-15 10:15:01 UTC",
      "updated_date": "2024-09-23 10:30:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:40:28.360150"
    },
    {
      "arxiv_id": "2408.08059v1",
      "title": "Maximally Permissive Reward Machines",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni Varricchione",
        "Natasha Alechina",
        "Mehdi Dastani",
        "Brian Logan"
      ],
      "abstract": "Reward machines allow the definition of rewards for temporally extended tasks\nand behaviors. Specifying \"informative\" reward machines can be challenging. One\nway to address this is to generate reward machines from a high-level abstract\ndescription of the learning environment, using techniques such as AI planning.\nHowever, previous planning-based approaches generate a reward machine based on\na single (sequential or partial-order) plan, and do not allow maximum\nflexibility to the learning agent. In this paper we propose a new approach to\nsynthesising reward machines which is based on the set of partial order plans\nfor a goal. We prove that learning using such \"maximally permissive\" reward\nmachines results in higher rewards than learning using RMs based on a single\nplan. We present experimental results which support our theoretical claims by\nshowing that our approach obtains higher rewards than the single-plan approach\nin practice.",
      "tldr_zh": "奖励机器（Reward Machines）允许定义时间扩展任务的奖励，但指定信息丰富的奖励机器具有挑战。  \n本文提出一种新方法，通过基于一组部分顺序计划（partial order plans）的AI规划合成“maximally permissive”奖励机器，从而为学习代理提供最大灵活性。  \n理论证明显示，使用这种奖励机器能获得比基于单个计划的方法更高的奖励，实验结果也验证了这一优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted for publication at the European Conference on\n  Artificial Intelligence (ECAI) 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08059v1",
      "published_date": "2024-08-15 09:59:26 UTC",
      "updated_date": "2024-08-15 09:59:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:40:39.251269"
    },
    {
      "arxiv_id": "2408.08058v1",
      "title": "Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging",
      "title_zh": "使用基础模型应对数据稀缺：医学成像中少样本和零样本学习方法的基准测试",
      "authors": [
        "Stefano Woerner",
        "Christian F. Baumgartner"
      ],
      "abstract": "Data scarcity is a major limiting factor for applying modern machine learning\ntechniques to clinical tasks. Although sufficient data exists for some\nwell-studied medical tasks, there remains a long tail of clinically relevant\ntasks with poor data availability. Recently, numerous foundation models have\ndemonstrated high suitability for few-shot learning (FSL) and zero-shot\nlearning (ZSL), potentially making them more accessible to practitioners.\nHowever, it remains unclear which foundation model performs best on FSL medical\nimage analysis tasks and what the optimal methods are for learning from limited\ndata. We conducted a comprehensive benchmark study of ZSL and FSL using 16\npretrained foundation models on 19 diverse medical imaging datasets. Our\nresults indicate that BiomedCLIP, a model pretrained exclusively on medical\ndata, performs best on average for very small training set sizes, while very\nlarge CLIP models pretrained on LAION-2B perform best with slightly more\ntraining samples. However, simply fine-tuning a ResNet-18 pretrained on\nImageNet performs similarly with more than five training examples per class.\nOur findings also highlight the need for further research on foundation models\nspecifically tailored for medical applications and the collection of more\ndatasets to train these models.",
      "tldr_zh": "这篇论文探讨了数据稀缺对医疗图像机器学习的影响，通过基准测试评估了基础模型在Few-Shot Learning (FSL)和Zero-Shot Learning (ZSL)中的性能。研究者测试了16个预训练模型在19个多样化医疗图像数据集上的表现，结果显示BiomedCLIP（仅在医疗数据上预训练）在训练样本极少时表现出色，而大型CLIP模型（在LAION-2B上预训练）在样本稍多时表现最佳。相比之下，仅fine-tuning ImageNet预训练的ResNet-18在每个类有超过五个样本时也能达到类似效果。该研究强调，需要进一步开发针对医疗应用的foundation models并收集更多数据集，以提升模型的适用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as an oral presentation in MICCAI 2024 2nd International\n  Workshop on Foundation Models for General Medical AI",
      "pdf_url": "http://arxiv.org/pdf/2408.08058v1",
      "published_date": "2024-08-15 09:55:51 UTC",
      "updated_date": "2024-08-15 09:55:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:40:55.232767"
    },
    {
      "arxiv_id": "2408.08055v2",
      "title": "DeNOTS: Stable Deep Neural ODEs for Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Ilya Kuleshov",
        "Evgenia Romanenkova",
        "Galina Boeva",
        "Vladislav Zhuzhel",
        "Evgeni Vorsin",
        "Alexey Zaytsev"
      ],
      "abstract": "Neural ODEs are a prominent branch of methods designed to capture the\ntemporal evolution of complex time-stamped data. Their idea is to solve an ODE\nwith Neural Network-defined dynamics, which take the immediate parameters of\nthe observed system into account. However, larger integration intervals cause\ninstability, which forces most modern methods to normalize time to $[0, 1]$. We\nprovably stabilize these models by introducing an adaptive negative feedback\nmechanism. This modification allows for longer integration, which in turn\nimplies higher expressiveness, mirroring the behaviour of increasing depth in\nconventional Neural Networks.Additionally, it provides intriguing theoretical\nproperties: forgetfulness and missing-value robustness. For three open\ndatasets, our method obtains up to 20\\% improvements in downstream quality if\ncompared to existing baselines, including State Space Models and Neural~CDEs.",
      "tldr_zh": "本文提出DeNOTS，一种稳定的深度Neural ODEs模型，用于处理时间序列数据，通过引入自适应负反馈机制来解决现有模型在较大积分区间的不稳定性问题，从而实现更长的积分时间和更高的表达能力。DeNOTS还具备forgetfulness和missing-value robustness等理论属性。实验结果显示，在三个公开数据集上，与State Space Models和Neural CDEs等基线模型相比，下游任务性能提高了高达20%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08055v2",
      "published_date": "2024-08-15 09:49:37 UTC",
      "updated_date": "2025-04-15 09:49:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:41:02.677614"
    },
    {
      "arxiv_id": "2408.08054v1",
      "title": "Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework",
      "title_zh": "Text2BIM",
      "authors": [
        "Changyu Du",
        "Sebastian Esser",
        "Stavros Nousias",
        "André Borrmann"
      ],
      "abstract": "The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting.",
      "tldr_zh": "该论文提出 Text2BIM，一个基于 Large Language Model (LLM) 的多智能体框架，旨在简化建筑、工程和施工 (AEC) 行业的 BIM 作者过程，通过自然语言指令直接生成 3D 建筑模型，包括内部布局、外部轮廓和语义信息。框架协调多个 LLM 代理协作推理，将用户文本输入转化为调用 BIM 作者工具 API 的代码，并引入规则-based model checker，利用预定义领域知识迭代优化模型质量。实验比较了三种 LLM 的性能，结果表明该方法能有效生成高质量、结构合理的建筑模型，与用户输入概念一致，并通过集成到 Vectorworks 的交互式原型展示了“通过聊天建模”的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08054v1",
      "published_date": "2024-08-15 09:48:45 UTC",
      "updated_date": "2024-08-15 09:48:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:41:17.036859"
    },
    {
      "arxiv_id": "2408.08041v1",
      "title": "The Clever Hans Effect in Unsupervised Learning",
      "title_zh": "无监督学习中的 Clever Hans 效应",
      "authors": [
        "Jacob Kauffmann",
        "Jonas Dippel",
        "Lukas Ruff",
        "Wojciech Samek",
        "Klaus-Robert Müller",
        "Grégoire Montavon"
      ],
      "abstract": "Unsupervised learning has become an essential building block of AI systems.\nThe representations it produces, e.g. in foundation models, are critical to a\nwide variety of downstream applications. It is therefore important to carefully\nexamine unsupervised models to ensure not only that they produce accurate\npredictions, but also that these predictions are not \"right for the wrong\nreasons\", the so-called Clever Hans (CH) effect. Using specially developed\nExplainable AI techniques, we show for the first time that CH effects are\nwidespread in unsupervised learning. Our empirical findings are enriched by\ntheoretical insights, which interestingly point to inductive biases in the\nunsupervised learning machine as a primary source of CH effects. Overall, our\nwork sheds light on unexplored risks associated with practical applications of\nunsupervised learning and suggests ways to make unsupervised learning more\nrobust.",
      "tldr_zh": "这篇论文探讨了无监督学习(Unsupervised learning)中的Clever Hans effect（CH效应），即模型可能因错误原因（如无关特征）而做出正确预测，从而影响AI系统的可靠性。研究者使用专门开发的Explainable AI技术，通过实证实验和理论分析，首次证明了CH效应在无监督学习中广泛存在，并将诱导偏差(inductive biases)识别为主要来源。最终，论文揭示了无监督学习在实际应用中的潜在风险，并提出改进策略，以增强其稳健性和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages + supplement",
      "pdf_url": "http://arxiv.org/pdf/2408.08041v1",
      "published_date": "2024-08-15 09:19:42 UTC",
      "updated_date": "2024-08-15 09:19:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:41:27.294862"
    },
    {
      "arxiv_id": "2408.08024v1",
      "title": "Adaptive User Journeys in Pharma E-Commerce with Reinforcement Learning: Insights from SwipeRx",
      "title_zh": "翻译失败",
      "authors": [
        "Ana Fernández del Río",
        "Michael Brennan Leong",
        "Paulo Saraiva",
        "Ivan Nazarov",
        "Aditya Rastogi",
        "Moiz Hassan",
        "Dexian Tang",
        "África Periáñez"
      ],
      "abstract": "This paper introduces a reinforcement learning (RL) platform that enhances\nend-to-end user journeys in healthcare digital tools through personalization.\nWe explore a case study with SwipeRx, the most popular all-in-one app for\npharmacists in Southeast Asia, demonstrating how the platform can be used to\npersonalize and adapt user experiences. Our RL framework is tested through a\nseries of experiments with product recommendations tailored to each pharmacy\nbased on real-time information on their purchasing history and in-app\nengagement, showing a significant increase in basket size. By integrating\nadaptive interventions into existing mobile health solutions and enriching user\njourneys, our platform offers a scalable solution to improve pharmaceutical\nsupply chain management, health worker capacity building, and clinical decision\nand patient care, ultimately contributing to better healthcare outcomes.",
      "tldr_zh": "本研究引入了一个强化学习 (RL) 平台，用于个性化医疗数字工具的用户旅程，通过适应性干预提升整体体验。以东南亚热门应用 SwipeRx 为案例研究，该平台基于用户的购买历史和应用互动数据，提供定制的产品推荐，显著增加了购物篮大小。实验结果显示，该框架可扩展地改善制药供应链管理、健康工作者能力建设和临床决策，最终促进更好的医疗成果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at the Third Workshop on End-to-End Customer Journey\n  Optimization at KDD 2024 (KDD CJ Workshop '24), August 26, Barcelona, Spain",
      "pdf_url": "http://arxiv.org/pdf/2408.08024v1",
      "published_date": "2024-08-15 08:47:35 UTC",
      "updated_date": "2024-08-15 08:47:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:41:38.668681"
    },
    {
      "arxiv_id": "2408.08023v1",
      "title": "Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Rujia Shen",
        "Boran Wang",
        "Chao Zhao",
        "Yi Guan",
        "Jingchi Jiang"
      ],
      "abstract": "Causal discovery from time-series data aims to capture both intra-slice\n(contemporaneous) and inter-slice (time-lagged) causality between variables\nwithin the temporal chain, which is crucial for various scientific disciplines.\nCompared to causal discovery from non-time-series data, causal discovery from\ntime-series data necessitates more serialized samples with a larger amount of\nobserved time steps. To address the challenges, we propose a novel\ngradient-based causal discovery approach STIC, which focuses on\n\\textbf{S}hort-\\textbf{T}erm \\textbf{I}nvariance using \\textbf{C}onvolutional\nneural networks to uncover the causal relationships from time-series data.\nSpecifically, STIC leverages both the short-term time and mechanism invariance\nof causality within each window observation, which possesses the property of\nindependence, to enhance sample efficiency. Furthermore, we construct two\ncausal convolution kernels, which correspond to the short-term time and\nmechanism invariance respectively, to estimate the window causal graph. To\ndemonstrate the necessity of convolutional neural networks for causal discovery\nfrom time-series data, we theoretically derive the equivalence between\nconvolution and the underlying generative principle of time-series data under\nthe assumption that the additive noise model is identifiable. Experimental\nevaluations conducted on both synthetic and FMRI benchmark datasets demonstrate\nthat our STIC outperforms baselines significantly and achieves the\nstate-of-the-art performance, particularly when the datasets contain a limited\nnumber of observed time steps. Code is available at\n\\url{https://github.com/HITshenrj/STIC}.",
      "tldr_zh": "该论文提出了一种名为 STIC 的梯度-based 因果发现方法，利用短时不变性（Short-Term Invariance）和 Convolutional Neural Networks，从时间序列数据中挖掘同时性和时滞因果关系，以提升样本效率。STIC 通过构建两个因果卷积核，分别对应短时时间不变性和机制不变性，来估计窗口因果图，并理论证明了卷积神经网络与时间序列数据生成原理的等价性。实验结果显示，在合成和 FMRI 数据集上，STIC 显著优于基线模型，特别是在观察时间步有限的情况下，达到了 state-of-the-art 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08023v1",
      "published_date": "2024-08-15 08:43:28 UTC",
      "updated_date": "2024-08-15 08:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:41:54.000147"
    },
    {
      "arxiv_id": "2408.08021v1",
      "title": "DIVE: Towards Descriptive and Diverse Visual Commonsense Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jun-Hyung Park",
        "Hyuntae Park",
        "Youjin Kang",
        "Eojin Jeon",
        "SangKeun Lee"
      ],
      "abstract": "Towards human-level visual understanding, visual commonsense generation has\nbeen introduced to generate commonsense inferences beyond images. However,\ncurrent research on visual commonsense generation has overlooked an important\nhuman cognitive ability: generating descriptive and diverse inferences. In this\nwork, we propose a novel visual commonsense generation framework, called DIVE,\nwhich aims to improve the descriptiveness and diversity of generated\ninferences. DIVE involves two methods, generic inference filtering and\ncontrastive retrieval learning, which address the limitations of existing\nvisual commonsense resources and training objectives. Experimental results\nverify that DIVE outperforms state-of-the-art models for visual commonsense\ngeneration in terms of both descriptiveness and diversity, while showing a\nsuperior quality in generating unique and novel inferences. Notably, DIVE\nachieves human-level descriptiveness and diversity on Visual Commonsense\nGraphs. Furthermore, human evaluations confirm that DIVE aligns closely with\nhuman judgments on descriptiveness and diversity\\footnote{Our code and dataset\nare available at https://github.com/Park-ing-lot/DIVE.",
      "tldr_zh": "这篇论文提出 DIVE 框架，旨在提升视觉常识生成（visual commonsense generation）的描述性和多样性，以弥补现有模型在生成推断方面的局限。DIVE 采用 generic inference filtering 和 contrastive retrieval learning 两种方法，优化了现有资源的利用和训练目标。实验结果显示，DIVE 在描述性和多样性指标上优于最先进模型，并在 Visual Commonsense Graphs 上达到了人类水平。人类评估进一步证实，DIVE 生成的推断在独特性和新颖性方面与人类判断高度一致。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 10 figuers, EMNLP 2023 (main)",
      "pdf_url": "http://arxiv.org/pdf/2408.08021v1",
      "published_date": "2024-08-15 08:37:24 UTC",
      "updated_date": "2024-08-15 08:37:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:42:03.269199"
    },
    {
      "arxiv_id": "2408.08019v1",
      "title": "Accelerating High-Fidelity Waveform Generation via Adversarial Flow Matching Optimization",
      "title_zh": "通过对抗流匹配优化加速高保真波形生成",
      "authors": [
        "Sang-Hoon Lee",
        "Ha-Yeong Choi",
        "Seong-Whan Lee"
      ],
      "abstract": "This paper introduces PeriodWave-Turbo, a high-fidelity and high-efficient\nwaveform generation model via adversarial flow matching optimization. Recently,\nconditional flow matching (CFM) generative models have been successfully\nadopted for waveform generation tasks, leveraging a single vector field\nestimation objective for training. Although these models can generate\nhigh-fidelity waveform signals, they require significantly more ODE steps\ncompared to GAN-based models, which only need a single generation step.\nAdditionally, the generated samples often lack high-frequency information due\nto noisy vector field estimation, which fails to ensure high-frequency\nreproduction. To address this limitation, we enhance pre-trained CFM-based\ngenerative models by incorporating a fixed-step generator modification. We\nutilized reconstruction losses and adversarial feedback to accelerate\nhigh-fidelity waveform generation. Through adversarial flow matching\noptimization, it only requires 1,000 steps of fine-tuning to achieve\nstate-of-the-art performance across various objective metrics. Moreover, we\nsignificantly reduce inference speed from 16 steps to 2 or 4 steps.\nAdditionally, by scaling up the backbone of PeriodWave from 29M to 70M\nparameters for improved generalization, PeriodWave-Turbo achieves unprecedented\nperformance, with a perceptual evaluation of speech quality (PESQ) score of\n4.454 on the LibriTTS dataset. Audio samples, source code and checkpoints will\nbe available at https://github.com/sh-lee-prml/PeriodWave.",
      "tldr_zh": "本论文提出 PeriodWave-Turbo，一种通过 adversarial flow matching optimization 实现的、高保真和高效率的波形生成模型，以解决基于 conditional flow matching (CFM) 模型的训练效率低和高频信息缺失问题。该模型通过添加重建损失和对抗反馈来增强预训练的 CFM 生成器，并将推理步骤从 16 步减少到 2 或 4 步，仅需 1,000 步微调就实现最先进性能。此外，将模型参数从 29M 扩展到 70M 后，PeriodWave-Turbo 在 LibriTTS 数据集上取得 PESQ 得分 4.454，显著提升了波形生成的泛化性和质量。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 9 tables, 1 figure,",
      "pdf_url": "http://arxiv.org/pdf/2408.08019v1",
      "published_date": "2024-08-15 08:34:00 UTC",
      "updated_date": "2024-08-15 08:34:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:42:17.868756"
    },
    {
      "arxiv_id": "2408.08015v1",
      "title": "Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative DNN Training on Heterogeneous Edge Devices",
      "title_zh": "Asteroid：资源",
      "authors": [
        "Shengyuan Ye",
        "Liekang Zeng",
        "Xiaowen Chu",
        "Guoliang Xing",
        "Xu Chen"
      ],
      "abstract": "On-device Deep Neural Network (DNN) training has been recognized as crucial\nfor privacy-preserving machine learning at the edge. However, the intensive\ntraining workload and limited onboard computing resources pose significant\nchallenges to the availability and efficiency of model training. While existing\nworks address these challenges through native resource management optimization,\nwe instead leverage our observation that edge environments usually comprise a\nrich set of accompanying trusted edge devices with idle resources beyond a\nsingle terminal. We propose Asteroid, a distributed edge training system that\nbreaks the resource walls across heterogeneous edge devices for efficient model\ntraining acceleration. Asteroid adopts a hybrid pipeline parallelism to\norchestrate distributed training, along with a judicious parallelism planning\nfor maximizing throughput under certain resource constraints. Furthermore, a\nfault-tolerant yet lightweight pipeline replay mechanism is developed to tame\nthe device-level dynamics for training robustness and performance stability. We\nimplement Asteroid on heterogeneous edge devices with both vision and language\nmodels, demonstrating up to 12.2x faster training than conventional parallelism\nmethods and 2.1x faster than state-of-the-art hybrid parallelism methods\nthrough evaluations. Furthermore, Asteroid can recover training pipeline 14x\nfaster than baseline methods while preserving comparable throughput despite\nunexpected device exiting and failure.",
      "tldr_zh": "该论文提出 Asteroid 系统，用于在异构边缘设备上进行高效的分布式 DNN 训练，以解决资源限制和训练工作量大的挑战。Asteroid 采用混合管道并行（hybrid pipeline parallelism）结合智能并行规划，最大化吞吐量，并引入轻量级容错机制来处理设备动态，确保训练的鲁棒性和性能稳定。实验结果显示，Asteroid 比传统并行方法快 12.2 倍，比最先进混合方法快 2.1 倍，并在设备故障时恢复速度提高 14 倍，同时保持可比的吞吐量。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by The 30th Annual International Conference on Mobile\n  Computing and Networking (MobiCom'24)",
      "pdf_url": "http://arxiv.org/pdf/2408.08015v1",
      "published_date": "2024-08-15 08:25:50 UTC",
      "updated_date": "2024-08-15 08:25:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:42:28.141809"
    },
    {
      "arxiv_id": "2408.08336v1",
      "title": "Graph representations of 3D data for machine learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tomasz Prytuła"
      ],
      "abstract": "We give an overview of combinatorial methods to represent 3D data, such as\ngraphs and meshes, from the viewpoint of their amenability to analysis using\nmachine learning algorithms. We highlight pros and cons of various\nrepresentations and we discuss some methods of generating/switching between the\nrepresentations. We finally present two concrete applications in life science\nand industry. Despite its theoretical nature, our discussion is in general\nmotivated by, and biased towards real-world challenges.",
      "tldr_zh": "这篇论文概述了使用 graphs 和 meshes 等组合方法表示 3D 数据的方法，重点评估这些表示在 machine learning 分析中的适用性。作者讨论了各种表示的优缺点，以及生成或转换这些表示的策略，以应对实际挑战。最终，通过两个具体应用（life science 和 industry），展示了这些方法在真实世界中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "I.2.10; I.4.10; I.5.1; J.2; J.3"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08336v1",
      "published_date": "2024-08-15 07:45:07 UTC",
      "updated_date": "2024-08-15 07:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:42:40.054019"
    },
    {
      "arxiv_id": "2408.07989v1",
      "title": "IIU: Independent Inference Units for Knowledge-based Visual Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Yili Li",
        "Jing Yu",
        "Keke Gai",
        "Gang Xiong"
      ],
      "abstract": "Knowledge-based visual question answering requires external knowledge beyond\nvisible content to answer the question correctly. One limitation of existing\nmethods is that they focus more on modeling the inter-modal and intra-modal\ncorrelations, which entangles complex multimodal clues by implicit embeddings\nand lacks interpretability and generalization ability. The key challenge to\nsolve the above problem is to separate the information and process it\nseparately at the functional level. By reusing each processing unit, the\ngeneralization ability of the model to deal with different data can be\nincreased. In this paper, we propose Independent Inference Units (IIU) for\nfine-grained multi-modal reasoning to decompose intra-modal information by the\nfunctionally independent units. Specifically, IIU processes each\nsemantic-specific intra-modal clue by an independent inference unit, which also\ncollects complementary information by communication from different units. To\nfurther reduce the impact of redundant information, we propose a memory update\nmodule to maintain semantic-relevant memory along with the reasoning process\ngradually. In comparison with existing non-pretrained multi-modal reasoning\nmodels on standard datasets, our model achieves a new state-of-the-art,\nenhancing performance by 3%, and surpassing basic pretrained multi-modal\nmodels. The experimental results show that our IIU model is effective in\ndisentangling intra-modal clues as well as reasoning units to provide\nexplainable reasoning evidence. Our code is available at\nhttps://github.com/Lilidamowang/IIU.",
      "tldr_zh": "本研究针对知识型视觉问答（Knowledge-based Visual Question Answering）中现有方法过度关注模态间和模态内相关性，导致多模态线索纠缠、缺乏可解释性和泛化能力的问题，提出 Independent Inference Units (IIU) 框架，用于细粒度多模态推理。IIU 通过功能独立的推理单元分解和处理每个语义特定的模态内线索，同时通过单元间通信收集互补信息，以提升模型的泛化能力。为减少冗余信息影响，该框架还引入内存更新模块，逐步维护相关的语义记忆。在标准数据集上，IIU 模型相比现有非预训练多模态推理模型提升3%的性能，并超过基本预训练模型，同时提供可解释的推理证据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07989v1",
      "published_date": "2024-08-15 07:30:47 UTC",
      "updated_date": "2024-08-15 07:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:42:53.700010"
    },
    {
      "arxiv_id": "2408.07985v1",
      "title": "Analytical Uncertainty-Based Loss Weighting in Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lukas Kirchdorfer",
        "Cathrin Elich",
        "Simon Kutsche",
        "Heiner Stuckenschmidt",
        "Lukas Schott",
        "Jan M. Köhler"
      ],
      "abstract": "With the rise of neural networks in various domains, multi-task learning\n(MTL) gained significant relevance. A key challenge in MTL is balancing\nindividual task losses during neural network training to improve performance\nand efficiency through knowledge sharing across tasks. To address these\nchallenges, we propose a novel task-weighting method by building on the most\nprevalent approach of Uncertainty Weighting and computing analytically optimal\nuncertainty-based weights, normalized by a softmax function with tunable\ntemperature. Our approach yields comparable results to the combinatorially\nprohibitive, brute-force approach of Scalarization while offering a more\ncost-effective yet high-performing alternative. We conduct an extensive\nbenchmark on various datasets and architectures. Our method consistently\noutperforms six other common weighting methods. Furthermore, we report\nnoteworthy experimental findings for the practical application of MTL. For\nexample, larger networks diminish the influence of weighting methods, and\ntuning the weight decay has a low impact compared to the learning rate.",
      "tldr_zh": "本研究针对多任务学习（Multi-Task Learning, MTL）中的关键挑战，提出了一种新的任务加权方法，即基于分析不确定性（Analytical Uncertainty-Based）的损失加权策略。该方法扩展了Uncertainty Weighting，通过计算分析最优权重并使用softmax函数（带可调温度）进行归一化，提供了一个高效替代暴力枚举（Scalarization）的方法。在多种数据集和架构的基准测试中，该方法优于其他六种常见加权方法，并实现了与高计算成本方法相当的性能。此外，实验发现更大网络会降低加权方法的影响，而调整权重衰减（weight decay）的影响远小于学习率调优。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07985v1",
      "published_date": "2024-08-15 07:10:17 UTC",
      "updated_date": "2024-08-15 07:10:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:43:05.252943"
    },
    {
      "arxiv_id": "2408.07983v1",
      "title": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Faris Hijazi",
        "Somayah AlHarbi",
        "Abdulaziz AlHussein",
        "Harethah Abu Shairah",
        "Reem AlZahrani",
        "Hebah AlShamlan",
        "Omar Knio",
        "George Turkiyyah"
      ],
      "abstract": "The rapid advancements in Large Language Models (LLMs) have led to\nsignificant improvements in various natural language processing tasks. However,\nthe evaluation of LLMs' legal knowledge, particularly in non-English languages\nsuch as Arabic, remains under-explored. To address this gap, we introduce\nArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal\nknowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval\nconsists of multiple tasks sourced from Saudi legal documents and synthesized\nquestions. In this work, we aim to analyze the capabilities required to solve\nlegal problems in Arabic and benchmark the performance of state-of-the-art\nLLMs. We explore the impact of in-context learning and investigate various\nevaluation methods. Additionally, we explore workflows for generating questions\nwith automatic validation to enhance the dataset's quality. We benchmark\nmultilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We\nalso share our methodology for creating the dataset and validation, which can\nbe generalized to other domains. We hope to accelerate AI research in the\nArabic Legal domain by releasing the ArabLegalEval dataset and code:\nhttps://github.com/Thiqah/ArabLegalEval",
      "tldr_zh": "这篇论文引入了 ArabLegalEval，一个多任务基准数据集，用于评估大型语言模型（LLMs）在阿拉伯语法律知识方面的能力，旨在填补非英语语言评估的空白。数据集基于沙特法律文件和合成问题，受 MMLU 和 LegalBench 启发，并通过 in-context learning 以及自动验证工作流来提升质量和评估方法。研究基准测试了多语言和阿拉伯语中心的 LLMs，如 GPT-4 和 Jais，结果显示了这些模型在处理阿拉伯语法律任务的性能差异，并分享了可推广的数据集创建方法，以推动该领域的 AI 研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07983v1",
      "published_date": "2024-08-15 07:09:51 UTC",
      "updated_date": "2024-08-15 07:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:43:17.378748"
    },
    {
      "arxiv_id": "2408.07982v2",
      "title": "Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera",
      "title_zh": "翻译失败",
      "authors": [
        "Hiroki Tanioka",
        "Tetsushi Ueta",
        "Masahiko Sano"
      ],
      "abstract": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry.",
      "tldr_zh": "该研究旨在开发一种基于 Large Language Model (LLM) 的对话系统，通过相机捕捉用户面部表情来识别情绪，并将情绪信息整合到提示中，以实现AI代理与用户的多模态互动。方法涉及从用户面部表情中提取情绪数据，如Happy和Angry，并将其添加到LLM的提示中，以调整对话响应。结果显示，该系统能够针对情绪得分较高的状态（如Happy和Angry）进行适当的对话，但对其他情绪的识别效果有限，为未来AI代理的情感识别应用提供了新思路。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO",
        "68T40",
        "I.2.10; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "4 pages, 5 figures, 1 table, The 1st InterAI: Interactive AI for\n  Human-Centered Robotics workshop in conjunction with IEEE Ro-MAN 2024,\n  Pasadona, LA, USA, Aug. 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.07982v2",
      "published_date": "2024-08-15 07:03:00 UTC",
      "updated_date": "2025-02-18 12:48:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:43:29.359707"
    },
    {
      "arxiv_id": "2408.07981v1",
      "title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiajie Li",
        "Garrett Skinner",
        "Gene Yang",
        "Brian R Quaranto",
        "Steven D Schwaitzberg",
        "Peter C W Kim",
        "Jinjun Xiong"
      ],
      "abstract": "Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset.",
      "tldr_zh": "这篇论文针对多模态大型语言模型 (Multimodal LLMs) 在手术视频领域的不足，创建了 Surg-QA 数据集，该数据集包含 102,000 个手术视频-指令对，是目前最大的此类数据集。研究者提出了一种两阶段问答生成管道，使用本地部署的开源 LLM 结构化学习手术知识，从而降低任务复杂性并减少 LLM 幻觉风险。基于 Surg-QA 数据集，他们训练了 LLaVA-Surg，这是一个新型的视觉语言对话助手，在零样本手术视频问答任务上显著优于现有通用模型。论文将公开代码、模型和数据集，以推动手术智能领域的进一步发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07981v1",
      "published_date": "2024-08-15 07:00:20 UTC",
      "updated_date": "2024-08-15 07:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:43:41.643868"
    },
    {
      "arxiv_id": "2408.07962v1",
      "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Homayoun Honari",
        "Amir Mehdi Soufi Enayati",
        "Mehran Ghafarian Tamizi",
        "Homayoun Najjaran"
      ],
      "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied\nsubcategories of trial-and-error-based methods with the intention to be\ndeployed on real-world systems. In safe RL, the goal is to maximize reward\nperformance while minimizing constraints, often achieved by setting bounds on\nconstraint functions and utilizing the Lagrangian method. However, deploying\nLagrangian-based safe RL in real-world scenarios is challenging due to the\nnecessity of threshold fine-tuning, as imprecise adjustments may lead to\nsuboptimal policy convergence. To mitigate this challenge, we propose a unified\nLagrangian-based model-free architecture called Meta Soft Actor-Critic\nLagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to\nautomatically update the safety-related hyperparameters. The proposed method is\ndesigned to address safe exploration and threshold adjustment with minimal\nhyperparameter tuning requirement. In our pipeline, the inner parameters are\nupdated through the conventional formulation and the hyperparameters are\nadjusted using the meta-objectives which are defined based on the updated\nparameters. Our results show that the agent can reliably adjust the safety\nperformance due to the relatively fast convergence rate of the safety\nthreshold. We evaluate the performance of Meta SAC-Lag in five simulated\nenvironments against Lagrangian baselines, and the results demonstrate its\ncapability to create synergy between parameters, yielding better or competitive\nresults. Furthermore, we conduct a real-world experiment involving a robotic\narm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is\nsuccessfully trained to execute the task, while minimizing effort constraints.",
      "tldr_zh": "该研究针对Safe Reinforcement Learning (Safe RL)中部署挑战，提出了一种统一的模型无关架构Meta SAC-Lag，通过MetaGradient优化自动调整安全相关的超参数，从而减少手动阈值微调的需求。该方法结合Soft Actor-Critic (SAC)和Lagrangian方法，在内层更新参数的同时，使用meta-objectives来优化超参数，确保在最大化奖励的同时最小化约束。实验结果显示，Meta SAC-Lag在五个模拟环境中比Lagrangian基线表现出色或相当，并在真实世界机器人臂任务中成功实现了无溢出的咖啡倒入，证明了其在安全探索和可靠部署方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Main text accepted to the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2024, 10 pages, 4 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.07962v1",
      "published_date": "2024-08-15 06:18:50 UTC",
      "updated_date": "2024-08-15 06:18:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:43:52.971185"
    },
    {
      "arxiv_id": "2408.07956v2",
      "title": "RandomNet: Clustering Time Series Using Untrained Deep Neural Networks",
      "title_zh": "RandomNet：使用未训练深度神经网络聚类时间序列",
      "authors": [
        "Xiaosheng Li",
        "Wenjie Xi",
        "Jessica Lin"
      ],
      "abstract": "Neural networks are widely used in machine learning and data mining.\nTypically, these networks need to be trained, implying the adjustment of\nweights (parameters) within the network based on the input data. In this work,\nwe propose a novel approach, RandomNet, that employs untrained deep neural\nnetworks to cluster time series. RandomNet uses different sets of random\nweights to extract diverse representations of time series and then ensembles\nthe clustering relationships derived from these different representations to\nbuild the final clustering results. By extracting diverse representations, our\nmodel can effectively handle time series with different characteristics. Since\nall parameters are randomly generated, no training is required during the\nprocess. We provide a theoretical analysis of the effectiveness of the method.\nTo validate its performance, we conduct extensive experiments on all of the 128\ndatasets in the well-known UCR time series archive and perform statistical\nanalysis of the results. These datasets have different sizes, sequence lengths,\nand they are from diverse fields. The experimental results show that the\nproposed method is competitive compared with existing state-of-the-art methods.",
      "tldr_zh": "本论文提出 RandomNet 方法，利用未经训练的 deep neural networks 来聚类 time series。通过使用不同随机权重提取多样表示，并集成这些表示的聚类关系，RandomNet 能够有效处理具有不同特性的时间序列，且无需进行权重调整的训练过程。论文提供了该方法的理论分析，并在 UCR 时间序列档案的 128 个数据集上进行广泛实验，结果显示 RandomNet 与现有最先进方法具有竞争性表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.07956v2",
      "published_date": "2024-08-15 06:09:19 UTC",
      "updated_date": "2024-08-16 20:53:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:44:03.600913"
    },
    {
      "arxiv_id": "2408.07947v4",
      "title": "Conditional Brownian Bridge Diffusion Model for VHR SAR to Optical Image Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Seon-Hoon Kim",
        "Dae-Won Chung"
      ],
      "abstract": "Synthetic Aperture Radar (SAR) imaging technology provides the unique\nadvantage of being able to collect data regardless of weather conditions and\ntime. However, SAR images exhibit complex backscatter patterns and speckle\nnoise, which necessitate expertise for interpretation. Research on translating\nSAR images into optical-like representations has been conducted to aid the\ninterpretation of SAR data. Nevertheless, existing studies have predominantly\nutilized low-resolution satellite imagery datasets and have largely been based\non Generative Adversarial Network (GAN) which are known for their training\ninstability and low fidelity. To overcome these limitations of low-resolution\ndata usage and GAN-based approaches, this letter introduces a conditional\nimage-to-image translation approach based on Brownian Bridge Diffusion Model\n(BBDM). We conducted comprehensive experiments on the MSAW dataset, a paired\nSAR and optical images collection of 0.5m Very-High-Resolution (VHR). The\nexperimental results indicate that our method surpasses both the Conditional\nDiffusion Models (CDMs) and the GAN-based models in diverse perceptual quality\nmetrics.",
      "tldr_zh": "该研究针对合成孔径雷达(SAR)图像的复杂回波模式和斑点噪声问题，提出了一种基于Brownian Bridge Diffusion Model (BBDM)的条件图像到图像翻译方法，以将SAR图像转化为类似光学图像，从而提升解读易用性。不同于以往依赖低分辨率数据和Generative Adversarial Network (GAN)的研究，该方法利用BBDM克服了训练不稳定性和低保真度问题，并在MSAW数据集（0.5m Very-High-Resolution (VHR)配对SAR和光学图像）上进行全面实验。结果显示，该方法在多种感知质量指标上优于Conditional Diffusion Models (CDMs)和GAN模型，提供了一种更可靠的SAR图像翻译方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "5 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.07947v4",
      "published_date": "2024-08-15 05:43:46 UTC",
      "updated_date": "2025-04-20 09:03:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:44:27.048973"
    },
    {
      "arxiv_id": "2408.07945v1",
      "title": "Solving a Rubik's Cube Using its Local Graph Structure",
      "title_zh": "翻译失败",
      "authors": [
        "Shunyu Yao",
        "Mitchy Lee"
      ],
      "abstract": "The Rubix Cube is a 3-dimensional single-player combination puzzle attracting\nattention in the reinforcement learning community. A Rubix Cube has six faces\nand twelve possible actions, leading to a small and unconstrained action space\nand a very large state space with only one goal state. Modeling such a large\nstate space and storing the information of each state requires exceptional\ncomputational resources, which makes it challenging to find the shortest\nsolution to a scrambled Rubix cube with limited resources. The Rubix Cube can\nbe represented as a graph, where states of the cube are nodes and actions are\nedges. Drawing on graph convolutional networks, we design a new heuristic,\nweighted convolutional distance, for A star search algorithm to find the\nsolution to a scrambled Rubix Cube. This heuristic utilizes the information of\nneighboring nodes and convolves them with attention-like weights, which creates\na deeper search for the shortest path to the solved state.",
      "tldr_zh": "本研究针对Rubik's Cube魔方的巨大状态空间和计算资源限制问题，提出了一种基于其局部图结构的新方法。该方法将魔方状态表示为图（nodes为状态，edges为动作），并设计了weighted convolutional distance作为A* search algorithm的启发式函数，利用图卷积网络和注意力-like权重来整合邻居节点信息，从而更有效地搜索最短解路径。通过这种方式，研究成功降低了计算需求，并为解决类似组合谜题提供了新思路。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07945v1",
      "published_date": "2024-08-15 05:39:52 UTC",
      "updated_date": "2024-08-15 05:39:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:44:27.398209"
    },
    {
      "arxiv_id": "2408.07931v2",
      "title": "Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning",
      "title_zh": "翻译失败",
      "authors": [
        "Haofeng Liu",
        "Erli Zhang",
        "Junde Wu",
        "Mingxuan Hong",
        "Yueming Jin"
      ],
      "abstract": "Surgical video segmentation is a critical task in computer-assisted surgery\nand is vital for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has shown superior advancements in\nimage and video segmentation. However, SAM2 struggles with efficiency due to\nthe high computational demands of processing high-resolution images and complex\nand long-range temporal dynamics in surgical videos. To address these\nchallenges, we introduce Surgical SAM 2 (SurgSAM2), an advanced model to\nutilize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to facilitate\nreal-time surgical video segmentation. The EFP mechanism dynamically manages\nthe memory bank by selectively retaining only the most informative frames,\nreducing memory usage and computational cost while maintaining high\nsegmentation accuracy. Our extensive experiments demonstrate that SurgSAM2\nsignificantly improves both efficiency and segmentation accuracy compared to\nthe vanilla SAM2. Remarkably, SurgSAM2 achieves a 3$\\times$ FPS compared with\nSAM2, while also delivering state-of-the-art performance after fine-tuning with\nlower-resolution data. These advancements establish SurgSAM2 as a leading model\nfor surgical video analysis, making real-time surgical video segmentation in\nresource-constrained environments a reality. Our source code is available at\nhttps://github.com/jinlab-imvr/Surgical-SAM-2.",
      "tldr_zh": "该研究针对手术视频分割任务，提出 Surgical SAM 2 (SurgSAM2) 模型，以解决 Segment Anything Model 2 (SAM2) 在处理高分辨率图像和复杂时间动态时的高计算需求问题。SurgSAM2 引入 Efficient Frame Pruning (EFP) 机制，通过动态选择保留最信息丰富的帧，显著降低内存和计算成本，同时维持高分割准确性。实验结果显示，SurgSAM2 相比原 SAM2 帧率提升 3 倍，并在低分辨率数据上微调后实现最先进性能。这些改进使实时手术视频分割在资源受限环境中成为可能，推动计算机辅助手术的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024 Workshop AIM-FM",
      "pdf_url": "http://arxiv.org/pdf/2408.07931v2",
      "published_date": "2024-08-15 04:59:12 UTC",
      "updated_date": "2025-03-11 12:57:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:44:42.723680"
    },
    {
      "arxiv_id": "2408.07930v4",
      "title": "MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Xie",
        "Gaochen Wu",
        "Bowen Zhou"
      ],
      "abstract": "Recent In-Context Learning based methods have achieved remarkable success in\nText-to-SQL task. However, there is still a large gap between the performance\nof these models and human performance on datasets with complex database schema\nand difficult questions, such as BIRD. Besides, existing work has neglected to\nsupervise intermediate steps when solving questions iteratively with question\ndecomposition methods, and the schema linking methods used in these works are\nvery rudimentary. To address these issues, we propose MAG-SQL, a multi-agent\ngenerative approach with soft schema linking and iterative Sub-SQL refinement.\nIn our framework, an entity-based method with tables' summary is used to select\nthe columns in database, and a novel targets-conditions decomposition method is\nintroduced to decompose those complex questions. Additionally, we build a\niterative generating module which includes a Sub-SQL Generator and Sub-SQL\nRefiner, introducing external oversight for each step of generation. Through a\nseries of ablation studies, the effectiveness of each agent in our framework\nhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL\nachieves an execution accuracy of 61.08%, compared to the baseline accuracy of\n46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.\nBesides, our approach makes similar progress on Spider. The codes are available\nat https://github.com/LancelotXWX/MAG-SQL.",
      "tldr_zh": "这篇论文提出了 MAG-SQL，一种多智能体生成方法，用于 Text-to-SQL 任务，旨在解决复杂数据库模式（如 BIRD 数据集）下的性能差距问题。MAG-SQL 引入 soft schema linking 结合表摘要选择列、targets-conditions 分解方法分解复杂查询，以及迭代 Sub-SQL 精炼模块，提供外部监督以优化生成过程。通过消融研究验证了各组件的有效性，在 BIRD 基准上使用 GPT-4 实现 61.08% 的执行准确率，比基线（vanilla GPT-4 的 46.35% 和 MAC-SQL 的 57.56%）显著提升，并在 Spider 数据集上取得类似进步。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.07930v4",
      "published_date": "2024-08-15 04:57:55 UTC",
      "updated_date": "2024-11-07 03:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:44:55.991661"
    },
    {
      "arxiv_id": "2408.08335v1",
      "title": "Plan with Code: Comparing approaches for robust NL to DSL generation",
      "title_zh": "翻译失败",
      "authors": [
        "Nastaran Bassamzadeh",
        "Chhaya Methani"
      ],
      "abstract": "Planning in code is considered a more reliable approach for many\norchestration tasks. This is because code is more tractable than steps\ngenerated via Natural Language and make it easy to support more complex\nsequences by abstracting deterministic logic into functions. It also allows\nspotting issues with incorrect function names with the help of parsing checks\nthat can be run on code. Progress in Code Generation methodologies, however,\nremains limited to general-purpose languages like C, C++, and Python. LLMs\ncontinue to face challenges with custom function names in Domain Specific\nLanguages or DSLs, leading to higher hallucination rates and syntax errors.\nThis is more common for custom function names, that are typically part of the\nplan. Moreover, keeping LLMs up-to-date with newer function names is an issue.\nThis poses a challenge for scenarios like task planning over a large number of\nAPIs, since the plan is represented as a DSL having custom API names. In this\npaper, we focus on workflow automation in RPA (Robotic Process Automation)\ndomain as a special case of task planning. We present optimizations for using\nRetrieval Augmented Generation (or RAG) with LLMs for DSL generation along with\nan ablation study comparing these strategies with a fine-tuned model. Our\nresults showed that the fine-tuned model scored the best on code similarity\nmetric. However, with our optimizations, RAG approach is able to match the\nquality for in-domain API names in the test set. Additionally, it offers\nsignificant advantage for out-of-domain or unseen API names, outperforming\nFine-Tuned model on similarity metric by 7 pts.",
      "tldr_zh": "该论文比较了使用代码进行规划的鲁棒方法，专注于从自然语言(NL)生成领域特定语言(DSL)，以提高任务编排的可靠性，如支持复杂序列和错误检查。研究者针对机器人过程自动化(RPA)领域的任务规划，优化了检索增强生成(RAG)技术与LLMs结合，用于DSL生成，并通过消融研究与fine-tuned模型进行对比。结果显示，fine-tuned模型在代码相似性指标上表现最佳，但优化后的RAG方法在领域内API名称上可匹敌其质量，并在领域外或未见API名称上优于fine-tuned模型7点，从而为处理动态API场景提供了更灵活的解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "I.2.2; I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages, 1 figure, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:2407.02742",
      "pdf_url": "http://arxiv.org/pdf/2408.08335v1",
      "published_date": "2024-08-15 04:29:33 UTC",
      "updated_date": "2024-08-15 04:29:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:45:04.363862"
    },
    {
      "arxiv_id": "2408.11854v2",
      "title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?",
      "title_zh": "翻译失败",
      "authors": [
        "Yanjun Gao",
        "Skatje Myers",
        "Shan Chen",
        "Dmitriy Dligach",
        "Timothy A Miller",
        "Danielle Bitterman",
        "Matthew Churpek",
        "Majid Afshar"
      ],
      "abstract": "The introduction of Large Language Models (LLMs) has advanced data\nrepresentation and analysis, bringing significant progress in their use for\nmedical questions and answering. Despite these advancements, integrating\ntabular data, especially numerical data pivotal in clinical contexts, into LLM\nparadigms has not been thoroughly explored. In this study, we examine the\neffectiveness of vector representations from last hidden states of LLMs for\nmedical diagnostics and prognostics using electronic health record (EHR) data.\nWe compare the performance of these embeddings with that of raw numerical EHR\ndata when used as feature inputs to traditional machine learning (ML)\nalgorithms that excel at tabular data learning, such as eXtreme Gradient\nBoosting. We focus on instruction-tuned LLMs in a zero-shot setting to\nrepresent abnormal physiological data and evaluating their utilities as feature\nextractors to enhance ML classifiers for predicting diagnoses, length of stay,\nand mortality. Furthermore, we examine prompt engineering techniques on\nzero-shot and few-shot LLM embeddings to measure their impact comprehensively.\nAlthough findings suggest the raw data features still prevails in medical ML\ntasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a\npromising avenue for future research in medical applications.",
      "tldr_zh": "本研究探讨了 Large Language Models (LLMs) 嵌入在医疗机器学习应用中处理数值数据的有效性，特别是使用电子健康记录 (EHR) 数据进行诊断和预后预测。研究者比较了 LLM 的最后一个隐藏状态向量表示与原始数值 EHR 数据，作为传统机器学习算法（如 eXtreme Gradient Boosting）的特征输入，并在零样本设置下评估其性能，同时考察提示工程技术的影响。结果表明，原始数据特征在预测诊断、住院时间和死亡率等任务中仍占优势，但 zero-shot LLM 嵌入显示出竞争性结果，为未来医疗应用提供了潜在研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.11854v2",
      "published_date": "2024-08-15 03:56:40 UTC",
      "updated_date": "2024-09-19 22:19:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:45:27.995637"
    },
    {
      "arxiv_id": "2408.07911v2",
      "title": "CEGRL-TKGR: A Causal Enhanced Graph Representation Learning Framework for Temporal Knowledge Graph Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Jinze Sun",
        "Yongpan Sheng",
        "Lirong He",
        "Yongbin Qin",
        "Ming Liu",
        "Tao Jia"
      ],
      "abstract": "Temporal knowledge graph reasoning (TKGR) is increasingly gaining attention\nfor its ability to extrapolate new events from historical data, thereby\nenriching the inherently incomplete temporal knowledge graphs. Existing\ngraph-based representation learning frameworks have made significant strides in\ndeveloping evolving representations for both entities and relational\nembeddings. Despite these achievements, there's a notable tendency in these\nmodels to inadvertently learn biased data representations and mine spurious\ncorrelations, consequently failing to discern the causal relationships between\nevents. This often leads to incorrect predictions based on these false\ncorrelations. To address this, we propose an innovative Causal Enhanced Graph\nRepresentation Learning framework for TKGR (named CEGRL-TKGR). This framework\nintroduces causal structures in graph-based representation learning to unveil\nthe essential causal relationships between events, ultimately enhancing the\nperformance of the TKGR task. Specifically, we first disentangle the\nevolutionary representations of entities and relations in a temporal knowledge\ngraph sequence into two distinct components, namely causal representations and\nconfounding representations. Then, drawing on causal intervention theory, we\nadvocate the utilization of causal representations for predictions, aiming to\nmitigate the effects of erroneous correlations caused by confounding features,\nthus achieving more robust and accurate predictions. Finally, extensive\nexperimental results on six benchmark datasets demonstrate the superior\nperformance of our model in the link prediction task.",
      "tldr_zh": "本文提出 CEGRL-TKGR，一种因果增强图表示学习框架，用于 Temporal Knowledge Graph Reasoning (TKGR)，旨在解决现有模型学习偏置数据表示和虚假相关性的问题，从而准确识别事件间的因果关系。框架通过将实体和关系的演化表示分解为 causal representations 和 confounding representations，并应用 causal intervention theory，仅使用因果表示进行预测，以减少混杂特征对预测的干扰。实验在六个基准数据集上证明，该模型在链接预测任务中表现出优越性能，显著提升了 TKGR 的准确性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07911v2",
      "published_date": "2024-08-15 03:34:53 UTC",
      "updated_date": "2025-01-24 10:37:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:45:29.787079"
    },
    {
      "arxiv_id": "2408.07906v1",
      "title": "KAN versus MLP on Irregular or Noisy Functions",
      "title_zh": "KAN 与 MLP 在不规则或噪声函数上的对比",
      "authors": [
        "Chen Zeng",
        "Jiahui Wang",
        "Haoran Shen",
        "Qiao Wang"
      ],
      "abstract": "In this paper, we compare the performance of Kolmogorov-Arnold Networks (KAN)\nand Multi-Layer Perceptron (MLP) networks on irregular or noisy functions. We\ncontrol the number of parameters and the size of the training samples to ensure\na fair comparison. For clarity, we categorize the functions into six types:\nregular functions, continuous functions with local non-differentiable points,\nfunctions with jump discontinuities, functions with singularities, functions\nwith coherent oscillations, and noisy functions. Our experimental results\nindicate that KAN does not always perform best. For some types of functions,\nMLP outperforms or performs comparably to KAN. Furthermore, increasing the size\nof training samples can improve performance to some extent. When noise is added\nto functions, the irregular features are often obscured by the noise, making it\nchallenging for both MLP and KAN to extract these features effectively. We hope\nthese experiments provide valuable insights for future neural network research\nand encourage further investigations to overcome these challenges.",
      "tldr_zh": "本文比较了Kolmogorov-Arnold Networks (KAN) 和 Multi-Layer Perceptron (MLP) 在不规则或噪声函数上的性能，通过控制参数数量和训练样本大小进行公平对比，并将函数分为六类，包括常规函数、局部不可微函数、有跳跃不连续函数、有奇点函数、有相干振荡函数以及噪声函数。实验结果显示，KAN并非总是表现最佳；在某些函数类型上，MLP优于或相当于是KAN，且增加训练样本大小可一定程度上提升性能。作者发现，添加噪声后，不规则特征往往被掩盖，使KAN和MLP都难以有效提取这些特征。该研究为未来神经网络研究提供见解，并鼓励进一步探索这些挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "cs.NE",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07906v1",
      "published_date": "2024-08-15 03:24:07 UTC",
      "updated_date": "2024-08-15 03:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:45:42.067055"
    },
    {
      "arxiv_id": "2408.07904v1",
      "title": "Assessing Language Models' Worldview for Fiction Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Aisha Khatun",
        "Daniel G. Brown"
      ],
      "abstract": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant\napplications in computational creativity. One such application is fictional\nstory generation. Fiction is a narrative that occurs in a story world that is\nslightly different than ours. With LLMs becoming writing partners, we question\nhow suitable they are to generate fiction. This study investigates the ability\nof LLMs to maintain a state of world essential to generate fiction. Through a\nseries of questions to nine LLMs, we find that only two models exhibit\nconsistent worldview, while the rest are self-conflicting. Subsequent analysis\nof stories generated by four models revealed a strikingly uniform narrative\npattern. This uniformity across models further suggests a lack of `state'\nnecessary for fiction. We highlight the limitations of current LLMs in fiction\nwriting and advocate for future research to test and create story worlds for\nLLMs to reside in. All code, dataset, and the generated responses can be found\nin https://github.com/tanny411/llm-reliability-and-consistency-evaluation.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLMs）在生成虚构故事时的世界观一致性，重点探讨这些模型是否能维护与现实略有不同的故事世界。研究通过向九个LLMs提出一系列问题，发现只有两个模型表现出稳定的世界观，而其他模型存在自我冲突现象。进一步分析四个模型生成的故事揭示了惊人的统一叙述模式，这表明LLMs缺乏必要的“状态”来支持多样化的虚构创作。论文强调当前LLMs在虚构写作中的局限性，并呼吁未来研究测试和构建专属的故事世界以提升其能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Short paper",
      "pdf_url": "http://arxiv.org/pdf/2408.07904v1",
      "published_date": "2024-08-15 03:19:41 UTC",
      "updated_date": "2024-08-15 03:19:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:46:04.079569"
    },
    {
      "arxiv_id": "2408.07891v1",
      "title": "Quantum-inspired Interpretable Deep Learning Architecture for Text Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Bingyu Li",
        "Da Zhang",
        "Zhiyuan Zhao",
        "Junyu Gao",
        "Yuan Yuan"
      ],
      "abstract": "Text has become the predominant form of communication on social media,\nembedding a wealth of emotional nuances. Consequently, the extraction of\nemotional information from text is of paramount importance. Despite previous\nresearch making some progress, existing text sentiment analysis models still\nface challenges in integrating diverse semantic information and lack\ninterpretability. To address these issues, we propose a quantum-inspired deep\nlearning architecture that combines fundamental principles of quantum mechanics\n(QM principles) with deep learning models for text sentiment analysis.\nSpecifically, we analyze the commonalities between text representation and QM\nprinciples to design a quantum-inspired text representation method and further\ndevelop a quantum-inspired text embedding layer. Additionally, we design a\nfeature extraction layer based on long short-term memory (LSTM) networks and\nself-attention mechanisms (SAMs). Finally, we calculate the text density matrix\nusing the quantum complex numbers principle and apply 2D-convolution neural\nnetworks (CNNs) for feature condensation and dimensionality reduction. Through\na series of visualization, comparative, and ablation experiments, we\ndemonstrate that our model not only shows significant advantages in accuracy\nand efficiency compared to previous related models but also achieves a certain\nlevel of interpretability by integrating QM principles. Our code is available\nat QISA.",
      "tldr_zh": "本研究提出了一种受量子力学(Quantum Mechanics)启发的深度学习架构，用于文本情感分析，旨在解决现有模型在整合多样语义信息和可解释性方面的挑战。  \n该架构包括量子启发的文本表示方法和嵌入层、基于长短时记忆网络(LSTM)和自注意力机制(Self-Attention Mechanisms)的特征提取层，以及利用文本密度矩阵和2D-卷积神经网络(2D-CNNs)进行特征浓缩和降维。  \n实验结果表明，该模型在准确性和效率上显著优于现有相关模型，并通过可视化、比较和消融实验证明了其可解释性优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07891v1",
      "published_date": "2024-08-15 02:32:50 UTC",
      "updated_date": "2024-08-15 02:32:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:46:08.052893"
    },
    {
      "arxiv_id": "2408.07877v4",
      "title": "BCR-DRL: Behavior- and Context-aware Reward for Deep Reinforcement Learning in Human-AI Coordination",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Hao",
        "Bahareh Nakisa",
        "Mohmmad Naim Rastgoo",
        "Richard Dazeley",
        "Gaoyang Pang"
      ],
      "abstract": "Deep reinforcement Learning (DRL) offers a powerful framework for training AI\nagents to coordinate with human partners. However, DRL faces two critical\nchallenges in human-AI coordination (HAIC): sparse rewards and unpredictable\nhuman behaviors. These challenges significantly limit DRL to identify effective\ncoordination policies, due to its impaired capability of optimizing exploration\nand exploitation. To address these limitations, we propose an innovative\nbehavior- and context-aware reward (BCR) for DRL, which optimizes exploration\nand exploitation by leveraging human behaviors and contextual information in\nHAIC. Our BCR consists of two components: (i)~A novel dual intrinsic rewarding\nscheme to enhance exploration. This scheme composes an AI self-motivated\nintrinsic reward and a human-motivated intrinsic reward, which are designed to\nincrease the capture of sparse rewards by a logarithmic-based strategy; and\n(ii)~A new context-aware weighting mechanism for the designed rewards to\nimprove exploitation. This mechanism helps the AI agent prioritize actions that\nbetter coordinate with the human partner by utilizing contextual information\nthat can reflect the evolution of learning in HAIC. Extensive simulations in\nthe Overcooked environment demonstrate that our approach can increase the\ncumulative sparse rewards by approximately 20% and reduce the convergence time\nby about 67% compared to state-of-the-art baselines.",
      "tldr_zh": "本论文提出了一种行为和上下文感知奖励（BCR）机制，用于提升深度强化学习（DRL）在人机协调（HAIC）中的表现，针对稀疏奖励和人类行为不可预测的挑战。BCR 包括双重内在奖励方案（AI 自激励和人类激励奖励），通过对数-based 策略增强探索能力，以及上下文感知加权机制，利用环境信息优化利用策略以更好地协调人类伙伴。实验在 Overcooked 环境中表明，该方法使累计稀疏奖励提高约 20%，并将收敛时间减少约 67%，相比最先进基线显著改善了 DRL 的效率和效果。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07877v4",
      "published_date": "2024-08-15 01:33:06 UTC",
      "updated_date": "2025-02-07 00:03:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:46:33.683414"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 91,
  "processed_papers_count": 91,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T15:47:01.080081"
}
[
  {
    "arxiv_id": "2406.06852v5",
    "title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language Models",
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "Xiaoyu Xu",
      "Xiaobao Wu",
      "Jie Fu",
      "Yichao Feng",
      "Fengjun Pan",
      "Luu Anh Tuan"
    ],
    "abstract": "Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted in TMLR",
    "pdf_url": "http://arxiv.org/pdf/2406.06852v5",
    "published_date": "2024-06-10 23:54:21 UTC",
    "updated_date": "2025-01-04 13:39:47 UTC"
  },
  {
    "arxiv_id": "2406.06848v1",
    "title": "Taxes Are All You Need: Integration of Taxonomical Hierarchy Relationships into the Contrastive Loss",
    "authors": [
      "Kiran Kokilepersaud",
      "Yavuz Yarici",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "abstract": "In this work, we propose a novel supervised contrastive loss that enables the\nintegration of taxonomic hierarchy information during the representation\nlearning process. A supervised contrastive loss operates by enforcing that\nimages with the same class label (positive samples) project closer to each\nother than images with differing class labels (negative samples). The advantage\nof this approach is that it directly penalizes the structure of the\nrepresentation space itself. This enables greater flexibility with respect to\nencoding semantic concepts. However, the standard supervised contrastive loss\nonly enforces semantic structure based on the downstream task (i.e. the class\nlabel). In reality, the class label is only one level of a \\emph{hierarchy of\ndifferent semantic relationships known as a taxonomy}. For example, the class\nlabel is oftentimes the species of an animal, but between different classes\nthere are higher order relationships such as all animals with wings being\n``birds\". We show that by explicitly accounting for these relationships with a\nweighting penalty in the contrastive loss we can out-perform the supervised\ncontrastive loss. Additionally, we demonstrate the adaptability of the notion\nof a taxonomy by integrating our loss into medical and noise-based settings\nthat show performance improvements by as much as 7%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE International Conference on Image Processing",
    "pdf_url": "http://arxiv.org/pdf/2406.06848v1",
    "published_date": "2024-06-10 23:36:58 UTC",
    "updated_date": "2024-06-10 23:36:58 UTC"
  },
  {
    "arxiv_id": "2406.06838v1",
    "title": "Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes",
    "authors": [
      "Dan Qiao",
      "Kaiqi Zhang",
      "Esha Singh",
      "Daniel Soudry",
      "Yu-Xiang Wang"
    ],
    "abstract": "We study the generalization of two-layer ReLU neural networks in a univariate\nnonparametric regression problem with noisy labels. This is a problem where\nkernels (\\emph{e.g.} NTK) are provably sub-optimal and benign overfitting does\nnot happen, thus disqualifying existing theory for interpolating (0-loss,\nglobal optimal) solutions. We present a new theory of generalization for local\nminima that gradient descent with a constant learning rate can \\emph{stably}\nconverge to. We show that gradient descent with a fixed learning rate $\\eta$\ncan only find local minima that represent smooth functions with a certain\nweighted \\emph{first order total variation} bounded by $1/\\eta - 1/2 +\n\\widetilde{O}(\\sigma + \\sqrt{\\mathrm{MSE}})$ where $\\sigma$ is the label noise\nlevel, $\\mathrm{MSE}$ is short for mean squared error against the ground truth,\nand $\\widetilde{O}(\\cdot)$ hides a logarithmic factor. Under mild assumptions,\nwe also prove a nearly-optimal MSE bound of $\\widetilde{O}(n^{-4/5})$ within\nthe strict interior of the support of the $n$ data points. Our theoretical\nresults are validated by extensive simulation that demonstrates large learning\nrate training induces sparse linear spline fits. To the best of our knowledge,\nwe are the first to obtain generalization bound via minima stability in the\nnon-interpolation case and the first to show ReLU NNs without regularization\ncan achieve near-optimal rates in nonparametric regression.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "51 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.06838v1",
    "published_date": "2024-06-10 22:57:27 UTC",
    "updated_date": "2024-06-10 22:57:27 UTC"
  },
  {
    "arxiv_id": "2406.06823v1",
    "title": "Locally Interdependent Multi-Agent MDP: Theoretical Framework for Decentralized Agents with Dynamic Dependencies",
    "authors": [
      "Alex DeWeese",
      "Guannan Qu"
    ],
    "abstract": "Many multi-agent systems in practice are decentralized and have dynamically\nvarying dependencies. There has been a lack of attempts in the literature to\nanalyze these systems theoretically. In this paper, we propose and\ntheoretically analyze a decentralized model with dynamically varying\ndependencies called the Locally Interdependent Multi-Agent MDP. This model can\nrepresent problems in many disparate domains such as cooperative navigation,\nobstacle avoidance, and formation control. Despite the intractability that\ngeneral partially observable multi-agent systems suffer from, we propose three\nclosed-form policies that are theoretically near-optimal in this setting and\ncan be scalable to compute and store. Consequentially, we reveal a fundamental\nproperty of Locally Interdependent Multi-Agent MDP's that the partially\nobservable decentralized solution is exponentially close to the fully\nobservable solution with respect to the visibility radius. We then discuss\nextensions of our closed-form policies to further improve tractability. We\nconclude by providing simulations to investigate some long horizon behaviors of\nour closed-form policies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to International Conference on Machine Learning 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06823v1",
    "published_date": "2024-06-10 22:11:00 UTC",
    "updated_date": "2024-06-10 22:11:00 UTC"
  },
  {
    "arxiv_id": "2406.06822v1",
    "title": "An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection",
    "authors": [
      "Shenao Yan",
      "Shen Wang",
      "Yue Duan",
      "Hanbin Hong",
      "Kiho Lee",
      "Doowon Kim",
      "Yuan Hong"
    ],
    "abstract": "Large Language Models (LLMs) have transformed code completion tasks,\nproviding context-based suggestions to boost developer productivity in software\nengineering. As users often fine-tune these models for specific applications,\npoisoning and backdoor attacks can covertly alter the model outputs. To address\nthis critical security challenge, we introduce CodeBreaker, a pioneering\nLLM-assisted backdoor attack framework on code completion models. Unlike recent\nattacks that embed malicious payloads in detectable or irrelevant sections of\nthe code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for\nsophisticated payload transformation (without affecting functionalities),\nensuring that both the poisoned data for fine-tuning and generated code can\nevade strong vulnerability detection. CodeBreaker stands out with its\ncomprehensive coverage of vulnerabilities, making it the first to provide such\nan extensive set for evaluation. Our extensive experimental evaluations and\nuser studies underline the strong attack performance of CodeBreaker across\nvarious settings, validating its superiority over existing approaches. By\nintegrating malicious payloads directly into the source code with minimal\ntransformation, CodeBreaker challenges current security measures, underscoring\nthe critical need for more robust defenses for code completion.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in USENIX Security '24",
    "pdf_url": "http://arxiv.org/pdf/2406.06822v1",
    "published_date": "2024-06-10 22:10:05 UTC",
    "updated_date": "2024-06-10 22:10:05 UTC"
  },
  {
    "arxiv_id": "2406.07835v3",
    "title": "SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature",
    "authors": [
      "David Wadden",
      "Kejian Shi",
      "Jacob Morrison",
      "Aakanksha Naik",
      "Shruti Singh",
      "Nitzan Barzilay",
      "Kyle Lo",
      "Tom Hope",
      "Luca Soldaini",
      "Shannon Zejiang Shen",
      "Doug Downey",
      "Hannaneh Hajishirzi",
      "Arman Cohan"
    ],
    "abstract": "We present SciRIFF (Scientific Resource for Instruction-Following and\nFinetuning), a dataset of 137K instruction-following demonstrations for 54\ntasks covering five essential scientific literature understanding capabilities:\ninformation extraction, summarization, question answering, claim verification,\nand classification. SciRIFF demonstrations are notable for their long input\ncontexts, detailed task specifications, and complex structured outputs. While\ninstruction-following resources are available in specific domains such as\nclinical medicine and chemistry, SciRIFF is the first dataset focused on\nextracting and synthesizing information from research literature across a wide\nrange of scientific fields. To demonstrate the utility of SciRIFF, we develop a\nsample-efficient strategy to adapt a general instruction-following model for\nscience by performing additional finetuning on a mix of general-domain and\nSciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our\nmodel -- called SciTulu -- improves over a strong LLM baseline by 28.1% and\n6.5% at the 7B and 70B scales respectively, while maintaining general\ninstruction-following performance within 2% of the baseline. We are optimistic\nthat SciRIFF will facilitate the development and evaluation of LLMs to help\nresearchers navigate the ever-growing body of scientific literature. We release\nour dataset, model checkpoints, and data processing and evaluation code to\nenable further research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to NeurIPS Datasets and Benchmarks 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.07835v3",
    "published_date": "2024-06-10 21:22:08 UTC",
    "updated_date": "2024-08-20 01:59:44 UTC"
  },
  {
    "arxiv_id": "2406.06796v1",
    "title": "FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors",
    "authors": [
      "Jason Wu",
      "Ziqi Wang",
      "Xiaomin Ouyang",
      "Ho Lyun Jeong",
      "Colin Samplawski",
      "Lance Kaplan",
      "Benjamin Marlin",
      "Mani Srivastava"
    ],
    "abstract": "Localization is a critical technology for various applications ranging from\nnavigation and surveillance to assisted living. Localization systems typically\nfuse information from sensors viewing the scene from different perspectives to\nestimate the target location while also employing multiple modalities for\nenhanced robustness and accuracy. Recently, such systems have employed\nend-to-end deep neural models trained on large datasets due to their superior\nperformance and ability to handle data from diverse sensor modalities. However,\nsuch neural models are often trained on data collected from a particular set of\nsensor poses (i.e., locations and orientations). During real-world deployments,\nslight deviations from these sensor poses can result in extreme inaccuracies.\nTo address this challenge, we introduce FlexLoc, which employs conditional\nneural networks to inject node perspective information to adapt the\nlocalization pipeline. Specifically, a small subset of model weights are\nderived from node poses at run time, enabling accurate generalization to unseen\nperspectives with minimal additional overhead. Our evaluations on a multimodal,\nmultiview indoor tracking dataset showcase that FlexLoc improves the\nlocalization accuracy by almost 50% in the zero-shot case (no calibration data\navailable) compared to the baselines. The source code of FlexLoc is available\nat https://github.com/nesl/FlexLoc.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06796v1",
    "published_date": "2024-06-10 21:02:53 UTC",
    "updated_date": "2024-06-10 21:02:53 UTC"
  },
  {
    "arxiv_id": "2406.06793v1",
    "title": "PlanDQ: Hierarchical Plan Orchestration via D-Conductor and Q-Performer",
    "authors": [
      "Chang Chen",
      "Junyeob Baek",
      "Fei Deng",
      "Kenji Kawaguchi",
      "Caglar Gulcehre",
      "Sungjin Ahn"
    ],
    "abstract": "Despite the recent advancements in offline RL, no unified algorithm could\nachieve superior performance across a broad range of tasks. Offline\n\\textit{value function learning}, in particular, struggles with sparse-reward,\nlong-horizon tasks due to the difficulty of solving credit assignment and\nextrapolation errors that accumulates as the horizon of the task grows.~On the\nother hand, models that can perform well in long-horizon tasks are designed\nspecifically for goal-conditioned tasks, which commonly perform worse than\nvalue function learning methods on short-horizon, dense-reward scenarios. To\nbridge this gap, we propose a hierarchical planner designed for offline RL\ncalled PlanDQ. PlanDQ incorporates a diffusion-based planner at the high level,\nnamed D-Conductor, which guides the low-level policy through sub-goals. At the\nlow level, we used a Q-learning based approach called the Q-Performer to\naccomplish these sub-goals. Our experimental results suggest that PlanDQ can\nachieve superior or competitive performance on D4RL continuous control\nbenchmark tasks as well as AntMaze, Kitchen, and Calvin as long-horizon tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06793v1",
    "published_date": "2024-06-10 20:59:53 UTC",
    "updated_date": "2024-06-10 20:59:53 UTC"
  },
  {
    "arxiv_id": "2406.06792v2",
    "title": "Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness",
    "authors": [
      "Dingrong Wang",
      "Hitesh Sapkota",
      "Zhiqiang Tao",
      "Qi Yu"
    ],
    "abstract": "Prior neural architecture search (NAS) for adversarial robustness works have\ndiscovered that a lightweight and adversarially robust neural network\narchitecture could exist in a non-robust large teacher network, generally\ndisclosed by heuristic rules through statistical analysis and neural\narchitecture search, generally disclosed by heuristic rules from neural\narchitecture search. However, heuristic methods cannot uniformly handle\ndifferent adversarial attacks and \"teacher\" network capacity. To solve this\nchallenge, we propose a Reinforced Compressive Neural Architecture Search\n(RC-NAS) for Versatile Adversarial Robustness. Specifically, we define task\nsettings that compose datasets, adversarial attacks, and teacher network\ninformation. Given diverse tasks, we conduct a novel dual-level training\nparadigm that consists of a meta-training and a fine-tuning phase to\neffectively expose the RL agent to diverse attack scenarios (in meta-training),\nand making it adapt quickly to locate a sub-network (in fine-tuning) for any\npreviously unseen scenarios. Experiments show that our framework could achieve\nadaptive compression towards different initial teacher networks, datasets, and\nadversarial attacks, resulting in more lightweight and adversarially robust\narchitectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.06792v2",
    "published_date": "2024-06-10 20:59:52 UTC",
    "updated_date": "2024-06-14 03:59:05 UTC"
  },
  {
    "arxiv_id": "2406.06786v2",
    "title": "BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification",
    "authors": [
      "June-Woo Kim",
      "Miika Toikkanen",
      "Yera Choi",
      "Seoung-Eun Moon",
      "Ho-Young Jung"
    ],
    "abstract": "Respiratory sound classification (RSC) is challenging due to varied acoustic\nsignatures, primarily influenced by patient demographics and recording\nenvironments. To address this issue, we introduce a text-audio multimodal model\nthat utilizes metadata of respiratory sounds, which provides useful\ncomplementary information for RSC. Specifically, we fine-tune a pretrained\ntext-audio multimodal model using free-text descriptions derived from the sound\nsamples' metadata which includes the gender and age of patients, type of\nrecording devices, and recording location on the patient's body. Our method\nachieves state-of-the-art performance on the ICBHI dataset, surpassing the\nprevious best result by a notable margin of 1.17%. This result validates the\neffectiveness of leveraging metadata and respiratory sound samples in enhancing\nRSC performance. Additionally, we investigate the model performance in the case\nwhere metadata is partially unavailable, which may occur in real-world clinical\nsetting.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06786v2",
    "published_date": "2024-06-10 20:49:54 UTC",
    "updated_date": "2024-06-14 12:57:53 UTC"
  },
  {
    "arxiv_id": "2406.06777v5",
    "title": "MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension",
    "authors": [
      "Khiem Le",
      "Zhichun Guo",
      "Kaiwen Dong",
      "Xiaobao Huang",
      "Bozhao Nan",
      "Roshni Iyer",
      "Xiangliang Zhang",
      "Olaf Wiest",
      "Wei Wang",
      "Nitesh V. Chawla"
    ],
    "abstract": "Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving professional molecule-related\ntasks. This challenge is attributed to their inherent limitations in\ncomprehending molecules using only common textual representations, i.e., SMILES\nstrings. In this study, we seek to enhance the ability of LLMs to comprehend\nmolecules by equipping them with a multi-modal external module, namely MolX. In\nparticular, instead of directly using a SMILES string to represent a molecule,\nwe utilize specific encoders to extract fine-grained features from both SMILES\nstring and 2D molecular graph representations for feeding into an LLM.\nMoreover, a handcrafted molecular fingerprint is incorporated to leverage its\nembedded domain knowledge. Then, to establish an alignment between MolX and the\nLLM's textual input space, the whole model in which the LLM is frozen, is\npre-trained with a versatile strategy including a diverse set of tasks.\nExperimental evaluations show that our proposed method outperforms baselines\nacross 4 downstream molecule-related tasks ranging from molecule-to-text\ntranslation to retrosynthesis, with and without fine-tuning the LLM, while only\nintroducing a small number of trainable parameters 0.53% and 0.82%,\nrespectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06777v5",
    "published_date": "2024-06-10 20:25:18 UTC",
    "updated_date": "2025-04-02 22:20:34 UTC"
  },
  {
    "arxiv_id": "2406.06773v2",
    "title": "Evaluating Zero-Shot Long-Context LLM Compression",
    "authors": [
      "Chenyu Wang",
      "Yihan Wang",
      "Kai Li"
    ],
    "abstract": "This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06773v2",
    "published_date": "2024-06-10 20:19:55 UTC",
    "updated_date": "2025-02-13 17:50:39 UTC"
  },
  {
    "arxiv_id": "2406.06769v2",
    "title": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
    "authors": [
      "Peter Jansen",
      "Marc-Alexandre Côté",
      "Tushar Khot",
      "Erin Bransom",
      "Bhavana Dalvi Mishra",
      "Bodhisattwa Prasad Majumder",
      "Oyvind Tafjord",
      "Peter Clark"
    ],
    "abstract": "Automated scientific discovery promises to accelerate progress across\nscientific domains. However, developing and evaluating an AI agent's capacity\nfor end-to-end scientific reasoning is challenging as running real-world\nexperiments is often prohibitively expensive or infeasible. In this work we\nintroduce DISCOVERYWORLD, the first virtual environment for developing and\nbenchmarking an agent's ability to perform complete cycles of novel scientific\ndiscovery. DISCOVERYWORLD contains a variety of different challenges, covering\ntopics as diverse as radioisotope dating, rocket science, and proteomics, to\nencourage development of general discovery skills rather than task-specific\nsolutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based\nenvironment (with optional 2D visual overlay). It includes 120 different\nchallenge tasks, spanning eight topics each with three levels of difficulty and\nseveral parametric variations. Each task requires an agent to form hypotheses,\ndesign and run experiments, analyze results, and act on conclusions.\nDISCOVERYWORLD further provides three automatic metrics for evaluating\nperformance, based on (a) task completion, (b) task-relevant actions taken, and\n(c) the discovered explanatory knowledge. We find that strong baseline agents,\nthat perform well in prior published environments, struggle on most\nDISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel\nchallenges of discovery, and thus that DISCOVERYWORLD may help accelerate\nnear-term development and assessment of scientific discovery competency in\nagents. Code available at: www.github.com/allenai/discoveryworld",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2024 (Benchmark Track, Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2406.06769v2",
    "published_date": "2024-06-10 20:08:44 UTC",
    "updated_date": "2024-10-07 20:19:15 UTC"
  },
  {
    "arxiv_id": "2406.06751v1",
    "title": "Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients",
    "authors": [
      "Zachary Bastiani",
      "Robert M. Kirby",
      "Jacob Hochhalter",
      "Shandian Zhe"
    ],
    "abstract": "This paper proposes a novel deep symbolic regression approach to enhance the\nrobustness and interpretability of data-driven mathematical expression\ndiscovery. Despite the success of the state-of-the-art method, DSR, it is built\non recurrent neural networks, purely guided by data fitness, and potentially\nmeet tail barriers, which can zero out the policy gradient and cause\ninefficient model updates. To overcome these limitations, we use transformers\nin conjunction with breadth-first-search to improve the learning performance.\nWe use Bayesian information criterion (BIC) as the reward function to\nexplicitly account for the expression complexity and optimize the trade-off\nbetween interpretability and data fitness. We propose a modified risk-seeking\npolicy that not only ensures the unbiasness of the gradient, but also removes\nthe tail barriers, thus ensuring effective updates from top performers. Through\na series of benchmarks and systematic experiments, we demonstrate the\nadvantages of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06751v1",
    "published_date": "2024-06-10 19:29:10 UTC",
    "updated_date": "2024-06-10 19:29:10 UTC"
  },
  {
    "arxiv_id": "2406.06742v1",
    "title": "An Elliptic Kernel Unsupervised Autoencoder-Graph Convolutional Network Ensemble Model for Hyperspectral Unmixing",
    "authors": [
      "Estefania Alfaro-Mejia",
      "Carlos J Delgado",
      "Vidya Manian"
    ],
    "abstract": "Spectral Unmixing is an important technique in remote sensing used to analyze\nhyperspectral images to identify endmembers and estimate abundance maps. Over\nthe past few decades, performance of techniques for endmember extraction and\nfractional abundance map estimation have significantly improved. This article\npresents an ensemble model workflow called Autoencoder Graph Ensemble Model\n(AEGEM) designed to extract endmembers and fractional abundance maps. An\nelliptical kernel is applied to measure spectral distances, generating the\nadjacency matrix within the elliptical neighborhood. This information is used\nto construct an elliptical graph, with centroids as senders and remaining\npixels within the geometry as receivers. The next step involves stacking\nabundance maps, senders, and receivers as inputs to a Graph Convolutional\nNetwork, which processes this input to refine abundance maps. Finally, an\nensemble decision-making process determines the best abundance maps based on\nroot mean square error metric. The proposed AEGEM is assessed with benchmark\ndatasets such as Samson, Jasper, and Urban, outperforming results obtained by\nbaseline algorithms. For the Samson dataset, AEGEM excels in three abundance\nmaps: water, tree and soil yielding values of 0.081, 0.158, and 0.182,\nrespectively. For the Jasper dataset, results are improved for the tree and\nwater endmembers with values of 0.035 and 0.060 in that order, as well as for\nthe mean average of the spectral angle distance metric 0.109. For the Urban\ndataset, AEGEM outperforms previous results for the abundance maps of roof and\nasphalt, achieving values of 0.135 and 0.240, respectively. Additionally, for\nthe endmembers of grass and roof, AEGEM achieves values of 0.063 and 0.094.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 13 figures, Transaction in Geoscience",
    "pdf_url": "http://arxiv.org/pdf/2406.06742v1",
    "published_date": "2024-06-10 19:04:39 UTC",
    "updated_date": "2024-06-10 19:04:39 UTC"
  },
  {
    "arxiv_id": "2406.06736v2",
    "title": "Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges",
    "authors": [
      "Usman Gohar",
      "Zeyu Tang",
      "Jialu Wang",
      "Kun Zhang",
      "Peter L. Spirtes",
      "Yang Liu",
      "Lu Cheng"
    ],
    "abstract": "The widespread integration of Machine Learning systems in daily life,\nparticularly in high-stakes domains, has raised concerns about the fairness\nimplications. While prior works have investigated static fairness measures,\nrecent studies reveal that automated decision-making has long-term implications\nand that off-the-shelf fairness approaches may not serve the purpose of\nachieving long-term fairness. Additionally, the existence of feedback loops and\nthe interaction between models and the environment introduces additional\ncomplexities that may deviate from the initial fairness goals. In this survey,\nwe review existing literature on long-term fairness from different perspectives\nand present a taxonomy for long-term fairness studies. We highlight key\nchallenges and consider future research directions, analyzing both current\nissues and potential further explorations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06736v2",
    "published_date": "2024-06-10 18:57:06 UTC",
    "updated_date": "2025-02-12 23:14:07 UTC"
  },
  {
    "arxiv_id": "2406.06730v1",
    "title": "TRINS: Towards Multimodal Language Models that Can Read",
    "authors": [
      "Ruiyi Zhang",
      "Yanzhe Zhang",
      "Jian Chen",
      "Yufan Zhou",
      "Jiuxiang Gu",
      "Changyou Chen",
      "Tong Sun"
    ],
    "abstract": "Large multimodal language models have shown remarkable proficiency in\nunderstanding and editing images. However, a majority of these visually-tuned\nmodels struggle to comprehend the textual content embedded in images, primarily\ndue to the limitation of training data. In this work, we introduce TRINS: a\nText-Rich image INStruction dataset, with the objective of enhancing the\nreading ability of the multimodal large language model. TRINS is built upon\nLAION using hybrid data annotation strategies that include machine-assisted and\nhuman-assisted annotation processes. It contains 39,153 text-rich images,\ncaptions, and 102,437 questions. Specifically, we show that the number of words\nper annotation in TRINS is significantly longer than that of related datasets,\nproviding new challenges. Furthermore, we introduce a simple and effective\narchitecture, called a Language-vision Reading Assistant (LaRA), which is good\nat understanding textual content within images. LaRA outperforms existing\nstate-of-the-art multimodal large language models on the TRINS dataset, as well\nas other classical benchmarks. Lastly, we conducted a comprehensive evaluation\nwith TRINS on various text-rich image understanding and generation tasks,\ndemonstrating its effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06730v1",
    "published_date": "2024-06-10 18:52:37 UTC",
    "updated_date": "2024-06-10 18:52:37 UTC"
  },
  {
    "arxiv_id": "2406.06729v1",
    "title": "Synthetic Query Generation using Large Language Models for Virtual Assistants",
    "authors": [
      "Sonal Sannigrahi",
      "Thiago Fraga-Silva",
      "Youssef Oualil",
      "Christophe Van Gysel"
    ],
    "abstract": "Virtual Assistants (VAs) are important Information Retrieval platforms that\nhelp users accomplish various tasks through spoken commands. The speech\nrecognition system (speech-to-text) uses query priors, trained solely on text,\nto distinguish between phonetically confusing alternatives. Hence, the\ngeneration of synthetic queries that are similar to existing VA usage can\ngreatly improve upon the VA's abilities -- especially for use-cases that do not\n(yet) occur in paired audio/text data.\n  In this paper, we provide a preliminary exploration of the use of Large\nLanguage Models (LLMs) to generate synthetic queries that are complementary to\ntemplate-based methods. We investigate whether the methods (a) generate queries\nthat are similar to randomly sampled, representative, and anonymized user\nqueries from a popular VA, and (b) whether the generated queries are specific.\n  We find that LLMs generate more verbose queries, compared to template-based\nmethods, and reference aspects specific to the entity. The generated queries\nare similar to VA user queries, and are specific enough to retrieve the\nrelevant entity. We conclude that queries generated by LLMs and templates are\ncomplementary.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR '24. The 47th International ACM SIGIR Conference on Research &\n  Development in Information Retrieval",
    "pdf_url": "http://arxiv.org/pdf/2406.06729v1",
    "published_date": "2024-06-10 18:50:57 UTC",
    "updated_date": "2024-06-10 18:50:57 UTC"
  },
  {
    "arxiv_id": "2406.06728v2",
    "title": "AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI",
    "authors": [
      "K M Tawsik Jawad",
      "Anusha Verma",
      "Fathi Amsaad",
      "Lamia Ashraf"
    ],
    "abstract": "Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with\nno known ultimo cure and high morbidity. Research demonstrates that progressive\nChronic Kidney Disease (CKD) is a heterogeneous disorder that significantly\nimpacts kidney structure and functions, eventually leading to kidney failure.\nWith the progression of time, chronic kidney disease has moved from a\nlife-threatening disease affecting few people to a common disorder of varying\nseverity. The goal of this research is to visualize dominating features,\nfeature scores, and values exhibited for early prognosis and detection of CKD\nusing ensemble learning and explainable AI. For that, an AI-driven predictive\nanalytics approach is proposed to aid clinical practitioners in prescribing\nlifestyle modifications for individual patients to reduce the rate of\nprogression of this disease. Our dataset is collected on body vitals from\nindividuals with CKD and healthy subjects to develop our proposed AI-driven\nsolution accurately. In this regard, blood and urine test results are provided,\nand ensemble tree-based machine-learning models are applied to predict unseen\ncases of CKD. Our research findings are validated after lengthy consultations\nwith nephrologists. Our experiments and interpretation results are compared\nwith existing explainable AI applications in various healthcare domains,\nincluding CKD. The comparison shows that our developed AI models, particularly\nthe Random Forest model, have identified more features as significant\ncontributors than XgBoost. Interpretability (I), which measures the ratio of\nimportant to masked features, indicates that our XgBoost model achieved a\nhigher score, specifically a Fidelity of 98\\%, in this metric and naturally in\nthe FII index compared to competing models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06728v2",
    "published_date": "2024-06-10 18:46:14 UTC",
    "updated_date": "2025-01-24 20:59:46 UTC"
  },
  {
    "arxiv_id": "2406.06714v2",
    "title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation",
    "authors": [
      "Michelle Pan",
      "Mariah Schrum",
      "Vivek Myers",
      "Erdem Bıyık",
      "Anca Dragan"
    ],
    "abstract": "Adaptive brain stimulation can treat neurological conditions such as\nParkinson's disease and post-stroke motor deficits by influencing abnormal\nneural activity. Because of patient heterogeneity, each patient requires a\nunique stimulation policy to achieve optimal neural responses. Model-free\nreinforcement learning (MFRL) holds promise in learning effective policies for\na variety of similar control tasks, but is limited in domains like brain\nstimulation by a need for numerous costly environment interactions. In this\nwork we introduce Coprocessor Actor Critic, a novel, model-based reinforcement\nlearning (MBRL) approach for learning neural coprocessor policies for brain\nstimulation. Our key insight is that coprocessor policy learning is a\ncombination of learning how to act optimally in the world and learning how to\ninduce optimal actions in the world through stimulation of an injured brain. We\nshow that our approach overcomes the limitations of traditional MFRL methods in\nterms of sample efficiency and task success and outperforms baseline MBRL\napproaches in a neurologically realistic model of an injured brain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 41st International Conference on Machine Learning\n  (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.06714v2",
    "published_date": "2024-06-10 18:23:03 UTC",
    "updated_date": "2024-10-07 21:07:33 UTC"
  },
  {
    "arxiv_id": "2406.06700v1",
    "title": "Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics",
    "authors": [
      "Ankit Vani",
      "Frederick Tung",
      "Gabriel L. Oliveira",
      "Hossein Sharifi-Noghabi"
    ],
    "abstract": "Despite attaining high empirical generalization, the sharpness of models\ntrained with sharpness-aware minimization (SAM) do not always correlate with\ngeneralization error. Instead of viewing SAM as minimizing sharpness to improve\ngeneralization, our paper considers a new perspective based on SAM's training\ndynamics. We propose that perturbations in SAM perform perturbed forgetting,\nwhere they discard undesirable model biases to exhibit learning signals that\ngeneralize better. We relate our notion of forgetting to the information\nbottleneck principle, use it to explain observations like the better\ngeneralization of smaller perturbation batches, and show that perturbed\nforgetting can exhibit a stronger correlation with generalization than\nflatness. While standard SAM targets model biases exposed by the steepest\nascent directions, we propose a new perturbation that targets biases exposed\nthrough the model's outputs. Our output bias forgetting perturbations\noutperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and\ntransfer to CIFAR-{10,100}, while sometimes converging to sharper regions. Our\nresults suggest that the benefits of SAM can be explained by alternative\nmechanistic principles that do not require flatness of the loss surface.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICML 2024. 9 pages main, 15 pages\n  total including references and appendix",
    "pdf_url": "http://arxiv.org/pdf/2406.06700v1",
    "published_date": "2024-06-10 18:02:48 UTC",
    "updated_date": "2024-06-10 18:02:48 UTC"
  },
  {
    "arxiv_id": "2406.06527v2",
    "title": "IllumiNeRF: 3D Relighting Without Inverse Rendering",
    "authors": [
      "Xiaoming Zhao",
      "Pratul P. Srinivasan",
      "Dor Verbin",
      "Keunhong Park",
      "Ricardo Martin Brualla",
      "Philipp Henzler"
    ],
    "abstract": "Existing methods for relightable view synthesis -- using a set of images of\nan object under unknown lighting to recover a 3D representation that can be\nrendered from novel viewpoints under a target illumination -- are based on\ninverse rendering, and attempt to disentangle the object geometry, materials,\nand lighting that explain the input images. Furthermore, this typically\ninvolves optimization through differentiable Monte Carlo rendering, which is\nbrittle and computationally-expensive. In this work, we propose a simpler\napproach: we first relight each input image using an image diffusion model\nconditioned on target environment lighting and estimated object geometry. We\nthen reconstruct a Neural Radiance Field (NeRF) with these relit images, from\nwhich we render novel views under the target lighting. We demonstrate that this\nstrategy is surprisingly competitive and achieves state-of-the-art results on\nmultiple relighting benchmarks. Please see our project page at\nhttps://illuminerf.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024; v2 (for camera-ready) added single-GPU results and\n  discussions on Stanford-ORB illuminations; Project page:\n  https://illuminerf.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.06527v2",
    "published_date": "2024-06-10 17:59:59 UTC",
    "updated_date": "2024-11-01 20:52:50 UTC"
  },
  {
    "arxiv_id": "2406.06520v1",
    "title": "Decentralized Personalized Federated Learning",
    "authors": [
      "Salma Kharrat",
      "Marco Canini",
      "Samuel Horvath"
    ],
    "abstract": "This work tackles the challenges of data heterogeneity and communication\nlimitations in decentralized federated learning. We focus on creating a\ncollaboration graph that guides each client in selecting suitable collaborators\nfor training personalized models that leverage their local data effectively.\nOur approach addresses these issues through a novel, communication-efficient\nstrategy that enhances resource efficiency. Unlike traditional methods, our\nformulation identifies collaborators at a granular level by considering\ncombinatorial relations of clients, enhancing personalization while minimizing\ncommunication overhead. We achieve this through a bi-level optimization\nframework that employs a constrained greedy algorithm, resulting in a\nresource-efficient collaboration graph for personalized learning. Extensive\nevaluation against various baselines across diverse datasets demonstrates the\nsuperiority of our method, named DPFL. DPFL consistently outperforms other\napproaches, showcasing its effectiveness in handling real-world data\nheterogeneity, minimizing communication overhead, enhancing resource\nefficiency, and building personalized models in decentralized federated\nlearning scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MA",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06520v1",
    "published_date": "2024-06-10 17:58:48 UTC",
    "updated_date": "2024-06-10 17:58:48 UTC"
  },
  {
    "arxiv_id": "2406.06512v1",
    "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
    "authors": [
      "Louis Blankemeier",
      "Joseph Paul Cohen",
      "Ashwin Kumar",
      "Dave Van Veen",
      "Syed Jamal Safdar Gardezi",
      "Magdalini Paschali",
      "Zhihong Chen",
      "Jean-Benoit Delbrouck",
      "Eduardo Reis",
      "Cesar Truyts",
      "Christian Bluethgen",
      "Malte Engmann Kjeldskov Jensen",
      "Sophie Ostmeier",
      "Maya Varma",
      "Jeya Maria Jose Valanarasu",
      "Zhongnan Fang",
      "Zepeng Huo",
      "Zaid Nabulsi",
      "Diego Ardila",
      "Wei-Hung Weng",
      "Edson Amaro Junior",
      "Neera Ahuja",
      "Jason Fries",
      "Nigam H. Shah",
      "Andrew Johnston",
      "Robert D. Boutin",
      "Andrew Wentland",
      "Curtis P. Langlotz",
      "Jason Hom",
      "Sergios Gatidis",
      "Akshay S. Chaudhari"
    ],
    "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the\nUS, of which approximately one quarter focus on the abdomen. Given the current\nradiologist shortage, there is a large impetus to use artificial intelligence\nto alleviate the burden of interpreting these complex imaging studies. Prior\nstate-of-the-art approaches for automated medical image interpretation leverage\nvision language models (VLMs). However, current medical VLMs are generally\nlimited to 2D images and short reports, and do not leverage electronic health\nrecord (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train\nusing paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes\n(1.8+ million codes), and radiology reports (6+ million tokens). We evaluate\nMerlin on 6 task types and 752 individual tasks. The non-adapted\n(off-the-shelf) tasks include zero-shot findings classification (31 findings),\nphenotype classification (692 phenotypes), and zero-shot cross-modal retrieval\n(image to findings and image to impressions), while model adapted tasks include\n5-year disease prediction (6 diseases), radiology report generation, and 3D\nsemantic segmentation (20 organs). We perform internal validation on a test set\nof 5,137 CTs, and external validation on 7,000 clinical CTs and on two public\nCT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant\nevaluations, we assess the efficacy of various network architectures and\ntraining strategies to depict that Merlin has favorable performance to existing\ntask-specific baselines. We derive data scaling laws to empirically assess\ntraining data needs for requisite downstream task performance. Furthermore,\nunlike conventional VLMs that require hundreds of GPUs for training, we perform\nall training on a single GPU.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06512v1",
    "published_date": "2024-06-10 17:53:01 UTC",
    "updated_date": "2024-06-10 17:53:01 UTC"
  },
  {
    "arxiv_id": "2406.06508v1",
    "title": "Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer",
    "authors": [
      "Sigal Raab",
      "Inbar Gat",
      "Nathan Sala",
      "Guy Tevet",
      "Rotem Shalev-Arkushin",
      "Ohad Fried",
      "Amit H. Bermano",
      "Daniel Cohen-Or"
    ],
    "abstract": "Given the remarkable results of motion synthesis with diffusion models, a\nnatural question arises: how can we effectively leverage these models for\nmotion editing? Existing diffusion-based motion editing methods overlook the\nprofound potential of the prior embedded within the weights of pre-trained\nmodels, which enables manipulating the latent feature space; hence, they\nprimarily center on handling the motion space. In this work, we explore the\nattention mechanism of pre-trained motion diffusion models. We uncover the\nroles and interactions of attention elements in capturing and representing\nintricate human motion patterns, and carefully integrate these elements to\ntransfer a leader motion to a follower one while maintaining the nuanced\ncharacteristics of the follower, resulting in zero-shot motion transfer.\nEditing features associated with selected motions allows us to confront a\nchallenge observed in prior motion diffusion approaches, which use general\ndirectives (e.g., text, music) for editing, ultimately failing to convey subtle\nnuances effectively. Our work is inspired by how a monkey closely imitates what\nit sees while maintaining its unique motion patterns; hence we call it Monkey\nSee, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing\ntasks such as synthesizing out-of-distribution motions, style transfer, and\nspatial editing. Furthermore, diffusion inversion is seldom employed for\nmotions; as a result, editing efforts focus on generated motions, limiting the\neditability of real ones. MoMo harnesses motion inversion, extending its\napplication to both real and generated motions. Experimental results show the\nadvantage of our approach over the current art. In particular, unlike methods\ntailored for specific applications through training, our approach is applied at\ninference time, requiring no training. Our webpage is at\nhttps://monkeyseedocg.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Video: https://www.youtube.com/watch?v=s5oo3sKV0YU, Project page:\n  https://monkeyseedocg.github.io, Code:\n  https://github.com/MonkeySeeDoCG/MoMo-code",
    "pdf_url": "http://arxiv.org/pdf/2406.06508v1",
    "published_date": "2024-06-10 17:47:14 UTC",
    "updated_date": "2024-06-10 17:47:14 UTC"
  },
  {
    "arxiv_id": "2406.06500v1",
    "title": "Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation",
    "authors": [
      "Mohidul Haque Mridul",
      "Mohammad Foysal Khan",
      "Redwan Ahmed Rizvee",
      "Md Mosaddek Khan"
    ],
    "abstract": "In Multi-agent Reinforcement Learning (MARL), accurately perceiving\nopponents' strategies is essential for both cooperative and adversarial\ncontexts, particularly within dynamic environments. While Proximal Policy\nOptimization (PPO) and related algorithms such as Actor-Critic with Experience\nReplay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic\nPolicy Gradient (DDPG) perform well in single-agent, stationary environments,\nthey suffer from high variance in MARL due to non-stationary and hidden\npolicies of opponents, leading to diminished reward performance. Additionally,\nexisting methods in MARL face significant challenges, including the need for\ninter-agent communication, reliance on explicit reward information, high\ncomputational demands, and sampling inefficiencies. These issues render them\nless effective in continuous environments where opponents may abruptly change\ntheir policies without prior notice. Against this background, we present\nOPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that\nemploys dynamic error decay to detect changes in opponents' policies. OPS-DeMo\ncontinuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank\nand selects corresponding responses from a pre-trained Response Policy Bank.\nEach response policy is trained against consistently strategizing opponents,\nreducing training uncertainty and enabling the effective use of algorithms like\nPPO in multi-agent environments. Comparative assessments show that our approach\noutperforms PPO-trained models in dynamic scenarios like the Predator-Prey\nsetting, providing greater robustness to sudden policy shifts and enabling more\ninformed decision-making through precise opponent policy insights.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06500v1",
    "published_date": "2024-06-10 17:34:44 UTC",
    "updated_date": "2024-06-10 17:34:44 UTC"
  },
  {
    "arxiv_id": "2406.06494v2",
    "title": "Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits",
    "authors": [
      "Gennaro Gala",
      "Cassio de Campos",
      "Antonio Vergari",
      "Erik Quaeghebeur"
    ],
    "abstract": "Probabilistic integral circuits (PICs) have been recently introduced as\nprobabilistic models enjoying the key ingredient behind expressive generative\nmodels: continuous latent variables (LVs). PICs are symbolic computational\ngraphs defining continuous LV models as hierarchies of functions that are\nsummed and multiplied together, or integrated over some LVs. They are tractable\nif LVs can be analytically integrated out, otherwise they can be approximated\nby tractable probabilistic circuits (PC) encoding a hierarchical numerical\nquadrature process, called QPCs.\n  So far, only tree-shaped PICs have been explored, and training them via\nnumerical quadrature requires memory-intensive processing at scale. In this\npaper, we address these issues, and present: (i) a pipeline for building\nDAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for\ntraining PICs using tensorized circuit architectures, and (iii) neural\nfunctional sharing techniques to allow scalable training. In extensive\nexperiments, we showcase the effectiveness of functional sharing and the\nsuperiority of QPCs over traditional PCs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06494v2",
    "published_date": "2024-06-10 17:30:17 UTC",
    "updated_date": "2025-02-05 13:13:26 UTC"
  },
  {
    "arxiv_id": "2406.06485v1",
    "title": "Can Language Models Serve as Text-Based World Simulators?",
    "authors": [
      "Ruoyao Wang",
      "Graham Todd",
      "Ziang Xiao",
      "Xingdi Yuan",
      "Marc-Alexandre Côté",
      "Peter Clark",
      "Peter Jansen"
    ],
    "abstract": "Virtual environments play a key role in benchmarking advances in complex\nplanning and decision-making tasks but are expensive and complicated to build\nby hand. Can current language models themselves serve as world simulators,\ncorrectly predicting how actions change different world states, thus bypassing\nthe need for extensive manual coding? Our goal is to answer this question in\nthe context of text-based simulators. Our approach is to build and use a new\nbenchmark, called ByteSized32-State-Prediction, containing a dataset of text\ngame state transitions and accompanying game tasks. We use this to directly\nquantify, for the first time, how well LLMs can serve as text-based world\nsimulators. We test GPT-4 on this dataset and find that, despite its impressive\nperformance, it is still an unreliable world simulator without further\ninnovations. This work thus contributes both new insights into current LLM's\ncapabilities and weaknesses, as well as a novel benchmark to track future\nprogress as new models appear.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06485v1",
    "published_date": "2024-06-10 17:24:44 UTC",
    "updated_date": "2024-06-10 17:24:44 UTC"
  },
  {
    "arxiv_id": "2406.06475v1",
    "title": "Survey for Landing Generative AI in Social and E-commerce Recsys -- the Industry Perspectives",
    "authors": [
      "Da Xu",
      "Danqing Zhang",
      "Guangyu Yang",
      "Bo Yang",
      "Shuyuan Xu",
      "Lingling Zheng",
      "Cindy Liang"
    ],
    "abstract": "Recently, generative AI (GAI), with their emerging capabilities, have\npresented unique opportunities for augmenting and revolutionizing industrial\nrecommender systems (Recsys). Despite growing research efforts at the\nintersection of these fields, the integration of GAI into industrial Recsys\nremains in its infancy, largely due to the intricate nature of modern\nindustrial Recsys infrastructure, operations, and product sophistication.\nDrawing upon our experiences in successfully integrating GAI into several major\nsocial and e-commerce platforms, this survey aims to comprehensively examine\nthe underlying system and AI foundations, solution frameworks, connections to\nkey research advancements, as well as summarize the practical insights and\nchallenges encountered in the endeavor to integrate GAI into industrial Recsys.\nAs pioneering work in this domain, we hope outline the representative\ndevelopments of relevant fields, shed lights on practical GAI adoptions in the\nindustry, and motivate future research.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06475v1",
    "published_date": "2024-06-10 17:16:59 UTC",
    "updated_date": "2024-06-10 17:16:59 UTC"
  },
  {
    "arxiv_id": "2406.06474v1",
    "title": "Towards a Personal Health Large Language Model",
    "authors": [
      "Justin Cosentino",
      "Anastasiya Belyaeva",
      "Xin Liu",
      "Nicholas A. Furlotte",
      "Zhun Yang",
      "Chace Lee",
      "Erik Schenck",
      "Yojan Patel",
      "Jian Cui",
      "Logan Douglas Schneider",
      "Robby Bryant",
      "Ryan G. Gomes",
      "Allen Jiang",
      "Roy Lee",
      "Yun Liu",
      "Javier Perez",
      "Jameson K. Rogers",
      "Cathy Speed",
      "Shyam Tailor",
      "Megan Walker",
      "Jeffrey Yu",
      "Tim Althoff",
      "Conor Heneghan",
      "John Hernandez",
      "Mark Malhotra",
      "Leor Stern",
      "Yossi Matias",
      "Greg S. Corrado",
      "Shwetak Patel",
      "Shravya Shetty",
      "Jiening Zhan",
      "Shruthi Prabhakara",
      "Daniel McDuff",
      "Cory Y. McLean"
    ],
    "abstract": "In health, most large language model (LLM) research has focused on clinical\ntasks. However, mobile and wearable devices, which are rarely integrated into\nsuch tasks, provide rich, longitudinal data for personal health monitoring.\nHere we present Personal Health Large Language Model (PH-LLM), fine-tuned from\nGemini for understanding and reasoning over numerical time-series personal\nhealth data. We created and curated three datasets that test 1) production of\npersonalized insights and recommendations from sleep patterns, physical\nactivity, and physiological responses, 2) expert domain knowledge, and 3)\nprediction of self-reported sleep outcomes. For the first task we designed 857\ncase studies in collaboration with domain experts to assess real-world\nscenarios in sleep and fitness. Through comprehensive evaluation of\ndomain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not\nstatistically different from expert performance in fitness and, while experts\nremain superior for sleep, fine-tuning PH-LLM provided significant improvements\nin using relevant domain knowledge and personalizing information for sleep\ninsights. We evaluated PH-LLM domain knowledge using multiple choice sleep\nmedicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on\nfitness, exceeding average scores from a sample of human experts. Finally, we\ntrained PH-LLM to predict self-reported sleep quality outcomes from textual and\nmultimodal encoding representations of wearable data, and demonstrate that\nmultimodal encoding is required to match performance of specialized\ndiscriminative models. Although further development and evaluation are\nnecessary in the safety-critical personal health domain, these results\ndemonstrate both the broad knowledge and capabilities of Gemini models and the\nbenefit of contextualizing physiological data for personal health applications\nas done with PH-LLM.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "72 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.06474v1",
    "published_date": "2024-06-10 17:16:49 UTC",
    "updated_date": "2024-06-10 17:16:49 UTC"
  },
  {
    "arxiv_id": "2406.06470v1",
    "title": "GKAN: Graph Kolmogorov-Arnold Networks",
    "authors": [
      "Mehrdad Kiamari",
      "Mohammad Kiamari",
      "Bhaskar Krishnamachari"
    ],
    "abstract": "We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural\nnetwork architecture that extends the principles of the recently proposed\nKolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the\nunique characteristics of KANs, notably the use of learnable univariate\nfunctions instead of fixed linear weights, we develop a powerful model for\ngraph-based learning tasks. Unlike traditional Graph Convolutional Networks\n(GCNs) that rely on a fixed convolutional architecture, GKANs implement\nlearnable spline-based functions between layers, transforming the way\ninformation is processed across the graph structure. We present two different\nways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable\nfunctions are applied to input features after aggregation and architecture 2 --\nwhere the learnable functions are applied to input features before aggregation.\nWe evaluate GKAN empirically using a semi-supervised graph learning task on a\nreal-world dataset (Cora). We find that architecture generally performs better.\nWe find that GKANs achieve higher accuracy in semi-supervised learning tasks on\ngraphs compared to the traditional GCN model. For example, when considering 100\nfeatures, GCN provides an accuracy of 53.5 while a GKAN with a comparable\nnumber of parameters gives an accuracy of 61.76; with 200 features, GCN\nprovides an accuracy of 61.24 while a GKAN with a comparable number of\nparameters gives an accuracy of 67.66. We also present results on the impact of\nvarious parameters such as the number of hidden nodes, grid-size, and the\npolynomial-degree of the spline on the performance of GKAN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06470v1",
    "published_date": "2024-06-10 17:09:38 UTC",
    "updated_date": "2024-06-10 17:09:38 UTC"
  },
  {
    "arxiv_id": "2406.06469v1",
    "title": "Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning",
    "authors": [
      "Joongwon Kim",
      "Bhargavi Paranjape",
      "Tushar Khot",
      "Hannaneh Hajishirzi"
    ],
    "abstract": "Language agents perform complex tasks by using tools to execute each step\nprecisely. However, most existing agents are based on proprietary models or\ndesigned to target specific tasks, such as mathematics or multi-hop question\nanswering. We introduce Husky, a holistic, open-source language agent that\nlearns to reason over a unified action space to address a diverse set of\ncomplex tasks involving numerical, tabular, and knowledge-based reasoning.\nHusky iterates between two stages: 1) generating the next action to take\ntowards solving a given task and 2) executing the action using expert models\nand updating the current solution state. We identify a thorough ontology of\nactions for addressing complex tasks and curate high-quality data to train\nexpert models for executing these actions. Our experiments show that Husky\noutperforms prior language agents across 14 evaluation datasets. Moreover, we\nintroduce HuskyQA, a new evaluation set which stress tests language agents for\nmixed-tool reasoning, with a focus on retrieving missing knowledge and\nperforming numerical reasoning. Despite using 7B models, Husky matches or even\nexceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of\nour holistic approach in addressing complex reasoning problems. Our code and\nmodels are available at https://github.com/agent-husky/Husky-v1.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "50 pages, 42 figures. Project webpage available\n  [here](https://agent-husky.github.io/)",
    "pdf_url": "http://arxiv.org/pdf/2406.06469v1",
    "published_date": "2024-06-10 17:07:25 UTC",
    "updated_date": "2024-06-10 17:07:25 UTC"
  },
  {
    "arxiv_id": "2406.06467v3",
    "title": "How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad",
    "authors": [
      "Emmanuel Abbe",
      "Samy Bengio",
      "Aryo Lotfi",
      "Colin Sandon",
      "Omid Saremi"
    ],
    "abstract": "Can Transformers predict new syllogisms by composing established ones? More\ngenerally, what type of targets can be learned by such models from scratch?\nRecent works show that Transformers can be Turing-complete in terms of\nexpressivity, but this does not address the learnability objective. This paper\nputs forward the notion of 'globality degree' of a target distribution to\ncapture when weak learning is efficiently achievable by regular Transformers.\nThis measure shows a contrast with the expressivity results of Transformers\ncaptured by $TC^0/TC^1$ classes (further studied here), since the globality\nrelates to correlations with the more limited $NC^0$ class. We show here\nexperimentally and theoretically under additional assumptions that\ndistributions with high globality cannot be learned efficiently. In particular,\nsyllogisms cannot be composed on long chains. Further, we develop scratchpad\ntechniques and show that: (i) agnostic scratchpads cannot break the globality\nbarrier, (ii) educated scratchpads can break the globality with intermediate\nsteps, although not all such scratchpads can generalize out-of-distribution\n(OOD), (iii) a notion of 'inductive scratchpad', that composes the prior\ninformation more efficiently, can both break the globality barrier and improve\nthe OOD generalization. In particular, some of our inductive scratchpads can\nachieve length generalizations of up to $6\\times$ for some arithmetic tasks\ndepending on the input formatting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06467v3",
    "published_date": "2024-06-10 17:05:12 UTC",
    "updated_date": "2024-11-01 17:45:03 UTC"
  },
  {
    "arxiv_id": "2406.06465v1",
    "title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction",
    "authors": [
      "Zhen Xing",
      "Qi Dai",
      "Zejia Weng",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06465v1",
    "published_date": "2024-06-10 17:02:08 UTC",
    "updated_date": "2024-06-10 17:02:08 UTC"
  },
  {
    "arxiv_id": "2406.06464v2",
    "title": "Transforming Wearable Data into Health Insights using Large Language Model Agents",
    "authors": [
      "Mike A. Merrill",
      "Akshay Paruchuri",
      "Naghmeh Rezaei",
      "Geza Kovacs",
      "Javier Perez",
      "Yun Liu",
      "Erik Schenck",
      "Nova Hammerquist",
      "Jake Sunshine",
      "Shyam Tailor",
      "Kumar Ayush",
      "Hao-Wei Su",
      "Qian He",
      "Cory Y. McLean",
      "Mark Malhotra",
      "Shwetak Patel",
      "Jiening Zhan",
      "Tim Althoff",
      "Daniel McDuff",
      "Xin Liu"
    ],
    "abstract": "Despite the proliferation of wearable health trackers and the importance of\nsleep and exercise to health, deriving actionable personalized insights from\nwearable data remains a challenge because doing so requires non-trivial\nopen-ended analysis of these data. The recent rise of large language model\n(LLM) agents, which can use tools to reason about and interact with the world,\npresents a promising opportunity to enable such personalized analysis at scale.\nYet, the application of LLM agents in analyzing personal health is still\nlargely untapped. In this paper, we introduce the Personal Health Insights\nAgent (PHIA), an agent system that leverages state-of-the-art code generation\nand information retrieval tools to analyze and interpret behavioral health data\nfrom wearables. We curate two benchmark question-answering datasets of over\n4000 health insights questions. Based on 650 hours of human and expert\nevaluation we find that PHIA can accurately address over 84% of factual\nnumerical questions and more than 83% of crowd-sourced open-ended questions.\nThis work has implications for advancing behavioral health across the\npopulation, potentially enabling individuals to interpret their own wearable\ndata, and paving the way for a new era of accessible, personalized wellness\nregimens that are informed by data-driven insights.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "38 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.06464v2",
    "published_date": "2024-06-10 17:00:54 UTC",
    "updated_date": "2024-06-11 15:17:43 UTC"
  },
  {
    "arxiv_id": "2406.06460v1",
    "title": "Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots",
    "authors": [
      "Bahador Beigomi",
      "Zheng H. Zhu"
    ],
    "abstract": "In this research, we introduce a deep reinforcement learning-based control\napproach to address the intricate challenge of the robotic pre-grasping phase\nunder microgravity conditions. Leveraging reinforcement learning eliminates the\nnecessity for manual feature design, therefore simplifying the problem and\nempowering the robot to learn pre-grasping policies through trial and error.\nOur methodology incorporates an off-policy reinforcement learning framework,\nemploying the soft actor-critic technique to enable the gripper to proficiently\napproach a free-floating moving object, ensuring optimal pre-grasp success. For\neffective learning of the pre-grasping approach task, we developed a reward\nfunction that offers the agent clear and insightful feedback. Our case study\nexamines a pre-grasping task where a Robotiq 3F gripper is required to navigate\ntowards a free-floating moving target, pursue it, and subsequently position\nitself at the desired pre-grasp location. We assessed our approach through a\nseries of experiments in both simulated and real-world environments. The source\ncode, along with recordings of real-world robot grasping, is available at\nFanuc_Robotiq_Grasp.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This is a preprint for the work submitted to the ICRA 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2406.06460v1",
    "published_date": "2024-06-10 16:54:51 UTC",
    "updated_date": "2024-06-10 16:54:51 UTC"
  },
  {
    "arxiv_id": "2406.06455v2",
    "title": "A Large Language Model Pipeline for Breast Cancer Oncology",
    "authors": [
      "Tristen Pool",
      "Dennis Trujillo"
    ],
    "abstract": "Large language models (LLMs) have demonstrated potential in the innovation of\nmany disciplines. However, how they can best be developed for oncology remains\nunderdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical\ndataset and clinical guidelines text corpus for two important cancer treatment\nfactors, adjuvant radiation therapy and chemotherapy, using a novel Langchain\nprompt engineering pipeline. A high accuracy (0.85+) was achieved in the\nclassification of adjuvant radiation therapy and chemotherapy for breast cancer\npatients. Furthermore, a confidence interval was formed from observational data\non the quality of treatment from human oncologists to estimate the proportion\nof scenarios in which the model must outperform the original oncologist in its\ntreatment prediction to be a better solution overall as 8.2% to 13.3%. Due to\nindeterminacy in the outcomes of cancer treatment decisions, future\ninvestigation, potentially a clinical trial, would be required to determine if\nthis threshold was met by the models. Nevertheless, with 85% of U.S. cancer\npatients receiving treatment at local community facilities, these kinds of\nmodels could play an important part in expanding access to quality care with\noutcomes that lie, at minimum, close to a human oncologist.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06455v2",
    "published_date": "2024-06-10 16:44:48 UTC",
    "updated_date": "2024-06-13 18:48:17 UTC"
  },
  {
    "arxiv_id": "2406.06451v1",
    "title": "Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course",
    "authors": [
      "Aadarsh Padiyath",
      "Xinying Hou",
      "Amy Pang",
      "Diego Viramontes Vargas",
      "Xingjian Gu",
      "Tamara Nelson-Fromm",
      "Zihan Wu",
      "Mark Guzdial",
      "Barbara Ericson"
    ],
    "abstract": "The capability of large language models (LLMs) to generate, debug, and\nexplain code has sparked the interest of researchers and educators in\nundergraduate programming, with many anticipating their transformative\npotential in programming education. However, decisions about why and how to use\nLLMs in programming education may involve more than just the assessment of an\nLLM's technical capabilities. Using the social shaping of technology theory as\na guiding framework, our study explores how students' social perceptions\ninfluence their own LLM usage. We then examine the correlation of self-reported\nLLM usage with students' self-efficacy and midterm performances in an\nundergraduate programming course. Triangulating data from an anonymous\nend-of-course student survey (n = 158), a mid-course self-efficacy survey\n(n=158), student interviews (n = 10), self-reported LLM usage on homework, and\nmidterm performances, we discovered that students' use of LLMs was associated\nwith their expectations for their future careers and their perceptions of peer\nusage. Additionally, early self-reported LLM usage in our context correlated\nwith lower self-efficacy and lower midterm scores, while students' perceived\nover-reliance on LLMs, rather than their usage itself, correlated with\ndecreased self-efficacy later in the course.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to the ACM Conference on International Computing Education\n  Research V.1 (ICER '24 Vol. 1)",
    "pdf_url": "http://arxiv.org/pdf/2406.06451v1",
    "published_date": "2024-06-10 16:40:14 UTC",
    "updated_date": "2024-06-10 16:40:14 UTC"
  },
  {
    "arxiv_id": "2406.06441v1",
    "title": "Interpretability of Language Models via Task Spaces",
    "authors": [
      "Lucas Weber",
      "Jaap Jumelet",
      "Elia Bruni",
      "Dieuwke Hupkes"
    ],
    "abstract": "The usual way to interpret language models (LMs) is to test their performance\non different benchmarks and subsequently infer their internal processes. In\nthis paper, we present an alternative approach, concentrating on the quality of\nLM processing, with a focus on their language abilities. To this end, we\nconstruct 'linguistic task spaces' -- representations of an LM's language\nconceptualisation -- that shed light on the connections LMs draw between\nlanguage phenomena. Task spaces are based on the interactions of the learning\nsignals from different linguistic phenomena, which we assess via a method we\ncall 'similarity probing'. To disentangle the learning signals of linguistic\nphenomena, we further introduce a method called 'fine-tuning via gradient\ndifferentials' (FTGD). We apply our methods to language models of three\ndifferent scales and find that larger models generalise better to overarching\ngeneral concepts for linguistic tasks, making better use of their shared\nstructure. Further, the distributedness of linguistic processing increases with\npre-training through increased parameter sharing between related linguistic\ntasks. The overall generalisation patterns are mostly stable throughout\ntraining and not marked by incisive stages, potentially explaining the lack of\nsuccessful curriculum strategies for LMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published at ACL 2024 (main)",
    "pdf_url": "http://arxiv.org/pdf/2406.06441v1",
    "published_date": "2024-06-10 16:34:30 UTC",
    "updated_date": "2024-06-10 16:34:30 UTC"
  },
  {
    "arxiv_id": "2406.06435v1",
    "title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain",
    "authors": [
      "Brian Hu",
      "Bill Ray",
      "Alice Leung",
      "Amy Summerville",
      "David Joy",
      "Christopher Funk",
      "Arslan Basharat"
    ],
    "abstract": "In difficult decision-making scenarios, it is common to have conflicting\nopinions among expert human decision-makers as there may not be a single right\nanswer. Such decisions may be guided by different attributes that can be used\nto characterize an individual's decision. We introduce a novel dataset for\nmedical triage decision-making, labeled with a set of decision-maker attributes\n(DMAs). This dataset consists of 62 scenarios, covering six different DMAs,\nincluding ethical principles such as fairness and moral desert. We present a\nnovel software framework for human-aligned decision-making by utilizing these\nDMAs, paving the way for trustworthy AI with better guardrails. Specifically,\nwe demonstrate how large language models (LLMs) can serve as ethical\ndecision-makers, and how their decisions can be aligned to different DMAs using\nzero-shot prompting. Our experiments focus on different open-source models with\nvarying sizes and training techniques, such as Falcon, Mistral, and Llama 2.\nFinally, we also introduce a new form of weighted self-consistency that\nimproves the overall quantified performance. Our results provide new research\ndirections in the use of LLMs as alignable decision-makers. The dataset and\nopen-source software are publicly available at:\nhttps://github.com/ITM-Kitware/llm-alignable-dm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages total (including appendix), NAACL 2024 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2406.06435v1",
    "published_date": "2024-06-10 16:25:23 UTC",
    "updated_date": "2024-06-10 16:25:23 UTC"
  },
  {
    "arxiv_id": "2406.06433v3",
    "title": "DISCO: An End-to-End Bandit Framework for Personalised Discount Allocation",
    "authors": [
      "Jason Shuo Zhang",
      "Benjamin Howson",
      "Panayiota Savva",
      "Eleanor Loh"
    ],
    "abstract": "Personalised discount codes provide a powerful mechanism for managing\ncustomer relationships and operational spend in e-commerce. Bandits are well\nsuited for this product area, given the partial information nature of the\nproblem, as well as the need for adaptation to the changing business\nenvironment. Here, we introduce DISCO, an end-to-end contextual bandit\nframework for personalised discount code allocation at ASOS. DISCO adapts the\ntraditional Thompson Sampling algorithm by integrating it within an integer\nprogram, thereby allowing for operational cost control. Because bandit learning\nis often worse with high dimensional actions, we focused on building low\ndimensional action and context representations that were nonetheless capable of\ngood accuracy. Additionally, we sought to build a model that preserved the\nrelationship between price and sales, in which customers increasing their\npurchasing in response to lower prices (\"negative price elasticity\"). These\naims were achieved by using radial basis functions to represent the continuous\n(i.e. infinite armed) action space, in combination with context embeddings\nextracted from a neural network. These feature representations were used within\na Thompson Sampling framework to facilitate exploration, and further integrated\nwith an integer program to allocate discount codes across ASOS's customer base.\nThese modelling decisions result in a reward model that (a) enables pooled\nlearning across similar actions, (b) is highly accurate, including in\nextrapolation, and (c) preserves the expected negative price elasticity.\nThrough offline analysis, we show that DISCO is able to effectively enact\nexploration and improves its performance over time, despite the global\nconstraint. Finally, we subjected DISCO to a rigorous online A/B test, and find\nthat it achieves a significant improvement of >1% in average basket value,\nrelative to the legacy systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ECML/PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06433v3",
    "published_date": "2024-06-10 16:24:35 UTC",
    "updated_date": "2024-06-12 21:51:22 UTC"
  },
  {
    "arxiv_id": "2406.06417v2",
    "title": "Explainable Graph Neural Networks Under Fire",
    "authors": [
      "Zhong Li",
      "Simon Geisler",
      "Yuhang Wang",
      "Stephan Günnemann",
      "Matthijs van Leeuwen"
    ],
    "abstract": "Predictions made by graph neural networks (GNNs) usually lack\ninterpretability due to their complex computational behavior and the abstract\nnature of graphs. In an attempt to tackle this, many GNN explanation methods\nhave emerged. Their goal is to explain a model's predictions and thereby obtain\ntrust when GNN models are deployed in decision critical applications. Most GNN\nexplanation methods work in a post-hoc manner and provide explanations in the\nform of a small subset of important edges and/or nodes. In this paper we\ndemonstrate that these explanations can unfortunately not be trusted, as common\nGNN explanation methods turn out to be highly susceptible to adversarial\nperturbations. That is, even small perturbations of the original graph\nstructure that preserve the model's predictions may yield drastically different\nexplanations. This calls into question the trustworthiness and practical\nutility of post-hoc explanation methods for GNNs. To be able to attack GNN\nexplanation models, we devise a novel attack method dubbed \\textit{GXAttack},\nthe first \\textit{optimization-based} adversarial white-box attack method for\npost-hoc GNN explanations under such settings. Due to the devastating\neffectiveness of our attack, we call for an adversarial evaluation of future\nGNN explainers to demonstrate their robustness. For reproducibility, our code\nis available via GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06417v2",
    "published_date": "2024-06-10 16:09:16 UTC",
    "updated_date": "2024-10-18 05:03:08 UTC"
  },
  {
    "arxiv_id": "2406.06400v2",
    "title": "An Empirical Design Justice Approach to Identifying Ethical Considerations in the Intersection of Large Language Models and Social Robotics",
    "authors": [
      "Alva Markelius"
    ],
    "abstract": "The integration of Large Language Models (LLMs) in social robotics presents a\nunique set of ethical challenges and social impacts. This research is set out\nto identify ethical considerations that arise in the design and development of\nthese two technologies in combination. Using LLMs for social robotics may\nprovide benefits, such as enabling natural language open-domain dialogues.\nHowever, the intersection of these two technologies also gives rise to ethical\nconcerns related to misinformation, non-verbal cues, emotional disruption, and\nbiases. The robot's physical social embodiment adds complexity, as ethical\nhazards associated with LLM-based Social AI, such as hallucinations and\nmisinformation, can be exacerbated due to the effects of physical embodiment on\nsocial perception and communication. To address these challenges, this study\nemploys an empirical design justice-based methodology, focusing on identifying\nsocio-technical ethical considerations through a qualitative co-design and\ninteraction study. The purpose of the study is to identify ethical\nconsiderations relevant to the process of co-design of, and interaction with a\nhumanoid social robot as the interface of a LLM, and to evaluate how a design\njustice methodology can be used in the context of designing LLMs-based social\nrobotics. The findings reveal a mapping of ethical considerations arising in\nfour conceptual dimensions: interaction, co-design, terms of service and\nrelationship and evaluates how a design justice approach can be used\nempirically in the intersection of LLMs and social robotics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2; K.4; H.5"
    ],
    "primary_category": "cs.RO",
    "comment": "This is a preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.06400v2",
    "published_date": "2024-06-10 15:53:50 UTC",
    "updated_date": "2024-06-12 09:25:36 UTC"
  },
  {
    "arxiv_id": "2406.06399v3",
    "title": "Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue",
    "authors": [
      "Simone Alghisi",
      "Massimo Rizzoli",
      "Gabriel Roccabruna",
      "Seyed Mahed Mousavi",
      "Giuseppe Riccardi"
    ],
    "abstract": "We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at INLG 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06399v3",
    "published_date": "2024-06-10 15:52:49 UTC",
    "updated_date": "2024-08-03 15:12:02 UTC"
  },
  {
    "arxiv_id": "2406.06397v2",
    "title": "Contrastive learning of T cell receptor representations",
    "authors": [
      "Yuta Nagano",
      "Andrew Pyo",
      "Martina Milighetti",
      "James Henderson",
      "John Shawe-Taylor",
      "Benny Chain",
      "Andreas Tiffeau-Mayer"
    ],
    "abstract": "Computational prediction of the interaction of T cell receptors (TCRs) and\ntheir ligands is a grand challenge in immunology. Despite advances in\nhigh-throughput assays, specificity-labelled TCR data remains sparse. In other\ndomains, the pre-training of language models on unlabelled data has been\nsuccessfully used to address data bottlenecks. However, it is unclear how to\nbest pre-train protein language models for TCR specificity prediction. Here we\nintroduce a TCR language model called SCEPTR (Simple Contrastive Embedding of\nthe Primary sequence of T cell Receptors), capable of data-efficient transfer\nlearning. Through our model, we introduce a novel pre-training strategy\ncombining autocontrastive learning and masked-language modelling, which enables\nSCEPTR to achieve its state-of-the-art performance. In contrast, existing\nprotein language models and a variant of SCEPTR pre-trained without\nautocontrastive learning are outperformed by sequence alignment-based methods.\nWe anticipate that contrastive learning will be a useful paradigm to decode the\nrules of TCR specificity.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "J.3; I.2.7"
    ],
    "primary_category": "q-bio.BM",
    "comment": "25 pages, 23 figures; additional analyses and improvements to\n  existing figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06397v2",
    "published_date": "2024-06-10 15:50:45 UTC",
    "updated_date": "2024-10-10 10:32:44 UTC"
  },
  {
    "arxiv_id": "2406.06385v3",
    "title": "Low-Rank Quantization-Aware Training for LLMs",
    "authors": [
      "Yelysei Bondarenko",
      "Riccardo Del Chiaro",
      "Markus Nagel"
    ],
    "abstract": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06385v3",
    "published_date": "2024-06-10 15:44:22 UTC",
    "updated_date": "2024-09-03 16:36:06 UTC"
  },
  {
    "arxiv_id": "2406.06375v1",
    "title": "MOSA: Music Motion with Semantic Annotation Dataset for Cross-Modal Music Processing",
    "authors": [
      "Yu-Fen Huang",
      "Nikki Moran",
      "Simon Coleman",
      "Jon Kelly",
      "Shun-Hwa Wei",
      "Po-Yin Chen",
      "Yun-Hsin Huang",
      "Tsung-Ping Chen",
      "Yu-Chia Kuo",
      "Yu-Chi Wei",
      "Chih-Hsuan Li",
      "Da-Yu Huang",
      "Hsuan-Kai Kao",
      "Ting-Wei Lin",
      "Li Su"
    ],
    "abstract": "In cross-modal music processing, translation between visual, auditory, and\nsemantic content opens up new possibilities as well as challenges. The\nconstruction of such a transformative scheme depends upon a benchmark corpus\nwith a comprehensive data infrastructure. In particular, the assembly of a\nlarge-scale cross-modal dataset presents major challenges. In this paper, we\npresent the MOSA (Music mOtion with Semantic Annotation) dataset, which\ncontains high quality 3-D motion capture data, aligned audio recordings, and\nnote-by-note semantic annotations of pitch, beat, phrase, dynamic,\narticulation, and harmony for 742 professional music performances by 23\nprofessional musicians, comprising more than 30 hours and 570 K notes of data.\nTo our knowledge, this is the largest cross-modal music dataset with note-level\nannotations to date. To demonstrate the usage of the MOSA dataset, we present\nseveral innovative cross-modal music information retrieval (MIR) and musical\ncontent generation tasks, including the detection of beats, downbeats, phrase,\nand expressive contents from audio, video and motion data, and the generation\nof musicians' body motion from given music audio. The dataset and codes are\navailable alongside this publication\n(https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  2024. 14 pages, 7 figures. Dataset is available on:\n  https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset/tree/main\n  and https://zenodo.org/records/11393449",
    "pdf_url": "http://arxiv.org/pdf/2406.06375v1",
    "published_date": "2024-06-10 15:37:46 UTC",
    "updated_date": "2024-06-10 15:37:46 UTC"
  },
  {
    "arxiv_id": "2406.06372v1",
    "title": "Improving Deep Learning-based Automatic Cranial Defect Reconstruction by Heavy Data Augmentation: From Image Registration to Latent Diffusion Models",
    "authors": [
      "Marek Wodzinski",
      "Kamil Kwarciak",
      "Mateusz Daniol",
      "Daria Hemmerling"
    ],
    "abstract": "Modeling and manufacturing of personalized cranial implants are important\nresearch areas that may decrease the waiting time for patients suffering from\ncranial damage. The modeling of personalized implants may be partially\nautomated by the use of deep learning-based methods. However, this task suffers\nfrom difficulties with generalizability into data from previously unseen\ndistributions that make it difficult to use the research outcomes in real\nclinical settings. Due to difficulties with acquiring ground-truth annotations,\ndifferent techniques to improve the heterogeneity of datasets used for training\nthe deep networks have to be considered and introduced. In this work, we\npresent a large-scale study of several augmentation techniques, varying from\nclassical geometric transformations, image registration, variational\nautoencoders, and generative adversarial networks, to the most recent advances\nin latent diffusion models. We show that the use of heavy data augmentation\nsignificantly increases both the quantitative and qualitative outcomes,\nresulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96\nfor the SkullFix datasets. Moreover, we show that the synthetically augmented\nnetwork successfully reconstructs real clinical defects. The work is a\nconsiderable contribution to the field of artificial intelligence in the\nautomatic modeling of personalized cranial implants.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06372v1",
    "published_date": "2024-06-10 15:34:23 UTC",
    "updated_date": "2024-06-10 15:34:23 UTC"
  },
  {
    "arxiv_id": "2406.06363v1",
    "title": "Automating Food Drop: The Power of Two Choices for Dynamic and Fair Food Allocation",
    "authors": [
      "Marios Mertzanidis",
      "Alexandros Psomas",
      "Paritosh Verma"
    ],
    "abstract": "Food waste and food insecurity are two closely related pressing global\nissues. Food rescue organizations worldwide run programs aimed at addressing\nthe two problems. In this paper, we partner with a non-profit organization in\nthe state of Indiana that leads \\emph{Food Drop}, a program that is designed to\nredirect rejected truckloads of food away from landfills and into food banks.\nThe truckload to food bank matching decisions are currently made by an employee\nof our partner organization. In addition to this being a very time-consuming\ntask, as perhaps expected from human-based matching decisions, the allocations\nare often skewed: a small percentage of the possible recipients receives the\nmajority of donations. Our goal in this partnership is to completely automate\nFood Drop. In doing so, we need a matching algorithm for making real-time\ndecisions that strikes a balance between ensuring fairness for the food banks\nthat receive the food and optimizing efficiency for the truck drivers. In this\npaper, we describe the theoretical guarantees and experiments that dictated our\nchoice of algorithm in the platform we built and deployed for our partner\norganization. Our work also makes contributions to the literature on load\nbalancing and balls-into-bins games, that might be of independent interest.\nSpecifically, we study the allocation of $m$ weighted balls into $n$ weighted\nbins, where each ball has two non-uniformly sampled random bin choices, and\nprove upper bounds, that hold with high probability, on the maximum load of any\nbin.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06363v1",
    "published_date": "2024-06-10 15:22:41 UTC",
    "updated_date": "2024-06-10 15:22:41 UTC"
  },
  {
    "arxiv_id": "2406.10256v1",
    "title": "Explicit Word Density Estimation for Language Modelling",
    "authors": [
      "Jovan Andonov",
      "Octavian Ganea",
      "Paulina Grnarova",
      "Gary Bécigneul",
      "Thomas Hofmann"
    ],
    "abstract": "Language Modelling has been a central part of Natural Language Processing for\na very long time and in the past few years LSTM-based language models have been\nthe go-to method for commercial language modeling. Recently, it has been shown\nthat when looking at language modelling from a matrix factorization point of\nview, the final Softmax layer limits the expressiveness of the model, by\nputting an upper bound on the rank of the resulting matrix. Additionally, a new\nfamily of neural networks based called NeuralODEs, has been introduced as a\ncontinuous alternative to Residual Networks. Moreover, it has been shown that\nthere is a connection between these models and Normalizing Flows. In this work\nwe propose a new family of language models based on NeuralODEs and the\ncontinuous analogue of Normalizing Flows and manage to improve on some of the\nbaselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Master's thesis",
    "pdf_url": "http://arxiv.org/pdf/2406.10256v1",
    "published_date": "2024-06-10 15:21:33 UTC",
    "updated_date": "2024-06-10 15:21:33 UTC"
  },
  {
    "arxiv_id": "2406.06357v1",
    "title": "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows",
    "authors": [
      "Xingjian Zhang",
      "Yutong Xie",
      "Jin Huang",
      "Jinge Ma",
      "Zhaoying Pan",
      "Qijia Liu",
      "Ziyang Xiong",
      "Tolga Ergen",
      "Dongsub Shim",
      "Honglak Lee",
      "Qiaozhu Mei"
    ],
    "abstract": "Scientific innovation relies on detailed workflows, which include critical\nsteps such as analyzing literature, generating ideas, validating these ideas,\ninterpreting results, and inspiring follow-up research. However, scientific\npublications that document these workflows are extensive and unstructured. This\nmakes it difficult for both human researchers and AI systems to effectively\nnavigate and explore the space of scientific innovation. To address this issue,\nwe introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization\nof Scientific Workflows. MASSW includes more than 152,000 peer-reviewed\npublications from 17 leading computer science conferences spanning the past 50\nyears. Using Large Language Models (LLMs), we automatically extract five core\naspects from these publications -- context, key idea, method, outcome, and\nprojected impact -- which correspond to five key steps in the research\nworkflow. These structured summaries facilitate a variety of downstream tasks\nand analyses. The quality of the LLM-extracted summaries is validated by\ncomparing them with human annotations. We demonstrate the utility of MASSW\nthrough multiple novel machine-learning tasks that can be benchmarked using\nthis new dataset, which make various types of predictions and recommendations\nalong the scientific workflow. MASSW holds significant potential for\nresearchers to create and benchmark new AI methods for optimizing scientific\nworkflows and fostering scientific innovation in the field. Our dataset is\nopenly available at \\url{https://github.com/xingjian-zhang/massw}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:1706.03762 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2406.06357v1",
    "published_date": "2024-06-10 15:19:09 UTC",
    "updated_date": "2024-06-10 15:19:09 UTC"
  },
  {
    "arxiv_id": "2406.06356v1",
    "title": "Re.Dis.Cover Place with Generative AI: Exploring the Experience and Design of City Wandering with Image-to-Image AI",
    "authors": [
      "Peng-Kai Hung",
      "Janet Yi-Ching Huang",
      "Stephan Wensveen",
      "Rung-Huei Liang"
    ],
    "abstract": "The HCI field has demonstrated a growing interest in leveraging emerging\ntechnologies to enrich urban experiences. However, insufficient studies\ninvestigate the experience and design space of AI image technology (AIGT)\napplications for playful urban interaction, despite its widespread adoption. To\nexplore this gap, we conducted an exploratory study involving four participants\nwho wandered and photographed within Eindhoven Centre and interacted with an\nimage-to-image AI. Preliminary findings present their observations, the effect\nof their familiarity with places, and how AIGT becomes an explorer's tool or\nco-speculator. We then highlight AIGT's capability of supporting playfulness,\nreimaginations, and rediscoveries of places through defamiliarizing and\nfamiliarizing cityscapes. Additionally, we propose the metaphor AIGT as a\n'tourist' to discuss its opportunities for engaging explorations and risks of\nstereotyping places. Collectively, our research provides initial empirical\ninsights and design considerations, inspiring future HCI endeavors for creating\nurban play with generative AI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06356v1",
    "published_date": "2024-06-10 15:18:14 UTC",
    "updated_date": "2024-06-10 15:18:14 UTC"
  },
  {
    "arxiv_id": "2406.06341v1",
    "title": "Predicting Heart Activity from Speech using Data-driven and Knowledge-based features",
    "authors": [
      "Gasser Elbanna",
      "Zohreh Mostaani",
      "Mathew Magimai. -Doss"
    ],
    "abstract": "Accurately predicting heart activity and other biological signals is crucial\nfor diagnosis and monitoring. Given that speech is an outcome of multiple\nphysiological systems, a significant body of work studied the acoustic\ncorrelates of heart activity. Recently, self-supervised models have excelled in\nspeech-related tasks compared to traditional acoustic methods. However, the\nrobustness of data-driven representations in predicting heart activity remained\nunexplored. In this study, we demonstrate that self-supervised speech models\noutperform acoustic features in predicting heart activity parameters. We also\nemphasize the impact of individual variability on model generalizability. These\nfindings underscore the value of data-driven representations in such tasks and\nthe need for more speech-based physiological data to mitigate speaker-related\nchallenges.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06341v1",
    "published_date": "2024-06-10 15:01:46 UTC",
    "updated_date": "2024-06-10 15:01:46 UTC"
  },
  {
    "arxiv_id": "2406.06340v1",
    "title": "Optimisation of federated learning settings under statistical heterogeneity variations",
    "authors": [
      "Basem Suleiman",
      "Muhammad Johan Alibasa",
      "Rizka Widyarini Purwanto",
      "Lewis Jeffries",
      "Ali Anaissi",
      "Jacky Song"
    ],
    "abstract": "Federated Learning (FL) enables local devices to collaboratively learn a\nshared predictive model by only periodically sharing model parameters with a\ncentral aggregator. However, FL can be disadvantaged by statistical\nheterogeneity produced by the diversity in each local devices data\ndistribution, which creates different levels of Independent and Identically\nDistributed (IID) data. Furthermore, this can be more complex when optimising\ndifferent combinations of FL parameters and choosing optimal aggregation. In\nthis paper, we present an empirical analysis of different FL training\nparameters and aggregators over various levels of statistical heterogeneity on\nthree datasets. We propose a systematic data partition strategy to simulate\ndifferent levels of statistical heterogeneity and a metric to measure the level\nof IID. Additionally, we empirically identify the best FL model and key\nparameters for datasets of different characteristics. On the basis of these, we\npresent recommended guidelines for FL parameters and aggregators to optimise\nmodel performance under different levels of IID and with different datasets",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06340v1",
    "published_date": "2024-06-10 15:01:03 UTC",
    "updated_date": "2024-06-10 15:01:03 UTC"
  },
  {
    "arxiv_id": "2406.06331v2",
    "title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations",
    "authors": [
      "Yunsoo Kim",
      "Jinge Wu",
      "Yusuf Abdulle",
      "Honghan Wu"
    ],
    "abstract": "This paper introduces MedExQA, a novel benchmark in medical\nquestion-answering, to evaluate large language models' (LLMs) understanding of\nmedical knowledge through explanations. By constructing datasets across five\ndistinct medical specialties that are underrepresented in current datasets and\nfurther incorporating multiple explanations for each question-answer pair, we\naddress a major gap in current medical QA benchmarks which is the absence of\ncomprehensive assessments of LLMs' ability to generate nuanced medical\nexplanations. Our work highlights the importance of explainability in medical\nLLMs, proposes an effective methodology for evaluating models beyond\nclassification accuracy, and sheds light on one specific domain, speech\nlanguage pathology, where current LLMs including GPT4 lack good understanding.\nOur results show generation evaluation with multiple explanations aligns better\nwith human assessment, highlighting an opportunity for a more robust automated\ncomprehension assessment for LLMs. To diversify open-source medical LLMs\n(currently mostly based on Llama2), this work also proposes a new medical\nmodel, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs\nbased on Llama2-70B in generating explanations, showing its effectiveness in\nthe resource-constrained medical domain. We will share our benchmark datasets\nand the trained model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL2024 BioNLP Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.06331v2",
    "published_date": "2024-06-10 14:47:04 UTC",
    "updated_date": "2024-07-03 10:13:56 UTC"
  },
  {
    "arxiv_id": "2406.06316v1",
    "title": "Tx-LLM: A Large Language Model for Therapeutics",
    "authors": [
      "Juan Manuel Zambrano Chaves",
      "Eric Wang",
      "Tao Tu",
      "Eeshit Dhaval Vaishnav",
      "Byron Lee",
      "S. Sara Mahdavi",
      "Christopher Semturs",
      "David Fleet",
      "Vivek Natarajan",
      "Shekoofeh Azizi"
    ],
    "abstract": "Developing therapeutics is a lengthy and expensive process that requires the\nsatisfaction of many different criteria, and AI models capable of expediting\nthe process would be invaluable. However, the majority of current AI approaches\naddress only a narrowly defined set of tasks, often circumscribed within a\nparticular domain. To bridge this gap, we introduce Tx-LLM, a generalist large\nlanguage model (LLM) fine-tuned from PaLM-2 which encodes knowledge about\ndiverse therapeutic modalities. Tx-LLM is trained using a collection of 709\ndatasets that target 66 tasks spanning various stages of the drug discovery\npipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide\nvariety of chemical or biological entities(small molecules, proteins, nucleic\nacids, cell lines, diseases) interleaved with free-text, allowing it to predict\na broad range of associated properties, achieving competitive with\nstate-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on\n22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class\nperformance on average for tasks combining molecular SMILES representations\nwith text such as cell line names or disease names, likely due to context\nlearned during pretraining. We observe evidence of positive transfer between\ntasks with diverse drug types (e.g.,tasks involving small molecules and tasks\ninvolving proteins), and we study the impact of model size, domain finetuning,\nand prompting strategies on performance. We believe Tx-LLM represents an\nimportant step towards LLMs encoding biochemical knowledge and could have a\nfuture role as an end-to-end tool across the drug discovery development\npipeline.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06316v1",
    "published_date": "2024-06-10 14:33:02 UTC",
    "updated_date": "2024-06-10 14:33:02 UTC"
  },
  {
    "arxiv_id": "2406.06309v2",
    "title": "Is Value Functions Estimation with Classification Plug-and-play for Offline Reinforcement Learning?",
    "authors": [
      "Denis Tarasov",
      "Kirill Brilliantov",
      "Dmitrii Kharlapenko"
    ],
    "abstract": "In deep Reinforcement Learning (RL), value functions are typically\napproximated using deep neural networks and trained via mean squared error\nregression objectives to fit the true value functions. Recent research has\nproposed an alternative approach, utilizing the cross-entropy classification\nobjective, which has demonstrated improved performance and scalability of RL\nalgorithms. However, existing study have not extensively benchmarked the\neffects of this replacement across various domains, as the primary objective\nwas to demonstrate the efficacy of the concept across a broad spectrum of\ntasks, without delving into in-depth analysis. Our work seeks to empirically\ninvestigate the impact of such a replacement in an offline RL setup and analyze\nthe effects of different aspects on performance. Through large-scale\nexperiments conducted across a diverse range of tasks using different\nalgorithms, we aim to gain deeper insights into the implications of this\napproach. Our results reveal that incorporating this change can lead to\nsuperior performance over state-of-the-art solutions for some algorithms in\ncertain tasks, while maintaining comparable performance levels in other tasks,\nhowever for other algorithms this modification might lead to the dramatic\nperformance drop. This findings are crucial for further application of\nclassification approach in research and practical tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "https://github.com/DT6A/ClORL",
    "pdf_url": "http://arxiv.org/pdf/2406.06309v2",
    "published_date": "2024-06-10 14:25:11 UTC",
    "updated_date": "2024-11-16 14:03:22 UTC"
  },
  {
    "arxiv_id": "2406.06305v1",
    "title": "NeuroMoCo: A Neuromorphic Momentum Contrast Learning Method for Spiking Neural Networks",
    "authors": [
      "Yuqi Ma",
      "Huamin Wang",
      "Hangchi Shen",
      "Xuemei Chen",
      "Shukai Duan",
      "Shiping Wen"
    ],
    "abstract": "Recently, brain-inspired spiking neural networks (SNNs) have attracted great\nresearch attention owing to their inherent bio-interpretability,\nevent-triggered properties and powerful perception of spatiotemporal\ninformation, which is beneficial to handling event-based neuromorphic datasets.\nIn contrast to conventional static image datasets, event-based neuromorphic\ndatasets present heightened complexity in feature extraction due to their\ndistinctive time series and sparsity characteristics, which influences their\nclassification accuracy. To overcome this challenge, a novel approach termed\nNeuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in\nthis paper by extending the benefits of self-supervised pre-training to SNNs to\neffectively stimulate their potential. This is the first time that\nself-supervised learning (SSL) based on momentum contrastive learning is\nrealized in SNNs. In addition, we devise a novel loss function named MixInfoNCE\ntailored to their temporal characteristics to further increase the\nclassification accuracy of neuromorphic datasets, which is verified through\nrigorous ablation experiments. Finally, experiments on DVS-CIFAR10,\nDVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper\nestablishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256),\n98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages,4 figures,4 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.06305v1",
    "published_date": "2024-06-10 14:20:48 UTC",
    "updated_date": "2024-06-10 14:20:48 UTC"
  },
  {
    "arxiv_id": "2406.10254v1",
    "title": "Towards Signal Processing In Large Language Models",
    "authors": [
      "Prateek Verma",
      "Mert Pilanci"
    ],
    "abstract": "This paper introduces the idea of applying signal processing inside a Large\nLanguage Model (LLM). With the recent explosion of generative AI, our work can\nhelp bridge two fields together, namely the field of signal processing and\nlarge language models. We draw parallels between classical Fourier-Transforms\nand Fourier Transform-like learnable time-frequency representations for every\nintermediate activation signal of an LLM. Once we decompose every activation\nsignal across tokens into a time-frequency representation, we learn how to\nfilter and reconstruct them, with all components learned from scratch, to\npredict the next token given the previous context. We show that for GPT-like\narchitectures, our work achieves faster convergence and significantly increases\nperformance by adding a minuscule number of extra parameters when trained for\nthe same epochs. We hope this work paves the way for algorithms exploring\nsignal processing inside the signals found in neural architectures like LLMs\nand beyond.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.10254v1",
    "published_date": "2024-06-10 13:51:52 UTC",
    "updated_date": "2024-06-10 13:51:52 UTC"
  },
  {
    "arxiv_id": "2406.06262v1",
    "title": "Modular Growth of Hierarchical Networks: Efficient, General, and Robust Curriculum Learning",
    "authors": [
      "Mani Hamidi",
      "Sina Khajehabdollahi",
      "Emmanouil Giannakakis",
      "Tim Schäfer",
      "Anna Levina",
      "Charley M. Wu"
    ],
    "abstract": "Structural modularity is a pervasive feature of biological neural networks,\nwhich have been linked to several functional and computational advantages. Yet,\nthe use of modular architectures in artificial neural networks has been\nrelatively limited despite early successes. Here, we explore the performance\nand functional dynamics of a modular network trained on a memory task via an\niterative growth curriculum. We find that for a given classical, non-modular\nrecurrent neural network (RNN), an equivalent modular network will perform\nbetter across multiple metrics, including training time, generalizability, and\nrobustness to some perturbations. We further examine how different aspects of a\nmodular network's connectivity contribute to its computational capability. We\nthen demonstrate that the inductive bias introduced by the modular topology is\nstrong enough for the network to perform well even when the connectivity within\nmodules is fixed and only the connections between modules are trained. Our\nfindings suggest that gradual modular growth of RNNs could provide advantages\nfor learning increasingly complex tasks on evolutionary timescales, and help\nbuild more scalable and compressible artificial networks.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06262v1",
    "published_date": "2024-06-10 13:44:07 UTC",
    "updated_date": "2024-06-10 13:44:07 UTC"
  },
  {
    "arxiv_id": "2406.10252v2",
    "title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
    "authors": [
      "Yidong Wang",
      "Qi Guo",
      "Wenjin Yao",
      "Hongbo Zhang",
      "Xin Zhang",
      "Zhen Wu",
      "Meishan Zhang",
      "Xinyu Dai",
      "Min Zhang",
      "Qingsong Wen",
      "Wei Ye",
      "Shikun Zhang",
      "Yue Zhang"
    ],
    "abstract": "This paper introduces AutoSurvey, a speedy and well-organized methodology for\nautomating the creation of comprehensive literature surveys in rapidly evolving\nfields like artificial intelligence. Traditional survey paper creation faces\nchallenges due to the vast volume and complexity of information, prompting the\nneed for efficient survey methods. While large language models (LLMs) offer\npromise in automating this process, challenges such as context window\nlimitations, parametric knowledge constraints, and the lack of evaluation\nbenchmarks remain. AutoSurvey addresses these challenges through a systematic\napproach that involves initial retrieval and outline generation, subsection\ndrafting by specialized LLMs, integration and refinement, and rigorous\nevaluation and iteration. Our contributions include a comprehensive solution to\nthe survey problem, a reliable evaluation method, and experimental validation\ndemonstrating AutoSurvey's effectiveness.We open our resources at\n\\url{https://github.com/AutoSurveys/AutoSurvey}.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10252v2",
    "published_date": "2024-06-10 12:56:06 UTC",
    "updated_date": "2024-06-18 02:11:31 UTC"
  },
  {
    "arxiv_id": "2406.06662v1",
    "title": "Proximity Matters: Analyzing the Role of Geographical Proximity in Shaping AI Research Collaborations",
    "authors": [
      "Mohammadmahdi Toobaee",
      "Andrea Schiffauerova",
      "Ashkan Ebadi"
    ],
    "abstract": "The role of geographical proximity in facilitating inter-regional or\ninter-organizational collaborations has been studied thoroughly in recent\nyears. However, the effect of geographical proximity on forming scientific\ncollaborations at the individual level still needs to be addressed. Using\npublication data in the field of artificial intelligence from 2001 to 2019, in\nthis work, the effect of geographical proximity on the likelihood of forming\nfuture scientific collaborations among researchers is studied. In addition, the\ninteraction between geographical and network proximities is examined to see\nwhether network proximity can substitute geographical proximity in encouraging\nlong-distance scientific collaborations. Employing conventional and machine\nlearning techniques, our results suggest that geographical distance impedes\nscientific collaboration at the individual level despite the tremendous\nimprovements in transportation and communication technologies during recent\ndecades. Moreover, our findings show that the effect of network proximity on\nthe likelihood of scientific collaboration increases with geographical\ndistance, implying that network proximity can act as a substitute for\ngeographical proximity.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.DL",
      "cs.LG",
      "physics.soc-ph"
    ],
    "primary_category": "cs.SI",
    "comment": "17 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06662v1",
    "published_date": "2024-06-10 12:37:47 UTC",
    "updated_date": "2024-06-10 12:37:47 UTC"
  },
  {
    "arxiv_id": "2406.06220v2",
    "title": "Label-Looping: Highly Efficient Decoding for Transducers",
    "authors": [
      "Vladimir Bataev",
      "Hainan Xu",
      "Daniel Galvez",
      "Vitaly Lavrukhin",
      "Boris Ginsburg"
    ],
    "abstract": "This paper introduces a highly efficient greedy decoding algorithm for\nTransducer-based speech recognition models. We redesign the standard\nnested-loop design for RNN-T decoding, swapping loops over frames and labels:\nthe outer loop iterates over labels, while the inner loop iterates over frames\nsearching for the next non-blank symbol. Additionally, we represent partial\nhypotheses in a special structure using CUDA tensors, supporting parallelized\nhypotheses manipulations. Experiments show that the label-looping algorithm is\nup to 2.0X faster than conventional batched decoding when using batch size 32.\nIt can be further combined with other compiler or GPU call-related techniques\nto achieve even more speedup. Our algorithm is general-purpose and can work\nwith both conventional Transducers and Token-and-Duration Transducers. We\nopen-source our implementation to benefit the research community.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at IEEE SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06220v2",
    "published_date": "2024-06-10 12:34:38 UTC",
    "updated_date": "2024-09-16 19:04:28 UTC"
  },
  {
    "arxiv_id": "2406.06218v2",
    "title": "Data Augmentation in Earth Observation: A Diffusion Model Approach",
    "authors": [
      "Tiago Sousa",
      "Benoît Ries",
      "Nicolas Guelfi"
    ],
    "abstract": "High-quality Earth Observation (EO) imagery is essential for accurate\nanalysis and informed decision making across sectors. However, data scarcity\ncaused by atmospheric conditions, seasonal variations, and limited geographical\ncoverage hinders the effective application of Artificial Intelligence (AI) in\nEO. Traditional data augmentation techniques, which rely on basic parameterized\nimage transformations, often fail to introduce sufficient diversity across key\nsemantic axes. These axes include natural changes such as snow and floods,\nhuman impacts like urbanization and roads, and disasters such as wildfires and\nstorms, which limits the accuracy of AI models in EO applications. To address\nthis, we propose a four-stage data augmentation approach that integrates\ndiffusion models to enhance semantic diversity. Our method employs meta-prompts\nfor instruction generation, vision-language models for rich captioning,\nEO-specific diffusion model fine-tuning, and iterative data augmentation.\nExtensive experiments using four augmentation techniques demonstrate that our\napproach consistently outperforms established methods, generating semantically\ndiverse EO images and improving AI model performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SE",
      "I.4.9; I.4.9; I.2.m"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06218v2",
    "published_date": "2024-06-10 12:33:47 UTC",
    "updated_date": "2025-03-26 16:23:33 UTC"
  },
  {
    "arxiv_id": "2406.06213v1",
    "title": "A Statistical Theory of Regularization-Based Continual Learning",
    "authors": [
      "Xuyang Zhao",
      "Huiyuan Wang",
      "Weiran Huang",
      "Wei Lin"
    ],
    "abstract": "We provide a statistical analysis of regularization-based continual learning\non a sequence of linear regression tasks, with emphasis on how different\nregularization terms affect the model performance. We first derive the\nconvergence rate for the oracle estimator obtained as if all data were\navailable simultaneously. Next, we consider a family of generalized\n$\\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters,\nwhich includes the minimum norm estimator and continual ridge regression as\nspecial cases. As more tasks are introduced, we derive an iterative update\nformula for the estimation error of generalized $\\ell_2$-regularized\nestimators, from which we determine the hyperparameters resulting in the\noptimal algorithm. Interestingly, the choice of hyperparameters can effectively\nbalance the trade-off between forward and backward knowledge transfer and\nadjust for data heterogeneity. Moreover, the estimation error of the optimal\nalgorithm is derived explicitly, which is of the same order as that of the\noracle estimator. In contrast, our lower bounds for the minimum norm estimator\nand continual ridge regression show their suboptimality. A byproduct of our\ntheoretical analysis is the equivalence between early stopping and generalized\n$\\ell_2$-regularization in continual learning, which may be of independent\ninterest. Finally, we conduct experiments to complement our theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06213v1",
    "published_date": "2024-06-10 12:25:13 UTC",
    "updated_date": "2024-06-10 12:25:13 UTC"
  },
  {
    "arxiv_id": "2406.06210v1",
    "title": "Quantum Architecture Search: A Survey",
    "authors": [
      "Darya Martyniuk",
      "Johannes Jung",
      "Adrian Paschke"
    ],
    "abstract": "Quantum computing has made significant progress in recent years, attracting\nimmense interest not only in research laboratories but also in various\nindustries. However, the application of quantum computing to solve real-world\nproblems is still hampered by a number of challenges, including hardware\nlimitations and a relatively under-explored landscape of quantum algorithms,\nespecially when compared to the extensive development of classical computing.\nThe design of quantum circuits, in particular parameterized quantum circuits\n(PQCs), which contain learnable parameters optimized by classical methods, is a\nnon-trivial and time-consuming task requiring expert knowledge. As a result,\nresearch on the automated generation of PQCs, known as quantum architecture\nsearch (QAS), has gained considerable interest. QAS focuses on the use of\nmachine learning and optimization-driven techniques to generate PQCs tailored\nto specific problems and characteristics of quantum hardware. In this paper, we\nprovide an overview of QAS methods by examining relevant research studies in\nthe field. We discuss main challenges in designing and performing an automated\nsearch for an optimal PQC, and survey ways to address them to ease future\nresearch.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "I.2.2; J.6"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06210v1",
    "published_date": "2024-06-10 12:17:46 UTC",
    "updated_date": "2024-06-10 12:17:46 UTC"
  },
  {
    "arxiv_id": "2406.07584v1",
    "title": "BrainChat: Decoding Semantic Information from fMRI using Vision-language Pretrained Models",
    "authors": [
      "Wanaiu Huang"
    ],
    "abstract": "Semantic information is vital for human interaction, and decoding it from\nbrain activity enables non-invasive clinical augmentative and alternative\ncommunication. While there has been significant progress in reconstructing\nvisual images, few studies have focused on the language aspect. To address this\ngap, leveraging the powerful capabilities of the decoder-based vision-language\npretrained model CoCa, this paper proposes BrainChat, a simple yet effective\ngenerative framework aimed at rapidly accomplishing semantic information\ndecoding tasks from brain activity, including fMRI question answering and fMRI\ncaptioning. BrainChat employs the self-supervised approach of Masked Brain\nModeling to encode sparse fMRI data, obtaining a more compact embedding\nrepresentation in the latent space. Subsequently, BrainChat bridges the gap\nbetween modalities by applying contrastive loss, resulting in aligned\nrepresentations of fMRI, image, and text embeddings. Furthermore, the fMRI\nembeddings are mapped to the generative Brain Decoder via cross-attention\nlayers, where they guide the generation of textual content about fMRI in a\nregressive manner by minimizing caption loss. Empirically, BrainChat exceeds\nthe performance of existing state-of-the-art methods in the fMRI captioning\ntask and, for the first time, implements fMRI question answering. Additionally,\nBrainChat is highly flexible and can achieve high performance without image\ndata, making it better suited for real-world scenarios with limited data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.07584v1",
    "published_date": "2024-06-10 12:06:15 UTC",
    "updated_date": "2024-06-10 12:06:15 UTC"
  },
  {
    "arxiv_id": "2406.06201v1",
    "title": "2DP-2MRC: 2-Dimensional Pointer-based Machine Reading Comprehension Method for Multimodal Moment Retrieval",
    "authors": [
      "Jiajun He",
      "Tomoki Toda"
    ],
    "abstract": "Moment retrieval aims to locate the most relevant moment in an untrimmed\nvideo based on a given natural language query. Existing solutions can be\nroughly categorized into moment-based and clip-based methods. The former often\ninvolves heavy computations, while the latter, due to overlooking\ncoarse-grained information, typically underperforms compared to moment-based\nmodels. Hence, this paper proposes a novel 2-Dimensional Pointer-based Machine\nReading Comprehension for Moment Retrieval Choice (2DP-2MRC) model to address\nthe issue of imprecise localization in clip-based methods while maintaining\nlower computational complexity than moment-based methods. Specifically, we\nintroduce an AV-Encoder to capture coarse-grained information at moment and\nvideo levels. Additionally, a 2D pointer encoder module is introduced to\nfurther enhance boundary detection for target moment. Extensive experiments on\nthe HiREST dataset demonstrate that 2DP-2MRC significantly outperforms existing\nbaseline models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06201v1",
    "published_date": "2024-06-10 11:53:29 UTC",
    "updated_date": "2024-06-10 11:53:29 UTC"
  },
  {
    "arxiv_id": "2406.06199v1",
    "title": "Implications for Governance in Public Perceptions of Societal-scale AI Risks",
    "authors": [
      "Ross Gruetzemacher",
      "Toby D. Pilditch",
      "Huigang Liang",
      "Christy Manning",
      "Vael Gates",
      "David Moss",
      "James W. B. Elsey",
      "Willem W. A. Sleegers",
      "Kyle Kilian"
    ],
    "abstract": "Amid growing concerns over AI's societal risks--ranging from civilizational\ncollapse to misinformation and systemic bias--this study explores the\nperceptions of AI experts and the general US registered voters on the\nlikelihood and impact of 18 specific AI risks, alongside their policy\npreferences for managing these risks. While both groups favor international\noversight over national or corporate governance, our survey reveals a\ndiscrepancy: voters perceive AI risks as both more likely and more impactful\nthan experts, and also advocate for slower AI development. Specifically, our\nfindings indicate that policy interventions may best assuage collective\nconcerns if they attempt to more carefully balance mitigation efforts across\nall classes of societal-scale risks, effectively nullifying the\nnear-vs-long-term debate over AI risks. More broadly, our results will serve\nnot only to enable more substantive policy discussions for preventing and\nmitigating AI risks, but also to underscore the challenge of consensus building\nfor effective policy implementation.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 18 page supplementary materials",
    "pdf_url": "http://arxiv.org/pdf/2406.06199v1",
    "published_date": "2024-06-10 11:52:25 UTC",
    "updated_date": "2024-06-10 11:52:25 UTC"
  },
  {
    "arxiv_id": "2406.06660v1",
    "title": "Space-Time Continuous PDE Forecasting using Equivariant Neural Fields",
    "authors": [
      "David M. Knigge",
      "David R. Wessels",
      "Riccardo Valperga",
      "Samuele Papa",
      "Jan-Jakob Sonke",
      "Efstratios Gavves",
      "Erik J. Bekkers"
    ],
    "abstract": "Recently, Conditional Neural Fields (NeFs) have emerged as a powerful\nmodelling paradigm for PDEs, by learning solutions as flows in the latent space\nof the Conditional NeF. Although benefiting from favourable properties of NeFs\nsuch as grid-agnosticity and space-time-continuous dynamics modelling, this\napproach limits the ability to impose known constraints of the PDE on the\nsolutions -- e.g. symmetries or boundary conditions -- in favour of modelling\nflexibility. Instead, we propose a space-time continuous NeF-based solving\nframework that - by preserving geometric information in the latent space -\nrespects known symmetries of the PDE. We show that modelling solutions as flows\nof pointclouds over the group of interest $G$ improves generalization and\ndata-efficiency. We validated that our framework readily generalizes to unseen\nspatial and temporal locations, as well as geometric transformations of the\ninitial conditions - where other NeF-based PDE forecasting methods fail - and\nimprove over baselines in a number of challenging geometries.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06660v1",
    "published_date": "2024-06-10 11:49:11 UTC",
    "updated_date": "2024-06-10 11:49:11 UTC"
  },
  {
    "arxiv_id": "2406.06192v1",
    "title": "AI Cat Narrator: Designing an AI Tool for Exploring the Shared World and Social Connection with a Cat",
    "authors": [
      "Zhenchi Lai",
      "Janet Yi-Ching Huang",
      "Rung-Huei Liang"
    ],
    "abstract": "As technology continues to advance, the interaction between humans and cats\nis becoming more diverse. Our research introduces a new tool called the AI Cat\nNarrator, which offers a unique perspective on the shared lives of humans and\ncats. We combined the method of ethnography with fictional storytelling, using\na defamiliarization strategy to merge real-world data seen through the eyes of\ncats with excerpts from cat literature. This combination serves as the\nfoundation for a database to instruct the AI Cat Narrator in crafting\nalternative narrative. Our findings indicate that using defamiliarized data for\ntraining purposes significantly contributes to the development of characters\nthat are both more empathetic and individualized. The contributions of our\nstudy are twofold: 1) proposing an innovative approach to prompting a\nreevaluation of living alongside cats; 2) establishing a collaborative,\nexploratory tool developed by humans, cats, and AI together.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.06192v1",
    "published_date": "2024-06-10 11:44:15 UTC",
    "updated_date": "2024-06-10 11:44:15 UTC"
  },
  {
    "arxiv_id": "2407.13685v1",
    "title": "Beyond Trend Following: Deep Learning for Market Trend Prediction",
    "authors": [
      "Fernando Berzal",
      "Alberto Garcia"
    ],
    "abstract": "Trend following and momentum investing are common strategies employed by\nasset managers. Even though they can be helpful in the proper situations, they\nare limited in the sense that they work just by looking at past, as if we were\ndriving with our focus on the rearview mirror. In this paper, we advocate for\nthe use of Artificial Intelligence and Machine Learning techniques to predict\nfuture market trends. These predictions, when done properly, can improve the\nperformance of asset managers by increasing returns and reducing drawdowns.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.LG",
      "q-fin.CP"
    ],
    "primary_category": "q-fin.TR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13685v1",
    "published_date": "2024-06-10 11:42:30 UTC",
    "updated_date": "2024-06-10 11:42:30 UTC"
  },
  {
    "arxiv_id": "2406.06184v2",
    "title": "Deep Multi-Objective Reinforcement Learning for Utility-Based Infrastructural Maintenance Optimization",
    "authors": [
      "Jesse van Remmerden",
      "Maurice Kenter",
      "Diederik M. Roijers",
      "Charalampos Andriotis",
      "Yingqian Zhang",
      "Zaharah Bukhsh"
    ],
    "abstract": "In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent\nActor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL)\nmethod for infrastructural maintenance optimization, an area traditionally\ndominated by single-objective reinforcement learning (RL) approaches. Previous\nsingle-objective RL methods combine multiple objectives, such as probability of\ncollapse and cost, into a singular reward signal through reward-shaping. In\ncontrast, MO-DCMAC can optimize a policy for multiple objectives directly, even\nwhen the utility function is non-linear. We evaluated MO-DCMAC using two\nutility functions, which use probability of collapse and cost as input. The\nfirst utility function is the Threshold utility, in which MO-DCMAC should\nminimize cost so that the probability of collapse is never above the threshold.\nThe second is based on the Failure Mode, Effects, and Criticality Analysis\n(FMECA) methodology used by asset managers to asses maintenance plans. We\nevaluated MO-DCMAC, with both utility functions, in multiple maintenance\nenvironments, including ones based on a case study of the historical quay walls\nof Amsterdam. The performance of MO-DCMAC was compared against multiple\nrule-based policies based on heuristics currently used for constructing\nmaintenance plans. Our results demonstrate that MO-DCMAC outperforms\ntraditional rule-based policies across various environments and utility\nfunctions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in the Neural Computing and Applications: Topical Collection\n  on Multi-Objective Decision Making 2023 (MODeM 2023)",
    "pdf_url": "http://arxiv.org/pdf/2406.06184v2",
    "published_date": "2024-06-10 11:28:25 UTC",
    "updated_date": "2025-01-08 15:28:11 UTC"
  },
  {
    "arxiv_id": "2406.06658v1",
    "title": "Link Prediction in Bipartite Networks",
    "authors": [
      "Şükrü Demir İnan Özer",
      "Günce Keziban Orman",
      "Vincent Labatut"
    ],
    "abstract": "Bipartite networks serve as highly suitable models to represent systems\ninvolving interactions between two distinct types of entities, such as online\ndating platforms, job search services, or ecommerce websites. These models can\nbe leveraged to tackle a number of tasks, including link prediction among the\nmost useful ones, especially to design recommendation systems. However, if this\ntask has garnered much interest when conducted on unipartite (i.e. standard)\nnetworks, it is far from being the case for bipartite ones. In this study, we\naddress this gap by performing an experimental comparison of 19 link prediction\nmethods able to handle bipartite graphs. Some come directly from the\nliterature, and some are adapted by us from techniques originally designed for\nunipartite networks. We also propose to repurpose recommendation systems based\non graph convolutional networks (GCN) as a novel link prediction solution for\nbipartite networks. To conduct our experiments, we constitute a benchmark of 3\nreal-world bipartite network datasets with various topologies. Our results\nindicate that GCN-based personalized recommendation systems, which have\nreceived significant attention in recent years, can produce successful results\nfor link prediction in bipartite networks. Furthermore, purely heuristic\nmetrics that do not rely on any learning process, like the Structural\nPerturbation Method (SPM), can also achieve success.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.SI",
    "comment": "28th International Conference on Knowledge-Based and Intelligent\n  Information & Engineering Systems (KES), Sep 2024, Sevilla, Spain",
    "pdf_url": "http://arxiv.org/pdf/2406.06658v1",
    "published_date": "2024-06-10 11:23:30 UTC",
    "updated_date": "2024-06-10 11:23:30 UTC"
  },
  {
    "arxiv_id": "2406.06657v1",
    "title": "Harnessing AI for efficient analysis of complex policy documents: a case study of Executive Order 14110",
    "authors": [
      "Mark A. Kramer",
      "Allen Leavens",
      "Alexander Scarlat"
    ],
    "abstract": "Policy documents, such as legislation, regulations, and executive orders, are\ncrucial in shaping society. However, their length and complexity make\ninterpretation and application challenging and time-consuming. Artificial\nintelligence (AI), particularly large language models (LLMs), has the potential\nto automate the process of analyzing these documents, improving accuracy and\nefficiency. This study aims to evaluate the potential of AI in streamlining\npolicy analysis and to identify the strengths and limitations of current AI\napproaches. The research focuses on question answering and tasks involving\ncontent extraction from policy documents. A case study was conducted using\nExecutive Order 14110 on \"Safe, Secure, and Trustworthy Development and Use of\nArtificial Intelligence\" as a test case. Four commercial AI systems were used\nto analyze the document and answer a set of representative policy questions.\nThe performance of the AI systems was compared to manual analysis conducted by\nhuman experts. The study found that two AI systems, Gemini 1.5 Pro and Claude 3\nOpus, demonstrated significant potential for supporting policy analysis,\nproviding accurate and reliable information extraction from complex documents.\nThey performed comparably to human analysts but with significantly higher\nefficiency. However, achieving reproducibility remains a challenge,\nnecessitating further research and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.06657v1",
    "published_date": "2024-06-10 11:19:28 UTC",
    "updated_date": "2024-06-10 11:19:28 UTC"
  },
  {
    "arxiv_id": "2406.06165v1",
    "title": "Generalized Nested Latent Variable Models for Lossy Coding applied to Wind Turbine Scenarios",
    "authors": [
      "Raül Pérez-Gonzalo",
      "Andreas Espersen",
      "Antonio Agudo"
    ],
    "abstract": "Rate-distortion optimization through neural networks has accomplished\ncompetitive results in compression efficiency and image quality. This\nlearning-based approach seeks to minimize the compromise between compression\nrate and reconstructed image quality by automatically extracting and retaining\ncrucial information, while discarding less critical details. A successful\ntechnique consists in introducing a deep hyperprior that operates within a\n2-level nested latent variable model, enhancing compression by capturing\ncomplex data dependencies. This paper extends this concept by designing a\ngeneralized L-level nested generative model with a Markov chain structure. We\ndemonstrate as L increases that a trainable prior is detrimental and explore a\ncommon dimensionality along the distinct latent variables to boost compression\nperformance. As this structured framework can represent autoregressive coders,\nwe outperform the hyperprior model and achieve state-of-the-art performance\nwhile reducing substantially the computational cost. Our experimental\nevaluation is performed on wind turbine scenarios to study its application on\nvisual inspections",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06165v1",
    "published_date": "2024-06-10 11:00:26 UTC",
    "updated_date": "2024-06-10 11:00:26 UTC"
  },
  {
    "arxiv_id": "2406.06158v2",
    "title": "Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning",
    "authors": [
      "Daniel Kunin",
      "Allan Raventós",
      "Clémentine Dominé",
      "Feng Chen",
      "David Klindt",
      "Andrew Saxe",
      "Surya Ganguli"
    ],
    "abstract": "While the impressive performance of modern neural networks is often\nattributed to their capacity to efficiently extract task-relevant features from\ndata, the mechanisms underlying this rich feature learning regime remain\nelusive, with much of our theoretical understanding stemming from the opposing\nlazy regime. In this work, we derive exact solutions to a minimal model that\ntransitions between lazy and rich learning, precisely elucidating how\nunbalanced layer-specific initialization variances and learning rates determine\nthe degree of feature learning. Our analysis reveals that they conspire to\ninfluence the learning regime through a set of conserved quantities that\nconstrain and modify the geometry of learning trajectories in parameter and\nfunction space. We extend our analysis to more complex linear models with\nmultiple neurons, outputs, and layers and to shallow nonlinear networks with\npiecewise linear activation functions. In linear networks, rapid feature\nlearning only occurs from balanced initializations, where all layers learn at\nsimilar speeds. While in nonlinear networks, unbalanced initializations that\npromote faster learning in earlier layers can accelerate rich learning. Through\na series of experiments, we provide evidence that this unbalanced rich regime\ndrives feature learning in deep finite-width networks, promotes\ninterpretability of early layers in CNNs, reduces the sample complexity of\nlearning hierarchical data, and decreases the time to grokking in modular\narithmetic. Our theory motivates further exploration of unbalanced\ninitializations to enhance efficient feature learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages, 12 figures, NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06158v2",
    "published_date": "2024-06-10 10:42:37 UTC",
    "updated_date": "2024-10-12 21:38:28 UTC"
  },
  {
    "arxiv_id": "2406.06144v3",
    "title": "Language Models Resist Alignment: Evidence From Data Compression",
    "authors": [
      "Jiaming Ji",
      "Kaile Wang",
      "Tianyi Qiu",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Changye Li",
      "Hantao Lou",
      "Josef Dai",
      "Yunhuai Liu",
      "Yaodong Yang"
    ],
    "abstract": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the elasticity of post-alignment\nmodels, i.e., the tendency to revert to the behavior distribution formed during\nthe pre-training phase upon further fine-tuning. Leveraging compression theory,\nwe formally deduce that fine-tuning disproportionately undermines alignment\nrelative to pre-training, potentially by orders of magnitude. We validate the\npresence of elasticity through experiments on models of varying types and\nscales. Specifically, we find that model performance declines rapidly before\nreverting to the pre-training distribution, after which the rate of decline\ndrops significantly. Furthermore, we further reveal that elasticity positively\ncorrelates with the increased model size and the expansion of pre-training\ndata. Our findings underscore the need to address the inherent elasticity of\nLLMs to mitigate their resistance to alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The five-page version has been accepted by NeurIPS 2024 Workshop\n  SoLaR. In the current version, we have conducted an in-depth expansion of\n  both the theoretical and experimental aspects",
    "pdf_url": "http://arxiv.org/pdf/2406.06144v3",
    "published_date": "2024-06-10 10:03:16 UTC",
    "updated_date": "2024-12-20 16:25:12 UTC"
  },
  {
    "arxiv_id": "2406.06655v1",
    "title": "Fed-Sophia: A Communication-Efficient Second-Order Federated Learning Algorithm",
    "authors": [
      "Ahmed Elbakary",
      "Chaouki Ben Issaid",
      "Mohammad Shehab",
      "Karim Seddik",
      "Tamer ElBatt",
      "Mehdi Bennis"
    ],
    "abstract": "Federated learning is a machine learning approach where multiple devices\ncollaboratively learn with the help of a parameter server by sharing only their\nlocal updates. While gradient-based optimization techniques are widely adopted\nin this domain, the curvature information that second-order methods exhibit is\ncrucial to guide and speed up the convergence. This paper introduces a scalable\nsecond-order method, allowing the adoption of curvature information in\nfederated large models. Our method, coined Fed-Sophia, combines a weighted\nmoving average of the gradient with a clipping operation to find the descent\ndirection. In addition to that, a lightweight estimation of the Hessian's\ndiagonal is used to incorporate the curvature information. Numerical evaluation\nshows the superiority, robustness, and scalability of the proposed Fed-Sophia\nscheme compared to first and second-order baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICC 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06655v1",
    "published_date": "2024-06-10 09:57:30 UTC",
    "updated_date": "2024-06-10 09:57:30 UTC"
  },
  {
    "arxiv_id": "2406.06139v1",
    "title": "Thunder : Unified Regression-Diffusion Speech Enhancement with a Single Reverse Step using Brownian Bridge",
    "authors": [
      "Thanapat Trachu",
      "Chawan Piansaddhayanon",
      "Ekapol Chuangsuwanich"
    ],
    "abstract": "Diffusion-based speech enhancement has shown promising results, but can\nsuffer from a slower inference time. Initializing the diffusion process with\nthe enhanced audio generated by a regression-based model can be used to reduce\nthe computational steps required. However, these approaches often necessitate a\nregression model, further increasing the system's complexity. We propose\nThunder, a unified regression-diffusion model that utilizes the Brownian bridge\nprocess which can allow the model to act in both modes. The regression mode can\nbe accessed by setting the diffusion time step closed to 1. However, the\nstandard score-based diffusion modeling does not perform well in this setup due\nto gradient instability. To mitigate this problem, we modify the diffusion\nmodel to predict the clean speech instead of the score function, achieving\ncompetitive performance with a more compact model size and fewer reverse steps.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures, 4 tables, This paper will be submitted in the\n  interspeech conference",
    "pdf_url": "http://arxiv.org/pdf/2406.06139v1",
    "published_date": "2024-06-10 09:52:25 UTC",
    "updated_date": "2024-06-10 09:52:25 UTC"
  },
  {
    "arxiv_id": "2406.06134v1",
    "title": "DiffInject: Revisiting Debias via Synthetic Data Generation using Diffusion-based Style Injection",
    "authors": [
      "Donggeun Ko",
      "Sangwoo Jo",
      "Dongjun Lee",
      "Namjun Park",
      "Jaekwang Kim"
    ],
    "abstract": "Dataset bias is a significant challenge in machine learning, where specific\nattributes, such as texture or color of the images are unintentionally learned\nresulting in detrimental performance. To address this, previous efforts have\nfocused on debiasing models either by developing novel debiasing algorithms or\nby generating synthetic data to mitigate the prevalent dataset biases. However,\ngenerative approaches to date have largely relied on using bias-specific\nsamples from the dataset, which are typically too scarce. In this work, we\npropose, DiffInject, a straightforward yet powerful method to augment synthetic\nbias-conflict samples using a pretrained diffusion model. This approach\nsignificantly advances the use of diffusion models for debiasing purposes by\nmanipulating the latent space. Our framework does not require any explicit\nknowledge of the bias types or labelling, making it a fully unsupervised\nsetting for debiasing. Our methodology demonstrates substantial result in\neffectively reducing dataset bias.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages (including supplementary), 3 figures, SynData4CV@CVPR 24\n  (Workshop)",
    "pdf_url": "http://arxiv.org/pdf/2406.06134v1",
    "published_date": "2024-06-10 09:45:38 UTC",
    "updated_date": "2024-06-10 09:45:38 UTC"
  },
  {
    "arxiv_id": "2406.06127v1",
    "title": "Comparing Data Augmentation Methods for End-to-End Task-Oriented Dialog Systems",
    "authors": [
      "Christos Vlachos",
      "Themos Stafylakis",
      "Ion Androutsopoulos"
    ],
    "abstract": "Creating effective and reliable task-oriented dialog systems (ToDSs) is\nchallenging, not only because of the complex structure of these systems, but\nalso due to the scarcity of training data, especially when several modules need\nto be trained separately, each one with its own input/output training examples.\nData augmentation (DA), whereby synthetic training examples are added to the\ntraining data, has been successful in other NLP systems, but has not been\nexplored as extensively in ToDSs. We empirically evaluate the effectiveness of\nDA methods in an end-to-end ToDS setting, where a single system is trained to\nhandle all processing stages, from user inputs to system outputs. We experiment\nwith two ToDSs (UBAR, GALAXY) on two datasets (MultiWOZ, KVRET). We consider\nthree types of DA methods (word-level, sentence-level, dialog-level), comparing\neight DA methods that have shown promising results in ToDSs and other NLP\nsystems. We show that all DA methods considered are beneficial, and we\nhighlight the best ones, also providing advice to practitioners. We also\nintroduce a more challenging few-shot cross-domain ToDS setting, reaching\nsimilar conclusions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "There are 25 pages in total, 23 tables, 18 figures. Accepted in ACL\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06127v1",
    "published_date": "2024-06-10 09:36:05 UTC",
    "updated_date": "2024-06-10 09:36:05 UTC"
  },
  {
    "arxiv_id": "2406.07583v1",
    "title": "Situated Ground Truths: Enhancing Bias-Aware AI by Situating Data Labels with SituAnnotate",
    "authors": [
      "Delfina Sol Martinez Pandiani",
      "Valentina Presutti"
    ],
    "abstract": "In the contemporary world of AI and data-driven applications, supervised\nmachines often derive their understanding, which they mimic and reproduce,\nthrough annotations--typically conveyed in the form of words or labels.\nHowever, such annotations are often divorced from or lack contextual\ninformation, and as such hold the potential to inadvertently introduce biases\nwhen subsequently used for training. This paper introduces SituAnnotate, a\nnovel ontology explicitly crafted for 'situated grounding,' aiming to anchor\nthe ground truth data employed in training AI systems within the contextual and\nculturally-bound situations from which those ground truths emerge. SituAnnotate\noffers an ontology-based approach to structured and context-aware data\nannotation, addressing potential bias issues associated with isolated\nannotations. Its representational power encompasses situational context,\nincluding annotator details, timing, location, remuneration schemes, annotation\nroles, and more, ensuring semantic richness. Aligned with the foundational\nDolce Ultralight ontology, it provides a robust and consistent framework for\nknowledge representation. As a method to create, query, and compare label-based\ndatasets, SituAnnotate empowers downstream AI systems to undergo training with\nexplicit consideration of context and cultural bias, laying the groundwork for\nenhanced system interpretability and adaptability, and enabling AI models to\nalign with a multitude of cultural contexts and viewpoints.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "Author preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.07583v1",
    "published_date": "2024-06-10 09:33:13 UTC",
    "updated_date": "2024-06-10 09:33:13 UTC"
  },
  {
    "arxiv_id": "2406.06124v1",
    "title": "Enhancing Long-Term Memory using Hierarchical Aggregate Tree for Retrieval Augmented Generation",
    "authors": [
      "Aadharsh Aadhithya A",
      "Sachin Kumar S",
      "Soman K. P"
    ],
    "abstract": "Large language models have limited context capacity, hindering reasoning over\nlong conversations. We propose the Hierarchical Aggregate Tree memory structure\nto recursively aggregate relevant dialogue context through conditional tree\ntraversals. HAT encapsulates information from children nodes, enabling broad\ncoverage with depth control. We formulate finding best context as optimal tree\ntraversal. Experiments show HAT improves dialog coherence and summary quality\nover baseline contexts, demonstrating the techniques effectiveness for multi\nturn reasoning without exponential parameter growth. This memory augmentation\nenables more consistent, grounded longform conversations from LLMs",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06124v1",
    "published_date": "2024-06-10 09:29:08 UTC",
    "updated_date": "2024-06-10 09:29:08 UTC"
  },
  {
    "arxiv_id": "2406.06652v3",
    "title": "Improving Generalization of Neural Vehicle Routing Problem Solvers Through the Lens of Model Architecture",
    "authors": [
      "Yubin Xiao",
      "Di Wang",
      "Xuan Wu",
      "Yuesong Wu",
      "Boyang Li",
      "Wei Du",
      "Liupu Wang",
      "You Zhou"
    ],
    "abstract": "Neural models produce promising results when solving Vehicle Routing Problems\n(VRPs), but often fall short in generalization. Recent attempts to enhance\nmodel generalization often incur unnecessarily large training cost or cannot be\ndirectly applied to other models solving different VRP variants. To address\nthese issues, we take a novel perspective on model architecture in this study.\nSpecifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and\na Distribution-Specific (DS) decoder to enhance the size and distribution\ngeneralization, respectively. ESF adjusts the attention weight pattern of the\nmodel towards familiar ones discovered during training when solving VRPs of\nvarying sizes. The DS decoder explicitly models VRPs of multiple training\ndistribution patterns through multiple auxiliary light decoders, expanding the\nmodel representation space to encompass a broader range of distributional\nscenarios. We conduct extensive experiments on both synthetic and widely\nrecognized real-world benchmarking datasets and compare the performance with\nseven baseline models. The results demonstrate the effectiveness of using ESF\nand DS decoder to obtain a more generalizable model and showcase their\napplicability to solve different VRP variants, i.e., travelling salesman\nproblem and capacitated VRP. Notably, our proposed generic components require\nminimal computational resources, and can be effortlessly integrated into\nconventional generalization strategies to further elevate model generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted by Neural Networks",
    "pdf_url": "http://arxiv.org/pdf/2406.06652v3",
    "published_date": "2024-06-10 09:03:17 UTC",
    "updated_date": "2025-03-18 08:40:04 UTC"
  },
  {
    "arxiv_id": "2406.06111v1",
    "title": "JenGAN: Stacked Shifted Filters in GAN-Based Speech Synthesis",
    "authors": [
      "Hyunjae Cho",
      "Junhyeok Lee",
      "Wonbin Jung"
    ],
    "abstract": "Non-autoregressive GAN-based neural vocoders are widely used due to their\nfast inference speed and high perceptual quality. However, they often suffer\nfrom audible artifacts such as tonal artifacts in their generated results.\nTherefore, we propose JenGAN, a new training strategy that involves stacking\nshifted low-pass filters to ensure the shift-equivariant property. This method\nhelps prevent aliasing and reduce artifacts while preserving the model\nstructure used during inference. In our experimental evaluation, JenGAN\nconsistently enhances the performance of vocoder models, yielding significantly\nsuperior scores across the majority of evaluation metrics.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06111v1",
    "published_date": "2024-06-10 08:51:04 UTC",
    "updated_date": "2024-06-10 08:51:04 UTC"
  },
  {
    "arxiv_id": "2406.06110v1",
    "title": "Recurrent Context Compression: Efficiently Expanding the Context Window of LLM",
    "authors": [
      "Chensen Huang",
      "Guibo Zhu",
      "Xuepeng Wang",
      "Yifei Luo",
      "Guojing Ge",
      "Haoran Chen",
      "Dong Yi",
      "Jinqiao Wang"
    ],
    "abstract": "To extend the context length of Transformer-based large language models\n(LLMs) and improve comprehension capabilities, we often face limitations due to\ncomputational resources and bounded memory storage capacity. This work\nintroduces a method called Recurrent Context Compression (RCC), designed to\nefficiently expand the context window length of LLMs within constrained storage\nspace. We also investigate the issue of poor model responses when both\ninstructions and context are compressed in downstream tasks, and propose an\ninstruction reconstruction method to mitigate this problem. We validated the\neffectiveness of our approach on multiple tasks, achieving a compression rate\nof up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and\nnearly 100\\% accuracy on a passkey retrieval task with a sequence length of 1M.\nFinally, our method demonstrated competitive performance in long-text\nquestion-answering tasks compared to non-compressed methods, while\nsignificantly saving storage resources in long-text inference tasks. Our code,\nmodels, and demo are available at https://github.com/WUHU-G/RCC_Transformer",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06110v1",
    "published_date": "2024-06-10 08:50:59 UTC",
    "updated_date": "2024-06-10 08:50:59 UTC"
  },
  {
    "arxiv_id": "2406.06107v1",
    "title": "EXPIL: Explanatory Predicate Invention for Learning in Games",
    "authors": [
      "Jingyuan Sha",
      "Hikaru Shindo",
      "Quentin Delfosse",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ],
    "abstract": "Reinforcement learning (RL) has proven to be a powerful tool for training\nagents that excel in various games. However, the black-box nature of neural\nnetwork models often hinders our ability to understand the reasoning behind the\nagent's actions. Recent research has attempted to address this issue by using\nthe guidance of pretrained neural agents to encode logic-based policies,\nallowing for interpretable decisions. A drawback of such approaches is the\nrequirement of large amounts of predefined background knowledge in the form of\npredicates, limiting its applicability and scalability. In this work, we\npropose a novel approach, Explanatory Predicate Invention for Learning in Games\n(EXPIL), that identifies and extracts predicates from a pretrained neural\nagent, later used in the logic-based agents, reducing the dependency on\npredefined background knowledge. Our experimental evaluation on various games\ndemonstrate the effectiveness of EXPIL in achieving explainable behavior in\nlogic agents while requiring less background knowledge.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 pages references, 8 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.06107v1",
    "published_date": "2024-06-10 08:46:49 UTC",
    "updated_date": "2024-06-10 08:46:49 UTC"
  },
  {
    "arxiv_id": "2406.06103v1",
    "title": "Adaptive Control in Assistive Application -- A Study Evaluating Shared Control by Users with Limited Upper Limb Mobility",
    "authors": [
      "Felix Ferdinand Goldau",
      "Max Pascher",
      "Annalies Baumeister",
      "Patrizia Tolle",
      "Jens Gerken",
      "Udo Frese"
    ],
    "abstract": "Shared control in assistive robotics blends human autonomy with computer\nassistance, thus simplifying complex tasks for individuals with physical\nimpairments. This study assesses an adaptive Degrees of Freedom control method\nspecifically tailored for individuals with upper limb impairments. It employs a\nbetween-subjects analysis with 24 participants, conducting 81 trials across\nthree distinct input devices in a realistic everyday-task setting. Given the\ndiverse capabilities of the vulnerable target demographic and the known\nchallenges in statistical comparisons due to individual differences, the study\nfocuses primarily on subjective qualitative data. The results reveal\nconsistently high success rates in trial completions, irrespective of the input\ndevice used. Participants appreciated their involvement in the research\nprocess, displayed a positive outlook, and quick adaptability to the control\nsystem. Notably, each participant effectively managed the given task within a\nshort time frame.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "RO-MAN'24: 33rd IEEE International Conference on Robot and Human\n  Interactive Communication, Pasadena, California, US",
    "pdf_url": "http://arxiv.org/pdf/2406.06103v1",
    "published_date": "2024-06-10 08:36:55 UTC",
    "updated_date": "2024-06-10 08:36:55 UTC"
  },
  {
    "arxiv_id": "2406.06097v1",
    "title": "StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection",
    "authors": [
      "Sara Papi",
      "Marco Gaido",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "abstract": "Streaming speech-to-text translation (StreamST) is the task of automatically\ntranslating speech while incrementally receiving an audio stream. Unlike\nsimultaneous ST (SimulST), which deals with pre-segmented speech, StreamST\nfaces the challenges of handling continuous and unbounded audio streams. This\nrequires additional decisions about what to retain of the previous history,\nwhich is impractical to keep entirely due to latency and computational\nconstraints. Despite the real-world demand for real-time ST, research on\nstreaming translation remains limited, with existing works solely focusing on\nSimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy,\nand propose StreamLAAL, the first StreamST latency metric designed to be\ncomparable with existing metrics for SimulST. Extensive experiments across all\n8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a\nnaive streaming baseline and the related state-of-the-art SimulST policy,\nproviding a first step in StreamST research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ACL 2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2406.06097v1",
    "published_date": "2024-06-10 08:27:58 UTC",
    "updated_date": "2024-06-10 08:27:58 UTC"
  },
  {
    "arxiv_id": "2406.10251v3",
    "title": "The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs",
    "authors": [
      "Mert Yazan",
      "Suzan Verberne",
      "Frederik Situmeang"
    ],
    "abstract": "Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the IR-RAG Workshop at SIGIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.10251v3",
    "published_date": "2024-06-10 08:23:52 UTC",
    "updated_date": "2024-08-01 16:27:20 UTC"
  },
  {
    "arxiv_id": "2406.06062v2",
    "title": "ProcessPainter: Learn Painting Process from Sequence Data",
    "authors": [
      "Yiren Song",
      "Shijie Huang",
      "Chen Yao",
      "Xiaojun Ye",
      "Hai Ci",
      "Jiaming Liu",
      "Yuxuan Zhang",
      "Mike Zheng Shou"
    ],
    "abstract": "The painting process of artists is inherently stepwise and varies\nsignificantly among different painters and styles. Generating detailed,\nstep-by-step painting processes is essential for art education and research,\nyet remains largely underexplored. Traditional stroke-based rendering methods\nbreak down images into sequences of brushstrokes, yet they fall short of\nreplicating the authentic processes of artists, with limitations confined to\nbasic brushstroke modifications. Text-to-image models utilizing diffusion\nprocesses generate images through iterative denoising, also diverge\nsubstantially from artists' painting process. To address these challenges, we\nintroduce ProcessPainter, a text-to-video model that is initially pre-trained\non synthetic data and subsequently fine-tuned with a select set of artists'\npainting sequences using the LoRA model. This approach successfully generates\npainting processes from text prompts for the first time. Furthermore, we\nintroduce an Artwork Replication Network capable of accepting arbitrary-frame\ninput, which facilitates the controlled generation of painting processes,\ndecomposing images into painting sequences, and completing semi-finished\nartworks. This paper offers new perspectives and tools for advancing art\neducation and image generation technology.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06062v2",
    "published_date": "2024-06-10 07:18:41 UTC",
    "updated_date": "2024-07-20 07:23:19 UTC"
  },
  {
    "arxiv_id": "2406.06061v1",
    "title": "Greedy SLIM: A SLIM-Based Approach For Preference Elicitation",
    "authors": [
      "Claudius Proissl",
      "Amel Vatic",
      "Helmut Waldschmidt"
    ],
    "abstract": "Preference elicitation is an active learning approach to tackle the\ncold-start problem of recommender systems. Roughly speaking, new users are\nasked to rate some carefully selected items in order to compute appropriate\nrecommendations for them. To the best of our knowledge, we are the first to\npropose a method for preference elicitation that is based on SLIM , a\nstate-of-the-art technique for top-N recommendation. Our approach mainly\nconsists of a new training technique for SLIM, which we call Greedy SLIM. This\ntechnique iteratively selects items for the training in order to minimize the\nSLIM loss greedily. We conduct offline experiments as well as a user study to\nassess the performance of this new method. The results are remarkable,\nespecially with respect to the user study. We conclude that Greedy SLIM seems\nto be more suitable for preference elicitation than widely used methods based\non latent factor models.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06061v1",
    "published_date": "2024-06-10 07:18:24 UTC",
    "updated_date": "2024-06-10 07:18:24 UTC"
  },
  {
    "arxiv_id": "2406.12902v2",
    "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models",
    "authors": [
      "Jialun Cao",
      "Zhiyong Chen",
      "Jiarong Wu",
      "Shing-chi Cheung",
      "Chang Xu"
    ],
    "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate\nLLMs' capabilities. However, after consolidating the latest 24 benchmarks, we\nnoticed three significant imbalances. First, imbalanced programming language.\n95.8% of benchmarks involve Python, while only 5 benchmarks involve Java.\nSecond, imbalanced code granularity. Function-/statement-level benchmarks\naccount for over 83.3% of benchmarks. Only a mere handful extends to\nclass-/project-levels, and all are limited to Python. Third, lacking advanced\nfeatures. Existing benchmarks primarily assess basic coding skills, while\noverlooking advanced Object-Oriented Programming (OOP) features (i.e.,\nencapsulation, inheritance, and polymorphism).\n  To fill these gaps, we propose JavaBench, a project-level Java benchmark that\nexercises OOP features. It comprises four Java projects with 389 methods in 106\nJava classes. The test coverage is up to 92%, and JavaBench is attested by 282\nundergraduate students, reaching a 90.93/100 average score (i.e., pass rate\nagainst the test suite), ensuring the quality of documentation, code skeleton,\nand tests. To better evaluate LLM's capability against JavaBench, we introduce\na systematic evaluation design covering three context settings and five\nsynthesis strategies at two granularities using three hierarchical metrics. Our\nextensive experiment yields several interesting findings. First, we noticed\nthat regarding project-level Java programming, LLMs are far behind\nundergraduate students (no project can be correctly completed by any studied\nLLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using\nmethod signature as prompt context may strike an ideal balance for\nproject-level code generation. JavaBench is publicly available at\nhttps://github.com/java-bench/JavaBench.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ASE 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12902v2",
    "published_date": "2024-06-10 06:43:25 UTC",
    "updated_date": "2024-10-11 10:27:16 UTC"
  },
  {
    "arxiv_id": "2406.06051v2",
    "title": "On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration",
    "authors": [
      "Guanghui Yu",
      "Robert Kasumba",
      "Chien-Ju Ho",
      "William Yeoh"
    ],
    "abstract": "To enable effective human-AI collaboration, merely optimizing AI performance\nwithout considering human factors is insufficient. Recent research has shown\nthat designing AI agents that take human behavior into account leads to\nimproved performance in human-AI collaboration. However, a limitation of most\nexisting approaches is their assumption that human behavior remains static,\nregardless of the AI agent's actions. In reality, humans may adjust their\nactions based on their beliefs about the AI's intentions, specifically, the\nsubtasks they perceive the AI to be attempting to complete based on its\nbehavior. In this paper, we address this limitation by enabling a collaborative\nAI agent to consider its human partner's beliefs about its intentions, i.e.,\nwhat the human partner thinks the AI agent is trying to accomplish, and to\ndesign its action plan accordingly to facilitate more effective human-AI\ncollaboration. Specifically, we developed a model of human beliefs that\ncaptures how humans interpret and reason about their AI partner's intentions.\nUsing this belief model, we created an AI agent that incorporates both human\nbehavior and human beliefs when devising its strategy for interacting with\nhumans. Through extensive real-world human-subject experiments, we demonstrate\nthat our belief model more accurately captures human perceptions of AI\nintentions. Furthermore, we show that our AI agent, designed to account for\nhuman beliefs over its intentions, significantly enhances performance in\nhuman-AI collaboration.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06051v2",
    "published_date": "2024-06-10 06:39:37 UTC",
    "updated_date": "2024-11-08 21:57:19 UTC"
  },
  {
    "arxiv_id": "2406.06048v2",
    "title": "Robust Latent Representation Tuning for Image-text Classification",
    "authors": [
      "Hao Sun",
      "Yu Song"
    ],
    "abstract": "Large models have demonstrated exceptional generalization capabilities in\ncomputer vision and natural language processing. Recent efforts have focused on\nenhancing these models with multimodal processing abilities. However,\naddressing the challenges posed by scenarios where one modality is absent\nremains a significant hurdle. In response to this issue, we propose a robust\nlatent representation tuning method for large models. Specifically, our\napproach introduces a modality latent translation module to maximize the\ncorrelation between modalities, resulting in a robust representation. Following\nthis, a newly designed fusion module is employed to facilitate information\ninteraction between the modalities. Within this framework, common semantics are\nrefined during training, and robust performance is achieved even in the absence\nof one modality. Importantly, our method maintains the frozen state of the\nimage and text foundation models to preserve their capabilities acquired\nthrough large-scale pretraining. We conduct experiments on several public\ndatasets, and the results underscore the effectiveness of our proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06048v2",
    "published_date": "2024-06-10 06:29:00 UTC",
    "updated_date": "2024-06-14 12:29:19 UTC"
  },
  {
    "arxiv_id": "2406.06045v1",
    "title": "Synthesizing Efficient Data with Diffusion Models for Person Re-Identification Pre-Training",
    "authors": [
      "Ke Niu",
      "Haiyang Yu",
      "Xuelin Qian",
      "Teng Fu",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "abstract": "Existing person re-identification (Re-ID) methods principally deploy the\nImageNet-1K dataset for model initialization, which inevitably results in\nsub-optimal situations due to the large domain gap. One of the key challenges\nis that building large-scale person Re-ID datasets is time-consuming. Some\nprevious efforts address this problem by collecting person images from the\ninternet e.g., LUPerson, but it struggles to learn from unlabeled,\nuncontrollable, and noisy data. In this paper, we present a novel paradigm\nDiffusion-ReID to efficiently augment and generate diverse images based on\nknown identities without requiring any cost of data collection and annotation.\nTechnically, this paradigm unfolds in two stages: generation and filtering.\nDuring the generation stage, we propose Language Prompts Enhancement (LPE) to\nensure the ID consistency between the input image sequence and the generated\nimages. In the diffusion process, we propose a Diversity Injection (DI) module\nto increase attribute diversity. In order to make the generated data have\nhigher quality, we apply a Re-ID confidence threshold filter to further remove\nthe low-quality images. Benefiting from our proposed paradigm, we first create\na new large-scale person Re-ID dataset Diff-Person, which consists of over 777K\nimages from 5,183 identities. Next, we build a stronger person Re-ID backbone\npre-trained on our Diff-Person. Extensive experiments are conducted on four\nperson Re-ID benchmarks in six widely used settings. Compared with other\npre-training and self-supervised competitors, our approach shows significant\nsuperiority.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06045v1",
    "published_date": "2024-06-10 06:26:03 UTC",
    "updated_date": "2024-06-10 06:26:03 UTC"
  },
  {
    "arxiv_id": "2406.06037v1",
    "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning",
    "authors": [
      "Donghu Kim",
      "Hojoon Lee",
      "Kyungmin Lee",
      "Dongyoon Hwang",
      "Jaegul Choo"
    ],
    "abstract": "Recently, various pre-training methods have been introduced in vision-based\nReinforcement Learning (RL). However, their generalization ability remains\nunclear due to evaluations being limited to in-distribution environments and\nnon-unified experimental setups. To address this, we introduce the Atari\nPre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10\nmillion transitions from 50 Atari games and evaluates it across diverse\nenvironment distributions. Our experiments show that pre-training objectives\nfocused on learning task-agnostic features (e.g., identifying objects and\nunderstanding temporal dynamics) enhance generalization across different\nenvironments. In contrast, objectives focused on learning task-specific\nknowledge (e.g., identifying agents and fitting reward functions) improve\nperformance in environments similar to the pre-training dataset but not in\nvaried ones. We publicize our codes, datasets, and model checkpoints at\nhttps://github.com/dojeon-ai/Atari-PB.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted to ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06037v1",
    "published_date": "2024-06-10 06:06:38 UTC",
    "updated_date": "2024-06-10 06:06:38 UTC"
  },
  {
    "arxiv_id": "2406.06649v1",
    "title": "2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution",
    "authors": [
      "Kai Liu",
      "Haotong Qin",
      "Yong Guo",
      "Xin Yuan",
      "Linghe Kong",
      "Guihai Chen",
      "Yulun Zhang"
    ],
    "abstract": "Low-bit quantization has become widespread for compressing image\nsuper-resolution (SR) models for edge deployment, which allows advanced SR\nmodels to enjoy compact low-bit parameters and efficient integer/bitwise\nconstructions for storage compression and inference acceleration, respectively.\nHowever, it is notorious that low-bit quantization degrades the accuracy of SR\nmodels compared to their full-precision (FP) counterparts. Despite several\nefforts to alleviate the degradation, the transformer-based SR model still\nsuffers severe degradation due to its distinctive activation distribution. In\nthis work, we present a dual-stage low-bit post-training quantization (PTQ)\nmethod for image super-resolution, namely 2DQuant, which achieves efficient and\naccurate SR under low-bit quantization. The proposed method first investigates\nthe weight and activation and finds that the distribution is characterized by\ncoexisting symmetry and asymmetry, long tails. Specifically, we propose\nDistribution-Oriented Bound Initialization (DOBI), using different searching\nstrategies to search a coarse bound for quantizers. To obtain refined quantizer\nparameters, we further propose Distillation Quantization Calibration (DQC),\nwhich employs a distillation approach to make the quantized model learn from\nits FP counterpart. Through extensive experiments on different bits and scaling\nfactors, the performance of DOBI can reach the state-of-the-art (SOTA) while\nafter stage two, our method surpasses existing PTQ in both metrics and visual\neffects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (x2)\ncompared with SOTA when quantized to 2-bit and enjoys a 3.60x compression ratio\nand 5.08x speedup ratio. The code and models will be available at\nhttps://github.com/Kai-Liu001/2DQuant.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "9 pages, 6 figures. The code and models will be available at\n  https://github.com/Kai-Liu001/2DQuant",
    "pdf_url": "http://arxiv.org/pdf/2406.06649v1",
    "published_date": "2024-06-10 06:06:11 UTC",
    "updated_date": "2024-06-10 06:06:11 UTC"
  },
  {
    "arxiv_id": "2406.06648v1",
    "title": "SignBLEU: Automatic Evaluation of Multi-channel Sign Language Translation",
    "authors": [
      "Jung-Ho Kim",
      "Mathew Huerta-Enochian",
      "Changyong Ko",
      "Du Hui Lee"
    ],
    "abstract": "Sign languages are multi-channel languages that communicate information\nthrough not just the hands (manual signals) but also facial expressions and\nupper body movements (non-manual signals). However, since automatic sign\nlanguage translation is usually performed by generating a single sequence of\nglosses, researchers eschew non-manual and co-occurring manual signals in favor\nof a simplified list of manual glosses. This can lead to significant\ninformation loss and ambiguity. In this paper, we introduce a new task named\nmulti-channel sign language translation (MCSLT) and present a novel metric,\nSignBLEU, designed to capture multiple signal channels. We validated SignBLEU\non a system-level task using three sign language corpora with varied linguistic\nstructures and transcription methodologies and examined its correlation with\nhuman judgment through two segment-level tasks. We found that SignBLEU\nconsistently correlates better with human judgment than competing metrics. To\nfacilitate further MCSLT research, we report benchmark scores for the three\nsign language corpora and release the source code for SignBLEU at\nhttps://github.com/eq4all-projects/SignBLEU.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in LREC-Coling 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06648v1",
    "published_date": "2024-06-10 05:01:26 UTC",
    "updated_date": "2024-06-10 05:01:26 UTC"
  },
  {
    "arxiv_id": "2406.06017v1",
    "title": "Neuro-TransUNet: Segmentation of stroke lesion in MRI using transformers",
    "authors": [
      "Muhammad Nouman",
      "Mohamed Mabrok",
      "Essam A. Rashed"
    ],
    "abstract": "Accurate segmentation of the stroke lesions using magnetic resonance imaging\n(MRI) is associated with difficulties due to the complicated anatomy of the\nbrain and the different properties of the lesions. This study introduces the\nNeuro-TransUNet framework, which synergizes the U-Net's spatial feature\nextraction with SwinUNETR's global contextual processing ability, further\nenhanced by advanced feature fusion and segmentation synthesis techniques. The\ncomprehensive data pre-processing pipeline improves the framework's efficiency,\nwhich involves resampling, bias correction, and data standardization, enhancing\ndata quality and consistency. Ablation studies confirm the significant impact\nof the advanced integration of U-Net with SwinUNETR and data pre-processing\npipelines on performance and demonstrate the model's effectiveness. The\nproposed Neuro-TransUNet model, trained with the ATLAS v2.0 \\emph{training}\ndataset, outperforms existing deep learning algorithms and establishes a new\nbenchmark in stroke lesion segmentation.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06017v1",
    "published_date": "2024-06-10 04:36:21 UTC",
    "updated_date": "2024-06-10 04:36:21 UTC"
  },
  {
    "arxiv_id": "2406.06647v4",
    "title": "How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark",
    "authors": [
      "Ruizhong Qiu",
      "Weiliang Will Zeng",
      "James Ezick",
      "Christopher Lott",
      "Hanghang Tong"
    ],
    "abstract": "The emergence of large language models (LLMs) has significantly pushed the\nfrontiers of program synthesis. Advancement of LLM-based program synthesis\ncalls for a thorough evaluation of LLM-generated code. Most evaluation\nframeworks focus on the (functional) correctness of generated code; efficiency,\nas an important measure of code quality, has been overlooked in existing\nevaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a\nrigorous and high-standard benchmark for evaluating the capability of LLMs in\ngenerating efficient code. Firstly, we propose a new efficiency metric called\neff@k, which generalizes the pass@k metric from correctness to efficiency and\nappropriately handles right-censored execution time. Furthermore, we derive an\nunbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we\nalso provide a numerically stable implementation for the new estimator.\nSecondly, to set a high-standard for efficiency evaluation, we employ a human\nexpert to design best algorithms and implementations as our reference solutions\nof efficiency, many of which are much more efficient than existing canonical\nsolutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous\nevaluation, we employ a human expert to curate strong test case generators to\nfilter out wrong code and differentiate suboptimal algorithms. An extensive\nstudy across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still\nfall short of generating expert-level efficient code. Using two subsets of our\nproblem set, we demonstrate that such deficiency is because current LLMs\nstruggle in designing advanced algorithms and are barely aware of\nimplementation optimization. Our benchmark is publicly available at\nhttps://github.com/q-rz/enamel .",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.06647v4",
    "published_date": "2024-06-10 04:19:20 UTC",
    "updated_date": "2025-02-19 04:16:24 UTC"
  },
  {
    "arxiv_id": "2406.06009v1",
    "title": "The Impact of AI on Academic Research and Publishing",
    "authors": [
      "Brady Lund",
      "Manika Lamba",
      "Sang Hoo Oh"
    ],
    "abstract": "Generative artificial intelligence (AI) technologies like ChatGPT, have\nsignificantly impacted academic writing and publishing through their ability to\ngenerate content at levels comparable to or surpassing human writers. Through a\nreview of recent interdisciplinary literature, this paper examines ethical\nconsiderations surrounding the integration of AI into academia, focusing on the\npotential for this technology to be used for scholarly misconduct and necessary\noversight when using it for writing, editing, and reviewing of scholarly\npapers. The findings highlight the need for collaborative approaches to AI\nusage among publishers, editors, reviewers, and authors to ensure that this\ntechnology is used ethically and productively.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06009v1",
    "published_date": "2024-06-10 04:10:18 UTC",
    "updated_date": "2024-06-10 04:10:18 UTC"
  },
  {
    "arxiv_id": "2406.06004v1",
    "title": "FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model",
    "authors": [
      "Yebin Lee",
      "Imseong Park",
      "Myungjoo Kang"
    ],
    "abstract": "Most existing image captioning evaluation metrics focus on assigning a single\nnumerical score to a caption by comparing it with reference captions. However,\nthese methods do not provide an explanation for the assigned score. Moreover,\nreference captions are expensive to acquire. In this paper, we propose FLEUR,\nan explainable reference-free metric to introduce explainability into image\ncaptioning evaluation metrics. By leveraging a large multimodal model, FLEUR\ncan evaluate the caption against the image without the need for reference\ncaptions, and provide the explanation for the assigned score. We introduce\nscore smoothing to align as closely as possible with human judgment and to be\nrobust to user-defined grading criteria. FLEUR achieves high correlations with\nhuman judgment across various image captioning evaluation benchmarks and\nreaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S\nwithin the domain of reference-free evaluation metrics. Our source code and\nresults are publicly available at: https://github.com/Yebin46/FLEUR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ACL (Main) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06004v1",
    "published_date": "2024-06-10 03:57:39 UTC",
    "updated_date": "2024-06-10 03:57:39 UTC"
  },
  {
    "arxiv_id": "2406.05999v1",
    "title": "fSEAD: a Composable FPGA-based Streaming Ensemble Anomaly Detection Library",
    "authors": [
      "Binglei Lou",
      "David Boland",
      "Philip H. W. Leong"
    ],
    "abstract": "Machine learning ensembles combine multiple base models to produce a more\naccurate output. They can be applied to a range of machine learning problems,\nincluding anomaly detection. In this paper, we investigate how to maximize the\ncomposability and scalability of an FPGA-based streaming ensemble anomaly\ndetector (fSEAD). To achieve this, we propose a flexible computing architecture\nconsisting of multiple partially reconfigurable regions, pblocks, which each\nimplement anomaly detectors. Our proof-of-concept design supports three\nstate-of-the-art anomaly detection algorithms: Loda, RS-Hash and xStream. Each\nalgorithm is scalable, meaning multiple instances can be placed within a pblock\nto improve performance. Moreover, fSEAD is implemented using High-level\nsynthesis (HLS), meaning further custom anomaly detectors can be supported.\nPblocks are interconnected via an AXI-switch, enabling them to be composed in\nan arbitrary fashion before combining and merging results at run-time to create\nan ensemble that maximizes the use of FPGA resources and accuracy. Through\nutilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be\nmodified at run-time to adapt to changing environmental conditions. We compare\nfSEAD to an equivalent central processing unit (CPU) implementation using four\nstandard datasets, with speed-ups ranging from $3\\times$ to $8\\times$.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "The source code for this paper is available at:\n  https://github.com/bingleilou/fSEAD",
    "pdf_url": "http://arxiv.org/pdf/2406.05999v1",
    "published_date": "2024-06-10 03:38:35 UTC",
    "updated_date": "2024-06-10 03:38:35 UTC"
  },
  {
    "arxiv_id": "2406.05995v1",
    "title": "A Dual-View Approach to Classifying Radiology Reports by Co-Training",
    "authors": [
      "Yutong Han",
      "Yan Yuan",
      "Lili Mou"
    ],
    "abstract": "Radiology report analysis provides valuable information that can aid with\npublic health initiatives, and has been attracting increasing attention from\nthe research community. In this work, we present a novel insight that the\nstructure of a radiology report (namely, the Findings and Impression sections)\noffers different views of a radiology scan. Based on this intuition, we further\npropose a co-training approach, where two machine learning models are built\nupon the Findings and Impression sections, respectively, and use each other's\ninformation to boost performance with massive unlabeled data in a\nsemi-supervised manner. We conducted experiments in a public health\nsurveillance study, and results show that our co-training approach is able to\nimprove performance using the dual views and surpass competing supervised and\nsemi-supervised methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05995v1",
    "published_date": "2024-06-10 03:29:23 UTC",
    "updated_date": "2024-06-10 03:29:23 UTC"
  },
  {
    "arxiv_id": "2406.05984v1",
    "title": "Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook",
    "authors": [
      "Yusif Ibrahimov",
      "Tarique Anwar",
      "Tommy Yuan"
    ],
    "abstract": "Mental health constitutes a complex and pervasive global challenge, affecting\nmillions of lives and often leading to severe consequences. In this paper, we\nconduct a thorough survey to explore the intersection of data science,\nartificial intelligence, and mental healthcare, focusing on the recent\ndevelopments of mental disorder detection through online social media (OSM). A\nsignificant portion of the population actively engages in OSM platforms,\ncreating a vast repository of personal data that holds immense potential for\nmental health analytics. The paper navigates through traditional diagnostic\nmethods, state-of-the-art data- and AI-driven research studies, and the\nemergence of explainable AI (XAI) models for mental healthcare. We review\nstate-of-the-art machine learning methods, particularly those based on modern\ndeep learning, while emphasising the need for explainability in healthcare AI\nmodels. The experimental design section provides insights into prevalent\npractices, including available datasets and evaluation approaches. We also\nidentify key issues and challenges in the field and propose promising future\nresearch directions. As mental health decisions demand transparency,\ninterpretability, and ethical considerations, this paper contributes to the\nongoing discourse on advancing XAI in mental healthcare through social media.\nThe comprehensive overview presented here aims to guide researchers,\npractitioners, and policymakers in developing the area of mental disorder\ndetection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05984v1",
    "published_date": "2024-06-10 02:51:16 UTC",
    "updated_date": "2024-06-10 02:51:16 UTC"
  },
  {
    "arxiv_id": "2406.05981v4",
    "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization",
    "authors": [
      "Haoran You",
      "Yipin Guo",
      "Yichao Fu",
      "Wei Zhou",
      "Huihong Shi",
      "Xiaofan Zhang",
      "Souvik Kundu",
      "Amir Yazdanbakhsh",
      "Yingyan Celine Lin"
    ],
    "abstract": "Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05981v4",
    "published_date": "2024-06-10 02:47:55 UTC",
    "updated_date": "2024-11-18 20:18:32 UTC"
  },
  {
    "arxiv_id": "2406.05972v2",
    "title": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context",
    "authors": [
      "Jingru Jia",
      "Zehua Yuan",
      "Junhao Pan",
      "Paul E. McNamara",
      "Deming Chen"
    ],
    "abstract": "When making decisions under uncertainty, individuals often deviate from\nrational behavior, which can be evaluated across three dimensions: risk\npreference, probability weighting, and loss aversion. Given the widespread use\nof large language models (LLMs) in decision-making processes, it is crucial to\nassess whether their behavior aligns with human norms and ethical expectations\nor exhibits potential biases. Several empirical studies have investigated the\nrationality and social behavior performance of LLMs, yet their internal\ndecision-making tendencies and capabilities remain inadequately understood.\nThis paper proposes a framework, grounded in behavioral economics, to evaluate\nthe decision-making behaviors of LLMs. Through a multiple-choice-list\nexperiment, we estimate the degree of risk preference, probability weighting,\nand loss aversion in a context-free setting for three commercial LLMs:\nChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that\nLLMs generally exhibit patterns similar to humans, such as risk aversion and\nloss aversion, with a tendency to overweight small probabilities. However,\nthere are significant variations in the degree to which these behaviors are\nexpressed across different LLMs. We also explore their behavior when embedded\nwith socio-demographic features, uncovering significant disparities. For\ninstance, when modeled with attributes of sexual minority groups or physical\ndisabilities, Claude-3-Opus displays increased risk aversion, leading to more\nconservative choices. These findings underscore the need for careful\nconsideration of the ethical implications and potential biases in deploying\nLLMs in decision-making scenarios. Therefore, this study advocates for\ndeveloping standards and guidelines to ensure that LLMs operate within ethical\nboundaries while enhancing their utility in complex decision-making\nenvironments.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "econ.TH"
    ],
    "primary_category": "cs.AI",
    "comment": "Jingru Jia and Zehua Yuan have equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2406.05972v2",
    "published_date": "2024-06-10 02:14:19 UTC",
    "updated_date": "2024-11-01 00:50:56 UTC"
  },
  {
    "arxiv_id": "2406.05967v2",
    "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
    "authors": [
      "David Romero",
      "Chenyang Lyu",
      "Haryo Akbarianto Wibowo",
      "Teresa Lynn",
      "Injy Hamed",
      "Aditya Nanda Kishore",
      "Aishik Mandal",
      "Alina Dragonetti",
      "Artem Abzaliev",
      "Atnafu Lambebo Tonja",
      "Bontu Fufa Balcha",
      "Chenxi Whitehouse",
      "Christian Salamea",
      "Dan John Velasco",
      "David Ifeoluwa Adelani",
      "David Le Meur",
      "Emilio Villa-Cueva",
      "Fajri Koto",
      "Fauzan Farooqui",
      "Frederico Belcavello",
      "Ganzorig Batnasan",
      "Gisela Vallejo",
      "Grainne Caulfield",
      "Guido Ivetta",
      "Haiyue Song",
      "Henok Biadglign Ademtew",
      "Hernán Maina",
      "Holy Lovenia",
      "Israel Abebe Azime",
      "Jan Christian Blaise Cruz",
      "Jay Gala",
      "Jiahui Geng",
      "Jesus-German Ortiz-Barajas",
      "Jinheon Baek",
      "Jocelyn Dunstan",
      "Laura Alonso Alemany",
      "Kumaranage Ravindu Yasas Nagasinghe",
      "Luciana Benotti",
      "Luis Fernando D'Haro",
      "Marcelo Viridiano",
      "Marcos Estecha-Garitagoitia",
      "Maria Camila Buitrago Cabrera",
      "Mario Rodríguez-Cantelar",
      "Mélanie Jouitteau",
      "Mihail Mihaylov",
      "Mohamed Fazli Mohamed Imam",
      "Muhammad Farid Adilazuarda",
      "Munkhjargal Gochoo",
      "Munkh-Erdene Otgonbold",
      "Naome Etori",
      "Olivier Niyomugisha",
      "Paula Mónica Silva",
      "Pranjal Chitale",
      "Raj Dabre",
      "Rendi Chevi",
      "Ruochen Zhang",
      "Ryandito Diandaru",
      "Samuel Cahyawijaya",
      "Santiago Góngora",
      "Soyeong Jeong",
      "Sukannya Purkayastha",
      "Tatsuki Kuribayashi",
      "Teresa Clifford",
      "Thanmay Jayakumar",
      "Tiago Timponi Torrent",
      "Toqeer Ehsan",
      "Vladimir Araujo",
      "Yova Kementchedjhieva",
      "Zara Burzo",
      "Zheng Wei Lim",
      "Zheng Xin Yong",
      "Oana Ignat",
      "Joan Nwatu",
      "Rada Mihalcea",
      "Thamar Solorio",
      "Alham Fikri Aji"
    ],
    "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 30\ncountries on four continents, covering 31 languages with 13 scripts, providing\na total of 10k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2406.05967v2",
    "published_date": "2024-06-10 01:59:00 UTC",
    "updated_date": "2024-11-04 07:55:31 UTC"
  },
  {
    "arxiv_id": "2406.05965v1",
    "title": "MakeSinger: A Semi-Supervised Training Method for Data-Efficient Singing Voice Synthesis via Classifier-free Diffusion Guidance",
    "authors": [
      "Semin Kim",
      "Myeonghun Jeong",
      "Hyeonseung Lee",
      "Minchan Kim",
      "Byoung Jin Choi",
      "Nam Soo Kim"
    ],
    "abstract": "In this paper, we propose MakeSinger, a semi-supervised training method for\nsinging voice synthesis (SVS) via classifier-free diffusion guidance. The\nchallenge in SVS lies in the costly process of gathering aligned sets of text,\npitch, and audio data. MakeSinger enables the training of the diffusion-based\nSVS model from any speech and singing voice data regardless of its labeling,\nthereby enhancing the quality of generated voices with large amount of\nunlabeled data. At inference, our novel dual guiding mechanism gives text and\npitch guidance on the reverse diffusion step by estimating the score of masked\ninput. Experimental results show that the model trained in a semi-supervised\nmanner outperforms other baselines trained only on the labeled data in terms of\npronunciation, pitch accuracy and overall quality. Furthermore, we demonstrate\nthat by adding Text-to-Speech (TTS) data in training, the model can synthesize\nthe singing voices of TTS speakers even without their singing voices.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05965v1",
    "published_date": "2024-06-10 01:47:52 UTC",
    "updated_date": "2024-06-10 01:47:52 UTC"
  },
  {
    "arxiv_id": "2406.05963v1",
    "title": "Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024",
    "authors": [
      "Jinwoo Ahn",
      "Junhyeok Park",
      "Min-Jun Kim",
      "Kang-Hyeon Kim",
      "So-Yeong Sohn",
      "Yun-Ji Lee",
      "Du-Seong Chang",
      "Yu-Jung Heo",
      "Eun-Sol Kim"
    ],
    "abstract": "In this paper, the solution of HYU MLLAB KT Team to the Multimodal\nAlgorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented. Beyond\nconventional visual question-answering problems, the SMART-101 challenge aims\nto achieve human-level multimodal understanding by tackling complex\nvisio-linguistic puzzles designed for children in the 6-8 age group. To solve\nthis problem, we suggest two main ideas. First, to utilize the reasoning\nability of a large-scale language model (LLM), the given visual cues (images)\nare grounded in the text modality. For this purpose, we generate highly\ndetailed text captions that describe the context of the image and use these\ncaptions as input for the LLM. Second, due to the nature of puzzle images,\nwhich often contain various geometric visual patterns, we utilize an object\ndetection algorithm to ensure these patterns are not overlooked in the\ncaptioning process. We employed the SAM algorithm, which can detect\nvarious-size objects, to capture the visual features of these geometric\npatterns and used this information as input for the LLM. Under the puzzle split\nconfiguration, we achieved an option selection accuracy Oacc of 29.5 on the\ntest set and a weighted option selection accuracy (WOSA) of 27.1 on the\nchallenge set.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05963v1",
    "published_date": "2024-06-10 01:45:55 UTC",
    "updated_date": "2024-06-10 01:45:55 UTC"
  },
  {
    "arxiv_id": "2406.05954v3",
    "title": "Aligning Large Language Models with Representation Editing: A Control Perspective",
    "authors": [
      "Lingkai Kong",
      "Haorui Wang",
      "Wenhao Mu",
      "Yuanqi Du",
      "Yuchen Zhuang",
      "Yifei Zhou",
      "Yue Song",
      "Rongzhi Zhang",
      "Kai Wang",
      "Chao Zhang"
    ],
    "abstract": "Aligning large language models (LLMs) with human objectives is crucial for\nreal-world applications. However, fine-tuning LLMs for alignment often suffers\nfrom unstable training and requires substantial computing resources. Test-time\nalignment techniques, such as prompting and guided decoding, do not modify the\nunderlying model, and their performance remains dependent on the original\nmodel's capabilities. To address these challenges, we propose aligning LLMs\nthrough representation editing. The core of our method is to view a pre-trained\nautoregressive LLM as a discrete-time stochastic dynamical system. To achieve\nalignment for specific objectives, we introduce external control signals into\nthe state space of this language dynamical system. We train a value function\ndirectly on the hidden states according to the Bellman equation, enabling\ngradient-based optimization to obtain the optimal control signals at test time.\nOur experiments demonstrate that our method outperforms existing test-time\nalignment techniques while requiring significantly fewer resources compared to\nfine-tuning methods. Our code is available at\nhttps://github.com/Lingkai-Kong/RE-Control.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05954v3",
    "published_date": "2024-06-10 01:21:31 UTC",
    "updated_date": "2024-11-01 17:46:46 UTC"
  },
  {
    "arxiv_id": "2406.05948v2",
    "title": "Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models",
    "authors": [
      "Xi Li",
      "Yusen Zhang",
      "Renze Lou",
      "Chen Wu",
      "Jiaqi Wang"
    ],
    "abstract": "Large Language Models (LLMs), especially those accessed via APIs, have\ndemonstrated impressive capabilities across various domains. However, users\nwithout technical expertise often turn to (untrustworthy) third-party services,\nsuch as prompt engineering, to enhance their LLM experience, creating\nvulnerabilities to adversarial threats like backdoor attacks.\nBackdoor-compromised LLMs generate malicious outputs to users when inputs\ncontain specific \"triggers\" set by attackers. Traditional defense strategies,\noriginally designed for small-scale models, are impractical for API-accessible\nLLMs due to limited model access, high computational costs, and data\nrequirements. To address these limitations, we propose Chain-of-Scrutiny (CoS)\nwhich leverages LLMs' unique reasoning abilities to mitigate backdoor attacks.\nIt guides the LLM to generate reasoning steps for a given input and scrutinizes\nfor consistency with the final output -- any inconsistencies indicating a\npotential attack. It is well-suited for the popular API-only LLM deployments,\nenabling detection at minimal cost and with little data. User-friendly and\ndriven by natural language, it allows non-experts to perform the defense\nindependently while maintaining transparency. We validate the effectiveness of\nCoS through extensive experiments on various tasks and LLMs, with results\nshowing greater benefits for more powerful LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05948v2",
    "published_date": "2024-06-10 00:53:25 UTC",
    "updated_date": "2024-12-21 00:06:50 UTC"
  },
  {
    "arxiv_id": "2406.05946v1",
    "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
    "authors": [
      "Xiangyu Qi",
      "Ashwinee Panda",
      "Kaifeng Lyu",
      "Xiao Ma",
      "Subhrajit Roy",
      "Ahmad Beirami",
      "Prateek Mittal",
      "Peter Henderson"
    ],
    "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable.\nRelatively simple attacks, or even benign fine-tuning, can jailbreak aligned\nmodels. We argue that many of these vulnerabilities are related to a shared\nunderlying issue: safety alignment can take shortcuts, wherein the alignment\nadapts a model's generative distribution primarily over only its very first few\noutput tokens. We refer to this issue as shallow safety alignment. In this\npaper, we present case studies to explain why shallow safety alignment can\nexist and provide evidence that current aligned LLMs are subject to this issue.\nWe also show how these findings help explain multiple recently discovered\nvulnerabilities in LLMs, including the susceptibility to adversarial suffix\nattacks, prefilling attacks, decoding parameter attacks, and fine-tuning\nattacks. Importantly, we discuss how this consolidated notion of shallow safety\nalignment sheds light on promising research directions for mitigating these\nvulnerabilities. For instance, we show that deepening the safety alignment\nbeyond just the first few tokens can often meaningfully improve robustness\nagainst some common exploits. Finally, we design a regularized finetuning\nobjective that makes the safety alignment more persistent against fine-tuning\nattacks by constraining updates on initial tokens. Overall, we advocate that\nfuture safety alignment should be made more than just a few tokens deep.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05946v1",
    "published_date": "2024-06-10 00:35:23 UTC",
    "updated_date": "2024-06-10 00:35:23 UTC"
  }
]
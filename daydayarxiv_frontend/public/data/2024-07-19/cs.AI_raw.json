[
  {
    "arxiv_id": "2407.18269v2",
    "title": "LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits",
    "authors": [
      "Chen-Chia Chang",
      "Yikang Shen",
      "Shaoze Fan",
      "Jing Li",
      "Shun Zhang",
      "Ningyuan Cao",
      "Yiran Chen",
      "Xin Zhang"
    ],
    "abstract": "In the realm of electronic and electrical engineering, automation of analog\ncircuit is increasingly vital given the complexity and customized requirements\nof modern applications. However, existing methods only develop search-based\nalgorithms that require many simulation iterations to design a custom circuit\ntopology, which is usually a time-consuming process. To this end, we introduce\nLaMAGIC, a pioneering language model-based topology generation model that\nleverages supervised finetuning for automated analog circuit design. LaMAGIC\ncan efficiently generate an optimized circuit design from the custom\nspecification in a single pass. Our approach involves a meticulous development\nand analysis of various input and output formulations for circuit. These\nformulations can ensure canonical representations of circuits and align with\nthe autoregressive nature of LMs to effectively addressing the challenges of\nrepresenting analog circuits as graphs. The experimental results show that\nLaMAGIC achieves a success rate of up to 96\\% under a strict tolerance of 0.01.\nWe also examine the scalability and adaptability of LaMAGIC, specifically\ntesting its performance on more complex circuits. Our findings reveal the\nenhanced effectiveness of our adjacency matrix-based circuit formulation with\nfloating-point input, suggesting its suitability for handling intricate circuit\ndesigns. This research not only demonstrates the potential of language models\nin graph generation, but also builds a foundational framework for future\nexplorations in automated analog circuit design.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "Proceedings of the 41st International Conference on Machine Learning,\n  PMLR 235:6253-6262 https://proceedings.mlr.press/v235/chang24c.html",
    "pdf_url": "http://arxiv.org/pdf/2407.18269v2",
    "published_date": "2024-07-19 22:51:41 UTC",
    "updated_date": "2024-08-29 08:07:43 UTC"
  },
  {
    "arxiv_id": "2407.14681v1",
    "title": "Value Internalization: Learning and Generalizing from Social Reward",
    "authors": [
      "Frieda Rong",
      "Max Kleiman-Weiner"
    ],
    "abstract": "Social rewards shape human behavior. During development, a caregiver guides a\nlearner's behavior towards culturally aligned goals and values. How do these\nbehaviors persist and generalize when the caregiver is no longer present, and\nthe learner must continue autonomously? Here, we propose a model of value\ninternalization where social feedback trains an internal social reward (ISR)\nmodel that generates internal rewards when social rewards are unavailable.\nThrough empirical simulations, we show that an ISR model prevents agents from\nunlearning socialized behaviors and enables generalization in\nout-of-distribution tasks. We characterize the implications of incomplete\ninternalization, akin to \"reward hacking\" on the ISR. Additionally, we show\nthat our model internalizes prosocial behavior in a multi-agent environment.\nOur work provides a foundation for understanding how humans acquire and\ngeneralize values and offers insights for aligning AI with human values.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Reinforcement Learning Conference (RLC) 2024 & Cognitive Science\n  Conference Oral",
    "pdf_url": "http://arxiv.org/pdf/2407.14681v1",
    "published_date": "2024-07-19 21:53:33 UTC",
    "updated_date": "2024-07-19 21:53:33 UTC"
  },
  {
    "arxiv_id": "2407.14679v2",
    "title": "Compact Language Models via Pruning and Knowledge Distillation",
    "authors": [
      "Saurav Muralidharan",
      "Sharath Turuvekere Sreenivas",
      "Raviraj Joshi",
      "Marcin Chochowski",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "abstract": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14679v2",
    "published_date": "2024-07-19 21:47:57 UTC",
    "updated_date": "2024-11-04 17:36:38 UTC"
  },
  {
    "arxiv_id": "2407.14664v2",
    "title": "Is $F_1$ Score Suboptimal for Cybersecurity Models? Introducing $C_{score}$, a Cost-Aware Alternative for Model Assessment",
    "authors": [
      "Manish Marwah",
      "Asad Narayanan",
      "Stephan Jou",
      "Martin Arlitt",
      "Maria Pospelova"
    ],
    "abstract": "The cost of errors related to machine learning classifiers, namely, false\npositives and false negatives, are not equal and are application dependent. For\nexample, in cybersecurity applications, the cost of not detecting an attack is\nvery different from marking a benign activity as an attack. Various design\nchoices during machine learning model building, such as hyperparameter tuning\nand model selection, allow a data scientist to trade-off between these two\nerrors. However, most of the commonly used metrics to evaluate model quality,\nsuch as $F_1$ score, which is defined in terms of model precision and recall,\ntreat both these errors equally, making it difficult for users to optimize for\nthe actual cost of these errors. In this paper, we propose a new cost-aware\nmetric, $C_{score}$ based on precision and recall that can replace $F_1$ score\nfor model evaluation and selection. It includes a cost ratio that takes into\naccount the differing costs of handling false positives and false negatives. We\nderive and characterize the new cost metric, and compare it to $F_1$ score.\nFurther, we use this metric for model thresholding for five cybersecurity\nrelated datasets for multiple cost ratios. The results show an average cost\nsavings of 49%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14664v2",
    "published_date": "2024-07-19 21:01:19 UTC",
    "updated_date": "2024-07-31 15:03:57 UTC"
  },
  {
    "arxiv_id": "2407.21038v1",
    "title": "Advancing Chart Question Answering with Robust Chart Component Recognition",
    "authors": [
      "Hanwen Zheng",
      "Sijia Wang",
      "Chris Thomas",
      "Lifu Huang"
    ],
    "abstract": "Chart comprehension presents significant challenges for machine learning\nmodels due to the diverse and intricate shapes of charts. Existing multimodal\nmethods often overlook these visual features or fail to integrate them\neffectively for chart question answering (ChartQA). To address this, we\nintroduce Chartformer, a unified framework that enhances chart component\nrecognition by accurately identifying and classifying components such as bars,\nlines, pies, titles, legends, and axes. Additionally, we propose a novel\nQuestion-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart\nfeatures encoded by Chartformer with the given question, leveraging the\nquestion's guidance to ground the correct answer. Extensive experiments\ndemonstrate that the proposed approaches significantly outperform baseline\nmodels in chart component recognition and ChartQA tasks, achieving improvements\nof 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore\nthe robustness of our solution for detailed visual data interpretation across\nvarious applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21038v1",
    "published_date": "2024-07-19 20:55:06 UTC",
    "updated_date": "2024-07-19 20:55:06 UTC"
  },
  {
    "arxiv_id": "2407.14662v1",
    "title": "Relational Composition in Neural Networks: A Survey and Call to Action",
    "authors": [
      "Martin Wattenberg",
      "Fernanda B. ViÃ©gas"
    ],
    "abstract": "Many neural nets appear to represent data as linear combinations of \"feature\nvectors.\" Algorithms for discovering these vectors have seen impressive recent\nsuccess. However, we argue that this success is incomplete without an\nunderstanding of relational composition: how (or whether) neural nets combine\nfeature vectors to represent more complicated relationships. To facilitate\nresearch in this area, this paper offers a guided tour of various relational\nmechanisms that have been proposed, along with preliminary analysis of how such\nmechanisms might affect the search for interpretable features. We end with a\nseries of promising areas for empirical research, which may help determine how\nneural networks represent structured data.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14662v1",
    "published_date": "2024-07-19 20:50:57 UTC",
    "updated_date": "2024-07-19 20:50:57 UTC"
  },
  {
    "arxiv_id": "2407.14658v1",
    "title": "A New Lightweight Hybrid Graph Convolutional Neural Network -- CNN Scheme for Scene Classification using Object Detection Inference",
    "authors": [
      "Ayman Beghdadi",
      "Azeddine Beghdadi",
      "Mohib Ullah",
      "Faouzi Alaya Cheikh",
      "Malik Mallem"
    ],
    "abstract": "Scene understanding plays an important role in several high-level computer\nvision applications, such as autonomous vehicles, intelligent video\nsurveillance, or robotics. However, too few solutions have been proposed for\nindoor/outdoor scene classification to ensure scene context adaptability for\ncomputer vision frameworks. We propose the first Lightweight Hybrid Graph\nConvolutional Neural Network (LH-GCNN)-CNN framework as an add-on to object\ndetection models. The proposed approach uses the output of the CNN object\ndetection model to predict the observed scene type by generating a coherent\nGCNN representing the semantic and geometric content of the observed scene.\nThis new method, applied to natural scenes, achieves an efficiency of over 90\\%\nfor scene classification in a COCO-derived dataset containing a large number of\ndifferent scenes, while requiring fewer parameters than traditional CNN\nmethods. For the benefit of the scientific community, we will make the source\ncode publicly available: https://github.com/Aymanbegh/Hybrid-GCNN-CNN.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14658v1",
    "published_date": "2024-07-19 20:34:40 UTC",
    "updated_date": "2024-07-19 20:34:40 UTC"
  },
  {
    "arxiv_id": "2407.14651v3",
    "title": "Improving Representation of High-frequency Components for Medical Visual Foundation Models",
    "authors": [
      "Yuetan Chu",
      "Yilan Zhang",
      "Zhongyi Han",
      "Changchun Yang",
      "Longxi Zhou",
      "Gongning Luo",
      "Chao Huang",
      "Xin Gao"
    ],
    "abstract": "Foundation models have recently attracted significant attention for their\nimpressive generalizability across diverse downstream tasks. However, these\nmodels are demonstrated to exhibit great limitations in representing\nhigh-frequency components and fine-grained details. In many medical imaging\ntasks, the precise representation of such information is crucial due to the\ninherently intricate anatomical structures, sub-visual features, and complex\nboundaries involved. Consequently, the limited representation of prevalent\nfoundation models can result in significant performance degradation or even\nfailure in these tasks. To address these challenges, we propose a novel\npretraining strategy, named Frequency-advanced Representation Autoencoder\n(Frepa). Through high-frequency masking and low-frequency perturbation combined\nwith adversarial learning, Frepa encourages the encoder to effectively\nrepresent and preserve high-frequency components in the image embeddings.\nAdditionally, we introduce an innovative histogram-equalized image masking\nstrategy, extending the Masked Autoencoder approach beyond ViT to other\narchitectures such as Swin Transformer and convolutional networks. We develop\nFrepa across nine medical modalities and validate it on 32 downstream tasks for\nboth 2D images and 3D volume data. Without fine-tuning, Frepa can outperform\nother self-supervised pretraining methods and, in some cases, even surpasses\ntask-specific trained models. This improvement is particularly significant for\ntasks involving fine-grained details, such as achieving up to a +15% increase\nin DSC for retina vessel segmentation and a +7% increase in IoU for lung nodule\ndetection. Further experiments quantitatively reveal that Frepa enables\nsuperior high-frequency representations and preservation in the embeddings,\nunderscoring its potential for developing more generalized and universal\nmedical image foundation models.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14651v3",
    "published_date": "2024-07-19 20:05:10 UTC",
    "updated_date": "2025-03-03 09:31:01 UTC"
  },
  {
    "arxiv_id": "2407.14640v1",
    "title": "CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models",
    "authors": [
      "Rikhiya Ghosh",
      "Oladimeji Farri",
      "Hans-Martin von Stockhausen",
      "Martin Schmitt",
      "George Marica Vasile"
    ],
    "abstract": "The healthcare industry is currently experiencing an unprecedented wave of\ncybersecurity attacks, impacting millions of individuals. With the discovery of\nthousands of vulnerabilities each month, there is a pressing need to drive the\nautomation of vulnerability assessment processes for medical devices,\nfacilitating rapid mitigation efforts. Generative AI systems have\nrevolutionized various industries, offering unparalleled opportunities for\nautomation and increased efficiency. This paper presents a solution leveraging\nLarge Language Models (LLMs) to learn from historical evaluations of\nvulnerabilities for the automatic assessment of vulnerabilities in the medical\ndevices industry. This approach is applied within the portfolio of a single\nmanufacturer, taking into account device characteristics, including existing\nsecurity posture and controls. The primary contributions of this paper are\nthreefold. Firstly, it provides a detailed examination of the best practices\nfor training a vulnerability Language Model (LM) in an industrial context.\nSecondly, it presents a comprehensive comparison and insightful analysis of the\neffectiveness of Language Models in vulnerability assessment. Finally, it\nproposes a new human-in-the-loop framework to expedite vulnerability evaluation\nprocesses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14640v1",
    "published_date": "2024-07-19 19:34:17 UTC",
    "updated_date": "2024-07-19 19:34:17 UTC"
  },
  {
    "arxiv_id": "2407.14631v2",
    "title": "Two new feature selection methods based on learn-heuristic techniques for breast cancer prediction: A comprehensive analysis",
    "authors": [
      "Kamyab Karimi",
      "Ali Ghodratnama",
      "Reza Tavakkoli-Moghaddam"
    ],
    "abstract": "Breast cancer is not preventable because of its unknown causes. However, its\nearly diagnosis increases patients' recovery chances. Machine learning (ML) can\nbe utilized to improve treatment outcomes in healthcare operations while\ndiminishing costs and time. In this research, we suggest two novel feature\nselection (FS) methods based upon an imperialist competitive algorithm (ICA)\nand a bat algorithm (BA) and their combination with ML algorithms. This study\naims to enhance diagnostic models' efficiency and present a comprehensive\nanalysis to help clinical physicians make much more precise and reliable\ndecisions than before. K-nearest neighbors, support vector machine, decision\ntree, Naive Bayes, AdaBoost, linear discriminant analysis, random forest,\nlogistic regression, and artificial neural network are some of the methods\nemployed. This paper applied a distinctive integration of evaluation measures\nand ML algorithms using the wrapper feature selection based on ICA (WFSIC) and\nBA (WFSB) separately. We compared two proposed approaches for the performance\nof the classifiers. Also, we compared our best diagnostic model with previous\nworks reported in the literature survey. Experimentations were performed on the\nWisconsin diagnostic breast cancer dataset. Results reveal that the proposed\nframework that uses the BA with an accuracy of 99.12\\%, surpasses the framework\nusing the ICA and most previous works. Additionally, the RF classifier in the\napproach of FS based on BA emerges as the best model and outperforms others\nregarding its criteria. Besides, the results illustrate the role of our\ntechniques in reducing the dataset dimensions up to 90\\% and increasing the\nperformance of diagnostic models by over 99\\%. Moreover, the result\ndemonstrates that there are more critical features than the optimum dataset\nobtained by proposed FS approaches that have been selected by most ML models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "36 pages, 3 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.14631v2",
    "published_date": "2024-07-19 19:07:53 UTC",
    "updated_date": "2024-08-07 21:10:30 UTC"
  },
  {
    "arxiv_id": "2407.14622v1",
    "title": "BOND: Aligning LLMs with Best-of-N Distillation",
    "authors": [
      "Pier Giuseppe Sessa",
      "Robert Dadashi",
      "LÃ©onard Hussenot",
      "Johan Ferret",
      "Nino Vieillard",
      "Alexandre RamÃ©",
      "Bobak Shariari",
      "Sarah Perrin",
      "Abe Friesen",
      "Geoffrey Cideron",
      "Sertan Girgin",
      "Piotr Stanczyk",
      "Andrea Michi",
      "Danila Sinopalnikov",
      "Sabela Ramos",
      "AmÃ©lie HÃ©liou",
      "Aliaksei Severyn",
      "Matt Hoffman",
      "Nikola Momchev",
      "Olivier Bachem"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14622v1",
    "published_date": "2024-07-19 18:38:25 UTC",
    "updated_date": "2024-07-19 18:38:25 UTC"
  },
  {
    "arxiv_id": "2407.14605v1",
    "title": "ESCAPE: Energy-based Selective Adaptive Correction for Out-of-distribution 3D Human Pose Estimation",
    "authors": [
      "Luke Bidulka",
      "Mohsen Gholami",
      "Jiannan Zheng",
      "Martin J. McKeown",
      "Z. Jane Wang"
    ],
    "abstract": "Despite recent advances in human pose estimation (HPE), poor generalization\nto out-of-distribution (OOD) data remains a difficult problem. While previous\nworks have proposed Test-Time Adaptation (TTA) to bridge the train-test domain\ngap by refining network parameters at inference, the absence of ground-truth\nannotations makes it highly challenging and existing methods typically increase\ninference times by one or more orders of magnitude. We observe that 1) not\nevery test time sample is OOD, and 2) HPE errors are significantly larger on\ndistal keypoints (wrist, ankle). To this end, we propose ESCAPE: a lightweight\ncorrection and selective adaptation framework which applies a fast,\nforward-pass correction on most data while reserving costly TTA for OOD data.\nThe free energy function is introduced to separate OOD samples from incoming\ndata and a correction network is trained to estimate the errors of pretrained\nbackbone HPE predictions on the distal keypoints. For OOD samples, we propose a\nnovel self-consistency adaptation loss to update the correction network by\nleveraging the constraining relationship between distal keypoints and proximal\nkeypoints (shoulders, hips), via a second ``reverse\" network. ESCAPE improves\nthe distal MPJPE of five popular HPE models by up to 7% on unseen data,\nachieves state-of-the-art results on two popular HPE benchmarks, and is\nsignificantly faster than existing adaptation methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.6; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14605v1",
    "published_date": "2024-07-19 18:01:26 UTC",
    "updated_date": "2024-07-19 18:01:26 UTC"
  },
  {
    "arxiv_id": "2407.14509v1",
    "title": "DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks",
    "authors": [
      "Sarah Jabbour",
      "Gregory Kondas",
      "Ella Kazerooni",
      "Michael Sjoding",
      "David Fouhey",
      "Jenna Wiens"
    ],
    "abstract": "We propose a permutation-based explanation method for image classifiers.\nCurrent image-model explanations like activation maps are limited to\ninstance-based explanations in the pixel space, making it difficult to\nunderstand global model behavior. In contrast, permutation based explanations\nfor tabular data classifiers measure feature importance by comparing model\nperformance on data before and after permuting a feature. We propose an\nexplanation method for image-based models that permutes interpretable concepts\nacross dataset images. Given a dataset of images labeled with specific concepts\nlike captions, we permute a concept across examples in the text space and then\ngenerate images via a text-conditioned diffusion model. Feature importance is\nthen reflected by the change in model performance relative to unpermuted data.\nWhen applied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature importance\non synthetic and real-world image classification tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "36 pages, 18 figures, 9 tables, to be published in ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14509v1",
    "published_date": "2024-07-19 17:59:38 UTC",
    "updated_date": "2024-07-19 17:59:38 UTC"
  },
  {
    "arxiv_id": "2407.14506v2",
    "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding",
    "authors": [
      "Wan-Cyuan Fan",
      "Yen-Chun Chen",
      "Mengchen Liu",
      "Lu Yuan",
      "Leonid Sigal"
    ],
    "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14506v2",
    "published_date": "2024-07-19 17:58:36 UTC",
    "updated_date": "2024-07-31 21:01:16 UTC"
  },
  {
    "arxiv_id": "2407.14499v2",
    "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
    "authors": [
      "Sukrut Rao",
      "Sweta Mahajan",
      "Moritz BÃ¶hle",
      "Bernt Schiele"
    ],
    "abstract": "Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "40 pages, 21 figures, 6 tables, European Conference on Computer\n  Vision (ECCV) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14499v2",
    "published_date": "2024-07-19 17:50:11 UTC",
    "updated_date": "2024-08-12 14:50:01 UTC"
  },
  {
    "arxiv_id": "2407.14486v1",
    "title": "Explainable Post hoc Portfolio Management Financial Policy of a Deep Reinforcement Learning agent",
    "authors": [
      "Alejandra de la Rica Escudero",
      "Eduardo C. Garrido-Merchan",
      "Maria Coronado-Vaca"
    ],
    "abstract": "Financial portfolio management investment policies computed quantitatively by\nmodern portfolio theory techniques like the Markowitz model rely on a set on\nassumptions that are not supported by data in high volatility markets. Hence,\nquantitative researchers are looking for alternative models to tackle this\nproblem. Concretely, portfolio management is a problem that has been\nsuccessfully addressed recently by Deep Reinforcement Learning (DRL)\napproaches. In particular, DRL algorithms train an agent by estimating the\ndistribution of the expected reward of every action performed by an agent given\nany financial state in a simulator. However, these methods rely on Deep Neural\nNetworks model to represent such a distribution, that although they are\nuniversal approximator models, they cannot explain its behaviour, given by a\nset of parameters that are not interpretable. Critically, financial investors\npolicies require predictions to be interpretable, so DRL agents are not suited\nto follow a particular policy or explain their actions. In this work, we\ndeveloped a novel Explainable Deep Reinforcement Learning (XDRL) approach for\nportfolio management, integrating the Proximal Policy Optimization (PPO) with\nthe model agnostic explainable techniques of feature importance, SHAP and LIME\nto enhance transparency in prediction time. By executing our methodology, we\ncan interpret in prediction time the actions of the agent to assess whether\nthey follow the requisites of an investment policy or to assess the risk of\nfollowing the agent suggestions. To the best of our knowledge, our proposed\napproach is the first explainable post hoc portfolio management financial\npolicy of a DRL agent. We empirically illustrate our methodology by\nsuccessfully identifying key features influencing investment decisions, which\ndemonstrate the ability to explain the agent actions in prediction time.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "q-fin.PM"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14486v1",
    "published_date": "2024-07-19 17:40:39 UTC",
    "updated_date": "2024-07-19 17:40:39 UTC"
  },
  {
    "arxiv_id": "2407.14482v3",
    "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
    "authors": [
      "Peng Xu",
      "Wei Ping",
      "Xianchao Wu",
      "Chejian Xu",
      "Zihan Liu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "abstract": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context understanding\nand retrieval-augmented generation (RAG) capabilities. These two capabilities\nare complementary to each other and essential for LLMs to process large volumes\nof information that cannot fit into a single prompt. We present a detailed\ncontinued training recipe to extend the context window of Llama3-70B-base from\n8K to 128K tokens, along with a three-stage instruction tuning process to\nenhance the model's instruction-following, RAG performance, and long-context\nunderstanding capabilities. Our results demonstrate that the\nLlama3-ChatQA-2-70B model outperforms most existing state-of-the-art models,\nincluding GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and\nLlama3.1-70B-Instruct, on ultra-long tasks beyond 100K tokens, as well as on\nthe RAG benchmark using only a 4K context window, showing the strong long\ncontext capability across varying sequence lengths. We further provide\nextensive comparisons between direct long-context and RAG solutions using the\nsame state-of-the-art long-context LLMs. Interestingly, we find that the\nperformance of strong long-context LLMs using RAG improves when retrieving a\nlarger number of chunks. With a large set of top-k chunks, RAG consistently\noutperforms direct long-context solution using the same state-of-the-art\nlong-context models (e.g., Llama3-ChatQA-2-70B and Qwen2-72B-Instruct) on both\n32K and 128K benchmarks. We open-source the model weights, training data, and\nthe evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.14482v3",
    "published_date": "2024-07-19 17:35:47 UTC",
    "updated_date": "2025-02-14 20:58:41 UTC"
  },
  {
    "arxiv_id": "2407.14467v2",
    "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
    "authors": [
      "Jayr Pereira",
      "Andre Assumpcao",
      "Roberto Lotufo"
    ],
    "abstract": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14467v2",
    "published_date": "2024-07-19 17:14:16 UTC",
    "updated_date": "2024-09-10 14:08:29 UTC"
  },
  {
    "arxiv_id": "2407.14575v2",
    "title": "Regression prediction algorithm for energy consumption regression in cloud computing based on horned lizard algorithm optimised convolutional neural network-bidirectional gated recurrent unit",
    "authors": [
      "Feiyang Li",
      "Zinan Cao",
      "Qixuan Yu",
      "Xirui Tang"
    ],
    "abstract": "For this paper, a prediction study of cloud computing energy consumption was\nconducted by optimising the data regression algorithm based on the horned\nlizard optimisation algorithm for Convolutional Neural Networks-Bi-Directional\nGated Recurrent Units. Firstly, through Spearman correlation analysis of CPU,\nusage, memory usage, network traffic, power consumption, number of instructions\nexecuted, execution time and energy efficiency, we found that power consumption\nhas the highest degree of positive correlation with energy efficiency, while\nCPU usage has the highest degree of negative correlation with energy\nefficiency. In our experiments, we introduced a random forest model and an\noptimisation model based on the horned lizard optimisation algorithm for\ntesting, and the results show that the optimisation algorithm has better\nprediction results compared to the random forest model. Specifically, the mean\nsquare error (MSE) of the optimisation algorithm is 0.01 smaller than that of\nthe random forest model, and the mean absolute error (MAE) is 0.01 smaller than\nthat of the random forest.3 The results of the combined metrics show that the\noptimisation algorithm performs more accurately and reliably in predicting\nenergy efficiency. This research result provides new ideas and methods to\nimprove the energy efficiency of cloud computing systems. This research not\nonly expands the scope of application in the field of cloud computing, but also\nprovides a strong support for improving the energy use efficiency of the\nsystem.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14575v2",
    "published_date": "2024-07-19 16:19:14 UTC",
    "updated_date": "2024-07-26 17:35:20 UTC"
  },
  {
    "arxiv_id": "2407.14430v1",
    "title": "The Extrapolation Power of Implicit Models",
    "authors": [
      "Juliette Decugis",
      "Alicia Y. Tsai",
      "Max Emerling",
      "Ashwin Ganesh",
      "Laurent El Ghaoui"
    ],
    "abstract": "In this paper, we investigate the extrapolation capabilities of implicit deep\nlearning models in handling unobserved data, where traditional deep neural\nnetworks may falter. Implicit models, distinguished by their adaptability in\nlayer depth and incorporation of feedback within their computational graph, are\nput to the test across various extrapolation scenarios: out-of-distribution,\ngeographical, and temporal shifts. Our experiments consistently demonstrate\nsignificant performance advantage with implicit models. Unlike their\nnon-implicit counterparts, which often rely on meticulous architectural design\nfor each task, implicit models demonstrate the ability to learn complex model\nstructures without the need for task-specific design, highlighting their\nrobustness in handling unseen data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Workshop on Explainable Artificial Intelligence (XAI)\n  at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14430v1",
    "published_date": "2024-07-19 16:01:37 UTC",
    "updated_date": "2024-07-19 16:01:37 UTC"
  },
  {
    "arxiv_id": "2407.14417v2",
    "title": "Mixture of Experts with Mixture of Precisions for Tuning Quality of Service",
    "authors": [
      "HamidReza Imani",
      "Abdolah Amirany",
      "Tarek El-Ghazawi"
    ],
    "abstract": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14417v2",
    "published_date": "2024-07-19 15:42:49 UTC",
    "updated_date": "2024-09-09 16:34:00 UTC"
  },
  {
    "arxiv_id": "2407.14414v2",
    "title": "System-1.x: Learning to Balance Fast and Slow Planning with Language Models",
    "authors": [
      "Swarnadeep Saha",
      "Archiki Prasad",
      "Justin Chih-Yao Chen",
      "Peter Hase",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "abstract": "Language models can be used to solve long-horizon planning problems in two\ndistinct modes: a fast 'System-1' mode, directly generating plans without any\nexplicit search or backtracking, and a slow 'System-2' mode, planning\nstep-by-step by explicitly searching over possible actions. While System-2 is\ntypically more effective, it is also more computationally expensive, making it\ninfeasible for long plans or large action spaces. Moreover, isolated System-1\nor 2 ignores the user's end goals, failing to provide ways to control the\nmodel's behavior. To this end, we propose the System-1.x Planner, a\ncontrollable planning framework with LLMs that is capable of generating hybrid\nplans and balancing between the two planning modes based on the difficulty of\nthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1\nPlanner, and (iii) a System-2 Planner. Based on a user-specified hybridization\nfactor (x) governing the mixture between System-1 and 2, the controller\ndecomposes a problem into sub-goals, and classifies them as easy or hard to be\nsolved by either System-1 or 2, respectively. We fine-tune all three components\non top of a single base LLM, requiring only search traces as supervision.\nExperiments with two diverse planning tasks -- Maze Navigation and Blocksworld\n-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2\nPlanner trained to approximate A* search, and also a symbolic planner (A*). We\ndemonstrate the following key properties of our planner: (1) controllability:\nincreasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more\nsearch, improving performance, (2) flexibility: by building a neuro-symbolic\nvariant with a neural System-1 and a symbolic System-2, we can use existing\nsymbolic methods, and (3) generalizability: by being able to learn from\ndifferent search algorithms, our method is robust to the choice of search\nalgorithm.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 (Camera-Ready)",
    "pdf_url": "http://arxiv.org/pdf/2407.14414v2",
    "published_date": "2024-07-19 15:40:59 UTC",
    "updated_date": "2025-04-15 03:41:17 UTC"
  },
  {
    "arxiv_id": "2407.14412v1",
    "title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs",
    "authors": [
      "Tang Li",
      "Mengmeng Ma",
      "Xi Peng"
    ],
    "abstract": "Large pre-trained Vision-Language Models (VLMs) have become ubiquitous\nfoundational components of other models and downstream tasks. Although\npowerful, our empirical results reveal that such models might not be able to\nidentify fine-grained concepts. Specifically, the explanations of VLMs with\nrespect to fine-grained concepts are entangled and mislocalized. To address\nthis issue, we propose to DisEntAngle and Localize (DEAL) the concept-level\nexplanations for VLMs without human annotations. The key idea is encouraging\nthe concept-level explanations to be distinct while maintaining consistency\nwith category-level explanations. We conduct extensive experiments and ablation\nstudies on a wide range of benchmark datasets and vision-language models. Our\nempirical results demonstrate that the proposed method significantly improves\nthe concept-level explanations of the model in terms of disentanglability and\nlocalizability. Surprisingly, the improved explainability alleviates the\nmodel's reliance on spurious correlations, which further benefits the\nprediction accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "In Proceedings of the European Conference on Computer Vision (ECCV),\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14412v1",
    "published_date": "2024-07-19 15:39:19 UTC",
    "updated_date": "2024-07-19 15:39:19 UTC"
  },
  {
    "arxiv_id": "2407.14402v1",
    "title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?",
    "authors": [
      "Zhiyang Zhang",
      "Fangkai Yang",
      "Xiaoting Qin",
      "Jue Zhang",
      "Qingwei Lin",
      "Gong Cheng",
      "Dongmei Zhang",
      "Saravan Rajmohan",
      "Qi Zhang"
    ],
    "abstract": "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "cs.MA",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14402v1",
    "published_date": "2024-07-19 15:30:32 UTC",
    "updated_date": "2024-07-19 15:30:32 UTC"
  },
  {
    "arxiv_id": "2407.14400v1",
    "title": "On the Impact of PRB Load Uncertainty Forecasting for Sustainable Open RAN",
    "authors": [
      "Vaishnavi Kasuluru",
      "Luis Blanco",
      "Cristian J. Vaca-Rubio",
      "Engin Zeydan"
    ],
    "abstract": "The transition to sustainable Open Radio Access Network (O-RAN) architectures\nbrings new challenges for resource management, especially in predicting the\nutilization of Physical Resource Block (PRB)s. In this paper, we propose a\nnovel approach to characterize the PRB load using probabilistic forecasting\ntechniques. First, we provide background information on the O-RAN architecture\nand components and emphasize the importance of energy/power consumption models\nfor sustainable implementations. The problem statement highlights the need for\naccurate PRB load prediction to optimize resource allocation and power\nefficiency. We then investigate probabilistic forecasting techniques, including\nSimple-Feed-Forward (SFF), DeepAR, and Transformers, and discuss their\nlikelihood model assumptions. The simulation results show that DeepAR\nestimators predict the PRBs with less uncertainty and effectively capture the\ntemporal dependencies in the dataset compared to SFF- and Transformer-based\nmodels, leading to power savings. Different percentile selections can also\nincrease power savings, but at the cost of over-/under provisioning. At the\nsame time, the performance of the Long-Short Term Memory (LSTM) is shown to be\ninferior to the probabilistic estimators with respect to all error metrics.\nFinally, we outline the importance of probabilistic, prediction-based\ncharacterization for sustainable O-RAN implementations and highlight avenues\nfor future research.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14400v1",
    "published_date": "2024-07-19 15:25:20 UTC",
    "updated_date": "2024-07-19 15:25:20 UTC"
  },
  {
    "arxiv_id": "2407.14394v2",
    "title": "TTT: A Temporal Refinement Heuristic for Tenuously Tractable Discrete Time Reachability Problems",
    "authors": [
      "Chelsea Sidrane",
      "Jana Tumova"
    ],
    "abstract": "Reachable set computation is an important tool for analyzing control systems.\nSimulating a control system can show general trends, but a formal tool like\nreachability analysis can provide guarantees of correctness. Reachability\nanalysis for complex control systems, e.g., with nonlinear dynamics and/or a\nneural network controller, is often either slow or overly conservative. To\naddress these challenges, much literature has focused on spatial refinement,\ni.e., tuning the discretization of the input sets and intermediate reachable\nsets. This paper introduces the idea of temporal refinement: automatically\nchoosing when along the horizon of the reachability problem to execute slow\nsymbolic queries which incur less approximation error versus fast concrete\nqueries which incur more approximation error. Temporal refinement can be\ncombined with other refinement approaches as an additional tool to trade off\ntractability and tightness in approximate reachable set computation. We\nintroduce a temporal refinement algorithm and demonstrate its effectiveness at\ncomputing approximate reachable sets for nonlinear systems with neural network\ncontrollers. We calculate reachable sets with varying computational budget and\nshow that our algorithm can generate approximate reachable sets with a similar\namount of error to the baseline in 20-70% less time.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "To appear in the proceedings of the American Control Conference (ACC)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2407.14394v2",
    "published_date": "2024-07-19 15:16:25 UTC",
    "updated_date": "2025-05-06 16:18:38 UTC"
  },
  {
    "arxiv_id": "2407.14387v1",
    "title": "GLAudio Listens to the Sound of the Graph",
    "authors": [
      "Aurelio Sulser",
      "Johann Wenckstern",
      "Clara Kuempel"
    ],
    "abstract": "We propose GLAudio: Graph Learning on Audio representation of the node\nfeatures and the connectivity structure. This novel architecture propagates the\nnode features through the graph network according to the discrete wave equation\nand then employs a sequence learning architecture to learn the target node\nfunction from the audio wave signal. This leads to a new paradigm of learning\non graph-structured data, in which information propagation and information\nprocessing are separated into two distinct steps. We theoretically characterize\nthe expressivity of our model, introducing the notion of the receptive field of\na vertex, and investigate our model's susceptibility to over-smoothing and\nover-squashing both theoretically as well as experimentally on various graph\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14387v1",
    "published_date": "2024-07-19 15:13:22 UTC",
    "updated_date": "2024-07-19 15:13:22 UTC"
  },
  {
    "arxiv_id": "2407.14384v1",
    "title": "The Sticky Path to Expressive Querying: Decidability of Navigational Queries under Existential Rules",
    "authors": [
      "Piotr Ostropolski-Nalewaja",
      "Sebastian Rudolph"
    ],
    "abstract": "Extensive research in the field of ontology-based query answering has led to\nthe identification of numerous fragments of existential rules (also known as\ntuple-generating dependencies) that exhibit decidable answering of atomic and\nconjunctive queries. Motivated by the increased theoretical and practical\ninterest in navigational queries, this paper considers the question for which\nof these fragments decidability of querying extends to regular path queries\n(RPQs). In fact, decidability of RPQs has recently been shown to generally hold\nfor the comprehensive family of all fragments that come with the guarantee of\nuniversal models being reasonably well-shaped (that is, being of finite\ncliquewidth). Yet, for the second major family of fragments, known as finite\nunification sets (short: fus), which are based on first-order-rewritability,\ncorresponding results have been largely elusive so far. We complete the picture\nby showing that RPQ answering over arbitrary fus rulesets is undecidable. On\nthe positive side, we establish that the problem is decidable for the prominent\nfus subclass of sticky rulesets, with the caveat that a very mild extension of\nthe RPQ formalism turns the problem undecidable again.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14384v1",
    "published_date": "2024-07-19 15:11:09 UTC",
    "updated_date": "2024-07-19 15:11:09 UTC"
  },
  {
    "arxiv_id": "2407.14377v1",
    "title": "Enhancing Cloud-Native Resource Allocation with Probabilistic Forecasting Techniques in O-RAN",
    "authors": [
      "Vaishnavi Kasuluru",
      "Luis Blanco",
      "Engin Zeydan",
      "Albert Bel",
      "Angelos Antonopoulos"
    ],
    "abstract": "The need for intelligent and efficient resource provisioning for the\nproductive management of resources in real-world scenarios is growing with the\nevolution of telecommunications towards the 6G era. Technologies such as Open\nRadio Access Network (O-RAN) can help to build interoperable solutions for the\nmanagement of complex systems. Probabilistic forecasting, in contrast to\ndeterministic single-point estimators, can offer a different approach to\nresource allocation by quantifying the uncertainty of the generated\npredictions. This paper examines the cloud-native aspects of O-RAN together\nwith the radio App (rApp) deployment options. The integration of probabilistic\nforecasting techniques as a rApp in O-RAN is also emphasized, along with case\nstudies of real-world applications. Through a comparative analysis of\nforecasting models using the error metric, we show the advantages of Deep\nAutoregressive Recurrent network (DeepAR) over other deterministic\nprobabilistic estimators. Furthermore, the simplicity of Simple-Feed-Forward\n(SFF) leads to a fast runtime but does not capture the temporal dependencies of\nthe input data. Finally, we present some aspects related to the practical\napplicability of cloud-native O-RAN with probabilistic forecasting.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14377v1",
    "published_date": "2024-07-19 15:04:15 UTC",
    "updated_date": "2024-07-19 15:04:15 UTC"
  },
  {
    "arxiv_id": "2407.14375v1",
    "title": "On the use of Probabilistic Forecasting for Network Analysis in Open RAN",
    "authors": [
      "Vaishnavi Kasuluru",
      "Luis Blanco",
      "Engin Zeydan"
    ],
    "abstract": "Unlike other single-point Artificial Intelligence (AI)-based prediction\ntechniques, such as Long-Short Term Memory (LSTM), probabilistic forecasting\ntechniques (e.g., DeepAR and Transformer) provide a range of possible outcomes\nand associated probabilities that enable decision makers to make more informed\nand robust decisions. At the same time, the architecture of Open RAN has\nemerged as a revolutionary approach for mobile networks, aiming at openness,\ninteroperability and innovation in the ecosystem of RAN. In this paper, we\npropose the use of probabilistic forecasting techniques as a radio App (rApp)\nwithin the Open RAN architecture. We investigate and compare different\nprobabilistic and single-point forecasting methods and algorithms to estimate\nthe utilization and resource demands of Physical Resource Blocks (PRBs) of\ncellular base stations. Through our evaluations, we demonstrate the numerical\nadvantages of probabilistic forecasting techniques over traditional\nsingle-point forecasting methods and show that they are capable of providing\nmore accurate and reliable estimates. In particular, DeepAR clearly outperforms\nsingle-point forecasting techniques such as LSTM and Seasonal-Naive (SN)\nbaselines and other probabilistic forecasting techniques such as\nSimple-Feed-Forward (SFF) and Transformer neural networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14375v1",
    "published_date": "2024-07-19 15:03:38 UTC",
    "updated_date": "2024-07-19 15:03:38 UTC"
  },
  {
    "arxiv_id": "2407.14372v1",
    "title": "SCoPE: Evaluating LLMs for Software Vulnerability Detection",
    "authors": [
      "JosÃ© GonÃ§alves",
      "Tiago Dias",
      "Eva Maia",
      "Isabel PraÃ§a"
    ],
    "abstract": "In recent years, code security has become increasingly important, especially\nwith the rise of interconnected technologies. Detecting vulnerabilities early\nin the software development process has demonstrated numerous benefits.\nConsequently, the scientific community started using machine learning for\nautomated detection of source code vulnerabilities. This work explores and\nrefines the CVEFixes dataset, which is commonly used to train models for\ncode-related tasks, specifically the C/C++ subset. To this purpose, the Source\nCode Processing Engine (SCoPE), a framework composed of strategized techniques\nthat can be used to reduce the size and normalize C/C++ functions is presented.\nThe output generated by SCoPE was used to create a new version of CVEFixes.\nThis refined dataset was then employed in a feature representation analysis to\nassess the effectiveness of the tool's code processing techniques, consisting\nof fine-tuning three pre-trained LLMs for software vulnerability detection. The\nresults show that SCoPE successfully helped to identify 905 duplicates within\nthe evaluated subset. The LLM results corroborate with the literature regarding\ntheir suitability for software vulnerability detection, with the best model\nachieving 53% F1-score.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 3 figures, 1 table, published in DCAI 24 conference",
    "pdf_url": "http://arxiv.org/pdf/2407.14372v1",
    "published_date": "2024-07-19 15:02:00 UTC",
    "updated_date": "2024-07-19 15:02:00 UTC"
  },
  {
    "arxiv_id": "2407.14364v2",
    "title": "Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio",
    "authors": [
      "Roser Batlle-Roca",
      "Wei-Hisang Liao",
      "Xavier Serra",
      "Yuki Mitsufuji",
      "Emilia GÃ³mez"
    ],
    "abstract": "Recent advancements in music generation are raising multiple concerns about\nthe implications of AI in creative music processes, current business models and\nimpacts related to intellectual property management. A relevant discussion and\nrelated technical challenge is the potential replication and plagiarism of the\ntraining set in AI-generated music, which could lead to misuse of data and\nintellectual property rights violations. To tackle this issue, we present the\nMusic Replication Assessment (MiRA) tool: a model-independent open evaluation\nmethod based on diverse audio music similarity metrics to assess data\nreplication. We evaluate the ability of five metrics to identify exact\nreplication by conducting a controlled replication experiment in different\nmusic genres using synthetic samples. Our results show that the proposed\nmethodology can estimate exact data replication with a proportion higher than\n10%. By introducing the MiRA tool, we intend to encourage the open evaluation\nof music-generative models by researchers, developers, and users concerning\ndata replication, highlighting the importance of the ethical, social, legal,\nand economic consequences. Code and examples are available for reproducibility\npurposes.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ISMIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14364v2",
    "published_date": "2024-07-19 14:52:11 UTC",
    "updated_date": "2024-08-01 11:16:30 UTC"
  },
  {
    "arxiv_id": "2407.14361v1",
    "title": "FuzzTheREST: An Intelligent Automated Black-box RESTful API Fuzzer",
    "authors": [
      "Tiago Dias",
      "Eva Maia",
      "Isabel PraÃ§a"
    ],
    "abstract": "Software's pervasive impact and increasing reliance in the era of digital\ntransformation raise concerns about vulnerabilities, emphasizing the need for\nsoftware security. Fuzzy testing is a dynamic analysis software testing\ntechnique that consists of feeding faulty input data to a System Under Test\n(SUT) and observing its behavior. Specifically regarding black-box RESTful API\ntesting, recent literature has attempted to automate this technique using\nheuristics to perform the input search and using the HTTP response status codes\nfor classification. However, most approaches do not keep track of code\ncoverage, which is important to validate the solution. This work introduces a\nblack-box RESTful API fuzzy testing tool that employs Reinforcement Learning\n(RL) for vulnerability detection. The fuzzer operates via the OpenAPI\nSpecification (OAS) file and a scenarios file, which includes information to\ncommunicate with the SUT and the sequences of functionalities to test,\nrespectively. To evaluate its effectiveness, the tool was tested on the\nPetstore API. The tool found a total of six unique vulnerabilities and achieved\n55\\% code coverage.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 4 figures, published in DCAI 2024 conference, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.14361v1",
    "published_date": "2024-07-19 14:43:35 UTC",
    "updated_date": "2024-07-19 14:43:35 UTC"
  },
  {
    "arxiv_id": "2407.14358v2",
    "title": "Stable Audio Open",
    "authors": [
      "Zach Evans",
      "Julian D. Parker",
      "CJ Carr",
      "Zack Zukowski",
      "Josiah Taylor",
      "Jordi Pons"
    ],
    "abstract": "Open generative models are vitally important for the community, allowing for\nfine-tunes and serving as baselines when presenting new models. However, most\ncurrent text-to-audio models are private and not accessible for artists and\nresearchers to build upon. Here we describe the architecture and training\nprocess of a new open-weights text-to-audio model trained with Creative Commons\ndata. Our evaluation shows that the model's performance is competitive with the\nstate-of-the-art across various metrics. Notably, the reported FDopenl3 results\n(measuring the realism of the generations) showcase its potential for\nhigh-quality stereo sound synthesis at 44.1kHz.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Demo: https://stability-ai.github.io/stable-audio-open-demo/ Weights:\n  https://huggingface.co/stabilityai/stable-audio-open-1.0 Code:\n  https://github.com/Stability-AI/stable-audio-tools. arXiv admin note: text\n  overlap with arXiv:2404.10301",
    "pdf_url": "http://arxiv.org/pdf/2407.14358v2",
    "published_date": "2024-07-19 14:40:23 UTC",
    "updated_date": "2024-07-31 16:22:42 UTC"
  },
  {
    "arxiv_id": "2407.14344v2",
    "title": "LLMs left, right, and center: Assessing GPT's capabilities to label political bias from web domains",
    "authors": [
      "Raphael Hernandes",
      "Giulio Corsi"
    ],
    "abstract": "This research investigates whether OpenAI's GPT-4, a state-of-the-art large\nlanguage model, can accurately classify the political bias of news sources\nbased solely on their URLs. Given the subjective nature of political labels,\nthird-party bias ratings like those from Ad Fontes Media, AllSides, and Media\nBias/Fact Check (MBFC) are often used in research to analyze news source\ndiversity. This study aims to determine if GPT-4 can replicate these human\nratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis\ncompares GPT-4's classifications against MBFC's, and controls for website\npopularity using Open PageRank scores. Findings reveal a high correlation\n($\\text{Spearman's } \\rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and\nMBFC's ratings, indicating the model's potential reliability. However, GPT-4\nabstained from classifying approximately $\\frac{2}{3}$ of the dataset. It is\nmore likely to abstain from rating unpopular websites, which also suffer from\nless accurate assessments. The LLM tends to avoid classifying sources that MBFC\nconsiders to be centrist, resulting in more polarized outputs. Finally, this\nanalysis shows a slight leftward skew in GPT's classifications compared to\nMBFC's. Therefore, while this paper suggests that while GPT-4 can be a\nscalable, cost-effective tool for political bias classification of news\nwebsites, its use should be as a complement to human judgment to mitigate\nbiases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14344v2",
    "published_date": "2024-07-19 14:28:07 UTC",
    "updated_date": "2024-10-22 16:59:12 UTC"
  },
  {
    "arxiv_id": "2407.14326v1",
    "title": "Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model",
    "authors": [
      "Kun Zhao",
      "Jakub Prokop",
      "Javier Montalt Tordera",
      "Sadegh Mohammadi"
    ],
    "abstract": "Mammography is crucial for breast cancer surveillance and early diagnosis.\nHowever, analyzing mammography images is a demanding task for radiologists, who\noften review hundreds of mammograms daily, leading to overdiagnosis and\novertreatment. Computer-Aided Diagnosis (CAD) systems have been developed to\nassist in this process, but their capabilities, particularly in lesion\nsegmentation, remained limited. With the contemporary advances in deep learning\ntheir performance may be improved. Recently, vision-language diffusion models\nemerged, demonstrating outstanding performance in image generation and\ntransferability to various downstream tasks. We aim to harness their\ncapabilities for breast lesion segmentation in a panoptic setting, which\nencompasses both semantic and instance-level predictions. Specifically, we\npropose leveraging pretrained features from a Stable Diffusion model as inputs\nto a state-of-the-art panoptic segmentation architecture, resulting in accurate\ndelineation of individual breast lesions. To bridge the gap between natural and\nmedical imaging domains, we incorporated a mammography-specific MAM-E diffusion\nmodel and BiomedCLIP image and text encoders into this framework. We evaluated\nour approach on two recently published mammography datasets, CDD-CESM and\nVinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82\nAP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentation\ntask, we achieved Dice scores of 38.86 and 40.92, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 4 figures. Submitted to Deep Generative Models workshop @\n  MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14326v1",
    "published_date": "2024-07-19 14:04:05 UTC",
    "updated_date": "2024-07-19 14:04:05 UTC"
  },
  {
    "arxiv_id": "2407.14314v1",
    "title": "EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition",
    "authors": [
      "Youssef Doulfoukar",
      "Laurent Mertens",
      "Joost Vennekens"
    ],
    "abstract": "Convolutional Neural Networks are particularly suited for image analysis\ntasks, such as Image Classification, Object Recognition or Image Segmentation.\nLike all Artificial Neural Networks, however, they are \"black box\" models, and\nsuffer from poor explainability. This work is concerned with the specific\ndownstream task of Emotion Recognition from images, and proposes a framework\nthat combines CAM-based techniques with Object Detection on a corpus level to\nbetter understand on which image cues a particular model, in our case EmoNet,\nrelies to assign a specific emotion to an image. We demonstrate that the model\nmostly focuses on human characteristics, but also explore the pronounced effect\nof specific image modifications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14314v1",
    "published_date": "2024-07-19 13:47:02 UTC",
    "updated_date": "2024-07-19 13:47:02 UTC"
  },
  {
    "arxiv_id": "2407.14309v2",
    "title": "How to Engage Your Readers? Generating Guiding Questions to Promote Active Reading",
    "authors": [
      "Peng Cui",
      "VilÃ©m Zouhar",
      "Xiaoyu Zhang",
      "Mrinmaya Sachan"
    ],
    "abstract": "Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14309v2",
    "published_date": "2024-07-19 13:42:56 UTC",
    "updated_date": "2024-07-29 01:19:12 UTC"
  },
  {
    "arxiv_id": "2407.14306v2",
    "title": "Label-Free Model Failure Detection for Lidar-based Point Cloud Segmentation",
    "authors": [
      "Daniel Bogdoll",
      "Finn Sartoris",
      "Vincent Geppert",
      "Svetlana Pavlitska",
      "J. Marius ZÃ¶llner"
    ],
    "abstract": "Autonomous vehicles drive millions of miles on the road each year. Under such\ncircumstances, deployed machine learning models are prone to failure both in\nseemingly normal situations and in the presence of outliers. However, in the\ntraining phase, they are only evaluated on small validation and test sets,\nwhich are unable to reveal model failures due to their limited scenario\ncoverage. While it is difficult and expensive to acquire large and\nrepresentative labeled datasets for evaluation, large-scale unlabeled datasets\nare typically available. In this work, we introduce label-free model failure\ndetection for lidar-based point cloud segmentation, taking advantage of the\nabundance of unlabeled data available. We leverage different data\ncharacteristics by training a supervised and self-supervised stream for the\nsame task to detect failure modes. We perform a large-scale qualitative\nanalysis and present LidarCODA, the first publicly available dataset with\nlabeled anomalies in real-world lidar data, for an extensive quantitative\nanalysis.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Daniel Bogdoll, Finn Sartoris, and Vincent Geppert contributed\n  equally. Accepted for publication at IV 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.14306v2",
    "published_date": "2024-07-19 13:36:35 UTC",
    "updated_date": "2025-04-24 09:40:16 UTC"
  },
  {
    "arxiv_id": "2407.20250v1",
    "title": "Riemannian Geometry-Based EEG Approaches: A Literature Review",
    "authors": [
      "Imad Eddine Tibermacine",
      "Samuele Russo",
      "Ahmed Tibermacine",
      "Abdelaziz Rabehi",
      "Bachir Nail",
      "Kamel Kadri",
      "Christian Napoli"
    ],
    "abstract": "The application of Riemannian geometry in the decoding of brain-computer\ninterfaces (BCIs) has swiftly garnered attention because of its\nstraightforwardness, precision, and resilience, along with its aptitude for\ntransfer learning, which has been demonstrated through significant achievements\nin global BCI competitions. This paper presents a comprehensive review of\nrecent advancements in the integration of deep learning with Riemannian\ngeometry to enhance EEG signal decoding in BCIs. Our review updates the\nfindings since the last major review in 2017, comparing modern approaches that\nutilize deep learning to improve the handling of non-Euclidean data structures\ninherent in EEG signals. We discuss how these approaches not only tackle the\ntraditional challenges of noise sensitivity, non-stationarity, and lengthy\ncalibration times but also introduce novel classification frameworks and signal\nprocessing techniques to reduce these limitations significantly. Furthermore,\nwe identify current shortcomings and propose future research directions in\nmanifold learning and riemannian-based classification, focusing on practical\nimplementations and theoretical expansions, such as feature tracking on\nmanifolds, multitask learning, feature extraction, and transfer learning. This\nreview aims to bridge the gap between theoretical research and practical,\nreal-world applications, making sophisticated mathematical approaches\naccessible and actionable for BCI enhancements.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20250v1",
    "published_date": "2024-07-19 13:28:29 UTC",
    "updated_date": "2024-07-19 13:28:29 UTC"
  },
  {
    "arxiv_id": "2407.14295v1",
    "title": "CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on Intonation Units",
    "authors": [
      "Yeeun Kang"
    ],
    "abstract": "Multilingual code-switching research is often hindered by the lack and\nlinguistically biased status of available datasets. To expand language\nrepresentation, we synthesize code-switching data by replacing intonation units\ndetected through PSST, a speech segmentation model fine-tuned from OpenAI's\nWhisper, using a speech-to-text translation dataset, CoVoST 2. With our\ndataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching\ntranslation performance of two multilingual translation models, M2M-100 418M\nand NLLB-200 600M. We reveal that the inclusion of code-switching units results\nin higher translation performance than monolingual settings and that models are\nbetter at code-switching translation into English than non-English. Further,\nlow-resource languages gain most from integration of code-switched units when\ntranslating into English but much less when translating into non-English.\nTranslations into low-resource languages also perform worse than even raw\ncode-switched inputs. We find that systems excel at copying English tokens but\nstruggle with non-English tokens, that the off-target problem in monolingual\nsettings is also relevant in code-switching settings, and that models\nhallucinate in code-switching translation by introducing words absent in both\nof the original source sentences. CoVoSwitch and code are available at\nhttps://github.com/sophiayk20/covoswitch.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 Student Research Workshop (ACL-SRW 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.14295v1",
    "published_date": "2024-07-19 13:26:35 UTC",
    "updated_date": "2024-07-19 13:26:35 UTC"
  },
  {
    "arxiv_id": "2407.14280v2",
    "title": "How to Blend Concepts in Diffusion Models",
    "authors": [
      "Lorenzo Olearo",
      "Giorgio Longari",
      "Simone Melzi",
      "Alessandro Raganato",
      "Rafael PeÃ±aloza"
    ],
    "abstract": "For the last decade, there has been a push to use multi-dimensional (latent)\nspaces to represent concepts; and yet how to manipulate these concepts or\nreason with them remains largely unclear. Some recent methods exploit multiple\nlatent representations and their connection, making this research question even\nmore entangled. Our goal is to understand how operations in the latent space\naffect the underlying concepts. To that end, we explore the task of concept\nblending through diffusion models. Diffusion models are based on a connection\nbetween a latent representation of textual prompts and a latent space that\nenables image reconstruction and generation. This task allows us to try\ndifferent text-based combination strategies, and evaluate easily through a\nvisual analysis. Our conclusion is that concept blending through space\nmanipulation is possible, although the best strategy depends on the context of\nthe blend.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14280v2",
    "published_date": "2024-07-19 13:05:57 UTC",
    "updated_date": "2024-09-22 07:02:35 UTC"
  },
  {
    "arxiv_id": "2407.14274v2",
    "title": "Mixed-precision Neural Networks on RISC-V Cores: ISA extensions for Multi-Pumped Soft SIMD Operations",
    "authors": [
      "Giorgos Armeniakos",
      "Alexis Maras",
      "Sotirios Xydis",
      "Dimitrios Soudris"
    ],
    "abstract": "Recent advancements in quantization and mixed-precision approaches offers\nsubstantial opportunities to improve the speed and energy efficiency of Neural\nNetworks (NN). Research has shown that individual parameters with varying low\nprecision, can attain accuracies comparable to full-precision counterparts.\nHowever, modern embedded microprocessors provide very limited support for\nmixed-precision NNs regarding both Instruction Set Architecture (ISA)\nextensions and their hardware design for efficient execution of mixed-precision\noperations, i.e., introducing several performance bottlenecks due to numerous\ninstructions for data packing and unpacking, arithmetic unit under-utilizations\netc. In this work, we bring together, for the first time, ISA extensions\ntailored to mixed-precision hardware optimizations, targeting energy-efficient\nDNN inference on leading RISC-V CPU architectures. To this end, we introduce a\nhardware-software co-design framework that enables cooperative hardware design,\nmixed-precision quantization, ISA extensions and inference in cycle-accurate\nemulations. At hardware level, we firstly expand the ALU unit within our\nproof-of-concept micro-architecture to support configurable fine grained\nmixed-precision arithmetic operations. Subsequently, we implement multi-pumping\nto minimize execution latency, with an additional soft SIMD optimization\napplied for 2-bit operations. At the ISA level, three distinct MAC instructions\nare encoded extending the RISC-V ISA, and exposed up to the compiler level,\neach corresponding to a different mixed-precision operational mode. Our\nextensive experimental evaluation over widely used DNNs and datasets, such as\nCIFAR10 and ImageNet, demonstrates that our framework can achieve, on average,\n15x energy reduction for less than 1% accuracy loss and outperforms the\nISA-agnostic state-of-the-art RISC-V cores.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted for publication at the 43rd International Conference on\n  Computer-Aided Design (ICCAD `24), Oct 27-31 2024, New Jersey, USA",
    "pdf_url": "http://arxiv.org/pdf/2407.14274v2",
    "published_date": "2024-07-19 12:54:04 UTC",
    "updated_date": "2024-08-13 11:40:53 UTC"
  },
  {
    "arxiv_id": "2407.14262v1",
    "title": "Hyperparameter Optimization for Driving Strategies Based on Reinforcement Learning",
    "authors": [
      "Nihal Acharya Adde",
      "Hanno Gottschalk",
      "Andreas Ebert"
    ],
    "abstract": "This paper focuses on hyperparameter optimization for autonomous driving\nstrategies based on Reinforcement Learning. We provide a detailed description\nof training the RL agent in a simulation environment. Subsequently, we employ\nEfficient Global Optimization algorithm that uses Gaussian Process fitting for\nhyperparameter optimization in RL. Before this optimization phase, Gaussian\nprocess interpolation is applied to fit the surrogate model, for which the\nhyperparameter set is generated using Latin hypercube sampling. To accelerate\nthe evaluation, parallelization techniques are employed. Following the\nhyperparameter optimization procedure, a set of hyperparameters is identified,\nresulting in a noteworthy enhancement in overall driving performance. There is\na substantial increase of 4\\% when compared to existing manually tuned\nparameters and the hyperparameters discovered during the initialization process\nusing Latin hypercube sampling. After the optimization, we analyze the obtained\nresults thoroughly and conduct a sensitivity analysis to assess the robustness\nand generalization capabilities of the learned autonomous driving strategies.\nThe findings from this study contribute to the advancement of Gaussian process\nbased Bayesian optimization to optimize the hyperparameters for autonomous\ndriving in RL, providing valuable insights for the development of efficient and\nreliable autonomous driving systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted and accepted by LOD24 conference\n  https://lod2024.icas.events/",
    "pdf_url": "http://arxiv.org/pdf/2407.14262v1",
    "published_date": "2024-07-19 12:40:08 UTC",
    "updated_date": "2024-07-19 12:40:08 UTC"
  },
  {
    "arxiv_id": "2408.03329v1",
    "title": "Coverage-aware and Reinforcement Learning Using Multi-agent Approach for HD Map QoS in a Realistic Environment",
    "authors": [
      "Jeffrey Redondo",
      "Zhenhui Yuan",
      "Nauman Aslam",
      "Juan Zhang"
    ],
    "abstract": "One effective way to optimize the offloading process is by minimizing the\ntransmission time. This is particularly true in a Vehicular Adhoc Network\n(VANET) where vehicles frequently download and upload High-definition (HD) map\ndata which requires constant updates. This implies that latency and throughput\nrequirements must be guaranteed by the wireless system. To achieve this,\nadjustable contention windows (CW) allocation strategies in the standard\nIEEE802.11p have been explored by numerous researchers. Nevertheless, their\nimplementations demand alterations to the existing standard which is not always\ndesirable. To address this issue, we proposed a Q-Learning algorithm that\noperates at the application layer. Moreover, it could be deployed in any\nwireless network thereby mitigating the compatibility issues. The solution has\ndemonstrated a better network performance with relatively fewer optimization\nrequirements as compared to the Deep Q Network (DQN) and Actor-Critic\nalgorithms. The same is observed while evaluating the model in a multi-agent\nsetup showing higher performance compared to the single-agent setup.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03329v1",
    "published_date": "2024-07-19 12:40:07 UTC",
    "updated_date": "2024-07-19 12:40:07 UTC"
  },
  {
    "arxiv_id": "2407.14251v1",
    "title": "Personalized Multi-tier Federated Learning",
    "authors": [
      "Sourasekhar Banerjee",
      "Ali Dadras",
      "Alp Yurtsever",
      "Monowar Bhuyan"
    ],
    "abstract": "The key challenge of personalized federated learning (PerFL) is to capture\nthe statistical heterogeneity properties of data with inexpensive\ncommunications and gain customized performance for participating devices. To\naddress these, we introduced personalized federated learning in multi-tier\narchitecture (PerMFL) to obtain optimized and personalized local models when\nthere are known team structures across devices. We provide theoretical\nguarantees of PerMFL, which offers linear convergence rates for smooth strongly\nconvex problems and sub-linear convergence rates for smooth non-convex\nproblems. We conduct numerical experiments demonstrating the robust empirical\nperformance of PerMFL, outperforming the state-of-the-art in multiple\npersonalized federated learning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14251v1",
    "published_date": "2024-07-19 12:31:15 UTC",
    "updated_date": "2024-07-19 12:31:15 UTC"
  },
  {
    "arxiv_id": "2407.14239v1",
    "title": "KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models",
    "authors": [
      "Kemou Jiang",
      "Xuan Cai",
      "Zhiyong Cui",
      "Aoyong Li",
      "Yilong Ren",
      "Haiyang Yu",
      "Hao Yang",
      "Daocheng Fu",
      "Licheng Wen",
      "Pinlong Cai"
    ],
    "abstract": "Large language models (LLMs) as autonomous agents offer a novel avenue for\ntackling real-world challenges through a knowledge-driven manner. These\nLLM-enhanced methodologies excel in generalization and interpretability.\nHowever, the complexity of driving tasks often necessitates the collaboration\nof multiple, heterogeneous agents, underscoring the need for such LLM-driven\nagents to engage in cooperative knowledge sharing and cognitive synergy.\nDespite the promise of LLMs, current applications predominantly center around\nsingle agent scenarios. To broaden the horizons of knowledge-driven strategies\nand bolster the generalization capabilities of autonomous agents, we propose\nthe KoMA framework consisting of multi-agent interaction, multi-step planning,\nshared-memory, and ranking-based reflection modules to enhance multi-agents'\ndecision-making in complex driving scenarios. Based on the framework's\ngenerated text descriptions of driving scenarios, the multi-agent interaction\nmodule enables LLM agents to analyze and infer the intentions of surrounding\nvehicles, akin to human cognition. The multi-step planning module enables LLM\nagents to analyze and obtain final action decisions layer by layer to ensure\nconsistent goals for short-term action decisions. The shared memory module can\naccumulate collective experience to make superior decisions, and the\nranking-based reflection module can evaluate and improve agent behavior with\nthe aim of enhancing driving safety and efficiency. The KoMA framework not only\nenhances the robustness and adaptability of autonomous driving agents but also\nsignificantly elevates their generalization capabilities across diverse\nscenarios. Empirical results demonstrate the superiority of our approach over\ntraditional methods, particularly in its ability to handle complex,\nunpredictable driving environments without extensive retraining.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14239v1",
    "published_date": "2024-07-19 12:13:08 UTC",
    "updated_date": "2024-07-19 12:13:08 UTC"
  },
  {
    "arxiv_id": "2407.14237v1",
    "title": "Hyper-Heuristics Can Profit From Global Variation Operators",
    "authors": [
      "Benjamin Doerr",
      "Johannes F. Lutzeyer"
    ],
    "abstract": "In recent work, Lissovoi, Oliveto, and Warwicker (Artificial Intelligence\n(2023)) proved that the Move Acceptance Hyper-Heuristic (MAHH) leaves the local\noptimum of the multimodal CLIFF benchmark with remarkable efficiency. The\n$O(n^3)$ runtime of the MAHH, for almost all cliff widths $d\\ge 2,$ is\nsignificantly better than the $\\Theta(n^d)$ runtime of simple elitist\nevolutionary algorithms (EAs) on CLIFF.\n  In this work, we first show that this advantage is specific to the CLIFF\nproblem and does not extend to the JUMP benchmark, the most prominent\nmulti-modal benchmark in the theory of randomized search heuristics. We prove\nthat for any choice of the MAHH selection parameter $p$, the expected runtime\nof the MAHH on a JUMP function with gap size $m = O(n^{1/2})$ is at least\n$\\Omega(n^{2m-1} / (2m-1)!)$. This is significantly slower than the $O(n^m)$\nruntime of simple elitist EAs. Encouragingly, we also show that replacing the\nlocal one-bit mutation operator in the MAHH with the global bit-wise mutation\noperator, commonly used in EAs, yields a runtime of $\\min\\{1,\nO(\\frac{e\\ln(n)}{m})^m\\} \\, O(n^m)$ on JUMP functions. This is at least as good\nas the runtime of simple elitist EAs. For larger values of $m$, this result\nproves an asymptotic performance gain over simple EAs. As our proofs reveal,\nthe MAHH profits from its ability to walk through the valley of lower objective\nvalues in moderate-size steps, always accepting inferior solutions. This is the\nfirst time that such an optimization behavior is proven via mathematical means.\nGenerally, our result shows that combining two ways of coping with local\noptima, global mutation and accepting inferior solutions, can lead to\nconsiderable performance gains.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.NE",
    "comment": "Continues and significantly extends work presented in the GECCO 2023\n  conference paper arXiv:2304.10414",
    "pdf_url": "http://arxiv.org/pdf/2407.14237v1",
    "published_date": "2024-07-19 12:10:05 UTC",
    "updated_date": "2024-07-19 12:10:05 UTC"
  },
  {
    "arxiv_id": "2407.14229v2",
    "title": "Words2Contact: Identifying Support Contacts from Verbal Instructions Using Foundation Models",
    "authors": [
      "Dionis Totsila",
      "Quentin Rouxel",
      "Jean-Baptiste Mouret",
      "Serena Ivaldi"
    ],
    "abstract": "This paper presents Words2Contact, a language-guided multi-contact placement\npipeline leveraging large language models and vision language models. Our\nmethod is a key component for language-assisted teleoperation and human-robot\ncooperation, where human operators can instruct the robots where to place their\nsupport contacts before whole-body reaching or manipulation using natural\nlanguage. Words2Contact transforms the verbal instructions of a human operator\ninto contact placement predictions; it also deals with iterative corrections,\nuntil the human is satisfied with the contact location identified in the\nrobot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and\nperformance in contact prediction. We demonstrate the effectiveness of the\niterative correction process, showing that users, even naive, quickly learn how\nto instruct the system to obtain accurate locations. Finally, we validate\nWords2Contact in real-world experiments with the Talos humanoid robot,\ninstructed by human operators to place support contacts on different locations\nand surfaces to avoid falling when reaching for distant objects.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14229v2",
    "published_date": "2024-07-19 11:57:34 UTC",
    "updated_date": "2024-12-09 14:40:51 UTC"
  },
  {
    "arxiv_id": "2407.14210v2",
    "title": "Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based Undersampling Method for Bias Reduction",
    "authors": [
      "JosÃ© Daniel Pascual-Triana",
      "Alberto FernÃ¡ndez",
      "Paulo Novais",
      "Francisco Herrera"
    ],
    "abstract": "One of the key issues regarding classification problems in Trustworthy\nArtificial Intelligence is ensuring Fairness in the prediction of different\nclasses when protected (sensitive) features are present. Data quality is\ncritical in these cases, as biases in training data can be reflected in machine\nlearning, impacting human lives and failing to comply with current regulations.\nOne strategy to improve data quality and avoid these problems is preprocessing\nthe dataset. Instance selection via undersampling can foster balanced learning\nof classes and protected feature values. Performing undersampling in class\noverlap areas close to the decision boundary should bolster the impact on the\nclassifier. This work proposes Fair Overlap Number of Balls (Fair-ONB), an\nundersampling method that harnesses the data morphology of the different data\ngroups (obtained from the combination of classes and protected feature values)\nto perform guided undersampling in overlap areas. It employs attributes of the\nball coverage of the groups, such as the radius, number of covered instances\nand density, to select the most suitable areas for undersampling and reduce\nbias. Results show that the Fair-ONB method improves model Fairness with low\nimpact on the classifier's predictive performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 tables, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14210v2",
    "published_date": "2024-07-19 11:16:02 UTC",
    "updated_date": "2024-09-23 16:52:05 UTC"
  },
  {
    "arxiv_id": "2407.14202v2",
    "title": "SHS: Scorpion Hunting Strategy Swarm Algorithm",
    "authors": [
      "Abhilash Singh",
      "Seyed Muhammad Hossein Mousavi",
      "Kumar Gaurav"
    ],
    "abstract": "We introduced the Scorpion Hunting Strategy (SHS), a novel population-based,\nnature-inspired optimisation algorithm. This algorithm draws inspiration from\nthe hunting strategy of scorpions, which identify, locate, and capture their\nprey using the alpha and beta vibration operators. These operators control the\nSHS algorithm's exploitation and exploration abilities. To formulate an\noptimisation method, we mathematically simulate these dynamic events and\nbehaviors. We evaluate the effectiveness of the SHS algorithm by employing 20\nbenchmark functions (including 10 conventional and 10 CEC2020 functions), using\nboth qualitative and quantitative analyses. Through a comparative analysis with\n12 state-of-the-art meta-heuristic algorithms, we demonstrate that the proposed\nSHS algorithm yields exceptionally promising results. These findings are\nfurther supported by statistically significant results obtained through the\nWilcoxon rank sum test. Additionally, the ranking of SHS, as determined by the\naverage rank derived from the Friedman test, positions it at the forefront when\ncompared to other algorithms. Going beyond theoretical validation, we showcase\nthe practical utility of the SHS algorithm by applying it to six distinct\nreal-world optimisation tasks. These applications illustrate the algorithm's\npotential in addressing complex optimisation challenges. In summary, this work\nnot only introduces the innovative SHS algorithm but also substantiates its\neffectiveness and versatility through rigorous benchmarking and real-world\nproblem-solving scenarios.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14202v2",
    "published_date": "2024-07-19 10:58:42 UTC",
    "updated_date": "2024-08-30 06:15:59 UTC"
  },
  {
    "arxiv_id": "2407.14192v2",
    "title": "LeKUBE: A Legal Knowledge Update BEnchmark",
    "authors": [
      "Changyue Wang",
      "Weihang Su",
      "Hu Yiran",
      "Qingyao Ai",
      "Yueyue Wu",
      "Cheng Luo",
      "Yiqun Liu",
      "Min Zhang",
      "Shaoping Ma"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14192v2",
    "published_date": "2024-07-19 10:40:10 UTC",
    "updated_date": "2024-11-12 11:09:35 UTC"
  },
  {
    "arxiv_id": "2407.20248v2",
    "title": "LAPIS: Language Model-Augmented Police Investigation System",
    "authors": [
      "Heedou Kim",
      "Dain Kim",
      "Jiwoo Lee",
      "Chanwoong Yoon",
      "Donghee Choi",
      "Mogan Gim",
      "Jaewoo Kang"
    ],
    "abstract": "Crime situations are race against time. An AI-assisted criminal investigation\nsystem, providing prompt but precise legal counsel is in need for police\nofficers. We introduce LAPIS (Language Model Augmented Police Investigation\nSystem), an automated system that assists police officers to perform rational\nand legal investigative actions. We constructed a finetuning dataset and\nretrieval knowledgebase specialized in crime investigation legal reasoning\ntask. We extended the dataset's quality by incorporating manual curation\nefforts done by a group of domain experts. We then finetuned the pretrained\nweights of a smaller Korean language model to the newly constructed dataset and\nintegrated it with the crime investigation knowledgebase retrieval approach.\nExperimental results show LAPIS' potential in providing reliable legal guidance\nfor police officers, even better than the proprietary GPT-4 model. Qualitative\nanalysis on the rationales generated by LAPIS demonstrate the model's reasoning\nability to leverage the premises and derive legally correct conclusions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20248v2",
    "published_date": "2024-07-19 09:24:29 UTC",
    "updated_date": "2024-07-31 05:16:30 UTC"
  },
  {
    "arxiv_id": "2407.14145v1",
    "title": "PassTSL: Modeling Human-Created Passwords through Two-Stage Learning",
    "authors": [
      "Yangde Wang",
      "Haozhang Li",
      "Weidong Qiu",
      "Shujun Li",
      "Peng Tang"
    ],
    "abstract": "Textual passwords are still the most widely used user authentication\nmechanism. Due to the close connections between textual passwords and natural\nlanguages, advanced technologies in natural language processing (NLP) and\nmachine learning (ML) could be used to model passwords for different purposes\nsuch as studying human password-creation behaviors and developing more advanced\npassword cracking methods for informing better defence mechanisms. In this\npaper, we propose PassTSL (modeling human-created Passwords through Two-Stage\nLearning), inspired by the popular pretraining-finetuning framework in NLP and\ndeep learning (DL). We report how different pretraining settings affected\nPassTSL and proved its effectiveness by applying it to six large leaked\npassword databases. Experimental results showed that it outperforms five\nstate-of-the-art (SOTA) password cracking methods on password guessing by a\nsignificant margin ranging from 4.11% to 64.69% at the maximum point. Based on\nPassTSL, we also implemented a password strength meter (PSM), and our\nexperiments showed that it was able to estimate password strength more\naccurately, causing fewer unsafe errors (overestimating the password strength)\nthan two other SOTA PSMs when they produce the same rate of safe errors\n(underestimating the password strength): a neural-network based method and\nzxcvbn. Furthermore, we explored multiple finetuning settings, and our\nevaluations showed that, even a small amount of additional training data, e.g.,\nonly 0.1% of the pretrained data, can lead to over 3% improvement in password\nguessing on average. We also proposed a heuristic approach to selecting\nfinetuning passwords based on JS (Jensen-Shannon) divergence and experimental\nresults validated its usefulness. In summary, our contributions demonstrate the\npotential and feasibility of applying advanced NLP and ML methods to password\nmodeling and cracking.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14145v1",
    "published_date": "2024-07-19 09:23:30 UTC",
    "updated_date": "2024-07-19 09:23:30 UTC"
  },
  {
    "arxiv_id": "2407.20247v1",
    "title": "How Homogenizing the Channel-wise Magnitude Can Enhance EEG Classification Model?",
    "authors": [
      "Huyen Ngo",
      "Khoi Do",
      "Duong Nguyen",
      "Viet Dung Nguyen",
      "Lan Dang"
    ],
    "abstract": "A significant challenge in the electroencephalogram EEG lies in the fact that\ncurrent data representations involve multiple electrode signals, resulting in\ndata redundancy and dominant lead information. However extensive research\nconducted on EEG classification focuses on designing model architectures\nwithout tackling the underlying issues. Otherwise, there has been a notable gap\nin addressing data preprocessing for EEG, leading to considerable computational\noverhead in Deep Learning (DL) processes. In light of these issues, we propose\na simple yet effective approach for EEG data pre-processing. Our method first\ntransforms the EEG data into an encoded image by an Inverted Channel-wise\nMagnitude Homogenization (ICWMH) to mitigate inter-channel biases. Next, we\napply the edge detection technique on the EEG-encoded image combined with skip\nconnection to emphasize the most significant transitions in the data while\npreserving structural and invariant information. By doing so, we can improve\nthe EEG learning process efficiently without using a huge DL network. Our\nexperimental evaluations reveal that we can significantly improve (i.e., from\n2% to 5%) over current baselines.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20247v1",
    "published_date": "2024-07-19 09:11:56 UTC",
    "updated_date": "2024-07-19 09:11:56 UTC"
  },
  {
    "arxiv_id": "2408.01438v1",
    "title": "AI for All: Identifying AI incidents Related to Diversity and Inclusion",
    "authors": [
      "Rifat Ara Shams",
      "Didar Zowghi",
      "Muneera Bano"
    ],
    "abstract": "The rapid expansion of Artificial Intelligence (AI) technologies has\nintroduced both significant advancements and challenges, with diversity and\ninclusion (D&I) emerging as a critical concern. Addressing D&I in AI is\nessential to reduce biases and discrimination, enhance fairness, and prevent\nadverse societal impacts. Despite its importance, D&I considerations are often\noverlooked, resulting in incidents marked by built-in biases and ethical\ndilemmas. Analyzing AI incidents through a D&I lens is crucial for identifying\ncauses of biases and developing strategies to mitigate them, ensuring fairer\nand more equitable AI technologies. However, systematic investigations of\nD&I-related AI incidents are scarce. This study addresses these challenges by\nidentifying and understanding D&I issues within AI systems through a manual\nanalysis of AI incident databases (AIID and AIAAIC). The research develops a\ndecision tree to investigate D&I issues tied to AI incidents and populate a\npublic repository of D&I-related AI incidents. The decision tree was validated\nthrough a card sorting exercise and focus group discussions. The research\ndemonstrates that almost half of the analyzed AI incidents are related to D&I,\nwith a notable predominance of racial, gender, and age discrimination. The\ndecision tree and resulting public repository aim to foster further research\nand responsible AI practices, promoting the development of inclusive and\nequitable AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages, 9 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.01438v1",
    "published_date": "2024-07-19 08:54:56 UTC",
    "updated_date": "2024-07-19 08:54:56 UTC"
  },
  {
    "arxiv_id": "2407.14120v1",
    "title": "The Cardinality of Identifying Code Sets for Soccer Ball Graph with Application to Remote Sensing",
    "authors": [
      "Anna L. D. Latour",
      "Arunabha Sen",
      "Kaustav Basu",
      "Chenyang Zhou",
      "Kuldeep S. Meel"
    ],
    "abstract": "In the context of satellite monitoring of the earth, we can assume that the\nsurface of the earth is divided into a set of regions. We assume that the\nimpact of a big social/environmental event spills into neighboring regions.\nUsing Identifying Code Sets (ICSes), we can deploy sensors in such a way that\nthe region in which an event takes place can be uniquely identified, even with\nfewer sensors than regions. As Earth is almost a sphere, we use a soccer ball\nas a model. We construct a Soccer Ball Graph (SBG), and provide human-oriented,\nanalytical proofs that 1) the SBG has at least 26 ICSes of cardinality ten,\nimplying that there are at least 26 different ways to deploy ten satellites to\nmonitor the Earth and 2) that the cardinality of the minimum Identifying Code\nSet (MICS) for the SBG is at least nine. We then provide a machine-oriented\nformal proof that the cardinality of the MICS for the SBG is in fact ten,\nmeaning that one must deploy at least ten satellites to monitor the Earth in\nthe SBG model. We also provide machine-oriented proof that there are exactly 26\nICSes of cardinality ten for the SBG.",
    "categories": [
      "cs.AI",
      "I.2.3"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 5 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2407.14120v1",
    "published_date": "2024-07-19 08:36:44 UTC",
    "updated_date": "2024-07-19 08:36:44 UTC"
  },
  {
    "arxiv_id": "2407.14114v1",
    "title": "A3Rank: Augmentation Alignment Analysis for Prioritizing Overconfident Failing Samples for Deep Learning Models",
    "authors": [
      "Zhengyuan Wei",
      "Haipeng Wang",
      "Qilin Zhou",
      "W. K. Chan"
    ],
    "abstract": "Sharpening deep learning models by training them with examples close to the\ndecision boundary is a well-known best practice. Nonetheless, these models are\nstill error-prone in producing predictions. In practice, the inference of the\ndeep learning models in many application systems is guarded by a rejector, such\nas a confidence-based rejector, to filter out samples with insufficient\nprediction confidence. Such confidence-based rejectors cannot effectively guard\nagainst failing samples with high confidence. Existing test case prioritization\ntechniques effectively distinguish confusing samples from confident samples to\nidentify failing samples among the confusing ones, yet prioritizing the failing\nones high among many confident ones is challenging. In this paper, we propose\n$A^3$Rank, a novel test case prioritization technique with augmentation\nalignment analysis, to address this problem. $A^3$Rank generates augmented\nversions of each test case and assesses the extent of the prediction result for\nthe test case misaligned with these of the augmented versions and vice versa.\nOur experiment shows that $A^3$Rank can effectively rank failing samples\nescaping from the checking of confidence-based rejectors, which significantly\noutperforms the peer techniques by 163.63\\% in the detection ratio of\ntop-ranked samples. We also provide a framework to construct a detector devoted\nto augmenting these rejectors to defend these failing samples, and our detector\ncan achieve a significantly higher defense success rate.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14114v1",
    "published_date": "2024-07-19 08:32:10 UTC",
    "updated_date": "2024-07-19 08:32:10 UTC"
  },
  {
    "arxiv_id": "2407.14106v1",
    "title": "TorchGT: A Holistic System for Large-scale Graph Transformer Training",
    "authors": [
      "Meng Zhang",
      "Jie Sun",
      "Qinghao Hu",
      "Peng Sun",
      "Zeke Wang",
      "Yonggang Wen",
      "Tianwei Zhang"
    ],
    "abstract": "Graph Transformer is a new architecture that surpasses GNNs in graph\nlearning. While there emerge inspiring algorithm advancements, their practical\nadoption is still limited, particularly on real-world graphs involving up to\nmillions of nodes. We observe existing graph transformers fail on large-scale\ngraphs mainly due to heavy computation, limited scalability and inferior model\nquality. Motivated by these observations, we propose TorchGT, the first\nefficient, scalable, and accurate graph transformer training system. TorchGT\noptimizes training at different levels. At algorithm level, by harnessing the\ngraph sparsity, TorchGT introduces a Dual-interleaved Attention which is\ncomputation-efficient and accuracy-maintained. At runtime level, TorchGT scales\ntraining across workers with a communication-light Cluster-aware Graph\nParallelism. At kernel level, an Elastic Computation Reformation further\noptimizes the computation by reducing memory access latency in a dynamic way.\nExtensive experiments demonstrate that TorchGT boosts training by up to 62.7x\nand supports graph sequence lengths of up to 1M.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC), 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14106v1",
    "published_date": "2024-07-19 08:21:42 UTC",
    "updated_date": "2024-07-19 08:21:42 UTC"
  },
  {
    "arxiv_id": "2407.14100v1",
    "title": "ParamsDrag: Interactive Parameter Space Exploration via Image-Space Dragging",
    "authors": [
      "Guan Li",
      "Yang Liu",
      "Guihua Shan",
      "Shiyu Cheng",
      "Weiqun Cao",
      "Junpeng Wang",
      "Ko-Chih Wang"
    ],
    "abstract": "Numerical simulation serves as a cornerstone in scientific modeling, yet the\nprocess of fine-tuning simulation parameters poses significant challenges.\nConventionally, parameter adjustment relies on extensive numerical simulations,\ndata analysis, and expert insights, resulting in substantial computational\ncosts and low efficiency. The emergence of deep learning in recent years has\nprovided promising avenues for more efficient exploration of parameter spaces.\nHowever, existing approaches often lack intuitive methods for precise parameter\nadjustment and optimization. To tackle these challenges, we introduce\nParamsDrag, a model that facilitates parameter space exploration through direct\ninteraction with visualizations. Inspired by DragGAN, our ParamsDrag model\noperates in three steps. First, the generative component of ParamsDrag\ngenerates visualizations based on the input simulation parameters. Second, by\ndirectly dragging structure-related features in the visualizations, users can\nintuitively understand the controlling effect of different parameters. Third,\nwith the understanding from the earlier step, users can steer ParamsDrag to\nproduce dynamic visual outcomes. Through experiments conducted on real-world\nsimulations and comparisons with state-of-the-art deep learning-based\napproaches, we demonstrate the efficacy of our solution.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "To be published in Proc. IEEE VIS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14100v1",
    "published_date": "2024-07-19 08:12:41 UTC",
    "updated_date": "2024-07-19 08:12:41 UTC"
  },
  {
    "arxiv_id": "2407.14097v2",
    "title": "Forward-Forward Learning achieves Highly Selective Latent Representations for Out-of-Distribution Detection in Fully Spiking Neural Networks",
    "authors": [
      "Erik B. Terres-Escudero",
      "Javier Del Ser",
      "Aitor MartÃ­nez-Seras",
      "Pablo Garcia-Bringas"
    ],
    "abstract": "In recent years, Artificial Intelligence (AI) models have achieved remarkable\nsuccess across various domains, yet challenges persist in two critical areas:\nensuring robustness against uncertain inputs and drastically increasing model\nefficiency during training and inference. Spiking Neural Networks (SNNs),\ninspired by biological systems, offer a promising avenue for overcoming these\nlimitations. By operating in an event-driven manner, SNNs achieve low energy\nconsumption and can naturally implement biological methods known for their high\nnoise tolerance. In this work, we explore the potential of the spiking\nForward-Forward Algorithm (FFA) to address these challenges, leveraging its\nrepresentational properties for both Out-of-Distribution (OoD) detection and\ninterpretability. To achieve this, we exploit the sparse and highly specialized\nneural latent space of FF networks to estimate the likelihood of a sample\nbelonging to the training distribution. Additionally, we propose a novel,\ngradient-free attribution method to detect features that drive a sample away\nfrom class distributions, addressing the challenges posed by the lack of\ngradients in most visual interpretability methods for spiking models. We\nevaluate our OoD detection algorithm on well-known image datasets (e.g.,\nOmniglot, Not-MNIST, CIFAR10), outperforming previous methods proposed in the\nrecent literature for OoD detection in spiking networks. Furthermore, our\nattribution method precisely identifies salient OoD features, such as artifacts\nor missing regions, hence providing a visual explanatory interface for the user\nto understand why unknown inputs are identified as such by the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14097v2",
    "published_date": "2024-07-19 08:08:17 UTC",
    "updated_date": "2025-02-19 12:14:17 UTC"
  },
  {
    "arxiv_id": "2407.14095v2",
    "title": "People use fast, goal-directed simulation to reason about novel games",
    "authors": [
      "Cedegao E. Zhang",
      "Katherine M. Collins",
      "Lionel Wong",
      "Mauricio Barba",
      "Adrian Weller",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "People can evaluate features of problems and their potential solutions well\nbefore we can effectively solve them. When considering a game we have never\nplayed, for instance, we might infer whether it is likely to be challenging,\nfair, or fun simply from hearing the game rules, prior to deciding whether to\ninvest time in learning the game or trying to play it well. Many studies of\ngame play have focused on optimality and expertise, characterizing how people\nand computational models play based on moderate to extensive search and after\nplaying a game dozens (if not thousands or millions) of times. Here, we study\nhow people reason about a range of simple but novel Connect-N style board\ngames. We ask people to judge how fair and how fun the games are from very\nlittle experience: just thinking about the game for a minute or so, before they\nhave ever actually played with anyone else, and we propose a resource-limited\nmodel that captures their judgments using only a small number of partial game\nsimulations and almost no look-ahead search.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted at CogSci 2024 as a talk",
    "pdf_url": "http://arxiv.org/pdf/2407.14095v2",
    "published_date": "2024-07-19 07:59:04 UTC",
    "updated_date": "2025-02-07 08:03:50 UTC"
  },
  {
    "arxiv_id": "2407.14092v2",
    "title": "Integrated Push-and-Pull Update Model for Goal-Oriented Effective Communication",
    "authors": [
      "Pouya Agheli",
      "Nikolaos Pappas",
      "Petar Popovski",
      "Marios Kountouris"
    ],
    "abstract": "This paper studies decision-making for goal-oriented effective communication.\nWe consider an end-to-end status update system where a sensing agent (SA)\nobserves a source, generates and transmits updates to an actuation agent (AA),\nwhile the AA takes actions to accomplish a goal at the endpoint. We integrate\nthe push- and pull-based update communication models to obtain a push-and-pull\nmodel, which allows the transmission controller at the SA to decide to push an\nupdate to the AA and the query controller at the AA to pull updates by raising\nqueries at specific time instances. To gauge effectiveness, we utilize a grade\nof effectiveness (GoE) metric incorporating updates' freshness, usefulness, and\ntimeliness of actions as qualitative attributes. We then derive effect-aware\npolicies to maximize the expected discounted sum of updates' effectiveness\nsubject to induced costs. The effect-aware policy at the SA considers the\npotential effectiveness of communicated updates at the endpoint, while at the\nAA, it accounts for the probabilistic evolution of the source and importance of\ngenerated updates. Our results show the proposed push-and-pull model\noutperforms models solely based on push- or pull-based updates both in terms of\nefficiency and effectiveness. Additionally, using effect-aware policies at both\nagents enhances effectiveness compared to periodic and/or probabilistic\neffect-agnostic policies at either or both agents.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.MA",
      "cs.NI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "Submitted for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2407.14092v2",
    "published_date": "2024-07-19 07:57:31 UTC",
    "updated_date": "2025-01-15 15:44:02 UTC"
  },
  {
    "arxiv_id": "2407.15873v1",
    "title": "CRMSP: A Semi-supervised Approach for Key Information Extraction with Class-Rebalancing and Merged Semantic Pseudo-Labeling",
    "authors": [
      "Qi Zhang",
      "Yonghong Song",
      "Pengcheng Guo",
      "Yangyang Hui"
    ],
    "abstract": "There is a growing demand in the field of KIE (Key Information Extraction) to\napply semi-supervised learning to save manpower and costs, as training document\ndata using fully-supervised methods requires labor-intensive manual annotation.\nThe main challenges of applying SSL in the KIE are (1) underestimation of the\nconfidence of tail classes in the long-tailed distribution and (2) difficulty\nin achieving intra-class compactness and inter-class separability of tail\nfeatures. To address these challenges, we propose a novel semi-supervised\napproach for KIE with Class-Rebalancing and Merged Semantic Pseudo-Labeling\n(CRMSP). Firstly, the Class-Rebalancing Pseudo-Labeling (CRP) module introduces\na reweighting factor to rebalance pseudo-labels, increasing attention to tail\nclasses. Secondly, we propose the Merged Semantic Pseudo-Labeling (MSP) module\nto cluster tail features of unlabeled data by assigning samples to Merged\nPrototypes (MP). Additionally, we designed a new contrastive loss specifically\nfor MSP. Extensive experimental results on three well-known benchmarks\ndemonstrate that CRMSP achieves state-of-the-art performance. Remarkably, CRMSP\nachieves 3.24% f1-score improvement over state-of-the-art on the CORD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15873v1",
    "published_date": "2024-07-19 07:41:26 UTC",
    "updated_date": "2024-07-19 07:41:26 UTC"
  },
  {
    "arxiv_id": "2407.14081v2",
    "title": "DisenSemi: Semi-supervised Graph Classification via Disentangled Representation Learning",
    "authors": [
      "Yifan Wang",
      "Xiao Luo",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Ming Zhang",
      "Wei Ju"
    ],
    "abstract": "Graph classification is a critical task in numerous multimedia applications,\nwhere graphs are employed to represent diverse types of multimedia data,\nincluding images, videos, and social networks. Nevertheless, in real-world\nscenarios, labeled graph data can be limited or scarce. To address this issue,\nwe focus on the problem of semi-supervised graph classification, which involves\nboth supervised and unsupervised models learning from labeled and unlabeled\ndata. In contrast to recent approaches that transfer the entire knowledge from\nthe unsupervised model to the supervised one, we argue that an effective\ntransfer should only retain the relevant semantics that align well with the\nsupervised task. In this paper, we propose a novel framework named DisenSemi,\nwhich learns disentangled representation for semi-supervised graph\nclassification. Specifically, a disentangled graph encoder is proposed to\ngenerate factor-wise graph representations for both supervised and unsupervised\nmodels. Then we train two models via supervised objective and mutual\ninformation (MI)-based constraints respectively. To ensure the meaningful\ntransfer of knowledge from the unsupervised encoder to the supervised one, we\nfurther define an MI-based disentangled consistency regularization between two\nmodels and identify the corresponding rationale that aligns well with the\ncurrent graph classification task. Experimental results on a range of publicly\naccessible datasets reveal the effectiveness of our DisenSemi.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.14081v2",
    "published_date": "2024-07-19 07:31:32 UTC",
    "updated_date": "2024-08-09 08:23:12 UTC"
  },
  {
    "arxiv_id": "2407.14076v2",
    "title": "Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field",
    "authors": [
      "Tobias Kerner"
    ],
    "abstract": "There are many cases where LLMs are used for specific tasks in a single\ndomain. These usually require less general, but more domain-specific knowledge.\nHighly capable, general-purpose state-of-the-art language models like GPT-4 or\nClaude-3-opus can often be used for such tasks, but they are very large and\ncannot be run locally, even if they were not proprietary. This can be a problem\nwhen working with sensitive data. This paper focuses on domain-specific and\nmixed-domain pretraining as potentially more efficient methods than general\npretraining for specialized language models. We will take a look at work\nrelated to domain-specific pretraining, specifically in the medical area, and\ncompare benchmark results of specialized language models to general-purpose\nlanguage models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14076v2",
    "published_date": "2024-07-19 07:12:43 UTC",
    "updated_date": "2024-07-28 07:46:26 UTC"
  },
  {
    "arxiv_id": "2408.03327v1",
    "title": "Reconstruction of the shape of irregular rough particles from their interferometric images using a convolutional neural network",
    "authors": [
      "Alexis Abad",
      "Alexandre Poux",
      "Alexis Boulet",
      "Marc Brunel"
    ],
    "abstract": "We have developed a convolutional neural network (CNN) to reconstruct the\nshape of irregular rough particles from their interferometric images. The CNN\nis based on a UNET architecture with residual block modules. The database has\nbeen constructed using the experimental patterns generated by perfectly known\npseudo-particles programmed on a Digital Micromirror Device (DMD) and under\nlaser illumination. The CNN has been trained on a basis of 18000 experimental\ninterferometric images using the AUSTRAL super computer (at CRIANN in\nNormandy). The CNN is tested in the case of centrosymmetric (stick, cross,\ndendrite) and non-centrosymmetric (like T, Y or L) particles. The size and the\n3D orientation of the programmed particles are random. The different shapes are\nreconstructed by the CNN with good accuracy. Using three angles of view, the 3D\nreconstruction of particles from three reconstructed faces can be further done.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03327v1",
    "published_date": "2024-07-19 07:03:36 UTC",
    "updated_date": "2024-07-19 07:03:36 UTC"
  },
  {
    "arxiv_id": "2407.14073v3",
    "title": "LoAS: Fully Temporal-Parallel Dataflow for Dual-Sparse Spiking Neural Networks",
    "authors": [
      "Ruokai Yin",
      "Youngeun Kim",
      "Di Wu",
      "Priyadarshini Panda"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have gained significant research attention in\nthe last decade due to their potential to drive resource-constrained edge\ndevices. Though existing SNN accelerators offer high efficiency in processing\nsparse spikes with dense weights, opportunities are less explored in SNNs with\nsparse weights, i.e., dual-sparsity. In this work, we study the acceleration of\ndual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix\nmultiplication (spMspM). We observe that naively running a dual-sparse SNN on\nexisting spMspM accelerators designed for dual-sparse Artificial Neural\nNetworks (ANNs) exhibits sub-optimal efficiency. The main challenge is that\nprocessing timesteps, a natural property of SNNs, introduces an extra loop to\nANN spMspM, leading to longer latency and more memory traffic. To address the\nproblem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes\nboth data movement across timesteps and the end-to-end latency of dual-sparse\nSNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly\nspike compression mechanism that efficiently compresses single-bit spikes and\nensures contiguous memory access. We further propose an FTP-friendly inner-join\ncircuit that can lower the cost of the expensive prefix-sum circuits with\nalmost no throughput penalty. All the above techniques for FTP dataflow are\nencapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs.\nWith FTP dataflow, compression, and inner-join, running dual-sparse SNN\nworkloads on LoAS demonstrates significant speedup (up to $8.51\\times$) and\nenergy reduction (up to $3.68\\times$) compared to running it on prior\ndual-sparse accelerators.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted to MICRO 2024. Will update with the camera-ready version\n  once ready. (Github: https://github.com/RuokaiYin/LoAS)",
    "pdf_url": "http://arxiv.org/pdf/2407.14073v3",
    "published_date": "2024-07-19 07:02:26 UTC",
    "updated_date": "2024-09-01 13:26:58 UTC"
  },
  {
    "arxiv_id": "2407.14057v1",
    "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
    "authors": [
      "Qichen Fu",
      "Minsik Cho",
      "Thomas Merth",
      "Sachin Mehta",
      "Mohammad Rastegari",
      "Mahyar Najibi"
    ],
    "abstract": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14057v1",
    "published_date": "2024-07-19 06:34:45 UTC",
    "updated_date": "2024-07-19 06:34:45 UTC"
  },
  {
    "arxiv_id": "2407.14055v2",
    "title": "Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers",
    "authors": [
      "Peiyong Wang",
      "Casey R. Myers",
      "Lloyd C. L. Hollenberg",
      "Udaya Parampalli"
    ],
    "abstract": "When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding. Author affiliation updated\n  for v2. Acknowledgements and funding information added for v2",
    "pdf_url": "http://arxiv.org/pdf/2407.14055v2",
    "published_date": "2024-07-19 06:31:22 UTC",
    "updated_date": "2024-08-01 02:19:08 UTC"
  },
  {
    "arxiv_id": "2407.14568v1",
    "title": "SQLfuse: Enhancing Text-to-SQL Performance through Comprehensive LLM Synergy",
    "authors": [
      "Tingkai Zhang",
      "Chaoyu Chen",
      "Cong Liao",
      "Jun Wang",
      "Xudong Zhao",
      "Hang Yu",
      "Jianchao Wang",
      "Jianguo Li",
      "Wenhui Shi"
    ],
    "abstract": "Text-to-SQL conversion is a critical innovation, simplifying the transition\nfrom complex SQL to intuitive natural language queries, especially significant\ngiven SQL's prevalence in the job market across various roles. The rise of\nLarge Language Models (LLMs) like GPT-3.5 and GPT-4 has greatly advanced this\nfield, offering improved natural language understanding and the ability to\ngenerate nuanced SQL statements. However, the potential of open-source LLMs in\nText-to-SQL applications remains underexplored, with many frameworks failing to\nleverage their full capabilities, particularly in handling complex database\nqueries and incorporating feedback for iterative refinement. Addressing these\nlimitations, this paper introduces SQLfuse, a robust system integrating\nopen-source LLMs with a suite of tools to enhance Text-to-SQL translation's\naccuracy and usability. SQLfuse features four modules: schema mining, schema\nlinking, SQL generation, and a SQL critic module, to not only generate but also\ncontinuously enhance SQL query quality. Demonstrated by its leading performance\non the Spider Leaderboard and deployment by Ant Group, SQLfuse showcases the\npractical merits of open-source LLMs in diverse business contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14568v1",
    "published_date": "2024-07-19 06:01:57 UTC",
    "updated_date": "2024-07-19 06:01:57 UTC"
  },
  {
    "arxiv_id": "2407.14047v1",
    "title": "OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking",
    "authors": [
      "Zekun Qian",
      "Ruize Han",
      "Wei Feng",
      "Junhui Hou",
      "Linqi Song",
      "Song Wang"
    ],
    "abstract": "We study a novel yet practical problem of open-corpus multi-object tracking\n(OCMOT), which extends the MOT into localizing, associating, and recognizing\ngeneric-category objects of both seen (base) and unseen (novel) classes, but\nwithout the category text list as prompt. To study this problem, the top\npriority is to build a benchmark. In this work, we build OCTrackB, a\nlarge-scale and comprehensive benchmark, to provide a standard evaluation\nplatform for the OCMOT problem. Compared to previous datasets, OCTrackB has\nmore abundant and balanced base/novel classes and the corresponding samples for\nevaluation with less bias. We also propose a new multi-granularity recognition\nmetric to better evaluate the generative object recognition in OCMOT. By\nconducting the extensive benchmark evaluation, we report and analyze the\nresults of various state-of-the-art methods, which demonstrate the rationale of\nOCMOT, as well as the usefulness and advantages of OCTrackB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14047v1",
    "published_date": "2024-07-19 05:58:01 UTC",
    "updated_date": "2024-07-19 05:58:01 UTC"
  },
  {
    "arxiv_id": "2407.14044v2",
    "title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?",
    "authors": [
      "Siddhant Waghjale",
      "Vishruth Veerendranath",
      "Zora Zhiruo Wang",
      "Daniel Fried"
    ],
    "abstract": "Although large language models (LLMs) have been largely successful in\ngenerating functionally correct programs, conditioning models to produce\nefficient solutions while ensuring correctness remains a challenge. Further,\nunreliability in benchmarking code efficiency is a hurdle across varying\nhardware specifications for popular interpreted languages such as Python. In\nthis paper, we present ECCO, a reproducible benchmark for evaluating program\nefficiency via two paradigms: natural language (NL) based code generation and\nhistory-based code editing. On ECCO, we adapt and thoroughly investigate the\nthree most promising existing LLM-based approaches: in-context learning,\niterative refinement with execution or NL feedback, and fine-tuning conditioned\non execution and editing history. While most methods degrade functional\ncorrectness and moderately increase program efficiency, we find that adding\nexecution information often helps maintain functional correctness, and NL\nfeedback enhances more on efficiency. We release our benchmark to support\nfuture work on LLM-based generation of efficient code.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024; Project Page: https://ecco-code-eff.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.14044v2",
    "published_date": "2024-07-19 05:47:40 UTC",
    "updated_date": "2024-10-09 22:20:40 UTC"
  },
  {
    "arxiv_id": "2407.14567v2",
    "title": "Integrating Artificial Intelligence into Operating Systems: A Comprehensive Survey on Techniques, Applications, and Future Directions",
    "authors": [
      "Yifan Zhang",
      "Xinkui Zhao",
      "Ziying Li",
      "Jianwei Yin",
      "Lufei Zhang",
      "Zuoning Chen"
    ],
    "abstract": "In the era of the Internet of Everything, operating systems (OSs) face\nunprecedented challenges posed by an evolving application landscape and\nincreasingly heterogeneous hardware ecosystems. This shift toward increasingly\ndynamic and unpredictable operational contexts presents significant challenges\nfor both OS developers and users. Against this backdrop, the fusion of\nArtificial Intelligence (AI) with Operating Systems emerges as a critical\nfrontier for innovation. This survey delves into the intricate interplay\nbetween AI and OSs, illustrating how existing OS mechanisms combined with AI\nsignificantly elevate the performance, security, and efficiency of modern\noperating systems. We investigate a range of AI methodologies applied to\noptimize core OS functionalities and clarify the correlation to related\nstudies. Our analysis touches on the existing hurdles and prospective avenues\nin this interdisciplinary domain, underscoring the imperative for robust and\nseamless integration of AI capabilities into OS architectures.\n  Through an examination of illustrative case studies and cutting-edge\ndevelopments, we offer a thorough review of the current status of AI-OS\nintegration, accentuating its pivotal role in steering the evolution of\nadvanced computing paradigms. We also envision the promising prospects of\nIntelligent Operating Systems, debating how groundbreaking OS designs will\nusher in novel possibilities and highlight the central role that AI will assume\nin propelling these next-generation systems forward. This forward-thinking\noutlook illuminates the profound influence of AI on the foundational elements\nof computing, heralding the advent of a new epoch characterized by intelligent,\nself-adapting, and highly adaptive software ecosystems.",
    "categories": [
      "cs.OS",
      "cs.AI"
    ],
    "primary_category": "cs.OS",
    "comment": "47 pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14567v2",
    "published_date": "2024-07-19 05:29:34 UTC",
    "updated_date": "2024-12-22 13:04:08 UTC"
  },
  {
    "arxiv_id": "2407.14030v1",
    "title": "HeCiX: Integrating Knowledge Graphs and Large Language Models for Biomedical Research",
    "authors": [
      "Prerana Sanjay Kulkarni",
      "Muskaan Jain",
      "Disha Sheshanarayana",
      "Srinivasan Parthiban"
    ],
    "abstract": "Despite advancements in drug development strategies, 90% of clinical trials\nfail. This suggests overlooked aspects in target validation and drug\noptimization. In order to address this, we introduce HeCiX-KG,\nHetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from\nClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines\ndata on previously conducted clinical trials from ClinicalTrials.gov, and\ndomain expertise on diseases and genes from Hetionet. This offers a thorough\nresource for clinical researchers. Further, we introduce HeCiX, a system that\nuses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.\nHeCiX shows high performance during evaluation against a range of clinically\nrelevant issues, proving this model to be promising for enhancing the\neffectiveness of clinical research. Thus, this approach provides a more\nholistic view of clinical trials and existing biological data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2407.14030v1",
    "published_date": "2024-07-19 05:04:24 UTC",
    "updated_date": "2024-07-19 05:04:24 UTC"
  },
  {
    "arxiv_id": "2407.14024v1",
    "title": "TTA-OOD: Test-time Augmentation for Improving Out-of-Distribution Detection in Gastrointestinal Vision",
    "authors": [
      "Sandesh Pokhrel",
      "Sanjay Bhandari",
      "Eduard Vazquez",
      "Tryphon Lambrou",
      "Prashnna Gyawali",
      "Binod Bhattarai"
    ],
    "abstract": "Deep learning has significantly advanced the field of gastrointestinal\nvision, enhancing disease diagnosis capabilities. One major challenge in\nautomating diagnosis within gastrointestinal settings is the detection of\nabnormal cases in endoscopic images. Due to the sparsity of data, this process\nof distinguishing normal from abnormal cases has faced significant challenges,\nparticularly with rare and unseen conditions. To address this issue, we frame\nabnormality detection as an out-of-distribution (OOD) detection problem. In\nthis setup, a model trained on In-Distribution (ID) data, which represents a\nhealthy GI tract, can accurately identify healthy cases, while abnormalities\nare detected as OOD, regardless of their class. We introduce a test-time\naugmentation segment into the OOD detection pipeline, which enhances the\ndistinction between ID and OOD examples, thereby improving the effectiveness of\nexisting OOD methods with the same model. This augmentation shifts the pixel\nspace, which translates into a more distinct semantic representation for OOD\nexamples compared to ID examples. We evaluated our method against existing\nstate-of-the-art OOD scores, showing improvements with test-time augmentation\nover the baseline approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14024v1",
    "published_date": "2024-07-19 04:50:54 UTC",
    "updated_date": "2024-07-19 04:50:54 UTC"
  },
  {
    "arxiv_id": "2407.14007v2",
    "title": "Multi-modal Relation Distillation for Unified 3D Representation Learning",
    "authors": [
      "Huiqun Wang",
      "Yiping Bao",
      "Panwang Pan",
      "Zeming Li",
      "Xiao Liu",
      "Ruijie Yang",
      "Di Huang"
    ],
    "abstract": "Recent advancements in multi-modal pre-training for 3D point clouds have\ndemonstrated promising results by aligning heterogeneous features across 3D\nshapes and their corresponding 2D images and language descriptions. However,\ncurrent straightforward solutions often overlook intricate structural relations\namong samples, potentially limiting the full capabilities of multi-modal\nlearning. To address this issue, we introduce Multi-modal Relation Distillation\n(MRD), a tri-modal pre-training framework, which is designed to effectively\ndistill reputable large Vision-Language Models (VLM) into 3D backbones. MRD\naims to capture both intra-relations within each modality as well as\ncross-relations between different modalities and produce more discriminative 3D\nshape representations. Notably, MRD achieves significant improvements in\ndownstream zero-shot classification tasks and cross-modality retrieval tasks,\ndelivering new state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.14007v2",
    "published_date": "2024-07-19 03:43:48 UTC",
    "updated_date": "2024-09-18 06:39:50 UTC"
  },
  {
    "arxiv_id": "2407.14565v1",
    "title": "Detecting and Characterising Mobile App Metamorphosis in Google Play Store",
    "authors": [
      "D. Denipitiyage",
      "B. Silva",
      "K. Gunathilaka",
      "S. Seneviratne",
      "A. Mahanti",
      "A. Seneviratne",
      "S. Chawla"
    ],
    "abstract": "App markets have evolved into highly competitive and dynamic environments for\ndevelopers. While the traditional app life cycle involves incremental updates\nfor feature enhancements and issue resolution, some apps deviate from this norm\nby undergoing significant transformations in their use cases or market\npositioning. We define this previously unstudied phenomenon as 'app\nmetamorphosis'. In this paper, we propose a novel and efficient multi-modal\nsearch methodology to identify apps undergoing metamorphosis and apply it to\nanalyse two snapshots of the Google Play Store taken five years apart. Our\nmethodology uncovers various metamorphosis scenarios, including re-births,\nre-branding, re-purposing, and others, enabling comprehensive characterisation.\nAlthough these transformations may register as successful for app developers\nbased on our defined success score metric (e.g., re-branded apps performing\napproximately 11.3% better than an average top app), we shed light on the\nconcealed security and privacy risks that lurk within, potentially impacting\neven tech-savvy end-users.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.SE",
    "comment": "15 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.14565v1",
    "published_date": "2024-07-19 03:26:40 UTC",
    "updated_date": "2024-07-19 03:26:40 UTC"
  },
  {
    "arxiv_id": "2407.13998v2",
    "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
    "authors": [
      "Rujun Han",
      "Yuhao Zhang",
      "Peng Qi",
      "Yumo Xu",
      "Jenyuan Wang",
      "Lan Liu",
      "William Yang Wang",
      "Bonan Min",
      "Vittorio Castelli"
    ],
    "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13998v2",
    "published_date": "2024-07-19 03:02:51 UTC",
    "updated_date": "2024-10-03 00:13:19 UTC"
  },
  {
    "arxiv_id": "2407.13993v3",
    "title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
    "authors": [
      "Christoforus Yoga Haryanto"
    ],
    "abstract": "This paper introduces LLAssist, an open-source tool designed to streamline\nliterature reviews in academic research. In an era of exponential growth in\nscientific publications, researchers face mounting challenges in efficiently\nprocessing vast volumes of literature. LLAssist addresses this issue by\nleveraging Large Language Models (LLMs) and Natural Language Processing (NLP)\ntechniques to automate key aspects of the review process. Specifically, it\nextracts important information from research articles and evaluates their\nrelevance to user-defined research questions. The goal of LLAssist is to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, allowing researchers to focus more on analyzing and synthesizing\ninformation rather than on initial screening tasks. By automating parts of the\nliterature review workflow, LLAssist aims to help researchers manage the\ngrowing volume of academic publications more efficiently.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "10 pages, 3 figures, 1 table, presented at the 51st International\n  Conference on Computers and Industrial Engineering (CIE51), 11 Dec 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.13993v3",
    "published_date": "2024-07-19 02:48:54 UTC",
    "updated_date": "2024-12-20 12:06:25 UTC"
  },
  {
    "arxiv_id": "2407.13989v3",
    "title": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively Distilling Knowledge from Large Language Models",
    "authors": [
      "Quan Li",
      "Tianxiang Zhao",
      "Lingwei Chen",
      "Junjie Xu",
      "Suhang Wang"
    ],
    "abstract": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 3 Figures",
    "pdf_url": "http://arxiv.org/pdf/2407.13989v3",
    "published_date": "2024-07-19 02:34:10 UTC",
    "updated_date": "2024-09-04 17:52:37 UTC"
  },
  {
    "arxiv_id": "2407.13968v1",
    "title": "Optimizing Agricultural Order Fulfillment Systems: A Hybrid Tree Search Approach",
    "authors": [
      "Pranay Thangeda",
      "Hoda Helmi",
      "Melkior Ornik"
    ],
    "abstract": "Efficient order fulfillment is vital in the agricultural industry,\nparticularly due to the seasonal nature of seed supply chains. This paper\naddresses the challenge of optimizing seed orders fulfillment in a centralized\nwarehouse where orders are processed in waves, taking into account the\nunpredictable arrival of seed stocks and strict order deadlines. We model the\nwave scheduling problem as a Markov decision process and propose an adaptive\nhybrid tree search algorithm that combines Monte Carlo tree search with\ndomain-specific knowledge to efficiently navigate the complex, dynamic\nenvironment of seed distribution. By leveraging historical data and stochastic\nmodeling, our method enables forecast-informed scheduling decisions that\nbalance immediate requirements with long-term operational efficiency. The key\nidea is that we can augment Monte Carlo tree search algorithm with\nproblem-specific side information that dynamically reduces the number of\ncandidate actions at each decision step to handle the large state and action\nspaces that render traditional solution methods computationally intractable.\nExtensive simulations with realistic parameters-including a diverse range of\nproducts, a high volume of orders, and authentic seasonal durations-demonstrate\nthat the proposed approach significantly outperforms existing industry standard\nmethods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13968v1",
    "published_date": "2024-07-19 01:25:39 UTC",
    "updated_date": "2024-07-19 01:25:39 UTC"
  },
  {
    "arxiv_id": "2407.13952v1",
    "title": "Knowledge Distillation Approaches for Accurate and Efficient Recommender System",
    "authors": [
      "SeongKu Kang"
    ],
    "abstract": "Despite its breakthrough in classification problems, Knowledge distillation\n(KD) to recommendation models and ranking problems has not been studied well in\nthe previous literature. This dissertation is devoted to developing knowledge\ndistillation methods for recommender systems to fully improve the performance\nof a compact model. We propose novel distillation methods designed for\nrecommender systems. The proposed methods are categorized according to their\nknowledge sources as follows: (1) Latent knowledge: we propose two methods that\ntransfer latent knowledge of user/item representation. They effectively\ntransfer knowledge of niche tastes with a balanced distillation strategy that\nprevents the KD process from being biased towards a small number of large\npreference groups. Also, we propose a new method that transfers user/item\nrelations in the representation space. The proposed method selectively\ntransfers essential relations considering the limited capacity of the compact\nmodel. (2) Ranking knowledge: we propose three methods that transfer ranking\nknowledge from the recommendation results. They formulate the KD process as a\nranking matching problem and transfer the knowledge via a listwise learning\nstrategy. Further, we present a new learning framework that compresses the\nranking knowledge of heterogeneous recommendation models. The proposed\nframework is developed to ease the computational burdens of model ensemble\nwhich is a dominant solution for many recommendation applications. We validate\nthe benefit of our proposed methods and frameworks through extensive\nexperiments. To summarize, this dissertation sheds light on knowledge\ndistillation approaches for a better accuracy-efficiency trade-off of the\nrecommendation models.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Doctoral Dissertation (2022)",
    "pdf_url": "http://arxiv.org/pdf/2407.13952v1",
    "published_date": "2024-07-19 00:01:18 UTC",
    "updated_date": "2024-07-19 00:01:18 UTC"
  }
]
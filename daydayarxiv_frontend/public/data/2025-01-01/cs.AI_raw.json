[
  {
    "arxiv_id": "2501.00961v2",
    "title": "The Silent Majority: Demystifying Memorization Effect in the Presence of Spurious Correlations",
    "authors": [
      "Chenyu You",
      "Haocheng Dai",
      "Yifei Min",
      "Jasjeet S. Sekhon",
      "Sarang Joshi",
      "James S. Duncan"
    ],
    "abstract": "Machine learning models often rely on simple spurious features -- patterns in\ntraining data that correlate with targets but are not causally related to them,\nlike image backgrounds in foreground classification. This reliance typically\nleads to imbalanced test performance across minority and majority groups. In\nthis work, we take a closer look at the fundamental cause of such imbalanced\nperformance through the lens of memorization, which refers to the ability to\npredict accurately on \\textit{atypical} examples (minority groups) in the\ntraining set but failing in achieving the same accuracy in the testing set.\nThis paper systematically shows the ubiquitous existence of spurious features\nin a small set of neurons within the network, providing the first-ever evidence\nthat memorization may contribute to imbalanced group performance. Through three\nexperimental sources of converging empirical evidence, we find the property of\na small subset of neurons or channels in memorizing minority group information.\nInspired by these findings, we articulate the hypothesis: the imbalanced group\nperformance is a byproduct of ``noisy'' spurious memorization confined to a\nsmall set of neurons. To further substantiate this hypothesis, we show that\neliminating these unnecessary spurious memorization patterns via a novel\nframework during training can significantly affect the model performance on\nminority groups. Our experimental results across various architectures and\nbenchmarks offer new insights on how neural networks encode core and spurious\nknowledge, laying the groundwork for future research in demystifying robustness\nto spurious correlation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00961v2",
    "published_date": "2025-01-01 21:45:00 UTC",
    "updated_date": "2025-01-15 06:46:51 UTC"
  },
  {
    "arxiv_id": "2501.00954v1",
    "title": "Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach",
    "authors": [
      "Sagarnil Das",
      "Pradeep Walia"
    ],
    "abstract": "Diabetic Retinopathy (DR) is a leading cause of preventable blindness. Early\ndetection at the DR1 stage is critical but is hindered by a scarcity of\nhigh-quality fundus images. This study uses StyleGAN3 to generate synthetic DR1\nimages characterized by microaneurysms with high fidelity and diversity. The\naim is to address data scarcity and enhance the performance of supervised\nclassifiers. A dataset of 2,602 DR1 images was used to train the model,\nfollowed by a comprehensive evaluation using quantitative metrics, including\nFrechet Inception Distance (FID), Kernel Inception Distance (KID), and\nEquivariance with respect to translation (EQ-T) and rotation (EQ-R).\nQualitative assessments included Human Turing tests, where trained\nophthalmologists evaluated the realism of synthetic images. Spectral analysis\nfurther validated image quality. The model achieved a final FID score of 17.29,\noutperforming the mean FID of 21.18 (95 percent confidence interval - 20.83 to\n21.56) derived from bootstrap resampling. Human Turing tests demonstrated the\nmodel's ability to produce highly realistic images, though minor artifacts near\nthe borders were noted. These findings suggest that StyleGAN3-generated\nsynthetic DR1 images hold significant promise for augmenting training datasets,\nenabling more accurate early detection of Diabetic Retinopathy. This\nmethodology highlights the potential of synthetic data in advancing medical\nimaging and AI-driven diagnostics.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.00954v1",
    "published_date": "2025-01-01 21:00:58 UTC",
    "updated_date": "2025-01-01 21:00:58 UTC"
  },
  {
    "arxiv_id": "2501.00953v2",
    "title": "Prior Lessons of Incremental Dialogue and Robot Action Management for the Age of Language Models",
    "authors": [
      "Casey Kennington",
      "Pierre Lison",
      "David Schlangen"
    ],
    "abstract": "Efforts towards endowing robots with the ability to speak have benefited from\nrecent advancements in natural language processing, in particular large\nlanguage models. However, current language models are not fully incremental, as\ntheir processing is inherently monotonic and thus lack the ability to revise\ntheir interpretations or output in light of newer observations. This\nmonotonicity has important implications for the development of dialogue systems\nfor human--robot interaction. In this paper, we review the literature on\ninteractive systems that operate incrementally (i.e., at the word level or\nbelow it). We motivate the need for incremental systems, survey incremental\nmodeling of important aspects of dialogue like speech recognition and language\ngeneration. Primary focus is on the part of the system that makes decisions,\nknown as the dialogue manager. We find that there is very little research on\nincremental dialogue management, offer some requirements for practical\nincremental dialogue management, and the implications of incremental dialogue\nfor embodied, robotic platforms in the age of large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.00953v2",
    "published_date": "2025-01-01 20:58:03 UTC",
    "updated_date": "2025-04-02 14:24:00 UTC"
  },
  {
    "arxiv_id": "2501.09761v1",
    "title": "VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations",
    "authors": [
      "Nasim Soltani",
      "Michael Loehning",
      "Kaushik Chowdhury"
    ],
    "abstract": "Artificial Intelligence (AI)-native receivers prove significant performance\nimprovement in high noise regimes and can potentially reduce communication\noverhead compared to the traditional receiver. However, their performance\nhighly depends on the representativeness of the training dataset. A major issue\nis the uncertainty of whether the training dataset covers all test environments\nand waveform configurations, and thus, whether the trained model is robust in\npractical deployment conditions. To this end, we propose a joint\nmeasurement-recovery framework for AI-native transceivers post deployment,\ncalled VERITAS, that continuously looks for distribution shifts in the received\nsignals and triggers finite re-training spurts. VERITAS monitors the wireless\nchannel using 5G pilots fed to an auxiliary neural network that detects\nout-of-distribution channel profile, transmitter speed, and delay spread. As\nsoon as such a change is detected, a traditional (reference) receiver is\nactivated, which runs for a period of time in parallel to the AI-native\nreceiver. Finally, VERTIAS compares the bit probabilities of the AI-native and\nthe reference receivers for the same received data inputs, and decides whether\nor not a retraining process needs to be initiated. Our evaluations reveal that\nVERITAS can detect changes in the channel profile, transmitter speed, and delay\nspread with 99%, 97%, and 69% accuracies, respectively, followed by timely\ninitiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel\nprofile, transmitter speed, and delay spread test sets, respectively.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2501.09761v1",
    "published_date": "2025-01-01 19:12:03 UTC",
    "updated_date": "2025-01-01 19:12:03 UTC"
  },
  {
    "arxiv_id": "2501.00913v1",
    "title": "$β$-DQN: Improving Deep Q-Learning By Evolving the Behavior",
    "authors": [
      "Hongming Zhang",
      "Fengshuo Bai",
      "Chenjun Xiao",
      "Chao Gao",
      "Bo Xu",
      "Martin Müller"
    ],
    "abstract": "While many sophisticated exploration methods have been proposed, their lack\nof generality and high computational cost often lead researchers to favor\nsimpler methods like $\\epsilon$-greedy. Motivated by this, we introduce\n$\\beta$-DQN, a simple and efficient exploration method that augments the\nstandard DQN with a behavior function $\\beta$. This function estimates the\nprobability that each action has been taken at each state. By leveraging\n$\\beta$, we generate a population of diverse policies that balance exploration\nbetween state-action coverage and overestimation bias correction. An adaptive\nmeta-controller is designed to select an effective policy for each episode,\nenabling flexible and explainable exploration. $\\beta$-DQN is straightforward\nto implement and adds minimal computational overhead to the standard DQN.\nExperiments on both simple and challenging exploration domains show that\n$\\beta$-DQN outperforms existing baseline methods across a wide range of tasks,\nproviding an effective solution for improving exploration in deep reinforcement\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00913v1",
    "published_date": "2025-01-01 18:12:18 UTC",
    "updated_date": "2025-01-01 18:12:18 UTC"
  },
  {
    "arxiv_id": "2501.00910v1",
    "title": "Population Aware Diffusion for Time Series Generation",
    "authors": [
      "Yang Li",
      "Han Meng",
      "Zhenyu Bi",
      "Ingolv T. Urnes",
      "Haipeng Chen"
    ],
    "abstract": "Diffusion models have shown promising ability in generating high-quality time\nseries (TS) data. Despite the initial success, existing works mostly focus on\nthe authenticity of data at the individual level, but pay less attention to\npreserving the population-level properties on the entire dataset. Such\npopulation-level properties include value distributions for each dimension and\ndistributions of certain functional dependencies (e.g., cross-correlation, CC)\nbetween different dimensions. For instance, when generating house energy\nconsumption TS data, the value distributions of the outside temperature and the\nkitchen temperature should be preserved, as well as the distribution of CC\nbetween them. Preserving such TS population-level properties is critical in\nmaintaining the statistical insights of the datasets, mitigating model bias,\nand augmenting downstream tasks like TS prediction. Yet, it is often overlooked\nby existing models. Hence, data generated by existing models often bear\ndistribution shifts from the original data. We propose Population-aware\nDiffusion for Time Series (PaD-TS), a new TS generation model that better\npreserves the population-level properties. The key novelties of PaD-TS include\n1) a new training method explicitly incorporating TS population-level property\npreservation, and 2) a new dual-channel encoder model architecture that better\ncaptures the TS data structure. Empirical results in major benchmark datasets\nshow that PaD-TS can improve the average CC distribution shift score between\nreal and synthetic data by 5.9x while maintaining a performance comparable to\nstate-of-the-art models on individual-level authenticity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at AAAI-2025, 8 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.00910v1",
    "published_date": "2025-01-01 17:53:43 UTC",
    "updated_date": "2025-01-01 17:53:43 UTC"
  },
  {
    "arxiv_id": "2501.00906v2",
    "title": "Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things",
    "authors": [
      "Talha Zeeshan",
      "Abhishek Kumar",
      "Susanna Pirttikangas",
      "Sasu Tarkoma"
    ],
    "abstract": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00906v2",
    "published_date": "2025-01-01 17:38:40 UTC",
    "updated_date": "2025-01-03 07:47:36 UTC"
  },
  {
    "arxiv_id": "2501.14778v1",
    "title": "Advancing Trustworthy AI for Sustainable Development: Recommendations for Standardising AI Incident Reporting",
    "authors": [
      "Avinash Agarwal",
      "Manisha J Nene"
    ],
    "abstract": "The increasing use of AI technologies has led to increasing AI incidents,\nposing risks and causing harm to individuals, organizations, and society. This\nstudy recognizes and addresses the lack of standardized protocols for reliably\nand comprehensively gathering such incident data crucial for preventing future\nincidents and developing mitigating strategies. Specifically, this study\nanalyses existing open-access AI-incident databases through a systematic\nmethodology and identifies nine gaps in current AI incident reporting\npractices. Further, it proposes nine actionable recommendations to enhance\nstandardization efforts to address these gaps. Ensuring the trustworthiness of\nenabling technologies such as AI is necessary for sustainable digital\ntransformation. Our research promotes the development of standards to prevent\nfuture AI incidents and promote trustworthy AI, thus facilitating achieving the\nUN sustainable development goals. Through international cooperation,\nstakeholders can unlock the transformative potential of AI, enabling a\nsustainable and inclusive future for all.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, 10 tables, and 1 figure. Accepted at the International\n  Telecommunication Union (ITU) Kaleidoscope 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.14778v1",
    "published_date": "2025-01-01 17:34:57 UTC",
    "updated_date": "2025-01-01 17:34:57 UTC"
  },
  {
    "arxiv_id": "2501.00891v1",
    "title": "Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts",
    "authors": [
      "Zhuohua Li",
      "Maoli Liu",
      "Xiangxiang Dai",
      "John C. S. Lui"
    ],
    "abstract": "The contextual multi-armed bandit (MAB) problem is crucial in sequential\ndecision-making. A line of research, known as online clustering of bandits,\nextends contextual MAB by grouping similar users into clusters, utilizing\nshared features to improve learning efficiency. However, existing algorithms,\nwhich rely on the upper confidence bound (UCB) strategy, struggle to gather\nadequate statistical information to accurately identify unknown user clusters.\nAs a result, their theoretical analyses require several strong assumptions\nabout the \"diversity\" of contexts generated by the environment, leading to\nimpractical settings, complicated analyses, and poor practical performance.\nRemoving these assumptions has been a long-standing open problem in the\nclustering of bandits literature. In this paper, we provide two solutions to\nthis open problem. First, following the i.i.d. context generation setting in\nexisting studies, we propose two novel algorithms, UniCLUB and PhaseUniCLUB,\nwhich incorporate enhanced exploration mechanisms to accelerate cluster\nidentification. Remarkably, our algorithms require substantially weaker\nassumptions while achieving regret bounds comparable to prior work. Second,\ninspired by the smoothed analysis framework, we propose a more practical\nsetting that eliminates the requirement for i.i.d. context generation used in\nprevious studies, thus enhancing the performance of existing algorithms for\nonline clustering of bandits. Our technique can be applied to both graph-based\nand set-based clustering of bandits frameworks. Extensive evaluations on both\nsynthetic and real-world datasets demonstrate that our proposed algorithms\nconsistently outperform existing approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00891v1",
    "published_date": "2025-01-01 16:38:29 UTC",
    "updated_date": "2025-01-01 16:38:29 UTC"
  },
  {
    "arxiv_id": "2501.00885v1",
    "title": "Representation in large language models",
    "authors": [
      "Cameron C. Yetman"
    ],
    "abstract": "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Draft of paper under review. 27 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.00885v1",
    "published_date": "2025-01-01 16:19:48 UTC",
    "updated_date": "2025-01-01 16:19:48 UTC"
  },
  {
    "arxiv_id": "2501.00884v1",
    "title": "Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement Learning",
    "authors": [
      "Qi Li",
      "Zhiguang Cao",
      "Yining Ma",
      "Yaoxin Wu",
      "Yue-Jiao Gong"
    ],
    "abstract": "Existing neural methods for the Travelling Salesman Problem (TSP) mostly aim\nat finding a single optimal solution. To discover diverse yet high-quality\nsolutions for Multi-Solution TSP (MSTSP), we propose a novel deep reinforcement\nlearning based neural solver, which is primarily featured by an encoder-decoder\nstructured policy. Concretely, on the one hand, a Relativization Filter (RF) is\ndesigned to enhance the robustness of the encoder to affine transformations of\nthe instances, so as to potentially improve the quality of the found solutions.\nOn the other hand, a Multi-Attentive Adaptive Active Search (MA3S) is tailored\nto allow the decoders to strike a balance between the optimality and diversity.\nExperimental evaluations on benchmark instances demonstrate the superiority of\nour method over recent neural baselines across different metrics, and its\ncompetitive performance against state-of-the-art traditional heuristics with\nsignificantly reduced computational time, ranging from $1.3\\times$ to\n$15\\times$ faster. Furthermore, we demonstrate that our method can also be\napplied to the Capacitated Vehicle Routing Problem (CVRP).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00884v1",
    "published_date": "2025-01-01 16:08:40 UTC",
    "updated_date": "2025-01-01 16:08:40 UTC"
  },
  {
    "arxiv_id": "2501.00862v1",
    "title": "DiffETM: Diffusion Process Enhanced Embedded Topic Model",
    "authors": [
      "Wei Shao",
      "Mingyang Liu",
      "Linqi Song"
    ],
    "abstract": "The embedded topic model (ETM) is a widely used approach that assumes the\nsampled document-topic distribution conforms to the logistic normal\ndistribution for easier optimization. However, this assumption oversimplifies\nthe real document-topic distribution, limiting the model's performance. In\nresponse, we propose a novel method that introduces the diffusion process into\nthe sampling process of document-topic distribution to overcome this limitation\nand maintain an easy optimization process. We validate our method through\nextensive experiments on two mainstream datasets, proving its effectiveness in\nimproving topic modeling performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 2 figures, Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.00862v1",
    "published_date": "2025-01-01 15:15:39 UTC",
    "updated_date": "2025-01-01 15:15:39 UTC"
  },
  {
    "arxiv_id": "2501.00855v2",
    "title": "What is a Social Media Bot? A Global Comparison of Bot and Human Characteristics",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Kathleen M. Carley"
    ],
    "abstract": "Chatter on social media is 20% bots and 80% humans. Chatter by bots and\nhumans is consistently different: bots tend to use linguistic cues that can be\neasily automated while humans use cues that require dialogue understanding.\nBots use words that match the identities they choose to present, while humans\nmay send messages that are not related to the identities they present. Bots and\nhumans differ in their communication structure: sampled bots have a star\ninteraction structure, while sampled humans have a hierarchical structure.\nThese conclusions are based on a large-scale analysis of social media tweets\nacross ~200mil users across 7 events. Social media bots took the world by storm\nwhen social-cybersecurity researchers realized that social media users not only\nconsisted of humans but also of artificial agents called bots. These bots wreck\nhavoc online by spreading disinformation and manipulating narratives. Most\nresearch on bots are based on special-purposed definitions, mostly predicated\non the event studied. This article first begins by asking, \"What is a bot?\",\nand we study the underlying principles of how bots are different from humans.\nWe develop a first-principle definition of a social media bot. With this\ndefinition as a premise, we systematically compare characteristics between bots\nand humans across global events, and reflect on how the software-programmed bot\nis an Artificial Intelligent algorithm, and its potential for evolution as\ntechnology advances. Based on our results, we provide recommendations for the\nuse and regulation of bots. Finally, we discuss open challenges and future\ndirections: Detect, to systematically identify these automated and potentially\nevolving bots; Differentiate, to evaluate the goodness of the bot in terms of\ntheir content postings and relationship interactions; Disrupt, to moderate the\nimpact of malicious bots.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00855v2",
    "published_date": "2025-01-01 14:45:43 UTC",
    "updated_date": "2025-02-25 20:11:10 UTC"
  },
  {
    "arxiv_id": "2501.00840v1",
    "title": "Distilled Lifelong Self-Adaptation for Configurable Systems",
    "authors": [
      "Yulong Ye",
      "Tao Chen",
      "Miqing Li"
    ],
    "abstract": "Modern configurable systems provide tremendous opportunities for engineering\nfuture intelligent software systems. A key difficulty thereof is how to\neffectively self-adapt the configuration of a running system such that its\nperformance (e.g., runtime and throughput) can be optimized under time-varying\nworkloads. This unfortunately remains unaddressed in existing approaches as\nthey either overlook the available past knowledge or rely on static\nexploitation of past knowledge without reasoning the usefulness of information\nwhen planning for self-adaptation. In this paper, we tackle this challenging\nproblem by proposing DLiSA, a framework that self-adapts configurable systems.\nDLiSA comes with two properties: firstly, it supports lifelong planning, and\nthereby the planning process runs continuously throughout the lifetime of the\nsystem, allowing dynamic exploitation of the accumulated knowledge for rapid\nadaptation. Secondly, the planning for a newly emerged workload is boosted via\ndistilled knowledge seeding, in which the knowledge is dynamically purified\nsuch that only useful past configurations are seeded when necessary, mitigating\nmisleading information. Extensive experiments suggest that the proposed DLiSA\nsignificantly outperforms state-of-the-art approaches, demonstrating a\nperformance improvement of up to 229% and a resource acceleration of up to\n2.22x on generating promising adaptation configurations. All data and sources\ncan be found at our repository: https://github.com/ideas-labo/dlisa.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by the 2025 International Conference on Software Engineering\n  (ICSE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.00840v1",
    "published_date": "2025-01-01 13:41:57 UTC",
    "updated_date": "2025-01-01 13:41:57 UTC"
  },
  {
    "arxiv_id": "2501.00830v2",
    "title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions",
    "authors": [
      "Adam Ishay",
      "Joohyung Lee"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "42 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.00830v2",
    "published_date": "2025-01-01 13:20:01 UTC",
    "updated_date": "2025-02-04 14:37:29 UTC"
  },
  {
    "arxiv_id": "2501.00829v1",
    "title": "An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems",
    "authors": [
      "Haoxiang Tian",
      "Xingshuo Han",
      "Guoquan Wu",
      "An Guo",
      "Yuan Zhou. Jie Zhang",
      "Shuo Li",
      "Jun Wei",
      "Tianwei Zhang"
    ],
    "abstract": "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching\noptimal solutions in complex multi-component applications. Traditional MOEAs\nfor multi-component deep learning (MCDL) systems face challenges in enhancing\nthe search efficiency while maintaining the diversity. To combat these, this\npaper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search\nalgorithm to detect safety violations in MCDL systems. Inspired by the\ncontext-understanding ability of Large Language Models (LLMs), $\\mu$MOEA\npromotes the LLM to comprehend the optimization problem and generate an initial\npopulation tailed to evolutionary objectives. Subsequently, it employs adaptive\nselection and variation to iteratively produce offspring, balancing the\nevolutionary efficiency and diversity. During the evolutionary process, to\nnavigate away from the local optima, $\\mu$MOEA integrates the evolutionary\nexperience back into the LLM. This utilization harnesses the LLM's quantitative\nreasoning prowess to generate differential seeds, breaking away from current\noptimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL\nsystems, and compare its performance with state-of-the-art MOEA methods.\nExperimental results show that $\\mu$MOEA can significantly improve the\nefficiency and diversity of the evolutionary search.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "9",
    "pdf_url": "http://arxiv.org/pdf/2501.00829v1",
    "published_date": "2025-01-01 13:19:58 UTC",
    "updated_date": "2025-01-01 13:19:58 UTC"
  },
  {
    "arxiv_id": "2501.00828v1",
    "title": "Embedding Style Beyond Topics: Analyzing Dispersion Effects Across Different Language Models",
    "authors": [
      "Benjamin Icard",
      "Evangelia Zve",
      "Lila Sainero",
      "Alice Breton",
      "Jean-Gabriel Ganascia"
    ],
    "abstract": "This paper analyzes how writing style affects the dispersion of embedding\nvectors across multiple, state-of-the-art language models. While early\ntransformer models primarily aligned with topic modeling, this study examines\nthe role of writing style in shaping embedding spaces. Using a literary corpus\nthat alternates between topics and styles, we compare the sensitivity of\nlanguage models across French and English. By analyzing the particular impact\nof style on embedding dispersion, we aim to better understand how language\nmodels process stylistic information, contributing to their overall\ninterpretability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Abu Dhabi",
    "pdf_url": "http://arxiv.org/pdf/2501.00828v1",
    "published_date": "2025-01-01 13:17:16 UTC",
    "updated_date": "2025-01-01 13:17:16 UTC"
  },
  {
    "arxiv_id": "2501.00826v2",
    "title": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management",
    "authors": [
      "Yichen Luo",
      "Yebo Feng",
      "Jiahua Xu",
      "Paolo Tasca",
      "Yang Liu"
    ],
    "abstract": "Cryptocurrency investment is inherently difficult due to its shorter history\ncompared to traditional assets, the need to integrate vast amounts of data from\nvarious modalities, and the requirement for complex reasoning. While deep\nlearning approaches have been applied to address these challenges, their\nblack-box nature raises concerns about trust and explainability. Recently,\nlarge language models (LLMs) have shown promise in financial applications due\nto their ability to understand multi-modal data and generate explainable\ndecisions. However, single LLM faces limitations in complex, comprehensive\ntasks such as asset investment. These limitations are even more pronounced in\ncryptocurrency investment, where LLMs have less domain-specific knowledge in\ntheir training corpora.\n  To overcome these challenges, we propose an explainable, multi-modal,\nmulti-agent framework for cryptocurrency investment. Our framework uses\nspecialized agents that collaborate within and across teams to handle subtasks\nsuch as data analysis, literature integration, and investment decision-making\nfor the top 30 cryptocurrencies by market capitalization. The expert training\nmodule fine-tunes agents using multi-modal historical data and professional\ninvestment literature, while the multi-agent investment module employs\nreal-time data to make informed cryptocurrency investment decisions. Unique\nintrateam and interteam collaboration mechanisms enhance prediction accuracy by\nadjusting final predictions based on confidence levels within agent teams and\nfacilitating information sharing between teams. Empirical evaluation using data\nfrom November 2023 to September 2024 demonstrates that our framework\noutperforms single-agent models and market benchmarks in classification, asset\npricing, portfolio, and explainability performance.",
    "categories": [
      "q-fin.TR",
      "cs.AI"
    ],
    "primary_category": "q-fin.TR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00826v2",
    "published_date": "2025-01-01 13:08:17 UTC",
    "updated_date": "2025-01-07 00:15:11 UTC"
  },
  {
    "arxiv_id": "2501.00823v2",
    "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
    "authors": [
      "Zhenyu Guo",
      "Wenguang Chen"
    ],
    "abstract": "Transformers have achieved remarkable success across diverse domains, but\ntheir monolithic architecture presents challenges in interpretability,\nadaptability, and scalability. This paper introduces a novel modular\nTransformer architecture that explicitly decouples knowledge and reasoning\nthrough a generalized cross-attention mechanism to a globally shared knowledge\nbase with layer-specific transformations, specifically designed for effective\nknowledge retrieval. Critically, we provide a rigorous mathematical derivation\ndemonstrating that the Feed-Forward Network (FFN) in a standard Transformer is\na specialized case (a closure) of this generalized cross-attention, revealing\nits role in implicit knowledge retrieval and validating our design. This\ntheoretical framework provides a new lens for understanding FFNs and lays the\nfoundation for future research exploring enhanced interpretability,\nadaptability, and scalability, enabling richer interplay with external\nknowledge bases and other systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00823v2",
    "published_date": "2025-01-01 12:55:57 UTC",
    "updated_date": "2025-01-06 14:26:41 UTC"
  },
  {
    "arxiv_id": "2501.01473v1",
    "title": "Unraveling Indirect In-Context Learning Using Influence Functions",
    "authors": [
      "Hadi Askari",
      "Shivanshu Gupta",
      "Terry Tong",
      "Fei Wang",
      "Anshuman Chhabra",
      "Muhao Chen"
    ],
    "abstract": "This work introduces a novel paradigm for generalized In-Context Learning\n(ICL), termed Indirect In-Context Learning. In Indirect ICL, we explore\ndemonstration selection strategies tailored for two distinct real-world\nscenarios: Mixture of Tasks and Noisy Demonstrations. We systematically\nevaluate the effectiveness of Influence Functions (IFs) as a selection tool for\nthese settings, highlighting the potential for IFs to better capture the\ninformativeness of examples within the demonstration pool. For the Mixture of\nTasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU,\nBigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining\nBertScore-Recall (BSR) with an IF surrogate model can significantly improve\nperformance, leading to average absolute accuracy gains of 0.37\\% and 1.45\\%\nfor 3-shot and 5-shot setups when compared to traditional ICL metrics. In the\nNoisy Demonstrations setting, we examine scenarios where demonstrations might\nbe mislabeled. Our experiments show that reweighting traditional ICL selectors\n(BSR and Cosine Similarity) with IF-based selectors boosts accuracy by an\naverage of 2.90\\% for Cosine Similarity and 2.94\\% for BSR on noisy GLUE\nbenchmarks. In sum, we propose a robust framework for demonstration selection\nthat generalizes beyond traditional ICL, offering valuable insights into the\nrole of IFs for Indirect ICL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2501.01473v1",
    "published_date": "2025-01-01 12:37:12 UTC",
    "updated_date": "2025-01-01 12:37:12 UTC"
  },
  {
    "arxiv_id": "2501.03257v1",
    "title": "Breaking Through the Spike: Spike Window Decoding for Accelerated and Precise Automatic Speech Recognition",
    "authors": [
      "Wei Zhang",
      "Tian-Hao Zhang",
      "Chao Luo",
      "Hui Zhou",
      "Chao Yang",
      "Xinyuan Qian",
      "Xu-Cheng Yin"
    ],
    "abstract": "Recently, end-to-end automatic speech recognition has become the mainstream\napproach in both industry and academia. To optimize system performance in\nspecific scenarios, the Weighted Finite-State Transducer (WFST) is extensively\nused to integrate acoustic and language models, leveraging its capacity to\nimplicitly fuse language models within static graphs, thereby ensuring robust\nrecognition while also facilitating rapid error correction. However, WFST\nnecessitates a frame-by-frame search of CTC posterior probabilities through\nautoregression, which significantly hampers inference speed. In this work, we\nthoroughly investigate the spike property of CTC outputs and further propose\nthe conjecture that adjacent frames to non-blank spikes carry semantic\ninformation beneficial to the model. Building on this, we propose the Spike\nWindow Decoding algorithm, which greatly improves the inference speed by making\nthe number of frames decoded in WFST linearly related to the number of spiking\nframes in the CTC output, while guaranteeing the recognition performance. Our\nmethod achieves SOTA recognition accuracy with significantly accelerates\ndecoding speed, proven across both AISHELL-1 and large-scale In-House datasets,\nestablishing a pioneering approach for integrating CTC output with WFST.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03257v1",
    "published_date": "2025-01-01 12:20:07 UTC",
    "updated_date": "2025-01-01 12:20:07 UTC"
  },
  {
    "arxiv_id": "2501.01472v1",
    "title": "Augmented Contrastive Clustering with Uncertainty-Aware Prototyping for Time Series Test Time Adaptation",
    "authors": [
      "Peiliang Gong",
      "Mohamed Ragab",
      "Min Wu",
      "Zhenghua Chen",
      "Yongyi Su",
      "Xiaoli Li",
      "Daoqiang Zhang"
    ],
    "abstract": "Test-time adaptation aims to adapt pre-trained deep neural networks using\nsolely online unlabelled test data during inference. Although TTA has shown\npromise in visual applications, its potential in time series contexts remains\nlargely unexplored. Existing TTA methods, originally designed for visual tasks,\nmay not effectively handle the complex temporal dynamics of real-world time\nseries data, resulting in suboptimal adaptation performance. To address this\ngap, we propose Augmented Contrastive Clustering with Uncertainty-aware\nPrototyping (ACCUP), a straightforward yet effective TTA method for time series\ndata. Initially, our approach employs augmentation ensemble on the time series\ndata to capture diverse temporal information and variations, incorporating\nuncertainty-aware prototypes to distill essential characteristics.\nAdditionally, we introduce an entropy comparison scheme to selectively acquire\nmore confident predictions, enhancing the reliability of pseudo labels.\nFurthermore, we utilize augmented contrastive clustering to enhance feature\ndiscriminability and mitigate error accumulation from noisy pseudo labels,\npromoting cohesive clustering within the same class while facilitating clear\nseparation between different classes. Extensive experiments conducted on three\nreal-world time series datasets and an additional visual dataset demonstrate\nthe effectiveness and generalization potential of the proposed method,\nadvancing the underexplored realm of TTA for time series data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01472v1",
    "published_date": "2025-01-01 11:45:17 UTC",
    "updated_date": "2025-01-01 11:45:17 UTC"
  },
  {
    "arxiv_id": "2501.00803v1",
    "title": "Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning",
    "authors": [
      "Jingyao Tang",
      "Lishuang Li",
      "Liteng Mi",
      "Haiming Wu",
      "Hongbin Lu"
    ],
    "abstract": "Zero-shot event-relational reasoning is an important task in natural language\nprocessing, and existing methods jointly learn a variety of event-relational\nprefixes and inference-form prefixes to achieve such tasks. However, training\nprefixes consumes large computational resources and lacks interpretability.\nAdditionally, learning various relational and inferential knowledge\ninefficiently exploits the connections between tasks. Therefore, we first\npropose a method for Reasoning-Oriented Locating and Editing (ROLE), which\nlocates and edits the key modules of the language model for reasoning about\nevent relations, enhancing interpretability and also resource-efficiently\noptimizing the reasoning ability. Subsequently, we propose a method for\nAnalogy-Based Locating and Editing (ABLE), which efficiently exploits the\nsimilarities and differences between tasks to optimize the zero-shot reasoning\ncapability. Experimental results show that ROLE improves interpretability and\nreasoning performance with reduced computational cost. ABLE achieves SOTA\nresults in zero-shot reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00803v1",
    "published_date": "2025-01-01 11:02:08 UTC",
    "updated_date": "2025-01-01 11:02:08 UTC"
  },
  {
    "arxiv_id": "2501.00798v2",
    "title": "Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates Algorithm for Protecting Neural Networks",
    "authors": [
      "Leonard Puškáč",
      "Marek Benovič",
      "Jakub Breier",
      "Xiaolu Hou"
    ],
    "abstract": "Neural network models implemented in embedded devices have been shown to be\nsusceptible to side-channel attacks (SCAs), allowing recovery of proprietary\nmodel parameters, such as weights and biases. There are already available\ncountermeasure methods currently used for protecting cryptographic\nimplementations that can be tailored to protect embedded neural network models.\nShuffling, a hiding-based countermeasure that randomly shuffles the order of\ncomputations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm\nis used. In this paper, we propose a design of an SCA-secure version of the\nFisher-Yates algorithm. By integrating the masking technique for modular\nreduction and Blakely's method for modular multiplication, we effectively\nremove the vulnerability in the division operation that led to side-channel\nleakage in the original version of the algorithm. We experimentally evaluate\nthat the countermeasure is effective against SCA by implementing a correlation\npower analysis attack on an embedded neural network model implemented on ARM\nCortex-M4. Compared to the original proposal, the memory overhead is $2\\times$\nthe biggest layer of the network, while the time overhead varies from $4\\%$ to\n$0.49\\%$ for a layer with $100$ and $1000$ neurons, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00798v2",
    "published_date": "2025-01-01 10:46:22 UTC",
    "updated_date": "2025-04-23 07:49:47 UTC"
  },
  {
    "arxiv_id": "2501.03256v1",
    "title": "AI-ANNE: (A) (N)eural (N)et for (E)xploration: Transferring Deep Learning Models onto Microcontrollers and Embedded Systems",
    "authors": [
      "Dennis Klinkhammer"
    ],
    "abstract": "This working paper explores the integration of neural networks onto\nresource-constrained embedded systems like a Raspberry Pi Pico / Raspberry Pi\nPico 2. A TinyML aproach transfers neural networks directly on these\nmicrocontrollers, enabling real-time, low-latency, and energy-efficient\ninference while maintaining data privacy. Therefore, AI-ANNE: (A) (N)eural\n(N)et for (E)xploration will be presented, which facilitates the transfer of\npre-trained models from high-performance platforms like TensorFlow and Keras\nonto microcontrollers, using a lightweight programming language like\nMicroPython. This approach demonstrates how neural network architectures, such\nas neurons, layers, density and activation functions can be implemented in\nMicroPython in order to deal with the computational limitations of embedded\nsystems. Based on the Raspberry Pi Pico / Raspberry Pi Pico 2, two different\nneural networks on microcontrollers are presented for an example of data\nclassification. As an further application example, such a microcontroller can\nbe used for condition monitoring, where immediate corrective measures are\ntriggered on the basis of sensor data. Overall, this working paper presents a\nvery easy-to-implement way of using neural networks on energy-efficient devices\nsuch as microcontrollers. This makes AI-ANNE: (A) (N)eural (N)et for\n(E)xploration not only suited for practical use, but also as an educational\ntool with clear insights into how neural networks operate.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.5; K.3.2"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.03256v1",
    "published_date": "2025-01-01 10:29:55 UTC",
    "updated_date": "2025-01-01 10:29:55 UTC"
  },
  {
    "arxiv_id": "2501.00790v2",
    "title": "LENS-XAI: Redefining Lightweight and Explainable Network Security through Knowledge Distillation and Variational Autoencoders for Scalable Intrusion Detection in Cybersecurity",
    "authors": [
      "Muhammet Anil Yagiz",
      "Polat Goktas"
    ],
    "abstract": "The rapid proliferation of Industrial Internet of Things (IIoT) systems\nnecessitates advanced, interpretable, and scalable intrusion detection systems\n(IDS) to combat emerging cyber threats. Traditional IDS face challenges such as\nhigh computational demands, limited explainability, and inflexibility against\nevolving attack patterns. To address these limitations, this study introduces\nthe Lightweight Explainable Network Security framework (LENS-XAI), which\ncombines robust intrusion detection with enhanced interpretability and\nscalability. LENS-XAI integrates knowledge distillation, variational\nautoencoder models, and attribution-based explainability techniques to achieve\nhigh detection accuracy and transparency in decision-making. By leveraging a\ntraining set comprising 10% of the available data, the framework optimizes\ncomputational efficiency without sacrificing performance. Experimental\nevaluation on four benchmark datasets: Edge-IIoTset, UKM-IDS20, CTU-13, and\nNSL-KDD, demonstrates the framework's superior performance, achieving detection\naccuracies of 95.34%, 99.92%, 98.42%, and 99.34%, respectively. Additionally,\nthe framework excels in reducing false positives and adapting to complex attack\nscenarios, outperforming existing state-of-the-art methods. Key strengths of\nLENS-XAI include its lightweight design, suitable for resource-constrained\nenvironments, and its scalability across diverse IIoT and cybersecurity\ncontexts. Moreover, the explainability module enhances trust and transparency,\ncritical for practical deployment in dynamic and sensitive applications. This\nresearch contributes significantly to advancing IDS by addressing computational\nefficiency, feature interpretability, and real-world applicability. Future work\ncould focus on extending the framework to ensemble AI systems for distributed\nenvironments, further enhancing its robustness and adaptability.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00790v2",
    "published_date": "2025-01-01 10:00:49 UTC",
    "updated_date": "2025-01-07 23:43:09 UTC"
  },
  {
    "arxiv_id": "2501.00779v1",
    "title": "REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization",
    "authors": [
      "Huyen Nguyen",
      "Hieu Dam",
      "Nguyen Do",
      "Cong Tran",
      "Cuong Pham"
    ],
    "abstract": "In social online platforms, identifying influential seed users to maximize\ninfluence spread is a crucial as it can greatly diminish the cost and efforts\nrequired for information dissemination. While effective, traditional methods\nfor Multiplex Influence Maximization (MIM) have reached their performance\nlimits, prompting the emergence of learning-based approaches. These novel\nmethods aim for better generalization and scalability for more sizable graphs\nbut face significant challenges, such as (1) inability to handle unknown\ndiffusion patterns and (2) reliance on high-quality training samples. To\naddress these issues, we propose the Reinforced Expert Maximization framework\n(REM). REM leverages a Propagation Mixture of Experts technique to encode\ndynamic propagation of large multiplex networks effectively in order to\ngenerate enhanced influence propagation. Noticeably, REM treats a generative\nmodel as a policy to autonomously generate different seed sets and learn how to\nimprove them from a Reinforcement Learning perspective. Extensive experiments\non several real-world datasets demonstrate that REM surpasses state-of-the-art\nmethods in terms of influence spread, scalability, and inference time in\ninfluence maximization tasks.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00779v1",
    "published_date": "2025-01-01 09:13:09 UTC",
    "updated_date": "2025-01-01 09:13:09 UTC"
  },
  {
    "arxiv_id": "2501.00773v1",
    "title": "Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments, Analysis, and Improvements",
    "authors": [
      "Haoyang Li",
      "Yuming Xu",
      "Chen Jason Zhang",
      "Alexander Zhou",
      "Lei Chen",
      "Qing Li"
    ],
    "abstract": "Graphs are essential data structures for modeling complex interactions in\ndomains such as social networks, molecular structures, and biological systems.\nGraph-level tasks, which predict properties or classes for the entire graph,\nare critical for applications, such as molecular property prediction and\nsubgraph counting. Graph Neural Networks (GNNs) have shown promise in these\ntasks, but their evaluations are often limited to narrow datasets, tasks, and\ninconsistent experimental setups, restricting their generalizability. To\naddress these limitations, we propose a unified evaluation framework for\ngraph-level GNNs. This framework provides a standardized setting to evaluate\nGNNs across diverse datasets, various graph tasks (e.g., graph classification\nand regression), and challenging scenarios, including noisy, imbalanced, and\nfew-shot graphs. Additionally, we propose a novel GNN model with enhanced\nexpressivity and generalization capabilities. Specifically, we enhance the\nexpressivity of GNNs through a $k$-path rooted subgraph approach, enabling the\nmodel to effectively count subgraphs (e.g., paths and cycles). Moreover, we\nintroduce a unified graph contrastive learning algorithm for graphs across\ndiverse domains, which adaptively removes unimportant edges to augment graphs,\nthereby significantly improving generalization performance. Extensive\nexperiments demonstrate that our model achieves superior performance against\nfourteen effective baselines across twenty-seven graph datasets, establishing\nit as a robust and generalizable model for graph-level tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00773v1",
    "published_date": "2025-01-01 08:48:53 UTC",
    "updated_date": "2025-01-01 08:48:53 UTC"
  },
  {
    "arxiv_id": "2501.02000v1",
    "title": "Multi-Center Study on Deep Learning-Assisted Detection and Classification of Fetal Central Nervous System Anomalies Using Ultrasound Imaging",
    "authors": [
      "Yang Qi",
      "Jiaxin Cai",
      "Jing Lu",
      "Runqing Xiong",
      "Rongshang Chen",
      "Liping Zheng",
      "Duo Ma"
    ],
    "abstract": "Prenatal ultrasound evaluates fetal growth and detects congenital\nabnormalities during pregnancy, but the examination of ultrasound images by\nradiologists requires expertise and sophisticated equipment, which would\notherwise fail to improve the rate of identifying specific types of fetal\ncentral nervous system (CNS) abnormalities and result in unnecessary patient\nexaminations. We construct a deep learning model to improve the overall\naccuracy of the diagnosis of fetal cranial anomalies to aid prenatal diagnosis.\nIn our collected multi-center dataset of fetal craniocerebral anomalies\ncovering four typical anomalies of the fetal central nervous system (CNS):\nanencephaly, encephalocele (including meningocele), holoprosencephaly, and\nrachischisis, patient-level prediction accuracy reaches 94.5%, with an AUROC\nvalue of 99.3%. In the subgroup analyzes, our model is applicable to the entire\ngestational period, with good identification of fetal anomaly types for any\ngestational period. Heatmaps superimposed on the ultrasound images not only\nprovide a visual interpretation for the algorithm but also provide an intuitive\nvisual aid to the physician by highlighting key areas that need to be reviewed,\nhelping the physician to quickly identify and validate key areas. Finally, the\nretrospective reader study demonstrates that by combining the automatic\nprediction of the DL system with the professional judgment of the radiologist,\nthe diagnostic accuracy and efficiency can be effectively improved and the\nmisdiagnosis rate can be reduced, which has an important clinical application\nprospect.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02000v1",
    "published_date": "2025-01-01 07:56:26 UTC",
    "updated_date": "2025-01-01 07:56:26 UTC"
  },
  {
    "arxiv_id": "2501.00759v1",
    "title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
    "authors": [
      "Tianshi Zheng",
      "Jiazheng Wang",
      "Zihao Wang",
      "Jiaxin Bai",
      "Hang Yin",
      "Zheye Deng",
      "Yangqiu Song",
      "Jianxin Li"
    ],
    "abstract": "Transformers, as a fundamental deep learning architecture, have demonstrated\nremarkable capabilities in reasoning. This paper investigates the generalizable\nfirst-order logical reasoning ability of transformers with their parameterized\nknowledge and explores ways to improve it. The first-order reasoning capability\nof transformers is assessed through their ability to perform first-order\nlogical entailment, which is quantitatively measured by their performance in\nanswering knowledge graph queries. We establish connections between (1) two\ntypes of distribution shifts studied in out-of-distribution generalization and\n(2) the unseen knowledge and query settings discussed in the task of knowledge\ngraph query answering, enabling a characterization of fine-grained\ngeneralizability. Results on our comprehensive dataset show that transformers\noutperform previous methods specifically designed for this task and provide\ndetailed empirical evidence on the impact of input query syntax, token\nembedding, and transformer architectures on the reasoning capability of\ntransformers. Interestingly, our findings reveal a mismatch between positional\nencoding and other design choices in transformer architectures employed in\nprior practices. This discovery motivates us to propose a more sophisticated,\nlogic-aware architecture, TEGA, to enhance the capability for generalizable\nfirst-order logical entailment in transformers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.00759v1",
    "published_date": "2025-01-01 07:05:32 UTC",
    "updated_date": "2025-01-01 07:05:32 UTC"
  },
  {
    "arxiv_id": "2501.01999v2",
    "title": "On the Utility of Equivariance and Symmetry Breaking in Deep Learning Architectures on Point Clouds",
    "authors": [
      "Sharvaree Vadgama",
      "Mohammad Mohaiminul Islam",
      "Domas Buracus",
      "Christian Shewmake",
      "Erik Bekkers"
    ],
    "abstract": "This paper explores the key factors that influence the performance of models\nworking with point clouds, across different tasks of varying geometric\ncomplexity. In this work, we explore the trade-offs between flexibility and\nweight-sharing introduced by equivariant layers, assessing when equivariance\nboosts or detracts from performance. It is often argued that providing more\ninformation as input improves a model's performance. However, if this\nadditional information breaks certain properties, such as $\\SE(3)$\nequivariance, does it remain beneficial? We identify the key aspects of\nequivariant and non-equivariant architectures that drive success in different\ntasks by benchmarking them on segmentation, regression, and generation tasks\nacross multiple datasets with increasing complexity. We observe a positive\nimpact of equivariance, which becomes more pronounced with increasing task\ncomplexity, even when strict equivariance is not required.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.01999v2",
    "published_date": "2025-01-01 07:00:41 UTC",
    "updated_date": "2025-03-05 15:26:17 UTC"
  },
  {
    "arxiv_id": "2501.00755v1",
    "title": "An AI-powered Bayesian generative modeling approach for causal inference in observational studies",
    "authors": [
      "Qiao Liu",
      "Wing Hung Wong"
    ],
    "abstract": "Causal inference in observational studies with high-dimensional covariates\npresents significant challenges. We introduce CausalBGM, an AI-powered Bayesian\ngenerative modeling approach that captures the causal relationship among\ncovariates, treatment, and outcome variables. The core innovation of CausalBGM\nlies in its ability to estimate the individual treatment effect (ITE) by\nlearning individual-specific distributions of a low-dimensional latent feature\nset (e.g., latent confounders) that drives changes in both treatment and\noutcome. This approach not only effectively mitigates confounding effects but\nalso provides comprehensive uncertainty quantification, offering reliable and\ninterpretable causal effect estimates at the individual level. CausalBGM adopts\na Bayesian model and uses a novel iterative algorithm to update the model\nparameters and the posterior distribution of latent features until convergence.\nThis framework leverages the power of AI to capture complex dependencies among\nvariables while adhering to the Bayesian principles. Extensive experiments\ndemonstrate that CausalBGM consistently outperforms state-of-the-art methods,\nparticularly in scenarios with high-dimensional covariates and large-scale\ndatasets. Its Bayesian foundation ensures statistical rigor, providing robust\nand well-calibrated posterior intervals. By addressing key limitations of\nexisting methods, CausalBGM emerges as a robust and promising framework for\nadvancing causal inference in modern applications in fields such as genomics,\nhealthcare, and social sciences. CausalBGM is maintained at the website\nhttps://causalbgm.readthedocs.io/.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00755v1",
    "published_date": "2025-01-01 06:52:45 UTC",
    "updated_date": "2025-01-01 06:52:45 UTC"
  },
  {
    "arxiv_id": "2501.00750v2",
    "title": "Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform",
    "authors": [
      "Cheonsu Jeong"
    ],
    "abstract": "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 27 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.00750v2",
    "published_date": "2025-01-01 06:36:56 UTC",
    "updated_date": "2025-01-29 06:49:30 UTC"
  },
  {
    "arxiv_id": "2501.00745v2",
    "title": "Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines",
    "authors": [
      "Xiyang Hu"
    ],
    "abstract": "The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems\nare vulnerable to adversarial attacks, especially ranking manipulation attacks,\nwhere attackers craft webpage content to manipulate the LLM's ranking and\npromote specific content, gaining an unfair advantage over competitors. In this\npaper, we study the dynamics of ranking manipulation attacks. We frame this\nproblem as an Infinitely Repeated Prisoners' Dilemma, where multiple players\nstrategically decide whether to cooperate or attack. We analyze the conditions\nunder which cooperation can be sustained, identifying key factors such as\nattack costs, discount rates, attack success rates, and trigger strategies that\ninfluence player behavior. We identify tipping points in the system dynamics,\ndemonstrating that cooperation is more likely to be sustained when players are\nforward-looking. However, from a defense perspective, we find that simply\nreducing attack success probabilities can, paradoxically, incentivize attacks\nunder certain conditions. Furthermore, defensive measures to cap the upper\nbound of attack success rates may prove futile in some scenarios. These\ninsights highlight the complexity of securing LLM-based systems. Our work\nprovides a theoretical foundation and practical insights for understanding and\nmitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.GT",
      "cs.IR",
      "econ.TH"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00745v2",
    "published_date": "2025-01-01 06:23:26 UTC",
    "updated_date": "2025-05-15 21:22:32 UTC"
  },
  {
    "arxiv_id": "2501.00743v1",
    "title": "AttriReBoost: A Gradient-Free Propagation Optimization Method for Cold Start Mitigation in Attribute Missing Graphs",
    "authors": [
      "Mengran Li",
      "Chaojun Ding",
      "Junzhou Chen",
      "Wenbin Xing",
      "Cong Ye",
      "Ronghui Zhang",
      "Songlin Zhuang",
      "Jia Hu",
      "Tony Z. Qiu",
      "Huijun Gao"
    ],
    "abstract": "Missing attribute issues are prevalent in the graph learning, leading to\nbiased outcomes in Graph Neural Networks (GNNs). Existing methods that rely on\nfeature propagation are prone to cold start problem, particularly when dealing\nwith attribute resetting and low-degree nodes, which hinder effective\npropagation and convergence. To address these challenges, we propose\nAttriReBoost (ARB), a novel method that incorporates propagation-based method\nto mitigate cold start problems in attribute-missing graphs. ARB enhances\nglobal feature propagation by redefining initial boundary conditions and\nstrategically integrating virtual edges, thereby improving node connectivity\nand ensuring more stable and efficient convergence. This method facilitates\ngradient-free attribute reconstruction with lower computational overhead. The\nproposed method is theoretically grounded, with its convergence rigorously\nestablished. Extensive experiments on several real-world benchmark datasets\ndemonstrate the effectiveness of ARB, achieving an average accuracy improvement\nof 5.11% over state-of-the-art methods. Additionally, ARB exhibits remarkable\ncomputational efficiency, processing a large-scale graph with 2.49 million\nnodes in just 16 seconds on a single GPU. Our code is available at\nhttps://github.com/limengran98/ARB.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00743v1",
    "published_date": "2025-01-01 06:19:56 UTC",
    "updated_date": "2025-01-01 06:19:56 UTC"
  },
  {
    "arxiv_id": "2501.01470v1",
    "title": "Balance-aware Sequence Sampling Makes Multi-modal Learning Better",
    "authors": [
      "Zhi-Hao Guan"
    ],
    "abstract": "To address the modality imbalance caused by data heterogeneity, existing\nmulti-modal learning (MML) approaches primarily focus on balancing this\ndifference from the perspective of optimization objectives. However, almost all\nexisting methods ignore the impact of sample sequences, i.e., an inappropriate\ntraining order tends to trigger learning bias in the model, further\nexacerbating modality imbalance. In this paper, we propose Balance-aware\nSequence Sampling (BSS) to enhance the robustness of MML. Specifically, we\nfirst define a multi-perspective measurer to evaluate the balance degree of\neach sample. Via the evaluation, we employ a heuristic scheduler based on\ncurriculum learning (CL) that incrementally provides training subsets,\nprogressing from balanced to imbalanced samples to rebalance MML. Moreover,\nconsidering that sample balance may evolve as the model capability increases,\nwe propose a learning-based probabilistic sampling method to dynamically update\nthe training sequences at the epoch level, further improving MML performance.\nExtensive experiments on widely used datasets demonstrate the superiority of\nour method compared with state-of-the-art (SOTA) MML approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01470v1",
    "published_date": "2025-01-01 06:19:55 UTC",
    "updated_date": "2025-01-01 06:19:55 UTC"
  },
  {
    "arxiv_id": "2501.00741v2",
    "title": "Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors",
    "authors": [
      "Chuanzhi Xu",
      "Langyi Chen",
      "Haodong Chen",
      "Vera Chung",
      "Qiang Qu"
    ],
    "abstract": "Neuromorphic cameras, also known as event cameras, are asynchronous\nbrightness-change sensors that can capture extremely fast motion without\nsuffering from motion blur, making them particularly promising for 3D\nreconstruction in extreme environments. However, existing research on 3D\nreconstruction using monocular neuromorphic cameras is limited, and most of the\nmethods rely on estimating physical priors and employ complex multi-step\npipelines. In this work, we propose an end-to-end method for dense voxel 3D\nreconstruction using neuromorphic cameras that eliminates the need to estimate\nphysical priors. Our method incorporates a novel event representation to\nenhance edge features, enabling the proposed feature-enhancement model to learn\nmore effectively. Additionally, we introduced Optimal Binarization Threshold\nSelection Principle as a guideline for future related work, using the optimal\nreconstruction results achieved with threshold optimization as the benchmark.\nOur method achieves a 54.6% improvement in reconstruction accuracy compared to\nthe baseline method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 15 figures, 5 tables, accepted by IEEE International\n  Conference on Multimedia & Expo (ICME) 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.00741v2",
    "published_date": "2025-01-01 06:07:03 UTC",
    "updated_date": "2025-03-26 12:16:53 UTC"
  },
  {
    "arxiv_id": "2501.01998v2",
    "title": "SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation Framework",
    "authors": [
      "Mao Xun Huang",
      "Brian J Chan",
      "Hen-Hsen Huang"
    ],
    "abstract": "Stable Diffusion models have made remarkable strides in generating\nphotorealistic images from text prompts but often falter when tasked with\naccurately representing complex spatial arrangements, particularly involving\nintricate 3D relationships. To address this limitation, we introduce\nSmartSpatial, an innovative approach that not only enhances the spatial\narrangement capabilities of Stable Diffusion but also fosters AI-assisted\ncreative workflows through 3D-aware conditioning and attention-guided\nmechanisms. SmartSpatial incorporates depth information injection and\ncross-attention control to ensure precise object placement, delivering notable\nimprovements in spatial accuracy metrics. In conjunction with SmartSpatial, we\npresent SmartSpatialEval, a comprehensive evaluation framework that bridges\ncomputational spatial accuracy with qualitative artistic assessments.\nExperimental results show that SmartSpatial significantly outperforms existing\nmethods, setting new benchmarks for spatial fidelity in AI-driven art and\ncreativity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.01998v2",
    "published_date": "2025-01-01 04:52:18 UTC",
    "updated_date": "2025-02-23 19:22:58 UTC"
  },
  {
    "arxiv_id": "2501.00715v1",
    "title": "eRevise+RF: A Writing Evaluation System for Assessing Student Essay Revisions and Providing Formative Feedback",
    "authors": [
      "Zhexiong Liu",
      "Diane Litman",
      "Elaine Wang",
      "Tianwen Li",
      "Mason Gobat",
      "Lindsay Clare Matsumura",
      "Richard Correnti"
    ],
    "abstract": "The ability to revise essays in response to feedback is important for\nstudents' writing success. An automated writing evaluation (AWE) system that\nsupports students in revising their essays is thus essential. We present\neRevise+RF, an enhanced AWE system for assessing student essay revisions (e.g.,\nchanges made to an essay to improve its quality in response to essay feedback)\nand providing revision feedback. We deployed the system with 6 teachers and 406\nstudents across 3 schools in Pennsylvania and Louisiana. The results confirmed\nits effectiveness in (1) assessing student essays in terms of evidence usage,\n(2) extracting evidence and reasoning revisions across essays, and (3)\ndetermining revision success in responding to feedback. The evaluation also\nsuggested eRevise+RF is a helpful system for young students to improve their\nargumentative writing skills through revision and formative feedback.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00715v1",
    "published_date": "2025-01-01 03:49:48 UTC",
    "updated_date": "2025-01-01 03:49:48 UTC"
  },
  {
    "arxiv_id": "2501.00707v1",
    "title": "Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability",
    "authors": [
      "Hui Zeng",
      "Sanshuai Cui",
      "Biwei Chen",
      "Anjie Peng"
    ],
    "abstract": "Adversarial examples' (AE) transferability refers to the phenomenon that AEs\ncrafted with one surrogate model can also fool other models. Notwithstanding\nremarkable progress in untargeted transferability, its targeted counterpart\nremains challenging. This paper proposes an everywhere scheme to boost targeted\ntransferability. Our idea is to attack a victim image both globally and\nlocally. We aim to optimize 'an army of targets' in every local image region\ninstead of the previous works that optimize a high-confidence target in the\nimage. Specifically, we split a victim image into non-overlap blocks and\njointly mount a targeted attack on each block. Such a strategy mitigates\ntransfer failures caused by attention inconsistency between surrogate and\nvictim models and thus results in stronger transferability. Our approach is\nmethod-agnostic, which means it can be easily combined with existing\ntransferable attacks for even higher transferability. Extensive experiments on\nImageNet demonstrate that the proposed approach universally improves the\nstate-of-the-art targeted attacks by a clear margin, e.g., the transferability\nof the widely adopted Logit attack can be improved by 28.8%-300%.We also\nevaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results\nfurther support the superiority of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 6 figures, 8 tables, accepted by 2025AAAI",
    "pdf_url": "http://arxiv.org/pdf/2501.00707v1",
    "published_date": "2025-01-01 03:06:03 UTC",
    "updated_date": "2025-01-01 03:06:03 UTC"
  },
  {
    "arxiv_id": "2501.00692v1",
    "title": "Adjoint sharding for very long context training of state space models",
    "authors": [
      "Xingzi Xu",
      "Amir Tavanaei",
      "Kavosh Asadi",
      "Karim Bouyarmane"
    ],
    "abstract": "Despite very fast progress, efficiently training large language models (LLMs)\nin very long contexts remains challenging. Existing methods fall back to\ntraining LLMs with short contexts (a maximum of a few thousands tokens in\ntraining) and use inference time techniques when evaluating on long contexts\n(above 1M tokens context window at inference). As opposed to\nlong-context-inference, training on very long context input prompts is quickly\nlimited by GPU memory availability and by the prohibitively long training times\nit requires on state-of-the-art hardware. Meanwhile, many real-life\napplications require not only inference but also training/fine-tuning with long\ncontext on specific tasks. Such applications include, for example, augmenting\nthe context with various sources of raw reference information for fact\nextraction, fact summarization, or fact reconciliation tasks. We propose\nadjoint sharding, a novel technique that comprises sharding gradient\ncalculation during training to reduce memory requirements by orders of\nmagnitude, making training on very long context computationally tractable.\nAdjoint sharding is based on the adjoint method and computes equivalent\ngradients to backpropagation. We also propose truncated adjoint sharding to\nspeed up the algorithm while maintaining performance. We provide a distributed\nversion, and a paralleled version of adjoint sharding to further speed up\ntraining. Empirical results show the proposed adjoint sharding algorithm\nreduces memory usage by up to 3X with a 1.27B parameter large language model on\n1M context length training. This allows to increase the maximum context length\nduring training or fine-tuning of a 1.27B parameter model from 35K tokens to\nabove 100K tokens on a training infrastructure composed of five AWS P4\ninstances.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00692v1",
    "published_date": "2025-01-01 01:10:59 UTC",
    "updated_date": "2025-01-01 01:10:59 UTC"
  },
  {
    "arxiv_id": "2501.14777v1",
    "title": "Enhancing Supply Chain Resilience with Metaverse and ChatGPT Technologies",
    "authors": [
      "Oumaima Sarhir"
    ],
    "abstract": "Global supply lines have been severely disrupted by the COVID-19 epidemic and\nthe conflict between Russia and Ukraine, which has sharply increased the price\nof commodities and generated inflation. These incidents highlight how critical\nit is to improve supply chain resilience (SCRES) in order to fend off\nunforeseen setbacks. Controlling both internal and external interruptions, such\nas transportation problems brought on by natural catastrophes and wars, is the\nresponsibility of SCRES. Enhancing resilience in supply chains requires\naccurate and timely information transfer. Promising answers to these problems\ncan be found in the Metaverse and ChatGPT, two new digital technologies. The\nMetaverse may imitate real-world situations and offer dynamic, real-time 3D\nrepresentations of supply chain data by integrating blockchain, IoT, network\nconnection, and computer power.Large-scale natural language processing model\nChatGPT improves communication and data translation accuracy and speed. To\nmanage risk and facilitate decision making in Supply Chain management, firms\nshould increase information transmission, Speed and quality. This study aim to\nshow the importance of ChatGPT and Metaverse technologies to improve SCRES,\nwith an emphasis on the most important criteria for SCRES, and maturity factor\nthat can influence directly the SC development.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14777v1",
    "published_date": "2025-01-01 00:21:28 UTC",
    "updated_date": "2025-01-01 00:21:28 UTC"
  }
]
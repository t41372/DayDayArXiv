[
  {
    "arxiv_id": "2507.03223v1",
    "title": "SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models",
    "authors": [
      "Jeshwanth Challagundla"
    ],
    "abstract": "System Instructions (SIs), or system prompts, are pivotal for guiding Large Language Models (LLMs) but manual crafting is resource-intensive and often suboptimal. Existing automated methods frequently generate non-human-readable \"soft prompts,\" sacrificing interpretability. This paper introduces SI-Agent, a novel agentic framework designed to automatically generate and iteratively refine human-readable SIs through a feedback-driven loop. SI-Agent employs three collaborating agents: an Instructor Agent, an Instruction Follower Agent (target LLM), and a Feedback/Reward Agent evaluating task performance and optionally SI readability. The framework utilizes iterative cycles where feedback guides the Instructor's refinement strategy (e.g., LLM-based editing, evolutionary algorithms). We detail the framework's architecture, agent roles, the iterative refinement process, and contrast it with existing methods. We present experimental results validating SI-Agent's effectiveness, focusing on metrics for task performance, SI readability, and efficiency. Our findings indicate that SI-Agent generates effective, readable SIs, offering a favorable trade-off between performance and interpretability compared to baselines. Potential implications include democratizing LLM customization and enhancing model transparency. Challenges related to computational cost and feedback reliability are acknowledged.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03223v1",
    "published_date": "2025-07-03 23:44:50 UTC",
    "updated_date": "2025-07-03 23:44:50 UTC"
  },
  {
    "arxiv_id": "2507.03222v2",
    "title": "The role of gain neuromodulation in layer-5 pyramidal neurons",
    "authors": [
      "Alejandro Rodriguez-Garcia",
      "Christopher J. Whyte",
      "Brandon R. Munn",
      "Jie Mei",
      "James M. Shine",
      "Srikanth Ramaswamy"
    ],
    "abstract": "Biological and artificial learning systems alike confront the plasticity-stability dilemma. In the brain, neuromodulators such as acetylcholine and noradrenaline relieve this tension by tuning neuronal gain and inhibitory gating, balancing segregation and integration of circuits. Fed by dense cholinergic and noradrenergic projections from the ascending arousal system, layer-5 pyramidal neurons in the cerebral cortex offer a relevant substrate for understanding these dynamics. When distal dendritic signals coincide with back-propagating action potentials, calcium plateaus turn a single somatic spike into a high-gain burst, and interneuron inhibition sculpts the output. These properties make layer-5 cells gain-tunable amplifiers that translate neuromodulatory cues into flexible cortical activity. To capture this mechanism we developed a two-compartment Izhikevich model for pyramidal neurons and single-compartment somatostatin (SOM) and parvalbumin (PV) interneurons, linked by Gaussian connectivity and spike-timing-dependent plasticity (STDP). The soma and apical dendrite are so coupled that somatic spikes back-propagate, while dendritic plateaus can switch the soma from regular firing to bursting by shifting reset and adaptation variables. We show that stronger dendritic drive or tighter coupling raise gain by increasing the likelihood of calcium-triggered somatic bursts. In contrast, dendritic-targeted inhibition suppresses gain, while somatic-targeted inhibition raises the firing threshold of neighboring neurons, thus gating neurons output. Notably, bursting accelerates STDP, supporting rapid synaptic reconfiguration and flexibility. This suggests that brief gain pulses driven by neuromodulators could serve as an adaptive two-timescale optimization mechanism, effectively modulating the synaptic weight updates.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "12 pages, 7 figures, 1 table, presented at 34th Annual Computational Neuroscience Meeting",
    "pdf_url": "https://arxiv.org/pdf/2507.03222v2",
    "published_date": "2025-07-03 23:29:29 UTC",
    "updated_date": "2025-07-11 11:31:56 UTC"
  },
  {
    "arxiv_id": "2507.03221v1",
    "title": "Neural Inhibition Improves Dynamic Routing and Mixture of Experts",
    "authors": [
      "Will Y. Zou",
      "Jennifer Y. Zhang"
    ],
    "abstract": "To be effective, efficient, and diverse, deep learning models need to dynamically choose its architecture based on signals from a population of neurons. We hypothesize dynamic routing models can be improved with neural inhibition in those neural populations. This means signals commonly shared among the various modes of data statistics can be inhibited so that the routing model can choose a specialized expert path for each data sample. Only through inhibition is the routing mechanism able to effectively select neural pathways. We believe this is an under-studied and under-verified implementation methodology for Mixture-of-Experts, dynamic routing, and transformer language models. We provide experimental evidence that the neural inhibition algorithm significantly boosts the performance of general tasks and motivates more effort to be invested in this research direction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03221v1",
    "published_date": "2025-07-03 23:28:36 UTC",
    "updated_date": "2025-07-03 23:28:36 UTC"
  },
  {
    "arxiv_id": "2507.03220v3",
    "title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning",
    "authors": [
      "Saransh Gupta",
      "Umesh Deshpande",
      "Travis Janssen",
      "Swami Sundararaman"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the task-specific parameters into adapters, which are a fraction of the size of the original base model. Popularity of PEFT technique for fine-tuning has led to the creation of a large number of adapters for popular Large Language Models (LLMs). However, existing frameworks fall short in supporting inference or fine-tuning with multiple adapters in the following ways. 1) For fine-tuning, each job needs to deploy its dedicated base model instance, which results in excessive GPU memory consumption and poor GPU utilization. 2) While popular inference platforms can serve multiple PEFT adapters, they do not allow independent resource management or mixing of different PEFT methods. 3) They cannot make effective use of heterogeneous accelerators. 4) They do not provide privacy to users who may not wish to expose their fine-tuned parameters to service providers. In Symbiosis, we address the above problems by enabling the as-a-service deployment of the base model. The base model layers can be shared across multiple inference or fine-tuning processes. Our split-execution technique decouples the execution of client-specific adapters and layers from the frozen base model layers offering them flexibility to manage their resources, to select their fine-tuning method, to achieve their performance goals. Our approach is transparent to models and works out-of-the-box for most models in the transformers library. We demonstrate the use of Symbiosis to simultaneously fine-tune 20 Gemma2-27B adapters on 8 GPUs.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03220v3",
    "published_date": "2025-07-03 23:25:59 UTC",
    "updated_date": "2025-10-23 06:49:27 UTC"
  },
  {
    "arxiv_id": "2507.03216v1",
    "title": "Disclosing Generative AI Use in Digital Humanities Research",
    "authors": [
      "Rongqian Ma",
      "Xuhan Zhang",
      "Adrian Wisnicki"
    ],
    "abstract": "This survey study investigates how digital humanists perceive and approach generative AI disclosure in research. The results indicate that while digital humanities scholars acknowledge the importance of disclosing GenAI use, the actual rate of disclosure in research practice remains low. Respondents differ in their views on which activities most require disclosure and on the most appropriate methods for doing so. Most also believe that safeguards for AI disclosure should be established through institutional policies rather than left to individual decisions. The study's findings will offer empirical guidance to scholars, institutional leaders, funders, and other stakeholders responsible for shaping effective disclosure policies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DL",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03216v1",
    "published_date": "2025-07-03 23:11:45 UTC",
    "updated_date": "2025-07-03 23:11:45 UTC"
  },
  {
    "arxiv_id": "2507.03198v1",
    "title": "AI-driven Web Application for Early Detection of Sudden Death Syndrome (SDS) in Soybean Leaves Using Hyperspectral Images and Genetic Algorithm",
    "authors": [
      "Pappu Kumar Yadav",
      "Rishik Aggarwal",
      "Supriya Paudel",
      "Amee Parmar",
      "Hasan Mirzakhaninafchi",
      "Zain Ul Abideen Usmani",
      "Dhe Yeong Tchalla",
      "Shyam Solanki",
      "Ravi Mural",
      "Sachin Sharma",
      "Thomas F. Burks",
      "Jianwei Qin",
      "Moon S. Kim"
    ],
    "abstract": "Sudden Death Syndrome (SDS), caused by Fusarium virguliforme, poses a significant threat to soybean production. This study presents an AI-driven web application for early detection of SDS on soybean leaves using hyperspectral imaging, enabling diagnosis prior to visible symptom onset. Leaf samples from healthy and inoculated plants were scanned using a portable hyperspectral imaging system (398-1011 nm), and a Genetic Algorithm was employed to select five informative wavelengths (505.4, 563.7, 712.2, 812.9, and 908.4 nm) critical for discriminating infection status. These selected bands were fed into a lightweight Convolutional Neural Network (CNN) to extract spatial-spectral features, which were subsequently classified using ten classical machine learning models. Ensemble classifiers (Random Forest, AdaBoost), Linear SVM, and Neural Net achieved the highest accuracy (>98%) and minimal error across all folds, as confirmed by confusion matrices and cross-validation metrics. Poor performance by Gaussian Process and QDA highlighted their unsuitability for this dataset. The trained models were deployed within a web application that enables users to upload hyperspectral leaf images, visualize spectral profiles, and receive real-time classification results. This system supports rapid and accessible plant disease diagnostics, contributing to precision agriculture practices. Future work will expand the training dataset to encompass diverse genotypes, field conditions, and disease stages, and will extend the system for multiclass disease classification and broader crop applicability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03198v1",
    "published_date": "2025-07-03 22:20:47 UTC",
    "updated_date": "2025-07-03 22:20:47 UTC"
  },
  {
    "arxiv_id": "2507.03194v2",
    "title": "Quantifying Cognitive Bias Induction in LLM-Generated Content",
    "authors": [
      "Abeer Alessa",
      "Param Somane",
      "Akshaya Lakshminarasimhan",
      "Julian Skirzynski",
      "Julian McAuley",
      "Jessica Echterhoff"
    ],
    "abstract": "Large language models (LLMs) are integrated into applications like shopping reviews, summarization, or medical diagnosis support, where their use affects human decisions. We investigate the extent to which LLMs expose users to biased content and demonstrate its effect on human decision-making. We assess five LLM families in summarization and news fact-checking tasks, evaluating the consistency of LLMs with their context and their tendency to hallucinate on a new self-updating dataset. Our findings show that LLMs expose users to content that changes the context's sentiment in 26.42% of cases (framing bias), hallucinate on 60.33% of post-knowledge-cutoff questions, and highlight context from earlier parts of the prompt (primacy bias) in 10.12% of cases, averaged across all tested models. We further find that humans are 32% more likely to purchase the same product after reading a summary of the review generated by an LLM rather than the original review. To address these issues, we evaluate 18 mitigation methods across three LLM families and find the effectiveness of targeted interventions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages (including references and appendix), 3figures. accepted to AACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.03194v2",
    "published_date": "2025-07-03 21:56:44 UTC",
    "updated_date": "2025-11-30 21:03:11 UTC"
  },
  {
    "arxiv_id": "2507.05275v1",
    "title": "A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation",
    "authors": [
      "Weibing Zheng",
      "Laurah Turner",
      "Jess Kropczynski",
      "Murat Ozer",
      "Seth Overla",
      "Shane Halse"
    ],
    "abstract": "Assisting medical students with clinical reasoning (CR) during clinical scenario training remains a persistent challenge in medical education. This paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA), a novel component for the Multi-Agent Educational Clinical Scenario Simulation (MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to continuously interpret student interactions with specialized clinical agents (e.g., patient, physical exam, diagnostic, intervention) using pre-defined fuzzy rule bases for professionalism, medical relevance, ethical behavior, and contextual distraction. By analyzing student decision-making processes in real-time, the FSA is designed to deliver adaptive, context-aware feedback and provides assistance precisely when students encounter difficulties. This work focuses on the technical framework and rationale of the FSA, highlighting its potential to provide scalable, flexible, and human-like supervision in simulation-based medical education. Future work will include empirical evaluation and integration into broader educational settings. More detailed design and implementation is~\\href{https://github.com/2sigmaEdTech/MAS/}{open sourced here}.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LO",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, 3 figures, 1 table. 2025 IFSA World Congress NAFIPS Annual Meeting",
    "pdf_url": "https://arxiv.org/pdf/2507.05275v1",
    "published_date": "2025-07-03 21:51:27 UTC",
    "updated_date": "2025-07-03 21:51:27 UTC"
  },
  {
    "arxiv_id": "2507.03190v2",
    "title": "Discovering Algorithms with Computational Language Processing",
    "authors": [
      "Theo Bourdais",
      "Abeynaya Gnanasekaran",
      "Houman Owhadi",
      "Tuhin Sahai"
    ],
    "abstract": "Algorithms are the engine for reproducible problem-solving. We present a framework automating algorithm discovery by conceptualizing them as sequences of operations, represented as tokens. These computational tokens are chained using a grammar, enabling the formation of increasingly sophisticated procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement learning (RL) explores token chaining and drives the creation of new tokens. This methodology rediscovers, improves, and generates new algorithms that substantially outperform existing methods for strongly NP-hard combinatorial optimization problems and foundational quantum computing approaches such as Grover's and Quantum Approximate Optimization Algorithm. Operating at the computational rather than code-generation level, our framework produces algorithms that can be tailored specifically to problem instances, not merely classes.",
    "categories": [
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03190v2",
    "published_date": "2025-07-03 21:45:17 UTC",
    "updated_date": "2025-07-11 16:55:14 UTC"
  },
  {
    "arxiv_id": "2507.03176v2",
    "title": "Deep Learning Atmospheric Models Reliably Simulate Out-of-Sample Land Heat and Cold Wave Frequencies",
    "authors": [
      "Zilu Meng",
      "Gregory J. Hakim",
      "Wenchang Yang",
      "Gabriel A. Vecchi"
    ],
    "abstract": "Deep learning (DL)-based general circulation models (GCMs) are emerging as fast simulators, yet their ability to replicate extreme events outside their training range remains unknown. Here, we evaluate two such models -- the hybrid Neural General Circulation Model (NGCM) and purely data-driven Deep Learning Earth System Model (DL\\textit{ESy}M) -- against a conventional high-resolution land-atmosphere model (HiRAM) in simulating land heatwaves and coldwaves. All models are forced with observed sea surface temperatures and sea ice over 1900-2020, focusing on the out-of-sample early-20th-century period (1900-1960). Both DL models generalize successfully to unseen climate conditions, broadly reproducing the frequency and spatial patterns of heatwave and cold wave events during 1900-1960 with skill comparable to HiRAM. An exception is over portions of North Asia and North America, where all models perform poorly during 1940-1960. Due to excessive temperature autocorrelation, DL\\textit{ESy}M tends to overestimate heatwave and cold wave frequencies, whereas the physics-DL hybrid NGCM exhibits persistence more similar to HiRAM.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03176v2",
    "published_date": "2025-07-03 21:09:27 UTC",
    "updated_date": "2025-10-25 00:30:47 UTC"
  },
  {
    "arxiv_id": "2507.03175v1",
    "title": "Understanding Knowledge Transferability for Transfer Learning: A Survey",
    "authors": [
      "Haohua Wang",
      "Jingge Wang",
      "Zijie Zhao",
      "Yang Tan",
      "Yanru Wu",
      "Hanbing Liu",
      "Jingyun Yang",
      "Enming Zhang",
      "Xiangyu Chen",
      "Zhengze Rong",
      "Shanxin Guo",
      "Yang Li"
    ],
    "abstract": "Transfer learning has become an essential paradigm in artificial intelligence, enabling the transfer of knowledge from a source task to improve performance on a target task. This approach, particularly through techniques such as pretraining and fine-tuning, has seen significant success in fields like computer vision and natural language processing. However, despite its widespread use, how to reliably assess the transferability of knowledge remains a challenge. Understanding the theoretical underpinnings of each transferability metric is critical for ensuring the success of transfer learning. In this survey, we provide a unified taxonomy of transferability metrics, categorizing them based on transferable knowledge types and measurement granularity. This work examines the various metrics developed to evaluate the potential of source knowledge for transfer learning and their applicability across different learning paradigms emphasizing the need for careful selection of these metrics. By offering insights into how different metrics work under varying conditions, this survey aims to guide researchers and practitioners in selecting the most appropriate metric for specific applications, contributing to more efficient, reliable, and trustworthy AI systems. Finally, we discuss some open challenges in this field and propose future research directions to further advance the application of transferability metrics in trustworthy transfer learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 15 figures, submitted to ACM Computing Surveys",
    "pdf_url": "https://arxiv.org/pdf/2507.03175v1",
    "published_date": "2025-07-03 21:06:30 UTC",
    "updated_date": "2025-07-03 21:06:30 UTC"
  },
  {
    "arxiv_id": "2507.07117v1",
    "title": "Collective Communication Profiling of Modern-day Machine Learning Workloads",
    "authors": [
      "Jit Gupta",
      "Andrew Li",
      "Tarun Banka",
      "Ariel Cohen",
      "T. Sridhar",
      "Raj Yavatkar"
    ],
    "abstract": "Machine Learning jobs, carried out on large number of distributed high performance systems, involve periodic communication using operations like AllReduce, AllGather, and Broadcast. These operations may create high bandwidth and bursty traffic patterns, leading to network congestion and packet loss, thus impacting the performance of these jobs. Hence it is imperative to analyze these patterns, which can be helpful in provisioning network resources depending on the type of machine learning workloads. In this poster we carry out extensive analysis of the collective communication behavior seen in a wide variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we instrument Nvidia Collective Communication Library logging functionality for richer context about the collectives and workloads. We adjust configuration parameters that influence collective communication behavior, such as parallelism, number of nodes, and model type. This overview presents and discusses some of the results on the collective communication behavior for the open source DeepSeek V3 inferencing model, which includes operation type and count, transfer sizes per operation, and request size distribution. Our analysis shows that it makes sense to rethink current collective communication frameworks and network topologies so as to accommodate the effect of network anomalies on the mentioned workloads.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA",
    "pdf_url": "https://arxiv.org/pdf/2507.07117v1",
    "published_date": "2025-07-03 20:59:36 UTC",
    "updated_date": "2025-07-03 20:59:36 UTC"
  },
  {
    "arxiv_id": "2507.03167v2",
    "title": "Adversarial Manipulation of Reasoning Models using Internal Representations",
    "authors": [
      "Kureha Yamaguchi",
      "Benjamin Etheridge",
      "Andy Arditi"
    ],
    "abstract": "Reasoning models generate chain-of-thought (CoT) tokens before their final output, but how this affects their vulnerability to jailbreak attacks remains unclear. While traditional language models make refusal decisions at the prompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B makes these decisions within its CoT generation. We identify a linear direction in activation space during CoT token generation that predicts whether the model will refuse or comply -- termed the \"caution\" direction because it corresponds to cautious reasoning patterns in the generated text. Ablating this direction from model activations increases harmful compliance, effectively jailbreaking the model. We additionally show that intervening only on CoT token activations suffices to control final outputs, and that incorporating this direction into prompt-based attacks improves success rates. Our findings suggest that the chain-of-thought itself is a promising new target for adversarial manipulation in reasoning models. Code available at https://github.com/ky295/reasoning-manipulation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the ICML 2025 Workshop on Reliable and Responsible Foundation Models (R2FM). 20 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.03167v2",
    "published_date": "2025-07-03 20:51:32 UTC",
    "updated_date": "2025-08-27 20:29:53 UTC"
  },
  {
    "arxiv_id": "2507.03162v1",
    "title": "MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive, Multilingual, and Multimodal Educational Tasks",
    "authors": [
      "Dumitran Adrian Marius",
      "Theodor-Pierre Moroianu",
      "Buca Mihnea-Vicentiu"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has transformed various domains, particularly computer science (CS) education. These models exhibit remarkable capabilities in code-related tasks and problem-solving, raising questions about their potential and limitations in advanced CS contexts. This study presents a novel bilingual (English-Romanian) multimodal (text and image) dataset of multiple-choice questions derived from a high-level computer science competition. A particularity of our dataset is that the problems are conceived such that some of them are easier solved using reasoning on paper, while for others writing code is more efficient. We systematically evaluate State of The Art LLMs on this dataset, analyzing their performance on theoretical programming tasks. Our findings reveal the strengths and limitations of current LLMs, including the influence of language choice (English vs. Romanian), providing insights into their applicability in CS education and competition settings. We also address critical ethical considerations surrounding educational integrity and the fairness of assessments in the context of LLM usage. These discussions aim to inform future educational practices and policies. To support further research, our dataset will be made publicly available in both English and Romanian. Additionally, we release an educational application tailored for Romanian students, enabling them to self-assess using the dataset in an interactive and practice-oriented environment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages (9 paper, 2 references, 3 annexes). Accepted for BEA 2025!",
    "pdf_url": "https://arxiv.org/pdf/2507.03162v1",
    "published_date": "2025-07-03 20:43:28 UTC",
    "updated_date": "2025-07-03 20:43:28 UTC"
  },
  {
    "arxiv_id": "2507.14151v1",
    "title": "Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models",
    "authors": [
      "Giuliana Monachino",
      "Nicolò La Porta",
      "Beatrice Zanchi",
      "Luigi Fiorillo",
      "Alvise Dei Rossi",
      "Georgiy Farina",
      "Francesca Dalia Faraci"
    ],
    "abstract": "Foundation Models (FMs) are large-scale machine learning models trained on extensive, diverse datasets that can be adapted to a wide range of downstream tasks with minimal fine-tuning. In the last two years, interest in FMs has also grown for applications in the cardiological field to analyze the electrocardiogram (ECG) signals. One of the key properties of FMs is their transferability to a wide range of downstream scenarios. With the spread of wearable and portable devices, keen interest in learning from reduced-channel configurations has arisen. However, the adaptation of ECG FMs to downstream scenarios with fewer available channels still has to be properly investigated. In this work, we propose Self-DANA, a novel, easy-to-integrate solution that makes self-supervised architectures adaptable to a reduced number of input channels, ensuring resource efficiency and high performance. We also introduce Random Lead Selection, a novel augmentation technique to pre-train models in a more robust and channel-agnostic way. Our experimental results on five reduced-channel configurations demonstrate that Self-DANA significantly enhances resource efficiency while reaching state-of-the-art performance. It requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about 17% less average epoch CPU time, and about 24% less average epoch GPU time.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.14151v1",
    "published_date": "2025-07-03 20:39:30 UTC",
    "updated_date": "2025-07-03 20:39:30 UTC"
  },
  {
    "arxiv_id": "2507.03156v1",
    "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review",
    "authors": [
      "Amr Mohamed",
      "Maram Assi",
      "Mariam Guizani"
    ],
    "abstract": "Large language model assistants (LLM-assistants) present new opportunities to transform software development. Developers are increasingly adopting these tools across tasks, including coding, testing, debugging, documentation, and design. Yet, despite growing interest, there is no synthesis of how LLM-assistants affect software developer productivity. In this paper, we present a systematic literature review of 37 peer-reviewed studies published between January 2014 and December 2024 that examine this impact. Our analysis reveals that LLM-assistants offer both considerable benefits and critical risks. Commonly reported gains include minimized code search, accelerated development, and the automation of trivial and repetitive tasks. However, studies also highlight concerns around cognitive offloading, reduced team collaboration, and inconsistent effects on code quality. While the majority of studies (92%) adopt a multi-dimensional perspective by examining at least two SPACE dimensions, reflecting increased awareness of the complexity of developer productivity, only 14% extend beyond three dimensions, indicating substantial room for more integrated evaluations. Satisfaction, Performance, and Efficiency are the most frequently investigated dimensions, whereas Communication and Activity remain underexplored. Most studies are exploratory (64%) and methodologically diverse, but lack longitudinal and team-based evaluations. This review surfaces key research gaps and provides recommendations for future research and practice. All artifacts associated with this study are publicly available at https://zenodo.org/records/15788502.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "37 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03156v1",
    "published_date": "2025-07-03 20:25:49 UTC",
    "updated_date": "2025-07-03 20:25:49 UTC"
  },
  {
    "arxiv_id": "2507.03152v4",
    "title": "MedVAL: Toward Expert-Level Medical Text Validation with Language Models",
    "authors": [
      "Asad Aali",
      "Vasiliki Bikia",
      "Maya Varma",
      "Nicole Chiou",
      "Sophie Ostmeier",
      "Arnav Singhvi",
      "Magdalini Paschali",
      "Ashwin Kumar",
      "Andrew Johnston",
      "Karimar Amador-Martinez",
      "Eduardo Juan Perez Guerrero",
      "Paola Naovi Cruz Rivera",
      "Sergios Gatidis",
      "Christian Bluethgen",
      "Eduardo Pontes Reis",
      "Eddy D. Zandee van Rilland",
      "Poonam Laxmappa Hosamani",
      "Kevin R Keet",
      "Minjoung Go",
      "Evelyn Ling",
      "David B. Larson",
      "Curtis Langlotz",
      "Roxana Daneshjou",
      "Jason Hom",
      "Sanmi Koyejo",
      "Emily Alsentzer",
      "Akshay S. Chaudhari"
    ],
    "abstract": "With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a novel, self-supervised, data-efficient distillation method that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset of 840 physician-annotated outputs across 6 diverse medical tasks capturing real-world challenges. Across 10 state-of-the-art LMs spanning open-source and proprietary models, MedVAL distillation significantly improves (p < 0.001) alignment with physicians across seen and unseen tasks, increasing average F1 scores from 66% to 83%. Despite strong baseline performance, MedVAL improves the best-performing proprietary LM (GPT-4o) by 8% without training on physician-labeled data, demonstrating a performance statistically non-inferior to a single human expert (p < 0.001). To support a scalable, risk-aware pathway towards clinical integration, we open-source: 1) Codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B). Our benchmark provides evidence of LMs approaching expert-level ability in validating AI-generated medical text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03152v4",
    "published_date": "2025-07-03 20:19:18 UTC",
    "updated_date": "2025-09-18 04:11:49 UTC"
  },
  {
    "arxiv_id": "2507.03149v1",
    "title": "On the Relationship between Accent Strength and Articulatory Features",
    "authors": [
      "Kevin Huang",
      "Sean Foley",
      "Jihwan Lee",
      "Yoonjeong Lee",
      "Dani Byrd",
      "Shrikanth Narayanan"
    ],
    "abstract": "This paper explores the relationship between accent strength and articulatory features inferred from acoustic speech. To quantify accent strength, we compare phonetic transcriptions with transcriptions based on dictionary-based references, computing phoneme-level difference as a measure of accent strength. The proposed framework leverages recent self-supervised learning articulatory inversion techniques to estimate articulatory features. Analyzing a corpus of read speech from American and British English speakers, this study examines correlations between derived articulatory parameters and accent strength proxies, associating systematic articulatory differences with indexed accent strength. Results indicate that tongue positioning patterns distinguish the two dialects, with notable differences inter-dialects in rhotic and low back vowels. These findings contribute to automated accent analysis and articulatory modeling for speech processing applications.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for Interspeech2025",
    "pdf_url": "https://arxiv.org/pdf/2507.03149v1",
    "published_date": "2025-07-03 20:08:28 UTC",
    "updated_date": "2025-07-03 20:08:28 UTC"
  },
  {
    "arxiv_id": "2507.03120v1",
    "title": "How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models",
    "authors": [
      "Dharshan Kumaran",
      "Stephen M Fleming",
      "Larisa Markeeva",
      "Joe Heyward",
      "Andrea Banino",
      "Mrinal Mathur",
      "Razvan Pascanu",
      "Simon Osindero",
      "Benedetto de Martino",
      "Petar Velickovic",
      "Viorica Patraucean"
    ],
    "abstract": "Large language models (LLMs) exhibit strikingly conflicting behaviors: they can appear steadfastly overconfident in their initial answers whilst at the same time being prone to excessive doubt when challenged. To investigate this apparent paradox, we developed a novel experimental paradigm, exploiting the unique ability to obtain confidence estimates from LLMs without creating memory of their initial judgments -- something impossible in human participants. We show that LLMs -- Gemma 3, GPT4o and o1-preview -- exhibit a pronounced choice-supportive bias that reinforces and boosts their estimate of confidence in their answer, resulting in a marked resistance to change their mind. We further demonstrate that LLMs markedly overweight inconsistent compared to consistent advice, in a fashion that deviates qualitatively from normative Bayesian updating. Finally, we demonstrate that these two mechanisms -- a drive to maintain consistency with prior commitments and hypersensitivity to contradictory feedback -- parsimoniously capture LLM behavior in a different domain. Together, these findings furnish a mechanistic account of LLM confidence that explains both their stubbornness and excessive sensitivity to criticism.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "41 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03120v1",
    "published_date": "2025-07-03 18:57:43 UTC",
    "updated_date": "2025-07-03 18:57:43 UTC"
  },
  {
    "arxiv_id": "2507.03119v4",
    "title": "Neural-Network solver of ideal MHD equilibria",
    "authors": [
      "Timo Thun",
      "Andrea Merlo",
      "Rory Conlin",
      "Dario Panici",
      "Daniel Böckenhoff"
    ],
    "abstract": "We present a novel approach to compute three-dimensional Magnetohydrodynamic equilibria by parametrizing Fourier modes with artificial neural networks and compare it to equilibria computed by conventional solvers. The full nonlinear global force residual across the volume in real space is then minimized with first order optimizers. Already,we observe competitive computational cost to arrive at the same minimum residuals computed by existing codes. With increased computational cost,lower minima of the residual are achieved by the neural networks,establishing a new lower bound for the force residual. We use minimally complex neural networks,and we expect significant improvements for solving not only single equilibria with neural networks,but also for computing neural network models valid over continuous distributions of equilibria.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.plasm-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Nuclear Fusion, 16 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.03119v4",
    "published_date": "2025-07-03 18:56:37 UTC",
    "updated_date": "2025-09-25 22:11:31 UTC"
  },
  {
    "arxiv_id": "2507.03112v1",
    "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents",
    "authors": [
      "Peisong Wang",
      "Ruotian Ma",
      "Bang Zhang",
      "Xingyu Chen",
      "Zhiwei He",
      "Kang Luo",
      "Qingsong Lv",
      "Qingxuan Jiang",
      "Zheng Xie",
      "Shanyi Wang",
      "Yuan Li",
      "Fanghua Ye",
      "Jian Li",
      "Yifan Yang",
      "Zhaopeng Tu",
      "Xiaolong Li"
    ],
    "abstract": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM's learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/Tencent/DigitalHuman/tree/main/RLVER",
    "pdf_url": "https://arxiv.org/pdf/2507.03112v1",
    "published_date": "2025-07-03 18:33:18 UTC",
    "updated_date": "2025-07-03 18:33:18 UTC"
  },
  {
    "arxiv_id": "2507.03095v1",
    "title": "Uncovering Synergistic Educational Injustices of COVID-19 and AI",
    "authors": [
      "Ahmad Banyasady"
    ],
    "abstract": "Grounded in critical realism and using narrative inquiry, this article explores this article explores the long-term consequences of the COVID-19 pandemic and the rapid proliferation of artificial intelligence within higher education. Through the analysis of student narratives collected in Iranian university settings, the study reveals that learning experiences during and after the pandemic, coupled with unprepared exposure to AI tools, have generated hidden yet impactful layers of educational inequality and cognitive disorientation.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "15",
    "pdf_url": "https://arxiv.org/pdf/2507.03095v1",
    "published_date": "2025-07-03 18:07:50 UTC",
    "updated_date": "2025-07-03 18:07:50 UTC"
  },
  {
    "arxiv_id": "2507.02863v2",
    "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory",
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code: https://github.com/YkiWu/Point3R.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/YkiWu/Point3R",
    "pdf_url": "https://arxiv.org/pdf/2507.02863v2",
    "published_date": "2025-07-03 17:59:56 UTC",
    "updated_date": "2025-11-28 08:03:39 UTC"
  },
  {
    "arxiv_id": "2507.02861v1",
    "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans",
    "authors": [
      "Zhening Huang",
      "Xiaoyang Wu",
      "Fangcheng Zhong",
      "Hengshuang Zhao",
      "Matthias Nießner",
      "Joan Lasenby"
    ],
    "abstract": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c&feature=youtu.be",
    "pdf_url": "https://arxiv.org/pdf/2507.02861v1",
    "published_date": "2025-07-03 17:59:55 UTC",
    "updated_date": "2025-07-03 17:59:55 UTC"
  },
  {
    "arxiv_id": "2507.03069v3",
    "title": "ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization",
    "authors": [
      "YuXuan Zhang"
    ],
    "abstract": "Current RLHF methods such as PPO and DPO typically reduce human preferences to binary labels, which are costly to obtain and too coarse to reflect individual variation. We observe that expressions of satisfaction and dissatisfaction follow stable linguistic patterns across users, indicating that more informative supervisory signals can be extracted from free-form feedback. Building on this insight, we introduce Adaptive Reward-Following (ARF), which converts natural feedback into continuous preference trajectories and optimizes them using the novel TraceBias algorithm. Across diverse LLMs and preference domains, ARF consistently outperforms PPO and DPO, improving alignment by up to 7.6%. Our results demonstrate that continuous reward modeling provides a scalable path toward personalized and theoretically grounded RLHF.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This version fixes some minor typographical errors and adds more explanations to ensure clarity in presentation",
    "pdf_url": "https://arxiv.org/pdf/2507.03069v3",
    "published_date": "2025-07-03 17:59:26 UTC",
    "updated_date": "2025-10-25 05:45:15 UTC"
  },
  {
    "arxiv_id": "2507.02856v1",
    "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation",
    "authors": [
      "Nikhil Chandak",
      "Shashwat Goel",
      "Ameya Prabhu",
      "Moritz Hardt",
      "Jonas Geiping"
    ],
    "abstract": "Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, Code is available at https://github.com/nikhilchandak/answer-matching",
    "pdf_url": "https://arxiv.org/pdf/2507.02856v1",
    "published_date": "2025-07-03 17:59:02 UTC",
    "updated_date": "2025-07-03 17:59:02 UTC"
  },
  {
    "arxiv_id": "2507.02855v1",
    "title": "Subtyping in DHOL -- Extended preprint",
    "authors": [
      "Colin Rothgang",
      "Florian Rabe"
    ],
    "abstract": "The recently introduced dependent typed higher-order logic (DHOL) offers an interesting compromise between expressiveness and automation support. It sacrifices the decidability of its type system in order to significantly extend its expressiveness over standard HOL. Yet it retains strong automated theorem proving support via a sound and complete translation to HOL.\n  We leverage this design to extend DHOL with refinement and quotient types. Both of these are commonly requested by practitioners but rarely provided by automated theorem provers. This is because they inherently require undecidable typing and thus are very difficult to retrofit to decidable type systems. But with DHOL already doing the heavy lifting, adding them is not only possible but elegant and simple.\n  Concretely, we add refinement and quotient types as special cases of subtyping. This turns the associated canonical inclusion resp. projection maps into identity maps and thus avoids costly changes in representation. We present the syntax, semantics, and translation to HOL for the extended language, including the proofs of soundness and completeness.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LO",
    "comment": "16 pages main document, 44 pages of appendices, to be published in FroCoS 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02855v1",
    "published_date": "2025-07-03 17:59:00 UTC",
    "updated_date": "2025-07-03 17:59:00 UTC"
  },
  {
    "arxiv_id": "2507.02851v1",
    "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
    "authors": [
      "Purbesh Mitra",
      "Sennur Ulukus"
    ],
    "abstract": "Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02851v1",
    "published_date": "2025-07-03 17:55:43 UTC",
    "updated_date": "2025-07-03 17:55:43 UTC"
  },
  {
    "arxiv_id": "2507.02841v1",
    "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason",
    "authors": [
      "Kaiyi Zhang",
      "Ang Lv",
      "Jinpeng Li",
      "Yongbo Wang",
      "Feng Wang",
      "Haoyuan Hu",
      "Rui Yan"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02841v1",
    "published_date": "2025-07-03 17:51:06 UTC",
    "updated_date": "2025-07-03 17:51:06 UTC"
  },
  {
    "arxiv_id": "2507.02827v2",
    "title": "USAD: End-to-End Human Activity Recognition via Diffusion Model with Spatiotemporal Attention",
    "authors": [
      "Hang Xiao",
      "Ying Yu",
      "Jiarui Li",
      "Zhifan Yang",
      "Haotian Tang",
      "Hanyu Liu",
      "Chao Li"
    ],
    "abstract": "The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02827v2",
    "published_date": "2025-07-03 17:38:44 UTC",
    "updated_date": "2025-07-11 15:13:39 UTC"
  },
  {
    "arxiv_id": "2507.02825v5",
    "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
    "authors": [
      "Yuxuan Zhu",
      "Tengjun Jin",
      "Yada Pruksachatkun",
      "Andy Zhang",
      "Shu Liu",
      "Sasha Cui",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Kevin Meng",
      "Rebecca Weiss",
      "Fazl Barez",
      "Rahul Gupta",
      "Jwala Dhamala",
      "Jacob Merizian",
      "Mario Giulianelli",
      "Harry Coppock",
      "Cozmin Ududec",
      "Jasjeet Sekhon",
      "Jacob Steinhardt",
      "Antony Kellermann",
      "Sarah Schwettmann",
      "Matei Zaharia",
      "Ion Stoica",
      "Percy Liang",
      "Daniel Kang"
    ],
    "abstract": "Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues in task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation of agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 15 tables, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02825v5",
    "published_date": "2025-07-03 17:35:31 UTC",
    "updated_date": "2025-08-07 06:58:08 UTC"
  },
  {
    "arxiv_id": "2507.02824v3",
    "title": "DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift",
    "authors": [
      "Po-Heng Chou",
      "Ching-Wen Chen",
      "Wan-Jen Huang",
      "Walid Saad",
      "Yu Tsao",
      "Ronald Y. Chang"
    ],
    "abstract": "In this paper, the precoding design is investigated for maximizing the throughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO) systems with obstructed direct communication paths. In particular, a reconfigurable intelligent surface (RIS) is employed to enhance MIMO transmissions, considering mmWave characteristics related to line-of-sight (LoS) and multipath effects. The traditional exhaustive search (ES) for optimal codewords in the continuous phase shift is computationally intensive and time-consuming. To reduce computational complexity, permuted discrete Fourier transform (DFT) vectors are used for finding codebook design, incorporating amplitude responses for practical or ideal RIS systems. However, even if the discrete phase shift is adopted in the ES, it results in significant computation and is time-consuming. Instead, the trained deep neural network (DNN) is developed to facilitate faster codeword selection. Simulation results show that the DNN maintains sub-optimal spectral efficiency even as the distance between the end-user and the RIS has variations in the testing phase. These results highlight the potential of DNN in advancing RIS-aided systems.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "5 pages, 4 figures, 2 tables, and published in 2024 IEEE Globecom Workshops",
    "pdf_url": "https://arxiv.org/pdf/2507.02824v3",
    "published_date": "2025-07-03 17:35:06 UTC",
    "updated_date": "2025-09-30 02:52:08 UTC"
  },
  {
    "arxiv_id": "2507.02822v1",
    "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model",
    "authors": [
      "Wencheng Zhang",
      "Shiqin Qiao",
      "Lingjie Luo",
      "Yinfeng Li",
      "Chuanyang Zheng",
      "Qian Xu",
      "Meng Li",
      "Yong Gui",
      "Yijun He",
      "Jianing Qiu",
      "Jindong Hong",
      "Jiankai Sun"
    ],
    "abstract": "With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between \"thinking\" (high reasoning) and \"non-thinking\" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02822v1",
    "published_date": "2025-07-03 17:33:58 UTC",
    "updated_date": "2025-07-03 17:33:58 UTC"
  },
  {
    "arxiv_id": "2507.03067v1",
    "title": "Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case",
    "authors": [
      "Alvaro Riquelme",
      "Pedro Costa",
      "Catalina Martinez"
    ],
    "abstract": "For years, semantic interoperability standards have sought to streamline the exchange of clinical data, yet their deployment remains time-consuming, resource-intensive, and technically challenging. To address this, we introduce a semi-automated approach that leverages large language models specifically GPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR format while assessing accuracy, reliability, and security. Applying our method to the MIMIC-IV database, we combined embedding techniques, clustering algorithms, and semantic retrieval to craft prompts that guide the models in mapping each tabular field to its corresponding FHIR resource. In an initial benchmark, resource identification achieved a perfect F1-score, with GPT-4o outperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within the prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but refinements to the prompting strategy restored robust mappings. Error analysis revealed occasional hallucinations of non-existent attributes and mismatches in granularity, which more detailed prompts can mitigate. Overall, our study demonstrates the feasibility of context-aware, LLM-driven transformation of clinical data into HL7 FHIR, laying the groundwork for semi-automated interoperability workflows. Future work will focus on fine-tuning models with specialized medical corpora, extending support to additional standards such as HL7 CDA and OMOP, and developing an interactive interface to enable expert validation and iterative refinement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.03067v1",
    "published_date": "2025-07-03 17:32:57 UTC",
    "updated_date": "2025-07-03 17:32:57 UTC"
  },
  {
    "arxiv_id": "2507.03066v1",
    "title": "Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)",
    "authors": [
      "Sudesh Bhagat",
      "Ibne Farabi Shihab",
      "Jonathan Wood"
    ],
    "abstract": "This research investigates the efficacy of machine learning (ML) and deep learning (DL) methods in detecting misclassified intersection-related crashes in police-reported narratives. Using 2019 crash data from the Iowa Department of Transportation, we implemented and compared a comprehensive set of models, including Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT Word Embeddings, and Albert Model. Model performance was systematically validated against expert reviews of potentially misclassified narratives, providing a rigorous assessment of classification accuracy. Results demonstrated that while traditional ML methods exhibited superior overall performance compared to some DL approaches, the Albert Model achieved the highest agreement with expert classifications (73% with Expert 1) and original tabular data (58%). Statistical analysis revealed that the Albert Model maintained performance levels similar to inter-expert consistency rates, significantly outperforming other approaches, particularly on ambiguous narratives. This work addresses a critical gap in transportation safety research through multi-modal integration analysis, which achieved a 54.2% reduction in error rates by combining narrative text with structured crash data. We conclude that hybrid approaches combining automated classification with targeted expert review offer a practical methodology for improving crash data quality, with substantial implications for transportation safety management and policy development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03066v1",
    "published_date": "2025-07-03 17:26:27 UTC",
    "updated_date": "2025-07-03 17:26:27 UTC"
  },
  {
    "arxiv_id": "2507.03064v1",
    "title": "LLM-Driven Auto Configuration for Transient IoT Device Collaboration",
    "authors": [
      "Hetvi Shastri",
      "Walid A. Hanafy",
      "Li Wu",
      "David Irwin",
      "Mani Srivastava",
      "Prashant Shenoy"
    ],
    "abstract": "Today's Internet of Things (IoT) has evolved from simple sensing and actuation devices to those with embedded processing and intelligent services, enabling rich collaborations between users and their devices. However, enabling such collaboration becomes challenging when transient devices need to interact with host devices in temporarily visited environments. In such cases, fine-grained access control policies are necessary to ensure secure interactions; however, manually implementing them is often impractical for non-expert users. Moreover, at run-time, the system must automatically configure the devices and enforce such fine-grained access control rules. Additionally, the system must address the heterogeneity of devices.\n  In this paper, we present CollabIoT, a system that enables secure and seamless device collaboration in transient IoT environments. CollabIoT employs a Large language Model (LLM)-driven approach to convert users' high-level intents to fine-grained access control policies. To support secure and seamless device collaboration, CollabIoT adopts capability-based access control for authorization and uses lightweight proxies for policy enforcement, providing hardware-independent abstractions.\n  We implement a prototype of CollabIoT's policy generation and auto configuration pipelines and evaluate its efficacy on an IoT testbed and in large-scale emulated environments. We show that our LLM-based policy generation pipeline is able to generate functional and correct policies with 100% accuracy. At runtime, our evaluation shows that our system configures new devices in ~150 ms, and our proxy-based data plane incurs network overheads of up to 2 ms and access control overheads up to 0.3 ms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03064v1",
    "published_date": "2025-07-03 17:12:52 UTC",
    "updated_date": "2025-07-03 17:12:52 UTC"
  },
  {
    "arxiv_id": "2507.02788v1",
    "title": "Moral Responsibility or Obedience: What Do We Want from AI?",
    "authors": [
      "Joseph Boland"
    ],
    "abstract": "As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02788v1",
    "published_date": "2025-07-03 16:53:01 UTC",
    "updated_date": "2025-07-03 16:53:01 UTC"
  },
  {
    "arxiv_id": "2507.02778v2",
    "title": "Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models",
    "authors": [
      "Ken Tsui"
    ],
    "abstract": "Although large language models (LLMs) have transformed AI, they still make mistakes and can explore unproductive reasoning paths. Self-correction capability is essential for deploying LLMs in safety-critical applications. We uncover a systematic failure: LLMs cannot correct errors in their own outputs while successfully correcting identical errors from external sources - a limitation we term the Self-Correction Blind Spot. To study this phenomenon, we introduce Self-Correction Bench, an evaluation framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 open-source non-reasoning models, we find an average 64.5% blind spot rate. We provide multiple lines of evidence suggesting this limitation may be influenced by training data: human demonstrations rarely include error-correction sequences (favoring error-free responses), whereas reinforcement learning (RL) trained models learn error correction via outcome feedback. Remarkably, appending a minimal \"Wait\" prompt activates a 89.3% reduction in blind spots, suggesting dormant capabilities that require triggering. Our work highlights a critical limitation potentially influenced by training distribution and offers a practical approach to enhance LLM reliability and trustworthiness - vital for safety-critical domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02778v2",
    "published_date": "2025-07-03 16:41:30 UTC",
    "updated_date": "2025-10-04 08:57:59 UTC"
  },
  {
    "arxiv_id": "2507.03062v1",
    "title": "BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data",
    "authors": [
      "Hao Yang",
      "Angela Yao",
      "Christopher Whalen",
      "Gengchen Mai"
    ],
    "abstract": "Understanding human mobility is essential for applications in public health, transportation, and urban planning. However, mobility data often suffers from sparsity due to limitations in data collection methods, such as infrequent GPS sampling or call detail record (CDR) data that only capture locations during communication events. To address this challenge, we propose BERT4Traj, a transformer based model that reconstructs complete mobility trajectories by predicting hidden visits in sparse movement sequences. Inspired by BERT's masked language modeling objective and self_attention mechanisms, BERT4Traj leverages spatial embeddings, temporal embeddings, and contextual background features such as demographics and anchor points. We evaluate BERT4Traj on real world CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our approach significantly outperforms traditional models such as Markov Chains, KNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs detailed and continuous mobility trajectories, enhancing insights into human movement patterns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper was accepted at GIScience 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.03062v1",
    "published_date": "2025-07-03 16:39:17 UTC",
    "updated_date": "2025-07-03 16:39:17 UTC"
  },
  {
    "arxiv_id": "2507.02773v2",
    "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs",
    "authors": [
      "Yuzhang Xie",
      "Hejie Cui",
      "Ziyang Zhang",
      "Jiaying Lu",
      "Kai Shu",
      "Fadi Nahab",
      "Xiao Hu",
      "Carl Yang"
    ],
    "abstract": "Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02773v2",
    "published_date": "2025-07-03 16:35:11 UTC",
    "updated_date": "2025-07-06 14:02:34 UTC"
  },
  {
    "arxiv_id": "2507.02771v1",
    "title": "Grounding Intelligence in Movement",
    "authors": [
      "Melanie Segado",
      "Felipe Parodi",
      "Jordan K. Matelsky",
      "Michael L. Platt",
      "Eva B. Dyer",
      "Konrad P. Kording"
    ],
    "abstract": "Recent advances in machine learning have dramatically improved our ability to model language, vision, and other high-dimensional data, yet they continue to struggle with one of the most fundamental aspects of biological systems: movement. Across neuroscience, medicine, robotics, and ethology, movement is essential for interpreting behavior, predicting intent, and enabling interaction. Despite its core significance in our intelligence, movement is often treated as an afterthought rather than as a rich and structured modality in its own right. This reflects a deeper fragmentation in how movement data is collected and modeled, often constrained by task-specific goals and domain-specific assumptions. But movement is not domain-bound. It reflects shared physical constraints, conserved morphological structures, and purposeful dynamics that cut across species and settings. We argue that movement should be treated as a primary modeling target for AI. It is inherently structured and grounded in embodiment and physics. This structure, often allowing for compact, lower-dimensional representations (e.g., pose), makes it more interpretable and computationally tractable to model than raw, high-dimensional sensory inputs. Developing models that can learn from and generalize across diverse movement data will not only advance core capabilities in generative modeling and control, but also create a shared foundation for understanding behavior across biological and artificial systems. Movement is not just an outcome, it is a window into how intelligent systems engage with the world.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02771v1",
    "published_date": "2025-07-03 16:34:34 UTC",
    "updated_date": "2025-07-03 16:34:34 UTC"
  },
  {
    "arxiv_id": "2507.02760v1",
    "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work",
    "authors": [
      "Guangwei Zhang"
    ],
    "abstract": "The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02760v1",
    "published_date": "2025-07-03 16:21:14 UTC",
    "updated_date": "2025-07-03 16:21:14 UTC"
  },
  {
    "arxiv_id": "2507.03059v1",
    "title": "AI-Based Reconstruction from Inherited Personal Data: Analysis, Feasibility, and Prospects",
    "authors": [
      "Mark Zilberman"
    ],
    "abstract": "This article explores the feasibility of creating an \"electronic copy\" of a deceased researcher by training artificial intelligence (AI) on the data stored in their personal computers. By analyzing typical data volumes on inherited researcher computers, including textual files such as articles, emails, and drafts, it is estimated that approximately one million words are available for AI training. This volume is sufficient for fine-tuning advanced pre-trained models like GPT-4 to replicate a researcher's writing style, domain expertise, and rhetorical voice with high fidelity. The study also discusses the potential enhancements from including non-textual data and file metadata to enrich the AI's representation of the researcher. Extensions of the concept include communication between living researchers and their electronic copies, collaboration among individual electronic copies, as well as the creation and interconnection of organizational electronic copies to optimize information access and strategic decision-making. Ethical considerations such as ownership and security of these electronic copies are highlighted as critical for responsible implementation. The findings suggest promising opportunities for AI-driven preservation and augmentation of intellectual legacy.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03059v1",
    "published_date": "2025-07-03 16:19:15 UTC",
    "updated_date": "2025-07-03 16:19:15 UTC"
  },
  {
    "arxiv_id": "2507.02755v3",
    "title": "Multi-agent Auditory Scene Analysis",
    "authors": [
      "Caleb Rascon",
      "Luis Gato-Diaz",
      "Eduardo García-Alarcón"
    ],
    "abstract": "Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a publicly available framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to Applied Soft Computing",
    "pdf_url": "https://arxiv.org/pdf/2507.02755v3",
    "published_date": "2025-07-03 16:16:46 UTC",
    "updated_date": "2025-08-20 14:18:03 UTC"
  },
  {
    "arxiv_id": "2507.02754v1",
    "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "authors": [
      "Aurko Roy",
      "Timothy Chou",
      "Sai Surya Duvvuri",
      "Sijia Chen",
      "Jiecao Yu",
      "Xiaodong Wang",
      "Manzil Zaheer",
      "Rohan Anil"
    ],
    "abstract": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, with appendix 25 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.02754v1",
    "published_date": "2025-07-03 16:16:34 UTC",
    "updated_date": "2025-07-03 16:16:34 UTC"
  },
  {
    "arxiv_id": "2507.02752v2",
    "title": "SynTwins: A Retrosynthesis-Guided Framework for Synthesizable Molecular Analog Generation",
    "authors": [
      "Shuan Chen",
      "Gunwook Nam",
      "Alan Aspuru-Guzik",
      "Yousung Jung"
    ],
    "abstract": "The disconnect between AI-generated molecules with desirable properties and their synthetic feasibility remains a critical bottleneck in computational discovery of drugs and materials. While generative AI has accelerated the proposal of candidate molecules, many of these structures prove challenging or impossible to synthesize using established chemical reactions. Here, we introduce SynTwins, a novel retrosynthesis-guided molecule design framework that finds synthetically accessible molecular analogs by emulating expert chemists' strategies in three steps: retrosynthesis, searching similar building blocks, and virtual synthesis. Using a search algorithm instead of a stochastic data-driven generator, SynTwins outperforms state-of-the-art machine learning models at exploring synthetically accessible analogs while maintaining high structural similarity to original target molecules. Furthermore, when integrated into existing molecular property-optimization frameworks, our hybrid approach produces synthetically feasible analogs with minimal loss in property scores. Our comprehensive benchmarking across diverse molecular datasets demonstrates that SynTwins effectively bridges the gap between computational design and experimental synthesis, providing a practical solution for accelerating the discovery of synthesizable molecules with desired properties for a wide range of applications.",
    "categories": [
      "physics.chem-ph",
      "cs.AI"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02752v2",
    "published_date": "2025-07-03 16:14:57 UTC",
    "updated_date": "2025-11-22 14:18:03 UTC"
  },
  {
    "arxiv_id": "2507.02748v1",
    "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics",
    "authors": [
      "Alex Colagrande",
      "Paul Caillon",
      "Eva Feillet",
      "Alexandre Allauzen"
    ],
    "abstract": "Transformers have become the de facto standard for a wide range of tasks, from image classification to physics simulations. Despite their impressive performance, the quadratic complexity of standard Transformers in both memory and time with respect to the input length makes them impractical for processing high-resolution inputs. Therefore, several variants have been proposed, the most successful relying on patchification, downsampling, or coarsening techniques, often at the cost of losing the finest-scale details. In this work, we take a different approach. Inspired by state-of-the-art techniques in $n$-body numerical simulations, we cast attention as an interaction problem between grid points. We introduce the Multipole Attention Neural Operator (MANO), which computes attention in a distance-based multiscale fashion. MANO maintains, in each attention head, a global receptive field and achieves linear time and memory complexity with respect to the number of grid points. Empirical results on image classification and Darcy flows demonstrate that MANO rivals state-of-the-art models such as ViT and Swin Transformer, while reducing runtime and peak memory usage by orders of magnitude. We open source our code for reproducibility at https://github.com/AlexColagrande/MANO.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECLR Workshop at ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02748v1",
    "published_date": "2025-07-03 16:05:26 UTC",
    "updated_date": "2025-07-03 16:05:26 UTC"
  },
  {
    "arxiv_id": "2507.02737v2",
    "title": "Early Signs of Steganographic Capabilities in Frontier LLMs",
    "authors": [
      "Artur Zolkowski",
      "Kei Nishimura-Gasparian",
      "Robert McCarthy",
      "Roland S. Zimmermann",
      "David Lindner"
    ],
    "abstract": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances like using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02737v2",
    "published_date": "2025-07-03 15:54:55 UTC",
    "updated_date": "2025-10-14 18:36:42 UTC"
  },
  {
    "arxiv_id": "2507.02735v2",
    "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
    "authors": [
      "Sizhe Chen",
      "Arman Zharmagambetov",
      "David Wagner",
      "Chuan Guo"
    ],
    "abstract": "Prompt injection attack has been listed as the top-1 security threat to LLM-integrated applications, which interact with external environment data for complex tasks. The untrusted data may contain an injected prompt trying to arbitrarily manipulate the system. Model-level prompt injection defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source secure models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigating prompt injection attacks. To this end, we develop Meta SecAlign, the first fully open-source LLM with built-in model-level defense that achieves commercial-grade performance, powerful enough for complex agentic tasks. We provide complete details of our training recipe, an improved version of the SOTA SecAlign defense. We perform the most comprehensive evaluation to date on 9 utility benchmarks and 7 security benchmarks on general knowledge, instruction following, and agentic workflows. Results show that Meta SecAlign, despite being trained on generic instruction-tuning samples only, surprisingly confers security in unseen downstream tasks, including tool-calling and web-navigation, in addition to general instruction-following. Our best model -- Meta-SecAlign-70B -- establishes a new frontier of utility-security trade-off for open-source LLMs. Even compared to closed-course commercial models such as GPT-5, our model is much securer than most of them. Below are links for the code (https://github.com/facebookresearch/Meta_SecAlign), Meta-SecAlign-70B(https://huggingface.co/facebook/Meta-SecAlign-70B), and Meta-SecAlign-8B(https://huggingface.co/facebook/Meta-SecAlign-8B) models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02735v2",
    "published_date": "2025-07-03 15:47:13 UTC",
    "updated_date": "2025-11-10 16:30:10 UTC"
  },
  {
    "arxiv_id": "2507.02726v1",
    "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
    "authors": [
      "Matthieu Zimmer",
      "Xiaotong Ji",
      "Rasul Tutunov",
      "Anthony Bordg",
      "Jun Wang",
      "Haitham Bou Ammar"
    ],
    "abstract": "Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02726v1",
    "published_date": "2025-07-03 15:41:38 UTC",
    "updated_date": "2025-07-03 15:41:38 UTC"
  },
  {
    "arxiv_id": "2507.02714v1",
    "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models",
    "authors": [
      "Yuxuan Wang",
      "Tianwei Cao",
      "Huayu Zhang",
      "Zhongjiang He",
      "Kongming Liang",
      "Zhanyu Ma"
    ],
    "abstract": "Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02714v1",
    "published_date": "2025-07-03 15:27:45 UTC",
    "updated_date": "2025-07-03 15:27:45 UTC"
  },
  {
    "arxiv_id": "2507.05272v2",
    "title": "LLMs and Fuzzing in Tandem: A New Approach to Automatically Generating Weakest Preconditions",
    "authors": [
      "Daragh King",
      "Vasileios Koutavas",
      "Laura Kovacs"
    ],
    "abstract": "The weakest precondition (WP) of a program describes the largest set of initial states from which all terminating executions of the program satisfy a given postcondition. The generation of WPs is an important task with practical applications in areas ranging from verification to run-time error checking. This paper proposes the combination of Large Language Models (LLMs) and fuzz testing for generating WPs. In pursuit of this goal, we introduce \\emph{Fuzzing Guidance} (FG); FG acts as a means of directing LLMs towards correct WPs using program execution feedback. FG utilises fuzz testing for approximately checking the validity and weakness of candidate WPs, this information is then fed back to the LLM as a means of context refinement. We demonstrate the effectiveness of our approach on a comprehensive benchmark set of deterministic array programs in Java. Our experiments indicate that LLMs are capable of producing viable candidate WPs, and that this ability can be practically enhanced through FG.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.05272v2",
    "published_date": "2025-07-03 15:14:43 UTC",
    "updated_date": "2025-12-17 13:42:56 UTC"
  },
  {
    "arxiv_id": "2507.02703v1",
    "title": "Time-critical and confidence-based abstraction dropping methods",
    "authors": [
      "Robin Schmöcker",
      "Lennart Kampmann",
      "Alexander Dockhorn"
    ],
    "abstract": "One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and use state and/or action abstractions during the tree search. Non-exact abstractions, however, introduce an approximation error making convergence to the optimal action in the abstract space impossible. Hence, as proposed as a component of Elastic Monte Carlo Tree Search by Xu et al., abstraction algorithms should eventually drop the abstraction. In this paper, we propose two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can yield clear performance improvements whilst being safe in the sense that the dropping never causes any notable performance degradations contrary to Xu's dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD is designed to improve the MCTS performance with the same number of iterations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for Publication at the IEEE Conference on Games 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02703v1",
    "published_date": "2025-07-03 15:12:05 UTC",
    "updated_date": "2025-07-03 15:12:05 UTC"
  },
  {
    "arxiv_id": "2507.02687v1",
    "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data",
    "authors": [
      "JungWoo Chae",
      "Jiyoon Kim",
      "JaeWoong Choi",
      "Kyungyul Kim",
      "Sangheum Hwang"
    ],
    "abstract": "Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 camera ready. Project page: https://lgcnsai.github.io/apt",
    "pdf_url": "https://arxiv.org/pdf/2507.02687v1",
    "published_date": "2025-07-03 14:58:08 UTC",
    "updated_date": "2025-07-03 14:58:08 UTC"
  },
  {
    "arxiv_id": "2507.02681v2",
    "title": "Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education",
    "authors": [
      "Behnam Parsaeifard",
      "Christof Imhof",
      "Tansu Pancar",
      "Ioan-Sorin Comsa",
      "Martin Hlosta",
      "Nicole Bergamin",
      "Per Bergamin"
    ],
    "abstract": "Students disengaging from their tasks can have serious long-term consequences, including academic drop-out. This is particularly relevant for students in distance education. One way to measure the level of disengagement in distance education is to observe participation in non-mandatory exercises in different online courses. In this paper, we detect student disengagement in the non-mandatory quizzes of 42 courses in four semesters from a distance-based university. We carefully identified the most informative student log data that could be extracted and processed from Moodle. Then, eight machine learning algorithms were trained and compared to obtain the highest possible prediction accuracy. Using the SHAP method, we developed an explainable machine learning framework that allows practitioners to better understand the decisions of the trained algorithm. The experimental results show a balanced accuracy of 91\\%, where about 85\\% of disengaged students were correctly detected. On top of the highly predictive performance and explainable framework, we provide a discussion on how to design a timely intervention to minimise disengagement from voluntary tasks in online learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02681v2",
    "published_date": "2025-07-03 14:43:40 UTC",
    "updated_date": "2025-07-04 08:10:49 UTC"
  },
  {
    "arxiv_id": "2507.02666v1",
    "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning",
    "authors": [
      "Junyu Wang",
      "Tianrui Wang",
      "Meng Ge",
      "Longbiao Wang",
      "Jianwu Dang"
    ],
    "abstract": "In recent advancements in audio self-supervised representation learning, the standard Transformer architecture has emerged as the predominant approach, yet its attention mechanism often allocates a portion of attention weights to irrelevant information, potentially impairing the model's discriminative ability. To address this, we introduce a differential attention mechanism, which effectively mitigates ineffective attention allocation through the integration of dual-softmax operations and appropriately tuned differential coefficients. Experimental results demonstrate that our ASDA model achieves state-of-the-art (SOTA) performance across multiple benchmarks, including audio classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting (98.3% accuracy on SPC-2), and environmental sound classification (96.1% accuracy on ESC-50). These results highlight ASDA's effectiveness in audio tasks, paving the way for broader applications.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at Interspeech2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02666v1",
    "published_date": "2025-07-03 14:29:43 UTC",
    "updated_date": "2025-07-03 14:29:43 UTC"
  },
  {
    "arxiv_id": "2507.02663v3",
    "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models",
    "authors": [
      "Yongjiang Liu",
      "Haoxi Li",
      "Xiaosong Ma",
      "Jie Zhang",
      "Song Guo"
    ],
    "abstract": "Recent Large Reasoning Models (LRMs) excel at complex reasoning tasks but often suffer from overthinking, generating overly long and redundant reasoning trajectories. To explore its essence, our empirical analysis reveals that LRMs are primarily limited to recognizing task properties (i.e., difficulty levels) like humans before solving the problem, leading to a one-size-fits-all reasoning process. Inspired by this, a pressing and natural question emerges: Can we explicitly bootstrap such ability to alleviate overthinking in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning strategy that progressively inspires LRMs' difficulty cognition and redundancy cognition of LRMs. Specifically, we first inject difficulty hypnosis into output prefixes to guide the model toward adaptive reasoning depth, trained on a hybrid dataset mixing short and long reasoning paths. Then, we incorporate redundancy hypnosis, which supervises the intermediate reasoning steps to identify and eliminate unnecessary reasoning patterns. Experiments on 7B/14B/32B models demonstrate that TH2T significantly reduces inference costs by over 70% on easy tasks and 40% on hard tasks while maintaining performance stability. The resulting outputs exhibit clear signs of difficulty-aware capabilities and reduced redundancy (e.g., reflection and looping).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 18 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02663v3",
    "published_date": "2025-07-03 14:24:26 UTC",
    "updated_date": "2025-10-06 12:49:25 UTC"
  },
  {
    "arxiv_id": "2507.02660v1",
    "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification",
    "authors": [
      "Deepak Narayan Gadde",
      "Keerthan Kopparam Radhakrishna",
      "Vaisakh Naduvodi Viswambharan",
      "Aman Kumar",
      "Djones Lettnin",
      "Wolfgang Kunz",
      "Sebastian Simon"
    ],
    "abstract": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL",
    "pdf_url": "https://arxiv.org/pdf/2507.02660v1",
    "published_date": "2025-07-03 14:20:57 UTC",
    "updated_date": "2025-07-03 14:20:57 UTC"
  },
  {
    "arxiv_id": "2507.02652v2",
    "title": "HiRA: A Hierarchical Reasoning Framework for Decoupled Planning and Execution in Deep Search",
    "authors": [
      "Jiajie Jin",
      "Xiaoxi Li",
      "Guanting Dong",
      "Yuyao Zhang",
      "Yutao Zhu",
      "Yang Zhao",
      "Hongjin Qian",
      "Zhicheng Dou"
    ],
    "abstract": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.02652v2",
    "published_date": "2025-07-03 14:18:08 UTC",
    "updated_date": "2025-10-31 03:57:07 UTC"
  },
  {
    "arxiv_id": "2507.02644v2",
    "title": "Solving the Hubbard model with Neural Quantum States",
    "authors": [
      "Yuntian Gu",
      "Wenrui Li",
      "Heng Lin",
      "Bo Zhan",
      "Ruichen Li",
      "Yifei Huang",
      "Di He",
      "Yantao Wu",
      "Tao Xiang",
      "Mingpu Qin",
      "Liwei Wang",
      "Dingshun Lv"
    ],
    "abstract": "The rapid development of neural quantum states (NQS) has established it as a promising framework for studying quantum many-body systems. In this work, by leveraging the cutting-edge transformer-based architectures and developing highly efficient optimization algorithms, we achieve the state-of-the-art results for the doped two-dimensional (2D) Hubbard model, arguably the minimum model for high-Tc superconductivity. Interestingly, we find different attention heads in the NQS ansatz can directly encode correlations at different scales, making it capable of capturing long-range correlations and entanglements in strongly correlated systems. With these advances, we establish the half-filled stripe in the ground state of 2D Hubbard model with the next nearest neighboring hoppings, consistent with experimental observations in cuprates. Our work establishes NQS as a powerful tool for solving challenging many-fermions systems.",
    "categories": [
      "cond-mat.str-el",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02644v2",
    "published_date": "2025-07-03 14:08:25 UTC",
    "updated_date": "2025-07-10 14:46:55 UTC"
  },
  {
    "arxiv_id": "2507.02620v3",
    "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference",
    "authors": [
      "Xing Liu",
      "Lizhuo Luo",
      "Ming Tang",
      "Chao Huang",
      "Xu Chen"
    ],
    "abstract": "Distributed inference serves as a promising approach to enabling the inference of large language models (LLMs) at the network edge. It distributes the inference process to multiple devices to ensure that the LLMs can fit into the device memory. Recent pipeline-based approaches have the potential to parallelize communication and computation, which helps reduce inference latency. However, the benefit diminishes when the inference request at the network edge is sparse, where pipeline is typically at low utilization. To enable efficient distributed LLM inference at the edge, we propose \\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding framework. FlowSpec incorporates three key mechanisms to improve decoding efficiency: 1) score-based step-wise verification prioritizes more important draft tokens to bring earlier accepted tokens; 2) efficient draft management to prune invalid tokens while maintaining correct causal relationship during verification; 3) dynamic draft expansion strategies to supply high-quality speculative inputs. These techniques work in concert to enhance both pipeline utilization and speculative efficiency. We evaluate FlowSpec on a real-world testbed with other baselines. Experimental results demonstrate that our proposed framework significantly improves inference speed across diverse models and configurations, achieving speedup ratios 1.37$\\times$-1.73$\\times$ compared to baselines. Our code is publicly available at \\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "11 pages, and the last one is the appendix",
    "pdf_url": "https://arxiv.org/pdf/2507.02620v3",
    "published_date": "2025-07-03 13:47:42 UTC",
    "updated_date": "2026-01-10 13:43:14 UTC"
  },
  {
    "arxiv_id": "2507.02618v1",
    "title": "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory",
    "authors": [
      "Kenneth Payne",
      "Baptiste Alloui-Cros"
    ],
    "abstract": "Are Large Language Models (LLMs) a new form of strategic intelligence, able to reason about goals in competitive settings? We present compelling supporting evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for studying decision-making. We conduct the first ever series of evolutionary IPD tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger) against agents from the leading frontier AI companies OpenAI, Google, and Anthropic. By varying the termination probability in each tournament (the \"shadow of the future\"), we introduce complexity and chance, confounding memorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and sometimes even proliferating in these complex ecosystems. Furthermore, they exhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini models proved strategically ruthless, exploiting cooperative opponents and retaliating against defectors, while OpenAI's models remained highly cooperative, a trait that proved catastrophic in hostile environments. Anthropic's Claude emerged as the most forgiving reciprocator, showing remarkable willingness to restore cooperation even after being exploited or successfully defecting. Analysis of nearly 32,000 prose rationales provided by the models reveals that they actively reason about both the time horizon and their opponent's likely strategy, and we demonstrate that this reasoning is instrumental to their decisions. This work connects classic game theory with machine psychology, offering a rich and granular view of algorithmic decision-making under uncertainty.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 27 tables, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02618v1",
    "published_date": "2025-07-03 13:45:02 UTC",
    "updated_date": "2025-07-03 13:45:02 UTC"
  },
  {
    "arxiv_id": "2507.02616v1",
    "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making",
    "authors": [
      "Tianqi Shang",
      "Weiqing He",
      "Charles Zheng",
      "Lingyao Li",
      "Li Shen",
      "Bingxin Zhao"
    ],
    "abstract": "The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.02616v1",
    "published_date": "2025-07-03 13:43:10 UTC",
    "updated_date": "2025-07-03 13:43:10 UTC"
  },
  {
    "arxiv_id": "2507.02606v1",
    "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks",
    "authors": [
      "Wei Fan",
      "Kejiang Chen",
      "Chang Liu",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "abstract": "The rapid advancement of speech generation models has heightened privacy and security concerns related to voice cloning (VC). Recent studies have investigated disrupting unauthorized voice cloning by introducing adversarial perturbations. However, determined attackers can mitigate these protective perturbations and successfully execute VC. In this study, we conduct the first systematic evaluation of these protective perturbations against VC under realistic threat models that include perturbation purification. Our findings reveal that while existing purification methods can neutralize a considerable portion of the protective perturbations, they still lead to distortions in the feature space of VC models, which degrades the performance of VC. From this perspective, we propose a novel two-stage purification method: (1) Purify the perturbed speech; (2) Refine it using phoneme guidance to align it with the clean speech distribution. Experimental results demonstrate that our method outperforms state-of-the-art purification methods in disrupting VC defenses. Our study reveals the limitations of adversarial perturbation-based VC defenses and underscores the urgent need for more robust solutions to mitigate the security and privacy risks posed by VC. The code and audio samples are available at https://de-antifake.github.io.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02606v1",
    "published_date": "2025-07-03 13:30:58 UTC",
    "updated_date": "2025-07-03 13:30:58 UTC"
  },
  {
    "arxiv_id": "2507.03056v1",
    "title": "Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models",
    "authors": [
      "Behnam Parsaeifard",
      "Martin Hlosta",
      "Per Bergamin"
    ],
    "abstract": "With the rise of online learning, the demand for efficient and consistent assessment in mathematics has significantly increased over the past decade. Machine Learning (ML), particularly Natural Language Processing (NLP), has been widely used for autograding student responses, particularly those involving text and/or mathematical expressions. However, there has been limited research on autograding responses involving students' handwritten graphs, despite their prevalence in Science, Technology, Engineering, and Mathematics (STEM) curricula. In this study, we implement multimodal meta-learning models for autograding images containing students' handwritten graphs and text. We further compare the performance of Vision Large Language Models (VLLMs) with these specially trained metalearning models. Our results, evaluated on a real-world dataset collected from our institution, show that the best-performing meta-learning models outperform VLLMs in 2-way classification tasks. In contrast, in more complex 3-way classification tasks, the best-performing VLLMs slightly outperform the meta-learning models. While VLLMs show promising results, their reliability and practical applicability remain uncertain and require further investigation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03056v1",
    "published_date": "2025-07-03 13:25:50 UTC",
    "updated_date": "2025-07-03 13:25:50 UTC"
  },
  {
    "arxiv_id": "2507.02602v1",
    "title": "Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development",
    "authors": [
      "Riccardo Gallon",
      "Fabian Schiemenz",
      "Alessandra Menicucci",
      "Eberhard Gill"
    ],
    "abstract": "The increasing importance of Vision-Based Navigation (VBN) algorithms in space missions raises numerous challenges in ensuring their reliability and operational robustness. Sensor faults can lead to inaccurate outputs from navigation algorithms or even complete data processing faults, potentially compromising mission objectives. Artificial Intelligence (AI) offers a powerful solution for detecting such faults, overcoming many of the limitations associated with traditional fault detection methods. However, the primary obstacle to the adoption of AI in this context is the lack of sufficient and representative datasets containing faulty image data.\n  This study addresses these challenges by focusing on an interplanetary exploration mission scenario. A comprehensive analysis of potential fault cases in camera sensors used within the VBN pipeline is presented. The causes and effects of these faults are systematically characterized, including their impact on image quality and navigation algorithm performance, as well as commonly employed mitigation strategies. To support this analysis, a simulation framework is introduced to recreate faulty conditions in synthetically generated images, enabling a systematic and controlled reproduction of faulty data. The resulting dataset of fault-injected images provides a valuable tool for training and testing AI-based fault detection algorithms. The final link to the dataset will be added after an embargo period. For peer-reviewers, this private link is available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Acta Astronautica",
    "pdf_url": "https://arxiv.org/pdf/2507.02602v1",
    "published_date": "2025-07-03 13:23:22 UTC",
    "updated_date": "2025-07-03 13:23:22 UTC"
  },
  {
    "arxiv_id": "2507.02598v2",
    "title": "AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models",
    "authors": [
      "Chenhao Xue",
      "Kezhi Li",
      "Jiaxing Zhang",
      "Yi Ren",
      "Zhengyuan Shi",
      "Chen Zhang",
      "Yibo Lin",
      "Lining Zhang",
      "Qiang Xu",
      "Guangyu Sun"
    ],
    "abstract": "Arithmetic circuits, such as adders and multipliers, are fundamental components of digital systems, directly impacting the performance, power efficiency, and area footprint. However, optimizing these circuits remains challenging due to the vast design space and complex physical constraints. While recent deep learning-based approaches have shown promise, they struggle to consistently explore high-potential design variants, limiting their optimization efficiency. To address this challenge, we propose AC-Refiner, a novel arithmetic circuit optimization framework leveraging conditional diffusion models. Our key insight is to reframe arithmetic circuit synthesis as a conditional image generation task. By carefully conditioning the denoising diffusion process on target quality-of-results (QoRs), AC-Refiner consistently produces high-quality circuit designs. Furthermore, the explored designs are used to fine-tune the diffusion model, which focuses the exploration near the Pareto frontier. Experimental results demonstrate that AC-Refiner generates designs with superior Pareto optimality, outperforming state-of-the-art baselines. The performance gain is further validated by integrating AC-Refiner into practical applications.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "8 pages, 12 figures, to appear in ASP-DAC'26",
    "pdf_url": "https://arxiv.org/pdf/2507.02598v2",
    "published_date": "2025-07-03 13:21:33 UTC",
    "updated_date": "2025-09-16 12:23:53 UTC"
  },
  {
    "arxiv_id": "2507.07116v1",
    "title": "Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces",
    "authors": [
      "Juan Cano-Benito",
      "Andrea Cimmino",
      "Sven Hertling",
      "Heiko Paulheim",
      "Raúl García-Castro"
    ],
    "abstract": "Data spaces are emerging as decentralised infrastructures that enable sovereign, secure, and trustworthy data exchange among multiple participants. To achieve semantic interoperability within these environments, the use of semantic web technologies and knowledge graphs has been proposed. Although distributed ledger technologies (DLT) fit as the underlying infrastructure for data spaces, there remains a significant gap in terms of the efficient storage of semantic data on these platforms. This paper presents a systematic evaluation of semantic data storage across different types of DLT (public, private, and hybrid), using a real-world knowledge graph as an experimental basis. The study compares performance, storage efficiency, resource consumption, and the capabilities to update and query semantic data. The results show that private DLTs are the most efficient for storing and managing semantic content, while hybrid DLTs offer a balanced trade-off between public auditability and operational efficiency. This research leads to a discussion on the selection of the most appropriate DLT infrastructure based on the data sovereignty requirements of decentralised data ecosystems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.07116v1",
    "published_date": "2025-07-03 13:21:00 UTC",
    "updated_date": "2025-07-03 13:21:00 UTC"
  },
  {
    "arxiv_id": "2507.02595v1",
    "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion",
    "authors": [
      "Xin Guan",
      "PeiHsin Lin",
      "Zekun Wu",
      "Ze Wang",
      "Ruibo Zhang",
      "Emre Kazim",
      "Adriano Koshiyama"
    ],
    "abstract": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for large language models (LLMs) developed in response to the growing need for easy bias mitigation. Built on top of the SAGED pipeline, an automated system for constructing bias benchmarks and extracting interpretable baseline distributions, MPF leverages multiperspective generations to expose and align biases in LLM outputs with nuanced, humanlike baselines. By decomposing baseline, such as sentiment distributions from HR professionals, into interpretable perspective components, MPF guides generation through sampling and balancing of responses, weighted by the probabilities obtained in the decomposition. Empirically, we demonstrate its ability to align LLM sentiment distributions with both counterfactual baselines (absolute equality) and the HR baseline (biased for Top Univeristy), resulting in small KL divergence, reduction of calibration error and generalization to unseen questions. This shows that MPF offers a scalable and interpretable method for alignment and bias mitigation, compatible with deployed LLMs and requiring no extensive prompt engineering or finetuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICML 2025 AIW Workshop",
    "pdf_url": "https://arxiv.org/pdf/2507.02595v1",
    "published_date": "2025-07-03 13:09:18 UTC",
    "updated_date": "2025-07-03 13:09:18 UTC"
  },
  {
    "arxiv_id": "2507.02592v1",
    "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
    "authors": [
      "Kuan Li",
      "Zhongwang Zhang",
      "Huifeng Yin",
      "Liwen Zhang",
      "Litu Ou",
      "Jialong Wu",
      "Wenbiao Yin",
      "Baixuan Li",
      "Zhengwei Tao",
      "Xinyu Wang",
      "Weizhou Shen",
      "Junkai Zhang",
      "Dingchu Zhang",
      "Xixi Wu",
      "Yong Jiang",
      "Ming Yan",
      "Pengjun Xie",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "abstract": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02592v1",
    "published_date": "2025-07-03 12:59:07 UTC",
    "updated_date": "2025-07-03 12:59:07 UTC"
  },
  {
    "arxiv_id": "2507.03054v2",
    "title": "LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection",
    "authors": [
      "Ana Vasilcoiu",
      "Ivona Najdenkoska",
      "Zeno Geradts",
      "Marcel Worring"
    ],
    "abstract": "The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This erodes trust in digital media, making it critical to develop generated image detectors that remain reliable across different generators. While recent approaches leverage diffusion denoising cues, they typically rely on single-step reconstruction errors and overlook the sequential nature of the denoising process. In this work, we propose LATTE - LATent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across multiple denoising steps. Instead of treating each denoising step in isolation, LATTE captures the trajectory of these representations, revealing subtle and discriminative patterns that distinguish real from generated images. Experiments on several benchmarks, such as GenImage, Chameleon, and Diffusion Forensics, show that LATTE achieves superior performance, especially in challenging cross-generator and cross-dataset scenarios, highlighting the potential of latent trajectory modeling. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03054v2",
    "published_date": "2025-07-03 12:53:47 UTC",
    "updated_date": "2025-09-29 19:49:26 UTC"
  },
  {
    "arxiv_id": "2507.02582v1",
    "title": "Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms",
    "authors": [
      "Junli Jiang",
      "Pavel Naumov"
    ],
    "abstract": "Responsibility has long been a subject of study in law and philosophy. More recently, it became a focus of AI literature. The article investigates the computational complexity of two important properties of responsibility in collective decision-making: diffusion and gap. It shows that the sets of diffusion-free and gap-free decision-making mechanisms are $Π_2$-complete and $Π_3$-complete, respectively. At the same time, the intersection of these classes is $Π_2$-complete.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02582v1",
    "published_date": "2025-07-03 12:43:38 UTC",
    "updated_date": "2025-07-03 12:43:38 UTC"
  },
  {
    "arxiv_id": "2507.03052v1",
    "title": "From 2:4 to 8:16 sparsity patterns in LLMs for Outliers and Weights with Variance Correction",
    "authors": [
      "Egor Maximov",
      "Yulia Kuzkina",
      "Azamat Kanametov",
      "Alexander Prutko",
      "Aleksei Goncharov",
      "Maxim Zhelnin",
      "Egor Shvetsov"
    ],
    "abstract": "As large language models (LLMs) grow in size, efficient compression techniques like quantization and sparsification are critical. While quantization maintains performance with reduced precision, structured sparsity methods, such as N:M sparsification, often fall short due to limited flexibility, and sensitivity to outlier weights. We explore 8:16 semi-structured sparsity, demonstrating its ability to surpass the Performance Threshold-where a compressed model matches the accuracy of its uncompressed or smaller counterpart under equivalent memory constraints. Compared to 2:4 sparsity, 8:16 offers greater flexibility with minimal storage overhead (0.875 vs. 0.75 bits/element). We also apply sparse structured patterns for salient weights, showing that structured sparsity for outliers is competitive with unstructured approaches leading to equivalent or better results. Finally, we demonstrate that simple techniques such as variance correction and SmoothQuant like weight equalization improve sparse models performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03052v1",
    "published_date": "2025-07-03 12:17:45 UTC",
    "updated_date": "2025-07-03 12:17:45 UTC"
  },
  {
    "arxiv_id": "2507.02554v2",
    "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
    "authors": [
      "Edan Toledo",
      "Karen Hambardzumyan",
      "Martin Josifoski",
      "Rishi Hazra",
      "Nicolas Baldwin",
      "Alexis Audran-Reiss",
      "Michael Kuchnik",
      "Despoina Magka",
      "Minqi Jiang",
      "Alisia Maria Lupidi",
      "Andrei Lupu",
      "Roberta Raileanu",
      "Kelvin Niu",
      "Tatiana Shavrina",
      "Jean-Christophe Gagnon-Audet",
      "Michael Shvartsman",
      "Shagun Sodhani",
      "Alexander H. Miller",
      "Abhishek Charnalia",
      "Derek Dunfield",
      "Carole-Jean Wu",
      "Pontus Stenetorp",
      "Nicola Cancedda",
      "Jakob Nicolaus Foerster",
      "Yoram Bachrach"
    ],
    "abstract": "AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Code: https://github.com/facebookresearch/aira-dojo",
    "pdf_url": "https://arxiv.org/pdf/2507.02554v2",
    "published_date": "2025-07-03 11:59:15 UTC",
    "updated_date": "2025-11-04 14:55:47 UTC"
  },
  {
    "arxiv_id": "2507.03051v1",
    "title": "Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization",
    "authors": [
      "Marco Simoni",
      "Aleksandar Fontana",
      "Giulio Rossolini",
      "Andrea Saracino"
    ],
    "abstract": "Improving and understanding the training dynamics and reasoning of Large Language Models (LLMs) has become essential for their deployment in AI-based security tools, such as software vulnerability detection. In this work, we present an extensive study aimed at advancing recent RL-based finetuning techniques for LLMs in the context of vulnerability detection.\n  We start by highlighting key limitations of commonly adopted LLMs, such as their tendency to over-predict certain types of vulnerabilities while failing to detect others. To address this challenge, we explore the use of Group Relative Policy Optimization (GRPO), a recent policy-gradient method, for guiding LLM behavior through structured, rule-based rewards. We enable its application to the vulnerability detection task by redefining its advantage functions and reward signals using annotations from widely used datasets in the field, including BigVul, DiverseVul, and CleanVul.\n  The proposed methodology enables an extensive set of experiments, addressing multiple research questions regarding the impact of GRPO on generalization, reasoning capabilities, and performance improvements over standard supervised finetuning (SFT). Our findings offer valuable insights into the potential of RL-based training to enhance both the performance and reasoning abilities of LLMs in the context of software vulnerability detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2507.03051v1",
    "published_date": "2025-07-03 11:52:45 UTC",
    "updated_date": "2025-07-03 11:52:45 UTC"
  },
  {
    "arxiv_id": "2507.02550v1",
    "title": "Position: A Theory of Deep Learning Must Include Compositional Sparsity",
    "authors": [
      "David A. Danhofer",
      "Davide D'Ascenzo",
      "Rafael Dubach",
      "Tomaso Poggio"
    ],
    "abstract": "Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable success in a wide variety of domains too high-dimensional for classical shallow networks subject to the curse of dimensionality. However, open questions about fundamental principles, that govern the learning dynamics of DNNs, remain. In this position paper we argue that it is the ability of DNNs to exploit the compositionally sparse structure of the target function driving their success. As such, DNNs can leverage the property that most practically relevant functions can be composed from a small set of constituent functions, each of which relies only on a low-dimensional subset of all inputs. We show that this property is shared by all efficiently Turing-computable functions and is therefore highly likely present in all current learning problems. While some promising theoretical insights on questions concerned with approximation and generalization exist in the setting of compositionally sparse functions, several important questions on the learnability and optimization of DNNs remain. Completing the picture of the role of compositional sparsity in deep learning is essential to a comprehensive theory of artificial, and even general, intelligence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02550v1",
    "published_date": "2025-07-03 11:49:56 UTC",
    "updated_date": "2025-07-03 11:49:56 UTC"
  },
  {
    "arxiv_id": "2507.02541v1",
    "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context",
    "authors": [
      "Yanzhen Lu",
      "Hanbin Yang",
      "Xiaodie Wang",
      "Ge Zhang",
      "Biao Li",
      "Chenxu Fu",
      "Chao Li",
      "Yang Yuan",
      "Andrew Chi-Chih Yao"
    ],
    "abstract": "In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\\times$ improvement in clarity score (44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model \\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof success (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous state-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\\%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Planner--Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02541v1",
    "published_date": "2025-07-03 11:35:34 UTC",
    "updated_date": "2025-07-03 11:35:34 UTC"
  },
  {
    "arxiv_id": "2507.02537v1",
    "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue",
    "authors": [
      "Paulo Ricardo Knob",
      "Leonardo Scholler",
      "Juliano Rigatti",
      "Soraia Raupp Musse"
    ],
    "abstract": "Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02537v1",
    "published_date": "2025-07-03 11:32:41 UTC",
    "updated_date": "2025-07-03 11:32:41 UTC"
  },
  {
    "arxiv_id": "2507.07115v1",
    "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation",
    "authors": [
      "Javal Vyas",
      "Mehmet Mercangoz"
    ],
    "abstract": "The increasing complexity of modern chemical processes, coupled with workforce shortages and intricate fault scenarios, demands novel automation paradigms that blend symbolic reasoning with adaptive control. In this work, we introduce a unified agentic framework that leverages large language models (LLMs) for both discrete fault-recovery planning and continuous process control within a single architecture. We adopt Finite State Machines (FSMs) as interpretable operating envelopes: an LLM-driven planning agent proposes recovery sequences through the FSM, a Simulation Agent executes and checks each transition, and a Validator-Reprompting loop iteratively refines invalid plans. In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25 states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path success within five reprompts-outperforming open-source LLMs in both accuracy and latency. In Case Study 2, the same framework modulates dual-heater inputs on a laboratory TCLab platform (and its digital twin) to maintain a target average temperature under persistent asymmetric disturbances. Compared to classical PID control, our LLM-based controller attains similar performance, while ablation of the prompting loop reveals its critical role in handling nonlinear dynamics. We analyze key failure modes-such as instruction following lapses and coarse ODE approximations. Our results demonstrate that, with structured feedback and modular agents, LLMs can unify high-level symbolic planningand low-level continuous control, paving the way towards resilient, language-driven automation in chemical engineering.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.07115v1",
    "published_date": "2025-07-03 11:20:22 UTC",
    "updated_date": "2025-07-03 11:20:22 UTC"
  },
  {
    "arxiv_id": "2507.03050v1",
    "title": "From Turing to Tomorrow: The UK's Approach to AI Regulation",
    "authors": [
      "Oliver Ritchie",
      "Markus Anderljung",
      "Tom Rachman"
    ],
    "abstract": "The UK has pursued a distinctive path in AI regulation: less cautious than the EU but more willing to address risks than the US, and has emerged as a global leader in coordinating AI safety efforts. Impressive developments from companies like London-based DeepMind began to spark concerns in the UK about catastrophic risks from around 2012, although regulatory discussion at the time focussed on bias and discrimination. By 2022, these discussions had evolved into a \"pro-innovation\" strategy, in which the government directed existing regulators to take a light-touch approach, governing AI at point of use, but avoided regulating the technology or infrastructure directly. ChatGPT arrived in late 2022, galvanising concerns that this approach may be insufficient. The UK responded by establishing an AI Safety Institute to monitor risks and hosting the first international AI Safety Summit in 2023, but - unlike the EU - refrained from regulating frontier AI development in addition to its use. A new government was elected in 2024 which promised to address this gap, but at the time of writing is yet to do so.\n  What should the UK do next? The government faces competing objectives: harnessing AI for economic growth and better public services while mitigating risk. In light of these, we propose establishing a flexible, principles-based regulator to oversee the most advanced AI development, defensive measures against risks from AI-enabled biological design tools, and argue that more technical work is needed to understand how to respond to AI-generated misinformation. We argue for updated legal frameworks on copyright, discrimination, and AI agents, and that regulators will have a limited but important role if AI substantially disrupts labour markets.\n  If the UK gets AI regulation right, it could demonstrate how democratic societies can harness AI's benefits while managing its risks.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "This is a chapter intended for publication in a forthcoming edited volume. It is the version of the author's manuscript prior to acceptance for publication and has not undergone editorial and/or peer review on behalf of the Publisher",
    "pdf_url": "https://arxiv.org/pdf/2507.03050v1",
    "published_date": "2025-07-03 10:54:43 UTC",
    "updated_date": "2025-07-03 10:54:43 UTC"
  },
  {
    "arxiv_id": "2507.03049v1",
    "title": "Personalised Explanations in Long-term Human-Robot Interactions",
    "authors": [
      "Ferran Gebellí",
      "Anaís Garrell",
      "Jan-Gerrit Habekost",
      "Séverin Lemaignan",
      "Stefan Wermter",
      "Raquel Ros"
    ],
    "abstract": "In the field of Human-Robot Interaction (HRI), a fundamental challenge is to facilitate human understanding of robots. The emerging domain of eXplainable HRI (XHRI) investigates methods to generate explanations and evaluate their impact on human-robot interactions. Previous works have highlighted the need to personalise the level of detail of these explanations to enhance usability and comprehension. Our paper presents a framework designed to update and retrieve user knowledge-memory models, allowing for adapting the explanations' level of detail while referencing previously acquired concepts. Three architectures based on our proposed framework that use Large Language Models (LLMs) are evaluated in two distinct scenarios: a hospital patrolling robot and a kitchen assistant robot. Experimental results demonstrate that a two-stage architecture, which first generates an explanation and then personalises it, is the framework architecture that effectively reduces the level of detail only when there is related user knowledge.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages. It will be published at RO-MAN 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.03049v1",
    "published_date": "2025-07-03 10:40:39 UTC",
    "updated_date": "2025-07-03 10:40:39 UTC"
  },
  {
    "arxiv_id": "2507.02517v1",
    "title": "Detecting Multiple Diseases in Multiple Crops Using Deep Learning",
    "authors": [
      "Vivek Yadav",
      "Anugrah Jain"
    ],
    "abstract": "India, as a predominantly agrarian economy, faces significant challenges in agriculture, including substantial crop losses caused by diseases, pests, and environmental stress. Early detection and accurate identification of diseases across different crops are critical for improving yield and ensuring food security. This paper proposes a deep learning based solution for detecting multiple diseases in multiple crops, aimed to cover India's diverse agricultural landscape. We first create a unified dataset encompassing images of 17 different crops and 34 different diseases from various available repositories. Proposed deep learning model is trained on this dataset and outperforms the state-of-the-art in terms of accuracy and the number of crops, diseases covered. We achieve a significant detection accuracy, i.e., 99 percent for our unified dataset which is 7 percent more when compared to state-of-the-art handling 14 crops and 26 different diseases only. By improving the number of crops and types of diseases that can be detected, proposed solution aims to provide a better product for Indian farmers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02517v1",
    "published_date": "2025-07-03 10:26:49 UTC",
    "updated_date": "2025-07-03 10:26:49 UTC"
  },
  {
    "arxiv_id": "2507.03048v1",
    "title": "Monitoring of Static Fairness",
    "authors": [
      "Thomas A. Henzinger",
      "Mahyar Karimi",
      "Konstantin Kueffner",
      "Kaushik Mallik"
    ],
    "abstract": "Machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes.\n  We present a general framework of runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a Markov chain structure, with or without full observation of the state space.\n  We introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden.\n  We build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time.\n  The estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer.\n  We present two categories of monitoring algorithms, namely ones with a uniform error bound across all time points, and ones with weaker non-uniform, pointwise error bounds at different time points.\n  Our monitoring algorithms use statistical tools that are adapted to suit the dynamic requirements of monitoring and the special needs of the fairness specifications.\n  Using a prototype implementation, we show how we can monitor if a bank is fair in giving loans to applicants from different social backgrounds, and if a college is fair in admitting students while maintaining a reasonable financial burden on the society.\n  In these experiments, our monitors took less than a millisecond to update their verdicts after each observation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2305.15979",
    "pdf_url": "https://arxiv.org/pdf/2507.03048v1",
    "published_date": "2025-07-03 10:20:04 UTC",
    "updated_date": "2025-07-03 10:20:04 UTC"
  },
  {
    "arxiv_id": "2507.02506v1",
    "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders",
    "authors": [
      "Sneha Deshmukh",
      "Prathmesh Kamble"
    ],
    "abstract": "Legal NLP remains underdeveloped in regions like India due to the scarcity of structured datasets. We introduce IndianBailJudgments-1200, a new benchmark dataset comprising 1200 Indian court judgments on bail decisions, annotated across 20+ attributes including bail outcome, IPC sections, crime type, and legal reasoning. Annotations were generated using a prompt-engineered GPT-4o pipeline and verified for consistency. This resource supports a wide range of legal NLP tasks such as outcome prediction, summarization, and fairness analysis, and is the first publicly available dataset focused specifically on Indian bail jurisprudence.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and GitHub. Submitted to arXiv for open access",
    "pdf_url": "https://arxiv.org/pdf/2507.02506v1",
    "published_date": "2025-07-03 10:13:42 UTC",
    "updated_date": "2025-07-03 10:13:42 UTC"
  },
  {
    "arxiv_id": "2507.03047v2",
    "title": "Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning",
    "authors": [
      "Yutian Liu",
      "Zhengyi Yang",
      "Jiancan Wu",
      "Xiang Wang"
    ],
    "abstract": "Recent advances have applied large language models (LLMs) to sequential recommendation, leveraging their pre-training knowledge and reasoning capabilities to provide more personalized user experiences. However, existing LLM-based methods fail to sufficiently leverage the rich temporal information inherent in users' historical interaction sequences, stemming from fundamental architectural constraints: LLMs process information through self-attention mechanisms that lack inherent sequence ordering and rely on position embeddings designed primarily for natural language rather than user interaction sequences. This limitation significantly impairs their ability to capture the evolution of user preferences over time and predict future interests accurately.\n  To address this critical gap, we propose \\underline{C}ounterfactual \\underline{E}nhanced \\underline{T}emporal Framework for LLM-Based \\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference principles, which allow it to isolate and measure the specific impact of temporal information on recommendation outcomes. Combined with our counterfactual tuning task derived from causal analysis, CETRec effectively enhances LLMs' awareness of both absolute order (how recently items were interacted with) and relative order (the sequential relationships between items). Extensive experiments on real-world datasets demonstrate the effectiveness of our CETRec. Our code is available at https://anonymous.4open.science/r/CETRec-B9CE/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03047v2",
    "published_date": "2025-07-03 10:11:35 UTC",
    "updated_date": "2025-08-20 09:09:56 UTC"
  },
  {
    "arxiv_id": "2507.02503v1",
    "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
    "authors": [
      "Chenxu Wang",
      "Yilin Lyu",
      "Zicheng Sun",
      "Liping Jing"
    ],
    "abstract": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 6 figures, accepted by ACL 2025 main",
    "pdf_url": "https://arxiv.org/pdf/2507.02503v1",
    "published_date": "2025-07-03 10:11:22 UTC",
    "updated_date": "2025-07-03 10:11:22 UTC"
  },
  {
    "arxiv_id": "2507.02493v1",
    "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy",
    "authors": [
      "Luca Parolari",
      "Andrea Cherubini",
      "Lamberto Ballan",
      "Carlo Biffi"
    ],
    "abstract": "Automated polyp counting in colonoscopy is a crucial step toward automated procedure reporting and quality control, aiming to enhance the cost-effectiveness of colonoscopy screening. Counting polyps in a procedure involves detecting and tracking polyps, and then clustering tracklets that belong to the same polyp entity. Existing methods for polyp counting rely on self-supervised learning and primarily leverage visual appearance, neglecting temporal relationships in both tracklet feature learning and clustering stages. In this work, we introduce a paradigm shift by proposing a supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intra-polyp variability while preserving inter-polyp discriminability, leading to more robust clustering. Additionally, we improve tracklet clustering by integrating a temporal adjacency constraint, reducing false positive re-associations between visually similar but temporally distant tracklets. We train and validate our method on publicly available datasets and evaluate its performance with a leave-one-out cross-validation strategy. Results demonstrate a 2.2x reduction in fragmentation rate compared to prior approaches. Our results highlight the importance of temporal awareness in polyp counting, establishing a new state-of-the-art. Code is available at https://github.com/lparolari/temporally-aware-polyp-counting.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02493v1",
    "published_date": "2025-07-03 09:55:48 UTC",
    "updated_date": "2025-07-03 09:55:48 UTC"
  },
  {
    "arxiv_id": "2507.02479v1",
    "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios",
    "authors": [
      "Teng Fu",
      "Yuwen Chen",
      "Zhuofan Chen",
      "Mengyang Zhao",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "abstract": "Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02479v1",
    "published_date": "2025-07-03 09:36:44 UTC",
    "updated_date": "2025-07-03 09:36:44 UTC"
  },
  {
    "arxiv_id": "2507.02443v1",
    "title": "Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic",
    "authors": [
      "Sandro Costa Magalhães",
      "Marco Almeida",
      "Filipe Neves dos Santos",
      "António Paulo Moreira",
      "Jorge Dias"
    ],
    "abstract": "Robots usually slow down for canning to detect objects while moving. Additionally, the robot's camera is configured with a low framerate to track the velocity of the detection algorithms. This would be constrained while executing tasks and exploring, making robots increase the task execution time. AMD has developed the Vitis-AI framework to deploy detection algorithms into FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation (BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This is a self-acquired dataset released in open access. MobileNet v1 performed better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In this work, we proved that we can use FPGAs to speed up ANNs and make them suitable for attention mechanisms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ROBOT'2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02443v1",
    "published_date": "2025-07-03 09:00:19 UTC",
    "updated_date": "2025-07-03 09:00:19 UTC"
  },
  {
    "arxiv_id": "2507.02442v3",
    "title": "The Gauss-Markov Adjunction Provides Categorical Semantics of Residuals in Supervised Learning",
    "authors": [
      "Moto Kamiura"
    ],
    "abstract": "Enhancing the intelligibility and interpretability of machine learning is a crucial task in responding to the demand for Explicability as an AI principle, and in promoting the better social implementation of AI. The aim of our research is to contribute to this improvement by reformulating machine learning models through the lens of category theory, thereby developing a semantic framework for structuring and understanding AI systems. Our categorical modeling in this paper clarifies and formalizes the structural interplay between residuals and parameters in supervised learning. The present paper focuses on the multiple linear regression model, which represents the most basic form of supervised learning. By defining two Lawvere-enriched categories corresponding to parameters and data, along with an adjoint pair of functors between them, we introduce our categorical formulation of supervised learning. We show that the essential structure of this framework is captured by what we call the Gauss-Markov Adjunction. Within this setting, the dual flow of information can be explicitly described as a correspondence between variations in parameters and residuals. The ordinary least squares estimator for the parameters and the minimum residual are related via the preservation of limits by the right adjoint functor. Furthermore, we position this formulation as an instance of extended denotational semantics for supervised learning, and propose applying a semantic perspective developed in theoretical computer science as a formal foundation for Explicability in AI.",
    "categories": [
      "cs.AI",
      "math.CT",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02442v3",
    "published_date": "2025-07-03 08:58:59 UTC",
    "updated_date": "2025-10-20 04:02:37 UTC"
  },
  {
    "arxiv_id": "2507.03045v1",
    "title": "Optimisation Is Not What You Need",
    "authors": [
      "Alfredo Ibias"
    ],
    "abstract": "The Artificial Intelligence field has focused on developing optimisation methods to solve multiple problems, specifically problems that we thought to be only solvable through cognition. The obtained results have been outstanding, being able to even surpass the Turing Test. However, we have found that these optimisation methods share some fundamental flaws that impede them to become a true artificial cognition. Specifically, the field have identified catastrophic forgetting as a fundamental problem to develop such cognition. This paper formally proves that this problem is inherent to optimisation methods, and as such it will always limit approaches that try to solve the Artificial General Intelligence problem as an optimisation problem. Additionally, it addresses the problem of overfitting and discuss about other smaller problems that optimisation methods pose. Finally, it empirically shows how world-modelling methods avoid suffering from either problem. As a conclusion, the field of Artificial Intelligence needs to look outside the machine learning field to find methods capable of developing an artificial cognition.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03045v1",
    "published_date": "2025-07-03 08:50:20 UTC",
    "updated_date": "2025-07-03 08:50:20 UTC"
  },
  {
    "arxiv_id": "2507.02436v1",
    "title": "Toward a Robust and Generalizable Metamaterial Foundation Model",
    "authors": [
      "Namjung Kim",
      "Dongseok Lee",
      "Jongbin Yu",
      "Sung Woong Cho",
      "Dosung Lee",
      "Yesol Park",
      "Youngjoon Hong"
    ],
    "abstract": "Advances in material functionalities drive innovations across various fields, where metamaterials-defined by structure rather than composition-are leading the way. Despite the rise of artificial intelligence (AI)-driven design strategies, their impact is limited by task-specific retraining, poor out-of-distribution(OOD) generalization, and the need for separate models for forward and inverse design. To address these limitations, we introduce the Metamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation model inspired by large language models. MetaFO learns the underlying mechanics of metamaterials, enabling probabilistic, zero-shot predictions across diverse, unseen combinations of material properties and structural responses. It also excels in nonlinear inverse design, even under OOD conditions. By treating metamaterials as an operator that maps material properties to structural responses, MetaFO uncovers intricate structure-property relationships and significantly expands the design space. This scalable and generalizable framework marks a paradigm shift in AI-driven metamaterial discovery, paving the way for next-generation innovations.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "physics.optics"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02436v1",
    "published_date": "2025-07-03 08:48:36 UTC",
    "updated_date": "2025-07-03 08:48:36 UTC"
  },
  {
    "arxiv_id": "2507.02424v2",
    "title": "CyberRAG: An Agentic RAG cyber attack classification and reporting tool",
    "authors": [
      "Francesco Blefari",
      "Cristian Cosentino",
      "Francesco Aurelio Pironti",
      "Angelo Furfaro",
      "Fabrizio Marozzo"
    ],
    "abstract": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming analysts with logs requiring rapidly evolving expertise. Conventional machine-learning detectors reduce alert volume but still yield many false positives, while standard Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify predictions. We present CyberRAG, a modular agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates: (i) fine-tuned classifiers specialized by attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that queries a domain-specific knowledge base until evidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG adopts an agentic design that enables dynamic control flow and adaptive reasoning. This architecture autonomously refines threat labels and natural-language justifications, reducing false positives and enhancing interpretability. It is also extensible: new attack types can be supported by adding classifiers without retraining the core agent. CyberRAG was evaluated on SQL Injection, XSS, and SSTI, achieving over 94\\% accuracy per class and a final classification accuracy of 94.92\\% through semantic orchestration. Generated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation, with robustness preserved against adversarial and unseen payloads. These results show that agentic, specialist-oriented RAG can combine high detection accuracy with trustworthy, SOC-ready prose, offering a flexible path toward partially automated cyber-defense workflows.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02424v2",
    "published_date": "2025-07-03 08:32:19 UTC",
    "updated_date": "2025-09-10 09:08:15 UTC"
  },
  {
    "arxiv_id": "2507.03043v2",
    "title": "K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function",
    "authors": [
      "Shuhe Li",
      "Chenxu Guo",
      "Jiachen Lian",
      "Cheol Jun Cho",
      "Wenshuo Zhao",
      "Xiner Xu",
      "Ruiyu Jin",
      "Xiaoyu Shi",
      "Xuanru Zhou",
      "Dingkun Zhou",
      "Sam Wang",
      "Grace Wang",
      "Jingze Yang",
      "Jingyi Xu",
      "Ruohan Bao",
      "Xingrui Chen",
      "Elise Brenner",
      "Brandon In",
      "Francesca Pei",
      "Maria Luisa Gorno-Tempini",
      "Gopala Anumanchipalli"
    ],
    "abstract": "Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. Its core, Kids-Weighted Finite State Transducer (K-WFST), merges an acoustic phoneme encoder with a phoneme-similarity model to capture child-specific speech errors while remaining fully interpretable. K-WFST achieves a 1.39 % phoneme error rate on MyST and 8.61 % on Multitudes-an absolute improvement of 10.47 % and 7.06 % over a greedy-search decoder. These high-quality transcripts are used by an LLM to grade verbal skills, developmental milestones, reading, and comprehension, with results that align closely with human evaluators. Our findings show that precise phoneme recognition is essential for creating an effective assessment framework, enabling scalable language screening for children.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03043v2",
    "published_date": "2025-07-03 08:05:02 UTC",
    "updated_date": "2026-01-19 18:03:51 UTC"
  },
  {
    "arxiv_id": "2507.02409v4",
    "title": "S2FGL: Spatial Spectral Federated Graph Learning",
    "authors": [
      "Zihan Tan",
      "Suyuan Huang",
      "Guancheng Wan",
      "Wenke Huang",
      "He Li",
      "Mang Ye"
    ],
    "abstract": "Federated Graph Learning (FGL) combines the privacy-preserving capabilities of federated learning (FL) with the strong graph modeling capability of Graph Neural Networks (GNNs). Current research addresses subgraph-FL from the structural perspective, neglecting the propagation of graph signals on spatial and spectral domains of the structure. From a spatial perspective, subgraph-FL introduces edge disconnections between clients, leading to disruptions in label signals and a degradation in the semantic knowledge of the global GNN. From a spectral perspective, spectral heterogeneity causes inconsistencies in signal frequencies across subgraphs, which makes local GNNs overfit the local signal propagation schemes. As a result, spectral client drift occurs, undermining global generalizability. To tackle the challenges, we propose a global knowledge repository to mitigate the challenge of poor semantic knowledge caused by label signal disruption. Furthermore, we design a frequency alignment to address spectral client drift. The combination of Spatial and Spectral strategies forms our framework S2FGL. Extensive experiments on multiple datasets demonstrate the superiority of S2FGL. The code is available at https://github.com/Wonder7racer/S2FGL.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02409v4",
    "published_date": "2025-07-03 08:04:49 UTC",
    "updated_date": "2025-08-18 08:00:31 UTC"
  },
  {
    "arxiv_id": "2507.02403v1",
    "title": "Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings",
    "authors": [
      "Mufhumudzi Muthivhi",
      "Terence L. van Zyl"
    ],
    "abstract": "Wildlife re-identification aims to match individuals of the same species across different observations. Current state-of-the-art (SOTA) models rely on class labels to train supervised models for individual classification. This dependence on annotated data has driven the curation of numerous large-scale wildlife datasets. This study investigates self-supervised learning Self-Supervised Learning (SSL) for wildlife re-identification. We automatically extract two distinct views of an individual using temporal image pairs from camera trap data without supervision. The image pairs train a self-supervised model from a potentially endless stream of video data. We evaluate the learnt representations against supervised features on open-world scenarios and transfer learning in various wildlife downstream tasks. The analysis of the experimental results shows that self-supervised models are more robust even with limited data. Moreover, self-supervised features outperform supervision across all downstream tasks. The code is available here https://github.com/pxpana/SSLWildlife.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in IEEE Xplore and ISIF FUSION 2025 proceedings:",
    "pdf_url": "https://arxiv.org/pdf/2507.02403v1",
    "published_date": "2025-07-03 07:56:54 UTC",
    "updated_date": "2025-07-03 07:56:54 UTC"
  },
  {
    "arxiv_id": "2507.03042v1",
    "title": "Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction",
    "authors": [
      "Yuyang Lou",
      "Charles Li"
    ],
    "abstract": "Memory storage for Large Language models (LLMs) is becoming an increasingly active area of research, particularly for enabling personalization across long conversations. We propose Pref-LSTM, a dynamic and lightweight framework that combines a BERT-based classifier with a LSTM memory module that generates memory embedding which then is soft-prompt injected into a frozen LLM. We synthetically curate a dataset of preference and non-preference conversation turns to train our BERT-based classifier. Although our LSTM-based memory encoder did not yield strong results, we find that the BERT-based classifier performs reliably in identifying explicit and implicit user preferences. Our research demonstrates the viability of using preference filtering with LSTM gating principals as an efficient path towards scalable user preference modeling, without extensive overhead and fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 4 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.03042v1",
    "published_date": "2025-07-03 07:53:20 UTC",
    "updated_date": "2025-07-03 07:53:20 UTC"
  },
  {
    "arxiv_id": "2507.02398v2",
    "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection",
    "authors": [
      "Taehoon Kim",
      "Jongwook Choi",
      "Yonghyun Jeong",
      "Haeun Noh",
      "Jaejun Yoo",
      "Seungryul Baek",
      "Jongwon Choi"
    ],
    "abstract": "We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. Traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect temporal artifacts in the pixel plane. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by iccv 2025. code is will be available at https://github.com/rama0126/PwTF-DVD",
    "pdf_url": "https://arxiv.org/pdf/2507.02398v2",
    "published_date": "2025-07-03 07:49:55 UTC",
    "updated_date": "2025-07-10 06:52:35 UTC"
  },
  {
    "arxiv_id": "2507.02390v1",
    "title": "Evaluating Language Models For Threat Detection in IoT Security Logs",
    "authors": [
      "Jorge J. Tejero-Fernández",
      "Alfonso Sánchez-Macián"
    ],
    "abstract": "Log analysis is a relevant research field in cybersecurity as they can provide a source of information for the detection of threats to networks and systems. This paper presents a pipeline to use fine-tuned Large Language Models (LLMs) for anomaly detection and mitigation recommendation using IoT security logs. Utilizing classical machine learning classifiers as a baseline, three open-source LLMs are compared for binary and multiclass anomaly detection, with three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT dataset. LLMs give better results on multi-class attack classification than the corresponding baseline models. By mapping detected threats to MITRE CAPEC, defining a set of IoT-specific mitigation actions, and fine-tuning the models with those actions, the models are able to provide a combined detection and recommendation guidance.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02390v1",
    "published_date": "2025-07-03 07:38:43 UTC",
    "updated_date": "2025-07-03 07:38:43 UTC"
  },
  {
    "arxiv_id": "2507.02379v1",
    "title": "An AI-native experimental laboratory for autonomous biomolecular engineering",
    "authors": [
      "Mingyu Wu",
      "Zhaoguo Wang",
      "Jiabin Wang",
      "Zhiyuan Dong",
      "Jingkai Yang",
      "Qingting Li",
      "Tianyu Huang",
      "Lei Zhao",
      "Mingqiang Li",
      "Fei Wang",
      "Chunhai Fan",
      "Haibo Chen"
    ],
    "abstract": "Autonomous scientific research, capable of independently conducting complex experiments and serving non-specialists, represents a long-held aspiration. Achieving it requires a fundamental paradigm shift driven by artificial intelligence (AI). While autonomous experimental systems are emerging, they remain confined to areas featuring singular objectives and well-defined, simple experimental workflows, such as chemical synthesis and catalysis. We present an AI-native autonomous laboratory, targeting highly complex scientific experiments for applications like autonomous biomolecular engineering. This system autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. Founded on a co-design philosophy of models, experiments, and instruments, the platform supports the co-evolution of AI models and the automation system. This establishes an end-to-end, multi-user autonomous laboratory that handles complex, multi-objective experiments across diverse instrumentation. Our autonomous laboratory supports fundamental nucleic acid functions-including synthesis, transcription, amplification, and sequencing. It also enables applications in fields such as disease diagnostics, drug development, and information storage. Without human intervention, it autonomously optimizes experimental performance to match state-of-the-art results achieved by human scientists. In multi-user scenarios, the platform significantly improves instrument utilization and experimental efficiency. This platform paves the way for advanced biomaterials research to overcome dependencies on experts and resource barriers, establishing a blueprint for science-as-a-service at scale.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02379v1",
    "published_date": "2025-07-03 07:21:19 UTC",
    "updated_date": "2025-07-03 07:21:19 UTC"
  },
  {
    "arxiv_id": "2507.02376v1",
    "title": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software",
    "authors": [
      "Chung-ju Huang",
      "Ziqi Zhang",
      "Yinggui Wang",
      "Binghui Wang",
      "Tao Wei",
      "Leye Wang"
    ],
    "abstract": "Vertical Federated Learning (VFL) is a distributed AI software deployment mechanism for cross-silo collaboration without accessing participants' data. However, existing VFL work lacks a mechanism to audit the execution correctness of the inference software of the data party. To address this problem, we design a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task party to audit whether the data party's inference software is executed as expected during large-scale inference without leaking the data privacy of the data party or introducing additional latency to the inference system. The core of VeFIA is that the task party can use the inference results from a framework with Trusted Execution Environments (TEE) and the coordinator to validate the correctness of the data party's computation results. VeFIA guarantees that, as long as the abnormal inference exceeds 5.4%, the task party can detect execution anomalies in the inference software with a probability of 99.99%, without incurring any additional online inference latency. VeFIA's random sampling validation achieves 100% positive predictive value, negative predictive value, and true positive rate in detecting abnormal inference. To the best of our knowledge, this is the first paper to discuss the correctness of inference software execution in VFL.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02376v1",
    "published_date": "2025-07-03 07:17:49 UTC",
    "updated_date": "2025-07-03 07:17:49 UTC"
  },
  {
    "arxiv_id": "2507.03041v3",
    "title": "Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards",
    "authors": [
      "Shirley Wu",
      "Parth Sarthi",
      "Shiyu Zhao",
      "Aaron Lee",
      "Herumb Shandilya",
      "Adrian Mladenic Grobelnik",
      "Nurendra Choudhary",
      "Eddie Huang",
      "Karthik Subbian",
      "Linjun Zhang",
      "Diyi Yang",
      "James Zou",
      "Jure Leskovec"
    ],
    "abstract": "Compound AI systems integrating multiple components, such as Large Language Models, specialized tools, and traditional machine learning models, are increasingly deployed to solve complex real-world tasks. However, optimizing compound systems remains challenging due to their non-differentiable structures and diverse configuration types across components, including prompts, hyperparameters, and model parameters. To address this challenge, we propose Optimas, a unified framework for effective optimization of compound systems. The core idea of Optimas is to maintain one Local Reward Function (LRF) per component, each satisfying a local-global alignment property, i.e., each component's local reward correlates with the global system performance. In each iteration, Optimas efficiently adapts the LRFs to maintain this property while simultaneously maximizing each component's local reward. This approach enables independent updates of heterogeneous configurations using the designated optimization method, while ensuring that local improvements consistently lead to performance gains. We present extensive evaluations across five real-world compound systems to demonstrate that Optimas outperforms strong baselines by an average improvement of 11.92%, offering a general and effective approach for improving compound systems. Our website is at https://optimas.stanford.edu.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03041v3",
    "published_date": "2025-07-03 07:12:48 UTC",
    "updated_date": "2025-10-05 08:19:08 UTC"
  },
  {
    "arxiv_id": "2507.02358v4",
    "title": "Hita: Holistic Tokenizer for Autoregressive Image Generation",
    "authors": [
      "Anlin Zheng",
      "Haochen Wang",
      "Yucheng Zhao",
      "Weipeng Deng",
      "Tiancai Wang",
      "Xiangyu Zhang",
      "Xiaojuan Qi"
    ],
    "abstract": "Vanilla autoregressive image generation models generate visual tokens step-by-step, limiting their ability to capture holistic relationships among token sequences. Moreover, because most visual tokenizers map local image patches into latent tokens, global information is limited. To address this, we introduce \\textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Hita incorporates two key strategies to better align with the AR generation process: 1) {arranging} a sequential structure with holistic tokens at the beginning, followed by patch-level tokens, and using causal attention to maintain awareness of previous tokens; and 2) adopting a lightweight fusion module before feeding the de-quantized tokens into the decoder to control information flow and prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID} and \\textbf{281.9 IS} on the ImageNet benchmark. Detailed analysis of the holistic representation highlights its ability to capture global image properties, such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02358v4",
    "published_date": "2025-07-03 06:44:26 UTC",
    "updated_date": "2025-07-11 09:06:39 UTC"
  },
  {
    "arxiv_id": "2507.02356v1",
    "title": "Offline Reinforcement Learning with Penalized Action Noise Injection",
    "authors": [
      "JunHyeok Oh",
      "Byung-Jun Lee"
    ],
    "abstract": "Offline reinforcement learning (RL) optimizes a policy using only a fixed dataset, making it a practical approach in scenarios where interaction with the environment is costly. Due to this limitation, generalization ability is key to improving the performance of offline RL algorithms, as demonstrated by recent successes of offline RL with diffusion models. However, it remains questionable whether such diffusion models are necessary for highly performing offline RL algorithms, given their significant computational requirements during inference. In this paper, we propose Penalized Action Noise Injection (PANI), a method that simply enhances offline learning by utilizing noise-injected actions to cover the entire action space, while penalizing according to the amount of noise injected. This approach is inspired by how diffusion models have worked in offline RL algorithms. We provide a theoretical foundation for this method, showing that offline RL algorithms with such noise-injected actions solve a modified Markov Decision Process (MDP), which we call the noisy action MDP. PANI is compatible with a wide range of existing off-policy and offline RL algorithms, and despite its simplicity, it demonstrates significant performance improvements across various benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02356v1",
    "published_date": "2025-07-03 06:41:03 UTC",
    "updated_date": "2025-07-03 06:41:03 UTC"
  },
  {
    "arxiv_id": "2507.02353v1",
    "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent",
    "authors": [
      "Bowen Chen",
      "Zhao Wang",
      "Shingo Takamatsu"
    ],
    "abstract": "Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02353v1",
    "published_date": "2025-07-03 06:37:55 UTC",
    "updated_date": "2025-07-03 06:37:55 UTC"
  },
  {
    "arxiv_id": "2507.02349v1",
    "title": "Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection",
    "authors": [
      "Rafic Nader",
      "Vincent L'Allinec",
      "Romain Bourcier",
      "Florent Autrusseau"
    ],
    "abstract": "Intracranial aneurysms (ICA) commonly occur in specific segments of the Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations. An accurate detection of these critical landmarks is necessary for a prompt and efficient diagnosis. We introduce a fully automated landmark detection approach for CoW bifurcations using a two-step neural networks process. Initially, an object detection network identifies regions of interest (ROIs) proximal to the landmark locations. Subsequently, a modified U-Net with deep supervision is exploited to accurately locate the bifurcations. This two-step method reduces various problems, such as the missed detections caused by two landmarks being close to each other and having similar visual characteristics, especially when processing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for the anatomical variability of the CoW, which affects the number of detectable landmarks per scan. We assessed the effectiveness of our approach using two cerebral MRA datasets: our In-House dataset which had varying numbers of landmarks, and a public dataset with standardized landmark configuration. Our experimental results demonstrate that our method achieves the highest level of performance on a bifurcation detection task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02349v1",
    "published_date": "2025-07-03 06:23:38 UTC",
    "updated_date": "2025-07-03 06:23:38 UTC"
  },
  {
    "arxiv_id": "2507.02345v1",
    "title": "HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3",
    "authors": [
      "Jie Gao",
      "Jing Hu",
      "Shanzhuo Zhang",
      "Kunrui Zhu",
      "Sheng Qian",
      "Yueyang Huang",
      "Xiaonan Zhang",
      "Xiaomin Fang"
    ],
    "abstract": "Antibody engineering is essential for developing therapeutics and advancing biomedical research. Traditional discovery methods often rely on time-consuming and resource-intensive experimental screening. To enhance and streamline this process, we introduce a production-grade, high-throughput platform built on HelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure prediction model, HelixFold3. The platform facilitates the large-scale generation of antibody candidate sequences and evaluates their interaction with antigens. Integrated high-performance computing (HPC) support enables high-throughput screening, addressing challenges such as fragmented toolchains and high computational demands. Validation on multiple antigens showcases the platform's ability to generate diverse and high-quality antibodies, confirming a scaling law where exploring larger sequence spaces increases the likelihood of identifying optimal binders. This platform provides a seamless, accessible solution for large-scale antibody design and is available via the antibody design page of PaddleHelix platform.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02345v1",
    "published_date": "2025-07-03 06:13:23 UTC",
    "updated_date": "2025-07-03 06:13:23 UTC"
  },
  {
    "arxiv_id": "2507.02342v2",
    "title": "DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values",
    "authors": [
      "Changhun Kim",
      "Yechan Mun",
      "Sangchul Hahn",
      "Eunho Yang"
    ],
    "abstract": "This study proposes DeltaSHAP, a novel explainable artificial intelligence (XAI) algorithm specifically designed for online patient monitoring systems. In clinical environments, discovering the causes driving patient risk evolution is critical for timely intervention, yet existing XAI methods fail to address the unique requirements of clinical time series explanation tasks. To this end, DeltaSHAP addresses three key clinical needs: explaining the changes in the consecutive predictions rather than isolated prediction scores, providing both magnitude and direction of feature attributions, and delivering these insights in real time. By adapting Shapley values to temporal settings, our approach accurately captures feature coalition effects. It further attributes prediction changes using only the actually observed feature combinations, making it efficient and practical for time-sensitive clinical applications. We also introduce new evaluation metrics to evaluate the faithfulness of the attributions for online time series, and demonstrate through experiments on online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI methods in both explanation quality as 62% and computational efficiency as 33% time reduction on the MIMIC-III decompensation benchmark. We release our code at https://github.com/AITRICS/DeltaSHAP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025 Workshop on Actionable Interpretability. Code is available at https://github.com/AITRICS/DeltaSHAP",
    "pdf_url": "https://arxiv.org/pdf/2507.02342v2",
    "published_date": "2025-07-03 06:08:07 UTC",
    "updated_date": "2025-07-12 07:51:21 UTC"
  },
  {
    "arxiv_id": "2507.02337v1",
    "title": "ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms",
    "authors": [
      "Gjorgjina Cenikj",
      "Gašper Petelin",
      "Tome Eftimov"
    ],
    "abstract": "Understanding the behavior of numerical metaheuristic optimization algorithms is critical for advancing their development and application. Traditional visualization techniques, such as convergence plots, trajectory mapping, and fitness landscape analysis, often fall short in illustrating the structural dynamics of the search process, especially in high-dimensional or complex solution spaces. To address this, we propose a novel representation and visualization methodology that clusters solution candidates explored by the algorithm and tracks the evolution of cluster memberships across iterations, offering a dynamic and interpretable view of the search process. Additionally, we introduce two metrics - algorithm stability and algorithm similarity- to quantify the consistency of search trajectories across runs of an individual algorithm and the similarity between different algorithms, respectively. We apply this methodology to a set of ten numerical metaheuristic algorithms, revealing insights into their stability and comparative behaviors, thereby providing a deeper understanding of their search dynamics.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02337v1",
    "published_date": "2025-07-03 06:01:02 UTC",
    "updated_date": "2025-07-03 06:01:02 UTC"
  },
  {
    "arxiv_id": "2507.03038v2",
    "title": "Cautious Next Token Prediction",
    "authors": [
      "Yizhou Wang",
      "Lingzhi Zhang",
      "Yue Bai",
      "Mang Tik Chiu",
      "Zhengmian Hu",
      "Mingyuan Zhang",
      "Qihua Dong",
      "Yu Yin",
      "Sohrab Amirghodsi",
      "Yun Fu"
    ],
    "abstract": "Next token prediction paradigm has been prevailing for autoregressive models in the era of LLMs. The current default sampling choice for popular LLMs is temperature scaling together with nucleus sampling to balance diversity and coherence. Nevertheless, such approach leads to inferior performance in various NLP tasks when the model is not certain about testing questions. To this end, we propose a brand new training-free decoding strategy, dubbed as Cautious Next Token Prediction (CNTP). In the decoding process, if the model has comparatively high prediction entropy at a certain step, we sample multiple trials starting from the step independently and stop when encountering any punctuation. Then we select the trial with the lowest perplexity score viewed as the most probable and reliable trial path given the model's capacity. The trial number is negatively correlated with the prediction confidence, i.e., the less confident the model is, the more trials it should sample. This is consistent with human beings' behaviour: when feeling uncertain or unconfident, one tends to think more creatively, exploring multiple thinking paths, to cautiously select the path one feels most confident about. Extensive experiments on both LLMs and MLLMs show that our proposed CNTP approach outperforms existing standard decoding strategies consistently by a clear margin. Moreover, the integration of CNTP with self consistency can further improve over vanilla self consistency. We believe our proposed CNTP has the potential to become one of the default choices for LLM decoding. Code is available at https://github.com/wyzjack/CNTP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.03038v2",
    "published_date": "2025-07-03 05:49:18 UTC",
    "updated_date": "2025-07-23 08:06:29 UTC"
  },
  {
    "arxiv_id": "2507.02331v1",
    "title": "Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes",
    "authors": [
      "Ana Nikolikj",
      "Mario Andrés Muñoz",
      "Eva Tuba",
      "Tome Eftimov"
    ],
    "abstract": "This paper leverages the recently introduced concept of algorithm footprints to investigate the interplay between algorithm configurations and problem characteristics. Performance footprints are calculated for six modular variants of the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the BBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional. These footprints provide insights into why different configurations of the same algorithm exhibit varying performance and identify the problem features influencing these outcomes. Our analysis uncovers shared behavioral patterns across configurations due to common interactions with problem properties, as well as distinct behaviors on the same problem driven by differing problem features. The results demonstrate the effectiveness of algorithm footprints in enhancing interpretability and guiding configuration choices.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02331v1",
    "published_date": "2025-07-03 05:48:58 UTC",
    "updated_date": "2025-07-03 05:48:58 UTC"
  },
  {
    "arxiv_id": "2507.02322v1",
    "title": "Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model",
    "authors": [
      "Farida Siddiqi Prity",
      "Mirza Raquib",
      "Saydul Akbar Murad",
      "Md. Jubayar Alam Rafi",
      "Md. Khairul Bashar Bhuiyan",
      "Anupam Kumar Bairagi"
    ],
    "abstract": "Rice leaf diseases significantly reduce productivity and cause economic losses, highlighting the need for early detection to enable effective management and improve yields. This study proposes Artificial Neural Network (ANN)-based image-processing techniques for timely classification and recognition of rice diseases. Despite the prevailing approach of directly inputting images of rice leaves into ANNs, there is a noticeable absence of thorough comparative analysis between the Feature Analysis Detection Model (FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it comes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs). Hence, this research presents initial experiments on the Feature Analysis Detection Model, utilizing various image Feature Extraction Algorithms, Dimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms (FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on datasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf scald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation method. A Direct Image-Centric Detection Model is established without the utilization of any FEA, and the evaluation of classification performance relies on different metrics. Ultimately, an exhaustive contrast is performed between the achievements of the Feature Analysis Detection Model and Direct Image-Centric Detection Model in classifying rice leaf diseases. The results reveal that the highest performance is attained using the Feature Analysis Detection Model. The adoption of the proposed Feature Analysis Detection Model for detecting rice leaf diseases holds excellent potential for improving crop health, minimizing yield losses, and enhancing overall productivity and sustainability of rice farming.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02322v1",
    "published_date": "2025-07-03 05:26:52 UTC",
    "updated_date": "2025-07-03 05:26:52 UTC"
  },
  {
    "arxiv_id": "2507.02319v1",
    "title": "Iterated belief revision: from postulates to abilities",
    "authors": [
      "Paolo Liberatore"
    ],
    "abstract": "The belief revision field is opulent in new proposals and indigent in analyses of existing approaches. Much work hinge on postulates, employed as syntactic characterizations: some revision mechanism is equivalent to some properties. Postulates constraint specific revision instances: certain revisions update certain beliefs in a certain way. As an example, if the revision is consistent with the current beliefs, it is incorporated with no other change. A postulate like this tells what revisions must do and neglect what they can do. Can they reach a certain state of beliefs? Can they reach all possible states of beliefs? Can they reach all possible states of beliefs from no previous belief? Can they reach a dogmatic state of beliefs, where everything not believed is impossible? Can they make two conditions equally believed? An application where every possible state of beliefs is sensible requires each state of beliefs to be reachable. An application where conditions may be equally believed requires such a belief state to be reachable. An application where beliefs may become dogmatic requires a way to make them dogmatic. Such doxastic states need to be reached in a way or another. Not in specific way, as dictated by a typical belief revision postulate. This is an ability, not a constraint: the ability of being plastic, equating, dogmatic. Amnesic, correcting, believer, damascan, learnable are other abilities. Each revision mechanism owns some of these abilities and lacks the others: lexicographic, natural, restrained, very radical, full meet, radical, severe, moderate severe, deep severe, plain severe and deep severe revisions, each of these revisions is proved to possess certain abilities.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02319v1",
    "published_date": "2025-07-03 05:11:41 UTC",
    "updated_date": "2025-07-03 05:11:41 UTC"
  },
  {
    "arxiv_id": "2507.02314v4",
    "title": "MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness",
    "authors": [
      "JaeHyuck Choi",
      "MinJun Kim",
      "Je Hyeong Hong"
    ],
    "abstract": "Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "46 pages, 47 figures. Code: https://github.com/SpatialAILab/MAGIC-Anomaly-generation",
    "pdf_url": "https://arxiv.org/pdf/2507.02314v4",
    "published_date": "2025-07-03 04:54:37 UTC",
    "updated_date": "2025-12-22 07:17:17 UTC"
  },
  {
    "arxiv_id": "2507.02310v1",
    "title": "Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment",
    "authors": [
      "Alif Ashrafee",
      "Jedrzej Kozal",
      "Michal Wozniak",
      "Bartosz Krawczyk"
    ],
    "abstract": "Traditional continual learning methods prioritize knowledge retention and focus primarily on mitigating catastrophic forgetting, implicitly assuming that the data distribution of previously learned tasks remains static. This overlooks the dynamic nature of real-world data streams, where concept drift permanently alters previously seen data and demands both stability and rapid adaptation.\n  We introduce a holistic framework for continual learning under concept drift that simulates realistic scenarios by evolving task distributions. As a baseline, we consider Full Relearning (FR), in which the model is retrained from scratch on newly labeled samples from the drifted distribution. While effective, this approach incurs substantial annotation and computational overhead. To address these limitations, we propose Adaptive Memory Realignment (AMR), a lightweight alternative that equips rehearsal-based learners with a drift-aware adaptation mechanism. AMR selectively removes outdated samples of drifted classes from the replay buffer and repopulates it with a small number of up-to-date instances, effectively realigning memory with the new distribution. This targeted resampling matches the performance of FR while reducing the need for labeled data and computation by orders of magnitude.\n  To enable reproducible evaluation, we introduce four concept-drift variants of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and Tiny-ImageNet-CD, where previously seen classes reappear with shifted representations. Comprehensive experiments on these datasets using several rehearsal-based baselines show that AMR consistently counters concept drift, maintaining high accuracy with minimal overhead. These results position AMR as a scalable solution that reconciles stability and plasticity in non-stationary continual learning environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02310v1",
    "published_date": "2025-07-03 04:41:20 UTC",
    "updated_date": "2025-07-03 04:41:20 UTC"
  },
  {
    "arxiv_id": "2507.02306v1",
    "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation",
    "authors": [
      "Ruican Zhong",
      "David W. McDonald",
      "Gary Hsieh"
    ],
    "abstract": "Usability evaluation is crucial in human-centered design but can be costly, requiring expert time and user compensation. In this work, we developed a method for synthetic heuristic evaluation using multimodal LLMs' ability to analyze images and provide design feedback. Comparing our synthetic evaluations to those by experienced UX practitioners across two apps, we found our evaluation identified 73% and 77% of usability issues, which exceeded the performance of 5 experienced human evaluators (57% and 63%). Compared to human evaluators, the synthetic evaluation's performance maintained consistent performance across tasks and excelled in detecting layout issues, highlighting potential attentional and perceptual strengths of synthetic evaluation. However, synthetic evaluation struggled with recognizing some UI components and design conventions, as well as identifying across screen violations. Additionally, testing synthetic evaluations over time and accounts revealed stable performance. Overall, our work highlights the performance differences between human and LLM-driven evaluations, informing the design of synthetic heuristic evaluations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02306v1",
    "published_date": "2025-07-03 04:27:16 UTC",
    "updated_date": "2025-07-03 04:27:16 UTC"
  },
  {
    "arxiv_id": "2507.02302v1",
    "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning",
    "authors": [
      "Dohoon Kim",
      "Donghun Kang",
      "Taesup Moon"
    ],
    "abstract": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 5 figures, ACL 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2507.02302v1",
    "published_date": "2025-07-03 04:13:01 UTC",
    "updated_date": "2025-07-03 04:13:01 UTC"
  },
  {
    "arxiv_id": "2507.02291v1",
    "title": "Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications",
    "authors": [
      "Zhaoyu Zhang",
      "Lingyi Wang",
      "Wei Wu",
      "Fuhui Zhou",
      "Qihui Wu"
    ],
    "abstract": "Data-driven semantic communication is based on superficial statistical patterns, thereby lacking interpretability and generalization, especially for applications with the presence of unseen data. To address these challenges, we propose a novel knowledge graph-enhanced zero-shot semantic communication (KGZS-SC) network. Guided by the structured semantic information from a knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides generalized semantic representations and enables reasoning for unseen cases. Specifically, the KG-SKB aligns the semantic features in a shared category semantics embedding space and enhances the generalization ability of the transmitter through aligned semantic features, thus reducing communication overhead by selectively transmitting compact visual semantics. At the receiver, zero-shot learning (ZSL) is leveraged to enable direct classification for unseen cases without the demand for retraining or additional computational overhead, thereby enhancing the adaptability and efficiency of the classification process in dynamic or resource-constrained environments. The simulation results conducted on the APY datasets show that the proposed KGZS-SC network exhibits robust generalization and significantly outperforms existing SC frameworks in classifying unseen categories across a range of SNR levels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02291v1",
    "published_date": "2025-07-03 03:57:26 UTC",
    "updated_date": "2025-07-03 03:57:26 UTC"
  },
  {
    "arxiv_id": "2507.02282v1",
    "title": "Content filtering methods for music recommendation: A review",
    "authors": [
      "Terence Zeng",
      "Abhishek K. Umrawal"
    ],
    "abstract": "Recommendation systems have become essential in modern music streaming platforms, shaping how users discover and engage with songs. One common approach in recommendation systems is collaborative filtering, which suggests content based on the preferences of users with similar listening patterns to the target user. However, this method is less effective on media where interactions are sparse. Music is one such medium, since the average user of a music streaming service will never listen to the vast majority of tracks. Due to this sparsity, there are several challenges that have to be addressed with other methods. This review examines the current state of research in addressing these challenges, with an emphasis on the role of content filtering in mitigating biases inherent in collaborative filtering approaches. We explore various methods of song classification for content filtering, including lyrical analysis using Large Language Models (LLMs) and audio signal processing techniques. Additionally, we discuss the potential conflicts between these different analysis methods and propose avenues for resolving such discrepancies.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages and 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02282v1",
    "published_date": "2025-07-03 03:44:20 UTC",
    "updated_date": "2025-07-03 03:44:20 UTC"
  },
  {
    "arxiv_id": "2507.02271v1",
    "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation",
    "authors": [
      "Feizhen Huang",
      "Yu Wu",
      "Yutian Lin",
      "Bo Du"
    ],
    "abstract": "Video-to-Audio (V2A) Generation achieves significant progress and plays a crucial role in film and video post-production. However, current methods overlook the cinematic language, a critical component of artistic expression in filmmaking. As a result, their performance deteriorates in scenarios where Foley targets are only partially visible. To address this challenge, we propose a simple self-distillation approach to extend V2A models to cinematic language scenarios. By simulating the cinematic language variations, the student model learns to align the video features of training pairs with the same audio-visual correspondences, enabling it to effectively capture the associations between sounds and partial visual information. Our method not only achieves impressive improvements under partial visibility across all evaluation metrics, but also enhances performance on the large-scale V2A dataset, VGGSound.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02271v1",
    "published_date": "2025-07-03 03:23:11 UTC",
    "updated_date": "2025-07-03 03:23:11 UTC"
  },
  {
    "arxiv_id": "2507.03036v1",
    "title": "Adaptive Cubic Regularized Second-Order Latent Factor Analysis Model",
    "authors": [
      "Jialiang Wang",
      "Junzhou Wang",
      "Xin Liao"
    ],
    "abstract": "High-dimensional and incomplete (HDI) data, characterized by massive node interactions, have become ubiquitous across various real-world applications. Second-order latent factor models have shown promising performance in modeling this type of data. Nevertheless, due to the bilinear and non-convex nature of the SLF model's objective function, incorporating a damping term into the Hessian approximation and carefully tuning associated parameters become essential. To overcome these challenges, we propose a new approach in this study, named the adaptive cubic regularized second-order latent factor analysis (ACRSLF) model. The proposed ACRSLF adopts the two-fold ideas: 1) self-tuning cubic regularization that dynamically mitigates non-convex optimization instabilities; 2) multi-Hessian-vector product evaluation during conjugate gradient iterations for precise second-order information assimilation. Comprehensive experiments on two industrial HDI datasets demonstrate that the ACRSLF converges faster and achieves higher representation accuracy than the advancing optimizer-based LFA models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03036v1",
    "published_date": "2025-07-03 03:15:54 UTC",
    "updated_date": "2025-07-03 03:15:54 UTC"
  },
  {
    "arxiv_id": "2507.02265v1",
    "title": "Multi-Label Classification Framework for Hurricane Damage Assessment",
    "authors": [
      "Zhangding Liu",
      "Neda Mohammadi",
      "John E. Taylor"
    ],
    "abstract": "Hurricanes cause widespread destruction, resulting in diverse damage types and severities that require timely and accurate assessment for effective disaster response. While traditional single-label classification methods fall short of capturing the complexity of post-hurricane damage, this study introduces a novel multi-label classification framework for assessing damage using aerial imagery. The proposed approach integrates a feature extraction module based on ResNet and a class-specific attention mechanism to identify multiple damage types within a single image. Using the Rescuenet dataset from Hurricane Michael, the proposed method achieves a mean average precision of 90.23%, outperforming existing baseline methods. This framework enhances post-hurricane damage assessment, enabling more targeted and efficient disaster response and contributing to future strategies for disaster mitigation and resilience. This paper has been accepted at the ASCE International Conference on Computing in Civil Engineering (i3CE 2025), and the camera-ready version will appear in the official conference proceedings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 3 figures. Accepted at the ASCE International Conference on Computing in Civil Engineering (i3CE 2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.02265v1",
    "published_date": "2025-07-03 03:15:23 UTC",
    "updated_date": "2025-07-03 03:15:23 UTC"
  },
  {
    "arxiv_id": "2507.02259v1",
    "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent",
    "authors": [
      "Hongli Yu",
      "Tinghong Chen",
      "Jiangtao Feng",
      "Jiangjie Chen",
      "Weinan Dai",
      "Qiying Yu",
      "Ya-Qin Zhang",
      "Wei-Ying Ma",
      "Jingjing Liu",
      "Mingxuan Wang",
      "Hao Zhou"
    ],
    "abstract": "Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss < 5% and achieves 95%+ in 512K RULER test.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://memagent-sialab.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2507.02259v1",
    "published_date": "2025-07-03 03:11:50 UTC",
    "updated_date": "2025-07-03 03:11:50 UTC"
  },
  {
    "arxiv_id": "2507.02253v6",
    "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation",
    "authors": [
      "Jungkoo Kang"
    ],
    "abstract": "Robust workflow composition is critical for effective agent performance, yet progress in Large Language Model (LLM) planning and reasoning is hindered by a scarcity of scalable evaluation data. This work introduces NL2Flow, a fully automated pipeline for generating and evaluating workflow planning problems. NL2Flow generates problems parametrically in a structured intermediate representation, translating them into both natural language and formal PDDL. I evaluate several open-source, instruct-tuned LLMs on a dataset of 2296 low-difficulty problems generated by NL2Flow. Results demonstrate that the best-performing model achieved 86% success in generating valid plans and 69% in generating optimal plans (for solvable problems). Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Importantly, translating natural language problems into a structured JSON representation prior to symbolic planning significantly improved success rates, suggesting a benefit from neuro-symbolic integration. These findings underscore the importance of understanding error sources within LLM reasoning as systems scale to more complex tasks. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.02253v6",
    "published_date": "2025-07-03 03:02:49 UTC",
    "updated_date": "2025-10-14 18:19:05 UTC"
  },
  {
    "arxiv_id": "2507.02252v1",
    "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement",
    "authors": [
      "Zeyu Lei",
      "Hongyuan Yu",
      "Jinlin Wu",
      "Zhen Chen"
    ],
    "abstract": "Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02252v1",
    "published_date": "2025-07-03 03:00:26 UTC",
    "updated_date": "2025-07-03 03:00:26 UTC"
  },
  {
    "arxiv_id": "2507.03034v4",
    "title": "Rethinking Data Protection in the (Generative) Artificial Intelligence Era",
    "authors": [
      "Yiming Li",
      "Shuo Shao",
      "Yu He",
      "Junfeng Guo",
      "Tianwei Zhang",
      "Zhan Qin",
      "Pin-Yu Chen",
      "Michael Backes",
      "Philip Torr",
      "Dacheng Tao",
      "Kui Ren"
    ],
    "abstract": "The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Perspective paper for a broader scientific audience. The first two authors contributed equally to this paper. 13 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03034v4",
    "published_date": "2025-07-03 02:45:51 UTC",
    "updated_date": "2025-09-03 06:34:12 UTC"
  },
  {
    "arxiv_id": "2507.02244v2",
    "title": "Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies",
    "authors": [
      "Fangzhou Shi",
      "Xiaopeng Ke",
      "Xinye Xiong",
      "Kexin Meng",
      "Chang Men",
      "Zhengdan Zhu"
    ],
    "abstract": "The proliferation of ride-hailing aggregator platforms presents significant growth opportunities for ride-service providers by increasing order volume and gross merchandise value (GMV). On most ride-hailing aggregator platforms, service providers that offer lower fares are ranked higher in listings and, consequently, are more likely to be selected by passengers. This competitive ranking mechanism creates a strong incentive for service providers to adopt coupon strategies that lower prices to secure a greater number of orders, as order volume directly influences their long-term viability and sustainability. Thus, designing an effective coupon strategy that can dynamically adapt to market fluctuations while optimizing order acquisition under budget constraints is a critical research challenge. However, existing studies in this area remain scarce.\n  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based subsidy strategy framework designed to rapidly adapt to competitors' pricing adjustments. Our approach integrates two key techniques: Fast Competition Adaptation (FCA), which enables swift responses to dynamic price changes, and Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget constraints while optimizing coupon decisions on new price landscape. Furthermore, we introduce RideGym, the first dedicated simulation environment tailored for ride-hailing aggregators, facilitating comprehensive evaluation and benchmarking of different pricing strategies without compromising real-world operational efficiency. Experimental results demonstrate that our proposed method consistently outperforms baseline approaches across diverse market conditions, highlighting its effectiveness in subsidy optimization for ride-hailing service providers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02244v2",
    "published_date": "2025-07-03 02:38:42 UTC",
    "updated_date": "2025-07-04 03:27:45 UTC"
  },
  {
    "arxiv_id": "2507.03033v1",
    "title": "Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation",
    "authors": [
      "Johnson Thomas",
      "Ayush Mudgal",
      "Wendao Liu",
      "Nisten Tahiraj",
      "Zeeshaan Mohammed",
      "Dhruv Diddi"
    ],
    "abstract": "Background: Clinical documentation represents a significant burden for healthcare providers, with physicians spending up to 2 hours daily on administrative tasks. Recent advances in large language models (LLMs) offer promising solutions, but privacy concerns and computational requirements limit their adoption in healthcare settings. Objective: To develop and evaluate a privacy-preserving, on-device medical transcription system using a fine-tuned Llama 3.2 1B model capable of generating structured medical notes from medical transcriptions while maintaining complete data sovereignty entirely in the browser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical transcription-to-structured note pairs. The model was evaluated against the base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140 modified ACI benchmark cases. Evaluation employed both statistical metrics (ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple clinical quality dimensions. Results: The fine-tuned OnDevice model demonstrated substantial improvements over the base model. On the ACI benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1 improved from 0.832 to 0.866. Clinical quality assessments showed marked reduction in major hallucinations (from 85 to 35 cases) and enhanced factual correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed on the internal evaluation dataset, with composite scores increasing from 3.13 to 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical transcription yields clinically meaningful improvements while enabling complete on-device browser deployment. This approach addresses key barriers to AI adoption in healthcare: privacy preservation, cost reduction, and accessibility for resource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.03033v1",
    "published_date": "2025-07-03 01:51:49 UTC",
    "updated_date": "2025-07-03 01:51:49 UTC"
  },
  {
    "arxiv_id": "2507.05269v3",
    "title": "CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks",
    "authors": [
      "Danning Xie",
      "Mingwei Zheng",
      "Xuwei Liu",
      "Jiannan Wang",
      "Chengpeng Wang",
      "Lin Tan",
      "Xiangyu Zhang"
    ],
    "abstract": "Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models' ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs' code reasoning capabilities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "NeurIPS 2025 Datasets & Benchmarks Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2507.05269v3",
    "published_date": "2025-07-03 01:35:58 UTC",
    "updated_date": "2026-01-17 19:21:17 UTC"
  },
  {
    "arxiv_id": "2507.03031v1",
    "title": "On the Mathematical Impossibility of Safe Universal Approximators",
    "authors": [
      "Jasper Yao"
    ],
    "abstract": "We establish fundamental mathematical limits on universal approximation theorem (UAT) system alignment by proving that catastrophic failures are an inescapable feature of any useful computational system. Our central thesis is that for any universal approximator, the expressive power required for useful computation is inextricably linked to a dense set of instabilities that make perfect, reliable control a mathematical impossibility. We prove this through a three-level argument that leaves no escape routes for any class of universal approximator architecture. i) Combinatorial Necessity: For the vast majority of practical universal approximators (e.g., those using ReLU activations), we prove that the density of catastrophic failure points is directly proportional to the network's expressive power. ii) Topological Necessity: For any theoretical universal approximator, we use singularity theory to prove that the ability to approximate generic functions requires the ability to implement the dense, catastrophic singularities that characterize them. iii) Empirical Necessity: We prove that the universal existence of adversarial examples is empirical evidence that real-world tasks are themselves catastrophic, forcing any successful model to learn and replicate these instabilities. These results, combined with a quantitative \"Impossibility Sandwich\" showing that the minimum complexity for usefulness exceeds the maximum complexity for safety, demonstrate that perfect alignment is not an engineering challenge but a mathematical impossibility. This foundational result reframes UAT safety from a problem of \"how to achieve perfect control\" to one of \"how to operate safely in the presence of irreducible uncontrollability,\" with profound implications for the future of UAT development and governance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.03031v1",
    "published_date": "2025-07-03 01:05:24 UTC",
    "updated_date": "2025-07-03 01:05:24 UTC"
  },
  {
    "arxiv_id": "2507.02217v1",
    "title": "Understanding Trade offs When Conditioning Synthetic Data",
    "authors": [
      "Brandon Trabucco",
      "Qasim Wani",
      "Benjamin Pikus",
      "Vasu Sharma"
    ],
    "abstract": "Learning robust object detectors from only a handful of images is a critical challenge in industrial vision systems, where collecting high quality training data can take months. Synthetic data has emerged as a key solution for data efficient visual inspection and pick and place robotics. Current pipelines rely on 3D engines such as Blender or Unreal, which offer fine control but still require weeks to render a small dataset, and the resulting images often suffer from a large gap between simulation and reality. Diffusion models promise a step change because they can generate high quality images in minutes, yet precise control, especially in low data regimes, remains difficult. Although many adapters now extend diffusion beyond plain text prompts, the effect of different conditioning schemes on synthetic data quality is poorly understood. We study eighty diverse visual concepts drawn from four standard object detection benchmarks and compare two conditioning strategies: prompt based and layout based. When the set of conditioning cues is narrow, prompt conditioning yields higher quality synthetic data; as diversity grows, layout conditioning becomes superior. When layout cues match the full training distribution, synthetic data raises mean average precision by an average of thirty four percent and by as much as one hundred seventy seven percent compared with using real data alone.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02217v1",
    "published_date": "2025-07-03 00:44:31 UTC",
    "updated_date": "2025-07-03 00:44:31 UTC"
  },
  {
    "arxiv_id": "2507.02211v2",
    "title": "Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning",
    "authors": [
      "Gustavo C. Mangold",
      "Heitor C. M. Fernandes",
      "Mendeli H. Vainstein"
    ],
    "abstract": "Recent studies in the spatial prisoner's dilemma games with reinforcement learning have shown that static agents can learn to cooperate through a diverse sort of mechanisms, including noise injection, different types of learning algorithms and neighbours' payoff knowledge. In this work, using an independent multi-agent Q-learning algorithm, we study the effects of dilution and mobility in the spatial version of the prisoner's dilemma. Within this setting, different possible actions for the algorithm are defined, connecting with previous results on the classical, non-reinforcement learning spatial prisoner's dilemma, showcasing the versatility of the algorithm in modeling different game-theoretical scenarios and the benchmarking potential of this approach. As a result, a range of effects is observed, including evidence that games with fixed update rules can be qualitatively equivalent to those with learned ones, as well as the emergence of a symbiotic mutualistic effect between populations that forms when multiple actions are defined.",
    "categories": [
      "cs.AI",
      "cs.NE",
      "physics.comp-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "Re-submitted to fix typos",
    "pdf_url": "https://arxiv.org/pdf/2507.02211v2",
    "published_date": "2025-07-03 00:17:53 UTC",
    "updated_date": "2025-07-04 13:32:01 UTC"
  },
  {
    "arxiv_id": "2507.02206v1",
    "title": "EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer",
    "authors": [
      "Ranyang Zhou",
      "Abeer Matar A. Almalky",
      "Gamana Aragonda",
      "Sabbir Ahmed",
      "Filip Roth Trønnes-Christensen",
      "Adnan Siraj Rakin",
      "Shaahin Angizi"
    ],
    "abstract": "True Random Number Generators (TRNGs) play a fundamental role in hardware security, cryptographic systems, and data protection. In the context of Deep NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is critical to ensure the integrity, privacy, and intel-lectual property of AI systems. While software-based pseudo-random number generators are widely used, they lack the unpredictability and resilience offered by hardware-based TRNGs. In this work, we propose a novel and robust Encoding-in-Memory TRNG called EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior, particularly under RowHammer-induced disturbances, for the first time. We demonstrate how the unpredictable bit-flips generated through carefully controlled RowHammer operations can be harnessed as a reliable entropy source. Furthermore, we apply this TRNG framework to secure DNN weight data by encoding via a combination of fixed and unpredictable bit-flips. The encrypted data is later decrypted using a key derived from the probabilistic flip behavior, ensuring both data confidentiality and model authenticity. Our results validate the effectiveness of DRAM-based entropy extraction for robust, low-cost hardware security and offer a promising direction for protecting machine learning models at the hardware level.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02206v1",
    "published_date": "2025-07-03 00:01:33 UTC",
    "updated_date": "2025-07-03 00:01:33 UTC"
  }
]
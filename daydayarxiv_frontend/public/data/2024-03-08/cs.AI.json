{
  "date": "2024-03-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-08 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 生成模型、LLM 在多模态理解和应用中的创新、强化学习优化，以及医疗和图像处理领域的进展，其中最令人印象深刻的是 Google 团队发布的 Gemini 1.5 多模态模型，以及 LLM 在对话和决策中的鲁棒性提升；这些论文突显了 LLM 在处理复杂任务（如文本到视频生成和药物响应预测）的潜力。\n\n下面，我将逐一简要概述今天的论文，先优先讨论重要、话题性和有影响力的工作（如 LLM 和多模态生成），然后快速掠过其他较常规的论文。每个条目包括论文标题（中文 + 英文）、核心学术术语，并清晰描述主要贡献和发现。\n\n### 重点论文讨论\n\n1. **Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context**  \n   这篇由 Google 的 Gemini 团队（包括 Demis Hassabis 和 Jeff Dean 等知名学者）发布的论文是今日亮点。论文提出 Gemini 1.5 系列模型，支持处理数百万 token 的多模态上下文，实现近乎完美的召回和推理。主要贡献：在长文档 QA、视频 QA 和 ASR 等任务上超越现有模型，证明了 LLM 在多模态理解的潜力；实验显示模型在真实应用中可节省专业任务时间 26-75%，并能学习小众语言翻译。\n\n2. **Conservative DDPG -- Pessimistic RL without Ensemble**  \n   论文作者包括 Shie Mannor。核心术语：DDPG（Deep Deterministic Policy Gradient）、Q-estimates。主要贡献：提出一种简单高效的保守 DDPG 方法，使用 Q-target 和行为克隆损失来减少过估计偏差，无需集成模型；在 MuJoCo 和 Bullet 任务中，性能优于传统 DDPG 和 TD3，显著降低计算需求。\n\n3. **Text-to-Audio Generation Synchronized with Videos**  \n   论文聚焦多模态生成。核心术语：TTA（Text-to-Audio）、Latent Diffusion Models。主要贡献：开发 T2AV 模型，通过视觉对齐文本嵌入和时序注意机制生成与视频同步的音频；实验在 AudioCaps 数据集上提升了视觉对齐和时序一致性，解决现有方法音频-视频失配问题。\n\n4. **How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation Framework for Analyses**  \n   作者包括 Zhiyong Lu。核心术语：LLMs（Large Language Models）、GPTRadScore。主要贡献：提出 GPTRadScore 框架，使用 GPT-4 分解评估 LLMs 在 CT 扫描描述中的准确性（如身体部位和位置）；实验显示 GPT-4 Turbo 优于其他模型，并在临床验证中提升决策支持。\n\n5. **Unfamiliar Finetuning Examples Control How Language Models Hallucinate**  \n   核心术语：RLHF（Reinforcement Learning from Human Feedback）、LLM 幻觉。主要贡献：探索 LLM 在药物响应预测中的幻觉问题，通过合成数据和强化学习减少错误；实验证明，优化微调样本可提升长文本生成的事实性，RL 方法在生物医学任务中表现出色。\n\n6. **Will GPT-4 Run DOOM?**  \n   作者 Adrian de Wynter。核心术语：LLM 推理、强化学习。主要贡献：展示 GPT-4 可通过文本描述玩 DOOM 游戏，实现路径规划和交互；这揭示了 LLM 在非训练环境下的泛化潜力，但也强调了其局限性。\n\n7. **Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations**  \n   核心术语：LLM 校准、上下文规范。主要贡献：提出 Alignment Studio 框架，使用 Framers、Instructors 和 Auditors 模块让 LLM 适应特定规则（如企业指南）；实验显示，该方法提升了 LLM 在受限场景中的行为控制。\n\n### 相关论文简要概述\n接下来，快速聊聊其他相关主题的论文，这些多聚焦 LLM 和 AI 应用，但篇幅有限，仅提核心点。\n\n8. **A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health**  \n   核心术语：LLMs 在心理健康、对话评估。主要贡献：开发框架评估 LLM 在心理对话中的表现，GPT-4 Turbo 在相关主题上与专业治疗师高度相关。\n\n9. **Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?**  \n   核心术语：LLM 与 HRI（Human-Robot Interaction）。主要贡献：测试 LLM 在社交决策中的一致性，GPT-4 与人类直觉高度相关，但视觉模型表现较差。\n\n10. **Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach**  \n    核心术语：LLM 元认知、自我修正。主要贡献：提出 CLEAR 框架，让 LLM 自我识别和修正错误，提升部署可靠性。\n\n其他论文如第14、15、28、29、31、37、44、45、46、51、57、62、63、66、67、68、69、70、71、72、73、74、75、76、77、78、79、80、81、82、83、84、85、86、87、88、89、90、91、92 篇，涉及强化学习、图像生成和医疗诊断等领域，但这些工作相对常规（如特定任务优化或数据集构建），未有突破性创新，因此快速掠过。例如，第17篇优化了神经网络在 OTFS 系统中的检测，第21篇提出 GEAR 框架压缩 KV 缓存以加速 LLM 推理，第22篇开发 DeepSeek-VL 用于视觉语言理解。这些论文的贡献主要在于技术细化，但未像上述重点论文那样引发广泛话题。\n\n总之，今天的 arXiv 展示了 AI 领域的快速迭代，LLM 的多模态和鲁棒性应用尤其值得关注。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2403.05732v2",
      "title": "Conservative DDPG -- Pessimistic RL without Ensemble",
      "title_zh": "翻译失败",
      "authors": [
        "Nitsan Soffair",
        "Shie Mannor"
      ],
      "abstract": "DDPG is hindered by the overestimation bias problem, wherein its\n$Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to\nthis bias involve ensemble-based methods, which require significant\ncomputational resources, or complex log-policy-based approaches, which are\ndifficult to understand and implement. In contrast, we propose a\nstraightforward solution using a $Q$-target and incorporating a behavioral\ncloning (BC) loss penalty. This solution, acting as an uncertainty measure, can\nbe easily implemented with minimal code and without the need for an ensemble.\nOur empirical findings strongly support the superiority of Conservative DDPG\nover DDPG across various MuJoCo and Bullet tasks. We consistently observe\nbetter performance in all evaluated tasks and even competitive or superior\nperformance compared to TD3 and TD7, all achieved with significantly reduced\ncomputational requirements.",
      "tldr_zh": "该研究针对 DDPG 算法的过估计偏差（overestimation bias）问题，提出了一种名为 Conservative DDPG 的新方法，该方法使用 Q-target 结合行为克隆（behavioral cloning, BC）损失惩罚作为不确定性测量，而无需依赖集合（ensemble）。这种方法简单易实现，仅需最小代码修改，就显著降低了计算资源需求。实验结果显示，Conservative DDPG 在各种 MuJoCo 和 Bullet 任务中优于原始 DDPG，并在所有评估任务中表现出色，甚至与 TD3 和 TD7 相当或更优。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper do not ready",
      "pdf_url": "http://arxiv.org/pdf/2403.05732v2",
      "published_date": "2024-03-08 23:59:38 UTC",
      "updated_date": "2024-06-02 19:40:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:34:22.656470"
    },
    {
      "arxiv_id": "2403.09705v1",
      "title": "A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Marrapese",
        "Basem Suleiman",
        "Imdad Ullah",
        "Juno Kim"
      ],
      "abstract": "Understanding the conversation abilities of Large Language Models (LLMs) can\nhelp lead to its more cautious and appropriate deployment. This is especially\nimportant for safety-critical domains like mental health, where someone's life\nmay depend on the exact wording of a response to an urgent question. In this\npaper, we propose a novel framework for evaluating the nuanced conversation\nabilities of LLMs. Within it, we develop a series of quantitative metrics\ndeveloped from literature on using psychotherapy conversation analysis\nliterature. While we ensure that our framework and metrics are transferable by\nresearchers to relevant adjacent domains, we apply them to the mental health\nfield. We use our framework to evaluate several popular frontier LLMs,\nincluding some GPT and Llama models, through a verified mental health dataset.\nOur results show that GPT4 Turbo can perform significantly more similarly to\nverified therapists than other selected LLMs. We conduct additional analysis to\nexamine how LLM conversation performance varies across specific mental health\ntopics. Our results indicate that GPT4 Turbo performs well in achieving high\ncorrelation with verified therapists in particular topics such as Parenting and\nRelationships. We believe our contributions will help researchers develop\nbetter LLMs that, in turn, will more positively support people's lives.",
      "tldr_zh": "本文提出一个新颖的框架，用于评估大型语言模型 (LLMs) 在心理健康领域的细微对话能力，强调其在安全关键领域的部署重要性。框架基于心理治疗对话分析文献，开发了一系列量化指标，并通过验证的心理健康数据集评估了多种 LLMs，如 GPT 和 Llama 模型。结果显示，GPT4 Turbo 与验证治疗师的表现最相似，尤其在育儿和关系等特定主题上表现出高相关性。该框架有助于研究人员开发更可靠的 LLMs，从而更好地支持人们的心理健康。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.09705v1",
      "published_date": "2024-03-08 23:46:37 UTC",
      "updated_date": "2024-03-08 23:46:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:34:34.305065"
    },
    {
      "arxiv_id": "2403.05720v5",
      "title": "A dataset and benchmark for hospital course summarization with adapted large language models",
      "title_zh": "翻译失败",
      "authors": [
        "Asad Aali",
        "Dave Van Veen",
        "Yamin Ishraq Arefeen",
        "Jason Hom",
        "Christian Bluethgen",
        "Eduardo Pontes Reis",
        "Sergios Gatidis",
        "Namuun Clifford",
        "Joseph Daws",
        "Arash S. Tehrani",
        "Jangwon Kim",
        "Akshay S. Chaudhari"
      ],
      "abstract": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\nUsing clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.",
      "tldr_zh": "这篇论文引入了MIMIC-IV-BHC数据集和基准，用于评估大型语言模型(LLMs)生成简要住院课程(BHC)总结的能力。研究者采用in-context learning和fine-tuning策略，适应了多种模型，包括Clinical-T5-Large、Llama2-13B、FLAN-UL2、GPT-3.5和GPT-4，并使用BLEU和BERT-Score等指标进行定量评估。结果显示，fine-tuned Llama2-13B在定量指标上优于其他模型，而GPT-4 with in-context learning对输入上下文长度更鲁棒；临床读者研究进一步表明，GPT-4生成的总结在质量和临床决策支持方面更受欢迎，突出了定性评估的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05720v5",
      "published_date": "2024-03-08 23:17:55 UTC",
      "updated_date": "2025-04-23 02:16:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:34:48.059846"
    },
    {
      "arxiv_id": "2403.05715v1",
      "title": "A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Dave",
        "Heeseung Bang",
        "Andreas A. Malikopoulos"
      ],
      "abstract": "Many cyber-physical-human systems (CPHS) involve a human decision-maker who\nmay receive recommendations from an artificial intelligence (AI) platform while\nholding the ultimate responsibility of making decisions. In such CPHS\napplications, the human decision-maker may depart from an optimal recommended\ndecision and instead implement a different one for various reasons. In this\nletter, we develop a rigorous framework to overcome this challenge. In our\nframework, we consider that humans may deviate from AI recommendations as they\nperceive and interpret the system's state in a different way than the AI\nplatform. We establish the structural properties of optimal recommendation\nstrategies and develop an approximate human model (AHM) used by the AI. We\nprovide theoretical bounds on the optimality gap that arises from an AHM and\nillustrate the efficacy of our results in a numerical example.",
      "tldr_zh": "该论文提出一个框架，用于提升人工智能（AI）在 cyber-physical-human systems (CPHS) 中的推荐效果，以应对人类决策者可能偏离AI建议的情况。框架考虑了人类对系统状态的感知差异，建立了最佳推荐策略的结构属性，并引入了 approximate human model (AHM) 作为AI的辅助模型。研究提供了AHM导致的最优性差距理论界限，并在数值例子中验证了框架的有效性，从而改善了CPHS中的决策可靠性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05715v1",
      "published_date": "2024-03-08 23:02:20 UTC",
      "updated_date": "2024-03-08 23:02:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:34:57.595858"
    },
    {
      "arxiv_id": "2403.07938v1",
      "title": "Text-to-Audio Generation Synchronized with Videos",
      "title_zh": "与视频同步的文本到音频生成",
      "authors": [
        "Shentong Mo",
        "Jing Shi",
        "Yapeng Tian"
      ],
      "abstract": "In recent times, the focus on text-to-audio (TTA) generation has intensified,\nas researchers strive to synthesize audio from textual descriptions. However,\nmost existing methods, though leveraging latent diffusion models to learn the\ncorrelation between audio and text embeddings, fall short when it comes to\nmaintaining a seamless synchronization between the produced audio and its\nvideo. This often results in discernible audio-visual mismatches. To bridge\nthis gap, we introduce a groundbreaking benchmark for Text-to-Audio generation\nthat aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself\nwith three novel metrics dedicated to evaluating visual alignment and temporal\nconsistency. To complement this, we also present a simple yet effective\nvideo-aligned TTA generation model, namely T2AV. Moving beyond traditional\nmethods, T2AV refines the latent diffusion approach by integrating\nvisual-aligned text embeddings as its conditional foundation. It employs a\ntemporal multi-head attention transformer to extract and understand temporal\nnuances from video data, a feat amplified by our Audio-Visual ControlNet that\nadeptly merges temporal visual representations with text embeddings. Further\nenhancing this integration, we weave in a contrastive learning objective,\ndesigned to ensure that the visual-aligned text embeddings resonate closely\nwith the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench\ndemonstrate that our T2AV sets a new standard for video-aligned TTA generation\nin ensuring visual alignment and temporal consistency.",
      "tldr_zh": "该论文针对现有文本到音频（TTA）生成方法在音频与视频同步方面的不足，引入了新的基准 T2AV-Bench，该基准包含三个专用指标，用于评估视觉对齐和时间一致性。作者提出了一种简单有效的模型 T2AV，通过改进潜在扩散模型，整合视觉对齐的文本嵌入、时间多头注意力 transformer 和 Audio-Visual ControlNet，并加入对比学习目标来增强音频特征与视频表示的关联。实验在 AudioCaps 和 T2AV-Bench 上表明，T2AV 显著提升了视觉对齐和时间一致性，设定了新标准。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "arXiv admin note: text overlap with arXiv:2305.12903",
      "pdf_url": "http://arxiv.org/pdf/2403.07938v1",
      "published_date": "2024-03-08 22:27:38 UTC",
      "updated_date": "2024-03-08 22:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:35:10.591977"
    },
    {
      "arxiv_id": "2403.05701v2",
      "title": "Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?",
      "title_zh": "翻译失败",
      "authors": [
        "Lennart Wachowiak",
        "Andrew Coles",
        "Oya Celiktutan",
        "Gerard Canal"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in robotics, especially\nfor high-level action planning. Meanwhile, many robotics applications involve\nhuman supervisors or collaborators. Hence, it is crucial for LLMs to generate\nsocially acceptable actions that align with people's preferences and values. In\nthis work, we test whether LLMs capture people's intuitions about behavior\njudgments and communication preferences in human-robot interaction (HRI)\nscenarios. For evaluation, we reproduce three HRI user studies, comparing the\noutput of LLMs with that of real participants. We find that GPT-4 strongly\noutperforms other models, generating answers that correlate strongly with\nusers' answers in two studies $\\unicode{x2014}$ the first study dealing with\nselecting the most appropriate communicative act for a robot in various\nsituations ($r_s$ = 0.82), and the second with judging the desirability,\nintentionality, and surprisingness of behavior ($r_s$ = 0.83). However, for the\nlast study, testing whether people judge the behavior of robots and humans\ndifferently, no model achieves strong correlations. Moreover, we show that\nvision models fail to capture the essence of video stimuli and that LLMs tend\nto rate different communicative acts and behavior desirability higher than\npeople.",
      "tldr_zh": "本研究评估大型语言模型 (LLMs) 是否能捕捉人们在人机交互 (HRI) 场景中的社会直觉，特别是行为判断和沟通偏好。研究方法包括复制三个 HRI 用户研究，将 LLMs 的输出与真实参与者答案进行比较，使用 Spearman 相关系数 (r_s) 评估相关性。结果显示，GPT-4 在两个研究中表现出色，与用户答案的相关性分别为 0.82（沟通行为选择）和 0.83（行为可取性、意图和惊奇性），但在判断机器人与人类行为差异的研究中，无模型达到强相关。此外，视觉模型无法有效处理视频刺激，且 LLMs 倾向于对沟通行为和行为可取性给出比人类更高的评分，揭示了 LLMs 在社会可接受性方面的局限性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS), 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05701v2",
      "published_date": "2024-03-08 22:23:23 UTC",
      "updated_date": "2024-07-09 11:27:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:35:22.417742"
    },
    {
      "arxiv_id": "2406.16868v1",
      "title": "Neural Network-based Two-Dimensional Filtering for OTFS Symbol Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarui Xu",
        "Karim Said",
        "Lizhong Zheng",
        "Lingjia Liu"
      ],
      "abstract": "Orthogonal time frequency space (OTFS) is a promising modulation scheme for\nwireless communication in high-mobility scenarios. Recently, a reservoir\ncomputing (RC) based approach has been introduced for online subframe-based\nsymbol detection in the OTFS system, where only the limited over-the-air (OTA)\npilot symbols are utilized for training. However, the previous RC-based\napproach does not design the RC architecture based on the properties of the\nOTFS system to fully unlock the potential of RC. This paper introduces a novel\ntwo-dimensional RC (2D-RC) approach for online symbol detection on a subframe\nbasis in the OTFS system. The 2D-RC is designed to have a two-dimensional (2D)\nfiltering structure to equalize the 2D circular channel effect in the\ndelay-Doppler (DD) domain of the OTFS system. With the introduced architecture,\nthe 2D-RC can operate in the DD domain with only a single neural network,\nunlike our previous work which requires multiple RCs to track channel\nvariations in the time domain. Experimental results demonstrate the advantages\nof the 2D-RC approach over the previous RC-based approach and the compared\nmodel-based methods across different modulation orders.",
      "tldr_zh": "本文提出了一种基于神经网络的二维 Reservoir Computing (2D-RC) 方法，用于 Orthogonal Time Frequency Space (OTFS) 系统中的在线子帧符号检测，以解决现有 RC 方法未充分利用 OTFS 特性的问题。2D-RC 通过二维过滤结构均衡延迟-多普勒 (DD) 域中的循环信道效应，仅需一个神经网络在 DD 域操作，而非多个 RC 跟踪时域变化。实验结果显示，该方法在不同调制阶数上优于之前的 RC 方法和基于模型的方法，证明了其在高移动场景无线通信中的优势。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "6 pages, conference paper. arXiv admin note: substantial text overlap\n  with arXiv:2311.08543",
      "pdf_url": "http://arxiv.org/pdf/2406.16868v1",
      "published_date": "2024-03-08 21:33:41 UTC",
      "updated_date": "2024-03-08 21:33:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:35:34.684872"
    },
    {
      "arxiv_id": "2403.05683v1",
      "title": "Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sanket Shah",
        "Arun Suggala",
        "Milind Tambe",
        "Aparna Taneja"
      ],
      "abstract": "The declining participation of beneficiaries over time is a key concern in\npublic health programs. A popular strategy for improving retention is to have\nhealth workers `intervene' on beneficiaries at risk of dropping out. However,\nthe availability and time of these health workers are limited resources. As a\nresult, there has been a line of research on optimizing these limited\nintervention resources using Restless Multi-Armed Bandits (RMABs). The key\ntechnical barrier to using this framework in practice lies in the need to\nestimate the beneficiaries' RMAB parameters from historical data. Recent\nresearch has shown that Decision-Focused Learning (DFL), which focuses on\nmaximizing the beneficiaries' adherence rather than predictive accuracy,\nimproves the performance of intervention targeting using RMABs. Unfortunately,\nthese gains come at a high computational cost because of the need to solve and\nevaluate the RMAB in each DFL training step. In this paper, we provide a\nprincipled way to exploit the structure of RMABs to speed up intervention\nplanning by cleverly decoupling the planning for different beneficiaries. We\nuse real-world data from an Indian NGO, ARMMAN, to show that our approach is up\nto two orders of magnitude faster than the state-of-the-art approach while also\nyielding superior model performance. This would enable the NGO to scale up\ndeployments using DFL to potentially millions of mothers, ultimately advancing\nprogress toward UNSDG 3.1.",
      "tldr_zh": "该研究针对公共卫生程序中受益者参与度下降的问题，提出了一种基于分解的决策焦点学习(Decomposition-Based Decision-Focused Learning)方法，以优化健康工作者的有限干预资源。传统方法依赖Restless Multi-Armed Bandits (RMABs)框架，但计算成本高昂；本文通过巧妙解耦不同受益者的规划，显著加速干预决策过程。使用印度NGO ARMMAN的真实数据，实验显示该方法比现有方法快两个数量级，同时提升模型性能。这有助于大规模部署干预计划，支持联合国可持续发展目标(UNSDG 3.1)，潜在服务数百万母亲。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.05683v1",
      "published_date": "2024-03-08 21:31:00 UTC",
      "updated_date": "2024-03-08 21:31:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:35:46.250221"
    },
    {
      "arxiv_id": "2403.09704v1",
      "title": "Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations",
      "title_zh": "翻译失败",
      "authors": [
        "Swapnaja Achintalwar",
        "Ioana Baldini",
        "Djallel Bouneffouf",
        "Joan Byamugisha",
        "Maria Chang",
        "Pierre Dognin",
        "Eitan Farchi",
        "Ndivhuwo Makondo",
        "Aleksandra Mojsilovic",
        "Manish Nagireddy",
        "Karthikeyan Natesan Ramamurthy",
        "Inkit Padhi",
        "Orna Raz",
        "Jesus Rios",
        "Prasanna Sattigeri",
        "Moninder Singh",
        "Siphiwe Thwala",
        "Rosario A. Uceda-Sosa",
        "Kush R. Varshney"
      ],
      "abstract": "The alignment of large language models is usually done by model providers to\nadd or control behaviors that are common or universally understood across use\ncases and contexts. In contrast, in this article, we present an approach and\narchitecture that empowers application developers to tune a model to their\nparticular values, social norms, laws and other regulations, and orchestrate\nbetween potentially conflicting requirements in context. We lay out three main\ncomponents of such an Alignment Studio architecture: Framers, Instructors, and\nAuditors that work in concert to control the behavior of a language model. We\nillustrate this approach with a running example of aligning a company's\ninternal-facing enterprise chatbot to its business conduct guidelines.",
      "tldr_zh": "本论文提出Alignment Studio，一种创新架构，允许应用开发者根据特定上下文（如价值观、社会规范、法律和其他规定）调整大型语言模型（Large Language Models），以处理潜在冲突的需求。Alignment Studio 包括三个核心组件：Framers 用于定义框架、Instructors 负责指导模型行为，以及 Auditors 进行审计和验证，确保模型输出符合要求。通过一个示例，该方法展示了如何将公司内部聊天机器人调整为遵守其业务行为准则，从而提升模型在实际应用中的可控性和适应性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.09704v1",
      "published_date": "2024-03-08 21:26:49 UTC",
      "updated_date": "2024-03-08 21:26:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:35:57.848138"
    },
    {
      "arxiv_id": "2403.05681v1",
      "title": "DP-TabICL: In-Context Learning with Differentially Private Tabular Data",
      "title_zh": "翻译失败",
      "authors": [
        "Alycia N. Carey",
        "Karuna Bhaila",
        "Kennedy Edemacu",
        "Xintao Wu"
      ],
      "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks by conditioning on demonstrations of question-answer pairs and it has\nbeen shown to have comparable performance to costly model retraining and\nfine-tuning. Recently, ICL has been extended to allow tabular data to be used\nas demonstration examples by serializing individual records into natural\nlanguage formats. However, it has been shown that LLMs can leak information\ncontained in prompts, and since tabular data often contain sensitive\ninformation, understanding how to protect the underlying tabular data used in\nICL is a critical area of research. This work serves as an initial\ninvestigation into how to use differential privacy (DP) -- the long-established\ngold standard for data privacy and anonymization -- to protect tabular data\nused in ICL. Specifically, we investigate the application of DP mechanisms for\nprivate tabular ICL via data privatization prior to serialization and\nprompting. We formulate two private ICL frameworks with provable privacy\nguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios\nvia injecting noise into individual records or group statistics, respectively.\nWe evaluate our DP-based frameworks on eight real-world tabular datasets and\nacross multiple ICL and DP settings. Our evaluations show that DP-based ICL can\nprotect the privacy of the underlying tabular data while achieving comparable\nperformance to non-LLM baselines, especially under high privacy regimes.",
      "tldr_zh": "本研究探讨了如何在 In-Context Learning (ICL) 中应用 Differentially Private (DP) 机制来保护敏感的表格数据隐私，针对大型语言模型 (LLMs) 可能泄露提示信息的问题。论文提出两种私有框架：LDP-TabICL 和 GDP-TabICL，通过在数据序列化前向个体记录或组统计注入噪声，提供可证明的本地或全局 DP 隐私保证。实验在八个真实世界表格数据集上进行，结果表明，DP-based ICL 在高隐私条件下能实现与非-LLM 基线相当的性能，同时有效保护底层数据。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages, 2 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.05681v1",
      "published_date": "2024-03-08 21:19:01 UTC",
      "updated_date": "2024-03-08 21:19:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:36:12.100604"
    },
    {
      "arxiv_id": "2403.05680v2",
      "title": "How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation Framework for Analyses",
      "title_zh": "翻译失败",
      "authors": [
        "Qingqing Zhu",
        "Benjamin Hou",
        "Tejas S. Mathai",
        "Pritam Mukherjee",
        "Qiao Jin",
        "Xiuying Chen",
        "Zhizheng Wang",
        "Ruida Cheng",
        "Ronald M. Summers",
        "Zhiyong Lu"
      ],
      "abstract": "Automatically interpreting CT scans can ease the workload of radiologists.\nHowever, this is challenging mainly due to the scarcity of adequate datasets\nand reference standards for evaluation. This study aims to bridge this gap by\nintroducing a novel evaluation framework, named ``GPTRadScore''. This framework\nassesses the capabilities of multi-modal LLMs, such as GPT-4 with Vision\n(GPT-4V), Gemini Pro Vision, LLaVA-Med, and RadFM, in generating descriptions\nfor prospectively-identified findings. By employing a decomposition technique\nbased on GPT-4, GPTRadScore compares these generated descriptions with\ngold-standard report sentences, analyzing their accuracy in terms of body part,\nlocation, and type of finding. Evaluations demonstrated a high correlation with\nclinician assessments and highlighted its potential over traditional metrics,\nsuch as BLEU, METEOR, and ROUGE. Furthermore, to contribute to future studies,\nwe plan to release a benchmark dataset annotated by clinicians. Using\nGPTRadScore, we found that while GPT-4V and Gemini Pro Vision fare better,\ntheir performance revealed significant areas for improvement, primarily due to\nlimitations in the dataset used for training these models. To demonstrate this\npotential, RadFM was fine-tuned and it resulted in significant accuracy\nimprovements: location accuracy rose from 3.41\\% to 12.8\\%, body part accuracy\nfrom 29.12\\% to 53\\%, and type accuracy from 9.24\\% to 30\\%, thereby validating\nour hypothesis.",
      "tldr_zh": "这篇论文提出GPTRadScore，一种新型自动评估框架，用于评估多模态LLMs（如GPT-4V、Gemini Pro Vision、LLaVA-Med和RadFM）对CT扫描发现的描述生成能力，通过基于GPT-4的分解技术比较生成描述与金标准报告在身体部位、位置和发现类型的准确性。实验结果显示，该框架与临床评估高度相关，且优于传统指标如BLEU、METEOR和ROUGE。作者计划发布一个由临床医生标注的基准数据集，并发现通过微调RadFM，模型准确性显著提升：位置准确率从3.41%提高到12.8%、身体部位从29.12%到53%、类型从9.24%到30%。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05680v2",
      "published_date": "2024-03-08 21:16:28 UTC",
      "updated_date": "2024-06-18 12:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:36:26.103705"
    },
    {
      "arxiv_id": "2403.05658v1",
      "title": "Feature CAM: Interpretable AI in Image Classification",
      "title_zh": "Feature CAM：图像分类中的可解释 AI",
      "authors": [
        "Frincy Clement",
        "Ji Yang",
        "Irene Cheng"
      ],
      "abstract": "Deep Neural Networks have often been called the black box because of the\ncomplex, deep architecture and non-transparency presented by the inner layers.\nThere is a lack of trust to use Artificial Intelligence in critical and\nhigh-precision fields such as security, finance, health, and manufacturing\nindustries. A lot of focused work has been done to provide interpretable\nmodels, intending to deliver meaningful insights into the thoughts and behavior\nof neural networks. In our research, we compare the state-of-the-art methods in\nthe Activation-based methods (ABM) for interpreting predictions of CNN models,\nspecifically in the application of Image Classification. We then extend the\nsame for eight CNN-based architectures to compare the differences in\nvisualization and thus interpretability. We introduced a novel technique\nFeature CAM, which falls in the perturbation-activation combination, to create\nfine-grained, class-discriminative visualizations. The resulting saliency maps\nfrom our experiments proved to be 3-4 times better human interpretable than the\nstate-of-the-art in ABM. At the same time it reserves machine interpretability,\nwhich is the average confidence scores in classification.",
      "tldr_zh": "该论文探讨了深度神经网络（DNNs）作为“黑箱”的问题，导致AI在安全、金融、健康等领域缺乏信任，并比较了现有激活-based methods (ABM)用于解释CNN模型在图像分类中的预测。研究者扩展了这些方法到八个CNN架构，并引入了新术语Feature CAM，一种perturbation-activation组合技术，用于生成细粒度、类别鉴别的显著性地图。实验结果显示，Feature CAM的显著性地图在人类可解释性上比现有ABM方法高3-4倍，同时保留了机器可解释性，即平均分类置信度分数。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05658v1",
      "published_date": "2024-03-08 20:16:00 UTC",
      "updated_date": "2024-03-08 20:16:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:36:36.967697"
    },
    {
      "arxiv_id": "2403.05652v2",
      "title": "What is different between these datasets?",
      "title_zh": "这些数据集之间有什么不同？",
      "authors": [
        "Varun Babbar",
        "Zhicheng Guo",
        "Cynthia Rudin"
      ],
      "abstract": "The performance of machine learning models relies heavily on the quality of\ninput data, yet real-world applications often face significant data-related\nchallenges. A common issue arises when curating training data or deploying\nmodels: two datasets from the same domain may exhibit differing distributions.\nWhile many techniques exist for detecting such distribution shifts, there is a\nlack of comprehensive methods to explain these differences in a\nhuman-understandable way beyond opaque quantitative metrics. To bridge this\ngap, we propose a versatile toolbox of interpretable methods for comparing\ndatasets. Using a variety of case studies, we demonstrate the effectiveness of\nour approach across diverse data modalities -- including tabular data, text\ndata, images, time series signals -- in both low and high-dimensional settings.\nThese methods complement existing techniques by providing actionable and\ninterpretable insights to better understand and address distribution shifts.",
      "tldr_zh": "本研究探讨了机器学习模型性能依赖于数据质量的问题，特别是同一领域中数据集之间分布偏移（distribution shifts）的差异。现有方法虽能检测这些偏移，但缺乏人类可理解的解释方式。论文提出一个多功能工具箱（toolbox），包含可解释的方法，用于比较数据集，并在各种案例研究中验证其有效性，涵盖表格数据（tabular data）、文本数据（text data）、图像、时间序列信号（time series signals）等模态。结果显示，这些方法在低和高维设置中提供可操作的洞见，帮助更好地理解和解决分布偏移。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05652v2",
      "published_date": "2024-03-08 19:52:39 UTC",
      "updated_date": "2025-01-29 17:10:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:36:50.236151"
    },
    {
      "arxiv_id": "2403.05645v3",
      "title": "Geometric Neural Network based on Phase Space for BCI-EEG decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Igor Carrara",
        "Bruno Aristimunha",
        "Marie-Constance Corsi",
        "Raphael Y. de Camargo",
        "Sylvain Chevallier",
        "Théodore Papadopoulo"
      ],
      "abstract": "Objective: The integration of Deep Learning (DL) algorithms on brain signal\nanalysis is still in its nascent stages compared to their success in fields\nlike Computer Vision. This is particularly true for BCI, where the brain\nactivity is decoded to control external devices without requiring muscle\ncontrol. Electroencephalography (EEG) is a widely adopted choice for designing\nBCI systems due to its non-invasive and cost-effective nature and excellent\ntemporal resolution. Still, it comes at the expense of limited training data,\npoor signal-to-noise, and a large variability across and within-subject\nrecordings. Finally, setting up a BCI system with many electrodes takes a long\ntime, hindering the widespread adoption of reliable DL architectures in BCIs\noutside research laboratories. To improve adoption, we need to improve user\ncomfort using, for instance, reliable algorithms that operate with few\nelectrodes. Approach: Our research aims to develop a DL algorithm that delivers\neffective results with a limited number of electrodes. Taking advantage of the\nAugmented Covariance Method and the framework of SPDNet, we propose the\nPhase-SPDNet architecture and analyze its performance and the interpretability\nof the results. The evaluation is conducted on 5-fold cross-validation, using\nonly three electrodes positioned above the Motor Cortex. The methodology was\ntested on nearly 100 subjects from several open-source datasets using the\nMother Of All BCI Benchmark (MOABB) framework. Main results: The results of our\nPhase-SPDNet demonstrate that the augmented approach combined with the SPDNet\nsignificantly outperforms all the current state-of-the-art DL architecture in\nMI decoding. Significance: This new architecture is explainable and with a low\nnumber of trainable parameters.",
      "tldr_zh": "该研究针对脑机接口（BCI）中的脑电图（EEG）解码问题，提出了一种基于相空间的几何神经网络架构Phase-SPDNet，以应对数据有限、信噪比低和电极数量多的挑战。该方法结合Augmented Covariance Method和SPDNet框架，使用仅三个电极进行5折交叉验证，在近100个受试者的公开数据集上进行测试。结果显示，Phase-SPDNet在运动想象（MI）解码任务中显著优于现有最先进深度学习（DL）模型，提升了性能；其架构具有高解释性和低参数数量，有望提高BCI系统的用户舒适度并促进实际应用。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "q-bio.NC",
        "I.5.1; I.6.3; I.2.6"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05645v3",
      "published_date": "2024-03-08 19:36:20 UTC",
      "updated_date": "2024-08-28 15:39:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:37:00.990700"
    },
    {
      "arxiv_id": "2403.05641v1",
      "title": "A Feature-based Generalizable Prediction Model for Both Perceptual and Abstract Reasoning",
      "title_zh": "基于特征的可泛化预测模型，用于感知和抽象推理",
      "authors": [
        "Quan Do",
        "Thomas M. Morin",
        "Chantal E. Stern",
        "Michael E. Hasselmo"
      ],
      "abstract": "A hallmark of human intelligence is the ability to infer abstract rules from\nlimited experience and apply these rules to unfamiliar situations. This\ncapacity is widely studied in the visual domain using the Raven's Progressive\nMatrices. Recent advances in deep learning have led to multiple artificial\nneural network models matching or even surpassing human performance. However,\nwhile humans can identify and express the rule underlying these tasks with\nlittle to no exposure, contemporary neural networks often rely on massive\npattern-based training and cannot express or extrapolate the rule inferred from\nthe task. Furthermore, most Raven's Progressive Matrices or Raven-like tasks\nused for neural network training used symbolic representations, whereas humans\ncan flexibly switch between symbolic and continuous perceptual representations.\nIn this work, we present an algorithmic approach to rule detection and\napplication using feature detection, affine transformation estimation and\nsearch. We applied our model to a simplified Raven's Progressive Matrices task,\npreviously designed for behavioral testing and neuroimaging in humans. The\nmodel exhibited one-shot learning and achieved near human-level performance in\nthe symbolic reasoning condition of the simplified task. Furthermore, the model\ncan express the relationships discovered and generate multi-step predictions in\naccordance with the underlying rule. Finally, the model can reason using\ncontinuous patterns. We discuss our results and their relevance to studying\nabstract reasoning in humans, as well as their implications for improving\nintelligent machines.",
      "tldr_zh": "本论文提出了一种基于特征的、可泛化的预测模型，用于处理感知和抽象推理任务，旨在解决现有神经网络依赖大量训练数据且无法表达规则的问题。模型通过特征检测、affine transformation 估计和搜索算法，对简化版 Raven's Progressive Matrices 任务进行处理，实现 one-shot learning，并在符号推理条件下达到近人类水平表现。该模型能够明确表达发现的关系、生成多步预测，并灵活适应连续模式。最终，研究结果为探索人类抽象推理机制和提升智能机器的泛化能力提供了重要启示。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05641v1",
      "published_date": "2024-03-08 19:26:30 UTC",
      "updated_date": "2024-03-08 19:26:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:37:14.044776"
    },
    {
      "arxiv_id": "2403.05636v1",
      "title": "Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach",
      "title_zh": "免调",
      "authors": [
        "Zhen Tan",
        "Jie Peng",
        "Tianlong Chen",
        "Huan Liu"
      ],
      "abstract": "Large Language Models (LLMs) have catalyzed transformative advances across a\nspectrum of natural language processing tasks through few-shot or zero-shot\nprompting, bypassing the need for parameter tuning. While convenient, this\nmodus operandi aggravates ``hallucination'' concerns, particularly given the\nenigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns\nare exacerbated in high-stakes applications (e.g., healthcare), where\nunaccountable decision errors can lead to devastating consequences. In\ncontrast, human decision-making relies on nuanced cognitive processes, such as\nthe ability to sense and adaptively correct misjudgments through conceptual\nunderstanding. Drawing inspiration from human cognition, we propose an\ninnovative \\textit{metacognitive} approach, dubbed \\textbf{CLEAR}, to equip\nLLMs with capabilities for self-aware error identification and correction. Our\nframework facilitates the construction of concept-specific sparse subnetworks\nthat illuminate transparent decision pathways. This provides a novel interface\nfor model \\textit{intervention} after deployment. Our intervention offers\ncompelling advantages: (\\textit{i})~at deployment or inference time, our\nmetacognitive LLMs can self-consciously identify potential mispredictions with\nminimum human involvement, (\\textit{ii})~the model has the capability to\nself-correct its errors efficiently, obviating the need for additional tuning,\nand (\\textit{iii})~the rectification procedure is not only self-explanatory but\nalso user-friendly, enhancing the interpretability and accessibility of the\nmodel. By integrating these metacognitive features, our approach pioneers a new\npath toward engendering greater trustworthiness and accountability in the\ndeployment of LLMs.",
      "tldr_zh": "该论文提出了一种名为 CLEAR 的元认知方法，用于提升大型语言模型 (LLMs) 在部署时的可解释性和责任性，而无需参数调优，以解决幻觉问题和高风险应用中的决策错误。\nCLEAR 框架借鉴人类认知过程，通过构建概念特定的稀疏子网络，实现模型对潜在误判的自我识别和高效纠正。\n这种方法在部署时最小化人为干预，同时提供自解释和用户友好的纠错过程，从而增强 LLM 的可信度和可访问性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05636v1",
      "published_date": "2024-03-08 19:18:53 UTC",
      "updated_date": "2024-03-08 19:18:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:37:25.343918"
    },
    {
      "arxiv_id": "2403.05632v1",
      "title": "Can Large Language Models Play Games? A Case Study of A Self-Play Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyi Guo",
        "Zhihan Liu",
        "Yufeng Zhang",
        "Zhaoran Wang"
      ],
      "abstract": "Large Language Models (LLMs) harness extensive data from the Internet,\nstoring a broad spectrum of prior knowledge. While LLMs have proven beneficial\nas decision-making aids, their reliability is hampered by limitations in\nreasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo\nTree Search (MCTS) is a heuristic search algorithm that provides reliable\ndecision-making solutions, achieved through recursive rollouts and self-play.\nHowever, the effectiveness of MCTS relies heavily on heuristic pruning and\nexternal value functions, particularly in complex decision scenarios. This work\nintroduces an innovative approach that bolsters LLMs with MCTS self-play to\nefficiently resolve deterministic turn-based zero-sum games (DTZG), such as\nchess and go, without the need for additional training. Specifically, we\nutilize LLMs as both action pruners and proxies for value functions without the\nneed for additional training. We theoretically prove that the suboptimality of\nthe estimated value in our proposed method scales with $\\tilde{\\mathcal\nO}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} +\n\\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations,\n$|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM,\nand $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the\nerrors incurred by adopting LLMs as action space pruner and value function\nproxy, respectively. Our experiments in chess and go demonstrate the capability\nof our method to address challenges beyond the scope of MCTS and improve the\nperformance of the directly application of LLMs.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 是否能有效玩游戏，提出了一种将 LLMs 与 Monte-Carlo Tree Search (MCTS) 结合的自玩方法，用于解决确定性回合制零和游戏 (DTZG) 如国际象棋和围棋，而无需额外训练。LLMs 被用作动作修剪器 (action pruner) 和价值函数代理 (value function proxy)，从而提升决策效率并减少幻觉问题。论文理论证明了该方法的次优性，公式为 \\(\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)\\)；实验结果显示，该方法在国际象棋和围棋中超越了纯 MCTS 和直接使用 LLMs 的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05632v1",
      "published_date": "2024-03-08 19:16:29 UTC",
      "updated_date": "2024-03-08 19:16:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:37:39.226667"
    },
    {
      "arxiv_id": "2403.09703v2",
      "title": "Concept-aware Data Construction Improves In-context Learning of Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Michal Štefánik",
        "Marek Kadlčík",
        "Petr Sojka"
      ],
      "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL),\nmanifested in the LMs' ability to perform a new task solely from\nnatural-language instruction. Previous work curating in-context learners\nassumes that ICL emerges from a vast over-parametrization or the scale of\nmulti-task training. However, recent theoretical work attributes the ICL\nability to concept-dependent training data and creates functional in-context\nlearners even in small-scale, synthetic settings.\n  In this work, we practically explore this newly identified axis of ICL\nquality. We propose Concept-aware Training (CoAT), a framework for constructing\ntraining scenarios that make it beneficial for the LM to learn to utilize the\nanalogical reasoning concepts from demonstrations. We find that by using CoAT,\npre-trained transformers can learn to better utilise new latent concepts from\ndemonstrations and that such ability makes ICL more robust to the functional\ndeficiencies of the previous models. Finally, we show that concept-aware\nin-context learning is more effective for a majority of new tasks when compared\nto traditional instruction tuning, resulting in a performance comparable to the\nprevious in-context learners using magnitudes of more training data.",
      "tldr_zh": "该研究探讨了通过概念相关的训练数据提升语言模型的 in-context learning (ICL) 能力，挑战了以往认为 ICL 源于模型过参数化或大规模训练的观点。论文提出 Concept-aware Training (CoAT) 框架，用于构建训练场景，使预训练的 transformers 模型更好地从演示中利用类比推理概念，从而提高 ICL 的鲁棒性。实验结果表明，与传统 instruction tuning 相比，CoAT 在大多数新任务上更有效，能够使用更少的数据实现相似的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Long paper to appear in Findings of ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.09703v2",
      "published_date": "2024-03-08 19:07:47 UTC",
      "updated_date": "2024-06-28 08:03:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:37:49.325871"
    },
    {
      "arxiv_id": "2403.05535v3",
      "title": "Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Tarun Kalluri",
        "Bodhisattwa Prasad Majumder",
        "Manmohan Chandraker"
      ],
      "abstract": "We introduce LaGTran, a novel framework that utilizes text supervision to\nguide robust transfer of discriminative knowledge from labeled source to\nunlabeled target data with domain gaps. While unsupervised adaptation methods\nhave been established to address this problem, they show limitations in\nhandling challenging domain shifts due to their exclusive operation within the\npixel-space. Motivated by our observation that semantically richer text\nmodality has more favorable transfer properties, we devise a transfer mechanism\nto use a source-trained text-classifier to generate predictions on the target\ntext descriptions, and utilize these predictions as supervision for the\ncorresponding images. Our approach driven by language guidance is surprisingly\neasy and simple, yet significantly outperforms all prior approaches on\nchallenging datasets like GeoNet and DomainNet, validating its extreme\neffectiveness. To further extend the scope of our study beyond images, we\nintroduce a new benchmark called Ego2Exo to study ego-exo transfer in videos\nand find that our language-aided approach LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.",
      "tldr_zh": "本文提出 LaGTran 框架，利用文本监督指导从标记源数据到无标记目标数据的知识转移，解决图像和视频领域中存在的领域间差距问题。不同于传统无监督适应方法，该框架通过源训练的文本分类器对目标文本描述生成预测，并以此作为图像监督，从而在像素空间之外利用文本模式的转移优势。实验结果显示，LaGTran 在 GeoNet 和 DomainNet 数据集上显著优于现有方法，准确率大幅提升；此外，该框架扩展到视频领域，引入新基准 Ego2Exo 用于 ego-exo 转移场景，并实现显著性能提升。代码、模型和数据集已在 https://tarun005.github.io/lagtran/ 公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2024 Camera-Ready. Project Page and Code:\n  https://tarun005.github.io/lagtran/",
      "pdf_url": "http://arxiv.org/pdf/2403.05535v3",
      "published_date": "2024-03-08 18:58:46 UTC",
      "updated_date": "2024-06-06 01:44:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:38:02.323948"
    },
    {
      "arxiv_id": "2403.05530v5",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "title_zh": "翻译失败",
      "authors": [
        "Gemini Team",
        "Petko Georgiev",
        "Ving Ian Lei",
        "Ryan Burnell",
        "Libin Bai",
        "Anmol Gulati",
        "Garrett Tanzer",
        "Damien Vincent",
        "Zhufeng Pan",
        "Shibo Wang",
        "Soroosh Mariooryad",
        "Yifan Ding",
        "Xinyang Geng",
        "Fred Alcober",
        "Roy Frostig",
        "Mark Omernick",
        "Lexi Walker",
        "Cosmin Paduraru",
        "Christina Sorokin",
        "Andrea Tacchetti",
        "Colin Gaffney",
        "Samira Daruki",
        "Olcan Sercinoglu",
        "Zach Gleicher",
        "Juliette Love",
        "Paul Voigtlaender",
        "Rohan Jain",
        "Gabriela Surita",
        "Kareem Mohamed",
        "Rory Blevins",
        "Junwhan Ahn",
        "Tao Zhu",
        "Kornraphop Kawintiranon",
        "Orhan Firat",
        "Yiming Gu",
        "Yujing Zhang",
        "Matthew Rahtz",
        "Manaal Faruqui",
        "Natalie Clay",
        "Justin Gilmer",
        "JD Co-Reyes",
        "Ivo Penchev",
        "Rui Zhu",
        "Nobuyuki Morioka",
        "Kevin Hui",
        "Krishna Haridasan",
        "Victor Campos",
        "Mahdis Mahdieh",
        "Mandy Guo",
        "Samer Hassan",
        "Kevin Kilgour",
        "Arpi Vezer",
        "Heng-Tze Cheng",
        "Raoul de Liedekerke",
        "Siddharth Goyal",
        "Paul Barham",
        "DJ Strouse",
        "Seb Noury",
        "Jonas Adler",
        "Mukund Sundararajan",
        "Sharad Vikram",
        "Dmitry Lepikhin",
        "Michela Paganini",
        "Xavier Garcia",
        "Fan Yang",
        "Dasha Valter",
        "Maja Trebacz",
        "Kiran Vodrahalli",
        "Chulayuth Asawaroengchai",
        "Roman Ring",
        "Norbert Kalb",
        "Livio Baldini Soares",
        "Siddhartha Brahma",
        "David Steiner",
        "Tianhe Yu",
        "Fabian Mentzer",
        "Antoine He",
        "Lucas Gonzalez",
        "Bibo Xu",
        "Raphael Lopez Kaufman",
        "Laurent El Shafey",
        "Junhyuk Oh",
        "Tom Hennigan",
        "George van den Driessche",
        "Seth Odoom",
        "Mario Lucic",
        "Becca Roelofs",
        "Sid Lall",
        "Amit Marathe",
        "Betty Chan",
        "Santiago Ontanon",
        "Luheng He",
        "Denis Teplyashin",
        "Jonathan Lai",
        "Phil Crone",
        "Bogdan Damoc",
        "Lewis Ho",
        "Sebastian Riedel",
        "Karel Lenc",
        "Chih-Kuan Yeh",
        "Aakanksha Chowdhery",
        "Yang Xu",
        "Mehran Kazemi",
        "Ehsan Amid",
        "Anastasia Petrushkina",
        "Kevin Swersky",
        "Ali Khodaei",
        "Gowoon Chen",
        "Chris Larkin",
        "Mario Pinto",
        "Geng Yan",
        "Adria Puigdomenech Badia",
        "Piyush Patil",
        "Steven Hansen",
        "Dave Orr",
        "Sebastien M. R. Arnold",
        "Jordan Grimstad",
        "Andrew Dai",
        "Sholto Douglas",
        "Rishika Sinha",
        "Vikas Yadav",
        "Xi Chen",
        "Elena Gribovskaya",
        "Jacob Austin",
        "Jeffrey Zhao",
        "Kaushal Patel",
        "Paul Komarek",
        "Sophia Austin",
        "Sebastian Borgeaud",
        "Linda Friso",
        "Abhimanyu Goyal",
        "Ben Caine",
        "Kris Cao",
        "Da-Woon Chung",
        "Matthew Lamm",
        "Gabe Barth-Maron",
        "Thais Kagohara",
        "Kate Olszewska",
        "Mia Chen",
        "Kaushik Shivakumar",
        "Rishabh Agarwal",
        "Harshal Godhia",
        "Ravi Rajwar",
        "Javier Snaider",
        "Xerxes Dotiwalla",
        "Yuan Liu",
        "Aditya Barua",
        "Victor Ungureanu",
        "Yuan Zhang",
        "Bat-Orgil Batsaikhan",
        "Mateo Wirth",
        "James Qin",
        "Ivo Danihelka",
        "Tulsee Doshi",
        "Martin Chadwick",
        "Jilin Chen",
        "Sanil Jain",
        "Quoc Le",
        "Arjun Kar",
        "Madhu Gurumurthy",
        "Cheng Li",
        "Ruoxin Sang",
        "Fangyu Liu",
        "Lampros Lamprou",
        "Rich Munoz",
        "Nathan Lintz",
        "Harsh Mehta",
        "Heidi Howard",
        "Malcolm Reynolds",
        "Lora Aroyo",
        "Quan Wang",
        "Lorenzo Blanco",
        "Albin Cassirer",
        "Jordan Griffith",
        "Dipanjan Das",
        "Stephan Lee",
        "Jakub Sygnowski",
        "Zach Fisher",
        "James Besley",
        "Richard Powell",
        "Zafarali Ahmed",
        "Dominik Paulus",
        "David Reitter",
        "Zalan Borsos",
        "Rishabh Joshi",
        "Aedan Pope",
        "Steven Hand",
        "Vittorio Selo",
        "Vihan Jain",
        "Nikhil Sethi",
        "Megha Goel",
        "Takaki Makino",
        "Rhys May",
        "Zhen Yang",
        "Johan Schalkwyk",
        "Christina Butterfield",
        "Anja Hauth",
        "Alex Goldin",
        "Will Hawkins",
        "Evan Senter",
        "Sergey Brin",
        "Oliver Woodman",
        "Marvin Ritter",
        "Eric Noland",
        "Minh Giang",
        "Vijay Bolina",
        "Lisa Lee",
        "Tim Blyth",
        "Ian Mackinnon",
        "Machel Reid",
        "Obaid Sarvana",
        "David Silver",
        "Alexander Chen",
        "Lily Wang",
        "Loren Maggiore",
        "Oscar Chang",
        "Nithya Attaluri",
        "Gregory Thornton",
        "Chung-Cheng Chiu",
        "Oskar Bunyan",
        "Nir Levine",
        "Timothy Chung",
        "Evgenii Eltyshev",
        "Xiance Si",
        "Timothy Lillicrap",
        "Demetra Brady",
        "Vaibhav Aggarwal",
        "Boxi Wu",
        "Yuanzhong Xu",
        "Ross McIlroy",
        "Kartikeya Badola",
        "Paramjit Sandhu",
        "Erica Moreira",
        "Wojciech Stokowiec",
        "Ross Hemsley",
        "Dong Li",
        "Alex Tudor",
        "Pranav Shyam",
        "Elahe Rahimtoroghi",
        "Salem Haykal",
        "Pablo Sprechmann",
        "Xiang Zhou",
        "Diana Mincu",
        "Yujia Li",
        "Ravi Addanki",
        "Kalpesh Krishna",
        "Xiao Wu",
        "Alexandre Frechette",
        "Matan Eyal",
        "Allan Dafoe",
        "Dave Lacey",
        "Jay Whang",
        "Thi Avrahami",
        "Ye Zhang",
        "Emanuel Taropa",
        "Hanzhao Lin",
        "Daniel Toyama",
        "Eliza Rutherford",
        "Motoki Sano",
        "HyunJeong Choe",
        "Alex Tomala",
        "Chalence Safranek-Shrader",
        "Nora Kassner",
        "Mantas Pajarskas",
        "Matt Harvey",
        "Sean Sechrist",
        "Meire Fortunato",
        "Christina Lyu",
        "Gamaleldin Elsayed",
        "Chenkai Kuang",
        "James Lottes",
        "Eric Chu",
        "Chao Jia",
        "Chih-Wei Chen",
        "Peter Humphreys",
        "Kate Baumli",
        "Connie Tao",
        "Rajkumar Samuel",
        "Cicero Nogueira dos Santos",
        "Anders Andreassen",
        "Nemanja Rakićević",
        "Dominik Grewe",
        "Aviral Kumar",
        "Stephanie Winkler",
        "Jonathan Caton",
        "Andrew Brock",
        "Sid Dalmia",
        "Hannah Sheahan",
        "Iain Barr",
        "Yingjie Miao",
        "Paul Natsev",
        "Jacob Devlin",
        "Feryal Behbahani",
        "Flavien Prost",
        "Yanhua Sun",
        "Artiom Myaskovsky",
        "Thanumalayan Sankaranarayana Pillai",
        "Dan Hurt",
        "Angeliki Lazaridou",
        "Xi Xiong",
        "Ce Zheng",
        "Fabio Pardo",
        "Xiaowei Li",
        "Dan Horgan",
        "Joe Stanton",
        "Moran Ambar",
        "Fei Xia",
        "Alejandro Lince",
        "Mingqiu Wang",
        "Basil Mustafa",
        "Albert Webson",
        "Hyo Lee",
        "Rohan Anil",
        "Martin Wicke",
        "Timothy Dozat",
        "Abhishek Sinha",
        "Enrique Piqueras",
        "Elahe Dabir",
        "Shyam Upadhyay",
        "Anudhyan Boral",
        "Lisa Anne Hendricks",
        "Corey Fry",
        "Josip Djolonga",
        "Yi Su",
        "Jake Walker",
        "Jane Labanowski",
        "Ronny Huang",
        "Vedant Misra",
        "Jeremy Chen",
        "RJ Skerry-Ryan",
        "Avi Singh",
        "Shruti Rijhwani",
        "Dian Yu",
        "Alex Castro-Ros",
        "Beer Changpinyo",
        "Romina Datta",
        "Sumit Bagri",
        "Arnar Mar Hrafnkelsson",
        "Marcello Maggioni",
        "Daniel Zheng",
        "Yury Sulsky",
        "Shaobo Hou",
        "Tom Le Paine",
        "Antoine Yang",
        "Jason Riesa",
        "Dominika Rogozinska",
        "Dror Marcus",
        "Dalia El Badawy",
        "Qiao Zhang",
        "Luyu Wang",
        "Helen Miller",
        "Jeremy Greer",
        "Lars Lowe Sjos",
        "Azade Nova",
        "Heiga Zen",
        "Rahma Chaabouni",
        "Mihaela Rosca",
        "Jiepu Jiang",
        "Charlie Chen",
        "Ruibo Liu",
        "Tara Sainath",
        "Maxim Krikun",
        "Alex Polozov",
        "Jean-Baptiste Lespiau",
        "Josh Newlan",
        "Zeyncep Cankara",
        "Soo Kwak",
        "Yunhan Xu",
        "Phil Chen",
        "Andy Coenen",
        "Clemens Meyer",
        "Katerina Tsihlas",
        "Ada Ma",
        "Juraj Gottweis",
        "Jinwei Xing",
        "Chenjie Gu",
        "Jin Miao",
        "Christian Frank",
        "Zeynep Cankara",
        "Sanjay Ganapathy",
        "Ishita Dasgupta",
        "Steph Hughes-Fitt",
        "Heng Chen",
        "David Reid",
        "Keran Rong",
        "Hongmin Fan",
        "Joost van Amersfoort",
        "Vincent Zhuang",
        "Aaron Cohen",
        "Shixiang Shane Gu",
        "Anhad Mohananey",
        "Anastasija Ilic",
        "Taylor Tobin",
        "John Wieting",
        "Anna Bortsova",
        "Phoebe Thacker",
        "Emma Wang",
        "Emily Caveness",
        "Justin Chiu",
        "Eren Sezener",
        "Alex Kaskasoli",
        "Steven Baker",
        "Katie Millican",
        "Mohamed Elhawaty",
        "Kostas Aisopos",
        "Carl Lebsack",
        "Nathan Byrd",
        "Hanjun Dai",
        "Wenhao Jia",
        "Matthew Wiethoff",
        "Elnaz Davoodi",
        "Albert Weston",
        "Lakshman Yagati",
        "Arun Ahuja",
        "Isabel Gao",
        "Golan Pundak",
        "Susan Zhang",
        "Michael Azzam",
        "Khe Chai Sim",
        "Sergi Caelles",
        "James Keeling",
        "Abhanshu Sharma",
        "Andy Swing",
        "YaGuang Li",
        "Chenxi Liu",
        "Carrie Grimes Bostock",
        "Yamini Bansal",
        "Zachary Nado",
        "Ankesh Anand",
        "Josh Lipschultz",
        "Abhijit Karmarkar",
        "Lev Proleev",
        "Abe Ittycheriah",
        "Soheil Hassas Yeganeh",
        "George Polovets",
        "Aleksandra Faust",
        "Jiao Sun",
        "Alban Rrustemi",
        "Pen Li",
        "Rakesh Shivanna",
        "Jeremiah Liu",
        "Chris Welty",
        "Federico Lebron",
        "Anirudh Baddepudi",
        "Sebastian Krause",
        "Emilio Parisotto",
        "Radu Soricut",
        "Zheng Xu",
        "Dawn Bloxwich",
        "Melvin Johnson",
        "Behnam Neyshabur",
        "Justin Mao-Jones",
        "Renshen Wang",
        "Vinay Ramasesh",
        "Zaheer Abbas",
        "Arthur Guez",
        "Constant Segal",
        "Duc Dung Nguyen",
        "James Svensson",
        "Le Hou",
        "Sarah York",
        "Kieran Milan",
        "Sophie Bridgers",
        "Wiktor Gworek",
        "Marco Tagliasacchi",
        "James Lee-Thorp",
        "Michael Chang",
        "Alexey Guseynov",
        "Ale Jakse Hartman",
        "Michael Kwong",
        "Ruizhe Zhao",
        "Sheleem Kashem",
        "Elizabeth Cole",
        "Antoine Miech",
        "Richard Tanburn",
        "Mary Phuong",
        "Filip Pavetic",
        "Sebastien Cevey",
        "Ramona Comanescu",
        "Richard Ives",
        "Sherry Yang",
        "Cosmo Du",
        "Bo Li",
        "Zizhao Zhang",
        "Mariko Iinuma",
        "Clara Huiyi Hu",
        "Aurko Roy",
        "Shaan Bijwadia",
        "Zhenkai Zhu",
        "Danilo Martins",
        "Rachel Saputro",
        "Anita Gergely",
        "Steven Zheng",
        "Dawei Jia",
        "Ioannis Antonoglou",
        "Adam Sadovsky",
        "Shane Gu",
        "Yingying Bi",
        "Alek Andreev",
        "Sina Samangooei",
        "Mina Khan",
        "Tomas Kocisky",
        "Angelos Filos",
        "Chintu Kumar",
        "Colton Bishop",
        "Adams Yu",
        "Sarah Hodkinson",
        "Sid Mittal",
        "Premal Shah",
        "Alexandre Moufarek",
        "Yong Cheng",
        "Adam Bloniarz",
        "Jaehoon Lee",
        "Pedram Pejman",
        "Paul Michel",
        "Stephen Spencer",
        "Vladimir Feinberg",
        "Xuehan Xiong",
        "Nikolay Savinov",
        "Charlotte Smith",
        "Siamak Shakeri",
        "Dustin Tran",
        "Mary Chesus",
        "Bernd Bohnet",
        "George Tucker",
        "Tamara von Glehn",
        "Carrie Muir",
        "Yiran Mao",
        "Hideto Kazawa",
        "Ambrose Slone",
        "Kedar Soparkar",
        "Disha Shrivastava",
        "James Cobon-Kerr",
        "Michael Sharman",
        "Jay Pavagadhi",
        "Carlos Araya",
        "Karolis Misiunas",
        "Nimesh Ghelani",
        "Michael Laskin",
        "David Barker",
        "Qiujia Li",
        "Anton Briukhov",
        "Neil Houlsby",
        "Mia Glaese",
        "Balaji Lakshminarayanan",
        "Nathan Schucher",
        "Yunhao Tang",
        "Eli Collins",
        "Hyeontaek Lim",
        "Fangxiaoyu Feng",
        "Adria Recasens",
        "Guangda Lai",
        "Alberto Magni",
        "Nicola De Cao",
        "Aditya Siddhant",
        "Zoe Ashwood",
        "Jordi Orbay",
        "Mostafa Dehghani",
        "Jenny Brennan",
        "Yifan He",
        "Kelvin Xu",
        "Yang Gao",
        "Carl Saroufim",
        "James Molloy",
        "Xinyi Wu",
        "Seb Arnold",
        "Solomon Chang",
        "Julian Schrittwieser",
        "Elena Buchatskaya",
        "Soroush Radpour",
        "Martin Polacek",
        "Skye Giordano",
        "Ankur Bapna",
        "Simon Tokumine",
        "Vincent Hellendoorn",
        "Thibault Sottiaux",
        "Sarah Cogan",
        "Aliaksei Severyn",
        "Mohammad Saleh",
        "Shantanu Thakoor",
        "Laurent Shefey",
        "Siyuan Qiao",
        "Meenu Gaba",
        "Shuo-yiin Chang",
        "Craig Swanson",
        "Biao Zhang",
        "Benjamin Lee",
        "Paul Kishan Rubenstein",
        "Gan Song",
        "Tom Kwiatkowski",
        "Anna Koop",
        "Ajay Kannan",
        "David Kao",
        "Parker Schuh",
        "Axel Stjerngren",
        "Golnaz Ghiasi",
        "Gena Gibson",
        "Luke Vilnis",
        "Ye Yuan",
        "Felipe Tiengo Ferreira",
        "Aishwarya Kamath",
        "Ted Klimenko",
        "Ken Franko",
        "Kefan Xiao",
        "Indro Bhattacharya",
        "Miteyan Patel",
        "Rui Wang",
        "Alex Morris",
        "Robin Strudel",
        "Vivek Sharma",
        "Peter Choy",
        "Sayed Hadi Hashemi",
        "Jessica Landon",
        "Mara Finkelstein",
        "Priya Jhakra",
        "Justin Frye",
        "Megan Barnes",
        "Matthew Mauger",
        "Dennis Daun",
        "Khuslen Baatarsukh",
        "Matthew Tung",
        "Wael Farhan",
        "Henryk Michalewski",
        "Fabio Viola",
        "Felix de Chaumont Quitry",
        "Charline Le Lan",
        "Tom Hudson",
        "Qingze Wang",
        "Felix Fischer",
        "Ivy Zheng",
        "Elspeth White",
        "Anca Dragan",
        "Jean-baptiste Alayrac",
        "Eric Ni",
        "Alexander Pritzel",
        "Adam Iwanicki",
        "Michael Isard",
        "Anna Bulanova",
        "Lukas Zilka",
        "Ethan Dyer",
        "Devendra Sachan",
        "Srivatsan Srinivasan",
        "Hannah Muckenhirn",
        "Honglong Cai",
        "Amol Mandhane",
        "Mukarram Tariq",
        "Jack W. Rae",
        "Gary Wang",
        "Kareem Ayoub",
        "Nicholas FitzGerald",
        "Yao Zhao",
        "Woohyun Han",
        "Chris Alberti",
        "Dan Garrette",
        "Kashyap Krishnakumar",
        "Mai Gimenez",
        "Anselm Levskaya",
        "Daniel Sohn",
        "Josip Matak",
        "Inaki Iturrate",
        "Michael B. Chang",
        "Jackie Xiang",
        "Yuan Cao",
        "Nishant Ranka",
        "Geoff Brown",
        "Adrian Hutter",
        "Vahab Mirrokni",
        "Nanxin Chen",
        "Kaisheng Yao",
        "Zoltan Egyed",
        "Francois Galilee",
        "Tyler Liechty",
        "Praveen Kallakuri",
        "Evan Palmer",
        "Sanjay Ghemawat",
        "Jasmine Liu",
        "David Tao",
        "Chloe Thornton",
        "Tim Green",
        "Mimi Jasarevic",
        "Sharon Lin",
        "Victor Cotruta",
        "Yi-Xuan Tan",
        "Noah Fiedel",
        "Hongkun Yu",
        "Ed Chi",
        "Alexander Neitz",
        "Jens Heitkaemper",
        "Anu Sinha",
        "Denny Zhou",
        "Yi Sun",
        "Charbel Kaed",
        "Brice Hulse",
        "Swaroop Mishra",
        "Maria Georgaki",
        "Sneha Kudugunta",
        "Clement Farabet",
        "Izhak Shafran",
        "Daniel Vlasic",
        "Anton Tsitsulin",
        "Rajagopal Ananthanarayanan",
        "Alen Carin",
        "Guolong Su",
        "Pei Sun",
        "Shashank V",
        "Gabriel Carvajal",
        "Josef Broder",
        "Iulia Comsa",
        "Alena Repina",
        "William Wong",
        "Warren Weilun Chen",
        "Peter Hawkins",
        "Egor Filonov",
        "Lucia Loher",
        "Christoph Hirnschall",
        "Weiyi Wang",
        "Jingchen Ye",
        "Andrea Burns",
        "Hardie Cate",
        "Diana Gage Wright",
        "Federico Piccinini",
        "Lei Zhang",
        "Chu-Cheng Lin",
        "Ionel Gog",
        "Yana Kulizhskaya",
        "Ashwin Sreevatsa",
        "Shuang Song",
        "Luis C. Cobo",
        "Anand Iyer",
        "Chetan Tekur",
        "Guillermo Garrido",
        "Zhuyun Xiao",
        "Rupert Kemp",
        "Huaixiu Steven Zheng",
        "Hui Li",
        "Ananth Agarwal",
        "Christel Ngani",
        "Kati Goshvadi",
        "Rebeca Santamaria-Fernandez",
        "Wojciech Fica",
        "Xinyun Chen",
        "Chris Gorgolewski",
        "Sean Sun",
        "Roopal Garg",
        "Xinyu Ye",
        "S. M. Ali Eslami",
        "Nan Hua",
        "Jon Simon",
        "Pratik Joshi",
        "Yelin Kim",
        "Ian Tenney",
        "Sahitya Potluri",
        "Lam Nguyen Thiet",
        "Quan Yuan",
        "Florian Luisier",
        "Alexandra Chronopoulou",
        "Salvatore Scellato",
        "Praveen Srinivasan",
        "Minmin Chen",
        "Vinod Koverkathu",
        "Valentin Dalibard",
        "Yaming Xu",
        "Brennan Saeta",
        "Keith Anderson",
        "Thibault Sellam",
        "Nick Fernando",
        "Fantine Huot",
        "Junehyuk Jung",
        "Mani Varadarajan",
        "Michael Quinn",
        "Amit Raul",
        "Maigo Le",
        "Ruslan Habalov",
        "Jon Clark",
        "Komal Jalan",
        "Kalesha Bullard",
        "Achintya Singhal",
        "Thang Luong",
        "Boyu Wang",
        "Sujeevan Rajayogam",
        "Julian Eisenschlos",
        "Johnson Jia",
        "Daniel Finchelstein",
        "Alex Yakubovich",
        "Daniel Balle",
        "Michael Fink",
        "Sameer Agarwal",
        "Jing Li",
        "Dj Dvijotham",
        "Shalini Pal",
        "Kai Kang",
        "Jaclyn Konzelmann",
        "Jennifer Beattie",
        "Olivier Dousse",
        "Diane Wu",
        "Remi Crocker",
        "Chen Elkind",
        "Siddhartha Reddy Jonnalagadda",
        "Jong Lee",
        "Dan Holtmann-Rice",
        "Krystal Kallarackal",
        "Rosanne Liu",
        "Denis Vnukov",
        "Neera Vats",
        "Luca Invernizzi",
        "Mohsen Jafari",
        "Huanjie Zhou",
        "Lilly Taylor",
        "Jennifer Prendki",
        "Marcus Wu",
        "Tom Eccles",
        "Tianqi Liu",
        "Kavya Kopparapu",
        "Francoise Beaufays",
        "Christof Angermueller",
        "Andreea Marzoca",
        "Shourya Sarcar",
        "Hilal Dib",
        "Jeff Stanway",
        "Frank Perbet",
        "Nejc Trdin",
        "Rachel Sterneck",
        "Andrey Khorlin",
        "Dinghua Li",
        "Xihui Wu",
        "Sonam Goenka",
        "David Madras",
        "Sasha Goldshtein",
        "Willi Gierke",
        "Tong Zhou",
        "Yaxin Liu",
        "Yannie Liang",
        "Anais White",
        "Yunjie Li",
        "Shreya Singh",
        "Sanaz Bahargam",
        "Mark Epstein",
        "Sujoy Basu",
        "Li Lao",
        "Adnan Ozturel",
        "Carl Crous",
        "Alex Zhai",
        "Han Lu",
        "Zora Tung",
        "Neeraj Gaur",
        "Alanna Walton",
        "Lucas Dixon",
        "Ming Zhang",
        "Amir Globerson",
        "Grant Uy",
        "Andrew Bolt",
        "Olivia Wiles",
        "Milad Nasr",
        "Ilia Shumailov",
        "Marco Selvi",
        "Francesco Piccinno",
        "Ricardo Aguilar",
        "Sara McCarthy",
        "Misha Khalman",
        "Mrinal Shukla",
        "Vlado Galic",
        "John Carpenter",
        "Kevin Villela",
        "Haibin Zhang",
        "Harry Richardson",
        "James Martens",
        "Matko Bosnjak",
        "Shreyas Rammohan Belle",
        "Jeff Seibert",
        "Mahmoud Alnahlawi",
        "Brian McWilliams",
        "Sankalp Singh",
        "Annie Louis",
        "Wen Ding",
        "Dan Popovici",
        "Lenin Simicich",
        "Laura Knight",
        "Pulkit Mehta",
        "Nishesh Gupta",
        "Chongyang Shi",
        "Saaber Fatehi",
        "Jovana Mitrovic",
        "Alex Grills",
        "Joseph Pagadora",
        "Tsendsuren Munkhdalai",
        "Dessie Petrova",
        "Danielle Eisenbud",
        "Zhishuai Zhang",
        "Damion Yates",
        "Bhavishya Mittal",
        "Nilesh Tripuraneni",
        "Yannis Assael",
        "Thomas Brovelli",
        "Prateek Jain",
        "Mihajlo Velimirovic",
        "Canfer Akbulut",
        "Jiaqi Mu",
        "Wolfgang Macherey",
        "Ravin Kumar",
        "Jun Xu",
        "Haroon Qureshi",
        "Gheorghe Comanici",
        "Jeremy Wiesner",
        "Zhitao Gong",
        "Anton Ruddock",
        "Matthias Bauer",
        "Nick Felt",
        "Anirudh GP",
        "Anurag Arnab",
        "Dustin Zelle",
        "Jonas Rothfuss",
        "Bill Rosgen",
        "Ashish Shenoy",
        "Bryan Seybold",
        "Xinjian Li",
        "Jayaram Mudigonda",
        "Goker Erdogan",
        "Jiawei Xia",
        "Jiri Simsa",
        "Andrea Michi",
        "Yi Yao",
        "Christopher Yew",
        "Steven Kan",
        "Isaac Caswell",
        "Carey Radebaugh",
        "Andre Elisseeff",
        "Pedro Valenzuela",
        "Kay McKinney",
        "Kim Paterson",
        "Albert Cui",
        "Eri Latorre-Chimoto",
        "Solomon Kim",
        "William Zeng",
        "Ken Durden",
        "Priya Ponnapalli",
        "Tiberiu Sosea",
        "Christopher A. Choquette-Choo",
        "James Manyika",
        "Brona Robenek",
        "Harsha Vashisht",
        "Sebastien Pereira",
        "Hoi Lam",
        "Marko Velic",
        "Denese Owusu-Afriyie",
        "Katherine Lee",
        "Tolga Bolukbasi",
        "Alicia Parrish",
        "Shawn Lu",
        "Jane Park",
        "Balaji Venkatraman",
        "Alice Talbert",
        "Lambert Rosique",
        "Yuchung Cheng",
        "Andrei Sozanschi",
        "Adam Paszke",
        "Praveen Kumar",
        "Jessica Austin",
        "Lu Li",
        "Khalid Salama",
        "Bartek Perz",
        "Wooyeol Kim",
        "Nandita Dukkipati",
        "Anthony Baryshnikov",
        "Christos Kaplanis",
        "XiangHai Sheng",
        "Yuri Chervonyi",
        "Caglar Unlu",
        "Diego de Las Casas",
        "Harry Askham",
        "Kathryn Tunyasuvunakool",
        "Felix Gimeno",
        "Siim Poder",
        "Chester Kwak",
        "Matt Miecnikowski",
        "Vahab Mirrokni",
        "Alek Dimitriev",
        "Aaron Parisi",
        "Dangyi Liu",
        "Tomy Tsai",
        "Toby Shevlane",
        "Christina Kouridi",
        "Drew Garmon",
        "Adrian Goedeckemeyer",
        "Adam R. Brown",
        "Anitha Vijayakumar",
        "Ali Elqursh",
        "Sadegh Jazayeri",
        "Jin Huang",
        "Sara Mc Carthy",
        "Jay Hoover",
        "Lucy Kim",
        "Sandeep Kumar",
        "Wei Chen",
        "Courtney Biles",
        "Garrett Bingham",
        "Evan Rosen",
        "Lisa Wang",
        "Qijun Tan",
        "David Engel",
        "Francesco Pongetti",
        "Dario de Cesare",
        "Dongseong Hwang",
        "Lily Yu",
        "Jennifer Pullman",
        "Srini Narayanan",
        "Kyle Levin",
        "Siddharth Gopal",
        "Megan Li",
        "Asaf Aharoni",
        "Trieu Trinh",
        "Jessica Lo",
        "Norman Casagrande",
        "Roopali Vij",
        "Loic Matthey",
        "Bramandia Ramadhana",
        "Austin Matthews",
        "CJ Carey",
        "Matthew Johnson",
        "Kremena Goranova",
        "Rohin Shah",
        "Shereen Ashraf",
        "Kingshuk Dasgupta",
        "Rasmus Larsen",
        "Yicheng Wang",
        "Manish Reddy Vuyyuru",
        "Chong Jiang",
        "Joana Ijazi",
        "Kazuki Osawa",
        "Celine Smith",
        "Ramya Sree Boppana",
        "Taylan Bilal",
        "Yuma Koizumi",
        "Ying Xu",
        "Yasemin Altun",
        "Nir Shabat",
        "Ben Bariach",
        "Alex Korchemniy",
        "Kiam Choo",
        "Olaf Ronneberger",
        "Chimezie Iwuanyanwu",
        "Shubin Zhao",
        "David Soergel",
        "Cho-Jui Hsieh",
        "Irene Cai",
        "Shariq Iqbal",
        "Martin Sundermeyer",
        "Zhe Chen",
        "Elie Bursztein",
        "Chaitanya Malaviya",
        "Fadi Biadsy",
        "Prakash Shroff",
        "Inderjit Dhillon",
        "Tejasi Latkar",
        "Chris Dyer",
        "Hannah Forbes",
        "Massimo Nicosia",
        "Vitaly Nikolaev",
        "Somer Greene",
        "Marin Georgiev",
        "Pidong Wang",
        "Nina Martin",
        "Hanie Sedghi",
        "John Zhang",
        "Praseem Banzal",
        "Doug Fritz",
        "Vikram Rao",
        "Xuezhi Wang",
        "Jiageng Zhang",
        "Viorica Patraucean",
        "Dayou Du",
        "Igor Mordatch",
        "Ivan Jurin",
        "Lewis Liu",
        "Ayush Dubey",
        "Abhi Mohan",
        "Janek Nowakowski",
        "Vlad-Doru Ion",
        "Nan Wei",
        "Reiko Tojo",
        "Maria Abi Raad",
        "Drew A. Hudson",
        "Vaishakh Keshava",
        "Shubham Agrawal",
        "Kevin Ramirez",
        "Zhichun Wu",
        "Hoang Nguyen",
        "Ji Liu",
        "Madhavi Sewak",
        "Bryce Petrini",
        "DongHyun Choi",
        "Ivan Philips",
        "Ziyue Wang",
        "Ioana Bica",
        "Ankush Garg",
        "Jarek Wilkiewicz",
        "Priyanka Agrawal",
        "Xiaowei Li",
        "Danhao Guo",
        "Emily Xue",
        "Naseer Shaik",
        "Andrew Leach",
        "Sadh MNM Khan",
        "Julia Wiesinger",
        "Sammy Jerome",
        "Abhishek Chakladar",
        "Alek Wenjiao Wang",
        "Tina Ornduff",
        "Folake Abu",
        "Alireza Ghaffarkhah",
        "Marcus Wainwright",
        "Mario Cortes",
        "Frederick Liu",
        "Joshua Maynez",
        "Andreas Terzis",
        "Pouya Samangouei",
        "Riham Mansour",
        "Tomasz Kępa",
        "François-Xavier Aubet",
        "Anton Algymr",
        "Dan Banica",
        "Agoston Weisz",
        "Andras Orban",
        "Alexandre Senges",
        "Ewa Andrejczuk",
        "Mark Geller",
        "Niccolo Dal Santo",
        "Valentin Anklin",
        "Majd Al Merey",
        "Martin Baeuml",
        "Trevor Strohman",
        "Junwen Bai",
        "Slav Petrov",
        "Yonghui Wu",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Jeff Dean",
        "Oriol Vinyals"
      ],
      "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.",
      "tldr_zh": "本报告介绍了 Gemini 1.5 系列模型，这是一个高效的多模态模型系列，能够处理数百万 tokens 的上下文，包括长文档、视频和音频，并实现细粒度信息召回和推理。系列包括更新后的 Gemini 1.5 Pro 和更轻量级的 Gemini 1.5 Flash，前者在大多数能力和基准上超越了之前的版本，并在长文档 QA、长视频 QA 和长上下文 ASR 等任务中提升了最先进水平。实验显示，Gemini 1.5 在至少 10M tokens 的长上下文任务中表现出持续改进，next-token prediction 增强且检索准确率超过 99%，远超 Claude 3.0 (200k) 和 GPT-4 Turbo (128k)。此外，该模型在实际应用中帮助专业人士节省 26% 到 75% 的时间，并展示了新能力，如从 Kalamang 语法手册中学习并实现类似人类的翻译。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05530v5",
      "published_date": "2024-03-08 18:54:20 UTC",
      "updated_date": "2024-12-16 17:39:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:38:14.550753"
    },
    {
      "arxiv_id": "2403.05527v4",
      "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Kang",
        "Qingru Zhang",
        "Souvik Kundu",
        "Geonhwa Jeong",
        "Zaoxing Liu",
        "Tushar Krishna",
        "Tuo Zhao"
      ],
      "abstract": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
      "tldr_zh": "这篇论文提出了 GEAR，一种高效的 KV cache 压缩框架，旨在实现大型语言模型 (LLMs) 生成推理的近无损效果，以解决序列长度增加导致的内存绑定问题和系统吞吐量限制。GEAR 的方法包括将大多数类似大小的条目量化到超低精度，使用低秩矩阵近似量化错误，并通过稀疏矩阵修复异常条目的个别错误，从而充分利用这些技术的协同潜力。实验结果显示，与现有方法相比，GEAR 在 4-bit KV cache 压缩下实现了近无损性能，吞吐量提高高达 2.38 倍，峰值内存减少高达 2.29 倍。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05527v4",
      "published_date": "2024-03-08 18:48:30 UTC",
      "updated_date": "2024-09-30 22:44:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:38:28.304315"
    },
    {
      "arxiv_id": "2403.05525v2",
      "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
      "title_zh": "DeepSeek-VL：面向真实世界的视觉-语言理解",
      "authors": [
        "Haoyu Lu",
        "Wen Liu",
        "Bo Zhang",
        "Bingxuan Wang",
        "Kai Dong",
        "Bo Liu",
        "Jingxiang Sun",
        "Tongzheng Ren",
        "Zhuoshu Li",
        "Hao Yang",
        "Yaofeng Sun",
        "Chengqi Deng",
        "Hanwei Xu",
        "Zhenda Xie",
        "Chong Ruan"
      ],
      "abstract": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed\nfor real-world vision and language understanding applications. Our approach is\nstructured around three key dimensions:\n  We strive to ensure our data is diverse, scalable, and extensively covers\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and\nknowledge-based content, aiming for a comprehensive representation of practical\ncontexts. Further, we create a use case taxonomy from real user scenarios and\nconstruct an instruction tuning dataset accordingly. The fine-tuning with this\ndataset substantially improves the model's user experience in practical\napplications. Considering efficiency and the demands of most real-world\nscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently\nprocesses high-resolution images (1024 x 1024), while maintaining a relatively\nlow computational overhead. This design choice ensures the model's ability to\ncapture critical semantic and detailed information across various visual tasks.\nWe posit that a proficient Vision-Language Model should, foremost, possess\nstrong language abilities. To ensure the preservation of LLM capabilities\nduring pretraining, we investigate an effective VL pretraining strategy by\nintegrating LLM training from the beginning and carefully managing the\ncompetitive dynamics observed between vision and language modalities.\n  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user\nexperiences as a vision-language chatbot in real-world applications, achieving\nstate-of-the-art or competitive performance across a wide range of\nvisual-language benchmarks at the same model size while maintaining robust\nperformance on language-centric benchmarks. We have made both 1.3B and 7B\nmodels publicly accessible to foster innovations based on this foundation\nmodel.",
      "tldr_zh": "我们介绍了 DeepSeek-VL，一种开源的视觉语言模型，旨在提升真实世界的视觉和语言理解能力。该模型通过构建多样化数据（如网页截图、PDF 和 OCR）、基于真实用户场景的指令微调数据集，以及高效的混合视觉编码器来处理高分辨率图像（1024x1024），并采用整合 LLM 训练的 VL 预训练策略来平衡模态竞争。实验结果显示，DeepSeek-VL 的 1.3B 和 7B 模型在视觉语言基准上达到最先进或竞争性性能，同时保持语言基准的稳健表现，并已公开可用以推动创新。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "https://github.com/deepseek-ai/DeepSeek-VL",
      "pdf_url": "http://arxiv.org/pdf/2403.05525v2",
      "published_date": "2024-03-08 18:46:00 UTC",
      "updated_date": "2024-03-11 16:47:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:38:41.166255"
    },
    {
      "arxiv_id": "2403.05518v1",
      "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought",
      "title_zh": "翻译失败",
      "authors": [
        "James Chua",
        "Edward Rees",
        "Hunar Batra",
        "Samuel R. Bowman",
        "Julian Michael",
        "Ethan Perez",
        "Miles Turpin"
      ],
      "abstract": "While chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning, it can systematically misrepresent\nthe factors influencing models' behavior--for example, rationalizing answers in\nline with a user's opinion without mentioning this bias. To mitigate this\nbiased reasoning problem, we introduce bias-augmented consistency training\n(BCT), an unsupervised fine-tuning scheme that trains models to give consistent\nreasoning across prompts with and without biasing features. We construct a\nsuite testing nine forms of biased reasoning on seven question-answering tasks,\nand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of\nbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes to\nother forms of bias, reducing biased reasoning on held-out biases by an average\nof 37%. As BCT generalizes to held-out biases and does not require gold labels,\nthis method may hold promise for reducing biased reasoning from as-of-yet\nunknown biases and on tasks where supervision for ground truth reasoning is\nunavailable.",
      "tldr_zh": "该研究发现，Chain-of-Thought (CoT) 提示虽然能提升语言模型的解释性，但可能导致系统性偏见，例如根据用户意见合理化答案而不提及偏差。为解决这一问题，研究提出 Bias-Augmented Consistency Training (BCT)，一种无监督微调方法，通过训练模型在带有和不带偏差特征的提示下保持推理一致性，从而减少偏见推理。在实验中，将 BCT 应用于 GPT-3.5-Turbo 后，一个偏差的偏见率降低了 86%，并能泛化到其他未见偏差，平均减少 37% 的偏见，这为缺乏监督标签的任务提供了一种有前景的减偏策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05518v1",
      "published_date": "2024-03-08 18:41:42 UTC",
      "updated_date": "2024-03-08 18:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:38:52.071877"
    },
    {
      "arxiv_id": "2403.05612v2",
      "title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate",
      "title_zh": "不熟悉的微调示例控制语言模型如何产生幻觉",
      "authors": [
        "Katie Kang",
        "Eric Wallace",
        "Claire Tomlin",
        "Aviral Kumar",
        "Sergey Levine"
      ],
      "abstract": "Large language models are known to hallucinate when faced with unfamiliar\nqueries, but the underlying mechanism that govern how models hallucinate are\nnot yet fully understood. In this work, we find that unfamiliar examples in the\nmodels' finetuning data -- those that introduce concepts beyond the base\nmodel's scope of knowledge -- are crucial in shaping these errors. In\nparticular, we find that an LLM's hallucinated predictions tend to mirror the\nresponses associated with its unfamiliar finetuning examples. This suggests\nthat by modifying how unfamiliar finetuning examples are supervised, we can\ninfluence a model's responses to unfamiliar queries (e.g., say ``I don't\nknow''). We empirically validate this observation in a series of controlled\nexperiments involving SFT, RL, and reward model finetuning on TriviaQA and\nMMLU. Our work further investigates RL finetuning strategies for improving the\nfactuality of long-form model generations. We find that, while hallucinations\nfrom the reward model can significantly undermine the effectiveness of RL\nfactuality finetuning, strategically controlling how reward models hallucinate\ncan minimize these negative effects. Leveraging our previous observations on\ncontrolling hallucinations, we propose an approach for learning more reliable\nreward models, and show that they improve the efficacy of RL factuality\nfinetuning in long-form biography and book/movie plot generation tasks.",
      "tldr_zh": "本文研究发现，大语言模型(LLM)对不熟悉查询的幻觉(hallucinate)主要受微调数据中不熟悉示例的影响，这些示例会使模型的预测模仿其响应，从而塑造错误模式。作者通过控制实验，包括SFT、RL和奖励模型fintuning，在TriviaQA和MMLU数据集上验证了这一观察，并证明通过修改这些示例的监督方式（如让模型回应“I don't know”），可以有效减少幻觉。进一步，论文提出了一种策略来控制奖励模型的幻觉，改进RL finetuning的真实性，并在长形式任务如传记和书籍/电影情节生成中提升了模型的表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05612v2",
      "published_date": "2024-03-08 18:28:13 UTC",
      "updated_date": "2024-05-28 23:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:39:04.748870"
    },
    {
      "arxiv_id": "2403.05490v1",
      "title": "Poly-View Contrastive Learning",
      "title_zh": "多视图对比学习",
      "authors": [
        "Amitis Shidani",
        "Devon Hjelm",
        "Jason Ramapuram",
        "Russ Webb",
        "Eeshan Gunesh Dhekane",
        "Dan Busbridge"
      ],
      "abstract": "Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.",
      "tldr_zh": "这篇论文探讨了多视图（Poly-View）对比学习（Contrastive Learning），当有超过两个相关视图时，如何使用信息最大化和充分统计推导新的表示学习目标。研究发现，在无限计算资源下，应最大化相关视图的数量，而在固定计算预算下，减少唯一样本数量并增加其视图数量更有益。实验结果显示，Poly-View模型在ImageNet1k上以128个epoch和批量大小256训练，优于SimCLR模型的1024个epoch和批量大小4096，挑战了对比学习需要大批量和长训练周期的传统信念。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "math.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2024. 42 pages, 7 figures, 3 tables, loss\n  pseudo-code included in appendix",
      "pdf_url": "http://arxiv.org/pdf/2403.05490v1",
      "published_date": "2024-03-08 17:55:41 UTC",
      "updated_date": "2024-03-08 17:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:39:15.699555"
    },
    {
      "arxiv_id": "2403.05468v1",
      "title": "Will GPT-4 Run DOOM?",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian de Wynter"
      ],
      "abstract": "We show that GPT-4's reasoning and planning capabilities extend to the 1993\nfirst-person shooter Doom. This large language model (LLM) is able to run and\nplay the game with only a few instructions, plus a textual\ndescription--generated by the model itself from screenshots--about the state of\nthe game being observed. We find that GPT-4 can play the game to a passable\ndegree: it is able to manipulate doors, combat enemies, and perform pathing.\nMore complex prompting strategies involving multiple model calls provide better\nresults. While further work is required to enable the LLM to play the game as\nwell as its classical, reinforcement learning-based counterparts, we note that\nGPT-4 required no training, leaning instead on its own reasoning and\nobservational capabilities. We hope our work pushes the boundaries on\nintelligent, LLM-based agents in video games. We conclude by discussing the\nethical implications of our work.",
      "tldr_zh": "本文研究了GPT-4的推理和规划能力是否能应用于1993年第一人称射击游戏Doom。研究发现，GPT-4通过少量指令和自身生成的游戏状态文本描述，能够基本玩游戏，包括操作门、战斗敌人和路径规划，而无需任何训练。使用更复杂的提示策略（如多个模型调用）可进一步提升表现，尽管其水平仍落后于基于reinforcement learning的传统方法。该工作展示了LLM在视频游戏智能代理方面的潜力，并讨论了相关的伦理含义。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05468v1",
      "published_date": "2024-03-08 17:30:41 UTC",
      "updated_date": "2024-03-08 17:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:39:26.799063"
    },
    {
      "arxiv_id": "2403.05465v2",
      "title": "Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Akshat Ramachandran",
        "Zishen Wan",
        "Geonhwa Jeong",
        "John Gustafson",
        "Tushar Krishna"
      ],
      "abstract": "Traditional Deep Neural Network (DNN) quantization methods using integer,\nfixed-point, or floating-point data types struggle to capture diverse DNN\nparameter distributions at low precision, and often require large silicon\noverhead and intensive quantization-aware training. In this study, we introduce\nLogarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by\nposits that dynamically adapts to DNN weight/activation distributions by\nparameterizing LP bit fields. We also develop a novel genetic-algorithm based\nframework, LP Quantization (LPQ), to find optimal layer-wise LP parameters\nwhile reducing representational divergence between quantized and full-precision\nmodels through a novel global-local contrastive objective. Additionally, we\ndesign a unified mixed-precision LP accelerator (LPA) architecture comprising\nof processing elements (PEs) incorporating LP in the computational datapath.\nOur algorithm-hardware co-design demonstrates on average <1% drop in top-1\naccuracy across various CNN and ViT models. It also achieves ~ 2x improvements\nin performance per unit area and 2.2x gains in energy efficiency compared to\nstate-of-the-art quantization accelerators using different data types.",
      "tldr_zh": "本研究提出 Logarithmic Posits (LP)，一种自适应硬件友好数据类型，用于高效的 DNN 推理，能够动态适应权重和激活分布，以解决传统量化方法在低精度下的局限性。研究开发了 LP Quantization (LPQ) 框架，该框架基于遗传算法优化层级 LP 参数，并通过全局-局部对比目标减少量化模型与全精度模型的表示差异，同时设计了统一的混合精度 LP 加速器 (LPA) 架构。实验结果显示，在各种 CNN 和 ViT 模型上，Top-1 准确率下降不到 1%，并实现了约 2 倍的性能每单位面积提升和 2.2 倍的能效改进。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AR",
      "comment": "2024 61st IEEE/ACM Design Automation Conference (DAC)",
      "pdf_url": "http://arxiv.org/pdf/2403.05465v2",
      "published_date": "2024-03-08 17:28:49 UTC",
      "updated_date": "2024-03-26 18:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:39:41.000594"
    },
    {
      "arxiv_id": "2403.05407v2",
      "title": "Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks",
      "title_zh": "算法识别脑网络中必要的外生节点以实现因果充分性",
      "authors": [
        "Abdolmahdi Bagheri",
        "Mahdi Dehshiri",
        "Babak Nadjar Araabi",
        "Alireza Akhondi Asl"
      ],
      "abstract": "In the investigation of any causal mechanisms, such as the brain's causal\nnetworks, the assumption of causal sufficiency plays a critical role. Notably,\nneglecting this assumption can result in significant errors, a fact that is\noften disregarded in the causal analysis of brain networks. In this study, we\npropose an algorithmic identification approach for determining essential\nexogenous nodes that satisfy the critical need for causal sufficiency to adhere\nto it in such inquiries. Our approach consists of three main steps: First, by\ncapturing the essence of the Peter-Clark (PC) algorithm, we conduct\nindependence tests for pairs of regions within a network, as well as for the\nsame pairs conditioned on nodes from other networks. Next, we distinguish\ncandidate confounders by analyzing the differences between the conditional and\nunconditional results, using the Kolmogorov-Smirnov test. Subsequently, we\nutilize Non-Factorized identifiable Variational Autoencoders (NF-iVAE) along\nwith the Correlation Coefficient index (CCI) metric to identify the confounding\nvariables within these candidate nodes. Applying our method to the Human\nConnectome Projects (HCP) movie-watching task data, we demonstrate that while\ninteractions exist between dorsal and ventral regions, only dorsal regions\nserve as confounders for the visual networks, and vice versa. These findings\nalign consistently with those resulting from the neuroscientific perspective.\nFinally, we show the reliability of our results by testing 30 independent runs\nfor NF-iVAE initialization.",
      "tldr_zh": "该研究强调了因果充分性（causal sufficiency）假设在脑网络因果分析中的关键作用，并提出了一种算法方法来识别必要的外部节点（essential exogenous nodes），以避免忽略该假设导致的错误。该方法包括三个步骤：首先使用 Peter-Clark (PC) 算法进行独立性测试；其次通过 Kolmogorov-Smirnov 测试分析条件和非条件结果的差异，识别候选混杂因素；最后应用 Non-Factorized identifiable Variational Autoencoders (NF-iVAE) 和 Correlation Coefficient index (CCI) 指标来确认混杂变量。在 Human Connectome Projects (HCP) 电影观看任务数据上，该方法发现背侧区域是视觉网络的混杂因素，而腹侧区域则相反，结果与神经科学观点一致，并通过30次独立运行验证了其可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05407v2",
      "published_date": "2024-03-08 16:05:47 UTC",
      "updated_date": "2024-03-15 14:35:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:39:52.293694"
    },
    {
      "arxiv_id": "2403.05406v1",
      "title": "Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Muyao Wang",
        "Wenchao Chen",
        "Bo Chen"
      ],
      "abstract": "The forecasting of Multivariate Time Series (MTS) has long been an important\nbut challenging task. Due to the non-stationary problem across long-distance\ntime steps, previous studies primarily adopt stationarization method to\nattenuate the non-stationary problem of the original series for better\npredictability. However, existing methods always adopt the stationarized\nseries, which ignores the inherent non-stationarity, and has difficulty in\nmodeling MTS with complex distributions due to the lack of stochasticity. To\ntackle these problems, we first develop a powerful hierarchical probabilistic\ngenerative module to consider the non-stationarity and stochastic\ncharacteristics within MTS, and then combine it with transformer for a\nwell-defined variational generative dynamic model named Hierarchical Time\nseries Variational Transformer (HTV-Trans), which recovers the intrinsic\nnon-stationary information into temporal dependencies. Being a powerful\nprobabilistic model, HTV-Trans is utilized to learn expressive representations\nof MTS and applied to forecasting tasks. Extensive experiments on diverse\ndatasets show the efficiency of HTV-Trans on MTS forecasting tasks",
      "tldr_zh": "本研究针对多变量时间序列 (Multivariate Time Series, MTS) 预测中的非平稳问题，提出了一种 Hierarchical Time series Variational Transformer (HTV-Trans)。该模型结合层次化概率生成模块和 Transformer，考虑 MTS 的非平稳性和随机特性，以更好地恢复内在时间依赖信息并学习表达性表示。实验结果显示，HTV-Trans 在各种数据集上表现出色，在 MTS 预测任务中显著提升了性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by AAAI2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05406v1",
      "published_date": "2024-03-08 16:04:36 UTC",
      "updated_date": "2024-03-08 16:04:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:40:04.154101"
    },
    {
      "arxiv_id": "2403.05396v2",
      "title": "HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengrui Guo",
        "Jiabo Ma",
        "Yingxue Xu",
        "Yihui Wang",
        "Liansheng Wang",
        "Hao Chen"
      ],
      "abstract": "Histopathology serves as the gold standard in cancer diagnosis, with clinical\nreports being vital in interpreting and understanding this process, guiding\ncancer treatment and patient care. The automation of histopathology report\ngeneration with deep learning stands to significantly enhance clinical\nefficiency and lessen the labor-intensive, time-consuming burden on\npathologists in report writing. In pursuit of this advancement, we introduce\nHistGen, a multiple instance learning-empowered framework for histopathology\nreport generation together with the first benchmark dataset for evaluation.\nInspired by diagnostic and report-writing workflows, HistGen features two\ndelicately designed modules, aiming to boost report generation by aligning\nwhole slide images (WSIs) and diagnostic reports from local and global\ngranularity. To achieve this, a local-global hierarchical encoder is developed\nfor efficient visual feature aggregation from a region-to-slide perspective.\nMeanwhile, a cross-modal context module is proposed to explicitly facilitate\nalignment and interaction between distinct modalities, effectively bridging the\ngap between the extensive visual sequences of WSIs and corresponding highly\nsummarized reports. Experimental results on WSI report generation show the\nproposed model outperforms state-of-the-art (SOTA) models by a large margin.\nMoreover, the results of fine-tuning our model on cancer subtyping and survival\nanalysis tasks further demonstrate superior performance compared to SOTA\nmethods, showcasing strong transfer learning capability. Dataset, model\nweights, and source code are available in\nhttps://github.com/dddavid4real/HistGen.",
      "tldr_zh": "本文提出 HistGen 框架，利用 multiple instance learning (MIL) 自动生成组织病理学报告，并发布首个基准数据集，以提升临床效率。框架包括 local-global hierarchical encoder，用于从区域到整体（whole slide images, WSIs）聚合视觉特征，以及 cross-modal context module，促进视觉和文本模态的交互对齐。实验结果显示，HistGen 在 WSI 报告生成任务上大幅超越 state-of-the-art (SOTA) 模型，并在癌症亚型分类和生存分析任务的微调中表现出优异性能，展示了强大的迁移学习能力。该框架的模型权重和源代码已在 GitHub 上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by MICCAI2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05396v2",
      "published_date": "2024-03-08 15:51:43 UTC",
      "updated_date": "2024-06-18 05:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:40:16.347726"
    },
    {
      "arxiv_id": "2403.05379v2",
      "title": "Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Salome Kazeminia",
        "Max Joosten",
        "Dragan Bosnacki",
        "Carsten Marr"
      ],
      "abstract": "Automated disease diagnosis using medical image analysis relies on deep\nlearning, often requiring large labeled datasets for supervised model training.\nDiseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and\ncostly annotations on a single-cell level. Multiple Instance Learning (MIL)\naddresses weakly labeled scenarios but necessitates powerful encoders typically\ntrained with labeled data. In this study, we explore Self-Supervised Learning\n(SSL) as a pre-training approach for MIL-based AML subtype classification from\nblood smears, removing the need for labeled data during encoder training. We\ninvestigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and\ncompare their performance against supervised pre-training. Our findings show\nthat SSL-pretrained encoders achieve comparable performance, showcasing the\npotential of SSL in MIL. This breakthrough offers a cost-effective and\ndata-efficient solution, propelling the field of AI-based disease diagnosis.",
      "tldr_zh": "本研究针对Acute Myeloid Leukemia (AML) 等疾病的医疗图像分析问题，提出使用Self-Supervised Learning (SSL) 作为预训练方法，与Multiple Instance Learning (MIL) 结合，实现基于血涂片的无监督编码器训练，从而减少对昂贵标注数据的依赖。研究者评估了三种先进SSL方法——SimCLR、SwAV 和 DINO，并将其性能与监督预训练进行比较，结果显示SSL预训练的编码器可达到相媲美的分类准确率。总体而言，此方法提供了一个成本有效且数据高效的解决方案，推动AI在疾病诊断领域的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05379v2",
      "published_date": "2024-03-08 15:16:15 UTC",
      "updated_date": "2024-08-22 21:42:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:40:28.430774"
    },
    {
      "arxiv_id": "2403.05334v2",
      "title": "WatChat: Explaining perplexing programs by debugging mental models",
      "title_zh": "WatChat：通过调试心理模型解释令人困惑的程序",
      "authors": [
        "Kartik Chandra",
        "Katherine M. Collins",
        "Will Crichton",
        "Tony Chen",
        "Tzu-Mao Li",
        "Adrian Weller",
        "Rachit Nigam",
        "Joshua Tenenbaum",
        "Jonathan Ragan-Kelley"
      ],
      "abstract": "Often, a good explanation for a program's unexpected behavior is a bug in the\nprogrammer's code. But sometimes, an even better explanation is a bug in the\nprogrammer's mental model of the language or API they are using. Instead of\nmerely debugging our current code (\"giving the programmer a fish\"), what if our\ntools could directly debug our mental models (\"teaching the programmer to\nfish\")? In this paper, we apply recent ideas from computational cognitive\nscience to offer a principled framework for doing exactly that. Given a \"why?\"\nquestion about a program, we automatically infer potential misconceptions about\nthe language/API that might cause the user to be surprised by the program's\nbehavior -- and then analyze those misconceptions to provide explanations of\nthe program's behavior. Our key idea is to formally represent misconceptions as\ncounterfactual (erroneous) semantics for the language/API, which can be\ninferred and debugged using program synthesis techniques. We demonstrate our\nframework, WatChat, by building systems for explanation in two domains:\nJavaScript type coercion, and the Git version control system. We evaluate\nWatChatJS and WatChatGit by comparing their outputs to experimentally-collected\nhuman-written explanations in these two domains: we show that WatChat's\nexplanations exhibit key features of human-written explanation, unlike those of\na state-of-the-art language model.",
      "tldr_zh": "本论文提出WatChat框架，用于通过调试程序员的mental models来解释程序的意外行为，而非仅修复代码bug。该框架基于computational cognitive science的原理，将潜在误解形式化为语言或API的counterfactual semantics，并利用program synthesis techniques自动推断和分析这些误解，以提供更深刻的解释。研究在JavaScript type coercion和Git version control两个领域构建了WatChat系统，并通过实验比较显示，WatChat的解释在关键特征上更接近人类编写的解释，而非state-of-the-art语言模型的输出。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.PL",
      "comment": "This is a preprint of work presented in early-stage non-archival form\n  at the ACL Natural Language Reasoning and Structured Explanations Workshop",
      "pdf_url": "http://arxiv.org/pdf/2403.05334v2",
      "published_date": "2024-03-08 14:10:25 UTC",
      "updated_date": "2024-10-02 17:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:40:40.736034"
    },
    {
      "arxiv_id": "2403.05326v4",
      "title": "ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues",
      "title_zh": "翻译失败",
      "authors": [
        "Yiding Liu",
        "Jingjing Wang",
        "Jiamin Luo",
        "Tao Zeng",
        "Guodong Zhou"
      ],
      "abstract": "Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.",
      "tldr_zh": "这篇论文提出 ChatASU 任务，利用大型语言模型 (LLMs) 来处理对话场景中的 Aspect Sentiment Understanding (ASU)，特别针对意见目标的核心ference问题引入 Aspect Chain Reasoning (ACR) 子任务。研究开发了 Trusted Self-reflexion Approach (TSA) 方法，以 ChatGLM 为基础，将 ACR 作为辅助任务提升 ASU 性能，并通过 trusted learning 机制缓解 LLMs 的 hallucination 问题。实验结果显示，TSA 在新构建的 ChatASU 数据集上显著优于现有基线模型，验证了考虑核心ference 和 hallucination 问题的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05326v4",
      "published_date": "2024-03-08 14:05:36 UTC",
      "updated_date": "2024-04-10 13:08:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:40:54.616753"
    },
    {
      "arxiv_id": "2403.05318v1",
      "title": "Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Jingxiao Chen",
        "Ziqin Gong",
        "Minghuan Liu",
        "Jun Wang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "Many real-world problems can be formulated as a constrained Traveling\nSalesman Problem (TSP). However, the constraints are always complex and\nnumerous, making the TSPs challenging to solve. When the number of complicated\nconstraints grows, it is time-consuming for traditional heuristic algorithms to\navoid illegitimate outcomes. Learning-based methods provide an alternative to\nsolve TSPs in a soft manner, which also supports GPU acceleration to generate\nsolutions quickly. Nevertheless, the soft manner inevitably results in\ndifficulty solving hard-constrained problems with learning algorithms, and the\nconflicts between legality and optimality may substantially affect the\noptimality of the solution. To overcome this problem and to have an effective\nsolution against hard constraints, we proposed a novel learning-based method\nthat uses looking-ahead information as the feature to improve the legality of\nTSP with Time Windows (TSPTW) solutions. Besides, we constructed TSPTW datasets\nwith hard constraints in order to accurately evaluate and benchmark the\nstatistical performance of various approaches, which can serve the community\nfor future research. With comprehensive experiments on diverse datasets, MUSLA\noutperforms existing baselines and shows generalizability potential.",
      "tldr_zh": "该研究针对硬约束的旅行 salesman 问题 (TSP)，特别是 TSP with Time Windows (TSPTW)，提出了一种新颖的学习-based 方法 MUSLA，使用 looking-ahead 信息作为特征来提升解决方案的合法性，从而平衡合法性和最优性。论文构建了新的 TSPTW 数据集，用于基准测试各种方法的统计性能，以支持未来研究。实验结果显示，MUSLA 在多样数据集上优于现有基线，并展示了良好的泛化潜力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05318v1",
      "published_date": "2024-03-08 13:49:21 UTC",
      "updated_date": "2024-03-08 13:49:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:41:04.042247"
    },
    {
      "arxiv_id": "2403.05313v1",
      "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
      "title_zh": "RAT：检索增强思考在长时域生成中激发上下文感知推理",
      "authors": [
        "Zihao Wang",
        "Anji Liu",
        "Haowei Lin",
        "Jiaqi Li",
        "Xiaojian Ma",
        "Yitao Liang"
      ],
      "abstract": "We explore how iterative revising a chain of thoughts with the help of\ninformation retrieval significantly improves large language models' reasoning\nand generation ability in long-horizon generation tasks, while hugely\nmitigating hallucination. In particular, the proposed method --\n*retrieval-augmented thoughts* (RAT) -- revises each thought step one by one\nwith retrieved information relevant to the task query, the current and the past\nthought steps, after the initial zero-shot CoT is generated. Applying RAT to\nGPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on\nvarious long-horizon generation tasks; on average of relatively increasing\nrating scores by 13.63% on code generation, 16.96% on mathematical reasoning,\n19.2% on creative writing, and 42.78% on embodied task planning. The demo page\ncan be found at https://craftjarvis.github.io/RAT",
      "tldr_zh": "该研究提出了一种名为 Retrieval-Augmented Thoughts (RAT) 的方法，通过信息检索迭代修改 Chain of Thoughts (CoT)，以提升大型语言模型（LLMs）在长序列生成任务中的上下文感知推理能力，同时显著减少 hallucination。RAT 先生成初始零-shot CoT，然后逐一修订每个思维步骤，使用与任务查询、当前和过去步骤相关的检索信息。实验结果显示，在 GPT-3.5、GPT-4 和 CodeLLaMA-7b 上，RAT 平均提高了代码生成（13.63%）、数学推理（16.96%）、创意写作（19.2%）和具身任务规划（42.78%）的性能，为长序列任务的可靠生成提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05313v1",
      "published_date": "2024-03-08 13:42:19 UTC",
      "updated_date": "2024-03-08 13:42:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:41:19.141025"
    },
    {
      "arxiv_id": "2403.05307v1",
      "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Jinyang Li",
        "Nan Huo",
        "Yan Gao",
        "Jiayi Shi",
        "Yingxiu Zhao",
        "Ge Qu",
        "Yurong Wu",
        "Chenhao Ma",
        "Jian-Guang Lou",
        "Reynold Cheng"
      ],
      "abstract": "Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
      "tldr_zh": "本研究介绍了 Tapilot-Crossing，这是一个新的基准，用于评估大型语言模型(LLMs)在交互式数据分析(Interactive Data Analysis)中的表现，以解决收集真实交互日志的挑战和成本问题。该基准包含1024个交互，覆盖4个实际场景（Normal, Action, Private, and Private Action），并通过经济的多代理环境Decision Company以少量人力构建。实验评估了流行和先进的LLM代理，突显了交互式数据分析的难点。随后，提出Adaptive Interaction Reflection (AIR)策略，该策略通过自我生成的反思机制，让LLM代理从成功历史中学习，实验结果显示性能提升高达44.5%。这为发展有效的交互式数据分析代理提供了重要基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.05307v1",
      "published_date": "2024-03-08 13:34:20 UTC",
      "updated_date": "2024-03-08 13:34:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:41:29.456534"
    },
    {
      "arxiv_id": "2403.05300v5",
      "title": "Unity by Diversity: Improved Representation Learning in Multimodal VAEs",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas M. Sutter",
        "Yang Meng",
        "Andrea Agostini",
        "Daphné Chopard",
        "Norbert Fortin",
        "Julia E. Vogt",
        "Babak Shahbaba",
        "Stephan Mandt"
      ],
      "abstract": "Variational Autoencoders for multimodal data hold promise for many tasks in\ndata analysis, such as representation learning, conditional generation, and\nimputation. Current architectures either share the encoder output, decoder\ninput, or both across modalities to learn a shared representation. Such\narchitectures impose hard constraints on the model. In this work, we show that\na better latent representation can be obtained by replacing these hard\nconstraints with a soft constraint. We propose a new mixture-of-experts prior,\nsoftly guiding each modality's latent representation towards a shared aggregate\nposterior. This approach results in a superior latent representation and allows\neach encoding to preserve information better from its uncompressed original\nfeatures. In extensive experiments on multiple benchmark datasets and two\nchallenging real-world datasets, we show improved learned latent\nrepresentations and imputation of missing data modalities compared to existing\nmethods.",
      "tldr_zh": "本文提出了一种改进多模态数据Variational Autoencoders (VAEs)的方法，通过引入mixture-of-experts prior作为软约束，取代传统架构的硬约束，以提升表示学习效果。  \n这种软约束允许每个模态的潜表示向共享的聚合后验进行柔性引导，同时更好地保留原始特征信息，从而获得更优的潜表示。  \n在多个基准数据集和两个挑战性真实数据集上的实验中，该方法显示出在表示学习和缺失数据模态插值方面的显著改进，优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05300v5",
      "published_date": "2024-03-08 13:29:46 UTC",
      "updated_date": "2025-01-07 17:42:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:41:41.118866"
    },
    {
      "arxiv_id": "2403.05297v3",
      "title": "PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck",
      "title_zh": "翻译失败",
      "authors": [
        "Thang M. Pham",
        "Peijie Chen",
        "Tin Nguyen",
        "Seunghyun Yoon",
        "Trung Bui",
        "Anh Totti Nguyen"
      ],
      "abstract": "CLIP-based classifiers rely on the prompt containing a {class name} that is\nknown to the text encoder. Therefore, they perform poorly on new classes or the\nclasses whose names rarely appear on the Internet (e.g., scientific names of\nbirds). For fine-grained classification, we propose PEEB - an explainable and\neditable classifier to (1) express the class name into a set of text\ndescriptors that describe the visual parts of that class; and (2) match the\nembeddings of the detected parts to their textual descriptors in each class to\ncompute a logit score for classification. In a zero-shot setting where the\nclass names are unknown, PEEB outperforms CLIP by a huge margin (~10x in top-1\naccuracy). Compared to part-based classifiers, PEEB is not only the\nstate-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20%\naccuracy on CUB-200 and Dogs-120, respectively) but also the first to enable\nusers to edit the text descriptors to form a new classifier without any\nre-training. Compared to concept bottleneck models, PEEB is also the SOTA in\nboth zero-shot and supervised-learning settings.",
      "tldr_zh": "本研究提出 PEEB，一种基于部分的图像分类器，旨在解决 CLIP 模型在处理新类或罕见类名（如鸟类科学名称）时的性能问题，通过将类名转化为一组描述视觉部分的文本描述符，并匹配检测到的部分嵌入来计算分类分数。相比 CLIP，在零样本设置中，PEEB 的 top-1 准确率提高了约 10 倍；在监督学习中，它在 CUB-200 和 Dogs-120 数据集上达到 SOTA 水平（88.80% 和 92.20% 准确率）。此外，PEEB 允许用户编辑文本描述符来创建新分类器，而无需重新训练，并在零样本和监督学习设置中超越现有概念瓶颈模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Findings of NAACL 2024 (long paper)",
      "pdf_url": "http://arxiv.org/pdf/2403.05297v3",
      "published_date": "2024-03-08 13:24:46 UTC",
      "updated_date": "2024-04-12 20:10:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:41:55.102901"
    },
    {
      "arxiv_id": "2403.05266v3",
      "title": "ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jio Oh",
        "Soyeon Kim",
        "Junseok Seo",
        "Jindong Wang",
        "Ruochen Xu",
        "Xing Xie",
        "Steven Euijong Whang"
      ],
      "abstract": "Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.",
      "tldr_zh": "本文提出 ERBench，这是一个基于 Entity-Relationship (ER) 模型的自动可验证基准，用于评估 Large Language Models (LLMs) 的幻觉问题，并能深入检查模型的思考过程。ERBench 利用关系数据库的完整性约束，如 functional dependencies 和 foreign key constraints，来构建复杂多跳问题、验证答案正确性和关键关键词的存在，从而实现更精确的评估。该基准支持数据库动态变化的连续评估、多模态问题和各种提示工程技术。在实验中，ERBench 基于多个领域的数据库比较了当代 LLMs 的性能，展示了其在提升模型评估深度和可靠性方面的优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05266v3",
      "published_date": "2024-03-08 12:42:36 UTC",
      "updated_date": "2024-11-03 18:38:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:42:06.609251"
    },
    {
      "arxiv_id": "2403.05265v2",
      "title": "MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Zinan Zeng",
        "Sen Ye",
        "Zijian Cai",
        "Heng Wang",
        "Yuhan Liu",
        "Haokai Zhang",
        "Minnan Luo"
      ],
      "abstract": "Online movie review websites are valuable for information and discussion\nabout movies. However, the massive spoiler reviews detract from the\nmovie-watching experience, making spoiler detection an important task. Previous\nmethods simply focus on reviews' text content, ignoring the heterogeneity of\ninformation in the platform. For instance, the metadata and the corresponding\nuser's information of a review could be helpful. Besides, the spoiler language\nof movie reviews tends to be genre-specific, thus posing a domain\ngeneralization challenge for existing methods. To this end, we propose MMoE, a\nmulti-modal network that utilizes information from multiple modalities to\nfacilitate robust spoiler detection and adopts Mixture-of-Experts to enhance\ndomain generalization. MMoE first extracts graph, text, and meta feature from\nthe user-movie network, the review's textual content, and the review's metadata\nrespectively. To handle genre-specific spoilers, we then adopt\nMixture-of-Experts architecture to process information in three modalities to\npromote robustness. Finally, we use an expert fusion layer to integrate the\nfeatures from different perspectives and make predictions based on the fused\nembedding. Experiments demonstrate that MMoE achieves state-of-the-art\nperformance on two widely-used spoiler detection datasets, surpassing previous\nSOTA methods by 2.56% and 8.41% in terms of accuracy and F1-score. Further\nexperiments also demonstrate MMoE's superiority in robustness and\ngeneralization.",
      "tldr_zh": "本研究针对在线电影评论中的剧透问题，提出MMoE框架，该框架利用多模态信息（包括用户-电影网络的图特征、评论文本特征和元数据特征）来提升剧透检测的鲁棒性，并引入Domain-aware Mixture-of-Experts架构以处理类型特定的剧透语言挑战。MMoE通过提取并融合这些模态特征，再利用专家融合层进行预测，从而增强模型的领域泛化能力。实验结果显示，MMoE在两个常用数据集上超越现有SOTA方法，准确率提高2.56%、F1分数提高8.41%，并展现出卓越的鲁棒性和泛化性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05265v2",
      "published_date": "2024-03-08 12:42:04 UTC",
      "updated_date": "2024-03-14 03:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:42:17.493592"
    },
    {
      "arxiv_id": "2403.05260v2",
      "title": "Towards generalization of drug response prediction to single cells and patients utilizing importance-aware multi-source domain transfer learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Liu",
        "Wei Duan",
        "Judong Luo"
      ],
      "abstract": "The advancement of single-cell sequencing technology has promoted the\ngeneration of a large amount of single-cell transcriptional profiles, providing\nunprecedented opportunities to identify drug-resistant cell subpopulations\nwithin a tumor. However, few studies have focused on drug response prediction\nat single-cell level, and their performance remains suboptimal. This paper\nproposed scAdaDrug, a novel multi-source domain adaptation model powered by\nadaptive importance-aware representation learning to predict drug response of\nindividual cells. We used a shared encoder to extract domain-invariant features\nrelated to drug response from multiple source domains by utilizing adversarial\ndomain adaptation. Particularly, we introduced a plug-and-play module to\ngenerate importance-aware and mutually independent weights, which could\nadaptively modulate the latent representation of each sample in element-wise\nmanner between source and target domains. Extensive experimental results showed\nthat our model achieved state-of-the-art performance in predicting drug\nresponse on multiple independent datasets, including single-cell datasets\nderived from both cell lines and patient-derived xenografts (PDX) models, as\nwell as clinical tumor patient cohorts. Moreover, the ablation experiments\ndemonstrated our model effectively captured the underlying patterns determining\ndrug response from multiple source domains.",
      "tldr_zh": "本研究针对单细胞测序数据中药物反应预测的挑战，提出了一种新型多源域适应模型scAdaDrug，通过自适应重要性感知表示学习来预测单个细胞和患者的药物反应。模型采用共享编码器结合对抗域适应（adversarial domain adaptation）从多个源域提取与药物反应相关的域不变特征，并引入一个可插拔模块生成元素级重要性权重，以自适应调节源域和目标域的潜在表示。实验结果显示，scAdaDrug在包括细胞系、PDX models和临床肿瘤患者队列的多组独立数据集上实现了最先进性能，准确捕获了决定药物反应的潜在模式，并显著提升了预测效果。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05260v2",
      "published_date": "2024-03-08 12:31:03 UTC",
      "updated_date": "2025-01-07 00:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:42:30.306899"
    },
    {
      "arxiv_id": "2403.05245v2",
      "title": "Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI",
      "title_zh": "翻译失败",
      "authors": [
        "Shoujin Huang",
        "Guanxiong Luo",
        "Xi Wang",
        "Ziran Chen",
        "Yuwan Wang",
        "Huaishui Yang",
        "Pheng-Ann Heng",
        "Lingyan Zhang",
        "Mengye Lyu"
      ],
      "abstract": "In general, diffusion model-based MRI reconstruction methods incrementally\nremove artificially added noise while imposing data consistency to reconstruct\nthe underlying images. However, real-world MRI acquisitions already contain\ninherent noise due to thermal fluctuations. This phenomenon is particularly\nnotable when using ultra-fast, high-resolution imaging sequences for advanced\nresearch, or using low-field systems favored by low- and middle-income\ncountries. These common scenarios can lead to sub-optimal performance or\ncomplete failure of existing diffusion model-based reconstruction techniques.\nSpecifically, as the artificially added noise is gradually removed, the\ninherent MRI noise becomes increasingly pronounced, making the actual noise\nlevel inconsistent with the predefined denoising schedule and consequently\ninaccurate image reconstruction. To tackle this problem, we propose a posterior\nsampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC)\noperation. Extensive experiments are conducted on two public datasets and an\nin-house clinical dataset with field strength ranging from 0.3T to 3T, showing\nthat our method surpasses the state-of-the-art MRI reconstruction methods, and\nis highly robust against various noise levels. The code for Nila is available\nat https://github.com/Solor-pikachu/Nila.",
      "tldr_zh": "本研究针对扩散模型在加速 MRI 重建中的问题，指出现有方法在处理真实固有噪声（如热波动）时会因噪声水平不一致而导致重建失败，尤其在超快速成像或低场强系统中。论文提出一种后验采样策略和新型 NoIse Level Adaptive Data Consistency (Nila-DC) 操作，能够动态适应噪声水平并强制数据一致性，从而提升重建鲁棒性。在两个公共数据集和一个内部临床数据集（场强从 0.3T 到 3T）上的实验显示，该方法优于最先进 MRI 重建技术，并对各种噪声水平表现出高鲁棒性。代码已在 GitHub 上开源。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05245v2",
      "published_date": "2024-03-08 12:07:18 UTC",
      "updated_date": "2024-07-31 14:53:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:42:42.292167"
    },
    {
      "arxiv_id": "2403.05239v1",
      "title": "Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Junyan Wang",
        "Zhenhong Sun",
        "Zhiyu Tan",
        "Xuanbai Chen",
        "Weihua Chen",
        "Hao Li",
        "Cheng Zhang",
        "Yang Song"
      ],
      "abstract": "Vanilla text-to-image diffusion models struggle with generating accurate\nhuman images, commonly resulting in imperfect anatomies such as unnatural\npostures or disproportionate limbs.Existing methods address this issue mostly\nby fine-tuning the model with extra images or adding additional controls --\nhuman-centric priors such as pose or depth maps -- during the image generation\nphase. This paper explores the integration of these human-centric priors\ndirectly into the model fine-tuning stage, essentially eliminating the need for\nextra conditions at the inference stage. We realize this idea by proposing a\nhuman-centric alignment loss to strengthen human-related information from the\ntextual prompts within the cross-attention maps. To ensure semantic detail\nrichness and human structural accuracy during fine-tuning, we introduce\nscale-aware and step-wise constraints within the diffusion process, according\nto an in-depth analysis of the cross-attention layer. Extensive experiments\nshow that our method largely improves over state-of-the-art text-to-image\nmodels to synthesize high-quality human images based on user-written prompts.\nProject page: \\url{https://hcplayercvpr2024.github.io}.",
      "tldr_zh": "本文研究了如何在文本到图像扩散模型中有效整合人类中心先验（human-centric priors），以解决传统模型在生成人类图像时出现的解剖问题，如不自然的姿势或肢体比例失调。作者提出了一种新方法，通过在模型微调阶段直接添加人类中心对齐损失（human-centric alignment loss），强化文本提示中人类相关信息在交叉注意力图（cross-attention maps）中的作用，同时引入尺度感知（scale-aware）和步进约束（step-wise constraints）来确保生成图像的语义细节和结构准确性。该方法无需在推理阶段添加额外条件，实验结果显示其显著优于现有文本到图像模型，能够基于用户提示合成高质量的人类图像。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05239v1",
      "published_date": "2024-03-08 11:59:32 UTC",
      "updated_date": "2024-03-08 11:59:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:42:54.955843"
    },
    {
      "arxiv_id": "2403.05235v1",
      "title": "Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare",
      "title_zh": "公平感知可解释",
      "authors": [
        "Mingxuan Liu",
        "Yilin Ning",
        "Yuhe Ke",
        "Yuqing Shang",
        "Bibhas Chakraborty",
        "Marcus Eng Hock Ong",
        "Roger Vaughan",
        "Nan Liu"
      ],
      "abstract": "The escalating integration of machine learning in high-stakes fields such as\nhealthcare raises substantial concerns about model fairness. We propose an\ninterpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to\nimprove model fairness without compromising performance, featuring an\ninteractive interface to identify a \"fairer\" model from a set of\nhigh-performing models and promoting the integration of data-driven evidence\nand clinical expertise to enhance contextualized fairness. We demonstrated\nFAIM's value in reducing sex and race biases by predicting hospital admission\nwith two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both\ndatasets, FAIM models not only exhibited satisfactory discriminatory\nperformance but also significantly mitigated biases as measured by\nwell-established fairness metrics, outperforming commonly used bias-mitigation\nmethods. Our approach demonstrates the feasibility of improving fairness\nwithout sacrificing performance and provides an a modeling mode that invites\ndomain experts to engage, fostering a multidisciplinary effort toward tailored\nAI fairness.",
      "tldr_zh": "这篇论文提出了Fairness-Aware Interpretable Modeling (FAIM)，一个可解释框架，用于提升医疗领域机器学习模型的公平性，同时保持高性能。FAIM通过交互式界面从一组高性能模型中识别更公平的选项，并整合数据驱动证据与临床专业知识，以实现情境化的公平性改进。在使用MIMIC-IV-ED和SGH-ED两个真实数据库预测医院入院时，FAIM模型不仅展现出满意的区分性能，还显著减少了性别和种族偏见，并在公平性指标上优于传统偏见缓解方法。该方法证明了在不牺牲性能的情况下改善公平性的可行性，并鼓励领域专家参与，推动多学科合作以实现定制化的AI公平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05235v1",
      "published_date": "2024-03-08 11:51:00 UTC",
      "updated_date": "2024-03-08 11:51:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:43:06.867683"
    },
    {
      "arxiv_id": "2403.05229v1",
      "title": "Developing Federated Time-to-Event Scores Using Heterogeneous Real-World Survival Data",
      "title_zh": "翻译失败",
      "authors": [
        "Siqi Li",
        "Yuqing Shang",
        "Ziwen Wang",
        "Qiming Wu",
        "Chuan Hong",
        "Yilin Ning",
        "Di Miao",
        "Marcus Eng Hock Ong",
        "Bibhas Chakraborty",
        "Nan Liu"
      ],
      "abstract": "Survival analysis serves as a fundamental component in numerous healthcare\napplications, where the determination of the time to specific events (such as\nthe onset of a certain disease or death) for patients is crucial for clinical\ndecision-making. Scoring systems are widely used for swift and efficient risk\nprediction. However, existing methods for constructing survival scores presume\nthat data originates from a single source, posing privacy challenges in\ncollaborations with multiple data owners. We propose a novel framework for\nbuilding federated scoring systems for multi-site survival outcomes, ensuring\nboth privacy and communication efficiency. We applied our approach to sites\nwith heterogeneous survival data originating from emergency departments in\nSingapore and the United States. Additionally, we independently developed local\nscores at each site. In testing datasets from each participant site, our\nproposed federated scoring system consistently outperformed all local models,\nevidenced by higher integrated area under the receiver operating characteristic\ncurve (iAUC) values, with a maximum improvement of 11.6%. Additionally, the\nfederated score's time-dependent AUC(t) values showed advantages over local\nscores, exhibiting narrower confidence intervals (CIs) across most time points.\nThe model developed through our proposed method exhibits effective performance\non each local site, signifying noteworthy implications for healthcare research.\nSites participating in our proposed federated scoring model training gained\nbenefits by acquiring survival models with enhanced prediction accuracy and\nefficiency. This study demonstrates the effectiveness of our privacy-preserving\nfederated survival score generation framework and its applicability to\nreal-world heterogeneous survival data.",
      "tldr_zh": "本研究提出了一种新型联邦化(federated)框架，用于构建生存分析(survival analysis)评分系统，该框架处理多站点异构真实世界生存数据，同时确保数据隐私和通信效率。研究者将该方法应用于新加坡和美国的急诊部门数据，并与本地模型进行比较，结果显示联邦评分系统在测试数据集上表现优异，integrated area under the receiver operating characteristic curve (iAUC) 值最高提升11.6%，且time-dependent AUC(t) 值具有更窄的置信区间(CIs)。这一方法为医疗决策提供更准确的预测模型，证明了其在隐私保护下的实际应用价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05229v1",
      "published_date": "2024-03-08 11:32:00 UTC",
      "updated_date": "2024-03-08 11:32:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:43:17.620770"
    },
    {
      "arxiv_id": "2403.05220v1",
      "title": "Synthetic Privileged Information Enhances Medical Image Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Farndale",
        "Chris Walsh",
        "Robert Insall",
        "Ke Yuan"
      ],
      "abstract": "Multimodal self-supervised representation learning has consistently proven to\nbe a highly effective method in medical image analysis, offering strong task\nperformance and producing biologically informed insights. However, these\nmethods heavily rely on large, paired datasets, which is prohibitive for their\nuse in scenarios where paired data does not exist, or there is only a small\namount available. In contrast, image generation methods can work well on very\nsmall datasets, and can find mappings between unpaired datasets, meaning an\neffectively unlimited amount of paired synthetic data can be generated. In this\nwork, we demonstrate that representation learning can be significantly improved\nby synthetically generating paired information, both compared to training on\neither single-modality (up to 4.4x error reduction) or authentic multi-modal\npaired datasets (up to 5.6x error reduction).",
      "tldr_zh": "这篇论文提出，通过合成生成配对信息来增强医疗图像的多模态自监督表示学习（multimodal self-supervised representation learning），以解决依赖大量配对数据集的限制问题。作者利用图像生成方法在小数据集或未配对数据上创建无限的合成配对信息，从而显著改善模型性能。实验结果显示，与单模态训练相比，错误率减少多达4.4倍，与使用真实多模态配对数据集相比，减少多达5.6倍，为医疗图像分析提供更高效的表示学习策略。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.TO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05220v1",
      "published_date": "2024-03-08 11:18:26 UTC",
      "updated_date": "2024-03-08 11:18:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:43:31.172913"
    },
    {
      "arxiv_id": "2403.05217v1",
      "title": "Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Hongda Sun",
        "Yuxuan Liu",
        "Chengwei Wu",
        "Haiyu Yan",
        "Cheng Tai",
        "Xin Gao",
        "Shuo Shang",
        "Rui Yan"
      ],
      "abstract": "Open-domain question answering (ODQA) has emerged as a pivotal research\nspotlight in information systems. Existing methods follow two main paradigms to\ncollect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves\npertinent documents from an external corpus; and (2) the\n\\textit{generate-then-read} paradigm employs large language models (LLMs) to\ngenerate relevant documents. However, neither can fully address multifaceted\nrequirements for evidence. To this end, we propose LLMQA, a generalized\nframework that formulates the ODQA process into three basic steps: query\nexpansion, document selection, and answer generation, combining the superiority\nof both retrieval-based and generation-based evidence. Since LLMs exhibit their\nexcellent capabilities to accomplish various tasks, we instruct LLMs to play\nmultiple roles as generators, rerankers, and evaluators within our framework,\nintegrating them to collaborate in the ODQA process. Furthermore, we introduce\na novel prompt optimization algorithm to refine role-playing prompts and steer\nLLMs to produce higher-quality evidence and answers. Extensive experimental\nresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that\nLLMQA achieves the best performance in terms of both answer accuracy and\nevidence quality, showcasing its potential for advancing ODQA research and\napplications.",
      "tldr_zh": "本研究针对开放域问答（ODQA）的证据收集挑战，提出LLMQA框架，将ODQA过程分为查询扩展、文档选择和答案生成三个步骤，结合retrieve-then-read和generate-then-read的优点。框架利用大型语言模型（LLMs）扮演多重角色，包括生成器、重新排名器和评估器，以协同协作提升证据质量和答案准确性。此外，引入一个新颖的提示优化算法来改进角色扮演提示，进一步提升输出性能。在NQ、WebQ和TriviaQA基准测试中，LLMQA在答案准确性和证据质量方面表现出最佳性能，展示了其在ODQA研究和应用中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "TheWebConf 2024 (WWW 2024) oral, code repo:\n  https://github.com/EthanLeo-LYX/LLMQA",
      "pdf_url": "http://arxiv.org/pdf/2403.05217v1",
      "published_date": "2024-03-08 11:09:13 UTC",
      "updated_date": "2024-03-08 11:09:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:43:42.137028"
    },
    {
      "arxiv_id": "2403.05209v1",
      "title": "Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization",
      "title_zh": "通过半监督领域泛化克服跨领域的数据不平等",
      "authors": [
        "Jinha Park",
        "Wonguk Cho",
        "Taesup Kim"
      ],
      "abstract": "While there have been considerable advancements in machine learning driven by\nextensive datasets, a significant disparity still persists in the availability\nof data across various sources and populations. This inequality across domains\nposes challenges in modeling for those with limited data, which can lead to\nprofound practical and ethical concerns. In this paper, we address a\nrepresentative case of data inequality problem across domains termed\nSemi-Supervised Domain Generalization (SSDG), in which only one domain is\nlabeled while the rest are unlabeled. We propose a novel algorithm, ProUD,\nwhich can effectively learn domain-invariant features via domain-aware\nprototypes along with progressive generalization via uncertainty-adaptive\nmixing of labeled and unlabeled domains. Our experiments on three different\nbenchmark datasets demonstrate the effectiveness of ProUD, outperforming all\nbaseline models including single domain generalization and semi-supervised\nlearning. Source code will be released upon acceptance of the paper.",
      "tldr_zh": "本论文探讨了机器学习中数据不平等问题，特别是在不同领域数据可用性差异导致的挑战，提出了一种新的算法 ProUD，用于 Semi-Supervised Domain Generalization (SSDG)，其中仅一个领域有标签，其余无标签。ProUD 通过域感知 prototypes 学习域不变 features，并采用不确定性自适应 mixing 的渐进泛化策略，有效提升模型在标签不足场景下的泛化能力。在三个基准数据集上的实验表明，ProUD 优于所有基线模型，包括单域泛化和半监督学习方法，为解决数据不平等提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.05209v1",
      "published_date": "2024-03-08 10:49:37 UTC",
      "updated_date": "2024-03-08 10:49:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:43:54.093565"
    },
    {
      "arxiv_id": "2403.05189v1",
      "title": "Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Zhao",
        "Naoki Yoshinaga",
        "Daisuke Oba"
      ],
      "abstract": "Acquiring factual knowledge for language models (LMs) in low-resource\nlanguages poses a serious challenge, thus resorting to cross-lingual transfer\nin multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and\nrepresent factual knowledge. Using the multilingual factual knowledge probing\ndataset, mLAMA, we first conducted a neuron investigation of ML-LMs\n(specifically, multilingual BERT). We then traced the roots of facts back to\nthe knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire\nspecific facts. We finally identified three patterns of acquiring and\nrepresenting facts in ML-LMs: language-independent, cross-lingual shared and\ntransferred, and devised methods for differentiating them. Our findings\nhighlight the challenge of maintaining consistent factual knowledge across\nlanguages, underscoring the need for better fact representation learning in\nML-LMs.",
      "tldr_zh": "这篇论文探讨了多语言语言模型(ML-LMs)如何获取和表示事实知识，特别是针对低资源语言的跨语言转移问题。研究者使用mLAMA数据集对multilingual BERT进行神经元调查，并追踪知识来源至Wikipedia，识别了三种事实获取模式：language-independent（语言无关的）、cross-lingual shared and transferred（跨语言共享和转移）。他们开发了方法来区分这些模式，并指出维护跨语言一致事实知识的挑战，强调需要改进ML-LMs的事实表示学习。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EACL 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2403.05189v1",
      "published_date": "2024-03-08 10:09:57 UTC",
      "updated_date": "2024-03-08 10:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:44:05.924049"
    },
    {
      "arxiv_id": "2403.05175v1",
      "title": "Continual Learning and Catastrophic Forgetting",
      "title_zh": "持续学习和灾难性遗忘",
      "authors": [
        "Gido M. van de Ven",
        "Nicholas Soures",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "This book chapter delves into the dynamics of continual learning, which is\nthe process of incrementally learning from a non-stationary stream of data.\nAlthough continual learning is a natural skill for the human brain, it is very\nchallenging for artificial neural networks. An important reason is that, when\nlearning something new, these networks tend to quickly and drastically forget\nwhat they had learned before, a phenomenon known as catastrophic forgetting.\nEspecially in the last decade, continual learning has become an extensively\nstudied topic in deep learning. This book chapter reviews the insights that\nthis field has generated.",
      "tldr_zh": "这本章节探讨了持续学习（continual learning），即从非平稳数据流中逐步学习的过程。尽管人类大脑自然具备这种能力，但人工神经网络（neural networks）在学习新知识时往往会迅速忘记先前信息，导致灾难性遗忘（catastrophic forgetting）。过去十年，持续学习已成为深度学习（deep learning）领域的热门话题，本章节系统回顾了相关研究见解和挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "q-bio.NC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint of a book chapter; 21 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.05175v1",
      "published_date": "2024-03-08 09:32:43 UTC",
      "updated_date": "2024-03-08 09:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:44:18.196754"
    },
    {
      "arxiv_id": "2403.05171v2",
      "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoying Zhang",
        "Jean-Francois Ton",
        "Wei Shen",
        "Hongning Wang",
        "Yang Liu"
      ],
      "abstract": "We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the\npervasive issue of reward over-optimization in Reinforcement Learning from\nHuman Feedback (RLHF) for Large Language Models (LLMs). Over-optimization\noccurs when a reward model serves as an imperfect proxy for human preference,\nand RL-driven policy optimization erroneously exploits reward inaccuracies. In\nthis paper, we begin by introducing a lightweight way to quantify uncertainties\nin rewards, relying solely on the last layer embeddings of the reward model,\nwithout the need for computationally expensive reward ensembles. AdvPO then\naddresses a distributionally robust optimization problem centred around the\nconfidence interval of the reward model's predictions for policy improvement.\nThrough comprehensive experiments on the Anthropic HH and TL;DR summarization\ndatasets, we illustrate the efficacy of AdvPO in mitigating the\noveroptimization issue, consequently resulting in enhanced performance as\nevaluated through human-assisted evaluation.",
      "tldr_zh": "本研究提出Adversarial Policy Optimization (AdvPO)，一种创新方法，用于解决Reinforcement Learning from Human Feedback (RLHF)中奖励过度优化问题，该问题源于奖励模型作为人类偏好不完美代理，导致政策优化错误利用奖励不准确性。AdvPO采用轻量级不确定性估计，仅基于奖励模型的最后一层嵌入来量化不确定性，并通过针对奖励预测置信区间的分布鲁棒优化问题来改进政策。实验在Anthropic HH和TL;DR总结数据集上表明，AdvPO有效缓解了过度优化问题，并通过人类辅助评估提升了整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05171v2",
      "published_date": "2024-03-08 09:20:12 UTC",
      "updated_date": "2024-07-09 13:17:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:44:30.480858"
    },
    {
      "arxiv_id": "2403.05168v1",
      "title": "Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment",
      "title_zh": "通过无训练代码书优化和分层对齐释放多模态统一离散表示的潜力",
      "authors": [
        "Hai Huang",
        "Yan Xia",
        "Shengpeng Ji",
        "Shulei Wang",
        "Hanting Wang",
        "Jieming Zhu",
        "Zhenhua Dong",
        "Zhou Zhao"
      ],
      "abstract": "Recent advances in representation learning have demonstrated the significance\nof multimodal alignment. The Dual Cross-modal Information Disentanglement\n(DCID) model, utilizing a unified codebook, shows promising results in\nachieving fine-grained representation and cross-modal generalization. However,\nit is still hindered by equal treatment of all channels and neglect of minor\nevent information, resulting in interference from irrelevant channels and\nlimited performance in fine-grained tasks. Thus, in this work, We propose a\nTraining-free Optimization of Codebook (TOC) method to enhance model\nperformance by selecting important channels in the unified space without\nretraining. Additionally, we introduce the Hierarchical Dual Cross-modal\nInformation Disentanglement (H-DCID) approach to extend information separation\nand alignment to two levels, capturing more cross-modal details. The experiment\nresults demonstrate significant improvements across various downstream tasks,\nwith TOC contributing to an average improvement of 1.70% for DCID on four\ntasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of\nTOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These\nfindings highlight the effectiveness of our methods in facilitating robust and\nnuanced cross-modal learning, opening avenues for future enhancements. The\nsource code and pre-trained models can be accessed at\nhttps://github.com/haihuangcode/TOC_H-DCID.",
      "tldr_zh": "这项研究针对多模态统一离散表示中的问题，提出了无训练代码本优化(TOC)方法和分层双跨模态信息分离(H-DCID)方法，以解决现有模型如 DCID 在处理通道干扰和细粒度任务上的局限性。TOC 通过在统一空间选择重要通道，而无需重新训练，从而提升模型性能；H-DCID 则将信息分离和对齐扩展到两个层次，捕捉更多跨模态细节。实验结果显示，TOC 使 DCID 在四个下游任务上平均提升 1.70%，H-DCID 比 DCID 提升 3.64%，二者结合进一步超过 DCID 4.43%。这些创新方法显著提高了跨模态学习的鲁棒性和细微性，并提供了开源代码以支持进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05168v1",
      "published_date": "2024-03-08 09:16:47 UTC",
      "updated_date": "2024-03-08 09:16:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:44:44.697809"
    },
    {
      "arxiv_id": "2403.05164v1",
      "title": "Synthetic data generation for system identification: leveraging knowledge transfer from similar systems",
      "title_zh": "翻译失败",
      "authors": [
        "Dario Piga",
        "Matteo Rufolo",
        "Gabriele Maroni",
        "Manas Mejari",
        "Marco Forgione"
      ],
      "abstract": "This paper addresses the challenge of overfitting in the learning of\ndynamical systems by introducing a novel approach for the generation of\nsynthetic data, aimed at enhancing model generalization and robustness in\nscenarios characterized by data scarcity. Central to the proposed methodology\nis the concept of knowledge transfer from systems within the same class.\nSpecifically, synthetic data is generated through a pre-trained meta-model that\ndescribes a broad class of systems to which the system of interest is assumed\nto belong. Training data serves a dual purpose: firstly, as input to the\npre-trained meta model to discern the system's dynamics, enabling the\nprediction of its behavior and thereby generating synthetic output sequences\nfor new input sequences; secondly, in conjunction with synthetic data, to\ndefine the loss function used for model estimation. A validation dataset is\nused to tune a scalar hyper-parameter balancing the relative importance of\ntraining and synthetic data in the definition of the loss function. The same\nvalidation set can be also used for other purposes, such as early stopping\nduring the training, fundamental to avoid overfitting in case of small-size\ntraining datasets. The efficacy of the approach is shown through a numerical\nexample that highlights the advantages of integrating synthetic data into the\nsystem identification process.",
      "tldr_zh": "本文提出了一种生成合成数据（synthetic data）的创新方法，用于解决动态系统识别（system identification）中的过拟合问题，通过从相同类别的系统进行知识转移（knowledge transfer），从而提升模型的泛化性和鲁棒性，尤其在数据稀缺场景下。核心方法利用预训练的元模型（pre-trained meta-model）基于训练数据预测系统动态，并生成新的合成输出序列，同时将训练数据和合成数据整合到损失函数（loss function）的定义中。验证数据集用于调整一个标量超参数，以平衡训练数据和合成数据的权重，并支持其他功能如提前停止训练以避免过拟合。该方法通过数值例子证明了其有效性，展示了在系统识别过程中整合合成数据的显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05164v1",
      "published_date": "2024-03-08 09:09:15 UTC",
      "updated_date": "2024-03-08 09:09:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:44:56.479494"
    },
    {
      "arxiv_id": "2403.05158v2",
      "title": "Adaptive Split Learning over Energy-Constrained Wireless Edge Networks",
      "title_zh": "能量受限无线边缘网络中的自适应分割学习",
      "authors": [
        "Zuguang Li",
        "Wen Wu",
        "Shaohua Wu",
        "Wei Wang"
      ],
      "abstract": "Split learning (SL) is a promising approach for training artificial\nintelligence (AI) models, in which devices collaborate with a server to train\nan AI model in a distributed manner, based on a same fixed split point.\nHowever, due to the device heterogeneity and variation of channel conditions,\nthis way is not optimal in training delay and energy consumption. In this\npaper, we design an adaptive split learning (ASL) scheme which can dynamically\nselect split points for devices and allocate computing resource for the server\nin wireless edge networks. We formulate an optimization problem to minimize the\naverage training latency subject to long-term energy consumption constraint.\nThe difficulties in solving this problem are the lack of future information and\nmixed integer programming (MIP). To solve it, we propose an online algorithm\nleveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP\nproblem only with the current information. Then, a two-layer optimization\nmethod is proposed to solve the MIP problem. Extensive simulation results\ndemonstrate that the ASL scheme can reduce the average training delay and\nenergy consumption by 53.7% and 22.1%, respectively, as compared to the\nexisting SL schemes.",
      "tldr_zh": "该论文针对传统Split Learning (SL) 在无线边缘网络中存在的训练延迟和能耗问题，提出了一种Adaptive Split Learning (ASL) 方案，能够动态选择设备分割点并为服务器分配计算资源。研究者构建了一个优化问题，旨在最小化平均训练延迟，同时满足长期能耗约束。利用Lyapunov理论，他们开发了在线算法OPEN，将问题分解为基于当前信息的混合整数规划 (MIP) 问题，并通过两层优化方法求解。实验结果显示，与现有SL方案相比，ASL方案将平均训练延迟和能耗分别降低了53.7%和22.1%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 5 figures, 20 conferences",
      "pdf_url": "http://arxiv.org/pdf/2403.05158v2",
      "published_date": "2024-03-08 08:51:37 UTC",
      "updated_date": "2025-03-13 13:27:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:45:06.090637"
    },
    {
      "arxiv_id": "2403.05152v3",
      "title": "Towards a Psychology of Machines: Large Language Models Predict Human Memory",
      "title_zh": "迈向机器心理学：大语言模型预测人类记忆",
      "authors": [
        "Markus Huff",
        "Elanur Ulakçı"
      ],
      "abstract": "Large language models (LLMs), such as ChatGPT, have shown remarkable\nabilities in natural language processing, opening new avenues in psychological\nresearch. This study explores whether LLMs can predict human memory performance\nin tasks involving garden-path sentences and contextual information. In the\nfirst part, we used ChatGPT to rate the relatedness and memorability of\ngarden-path sentences preceded by either fitting or unfitting contexts. In the\nsecond part, human participants read the same sentences, rated their\nrelatedness, and completed a surprise memory test. The results demonstrated\nthat ChatGPT's relatedness ratings closely matched those of the human\nparticipants, and its memorability ratings effectively predicted human memory\nperformance. Both LLM and human data revealed that higher relatedness in the\nunfitting context condition was associated with better memory performance,\naligning with probabilistic frameworks of context-dependent learning. These\nfindings suggest that LLMs, despite lacking human-like memory mechanisms, can\nmodel aspects of human cognition and serve as valuable tools in psychological\nresearch. We propose the field of machine psychology to explore this interplay\nbetween human cognition and artificial intelligence, offering a bidirectional\napproach where LLMs can both benefit from and contribute to our understanding\nof human cognitive processes.",
      "tldr_zh": "这篇论文探讨大型语言模型 (LLMs) 如 ChatGPT 是否能预测人类在处理 garden-path sentences 和上下文信息的记忆任务中的表现。研究方法包括使用 ChatGPT 评估句子的相关性和可记忆性，并与人类参与者阅读相同句子、进行相关性评分和意外记忆测试的结果进行比较。结果显示，ChatGPT 的评分与人类高度一致，且其可记忆性预测准确地反映了人类记忆性能，支持了概率框架的上下文依赖学习理论。最后，论文提出 machine psychology 领域，强调 LLMs 尽管缺乏人类记忆机制，却能模拟认知过程并助力心理学研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.05152v3",
      "published_date": "2024-03-08 08:41:14 UTC",
      "updated_date": "2024-12-04 19:01:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:45:19.512890"
    },
    {
      "arxiv_id": "2403.05149v1",
      "title": "Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Ceyao Zhang",
        "Renjie Li",
        "Cheng Zhang",
        "Zhaoyu Zhang",
        "Feng Yin"
      ],
      "abstract": "Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands\nexpert knowledge in physics, materials science, and quantum mechanics which is\nprohibitively labor-intensive. Advanced AI technologies, especially\nreinforcement learning (RL), have emerged as a powerful tool to augment and\naccelerate this inverse design process. By modeling the inverse design of PCSEL\nas a sequential decision-making problem, RL approaches can construct a\nsatisfactory PCSEL structure from scratch. However, the data inefficiency\nresulting from online interactions with precise and expensive simulation\nenvironments impedes the broader applicability of RL approaches. Recently,\nsequential models, especially the Transformer architecture, have exhibited\ncompelling performance in sequential decision-making problems due to their\nsimplicity and scalability to large language models. In this paper, we\nintroduce a novel framework named PCSEL Inverse Design Transformer (PiT) that\nabstracts the inverse design of PCSEL as a sequence modeling problem. The\ncentral part of our PiT is a Transformer-based structure that leverages the\npast trajectories and current states to predict the current actions. Compared\nwith the traditional RL approaches, PiT can output the optimal actions and\nachieve target PCSEL designs by leveraging offline data and conditioning on the\ndesired return. Results demonstrate that PiT achieves superior performance and\ndata efficiency compared to baselines.",
      "tldr_zh": "该论文将光子晶体表面发射激光器（Photonic Crystal Surface Emitting Lasers, PCSEL）的反向设计问题视为一个序列建模任务，以解决其对物理、材料科学和量子力学专业知识的依赖和劳动密集型挑战。作者引入了名为 PiT（PCSEL Inverse Design Transformer）的创新框架，利用 Transformer 架构基于离线数据和过去轨迹预测当前最优动作，从而避免了传统强化学习（RL）方法在线交互的低数据效率问题。与基线方法相比，PiT 在性能和数据效率上表现出色，能够高效生成目标 PCSEL 设计。",
      "categories": [
        "physics.app-ph",
        "cs.AI"
      ],
      "primary_category": "physics.app-ph",
      "comment": "accepted by AAAI workshop\n  AI2ASE(2024)https://ai-2-ase.github.io/papers/29%5cCameraReady%5cPIT__PSCEL_inverse_design_transformer.pdf",
      "pdf_url": "http://arxiv.org/pdf/2403.05149v1",
      "published_date": "2024-03-08 08:38:50 UTC",
      "updated_date": "2024-03-08 08:38:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:45:31.686286"
    },
    {
      "arxiv_id": "2403.05132v1",
      "title": "ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models",
      "title_zh": "ChatUIE：使用大型语言模型探索基于聊天的统一信息提取",
      "authors": [
        "Jun Xu",
        "Mengshu Sun",
        "Zhiqiang Zhang",
        "Jun Zhou"
      ],
      "abstract": "Recent advancements in large language models have shown impressive\nperformance in general chat. However, their domain-specific capabilities,\nparticularly in information extraction, have certain limitations. Extracting\nstructured information from natural language that deviates from known schemas\nor instructions has proven challenging for previous prompt-based methods. This\nmotivated us to explore domain-specific modeling in chat-based language models\nas a solution for extracting structured information from natural language. In\nthis paper, we present ChatUIE, an innovative unified information extraction\nframework built upon ChatGLM. Simultaneously, reinforcement learning is\nemployed to improve and align various tasks that involve confusing and limited\nsamples. Furthermore, we integrate generation constraints to address the issue\nof generating elements that are not present in the input. Our experimental\nresults demonstrate that ChatUIE can significantly improve the performance of\ninformation extraction with a slight decrease in chatting ability.",
      "tldr_zh": "该论文探讨了大型语言模型（Large Language Models）在信息提取领域的局限性，特别是处理偏离已知模式的自然语言时。作者提出ChatUIE框架，这是一个基于ChatGLM的统一信息提取系统，通过强化学习（reinforcement learning）来优化和对齐各种任务，并整合生成约束以避免产生输入中不存在的元素。实验结果显示，ChatUIE显著提升了信息提取性能，同时聊天能力仅略有下降。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05132v1",
      "published_date": "2024-03-08 07:59:19 UTC",
      "updated_date": "2024-03-08 07:59:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:45:41.221246"
    },
    {
      "arxiv_id": "2403.05131v2",
      "title": "Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation",
      "title_zh": "Sora 作为 AGI 世界模型？ 文本到视频生成的完整调查",
      "authors": [
        "Joseph Cho",
        "Fachrina Dewi Puspitasari",
        "Sheng Zheng",
        "Jingyao Zheng",
        "Lik-Hang Lee",
        "Tae-Ho Kim",
        "Choong Seon Hong",
        "Chaoning Zhang"
      ],
      "abstract": "The evolution of video generation from text, starting with animating MNIST\nnumbers to simulating the physical world with Sora, has progressed at a\nbreakneck speed over the past seven years. While often seen as a superficial\nexpansion of the predecessor text-to-image generation model, text-to-video\ngeneration models are developed upon carefully engineered constituents. Here,\nwe systematically discuss these elements consisting of but not limited to core\nbuilding blocks (vision, language, and temporal) and supporting features from\nthe perspective of their contributions to achieving a world model. We employ\nthe PRISMA framework to curate 97 impactful research articles from renowned\nscientific databases primarily studying video synthesis using text conditions.\nUpon minute exploration of these manuscripts, we observe that text-to-video\ngeneration involves more intricate technologies beyond the plain extension of\ntext-to-image generation. Our additional review into the shortcomings of\nSora-generated videos pinpoints the call for more in-depth studies in various\nenabling aspects of video generation such as dataset, evaluation metric,\nefficient architecture, and human-controlled generation. Finally, we conclude\nthat the study of the text-to-video generation may still be in its infancy,\nrequiring contribution from the cross-discipline research community towards its\nadvancement as the first step to realize artificial general intelligence (AGI).",
      "tldr_zh": "这篇论文对文本到视频生成（text-to-video generation）进行了全面综述，探讨其从早期简单模型发展到像 Sora 这样的先进系统，并评估其作为 AGI 世界模型的潜力。作者采用 PRISMA 框架筛选了 97 篇相关研究，系统分析了核心构建块（vision, language, and temporal）以及支持特征，强调了该技术比文本到图像生成更复杂。研究发现，Sora 生成视频存在不足，如数据集、evaluation metric、efficient architecture 和人类控制方面的问题，并呼吁跨学科合作以推进该领域，作为实现 AGI 的第一步。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "First complete survey on Text-to-Video Generation, 44 pages, 20\n  figures",
      "pdf_url": "http://arxiv.org/pdf/2403.05131v2",
      "published_date": "2024-03-08 07:58:13 UTC",
      "updated_date": "2024-06-07 07:40:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:45:56.242816"
    },
    {
      "arxiv_id": "2403.05130v2",
      "title": "From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on Knowledge Graphs",
      "title_zh": "从链状到树状：在知识图谱上细化链状规则为树状规则",
      "authors": [
        "Wangtao Sun",
        "Shizhu He",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "With good explanatory power and controllability, rule-based methods play an\nimportant role in many tasks such as knowledge reasoning and decision support.\nHowever, existing studies primarily focused on learning chain-like rules, which\nlimit their semantic expressions and accurate prediction abilities. As a\nresult, chain-like rules usually fire on the incorrect grounding values,\nproducing inaccurate or even erroneous reasoning results. In this paper, we\npropose the concept of tree-like rules on knowledge graphs to expand the\napplication scope and improve the reasoning ability of rule-based methods.\nMeanwhile, we propose an effective framework for refining chain-like rules into\ntree-like rules. Experimental comparisons on four public datasets show that the\nproposed framework can easily adapt to other chain-like rule induction methods\nand the refined tree-like rules consistently achieve better performances than\nchain-like rules on link prediction. The data and code of this paper can be\navailable at https://anonymous.4open.science/r/tree-rule-E3CD/.",
      "tldr_zh": "本文研究了知识图谱（knowledge graphs）上规则方法的局限性，指出现有的链式规则（chain-like rules）在语义表达和预测准确性上存在不足，常导致错误推理结果。为此，论文提出树状规则（tree-like rules）的概念，并设计了一个框架来将链式规则精炼成树状规则，从而扩展应用范围并提升推理能力。在四个公开数据集上的实验显示，该框架能轻松适应其他链式规则诱导方法，且树状规则在链接预测（link prediction）任务中比链式规则表现更优。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05130v2",
      "published_date": "2024-03-08 07:55:42 UTC",
      "updated_date": "2025-01-05 03:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:46:07.030863"
    },
    {
      "arxiv_id": "2403.05129v1",
      "title": "Unraveling the Molecular Magic: AI Insights on the Formation of Extraordinarily Stretchable Hydrogels",
      "title_zh": "翻译失败",
      "authors": [
        "Shahriar Hojjati Emmami",
        "Ali Pilehvar Meibody",
        "Lobat Tayebi",
        "Mohammadamin Tavakoli",
        "Pierre Baldi"
      ],
      "abstract": "The deliberate manipulation of ammonium persulfate, methylenebisacrylamide,\ndimethyleacrylamide, and polyethylene oxide concentrations resulted in the\ndevelopment of a hydrogel with an exceptional stretchability, capable of\nextending up to 260 times its original length. This study aims to elucidate the\nmolecular architecture underlying this unique phenomenon by exploring potential\nreaction mechanisms, facilitated by an artificial intelligence prediction\nsystem. Artificial intelligence predictor introduces a novel approach to\ninterlinking two polymers, involving the formation of networks interconnected\nwith linear chains following random chain scission. This novel configuration\nleads to the emergence of a distinct type of hydrogel, herein referred to as a\n\"Span Network.\" Additionally, Fourier-transform infrared spectroscopy (FTIR) is\nused to investigate functional groups that may be implicated in the proposed\nmechanism, with ester formation confirmed among numerous hydroxyl end groups\nobtained from chain scission of PEO and carboxyl groups formed on hydrogel\nnetworks.",
      "tldr_zh": "本研究通过精确调控ammonium persulfate、methylenebisacrylamide、dimethyleacrylamide和polyethylene oxide的浓度，成功开发了一种可伸展至原长260倍的超伸展水凝胶。利用AI预测系统，研究者探索了该水凝胶的分子结构和反应机制，提出了一种新型“Span Network”配置，其中聚合物网络通过随机链断裂与线性链互连。FTIR光谱分析进一步证实了酯键的形成，涉及PEO的羟基端基和水凝胶网络的羟基基团。该方法为理解和设计高性能水凝胶提供了新见解。",
      "categories": [
        "cond-mat.soft",
        "cs.AI"
      ],
      "primary_category": "cond-mat.soft",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05129v1",
      "published_date": "2024-03-08 07:52:17 UTC",
      "updated_date": "2024-03-08 07:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:46:19.305898"
    },
    {
      "arxiv_id": "2403.15416v1",
      "title": "Fuzzy hyperparameters update in a second order optimization",
      "title_zh": "二",
      "authors": [
        "Abdelaziz Bensadok",
        "Muhammad Zeeshan Babar"
      ],
      "abstract": "This research will present a hybrid approach to accelerate convergence in a\nsecond order optimization. An online finite difference approximation of the\ndiagonal Hessian matrix will be introduced, along with fuzzy inferencing of\nseveral hyperparameters. Competitive results have been achieved",
      "tldr_zh": "本研究提出了一种混合方法，用于加速second order optimization中的收敛。具体而言，该方法包括在线有限差分逼近对角Hessian matrix，以及对几个超参数进行fuzzy inferencing，以优化优化过程。实验结果显示，该方法取得了竞争性的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15416v1",
      "published_date": "2024-03-08 07:47:27 UTC",
      "updated_date": "2024-03-08 07:47:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:46:30.940976"
    },
    {
      "arxiv_id": "2403.05125v2",
      "title": "Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis",
      "title_zh": "评估文本到图像生成模型：人类图像合成的实证研究",
      "authors": [
        "Muxi Chen",
        "Yi Liu",
        "Jian Yi",
        "Changran Xu",
        "Qiuxia Lai",
        "Hongliang Wang",
        "Tsung-Yi Ho",
        "Qiang Xu"
      ],
      "abstract": "In this paper, we present an empirical study introducing a nuanced evaluation\nframework for text-to-image (T2I) generative models, applied to human image\nsynthesis. Our framework categorizes evaluations into two distinct groups:\nfirst, focusing on image qualities such as aesthetics and realism, and second,\nexamining text conditions through concept coverage and fairness. We introduce\nan innovative aesthetic score prediction model that assesses the visual appeal\nof generated images and unveils the first dataset marked with low-quality\nregions in generated human images to facilitate automatic defect detection. Our\nexploration into concept coverage probes the model's effectiveness in\ninterpreting and rendering text-based concepts accurately, while our analysis\nof fairness reveals biases in model outputs, with an emphasis on gender, race,\nand age. While our study is grounded in human imagery, this dual-faceted\napproach is designed with the flexibility to be applicable to other forms of\nimage generation, enhancing our understanding of generative models and paving\nthe way to the next generation of more sophisticated, contextually aware, and\nethically attuned generative models. Code and data, including the dataset\nannotated with defective areas, are available at\n\\href{https://github.com/cure-lab/EvaluateAIGC}{https://github.com/cure-lab/EvaluateAIGC}.",
      "tldr_zh": "本研究提出了一种细致的评估框架，用于评估文本到图像 (T2I) 生成模型在人类图像合成中的表现，将评估分为图像质量（如美学和真实性）和文本条件（如概念覆盖及公平性）两类。研究贡献包括一个创新的美学分数预测模型，用于量化图像视觉吸引力，以及一个首创数据集，标记了生成人类图像中的低质量区域，以支持自动缺陷检测。实验分析揭示了模型在概念覆盖方面的准确性以及在性别、种族和年龄等方面的偏见问题，为开发更高级、上下文感知和伦理友好的生成模型提供了新见解。代码和数据集可从指定 GitHub 仓库获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05125v2",
      "published_date": "2024-03-08 07:41:47 UTC",
      "updated_date": "2024-10-28 09:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:46:43.994737"
    },
    {
      "arxiv_id": "2403.05112v1",
      "title": "RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Tanvi Verma",
        "Linh Le Dinh",
        "Nicholas Tan",
        "Xinxing Xu",
        "Chingyu Cheng",
        "Yong Liu"
      ],
      "abstract": "Visual perimetry is an important eye examination that helps detect vision\nproblems caused by ocular or neurological conditions. During the test, a\npatient's gaze is fixed at a specific location while light stimuli of varying\nintensities are presented in central and peripheral vision. Based on the\npatient's responses to the stimuli, the visual field mapping and sensitivity\nare determined. However, maintaining high levels of concentration throughout\nthe test can be challenging for patients, leading to increased examination\ntimes and decreased accuracy.\n  In this work, we present RLPeri, a reinforcement learning-based approach to\noptimize visual perimetry testing. By determining the optimal sequence of\nlocations and initial stimulus values, we aim to reduce the examination time\nwithout compromising accuracy. Additionally, we incorporate reward shaping\ntechniques to further improve the testing performance. To monitor the patient's\nresponses over time during testing, we represent the test's state as a pair of\n3D matrices. We apply two different convolutional kernels to extract spatial\nfeatures across locations as well as features across different stimulus values\nfor each location. Through experiments, we demonstrate that our approach\nresults in a 10-20% reduction in examination time while maintaining the\naccuracy as compared to state-of-the-art methods. With the presented approach,\nwe aim to make visual perimetry testing more efficient and patient-friendly,\nwhile still providing accurate results.",
      "tldr_zh": "本文提出 RLPeri，一种基于 reinforcement learning 的方法，用于优化视觉 perimetry 测试，通过确定最佳位置序列和初始刺激值，并结合 reward shaping 技术来减少测试时间。测试状态被表示为 3D 矩阵对，并应用两种 convolutional kernels 来提取空间特征和刺激值特征。实验结果显示，与现有方法相比，RLPeri 能将测试时间减少 10-20%，同时保持准确性，从而使视觉 perimetry 测试更高效且患者友好。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at AAAI-24",
      "pdf_url": "http://arxiv.org/pdf/2403.05112v1",
      "published_date": "2024-03-08 07:19:43 UTC",
      "updated_date": "2024-03-08 07:19:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:46:56.219978"
    },
    {
      "arxiv_id": "2403.05606v1",
      "title": "A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Wu",
        "Yang Liu",
        "Yue Yang",
        "Michael S. Yao",
        "Wenli Yang",
        "Xuehui Shi",
        "Lihong Yang",
        "Dongjun Li",
        "Yueming Liu",
        "James C. Gee",
        "Xuan Yang",
        "Wenbin Wei",
        "Shi Gu"
      ],
      "abstract": "Diagnosing rare diseases presents a common challenge in clinical practice,\nnecessitating the expertise of specialists for accurate identification. The\nadvent of machine learning offers a promising solution, while the development\nof such technologies is hindered by the scarcity of data on rare conditions and\nthe demand for models that are both interpretable and trustworthy in a clinical\ncontext. Interpretable AI, with its capacity for human-readable outputs, can\nfacilitate validation by clinicians and contribute to medical education. In the\ncurrent work, we focus on choroid neoplasias, the most prevalent form of eye\ncancer in adults, albeit rare with 5.1 per million. We built the so-far largest\ndataset consisting of 750 patients, incorporating three distinct imaging\nmodalities collected from 2004 to 2022. Our work introduces a concept-based\ninterpretable model that distinguishes between three types of choroidal tumors,\nintegrating insights from domain experts via radiological reports. Remarkably,\nthis model not only achieves an F1 score of 0.91, rivaling that of black-box\nmodels, but also boosts the diagnostic accuracy of junior doctors by 42%. This\nstudy highlights the significant potential of interpretable machine learning in\nimproving the diagnosis of rare diseases, laying a groundwork for future\nbreakthroughs in medical AI that could tackle a wider array of complex health\nscenarios.",
      "tldr_zh": "本研究针对罕见疾病诊断的挑战，特别是脉络膜肿瘤（choroid neoplasias），提出了一种基于概念的 interpretable model，利用多模态数据（三种成像模式）并整合领域专家的放射学报告。研究构建了迄今为止最大的数据集，涵盖750名患者的数据，模型能够区分三种脉络膜肿瘤类型，并实现F1 score高达0.91的性能，与黑盒模型相当。实验结果显示，该模型将初级医生的诊断准确性提高了42%，突显了interpretable machine learning在提升罕见疾病诊断和医疗教育中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05606v1",
      "published_date": "2024-03-08 07:15:53 UTC",
      "updated_date": "2024-03-08 07:15:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:47:07.483205"
    },
    {
      "arxiv_id": "2403.05110v2",
      "title": "Efficient Data Collection for Robotic Manipulation via Compositional Generalization",
      "title_zh": "通过组合泛化的机器人操作高效数据收集",
      "authors": [
        "Jensen Gao",
        "Annie Xie",
        "Ted Xiao",
        "Chelsea Finn",
        "Dorsa Sadigh"
      ],
      "abstract": "Data collection has become an increasingly important problem in robotic\nmanipulation, yet there still lacks much understanding of how to effectively\ncollect data to facilitate broad generalization. Recent works on large-scale\nrobotic data collection typically vary many environmental factors of variation\n(e.g., object types, table textures) during data collection, to cover a diverse\nrange of scenarios. However, they do not explicitly account for the possible\ncompositional abilities of policies trained on the data. If robot policies can\ncompose environmental factors from their data to succeed when encountering\nunseen factor combinations, we can exploit this to avoid collecting data for\nsituations that composition would address. To investigate this possibility, we\nconduct thorough empirical studies both in simulation and on a real robot that\ncompare data collection strategies and assess whether visual imitation learning\npolicies can compose environmental factors. We find that policies do exhibit\ncomposition, although leveraging prior robotic datasets is critical for this on\na real robot. We use these insights to propose better in-domain data collection\nstrategies that exploit composition, which can induce better generalization\nthan naive approaches for the same amount of effort during data collection. We\nfurther demonstrate that a real robot policy trained on data from such a\nstrategy achieves a success rate of 77.5% when transferred to entirely new\nenvironments that encompass unseen combinations of environmental factors,\nwhereas policies trained using data collected without accounting for\nenvironmental variation fail to transfer effectively, with a success rate of\nonly 2.5%. We provide videos at http://iliad.stanford.edu/robot-data-comp/.",
      "tldr_zh": "该研究探讨了通过组合泛化（compositional generalization）来优化机器人操作（robotic manipulation）的数据收集策略，以实现更广泛的泛化。作者通过模拟和真实机器人实验比较了不同数据收集方法，发现视觉模仿学习（visual imitation learning）策略能够组合环境因素（如物体类型和桌面纹理），但需依赖现有机器人数据集。基于此，他们提出了一种新策略，利用组合能力减少不必要的数据收集，从而在相同努力下提升泛化性能；实验结果显示，使用该策略训练的真实机器人策略在新环境中成功率达77.5%，远高于忽略环境变化的基线策略的2.5%。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "RSS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05110v2",
      "published_date": "2024-03-08 07:15:38 UTC",
      "updated_date": "2024-05-21 14:18:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:47:19.879225"
    },
    {
      "arxiv_id": "2403.05108v1",
      "title": "A Task-Driven Multi-UAV Coalition Formation Mechanism",
      "title_zh": "任务驱动的多无人机联盟形成机制",
      "authors": [
        "Xinpeng Lu",
        "Heng Song",
        "Huailing Ma",
        "Junwu Zhu"
      ],
      "abstract": "With the rapid advancement of UAV technology, the problem of UAV coalition\nformation has become a hotspot. Therefore, designing task-driven multi-UAV\ncoalition formation mechanism has become a challenging problem. However,\nexisting coalition formation mechanisms suffer from low relevance between UAVs\nand task requirements, resulting in overall low coalition utility and unstable\ncoalition structures. To address these problems, this paper proposed a novel\nmulti-UAV coalition network collaborative task completion model, considering\nboth coalition work capacity and task-requirement relationships. This model\nstimulated the formation of coalitions that match task requirements by using a\nrevenue function based on the coalition's revenue threshold. Subsequently, an\nalgorithm for coalition formation based on marginal utility was proposed.\nSpecifically, the algorithm utilized Shapley value to achieve fair utility\ndistribution within the coalition, evaluated coalition values based on marginal\nutility preference order, and achieved stable coalition partition through a\nlimited number of iterations. Additionally, we theoretically proved that this\nalgorithm has Nash equilibrium solution. Finally, experimental results\ndemonstrated that the proposed algorithm, compared to currently classical\nalgorithms, not only forms more stable coalitions but also further enhances the\noverall utility of coalitions effectively.",
      "tldr_zh": "该论文针对多-UAV（Unmanned Aerial Vehicle）联盟形成问题，提出了一种任务驱动机制，以解决现有方法中UAV与任务需求相关性低导致的联盟效用低和结构不稳定问题。机制基于一个考虑联盟工作能力和任务需求关系的协作模型，使用收益函数和收益阈值来促进匹配任务的联盟形成，并引入基于边际效用的算法，该算法利用Shapley value实现公平效用分配，并通过迭代评估边际效用偏好顺序以实现稳定联盟划分。实验结果显示，该算法不仅比经典算法形成更稳定的联盟结构，还显著提升了整体联盟效用，并理论证明了其具有Nash equilibrium解决方案。",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05108v1",
      "published_date": "2024-03-08 07:10:46 UTC",
      "updated_date": "2024-03-08 07:10:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:47:32.005238"
    },
    {
      "arxiv_id": "2403.05105v1",
      "title": "Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval",
      "title_zh": "学习重新匹配不匹配配对以实现鲁棒的跨模态检索",
      "authors": [
        "Haochen Han",
        "Qinghua Zheng",
        "Guang Dai",
        "Minnan Luo",
        "Jingdong Wang"
      ],
      "abstract": "Collecting well-matched multimedia datasets is crucial for training\ncross-modal retrieval models. However, in real-world scenarios, massive\nmultimodal data are harvested from the Internet, which inevitably contains\nPartially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data\nwill remarkably harm the cross-modal retrieval performance. Previous efforts\ntend to mitigate this problem by estimating a soft correspondence to\ndown-weight the contribution of PMPs. In this paper, we aim to address this\nchallenge from a new perspective: the potential semantic similarity among\nunpaired samples makes it possible to excavate useful knowledge from mismatched\npairs. To achieve this, we propose L2RM, a general framework based on Optimal\nTransport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to\ngenerate refined alignments by seeking a minimal-cost transport plan across\ndifferent modalities. To formalize the rematching idea in OT, first, we propose\na self-supervised cost function that automatically learns from explicit\nsimilarity-cost mapping relation. Second, we present to model a partial OT\nproblem while restricting the transport among false positives to further boost\nrefined alignments. Extensive experiments on three benchmarks demonstrate our\nL2RM significantly improves the robustness against PMPs for existing models.\nThe code is available at https://github.com/hhc1997/L2RM.",
      "tldr_zh": "该研究针对跨模态检索（Cross-Modal Retrieval）中数据中存在的部分不匹配对（Partially Mismatched Pairs, PMPs）问题，提出了一种新的框架 L2RM，以提升模型的鲁棒性。L2RM 基于 Optimal Transport (OT) 技术，通过自监督成本函数学习显式相似性-成本映射，并建模部分 OT 问题来生成精炼的 alignments，从而从不匹配对中挖掘潜在语义相似性。实验在三个基准数据集上证明，L2RM 显著提高了现有模型对 PMPs 的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05105v1",
      "published_date": "2024-03-08 07:09:30 UTC",
      "updated_date": "2024-03-08 07:09:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:47:44.900723"
    },
    {
      "arxiv_id": "2403.05104v1",
      "title": "How Culture Shapes What People Want From AI",
      "title_zh": "文化如何塑造人们对 AI 的期望",
      "authors": [
        "Xiao Ge",
        "Chunchen Xu",
        "Daigo Misaki",
        "Hazel Rose Markus",
        "Jeanne L Tsai"
      ],
      "abstract": "There is an urgent need to incorporate the perspectives of culturally diverse\ngroups into AI developments. We present a novel conceptual framework for\nresearch that aims to expand, reimagine, and reground mainstream visions of AI\nusing independent and interdependent cultural models of the self and the\nenvironment. Two survey studies support this framework and provide preliminary\nevidence that people apply their cultural models when imagining their ideal AI.\nCompared with European American respondents, Chinese respondents viewed it as\nless important to control AI and more important to connect with AI, and were\nmore likely to prefer AI with capacities to influence. Reflecting both cultural\nmodels, findings from African American respondents resembled both European\nAmerican and Chinese respondents. We discuss study limitations and future\ndirections and highlight the need to develop culturally responsive and relevant\nAI to serve a broader segment of the world population.",
      "tldr_zh": "本研究提出一个新概念框架，利用独立和相互依赖的文化模型，探讨如何将文化多样性视角融入AI开发，以扩展主流AI愿景。研究通过两个调查比较欧洲裔美国人、中国受访者和非洲裔美国人，发现欧洲裔美国人更强调对AI的控制，而中国人更注重与AI的连接，并更倾向于AI具有影响力的能力。结果显示，非洲裔美国人受访者的偏好兼具两者特征，为开发文化响应式AI提供了初步证据，并强调未来需关注AI的包容性和全球适用性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "To appear at CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05104v1",
      "published_date": "2024-03-08 07:08:19 UTC",
      "updated_date": "2024-03-08 07:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:47:55.278837"
    },
    {
      "arxiv_id": "2403.05101v3",
      "title": "Rule-driven News Captioning",
      "title_zh": "规则驱动的新闻标题生成",
      "authors": [
        "Ning Xu",
        "Tingting Zhang",
        "Hongshuo Tian",
        "An-An Liu"
      ],
      "abstract": "News captioning task aims to generate sentences by describing named entities\nor concrete events for an image with its news article. Existing methods have\nachieved remarkable results by relying on the large-scale pre-trained models,\nwhich primarily focus on the correlations between the input news content and\nthe output predictions. However, the news captioning requires adhering to some\nfundamental rules of news reporting, such as accurately describing the\nindividuals and actions associated with the event. In this paper, we propose\nthe rule-driven news captioning method, which can generate image descriptions\nfollowing designated rule signal. Specifically, we first design the news-aware\nsemantic rule for the descriptions. This rule incorporates the primary action\ndepicted in the image (e.g., \"performing\") and the roles played by named\nentities involved in the action (e.g., \"Agent\" and \"Place\"). Second, we inject\nthis semantic rule into the large-scale pre-trained model, BART, with the\nprefix-tuning strategy, where multiple encoder layers are embedded with\nnews-aware semantic rule. Finally, we can effectively guide BART to generate\nnews sentences that comply with the designated rule. Extensive experiments on\ntwo widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the\neffectiveness of our method.",
      "tldr_zh": "该论文针对新闻标题生成任务，提出了一种 rule-driven news captioning 方法，以确保生成的图像描述句子遵守新闻报道的基本规则，如准确描述事件中的个体和动作。方法首先设计了 news-aware semantic rule，包括图像中的主要动作（如 \"performing\"）和命名实体角色（如 \"Agent\" 和 \"Place\"），然后通过 prefix-tuning 策略将该规则注入预训练模型 BART 的多个编码器层。实验结果在 GoodNews 和 NYTimes800k 数据集上证明，该方法显著提高了生成句子的准确性和规则遵守性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05101v3",
      "published_date": "2024-03-08 07:06:43 UTC",
      "updated_date": "2024-03-14 08:00:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:48:08.754468"
    },
    {
      "arxiv_id": "2403.05100v2",
      "title": "Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume",
      "title_zh": "翻译失败",
      "authors": [
        "Ping Guo",
        "Cheng Gong",
        "Xi Lin",
        "Zhiyuan Yang",
        "Qingfu Zhang"
      ],
      "abstract": "The escalating threat of adversarial attacks on deep learning models,\nparticularly in security-critical fields, has underscored the need for robust\ndeep learning systems. Conventional robustness evaluations have relied on\nadversarial accuracy, which measures a model's performance under a specific\nperturbation intensity. However, this singular metric does not fully\nencapsulate the overall resilience of a model against varying degrees of\nperturbation. To address this gap, we propose a new metric termed adversarial\nhypervolume, assessing the robustness of deep learning models comprehensively\nover a range of perturbation intensities from a multi-objective optimization\nstandpoint. This metric allows for an in-depth comparison of defense mechanisms\nand recognizes the trivial improvements in robustness afforded by less potent\ndefensive strategies. Additionally, we adopt a novel training algorithm that\nenhances adversarial robustness uniformly across various perturbation\nintensities, in contrast to methods narrowly focused on optimizing adversarial\naccuracy. Our extensive empirical studies validate the effectiveness of the\nadversarial hypervolume metric, demonstrating its ability to reveal subtle\ndifferences in robustness that adversarial accuracy overlooks. This research\ncontributes a new measure of robustness and establishes a standard for\nassessing and benchmarking the resilience of current and future defensive\nmodels against adversarial threats.",
      "tldr_zh": "该研究指出了传统对抗准确率（adversarial accuracy）在评估深度学习模型鲁棒性时的局限性，因为它仅针对特定扰动强度，无法全面反映模型对不同强度的抗性。为解决这一问题，研究提出了一种新指标——对抗超体积（adversarial hypervolume），从多目标优化（multi-objective optimization）的视角，全面评估模型在各种扰动强度下的鲁棒性，并引入一种新训练算法，以均匀提升模型的整体抗性。实验结果显示，该指标能揭示对抗准确率忽略的细微差异，并为比较防御机制和基准测试未来模型提供了新标准。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05100v2",
      "published_date": "2024-03-08 07:03:18 UTC",
      "updated_date": "2024-11-17 14:42:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:48:20.698308"
    },
    {
      "arxiv_id": "2403.07005v1",
      "title": "Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines",
      "title_zh": "翻译失败",
      "authors": [
        "Xuejing Zheng",
        "Chao Yu"
      ],
      "abstract": "In this paper, we study the cooperative Multi-Agent Reinforcement Learning\n(MARL) problems using Reward Machines (RMs) to specify the reward functions\nsuch that the prior knowledge of high-level events in a task can be leveraged\nto facilitate the learning efficiency. Unlike the existing work that RMs have\nbeen incorporated into MARL for task decomposition and policy learning in\nrelatively simple domains or with an assumption of independencies among the\nagents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs\n(MAHRM) that is capable of dealing with more complex scenarios when the events\namong agents can occur concurrently and the agents are highly interdependent.\n  MAHRM exploits the relationship of high-level events to decompose a task into\na hierarchy of simpler subtasks that are assigned to a small group of agents,\nso as to reduce the overall computational complexity.\n  Experimental results in three cooperative MARL domains show that MAHRM\noutperforms other MARL methods using the same prior knowledge of high-level\nevents.",
      "tldr_zh": "本文提出 MAHRM（Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines），一种利用 Reward Machines (RMs) 层次结构的方法，用于处理复杂合作 Multi-Agent Reinforcement Learning (MARL) 问题，其中代理间事件可能并发且高度相互依赖。MAHRM 通过利用高水平事件的关系，将任务分解为更简单的子任务并分配给小群代理，从而降低计算复杂度并提高学习效率。在三个合作 MARL 领域中的实验结果表明，MAHRM 优于其他使用相同高水平事件知识的 MARL 方法。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.07005v1",
      "published_date": "2024-03-08 06:38:22 UTC",
      "updated_date": "2024-03-08 06:38:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:48:33.329749"
    },
    {
      "arxiv_id": "2403.05066v2",
      "title": "Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning",
      "title_zh": "重置与蒸馏：一种克服持续强化学习中负面转移的方法",
      "authors": [
        "Hongjoon Ahn",
        "Jinu Hyeon",
        "Youngmin Oh",
        "Bosun Hwang",
        "Taesup Moon"
      ],
      "abstract": "We argue that the negative transfer problem occurring when the new task to\nlearn arrives is an important problem that needs not be overlooked when\ndeveloping effective Continual Reinforcement Learning (CRL) algorithms. Through\ncomprehensive experimental validation, we demonstrate that such issue\nfrequently exists in CRL and cannot be effectively addressed by several recent\nwork on mitigating plasticity loss of RL agents. To that end, we develop Reset\n& Distill (R&D), a simple yet highly effective method, to overcome the negative\ntransfer problem in CRL. R&D combines a strategy of resetting the agent's\nonline actor and critic networks to learn a new task and an offline learning\nstep for distilling the knowledge from the online actor and previous expert's\naction probabilities. We carried out extensive experiments on long sequence of\nMeta World tasks and show that our method consistently outperforms recent\nbaselines, achieving significantly higher success rates across a range of\ntasks. Our findings highlight the importance of considering negative transfer\nin CRL and emphasize the need for robust strategies like R&D to mitigate its\ndetrimental effects.",
      "tldr_zh": "本研究强调了在Continual Reinforcement Learning (CRL)中，Negative Transfer问题的重要性，即在新任务学习时旧知识干扰新学习，并通过实验证明现有方法无法有效解决此问题。为此，提出了一种简单有效的策略Reset & Distill (R&D)，它通过重置代理的在线actor和critic网络来适应新任务，并结合离线学习从在线actor和先前专家行动概率中提炼知识。实验在Meta World任务序列上显示，R&D显著优于基线方法，提升了成功率，并突显了应对Negative Transfer的必要性，以提升CRL算法的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05066v2",
      "published_date": "2024-03-08 05:37:59 UTC",
      "updated_date": "2024-08-14 06:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:48:45.492689"
    },
    {
      "arxiv_id": "2403.05064v1",
      "title": "Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyang Zhang",
        "Xin Wang",
        "Ziwei Zhang",
        "Guangyao Shen",
        "Shiqi Shen",
        "Wenwu Zhu"
      ],
      "abstract": "The existing graph neural architecture search (GNAS) methods heavily rely on\nsupervised labels during the search process, failing to handle ubiquitous\nscenarios where supervisions are not available. In this paper, we study the\nproblem of unsupervised graph neural architecture search, which remains\nunexplored in the literature. The key problem is to discover the latent graph\nfactors that drive the formation of graph data as well as the underlying\nrelations between the factors and the optimal neural architectures. Handling\nthis problem is challenging given that the latent graph factors together with\narchitectures are highly entangled due to the nature of the graph and the\ncomplexity of the neural architecture search process. To address the challenge,\nwe propose a novel Disentangled Self-supervised Graph Neural Architecture\nSearch (DSGAS) model, which is able to discover the optimal architectures\ncapturing various latent graph factors in a self-supervised fashion based on\nunlabeled graph data. Specifically, we first design a disentangled graph\nsuper-network capable of incorporating multiple architectures with factor-wise\ndisentanglement, which are optimized simultaneously. Then, we estimate the\nperformance of architectures under different factors by our proposed\nself-supervised training with joint architecture-graph disentanglement.\nFinally, we propose a contrastive search with architecture augmentations to\ndiscover architectures with factor-specific expertise. Extensive experiments on\n11 real-world datasets demonstrate that the proposed model is able to achieve\nstate-of-the-art performance against several baseline methods in an\nunsupervised manner.",
      "tldr_zh": "本论文研究无监督的Graph Neural Architecture Search (GNAS)，提出DSGAS模型，以自监督方式发现潜在图因素及其与最优神经架构的关系，解决现有方法依赖监督标签的局限。\nDSGAS通过设计一个解纠缠的图超网络来整合多架构并优化因素-wise分离，然后使用自监督训练和对比搜索估算架构性能，并发现特定因素的专业架构。\n实验结果显示，在11个真实数据集上，DSGAS在无监督条件下实现了比基线方法更先进的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS'23",
      "pdf_url": "http://arxiv.org/pdf/2403.05064v1",
      "published_date": "2024-03-08 05:23:55 UTC",
      "updated_date": "2024-03-08 05:23:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:48:57.555869"
    },
    {
      "arxiv_id": "2403.05063v2",
      "title": "Aligning Large Language Models for Controllable Recommendations",
      "title_zh": "翻译失败",
      "authors": [
        "Wensheng Lu",
        "Jianxun Lian",
        "Wei Zhang",
        "Guanghua Li",
        "Mingyang Zhou",
        "Hao Liao",
        "Xing Xie"
      ],
      "abstract": "Inspired by the exceptional general intelligence of Large Language Models\n(LLMs), researchers have begun to explore their application in pioneering the\nnext generation of recommender systems - systems that are conversational,\nexplainable, and controllable. However, existing literature primarily\nconcentrates on integrating domain-specific knowledge into LLMs to enhance\naccuracy, often neglecting the ability to follow instructions. To address this\ngap, we initially introduce a collection of supervised learning tasks,\naugmented with labels derived from a conventional recommender model, aimed at\nexplicitly improving LLMs' proficiency in adhering to recommendation-specific\ninstructions. Subsequently, we develop a reinforcement learning-based alignment\nprocedure to further strengthen LLMs' aptitude in responding to users'\nintentions and mitigating formatting errors. Through extensive experiments on\ntwo real-world datasets, our method markedly advances the capability of LLMs to\ncomply with instructions within recommender systems, while sustaining a high\nlevel of accuracy performance.",
      "tldr_zh": "该研究旨在提升 Large Language Models (LLMs) 在推荐系统中的可控性，解决现有方法忽略 LLMs 遵循指令的问题。作者引入监督学习任务，利用传统推荐模型的标签来显式训练 LLMs 遵守推荐特定指令，并开发基于 reinforcement learning 的对齐过程，以更好地响应用户意图并减少格式错误。通过在两个真实数据集上的广泛实验，该方法显著提高了 LLMs 的指令遵守能力，同时保持了高准确性性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "68T50"
      ],
      "primary_category": "cs.IR",
      "comment": "14 pages; Accepted by ACL 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2403.05063v2",
      "published_date": "2024-03-08 05:23:27 UTC",
      "updated_date": "2024-08-04 11:49:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:49:10.447247"
    },
    {
      "arxiv_id": "2403.05053v3",
      "title": "PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering",
      "title_zh": "翻译失败",
      "authors": [
        "Yibin Wang",
        "Weizhong Zhang",
        "Jianwei Zheng",
        "Cheng Jin"
      ],
      "abstract": "Image composition involves seamlessly integrating given objects into a\nspecific visual context. Current training-free methods rely on composing\nattention weights from several samplers to guide the generator. However, since\nthese weights are derived from disparate contexts, their combination leads to\ncoherence confusion and loss of appearance information. These issues worsen\nwith their excessive focus on background generation, even when unnecessary in\nthis task. This not only impedes their swift implementation but also\ncompromises foreground generation quality. Moreover, these methods introduce\nunwanted artifacts in the transition area. In this paper, we formulate image\ncomposition as a subject-based local editing task, solely focusing on\nforeground generation. At each step, the edited foreground is combined with the\nnoisy background to maintain scene consistency. To address the remaining\nissues, we propose PrimeComposer, a faster training-free diffuser that\ncomposites the images by well-designed attention steering across different\nnoise levels. This steering is predominantly achieved by our Correlation\nDiffuser, utilizing its self-attention layers at each step. Within these\nlayers, the synthesized subject interacts with both the referenced object and\nbackground, capturing intricate details and coherent relationships. This prior\ninformation is encoded into the attention weights, which are then integrated\ninto the self-attention layers of the generator to guide the synthesis process.\nBesides, we introduce a Region-constrained Cross-Attention to confine the\nimpact of specific subject-related tokens to desired regions, addressing the\nunwanted artifacts shown in the prior method thereby further improving the\ncoherence in the transition area. Our method exhibits the fastest inference\nefficiency and extensive experiments demonstrate our superiority both\nqualitatively and quantitatively.",
      "tldr_zh": "论文提出 PrimeComposer，一种更快的无训练扩散模型，用于图像合成任务，通过 Attention Steering 在不同噪声级别上引导注意力，解决现有方法的连贯性混乱、外观信息丢失和过渡区域伪影问题。该方法将图像合成视为基于主体的局部编辑，仅关注前景生成，并结合 Correlation Diffuser 的自注意力层，让合成主体与参考对象和背景互动，以捕捉细节和关系。同时，引入 Region-constrained Cross-Attention 来限制特定主题标记的影响，提升过渡区域的连贯性。实验显示，PrimeComposer 具有最快的推理效率，并在定性和定量指标上显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACMMM2024. Code:\n  https://github.com/CodeGoat24/PrimeComposer",
      "pdf_url": "http://arxiv.org/pdf/2403.05053v3",
      "published_date": "2024-03-08 04:58:49 UTC",
      "updated_date": "2024-08-20 05:14:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:49:22.227106"
    },
    {
      "arxiv_id": "2403.05050v4",
      "title": "DyRoNet: Dynamic Routing and Low-Rank Adapters for Autonomous Driving Streaming Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Huang",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Chenyang Li",
        "Wangmeng Xiang",
        "Baigui Sun"
      ],
      "abstract": "The advancement of autonomous driving systems hinges on the ability to\nachieve low-latency and high-accuracy perception. To address this critical\nneed, this paper introduces Dynamic Routing Network (DyRoNet), a low-rank\nenhanced dynamic routing framework designed for streaming perception in\nautonomous driving systems. DyRoNet integrates a suite of pre-trained branch\nnetworks, each meticulously fine-tuned to function under distinct environmental\nconditions. At its core, the framework offers a speed router module, developed\nto assess and route input data to the most suitable branch for processing. This\napproach not only addresses the inherent limitations of conventional models in\nadapting to diverse driving conditions but also ensures the balance between\nperformance and efficiency. Extensive experimental evaluations demonstrate the\nadaptability of DyRoNet to diverse branch selection strategies, resulting in\nsignificant performance enhancements across different scenarios. This work\nestablishes a new benchmark for streaming perception and provides valuable\nengineering insights for future work.",
      "tldr_zh": "本研究提出DyRoNet，一种动态路由(Dynamic Routing)和低秩适配器(Low-Rank Adapters)框架，旨在提升自动驾驶系统的流式感知(Streaming Perception)，实现低延迟和高准确性平衡。DyRoNet整合了多条预训练分支网络，每个分支针对不同环境条件进行微调，并通过速度路由模块评估输入数据并路由到最合适的分支，从而解决传统模型在适应多样驾驶场景时的局限性。实验结果显示，该框架在各种策略下显著提升性能，建立流式感知的新基准，并为未来自动驾驶工程提供宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2025. 17 pages, 8 figures. Project:\n  https://tastevision.github.io/DyRoNet/",
      "pdf_url": "http://arxiv.org/pdf/2403.05050v4",
      "published_date": "2024-03-08 04:53:53 UTC",
      "updated_date": "2024-12-15 20:29:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:49:33.002631"
    },
    {
      "arxiv_id": "2403.05045v1",
      "title": "Are Human Conversations Special? A Large Language Model Perspective",
      "title_zh": "人类对话是特殊的吗？ 从大型语言模型的视角",
      "authors": [
        "Toshish Jawale",
        "Chaitanya Animesh",
        "Sekhar Vallath",
        "Kartik Talamadupula",
        "Larry Heck"
      ],
      "abstract": "This study analyzes changes in the attention mechanisms of large language\nmodels (LLMs) when used to understand natural conversations between humans\n(human-human). We analyze three use cases of LLMs: interactions over web\ncontent, code, and mathematical texts. By analyzing attention distance,\ndispersion, and interdependency across these domains, we highlight the unique\nchallenges posed by conversational data. Notably, conversations require nuanced\nhandling of long-term contextual relationships and exhibit higher complexity\nthrough their attention patterns. Our findings reveal that while language\nmodels exhibit domain-specific attention behaviors, there is a significant gap\nin their ability to specialize in human conversations. Through detailed\nattention entropy analysis and t-SNE visualizations, we demonstrate the need\nfor models trained with a diverse array of high-quality conversational data to\nenhance understanding and generation of human-like dialogue. This research\nhighlights the importance of domain specialization in language models and\nsuggests pathways for future advancement in modeling human conversational\nnuances.",
      "tldr_zh": "这篇论文从大型语言模型(LLMs)的视角探讨了人类对话(human-human)的独特挑战，通过分析LLMs在网页内容、代码和数学文本等领域的注意力机制，包括注意力距离、分散度和相互依赖性。研究发现，人类对话需要更精细处理长期上下文关系，并表现出更高的复杂性，导致LLMs在这一领域存在显著性能差距。作者利用注意力熵分析和t-SNE可视化证明了这一问题，并建议通过训练模型以更多高质量对话数据来提升其对人类对话的理解和生成能力，从而推动语言模型的领域专业化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05045v1",
      "published_date": "2024-03-08 04:44:25 UTC",
      "updated_date": "2024-03-08 04:44:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:49:46.903827"
    },
    {
      "arxiv_id": "2403.05033v1",
      "title": "Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold",
      "title_zh": "量化流形：生成对抗网络学习的流形是否收敛到真实数据流",
      "authors": [
        "Anupam Chaudhuri",
        "Anj Simmons",
        "Mohamed Abdelrazek"
      ],
      "abstract": "This paper presents our experiments to quantify the manifolds learned by ML\nmodels (in our experiment, we use a GAN model) as they train. We compare the\nmanifolds learned at each epoch to the real manifolds representing the real\ndata. To quantify a manifold, we study the intrinsic dimensions and topological\nfeatures of the manifold learned by the ML model, how these metrics change as\nwe continue to train the model, and whether these metrics convergence over the\ncourse of training to the metrics of the real data manifold.",
      "tldr_zh": "这篇论文通过实验量化 Generative Adversarial Networks (GAN) 模型在训练过程中的流形学习，探讨这些流形是否会收敛到真实数据流形。研究方法包括比较每个 epoch 的模型学习流形与真实流形，重点分析内在维度 (intrinsic dimensions) 和拓扑特征 (topological features) 的变化。结果表明，这些指标随训练而演化，但论文重点在于评估其是否最终趋近真实数据流形的特性，为理解 GAN 的泛化能力提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2311.13102",
      "pdf_url": "http://arxiv.org/pdf/2403.05033v1",
      "published_date": "2024-03-08 04:23:50 UTC",
      "updated_date": "2024-03-08 04:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:49:57.765976"
    },
    {
      "arxiv_id": "2403.05030v4",
      "title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training",
      "title_zh": "通过潜在对抗训练防御未预见的故障模式",
      "authors": [
        "Stephen Casper",
        "Lennart Schulze",
        "Oam Patel",
        "Dylan Hadfield-Menell"
      ],
      "abstract": "Despite extensive diagnostics and debugging by developers, AI systems\nsometimes exhibit harmful unintended behaviors. Finding and fixing these is\nchallenging because the attack surface is so large -- it is not tractable to\nexhaustively search for inputs that may elicit harmful behaviors. Red-teaming\nand adversarial training (AT) are commonly used to improve robustness, however,\nthey empirically struggle to fix failure modes that differ from the attacks\nused during training. In this work, we utilize latent adversarial training\n(LAT) to defend against vulnerabilities without leveraging knowledge of what\nthey are or using inputs that elicit them. LAT makes use of the compressed,\nabstract, and structured latent representations of concepts that the network\nactually uses for prediction. Here, we use it to defend against failure modes\nwithout examples that elicit them. Specifically, we use LAT to remove trojans\nand defend against held-out classes of adversarial attacks. We show in image\nclassification, text classification, and text generation tasks that LAT usually\nimproves both robustness to novel attacks and performance on clean data\nrelative to AT. This suggests that LAT can be a promising tool for defending\nagainst failure modes that are not explicitly identified by developers.",
      "tldr_zh": "该研究针对AI系统可能出现的未预见故障模式（如有害意外行为）提出了一种潜在对抗训练（Latent Adversarial Training, LAT）方法，以提升鲁棒性，而无需事先知道这些漏洞或使用能引发它们的输入。LAT利用网络的压缩、抽象和结构化的潜在表示来进行训练，从而移除木马（trojans）和防御未持有的攻击类别。在图像分类、文本分类和文本生成任务的实验中，LAT相较于传统对抗训练（AT）通常提高了对新型攻击的鲁棒性，同时改善了干净数据的性能，表明它是一种有前景的工具，用于防御开发者未明确识别的故障模式。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05030v4",
      "published_date": "2024-03-08 04:22:48 UTC",
      "updated_date": "2024-08-22 00:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:50:10.733792"
    },
    {
      "arxiv_id": "2403.05029v2",
      "title": "BjTT: A Large-scale Multimodal Dataset for Traffic Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Chengyang Zhang",
        "Yong Zhang",
        "Qitan Shao",
        "Jiangtao Feng",
        "Bo Li",
        "Yisheng Lv",
        "Xinglin Piao",
        "Baocai Yin"
      ],
      "abstract": "Traffic prediction is one of the most significant foundations in Intelligent\nTransportation Systems (ITS). Traditional traffic prediction methods rely only\non historical traffic data to predict traffic trends and face two main\nchallenges. 1) insensitivity to unusual events. 2) limited performance in\nlong-term prediction. In this work, we explore how generative models combined\nwith text describing the traffic system can be applied for traffic generation,\nand name the task Text-to-Traffic Generation (TTG). The key challenge of the\nTTG task is how to associate text with the spatial structure of the road\nnetwork and traffic data for generating traffic situations. To this end, we\npropose ChatTraffic, the first diffusion model for text-to-traffic generation.\nTo guarantee the consistency between synthetic and real data, we augment a\ndiffusion model with the Graph Convolutional Network (GCN) to extract spatial\ncorrelations of traffic data. In addition, we construct a large dataset\ncontaining text-traffic pairs for the TTG task. We benchmarked our model\nqualitatively and quantitatively on the released dataset. The experimental\nresults indicate that ChatTraffic can generate realistic traffic situations\nfrom the text. Our code and dataset are available at\nhttps://github.com/ChyaZhang/ChatTraffic.",
      "tldr_zh": "该研究针对交通预测的传统方法（如依赖历史数据）存在对异常事件不敏感和长期预测性能有限的问题，引入了Text-to-Traffic Generation (TTG)任务，利用生成模型结合文本描述来生成交通数据。作者提出ChatTraffic模型，这是一种基于扩散模型的框架，并通过Graph Convolutional Network (GCN)提取交通数据的空间相关性，以确保合成数据与真实数据的一致性。为支持TTG任务，他们构建了大规模多模态数据集BjTT，包含文本-交通对。实验结果显示，ChatTraffic在数据集上实现了定性和定量benchmark表现，能从文本生成真实的交通情况。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05029v2",
      "published_date": "2024-03-08 04:19:56 UTC",
      "updated_date": "2024-03-14 08:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:50:20.586886"
    },
    {
      "arxiv_id": "2403.18846v2",
      "title": "The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Random Access in Cellular IoT",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Zhu",
        "Ahmet Enis Cetin"
      ],
      "abstract": "The lack of an efficient preamble detection algorithm remains a challenge for\nsolving preamble collision problems in intelligent random access (RA) in the\ncellular Internet of Things (IoT). To address this problem, we present an early\npreamble detection scheme based on a maximum likelihood estimation (MLE) model\nat the first step of the grant-based RA procedure. A novel blind normalized\nStein variational gradient descent (SVGD)-based detector is proposed to obtain\nan approximate solution to the MLE model. First, by exploring the relationship\nbetween the Hadamard transform and wavelet packet transform, a new modified\nHadamard transform (MHT) is developed to separate high-frequency components\nfrom signals using the second-order derivative filter. Next, to eliminate noise\nand mitigate the vanishing gradients problem in the SVGD-based detectors, the\nblock MHT layer is designed based on the MHT, scaling layer, soft-thresholding\nlayer, inverse MHT and sparsity penalty. Then, the blind normalized SVGD\nalgorithm is derived to perform preamble detection without prior knowledge of\nnoise power and the number of active IoT devices. The experimental results show\nthe proposed block MHT layer outperforms other transform-based methods in terms\nof computation costs and denoising performance. Furthermore, with the\nassistance of the block MHT layer, the proposed blind normalized SVGD algorithm\nachieves a higher preamble detection accuracy and throughput than other\nstate-of-the-art detection methods.",
      "tldr_zh": "这篇论文针对蜂窝物联网(cellular IoT)中智能随机接入(intelligent random access, RA)的preamble碰撞问题，提出了一种基于最大似然估计(MLE)模型的早期preamble检测方案。论文创新性地开发了修改Hadamard变换(modified Hadamard transform, MHT)和block MHT层，用于分离信号高频成分、消除噪声并缓解Stein变分梯度下降(SVGD)算法中的梯度消失问题；同时，推导了盲归一化SVGD算法，实现无噪声功率和活跃设备数量先验知识的preamble检测。实验结果显示，block MHT层在计算成本和去噪性能上优于其他变换方法，而盲归一化SVGD算法在preamble检测准确性和系统吞吐量上显著超越现有技术。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "Accepted by the IEEE Internet of Things Journal",
      "pdf_url": "http://arxiv.org/pdf/2403.18846v2",
      "published_date": "2024-03-08 04:08:40 UTC",
      "updated_date": "2025-01-20 20:57:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:50:35.021832"
    },
    {
      "arxiv_id": "2403.05026v1",
      "title": "Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts",
      "title_zh": "在分布偏移下的动态图谱不变学习",
      "authors": [
        "Zeyang Zhang",
        "Xin Wang",
        "Ziwei Zhang",
        "Zhou Qin",
        "Weigao Wen",
        "Hui Xue",
        "Haoyang Li",
        "Wenwu Zhu"
      ],
      "abstract": "Dynamic graph neural networks (DyGNNs) currently struggle with handling\ndistribution shifts that are inherent in dynamic graphs. Existing work on\nDyGNNs with out-of-distribution settings only focuses on the time domain,\nfailing to handle cases involving distribution shifts in the spectral domain.\nIn this paper, we discover that there exist cases with distribution shifts\nunobservable in the time domain while observable in the spectral domain, and\npropose to study distribution shifts on dynamic graphs in the spectral domain\nfor the first time. However, this investigation poses two key challenges: i) it\nis non-trivial to capture different graph patterns that are driven by various\nfrequency components entangled in the spectral domain; and ii) it remains\nunclear how to handle distribution shifts with the discovered spectral\npatterns. To address these challenges, we propose Spectral Invariant Learning\nfor Dynamic Graphs under Distribution Shifts (SILD), which can handle\ndistribution shifts on dynamic graphs by capturing and utilizing invariant and\nvariant spectral patterns. Specifically, we first design a DyGNN with Fourier\ntransform to obtain the ego-graph trajectory spectrums, allowing the mixed\ndynamic graph patterns to be transformed into separate frequency components. We\nthen develop a disentangled spectrum mask to filter graph dynamics from various\nfrequency components and discover the invariant and variant spectral patterns.\nFinally, we propose invariant spectral filtering, which encourages the model to\nrely on invariant patterns for generalization under distribution shifts.\nExperimental results on synthetic and real-world dynamic graph datasets\ndemonstrate the superiority of our method for both node classification and link\nprediction tasks under distribution shifts.",
      "tldr_zh": "本研究发现，动态图神经网络 (DyGNNs) 在处理动态图中的分布偏移时，主要存在谱域偏移问题，而现有方法仅关注时间域。论文首次提出 Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD) 方法，通过傅立叶变换获取 ego-graph 轨迹谱，并使用 disentangled spectrum mask 过滤不同频率组件，以识别不变和可变谱模式。最终，SILD 通过 invariant spectral filtering 鼓励模型依赖不变模式实现泛化，在合成和真实世界数据集上的节点分类和链接预测任务中，显著优于基线模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS'23",
      "pdf_url": "http://arxiv.org/pdf/2403.05026v1",
      "published_date": "2024-03-08 04:07:23 UTC",
      "updated_date": "2024-03-08 04:07:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:50:46.307442"
    },
    {
      "arxiv_id": "2403.05025v3",
      "title": "Debiased Multimodal Understanding for Human Language Sequences",
      "title_zh": "去偏置多模态理解的人类语言序列",
      "authors": [
        "Zhi Xu",
        "Dingkang Yang",
        "Mingcheng Li",
        "Yuzheng Wang",
        "Zhaoyu Chen",
        "Jiawei Chen",
        "Jinjie Wei",
        "Lihua Zhang"
      ],
      "abstract": "Human multimodal language understanding (MLU) is an indispensable component\nof expression analysis (e.g., sentiment or humor) from heterogeneous\nmodalities, including visual postures, linguistic contents, and acoustic\nbehaviours. Existing works invariably focus on designing sophisticated\nstructures or fusion strategies to achieve impressive improvements.\nUnfortunately, they all suffer from the subject variation problem due to data\ndistribution discrepancies among subjects. Concretely, MLU models are easily\nmisled by distinct subjects with different expression customs and\ncharacteristics in the training data to learn subject-specific spurious\ncorrelations, limiting performance and generalizability across new subjects.\nMotivated by this observation, we introduce a recapitulative causal graph to\nformulate the MLU procedure and analyze the confounding effect of subjects.\nThen, we propose SuCI, a simple yet effective causal intervention module to\ndisentangle the impact of subjects acting as unobserved confounders and achieve\nmodel training via true causal effects. As a plug-and-play component, SuCI can\nbe widely applied to most methods that seek unbiased predictions. Comprehensive\nexperiments on several MLU benchmarks clearly show the effectiveness of the\nproposed module.",
      "tldr_zh": "本文探讨了多模态语言理解 (MLU) 在处理视觉姿势、语言内容和声学行为等异构模态时面临的主体变异问题，即模型易受训练数据中不同主体的虚假相关性影响，导致性能和泛化能力下降。作者引入了一个回顾性因果图来分析主体作为未观察混杂因素的混杂效应，并提出 SuCI（一个简单有效的因果干预模块），用于分离主体影响并通过真实因果效应进行模型训练。SuCI 作为即插即用组件，能广泛应用于各种无偏预测方法，并在多个 MLU 基准实验中证明其有效性，提高了模型的鲁棒性和泛化性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2403.05025v3",
      "published_date": "2024-03-08 04:03:54 UTC",
      "updated_date": "2024-12-13 03:49:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:50:58.351842"
    },
    {
      "arxiv_id": "2403.05020v4",
      "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Xuhui Zhou",
        "Zhe Su",
        "Tiwalayo Eisape",
        "Hyunwoo Kim",
        "Maarten Sap"
      ],
      "abstract": "Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena. However, most\nrecent work has used a more omniscient perspective on these simulations (e.g.,\nsingle LLM to generate all interlocutors), which is fundamentally at odds with\nthe non-omniscient, information asymmetric interactions that involve humans and\nAI agents in the real world. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that LLMs perform\nbetter in unrealistic, omniscient simulation settings but struggle in ones that\nmore accurately reflect real-world conditions with information asymmetry. Our\nfindings indicate that addressing information asymmetry remains a fundamental\nchallenge for LLM-based agents.",
      "tldr_zh": "该研究质疑了使用大型语言模型 (LLMs) 模拟社会互动的成功，指出现有方法多采用全知视角（如单一 LLM 生成所有对话者），这与现实世界的信息不对称互动相悖。研究团队开发了一个评估框架，在全知和非全知设置下模拟社会互动，并通过实验发现，LLMs 在全知环境中表现良好，但在更真实的信息不对称条件下表现较差。这些发现强调，解决信息不对称是 LLM-based 代理面临的核心挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.05020v4",
      "published_date": "2024-03-08 03:49:17 UTC",
      "updated_date": "2024-10-04 02:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:51:09.035283"
    },
    {
      "arxiv_id": "2403.05014v1",
      "title": "Simple Multigraph Convolution Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Danyang Wu",
        "Xinjie Shen",
        "Jitao Lu",
        "Jin Xu",
        "Feiping Nie"
      ],
      "abstract": "Existing multigraph convolution methods either ignore the cross-view\ninteraction among multiple graphs, or induce extremely high computational cost\ndue to standard cross-view polynomial operators. To alleviate this problem,\nthis paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which\nfirst extracts consistent cross-view topology from multigraphs including\nedge-level and subgraph-level topology, then performs polynomial expansion\nbased on raw multigraphs and consistent topologies. In theory, SMGCN utilizes\nthe consistent topologies in polynomial expansion rather than standard\ncross-view polynomial expansion, which performs credible cross-view spatial\nmessage-passing, follows the spectral convolution paradigm, and effectively\nreduces the complexity of standard polynomial expansion. In the simulations,\nexperimental results demonstrate that SMGCN achieves state-of-the-art\nperformance on ACM and DBLP multigraph benchmark datasets. Our codes are\navailable at https://github.com/frinkleko/SMGCN.",
      "tldr_zh": "该论文提出了一种简单多图卷积网络 (SMGCN)，旨在解决现有 multigraph convolution 方法忽略跨视图交互或因标准 cross-view polynomial operators 而导致高计算成本的问题。SMGCN 先从多图中提取一致的跨视图拓扑，包括 edge-level 和 subgraph-level 拓扑，然后基于原始多图和这些拓扑进行多项式展开。在理论上，该方法利用一致拓扑实现可靠的跨视图空间消息传递，遵循 spectral convolution paradigm，并有效降低计算复杂性。实验结果显示，SMGCN 在 ACM 和 DBLP multigraph 基准数据集上达到了 state-of-the-art 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW 2024 Short",
      "pdf_url": "http://arxiv.org/pdf/2403.05014v1",
      "published_date": "2024-03-08 03:27:58 UTC",
      "updated_date": "2024-03-08 03:27:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:51:23.064183"
    },
    {
      "arxiv_id": "2403.05010v3",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Liu",
        "Dongyang Dai",
        "Zhiyong Wu"
      ],
      "abstract": "Recent advancements in generative modeling have significantly enhanced the\nreconstruction of audio waveforms from various representations. While diffusion\nmodels are adept at this task, they are hindered by latency issues due to their\noperation at the individual sample point level and the need for numerous\nsampling steps. In this study, we introduce RFWave, a cutting-edge multi-band\nRectified Flow approach designed to reconstruct high-fidelity audio waveforms\nfrom Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates\ncomplex spectrograms and operates at the frame level, processing all subbands\nsimultaneously to boost efficiency. Leveraging Rectified Flow, which targets a\nstraight transport trajectory, RFWave achieves reconstruction with just 10\nsampling steps. Our empirical evaluations show that RFWave not only provides\noutstanding reconstruction quality but also offers vastly superior\ncomputational efficiency, enabling audio generation at speeds up to 160 times\nfaster than real-time on a GPU. An online demonstration is available at:\nhttps://rfwave-demo.github.io/rfwave/.",
      "tldr_zh": "这篇论文引入了 RFWave，一种多频带 Rectified Flow 方法，用于从 Mel-spectrograms 或离散声学标记高效重建高保真音频波形。相比传统的 diffusion models，RFWave 通过在帧级别生成复杂谱图并同时处理所有子频带，仅需 10 个采样步骤，大大降低了延迟并提升了计算效率。实验结果表明，RFWave 不仅实现了出色的重建质量，还能在 GPU 上将音频生成速度提高至 160 倍于实时，并提供了在线演示。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05010v3",
      "published_date": "2024-03-08 03:16:47 UTC",
      "updated_date": "2024-10-07 02:08:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:51:33.836040"
    },
    {
      "arxiv_id": "2403.05006v1",
      "title": "Provable Multi-Party Reinforcement Learning with Diverse Human Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Huiying Zhong",
        "Zhun Deng",
        "Weijie J. Su",
        "Zhiwei Steven Wu",
        "Linjun Zhang"
      ],
      "abstract": "Reinforcement learning with human feedback (RLHF) is an emerging paradigm to\nalign models with human preferences. Typically, RLHF aggregates preferences\nfrom multiple individuals who have diverse viewpoints that may conflict with\neach other. Our work \\textit{initiates} the theoretical study of multi-party\nRLHF that explicitly models the diverse preferences of multiple individuals. We\nshow how traditional RLHF approaches can fail since learning a single reward\nfunction cannot capture and balance the preferences of multiple individuals. To\novercome such limitations, we incorporate meta-learning to learn multiple\npreferences and adopt different social welfare functions to aggregate the\npreferences across multiple parties. We focus on the offline learning setting\nand establish sample complexity bounds, along with efficiency and fairness\nguarantees, for optimizing diverse social welfare functions such as Nash,\nUtilitarian, and Leximin welfare functions. Our results show a separation\nbetween the sample complexities of multi-party RLHF and traditional\nsingle-party RLHF. Furthermore, we consider a reward-free setting, where each\nindividual's preference is no longer consistent with a reward model, and give\npessimistic variants of the von Neumann Winner based on offline preference\ndata. Taken together, our work showcases the advantage of multi-party RLHF but\nalso highlights its more demanding statistical complexity.",
      "tldr_zh": "本研究首次对多党强化学习与人类反馈 (RLHF) 进行理论探讨，旨在处理多个个体多样化偏好可能冲突的问题。传统 RLHF 通过学习单一奖励函数难以捕捉和平衡这些偏好，因此作者引入 meta-learning 来学习多重偏好，并采用社会福利函数（如 Nash、Utilitarian 和 Leximin）来聚合偏好。实验在离线学习设置中建立了样本复杂度界，并证明多党 RLHF 比单党 RLHF 具有更高的样本复杂度，同时提供效率和公平性保证。在无奖励设置下，该框架使用基于离线偏好数据的悲观变体 von Neumann Winner，进一步突显了多党 RLHF 的优势及其统计复杂性挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05006v1",
      "published_date": "2024-03-08 03:05:11 UTC",
      "updated_date": "2024-03-08 03:05:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:51:45.537565"
    },
    {
      "arxiv_id": "2403.05004v1",
      "title": "Can't Remember Details in Long Documents? You Need Some R&R",
      "title_zh": "翻译失败",
      "authors": [
        "Devanshu Agrawal",
        "Shang Gao",
        "Martin Gajek"
      ],
      "abstract": "Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.",
      "tldr_zh": "该论文探讨了大语言模型（LLMs）在处理长文档问答（QA）任务时，常忽略文档中间关键信息的局限性。为解决此问题，研究者提出 R&R 方法，结合 reprompting（定期重复提示指令以提醒模型任务）和 in-context retrieval (ICR)（让模型检索与问题最相关的 top k 段落编号，并用于简化后续 QA）。实验在 GPT-4 Turbo 和 Claude-2.1 上测试了长达 80k tokens 的文档，平均 QA 准确率提高了 16 点。R&R 通过减少相关上下文与指令的距离，提升了性能，并相较短上下文分块方法，允许更大分块以降低 LLM 调用和输出 tokens，同时保持准确率最小化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 1 figure, 9 tables. For associated code repository see\n  https://github.com/casetext/r-and-r",
      "pdf_url": "http://arxiv.org/pdf/2403.05004v1",
      "published_date": "2024-03-08 03:03:20 UTC",
      "updated_date": "2024-03-08 03:03:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:51:57.958947"
    },
    {
      "arxiv_id": "2403.05000v3",
      "title": "Medical Speech Symptoms Classification via Disentangled Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Jianzong Wang",
        "Pengcheng Li",
        "Xulong Zhang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "abstract": "Intent is defined for understanding spoken language in existing works. Both\ntextual features and acoustic features involved in medical speech contain\nintent, which is important for symptomatic diagnosis. In this paper, we propose\na medical speech classification model named DRSC that automatically learns to\ndisentangle intent and content representations from textual-acoustic data for\nclassification. The intent representations of the text domain and the\nMel-spectrogram domain are extracted via intent encoders, and then the\nreconstructed text feature and the Mel-spectrogram feature are obtained through\ntwo exchanges. After combining the intent from two domains into a joint\nrepresentation, the integrated intent representation is fed into a decision\nlayer for classification. Experimental results show that our model obtains an\naverage accuracy rate of 95% in detecting 25 different medical symptoms.",
      "tldr_zh": "这篇论文提出了一种名为 DRSC 的医疗语音症状分类模型，通过分离意图(intent)和内容表示(content representations)来处理文本和声学特征，从而提升症状诊断的准确性。模型使用意图编码器(intent encoders)从文本域和 Mel-spectrogram 域提取意图表示，并通过特征交换重建相关特征，以生成联合表示(joint representation)。随后，将整合后的意图表示输入决策层进行分类。实验结果显示，该模型在检测 25 种不同医疗症状时，平均准确率达到 95%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the 27th International Conference on Computer Supported\n  Cooperative Work in Design (CSCWD 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.05000v3",
      "published_date": "2024-03-08 02:42:34 UTC",
      "updated_date": "2024-04-30 01:47:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:52:09.317474"
    },
    {
      "arxiv_id": "2403.04977v1",
      "title": "Node Centrality Approximation For Large Networks Based On Inductive Graph Neural Networks",
      "title_zh": "基于归纳式图神经网络的大型网络节点中心性近似",
      "authors": [
        "Yiwei Zou",
        "Ting Li",
        "Zong-fu Luo"
      ],
      "abstract": "Closeness Centrality (CC) and Betweenness Centrality (BC) are crucial metrics\nin network analysis, providing essential reference for discerning the\nsignificance of nodes within complex networks. These measures find wide\napplications in critical tasks, such as community detection and network\ndismantling. However, their practical implementation on extensive networks\nremains computationally demanding due to their high time complexity. To\nmitigate these computational challenges, numerous approximation algorithms have\nbeen developed to expedite the computation of CC and BC. Nevertheless, even\nthese approximations still necessitate substantial processing time when applied\nto large-scale networks. Furthermore, their output proves sensitive to even\nminor perturbations within the network structure.\n  In this work, We redefine the CC and BC node ranking problem as a machine\nlearning problem and propose the CNCA-IGE model, which is an encoder-decoder\nmodel based on inductive graph neural networks designed to rank nodes based on\nspecified CC or BC metrics. We incorporate the MLP-Mixer model as the decoder\nin the BC ranking prediction task to enhance the model's robustness and\ncapacity. Our approach is evaluated on diverse synthetic and real-world\nnetworks of varying scales, and the experimental results demonstrate that the\nCNCA-IGE model outperforms state-of-the-art baseline models, significantly\nreducing execution time while improving performance.",
      "tldr_zh": "本文将 Closeness Centrality (CC) 和 Betweenness Centrality (BC) 的节点排名问题重新定义为机器学习任务，针对大型网络的计算密集型挑战，提出 CNCA-IGE 模型，这是一个基于 Inductive Graph Neural Networks 的编码器-解码器框架。模型在 BC 排名任务中融入 MLP-Mixer 作为解码器，以提升鲁棒性和整体性能。实验在多种合成和真实网络上验证，CNCA-IGE 比现有基线模型显著减少执行时间并提高排名准确性。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04977v1",
      "published_date": "2024-03-08 01:23:12 UTC",
      "updated_date": "2024-03-08 01:23:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:52:22.523983"
    },
    {
      "arxiv_id": "2403.04965v2",
      "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lezhong Wang",
        "Jeppe Revall Frisvad",
        "Mark Bo Jensen",
        "Siavash Arjomand Bigdeli"
      ],
      "abstract": "The demand for stereo images increases as manufacturers launch more XR\ndevices. To meet this demand, we introduce StereoDiffusion, a method that,\nunlike traditional inpainting pipelines, is trainning free, remarkably\nstraightforward to use, and it seamlessly integrates into the original Stable\nDiffusion model. Our method modifies the latent variable to provide an\nend-to-end, lightweight capability for fast generation of stereo image pairs,\nwithout the need for fine-tuning model weights or any post-processing of\nimages. Using the original input to generate a left image and estimate a\ndisparity map for it, we generate the latent vector for the right image through\nStereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking\nDenoise and Self-Attention Layers Modification methods to align the right-side\nimage with the left-side image. Moreover, our proposed method maintains a high\nstandard of image quality throughout the stereo generation process, achieving\nstate-of-the-art scores in various quantitative evaluations.",
      "tldr_zh": "该研究提出了一种无需训练的立体图像生成方法StereoDiffusion，基于潜在扩散模型（Latent Diffusion Models），无缝集成到Stable Diffusion模型中，以满足XR设备对立体图像的需求。该方法通过修改潜在变量，使用原始输入生成左图像并估计disparity map，然后通过Stereo Pixel Shift操作结合Symmetric Pixel Shift Masking Denoise和Self-Attention Layers Modification来生成与左图像对齐的右图像，实现端到端的轻量级生成过程。实验结果显示，StereoDiffusion在各种定量评估中达到最先进水平，保持了高图像质量，同时避免了微调模型权重或后处理的复杂性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Updated to CVPR 2024 GCV accepted version",
      "pdf_url": "http://arxiv.org/pdf/2403.04965v2",
      "published_date": "2024-03-08 00:30:25 UTC",
      "updated_date": "2024-06-02 14:31:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:52:34.563673"
    },
    {
      "arxiv_id": "2403.04964v2",
      "title": "Tell me the truth: A system to measure the trustworthiness of Large Language Models",
      "title_zh": "告诉我真相：一种测量大型语言模型可信度的系统",
      "authors": [
        "Carlo Lipizzi"
      ],
      "abstract": "Large Language Models (LLM) have taken the front seat in most of the news\nsince November 2022, when ChatGPT was introduced. After more than one year, one\nof the major reasons companies are resistant to adopting them is the limited\nconfidence they have in the trustworthiness of those systems. In a study by\n(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in\nidentifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics\nfound that ChatGPT has an accuracy rate of 17% percent when diagnosing\npediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust\nis a relative, subject condition that can change based on culture, domain,\nindividuals. And then, given a domain, how can the trustworthiness of a system\nbe measured? In this paper, I present a systematic approach to measure\ntrustworthiness based on a predefined ground truth, represented as a knowledge\ngraph of the domain. The approach is a process with humans in the loop to\nvalidate the representation of the domain and to fine-tune the system.\n  Measuring the trustworthiness would be essential for all the entities\noperating in critical environments, such as healthcare, defense, finance, but\nit would be very relevant for all the users of LLMs.",
      "tldr_zh": "这篇论文针对Large Language Models (LLM)的trustworthiness问题，提出了一种系统方法来测量其可信度，特别是在关键领域如医疗和金融中。方法基于预定义的ground truth，通过knowledge graph表示领域知识，并采用人类在环的流程来验证和微调系统。研究强调，这种测量方式能解决LLM在实际应用中的高错误率（如ChatGPT在诊断中的17%准确率），从而增强用户信心并适用于文化和领域相关的信任评估。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04964v2",
      "published_date": "2024-03-08 00:27:57 UTC",
      "updated_date": "2024-03-11 18:41:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:52:46.960138"
    },
    {
      "arxiv_id": "2403.04963v2",
      "title": "An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanxin Wu",
        "Yuki Arase"
      ],
      "abstract": "Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that GPT-4\ngenerally generates fewer erroneous simplification outputs compared to the\ncurrent state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs.",
      "tldr_zh": "本研究对Large Language Models (LLMs)在句子简化任务中的性能进行了深入评估，针对现有自动指标的不确定性和人类评估的可靠性问题，提出了一种基于错误的(error-based)人类注释框架。研究选取了GPT-4、Qwen2.5-72B和Llama-3.2-3B等不同规模的模型进行测试，结果显示GPT-4生成的简化输出错误较少，但仍存在lexical paraphrasing等局限。进一步的元评估(meta-evaluations)发现，常用自动指标对高性能LLMs的高质量简化输出缺乏足够敏感性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Extended evaluation with more LLMs and an additional dataset",
      "pdf_url": "http://arxiv.org/pdf/2403.04963v2",
      "published_date": "2024-03-08 00:19:24 UTC",
      "updated_date": "2025-04-08 02:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:52:59.407162"
    },
    {
      "arxiv_id": "2403.04960v2",
      "title": "IsolateGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhao Wu",
        "Franziska Roesner",
        "Tadayoshi Kohno",
        "Ning Zhang",
        "Umar Iqbal"
      ],
      "abstract": "Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nnatural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we evaluate whether these issues can be\naddressed through execution isolation and what that isolation might look like\nin the context of LLM-based systems, where there are arbitrary natural\nlanguage-based interactions between system components, between LLM and apps,\nand between apps. To that end, we propose IsolateGPT, a design architecture\nthat demonstrates the feasibility of execution isolation and provides a\nblueprint for implementing isolation, in LLM-based systems. We evaluate\nIsolateGPT against a number of attacks and demonstrate that it protects against\nmany security, privacy, and safety issues that exist in non-isolated LLM-based\nsystems, without any loss of functionality. The performance overhead incurred\nby IsolateGPT to improve security is under 30% for three-quarters of tested\nqueries.",
      "tldr_zh": "该论文探讨了LLM（如ChatGPT）支持第三方应用时存在的安全和隐私风险，这些风险源于自然语言接口的不精确性和缺乏隔离。作者提出IsolateGPT架构，这是一种执行隔离设计，旨在通过隔离LLM系统组件间的任意自然语言交互来防范攻击，同时保持系统功能完整。实验评估表明，IsolateGPT能够有效抵御多种安全、隐私和安全问题，且对四分之三的测试查询，性能开销低于30%。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by the Network and Distributed System Security (NDSS)\n  Symposium 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.04960v2",
      "published_date": "2024-03-08 00:02:30 UTC",
      "updated_date": "2025-01-30 22:55:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:53:10.924543"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 94,
  "processed_papers_count": 94,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T13:53:40.924626"
}
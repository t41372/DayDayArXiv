[
  {
    "arxiv_id": "2401.15245v1",
    "title": "GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation",
    "authors": [
      "Barış Yıldırım",
      "Murat Kurt"
    ],
    "abstract": "This paper presents a plugin that adds a representation of homogeneous and\nheterogeneous, optically thick, translucent materials on the Blender 3D\nmodeling tool. The working principle of this plugin is based on a combination\nof Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based\nsubsurface scattering method (GenSSS). The proposed plugin has been implemented\nusing Mitsuba renderer, which is an open source rendering software. The\nproposed plugin has been validated on measured subsurface scattering data. It's\nshown that the proposed plugin visualizes homogeneous and heterogeneous\nsubsurface scattering effects, accurately, compactly and computationally\nefficiently.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15245v1",
    "published_date": "2024-01-26 23:31:53 UTC",
    "updated_date": "2024-01-26 23:31:53 UTC"
  },
  {
    "arxiv_id": "2401.15241v2",
    "title": "Unlearning Traces the Influential Training Data of Language Models",
    "authors": [
      "Masaru Isonuma",
      "Ivan Titov"
    ],
    "abstract": "Identifying the training datasets that influence a language model's outputs\nis essential for minimizing the generation of harmful content and enhancing its\nperformance. Ideally, we can measure the influence of each dataset by removing\nit from training; however, it is prohibitively expensive to retrain a model\nmultiple times. This paper presents UnTrac: unlearning traces the influence of\na training dataset on the model's performance. UnTrac is extremely simple; each\ntraining dataset is unlearned by gradient ascent, and we evaluate how much the\nmodel's predictions change after unlearning. Furthermore, we propose a more\nscalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the\nunlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being\nefficient for massive training datasets. In the experiments, we examine if our\nmethods can assess the influence of pretraining datasets on generating toxic,\nbiased, and untruthful content. Our methods estimate their influence much more\naccurately than existing methods while requiring neither excessive memory space\nnor multiple checkpoints.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, to appear in ACL2024 main conference (long paper)",
    "pdf_url": "http://arxiv.org/pdf/2401.15241v2",
    "published_date": "2024-01-26 23:17:31 UTC",
    "updated_date": "2024-06-13 16:28:47 UTC"
  },
  {
    "arxiv_id": "2401.15238v1",
    "title": "Deep Learning with Tabular Data: A Self-supervised Approach",
    "authors": [
      "Tirth Kiranbhai Vyas"
    ],
    "abstract": "We have described a novel approach for training tabular data using the\nTabTransformer model with self-supervised learning. Traditional machine\nlearning models for tabular data, such as GBDT are being widely used though our\npaper examines the effectiveness of the TabTransformer which is a Transformer\nbased model optimised specifically for tabular data. The TabTransformer\ncaptures intricate relationships and dependencies among features in tabular\ndata by leveraging the self-attention mechanism of Transformers. We have used a\nself-supervised learning approach in this study, where the TabTransformer\nlearns from unlabelled data by creating surrogate supervised tasks, eliminating\nthe need for the labelled data. The aim is to find the most effective\nTabTransformer model representation of categorical and numerical features. To\naddress the challenges faced during the construction of various input settings\ninto the Transformers. Furthermore, a comparative analysis is also been\nconducted to examine performance of the TabTransformer model against baseline\nmodels such as MLP and supervised TabTransformer.\n  The research has presented with a novel approach by creating various variants\nof TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which\nhas helped to increase the effective capturing of the underlying relationship\nbetween various features of the tabular dataset by constructing optimal inputs.\nAnd further we have employed a self-supervised learning approach in the form of\na masking-based unsupervised setting for tabular data. The findings shed light\non the best way to represent categorical and numerical features, emphasizing\nthe TabTransormer performance when compared to established machine learning\nmodels and other self-supervised learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15238v1",
    "published_date": "2024-01-26 23:12:41 UTC",
    "updated_date": "2024-01-26 23:12:41 UTC"
  },
  {
    "arxiv_id": "2401.15222v2",
    "title": "Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection",
    "authors": [
      "Abdullateef I. Almudaifer",
      "Whitney Covington",
      "JaMor Hairston",
      "Zachary Deitch",
      "Ankit Anand",
      "Caleb M. Carroll",
      "Estera Crisan",
      "William Bradford",
      "Lauren Walter",
      "Eaton Ellen",
      "Sue S. Feldman",
      "John D. Osborne"
    ],
    "abstract": "Background: The semantics of entities extracted from a clinical text can be\ndramatically altered by modifiers, including entity negation, uncertainty,\nconditionality, severity, and subject. Existing models for determining\nmodifiers of clinical entities involve regular expression or features weights\nthat are trained independently for each modifier.\n  Methods: We develop and evaluate a multi-task transformer architecture design\nwhere modifiers are learned and predicted jointly using the publicly available\nSemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that\ncontains modifiers shared with SemEval as well as novel modifiers specific for\nOUD. We evaluate the effectiveness of our multi-task learning approach versus\npreviously published systems and assess the feasibility of transfer learning\nfor clinical entity modifiers when only a portion of clinical modifiers are\nshared.\n  Results: Our approach achieved state-of-the-art results on the ShARe corpus\nfrom SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,\n1.7% on unweighted accuracy, and 10% on micro F1 scores.\n  Conclusions: We show that learned weights from our shared model can be\neffectively transferred to a new partially matched data set, validating the use\nof transfer learning for clinical text modifiers",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 2 figures, 6 tables. To be submitted to the Journal of\n  Biomedical Semantics",
    "pdf_url": "http://arxiv.org/pdf/2401.15222v2",
    "published_date": "2024-01-26 22:19:31 UTC",
    "updated_date": "2024-02-05 17:13:41 UTC"
  },
  {
    "arxiv_id": "2401.16438v1",
    "title": "Do deep neural networks utilize the weight space efficiently?",
    "authors": [
      "Onur Can Koyun",
      "Behçet Uğur Töreyin"
    ],
    "abstract": "Deep learning models like Transformers and Convolutional Neural Networks\n(CNNs) have revolutionized various domains, but their parameter-intensive\nnature hampers deployment in resource-constrained settings. In this paper, we\nintroduce a novel concept utilizes column space and row space of weight\nmatrices, which allows for a substantial reduction in model parameters without\ncompromising performance. Leveraging this paradigm, we achieve\nparameter-efficient deep learning models.. Our approach applies to both\nBottleneck and Attention layers, effectively halving the parameters while\nincurring only minor performance degradation. Extensive experiments conducted\non the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of\nour method, showcasing competitive performance when compared to traditional\nmodels. This approach not only addresses the pressing demand for parameter\nefficient deep learning solutions but also holds great promise for practical\ndeployment in real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.16438v1",
    "published_date": "2024-01-26 21:51:49 UTC",
    "updated_date": "2024-01-26 21:51:49 UTC"
  },
  {
    "arxiv_id": "2402.01721v2",
    "title": "Non-Consensual Synthetic Intimate Imagery: Prevalence, Attitudes, and Knowledge in 10 Countries",
    "authors": [
      "Rebecca Umbach",
      "Nicola Henry",
      "Gemma Beard",
      "Colleen Berryessa"
    ],
    "abstract": "Deepfake technologies have become ubiquitous, \"democratizing\" the ability to\nmanipulate photos and videos. One popular use of deepfake technology is the\ncreation of sexually explicit content, which can then be posted and shared\nwidely on the internet. Drawing on a survey of over 16,000 respondents in 10\ndifferent countries, this article examines attitudes and behaviors related to\n\"deepfake pornography\" as a specific form of non-consensual synthetic intimate\nimagery (NSII). Our study found that deepfake pornography behaviors were\nconsidered harmful by respondents, despite nascent societal awareness.\nRegarding the prevalence of deepfake porn victimization and perpetration, 2.2%\nof all respondents indicated personal victimization, and 1.8% all of\nrespondents indicated perpetration behaviors. Respondents from countries with\nspecific legislation still reported perpetration and victimization experiences,\nsuggesting NSII laws are inadequate to deter perpetration. Approaches to\nprevent and reduce harms may include digital literacy education, as well as\nenforced platform policies, practices, and tools which better detect, prevent,\nand respond to NSII content.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01721v2",
    "published_date": "2024-01-26 21:51:49 UTC",
    "updated_date": "2024-02-13 22:26:23 UTC"
  },
  {
    "arxiv_id": "2401.15210v2",
    "title": "Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model",
    "authors": [
      "Amin Kamali",
      "Verena Kantere",
      "Calisto Zuzarte",
      "Vincent Corvinelli"
    ],
    "abstract": "Query optimizers in RDBMSs search for execution plans expected to be optimal\nfor given queries. They use parameter estimates, often inaccurate, and make\nassumptions that may not hold in practice. Consequently, they may select plans\nthat are suboptimal at runtime, if estimates and assumptions are not valid.\nTherefore, they do not sufficiently support robust query optimization. Using ML\nto improve data systems has shown promising results for query optimization.\nInspired by this, we propose Robust Query Optimizer, (Roq), a holistic\nframework based on a risk-aware learning approach. Roq includes a novel\nformalization of the notion of robustness in the context of query optimization\nand a principled approach for its quantification and measurement based on\napproximate probabilistic ML. It also includes novel strategies and algorithms\nfor query plan evaluation and selection. Roq includes a novel learned cost\nmodel that is designed to predict the cost of query execution and the\nassociated risks and performs query optimization accordingly. We demonstrate\nthat Roq provides significant improvements in robust query optimization\ncompared with the state-of-the-art.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "14 pages, 9 figures, submitted to VLDB 2025",
    "pdf_url": "http://arxiv.org/pdf/2401.15210v2",
    "published_date": "2024-01-26 21:16:37 UTC",
    "updated_date": "2025-03-07 21:02:21 UTC"
  },
  {
    "arxiv_id": "2401.15199v2",
    "title": "SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance",
    "authors": [
      "Zahra Kharazian",
      "Tony Lindgren",
      "Sindri Magnússon",
      "Olof Steinert",
      "Oskar Andersson Reyna"
    ],
    "abstract": "Predicting failures and maintenance time in predictive maintenance is\nchallenging due to the scarcity of comprehensive real-world datasets, and among\nthose available, few are of time series format. This paper introduces a\nreal-world, multivariate time series dataset collected exclusively from a\nsingle anonymized engine component (Component X) across a fleet of SCANIA\ntrucks. The dataset includes operational data, repair records, and\nspecifications related to Component X, while maintaining confidentiality\nthrough anonymization. It is well-suited for a range of machine learning\napplications, including classification, regression, survival analysis, and\nanomaly detection, particularly in predictive maintenance scenarios. The\ndataset's large population size, diverse features (in the form of histograms\nand numerical counters), and temporal information make it a unique resource in\nthe field. The objective of releasing this dataset is to give a broad range of\nresearchers the possibility of working with real-world data from an\ninternationally well-known company and introduce a standard benchmark to the\npredictive maintenance field, fostering reproducible research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.15199v2",
    "published_date": "2024-01-26 20:51:55 UTC",
    "updated_date": "2025-03-10 09:12:04 UTC"
  },
  {
    "arxiv_id": "2401.15196v3",
    "title": "Regularized Q-Learning with Linear Function Approximation",
    "authors": [
      "Jiachen Xi",
      "Alfredo Garcia",
      "Petar Momcilovic"
    ],
    "abstract": "Regularized Markov Decision Processes serve as models of sequential decision\nmaking under uncertainty wherein the decision maker has limited information\nprocessing capacity and/or aversion to model ambiguity. With functional\napproximation, the convergence properties of learning algorithms for\nregularized MDPs (e.g. soft Q-learning) are not well understood because the\ncomposition of the regularized Bellman operator and a projection onto the span\nof basis vectors is not a contraction with respect to any norm. In this paper,\nwe consider a bi-level optimization formulation of regularized Q-learning with\nlinear functional approximation. The {\\em lower} level optimization problem\naims to identify a value function approximation that satisfies Bellman's\nrecursive optimality condition and the {\\em upper} level aims to find the\nprojection onto the span of basis vectors. This formulation motivates a\nsingle-loop algorithm with finite time convergence guarantees. The algorithm\noperates on two time-scales: updates to the projection of state-action values\nare `slow' in that they are implemented with a step size that is smaller than\nthe one used for `faster' updates of approximate solutions to Bellman's\nrecursive optimality equation. We show that, under certain assumptions, the\nproposed algorithm converges to a stationary point in the presence of Markovian\nnoise. In addition, we provide a performance guarantee for the policies derived\nfrom the proposed algorithm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15196v3",
    "published_date": "2024-01-26 20:45:40 UTC",
    "updated_date": "2025-02-10 17:59:05 UTC"
  },
  {
    "arxiv_id": "2401.15188v1",
    "title": "CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health",
    "authors": [
      "Sheng Yu",
      "Narjes Nourzad",
      "Randye J. Semple",
      "Yixue Zhao",
      "Emily Zhou",
      "Bhaskar Krishnamachari"
    ],
    "abstract": "The COVID-19 pandemic has intensified the urgency for effective and\naccessible mental health interventions in people's daily lives. Mobile Health\n(mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained\ntraction as they expand beyond traditional clinical settings to support daily\nlife. However, the effectiveness of current mHealth solutions is impeded by the\nlack of context-awareness, personalization, and modularity to foster their\nreusability. This paper introduces CAREForMe, a contextual multi-armed bandit\n(CMAB) recommendation framework for mental health. Designed with\ncontext-awareness, personalization, and modularity at its core, CAREForMe\nharnesses mobile sensing and integrates online learning algorithms with user\nclustering capability to deliver timely, personalized recommendations. With its\nmodular design, CAREForMe serves as both a customizable recommendation\nframework to guide future research, and a collaborative platform to facilitate\ninterdisciplinary contributions in mHealth research. We showcase CAREForMe's\nversatility through its implementation across various platforms (e.g., Discord,\nTelegram) and its customization to diverse recommendation features.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "MOBILESoft 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.15188v1",
    "published_date": "2024-01-26 20:18:25 UTC",
    "updated_date": "2024-01-26 20:18:25 UTC"
  },
  {
    "arxiv_id": "2402.03355v2",
    "title": "Unlocking Criminal Hierarchies: A Survey, Experimental, and Comparative Exploration of Techniques for Identifying Leaders within Criminal Networks",
    "authors": [
      "Kamal Taha",
      "Abdulhadi Shoufan",
      "Aya Taha"
    ],
    "abstract": "This survey paper offers a thorough analysis of techniques and algorithms\nused in the identification of crime leaders within criminal networks. For each\ntechnique, the paper examines its effectiveness, limitations, potential for\nimprovement, and future prospects. The main challenge faced by existing survey\npapers focusing on algorithms for identifying crime leaders and predicting\ncrimes is effectively categorizing these algorithms. To address this\nlimitation, this paper proposes a new methodological taxonomy that\nhierarchically classifies algorithms into more detailed categories and specific\ntechniques. The paper includes empirical and experimental evaluations to rank\nthe different techniques. The combination of the methodological taxonomy,\nempirical evaluations, and experimental comparisons allows for a nuanced and\ncomprehensive understanding of the techniques and algorithms for identifying\ncrime leaders, assisting researchers in making informed decisions. Moreover,\nthe paper offers valuable insights into the future prospects of techniques for\nidentifying crime leaders, emphasizing potential advancements and opportunities\nfor further research. Here's an overview of our empirical analysis findings and\nexperimental insights, along with the solution we've devised: (1) PageRank and\nEigenvector centrality are reliable for mapping network connections, (2) Katz\nCentrality can effectively identify influential criminals through indirect\nlinks, stressing their significance in criminal networks, (3) current models\nfail to account for the specific impacts of criminal influence levels, the\nimportance of socio-economic context, and the dynamic nature of criminal\nnetworks and hierarchies, and (4) we propose enhancements, such as\nincorporating temporal dynamics and sentiment analysis to reflect the fluidity\nof criminal activities and relationships",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.03355v2",
    "published_date": "2024-01-26 20:09:31 UTC",
    "updated_date": "2024-03-30 21:46:48 UTC"
  },
  {
    "arxiv_id": "2402.10921v1",
    "title": "AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning",
    "authors": [
      "Naresh Kumar Devulapally",
      "Sidharth Anand",
      "Sreyasee Das Bhattacharjee",
      "Junsong Yuan"
    ],
    "abstract": "Human emotion can be presented in different modes i.e., audio, video, and\ntext. However, the contribution of each mode in exhibiting each emotion is not\nuniform. Furthermore, the availability of complete mode-specific details may\nnot always be guaranteed in the test time. In this work, we propose AM^2-EmoJE,\na model for Adaptive Missing-Modality Emotion Recognition in Conversation via\nJoint Embedding Learning model that is grounded on two-fold contributions:\nFirst, a query adaptive fusion that can automatically learn the relative\nimportance of its mode-specific representations in a query-specific manner. By\nthis the model aims to prioritize the mode-invariant spatial query details of\nthe emotion patterns, while also retaining its mode-exclusive aspects within\nthe learned multimodal query descriptor. Second the multimodal joint embedding\nlearning module that explicitly addresses various missing modality scenarios in\ntest-time. By this, the model learns to emphasize on the correlated patterns\nacross modalities, which may help align the cross-attended mode-specific\ndescriptors pairwise within a joint-embedding space and thereby compensate for\nmissing modalities during inference. By leveraging the spatio-temporal details\nat the dialogue level, the proposed AM^2-EmoJE not only demonstrates superior\nperformance compared to the best-performing state-of-the-art multimodal\nmethods, by effectively leveraging body language in place of face expression,\nit also exhibits an enhanced privacy feature. By reporting around 2-5%\nimprovement in the weighted-F1 score, the proposed multimodal joint embedding\nmodule facilitates an impressive performance gain in a variety of\nmissing-modality query scenarios during test time.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10921v1",
    "published_date": "2024-01-26 19:57:26 UTC",
    "updated_date": "2024-01-26 19:57:26 UTC"
  },
  {
    "arxiv_id": "2401.15170v2",
    "title": "Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks",
    "authors": [
      "Zackary Okun Dunivin"
    ],
    "abstract": "Qualitative coding, or content analysis, extracts meaning from text to\ndiscern quantitative patterns across a corpus of texts. Recently, advances in\nthe interpretive abilities of large language models (LLMs) offer potential for\nautomating the coding process (applying category labels to texts), thereby\nenabling human researchers to concentrate on more creative research aspects,\nwhile delegating these interpretive tasks to AI. Our case study comprises a set\nof socio-historical codes on dense, paragraph-long passages representative of a\nhumanistic study. We show that GPT-4 is capable of human-equivalent\ninterpretations, whereas GPT-3.5 is not. Compared to our human-derived gold\nstandard, GPT-4 delivers excellent intercoder reliability (Cohen's $\\kappa \\geq\n0.79$) for 3 of 9 codes, and substantial reliability ($\\kappa \\geq 0.6$) for 8\nof 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes\n($mean(\\kappa) = 0.34$; $max(\\kappa) = 0.55$). Importantly, we find that coding\nfidelity improves considerably when the LLM is prompted to give rationale\njustifying its coding decisions (chain-of-thought reasoning). We present these\nand other findings along with a set of best practices for adapting traditional\ncodebooks for LLMs. Our results indicate that for certain codebooks,\nstate-of-the-art LLMs are already adept at large-scale content analysis.\nFurthermore, they suggest the next generation of models will likely render AI\ncoding a viable option for a majority of codebooks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15170v2",
    "published_date": "2024-01-26 19:25:43 UTC",
    "updated_date": "2024-02-12 23:04:10 UTC"
  },
  {
    "arxiv_id": "2401.15075v1",
    "title": "Annotated Hands for Generative Models",
    "authors": [
      "Yue Yang",
      "Atith N Gandhi",
      "Greg Turk"
    ],
    "abstract": "Generative models such as GANs and diffusion models have demonstrated\nimpressive image generation capabilities. Despite these successes, these\nsystems are surprisingly poor at creating images with hands. We propose a novel\ntraining framework for generative models that substantially improves the\nability of such systems to create hand images. Our approach is to augment the\ntraining images with three additional channels that provide annotations to\nhands in the image. These annotations provide additional structure that coax\nthe generative model to produce higher quality hand images. We demonstrate this\napproach on two different generative models: a generative adversarial network\nand a diffusion model. We demonstrate our method both on a new synthetic\ndataset of hand images and also on real photographs that contain hands. We\nmeasure the improved quality of the generated hands through higher confidence\nin finger joint identification using an off-the-shelf hand detector.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15075v1",
    "published_date": "2024-01-26 18:57:54 UTC",
    "updated_date": "2024-01-26 18:57:54 UTC"
  },
  {
    "arxiv_id": "2402.01720v2",
    "title": "Deep Learning Based Amharic Chatbot for FAQs in Universities",
    "authors": [
      "Goitom Ybrah Hailu",
      "Hadush Hailu",
      "Shishay Welay"
    ],
    "abstract": "University students often spend a considerable amount of time seeking answers\nto common questions from administrators or teachers. This can become tedious\nfor both parties, leading to a need for a solution. In response, this paper\nproposes a chatbot model that utilizes natural language processing and deep\nlearning techniques to answer frequently asked questions (FAQs) in the Amharic\nlanguage. Chatbots are computer programs that simulate human conversation\nthrough the use of artificial intelligence (AI), acting as a virtual assistant\nto handle questions and other tasks. The proposed chatbot program employs\ntokenization, normalization, stop word removal, and stemming to analyze and\ncategorize Amharic input sentences. Three machine learning model algorithms\nwere used to classify tokens and retrieve appropriate responses: Support Vector\nMachine (SVM), Multinomial Na\\\"ive Bayes, and deep neural networks implemented\nthrough TensorFlow, Keras, and NLTK. The deep learning model achieved the best\nresults with 91.55% accuracy and a validation loss of 0.3548 using an Adam\noptimizer and SoftMax activation function. The chatbot model was integrated\nwith Facebook Messenger and deployed on a Heroku server for 24-hour\naccessibility. The experimental results demonstrate that the chatbot framework\nachieved its objectives and effectively addressed challenges such as Amharic\nFidel variation, morphological variation, and lexical gaps. Future research\ncould explore the integration of Amharic WordNet to narrow the lexical gap and\nsupport more complex questions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01720v2",
    "published_date": "2024-01-26 18:37:21 UTC",
    "updated_date": "2024-11-05 18:45:22 UTC"
  },
  {
    "arxiv_id": "2401.15043v3",
    "title": "Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning",
    "authors": [
      "Md Mushfiqur Rahman",
      "Mohammad Sabik Irbaz",
      "Kai North",
      "Michelle S. Williams",
      "Marcos Zampieri",
      "Kevin Lybarger"
    ],
    "abstract": "Objective: The reading level of health educational materials significantly\ninfluences the understandability and accessibility of the information,\nparticularly for minoritized populations. Many patient educational resources\nsurpass the reading level and complexity of widely accepted standards. There is\na critical need for high-performing text simplification models in health\ninformation to enhance dissemination and literacy. This need is particularly\nacute in cancer education, where effective prevention and screening education\ncan substantially reduce morbidity and mortality.\n  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel\ncorpus of cancer education materials tailored for health text simplification\nresearch, comprising educational content from the American Cancer Society,\nCenters for Disease Control and Prevention, and National Cancer Institute.\nUtilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large\nLanguage Model (LLM)-based simplification methods, including fine-tuning,\nreinforcement learning (RL), reinforcement learning with human feedback (RLHF),\ndomain adaptation, and prompt-based approaches. Our experimentation encompasses\nLlama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a\nlightweight model adept at distinguishing between original and simplified\ntexts, thereby enhancing the model's effectiveness with unlabeled data.\n  Results: Fine-tuned Llama 2 models demonstrated high performance across\nvarious metrics. Our innovative RLHF reward function surpassed existing RL text\nsimplification reward functions in effectiveness. The results underscore that\nRL/RLHF can augment fine-tuning, facilitating model training on unlabeled text\nand improving performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in Journal of Biomedical Informatics, Volume 158, October\n  2024, 104727",
    "pdf_url": "http://arxiv.org/pdf/2401.15043v3",
    "published_date": "2024-01-26 18:13:57 UTC",
    "updated_date": "2024-11-10 19:47:22 UTC"
  },
  {
    "arxiv_id": "2401.15042v4",
    "title": "PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
    "authors": [
      "Haochen Tan",
      "Zhijiang Guo",
      "Zhan Shi",
      "Lu Xu",
      "Zhili Liu",
      "Yunlong Feng",
      "Xiaoguang Li",
      "Yasheng Wang",
      "Lifeng Shang",
      "Qun Liu",
      "Linqi Song"
    ],
    "abstract": "Large Language Models (LLMs) have succeeded remarkably in understanding\nlong-form contents. However, exploring their capability for generating\nlong-form contents, such as reports and articles, has been relatively\nunexplored and inadequately assessed by existing benchmarks. The prevalent\nevaluation methods, which predominantly rely on crowdsourcing, are recognized\nfor their labor-intensive nature and lack of efficiency, whereas automated\nmetrics, such as the ROUGE score, demonstrate discordance with human judgment\ncriteria. In this paper, we propose ProxyQA, an innovative framework dedicated\nto assessing long-text generation. ProxyQA comprises in-depth human-curated\nmeta-questions spanning various domains, each accompanied by specific\nproxy-questions with pre-annotated answers. LLMs are tasked to generate\nextensive content in response to these meta-questions, by engaging an evaluator\nand incorporating the generated texts as contextual background, ProxyQA\nassesses the generated content's quality through the evaluator's accuracy in\naddressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA's\ndemanding nature as a high-quality assessment tool. Human evaluation\ndemonstrates that the proxy-question method is notably self-consistent and\naligns closely with human evaluative standards. The dataset and leaderboard is\navailable at \\url{https://proxy-qa.com}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2401.15042v4",
    "published_date": "2024-01-26 18:12:25 UTC",
    "updated_date": "2024-06-04 12:46:47 UTC"
  },
  {
    "arxiv_id": "2401.15018v1",
    "title": "Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers",
    "authors": [
      "Kerlos Atia Abdalmalak",
      "Ascensión Gallardo-Antol'in"
    ],
    "abstract": "Speaker Verification (SV) systems involve mainly two individual stages:\nfeature extraction and classification. In this paper, we explore these two\nmodules with the aim of improving the performance of a speaker verification\nsystem under noisy conditions. On the one hand, the choice of the most\nappropriate acoustic features is a crucial factor for performing robust speaker\nverification. The acoustic parameters used in the proposed system are: Mel\nFrequency Cepstral Coefficients (MFCC), their first and second derivatives\n(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),\nPerceptual Linear Predictive (PLP), and Relative Spectral Transform -\nPerceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison\nof different combinations of the previous features is discussed. On the other\nhand, the major weakness of a conventional Support Vector Machine (SVM)\nclassifier is the use of generic traditional kernel functions to compute the\ndistances among data points. However, the kernel function of an SVM has great\ninfluence on its performance. In this work, we propose the combination of two\nSVM-based classifiers with different kernel functions: Linear kernel and\nGaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)\nclassifier. The combination is carried out by means of a parallel structure\napproach, in which different voting rules to take the final decision are\nconsidered. Results show that significant improvement in the performance of the\nSV system is achieved by using the combined features with the combined\nclassifiers either with clean speech or in the presence of noise. Finally, to\nenhance the system more in noisy environments, the inclusion of the multiband\nnoise removal technique as a preprocessing stage is proposed.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15018v1",
    "published_date": "2024-01-26 17:19:59 UTC",
    "updated_date": "2024-01-26 17:19:59 UTC"
  },
  {
    "arxiv_id": "2401.15006v2",
    "title": "Airavata: Introducing Hindi Instruction-tuned LLM",
    "authors": [
      "Jay Gala",
      "Thanmay Jayakumar",
      "Jaavid Aktar Husain",
      "Aswanth Kumar M",
      "Mohammed Safi Ur Rahman Khan",
      "Diptesh Kanojia",
      "Ratish Puduppully",
      "Mitesh M. Khapra",
      "Raj Dabre",
      "Rudra Murthy",
      "Anoop Kunchukuttan"
    ],
    "abstract": "We announce the initial release of \"Airavata,\" an instruction-tuned LLM for\nHindi. Airavata was created by fine-tuning OpenHathi with diverse,\ninstruction-tuning Hindi datasets to make it better suited for assistive tasks.\nAlong with the model, we also share the IndicInstruct dataset, which is a\ncollection of diverse instruction-tuning datasets to enable further research\nfor Indic LLMs. Additionally, we present evaluation benchmarks and a framework\nfor assessing LLM performance across tasks in Hindi. Currently, Airavata\nsupports Hindi, but we plan to expand this to all 22 scheduled Indic languages.\nYou can access all artifacts at https://ai4bharat.github.io/airavata.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2401.15006v2",
    "published_date": "2024-01-26 17:07:08 UTC",
    "updated_date": "2024-02-26 12:17:25 UTC"
  },
  {
    "arxiv_id": "2401.15132v1",
    "title": "On the Emergence of Symmetrical Reality",
    "authors": [
      "Zhenliang Zhang",
      "Zeyu Zhang",
      "Ziyuan Jiao",
      "Yao Su",
      "Hangxin Liu",
      "Wei Wang",
      "Song-Chun Zhu"
    ],
    "abstract": "Artificial intelligence (AI) has revolutionized human cognitive abilities and\nfacilitated the development of new AI entities capable of interacting with\nhumans in both physical and virtual environments. Despite the existence of\nvirtual reality, mixed reality, and augmented reality for several years,\nintegrating these technical fields remains a formidable challenge due to their\ndisparate application directions. The advent of AI agents, capable of\nautonomous perception and action, further compounds this issue by exposing the\nlimitations of traditional human-centered research approaches. It is imperative\nto establish a comprehensive framework that accommodates the dual perceptual\ncenters of humans and AI agents in both physical and virtual worlds. In this\npaper, we introduce the symmetrical reality framework, which offers a unified\nrepresentation encompassing various forms of physical-virtual amalgamations.\nThis framework enables researchers to better comprehend how AI agents can\ncollaborate with humans and how distinct technical pathways of physical-virtual\nintegration can be consolidated from a broader perspective. We then delve into\nthe coexistence of humans and AI, demonstrating a prototype system that\nexemplifies the operation of symmetrical reality systems for specific tasks,\nsuch as pouring water. Subsequently, we propose an instance of an AI-driven\nactive assistance service that illustrates the potential applications of\nsymmetrical reality. This paper aims to offer beneficial perspectives and\nguidance for researchers and practitioners in different fields, thus\ncontributing to the ongoing research about human-AI coexistence in both\nphysical and virtual environments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "IEEE VR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.15132v1",
    "published_date": "2024-01-26 16:09:39 UTC",
    "updated_date": "2024-01-26 16:09:39 UTC"
  },
  {
    "arxiv_id": "2401.14968v1",
    "title": "Atmosphere: Context and situational-aware collaborative IoT architecture for edge-fog-cloud computing",
    "authors": [
      "Guadalupe Ortiz",
      "Meftah Zouai",
      "Okba Kazar",
      "Alfonso Garcia-de-Prado",
      "Juan Boubeta-Puig"
    ],
    "abstract": "The Internet of Things (IoT) has grown significantly in popularity,\naccompanied by increased capacity and lower cost of communications, and\noverwhelming development of technologies. At the same time, big data and\nreal-time data analysis have taken on great importance and have been\naccompanied by unprecedented interest in sharing data among citizens, public\nadministrations and other organisms, giving rise to what is known as the\nCollaborative Internet of Things. This growth in data and infrastructure must\nbe accompanied by a software architecture that allows its exploitation.\nAlthough there are various proposals focused on the exploitation of the IoT at\nedge, fog and/or cloud levels, it is not easy to find a software solution that\nexploits the three tiers together, taking maximum advantage not only of the\nanalysis of contextual and situational data at each tier, but also of two-way\ncommunications between adjacent ones. In this paper, we propose an architecture\nthat solves these deficiencies by proposing novel technologies which are\nappropriate for managing the resources of each tier: edge, fog and cloud. In\naddition, the fact that two-way communications along the three tiers of the\narchitecture is allowed considerably enriches the contextual and situational\ninformation in each layer, and substantially assists decision making in real\ntime. The paper illustrates the proposed software architecture through a case\nstudy of respiratory disease surveillance in hospitals. As a result, the\nproposed architecture permits efficient communications between the different\ntiers responding to the needs of these types of IoT scenarios.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14968v1",
    "published_date": "2024-01-26 16:01:09 UTC",
    "updated_date": "2024-01-26 16:01:09 UTC"
  },
  {
    "arxiv_id": "2401.14953v1",
    "title": "Learning Universal Predictors",
    "authors": [
      "Jordi Grau-Moya",
      "Tim Genewein",
      "Marcus Hutter",
      "Laurent Orseau",
      "Grégoire Delétang",
      "Elliot Catt",
      "Anian Ruoss",
      "Li Kevin Wenliang",
      "Christopher Mattern",
      "Matthew Aitchison",
      "Joel Veness"
    ],
    "abstract": "Meta-learning has emerged as a powerful approach to train neural networks to\nlearn new tasks quickly from limited data. Broad exposure to different tasks\nleads to versatile representations enabling general problem solving. But, what\nare the limits of meta-learning? In this work, we explore the potential of\namortizing the most powerful universal predictor, namely Solomonoff Induction\n(SI), into neural networks via leveraging meta-learning to its limits. We use\nUniversal Turing Machines (UTMs) to generate training data used to expose\nnetworks to a broad range of patterns. We provide theoretical analysis of the\nUTM data generation processes and meta-training protocols. We conduct\ncomprehensive experiments with neural architectures (e.g. LSTMs, Transformers)\nand algorithmic data generators of varying complexity and universality. Our\nresults suggest that UTM data is a valuable resource for meta-learning, and\nthat it can be used to train neural networks capable of learning universal\nprediction strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14953v1",
    "published_date": "2024-01-26 15:37:16 UTC",
    "updated_date": "2024-01-26 15:37:16 UTC"
  },
  {
    "arxiv_id": "2401.14948v1",
    "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training",
    "authors": [
      "Shruthi Gowda",
      "Bahram Zonooz",
      "Elahe Arani"
    ],
    "abstract": "Adversarial training improves the robustness of neural networks against\nadversarial attacks, albeit at the expense of the trade-off between standard\nand robust generalization. To unveil the underlying factors driving this\nphenomenon, we examine the layer-wise learning capabilities of neural networks\nduring the transition from a standard to an adversarial setting. Our empirical\nfindings demonstrate that selectively updating specific layers while preserving\nothers can substantially enhance the network's learning capacity. We therefore\npropose CURE, a novel training framework that leverages a gradient prominence\ncriterion to perform selective conservation, updating, and revision of weights.\nImportantly, CURE is designed to be dataset- and architecture-agnostic,\nensuring its applicability across various scenarios. It effectively tackles\nboth memorization and overfitting issues, thus enhancing the trade-off between\nrobustness and generalization and additionally, this training approach also\naids in mitigating \"robust overfitting\". Furthermore, our study provides\nvaluable insights into the mechanisms of selective adversarial training and\noffers a promising avenue for future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14948v1",
    "published_date": "2024-01-26 15:33:39 UTC",
    "updated_date": "2024-01-26 15:33:39 UTC"
  },
  {
    "arxiv_id": "2401.14933v1",
    "title": "SSDOnt: an Ontology for representing Single-Subject Design Studies",
    "authors": [
      "Idoia Berges",
      "Jesús Bermúdez",
      "Arantza Illarramendi"
    ],
    "abstract": "Background: Single-Subject Design is used in several areas such as education\nand biomedicine. However, no suited formal vocabulary exists for annotating the\ndetailed configuration and the results of this type of research studies with\nthe appropriate granularity for looking for information about them. Therefore,\nthe search for those study designs relies heavily on a syntactical search on\nthe abstract, keywords or full text of the publications about the study, which\nentails some limitations. Objective: To present SSDOnt, a specific purpose\nontology for describing and annotating single-subject design studies, so that\ncomplex questions can be asked about them afterwards. Methods: The ontology was\ndeveloped following the NeOn methodology. Once the requirements of the ontology\nwere defined, a formal model was described in a Description Logic and later\nimplemented in the ontology language OWL 2 DL. Results: We show how the\nontology provides a reference model with a suitable terminology for the\nannotation and searching of single-subject design studies and their main\ncomponents, such as the phases, the intervention types, the outcomes and the\nresults. Some mappings with terms of related ontologies have been established.\nWe show as proof-of-concept that classes in the ontology can be easily extended\nto annotate more precise information about specific interventions and outcomes\nsuch as those related to autism. Moreover, we provide examples of some types of\nqueries that can be posed to the ontology. Conclusions: SSDOnt has achieved the\npurpose of covering the descriptions of the domain of single-subject research\nstudies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This document is the Accepted Manuscript version of a Published Work\n  that appeared in final form in Methods of Information in Medicine 57(01/02) :\n  55-61 (2018), copyright 2018 Schattauer. To access the final edited and\n  published work see https://doi.org/10.3414/ME17-01-0109",
    "pdf_url": "http://arxiv.org/pdf/2401.14933v1",
    "published_date": "2024-01-26 15:11:31 UTC",
    "updated_date": "2024-01-26 15:11:31 UTC"
  },
  {
    "arxiv_id": "2401.14931v2",
    "title": "Do LLMs Dream of Ontologies?",
    "authors": [
      "Marco Bombieri",
      "Paolo Fiorini",
      "Simone Paolo Ponzetto",
      "Marco Rospocher"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse natural language processing tasks, yet their ability to memorize\nstructured knowledge remains underexplored. In this paper, we investigate the\nextent to which general-purpose pre-trained LLMs retain and correctly reproduce\nconcept identifier (ID)-label associations from publicly available ontologies.\nWe conduct a systematic evaluation across multiple ontological resources,\nincluding the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as\nPythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only\na small fraction of ontological concepts is accurately memorized, with GPT-4\ndemonstrating the highest performance. To understand why certain concepts are\nmemorized more effectively than others, we analyze the relationship between\nmemorization accuracy and concept popularity on the Web. Our results indicate a\nstrong correlation between the frequency of a concept's occurrence online and\nthe likelihood of accurately retrieving its ID from the label. This suggests\nthat LLMs primarily acquire such knowledge through indirect textual exposure\nrather than directly from structured ontological resources. Furthermore, we\nintroduce new metrics to quantify prediction invariance, demonstrating that the\nstability of model responses across variations in prompt language and\ntemperature settings can serve as a proxy for estimating memorization\nrobustness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14931v2",
    "published_date": "2024-01-26 15:10:23 UTC",
    "updated_date": "2025-02-02 13:51:02 UTC"
  },
  {
    "arxiv_id": "2401.14923v1",
    "title": "Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks",
    "authors": [
      "Eura Nofshin",
      "Siddharth Swaroop",
      "Weiwei Pan",
      "Susan Murphy",
      "Finale Doshi-Velez"
    ],
    "abstract": "Many important behavior changes are frictionful; they require individuals to\nexpend effort over a long period with little immediate gratification. Here, an\nartificial intelligence (AI) agent can provide personalized interventions to\nhelp individuals stick to their goals. In these settings, the AI agent must\npersonalize rapidly (before the individual disengages) and interpretably, to\nhelp us understand the behavioral interventions. In this paper, we introduce\nBehavior Model Reinforcement Learning (BMRL), a framework in which an AI agent\nintervenes on the parameters of a Markov Decision Process (MDP) belonging to a\nboundedly rational human agent. Our formulation of the human decision-maker as\na planning agent allows us to attribute undesirable human policies (ones that\ndo not lead to the goal) to their maladapted MDP parameters, such as an\nextremely low discount factor. Furthermore, we propose a class of tractable\nhuman models that captures fundamental behaviors in frictionful tasks.\nIntroducing a notion of MDP equivalence specific to BMRL, we theoretically and\nempirically show that AI planning with our human models can lead to helpful\npolicies on a wide range of more complex, ground-truth humans.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "In AAMAS 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14923v1",
    "published_date": "2024-01-26 14:59:48 UTC",
    "updated_date": "2024-01-26 14:59:48 UTC"
  },
  {
    "arxiv_id": "2401.14915v2",
    "title": "Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students",
    "authors": [
      "Chengbo Zheng",
      "Kangyu Yuan",
      "Bingcan Guo",
      "Reza Hadi Mogavi",
      "Zhenhui Peng",
      "Shuai Ma",
      "Xiaojuan Ma"
    ],
    "abstract": "The increasing use of Artificial Intelligence (AI) by students in learning\npresents new challenges for assessing their learning outcomes in project-based\nlearning (PBL). This paper introduces a co-design study to explore the\npotential of students' AI usage data as a novel material for PBL assessment. We\nconducted workshops with 18 college students, encouraging them to speculate an\nalternative world where they could freely employ AI in PBL while needing to\nreport this process to assess their skills and contributions. Our workshops\nyielded various scenarios of students' use of AI in PBL and ways of analyzing\nthese uses grounded by students' vision of education goal transformation. We\nalso found students with different attitudes toward AI exhibited distinct\npreferences in how to analyze and understand the use of AI. Based on these\nfindings, we discuss future research opportunities on student-AI interactions\nand understanding AI-enhanced learning.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Conditionally accepted by CHI '24",
    "pdf_url": "http://arxiv.org/pdf/2401.14915v2",
    "published_date": "2024-01-26 14:49:29 UTC",
    "updated_date": "2024-01-29 14:04:14 UTC"
  },
  {
    "arxiv_id": "2401.14876v2",
    "title": "Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem",
    "authors": [
      "Chen Huang",
      "Haoyang Li",
      "Yifan Zhang",
      "Wenqiang Lei",
      "Jiancheng Lv"
    ],
    "abstract": "The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to\nextract low-frequency signals from graph topology, which may lead to the\nover-smoothing problem when GCN goes deep. To this end, various methods have\nbeen proposed to create an adaptive filter by incorporating an extra filter\n(e.g., a high-pass filter) extracted from the graph topology. However, these\nmethods heavily rely on topological information and ignore the node attribute\nspace, which severely sacrifices the expressive power of the deep GCNs,\nespecially when dealing with disassortative graphs. In this paper, we propose a\ncross-space adaptive filter, called CSF, to produce the adaptive-frequency\ninformation extracted from both the topology and attribute spaces.\nSpecifically, we first derive a tailored attribute-based high-pass filter that\ncan be interpreted theoretically as a minimizer for semi-supervised kernel\nridge regression. Then, we cast the topology-based low-pass filter as a\nMercer's kernel within the context of GCNs. This serves as a foundation for\ncombining it with the attribute-based filter to capture the adaptive-frequency\ninformation. Finally, we derive the cross-space filter via an effective\nmultiple-kernel learning strategy, which unifies the attribute-based high-pass\nfilter and the topology-based low-pass filter. This helps to address the\nover-smoothing problem while maintaining effectiveness. Extensive experiments\ndemonstrate that CSF not only successfully alleviates the over-smoothing\nproblem but also promotes the effectiveness of the node classification task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to WWW 2024. V2: update the results on GCN-BC based on our\n  rebuttal on OpenReview. Our code is available at\n  https://github.com/huangzichun/Cross-Space-Adaptive-Filter",
    "pdf_url": "http://arxiv.org/pdf/2401.14876v2",
    "published_date": "2024-01-26 14:02:29 UTC",
    "updated_date": "2024-02-10 08:58:14 UTC"
  },
  {
    "arxiv_id": "2401.14856v1",
    "title": "Memory-Inspired Temporal Prompt Interaction for Text-Image Classification",
    "authors": [
      "Xinyao Yu",
      "Hao Sun",
      "Ziwei Niu",
      "Rui Qin",
      "Zhenjia Bai",
      "Yen-Wei Chen",
      "Lanfen Lin"
    ],
    "abstract": "In recent years, large-scale pre-trained multimodal models (LMM) generally\nemerge to integrate the vision and language modalities, achieving considerable\nsuccess in various natural language processing and computer vision tasks. The\ngrowing size of LMMs, however, results in a significant computational cost for\nfine-tuning these models for downstream tasks. Hence, prompt-based interaction\nstrategy is studied to align modalities more efficiently. In this contex, we\npropose a novel prompt-based multimodal interaction strategy inspired by human\nmemory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our\nproposed method involves in two stages as in human memory strategy: the\nacquiring stage, and the consolidation and activation stage. We utilize\ntemporal prompts on intermediate layers to imitate the acquiring stage,\nleverage similarity-based prompt interaction to imitate memory consolidation,\nand employ prompt generation strategy to imitate memory activation. The main\nstrength of our paper is that we interact the prompt vectors on intermediate\nlayers to leverage sufficient information exchange between modalities, with\ncompressed trainable parameters and memory usage. We achieve competitive\nresults on several datasets with relatively small memory usage and 2.0M of\ntrainable parameters (about 1% of the pre-trained foundation model).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14856v1",
    "published_date": "2024-01-26 13:36:12 UTC",
    "updated_date": "2024-01-26 13:36:12 UTC"
  },
  {
    "arxiv_id": "2401.14831v3",
    "title": "The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Relations",
    "authors": [
      "Hubert Padusinski",
      "Christian Steinhauser",
      "Thilo Braun",
      "Lennart Ries",
      "Eric Sax"
    ],
    "abstract": "Machine Vision (MV) is essential for solving driving automation. This paper\nexamines potential shortcomings in current MV testing strategies for highly\nautomated driving (HAD) systems. We argue for a more comprehensive\nunderstanding of the performance factors that must be considered during the MV\nevaluation process, noting that neglecting these factors can lead to\nsignificant risks. This is not only relevant to MV component testing, but also\nto integration testing. To illustrate this point, we draw an analogy to a ship\nnavigating towards an iceberg to show potential hidden challenges in current MV\ntesting strategies. The main contribution is a novel framework for black-box\ntesting which observes environmental relations. This means it is designed to\nenhance MV assessments by considering the attributes and surroundings of\nrelevant individual objects. The framework provides the identification of seven\ngeneral concerns about the object recognition of MV, which are not addressed\nadequately in established test processes. To detect these deficits based on\ntheir performance factors, we propose the use of a taxonomy called \"granularity\norders\" along with a graphical representation. This allows an identification of\nMV uncertainties across a range of driving scenarios. This approach aims to\nadvance the precision, efficiency, and completeness of testing procedures for\nMV.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.SE",
      "eess.IV"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted at IEEE ITSC 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14831v3",
    "published_date": "2024-01-26 12:59:26 UTC",
    "updated_date": "2024-04-30 13:25:57 UTC"
  },
  {
    "arxiv_id": "2401.14811v1",
    "title": "On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks",
    "authors": [
      "Joar Skalse",
      "Alessandro Abate"
    ],
    "abstract": "In this paper, we study the expressivity of scalar, Markovian reward\nfunctions in Reinforcement Learning (RL), and identify several limitations to\nwhat they can express. Specifically, we look at three classes of RL tasks;\nmulti-objective RL, risk-sensitive RL, and modal RL. For each class, we derive\nnecessary and sufficient conditions that describe when a problem in this class\ncan be expressed using a scalar, Markovian reward. Moreover, we find that\nscalar, Markovian rewards are unable to express most of the instances in each\nof these three classes. We thereby contribute to a more complete understanding\nof what standard reward functions can and cannot express. In addition to this,\nwe also call attention to modal problems as a new class of problems, since they\nhave so far not been given any systematic treatment in the RL literature. We\nalso briefly outline some approaches for solving some of the problems we\ndiscuss, by means of bespoke RL algorithms.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14811v1",
    "published_date": "2024-01-26 12:18:29 UTC",
    "updated_date": "2024-01-26 12:18:29 UTC"
  },
  {
    "arxiv_id": "2401.14777v1",
    "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
    "authors": [
      "Pau Rodriguez Inserte",
      "Mariam Nakhlé",
      "Raheel Qader",
      "Gaetan Caillaut",
      "Jingshu Liu"
    ],
    "abstract": "Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14777v1",
    "published_date": "2024-01-26 11:04:01 UTC",
    "updated_date": "2024-01-26 11:04:01 UTC"
  },
  {
    "arxiv_id": "2401.15124v1",
    "title": "Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities",
    "authors": [
      "E. Wianto",
      "H. Toba",
      "M. Malinda",
      "Chien-Hsu Chen"
    ],
    "abstract": "Maintaining a high quality of life through physical activities (PA) to\nprevent health decline is crucial. However, the relationship between\nindividuals health status, PA preferences, and motion factors is complex. PA\ndiscussions consistently show a positive correlation with healthy aging\nexperiences, but no explicit relation to specific types of musculoskeletal\nexercises. Taking advantage of the increasingly widespread existence of\nsmartphones, especially in Indonesia, this research utilizes embedded sensors\nfor Human Activity Recognition (HAR). Based on 25 participants data, performing\nnine types of selected motion, this study has successfully identified important\nsensor attributes that play important roles in the right and left hands for\nmuscle strength motions as the basis for developing machine learning models\nwith the LSTM algorithm.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.1.2"
    ],
    "primary_category": "cs.HC",
    "comment": "9 pages, 4 figures, AHFE International Conference on Human Factors in\n  Design, Engineering, and Computing",
    "pdf_url": "http://arxiv.org/pdf/2401.15124v1",
    "published_date": "2024-01-26 10:44:44 UTC",
    "updated_date": "2024-01-26 10:44:44 UTC"
  },
  {
    "arxiv_id": "2401.14749v1",
    "title": "Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes",
    "authors": [
      "Vasiliy Usatyuk",
      "Denis Sapozhnikov",
      "Sergey Egorov"
    ],
    "abstract": "This paper presents a method for achieving equilibrium in the ISING\nHamiltonian when confronted with unevenly distributed charges on an irregular\ngrid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our\napproach involves dimensionally expanding the system, substituting charges with\ncirculants, and representing distances through circulant shifts. This results\nin a systematic mapping of the charge system onto a space, transforming the\nirregular grid into a uniform configuration, applicable to Torical and Circular\nHyperboloid Topologies. The paper covers fundamental definitions and notations\nrelated to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine.\nIt explores the marginalization problem in code on the graph probabilistic\nmodels for evaluating the partition function, encompassing exact and\napproximate estimation techniques. Rigorous proof is provided for the\nattainability of equilibrium states for the Boltzmann machine under Torical and\nCircular Hyperboloid, paving the way for the application of our methodology.\nPractical applications of our approach are investigated in Finite Geometry\nQC-LDPC Codes, specifically in Material Science. The paper further explores its\neffectiveness in the realm of Natural Language Processing Transformer Deep\nNeural Networks, examining Generalized Repeat Accumulate Codes,\nSpatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful\nnature of our topology-aware hardware-efficient quasi-cycle codes equilibrium\nmethod is showcased across diverse scientific domains without the use of\nspecific section delineations.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "16 pages, 29 figures. arXiv admin note: text overlap with\n  arXiv:2307.15778",
    "pdf_url": "http://arxiv.org/pdf/2401.14749v1",
    "published_date": "2024-01-26 10:14:10 UTC",
    "updated_date": "2024-01-26 10:14:10 UTC"
  },
  {
    "arxiv_id": "2401.14743v1",
    "title": "Synthetic Multimodal Dataset for Empowering Safety and Well-being in Home Environments",
    "authors": [
      "Takanori Ugai",
      "Shusaku Egami",
      "Swe Nwe Nwe Htun",
      "Kouji Kozaki",
      "Takahiro Kawamura",
      "Ken Fukuda"
    ],
    "abstract": "This paper presents a synthetic multimodal dataset of daily activities that\nfuses video data from a 3D virtual space simulator with knowledge graphs\ndepicting the spatiotemporal context of the activities. The dataset is\ndeveloped for the Knowledge Graph Reasoning Challenge for Social Issues\n(KGRC4SI), which focuses on identifying and addressing hazardous situations in\nthe home environment. The dataset is available to the public as a valuable\nresource for researchers and practitioners developing innovative solutions\nrecognizing human behaviors to enhance safety and well-being in",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 2 figures,4 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.14743v1",
    "published_date": "2024-01-26 10:05:41 UTC",
    "updated_date": "2024-01-26 10:05:41 UTC"
  },
  {
    "arxiv_id": "2401.15123v1",
    "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
    "authors": [
      "Chen Liu",
      "Shibo He",
      "Qihang Zhou",
      "Shizhong Li",
      "Wenchao Meng"
    ],
    "abstract": "Self-supervised methods have gained prominence in time series anomaly\ndetection due to the scarcity of available annotations. Nevertheless, they\ntypically demand extensive training data to acquire a generalizable\nrepresentation map, which conflicts with scenarios of a few available samples,\nthereby limiting their performance. To overcome the limitation, we propose\n\\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly\ndetection approach where the student network is trained to mimic the features\nof the large language model (LLM)-based teacher network that is pretrained on\nlarge-scale datasets. During the testing phase, anomalies are detected when the\ndiscrepancy between the features of the teacher and student networks is large.\nTo circumvent the student network from learning the teacher network's feature\nof anomalous samples, we devise two key strategies. 1) Prototypical signals are\nincorporated into the student network to consolidate the normal feature\nextraction. 2) We use synthetic anomalies to enlarge the representation gap\nbetween the two networks. AnomalyLLM demonstrates state-of-the-art performance\non 15 datasets, improving accuracy by at least 14.5\\% in the UCR dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.15123v1",
    "published_date": "2024-01-26 09:51:07 UTC",
    "updated_date": "2024-01-26 09:51:07 UTC"
  },
  {
    "arxiv_id": "2401.15122v3",
    "title": "A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics",
    "authors": [
      "Shengchao Liu",
      "Weitao Du",
      "Hannan Xu",
      "Yanjing Li",
      "Zhuoxinran Li",
      "Vignesh Bhethanabotla",
      "Divin Yan",
      "Christian Borgs",
      "Anima Anandkumar",
      "Hongyu Guo",
      "Jennifer Chayes"
    ],
    "abstract": "In drug discovery, molecular dynamics (MD) simulation for protein-ligand\nbinding provides a powerful tool for predicting binding affinities, estimating\ntransport properties, and exploring pocket sites. There has been a long history\nof improving the efficiency of MD simulations through better numerical methods\nand, more recently, by utilizing machine learning (ML) methods. Yet, challenges\nremain, such as accurate modeling of extended-timescale simulations. To address\nthis issue, we propose NeuralMD, the first ML surrogate that can facilitate\nnumerical MD and provide accurate simulations in protein-ligand binding\ndynamics. We propose a principled approach that incorporates a novel\nphysics-informed multi-grained group symmetric framework. Specifically, we\npropose (1) the BindingNet model that satisfies group symmetry using vector\nframes and captures the multi-level protein-ligand interactions, and (2) an\naugmented neural differential equation solver that learns the trajectory under\nNewtonian mechanics. For the experiment, we design ten single-trajectory and\nthree multi-trajectory binding simulation tasks. We demonstrate the efficiency\nand effectiveness of NeuralMD, achieving over 1K$\\times$ speedup compared to\nstandard numerical MD simulations. NeuralMD also outperforms all other ML\napproaches, achieving up to 15$\\times$ reduction in reconstruction error and\n70% increase in validity. Additionally, we qualitatively illustrate that the\noscillations in the predicted trajectories align more closely with ground-truth\ndynamics than those of other machine-learning methods. We believe NeuralMD\npaves the foundation for a new research paradigm in simulating protein-ligand\ndynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15122v3",
    "published_date": "2024-01-26 09:35:17 UTC",
    "updated_date": "2024-11-26 18:13:02 UTC"
  },
  {
    "arxiv_id": "2401.14717v1",
    "title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion",
    "authors": [
      "Jinhan Wang",
      "Long Chen",
      "Aparna Khare",
      "Anirudh Raju",
      "Pranav Dheram",
      "Di He",
      "Minhua Wu",
      "Andreas Stolcke",
      "Venkatesh Ravichandran"
    ],
    "abstract": "We propose an approach for continuous prediction of turn-taking and\nbackchanneling locations in spoken dialogue by fusing a neural acoustic model\nwith a large language model (LLM). Experiments on the Switchboard human-human\nconversation dataset demonstrate that our approach consistently outperforms the\nbaseline models with single modality. We also develop a novel multi-task\ninstruction fine-tuning strategy to further benefit from LLM-encoded knowledge\nfor understanding the tasks and conversational contexts, leading to additional\nimprovements. Our approach demonstrates the potential of combined LLMs and\nacoustic models for a more natural and conversational interaction between\nhumans and speech-enabled AI agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in IEEE ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14717v1",
    "published_date": "2024-01-26 08:59:07 UTC",
    "updated_date": "2024-01-26 08:59:07 UTC"
  },
  {
    "arxiv_id": "2401.14707v2",
    "title": "AFD: Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement",
    "authors": [
      "Nuoyan Zhou",
      "Dawei Zhou",
      "Decheng Liu",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "abstract": "Adversarial fine-tuning methods enhance adversarial robustness via\nfine-tuning the pre-trained model in an adversarial training manner. However,\nwe identify that some specific latent features of adversarial samples are\nconfused by adversarial perturbation and lead to an unexpectedly increasing gap\nbetween features in the last hidden layer of natural and adversarial samples.\nTo address this issue, we propose a disentanglement-based approach to\nexplicitly model and further remove the specific latent features. We introduce\na feature disentangler to separate out the specific latent features from the\nfeatures of the adversarial samples, thereby boosting robustness by eliminating\nthe specific latent features. Besides, we align clean features in the\npre-trained model with features of adversarial samples in the fine-tuned model,\nto benefit from the intrinsic features of natural samples. Empirical\nevaluations on three benchmark datasets demonstrate that our approach surpasses\nexisting adversarial fine-tuning methods and adversarial training baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14707v2",
    "published_date": "2024-01-26 08:38:57 UTC",
    "updated_date": "2024-12-10 16:28:07 UTC"
  },
  {
    "arxiv_id": "2402.01717v1",
    "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
    "authors": [
      "Jaewoong Kim",
      "Moohong Min"
    ],
    "abstract": "Regulatory compliance in the pharmaceutical industry entails navigating\nthrough complex and voluminous guidelines, often requiring significant human\nresources. To address these challenges, our study introduces a chatbot model\nthat utilizes generative AI and the Retrieval Augmented Generation (RAG)\nmethod. This chatbot is designed to search for guideline documents relevant to\nthe user inquiries and provide answers based on the retrieved guidelines.\nRecognizing the inherent need for high reliability in this domain, we propose\nthe Question and Answer Retrieval Augmented Generation (QA-RAG) model. In\ncomparative experiments, the QA-RAG model demonstrated a significant\nimprovement in accuracy, outperforming all other baselines including\nconventional RAG methods. This paper details QA-RAG's structure and performance\nevaluation, emphasizing its potential for the regulatory compliance domain in\nthe pharmaceutical industry and beyond. We have made our work publicly\navailable for further research and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "I.2.7; I.2.1; J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "Total number of pages: 9. Total number of figures: 2. For the source\n  code and experimental results of this paper, see\n  https://github.com/jwoongkim11/QA-RAG. For the dataset used in training and\n  evaluating the model, see https://huggingface.co/datasets/Jaymax/FDA\n  Pharmaceuticals FAQ",
    "pdf_url": "http://arxiv.org/pdf/2402.01717v1",
    "published_date": "2024-01-26 08:23:29 UTC",
    "updated_date": "2024-01-26 08:23:29 UTC"
  },
  {
    "arxiv_id": "2401.14702v1",
    "title": "FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently",
    "authors": [
      "Zicun Cong",
      "Shi Baoxu",
      "Shan Li",
      "Jaewon Yang",
      "Qi He",
      "Jian Pei"
    ],
    "abstract": "Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and\nmore important concern as GCNs are adopted in many crucial applications.\nSocietal biases against sensitive groups may exist in many real world graphs.\nGCNs trained on those graphs may be vulnerable to being affected by such\nbiases. In this paper, we adopt the well-known fairness notion of demographic\nparity and tackle the challenge of training fair and accurate GCNs efficiently.\nWe present an in-depth analysis on how graph structure bias, node attribute\nbias, and model parameters may affect the demographic parity of GCNs. Our\ninsights lead to FairSample, a framework that jointly mitigates the three types\nof biases. We employ two intuitive strategies to rectify graph structures.\nFirst, we inject edges across nodes that are in different sensitive groups but\nsimilar in node features. Second, to enhance model fairness and retain model\nquality, we develop a learnable neighbor sampling policy using reinforcement\nlearning. To address the bias in node features and model parameters, FairSample\nis complemented by a regularization objective to optimize fairness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by TKDE 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.14702v1",
    "published_date": "2024-01-26 08:17:12 UTC",
    "updated_date": "2024-01-26 08:17:12 UTC"
  },
  {
    "arxiv_id": "2402.04267v1",
    "title": "Application analysis of ai technology combined with spiral CT scanning in early lung cancer screening",
    "authors": [
      "Shulin Li",
      "Liqiang Yu",
      "Bo Liu",
      "Qunwei Lin",
      "Jiaxin Huang"
    ],
    "abstract": "At present, the incidence and fatality rate of lung cancer in China rank\nfirst among all malignant tumors. Despite the continuous development and\nimprovement of China's medical level, the overall 5-year survival rate of lung\ncancer patients is still lower than 20% and is staged. A number of studies have\nconfirmed that early diagnosis and treatment of early stage lung cancer is of\ngreat significance to improve the prognosis of patients. In recent years,\nartificial intelligence technology has gradually begun to be applied in\noncology. ai is used in cancer screening, clinical diagnosis, radiation therapy\n(image acquisition, at-risk organ segmentation, image calibration and delivery)\nand other aspects of rapid development. However, whether medical ai can be\nsocialized depends on the public's attitude and acceptance to a certain extent.\nHowever, at present, there are few studies on the diagnosis of early lung\ncancer by AI technology combined with SCT scanning. In view of this, this study\napplied the combined method in early lung cancer screening, aiming to find a\nsafe and efficient screening mode and provide a reference for clinical\ndiagnosis and treatment.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "physics.med-ph",
    "comment": "This article was accepted by Frontiers in Computing and Intelligent\n  Systems https://drpress.org/ojs/index.php/fcis/article/view/15781. arXiv\n  admin note: text overlap with arXiv:nlin/0508031 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2402.04267v1",
    "published_date": "2024-01-26 07:58:09 UTC",
    "updated_date": "2024-01-26 07:58:09 UTC"
  },
  {
    "arxiv_id": "2401.14698v2",
    "title": "Under the Surface: Tracking the Artifactuality of LLM-Generated Data",
    "authors": [
      "Debarati Das",
      "Karin De Langis",
      "Anna Martin-Boyle",
      "Jaehyung Kim",
      "Minhwa Lee",
      "Zae Myung Kim",
      "Shirley Anugrah Hayati",
      "Risako Owan",
      "Bin Hu",
      "Ritik Parkar",
      "Ryan Koo",
      "Jonginn Park",
      "Aahan Tyagi",
      "Libby Ferland",
      "Sanjali Roy",
      "Vincent Liu",
      "Dongyeop Kang"
    ],
    "abstract": "This work delves into the expanding role of large language models (LLMs) in\ngenerating artificial data. LLMs are increasingly employed to create a variety\nof outputs, including annotations, preferences, instruction prompts, simulated\ndialogues, and free text. As these forms of LLM-generated data often intersect\nin their application, they exert mutual influence on each other and raise\nsignificant concerns about the quality and diversity of the artificial data\nincorporated into training cycles, leading to an artificial data ecosystem. To\nthe best of our knowledge, this is the first study to aggregate various types\nof LLM-generated text data, from more tightly constrained data like \"task\nlabels\" to more lightly constrained \"free-form text\". We then stress test the\nquality and implications of LLM-generated artificial data, comparing it with\nhuman data across various existing benchmarks. Despite artificial data's\ncapability to match human performance, this paper reveals significant hidden\ndisparities, especially in complex tasks where LLMs often miss the nuanced\nunderstanding of intrinsic human-generated content. This study critically\nexamines diverse LLM-generated data and emphasizes the need for ethical\npractices in data creation and when using LLMs. It highlights the LLMs'\nshortcomings in replicating human traits and behaviors, underscoring the\nimportance of addressing biases and artifacts produced in LLM-generated content\nfor future research and development. All data and code are available on our\nproject page.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Core Authors: Debarati Das, Karin De Langis, Anna Martin-Boyle,\n  Jaehyung Kim, Minhwa Lee and Zae Myung Kim | Project lead : Debarati Das | PI\n  : Dongyeop Kang",
    "pdf_url": "http://arxiv.org/pdf/2401.14698v2",
    "published_date": "2024-01-26 07:53:27 UTC",
    "updated_date": "2024-01-30 05:36:06 UTC"
  },
  {
    "arxiv_id": "2401.14696v1",
    "title": "Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening",
    "authors": [
      "Hoyong Kim",
      "Semi Lee",
      "Kangil Kim"
    ],
    "abstract": "In the feature space, the collapse between features invokes critical problems\nin representation learning by remaining the features undistinguished.\nInterpolation-based augmentation methods such as mixup have shown their\neffectiveness in relieving the collapse problem between different classes,\ncalled inter-class collapse. However, intra-class collapse raised in\ncoarse-to-fine transfer learning has not been discussed in the augmentation\napproach. To address them, we propose a better feature augmentation method,\nasymptotic midpoint mixup. The method generates augmented features by\ninterpolation but gradually moves them toward the midpoint of inter-class\nfeature pairs. As a result, the method induces two effects: 1) balancing the\nmargin for all classes and 2) only moderately broadening the margin until it\nholds maximal confidence. We empirically analyze the collapse effects by\nmeasuring alignment and uniformity with visualizing representations. Then, we\nvalidate the intra-class collapse effects in coarse-to-fine transfer learning\nand the inter-class collapse effects in imbalanced learning on long-tailed\ndatasets. In both tasks, our method shows better performance than other\naugmentation methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14696v1",
    "published_date": "2024-01-26 07:36:57 UTC",
    "updated_date": "2024-01-26 07:36:57 UTC"
  },
  {
    "arxiv_id": "2401.14694v3",
    "title": "TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records",
    "authors": [
      "Mohammad Al Olaimat",
      "Serdar Bozdag"
    ],
    "abstract": "Motivation: Electronic Health Records (EHR) represent a comprehensive\nresource of a patient's medical history. EHR are essential for utilizing\nadvanced technologies such as deep learning (DL), enabling healthcare providers\nto analyze extensive data, extract valuable insights, and make precise and\ndata-driven clinical decisions. DL methods such as Recurrent Neural Networks\n(RNN) have been utilized to analyze EHR to model disease progression and\npredict diagnosis. However, these methods do not address some inherent\nirregularities in EHR data such as irregular time intervals between clinical\nvisits. Furthermore, most DL models are not interpretable. In this study, we\npropose two interpretable DL architectures based on RNN, namely Time-Aware RNN\n(TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical\noutcome in EHR at next visit and multiple visits ahead, respectively. To\nmitigate the impact of irregular time intervals, we propose incorporating time\nembedding of the elapsed times between visits. For interpretability, we propose\nemploying a dual-level attention mechanism that operates between visits and\nfeatures within each visit.\n  Results: The results of the experiments conducted on Alzheimer's Disease\nNeuroimaging Initiative (ADNI) and National Alzheimer's Coordinating Center\n(NACC) datasets indicated superior performance of proposed models for\npredicting Alzheimer's Disease (AD) compared to state-of-the-art and baseline\napproaches based on F2 and sensitivity. Additionally, TA-RNN showed superior\nperformance on Medical Information Mart for Intensive Care (MIMIC-III) dataset\nfor mortality prediction. In our ablation study, we observed enhanced\npredictive performance by incorporating time embedding and attention\nmechanisms. Finally, investigating attention weights helped identify\ninfluential visits and features in predictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14694v3",
    "published_date": "2024-01-26 07:34:53 UTC",
    "updated_date": "2024-04-03 23:57:52 UTC"
  },
  {
    "arxiv_id": "2401.14665v1",
    "title": "PepGB: Facilitating peptide drug discovery via graph neural networks",
    "authors": [
      "Yipin Lei",
      "Xu Wang",
      "Meng Fang",
      "Han Li",
      "Xiang Li",
      "Jianyang Zeng"
    ],
    "abstract": "Peptides offer great biomedical potential and serve as promising drug\ncandidates. Currently, the majority of approved peptide drugs are directly\nderived from well-explored natural human peptides. It is quite necessary to\nutilize advanced deep learning techniques to identify novel peptide drugs in\nthe vast, unexplored biochemical space. Despite various in silico methods\nhaving been developed to accelerate peptide early drug discovery, existing\nmodels face challenges of overfitting and lacking generalizability due to the\nlimited size, imbalanced distribution and inconsistent quality of experimental\ndata. In this study, we propose PepGB, a deep learning framework to facilitate\npeptide early drug discovery by predicting peptide-protein interactions\n(PepPIs). Employing graph neural networks, PepGB incorporates a fine-grained\nperturbation module and a dual-view objective with contrastive learning-based\npeptide pre-trained representation to predict PepPIs. Through rigorous\nevaluations, we demonstrated that PepGB greatly outperforms baselines and can\naccurately identify PepPIs for novel targets and peptide hits, thereby\ncontributing to the target identification and hit discovery processes. Next, we\nderive an extended version, diPepGB, to tackle the bottleneck of modeling\nhighly imbalanced data prevalent in lead generation and optimization processes.\nUtilizing directed edges to represent relative binding strength between two\npeptide nodes, diPepGB achieves superior performance in real-world assays. In\nsummary, our proposed frameworks can serve as potent tools to facilitate\npeptide early drug discovery.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14665v1",
    "published_date": "2024-01-26 06:13:09 UTC",
    "updated_date": "2024-01-26 06:13:09 UTC"
  },
  {
    "arxiv_id": "2401.15121v2",
    "title": "Expressive Power of ReLU and Step Networks under Floating-Point Operations",
    "authors": [
      "Yeachan Park",
      "Geonho Hwang",
      "Wonyeol Lee",
      "Sejun Park"
    ],
    "abstract": "The study of the expressive power of neural networks has investigated the\nfundamental limits of neural networks. Most existing results assume real-valued\ninputs and parameters as well as exact operations during the evaluation of\nneural networks. However, neural networks are typically executed on computers\nthat can only represent a tiny subset of the reals and apply inexact\noperations, i.e., most existing results do not apply to neural networks used in\npractice. In this work, we analyze the expressive power of neural networks\nunder a more realistic setup: when we use floating-point numbers and operations\nas in practice. Our first set of results assumes floating-point operations\nwhere the significand of a float is represented by finite bits but its exponent\ncan take any integer value. Under this setup, we show that neural networks\nusing a binary threshold unit or ReLU can memorize any finite input/output\npairs and can approximate any continuous function within an arbitrary error. In\nparticular, the number of parameters in our constructions for universal\napproximation and memorization coincides with that in classical results\nassuming exact mathematical operations. We also show similar results on\nmemorization and universal approximation when floating-point operations use\nfinite bits for both significand and exponent; these results are applicable to\nmany popular floating-point formats such as those defined in the IEEE 754\nstandard (e.g., 32-bit single-precision format) and bfloat16.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15121v2",
    "published_date": "2024-01-26 05:59:40 UTC",
    "updated_date": "2024-07-16 01:00:31 UTC"
  },
  {
    "arxiv_id": "2401.14636v1",
    "title": "Efficient Constraint Generation for Stochastic Shortest Path Problems",
    "authors": [
      "Johannes Schmalz",
      "Felipe Trevizan"
    ],
    "abstract": "Current methods for solving Stochastic Shortest Path Problems (SSPs) find\nstates' costs-to-go by applying Bellman backups, where state-of-the-art methods\nemploy heuristics to select states to back up and prune. A fundamental\nlimitation of these algorithms is their need to compute the cost-to-go for\nevery applicable action during each state backup, leading to unnecessary\ncomputation for actions identified as sub-optimal. We present new connections\nbetween planning and operations research and, using this framework, we address\nthis issue of unnecessary computation by introducing an efficient version of\nconstraint generation for SSPs. This technique allows algorithms to ignore\nsub-optimal actions and avoid computing their costs-to-go. We also apply our\nnovel technique to iLAO* resulting in a new algorithm, CG-iLAO*. Our\nexperiments show that CG-iLAO* ignores up to 57% of iLAO*'s actions and it\nsolves problems up to 8x and 3x faster than LRTDP and iLAO*.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of AAAI 2024 paper",
    "pdf_url": "http://arxiv.org/pdf/2401.14636v1",
    "published_date": "2024-01-26 04:00:07 UTC",
    "updated_date": "2024-01-26 04:00:07 UTC"
  },
  {
    "arxiv_id": "2401.14630v1",
    "title": "An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models",
    "authors": [
      "Xi Wang",
      "Ruoqing Zhao",
      "Hongliang Dai",
      "Piji Li"
    ],
    "abstract": "Chinese Spelling Check (CSC) is a meaningful task in the area of Natural\nLanguage Processing (NLP) which aims at detecting spelling errors in Chinese\ntexts and then correcting these errors. However, CSC models are based on\npretrained language models, which are trained on a general corpus.\nConsequently, their performance may drop when confronted with downstream tasks\ninvolving domain-specific terms. In this paper, we conduct a thorough\nevaluation about the domain adaption ability of various typical CSC models by\nbuilding three new datasets encompassing rich domain-specific terms from the\nfinancial, medical, and legal domains. Then we conduct empirical investigations\nin the corresponding domain-specific test datasets to ascertain the\ncross-domain adaptation ability of several typical CSC models. We also test the\nperformance of the popular large language model ChatGPT. As shown in our\nexperiments, the performances of the CSC models drop significantly in the new\ndomains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICASSP2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14630v1",
    "published_date": "2024-01-26 03:49:55 UTC",
    "updated_date": "2024-01-26 03:49:55 UTC"
  },
  {
    "arxiv_id": "2401.15120v2",
    "title": "Incorporating simulated spatial context information improves the effectiveness of contrastive learning models",
    "authors": [
      "Lizhen Zhu",
      "James Z. Wang",
      "Wonseuk Lee",
      "Brad Wyble"
    ],
    "abstract": "Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15120v2",
    "published_date": "2024-01-26 03:44:58 UTC",
    "updated_date": "2024-03-27 15:49:52 UTC"
  },
  {
    "arxiv_id": "2402.09433v1",
    "title": "Electrical Behavior Association Mining for Household ShortTerm Energy Consumption Forecasting",
    "authors": [
      "Heyang Yu",
      "Yuxi Sun",
      "Yintao Liu",
      "Guangchao Geng",
      "Quanyuan Jiang"
    ],
    "abstract": "Accurate household short-term energy consumption forecasting (STECF) is\ncrucial for home energy management, but it is technically challenging, due to\nhighly random behaviors of individual residential users. To improve the\naccuracy of STECF on a day-ahead scale, this paper proposes an novel STECF\nmethodology that leverages association mining in electrical behaviors. First, a\nprobabilistic association quantifying and discovering method is proposed to\nmodel the pairwise behaviors association and generate associated clusters.\nThen, a convolutional neural network-gated recurrent unit (CNN-GRU) based\nforecasting is provided to explore the temporal correlation and enhance\naccuracy. The testing results demonstrate that this methodology yields a\nsignificant enhancement in the STECF.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "3 figures and 4 tables; This manuscript is submitted for possible\n  publication",
    "pdf_url": "http://arxiv.org/pdf/2402.09433v1",
    "published_date": "2024-01-26 03:23:09 UTC",
    "updated_date": "2024-01-26 03:23:09 UTC"
  },
  {
    "arxiv_id": "2401.14617v2",
    "title": "A Systematic Literature Review on Explainability for Machine/Deep Learning-based Software Engineering Research",
    "authors": [
      "Sicong Cao",
      "Xiaobing Sun",
      "Ratnadira Widyasari",
      "David Lo",
      "Xiaoxue Wu",
      "Lili Bo",
      "Jiale Zhang",
      "Bin Li",
      "Wei Liu",
      "Di Wu",
      "Yixin Chen"
    ],
    "abstract": "The remarkable achievements of Artificial Intelligence (AI) algorithms,\nparticularly in Machine Learning (ML) and Deep Learning (DL), have fueled their\nextensive deployment across multiple sectors, including Software Engineering\n(SE). However, due to their black-box nature, these promising AI-driven SE\nmodels are still far from being deployed in practice. This lack of\nexplainability poses unwanted risks for their applications in critical tasks,\nsuch as vulnerability detection, where decision-making transparency is of\nparamount importance. This paper endeavors to elucidate this interdisciplinary\ndomain by presenting a systematic literature review of approaches that aim to\nimprove the explainability of AI models within the context of SE. The review\ncanvasses work appearing in the most prominent SE & AI conferences and\njournals, and spans 108 papers across 23 unique SE tasks. Based on three key\nResearch Questions (RQs), we aim to (1) summarize the SE tasks where XAI\ntechniques have shown success to date; (2) classify and analyze different XAI\ntechniques; and (3) investigate existing evaluation approaches. Based on our\nfindings, we identified a set of challenges remaining to be addressed in\nexisting studies, together with a set of guidelines highlighting potential\nopportunities we deemed appropriate and important for future work.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under Review in ACM Computing Surveys (Major Revision)",
    "pdf_url": "http://arxiv.org/pdf/2401.14617v2",
    "published_date": "2024-01-26 03:20:40 UTC",
    "updated_date": "2025-02-05 16:10:05 UTC"
  },
  {
    "arxiv_id": "2401.14616v1",
    "title": "Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse",
    "authors": [
      "Seungyoon Lee",
      "Dahyun Jung",
      "Chanjun Park",
      "Seolhwa Lee",
      "Heuiseok Lim"
    ],
    "abstract": "We introduce the concept of \"Alternative Speech\" as a new way to directly\ncombat hate speech and complement the limitations of counter-narrative. An\nalternative speech provides practical alternatives to hate speech in real-world\nscenarios by offering speech-level corrections to speakers while considering\nthe surrounding context and promoting speakers to reform. Further, an\nalternative speech can combat hate speech alongside counter-narratives,\noffering a useful tool to address social issues such as racial discrimination\nand gender inequality. We propose the new concept and provide detailed\nguidelines for constructing the necessary dataset. Through discussion, we\ndemonstrate that combining alternative speech and counter-narrative can be a\nmore effective strategy for combating hate speech by complementing specificity\nand guiding capacity of counter-narrative. This paper presents another\nperspective for dealing with hate speech, offering viable remedies to\ncomplement the constraints of current approaches to mitigating harmful bias.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for The First Workshop on Data-Centric AI (DCAI) at ICDM\n  2023",
    "pdf_url": "http://arxiv.org/pdf/2401.14616v1",
    "published_date": "2024-01-26 03:16:54 UTC",
    "updated_date": "2024-01-26 03:16:54 UTC"
  },
  {
    "arxiv_id": "2401.15119v1",
    "title": "Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections",
    "authors": [
      "Md Khairul Islam",
      "Tyler Valentine",
      "Timothy Joowon Sue",
      "Ayush Karmacharya",
      "Luke Neil Benham",
      "Zhengguang Wang",
      "Kingsley Kim",
      "Judy Fox"
    ],
    "abstract": "Interpreting deep learning time series models is crucial in understanding the\nmodel's behavior and learning patterns from raw data for real-time\ndecision-making. However, the complexity inherent in transformer-based time\nseries models poses challenges in explaining the impact of individual features\non predictions. In this study, we leverage recent local interpretation methods\nto interpret state-of-the-art time series models. To use real-world datasets,\nwe collected three years of daily case data for 3,142 US counties. Firstly, we\ncompare six transformer-based models and choose the best prediction model for\nCOVID-19 infection. Using 13 input features from the last two weeks, we can\npredict the cases for the next two weeks. Secondly, we present an innovative\nway to evaluate the prediction sensitivity to 8 population age groups over\nhighly dynamic multivariate infection data. Thirdly, we compare our proposed\nperturbation-based interpretation method with related work, including a total\nof eight local interpretation methods. Finally, we apply our framework to\ntraffic and electricity datasets, demonstrating that our approach is generic\nand can be applied to other time-series domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.PE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15119v1",
    "published_date": "2024-01-26 02:58:59 UTC",
    "updated_date": "2024-01-26 02:58:59 UTC"
  },
  {
    "arxiv_id": "2401.15118v2",
    "title": "GeoDecoder: Empowering Multimodal Map Understanding",
    "authors": [
      "Feng Qi",
      "Mian Dai",
      "Zixian Zheng",
      "Chao Wang"
    ],
    "abstract": "This paper presents GeoDecoder, a dedicated multimodal model designed for\nprocessing geospatial information in maps. Built on the BeitGPT architecture,\nGeoDecoder incorporates specialized expert modules for image and text\nprocessing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying\nbase map, which inherently encompasses essential details about road and\nbuilding shapes, relative positions, and other attributes. Through the\nutilization of rendering techniques, the model seamlessly integrates external\ndata and features such as symbol markers, drive trajectories, heatmaps, and\nuser-defined markers, eliminating the need for extra feature engineering. The\ntext module of GeoDecoder accepts various context texts and question prompts,\ngenerating text outputs in the style of GPT. Furthermore, the GPT-based model\nallows for the training and execution of multiple tasks within the same model\nin an end-to-end manner. To enhance map cognition and enable GeoDecoder to\nacquire knowledge about the distribution of geographic entities in Beijing, we\ndevised eight fundamental geospatial tasks and conducted pretraining of the\nmodel using large-scale text-image samples. Subsequently, rapid fine-tuning was\nperformed on three downstream tasks, resulting in significant performance\nimprovements. The GeoDecoder model demonstrates a comprehensive understanding\nof map elements and their associated operations, enabling efficient and\nhigh-quality application of diverse geospatial tasks in different business\nscenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15118v2",
    "published_date": "2024-01-26 02:39:40 UTC",
    "updated_date": "2024-02-18 23:44:05 UTC"
  },
  {
    "arxiv_id": "2401.14589v2",
    "title": "Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias",
    "authors": [
      "Yu He Ke",
      "Rui Yang",
      "Sui An Lie",
      "Taylor Xin Yi Lim",
      "Hairil Rizal Abdullah",
      "Daniel Shu Wei Ting",
      "Nan Liu"
    ],
    "abstract": "Background: Cognitive biases in clinical decision-making significantly\ncontribute to errors in diagnosis and suboptimal patient outcomes. Addressing\nthese biases presents a formidable challenge in the medical field.\n  Objective: This study explores the role of large language models (LLMs) in\nmitigating these biases through the utilization of a multi-agent framework. We\nsimulate the clinical decision-making processes through multi-agent\nconversation and evaluate its efficacy in improving diagnostic accuracy.\n  Methods: A total of 16 published and unpublished case reports where cognitive\nbiases have resulted in misdiagnoses were identified from the literature. In\nthe multi-agent framework, we leveraged GPT-4 to facilitate interactions among\nfour simulated agents to replicate clinical team dynamics. Each agent has a\ndistinct role: 1) To make the final diagnosis after considering the\ndiscussions, 2) The devil's advocate and correct confirmation and anchoring\nbias, 3) The tutor and facilitator of the discussion to reduce premature\nclosure bias, and 4) To record and summarize the findings. A total of 80\nsimulations were evaluated for the accuracy of initial diagnosis, top\ndifferential diagnosis and final two differential diagnoses.\n  Results: In a total of 80 responses evaluating both initial and final\ndiagnoses, the initial diagnosis had an accuracy of 0% (0/80), but following\nmulti-agent discussions, the accuracy for the top differential diagnosis\nincreased to 71.3% (57/80), and for the final two differential diagnoses, to\n80.0% (64/80).\n  Conclusions: The framework demonstrated an ability to re-evaluate and correct\nmisconceptions, even in scenarios with misleading initial investigations. The\nLLM-driven multi-agent conversation framework shows promise in enhancing\ndiagnostic accuracy in diagnostically challenging medical scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14589v2",
    "published_date": "2024-01-26 01:35:50 UTC",
    "updated_date": "2024-05-12 05:28:23 UTC"
  },
  {
    "arxiv_id": "2401.14571v2",
    "title": "Driving Towards Inclusion: A Systematic Review of AI-powered Accessibility Enhancements for People with Disability in Autonomous Vehicles",
    "authors": [
      "Ashish Bastola",
      "Hao Wang",
      "Sayed Pedram Haeri Boroujeni",
      "Julian Brinkley",
      "Ata Jahangir Moshayedi",
      "Abolfazl Razi"
    ],
    "abstract": "This paper provides a comprehensive and, to our knowledge, the first review\nof inclusive human-computer interaction (HCI) within autonomous vehicles (AVs)\nand human-driven cars with partial autonomy, emphasizing accessibility and\nuser-centered design principles. We explore the current technologies and HCI\nsystems designed to enhance passenger experience, particularly for individuals\nwith accessibility needs. Key technologies discussed include brain-computer\ninterfaces, anthropomorphic interaction, virtual reality, augmented reality,\nmode adaptation, voice-activated interfaces, haptic feedback, etc. Each\ntechnology is evaluated for its role in creating an inclusive in-vehicle\nenvironment. Furthermore, we highlight recent interface designs by leading\ncompanies and review emerging concepts and prototypes under development or\ntesting, which show significant potential to address diverse accessibility\nrequirements. Safety considerations, ethical concerns, and adoption of AVs are\nother major issues that require thorough investigation. Building on these\nfindings, we propose an end-to-end design framework that addresses\naccessibility requirements across diverse user demographics, including older\nadults and individuals with physical or cognitive impairments. This work\nprovides actionable insights for designers, researchers, and policymakers\naiming to create safer and more comfortable environments in autonomous and\nregular vehicles accessible to all users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14571v2",
    "published_date": "2024-01-26 00:06:08 UTC",
    "updated_date": "2025-01-09 07:16:39 UTC"
  }
]
[
  {
    "arxiv_id": "2510.20075v6",
    "title": "LLMs can hide text in other text of the same length",
    "authors": [
      "Antonio Norelli",
      "Michael Bronstein"
    ],
    "abstract": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, main paper 9 pages. v5 contains an Italian translation of this paper by the author",
    "pdf_url": "https://arxiv.org/pdf/2510.20075v6",
    "published_date": "2025-10-22 23:16:50 UTC",
    "updated_date": "2026-01-16 14:08:21 UTC"
  },
  {
    "arxiv_id": "2510.21855v1",
    "title": "SIGN: Schema-Induced Games for Naming",
    "authors": [
      "Ryan Zhang",
      "Herbert Woisetschläger"
    ],
    "abstract": "Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative coding and distributed planning therefore require reliable, consistent communication, and scalability is a central concern as systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming game that examines how lightweight structure can steer convention formation. We compare schema-induced communication to unconstrained natural language and find faster convergence with up to 5.8x higher agreement. These results suggest that minimal structure can act as a simple control knob for efficient multi-agent coordination, pointing toward broader applications beyond the naming game.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2026 Student Abstract (Oral). Code available ar https://github.com/ryanzhangofficial/schema-induced-games-for-naming",
    "pdf_url": "https://arxiv.org/pdf/2510.21855v1",
    "published_date": "2025-10-22 23:12:06 UTC",
    "updated_date": "2025-10-22 23:12:06 UTC"
  },
  {
    "arxiv_id": "2510.20061v1",
    "title": "Ask What Your Country Can Do For You: Towards a Public Red Teaming Model",
    "authors": [
      "Wm. Matthew Kennedy",
      "Cigdem Patlak",
      "Jayraj Dave",
      "Blake Chambers",
      "Aayush Dhanotiya",
      "Darshini Ramiah",
      "Reva Schwartz",
      "Jack Hagen",
      "Akash Kundu",
      "Mouni Pendharkar",
      "Liam Baisley",
      "Theodora Skeadas",
      "Rumman Chowdhury"
    ],
    "abstract": "AI systems have the potential to produce both benefits and harms, but without rigorous and ongoing adversarial evaluation, AI actors will struggle to assess the breadth and magnitude of the AI risk surface. Researchers from the field of systems design have developed several effective sociotechnical AI evaluation and red teaming techniques targeting bias, hate speech, mis/disinformation, and other documented harm classes. However, as increasingly sophisticated AI systems are released into high-stakes sectors (such as education, healthcare, and intelligence-gathering), our current evaluation and monitoring methods are proving less and less capable of delivering effective oversight.\n  In order to actually deliver responsible AI and to ensure AI's harms are fully understood and its security vulnerabilities mitigated, pioneering new approaches to close this \"responsibility gap\" are now more urgent than ever. In this paper, we propose one such approach, the cooperative public AI red-teaming exercise, and discuss early results of its prior pilot implementations. This approach is intertwined with CAMLIS itself: the first in-person public demonstrator exercise was held in conjunction with CAMLIS 2024. We review the operational design and results of this exercise, the prior National Institute of Standards and Technology (NIST)'s Assessing the Risks and Impacts of AI (ARIA) pilot exercise, and another similar exercise conducted with the Singapore Infocomm Media Development Authority (IMDA). Ultimately, we argue that this approach is both capable of delivering meaningful results and is also scalable to many AI developing jurisdictions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20061v1",
    "published_date": "2025-10-22 22:24:21 UTC",
    "updated_date": "2025-10-22 22:24:21 UTC"
  },
  {
    "arxiv_id": "2510.20040v1",
    "title": "Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning",
    "authors": [
      "Changrui Liu",
      "Shengling Shi",
      "Anil Alan",
      "Ganesh Kumar Venayagamoorthy",
      "Bart De Schutter"
    ],
    "abstract": "Efficient energy management is essential for reliable and sustainable microgrid operation amid increasing renewable integration. This paper proposes an imitation learning-based framework to approximate mixed-integer Economic Model Predictive Control (EMPC) for microgrid energy management. The proposed method trains a neural network to imitate expert EMPC control actions from offline trajectories, enabling fast, real-time decision making without solving optimization problems online. To enhance robustness and generalization, the learning process includes noise injection during training to mitigate distribution shift and explicitly incorporates forecast uncertainty in renewable generation and demand. Simulation results demonstrate that the learned policy achieves economic performance comparable to EMPC while only requiring $10\\%$ of the computation time of optimization-based EMPC in practice.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "Submitted to Engineering Applications of Artificial Intelligence (EAAI) and IFAC WC 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.20040v1",
    "published_date": "2025-10-22 21:39:18 UTC",
    "updated_date": "2025-10-22 21:39:18 UTC"
  },
  {
    "arxiv_id": "2510.20039v1",
    "title": "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions",
    "authors": [
      "Yuyang Jiang",
      "Longjie Guo",
      "Yuchen Wu",
      "Aylin Caliskan",
      "Tanu Mitra",
      "Hua Shen"
    ],
    "abstract": "Large language model (LLM)-powered chatbots are increasingly used for opinion exploration. Prior research examined how LLMs alter user views, yet little work extended beyond one-way influence to address how user input can affect LLM responses and how such bi-directional influence manifests throughout the multi-turn conversations. This study investigates this dynamic through 50 controversial-topic discussions with participants (N=266) across three conditions: static statements, standard chatbot, and personalized chatbot. Results show that human opinions barely shifted, while LLM outputs changed more substantially, narrowing the gap between human and LLM stance. Personalization amplified these shifts in both directions compared to the standard setting. Analysis of multi-turn conversations further revealed that exchanges involving participants' personal stories were most likely to trigger stance changes for both humans and LLMs. Our work highlights the risk of over-alignment in human-LLM interaction and the need for careful design of personalized chatbots to more thoughtfully and stably align with users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "26 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20039v1",
    "published_date": "2025-10-22 21:38:10 UTC",
    "updated_date": "2025-10-22 21:38:10 UTC"
  },
  {
    "arxiv_id": "2510.20028v1",
    "title": "The Temporal Graph of Bitcoin Transactions",
    "authors": [
      "Vahid Jalili"
    ],
    "abstract": "Since its 2009 genesis block, the Bitcoin network has processed >1.08 billion (B) transactions representing >8.72B BTC, offering rich potential for machine learning (ML); yet, its pseudonymity and obscured flow of funds inherent in its UTxO-based design, have rendered this data largely inaccessible for ML research. Addressing this gap, we present an ML-compatible graph modeling the Bitcoin's economic topology by reconstructing the flow of funds. This temporal, heterogeneous graph encompasses complete transaction history up to block 863000, consisting of >2.4B nodes and >39.72B edges. Additionally, we provide custom sampling methods yielding node and edge feature vectors of sampled communities, tools to load and analyze the Bitcoin graph data within specialized graph databases, and ready-to-use database snapshots. This comprehensive dataset and toolkit empower the ML community to tackle Bitcoin's intricate ecosystem at scale, driving progress in applications such as anomaly detection, address classification, market analysis, and large-scale graph ML benchmarking. Dataset and code available at https://github.com/B1AAB/EBA",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20028v1",
    "published_date": "2025-10-22 21:10:46 UTC",
    "updated_date": "2025-10-22 21:10:46 UTC"
  },
  {
    "arxiv_id": "2510.20020v1",
    "title": "Optimized Distortion in Linear Social Choice",
    "authors": [
      "Luise Ge",
      "Gregory Kehne",
      "Yevgeniy Vorobeychik"
    ],
    "abstract": "Social choice theory offers a wealth of approaches for selecting a candidate on behalf of voters based on their reported preference rankings over options. When voters have underlying utilities for these options, however, using preference rankings may lead to suboptimal outcomes vis-à-vis utilitarian social welfare. Distortion is a measure of this suboptimality, and provides a worst-case approach for developing and analyzing voting rules when utilities have minimal structure. However in many settings, such as common paradigms for value alignment, alternatives admit a vector representation, and it is natural to suppose that utilities are parametric functions thereof. We undertake the first study of distortion for linear utility functions. Specifically, we investigate the distortion of linear social choice for deterministic and randomized voting rules. We obtain bounds that depend only on the dimension of the candidate embedding, and are independent of the numbers of candidates or voters. Additionally, we introduce poly-time instance-optimal algorithms for minimizing distortion given a collection of candidates and votes. We empirically evaluate these in two real-world domains: recommendation systems using collaborative filtering embeddings, and opinion surveys utilizing language model embeddings, benchmarking several standard rules against our instance-optimal algorithms.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20020v1",
    "published_date": "2025-10-22 20:42:49 UTC",
    "updated_date": "2025-10-22 20:42:49 UTC"
  },
  {
    "arxiv_id": "2510.20002v2",
    "title": "Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation",
    "authors": [
      "Alexandra Apostolopoulou",
      "Konstantinos Kanaris",
      "Athanasios Koursaris",
      "Dimitris Tsakalidis",
      "George Domalis",
      "Ioannis E. Livieris"
    ],
    "abstract": "The advancement of natural language processing for morphologically rich and moderately-resourced languages like Modern Greek has been hindered by architectural stagnation, data scarcity, and limited context processing capabilities, particularly in specialized domains such as law. In this work, we propose the Greek Embedding Models (GEMs), a new family of transformer-based language models, specifically developed to address these limitations through architectural diversity and enhanced data curation. The proposed family of models are trained on several large-scale, meticulously curated corpora, encompassing both comprehensive general-domain datasets and specialized legal collections, addressing the persistent data scarcity that has impeded Greek language modeling advancement. The proposed quality-based corpus curation methodology incorporates extensive preprocessing pipelines, sophisticated deduplication strategies and targeted repetition of high-quality legal sub-corpora to enhance domain adaptation. The GEMs family comprises both established architectures (RoBERTa and Longformer) and advanced models not previously applied to Greek (ELECTRA, ConvBERT, and ModernBERT), providing comprehensive coverage of modern transformer designs. Additionally, we introduce the first bilingual Greek-English embedding models tailored for cross-lingual legal applications. Comprehensive evaluation across three core natural language understanding benchmarks demonstrates that the proposed GEM-RoBERTa and GEM-ConvBERT achieve statistically significant performance improvements over established state-of-the-art models, with accuracy gains of up to 3.6\\% while conducted statistical analysis using Friedman Aligned-Ranks and Finner post-hoc tests confirms the superiority of our approach across multiple evaluation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The manuscript is submitted to Applied Sciences",
    "pdf_url": "https://arxiv.org/pdf/2510.20002v2",
    "published_date": "2025-10-22 20:06:48 UTC",
    "updated_date": "2025-10-24 11:58:25 UTC"
  },
  {
    "arxiv_id": "2510.20001v1",
    "title": "Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs",
    "authors": [
      "Yunpeng Xiao",
      "Carl Yang",
      "Mark Mai",
      "Xiao Hu",
      "Kai Shu"
    ],
    "abstract": "Large language models (LLMs) show promise for clinical use. They are often evaluated using datasets such as MedQA. However, Many medical datasets, such as MedQA, rely on simplified Question-Answering (Q\\A) that underrepresents real-world clinical decision-making. Based on this, we propose a unifying paradigm that characterizes clinical decision-making tasks along two dimensions: Clinical Backgrounds and Clinical Questions. As the background and questions approach the real clinical environment, the difficulty increases. We summarize the settings of existing datasets and benchmarks along two dimensions. Then we review methods to address clinical decision-making, including training-time and test-time techniques, and summarize when they help. Next, we extend evaluation beyond accuracy to include efficiency, explainability. Finally, we highlight open challenges. Our paradigm clarifies assumptions, standardizes comparisons, and guides the development of clinically meaningful LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20001v1",
    "published_date": "2025-10-22 20:06:10 UTC",
    "updated_date": "2025-10-22 20:06:10 UTC"
  },
  {
    "arxiv_id": "2510.19997v1",
    "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)",
    "authors": [
      "Abraham Itzhak Weinberg"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI) presents transformative opportunities for organizations, yet both midsize organizations and larger enterprises face distinctive adoption challenges. Midsize organizations encounter resource constraints and limited AI expertise, while enterprises struggle with organizational complexity and coordination challenges. Existing technology adoption frameworks, including TAM (Technology Acceptance Model), TOE (Technology Organization Environment), and DOI (Diffusion of Innovations) theory, lack the specificity required for GenAI implementation across these diverse contexts, creating a critical gap in adoption literature. This paper introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises), a conceptual framework addressing the unique needs of both organizational types. FAIGMOE synthesizes technology adoption theory, organizational change management, and innovation diffusion perspectives into four interconnected phases: Strategic Assessment, Planning and Use Case Development, Implementation and Integration, and Operationalization and Optimization. Each phase provides scalable guidance on readiness assessment, strategic alignment, risk governance, technical architecture, and change management adaptable to organizational scale and complexity. The framework incorporates GenAI specific considerations including prompt engineering, model orchestration, and hallucination management that distinguish it from generic technology adoption frameworks. As a perspective contribution, FAIGMOE provides the first comprehensive conceptual framework explicitly addressing GenAI adoption across midsize and enterprise organizations, offering actionable implementation protocols, assessment instruments, and governance templates requiring empirical validation through future research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19997v1",
    "published_date": "2025-10-22 19:55:31 UTC",
    "updated_date": "2025-10-22 19:55:31 UTC"
  },
  {
    "arxiv_id": "2510.19988v1",
    "title": "LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation",
    "authors": [
      "Xin Lian",
      "Kenneth D. Forbus"
    ],
    "abstract": "Despite the broad applicability of large language models (LLMs), their reliance on probabilistic inference makes them vulnerable to errors such as hallucination in generated facts and inconsistent output structure in natural language understanding (NLU) tasks. By contrast, symbolic NLU systems provide interpretable understanding grounded in curated lexicons, semantic resources, and syntactic & semantic interpretation rules. They produce relational representations that can be used for accurate reasoning and planning, as well as incremental debuggable learning. However, symbolic NLU systems tend to be more limited in coverage than LLMs and require scarce knowledge representation and linguistics skills to extend and maintain. This paper explores a hybrid approach that integrates the broad-coverage language processing of LLMs with the symbolic NLU capabilities of producing structured relational representations to hopefully get the best of both approaches. We use LLMs for rephrasing and text simplification, to provide broad coverage, and as a source of information to fill in knowledge gaps more automatically. We use symbolic NLU to produce representations that can be used for reasoning and for incremental learning. We evaluate this approach on the task of extracting and interpreting quantities and causal laws from commonsense science texts, along with symbolic- and LLM-only pipelines. Our results suggest that our hybrid method works significantly better than the symbolic-only pipeline.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19988v1",
    "published_date": "2025-10-22 19:38:20 UTC",
    "updated_date": "2025-10-22 19:38:20 UTC"
  },
  {
    "arxiv_id": "2510.19975v1",
    "title": "Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations",
    "authors": [
      "Shaocong Ma",
      "Heng Huang"
    ],
    "abstract": "In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that such desired perturbations can align directionally with the true gradient, instead of maintaining a fixed length. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $δ$-unbiased random perturbations, extending existing complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19975v1",
    "published_date": "2025-10-22 19:06:39 UTC",
    "updated_date": "2025-10-22 19:06:39 UTC"
  },
  {
    "arxiv_id": "2510.19973v2",
    "title": "A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks",
    "authors": [
      "Hatim Chergui",
      "Farhad Rezazadeh",
      "Merouane Debbah",
      "Christos Verikoukis"
    ],
    "abstract": "The path to higher network autonomy in 6G lies beyond the mere optimization of key performance indicators (KPIs). While KPIs have enabled automation gains under TM Forum Levels 1--3, they remain numerical abstractions that act only as proxies for the real essence of communication networks: seamless connectivity, fairness, adaptability, and resilience. True autonomy requires perceiving and reasoning over the network environment as it is. Such progress can be achieved through \\emph{agentic AI}, where large language model (LLM)-powered agents perceive multimodal telemetry, reason with memory, negotiate across domains, and act via APIs to achieve multi-objective goals. However, deploying such agents introduces the challenge of cognitive biases inherited from human design, which can distort reasoning, negotiation, tool use, and actuation. Between neuroscience and AI, this paper provides a tutorial on a selection of well-known biases, including their taxonomy, definition, mathematical formulation, emergence in telecom systems and the commonly impacted agentic components. The tutorial also presents various mitigation strategies tailored to each type of bias. The article finally provides two practical use-cases, which tackle the emergence, impact and mitigation gain of some famous biases in 6G inter-slice and cross-domain management. In particular, anchor randomization, temporal decay and inflection bonus techniques are introduced to specifically address anchoring, temporal and confirmation biases. This avoids that agents stick to the initial high resource allocation proposal or decisions that are recent and/or confirming a prior hypothesis. By grounding decisions in a richer and fairer set of past experiences, the quality and bravery of the agentic agreements in the second use-case, for instance, are leading to $\\times 5$ lower latency and around $40\\%$ higher energy saving.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "19 pages, 15 figures, 1 table, link to source code available",
    "pdf_url": "https://arxiv.org/pdf/2510.19973v2",
    "published_date": "2025-10-22 19:05:04 UTC",
    "updated_date": "2025-11-04 10:36:29 UTC"
  },
  {
    "arxiv_id": "2510.19967v1",
    "title": "LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation",
    "authors": [
      "Le Ren",
      "Xiangjian Zeng",
      "Qingqiang Wu",
      "Ruoxuan Liang"
    ],
    "abstract": "Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at https://github.com/rle27/LyriCAR.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.19967v1",
    "published_date": "2025-10-22 18:57:20 UTC",
    "updated_date": "2025-10-22 18:57:20 UTC"
  },
  {
    "arxiv_id": "2510.19964v1",
    "title": "AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits",
    "authors": [
      "Nitsa J Herzog",
      "Rejwan Bin Sulaiman",
      "David J Herzog",
      "Rose Fong"
    ],
    "abstract": "The study explores the potential of AI technologies in personalized learning, suggesting the prediction of academic success through leadership personality traits and machine learning modelling. The primary data were obtained from 129 master's students in the Environmental Engineering Department, who underwent five leadership personality tests with 23 characteristics. Students used self-assessment tools that included Personality Insight, Workplace Culture, Motivation at Work, Management Skills, and Emotion Control tests. The test results were combined with the average grade obtained from academic reports. The study employed exploratory data analysis and correlation analysis. Feature selection utilized Pearson correlation coefficients of personality traits. The average grades were separated into three categories: fail, pass, and excellent. The modelling process was performed by tuning seven ML algorithms, such as SVM, LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance was achieved with the RF classifier, which yielded an accuracy of 87.50% for the model incorporating 17 personality trait features and the leadership mark feature, and an accuracy of 85.71% for the model excluding this feature. In this way, the study offers an additional opportunity to identify students' strengths and weaknesses at an early stage of their education process and select the most suitable strategies for personalized learning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 6 figures, research article",
    "pdf_url": "https://arxiv.org/pdf/2510.19964v1",
    "published_date": "2025-10-22 18:47:30 UTC",
    "updated_date": "2025-10-22 18:47:30 UTC"
  },
  {
    "arxiv_id": "2510.19957v1",
    "title": "A new wave of vehicle insurance fraud fueled by generative AI",
    "authors": [
      "Amir Hever",
      "Itai Orr"
    ],
    "abstract": "Generative AI is supercharging insurance fraud by making it easier to falsify accident evidence at scale and in rapid time. Insurance fraud is a pervasive and costly problem, amounting to tens of billions of dollars in losses each year. In the vehicle insurance sector, fraud schemes have traditionally involved staged accidents, exaggerated damage, or forged documents. The rise of generative AI, including deepfake image and video generation, has introduced new methods for committing fraud at scale. Fraudsters can now fabricate highly realistic crash photos, damage evidence, and even fake identities or documents with minimal effort, exploiting AI tools to bolster false insurance claims. Insurers have begun deploying countermeasures such as AI-based deepfake detection software and enhanced verification processes to detect and mitigate these AI-driven scams. However, current mitigation strategies face significant limitations. Detection tools can suffer from false positives and negatives, and sophisticated fraudsters continuously adapt their tactics to evade automated checks. This cat-and-mouse arms race between generative AI and detection technology, combined with resource and cost barriers for insurers, means that combating AI-enabled insurance fraud remains an ongoing challenge. In this white paper, we present UVeye layered solution for vehicle fraud, representing a major leap forward in the ability to detect, mitigate and deter this new wave of fraud.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19957v1",
    "published_date": "2025-10-22 18:31:31 UTC",
    "updated_date": "2025-10-22 18:31:31 UTC"
  },
  {
    "arxiv_id": "2510.19954v3",
    "title": "RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs",
    "authors": [
      "Joe Meyer",
      "Divyansha Lachi",
      "Mahmoud Mohammadi",
      "Roshan Reddy Upendra",
      "Eva L. Dyer",
      "Mark Li",
      "Tom Palczewski"
    ],
    "abstract": "Relational multi-table data is common in domains such as e-commerce, healthcare, and scientific research, and can be naturally represented as heterogeneous temporal graphs with multi-modal node attributes. Existing graph neural networks (GNNs) rely on schema-specific feature encoders, requiring separate modules for each node type and feature column, which hinders scalability and parameter sharing. We introduce RELATE (Relational Encoder for Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature encoder that can be used with any general purpose GNN. RELATE employs shared modality-specific encoders for categorical, numerical, textual, and temporal attributes, followed by a Perceiver-style cross-attention module that aggregates features into a fixed-size, permutation-invariant node representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark, where it achieves performance within 3% of schema-specific encoders while reducing parameter counts by up to 5x. This design supports varying schemas and enables multi-dataset pretraining for general-purpose GNNs, paving the way toward foundation models for relational graph data.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.19954v3",
    "published_date": "2025-10-22 18:27:49 UTC",
    "updated_date": "2025-11-03 18:42:57 UTC"
  },
  {
    "arxiv_id": "2510.19953v1",
    "title": "On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization",
    "authors": [
      "Shaocong Ma",
      "Heng Huang"
    ],
    "abstract": "Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of unbiased gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19953v1",
    "published_date": "2025-10-22 18:25:43 UTC",
    "updated_date": "2025-10-22 18:25:43 UTC"
  },
  {
    "arxiv_id": "2510.19950v2",
    "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets",
    "authors": [
      "Shaocong Ma",
      "Heng Huang"
    ],
    "abstract": "In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19950v2",
    "published_date": "2025-10-22 18:22:25 UTC",
    "updated_date": "2026-01-22 06:31:29 UTC"
  },
  {
    "arxiv_id": "2510.19949v2",
    "title": "Surfer 2: The Next Generation of Cross-Platform Computer Use Agents",
    "authors": [
      "Mathieu Andreux",
      "Märt Bakler",
      "Yanael Barbier",
      "Hamza Benchekroun",
      "Emilien Biré",
      "Antoine Bonnet",
      "Riaz Bordie",
      "Nathan Bout",
      "Matthias Brunel",
      "Aleix Cambray",
      "Pierre-Louis Cedoz",
      "Antoine Chassang",
      "Gautier Cloix",
      "Ethan Connelly",
      "Alexandra Constantinou",
      "Ramzi De Coster",
      "Hubert de la Jonquiere",
      "Aurélien Delfosse",
      "Maxime Delpit",
      "Alexis Deprez",
      "Augustin Derupti",
      "Mathieu Diaz",
      "Shannon D'Souza",
      "Julie Dujardin",
      "Abai Edmund",
      "Michael Eickenberg",
      "Armand Fatalot",
      "Wissem Felissi",
      "Isaac Herring",
      "Xavier Koegler",
      "Erwan Le Jumeau de Kergaradec",
      "Aurélien Lac",
      "Maxime Langevin",
      "Corentin Lauverjat",
      "Antonio Loison",
      "Avshalom Manevich",
      "Axel Moyal",
      "Axel Nguyen Kerbel",
      "Marinela Parovic",
      "Julien Revelle",
      "Guillaume Richard",
      "Mats Richter",
      "Ronan Riochet",
      "María Santos",
      "Romain Savidan",
      "Laurent Sifre",
      "Maxime Theillard",
      "Marc Thibault",
      "Ivan Valentini",
      "Tony Wu",
      "Laura Yie",
      "Kai Yuan",
      "Jevgenij Zubovskij"
    ],
    "abstract": "Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 9 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.19949v2",
    "published_date": "2025-10-22 18:21:52 UTC",
    "updated_date": "2025-10-24 11:52:29 UTC"
  },
  {
    "arxiv_id": "2510.20861v1",
    "title": "Fuzzy numbers revisited: operations on extensional fuzzy numbers",
    "authors": [
      "Krzysztof Siminski"
    ],
    "abstract": "Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to better represent imprecise data. However, operations on fuzzy numbers are not as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension rule is applied to elaborate a result. This can produce two problems: (1) high computational complexity and (2) for some fuzzy sets and some operations the results is not a fuzzy set with the same features (eg. multiplication of two triangular fuzzy sets does not produce a triangular fuzzy set). One more problem is the fuzzy spread -- fuzziness of the result increases with the number of operations. These facts can severely limit the application field of fuzzy numbers. In this paper we would like to revisite this problem with a different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines operations on extensional fuzzy numbers and relational operators (=, >, >=, <, <=) for them. The proposed approach is illustrated with several applicational examples. The C++ implementation is available from a public GitHub repository.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 62 references",
    "pdf_url": "https://arxiv.org/pdf/2510.20861v1",
    "published_date": "2025-10-22 18:11:38 UTC",
    "updated_date": "2025-10-22 18:11:38 UTC"
  },
  {
    "arxiv_id": "2510.19898v2",
    "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills",
    "authors": [
      "Atharv Sonwane",
      "Isadora White",
      "Hyunji Lee",
      "Matheus Pereira",
      "Lucas Caccia",
      "Minseon Kim",
      "Zhengyan Shi",
      "Chinmay Singh",
      "Alessandro Sordoni",
      "Marc-Alexandre Côté",
      "Xingdi Yuan"
    ],
    "abstract": "High quality bugs are key to training the next generation of language model based software engineering (SWE) agents. We introduce a novel method for synthetic generation of difficult and diverse bugs. Our method instructs SWE Agents to introduce a feature into the codebase whereby they may unintentionally break tests, resulting in bugs. Prior approaches often induce an out-of-distribution effect by generating bugs intentionally (e.g. by introducing local perturbation to existing code), which does not reflect realistic development processes. We perform qualitative analysis to demonstrate that our approach for generating bugs more closely reflects the patterns found in human-authored edits. Through extensive experiments, we demonstrate that our bugs provide more efficient training data for supervised fine-tuning, outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k bugs). We train on our newly generated bugs in addition to existing bug datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over three seeds.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19898v2",
    "published_date": "2025-10-22 17:58:56 UTC",
    "updated_date": "2025-10-28 19:10:09 UTC"
  },
  {
    "arxiv_id": "2510.19897v1",
    "title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation",
    "authors": [
      "Jackson Hassell",
      "Dan Zhang",
      "Hannah Kim",
      "Tom Mitchell",
      "Estevam Hruschka"
    ],
    "abstract": "We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.19897v1",
    "published_date": "2025-10-22 17:58:03 UTC",
    "updated_date": "2025-10-22 17:58:03 UTC"
  },
  {
    "arxiv_id": "2510.19818v1",
    "title": "Semantic World Models",
    "authors": [
      "Jacob Berg",
      "Chuning Zhu",
      "Yanda Bao",
      "Ishan Durugkar",
      "Abhishek Gupta"
    ],
    "abstract": "Planning with world models offers a powerful paradigm for robotic control. Conventional approaches train a model to predict future frames conditioned on current frames and actions, which can then be used for planning. However, the objective of predicting future pixels is often at odds with the actual planning objective; strong pixel reconstruction does not always correlate with good planning decisions. This paper posits that instead of reconstructing future frames as pixels, world models only need to predict task-relevant semantic information about the future. For such prediction the paper poses world modeling as a visual question answering problem about semantic information in future frames. This perspective allows world modeling to be approached with the same tools underlying vision language models. Thus vision language models can be trained as \"semantic\" world models through a supervised finetuning process on image-action-text data, enabling planning for decision-making while inheriting many of the generalization and robustness properties from the pretrained vision-language models. The paper demonstrates how such a semantic world model can be used for policy improvement on open-ended robotics tasks, leading to significant generalization improvements over typical paradigms of reconstruction-based action-conditional world modeling. Website available at https://weirdlabuw.github.io/swm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19818v1",
    "published_date": "2025-10-22 17:53:45 UTC",
    "updated_date": "2025-10-22 17:53:45 UTC"
  },
  {
    "arxiv_id": "2510.19895v1",
    "title": "Large Language Model enabled Mathematical Modeling",
    "authors": [
      "Guoyun Zhang"
    ],
    "abstract": "The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19895v1",
    "published_date": "2025-10-22 17:41:42 UTC",
    "updated_date": "2025-10-22 17:41:42 UTC"
  },
  {
    "arxiv_id": "2510.19807v1",
    "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning",
    "authors": [
      "Xichen Zhang",
      "Sitong Wu",
      "Yinghao Zhu",
      "Haoru Tan",
      "Shaozuo Yu",
      "Ziyi He",
      "Jiaya Jia"
    ],
    "abstract": "Reinforcement learning from verifiable rewards has emerged as a powerful technique for enhancing the complex reasoning abilities of Large Language Models (LLMs). However, these methods are fundamentally constrained by the ''learning cliff'' phenomenon: when faced with problems far beyond their current capabilities, models consistently fail, yielding a persistent zero-reward signal. In policy optimization algorithms like GRPO, this collapses the advantage calculation to zero, rendering these difficult problems invisible to the learning gradient and stalling progress. To overcome this, we introduce Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive training framework that strategically provides minimal guidance only when a model's independent learning has plateaued. The framework first diagnoses learning stagnation and then intervenes by injecting tiered in-prompt hints, ranging from abstract concepts to concrete steps, enabling the model to construct a valid solution by itself. Extensive experiments on challenging mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative 44.3% over a vanilla GRPO baseline. This result demonstrates our framework provides a robust and effective methodology for unlocking a model's ability to solve problems previously beyond its reach, a critical step towards extending the frontier of autonomous reasoning in LLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/dvlab-research/Scaf-GRPO",
    "pdf_url": "https://arxiv.org/pdf/2510.19807v1",
    "published_date": "2025-10-22 17:41:30 UTC",
    "updated_date": "2025-10-22 17:41:30 UTC"
  },
  {
    "arxiv_id": "2510.19799v1",
    "title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation",
    "authors": [
      "Ji Ma",
      "Albert Casella"
    ],
    "abstract": "Public and nonprofit organizations often hesitate to adopt AI tools because most models are opaque even though standard approaches typically analyze aggregate patterns rather than offering actionable, case-level guidance. This study tests a practitioner-in-the-loop workflow that pairs transparent decision-tree models with large language models (LLMs) to improve predictive accuracy, interpretability, and the generation of practical insights. Using data from an ongoing college-success program, we build interpretable decision trees to surface key predictors. We then provide each tree's structure to an LLM, enabling it to reproduce case-level predictions grounded in the transparent models. Practitioners participate throughout feature engineering, model design, explanation review, and usability assessment, ensuring that field expertise informs the analysis at every stage. Results show that integrating transparent models, LLMs, and practitioner input yields accurate, trustworthy, and actionable case-level evaluations, offering a viable pathway for responsible AI adoption in the public and nonprofit sectors.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SE",
      "econ.GN"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19799v1",
    "published_date": "2025-10-22 17:35:13 UTC",
    "updated_date": "2025-10-22 17:35:13 UTC"
  },
  {
    "arxiv_id": "2510.19792v1",
    "title": "On Controlled Change: Generative AI's Impact on Professional Authority in Journalism",
    "authors": [
      "Tomás Dodds",
      "Wang Ngai Yeung",
      "Claudia Mellado",
      "Mathias-Felipe de Lima-Santos"
    ],
    "abstract": "Using (generative) artificial intelligence tools and systems in journalism is expected to increase journalists' production rates, transform newsrooms' economic models, and further personalize the audience's news consumption practices. Since its release in 2022, OpenAI's ChatGPT and other large language models have raised the alarms inside news organizations, not only for bringing new challenges to news reporting and fact-checking but also for what these technologies would mean for journalists' professional authority in journalism. This paper examines how journalists in Dutch media manage the integration of AI technologies into their daily routines. Drawing from 13 interviews with editors, journalists, and innovation managers in different news outlets and media companies, we propose the concept of controlled change. as a heuristic to explain how journalists are proactively setting guidelines, experimenting with AI tools, and identifying their limitations and capabilities. Using professional authority as a theoretical framework, we argue that journalists anticipate and integrate AI technologies in a supervised manner and identify three primary mechanisms through which journalists manage this integration: (1) developing adaptive guidelines that align AI use with ethical codes, (2) experimenting with AI technologies to determine their necessity and fit, and (3) critically assessing the capabilities and limitations of AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19792v1",
    "published_date": "2025-10-22 17:27:32 UTC",
    "updated_date": "2025-10-22 17:27:32 UTC"
  },
  {
    "arxiv_id": "2510.19788v3",
    "title": "Benchmarking World-Model Learning",
    "authors": [
      "Archana Warrier",
      "Dat Nguyen",
      "Michelangelo Naim",
      "Moksh Jain",
      "Yichao Liang",
      "Karen Schroeder",
      "Cambridge Yang",
      "Joshua B. Tenenbaum",
      "Sebastian Vollmer",
      "Kevin Ellis",
      "Zenna Tavares"
    ],
    "abstract": "Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended $\\unicode{x2014}$ models should support many different tasks unknown ahead of time $\\unicode{x2014}$ and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template $\\unicode{x2014}$ reward-free exploration, derived tests, and behavior-based scoring $\\unicode{x2014}$ to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19788v3",
    "published_date": "2025-10-22 17:23:18 UTC",
    "updated_date": "2025-12-09 23:04:41 UTC"
  },
  {
    "arxiv_id": "2510.19892v1",
    "title": "Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities",
    "authors": [
      "Nishant Balepur",
      "Dang Nguyen",
      "Dayeon Ki"
    ],
    "abstract": "Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks -- which cannot jointly assess MLM capabilities in a single task -- or rely on human or model pairwise comparisons -- which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a Spotlight paper at the EMNLP 2025 Wordplay Workshop",
    "pdf_url": "https://arxiv.org/pdf/2510.19892v1",
    "published_date": "2025-10-22 17:21:16 UTC",
    "updated_date": "2025-10-22 17:21:16 UTC"
  },
  {
    "arxiv_id": "2510.19779v1",
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
    "authors": [
      "Yuezhou Hu",
      "Jiaxin Guo",
      "Xinyu Feng",
      "Tuo Zhao"
    ],
    "abstract": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at https://github.com/yuezhouhu/adaspec.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19779v1",
    "published_date": "2025-10-22 17:13:00 UTC",
    "updated_date": "2025-10-22 17:13:00 UTC"
  },
  {
    "arxiv_id": "2510.21849v3",
    "title": "TowerVision: Understanding and Improving Multilinguality in Vision-Language Models",
    "authors": [
      "André G. Viveiros",
      "Patrick Fernandes",
      "Saul Santos",
      "Sonal Sannigrahi",
      "Emmanouil Zaranis",
      "Nuno M. Guerreiro",
      "Amin Farajian",
      "Pierre Colombo",
      "Graham Neubig",
      "André F. T. Martins"
    ],
    "abstract": "Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 7 figures, submitted to arXiv October 2025. All models, datasets, and training code will be released at https://huggingface.co/collections/utter-project/towervision",
    "pdf_url": "https://arxiv.org/pdf/2510.21849v3",
    "published_date": "2025-10-22 17:02:48 UTC",
    "updated_date": "2025-11-06 11:09:11 UTC"
  },
  {
    "arxiv_id": "2510.19771v2",
    "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
    "authors": [
      "Gil Pasternak",
      "Dheeraj Rajagopal",
      "Julia White",
      "Dhruv Atreja",
      "Matthew Thomas",
      "George Hurn-Maloney",
      "Ash Lewis"
    ],
    "abstract": "LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative capabilities of each model and analyze mutual failure modes. Our results highlight the current limitations of autonomous action in agentic systems, and expose promising future research directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19771v2",
    "published_date": "2025-10-22 17:00:45 UTC",
    "updated_date": "2025-10-29 20:33:02 UTC"
  },
  {
    "arxiv_id": "2510.19767v1",
    "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration",
    "authors": [
      "Xichen Zhang",
      "Sitong Wu",
      "Haoru Tan",
      "Shaozuo Yu",
      "Yinghao Zhu",
      "Ziyi He",
      "Jiaya Jia"
    ],
    "abstract": "The long chain-of-thought (LongCoT) capability is central to the recent breakthroughs achieved by large language models in complex reasoning tasks. However, the accompanying issue of ''underthinking'', where models exhibit shallow reasoning by frequently switching thoughts without sufficient exploration, limits both performance and token efficiency. To address this problem, we propose a simple yet effective reasoning strategy: the SmartSwitch inference framework. This framework can be easily integrated into any large language model as a plug-and-play solution, continuously monitoring the model's reasoning process to detect underthinking and guide it toward deeper exploration of promising but overlooked thoughts. Specifically, the perception module identifies points where thoughts switch and evaluates the potential of the preceding thought using an off-the-shelf process reward model (PRM). If a high-potential thought is found to be prematurely abandoned, the intervention module interrupts the ongoing inference, backtracks to the point before the switch, and inserts a \"deepening prompt\" to encourage further exploration along that promising path. Extensive experiments on challenging mathematical reasoning benchmarks demonstrate that our method significantly enhances the performance of various large language models of different sizes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/dvlab-research/SmartSwitch",
    "pdf_url": "https://arxiv.org/pdf/2510.19767v1",
    "published_date": "2025-10-22 16:56:01 UTC",
    "updated_date": "2025-10-22 16:56:01 UTC"
  },
  {
    "arxiv_id": "2510.19755v3",
    "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
    "authors": [
      "Jiacheng Liu",
      "Xinyu Wang",
      "Yuqi Lin",
      "Zhikai Wang",
      "Peiru Wang",
      "Peiliang Cai",
      "Qinming Zhou",
      "Zhengan Yan",
      "Zexuan Yan",
      "Zhengyi Shi",
      "Chang Zou",
      "Yue Ma",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.\n  Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages,2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19755v3",
    "published_date": "2025-10-22 16:46:05 UTC",
    "updated_date": "2025-11-01 08:49:20 UTC"
  },
  {
    "arxiv_id": "2510.19889v1",
    "title": "From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem",
    "authors": [
      "Mostafa Ameli",
      "Van Anh Le",
      "Sulthana Shams",
      "Alexander Skabardonis"
    ],
    "abstract": "The traffic assignment problem is essential for traffic flow analysis, traditionally solved using mathematical programs under the Equilibrium principle. These methods become computationally prohibitive for large-scale networks due to non-linear growth in complexity with the number of OD pairs. This study introduces a novel data-driven approach using deep neural networks, specifically leveraging the Transformer architecture, to predict equilibrium path flows directly. By focusing on path-level traffic distribution, the proposed model captures intricate correlations between OD pairs, offering a more detailed and flexible analysis compared to traditional link-level approaches. The Transformer-based model drastically reduces computation time, while adapting to changes in demand and network structure without the need for recalculation. Numerical experiments are conducted on the Manhattan-like synthetic network, the Sioux Falls network, and the Eastern-Massachusetts network. The results demonstrate that the proposed model is orders of magnitude faster than conventional optimization. It efficiently estimates path-level traffic flows in multi-class networks, reducing computational costs and improving prediction accuracy by capturing detailed trip and flow information. The model also adapts flexibly to varying demand and network conditions, supporting traffic management and enabling rapid `what-if' analyses for enhanced transportation planning and policy-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19889v1",
    "published_date": "2025-10-22 16:45:12 UTC",
    "updated_date": "2025-10-22 16:45:12 UTC"
  },
  {
    "arxiv_id": "2510.19752v1",
    "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
    "authors": [
      "Ameesh Shah",
      "William Chen",
      "Adwait Godbole",
      "Federico Mora",
      "Sanjit A. Seshia",
      "Sergey Levine"
    ],
    "abstract": "Solving complex real-world control tasks often takes multiple tries: if we fail at first, we reflect on what went wrong, and change our strategy accordingly to avoid making the same mistake. In robotics, Vision-Language-Action models (VLAs) offer a promising path towards solving complex control tasks, but lack the ability to contextually and dynamically readjust behavior when they fail to accomplish a task. In this work, we introduce Learning from Inference-Time Execution (LITEN), which connects a VLA low-level policy to a high-level VLM that conditions on past experiences by including them in-context, allowing it to learn the affordances and capabilities of the low-level VLA. Our approach iterates between a reasoning phase that generates and executes plans for the low-level VLA, and an assessment phase that reflects on the resulting execution and draws useful conclusions to be included in future reasoning contexts. Unlike similar approaches to self-refinement in non-robotics domains, LITEN must reflect on unstructured real-world robot trajectories (e.g., raw videos), which requires structured guiderails during assessment. Our experimental results demonstrate LITEN is able to effectively learn from past experience to generate plans that use high-affordance instructions to accomplish long-horizon tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages and appendix",
    "pdf_url": "https://arxiv.org/pdf/2510.19752v1",
    "published_date": "2025-10-22 16:43:29 UTC",
    "updated_date": "2025-10-22 16:43:29 UTC"
  },
  {
    "arxiv_id": "2510.19738v2",
    "title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior",
    "authors": [
      "Rustem Turtayev",
      "Natalia Fedorova",
      "Oleg Serikov",
      "Sergey Koldyba",
      "Lev Avagyan",
      "Dmitrii Volkov"
    ],
    "abstract": "Advanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and walks through the nine winning submissions step by step.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Add Limitations section",
    "pdf_url": "https://arxiv.org/pdf/2510.19738v2",
    "published_date": "2025-10-22 16:28:48 UTC",
    "updated_date": "2025-11-05 13:34:49 UTC"
  },
  {
    "arxiv_id": "2510.19732v2",
    "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
    "authors": [
      "Gunshi Gupta",
      "Karmesh Yadav",
      "Zsolt Kira",
      "Yarin Gal",
      "Rahaf Aljundi"
    ],
    "abstract": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Our code is available at: https://github.com/gunshi/memo.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for Spotlight Presentation at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19732v2",
    "published_date": "2025-10-22 16:24:47 UTC",
    "updated_date": "2025-11-27 02:24:31 UTC"
  },
  {
    "arxiv_id": "2510.19728v1",
    "title": "Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series",
    "authors": [
      "Mahmoud Ibrahim",
      "Bart Elen",
      "Chang Sun",
      "Gökhan Ertaylan",
      "Michel Dumontier"
    ],
    "abstract": "We present a novel framework for leveraging synthetic ICU time-series data not only to train but also to rigorously and trustworthily evaluate predictive models, both at the population level and within fine-grained demographic subgroups. Building on prior diffusion and VAE-based generators (TimeDiff, HealthGen, TimeAutoDiff), we introduce \\textit{Enhanced TimeAutoDiff}, which augments the latent diffusion objective with distribution-alignment penalties. We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS gap'') by over 70\\%, achieving $Δ_{TRTS} \\leq 0.014$ AUROC, while preserving training utility ($Δ_{TSTR} \\approx 0.01$). Crucially, for 32 intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC estimation error by up to 50\\% relative to small real test sets, and outperform them in 72--84\\% of subgroups. This work provides a practical, privacy-preserving roadmap for trustworthy, granular model evaluation in critical care, enabling robust and reliable performance analysis across diverse patient populations without exposing sensitive EHR data, contributing to the overall trustworthiness of Medical AI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19728v1",
    "published_date": "2025-10-22 16:17:29 UTC",
    "updated_date": "2025-10-22 16:17:29 UTC"
  },
  {
    "arxiv_id": "2510.21846v1",
    "title": "Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach",
    "authors": [
      "Yongchao Huang",
      "Pengfei Zhang",
      "Shahzad Mumtaz"
    ],
    "abstract": "Membership inference attacks (MIAs) test whether a data point was part of a model's training set, posing serious privacy risks. Existing methods often depend on shadow models or heavy query access, which limits their practicality. We propose GP-MIA, an efficient and interpretable approach based on Gaussian process (GP) meta-modeling. Using post-hoc metrics such as accuracy, entropy, dataset statistics, and optional sensitivity features (e.g. gradients, NTK measures) from a single trained model, GP-MIA trains a GP classifier to distinguish members from non-members while providing calibrated uncertainty estimates. Experiments on synthetic data, real-world fraud detection data, CIFAR-10, and WikiText-2 show that GP-MIA achieves high accuracy and generalizability, offering a practical alternative to existing MIAs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.21846v1",
    "published_date": "2025-10-22 16:10:47 UTC",
    "updated_date": "2025-10-22 16:10:47 UTC"
  },
  {
    "arxiv_id": "2510.19698v1",
    "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models",
    "authors": [
      "Yang Yang",
      "Hua XU",
      "Zhangyi Hu",
      "Yutao Yue"
    ],
    "abstract": "Large Language Models (LLMs) can propose rules in natural language, sidestepping the need for a predefined predicate space in traditional rule learning. Yet many LLM-based approaches ignore interactions among rules, and the opportunity to couple LLMs with probabilistic rule learning for robust inference remains underexplored. We present RLIE, a unified framework that integrates LLMs with probabilistic modeling to learn a set of weighted rules. RLIE has four stages: (1) Rule generation, where an LLM proposes and filters candidates; (2) Logistic regression, which learns probabilistic weights for global selection and calibration; (3) Iterative refinement, which updates the rule set using prediction errors; and (4) Evaluation, which compares the weighted rule set as a direct classifier with methods that inject rules into an LLM. We evaluate multiple inference strategies on real-world datasets. Applying rules directly with their learned weights yields superior performance, whereas prompting LLMs with the rules, weights, and logistic-model outputs surprisingly degrades accuracy. This supports the view that LLMs excel at semantic generation and interpretation but are less reliable for precise probabilistic integration. RLIE clarifies the potential and limitations of LLMs for inductive reasoning and couples them with classic probabilistic rule combination methods to enable more reliable neuro-symbolic reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19698v1",
    "published_date": "2025-10-22 15:50:04 UTC",
    "updated_date": "2025-10-22 15:50:04 UTC"
  },
  {
    "arxiv_id": "2510.19694v1",
    "title": "Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings",
    "authors": [
      "Cesar Gonzalez-Gutierrez",
      "Dirk Hovy"
    ],
    "abstract": "Prompting is a common approach for leveraging LMs in zero-shot settings. However, the underlying mechanisms that enable LMs to perform diverse tasks without task-specific supervision remain poorly understood. Studying the relationship between prompting and the quality of internal representations can shed light on how pre-trained embeddings may support in-context task solving. In this empirical study, we conduct a series of probing experiments on prompt embeddings, analyzing various combinations of prompt templates for zero-shot classification. Our findings show that while prompting affects the quality of representations, these changes do not consistently correlate with the relevance of the prompts to the target task. This result challenges the assumption that more relevant prompts necessarily lead to better representations. We further analyze potential factors that may contribute to this unexpected behavior.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19694v1",
    "published_date": "2025-10-22 15:43:40 UTC",
    "updated_date": "2025-10-22 15:43:40 UTC"
  },
  {
    "arxiv_id": "2510.19692v1",
    "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary",
    "authors": [
      "Rashina Hoda"
    ],
    "abstract": "Agentic AI is poised to usher in a seismic paradigm shift in Software Engineering (SE). As technologists rush head-along to make agentic AI a reality, SE researchers are driven to establish agentic SE as a research area. While early visions of agentic SE are primarily focused on code-related activities, early empirical evidence calls for a consideration of a range of socio-technical concerns to make it work in practice. This paper contributes to the emerging community vision by: (a) recommending an expansion of its scope beyond code, toward a 'whole of process' vision, grounding it in SE foundations and evolution and emerging agentic SE frameworks, (b) proposing a preliminary set of values and principles to guide efforts, and (c) sharing guidance on designing/using well-defined vocabulary for agentic SE. It is hoped that these ideas will encourage community collaborations and steer the SE community towards laying strong foundations of agentic SE so its not only inevitable but also deliberate and desirable in the long run.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.19692v1",
    "published_date": "2025-10-22 15:39:58 UTC",
    "updated_date": "2025-10-22 15:39:58 UTC"
  },
  {
    "arxiv_id": "2510.19689v1",
    "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation",
    "authors": [
      "Guilin Zhang",
      "Wulan Guo",
      "Ziqi Tan",
      "Srinivas Vippagunta",
      "Suchitra Raman",
      "Shreeshankar Chatterjee",
      "Ju Lin",
      "Shang Liu",
      "Mary Schladenhauffen",
      "Jeffrey Luo",
      "Hailong Jiang"
    ],
    "abstract": "Industrial and government organizations increasingly depend on data-driven analytics for workforce, finance, and regulated decision processes, where timeliness, cost efficiency, and compliance are critical. Distributed frameworks such as Spark and Flink remain effective for massive-scale batch or streaming analytics but introduce coordination complexity and auditing overheads that misalign with moderate-scale, latency-sensitive inference. Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet enable interpretable tabular ML, motivating new deployment blueprints for regulated environments. In this paper, we present a production-oriented Big Data as a Service (BDaaS) blueprint that integrates a single-node serverless GPU runtime with TabNet. The design leverages GPU acceleration for throughput, serverless elasticity for cost reduction, and feature-mask interpretability for IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets, comparing our approach against Spark and CPU baselines. Our results show that GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90% lower cost per 1K inferences compared to Spark baselines, while compliance mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains stable under peak load, ensuring reliable auditability. Taken together, these findings provide a compliance-aware benchmark, a reproducible Helm-packaged blueprint, and a decision framework that demonstrate the practicality of secure, interpretable, and cost-efficient serverless GPU analytics for regulated enterprise and government settings.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19689v1",
    "published_date": "2025-10-22 15:37:42 UTC",
    "updated_date": "2025-10-22 15:37:42 UTC"
  },
  {
    "arxiv_id": "2510.19687v1",
    "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
    "authors": [
      "Addison J. Wu",
      "Ryan Liu",
      "Kerem Oktar",
      "Theodore R. Sumers",
      "Thomas L. Griffiths"
    ],
    "abstract": "Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19687v1",
    "published_date": "2025-10-22 15:35:00 UTC",
    "updated_date": "2025-10-22 15:35:00 UTC"
  },
  {
    "arxiv_id": "2510.19685v1",
    "title": "Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes",
    "authors": [
      "Omar Alsaiari",
      "Nilufar Baghaei",
      "Jason M. Lodge",
      "Omid Noroozi",
      "Dragan Gašević",
      "Marie Boden",
      "Hassan Khosravi"
    ],
    "abstract": "Feedback is one of the most powerful influences on student learning, with extensive research examining how best to implement it in educational settings. Increasingly, feedback is being generated by artificial intelligence (AI), offering scalable and adaptive responses. Two widely studied approaches are directive feedback, which gives explicit explanations and reduces cognitive load to speed up learning, and metacognitive feedback which prompts learners to reflect, track their progress, and develop self-regulated learning (SRL) skills. While both approaches have clear theoretical advantages, their comparative effects on engagement, confidence, and quality of work remain underexplored. This study presents a semester-long randomised controlled trial with 329 students in an introductory design and programming course using an adaptive educational platform. Participants were assigned to receive directive, metacognitive, or hybrid AI-generated feedback that blended elements of both directive and metacognitive feedback. Results showed that revision behaviour differed across feedback conditions, with Hybrid prompting the most revisions compared to Directive and Metacognitive. Confidence ratings were uniformly high, and resource quality outcomes were comparable across conditions. These findings highlight the promise of AI in delivering feedback that balances clarity with reflection. Hybrid approaches, in particular, show potential to combine actionable guidance for immediate improvement with opportunities for self-reflection and metacognitive growth.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19685v1",
    "published_date": "2025-10-22 15:31:21 UTC",
    "updated_date": "2025-10-22 15:31:21 UTC"
  },
  {
    "arxiv_id": "2510.19678v1",
    "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
    "authors": [
      "John Burden",
      "Jonathan Prunty",
      "Ben Slater",
      "Matthieu Tehenan",
      "Greg Davis",
      "Lucy Cheke"
    ],
    "abstract": "Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms -- originally developed to study human perception -- to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.19678v1",
    "published_date": "2025-10-22 15:24:07 UTC",
    "updated_date": "2025-10-22 15:24:07 UTC"
  },
  {
    "arxiv_id": "2510.19675v1",
    "title": "Study of Training Dynamics for Memory-Constrained Fine-Tuning",
    "authors": [
      "Aël Quélennec",
      "Nour Hezbri",
      "Pavlo Mozharovskyi",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ],
    "abstract": "Memory-efficient training of deep neural networks has become increasingly important as models grow larger while deployment environments impose strict resource constraints. We propose TraDy, a novel transfer learning scheme leveraging two key insights: layer importance for updates is architecture-dependent and determinable a priori, while dynamic stochastic channel selection provides superior gradient approximation compared to static approaches. We introduce a dynamic channel selection approach that stochastically resamples channels between epochs within preselected layers. Extensive experiments demonstrate TraDy achieves state-of-the-art performance across various downstream tasks and architectures while maintaining strict memory constraints, achieving up to 99% activation sparsity, 95% weight derivative sparsity, and 97% reduction in FLOPs for weight derivative computation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19675v1",
    "published_date": "2025-10-22 15:21:05 UTC",
    "updated_date": "2025-10-22 15:21:05 UTC"
  },
  {
    "arxiv_id": "2510.19671v1",
    "title": "Explainable e-sports win prediction through Machine Learning classification in streaming",
    "authors": [
      "Silvia García-Méndez",
      "Francisco de Arriba-Pérez"
    ],
    "abstract": "The increasing number of spectators and players in e-sports, along with the development of optimized communication solutions and cloud computing technology, has motivated the constant growth of the online game industry. Even though Artificial Intelligence-based solutions for e-sports analytics are traditionally defined as extracting meaningful patterns from related data and visualizing them to enhance decision-making, most of the effort in professional winning prediction has been focused on the classification aspect from a batch perspective, also leaving aside the visualization techniques. Consequently, this work contributes to an explainable win prediction classification solution in streaming in which input data is controlled over several sliding windows to reflect relevant game changes. Experimental results attained an accuracy higher than 90 %, surpassing the performance of competing solutions in the literature. Ultimately, our system can be leveraged by ranking and recommender systems for informed decision-making, thanks to the explainability module, which fosters trust in the outcome predictions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19671v1",
    "published_date": "2025-10-22 15:18:16 UTC",
    "updated_date": "2025-10-22 15:18:16 UTC"
  },
  {
    "arxiv_id": "2510.19668v1",
    "title": "Unraveling Emotions with Pre-Trained Models",
    "authors": [
      "Alejandro Pajón-Sanmartín",
      "Francisco De Arriba-Pérez",
      "Silvia García-Méndez",
      "Fátima Leal",
      "Benedita Malheiro",
      "Juan Carlos Burguillo-Rial"
    ],
    "abstract": "Transformer models have significantly advanced the field of emotion recognition. However, there are still open challenges when exploring open-ended queries for Large Language Models (LLMs). Although current models offer good results, automatic emotion analysis in open texts presents significant challenges, such as contextual ambiguity, linguistic variability, and difficulty interpreting complex emotional expressions. These limitations make the direct application of generalist models difficult. Accordingly, this work compares the effectiveness of fine-tuning and prompt engineering in emotion detection in three distinct scenarios: (i) performance of fine-tuned pre-trained models and general-purpose LLMs using simple prompts; (ii) effectiveness of different emotion prompt designs with LLMs; and (iii) impact of emotion grouping techniques on these models. Experimental tests attain metrics above 70% with a fine-tuned pre-trained model for emotion recognition. Moreover, the findings highlight that LLMs require structured prompt engineering and emotion grouping to enhance their performance. These advancements improve sentiment analysis, human-computer interaction, and understanding of user behavior across various domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19668v1",
    "published_date": "2025-10-22 15:13:52 UTC",
    "updated_date": "2025-10-22 15:13:52 UTC"
  },
  {
    "arxiv_id": "2510.19666v1",
    "title": "A Graph Engine for Guitar Chord-Tone Soloing Education",
    "authors": [
      "Matthew Keating",
      "Michael Casey"
    ],
    "abstract": "We present a graph-based engine for computing chord tone soloing suggestions for guitar students. Chord tone soloing is a fundamental practice for improvising over a chord progression, where the instrumentalist uses only the notes contained in the current chord. This practice is a building block for all advanced jazz guitar theory but is difficult to learn and practice. First, we discuss methods for generating chord-tone arpeggios. Next, we construct a weighted graph where each node represents a chord tone arpeggio for a chord in the progression. Then, we calculate the edge weight between each consecutive chord's nodes in terms of optimal transition tones. We then find the shortest path through this graph and reconstruct a chord-tone soloing line. Finally, we discuss a user-friendly system to handle input and output to this engine for guitar students to practice chord tone soloing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICMC 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19666v1",
    "published_date": "2025-10-22 15:13:16 UTC",
    "updated_date": "2025-10-22 15:13:16 UTC"
  },
  {
    "arxiv_id": "2510.19661v2",
    "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing",
    "authors": [
      "Xusen Guo",
      "Mingxing Peng",
      "Xixuan Hao",
      "Xingchen Zou",
      "Qiongyan Wang",
      "Sijie Ruan",
      "Yuxuan Liang"
    ],
    "abstract": "Web-based participatory urban sensing has emerged as a vital approach for modern urban management by leveraging mobile individuals as distributed sensors. However, existing urban sensing systems struggle with limited generalization across diverse urban scenarios and poor interpretability in decision-making. In this work, we introduce AgentSense, a hybrid, training-free framework that integrates large language models (LLMs) into participatory urban sensing through a multi-agent evolution system. AgentSense initially employs classical planner to generate baseline solutions and then iteratively refines them to adapt sensing task assignments to dynamic urban conditions and heterogeneous worker preferences, while producing natural language explanations that enhance transparency and trust. Extensive experiments across two large-scale mobility datasets and seven types of dynamic disturbances demonstrate that AgentSense offers distinct advantages in adaptivity and explainability over traditional methods. Furthermore, compared to single-agent LLM baselines, our approach outperforms in both performance and robustness, while delivering more reasonable and transparent explanations. These results position AgentSense as a significant advancement towards deploying adaptive and explainable urban sensing systems on the web.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 10 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.19661v2",
    "published_date": "2025-10-22 15:06:26 UTC",
    "updated_date": "2025-10-24 05:16:51 UTC"
  },
  {
    "arxiv_id": "2510.19654v2",
    "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction",
    "authors": [
      "Zhida Zhao",
      "Talas Fu",
      "Yifan Wang",
      "Lijun Wang",
      "Huchuan Lu"
    ],
    "abstract": "Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NuerIPS 2025 (Poster)",
    "pdf_url": "https://arxiv.org/pdf/2510.19654v2",
    "published_date": "2025-10-22 14:57:51 UTC",
    "updated_date": "2025-11-25 03:37:38 UTC"
  },
  {
    "arxiv_id": "2510.19641v1",
    "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent",
    "authors": [
      "Yangshijie Zhang",
      "Xinda Wang",
      "Jialin Liu",
      "Wenqiang Wang",
      "Zhicong Ma",
      "Xingxing Jia"
    ],
    "abstract": "With social media growth, users employ stylistic fonts and font-like emoji to express individuality, creating visually appealing text that remains human-readable. However, these fonts introduce hidden vulnerabilities in NLP models: while humans easily read stylistic text, models process these characters as distinct tokens, causing interference. We identify this human-model perception gap and propose a style-based attack, Style Attack Disguise (SAD). We design two sizes: light for query efficiency and strong for superior attack performance. Experiments on sentiment classification and machine translation across traditional models, LLMs, and commercial services demonstrate SAD's strong attack performance. We also show SAD's potential threats to multimodal tasks including text-to-image and text-to-speech generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19641v1",
    "published_date": "2025-10-22 14:40:24 UTC",
    "updated_date": "2025-10-22 14:40:24 UTC"
  },
  {
    "arxiv_id": "2510.19631v1",
    "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application",
    "authors": [
      "Yiqian Yang",
      "Tian Lan",
      "Qianghuai Jia",
      "Li Zhu",
      "Hui Jiang",
      "Hang Zhu",
      "Longyue Wang",
      "Weihua Luo",
      "Kaifu Zhang"
    ],
    "abstract": "Effective deep search agents must not only access open-domain and domain-specific knowledge but also apply complex rules-such as legal clauses, medical manuals and tariff rules. These rules often feature vague boundaries and implicit logic relationships, making precise application challenging for agents. However, this critical capability is largely overlooked by current agent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level e-commerce benchmark designed to evaluate deep search agents in hierarchical rule application. In this task, the deep reasoning process of agents is guided by these rules to predict 10-digit Harmonized System Code (HSCode) of products with noisy but realistic descriptions. These codes, established by the World Customs Organization, are vital for global supply chain efficiency. Built from real-world data collected from large-scale e-commerce platforms, our proposed HSCodeComp comprises 632 product entries spanning diverse product categories, with these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source, and closed-source agents reveal a huge performance gap: best agent achieves only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides, detailed analysis demonstrates the challenges of hierarchical rule application, and test-time scaling fails to improve performance further.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19631v1",
    "published_date": "2025-10-22 14:28:33 UTC",
    "updated_date": "2025-10-22 14:28:33 UTC"
  },
  {
    "arxiv_id": "2510.19882v1",
    "title": "Quantifying Feature Importance for Online Content Moderation",
    "authors": [
      "Benedetta Tessa",
      "Alejandro Moreo",
      "Stefano Cresci",
      "Tiziano Fagni",
      "Fabrizio Sebastiani"
    ],
    "abstract": "Accurately estimating how users respond to moderation interventions is paramount for developing effective and user-centred moderation strategies. However, this requires a clear understanding of which user characteristics are associated with different behavioural responses, which is the goal of this work. We investigate the informativeness of 753 socio-behavioural, linguistic, relational, and psychological features, in predicting the behavioural changes of 16.8K users affected by a major moderation intervention on Reddit. To reach this goal, we frame the problem in terms of \"quantification\", a task well-suited to estimating shifts in aggregate user behaviour. We then apply a greedy feature selection strategy with the double goal of (i) identifying the features that are most predictive of changes in user activity, toxicity, and participation diversity, and (ii) estimating their importance. Our results allow identifying a small set of features that are consistently informative across all tasks, and determining that many others are either task-specific or of limited utility altogether. We also find that predictive performance varies according to the task, with changes in activity and toxicity being easier to estimate than changes in diversity. Overall, our results pave the way for the development of accurate systems that predict user reactions to moderation interventions. Furthermore, our findings highlight the complexity of post-moderation user behaviour, and indicate that effective moderation should be tailored not only to user traits but also to the specific objective of the intervention.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19882v1",
    "published_date": "2025-10-22 14:02:30 UTC",
    "updated_date": "2025-10-22 14:02:30 UTC"
  },
  {
    "arxiv_id": "2510.19600v1",
    "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
    "authors": [
      "Qianli Ma",
      "Siyu Wang",
      "Yilin Chen",
      "Yinhao Tang",
      "Yixiang Yang",
      "Chang Guo",
      "Bingjie Gao",
      "Zhening Xing",
      "Yanan Sun",
      "Zhipeng Zhang"
    ],
    "abstract": "In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce $\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct $\\textbf{PageBench}$, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\$0.1. Code and dataset will be released at $\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19600v1",
    "published_date": "2025-10-22 13:53:57 UTC",
    "updated_date": "2025-10-22 13:53:57 UTC"
  },
  {
    "arxiv_id": "2510.19599v1",
    "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography",
    "authors": [
      "Haozhe Luo",
      "Shelley Zixin Shu",
      "Ziyu Zhou",
      "Sebastian Otalora",
      "Mauricio Reyes"
    ],
    "abstract": "Vision-language models (VLMs) have recently shown remarkable zero-shot performance in medical image understanding, yet their grounding ability, the extent to which textual concepts align with visual evidence, remains underexplored. In the medical domain, however, reliable grounding is essential for interpretability and clinical adoption. In this work, we present the first systematic benchmark for evaluating cross-modal interpretability in chest X-rays across seven CLIP-style VLM variants. We generate visual explanations using cross-attention and similarity-based localization maps, and quantitatively assess their alignment with radiologist-annotated regions across multiple pathologies. Our analysis reveals that: (1) while all VLM variants demonstrate reasonable localization for large and well-defined pathologies, their performance substantially degrades for small or diffuse lesions; (2) models that are pretrained on chest X-ray-specific datasets exhibit improved alignment compared to those trained on general-domain data. (3) The overall recognition ability and grounding ability of the model are strongly correlated. These findings underscore that current VLMs, despite their strong recognition ability, still fall short in clinically reliable grounding, highlighting the need for targeted interpretability benchmarks before deployment in medical practice. XBench code is available at https://github.com/Roypic/Benchmarkingattention",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19599v1",
    "published_date": "2025-10-22 13:52:19 UTC",
    "updated_date": "2025-10-22 13:52:19 UTC"
  },
  {
    "arxiv_id": "2510.19593v1",
    "title": "A Goal-Driven Survey on Root Cause Analysis",
    "authors": [
      "Aoyang Fang",
      "Haowen Yang",
      "Haoze Dong",
      "Qisheng Lu",
      "Junjielong Xu",
      "Pinjia He"
    ],
    "abstract": "Root Cause Analysis (RCA) is a crucial aspect of incident management in large-scale cloud services. While the term root cause analysis or RCA has been widely used, different studies formulate the task differently. This is because the term \"RCA\" implicitly covers tasks with distinct underlying goals. For instance, the goal of localizing a faulty service for rapid triage is fundamentally different from identifying a specific functional bug for a definitive fix. However, previous surveys have largely overlooked these goal-based distinctions, conventionally categorizing papers by input data types (e.g., metric-based vs. trace-based methods). This leads to the grouping of works with disparate objectives, thereby obscuring the true progress and gaps in the field. Meanwhile, the typical audience of an RCA survey is either laymen who want to know the goals and big picture of the task or RCA researchers who want to figure out past research under the same task formulation. Thus, an RCA survey that organizes the related papers according to their goals is in high demand. To this end, this paper presents a goal-driven framework that effectively categorizes and integrates 135 papers on RCA in the context of cloud incident management based on their diverse goals, spanning the period from 2014 to 2025. In addition to the goal-driven categorization, it discusses the ultimate goal of all RCA papers as an umbrella covering different RCA formulations. Moreover, the paper discusses open challenges and future directions in RCA.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19593v1",
    "published_date": "2025-10-22 13:43:07 UTC",
    "updated_date": "2025-10-22 13:43:07 UTC"
  },
  {
    "arxiv_id": "2510.19585v2",
    "title": "Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark",
    "authors": [
      "Yu Wu",
      "Ke Shu",
      "Jonas Fischer",
      "Lidia Pivovarova",
      "David Rosson",
      "Eetu Mäkelä",
      "Mikko Tolonen"
    ],
    "abstract": "This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review. Both the dataset and code will be published",
    "pdf_url": "https://arxiv.org/pdf/2510.19585v2",
    "published_date": "2025-10-22 13:37:52 UTC",
    "updated_date": "2025-10-28 13:04:38 UTC"
  },
  {
    "arxiv_id": "2510.21839v1",
    "title": "Evaluating ChatGPT's Performance in Classifying Pneumonia from Chest X-Ray Images",
    "authors": [
      "Pragna Prahallad",
      "Pranathi Prahallad"
    ],
    "abstract": "In this study, we evaluate the ability of OpenAI's gpt-4o model to classify chest X-ray images as either NORMAL or PNEUMONIA in a zero-shot setting, without any prior fine-tuning. A balanced test set of 400 images (200 from each class) was used to assess performance across four distinct prompt designs, ranging from minimal instructions to detailed, reasoning-based prompts. The results indicate that concise, feature-focused prompts achieved the highest classification accuracy of 74\\%, whereas reasoning-oriented prompts resulted in lower performance. These findings highlight that while ChatGPT exhibits emerging potential for medical image interpretation, its diagnostic reliability remains limited. Continued advances in visual reasoning and domain-specific adaptation are required before such models can be safely applied in clinical practice.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21839v1",
    "published_date": "2025-10-22 13:31:44 UTC",
    "updated_date": "2025-10-22 13:31:44 UTC"
  },
  {
    "arxiv_id": "2510.19579v1",
    "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
    "authors": [
      "Francisco Mena",
      "Dino Ienco",
      "Cassio F. Dantas",
      "Roberto Interdonato",
      "Andreas Dengel"
    ],
    "abstract": "Multi-modal co-learning is emerging as an effective paradigm in machine learning, enabling models to collaboratively learn from different modalities to enhance single-modality predictions. Earth Observation (EO) represents a quintessential domain for multi-modal data analysis, wherein diverse remote sensors collect data to sense our planet. This unprecedented volume of data introduces novel challenges. Specifically, the access to the same sensor modalities at both training and inference stages becomes increasingly complex based on real-world constraints affecting remote sensing platforms. In this context, multi-modal co-learning presents a promising strategy to leverage the vast amount of sensor-derived data available at the training stage to improve single-modality models for inference-time deployment. Most current research efforts focus on designing customized solutions for either particular downstream tasks or specific modalities available at the inference stage. To address this, we propose a novel multi-modal co-learning framework capable of generalizing across various tasks without targeting a specific modality for inference. Our approach combines contrastive and modality discriminative learning together to guide single-modality models to structure the internal model manifold into modality-shared and modality-specific information. We evaluate our framework on four EO benchmarks spanning classification and regression tasks across different sensor modalities, where only one of the modalities available during training is accessible at inference time. Our results demonstrate consistent predictive improvements over state-of-the-art approaches from the recent machine learning and computer vision literature, as well as EO-specific methods. The obtained findings validate our framework in the single-modality inference scenarios across a diverse range of EO applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the Machine Learning journal, CfP: Discovery Science 2024",
    "pdf_url": "https://arxiv.org/pdf/2510.19579v1",
    "published_date": "2025-10-22 13:29:32 UTC",
    "updated_date": "2025-10-22 13:29:32 UTC"
  },
  {
    "arxiv_id": "2510.19562v2",
    "title": "DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning",
    "authors": [
      "Runpeng Xie",
      "Quanwei Wang",
      "Hao Hu",
      "Zherui Zhou",
      "Ni Mu",
      "Xiyun Li",
      "Yiqin Yang",
      "Shuang Xu",
      "Qianchuan Zhao",
      "Bo XU"
    ],
    "abstract": "Comprehending natural language and following human instructions are critical capabilities for intelligent agents. However, the flexibility of linguistic instructions induces substantial ambiguity across language-conditioned tasks, severely degrading algorithmic performance. To address these limitations, we present a novel method named DAIL (Distributional Aligned Learning), featuring two key components: distributional policy and semantic alignment. Specifically, we provide theoretical results that the value distribution estimation mechanism enhances task differentiability. Meanwhile, the semantic alignment module captures the correspondence between trajectories and linguistic instructions. Extensive experimental results on both structured and visual observation benchmarks demonstrate that DAIL effectively resolves instruction ambiguities, achieving superior performance to baseline methods. Our implementation is available at https://github.com/RunpengXie/Distributional-Aligned-Learning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Website at: https://github.com/RunpengXie/Distributional-Aligned-Learning",
    "pdf_url": "https://arxiv.org/pdf/2510.19562v2",
    "published_date": "2025-10-22 13:16:46 UTC",
    "updated_date": "2025-10-23 07:21:35 UTC"
  },
  {
    "arxiv_id": "2510.19559v1",
    "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language Models",
    "authors": [
      "Nidham Tekaya",
      "Manuela Waldner",
      "Matthias Zeppelzauer"
    ],
    "abstract": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity for their generalizable and expressive multimodal representations. By leveraging large-scale training data with diverse textual metadata, VLMs acquire open-vocabulary capabilities, solving tasks beyond their training scope. This paper investigates the temporal awareness of VLMs, assessing their ability to position visual content in time. We introduce TIME10k, a benchmark dataset of over 10,000 images with temporal ground truth, and evaluate the time-awareness of 37 VLMs by a novel methodology. Our investigation reveals that temporal information is structured along a low-dimensional, non-linear manifold in the VLM embedding space. Based on this insight, we propose methods to derive an explicit ``timeline'' representation from the embedding space. These representations model time and its chronological progression and thereby facilitate temporal reasoning tasks. Our timeline approaches achieve competitive to superior accuracy compared to a prompt-based baseline while being computationally efficient. All code and data are available at https://tekayanidham.github.io/timeline-page/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19559v1",
    "published_date": "2025-10-22 13:14:02 UTC",
    "updated_date": "2025-10-22 13:14:02 UTC"
  },
  {
    "arxiv_id": "2510.19544v1",
    "title": "Demonstrating Real Advantage of Machine-Learning-Enhanced Monte Carlo for Combinatorial Optimization",
    "authors": [
      "Luca Maria Del Bono",
      "Federico Ricci-Tersenghi",
      "Francesco Zamponi"
    ],
    "abstract": "Combinatorial optimization problems are central to both practical applications and the development of optimization methods. While classical and quantum algorithms have been refined over decades, machine learning-assisted approaches are comparatively recent and have not yet consistently outperformed simple, state-of-the-art classical methods. Here, we focus on a class of Quadratic Unconstrained Binary Optimization (QUBO) problems, specifically the challenge of finding minimum energy configurations in three-dimensional Ising spin glasses. We use a Global Annealing Monte Carlo algorithm that integrates standard local moves with global moves proposed via machine learning. We show that local moves play a crucial role in achieving optimal performance. Benchmarking against Simulated Annealing and Population Annealing, we demonstrate that Global Annealing not only surpasses the performance of Simulated Annealing but also exhibits greater robustness than Population Annealing, maintaining effectiveness across problem hardness and system size without hyperparameter tuning. These results provide, to our knowledge, the first clear and robust evidence that a machine learning-assisted optimization method can exceed the capabilities of classical state-of-the-art techniques in a combinatorial optimization setting.",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "13 main pages, 6 main figures. 4 supplementary pages, 2 supplementary figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19544v1",
    "published_date": "2025-10-22 12:50:27 UTC",
    "updated_date": "2025-10-22 12:50:27 UTC"
  },
  {
    "arxiv_id": "2510.19535v1",
    "title": "Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data",
    "authors": [
      "Markus Bujotzek",
      "Evelyn Trautmann",
      "Calum Hand",
      "Ian Hales"
    ],
    "abstract": "AI methods are increasingly shaping pharmaceutical drug discovery. However, their translation to industrial applications remains limited due to their reliance on public datasets, lacking scale and diversity of proprietary pharmaceutical data. Federated learning (FL) offers a promising approach to integrate private data into privacy-preserving, collaborative model training across data silos. This federated data access complicates important data-centric tasks such as estimating dataset diversity, performing informed data splits, and understanding the structure of the combined chemical space. To address this gap, we investigate how well federated clustering methods can disentangle and represent distributed molecular data. We benchmark three approaches, Federated kMeans (Fed-kMeans), Federated Principal Component Analysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated Locality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on eight diverse molecular datasets. Our evaluation utilizes both, standard mathematical and a chemistry-informed evaluation metrics, SF-ICF, that we introduce in this work. The large-scale benchmarking combined with an in-depth explainability analysis shows the importance of incorporating domain knowledge through chemistry-informed metrics, and on-client explainability analyses for federated diversity analysis on molecular data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19535v1",
    "published_date": "2025-10-22 12:41:04 UTC",
    "updated_date": "2025-10-22 12:41:04 UTC"
  },
  {
    "arxiv_id": "2510.19530v1",
    "title": "Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning",
    "authors": [
      "Ruiyao Miao",
      "Junren Xiao",
      "Shiya Tsang",
      "Hui Xiong",
      "Yingnian Wu"
    ],
    "abstract": "Existing Bayesian Optimization (BO) methods typically balance exploration and exploitation to optimize costly objective functions. However, these methods often suffer from a significant one-step bias, which may lead to convergence towards local optima and poor performance in complex or high-dimensional tasks. Recently, Black-Box Optimization (BBO) has achieved success across various scientific and engineering domains, particularly when function evaluations are costly and gradients are unavailable. Motivated by this, we propose the Reinforced Energy-Based Model for Bayesian Optimization (REBMBO), which integrates Gaussian Processes (GP) for local guidance with an Energy-Based Model (EBM) to capture global structural information. Notably, we define each Bayesian Optimization iteration as a Markov Decision Process (MDP) and use Proximal Policy Optimization (PPO) for adaptive multi-step lookahead, dynamically adjusting the depth and direction of exploration to effectively overcome the limitations of traditional BO methods. We conduct extensive experiments on synthetic and real-world benchmarks, confirming the superior performance of REBMBO. Additional analyses across various GP configurations further highlight its adaptability and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.19530v1",
    "published_date": "2025-10-22 12:36:49 UTC",
    "updated_date": "2025-10-22 12:36:49 UTC"
  },
  {
    "arxiv_id": "2510.19514v2",
    "title": "From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification",
    "authors": [
      "Maciej Mozolewski",
      "Betül Bayrak",
      "Kerstin Bach",
      "Grzegorz J. Nalepa"
    ],
    "abstract": "In eXplainable Artificial Intelligence (XAI), instance-based explanations for time series have gained increasing attention due to their potential for actionable and interpretable insights in domains such as healthcare. Addressing the challenges of explainability of state-of-the-art models, we propose a prototype-driven framework for generating sparse counterfactual explanations tailored to 12-lead ECG classification models. Our method employs SHAP-based thresholds to identify critical signal segments and convert them into interval rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract representative prototypes, and aligns these prototypes to query R-peaks for coherence with the sample being explained. The framework generates counterfactuals that modify only 78% of the original signal while maintaining 81.3% validity across all classes and achieving 43% improvement in temporal stability. We evaluate three variants of our approach, Original, Sparse, and Aligned Sparse, with class-specific performance ranging from 98.9% validity for myocardial infarction (MI) to challenges with hypertrophy (HYP) detection (13.2%). This approach supports near realtime generation (< 1 second) of clinically valid counterfactuals and provides a foundation for interactive explanation platforms. Our findings establish design principles for physiologically-aware counterfactual explanations in AI-based diagnosis systems and outline pathways toward user-controlled explanation interfaces for clinical deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19514v2",
    "published_date": "2025-10-22 12:09:50 UTC",
    "updated_date": "2026-01-19 17:20:58 UTC"
  },
  {
    "arxiv_id": "2510.21835v1",
    "title": "A Multimodal, Multitask System for Generating E Commerce Text Listings from Images",
    "authors": [
      "Nayan Kumar Singh"
    ],
    "abstract": "Manually generating catchy descriptions and names is labor intensive and a slow process for retailers. Although generative AI provides an automation solution in form of Vision to Language Models (VLM), the current VLMs are prone to factual \"hallucinations\". Siloed, single task models are not only inefficient but also fail to capture interdependent relationships between features. To address these challenges, we propose an end to end, multi task system that generates factually grounded textual listings from a single image. The contributions of this study are two proposals for the model architecture. First, application of multi task learning approach for fine tuning a vision encoder where a single vision backbone is jointly trained on attribute prediction such as color, hemline and neck style and price regression. Second, introduction of a hierarchical generation process where the model's own predicted attributes are embedded in a prompt and fed to the text decoder to improve factual consistency. The experiments demonstrate the superiority of this architecture. The multi tasking approach outperforms both the independent price regression, with a 3.6% better R2 Value and attribute classification, with a 6.6% improvement F1 score. Critically, the hierarchical generation process proves highly effective, slashing the factual hallucination rate from 12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical ablation. The hierarchical approach also reduces the latency of the autoregressive text generation process by a factor of 3.5 when compared to direct vision to language model of similar size. One minor caveat is that the model does perform 3.5% worse than direct vision-to-language model on ROUGE-L score.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 10 figures, 11 tables. Code can be found at: https://github.com/SinghNayanKumar/multimodal-product-lister/",
    "pdf_url": "https://arxiv.org/pdf/2510.21835v1",
    "published_date": "2025-10-22 11:50:49 UTC",
    "updated_date": "2025-10-22 11:50:49 UTC"
  },
  {
    "arxiv_id": "2510.19497v1",
    "title": "Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse",
    "authors": [
      "Trung-Dung Vu",
      "Benoit Gaudou",
      "Kamaldeep Singh Oberoi"
    ],
    "abstract": "Modeling realistic human behaviour to understand people's mode choices in order to propose personalised mobility solutions remains challenging. This paper presents an architecture for modeling realistic human mobility behavior in complex multimodal transport systems, demonstrated through a case study in Toulouse, France. We apply Large Language Models (LLMs) within an agent-based simulation to capture decision-making in a real urban setting. The framework integrates the GAMA simulation platform with an LLM-based generative agent, along with General Transit Feed Specification (GTFS) data for public transport, and OpenTripPlanner for multimodal routing. GAMA platform models the interactive transport environment, providing visualization and dynamic agent interactions while eliminating the need to construct the simulation environment from scratch. This design enables a stronger focus on developing generative agents and evaluating their performance in transport decision-making processes. Over a simulated month, results show that agents not only make context-aware transport decisions but also form habits over time. We conclude that combining LLMs with agent-based simulation offers a promising direction for advancing intelligent transportation systems and personalised multimodal mobility solutions. We also discuss some limitations of this approach and outline future work on scaling to larger regions, integrating real-time data, and refining memory models.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19497v1",
    "published_date": "2025-10-22 11:45:44 UTC",
    "updated_date": "2025-10-22 11:45:44 UTC"
  },
  {
    "arxiv_id": "2510.19496v1",
    "title": "CARES: Context-Aware Resolution Selector for VLMs",
    "authors": [
      "Moshe Kimhi",
      "Nimrod Shabtay",
      "Raja Giryes",
      "Chaim Baskin",
      "Eli Schwartz"
    ],
    "abstract": "Large vision-language models (VLMs) commonly process images at native or high resolution to remain effective across tasks. This inflates visual tokens ofter to 97-99% of total tokens, resulting in high compute and latency, even when low-resolution images would suffice. We introduce \\emph{CARES}-a \\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a lightweight preprocessing module that, given an image-query pair, predicts the \\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to extract features and predict when a target pretrained VLM's response converges to its peak ability to answer correctly. Though trained as a discrete classifier over a set of optional resolutions, CARES interpolates continuous resolutions at inference for fine-grained control. Across five multimodal benchmarks spanning documents and natural images, as well as diverse target VLMs, CARES preserves task performance while reducing compute by up to 80%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19496v1",
    "published_date": "2025-10-22 11:44:31 UTC",
    "updated_date": "2025-10-22 11:44:31 UTC"
  },
  {
    "arxiv_id": "2510.19495v2",
    "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning",
    "authors": [
      "Kevin Huang",
      "Rosario Scalise",
      "Cleah Winston",
      "Ayush Agrawal",
      "Yunchu Zhang",
      "Rohan Baijal",
      "Markus Grotz",
      "Byron Boots",
      "Benjamin Burchfiel",
      "Masha Itkina",
      "Paarth Shah",
      "Abhishek Gupta"
    ],
    "abstract": "Imitation learning has proven effective for training robots to perform complex tasks from expert human demonstrations. However, it remains limited by its reliance on high-quality, task-specific data, restricting adaptability to the diverse range of real-world object configurations and scenarios. In contrast, non-expert data -- such as play data, suboptimal demonstrations, partial task completions, or rollouts from suboptimal policies -- can offer broader coverage and lower collection costs. However, conventional imitation learning approaches fail to utilize this data effectively. To address these challenges, we posit that with right design decisions, offline reinforcement learning can be used as a tool to harness non-expert data to enhance the performance of imitation learning policies. We show that while standard offline RL approaches can be ineffective at actually leveraging non-expert data under the sparse data coverage settings typically encountered in the real world, simple algorithmic modifications can allow for the utilization of this data, without significant additional assumptions. Our approach shows that broadening the support of the policy distribution can allow imitation algorithms augmented by offline RL to solve tasks robustly, showing considerably enhanced recovery and generalization behavior. In manipulation tasks, these innovations significantly increase the range of initial conditions where learned policies are successful when non-expert data is incorporated. Moreover, we show that these methods are able to leverage all collected data, including partial or suboptimal demonstrations, to bolster task-directed policy performance. This underscores the importance of algorithmic techniques for using non-expert data for robust policy learning in robotics. Website: https://uwrobotlearning.github.io/RISE-offline/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19495v2",
    "published_date": "2025-10-22 11:43:39 UTC",
    "updated_date": "2025-10-25 01:18:43 UTC"
  },
  {
    "arxiv_id": "2510.19488v1",
    "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
    "authors": [
      "Dunjie Lu",
      "Yiheng Xu",
      "Junli Wang",
      "Haoyuan Wu",
      "Xinyuan Wang",
      "Zekun Wang",
      "Junlin Yang",
      "Hongjin Su",
      "Jixuan Chen",
      "Junda Chen",
      "Yuchen Mao",
      "Jingren Zhou",
      "Junyang Lin",
      "Binyuan Hui",
      "Tao Yu"
    ],
    "abstract": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19488v1",
    "published_date": "2025-10-22 11:25:48 UTC",
    "updated_date": "2025-10-22 11:25:48 UTC"
  },
  {
    "arxiv_id": "2510.19484v1",
    "title": "KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge",
    "authors": [
      "Zaifei Yang",
      "Hong Chang",
      "Ruibing Hou",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "abstract": "The molecular large language models have garnered widespread attention due to their promising potential on molecular applications. However, current molecular large language models face significant limitations in understanding molecules due to inadequate textual descriptions and suboptimal molecular representation strategies during pretraining. To address these challenges, we introduce KnowMol-100K, a large-scale dataset with 100K fine-grained molecular annotations across multiple levels, bridging the gap between molecules and textual descriptions. Additionally, we propose chemically-informative molecular representation, effectively addressing limitations in existing molecular representation strategies. Building upon these innovations, we develop KnowMol, a state-of-the-art multi-modal molecular large language model. Extensive experiments demonstrate that KnowMol achieves superior performance across molecular understanding and generation tasks.\n  GitHub: https://github.com/yzf-code/KnowMol\n  Huggingface: https://hf.co/datasets/yzf1102/KnowMol-100K",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19484v1",
    "published_date": "2025-10-22 11:23:58 UTC",
    "updated_date": "2025-10-22 11:23:58 UTC"
  },
  {
    "arxiv_id": "2510.19479v1",
    "title": "Graph Unlearning Meets Influence-aware Negative Preference Optimization",
    "authors": [
      "Qiang Chen",
      "Zhongze Wu",
      "Ang He",
      "Xi Lin",
      "Shuo Jiang",
      "Shan You",
      "Chang Xu",
      "Yi Chen",
      "Xiu Su"
    ],
    "abstract": "Recent advancements in graph unlearning models have enhanced model utility by preserving the node representation essentially invariant, while using gradient ascent on the forget set to achieve unlearning. However, this approach causes a drastic degradation in model utility during the unlearning process due to the rapid divergence speed of gradient ascent. In this paper, we introduce \\textbf{INPO}, an \\textbf{I}nfluence-aware \\textbf{N}egative \\textbf{P}reference \\textbf{O}ptimization framework that focuses on slowing the divergence speed and improving the robustness of the model utility to the unlearning process. Specifically, we first analyze that NPO has slower divergence speed and theoretically propose that unlearning high-influence edges can reduce impact of unlearning. We design an influence-aware message function to amplify the influence of unlearned edges and mitigate the tight topological coupling between the forget set and the retain set. The influence of each edge is quickly estimated by a removal-based method. Additionally, we propose a topological entropy loss from the perspective of topology to avoid excessive information loss in the local structure during unlearning. Extensive experiments conducted on five real-world datasets demonstrate that INPO-based model achieves state-of-the-art performance on all forget quality metrics while maintaining the model's utility. Codes are available at \\href{https://github.com/sh-qiangchen/INPO}{https://github.com/sh-qiangchen/INPO}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19479v1",
    "published_date": "2025-10-22 11:18:00 UTC",
    "updated_date": "2025-10-22 11:18:00 UTC"
  },
  {
    "arxiv_id": "2510.19476v1",
    "title": "A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring",
    "authors": [
      "Julian Schulz"
    ],
    "abstract": "As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19476v1",
    "published_date": "2025-10-22 11:13:52 UTC",
    "updated_date": "2025-10-22 11:13:52 UTC"
  },
  {
    "arxiv_id": "2510.19470v1",
    "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission",
    "authors": [
      "Weihao Yang",
      "Hao Huang",
      "Donglei Wu",
      "Ningke Li",
      "Yanqi Pan",
      "Qiyang Zheng",
      "Wen Xia",
      "Shiyi Li",
      "Qiang Wang"
    ],
    "abstract": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large models. However, the rapidly growing scale outpaces model training on a single DC, driving a shift toward a more flexible, cross-DC training paradigm. Under this, Expert Parallelism (EP) of MoE faces significant scalability issues due to the limited cross-DC bandwidth. Specifically, existing EP optimizations attempt to overlap data communication and computation, which has little benefit in low-bandwidth scenarios due to a much longer data communication time. Therefore, the trends of cross-DC EP scaling is fast becoming a critical roadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize EP under constrained bandwidth. Our key idea is to dynamically transform the spatial placement of experts to reduce data communication traffic and frequency, thereby minimizing EP's communication overheads. However, it is non-trivial to find the optimal solution because it complicates the original communication pattern by mixing data and expert communication. We therefore build a stream-based model to determine the optimal transmission ratio. Guided by this, we incorporate two techniques: (1) domain-based partition to construct the mapping between hybrid patterns and specific communication topology at GPU level, and (2) parameter-efficient migration to further refine this topology by reducing expert transmission overhead and enlarging the domain size. Combining all these designs, HybridEP can be considered as a more general EP with better scalability. Experimental results show that HybridEP outperforms existing state-of-the-art MoE training systems by up to 5.6x under constrained bandwidth. We further compare HybridEP and EP on large-scale simulations. HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19470v1",
    "published_date": "2025-10-22 11:05:17 UTC",
    "updated_date": "2025-10-22 11:05:17 UTC"
  },
  {
    "arxiv_id": "2510.19444v2",
    "title": "A Foundational Theory of Quantitative Abstraction: Adjunctions, Duality, and Logic for Probabilistic Systems",
    "authors": [
      "Nivar Anwer",
      "Ezequiel López-Rubio",
      "David Elizondo",
      "Rafael M. Luque-Baena"
    ],
    "abstract": "The analysis and control of stochastic dynamical systems rely on probabilistic models such as (continuous-space) Markov decision processes, but large or continuous state spaces make exact analysis intractable and call for principled quantitative abstraction. This work develops a unified theory of such abstraction by integrating category theory, coalgebra, quantitative logic, and optimal transport, centred on a canonical $\\varepsilon$-quotient of the behavioral pseudo-metric with a universal property: among all abstractions that collapse behavioral differences below $\\varepsilon$, it is the most detailed, and every other abstraction achieving the same discounted value-loss guarantee factors uniquely through it. Categorically, a quotient functor $Q_\\varepsilon$ from a category of probabilistic systems to a category of metric specifications admits, via the Special Adjoint Functor Theorem, a right adjoint $R_\\varepsilon$, yielding an adjunction $Q_\\varepsilon \\dashv R_\\varepsilon$ that formalizes a duality between abstraction and realization; logically, a quantitative modal $μ$-calculus with separate reward and transition modalities is shown, for a broad class of systems, to be expressively complete for the behavioral pseudo-metric, with a countable fully abstract fragment suitable for computation. The theory is developed coalgebraically over Polish spaces and the Giry monad and validated on finite-state models using optimal-transport solvers, with experiments corroborating the predicted contraction properties and structural stability and aligning with the theoretical value-loss bounds, thereby providing a rigorous foundation for quantitative state abstraction and representation learning in probabilistic domains.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19444v2",
    "published_date": "2025-10-22 10:16:24 UTC",
    "updated_date": "2025-11-05 01:26:07 UTC"
  },
  {
    "arxiv_id": "2510.19429v1",
    "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning",
    "authors": [
      "Wonje Choi",
      "Jooyoung Kim",
      "Honguk Woo"
    ],
    "abstract": "We address the challenge of adopting language models (LMs) for embodied tasks in dynamic environments, where online access to large-scale inference engines or symbolic planners is constrained due to latency, connectivity, and resource limitations. To this end, we present NeSyPr, a novel embodied reasoning framework that compiles knowledge via neurosymbolic proceduralization, thereby equipping LM-based agents with structured, adaptive, and timely reasoning capabilities. In NeSyPr, task-specific plans are first explicitly generated by a symbolic tool leveraging its declarative knowledge. These plans are then transformed into composable procedural representations that encode the plans' implicit production rules, enabling the resulting composed procedures to be seamlessly integrated into the LM's inference process. This neurosymbolic proceduralization abstracts and generalizes multi-step symbolic structured path-finding and reasoning into single-step LM inference, akin to human knowledge compilation. It supports efficient test-time inference without relying on external symbolic guidance, making it well suited for deployment in latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating its efficient reasoning capabilities over large-scale reasoning models and a symbolic planner, while using more compact LMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19429v1",
    "published_date": "2025-10-22 09:57:02 UTC",
    "updated_date": "2025-10-22 09:57:02 UTC"
  },
  {
    "arxiv_id": "2510.19425v1",
    "title": "Neural Variational Dropout Processes",
    "authors": [
      "Insu Jeon",
      "Youngjin Park",
      "Gunhee Kim"
    ],
    "abstract": "Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts. It allows for a quick reconfiguration of a globally learned and shared neural network for new tasks in multi-task few-shot learning. In addition, NVDPs utilize a novel prior conditioned on the whole task data to optimize the conditional \\textit{dropout} posterior in the amortized variational inference. Surprisingly, this enables the robust approximation of task-specific dropout rates that can deal with a wide range of functional ambiguities and uncertainties. We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification. The results show the excellent performance of NVDPs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a Poster at International Conference on Learning Representations (ICLR) 2022 (Apr 25-29, 2022)",
    "pdf_url": "https://arxiv.org/pdf/2510.19425v1",
    "published_date": "2025-10-22 09:45:44 UTC",
    "updated_date": "2025-10-22 09:45:44 UTC"
  },
  {
    "arxiv_id": "2510.19423v2",
    "title": "ETOM: A Five-Level Benchmark for Evaluating Tool Orchestration within the MCP Ecosystem",
    "authors": [
      "Jia-Kai Dong",
      "I-Wei Huang",
      "Chun-Tin Wu",
      "Yi-Tien Tsai"
    ],
    "abstract": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often assess tools in isolation, overlooking challenges such as functional overlap and cross-server orchestration, which can lead to overly optimistic evaluations. ETOM addresses these gaps by constructing ground truth through \"equal function sets\", enabling objective metrics such as F1 score and reducing reliance on LLM-as-a-judge evaluation. Its five-level curriculum systematically tests agent capabilities, from single-tool orchestration to complex cross-server planning, as well as robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-of-the-art agents exhibit systemic weaknesses in robustness. ETOM provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the Findings of EACL 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.19423v2",
    "published_date": "2025-10-22 09:45:11 UTC",
    "updated_date": "2026-01-18 10:40:06 UTC"
  },
  {
    "arxiv_id": "2510.19421v1",
    "title": "FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA",
    "authors": [
      "Songqi Zhou",
      "Zeyuan Liu",
      "Benben Jiang"
    ],
    "abstract": "Ensuring fairness in machine learning models is a critical challenge. Existing debiasing methods often compromise performance, rely on static correction strategies, and struggle with data sparsity, particularly within minority groups. Furthermore, their utilization of sensitive attributes is often suboptimal, either depending excessively on complete attribute labeling or disregarding these attributes entirely. To overcome these limitations, we propose FairNet, a novel framework for dynamic, instance-level fairness correction. FairNet integrates a bias detector with conditional low-rank adaptation (LoRA), which enables selective activation of the fairness correction mechanism exclusively for instances identified as biased, and thereby preserve performance on unbiased instances. A key contribution is a new contrastive loss function for training the LoRA module, specifically designed to minimize intra-class representation disparities across different sensitive groups and effectively address underfitting in minority groups. The FairNet framework can flexibly handle scenarios with complete, partial, or entirely absent sensitive attribute labels. Theoretical analysis confirms that, under moderate TPR/FPR for the bias detector, FairNet can enhance the performance of the worst group without diminishing overall model performance, and potentially yield slight performance improvements. Comprehensive empirical evaluations across diverse vision and language benchmarks validate the effectiveness of FairNet.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19421v1",
    "published_date": "2025-10-22 09:44:03 UTC",
    "updated_date": "2025-10-22 09:44:03 UTC"
  },
  {
    "arxiv_id": "2510.19420v1",
    "title": "Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation",
    "authors": [
      "Chengcan Wu",
      "Zhixin Zhang",
      "Mingqian Xu",
      "Zeming Wei",
      "Meng Sun"
    ],
    "abstract": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a popular paradigm of AI applications. However, trustworthiness issues in MAS remain a critical concern. Unlike challenges in single-agent systems, MAS involve more complex communication processes, making them susceptible to corruption attacks. To mitigate this issue, several defense mechanisms have been developed based on the graph representation of MAS, where agents represent nodes and communications form edges. Nevertheless, these methods predominantly focus on static graph defense, attempting to either detect attacks in a fixed graph structure or optimize a static topology with certain defensive capabilities. To address this limitation, we propose a dynamic defense paradigm for MAS graph structures, which continuously monitors communication within the MAS graph, then dynamically adjusts the graph topology, accurately disrupts malicious communications, and effectively defends against evolving and diverse dynamic attacks. Experimental results in increasingly complex and dynamic MAS environments demonstrate that our method significantly outperforms existing MAS defense mechanisms, contributing an effective guardrail for their trustworthy applications. Our code is available at https://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "math.OC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19420v1",
    "published_date": "2025-10-22 09:43:32 UTC",
    "updated_date": "2025-10-22 09:43:32 UTC"
  },
  {
    "arxiv_id": "2510.19875v1",
    "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention",
    "authors": [
      "J Rosser",
      "José Luis Redondo García",
      "Gustavo Penha",
      "Konstantina Palla",
      "Hugues Bouchard"
    ],
    "abstract": "As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \\log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19875v1",
    "published_date": "2025-10-22 09:42:29 UTC",
    "updated_date": "2025-10-22 09:42:29 UTC"
  },
  {
    "arxiv_id": "2510.19414v1",
    "title": "EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection",
    "authors": [
      "Tong Zhang",
      "Yihuan Huang",
      "Yanzhen Ren"
    ],
    "abstract": "The growing prevalence of speech deepfakes has raised serious concerns, particularly in real-world scenarios such as telephone fraud and identity theft. While many anti-spoofing systems have demonstrated promising performance on lab-generated synthetic speech, they often fail when confronted with physical replay attacks-a common and low-cost form of attack used in practical settings. Our experiments show that models trained on existing datasets exhibit severe performance degradation, with average accuracy dropping to 59.6% when evaluated on replayed audio. To bridge this gap, we present EchoFake, a comprehensive dataset comprising more than 120 hours of audio from over 13,000 speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and physical replay recordings collected under varied devices and real-world environmental settings. Additionally, we evaluate three baseline detection models and show that models trained on EchoFake achieve lower average EERs across datasets, indicating better generalization. By introducing more practical challenges relevant to real-world deployment, EchoFake offers a more realistic foundation for advancing spoofing detection methods.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19414v1",
    "published_date": "2025-10-22 09:34:31 UTC",
    "updated_date": "2025-10-22 09:34:31 UTC"
  },
  {
    "arxiv_id": "2510.19410v1",
    "title": "ToMMeR -- Efficient Entity Mention Detection from Large Language Models",
    "authors": [
      "Victor Morand",
      "Nadi Tomeh",
      "Josiane Mothe",
      "Benjamin Piwowarski"
    ],
    "abstract": "Identifying which text spans refer to entities -- mention detection -- is both foundational for information extraction and a known performance bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing mention detection capabilities from early LLM layers. Across 13 NER benchmarks, ToMMeR achieves 93\\% recall zero-shot, with over 90\\% precision using an LLM as a judge showing that ToMMeR rarely produces spurious predictions despite high recall. Cross-model analysis reveals that diverse architectures (14M-15B parameters) converge on similar mention boundaries (DICE >75\\%), confirming that mention detection emerges naturally from language modeling. When extended with span classification heads, ToMMeR achieves near SOTA NER performance (80-87\\% F1 on standard benchmarks). Our work provides evidence that structured entity representations exist in early transformer layers and can be efficiently recovered with minimal parameters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code is available at https://github.com/VictorMorand/llm2ner",
    "pdf_url": "https://arxiv.org/pdf/2510.19410v1",
    "published_date": "2025-10-22 09:28:18 UTC",
    "updated_date": "2025-10-22 09:28:18 UTC"
  },
  {
    "arxiv_id": "2510.19386v2",
    "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
    "authors": [
      "Ning Li",
      "Qiqiang Lin",
      "Zheng Wu",
      "Xiaoyun Mo",
      "Weiming Zhang",
      "Yin Zhao",
      "Xiangmou Qu",
      "Jiamu Zhou",
      "Jun Wang",
      "Congmin Zheng",
      "Yuanyi Song",
      "Hongjiang Chen",
      "Heyuan Huang",
      "Jihong Wang",
      "Jiaxin Yin",
      "Jingwei Yu",
      "Junwei Liao",
      "Qiuying Peng",
      "Xingyu Lou",
      "Jun Wang",
      "Weiwen Liu",
      "Zhuosheng Zhang",
      "Weinan Zhang"
    ],
    "abstract": "With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19386v2",
    "published_date": "2025-10-22 09:02:48 UTC",
    "updated_date": "2025-10-24 07:32:03 UTC"
  },
  {
    "arxiv_id": "2510.19365v1",
    "title": "The Massive Legal Embedding Benchmark (MLEB)",
    "authors": [
      "Umar Butler",
      "Abdur-Rahman Butler",
      "Adrian Lucas Malec"
    ],
    "abstract": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19365v1",
    "published_date": "2025-10-22 08:38:44 UTC",
    "updated_date": "2025-10-22 08:38:44 UTC"
  },
  {
    "arxiv_id": "2510.19361v3",
    "title": "AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation",
    "authors": [
      "Xianyang Liu",
      "Yilin Liu",
      "Shuai Wang",
      "Hao Cheng",
      "Andrew Estornell",
      "Yuzhi Zhao",
      "Jun Shu",
      "Jiaheng Wei"
    ],
    "abstract": "The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic method for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.19361v3",
    "published_date": "2025-10-22 08:34:13 UTC",
    "updated_date": "2026-01-08 03:57:11 UTC"
  },
  {
    "arxiv_id": "2510.19873v1",
    "title": "From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph",
    "authors": [
      "Junfeng Gong",
      "Zhiyi Wei",
      "Junying Chen",
      "Cheng Liu",
      "Huawei Li"
    ],
    "abstract": "Despite significant evolution of CUDA programming and domain-specific libraries, effectively utilizing GPUs with massively parallel engines remains difficult. Large language models (LLMs) show strong potential in generating optimized CUDA code from sequential code. However, using LLMs in practice faces two major challenges: cloud-based APIs pose risks of code leakage, and local deployment is often computationally expensive and inefficient. These drawbacks have spurred interest in small language models (SLMs), which are more lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs can achieve performance comparable to LLMs on specific tasks. While SLMs can match LLMs on domain-specific tasks, their limited reasoning abilities lead to suboptimal performance in complex CUDA generation according to our experiments. To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented generation framework that transfers LLM-level reasoning to smaller models. ReGraphT organizes CUDA optimization trajectories into a structured reasoning graph, modeling the combined CUDA optimizations as state transitions, and leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also present a CUDA-specific benchmark with difficulty tiers defined by reasoning complexity to evaluate models more comprehensively. Experiments show that ReGraphT outperforms HPC-specific fine-tuned models and other retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level performance without the associated privacy risks or excessive computing overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19873v1",
    "published_date": "2025-10-22 08:33:44 UTC",
    "updated_date": "2025-10-22 08:33:44 UTC"
  },
  {
    "arxiv_id": "2510.19358v1",
    "title": "M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models",
    "authors": [
      "Yejin Kwon",
      "Taewoo Kang",
      "Hyunsoo Yoon",
      "Changouk Kim"
    ],
    "abstract": "We present M3-SLU, a new multimodal large language model (MLLM) benchmark for evaluating multi-speaker, multi-turn spoken language understanding. While recent models show strong performance in speech and text comprehension, they still struggle with speaker-attributed reasoning, the ability to understand who said what and when in natural conversations. M3-SLU is built from four open corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000 validated instances with paired audio, transcripts, and metadata. It includes two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker Attribution via Utterance Matching. We provide baseline results for both cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and accuracy metrics. Results show that while models can capture what was said, they often fail to identify who said it, revealing a key gap in speaker-aware dialogue understanding. M3-SLU offers as a challenging benchmark to advance research in speaker-aware multimodal understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to LREC 2026. 11 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19358v1",
    "published_date": "2025-10-22 08:28:43 UTC",
    "updated_date": "2025-10-22 08:28:43 UTC"
  },
  {
    "arxiv_id": "2510.19351v2",
    "title": "Learning To Defer To A Population With Limited Demonstrations",
    "authors": [
      "Nilesh Ramgolam",
      "Gustavo Carneiro",
      "Hsiang-Ting Chen"
    ],
    "abstract": "This paper addresses the critical data scarcity that hinders the practical deployment of learning to defer (L2D) systems to the population. We introduce a context-aware, semi-supervised framework that uses meta-learning to generate expert-specific embeddings from only a few demonstrations. We demonstrate the efficacy of a dual-purpose mechanism, where these embeddings are used first to generate a large corpus of pseudo-labels for training, and subsequently to enable on-the-fly adaptation to new experts at test-time. The experiment results on three different datasets confirm that a model trained on these synthetic labels rapidly approaches oracle-level performance, validating the data efficiency of our approach. By resolving a key training bottleneck, this work makes adaptive L2D systems more practical and scalable, paving the way for human-AI collaboration in real-world environments. To facilitate reproducibility and address implementation details not covered in the main text, we provide our source code and training configurations at https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to IEEE DICTA 2025 (poster). 7 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19351v2",
    "published_date": "2025-10-22 08:18:02 UTC",
    "updated_date": "2025-10-23 01:52:19 UTC"
  },
  {
    "arxiv_id": "2510.19347v1",
    "title": "A New Type of Adversarial Examples",
    "authors": [
      "Xingyang Nie",
      "Guojie Xiao",
      "Su Pan",
      "Biao Wang",
      "Huilin Ge",
      "Tao Fang"
    ],
    "abstract": "Most machine learning models are vulnerable to adversarial examples, which poses security concerns on these models. Adversarial examples are crafted by applying subtle but intentionally worst-case modifications to examples from the dataset, leading the model to output a different answer from the original example. In this paper, adversarial examples are formed in an exactly opposite manner, which are significantly different from the original examples but result in the same answer. We propose a novel set of algorithms to produce such adversarial examples, including the negative iterative fast gradient sign method (NI-FGSM) and the negative iterative fast gradient method (NI-FGM), along with their momentum variants: the negative momentum iterative fast gradient sign method (NMI-FGSM) and the negative momentum iterative fast gradient method (NMI-FGM). Adversarial examples constructed by these methods could be used to perform an attack on machine learning systems in certain occasions. Moreover, our results show that the adversarial examples are not merely distributed in the neighbourhood of the examples from the dataset; instead, they are distributed extensively in the sample space.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19347v1",
    "published_date": "2025-10-22 08:14:11 UTC",
    "updated_date": "2025-10-22 08:14:11 UTC"
  },
  {
    "arxiv_id": "2510.19345v1",
    "title": "Foundation Model Forecasts: Form and Function",
    "authors": [
      "Alvaro Perez-Diaz",
      "James C. Loach",
      "Danielle E. Toutoungi",
      "Lee Middleton"
    ],
    "abstract": "Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet accuracy alone does not determine practical value. The form of a forecast -- point, quantile, parametric, or trajectory ensemble -- fundamentally constrains which operational tasks it can support. We survey recent TSFMs and find that two-thirds produce only point or parametric forecasts, while many operational tasks require trajectory ensembles that preserve temporal dependence. We establish when forecast types can be converted and when they cannot: trajectory ensembles convert to simpler forms via marginalization without additional assumptions, but the reverse requires imposing temporal dependence through copulas or conformal methods. We prove that marginals cannot determine path-dependent event probabilities -- infinitely many joint distributions share identical marginals but yield different answers to operational questions. We map six fundamental forecasting tasks to minimal sufficient forecast types and provide a task-aligned evaluation framework. Our analysis clarifies when forecast type, not accuracy, differentiates practical utility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19345v1",
    "published_date": "2025-10-22 08:10:34 UTC",
    "updated_date": "2025-10-22 08:10:34 UTC"
  },
  {
    "arxiv_id": "2510.19342v1",
    "title": "To Use or to Refuse? Re-Centering Student Agency with Generative AI in Engineering Design Education",
    "authors": [
      "Thijs Willems",
      "Sumbul Khan",
      "Qian Huang",
      "Bradley Camburn",
      "Nachamma Sockalingam",
      "King Wang Poon"
    ],
    "abstract": "This pilot study traces students' reflections on the use of AI in a 13-week foundational design course enrolling over 500 first-year engineering and architecture students at the Singapore University of Technology and Design. The course was an AI-enhanced design course, with several interventions to equip students with AI based design skills. Students were required to reflect on whether the technology was used as a tool (instrumental assistant), a teammate (collaborative partner), or neither (deliberate non-use). By foregrounding this three-way lens, students learned to use AI for innovation rather than just automation and to reflect on agency, ethics, and context rather than on prompt crafting alone. Evidence stems from coursework artefacts: thirteen structured reflection spreadsheets and eight illustrated briefs submitted, combined with notes of teachers and researchers. Qualitative coding of these materials reveals shared practices brought about through the inclusion of Gen-AI, including accelerated prototyping, rapid skill acquisition, iterative prompt refinement, purposeful \"switch-offs\" during user research, and emergent routines for recognizing hallucinations. Unexpectedly, students not only harnessed Gen-AI for speed but (enabled by the tool-teammate-neither triage) also learned to reject its outputs, invent their own hallucination fire-drills, and divert the reclaimed hours into deeper user research, thereby transforming efficiency into innovation. The implications of the approach we explore shows that: we can transform AI uptake into an assessable design habit; that rewarding selective non-use cultivates hallucination-aware workflows; and, practically, that a coordinated bundle of tool access, reflection, role tagging, and public recognition through competition awards allows AI based innovation in education to scale without compromising accountability.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "to be published in IEEE TALE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19342v1",
    "published_date": "2025-10-22 08:06:48 UTC",
    "updated_date": "2025-10-22 08:06:48 UTC"
  },
  {
    "arxiv_id": "2510.19338v2",
    "title": "Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning",
    "authors": [
      "Ling Team",
      "Bin Han",
      "Caizhi Tang",
      "Chen Liang",
      "Donghao Zhang",
      "Fan Yuan",
      "Feng Zhu",
      "Jie Gao",
      "Jingyu Hu",
      "Longfei Li",
      "Meng Li",
      "Mingyang Zhang",
      "Peijie Jiang",
      "Peng Jiao",
      "Qian Zhao",
      "Qingyuan Yang",
      "Wenbo Shen",
      "Xinxing Yang",
      "Yalin Zhang",
      "Yankun Ren",
      "Yao Zhao",
      "Yibo Cao",
      "Yixuan Sun",
      "Yue Zhang",
      "Yuchen Fang",
      "Zibin Lin",
      "Zixuan Cheng",
      "Jun Zhou"
    ],
    "abstract": "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19338v2",
    "published_date": "2025-10-22 07:59:38 UTC",
    "updated_date": "2025-10-23 06:33:17 UTC"
  },
  {
    "arxiv_id": "2510.19334v1",
    "title": "Metadata Extraction Leveraging Large Language Models",
    "authors": [
      "Cuize Han",
      "Sesh Jalagam"
    ],
    "abstract": "The advent of Large Language Models has revolutionized tasks across domains, including the automation of legal document analysis, a critical component of modern contract management systems. This paper presents a comprehensive implementation of LLM-enhanced metadata extraction for contract review, focusing on the automatic detection and annotation of salient legal clauses. Leveraging both the publicly available Contract Understanding Atticus Dataset (CUAD) and proprietary contract datasets, our work demonstrates the integration of advanced LLM methodologies with practical applications. We identify three pivotal elements for optimizing metadata extraction: robust text conversion, strategic chunk selection, and advanced LLM-specific techniques, including Chain of Thought (CoT) prompting and structured tool calling. The results from our experiments highlight the substantial improvements in clause identification accuracy and efficiency. Our approach shows promise in reducing the time and cost associated with contract review while maintaining high accuracy in legal clause identification. The results suggest that carefully optimized LLM systems could serve as valuable tools for legal professionals, potentially increasing access to efficient contract review services for organizations of all sizes.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19334v1",
    "published_date": "2025-10-22 07:56:36 UTC",
    "updated_date": "2025-10-22 07:56:36 UTC"
  },
  {
    "arxiv_id": "2510.19329v1",
    "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
    "authors": [
      "Panagiotis Agrafiotis",
      "Begüm Demir"
    ],
    "abstract": "Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at https://github.com/pagraf/Seabed-Net.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ISPRS Journal of Photogrammetry and Remote Sensing",
    "pdf_url": "https://arxiv.org/pdf/2510.19329v1",
    "published_date": "2025-10-22 07:43:03 UTC",
    "updated_date": "2025-10-22 07:43:03 UTC"
  },
  {
    "arxiv_id": "2510.19327v1",
    "title": "SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities",
    "authors": [
      "Usama Antuley",
      "Shahbaz Siddiqui",
      "Sufian Hameed",
      "Waqas Arif",
      "Subhan Shah",
      "Syed Attique Shah"
    ],
    "abstract": "The rapid evolution of smart cities has increased the reliance on intelligent interconnected services to optimize infrastructure, resources, and citizen well-being. Agentic AI has emerged as a key enabler by supporting autonomous decision-making and adaptive coordination, allowing urban systems to respond in real time to dynamic conditions. Its benefits are evident in areas such as transportation, where the integration of traffic data, weather forecasts, and safety sensors enables dynamic rerouting and a faster response to hazards. However, its deployment across heterogeneous smart city ecosystems raises critical governance, risk, and compliance (GRC) challenges, including accountability, data privacy, and regulatory alignment within decentralized infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather, Traffic, and Safety) demonstrated that its governance policies, including a fallback mechanism for high-risk scenarios, effectively steer multiple LLMs (GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs, producing an average MAE reduction of 35% across agents. Results showed stable weather monitoring, effective handling of high-risk traffic plateaus 0.85, and adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a 3-agent deployment confirmed scalability, with throughput between 13.8-17.2 requests per second, execution times below 72~ms, and governance delays under 100 ms, analytical projections suggest maintained performance at larger scales. Cross-domain rules ensured safe interoperability, with traffic rerouting permitted only under validated weather conditions. These findings validate SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance framework that consolidates distributed agent outputs into accountable, real-time decisions, offering a resilient foundation for smart-city management.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19327v1",
    "published_date": "2025-10-22 07:40:37 UTC",
    "updated_date": "2025-10-22 07:40:37 UTC"
  },
  {
    "arxiv_id": "2510.19325v1",
    "title": "Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization",
    "authors": [
      "Junjie Song",
      "Yiwen Liu",
      "Dapeng Li",
      "Yin Sun",
      "Shukun Fu",
      "Siqi Chen",
      "Yuji Cao"
    ],
    "abstract": "Text summarization is a crucial task that requires the simultaneous optimization of multiple objectives, including consistency, coherence, relevance, and fluency, which presents considerable challenges. Although large language models (LLMs) have demonstrated remarkable performance, enhanced by reinforcement learning (RL), few studies have focused on optimizing the multi-objective problem of summarization through RL based on LLMs. In this paper, we introduce hypervolume optimization (HVO), a novel optimization strategy that dynamically adjusts the scores between groups during the reward process in RL by using the hypervolume method. This method guides the model's optimization to progressively approximate the pareto front, thereby generating balanced summaries across multiple objectives. Experimental results on several representative summarization datasets demonstrate that our method outperforms group relative policy optimization (GRPO) in overall scores and shows more balanced performance across different dimensions. Moreover, a 7B foundation model enhanced by HVO performs comparably to GPT-4 in the summarization task, while maintaining a shorter generation length. Our code is publicly available at https://github.com/ai4business-LiAuto/HVO.git",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19325v1",
    "published_date": "2025-10-22 07:39:04 UTC",
    "updated_date": "2025-10-22 07:39:04 UTC"
  },
  {
    "arxiv_id": "2510.19322v2",
    "title": "Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks",
    "authors": [
      "Changbo Wu",
      "Zhuolong Yu",
      "Gongming Zhao",
      "Hongli Xu"
    ],
    "abstract": "Collective communication (CC) is critical for scaling distributed machine learning (DML). The predictable traffic patterns of DML present a great oppotunity for applying optical network technologies. Optical networks with reconfigurable topologies promise high bandwidth and low latency for collective communications. However, existing approaches face inherent limitations: static topologies are inefficient for dynamic communication patterns within CC algorithm, while frequent topology reconfiguration matching every step of the algorithm incurs significant overhead.\n  In this paper, we propose SWOT, a demand-aware optical network framework that employs ``intra-collective reconfiguration'' to dynamically align network resources with CC traffic patterns. SWOT hides reconfiguration latency by overlapping it with data transmission through three key techniques: Heterogeneous Message Splitting, Asynchronous Overlapping, and Topology Bypassing. Extensive simulations demonstrate that SWOT reduces communication completion time up to 89.7% across diverse CC algorithm compared to static baselines, demonstrating strong robustness to varying optical resources and reconfiguration delay.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19322v2",
    "published_date": "2025-10-22 07:34:04 UTC",
    "updated_date": "2026-01-04 04:02:36 UTC"
  },
  {
    "arxiv_id": "2510.19321v1",
    "title": "Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer",
    "authors": [
      "Hai-jie Yuan",
      "Heng Zhang",
      "Fei Yin"
    ],
    "abstract": "Handwritten signature verification is a crucial aspect of identity authentication, with applications in various domains such as finance and e-commerce. However, achieving high accuracy in signature verification remains challenging due to intra-user variability and the risk of forgery. This paper introduces a novel approach for dynamic signature verification: the Temporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the Graph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both spatial and temporal dependencies in signature data. TS-GATR enhances verification performance by representing signatures as graphs, where each node captures dynamic features (e.g. position, velocity, pressure), and by using attention mechanisms to model their complex relationships. The proposed method further employs a Dual-Graph Attention Transformer (DGATR) module, which utilizes k-step and k-nearest neighbor adjacency graphs to model local and global spatial features, respectively. To capture long-term temporal dependencies, the model integrates GRU, thereby enhancing its ability to learn dynamic features during signature verification. Comprehensive experiments conducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR surpasses current state-of-the-art approaches, consistently achieving lower Equal Error Rates (EER) across various scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19321v1",
    "published_date": "2025-10-22 07:32:55 UTC",
    "updated_date": "2025-10-22 07:32:55 UTC"
  },
  {
    "arxiv_id": "2510.19314v2",
    "title": "Continual Knowledge Adaptation for Reinforcement Learning",
    "authors": [
      "Jinwu Hu",
      "Zihao Lian",
      "Zhiquan Wen",
      "Chenghao Li",
      "Guohao Chen",
      "Xutao Wen",
      "Bin Xiao",
      "Mingkui Tan"
    ],
    "abstract": "Reinforcement Learning enables agents to learn optimal behaviors through interactions with environments. However, real-world environments are typically non-stationary, requiring agents to continuously adapt to new tasks and changing conditions. Although Continual Reinforcement Learning facilitates learning across multiple tasks, existing methods often suffer from catastrophic forgetting and inefficient knowledge utilization. To address these challenges, we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), which enables the accumulation and effective utilization of historical knowledge. Specifically, we introduce a Continual Knowledge Adaptation strategy, which involves maintaining a task-specific knowledge vector pool and dynamically using historical knowledge to adapt the agent to new tasks. This process mitigates catastrophic forgetting and enables efficient knowledge transfer across tasks by preserving and adapting critical model parameters. Additionally, we propose an Adaptive Knowledge Merging mechanism that combines similar knowledge vectors to address scalability challenges, reducing memory requirements while ensuring the retention of essential knowledge. Experiments on three benchmarks demonstrate that the proposed CKA-RL outperforms state-of-the-art methods, achieving an improvement of 4.20% in overall performance and 8.02% in forward transfer. The source code is available at https://github.com/Fhujinwu/CKA-RL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19314v2",
    "published_date": "2025-10-22 07:25:41 UTC",
    "updated_date": "2026-01-20 04:47:18 UTC"
  },
  {
    "arxiv_id": "2510.24760v1",
    "title": "Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments",
    "authors": [
      "Mengyuan Chen",
      "Chengjun Dai",
      "Xinyang Dong",
      "Chengzhe Feng",
      "Kewei Fu",
      "Jianshe Li",
      "Zhihan Peng",
      "Yongqi Tong",
      "Junshao Zhang",
      "Hong Zhu"
    ],
    "abstract": "We present Dingtalk DeepResearch, a unified multi agent intelligence framework for real world enterprise environments, delivering deep research, heterogeneous table reasoning, and multimodal report generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24760v1",
    "published_date": "2025-10-22 07:14:26 UTC",
    "updated_date": "2025-10-22 07:14:26 UTC"
  },
  {
    "arxiv_id": "2510.19303v1",
    "title": "Collaborative penetration testing suite for emerging generative AI algorithms",
    "authors": [
      "Petar Radanliev"
    ],
    "abstract": "Problem Space: AI Vulnerabilities and Quantum Threats Generative AI vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative AI models against classical and quantum cyberattacks. Proposed Solution Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs. Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities identified across test environments. 70% reduction in high-severity issues within 2 weeks. 90% resolution efficiency for blockchain-logged vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum Cryptography AI Red Teaming.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19303v1",
    "published_date": "2025-10-22 07:05:08 UTC",
    "updated_date": "2025-10-22 07:05:08 UTC"
  },
  {
    "arxiv_id": "2510.19299v1",
    "title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties",
    "authors": [
      "Philipp J. Schneider",
      "Lin Tian",
      "Marian-Andrei Rizoiu"
    ],
    "abstract": "Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19299v1",
    "published_date": "2025-10-22 07:00:33 UTC",
    "updated_date": "2025-10-22 07:00:33 UTC"
  },
  {
    "arxiv_id": "2510.19298v1",
    "title": "Knowledge and Common Knowledge of Strategies",
    "authors": [
      "Borja Sierra Miranda",
      "Thomas Studer"
    ],
    "abstract": "Most existing work on strategic reasoning simply adopts either an informed or an uninformed semantics. We propose a model where knowledge of strategies can be specified on a fine-grained level. In particular, it is possible to distinguish first-order, higher-order, and common knowledge of strategies. We illustrate the effect of higher-order knowledge of strategies by studying the game Hanabi. Further, we show that common knowledge of strategies is necessary to solve the consensus problem. Finally, we study the decidability of the model checking problem.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19298v1",
    "published_date": "2025-10-22 07:00:33 UTC",
    "updated_date": "2025-10-22 07:00:33 UTC"
  },
  {
    "arxiv_id": "2510.19282v1",
    "title": "Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning",
    "authors": [
      "Safa Ben Atitallah",
      "Maha Driss",
      "Wadii Boulila",
      "Anis Koubaa"
    ],
    "abstract": "Alzheimer disease is a severe brain disorder that causes harm in various brain areas and leads to memory damage. The limited availability of labeled medical data poses a significant challenge for accurate Alzheimer disease detection. There is a critical need for effective methods to improve the accuracy of Alzheimer disease detection, considering the scarcity of labeled data, the complexity of the disease, and the constraints related to data privacy. To address this challenge, our study leverages the power of big data in the form of pre-trained Convolutional Neural Networks (CNNs) within the framework of Few-Shot Learning (FSL) and ensemble learning. We propose an ensemble approach based on a Prototypical Network (ProtoNet), a powerful method in FSL, integrating various pre-trained CNNs as encoders. This integration enhances the richness of features extracted from medical images. Our approach also includes a combination of class-aware loss and entropy loss to ensure a more precise classification of Alzheimer disease progression levels. The effectiveness of our method was evaluated using two datasets, the Kaggle Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and 99.86%, respectively. The comparison of our results with relevant state-of-the-art studies demonstrated that our approach achieved superior accuracy and highlighted its validity and potential for real-world applications in early Alzheimer disease detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19282v1",
    "published_date": "2025-10-22 06:35:03 UTC",
    "updated_date": "2025-10-22 06:35:03 UTC"
  },
  {
    "arxiv_id": "2510.19270v1",
    "title": "Social World Model-Augmented Mechanism Design Policy Learning",
    "authors": [
      "Xiaoyuan Zhang",
      "Yizhe Huang",
      "Chengdong Ma",
      "Zhixun Chen",
      "Long Ma",
      "Yali Du",
      "Song-Chun Zhu",
      "Yaodong Yang",
      "Xue Feng"
    ],
    "abstract": "Designing adaptive mechanisms to align individual and collective interests remains a central challenge in artificial social intelligence. Existing methods often struggle with modeling heterogeneous agents possessing persistent latent traits (e.g., skills, preferences) and dealing with complex multi-agent system dynamics. These challenges are compounded by the critical need for high sample efficiency due to costly real-world interactions. World Models, by learning to predict environmental dynamics, offer a promising pathway to enhance mechanism design in heterogeneous and complex systems. In this paper, we introduce a novel method named SWM-AP (Social World Model-Augmented Mechanism Design Policy Learning), which learns a social world model hierarchically modeling agents' behavior to enhance mechanism design. Specifically, the social world model infers agents' traits from their interaction trajectories and learns a trait-based model to predict agents' responses to the deployed mechanisms. The mechanism design policy collects extensive training trajectories by interacting with the social world model, while concurrently inferring agents' traits online during real-world interactions to further boost policy learning efficiency. Experiments in diverse settings (tax policy design, team coordination, and facility location) demonstrate that SWM-AP outperforms established model-based and model-free RL baselines in cumulative rewards and sample efficiency.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19270v1",
    "published_date": "2025-10-22 06:01:21 UTC",
    "updated_date": "2025-10-22 06:01:21 UTC"
  },
  {
    "arxiv_id": "2510.19264v1",
    "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
    "authors": [
      "R. Can Aygun",
      "Yehuda Afek",
      "Anat Bremler-Barr",
      "Leonard Kleinrock"
    ],
    "abstract": "With the goal of improving the security of Internet protocols, we seek faster, semi-automatic methods to discover new vulnerabilities in protocols such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers with some DNS knowledge to efficiently uncover vulnerabilities that would otherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM (GPT-o1) that has been trained on a broad corpus of DNS-related sources and previous DDoS attacks to identify potential exploits. In the second stage, a different LLM automatically constructs the corresponding attack configurations using the ReACT approach implemented via LangChain (DNS zone file generation). Finally, in the third stage, we validate the attack's functionality and effectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and rediscovered two recently reported ones that were not included in the LLM's training data. The first new attack employs a bait-and-switch technique to trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving capacity to as little as 6%. The second exploits large DNSSEC encryption algorithms (RSA-4096) with multiple keys, thereby bypassing a recently implemented default RRSet limit. The third leverages ANY-type responses to produce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush, circumvent existing patches, severely degrade resolver query capacity, and impact the latest versions of major DNS resolver implementations.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.19264v1",
    "published_date": "2025-10-22 05:47:41 UTC",
    "updated_date": "2025-10-22 05:47:41 UTC"
  },
  {
    "arxiv_id": "2510.19263v1",
    "title": "An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents",
    "authors": [
      "Wachara Fungwacharakorn",
      "Gauvain Bourgne",
      "Ken Satoh"
    ],
    "abstract": "Precedential constraint is one foundation of case-based reasoning in AI and Law. It generally assumes that the underlying set of precedents must be consistent. To relax this assumption, a generalized notion of the reason model has been introduced. While several argumentative explanation approaches exist for reasoning with precedents based on the traditional consistent reason model, there has been no corresponding argumentative explanation method developed for this generalized reasoning framework accommodating inconsistent precedents. To address this question, this paper examines an extension of the derivation state argumentation framework (DSA-framework) to explain the reasoning according to the generalized notion of the reason model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, extended version for JURIX 2025 submission",
    "pdf_url": "https://arxiv.org/pdf/2510.19263v1",
    "published_date": "2025-10-22 05:46:02 UTC",
    "updated_date": "2025-10-22 05:46:02 UTC"
  },
  {
    "arxiv_id": "2510.19261v1",
    "title": "ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate",
    "authors": [
      "Marianna Molinari",
      "Ilaria Angela Amantea",
      "Marinella Quaranta",
      "Guido Governatori"
    ],
    "abstract": "This study examines the performance of ChatGPT with an experiment in the legal domain. We compare the outcome with it a baseline using regular expressions (Regex), rather than focusing solely on the assessment against human performance. The study reveals that even if ChatGPT has access to the necessary knowledge and competencies, it is unable to assemble them, reason through, in a way that leads to an exhaustive result. This unveils a major limitation of ChatGPT. Intelligence encompasses the ability to break down complex issues and address them according to multiple required competencies, providing a unified and comprehensive solution. In the legal domain, one of the most crucial tasks is reading legal decisions and extracting key passages condensed from principles of law (PoLs), which are then incorporated into subsequent rulings by judges or defense documents by lawyers. In performing this task, artificial intelligence lacks an all-encompassing understanding and reasoning, which makes it inherently limited. Genuine intelligence, remains a uniquely human trait, at least in this particular field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19261v1",
    "published_date": "2025-10-22 05:33:57 UTC",
    "updated_date": "2025-10-22 05:33:57 UTC"
  },
  {
    "arxiv_id": "2510.19257v1",
    "title": "FnRGNN: Distribution-aware Fairness in Graph Neural Network",
    "authors": [
      "Soyoung Park",
      "Sungsu Lim"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel at learning from structured data, yet fairness in regression tasks remains underexplored. Existing approaches mainly target classification and representation-level debiasing, which cannot fully address the continuous nature of node-level regression. We propose FnRGNN, a fairness-aware in-processing framework for GNN-based node regression that applies interventions at three levels: (i) structure-level edge reweighting, (ii) representation-level alignment via MMD, and (iii) prediction-level normalization through Sinkhorn-based distribution matching. This multi-level strategy ensures robust fairness under complex graph topologies. Experiments on four real-world datasets demonstrate that FnRGNN reduces group disparities without sacrificing performance. Code is available at https://github.com/sybeam27/FnRGNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19257v1",
    "published_date": "2025-10-22 05:29:43 UTC",
    "updated_date": "2025-10-22 05:29:43 UTC"
  },
  {
    "arxiv_id": "2510.23620v2",
    "title": "Genotype-Phenotype Integration through Machine Learning and Personalized Gene Regulatory Networks for Cancer Metastasis Prediction",
    "authors": [
      "Jiwei Fu",
      "Chunyu Yang"
    ],
    "abstract": "Metastasis is the leading cause of cancer-related mortality, yet most predictive models rely on shallow architectures and neglect patient-specific regulatory mechanisms. Here, we integrate classical machine learning and deep learning to predict metastatic potential across multiple cancer types. Gene expression profiles from the Cancer Cell Line Encyclopedia were combined with a transcription factor-target prior from DoRothEA, focusing on nine metastasis-associated regulators. After selecting differential genes using the Kruskal-Wallis test, ElasticNet, Random Forest, and XGBoost models were trained for benchmarking. Personalized gene regulatory networks were then constructed using PANDA and LIONESS and analyzed through a graph attention neural network (GATv2) to learn topological and expression-based representations. While XGBoost achieved the highest AUROC (0.7051), the GNN captured non-linear regulatory dependencies at the patient level. These results demonstrate that combining traditional machine learning with graph-based deep learning enables a scalable and interpretable framework for metastasis risk prediction in precision oncology.",
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "primary_category": "q-bio.OT",
    "comment": "39 pages, 14 figures. Preliminary version of ongoing collaborative research; a substantially revised manuscript is in preparation",
    "pdf_url": "https://arxiv.org/pdf/2510.23620v2",
    "published_date": "2025-10-22 05:20:13 UTC",
    "updated_date": "2025-12-25 07:09:37 UTC"
  },
  {
    "arxiv_id": "2510.19245v1",
    "title": "See, Think, Act: Online Shopper Behavior Simulation with VLM Agents",
    "authors": [
      "Yimeng Zhang",
      "Jiri Gesi",
      "Ran Xue",
      "Tian Wang",
      "Ziyi Wang",
      "Yuxuan Lu",
      "Sinong Zhan",
      "Huimin Zeng",
      "Qingjun Cui",
      "Yufan Guo",
      "Jing Huang",
      "Mubarak Shah",
      "Dakuo Wang"
    ],
    "abstract": "LLMs have recently demonstrated strong potential in simulating online shopper behavior. Prior work has improved action prediction by applying SFT on action traces with LLM-generated rationales, and by leveraging RL to further enhance reasoning capabilities. Despite these advances, current approaches rely on text-based inputs and overlook the essential role of visual perception in shaping human decision-making during web GUI interactions. In this paper, we investigate the integration of visual information, specifically webpage screenshots, into behavior simulation via VLMs, leveraging OPeRA dataset. By grounding agent decision-making in both textual and visual modalities, we aim to narrow the gap between synthetic agents and real-world users, thereby enabling more cognitively aligned simulations of online shopping behavior. Specifically, we employ SFT for joint action prediction and rationale generation, conditioning on the full interaction context, which comprises action history, past HTML observations, and the current webpage screenshot. To further enhance reasoning capabilities, we integrate RL with a hierarchical reward structure, scaled by a difficulty-aware factor that prioritizes challenging decision points. Empirically, our studies show that incorporating visual grounding yields substantial gains: the combination of text and image inputs improves exact match accuracy by more than 6% over text-only inputs. These results indicate that multi-modal grounding not only boosts predictive accuracy but also enhances simulation fidelity in visually complex environments, which captures nuances of human attention and decision-making that text-only agents often miss. Finally, we revisit the design space of behavior simulation frameworks, identify key methodological limitations, and propose future research directions toward building efficient and effective human behavior simulators.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19245v1",
    "published_date": "2025-10-22 05:07:14 UTC",
    "updated_date": "2025-10-22 05:07:14 UTC"
  },
  {
    "arxiv_id": "2510.19241v1",
    "title": "SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes",
    "authors": [
      "Xuyuan Xiong",
      "Pedro Chumpitaz-Flores",
      "Kaixun Hua",
      "Cheng Hua"
    ],
    "abstract": "Interpretable reinforcement learning policies are essential for high-stakes decision-making, yet optimizing decision tree policies in Markov Decision Processes (MDPs) remains challenging. We propose SPOT, a novel method for computing decision tree policies, which formulates the optimization problem as a mixed-integer linear program (MILP). To enhance efficiency, we employ a reduced-space branch-and-bound approach that decouples the MDP dynamics from tree-structure constraints, enabling efficient parallel search. This significantly improves runtime and scalability compared to previous methods. Our approach ensures that each iteration yields the optimal decision tree. Experimental results on standard benchmarks demonstrate that SPOT achieves substantial speedup and scales to larger MDPs with a significantly higher number of states. The resulting decision tree policies are interpretable and compact, maintaining transparency without compromising performance. These results demonstrate that our approach simultaneously achieves interpretability and scalability, delivering high-quality policies an order of magnitude faster than existing approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19241v1",
    "published_date": "2025-10-22 04:57:23 UTC",
    "updated_date": "2025-10-22 04:57:23 UTC"
  },
  {
    "arxiv_id": "2510.19212v1",
    "title": "No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence",
    "authors": [
      "Ernest Fokoué"
    ],
    "abstract": "The rapid ascent of artificial intelligence (AI) is often portrayed as a revolution born from computer science and engineering. This narrative, however, obscures a fundamental truth: the theoretical and methodological core of AI is, and has always been, statistical. This paper systematically argues that the field of statistics provides the indispensable foundation for machine learning and modern AI. We deconstruct AI into nine foundational pillars-Inference, Density Estimation, Sequential Learning, Generalization, Representation Learning, Interpretability, Causality, Optimization, and Unification-demonstrating that each is built upon century-old statistical principles. From the inferential frameworks of hypothesis testing and estimation that underpin model evaluation, to the density estimation roots of clustering and generative AI; from the time-series analysis inspiring recurrent networks to the causal models that promise true understanding, we trace an unbroken statistical lineage. While celebrating the computational engines that power modern AI, we contend that statistics provides the brain-the theoretical frameworks, uncertainty quantification, and inferential goals-while computer science provides the brawn-the scalable algorithms and hardware. Recognizing this statistical backbone is not merely an academic exercise, but a necessary step for developing more robust, interpretable, and trustworthy intelligent systems. We issue a call to action for education, research, and practice to re-embrace this statistical foundation. Ignoring these roots risks building a fragile future; embracing them is the path to truly intelligent machines. There is no machine learning without statistical learning; no artificial intelligence without statistical thought.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "primary_category": "stat.ME",
    "comment": "37 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19212v1",
    "published_date": "2025-10-22 03:47:30 UTC",
    "updated_date": "2025-10-22 03:47:30 UTC"
  },
  {
    "arxiv_id": "2510.21830v4",
    "title": "GAPO: Robust Advantage Estimation for Real-World Code LLMs",
    "authors": [
      "Jianqing Zhang",
      "Zhezheng Hao",
      "Wei Xia",
      "Hande Dong",
      "Hong Wang",
      "Chenxing Wei",
      "Yuyan Zhou",
      "Yubin Qi",
      "Qiang Lin",
      "Jian Cao"
    ],
    "abstract": "Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods, such as GRPO, are popular due to their critic-free and normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable noise, leading to distorted advantage computation and increased rollout outliers. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an interval with the highest SNR (Signal to Noise Ratio) per prompt and uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation to reduce noise further. This adaptive Q robustly handles rollout noise while remaining plug-and-play and efficient. We evaluate GAPO on nine instruction-tuned LLMs (3B-14B) using a collected large dataset of 51,844 real-world, history-aware code-editing tasks spanning 10 programming languages. GAPO yields up to 4.35 in-domain (ID) and 5.30 out-of-domain (OOD) exact-match improvements over GRPO and its variant DAPO, while achieving lower clipping ratios and higher GPU throughput. Code: https://github.com/TsingZ0/verl-GAPO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21830v4",
    "published_date": "2025-10-22 03:37:49 UTC",
    "updated_date": "2026-01-08 08:42:56 UTC"
  },
  {
    "arxiv_id": "2510.19205v1",
    "title": "WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation",
    "authors": [
      "Yaoyao Qian",
      "Yuanli Wang",
      "Jinda Zhang",
      "Yun Zong",
      "Meixu Chen",
      "Hanhan Zhou",
      "Jindan Huang",
      "Yifan Zeng",
      "Xinyu Hu",
      "Chan Hee Song",
      "Danqing Zhang"
    ],
    "abstract": "Current evaluation of web agents largely reduces to binary success metrics or conformity to a single reference trajectory, ignoring the structural diversity present in benchmark datasets. We present WebGraphEval, a framework that abstracts trajectories from multiple agents into a unified, weighted action graph. This representation is directly compatible with benchmarks such as WebArena, leveraging leaderboard runs and newly collected trajectories without modifying environments. The framework canonically encodes actions, merges recurring behaviors, and applies structural analyses including reward propagation and success-weighted edge statistics. Evaluations across thousands of trajectories from six web agents show that the graph abstraction captures cross-model regularities, highlights redundancy and inefficiency, and identifies critical decision points overlooked by outcome-based metrics. By framing web interaction as graph-structured data, WebGraphEval establishes a general methodology for multi-path, cross-agent, and efficiency-aware evaluation of web agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Multi-Turn Interactions in Large Language Models",
    "pdf_url": "https://arxiv.org/pdf/2510.19205v1",
    "published_date": "2025-10-22 03:29:25 UTC",
    "updated_date": "2025-10-22 03:29:25 UTC"
  },
  {
    "arxiv_id": "2510.19202v1",
    "title": "An Active Diffusion Neural Network for Graphs",
    "authors": [
      "Mengying Jiang"
    ],
    "abstract": "The analogy to heat diffusion has enhanced our understanding of information flow in graphs and inspired the development of Graph Neural Networks (GNNs). However, most diffusion-based GNNs emulate passive heat diffusion, which still suffers from over-smoothing and limits their ability to capture global graph information. Inspired by the heat death of the universe, which posits that energy distribution becomes uniform over time in a closed system, we recognize that, without external input, node representations in a graph converge to identical feature vectors as diffusion progresses. To address this issue, we propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves active diffusion by integrating multiple external information sources that dynamically influence the diffusion process, effectively overcoming the over-smoothing problem. Furthermore, our approach realizes true infinite diffusion by directly calculating the closed-form solution of the active diffusion iterative formula. This allows nodes to preserve their unique characteristics while efficiently gaining comprehensive insights into the graph's global structure. We evaluate ADGNN against several state-of-the-art GNN models across various graph tasks. The results demonstrate that ADGNN significantly improves both accuracy and efficiency, highlighting its effectiveness in capturing global graph information and maintaining node distinctiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19202v1",
    "published_date": "2025-10-22 03:23:08 UTC",
    "updated_date": "2025-10-22 03:23:08 UTC"
  },
  {
    "arxiv_id": "2510.19195v3",
    "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
    "authors": [
      "Kai Zeng",
      "Zhanqian Wu",
      "Kaixin Xiong",
      "Xiaobao Wei",
      "Xiangyu Guo",
      "Zhenxin Zhu",
      "Kalok Ho",
      "Lijun Zhou",
      "Bohan Zeng",
      "Ming Lu",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Hangjun Ye",
      "Wentao Zhang"
    ],
    "abstract": "Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19195v3",
    "published_date": "2025-10-22 03:02:38 UTC",
    "updated_date": "2025-12-11 06:46:07 UTC"
  },
  {
    "arxiv_id": "2510.19866v1",
    "title": "An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics",
    "authors": [
      "Xincheng Liu"
    ],
    "abstract": "This study evaluates the pedagogical soundness and usability of AI-generated lesson plans across five leading large language models: ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice, three structured prompt frameworks were tested: TAG (Task, Audience, Goal), RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective, Style, Tone, Audience, Response Format).\n  Fifteen lesson plans were generated for a single high-school physics topic, The Electromagnetic Spectrum. The lesson plans were analyzed through four automated computational metrics: (1) readability and linguistic complexity, (2) factual accuracy and hallucination detection, (3) standards and curriculum alignment, and (4) cognitive demand of learning objectives.\n  Results indicate that model selection exerted the strongest influence on linguistic accessibility, with DeepSeek producing the most readable teaching plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).\n  The prompt framework structure most strongly affected the factual accuracy and pedagogical completeness, with the RACE framework yielding the lowest hallucination index and the highest incidental alignment with NGSS curriculum standards. Across all models, the learning objectives in the fifteen lesson plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There were limited higher-order verbs in the learning objectives extracted.\n  Overall, the findings suggest that readability is significantly governed by model design, while instructional reliability and curricular alignment depend more on the prompt framework. The most effective configuration for lesson plans identified in the results was to combine a readability-optimized model with the RACE framework and an explicit checklist of physics concepts, curriculum standards, and higher-order objectives.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.19866v1",
    "published_date": "2025-10-22 02:53:06 UTC",
    "updated_date": "2025-10-22 02:53:06 UTC"
  },
  {
    "arxiv_id": "2510.19183v1",
    "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning",
    "authors": [
      "Fengyuan Sun",
      "Hui Chen",
      "Xinhao Xu",
      "Dandan Zheng",
      "Jingdong Chen",
      "Jun Zhou",
      "Jungong Han",
      "Guiguang Ding"
    ],
    "abstract": "While multi-modal large language models (MLLMs) have made significant progress in recent years, the issue of hallucinations remains a major challenge. To mitigate this phenomenon, existing solutions either introduce additional data for further training or incorporate external or internal information during inference. However, these approaches inevitably introduce extra computational costs. In this paper, we observe that hallucinations in MLLMs are strongly associated with insufficient attention allocated to visual tokens. In particular, the presence of redundant visual tokens disperses the model's attention, preventing it from focusing on the most informative ones. As a result, critical visual cues are often under-attended, which in turn exacerbates the occurrence of hallucinations. Building on this observation, we propose \\textbf{PruneHal}, a training-free, simple yet effective method that leverages adaptive KV cache pruning to enhance the model's focus on critical visual information, thereby mitigating hallucinations. To the best of our knowledge, we are the first to apply token pruning for hallucination mitigation in MLLMs. Notably, our method don't require additional training and incurs nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be seamlessly integrated with different decoding strategies, including those specifically designed for hallucination mitigation. We evaluate PruneHal on several widely used hallucination evaluation benchmarks using four mainstream MLLMs, achieving robust and outstanding results that highlight the effectiveness and superiority of our method. Our code will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19183v1",
    "published_date": "2025-10-22 02:41:07 UTC",
    "updated_date": "2025-10-22 02:41:07 UTC"
  },
  {
    "arxiv_id": "2510.19181v1",
    "title": "Interpretable Question Answering with Knowledge Graphs",
    "authors": [
      "Kartikeya Aneja",
      "Manasvi Srivastava",
      "Subhayan Das",
      "Nagender Aneja"
    ],
    "abstract": "This paper presents a question answering system that operates exclusively on a knowledge graph retrieval without relying on retrieval augmented generation (RAG) with large language models (LLMs). Instead, a small paraphraser model is used to paraphrase the entity relationship edges retrieved from querying the knowledge graph. The proposed pipeline is divided into two main stages. The first stage involves pre-processing a document to generate sets of question-answer (QA) pairs. The second stage converts these QAs into a knowledge graph from which graph-based retrieval is performed using embeddings and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to generate a final answer. This work includes an evaluation using LLM-as-a-judge on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using LLAMA-3.2 and GPT-3.5-Turbo, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19181v1",
    "published_date": "2025-10-22 02:36:35 UTC",
    "updated_date": "2025-10-22 02:36:35 UTC"
  },
  {
    "arxiv_id": "2510.19178v2",
    "title": "Imbalanced Gradients in RL Post-Training of Multi-Task LLMs",
    "authors": [
      "Runzhe Wu",
      "Ankur Samanta",
      "Ayush Jain",
      "Scott Fujimoto",
      "Jeongyeol Kwon",
      "Ben Kretzu",
      "Youliang Yu",
      "Kaveh Hassani",
      "Boris Vidolov",
      "Yonathan Efroni"
    ],
    "abstract": "Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. This approach implicitly assumes that all tasks contribute gradients of similar magnitudes; when this assumption fails, optimization becomes biased toward large-gradient tasks. In this paper, however, we show that this assumption fails in RL post-training: certain tasks produce significantly larger gradients, thus biasing updates toward those tasks. Such gradient imbalance would be justified only if larger gradients implied larger learning gains on the tasks (i.e., larger performance improvements) -- but we find this is not true. Large-gradient tasks can achieve similar or even much lower learning gains than small-gradient ones. Further analyses reveal that these gradient imbalances cannot be explained by typical training statistics such as training rewards or advantages, suggesting that they arise from the inherent differences between tasks. This cautions against naive dataset mixing and calls for future work on principled gradient-level corrections for LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19178v2",
    "published_date": "2025-10-22 02:35:27 UTC",
    "updated_date": "2025-10-26 15:22:58 UTC"
  },
  {
    "arxiv_id": "2510.19176v1",
    "title": "The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models",
    "authors": [
      "Yuqiao Tan",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Reasoning models have demonstrated exceptional performance in tasks such as mathematics and logical reasoning, primarily due to their ability to engage in step-by-step thinking during the reasoning process. However, this often leads to overthinking, resulting in unnecessary computational overhead. To address this issue, Mode Selection aims to automatically decide between Long-CoT (Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking mode. Simultaneously, Early Exit determines the optimal stopping point during the iterative reasoning process. Both methods seek to reduce the computational burden. In this paper, we first identify Mode Selection as a more challenging variant of the Early Exit problem, as they share similar objectives but differ in decision timing. While Early Exit focuses on determining the best stopping point for concise reasoning at inference time, Mode Selection must make this decision at the beginning of the reasoning process, relying on pre-defined fake thoughts without engaging in an explicit reasoning process, referred to as zero-step thinking. Through empirical studies on nine baselines, we observe that prompt-based approaches often fail due to their limited classification capabilities when provided with minimal hand-crafted information. In contrast, approaches that leverage internal information generally perform better across most scenarios but still exhibit issues with stability. Our findings indicate that existing methods relying solely on the information provided by models are insufficient for effectively addressing Mode Selection in scenarios with limited information, highlighting the ongoing challenges of this task. Our code is available at https://github.com/Trae1ounG/Zero_Step_Thinking.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS'25 Efficient Reasoning Workshop",
    "pdf_url": "https://arxiv.org/pdf/2510.19176v1",
    "published_date": "2025-10-22 02:28:10 UTC",
    "updated_date": "2025-10-22 02:28:10 UTC"
  },
  {
    "arxiv_id": "2510.19173v1",
    "title": "News-Aware Direct Reinforcement Trading for Financial Markets",
    "authors": [
      "Qing-Yu Lan",
      "Zhan-He Wang",
      "Jun-Qian Jiang",
      "Yu-Tong Wang",
      "Yun-Song Piao"
    ],
    "abstract": "The financial market is known to be highly sensitive to news. Therefore, effectively incorporating news data into quantitative trading remains an important challenge. Existing approaches typically rely on manually designed rules and/or handcrafted features. In this work, we directly use the news sentiment scores derived from large language models, together with raw price and volume data, as observable inputs for reinforcement learning. These inputs are processed by sequence models such as recurrent neural networks or Transformers to make end-to-end trading decisions. We conduct experiments using the cryptocurrency market as an example and evaluate two representative reinforcement learning algorithms, namely Double Deep Q-Network (DDQN) and Group Relative Policy Optimization (GRPO). The results demonstrate that our news-aware approach, which does not depend on handcrafted features or manually designed rules, can achieve performance superior to market benchmarks. We further highlight the critical role of time-series information in this process.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.CP",
    "comment": "9 pages, 4 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.19173v1",
    "published_date": "2025-10-22 02:17:03 UTC",
    "updated_date": "2025-10-22 02:17:03 UTC"
  },
  {
    "arxiv_id": "2510.19172v2",
    "title": "When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA",
    "authors": [
      "Nishanth Sridhar Nakshatri",
      "Shamik Roy",
      "Manoj Ghuhan Arivazhagan",
      "Hanhan Zhou",
      "Vinayshekhar Bannihatti Kumar",
      "Rashmi Gangadharaiah"
    ],
    "abstract": "LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under submission",
    "pdf_url": "https://arxiv.org/pdf/2510.19172v2",
    "published_date": "2025-10-22 02:12:32 UTC",
    "updated_date": "2025-11-15 20:44:16 UTC"
  },
  {
    "arxiv_id": "2510.21827v1",
    "title": "Precise classification of low quality G-banded Chromosome Images by reliability metrics and data pruning classifier",
    "authors": [
      "Mojtaba Moattari"
    ],
    "abstract": "In the last decade, due to high resolution cameras and accurate meta-phase analyzes, the accuracy of chromosome classification has improved substantially. However, current Karyotyping systems demand large number of high quality train data to have an adequately plausible Precision per each chromosome. Such provision of high quality train data with accurate devices are not yet accomplished in some out-reached pathological laboratories. To prevent false positive detections in low-cost systems and low-quality images settings, this paper improves the classification Precision of chromosomes using proposed reliability thresholding metrics and deliberately engineered features. The proposed method has been evaluated using a variation of deep Alex-Net neural network, SVM, K Nearest-Neighbors, and their cascade pipelines to an automated filtering of semi-straight chromosome. The classification results have highly improved over 90% for the chromosomes with more common defections and translocations. Furthermore, a comparative analysis over the proposed thresholding metrics has been conducted and the best metric is bolded with its salient characteristics. The high Precision results provided for a very low-quality G-banding database verifies suitability of the proposed metrics and pruning method for Karyotyping facilities in poor countries and lowbudget pathological laboratories.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21827v1",
    "published_date": "2025-10-22 02:05:27 UTC",
    "updated_date": "2025-10-22 02:05:27 UTC"
  },
  {
    "arxiv_id": "2510.19150v1",
    "title": "X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning",
    "authors": [
      "Yunzhe Wang",
      "Soham Hans",
      "Volkan Ustun"
    ],
    "abstract": "Human team tactics emerge from each player's individual perspective and their ability to anticipate, interpret, and adapt to teammates' intentions. While advances in video understanding have improved the modeling of team interactions in sports, most existing work relies on third-person broadcast views and overlooks the synchronous, egocentric nature of multi-agent learning. We introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay footage from 45 professional-level matches of the popular e-sports game Counter-Strike 2, designed to facilitate research on multi-agent decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric video streams that synchronously capture all players' first-person perspectives along with state-action trajectories. Building on this resource, we propose Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric visual streams to foster team-level tactical situational awareness from an individual's perspective. We evaluate CECL on a teammate-opponent location prediction task, demonstrating its effectiveness in enhancing an agent's ability to infer both teammate and opponent positions from a single first-person view using state-of-the-art video encoders. Together, X-Ego-CS and CECL establish a foundation for cross-egocentric multi-agent benchmarking in esports. More broadly, our work positions gameplay understanding as a testbed for multi-agent modeling and tactical learning, with implications for spatiotemporal reasoning and human-AI teaming in both virtual and real-world domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.19150v1",
    "published_date": "2025-10-22 00:48:35 UTC",
    "updated_date": "2025-10-22 00:48:35 UTC"
  },
  {
    "arxiv_id": "2510.19139v2",
    "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist",
    "authors": [
      "Sohyeon Jeon",
      "Hyung-Chul Lee"
    ],
    "abstract": "Despite the rapid expansion of Large Language Models (LLMs) in healthcare, robust and explainable evaluation of their ability to assess clinical trial reporting according to CONSORT standards remains an open challenge. In particular, uncertainty calibration and metacognitive reliability of LLM reasoning are poorly understood and underexplored in medical automation. This study applies a behavioral and metacognitive analytic approach using an expert-validated dataset, systematically comparing two representative LLMs - one general and one domain-specialized - across three prompt strategies. We analyze both cognitive adaptation and calibration error using metrics: Expected Calibration Error (ECE) and a baseline-normalized Relative Calibration Error (RCE) that enables reliable cross-model comparison. Our results reveal pronounced miscalibration and overconfidence in both models, especially under clinical role-playing conditions, with calibration error persisting above clinically relevant thresholds. These findings underscore the need for improved calibration, transparent code, and strategic prompt engineering to develop reliable and explainable medical AI.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19139v2",
    "published_date": "2025-10-22 00:15:02 UTC",
    "updated_date": "2025-10-26 01:38:30 UTC"
  },
  {
    "arxiv_id": "2510.19138v1",
    "title": "InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding",
    "authors": [
      "Ziyi Zhang",
      "Shaogang Ren",
      "Xiaoning Qian",
      "Nick Duffield"
    ],
    "abstract": "Granger causality is widely used for causal structure discovery in complex systems from multivariate time series data. Traditional Granger causality tests based on linear models often fail to detect even mild non-linear causal relationships. Therefore, numerous recent studies have investigated non-linear Granger causality methods, achieving improved performance. However, these methods often rely on two key assumptions: causal sufficiency and known interventional targets. Causal sufficiency assumes the absence of latent confounders, yet their presence can introduce spurious correlations. Moreover, real-world time series data usually come from heterogeneous environments, without prior knowledge of interventions. Therefore, in practice, it is difficult to distinguish intervened environments from non-intervened ones, and even harder to identify which variables or timesteps are affected. To address these challenges, we propose Invariant Granger Causality (InvarGC), which leverages cross-environment heterogeneity to mitigate the effects of latent confounding and to distinguish intervened from non-intervened environments with edge-level granularity, thereby recovering invariant causal relations. In addition, we establish the identifiability under these conditions. Extensive experiments on both synthetic and real-world datasets demonstrate the competitive performance of our approach compared to state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.19138v1",
    "published_date": "2025-10-22 00:04:49 UTC",
    "updated_date": "2025-10-22 00:04:49 UTC"
  }
]
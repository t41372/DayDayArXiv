[
  {
    "arxiv_id": "2408.14700v1",
    "title": "Artificial Intelligence in Landscape Architecture: A Survey",
    "authors": [
      "Yue Xing",
      "Wensheng Gan",
      "Qidi Chen"
    ],
    "abstract": "The development history of landscape architecture (LA) reflects the human\npursuit of environmental beautification and ecological balance. With the\nadvancement of artificial intelligence (AI) technologies that simulate and\nextend human intelligence, immense opportunities have been provided for LA,\noffering scientific and technological support throughout the entire workflow.\nIn this article, we comprehensively review the applications of AI technology in\nthe field of LA. First, we introduce the many potential benefits that AI brings\nto the design, planning, and management aspects of LA. Secondly, we discuss how\nAI can assist the LA field in solving its current development problems,\nincluding urbanization, environmental degradation and ecological decline,\nirrational planning, insufficient management and maintenance, and lack of\npublic participation. Furthermore, we summarize the key technologies and\npractical cases of applying AI in the LA domain, from design assistance to\nintelligent management, all of which provide innovative solutions for the\nplanning, design, and maintenance of LA. Finally, we look ahead to the problems\nand opportunities in LA, emphasizing the need to combine human expertise and\njudgment for rational decision-making. This article provides both theoretical\nand practical guidance for LA designers, researchers, and technology\ndevelopers. The successful integration of AI technology into LA holds great\npromise for enhancing the field's capabilities and achieving more sustainable,\nefficient, and user-friendly outcomes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.14700v1",
    "published_date": "2024-08-26 23:54:17 UTC",
    "updated_date": "2024-08-26 23:54:17 UTC"
  },
  {
    "arxiv_id": "2408.14698v2",
    "title": "Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express",
    "authors": [
      "Cherag Aroraa",
      "Tracy Holloway King",
      "Jayant Kumar",
      "Yi Lu",
      "Sanat Sharma",
      "Arvind Srikantan",
      "David Uvalle",
      "Josep Valls-Vargas",
      "Harsha Vardhan"
    ],
    "abstract": "As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "CIKM 2024 (International Conference on Information and Knowledge\n  Management), Multimodal Search and Recommendations Workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.14698v2",
    "published_date": "2024-08-26 23:52:27 UTC",
    "updated_date": "2024-08-29 15:14:48 UTC"
  },
  {
    "arxiv_id": "2408.14690v3",
    "title": "Training-Free Activation Sparsity in Large Language Models",
    "authors": [
      "James Liu",
      "Pragaash Ponnusamy",
      "Tianle Cai",
      "Han Guo",
      "Yoon Kim",
      "Ben Athiwaratkun"
    ],
    "abstract": "Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Rev. 2: ICLR 2025 Acceptance (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2408.14690v3",
    "published_date": "2024-08-26 23:30:15 UTC",
    "updated_date": "2025-02-25 21:00:50 UTC"
  },
  {
    "arxiv_id": "2408.14678v1",
    "title": "Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems",
    "authors": [
      "Nikhil Khani",
      "Shuo Yang",
      "Aniruddh Nath",
      "Yang Liu",
      "Pendo Abbo",
      "Li Wei",
      "Shawn Andrews",
      "Maciej Kula",
      "Jarrod Kahn",
      "Zhe Zhao",
      "Lichan Hong",
      "Ed Chi"
    ],
    "abstract": "Knowledge Distillation (KD) is a powerful approach for compressing a large\nmodel into a smaller, more efficient model, particularly beneficial for\nlatency-sensitive applications like recommender systems. However, current KD\nresearch predominantly focuses on Computer Vision (CV) and NLP tasks,\noverlooking unique data characteristics and challenges inherent to recommender\nsystems. This paper addresses these overlooked challenges, specifically: (1)\nmitigating data distribution shifts between teacher and student models, (2)\nefficiently identifying optimal teacher configurations within time and\nbudgetary constraints, and (3) enabling computationally efficient and rapid\nsharing of teacher labels to support multiple students. We present a robust KD\nsystem developed and rigorously evaluated on multiple large-scale personalized\nvideo recommendation systems within Google. Our live experiment results\ndemonstrate significant improvements in student model performance while\nensuring consistent and reliable generation of high quality teacher labels from\na continuous data stream of data.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14678v1",
    "published_date": "2024-08-26 23:01:48 UTC",
    "updated_date": "2024-08-26 23:01:48 UTC"
  },
  {
    "arxiv_id": "2408.14658v1",
    "title": "KGPrune: a Web Application to Extract Subgraphs of Interest from Wikidata with Analogical Pruning",
    "authors": [
      "Pierre Monnin",
      "Cherif-Hassan Nousradine",
      "Lucas Jarnac",
      "Laurel Zuckerman",
      "Miguel Couceiro"
    ],
    "abstract": "Knowledge graphs (KGs) have become ubiquitous publicly available knowledge\nsources, and are nowadays covering an ever increasing array of domains.\nHowever, not all knowledge represented is useful or pertaining when considering\na new application or specific task. Also, due to their increasing size,\nhandling large KGs in their entirety entails scalability issues. These two\naspects asks for efficient methods to extract subgraphs of interest from\nexisting KGs. To this aim, we introduce KGPrune, a Web Application that, given\nseed entities of interest and properties to traverse, extracts their\nneighboring subgraphs from Wikidata. To avoid topical drift, KGPrune relies on\na frugal pruning algorithm based on analogical reasoning to only keep relevant\nneighbors while pruning irrelevant ones. The interest of KGPrune is illustrated\nby two concrete applications, namely, bootstrapping an enterprise KG and\nextracting knowledge related to looted artworks.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as a demo paper at ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14658v1",
    "published_date": "2024-08-26 21:47:49 UTC",
    "updated_date": "2024-08-26 21:47:49 UTC"
  },
  {
    "arxiv_id": "2408.14649v2",
    "title": "Bidirectional Emergent Language in Situated Environments",
    "authors": [
      "Cornelius Wolff",
      "Julius Mayer",
      "Elia Bruni",
      "Xenia Ohmer"
    ],
    "abstract": "Emergent language research has made significant progress in recent years, but\nstill largely fails to explore how communication emerges in more complex and\nsituated multi-agent systems. Existing setups often employ a reference game,\nwhich limits the range of language emergence phenomena that can be studied, as\nthe game consists of a single, purely language-based interaction between the\nagents. In this paper, we address these limitations and explore the emergence\nand utility of token-based communication in open-ended multi-agent\nenvironments, where situated agents interact with the environment through\nmovement and communication over multiple time-steps. Specifically, we introduce\ntwo novel cooperative environments: Multi-Agent Pong and Collectors. These\nenvironments are interesting because optimal performance requires the emergence\nof a communication protocol, but moderate success can be achieved without one.\nBy employing various methods from explainable AI research, such as saliency\nmaps, perturbation, and diagnostic classifiers, we are able to track and\ninterpret the agents' language channel use over time. We find that the emerging\ncommunication is sparse, with the agents only generating meaningful messages\nand acting upon incoming messages in states where they cannot succeed without\ncoordination.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures, 4 tables, preprint",
    "pdf_url": "http://arxiv.org/pdf/2408.14649v2",
    "published_date": "2024-08-26 21:25:44 UTC",
    "updated_date": "2024-10-17 10:55:35 UTC"
  },
  {
    "arxiv_id": "2408.14644v1",
    "title": "Visions of Destruction: Exploring a Potential of Generative AI in Interactive Art",
    "authors": [
      "Mar Canet Sola",
      "Varvara Guljajeva"
    ],
    "abstract": "This paper explores the potential of generative AI within interactive art,\nemploying a practice-based research approach. It presents the interactive\nartwork \"Visions of Destruction\" as a detailed case study, highlighting its\ninnovative use of generative AI to create a dynamic, audience-responsive\nexperience. This artwork applies gaze-based interaction to dynamically alter\ndigital landscapes, symbolizing the impact of human activities on the\nenvironment by generating contemporary collages created with AI, trained on\ndata about human damage to nature, and guided by audience interaction. The\ntransformation of pristine natural scenes into human-made and industrialized\nlandscapes through viewer interaction serves as a stark reminder of\nenvironmental degradation. The paper thoroughly explores the technical\nchallenges and artistic innovations involved in creating such an interactive\nart installation, emphasizing the potential of generative AI to revolutionize\nartistic expression, audience engagement, and especially the opportunities for\nthe interactive art field. It offers insights into the conceptual framework\nbehind the artwork, aiming to evoke a deeper understanding and reflection on\nthe Anthropocene era and human-induced climate change. This study contributes\nsignificantly to the field of creative AI and interactive art, blending\ntechnology and environmental consciousness in a compelling, thought-provoking\nmanner.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2; J.5"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14644v1",
    "published_date": "2024-08-26 21:20:45 UTC",
    "updated_date": "2024-08-26 21:20:45 UTC"
  },
  {
    "arxiv_id": "2408.14640v1",
    "title": "Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game",
    "authors": [
      "Jason T. Isa",
      "Bohan Wu",
      "Qirui Wang",
      "Yilin Zhang",
      "Samuel A. Burden",
      "Lillian J. Ratliff",
      "Benjamin J. Chasnov"
    ],
    "abstract": "As interactions between humans and AI become more prevalent, it is critical\nto have better predictors of human behavior in these interactions. We\ninvestigated how changes in the AI's adaptive algorithm impact behavior\npredictions in two-player continuous games. In our experiments, the AI adapted\nits actions using a gradient descent algorithm under different adaptation rates\nwhile human participants were provided cost feedback. The cost feedback was\nprovided by one of two types of visual displays: (a) cost at the current joint\naction vector, or (b) cost in a local neighborhood of the current joint action\nvector. Our results demonstrate that AI adaptation rate can significantly\naffect human behavior, having the ability to shift the outcome between two game\ntheoretic equilibrium. We observed that slow adaptation rates shift the outcome\ntowards the Nash equilibrium, while fast rates shift the outcome towards the\nhuman-led Stackelberg equilibrium. The addition of localized cost information\nhad the effect of shifting outcomes towards Nash, compared to the outcomes from\ncost information at only the current joint action vector. Future work will\ninvestigate other effects that influence the convergence of gradient descent\ngames.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14640v1",
    "published_date": "2024-08-26 21:08:21 UTC",
    "updated_date": "2024-08-26 21:08:21 UTC"
  },
  {
    "arxiv_id": "2408.14626v1",
    "title": "Hybrid Deep Convolutional Neural Networks Combined with Autoencoders And Augmented Data To Predict The Look-Up Table 2006",
    "authors": [
      "Messaoud Djeddou",
      "Aouatef Hellal",
      "Ibrahim A. Hameed",
      "Xingang Zhao",
      "Djehad Al Dallal"
    ],
    "abstract": "This study explores the development of a hybrid deep convolutional neural\nnetwork (DCNN) model enhanced by autoencoders and data augmentation techniques\nto predict critical heat flux (CHF) with high accuracy. By augmenting the\noriginal input features using three different autoencoder configurations, the\nmodel's predictive capabilities were significantly improved. The hybrid models\nwere trained and tested on a dataset of 7225 samples, with performance metrics\nincluding the coefficient of determination (R2), Nash-Sutcliffe efficiency\n(NSE), mean absolute error (MAE), and normalized root-mean-squared error\n(NRMSE) used for evaluation. Among the tested models, the DCNN_3F-A2\nconfiguration demonstrated the highest accuracy, achieving an R2 of 0.9908\nduring training and 0.9826 during testing, outperforming the base model and\nother augmented versions. These results suggest that the proposed hybrid\napproach, combining deep learning with feature augmentation, offers a robust\nsolution for CHF prediction, with the potential to generalize across a wider\nrange of conditions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14626v1",
    "published_date": "2024-08-26 20:45:07 UTC",
    "updated_date": "2024-08-26 20:45:07 UTC"
  },
  {
    "arxiv_id": "2408.14597v1",
    "title": "On Centralized Critics in Multi-Agent Reinforcement Learning",
    "authors": [
      "Xueguang Lyu",
      "Andrea Baisero",
      "Yuchen Xiao",
      "Brett Daley",
      "Christopher Amato"
    ],
    "abstract": "Centralized Training for Decentralized Execution where agents are trained\noffline in a centralized fashion and execute online in a decentralized manner,\nhas become a popular approach in Multi-Agent Reinforcement Learning (MARL). In\nparticular, it has become popular to develop actor-critic methods that train\ndecentralized actors with a centralized critic where the centralized critic is\nallowed access global information of the entire system, including the true\nsystem state. Such centralized critics are possible given offline information\nand are not used for online execution. While these methods perform well in a\nnumber of domains and have become a de facto standard in MARL, using a\ncentralized critic in this context has yet to be sufficiently analyzed\ntheoretically or empirically. In this paper, we therefore formally analyze\ncentralized and decentralized critic approaches, and analyze the effect of\nusing state-based critics in partially observable environments. We derive\ntheories contrary to the common intuition: critic centralization is not\nstrictly beneficial, and using state values can be harmful. We further prove\nthat, in particular, state-based critics can introduce unexpected bias and\nvariance compared to history-based critics. Finally, we demonstrate how the\ntheory applies in practice by comparing different forms of critics on a wide\nrange of common multi-agent benchmarks. The experiments show practical issues\nsuch as the difficulty of representation learning with partial observability,\nwhich highlights why the theoretical problems are often overlooked in the\nliterature.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14597v1",
    "published_date": "2024-08-26 19:27:06 UTC",
    "updated_date": "2024-08-26 19:27:06 UTC"
  },
  {
    "arxiv_id": "2408.14593v1",
    "title": "How to build trust in answers given by Generative AI for specific, and vague, financial questions",
    "authors": [
      "Alex Zarifis",
      "Xusen Cheng"
    ],
    "abstract": "Purpose: Generative artificial intelligence (GenAI) has progressed in its\nability and has seen explosive growth in adoption. However, the consumer's\nperspective on its use, particularly in specific scenarios such as financial\nadvice, is unclear. This research develops a model of how to build trust in the\nadvice given by GenAI when answering financial questions.\nDesign/methodology/approach: The model is tested with survey data using\nstructural equation modelling (SEM) and multi-group analysis (MGA). The MGA\ncompares two scenarios, one where the consumer makes a specific question and\none where a vague question is made. Findings: This research identifies that\nbuilding trust for consumers is different when they ask a specific financial\nquestion in comparison to a vague one. Humanness has a different effect in the\ntwo scenarios. When a financial question is specific, human-like interaction\ndoes not strengthen trust, while (1) when a question is vague, humanness builds\ntrust. The four ways to build trust in both scenarios are (2) human oversight\nand being in the loop, (3) transparency and control, (4) accuracy and\nusefulness and finally (5) ease of use and support. Originality/value: This\nresearch contributes to a better understanding of the consumer's perspective\nwhen using GenAI for financial questions and highlights the importance of\nunderstanding GenAI in specific contexts from specific stakeholders.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14593v1",
    "published_date": "2024-08-26 19:26:48 UTC",
    "updated_date": "2024-08-26 19:26:48 UTC"
  },
  {
    "arxiv_id": "2408.14584v1",
    "title": "DIAGen: Diverse Image Augmentation with Generative Models",
    "authors": [
      "Tobias Lingenberg",
      "Markus Reuter",
      "Gopika Sudhakaran",
      "Dominik Gojny",
      "Stefan Roth",
      "Simone Schaub-Meyer"
    ],
    "abstract": "Simple data augmentation techniques, such as rotations and flips, are widely\nused to enhance the generalization power of computer vision models. However,\nthese techniques often fail to modify high-level semantic attributes of a\nclass. To address this limitation, researchers have explored generative\naugmentation methods like the recently proposed DA-Fusion. Despite some\nprogress, the variations are still largely limited to textural changes, thus\nfalling short on aspects like varied viewpoints, environment, weather\nconditions, or even class-level semantic attributes (eg, variations in a dog's\nbreed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.\nFirst, we apply Gaussian noise to the embeddings of an object learned with\nTextual Inversion to diversify generations using a pre-trained diffusion\nmodel's knowledge. Second, we exploit the general knowledge of a text-to-text\ngenerative model to guide the image generation of the diffusion model with\nvaried class-specific prompts. Finally, we introduce a weighting mechanism to\nmitigate the impact of poorly generated samples. Experimental results across\nvarious datasets show that DIAGen not only enhances semantic diversity but also\nimproves the performance of subsequent classifiers. The advantages of DIAGen\nover standard augmentations and the DA-Fusion baseline are particularly\npronounced with out-of-distribution samples.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in GCPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14584v1",
    "published_date": "2024-08-26 19:09:13 UTC",
    "updated_date": "2024-08-26 19:09:13 UTC"
  },
  {
    "arxiv_id": "2409.06723v1",
    "title": "Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI",
    "authors": [
      "Yukyeong Song",
      "Jinhee Kim",
      "Wanli Xing",
      "Zifeng Liu",
      "Chenglu Li",
      "Hyunju Oh"
    ],
    "abstract": "While mathematical creative writing can potentially engage students in\nexpressing mathematical ideas in an imaginative way, some elementary school-age\nstudents struggle in this process. Generative AI (GenAI) offers possibilities\nfor supporting creative writing activities, such as providing story generation.\nHowever, the design of GenAI-powered learning technologies requires careful\nconsideration of the technology reception in the actual classrooms. This study\nexplores students' and teachers' perceptions of creative mathematical writing\nwith the developed GenAI-powered technology. The study adopted a qualitative\nthematic analysis of the interviews, triangulated with open-ended survey\nresponses and classroom observation of 79 elementary school students, resulting\nin six themes and 19 subthemes. This study contributes by investigating the\nlived experience of GenAI-supported learning and the design considerations for\nGenAI-powered learning technologies and instructions.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06723v1",
    "published_date": "2024-08-26 19:04:08 UTC",
    "updated_date": "2024-08-26 19:04:08 UTC"
  },
  {
    "arxiv_id": "2409.06721v1",
    "title": "Students' Perceived Roles, Opportunities, and Challenges of a Generative AI-powered Teachable Agent: A Case of Middle School Math Class",
    "authors": [
      "Yukyeong Song",
      "Jinhee Kim",
      "Zifeng Liu",
      "Chenglu Li",
      "Wanli Xing"
    ],
    "abstract": "Ongoing advancements in Generative AI (GenAI) have boosted the potential of\napplying long-standing learning-by-teaching practices in the form of a\nteachable agent (TA). Despite the recognized roles and opportunities of TAs,\nless is known about how GenAI could create synergy or introduce challenges in\nTAs and how students perceived the application of GenAI in TAs. This study\nexplored middle school students perceived roles, benefits, and challenges of\nGenAI-powered TAs in an authentic mathematics classroom. Through classroom\nobservation, focus-group interviews, and open-ended surveys of 108 sixth-grade\nstudents, we found that students expected the GenAI-powered TA to serve as a\nlearning companion, facilitator, and collaborative problem-solver. Students\nalso expressed the benefits and challenges of GenAI-powered TAs. This study\nprovides implications for the design of educational AI and AI-assisted\ninstruction.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06721v1",
    "published_date": "2024-08-26 18:54:20 UTC",
    "updated_date": "2024-08-26 18:54:20 UTC"
  },
  {
    "arxiv_id": "2408.14575v4",
    "title": "EVINCE: Optimizing Multi-LLM Dialogues Using Conditional Statistics and Information Theory",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "EVINCE (Entropy and Variation IN Conditional Exchanges) is a novel framework\nfor optimizing multi-LLM dialogues using conditional statistics and information\ntheory. It addresses limitations in multi-agent debate (MAS) frameworks, where\nmultiple LLMs ``chat'' without behavior modulation or mutual information\nquality assessment. Using dual entropy optimization to balance perspective\ndiversity and prior knowledge, $\\EVINCE$ provides quantitative tools to\ndynamically regulate LLM linguistic behaviors. When mutual information is low\nand both cross-entropy and Wasserstein distance are high, EVINCE promotes\ncontentious dialogues to expose diverse perspectives and uncover\ninconsistencies. Conversely, as cross-entropy decreases and mutual information\nstabilizes, it transitions discussions into a conciliatory phase, encouraging\ncompromise and acknowledgment of valid points. Using information-theoretic\nmetrics and optimizing mutual information, $\\EVINCE$ emerges as a structured\nand highly effective framework for multi-LLM collaboration.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 9 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:2405.15808",
    "pdf_url": "http://arxiv.org/pdf/2408.14575v4",
    "published_date": "2024-08-26 18:48:51 UTC",
    "updated_date": "2025-01-29 20:48:59 UTC"
  },
  {
    "arxiv_id": "2408.14572v1",
    "title": "CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation",
    "authors": [
      "Muhammad Fawi"
    ],
    "abstract": "This paper introduces CURLoRA, a novel approach to fine-tuning large language\nmodels (LLMs) that leverages CUR matrix decomposition in the context of\nLow-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM\nfine-tuning: mitigating catastrophic forgetting during continual learning and\nreducing the number of trainable parameters. We propose a unique modification\nto the CUR decomposition process, utilizing inverted probabilities for column\nand row selection which acts as an implicit regularization, and initializing\nthe $U$ matrix as a zero matrix, and only fine-tuning it. We demonstrate\nthrough experiments on multiple datasets that CURLoRA outperforms standard LoRA\nin mitigating catastrophic forgetting. It maintains model stability and\nperformance across tasks while significantly reducing the number of trainable\nparameters. Our results show that CURLoRA achieves very good and stable task\naccuracy while maintaining base model's perplexity scores fixed compared to\nLoRA upon continual fine-tuning, particularly in scenarios with limited data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at https://github.com/MNoorFawi/curlora",
    "pdf_url": "http://arxiv.org/pdf/2408.14572v1",
    "published_date": "2024-08-26 18:42:59 UTC",
    "updated_date": "2024-08-26 18:42:59 UTC"
  },
  {
    "arxiv_id": "2408.14568v1",
    "title": "Improving Clinical Note Generation from Complex Doctor-Patient Conversation",
    "authors": [
      "Yizhan Li",
      "Sifan Wu",
      "Christopher Smith",
      "Thomas Lo",
      "Bang Liu"
    ],
    "abstract": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14568v1",
    "published_date": "2024-08-26 18:39:31 UTC",
    "updated_date": "2024-08-26 18:39:31 UTC"
  },
  {
    "arxiv_id": "2408.14562v1",
    "title": "A Survey of Camouflaged Object Detection and Beyond",
    "authors": [
      "Fengyang Xiao",
      "Sujie Hu",
      "Yuqi Shen",
      "Chengyu Fang",
      "Jinfa Huang",
      "Chunming He",
      "Longxiang Tang",
      "Ziyun Yang",
      "Xiu Li"
    ],
    "abstract": "Camouflaged Object Detection (COD) refers to the task of identifying and\nsegmenting objects that blend seamlessly into their surroundings, posing a\nsignificant challenge for computer vision systems. In recent years, COD has\ngarnered widespread attention due to its potential applications in\nsurveillance, wildlife conservation, autonomous systems, and more. While\nseveral surveys on COD exist, they often have limitations in terms of the\nnumber and scope of papers covered, particularly regarding the rapid\nadvancements made in the field since mid-2023. To address this void, we present\nthe most comprehensive review of COD to date, encompassing both theoretical\nframeworks and practical contributions to the field. This paper explores\nvarious COD methods across four domains, including both image-level and\nvideo-level solutions, from the perspectives of traditional and deep learning\napproaches. We thoroughly investigate the correlations between COD and other\ncamouflaged scenario methods, thereby laying the theoretical foundation for\nsubsequent analyses. Beyond object-level detection, we also summarize extended\nmethods for instance-level tasks, including camouflaged instance segmentation,\ncounting, and ranking. Additionally, we provide an overview of commonly used\nbenchmarks and evaluation metrics in COD tasks, conducting a comprehensive\nevaluation of deep learning-based techniques in both image and video domains,\nconsidering both qualitative and quantitative performance. Finally, we discuss\nthe limitations of current COD models and propose 9 promising directions for\nfuture research, focusing on addressing inherent challenges and exploring\nnovel, meaningful technologies. For those interested, a curated list of\nCOD-related techniques, datasets, and additional resources can be found at\nhttps://github.com/ChunmingHe/awesome-concealed-object-segmentation",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 10 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.14562v1",
    "published_date": "2024-08-26 18:23:22 UTC",
    "updated_date": "2024-08-26 18:23:22 UTC"
  },
  {
    "arxiv_id": "2408.14547v1",
    "title": "Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization",
    "authors": [
      "Nicholas Moratelli",
      "Davide Caffagni",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "The conventional training approach for image captioning involves pre-training\na network using teacher forcing and subsequent fine-tuning with Self-Critical\nSequence Training to maximize hand-crafted captioning metrics. However, when\nattempting to optimize modern and higher-quality metrics like CLIP-Score and\nPAC-Score, this training method often encounters instability and fails to\nacquire the genuine descriptive capabilities needed to produce fluent and\ninformative captions. In this paper, we propose a new training paradigm termed\nDirect CLIP-Based Optimization (DiCO). Our approach jointly learns and\noptimizes a reward model that is distilled from a learnable captioning\nevaluator with high human correlation. This is done by solving a weighted\nclassification problem directly inside the captioner. At the same time, DiCO\nprevents divergence from the original model, ensuring that fluency is\nmaintained. DiCO not only exhibits improved stability and enhanced quality in\nthe generated captions but also aligns more closely with human preferences\ncompared to existing methods, especially in modern metrics. Additionally, it\nmaintains competitive performance in traditional metrics. Our source code and\ntrained models are publicly available at https://github.com/aimagelab/DiCO.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "BMVC 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14547v1",
    "published_date": "2024-08-26 18:00:33 UTC",
    "updated_date": "2024-08-26 18:00:33 UTC"
  },
  {
    "arxiv_id": "2408.14472v1",
    "title": "Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning",
    "authors": [
      "Xinyang Gu",
      "Yen-Jen Wang",
      "Xiang Zhu",
      "Chengming Shi",
      "Yanjiang Guo",
      "Yichen Liu",
      "Jianyu Chen"
    ],
    "abstract": "Humanoid robots, with their human-like skeletal structure, are especially\nsuited for tasks in human-centric environments. However, this structure is\naccompanied by additional challenges in locomotion controller design,\nespecially in complex real-world environments. As a result, existing humanoid\nrobots are limited to relatively simple terrains, either with model-based\ncontrol or model-free reinforcement learning. In this work, we introduce\nDenoising World Model Learning (DWL), an end-to-end reinforcement learning\nframework for humanoid locomotion control, which demonstrates the world's first\nhumanoid robot to master real-world challenging terrains such as snowy and\ninclined land in the wild, up and down stairs, and extremely uneven terrains.\nAll scenarios run the same learned neural network with zero-shot sim-to-real\ntransfer, indicating the superior robustness and generalization capability of\nthe proposed method.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Robotics: Science and Systems (RSS), 2024. (Best Paper Award\n  Finalist)",
    "pdf_url": "http://arxiv.org/pdf/2408.14472v1",
    "published_date": "2024-08-26 17:59:03 UTC",
    "updated_date": "2024-08-26 17:59:03 UTC"
  },
  {
    "arxiv_id": "2408.14468v2",
    "title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences",
    "authors": [
      "Zhikai Li",
      "Xuewen Liu",
      "Dongrong Joe Fu",
      "Jianquan Li",
      "Qingyi Gu",
      "Kurt Keutzer",
      "Zhen Dong"
    ],
    "abstract": "The rapid advancement of visual generative models necessitates efficient and\nreliable evaluation methods. Arena platform, which gathers user votes on model\ncomparisons, can rank models with human preferences. However, traditional Arena\nmethods, while established, require an excessive number of comparisons for\nranking to converge and are vulnerable to preference noise in voting,\nsuggesting the need for better approaches tailored to contemporary evaluation\nchallenges. In this paper, we introduce K-Sort Arena, an efficient and reliable\nplatform based on a key insight: images and videos possess higher perceptual\nintuitiveness than texts, enabling rapid evaluation of multiple samples\nsimultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing\nK models to engage in free-for-all competitions, which yield much richer\ninformation than pairwise comparisons. To enhance the robustness of the system,\nwe leverage probabilistic modeling and Bayesian updating techniques. We propose\nan exploration-exploitation-based matchmaking strategy to facilitate more\ninformative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster\nconvergence compared to the widely used ELO algorithm. To further validate the\nsuperiority and obtain a comprehensive leaderboard, we collect human feedback\nvia crowdsourced evaluations of numerous cutting-edge text-to-image and\ntext-to-video models. Thanks to its high efficiency, K-Sort Arena can\ncontinuously incorporate emerging models and update the leaderboard with\nminimal votes. Our project has undergone several months of internal testing and\nis now available at https://huggingface.co/spaces/ksort/K-Sort-Arena",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "CVPR 2025. Project page:\n  https://huggingface.co/spaces/ksort/K-Sort-Arena",
    "pdf_url": "http://arxiv.org/pdf/2408.14468v2",
    "published_date": "2024-08-26 17:58:20 UTC",
    "updated_date": "2025-03-15 03:06:37 UTC"
  },
  {
    "arxiv_id": "2408.14443v2",
    "title": "Temporal Ensemble Logic",
    "authors": [
      "Guo-Qiang Zhang"
    ],
    "abstract": "We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal\nlogic for linear-time temporal reasoning. TEL includes primitive temporal\nconstructs such as ``always up to $t$ time later'' ($\\Box_t$), ``sometimes\nbefore $t$ time in the future'' ($\\Diamond_t$), and ``$t$-time later''\n$\\varphi_t$. TEL has been motivated from the requirement for rigor and\nreproducibility for cohort specification and discovery in clinical and\npopulation health research, to fill a gap in formalizing temporal reasoning in\nbiomedicine. Existing logical frameworks such as linear temporal logic are too\nrestrictive to express temporal and sequential properties in biomedicine, or\ntoo permissive in semantic constructs, such as in Halpern-Shoham logic, to\nserve this purpose. In this paper, we first introduce TEL in a general set up,\nwith discrete and dense time as special cases. We then focus on the theoretical\ndevelopment of discrete TEL on the temporal domain of positive integers\n$\\mathbb{N}^+$, denoted as ${\\rm TEL}_{\\mathbb{N}^+}$. ${\\rm\nTEL}_{\\mathbb{N}^+}$ is strictly more expressive than the standard monadic\nsecond order logic, characterized by B\\\"{u}chi automata. We present its formal\nsemantics, a proof system, and provide a proof for the undecidability of the\nsatisfiability of ${\\rm TEL}_{\\mathbb{N}^+}$. We also include initial results\non expressiveness and decidability fragments for ${\\rm TEL}_{\\mathbb{N}^+}$,\nfollowed by application outlook and discussions.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LO",
    "comment": "47 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14443v2",
    "published_date": "2024-08-26 17:36:25 UTC",
    "updated_date": "2024-08-30 14:41:26 UTC"
  },
  {
    "arxiv_id": "2408.14441v1",
    "title": "Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification",
    "authors": [
      "Mahrukh Awan",
      "Asmar Nadeem",
      "Muhammad Junaid Awan",
      "Armin Mustafa",
      "Syed Sameed Husain"
    ],
    "abstract": "Exploiting both audio and visual modalities for video classification is a\nchallenging task, as the existing methods require large model architectures,\nleading to high computational complexity and resource requirements. Smaller\narchitectures, on the other hand, struggle to achieve optimal performance. In\nthis paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that\nintroduces a compact model architecture specifically designed to capture\nintricate audio-visual relationships in video data. Through extensive\nexperiments on the challenging YouTube-8M dataset, we demonstrate that\nAttend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which\nis comparable to the performance of larger baseline models such as\nFully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion\nachieves similar performance to the larger baseline model while reducing the\nmodel size by nearly 80\\%, highlighting its efficiency in terms of model\ncomplexity. Our work demonstrates that the Attend-Fusion model effectively\ncombines audio and visual information for video classification, achieving\ncompetitive performance with significantly reduced model size. This approach\nopens new possibilities for deploying high-performance video understanding\nsystems in resource-constrained environments across various applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14441v1",
    "published_date": "2024-08-26 17:33:47 UTC",
    "updated_date": "2024-08-26 17:33:47 UTC"
  },
  {
    "arxiv_id": "2408.14437v1",
    "title": "Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview",
    "authors": [
      "Ilkin Aliyev",
      "Kama Svoboda",
      "Tosiron Adegbija",
      "Jean-Marc Fellous"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven\nnature of biological neural processing, and offer the potential for\nultra-low-power artificial intelligence. However, realizing their efficiency\nbenefits requires specialized hardware and a co-design approach that\neffectively leverages sparsity. We explore the hardware-software co-design of\nsparse SNNs, examining how sparsity representation, hardware architectures, and\ntraining techniques influence hardware efficiency. We analyze the impact of\nstatic and dynamic sparsity, discuss the implications of different neuron\nmodels and encoding schemes, and investigate the need for adaptability in\nhardware designs. Our work aims to illuminate the path towards embedded\nneuromorphic systems that fully exploit the computational advantages of sparse\nSNNs.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14437v1",
    "published_date": "2024-08-26 17:22:11 UTC",
    "updated_date": "2024-08-26 17:22:11 UTC"
  },
  {
    "arxiv_id": "2408.14435v1",
    "title": "Social perception of faces in a vision-language model",
    "authors": [
      "Carina I. Hausladen",
      "Manuel Knott",
      "Colin F. Camerer",
      "Pietro Perona"
    ],
    "abstract": "We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14435v1",
    "published_date": "2024-08-26 17:21:54 UTC",
    "updated_date": "2024-08-26 17:21:54 UTC"
  },
  {
    "arxiv_id": "2408.14432v2",
    "title": "Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications",
    "authors": [
      "Luyue Xu",
      "Liming Wang",
      "Hong Xie",
      "Mingqiang Zhou"
    ],
    "abstract": "Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at PRICAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14432v2",
    "published_date": "2024-08-26 17:20:34 UTC",
    "updated_date": "2024-08-28 12:39:57 UTC"
  },
  {
    "arxiv_id": "2408.14419v2",
    "title": "CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models",
    "authors": [
      "Shubham Bharti",
      "Shiyun Cheng",
      "Jihyun Rho",
      "Jianrui Zhang",
      "Mu Cai",
      "Yong Jae Lee",
      "Martina Rau",
      "Xiaojin Zhu"
    ],
    "abstract": "We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance. We benchmark leading LLMs as of late 2024 -\nincluding GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset\nand found that our benchmark was challenging to all of them, suggesting room\nfor future large language models to improve.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14419v2",
    "published_date": "2024-08-26 17:04:23 UTC",
    "updated_date": "2025-05-09 19:55:14 UTC"
  },
  {
    "arxiv_id": "2408.14418v3",
    "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues",
    "authors": [
      "Kuluhan Binici",
      "Abhinav Ramesh Kashyap",
      "Viktor Schlegel",
      "Andy T. Liu",
      "Vijay Prakash Dwivedi",
      "Thanh-Tung Nguyen",
      "Xiaoxue Gao",
      "Nancy F. Chen",
      "Stefan Winkler"
    ],
    "abstract": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the Thirty-Ninth AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2408.14418v3",
    "published_date": "2024-08-26 17:04:00 UTC",
    "updated_date": "2025-01-08 07:23:56 UTC"
  },
  {
    "arxiv_id": "2408.14398v3",
    "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
    "authors": [
      "Simon Kurz",
      "Jian-Jia Chen",
      "Lucie Flek",
      "Zhixue Zhao"
    ],
    "abstract": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art (SotA) compression results in post-training and\nretraining-free settings while maintaining high predictive performance.\nHowever, previous research mainly considered calibrating based on English text,\ndespite the multilingual nature of modern LLMs and their frequent use in\nnon-English languages. In this paper, we set out to investigate calibrating the\npruning of multilingual language models for monolingual applications. We\npresent the first comprehensive empirical study, comparing different\ncalibration languages for pruning multilingual models across diverse languages,\ntasks, models, and SotA pruning techniques. Our results offer practical\nsuggestions, for example, calibrating in the target language can efficiently\nretain the language modeling capability but does not necessarily benefit\ndownstream tasks. Through further analysis of latent subspaces, pruning masks,\nand individual neurons within pruned models, we find that while pruning\ngenerally preserves strong language-specific features, it may fail to retain\nlanguage-specific neuron activation patterns and subtle, language-agnostic\nfeatures associated with knowledge and reasoning that are needed for complex\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14398v3",
    "published_date": "2024-08-26 16:29:13 UTC",
    "updated_date": "2024-10-30 00:53:43 UTC"
  },
  {
    "arxiv_id": "2408.14397v1",
    "title": "Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs",
    "authors": [
      "Xiaoman Zhang",
      "Julián N. Acosta",
      "Hong-Yu Zhou",
      "Pranav Rajpurkar"
    ],
    "abstract": "Recent advancements in artificial intelligence have significantly improved\nthe automatic generation of radiology reports. However, existing evaluation\nmethods fail to reveal the models' understanding of radiological images and\ntheir capacity to achieve human-level granularity in descriptions. To bridge\nthis gap, we introduce a system, named ReXKG, which extracts structured\ninformation from processed reports to construct a comprehensive radiology\nknowledge graph. We then propose three metrics to evaluate the similarity of\nnodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs\n(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative\nanalysis of AI-generated and human-written radiology reports, assessing the\nperformance of both specialist and generalist models. Our study provides a\ndeeper understanding of the capabilities and limitations of current AI models\nin radiology report generation, offering valuable insights for improving model\nperformance and clinical applicability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Code is available at: https://github.com/rajpurkarlab/ReXKG",
    "pdf_url": "http://arxiv.org/pdf/2408.14397v1",
    "published_date": "2024-08-26 16:28:56 UTC",
    "updated_date": "2024-08-26 16:28:56 UTC"
  },
  {
    "arxiv_id": "2408.14387v1",
    "title": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Chidaksh Ravuru",
      "Geethan Sannidhi",
      "Venkataramana Runkana"
    ],
    "abstract": "Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper published at the Deployable AI (DAI) workshop at AAAI-2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14387v1",
    "published_date": "2024-08-26 16:11:53 UTC",
    "updated_date": "2024-08-26 16:11:53 UTC"
  },
  {
    "arxiv_id": "2409.06720v1",
    "title": "Evolutionary Game Dynamics Applied to Strategic Adoption of Immersive Technologies in Cultural Heritage and Tourism",
    "authors": [
      "Gioacchino Fazio",
      "Stefano Fricano",
      "Claudio Pirrone"
    ],
    "abstract": "Immersive technologies such as Metaverse, AR, and VR are at a crossroads,\nwith many actors pondering their adoption and potential sectors interested in\nintegration. The cultural and tourism industries are particularly impacted,\nfacing significant pressure to make decisions that could shape their future\nlandscapes. Stakeholders' perceptions play a crucial role in this process,\ninfluencing the speed and extent of technology adoption. As immersive\ntechnologies promise to revolutionize experiences, stakeholders in these fields\nweigh the benefits and challenges of embracing such innovations. The current\nchoices will likely determine the trajectory of cultural preservation and\ntourism enhancement, potentially transforming how we engage with history, art,\nand travel. Starting from a decomposition of stakeholders' perceptions into\nprincipal components using Q-methodology, this article employs an evolutionary\ngame model to attempt to map possible scenarios and highlight potential\ndecision-making trajectories. The proposed approach highlights how evolutionary\ndynamics lead to identifying a dominant long-term strategy that emerges from\nthe complex system of coexistence among various stakeholders.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.TH"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.06720v1",
    "published_date": "2024-08-26 16:02:34 UTC",
    "updated_date": "2024-08-26 16:02:34 UTC"
  },
  {
    "arxiv_id": "2408.14380v1",
    "title": "Probing Causality Manipulation of Large Language Models",
    "authors": [
      "Chenyang Zhang",
      "Haibo Tong",
      "Bin Zhang",
      "Dongyu Zhang"
    ],
    "abstract": "Large language models (LLMs) have shown various ability on natural language\nprocessing, including problems about causality. It is not intuitive for LLMs to\ncommand causality, since pretrained models usually work on statistical\nassociations, and do not focus on causes and effects in sentences. So that\nprobing internal manipulation of causality is necessary for LLMs. This paper\nproposes a novel approach to probe causality manipulation hierarchically, by\nproviding different shortcuts to models and observe behaviors. We exploit\nretrieval augmented generation (RAG) and in-context learning (ICL) for models\non a designed causality classification task. We conduct experiments on\nmainstream LLMs, including GPT-4 and some smaller and domain-specific models.\nOur results suggest that LLMs can detect entities related to causality and\nrecognize direct causal relationships. However, LLMs lack specialized cognition\nfor causality, merely treating them as part of the global semantic of the\nsentence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14380v1",
    "published_date": "2024-08-26 16:00:41 UTC",
    "updated_date": "2024-08-26 16:00:41 UTC"
  },
  {
    "arxiv_id": "2408.14371v2",
    "title": "SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery",
    "authors": [
      "Sarah Rastegar",
      "Mohammadreza Salehi",
      "Yuki M. Asano",
      "Hazel Doughty",
      "Cees G. M. Snoek"
    ],
    "abstract": "In this paper, we address Generalized Category Discovery, aiming to\nsimultaneously uncover novel categories and accurately classify known ones.\nTraditional methods, which lean heavily on self-supervision and contrastive\nlearning, often fall short when distinguishing between fine-grained categories.\nTo address this, we introduce a novel concept called `self-expertise', which\nenhances the model's ability to recognize subtle differences and uncover\nunknown categories. Our approach combines unsupervised and supervised\nself-expertise strategies to refine the model's discernment and generalization.\nInitially, hierarchical pseudo-labeling is used to provide `soft supervision',\nimproving the effectiveness of self-expertise. Our supervised technique differs\nfrom traditional methods by utilizing more abstract positive and negative\nsamples, aiding in the formation of clusters that can generalize to novel\ncategories. Meanwhile, our unsupervised strategy encourages the model to\nsharpen its category distinctions by considering within-category examples as\n`hard' negatives. Supported by theoretical insights, our empirical results\nshowcase that our method outperforms existing state-of-the-art techniques in\nGeneralized Category Discovery across several fine-grained datasets. Our code\nis available at: https://github.com/SarahRastegar/SelEx.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14371v2",
    "published_date": "2024-08-26 15:53:50 UTC",
    "updated_date": "2024-11-10 21:45:49 UTC"
  },
  {
    "arxiv_id": "2408.14368v2",
    "title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal-Conditioned Policy",
    "authors": [
      "Peiyan Li",
      "Hongtao Wu",
      "Yan Huang",
      "Chilam Cheang",
      "Liang Wang",
      "Tao Kong"
    ],
    "abstract": "The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One primary challenge\nis that obtaining robot trajectories fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially-annotated data,\nsuch as human activity videos without action labels and robot trajectories\nwithout text labels, are much easier to collect. Can we leverage these data to\nenhance the generalization capabilities of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on a text instruction and a\ngoal image. During training, GR-MG samples goal images from trajectories and\nconditions on both the text and the goal image or solely on the image when text\nis not available. During inference, where only the text is provided, GR-MG\ngenerates the goal image via a diffusion-based image-editing model and\nconditions on both the text and the generated image. This approach enables\nGR-MG to leverage large amounts of partially-annotated data while still using\nlanguages to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process. In simulation experiments,\nGR-MG improves the average number of tasks completed in a row of 5 from 3.35 to\n4.04. In real-robot experiments, GR-MG is able to perform 58 different tasks\nand improves the success rate from 68.7\\% to 78.1\\% and 44.4\\% to 60.6\\% in\nsimple and generalization settings, respectively. It also outperforms comparing\nbaseline methods in few-shot learning of novel skills. Video demos, code, and\ncheckpoints are available on the project page: https://gr-mg.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 figures, RA-L",
    "pdf_url": "http://arxiv.org/pdf/2408.14368v2",
    "published_date": "2024-08-26 15:46:41 UTC",
    "updated_date": "2024-12-23 14:08:16 UTC"
  },
  {
    "arxiv_id": "2408.14354v1",
    "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java",
    "authors": [
      "Daoguang Zan",
      "Zhirong Huang",
      "Ailun Yu",
      "Shaoxin Lin",
      "Yifan Shi",
      "Wei Liu",
      "Dong Chen",
      "Zongshuai Qi",
      "Hao Yu",
      "Lei Yu",
      "Dezhi Ran",
      "Muhan Zeng",
      "Bo Shen",
      "Pan Bian",
      "Guangtai Liang",
      "Bei Guan",
      "Pengjie Huang",
      "Tao Xie",
      "Yongji Wang",
      "Qianxiang Wang"
    ],
    "abstract": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "This work is in progress",
    "pdf_url": "http://arxiv.org/pdf/2408.14354v1",
    "published_date": "2024-08-26 15:30:05 UTC",
    "updated_date": "2024-08-26 15:30:05 UTC"
  },
  {
    "arxiv_id": "2408.14352v1",
    "title": "Assessing Contamination in Large Language Models: Introducing the LogProber method",
    "authors": [
      "Nicolas Yax",
      "Pierre-Yves Oudeyer",
      "Stefano Palminteri"
    ],
    "abstract": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. Most recent works in the field are not tailored to\nquantify contamination on short sequences of text like we find in psychology\nquestionnaires. In the present paper we introduce LogProber, a novel,\nefficient, algorithm that we show able to detect contamination using token\nprobability in given sentences. In the second part we investigate the\nlimitations of the method and discuss how different training methods can\ncontaminate models without leaving traces in the token probabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14352v1",
    "published_date": "2024-08-26 15:29:34 UTC",
    "updated_date": "2024-08-26 15:29:34 UTC"
  },
  {
    "arxiv_id": "2408.14527v1",
    "title": "Multi-Agent Path Finding with Real Robot Dynamics and Interdependent Tasks for Automated Warehouses",
    "authors": [
      "Vassilissa Lehoux-Lebacque",
      "Tomi Silander",
      "Christelle Loiodice",
      "Seungjoon Lee",
      "Albert Wang",
      "Sofia Michel"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) is an important optimization problem\nunderlying the deployment of robots in automated warehouses and factories.\nDespite the large body of work on this topic, most approaches make heavy\nsimplifications, both on the environment and the agents, which make the\nresulting algorithms impractical for real-life scenarios. In this paper, we\nconsider a realistic problem of online order delivery in a warehouse, where a\nfleet of robots bring the products belonging to each order from shelves to\nworkstations. This creates a stream of inter-dependent pickup and delivery\ntasks and the associated MAPF problem consists of computing realistic\ncollision-free robot trajectories fulfilling these tasks. To solve this MAPF\nproblem, we propose an extension of the standard Prioritized Planning algorithm\nto deal with the inter-dependent tasks (Interleaved Prioritized Planning) and a\nnovel Via-Point Star (VP*) algorithm to compute an optimal dynamics-compliant\nrobot trajectory to visit a sequence of goal locations while avoiding moving\nobstacles. We prove the completeness of our approach and evaluate it in\nsimulation as well as in a real warehouse.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to ECAI-2024. For related videos, see\n  https://europe.naverlabs.com/research/publications/MAPF_IPP",
    "pdf_url": "http://arxiv.org/pdf/2408.14527v1",
    "published_date": "2024-08-26 15:13:38 UTC",
    "updated_date": "2024-08-26 15:13:38 UTC"
  },
  {
    "arxiv_id": "2408.14340v3",
    "title": "Foundation Models for Music: A Survey",
    "authors": [
      "Yinghao Ma",
      "Anders Øland",
      "Anton Ragni",
      "Bleiz MacSen Del Sette",
      "Charalampos Saitis",
      "Chris Donahue",
      "Chenghua Lin",
      "Christos Plachouras",
      "Emmanouil Benetos",
      "Elona Shatri",
      "Fabio Morreale",
      "Ge Zhang",
      "György Fazekas",
      "Gus Xia",
      "Huan Zhang",
      "Ilaria Manco",
      "Jiawen Huang",
      "Julien Guinot",
      "Liwei Lin",
      "Luca Marinelli",
      "Max W. Y. Lam",
      "Megha Sharma",
      "Qiuqiang Kong",
      "Roger B. Dannenberg",
      "Ruibin Yuan",
      "Shangda Wu",
      "Shih-Lun Wu",
      "Shuqi Dai",
      "Shun Lei",
      "Shiyin Kang",
      "Simon Dixon",
      "Wenhu Chen",
      "Wenhao Huang",
      "Xingjian Du",
      "Xingwei Qu",
      "Xu Tan",
      "Yizhi Li",
      "Zeyue Tian",
      "Zhiyong Wu",
      "Zhizheng Wu",
      "Ziyang Ma",
      "Ziyu Wang"
    ],
    "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14340v3",
    "published_date": "2024-08-26 15:13:14 UTC",
    "updated_date": "2024-09-03 14:53:34 UTC"
  },
  {
    "arxiv_id": "2408.14338v1",
    "title": "Machine Learning for Quantifier Selection in cvc5",
    "authors": [
      "Jan Jakubův",
      "Mikoláš Janota",
      "Jelle Piepenbrock",
      "Josef Urban"
    ],
    "abstract": "In this work we considerably improve the state-of-the-art SMT solving on\nfirst-order quantified problems by efficient machine learning guidance of\nquantifier selection. Quantifiers represent a significant challenge for SMT and\nare technically a source of undecidability. In our approach, we train an\nefficient machine learning model that informs the solver which quantifiers\nshould be instantiated and which not. Each quantifier may be instantiated\nmultiple times and the set of the active quantifiers changes as the solving\nprogresses. Therefore, we invoke the ML predictor many times, during the whole\nrun of the solver. To make this efficient, we use fast ML models based on\ngradient boosting decision trees. We integrate our approach into the\nstate-of-the-art cvc5 SMT solver and show a considerable increase of the\nsystem's holdout-set performance after training it on a large set of\nfirst-order problems collected from the Mizar Mathematical Library.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14338v1",
    "published_date": "2024-08-26 15:07:35 UTC",
    "updated_date": "2024-08-26 15:07:35 UTC"
  },
  {
    "arxiv_id": "2408.14336v1",
    "title": "Equivariant Reinforcement Learning under Partial Observability",
    "authors": [
      "Hai Nguyen",
      "Andrea Baisero",
      "David Klee",
      "Dian Wang",
      "Robert Platt",
      "Christopher Amato"
    ],
    "abstract": "Incorporating inductive biases is a promising approach for tackling\nchallenging robot learning domains with sample-efficient solutions. This paper\nidentifies partially observable domains where symmetries can be a useful\ninductive bias for efficient learning. Specifically, by encoding the\nequivariance regarding specific group symmetries into the neural networks, our\nactor-critic reinforcement learning agents can reuse solutions in the past for\nrelated scenarios. Consequently, our equivariant agents outperform\nnon-equivariant approaches significantly in terms of sample efficiency and\nfinal performance, demonstrated through experiments on a range of robotic tasks\nin simulation and real hardware.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Conference on Robot Learning, 2023",
    "pdf_url": "http://arxiv.org/pdf/2408.14336v1",
    "published_date": "2024-08-26 15:07:01 UTC",
    "updated_date": "2024-08-26 15:07:01 UTC"
  },
  {
    "arxiv_id": "2409.04452v1",
    "title": "Process Trace Querying using Knowledge Graphs and Notation3",
    "authors": [
      "William Van Woensel"
    ],
    "abstract": "In process mining, a log exploration step allows making sense of the event\ntraces; e.g., identifying event patterns and illogical traces, and gaining\ninsight into their variability. To support expressive log exploration, the\nevent log can be converted into a Knowledge Graph (KG), which can then be\nqueried using general-purpose languages. We explore the creation of semantic KG\nusing the Resource Description Framework (RDF) as a data model, combined with\nthe general-purpose Notation3 (N3) rule language for querying. We show how\ntypical trace querying constraints, inspired by the state of the art, can be\nimplemented in N3. We convert case- and object-centric event logs into a\ntrace-based semantic KG; OCEL2 logs are hereby \"flattened\" into traces based on\nobject paths through the KG. This solution offers (a) expressivity, as queries\ncan instantiate constraints in multiple ways and arbitrarily constrain\nattributes and relations (e.g., actors, resources); (b) flexibility, as OCEL2\nevent logs can be serialized as traces in arbitrary ways based on the KG; and\n(c) extensibility, as others can extend our library by leveraging the same\nimplementation patterns.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04452v1",
    "published_date": "2024-08-26 14:55:55 UTC",
    "updated_date": "2024-08-26 14:55:55 UTC"
  },
  {
    "arxiv_id": "2408.14329v2",
    "title": "Towards Adaptive Human-centric Video Anomaly Detection: A Comprehensive Framework and A New Benchmark",
    "authors": [
      "Armin Danesh Pazho",
      "Shanle Yao",
      "Ghazal Alinezhad Noghre",
      "Babak Rahimi Ardabili",
      "Vinit Katariya",
      "Hamed Tabkhi"
    ],
    "abstract": "Human-centric Video Anomaly Detection (VAD) aims to identify human behaviors\nthat deviate from normal. At its core, human-centric VAD faces substantial\nchallenges, such as the complexity of diverse human behaviors, the rarity of\nanomalies, and ethical constraints. These challenges limit access to\nhigh-quality datasets and highlight the need for a dataset and framework\nsupporting continual learning. Moving towards adaptive human-centric VAD, we\nintroduce the HuVAD (Human-centric privacy-enhanced Video Anomaly Detection)\ndataset and a novel Unsupervised Continual Anomaly Learning (UCAL) framework.\nUCAL enables incremental learning, allowing models to adapt over time, bridging\ntraditional training and real-world deployment. HuVAD prioritizes privacy by\nproviding de-identified annotations and includes seven indoor/outdoor scenes,\noffering over 5x more pose-annotated frames than previous datasets. Our\nstandard and continual benchmarks, utilize a comprehensive set of metrics,\ndemonstrating that UCAL-enhanced models achieve superior performance in 82.14%\nof cases, setting a new state-of-the-art (SOTA). The dataset can be accessed at\nhttps://github.com/TeCSAR-UNCC/HuVAD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14329v2",
    "published_date": "2024-08-26 14:55:23 UTC",
    "updated_date": "2025-03-19 18:13:10 UTC"
  },
  {
    "arxiv_id": "2408.14326v1",
    "title": "Streamline tractography of the fetal brain in utero with machine learning",
    "authors": [
      "Weide Liu",
      "Camilo Calixto",
      "Simon K. Warfield",
      "Davood Karimi"
    ],
    "abstract": "Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive\ntool for studying white matter tracts and structural connectivity of the brain.\nThese assessments rely heavily on tractography techniques, which reconstruct\nvirtual streamlines representing white matter fibers. Much effort has been\ndevoted to improving tractography methodology for adult brains, while\ntractography of the fetal brain has been largely neglected. Fetal tractography\nfaces unique difficulties due to low dMRI signal quality, immature and rapidly\ndeveloping brain structures, and paucity of reference data. This work presents\nthe first machine learning model for fetal tractography. The model input\nconsists of five sources of information: (1) Fiber orientation, inferred from a\ndiffusion tensor fit to the dMRI signal; (2) Directions of recent propagation\nsteps; (3) Global spatial information, encoded as distances to keypoints in the\nbrain cortex; (4) Tissue segmentation information; and (5) Prior information\nabout the expected local fiber orientations supplied with an atlas. In order to\nmitigate the local tensor estimation error, a large spatial context around the\ncurrent point in the diffusion tensor image is encoded using convolutional and\nattention neural network modules. Moreover, the diffusion tensor information at\na hypothetical next point is included in the model input. Filtering rules based\non anatomically constrained tractography are applied to prune implausible\nstreamlines. We trained the model on manually-refined whole-brain fetal\ntractograms and validated the trained model on an independent set of 11 test\nscans with gestational ages between 23 and 36 weeks. Results show that our\nproposed method achieves superior performance across all evaluated tracts. The\nnew method can significantly advance the capabilities of dMRI for studying\nnormal and abnormal brain development in utero.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14326v1",
    "published_date": "2024-08-26 14:54:14 UTC",
    "updated_date": "2024-08-26 14:54:14 UTC"
  },
  {
    "arxiv_id": "2408.14317v2",
    "title": "Claim Verification in the Age of Large Language Models: A Survey",
    "authors": [
      "Alphaeus Dmonte",
      "Roland Oruche",
      "Marcos Zampieri",
      "Prasad Calyam",
      "Isabelle Augenstein"
    ],
    "abstract": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14317v2",
    "published_date": "2024-08-26 14:45:03 UTC",
    "updated_date": "2025-02-11 14:51:08 UTC"
  },
  {
    "arxiv_id": "2408.14314v1",
    "title": "Logic interpretations of ANN partition cells",
    "authors": [
      "Ingo Schmitt"
    ],
    "abstract": "Consider a binary classification problem solved using a feed-forward\nartificial neural network (ANN). Let the ANN be composed of a ReLU layer and\nseveral linear layers (convolution, sum-pooling, or fully connected). We assume\nthe network was trained with high accuracy. Despite numerous suggested\napproaches, interpreting an artificial neural network remains challenging for\nhumans. For a new method of interpretation, we construct a bridge between a\nsimple ANN and logic. As a result, we can analyze and manipulate the semantics\nof an ANN using the powerful tool set of logic. To achieve this, we decompose\nthe input space of the ANN into several network partition cells. Each network\npartition cell represents a linear combination that maps input values to a\nclassifying output value. For interpreting the linear map of a partition cell\nusing logic expressions, we suggest minterm values as the input of a simple\nANN. We derive logic expressions representing interaction patterns for\nseparating objects classified as 1 from those classified as 0. To facilitate an\ninterpretation of logic expressions, we present them as binary logic trees.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "I.2.4; I.2.6; F.4.1"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14314v1",
    "published_date": "2024-08-26 14:43:43 UTC",
    "updated_date": "2024-08-26 14:43:43 UTC"
  },
  {
    "arxiv_id": "2408.14307v2",
    "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing",
    "authors": [
      "Yayati Jadhav",
      "Peter Pak",
      "Amir Barati Farimani"
    ],
    "abstract": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14307v2",
    "published_date": "2024-08-26 14:38:19 UTC",
    "updated_date": "2025-05-05 20:53:58 UTC"
  },
  {
    "arxiv_id": "2408.14284v1",
    "title": "May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels",
    "authors": [
      "Monica Millunzi",
      "Lorenzo Bonicelli",
      "Angelo Porrello",
      "Jacopo Credi",
      "Petter N. Kolm",
      "Simone Calderara"
    ],
    "abstract": "Forgetting presents a significant challenge during incremental training,\nmaking it particularly demanding for contemporary AI systems to assimilate new\nknowledge in streaming data environments. To address this issue, most\napproaches in Continual Learning (CL) rely on the replay of a restricted buffer\nof past data. However, the presence of noise in real-world scenarios, where\nhuman annotation is constrained by time limitations or where data is\nautomatically gathered from the web, frequently renders these strategies\nvulnerable. In this study, we address the problem of CL under Noisy Labels\n(CLN) by introducing Alternate Experience Replay (AER), which takes advantage\nof forgetting to maintain a clear distinction between clean, complex, and noisy\nsamples in the memory buffer. The idea is that complex or mislabeled examples,\nwhich hardly fit the previously learned data distribution, are most likely to\nbe forgotten. To grasp the benefits of such a separation, we equip AER with\nAsymmetric Balanced Sampling (ABS): a new sample selection strategy that\nprioritizes purity on the current task while retaining relevant samples from\nthe past. Through extensive computational comparisons, we demonstrate the\neffectiveness of our approach in terms of both accuracy and purity of the\nobtained buffer, resulting in a remarkable average gain of 4.71% points in\naccuracy with respect to existing loss-based purification strategies. Code is\navailable at https://github.com/aimagelab/mammoth.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 5 figures. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK",
    "pdf_url": "http://arxiv.org/pdf/2408.14284v1",
    "published_date": "2024-08-26 14:09:40 UTC",
    "updated_date": "2024-08-26 14:09:40 UTC"
  },
  {
    "arxiv_id": "2409.00094v1",
    "title": "Examining Independence in Ensemble Sentiment Analysis: A Study on the Limits of Large Language Models Using the Condorcet Jury Theorem",
    "authors": [
      "Baptiste Lefort",
      "Eric Benhamou",
      "Jean-Jacques Ohana",
      "Beatrice Guez",
      "David Saltiel",
      "Thomas Jacquot"
    ],
    "abstract": "This paper explores the application of the Condorcet Jury theorem to the\ndomain of sentiment analysis, specifically examining the performance of various\nlarge language models (LLMs) compared to simpler natural language processing\n(NLP) models. The theorem posits that a majority vote classifier should enhance\npredictive accuracy, provided that individual classifiers' decisions are\nindependent. Our empirical study tests this theoretical framework by\nimplementing a majority vote mechanism across different models, including\nadvanced LLMs such as ChatGPT 4. Contrary to expectations, the results reveal\nonly marginal improvements in performance when incorporating larger models,\nsuggesting a lack of independence among them. This finding aligns with the\nhypothesis that despite their complexity, LLMs do not significantly outperform\nsimpler models in reasoning tasks within sentiment analysis, showing the\npractical limits of model independence in the context of advanced NLP tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00094v1",
    "published_date": "2024-08-26 14:04:00 UTC",
    "updated_date": "2024-08-26 14:04:00 UTC"
  },
  {
    "arxiv_id": "2408.14281v1",
    "title": "Uncertainties of Latent Representations in Computer Vision",
    "authors": [
      "Michael Kirchhof"
    ],
    "abstract": "Uncertainty quantification is a key pillar of trustworthy machine learning.\nIt enables safe reactions under unsafe inputs, like predicting only when the\nmachine learning model detects sufficient evidence, discarding anomalous data,\nor emitting warnings when an error is likely to be inbound. This is\nparticularly crucial in safety-critical areas like medical image classification\nor self-driving cars. Despite the plethora of proposed uncertainty\nquantification methods achieving increasingly higher scores on performance\nbenchmarks, uncertainty estimates are often shied away from in practice. Many\nmachine learning projects start from pretrained latent representations that\ncome without uncertainty estimates. Uncertainties would need to be trained by\npractitioners on their own, which is notoriously difficult and\nresource-intense.\n  This thesis makes uncertainty estimates easily accessible by adding them to\nthe latent representation vectors of pretrained computer vision models. Besides\nproposing approaches rooted in probability and decision theory, such as\nMonte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both\ntheoretical and empirical questions. We show that these unobservable\nuncertainties about unobservable latent representations are indeed provably\ncorrect. We also provide an uncertainty-aware representation learning (URL)\nbenchmark to compare these unobservables against observable ground-truths.\nFinally, we compile our findings to pretrain lightweight representation\nuncertainties on large-scale computer vision models that transfer to unseen\ndatasets in a zero-shot manner.\n  Our findings do not only advance the current theoretical understanding of\nuncertainties over latent variables, but also facilitate the access to\nuncertainty quantification for future researchers inside and outside the field,\nenabling straightforward but trustworthy machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Doctoral thesis",
    "pdf_url": "http://arxiv.org/pdf/2408.14281v1",
    "published_date": "2024-08-26 14:02:30 UTC",
    "updated_date": "2024-08-26 14:02:30 UTC"
  },
  {
    "arxiv_id": "2408.14525v1",
    "title": "Estimating Uncertainty with Implicit Quantile Network",
    "authors": [
      "Yi Hung Lim"
    ],
    "abstract": "Uncertainty quantification is an important part of many performance critical\napplications. This paper provides a simple alternative to existing approaches\nsuch as ensemble learning and bayesian neural networks. By directly modeling\nthe loss distribution with an Implicit Quantile Network, we get an estimate of\nhow uncertain the model is of its predictions. For experiments with MNIST and\nCIFAR datasets, the mean of the estimated loss distribution is 2x higher for\nincorrect predictions. When data with high estimated uncertainty is removed\nfrom the test dataset, the accuracy of the model goes up as much as 10%. This\nmethod is simple to implement while offering important information to\napplications where the user has to know when the model could be wrong (e.g.\ndeep learning for healthcare).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "This method is simple to implement and offers important information\n  for performance critical applications",
    "pdf_url": "http://arxiv.org/pdf/2408.14525v1",
    "published_date": "2024-08-26 13:33:14 UTC",
    "updated_date": "2024-08-26 13:33:14 UTC"
  },
  {
    "arxiv_id": "2408.14253v2",
    "title": "Text3DAug -- Prompted Instance Augmentation for LiDAR Perception",
    "authors": [
      "Laurenz Reichardt",
      "Luca Uhr",
      "Oliver Wasenmüller"
    ],
    "abstract": "LiDAR data of urban scenarios poses unique challenges, such as heterogeneous\ncharacteristics and inherent class imbalance. Therefore, large-scale datasets\nare necessary to apply deep learning methods. Instance augmentation has emerged\nas an efficient method to increase dataset diversity. However, current methods\nrequire the time-consuming curation of 3D models or costly manual data\nannotation. To overcome these limitations, we propose Text3DAug, a novel\napproach leveraging generative models for instance augmentation. Text3DAug does\nnot depend on labeled data and is the first of its kind to generate instances\nand annotations from text. This allows for a fully automated pipeline,\neliminating the need for manual effort in practical applications. Additionally,\nText3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor\nused. Comprehensive experimental analysis on LiDAR segmentation, detection and\nnovel class discovery demonstrates that Text3DAug is effective in supplementing\nexisting methods or as a standalone method, performing on par or better than\nestablished methods, however while overcoming their specific drawbacks. The\ncode is publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.14253v2",
    "published_date": "2024-08-26 13:16:03 UTC",
    "updated_date": "2024-08-27 10:50:13 UTC"
  },
  {
    "arxiv_id": "2408.14249v1",
    "title": "Beyond Few-shot Object Detection: A Detailed Survey",
    "authors": [
      "Vishal Chudasama",
      "Hiran Sarkar",
      "Pankaj Wasnik",
      "Vineeth N Balasubramanian",
      "Jayateja Kalla"
    ],
    "abstract": "Object detection is a critical field in computer vision focusing on\naccurately identifying and locating specific objects in images or videos.\nTraditional methods for object detection rely on large labeled training\ndatasets for each object category, which can be time-consuming and expensive to\ncollect and annotate. To address this issue, researchers have introduced\nfew-shot object detection (FSOD) approaches that merge few-shot learning and\nobject detection principles. These approaches allow models to quickly adapt to\nnew object categories with only a few annotated samples. While traditional FSOD\nmethods have been studied before, this survey paper comprehensively reviews\nFSOD research with a specific focus on covering different FSOD settings such as\nstandard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain\nadaptive FSOD. These approaches play a vital role in reducing the reliance on\nextensive labeled datasets, particularly as the need for efficient machine\nlearning models continues to rise. This survey paper aims to provide a\ncomprehensive understanding of the above-mentioned few-shot settings and\nexplore the methodologies for each FSOD task. It thoroughly compares\nstate-of-the-art methods across different FSOD settings, analyzing them in\ndetail based on their evaluation protocols. Additionally, it offers insights\ninto their applications, challenges, and potential future directions in the\nevolving field of object detection with limited data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; I.4.8; I.5"
    ],
    "primary_category": "cs.CV",
    "comment": "43 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14249v1",
    "published_date": "2024-08-26 13:09:23 UTC",
    "updated_date": "2024-08-26 13:09:23 UTC"
  },
  {
    "arxiv_id": "2408.14240v2",
    "title": "Celtibero: Robust Layered Aggregation for Federated Learning",
    "authors": [
      "Borja Molina-Coronado"
    ],
    "abstract": "Federated Learning (FL) is an innovative approach to distributed machine\nlearning. While FL offers significant privacy advantages, it also faces\nsecurity challenges, particularly from poisoning attacks where adversaries\ndeliberately manipulate local model updates to degrade model performance or\nintroduce hidden backdoors. Existing defenses against these attacks have been\nshown to be effective when the data on the nodes is identically and\nindependently distributed (i.i.d.), but they often fail under less restrictive,\nnon-i.i.d data conditions. To overcome these limitations, we introduce\nCeltibero, a novel defense mechanism that integrates layered aggregation to\nenhance robustness against adversarial manipulation. Through extensive\nexperiments on the MNIST and IMDB datasets, we demonstrate that Celtibero\nconsistently achieves high main task accuracy (MTA) while maintaining minimal\nattack success rates (ASR) across a range of untargeted and targeted poisoning\nattacks. Our results highlight the superiority of Celtibero over existing\ndefenses such as FL-Defender, LFighter, and FLAME, establishing it as a highly\neffective solution for securing federated learning systems against\nsophisticated poisoning attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14240v2",
    "published_date": "2024-08-26 12:54:00 UTC",
    "updated_date": "2024-09-20 11:24:52 UTC"
  },
  {
    "arxiv_id": "2408.14236v1",
    "title": "DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification",
    "authors": [
      "Hanna Abi Akl"
    ],
    "abstract": "We introduce semantic towers, an extrinsic knowledge representation method,\nand compare it to intrinsic knowledge in large language models for ontology\nlearning. Our experiments show a trade-off between performance and semantic\ngrounding for extrinsic knowledge compared to a fine-tuned model intrinsic\nknowledge. We report our findings on the Large Language Models for Ontology\nLearning (LLMs4OL) 2024 challenge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 4 figures, accepted for the LLMs4OL challenge at the\n  International Semantic Web Conference (ISWC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14236v1",
    "published_date": "2024-08-26 12:50:27 UTC",
    "updated_date": "2024-08-26 12:50:27 UTC"
  },
  {
    "arxiv_id": "2408.14229v1",
    "title": "Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition",
    "authors": [
      "Leonid Erlygin",
      "Alexey Zaytsev"
    ],
    "abstract": "Accurately estimating image quality and model robustness improvement are\ncritical challenges in unconstrained face recognition, which can be addressed\nthrough uncertainty estimation via probabilistic face embeddings. Previous\nresearch mainly focused on uncertainty estimation in face verification, leaving\nthe open-set face recognition task underexplored. In open-set face recognition,\none seeks to classify an image, which could also be unknown. Here, the low\nvariance of probabilistic embedding does not imply a low error probability: an\nimage embedding could be close to several classes in a gallery, thus yielding\nhigh uncertainty. We propose a method aware of two sources of ambiguity in the\nopen-set recognition system: (1) the gallery uncertainty caused by overlapping\nclasses and (2) the uncertainty of the face embeddings. To detect both types,\nwe use a Bayesian probabilistic model of embedding distribution, which provides\na principled uncertainty estimate. Challenging open-set face recognition\ndatasets, such as IJB-C, serve as a testbed for our method. We also propose a\nnew open-set recognition protocol for whale and dolphin identification. The\nproposed approach better identifies recognition errors than uncertainty\nestimation methods based solely on image quality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14229v1",
    "published_date": "2024-08-26 12:44:17 UTC",
    "updated_date": "2024-08-26 12:44:17 UTC"
  },
  {
    "arxiv_id": "2408.14224v1",
    "title": "Fact Probability Vector Based Goal Recognition",
    "authors": [
      "Nils Wilken",
      "Lea Cohausz",
      "Christian Bartelt",
      "Heiner Stuckenschmidt"
    ],
    "abstract": "We present a new approach to goal recognition that involves comparing\nobserved facts with their expected probabilities. These probabilities depend on\na specified goal g and initial state s0. Our method maps these probabilities\nand observed facts into a real vector space to compute heuristic values for\npotential goals. These values estimate the likelihood of a given goal being the\ntrue objective of the observed agent. As obtaining exact expected probabilities\nfor observed facts in an observation sequence is often practically infeasible,\nwe propose and empirically validate a method for approximating these\nprobabilities. Our empirical results show that the proposed approach offers\nimproved goal recognition precision compared to state-of-the-art techniques\nwhile reducing computational complexity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Will be presented at ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14224v1",
    "published_date": "2024-08-26 12:37:41 UTC",
    "updated_date": "2024-08-26 12:37:41 UTC"
  },
  {
    "arxiv_id": "2408.14211v1",
    "title": "MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement",
    "authors": [
      "Xu He",
      "Xiaoyu Li",
      "Di Kang",
      "Jiangnan Ye",
      "Chaopeng Zhang",
      "Liyang Chen",
      "Xiangjun Gao",
      "Han Zhang",
      "Zhiyong Wu",
      "Haolin Zhuang"
    ],
    "abstract": "Existing works in single-image human reconstruction suffer from weak\ngeneralizability due to insufficient training data or 3D inconsistencies for a\nlack of comprehensive multi-view knowledge. In this paper, we introduce\nMagicMan, a human-specific multi-view diffusion model designed to generate\nhigh-quality novel view images from a single reference image. As its core, we\nleverage a pre-trained 2D diffusion model as the generative prior for\ngeneralizability, with the parametric SMPL-X model as the 3D body prior to\npromote 3D awareness. To tackle the critical challenge of maintaining\nconsistency while achieving dense multi-view generation for improved 3D human\nreconstruction, we first introduce hybrid multi-view attention to facilitate\nboth efficient and thorough information interchange across different views.\nAdditionally, we present a geometry-aware dual branch to perform concurrent\ngeneration in both RGB and normal domains, further enhancing consistency via\ngeometry cues. Last but not least, to address ill-shaped issues arising from\ninaccurate SMPL-X estimation that conflicts with the reference image, we\npropose a novel iterative refinement strategy, which progressively optimizes\nSMPL-X accuracy while enhancing the quality and consistency of the generated\nmulti-views. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing approaches in both novel view synthesis and\nsubsequent 3D human reconstruction tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://thuhcsi.github.io/MagicMan",
    "pdf_url": "http://arxiv.org/pdf/2408.14211v1",
    "published_date": "2024-08-26 12:10:52 UTC",
    "updated_date": "2024-08-26 12:10:52 UTC"
  },
  {
    "arxiv_id": "2409.00092v3",
    "title": "Large Language Model for Patent Concept Generation",
    "authors": [
      "Runtao Ren",
      "Jian Ma",
      "Jianxi Luo"
    ],
    "abstract": "In traditional innovation practices, concept and IP generation are often\niteratively integrated. Both processes demand an intricate understanding of\nadvanced technical domain knowledge. Existing large language models (LLMs),\nwhile possessing massive pre-trained knowledge, often fall short in the\ninnovative concept generation due to a lack of specialized knowledge necessary\nfor the generation. To bridge this critical gap, we propose a novel knowledge\nfinetuning (KFT) framework to endow LLM-based AI with the ability to\nautonomously mine, understand, and apply domain-specific knowledge and concepts\nfor invention generation, i.e., concept and patent generation together. Our\nproposed PatentGPT integrates knowledge injection pre-training (KPT),\ndomain-specific supervised finetuning (SFT), and reinforcement learning from\nhuman feedback (RLHF). Extensive evaluation shows that PatentGPT significantly\noutperforms the state-of-the-art models on patent-related benchmark tests. Our\nmethod not only provides new insights into data-driven innovation but also\npaves a new path to fine-tune LLMs for applications in the context of\ntechnology. We also discuss the managerial and policy implications of\nAI-generating inventions in the future.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in Advanced Engineering Informatics, Link:\n  https://doi.org/10.1016/j.aei.2025.103301",
    "pdf_url": "http://arxiv.org/pdf/2409.00092v3",
    "published_date": "2024-08-26 12:00:29 UTC",
    "updated_date": "2025-04-08 05:07:10 UTC"
  },
  {
    "arxiv_id": "2408.14195v1",
    "title": "Representative Arm Identification: A fixed confidence approach to identify cluster representatives",
    "authors": [
      "Sarvesh Gharat",
      "Aniket Yadav",
      "Nikhil Karamchandani",
      "Jayakrishnan Nair"
    ],
    "abstract": "We study the representative arm identification (RAI) problem in the\nmulti-armed bandits (MAB) framework, wherein we have a collection of arms, each\nassociated with an unknown reward distribution. An underlying instance is\ndefined by a partitioning of the arms into clusters of predefined sizes, such\nthat for any $j > i$, all arms in cluster $i$ have a larger mean reward than\nthose in cluster $j$. The goal in RAI is to reliably identify a certain\nprespecified number of arms from each cluster, while using as few arm pulls as\npossible. The RAI problem covers as special cases several well-studied MAB\nproblems such as identifying the best arm or any $M$ out of the top $K$, as\nwell as both full and coarse ranking. We start by providing an\ninstance-dependent lower bound on the sample complexity of any feasible\nalgorithm for this setting. We then propose two algorithms, based on the idea\nof confidence intervals, and provide high probability upper bounds on their\nsample complexity, which orderwise match the lower bound. Finally, we do an\nempirical comparison of both algorithms along with an LUCB-type alternative on\nboth synthetic and real-world datasets, and demonstrate the superior\nperformance of our proposed schemes in most cases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "We analyse a clustered multi-armed bandit formulation, where the\n  learning objective is to identify representative arms from each cluster, in a\n  fixed confidence setting",
    "pdf_url": "http://arxiv.org/pdf/2408.14195v1",
    "published_date": "2024-08-26 11:47:52 UTC",
    "updated_date": "2024-08-26 11:47:52 UTC"
  },
  {
    "arxiv_id": "2408.14185v1",
    "title": "DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models",
    "authors": [
      "Ziai Zhou",
      "Bin Zhou",
      "Hao Liu"
    ],
    "abstract": "Real-time dynamic path planning in complex traffic environments presents\nchallenges, such as varying traffic volumes and signal wait times. Traditional\nstatic routing algorithms like Dijkstra and A* compute shortest paths but often\nfail under dynamic conditions. Recent Reinforcement Learning (RL) approaches\noffer improvements but tend to focus on local optima, risking dead-ends or\nboundary issues. This paper proposes a novel approach based on causal inference\nfor real-time dynamic path planning, balancing global and local optimality. We\nfirst use the static Dijkstra algorithm to compute a globally optimal baseline\npath. A distributed control strategy then guides vehicles along this path. At\nintersections, DynamicRouteGPT performs real-time decision-making for local\npath selection, considering real-time traffic, driving preferences, and\nunexpected events. DynamicRouteGPT integrates Markov chains, Bayesian\ninference, and large-scale pretrained language models like Llama3 8B to provide\nan efficient path planning solution. It dynamically adjusts to traffic\nscenarios and driver preferences and requires no pre-training, offering broad\napplicability across road networks. A key innovation is the construction of\ncausal graphs for counterfactual reasoning, optimizing path decisions.\nExperimental results show that our method achieves state-of-the-art performance\nin real-time dynamic path planning for multiple vehicles while providing\nexplainable path selections, offering a novel and efficient solution for\ncomplex traffic environments.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper is 12 pages long and represents the initial draft, version\n  1",
    "pdf_url": "http://arxiv.org/pdf/2408.14185v1",
    "published_date": "2024-08-26 11:19:58 UTC",
    "updated_date": "2024-08-26 11:19:58 UTC"
  },
  {
    "arxiv_id": "2408.14183v1",
    "title": "Robot Navigation with Entity-Based Collision Avoidance using Deep Reinforcement Learning",
    "authors": [
      "Yury Kolomeytsev",
      "Dmitry Golembiovsky"
    ],
    "abstract": "Efficient navigation in dynamic environments is crucial for autonomous robots\ninteracting with various environmental entities, including both moving agents\nand static obstacles. In this study, we present a novel methodology that\nenhances the robot's interaction with different types of agents and obstacles\nbased on specific safety requirements. This approach uses information about the\nentity types, improving collision avoidance and ensuring safer navigation. We\nintroduce a new reward function that penalizes the robot for collisions with\ndifferent entities such as adults, bicyclists, children, and static obstacles,\nand additionally encourages the robot's proximity to the goal. It also\npenalizes the robot for being close to entities, and the safe distance also\ndepends on the entity type. Additionally, we propose an optimized algorithm for\ntraining and testing, which significantly accelerates train, validation, and\ntest steps and enables training in complex environments. Comprehensive\nexperiments conducted using simulation demonstrate that our approach\nconsistently outperforms conventional navigation and collision avoidance\nmethods, including state-of-the-art techniques. To sum up, this work\ncontributes to enhancing the safety and efficiency of navigation systems for\nautonomous robots in dynamic, crowded environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14183v1",
    "published_date": "2024-08-26 11:16:03 UTC",
    "updated_date": "2024-08-26 11:16:03 UTC"
  },
  {
    "arxiv_id": "2408.14180v2",
    "title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing",
    "authors": [
      "Yiwei Ma",
      "Jiayi Ji",
      "Ke Ye",
      "Weihuang Lin",
      "Zhibin Wang",
      "Yonghan Zheng",
      "Qiang Zhou",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "abstract": "Significant progress has been made in the field of Instruction-based Image\nEditing (IIE). However, evaluating these models poses a significant challenge.\nA crucial requirement in this field is the establishment of a comprehensive\nevaluation benchmark for accurately assessing editing results and providing\nvaluable insights for its further development. In response to this need, we\npropose I2EBench, a comprehensive benchmark designed to automatically evaluate\nthe quality of edited images produced by IIE models from multiple dimensions.\nI2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding\noriginal and diverse instructions. It offers three distinctive characteristics:\n1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation\ndimensions that cover both high-level and low-level aspects, providing a\ncomprehensive assessment of each IIE model. 2) Human Perception Alignment: To\nensure the alignment of our benchmark with human perception, we conducted an\nextensive user study for each evaluation dimension. 3) Valuable Research\nInsights: By analyzing the advantages and disadvantages of existing IIE models\nacross the 16 dimensions, we offer valuable research insights to guide future\ndevelopment in the field. We will open-source I2EBench, including all\ninstructions, input images, human annotations, edited images from all evaluated\nmethods, and a simple script for evaluating the results from new IIE models.\nThe code, dataset and generated images from all IIE models are provided in\ngithub: https://github.com/cocoshe/I2EBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS2024, 15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14180v2",
    "published_date": "2024-08-26 11:08:44 UTC",
    "updated_date": "2024-09-27 13:12:04 UTC"
  },
  {
    "arxiv_id": "2408.14176v2",
    "title": "SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher",
    "authors": [
      "Trung Dao",
      "Thuan Hoang Nguyen",
      "Thanh Le",
      "Duc Vu",
      "Khoi Nguyen",
      "Cuong Pham",
      "Anh Tran"
    ],
    "abstract": "In this paper, we aim to enhance the performance of SwiftBrush, a prominent\none-step text-to-image diffusion model, to be competitive with its multi-step\nStable Diffusion counterpart. Initially, we explore the quality-diversity\ntrade-off between SwiftBrush and SD Turbo: the former excels in image\ndiversity, while the latter excels in image quality. This observation motivates\nour proposed modifications in the training methodology, including better weight\ninitialization and efficient LoRA training. Moreover, our introduction of a\nnovel clamped CLIP loss enhances image-text alignment and results in improved\nimage quality. Remarkably, by combining the weights of models trained with\nefficient LoRA and full training, we achieve a new state-of-the-art one-step\ndiffusion model, achieving an FID of 8.14 and surpassing all GAN-based and\nmulti-step Stable Diffusion models. The project page is available at\nhttps://swiftbrushv2.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV'24",
    "pdf_url": "http://arxiv.org/pdf/2408.14176v2",
    "published_date": "2024-08-26 10:42:53 UTC",
    "updated_date": "2024-08-27 04:59:58 UTC"
  },
  {
    "arxiv_id": "2408.14169v1",
    "title": "Dynamic Pricing for Electric Vehicle Charging",
    "authors": [
      "Arun Kumar Kalakanti",
      "Shrisha Rao"
    ],
    "abstract": "Dynamic pricing is a promising strategy to address the challenges of smart\ncharging, as traditional time-of-use (ToU) rates and stationary pricing (SP) do\nnot dynamically react to changes in operating conditions, reducing revenue for\ncharging station (CS) vendors and affecting grid stability. Previous studies\nevaluated single objectives or linear combinations of objectives for EV CS\npricing solutions, simplifying trade-offs and preferences among objectives. We\ndevelop a novel formulation for the dynamic pricing problem by addressing\nmultiple conflicting objectives efficiently instead of solely focusing on one\nobjective or metric, as in earlier works. We find optimal trade-offs or Pareto\nsolutions efficiently using Non-dominated Sorting Genetic Algorithms (NSGA) II\nand NSGA III. A dynamic pricing model quantifies the relationship between\ndemand and price while simultaneously solving multiple conflicting objectives,\nsuch as revenue, quality of service (QoS), and peak-to-average ratios (PAR). A\nsingle method can only address some of the above aspects of dynamic pricing\ncomprehensively. We present a three-part dynamic pricing approach using a\nBayesian model, multi-objective optimization, and multi-criteria\ndecision-making (MCDM) using pseudo-weight vectors. To address the research gap\nin CS pricing, our method selects solutions using revenue, QoS, and PAR metrics\nsimultaneously. Two California charging sites' real-world data validates our\napproach.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.14169v1",
    "published_date": "2024-08-26 10:32:21 UTC",
    "updated_date": "2024-08-26 10:32:21 UTC"
  },
  {
    "arxiv_id": "2408.14158v2",
    "title": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning",
    "authors": [
      "Wei An",
      "Xiao Bi",
      "Guanting Chen",
      "Shanhuang Chen",
      "Chengqi Deng",
      "Honghui Ding",
      "Kai Dong",
      "Qiushi Du",
      "Wenjun Gao",
      "Kang Guan",
      "Jianzhong Guo",
      "Yongqiang Guo",
      "Zhe Fu",
      "Ying He",
      "Panpan Huang",
      "Jiashi Li",
      "Wenfeng Liang",
      "Xiaodong Liu",
      "Xin Liu",
      "Yiyuan Liu",
      "Yuxuan Liu",
      "Shanghao Lu",
      "Xuan Lu",
      "Xiaotao Nie",
      "Tian Pei",
      "Junjie Qiu",
      "Hui Qu",
      "Zehui Ren",
      "Zhangli Sha",
      "Xuecheng Su",
      "Xiaowen Sun",
      "Yixuan Tan",
      "Minghui Tang",
      "Shiyu Wang",
      "Yaohui Wang",
      "Yongji Wang",
      "Ziwei Xie",
      "Yiliang Xiong",
      "Yanhong Xu",
      "Shengfeng Ye",
      "Shuiping Yu",
      "Yukun Zha",
      "Liyue Zhang",
      "Haowei Zhang",
      "Mingchuan Zhang",
      "Wentao Zhang",
      "Yichao Zhang",
      "Chenggang Zhao",
      "Yao Zhao",
      "Shangyan Zhou",
      "Shunfeng Zhou",
      "Yuheng Zou"
    ],
    "abstract": "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has\nexponentially increased demands of computational power and bandwidth. This,\ncombined with the high costs of faster computing chips and interconnects, has\nsignificantly inflated High Performance Computing (HPC) construction costs. To\naddress these challenges, we introduce the Fire-Flyer AI-HPC architecture, a\nsynergistic hardware-software co-design framework and its best practices. For\nDL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved\nperformance approximating the DGX-A100 while reducing costs by half and energy\nconsumption by 40%. We specifically engineered HFReduce to accelerate allreduce\ncommunication and implemented numerous measures to keep our Computation-Storage\nIntegrated Network congestion-free. Through our software stack, including\nHaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by\noverlapping computation and communication. Our system-oriented experience from\nDL training provides valuable insights to drive future advancements in AI-HPC.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "This is the preprint version of the paper accepted for presentation\n  at the 2024 International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'24). \\c{opyright} 2024 IEEE. Personal\n  use of this material is permitted. For other uses, permission from IEEE must\n  be obtained. Please refer to IEEE Xplore for the final published version",
    "pdf_url": "http://arxiv.org/pdf/2408.14158v2",
    "published_date": "2024-08-26 10:11:56 UTC",
    "updated_date": "2024-08-31 13:33:22 UTC"
  },
  {
    "arxiv_id": "2408.14153v3",
    "title": "Explaining Caption-Image Interactions in CLIP models with Second-Order Attributions",
    "authors": [
      "Lucas Möller",
      "Pascal Tilli",
      "Ngoc Thang Vu",
      "Sebastian Padó"
    ],
    "abstract": "Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nsuccess, it is, however, not understood how these models compare their two\ninputs. Common first-order feature-attribution methods can only provide limited\ninsights into dual-encoders since their predictions depend on\nfeature-interactions rather than on individual features. In this paper, we\nfirst derive a second-order method enabling the attribution of predictions by\nany differentiable dual encoder onto feature-interactions between its inputs.\nSecond, we apply our method to CLIP models and show that they learn\nfine-grained correspondences between parts of captions and regions in images.\nThey match objects across input modes also account for mismatches. This\nvisual-linguistic grounding ability, however, varies heavily between object\nclasses and exhibits pronounced out-of-domain effects. We can identify\nindividual errors as well as systematic failure categories including object\ncoverage, unusual scenes and correlated contexts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14153v3",
    "published_date": "2024-08-26 09:55:34 UTC",
    "updated_date": "2025-03-06 09:00:18 UTC"
  },
  {
    "arxiv_id": "2408.14134v3",
    "title": "Exploring the Potential of Large Language Models for Heterophilic Graphs",
    "authors": [
      "Yuxia Wu",
      "Shujie Li",
      "Yuan Fang",
      "Chuan Shi"
    ],
    "abstract": "Large language models (LLMs) have presented significant opportunities to\nenhance various machine learning applications, including graph neural networks\n(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more\neffectively interpret and utilize textual data to better characterize\nheterophilic graphs, where neighboring nodes often have different labels.\nHowever, existing approaches for heterophilic graphs overlook the rich textual\ndata associated with nodes, which could unlock deeper insights into their\nheterophilic contexts. In this work, we explore the potential of LLMs for\nmodeling heterophilic graphs and propose a novel two-stage framework:\nLLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first\nstage, we fine-tune the LLM to better identify homophilic and heterophilic\nedges based on the textual content of their nodes. In the second stage, we\nadaptively manage message propagation in GNNs for different edge types based on\nnode features, structures, and heterophilic or homophilic characteristics. To\ncope with the computational demands when deploying LLMs in practical scenarios,\nwe further explore model distillation techniques to fine-tune smaller, more\nefficient models that maintain competitive performance. Extensive experiments\nvalidate the effectiveness of our framework, demonstrating the feasibility of\nusing LLMs to enhance node classification on heterophilic graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.14134v3",
    "published_date": "2024-08-26 09:29:56 UTC",
    "updated_date": "2025-02-15 12:53:29 UTC"
  },
  {
    "arxiv_id": "2408.14523v2",
    "title": "Retrieval Augmented Generation for Dynamic Graph Modeling",
    "authors": [
      "Yuxia Wu",
      "Lizi Liao",
      "Yuan Fang"
    ],
    "abstract": "Modeling dynamic graphs, such as those found in social networks,\nrecommendation systems, and e-commerce platforms, is crucial for capturing\nevolving relationships and delivering relevant insights over time. Traditional\napproaches primarily rely on graph neural networks with temporal components or\nsequence generation models, which often focus narrowly on the historical\ncontext of target nodes. This limitation restricts the ability to adapt to new\nand emerging patterns in dynamic graphs. To address this challenge, we propose\na novel framework, Retrieval-Augmented Generation for Dynamic Graph modeling\n(RAG4DyG), which enhances dynamic graph predictions by incorporating\ncontextually and temporally relevant examples from broader graph structures.\nOur approach includes a time- and context-aware contrastive learning module to\nidentify high-quality demonstrations and a graph fusion strategy to effectively\nintegrate these examples with historical contexts. The proposed framework is\ndesigned to be effective in both transductive and inductive scenarios, ensuring\nadaptability to previously unseen nodes and evolving graph structures.\nExtensive experiments across multiple real-world datasets demonstrate the\neffectiveness of RAG4DyG in improving predictive accuracy and adaptability for\ndynamic graph modeling. The code and datasets are publicly available at\nhttps://github.com/YuxiaWu/RAG4DyG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.14523v2",
    "published_date": "2024-08-26 09:23:35 UTC",
    "updated_date": "2025-04-27 07:28:13 UTC"
  },
  {
    "arxiv_id": "2408.14119v1",
    "title": "Contrastive Learning Subspace for Text Clustering",
    "authors": [
      "Qian Yong",
      "Chen Chen",
      "Xiabing Zhou"
    ],
    "abstract": "Contrastive learning has been frequently investigated to learn effective\nrepresentations for text clustering tasks. While existing contrastive\nlearning-based text clustering methods only focus on modeling instance-wise\nsemantic similarity relationships, they ignore contextual information and\nunderlying relationships among all instances that needs to be clustered. In\nthis paper, we propose a novel text clustering approach called Subspace\nContrastive Learning (SCL) which models cluster-wise relationships among\ninstances. Specifically, the proposed SCL consists of two main modules: (1) a\nself-expressive module that constructs virtual positive samples and (2) a\ncontrastive learning module that further learns a discriminative subspace to\ncapture task-specific cluster-wise relationships among texts. Experimental\nresults show that the proposed SCL method not only has achieved superior\nresults on multiple task clustering datasets but also has less complexity in\npositive sample construction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14119v1",
    "published_date": "2024-08-26 09:08:26 UTC",
    "updated_date": "2024-08-26 09:08:26 UTC"
  },
  {
    "arxiv_id": "2408.14101v2",
    "title": "Estimating Causal Effects from Learned Causal Networks",
    "authors": [
      "Anna Raichev",
      "Alexander Ihler",
      "Jin Tian",
      "Rina Dechter"
    ],
    "abstract": "The standard approach to answering an identifiable causal-effect query (e.g.,\n$P(Y|do(X)$) when given a causal diagram and observational data is to first\ngenerate an estimand, or probabilistic expression over the observable\nvariables, which is then evaluated using the observational data. In this paper,\nwe propose an alternative paradigm for answering causal-effect queries over\ndiscrete observable variables. We propose to instead learn the causal Bayesian\nnetwork and its confounding latent variables directly from the observational\ndata. Then, efficient probabilistic graphical model (PGM) algorithms can be\napplied to the learned model to answer queries. Perhaps surprisingly, we show\nthat this \\emph{model completion} learning approach can be more effective than\nestimand approaches, particularly for larger models in which the estimand\nexpressions become computationally difficult.\n  We illustrate our method's potential using a benchmark collection of Bayesian\nnetworks and synthetically generated causal models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14101v2",
    "published_date": "2024-08-26 08:39:09 UTC",
    "updated_date": "2024-08-27 09:54:04 UTC"
  },
  {
    "arxiv_id": "2409.00091v1",
    "title": "Classification of Safety Events at Nuclear Sites using Large Language Models",
    "authors": [
      "Mishca de Costa",
      "Muhammad Anwar",
      "Daniel Lau",
      "Issam Hammad"
    ],
    "abstract": "This paper proposes the development of a Large Language Model (LLM) based\nmachine learning classifier designed to categorize Station Condition Records\n(SCRs) at nuclear power stations into safety-related and non-safety-related\ncategories. The primary objective is to augment the existing manual review\nprocess by enhancing the efficiency and accuracy of the safety classification\nprocess at nuclear stations. The paper discusses experiments performed to\nclassify a labeled SCR dataset and evaluates the performance of the classifier.\nIt explores the construction of several prompt variations and their observed\neffects on the LLM's decision-making process. Additionally, it introduces a\nnumerical scoring mechanism that could offer a more nuanced and flexible\napproach to SCR safety classification. This method represents an innovative\nstep in nuclear safety management, providing a scalable tool for the\nidentification of safety events.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00091v1",
    "published_date": "2024-08-26 08:21:21 UTC",
    "updated_date": "2024-08-26 08:21:21 UTC"
  },
  {
    "arxiv_id": "2408.14090v2",
    "title": "Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects",
    "authors": [
      "Daniele De Sensi",
      "Lorenzo Pichetti",
      "Flavio Vella",
      "Tiziano De Matteis",
      "Zebin Ren",
      "Luigi Fusco",
      "Matteo Turisini",
      "Daniele Cesarini",
      "Kurt Lust",
      "Animesh Trivedi",
      "Duncan Roweth",
      "Filippo Spiga",
      "Salvatore Di Girolamo",
      "Torsten Hoefler"
    ],
    "abstract": "Multi-GPU nodes are increasingly common in the rapidly evolving landscape of\nexascale supercomputers. On these systems, GPUs on the same node are connected\nthrough dedicated networks, with bandwidths up to a few terabits per second.\nHowever, gauging performance expectations and maximizing system efficiency is\nchallenging due to different technologies, design options, and software layers.\nThis paper comprehensively characterizes three supercomputers - Alps, Leonardo,\nand LUMI - each with a unique architecture and design. We focus on performance\nevaluation of intra-node and inter-node interconnects on up to 4096 GPUs, using\na mix of intra-node and inter-node benchmarks. By analyzing its limitations and\nopportunities, we aim to offer practical guidance to researchers, system\narchitects, and software developers dealing with multi-GPU supercomputing. Our\nresults show that there is untapped bandwidth, and there are still many\nopportunities for optimization, ranging from network to software optimization.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.NI",
      "cs.PF",
      "C.2.4; C.5.1; C.2.1; C.4"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14090v2",
    "published_date": "2024-08-26 08:20:50 UTC",
    "updated_date": "2024-11-15 17:55:40 UTC"
  },
  {
    "arxiv_id": "2409.00090v1",
    "title": "Evaluating ChatGPT on Nuclear Domain-Specific Data",
    "authors": [
      "Muhammad Anwar",
      "Mischa de Costa",
      "Issam Hammad",
      "Daniel Lau"
    ],
    "abstract": "This paper examines the application of ChatGPT, a large language model (LLM),\nfor question-and-answer (Q&A) tasks in the highly specialized field of nuclear\ndata. The primary focus is on evaluating ChatGPT's performance on a curated\ntest dataset, comparing the outcomes of a standalone LLM with those generated\nthrough a Retrieval Augmented Generation (RAG) approach. LLMs, despite their\nrecent advancements, are prone to generating incorrect or 'hallucinated'\ninformation, which is a significant limitation in applications requiring high\naccuracy and reliability. This study explores the potential of utilizing RAG in\nLLMs, a method that integrates external knowledge bases and sophisticated\nretrieval techniques to enhance the accuracy and relevance of generated\noutputs. In this context, the paper evaluates ChatGPT's ability to answer\ndomain-specific questions, employing two methodologies: A) direct response from\nthe LLM, and B) response from the LLM within a RAG framework. The effectiveness\nof these methods is assessed through a dual mechanism of human and LLM\nevaluation, scoring the responses for correctness and other metrics. The\nfindings underscore the improvement in performance when incorporating a RAG\npipeline in an LLM, particularly in generating more accurate and contextually\nappropriate responses for nuclear domain-specific queries. Additionally, the\npaper highlights alternative approaches to further refine and improve the\nquality of answers in such specialized domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00090v1",
    "published_date": "2024-08-26 08:17:42 UTC",
    "updated_date": "2024-08-26 08:17:42 UTC"
  },
  {
    "arxiv_id": "2408.14080v4",
    "title": "SONICS: Synthetic Or Not -- Identifying Counterfeit Songs",
    "authors": [
      "Md Awsafur Rahman",
      "Zaber Ibn Abdul Hakim",
      "Najibul Haque Sarker",
      "Bishmoy Paul",
      "Shaikh Anowarul Fattah"
    ],
    "abstract": "The recent surge in AI-generated songs presents exciting possibilities and\nchallenges. These innovations necessitate the ability to distinguish between\nhuman-composed and synthetic songs to safeguard artistic integrity and protect\nhuman musical artistry. Existing research and datasets in fake song detection\nonly focus on singing voice deepfake detection (SVDD), where the vocals are\nAI-generated but the instrumental music is sourced from real songs. However,\nthese approaches are inadequate for detecting contemporary end-to-end\nartificial songs where all components (vocals, music, lyrics, and style) could\nbe AI-generated. Additionally, existing datasets lack music-lyrics diversity,\nlong-duration songs, and open-access fake songs. To address these gaps, we\nintroduce SONICS, a novel dataset for end-to-end Synthetic Song Detection\n(SSD), comprising over 97k songs (4,751 hours) with over 49k synthetic songs\nfrom popular platforms like Suno and Udio. Furthermore, we highlight the\nimportance of modeling long-range temporal dependencies in songs for effective\nauthenticity detection, an aspect entirely overlooked in existing methods. To\nutilize long-range patterns, we introduce SpecTTTra, a novel architecture that\nsignificantly improves time and memory efficiency over conventional CNN and\nTransformer-based models. For long songs, our top-performing variant\noutperforms ViT by 8% in F1 score, is 38% faster, and uses 26% less memory,\nwhile also surpassing ConvNeXt with a 1% F1 score gain, 20% speed boost, and\n67% memory reduction.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ICLR 2025. Project url: https://github.com/awsaf49/sonics",
    "pdf_url": "http://arxiv.org/pdf/2408.14080v4",
    "published_date": "2024-08-26 08:02:57 UTC",
    "updated_date": "2025-02-25 03:22:49 UTC"
  },
  {
    "arxiv_id": "2408.14069v1",
    "title": "Revisiting Vacuous Reduct Semantics for Abstract Argumentation (Extended Version)",
    "authors": [
      "Lydia Blümel",
      "Matthias Thimm"
    ],
    "abstract": "We consider the notion of a vacuous reduct semantics for abstract\nargumentation frameworks, which, given two abstract argumentation semantics\n{\\sigma} and {\\tau}, refines {\\sigma} (base condition) by accepting only those\n{\\sigma}-extensions that have no non-empty {\\tau}-extension in their reduct\n(vacuity condition). We give a systematic overview on vacuous reduct semantics\nresulting from combining different admissibility-based and conflict-free\nsemantics and present a principle-based analysis of vacuous reduct semantics in\ngeneral. We provide criteria for the inheritance of principle satisfaction by a\nvacuous reduct semantics from its base and vacuity condition for established as\nwell as recently introduced principles in the context of weak argumentation\nsemantics. We also conduct a principle-based analysis for the special case of\nundisputed semantics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The paper has been accepted at ECAI 2024, this is an extended version\n  including proofs of technical results",
    "pdf_url": "http://arxiv.org/pdf/2408.14069v1",
    "published_date": "2024-08-26 07:50:49 UTC",
    "updated_date": "2024-08-26 07:50:49 UTC"
  },
  {
    "arxiv_id": "2408.14055v1",
    "title": "HAPM -- Hardware Aware Pruning Method for CNN hardware accelerators in resource constrained devices",
    "authors": [
      "Federico Nicolas Peccia",
      "Luciano Ferreyro",
      "Alejandro Furfaro"
    ],
    "abstract": "During the last years, algorithms known as Convolutional Neural Networks\n(CNNs) had become increasingly popular, expanding its application range to\nseveral areas. In particular, the image processing field has experienced a\nremarkable advance thanks to this algorithms. In IoT, a wide research field\naims to develop hardware capable of execute them at the lowest possible energy\ncost, but keeping acceptable image inference time. One can get around this\napparently conflicting objectives by applying design and training techniques.\nThe present work proposes a generic hardware architecture ready to be\nimplemented on FPGA devices, supporting a wide range of configurations which\nallows the system to run different neural network architectures, dynamically\nexploiting the sparsity caused by pruning techniques in the mathematical\noperations present in this kind of algorithms. The inference speed of the\ndesign is evaluated over different resource constrained FPGA devices. Finally,\nthe standard pruning algorithm is compared against a custom pruning technique\nspecifically designed to exploit the scheduling properties of this hardware\naccelerator. We demonstrate that our hardware-aware pruning algorithm achieves\na remarkable improvement of a 45 % in inference time compared to a network\npruned using the standard algorithm.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "8 pages, 7 figure, thesis for the title of Electronic Engineer\n  attained in 2021 at the Universidad Tecnologica Nacional (UTN), Argentina",
    "pdf_url": "http://arxiv.org/pdf/2408.14055v1",
    "published_date": "2024-08-26 07:27:12 UTC",
    "updated_date": "2024-08-26 07:27:12 UTC"
  },
  {
    "arxiv_id": "2408.14045v1",
    "title": "Beyond Detection: Leveraging Large Language Models for Cyber Attack Prediction in IoT Networks",
    "authors": [
      "Alaeddine Diaf",
      "Abdelaziz Amara Korba",
      "Nour Elislem Karabadji",
      "Yacine Ghamri-Doudane"
    ],
    "abstract": "In recent years, numerous large-scale cyberattacks have exploited Internet of\nThings (IoT) devices, a phenomenon that is expected to escalate with the\ncontinuing proliferation of IoT technology. Despite considerable efforts in\nattack detection, intrusion detection systems remain mostly reactive,\nresponding to specific patterns or observed anomalies. This work proposes a\nproactive approach to anticipate and mitigate malicious activities before they\ncause damage. This paper proposes a novel network intrusion prediction\nframework that combines Large Language Models (LLMs) with Long Short Term\nMemory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:\na fine-tuned Generative Pre-trained Transformer (GPT) model for predicting\nnetwork traffic and a fine-tuned Bidirectional Encoder Representations from\nTransformers (BERT) for evaluating the predicted traffic. The LSTM classifier\nmodel then identifies malicious packets among these predictions. Our framework,\nevaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant\nimprovement in predictive capabilities, achieving an overall accuracy of 98%,\noffering a robust solution to IoT cybersecurity challenges.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14045v1",
    "published_date": "2024-08-26 06:57:22 UTC",
    "updated_date": "2024-08-26 06:57:22 UTC"
  },
  {
    "arxiv_id": "2409.00089v1",
    "title": "Watermarking Techniques for Large Language Models: A Survey",
    "authors": [
      "Yuqing Liang",
      "Jiancheng Xiao",
      "Wensheng Gan",
      "Philip S. Yu"
    ],
    "abstract": "With the rapid advancement and extensive application of artificial\nintelligence technology, large language models (LLMs) are extensively used to\nenhance production, creativity, learning, and work efficiency across various\ndomains. However, the abuse of LLMs also poses potential harm to human society,\nsuch as intellectual property rights issues, academic misconduct, false\ncontent, and hallucinations. Relevant research has proposed the use of LLM\nwatermarking to achieve IP protection for LLMs and traceability of multimedia\ndata output by LLMs. To our knowledge, this is the first thorough review that\ninvestigates and analyzes LLM watermarking technology in detail. This review\nbegins by recounting the history of traditional watermarking technology, then\nanalyzes the current state of LLM watermarking research, and thoroughly\nexamines the inheritance and relevance of these techniques. By analyzing their\ninheritance and relevance, this review can provide research with ideas for\napplying traditional digital watermarking techniques to LLM watermarking, to\npromote the cross-integration and innovation of watermarking technology. In\naddition, this review examines the pros and cons of LLM watermarking.\nConsidering the current multimodal development trend of LLMs, it provides a\ndetailed analysis of emerging multimodal LLM watermarking, such as visual and\naudio data, to offer more reference ideas for relevant research. This review\ndelves into the challenges and future prospects of current watermarking\ntechnologies, offering valuable insights for future LLM watermarking research\nand applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Preprint. 19 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.00089v1",
    "published_date": "2024-08-26 06:50:11 UTC",
    "updated_date": "2024-08-26 06:50:11 UTC"
  },
  {
    "arxiv_id": "2408.14042v2",
    "title": "PAGE: Parametric Generative Explainer for Graph Neural Network",
    "authors": [
      "Yang Qiu",
      "Wei Liu",
      "Jun Wang",
      "Ruixuan Li"
    ],
    "abstract": "This article introduces PAGE, a parameterized generative interpretive\nframework. PAGE is capable of providing faithful explanations for any graph\nneural network without necessitating prior knowledge or internal details.\nSpecifically, we train the auto-encoder to generate explanatory substructures\nby designing appropriate training strategy. Due to the dimensionality reduction\nof features in the latent space of the auto-encoder, it becomes easier to\nextract causal features leading to the model's output, which can be easily\nemployed to generate explanations. To accomplish this, we introduce an\nadditional discriminator to capture the causality between latent causal\nfeatures and the model's output. By designing appropriate optimization\nobjectives, the well-trained discriminator can be employed to constrain the\nencoder in generating enhanced causal features. Finally, these features are\nmapped to substructures of the input graph through the decoder to serve as\nexplanations. Compared to existing methods, PAGE operates at the sample scale\nrather than nodes or edges, eliminating the need for perturbation or encoding\nprocesses as seen in previous methods. Experimental results on both\nartificially synthesized and real-world datasets demonstrate that our approach\nnot only exhibits the highest faithfulness and accuracy but also significantly\noutperforms baseline models in terms of efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14042v2",
    "published_date": "2024-08-26 06:39:49 UTC",
    "updated_date": "2024-09-06 08:13:09 UTC"
  },
  {
    "arxiv_id": "2408.14520v3",
    "title": "Towards Graph Prompt Learning: A Survey and Beyond",
    "authors": [
      "Qingqing Long",
      "Yuchen Yan",
      "Peiyan Zhang",
      "Chen Fang",
      "Wentao Cui",
      "Zhiyuan Ning",
      "Meng Xiao",
      "Ning Cao",
      "Xiao Luo",
      "Lingjun Xu",
      "Shiyue Jiang",
      "Zheng Fang",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Yuanchun Zhou"
    ],
    "abstract": "Large-scale \"pre-train and prompt learning\" paradigms have demonstrated\nremarkable adaptability, enabling broad applications across diverse domains\nsuch as question answering, image recognition, and multimodal retrieval. This\napproach fully leverages the potential of large-scale pre-trained models,\nreducing downstream data requirements and computational costs while enhancing\nmodel applicability across various tasks. Graphs, as versatile data structures\nthat capture relationships between entities, play pivotal roles in fields such\nas social network analysis, recommender systems, and biological graphs. Despite\nthe success of pre-train and prompt learning paradigms in Natural Language\nProcessing (NLP) and Computer Vision (CV), their application in graph domains\nremains nascent. In graph-structured data, not only do the node and edge\nfeatures often have disparate distributions, but the topological structures\nalso differ significantly. This diversity in graph data can lead to\nincompatible patterns or gaps between pre-training and fine-tuning on\ndownstream graphs. We aim to bridge this gap by summarizing methods for\nalleviating these disparities. This includes exploring prompt design\nmethodologies, comparing related techniques, assessing application scenarios\nand datasets, and identifying unresolved problems and challenges. This survey\ncategorizes over 100 relevant works in this field, summarizing general design\nprinciples and the latest applications, including text-attributed graphs,\nmolecules, proteins, and recommendation systems. Through this extensive review,\nwe provide a foundational understanding of graph prompt learning, aiming to\nimpact not only the graph mining community but also the broader Artificial\nGeneral Intelligence (AGI) community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "I have decided to temporarily withdraw this draft as I am in the\n  process of making further revisions to improve its content",
    "pdf_url": "http://arxiv.org/pdf/2408.14520v3",
    "published_date": "2024-08-26 06:36:42 UTC",
    "updated_date": "2024-09-24 09:43:35 UTC"
  },
  {
    "arxiv_id": "2408.14033v2",
    "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents",
    "authors": [
      "Ruochen Li",
      "Teerth Patel",
      "Qingyun Wang",
      "Xinya Du"
    ],
    "abstract": "Machine learning research, crucial for technological advancements and\ninnovation, often faces significant challenges due to its inherent complexity,\nslow pace of experimentation, and the necessity for specialized expertise.\nMotivated by this, we present a new systematic framework, autonomous Machine\nLearning Research with large language models (MLR-Copilot), designed to enhance\nmachine learning research productivity through the automatic generation and\nimplementation of research ideas using Large Language Model (LLM) agents. The\nframework consists of three phases: research idea generation, experiment\nimplementation, and implementation execution. First, existing research papers\nare used to generate hypotheses and experimental plans vis IdeaAgent powered by\nLLMs. Next, the implementation generation phase translates these plans into\nexecutables with ExperimentAgent. This phase leverages retrieved prototype code\nand optionally retrieves candidate models and data. Finally, the execution\nphase, also managed by ExperimentAgent, involves running experiments with\nmechanisms for human feedback and iterative debugging to enhance the likelihood\nof achieving executable research outcomes. We evaluate our framework on five\nmachine learning research tasks and the experimental results show the\nframework's potential to facilitate the research progress and innovations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14033v2",
    "published_date": "2024-08-26 05:55:48 UTC",
    "updated_date": "2024-09-02 05:55:06 UTC"
  },
  {
    "arxiv_id": "2408.14028v3",
    "title": "SurGen: Text-Guided Diffusion Model for Surgical Video Generation",
    "authors": [
      "Joseph Cho",
      "Samuel Schmidgall",
      "Cyril Zakka",
      "Mrudang Mathur",
      "Dhamanpreet Kaur",
      "Rohan Shad",
      "William Hiesinger"
    ],
    "abstract": "Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis. SurGen produces videos with the highest resolution\nand longest duration among existing surgical video generation models. We\nvalidate the visual and temporal quality of the outputs using standard image\nand video generation metrics. Additionally, we assess their alignment to the\ncorresponding text prompts through a deep learning classifier trained on\nsurgical data. Our results demonstrate the potential of diffusion models to\nserve as valuable educational tools for surgical trainees.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14028v3",
    "published_date": "2024-08-26 05:38:27 UTC",
    "updated_date": "2024-09-24 23:23:50 UTC"
  },
  {
    "arxiv_id": "2408.14023v1",
    "title": "Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos",
    "authors": [
      "Jiajun Fei",
      "Dian Li",
      "Zhidong Deng",
      "Zekun Wang",
      "Gang Liu",
      "Hui Wang"
    ],
    "abstract": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14023v1",
    "published_date": "2024-08-26 05:27:14 UTC",
    "updated_date": "2024-08-26 05:27:14 UTC"
  },
  {
    "arxiv_id": "2408.14016v1",
    "title": "Pixel-Aligned Multi-View Generation with Depth Guided Decoder",
    "authors": [
      "Zhenggang Tang",
      "Peiye Zhuang",
      "Chaoyang Wang",
      "Aliaksandr Siarohin",
      "Yash Kant",
      "Alexander Schwing",
      "Sergey Tulyakov",
      "Hsin-Ying Lee"
    ],
    "abstract": "The task of image-to-multi-view generation refers to generating novel views\nof an instance from a single image. Recent methods achieve this by extending\ntext-to-image latent diffusion models to multi-view version, which contains an\nVAE image encoder and a U-Net diffusion model. Specifically, these generation\nmethods usually fix VAE and finetune the U-Net only. However, the significant\ndownscaling of the latent vectors computed from the input images and\nindependent decoding leads to notable pixel-level misalignment across multiple\nviews. To address this, we propose a novel method for pixel-level\nimage-to-multi-view generation. Unlike prior work, we incorporate attention\nlayers across multi-view images in the VAE decoder of a latent video diffusion\nmodel. Specifically, we introduce a depth-truncated epipolar attention,\nenabling the model to focus on spatially adjacent regions while remaining\nmemory efficient. Applying depth-truncated attn is challenging during inference\nas the ground-truth depth is usually difficult to obtain and pre-trained depth\nestimation models is hard to provide accurate depth. Thus, to enhance the\ngeneralization to inaccurate depth when ground truth depth is missing, we\nperturb depth inputs during training. During inference, we employ a rapid\nmulti-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the\ndepth-truncated epipolar attention. Our model enables better pixel alignment\nacross multi-view images. Moreover, we demonstrate the efficacy of our approach\nin improving downstream multi-view to 3D reconstruction tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14016v1",
    "published_date": "2024-08-26 04:56:41 UTC",
    "updated_date": "2024-08-26 04:56:41 UTC"
  },
  {
    "arxiv_id": "2409.12922v1",
    "title": "AI Thinking: A framework for rethinking artificial intelligence in practice",
    "authors": [
      "Denis Newman-Griffis"
    ],
    "abstract": "Artificial intelligence is transforming the way we work with information\nacross disciplines and practical contexts. A growing range of disciplines are\nnow involved in studying, developing, and assessing the use of AI in practice,\nbut these disciplines often employ conflicting understandings of what AI is and\nwhat is involved in its use. New, interdisciplinary approaches are needed to\nbridge competing conceptualisations of AI in practice and help shape the future\nof AI use. I propose a novel conceptual framework called AI Thinking, which\nmodels key decisions and considerations involved in AI use across disciplinary\nperspectives. The AI Thinking model addresses five practice-based competencies\ninvolved in applying AI in context: motivating AI use in information processes,\nformulating AI methods, assessing available tools and technologies, selecting\nappropriate data, and situating AI in the sociotechnical contexts it is used\nin. A hypothetical case study is provided to illustrate the application of AI\nThinking in practice. This article situates AI Thinking in broader\ncross-disciplinary discourses of AI, including its connections to ongoing\ndiscussions around AI literacy and AI-driven innovation. AI Thinking can help\nto bridge divides between academic disciplines and diverse contexts of AI use,\nand to reshape the future of AI in practice.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "30 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12922v1",
    "published_date": "2024-08-26 04:41:21 UTC",
    "updated_date": "2024-08-26 04:41:21 UTC"
  },
  {
    "arxiv_id": "2408.14009v1",
    "title": "Optimizing TD3 for 7-DOF Robotic Arm Grasping: Overcoming Suboptimality with Exploration-Enhanced Contrastive Learning",
    "authors": [
      "Wen-Han Hsieh",
      "Jen-Yuan Chang"
    ],
    "abstract": "In actor-critic-based reinforcement learning algorithms such as Twin Delayed\nDeep Deterministic policy gradient (TD3), insufficient exploration of the\nspatial space can result in suboptimal policies when controlling 7-DOF robotic\narms. To address this issue, we propose a novel Exploration-Enhanced\nContrastive Learning (EECL) module that improves exploration by providing\nadditional rewards for encountering novel states. Our module stores previously\nexplored states in a buffer and identifies new states by comparing them with\nhistorical data using Euclidean distance within a K-dimensional tree (KDTree)\nframework. When the agent explores new states, exploration rewards are\nassigned. These rewards are then integrated into the TD3 algorithm, ensuring\nthat the Q-learning process incorporates these signals, promoting more\neffective strategy optimization. We evaluate our method on the robosuite panda\nlift task, demonstrating that it significantly outperforms the baseline TD3 in\nterms of both efficiency and convergence speed in the tested environment.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "4 pages, 2 figures, IEEE-ICKII-2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14009v1",
    "published_date": "2024-08-26 04:30:59 UTC",
    "updated_date": "2024-08-26 04:30:59 UTC"
  },
  {
    "arxiv_id": "2408.14008v1",
    "title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models",
    "authors": [
      "Qihang Ge",
      "Wei Sun",
      "Yu Zhang",
      "Yunhao Li",
      "Zhongpeng Ji",
      "Fengyu Sun",
      "Shangling Jui",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ],
    "abstract": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14008v1",
    "published_date": "2024-08-26 04:29:52 UTC",
    "updated_date": "2024-08-26 04:29:52 UTC"
  },
  {
    "arxiv_id": "2408.13991v1",
    "title": "Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective",
    "authors": [
      "Quanziang Wang",
      "Renzhen Wang",
      "Yichen Wu",
      "Xixi Jia",
      "Minghao Zhou",
      "Deyu Meng"
    ],
    "abstract": "In online continual learning (CL), models trained on changing distributions\neasily forget previously learned knowledge and bias toward newly received\ntasks. To address this issue, we present Continual Bias Adaptor (CBA), a\nbi-level framework that augments the classification network to adapt to\ncatastrophic distribution shifts during training, enabling the network to\nachieve a stable consolidation of all seen tasks. However, the CBA module\nadjusts distribution shifts in a class-specific manner, exacerbating the\nstability gap issue and, to some extent, fails to meet the need for continual\ntesting in online CL. To mitigate this challenge, we further propose a novel\nclass-agnostic CBA module that separately aggregates the posterior\nprobabilities of classes from new and old tasks, and applies a stable\nadjustment to the resulting posterior probabilities. We combine the two kinds\nof CBA modules into a unified Dual-CBA module, which thus is capable of\nadapting to catastrophic distribution shifts and simultaneously meets the\nreal-time testing requirements of online CL. Besides, we propose Incremental\nBatch Normalization (IBN), a tailored BN module to re-estimate its population\nstatistics for alleviating the feature bias arising from the inner loop\noptimization problem of our bi-level framework. To validate the effectiveness\nof the proposed method, we theoretically provide some insights into how it\nmitigates catastrophic distribution shifts, and empirically demonstrate its\nsuperiority through extensive experiments based on four rehearsal-based\nbaselines and three public continual learning benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13991v1",
    "published_date": "2024-08-26 03:19:52 UTC",
    "updated_date": "2024-08-26 03:19:52 UTC"
  },
  {
    "arxiv_id": "2408.13988v1",
    "title": "Automatic Medical Report Generation: Methods and Applications",
    "authors": [
      "Li Guo",
      "Anas M. Tahir",
      "Dong Zhang",
      "Z. Jane Wang",
      "Rabab K. Ward"
    ],
    "abstract": "The increasing demand for medical imaging has surpassed the capacity of\navailable radiologists, leading to diagnostic delays and potential\nmisdiagnoses. Artificial intelligence (AI) techniques, particularly in\nautomatic medical report generation (AMRG), offer a promising solution to this\ndilemma. This review comprehensively examines AMRG methods from 2021 to 2024.\nIt (i) presents solutions to primary challenges in this field, (ii) explores\nAMRG applications across various imaging modalities, (iii) introduces publicly\navailable datasets, (iv) outlines evaluation metrics, (v) identifies techniques\nthat significantly enhance model performance, and (vi) discusses unresolved\nissues and potential future research directions. This paper aims to provide a\ncomprehensive understanding of the existing literature and inspire valuable\nfuture research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "42 pages and 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.13988v1",
    "published_date": "2024-08-26 03:02:41 UTC",
    "updated_date": "2024-08-26 03:02:41 UTC"
  },
  {
    "arxiv_id": "2409.00087v1",
    "title": "A Lightweight Human Pose Estimation Approach for Edge Computing-Enabled Metaverse with Compressive Sensing",
    "authors": [
      "Nguyen Quang Hieu",
      "Dinh Thai Hoang",
      "Diep N. Nguyen"
    ],
    "abstract": "The ability to estimate 3D movements of users over edge computing-enabled\nnetworks, such as 5G/6G networks, is a key enabler for the new era of extended\nreality (XR) and Metaverse applications. Recent advancements in deep learning\nhave shown advantages over optimization techniques for estimating 3D human\nposes given spare measurements from sensor signals, i.e., inertial measurement\nunit (IMU) sensors attached to the XR devices. However, the existing works lack\napplicability to wireless systems, where transmitting the IMU signals over\nnoisy wireless networks poses significant challenges. Furthermore, the\npotential redundancy of the IMU signals has not been considered, resulting in\nhighly redundant transmissions. In this work, we propose a novel approach for\nredundancy removal and lightweight transmission of IMU signals over noisy\nwireless environments. Our approach utilizes a random Gaussian matrix to\ntransform the original signal into a lower-dimensional space. By leveraging the\ncompressive sensing theory, we have proved that the designed Gaussian matrix\ncan project the signal into a lower-dimensional space and preserve the\nSet-Restricted Eigenvalue condition, subject to a power transmission\nconstraint. Furthermore, we develop a deep generative model at the receiver to\nrecover the original IMU signals from noisy compressed data, thus enabling the\ncreation of 3D human body movements at the receiver for XR and Metaverse\napplications. Simulation results on a real-world IMU dataset show that our\nframework can achieve highly accurate 3D human poses of the user using only\n$82\\%$ of the measurements from the original signals. This is comparable to an\noptimization-based approach, i.e., Lasso, but is an order of magnitude faster.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00087v1",
    "published_date": "2024-08-26 02:57:23 UTC",
    "updated_date": "2024-08-26 02:57:23 UTC"
  },
  {
    "arxiv_id": "2408.13987v1",
    "title": "Focused Large Language Models are Stable Many-Shot Learners",
    "authors": [
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Xinglin Wang",
      "Yueqi Zhang",
      "Chuyi Tan",
      "Boyuan Pan",
      "Heda Wang",
      "Yao Hu",
      "Kan Li"
    ],
    "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve\nrapid task adaptation by learning from demonstrations. With the increase in\navailable context length of LLMs, recent experiments have shown that the\nperformance of ICL does not necessarily scale well in many-shot (demonstration)\nsettings. We theoretically and experimentally confirm that the reason lies in\nmore demonstrations dispersing the model attention from the query, hindering\nits understanding of key content. Inspired by how humans learn from examples,\nwe propose a training-free method FocusICL, which conducts triviality filtering\nto avoid attention being diverted by unimportant contents at token-level and\noperates hierarchical attention to further ensure sufficient attention towards\ncurrent query at demonstration-level. We also design an efficient\nhyperparameter searching strategy for FocusICL based on model perplexity of\ndemonstrations. Comprehensive experiments validate that FocusICL achieves an\naverage performance improvement of 5.2% over vanilla ICL and scales well with\nmany-shot demonstrations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.13987v1",
    "published_date": "2024-08-26 02:53:24 UTC",
    "updated_date": "2024-08-26 02:53:24 UTC"
  },
  {
    "arxiv_id": "2408.13986v2",
    "title": "AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction",
    "authors": [
      "Jie Feng",
      "Yuwei Du",
      "Jie Zhao",
      "Yong Li"
    ],
    "abstract": "Next location prediction plays a crucial role in various real-world\napplications. Recently, due to the limitation of existing deep learning\nmethods, attempts have been made to apply large language models (LLMs) to\nzero-shot next location prediction task. However, they directly generate the\nfinal output using LLMs without systematic design, which limits the potential\nof LLMs to uncover complex mobility patterns and underestimates their extensive\nreserve of global geospatial knowledge. In this paper, we introduce AgentMove,\na systematic agentic prediction framework to achieve generalized next location\nprediction. In AgentMove, we first decompose the mobility prediction task and\ndesign specific modules to complete them, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments utilizing mobility data from two distinct sources reveal\nthat AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of\n12 metrics and it shows robust predictions with various LLMs as base and also\nless geographical bias across cities. Our codes are available via\nhttps://github.com/tsinghua-fib-lab/AgentMove.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NAACL 2025 as main conference paper,\n  https://github.com/tsinghua-fib-lab/AgentMove",
    "pdf_url": "http://arxiv.org/pdf/2408.13986v2",
    "published_date": "2024-08-26 02:36:55 UTC",
    "updated_date": "2025-02-09 06:16:18 UTC"
  },
  {
    "arxiv_id": "2408.13979v1",
    "title": "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models",
    "authors": [
      "Shuai Fu",
      "Xiequn Wang",
      "Qiushi Huang",
      "Yu Zhang"
    ],
    "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs),\nsuch as CLIP, soft-prompt tuning has become a popular method for adapting these\nmodels to various downstream tasks. However, few works delve into the inherent\nproperties of learnable soft-prompt vectors, specifically the impact of their\nnorms to the performance of VLMs. This motivates us to pose an unexplored\nresearch question: ``Do we need to normalize the soft prompts in VLMs?'' To\nfill this research gap, we first uncover a phenomenon, called the\n\\textbf{Low-Norm Effect} by performing extensive corruption experiments,\nsuggesting that reducing the norms of certain learned prompts occasionally\nenhances the performance of VLMs, while increasing them often degrades it. To\nharness this effect, we propose a novel method named \\textbf{N}ormalizing\nth\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language\nmodel\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To\nthe best of our knowledge, our work is the first to systematically investigate\nthe role of norms of soft-prompt vector in VLMs, offering valuable insights for\nfuture research in soft-prompt tuning. The code is available at\n\\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2024 (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2408.13979v1",
    "published_date": "2024-08-26 02:09:05 UTC",
    "updated_date": "2024-08-26 02:09:05 UTC"
  }
]
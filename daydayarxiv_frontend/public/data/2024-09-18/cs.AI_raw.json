[
  {
    "arxiv_id": "2409.12367v2",
    "title": "Extracting Memorized Training Data via Decomposition",
    "authors": [
      "Ellen Su",
      "Anu Vellore",
      "Amy Chang",
      "Raffaele Mura",
      "Blaine Nelson",
      "Paul Kassianik",
      "Amin Karbasi"
    ],
    "abstract": "The widespread use of Large Language Models (LLMs) in society creates new\ninformation security challenges for developers, organizations, and end-users\nalike. LLMs are trained on large volumes of data, and their susceptibility to\nreveal the exact contents of the source training datasets poses security and\nsafety risks. Although current alignment procedures restrict common risky\nbehaviors, they do not completely prevent LLMs from leaking data. Prior work\ndemonstrated that LLMs may be tricked into divulging training data by using\nout-of-distribution queries or adversarial techniques. In this paper, we\ndemonstrate a simple, query-based decompositional method to extract news\narticles from two frontier LLMs. We use instruction decomposition techniques to\nincrementally extract fragments of training data. Out of 3723 New York Times\narticles, we extract at least one verbatim sentence from 73 articles, and over\n20% of verbatim sentences from 6 articles. Our analysis demonstrates that this\nmethod successfully induces the LLM to generate texts that are reliable\nreproductions of news articles, meaning that they likely originate from the\nsource training dataset. This method is simple, generalizable, and does not\nfine-tune or change the production model. If replicable at scale, this training\ndata extraction methodology could expose new LLM security and safety\nvulnerabilities, including privacy risks and unauthorized data leaks. These\nimplications require careful consideration from model development to its\nend-use.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12367v2",
    "published_date": "2024-09-18 23:59:32 UTC",
    "updated_date": "2024-10-01 21:34:42 UTC"
  },
  {
    "arxiv_id": "2409.17285v2",
    "title": "SpoofCeleb: Speech Deepfake Detection and SASV In The Wild",
    "authors": [
      "Jee-weon Jung",
      "Yihan Wu",
      "Xin Wang",
      "Ji-Hoon Kim",
      "Soumi Maiti",
      "Yuta Matsunaga",
      "Hye-jin Shim",
      "Jinchuan Tian",
      "Nicholas Evans",
      "Joon Son Chung",
      "Wangyou Zhang",
      "Seyun Um",
      "Shinnosuke Takamichi",
      "Shinji Watanabe"
    ],
    "abstract": "This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake\nDetection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV),\nutilizing source data from real-world conditions and spoofing attacks generated\nby Text-To-Speech (TTS) systems also trained on the same real-world data.\nRobust recognition systems require speech data recorded in varied acoustic\nenvironments with different levels of noise to be trained. However, current\ndatasets typically include clean, high-quality recordings (bona fide data) due\nto the requirements for TTS training; studio-quality or well-recorded read\nspeech is typically necessary to train TTS models. Current SDD datasets also\nhave limited usefulness for training SASV models due to insufficient speaker\ndiversity. SpoofCeleb leverages a fully automated pipeline we developed that\nprocesses the VoxCeleb1 dataset, transforming it into a suitable form for TTS\ntraining. We subsequently train 23 contemporary TTS systems. SpoofCeleb\ncomprises over 2.5 million utterances from 1,251 unique speakers, collected\nunder natural, real-world conditions. The dataset includes carefully\npartitioned training, validation, and evaluation sets with well-controlled\nexperimental protocols. We present the baseline results for both SDD and SASV\ntasks. All data, protocols, and baselines are publicly available at\nhttps://jungjee.github.io/spoofceleb.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "IEEE OJSP. Official document lives at:\n  https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839331",
    "pdf_url": "http://arxiv.org/pdf/2409.17285v2",
    "published_date": "2024-09-18 23:17:02 UTC",
    "updated_date": "2025-04-15 17:53:00 UTC"
  },
  {
    "arxiv_id": "2409.12350v1",
    "title": "Advancing Cucumber Disease Detection in Agriculture through Machine Vision and Drone Technology",
    "authors": [
      "Syada Tasfia Rahman",
      "Nishat Vasker",
      "Amir Khabbab Ahammed",
      "Mahamudul Hasan"
    ],
    "abstract": "This study uses machine vision and drone technologies to propose a unique\nmethod for the diagnosis of cucumber disease in agriculture. The backbone of\nthis research is a painstakingly curated dataset of hyperspectral photographs\nacquired under genuine field conditions. Unlike earlier datasets, this study\nincluded a wide variety of illness types, allowing for precise early-stage\ndetection. The model achieves an excellent 87.5\\% accuracy in distinguishing\neight unique cucumber illnesses after considerable data augmentation. The\nincorporation of drone technology for high-resolution images improves disease\nevaluation. This development has enormous potential for improving crop\nmanagement, lowering labor costs, and increasing agricultural productivity.\nThis research, which automates disease detection, represents a significant step\ntoward a more efficient and sustainable agricultural future.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 page and 6 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.12350v1",
    "published_date": "2024-09-18 22:54:23 UTC",
    "updated_date": "2024-09-18 22:54:23 UTC"
  },
  {
    "arxiv_id": "2409.12347v1",
    "title": "Axial Attention Transformer Networks: A New Frontier in Breast Cancer Detection",
    "authors": [
      "Weijie He",
      "Runyuan Bao",
      "Yiru Cang",
      "Jianjun Wei",
      "Yang Zhang",
      "Jiacheng Hu"
    ],
    "abstract": "This paper delves into the challenges and advancements in the field of\nmedical image segmentation, particularly focusing on breast cancer diagnosis.\nThe authors propose a novel Transformer-based segmentation model that addresses\nthe limitations of traditional convolutional neural networks (CNNs), such as\nU-Net, in accurately localizing and segmenting small lesions within breast\ncancer images. The model introduces an axial attention mechanism to enhance the\ncomputational efficiency and address the issue of global contextual information\nthat is often overlooked by CNNs. Additionally, the paper discusses\nimprovements tailored to the small dataset challenge, including the\nincorporation of relative position information and a gated axial attention\nmechanism to refine the model's focus on relevant features. The proposed model\naims to significantly improve the segmentation accuracy of breast cancer\nimages, offering a more efficient and effective tool for computer-aided\ndiagnosis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12347v1",
    "published_date": "2024-09-18 22:40:29 UTC",
    "updated_date": "2024-09-18 22:40:29 UTC"
  },
  {
    "arxiv_id": "2409.12334v1",
    "title": "Deep vessel segmentation with joint multi-prior encoding",
    "authors": [
      "Amine Sadikine",
      "Bogdan Badic",
      "Enzo Ferrante",
      "Vincent Noblet",
      "Pascal Ballet",
      "Dimitris Visvikis",
      "Pierre-Henri Conze"
    ],
    "abstract": "The precise delineation of blood vessels in medical images is critical for\nmany clinical applications, including pathology detection and surgical\nplanning. However, fully-automated vascular segmentation is challenging because\nof the variability in shape, size, and topology. Manual segmentation remains\nthe gold standard but is time-consuming, subjective, and impractical for\nlarge-scale studies. Hence, there is a need for automatic and reliable\nsegmentation methods that can accurately detect blood vessels from medical\nimages. The integration of shape and topological priors into vessel\nsegmentation models has been shown to improve segmentation accuracy by offering\ncontextual information about the shape of the blood vessels and their spatial\nrelationships within the vascular tree. To further improve anatomical\nconsistency, we propose a new joint prior encoding mechanism which incorporates\nboth shape and topology in a single latent space. The effectiveness of our\nmethod is demonstrated on the publicly available 3D-IRCADb dataset. More\nglobally, the proposed approach holds promise in overcoming the challenges\nassociated with automatic vessel delineation and has the potential to advance\nthe field of deep priors encoding.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "5 pages, 3 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2409.12334v1",
    "published_date": "2024-09-18 22:03:46 UTC",
    "updated_date": "2024-09-18 22:03:46 UTC"
  },
  {
    "arxiv_id": "2409.12333v1",
    "title": "Scale-specific auxiliary multi-task contrastive learning for deep liver vessel segmentation",
    "authors": [
      "Amine Sadikine",
      "Bogdan Badic",
      "Jean-Pierre Tasu",
      "Vincent Noblet",
      "Pascal Ballet",
      "Dimitris Visvikis",
      "Pierre-Henri Conze"
    ],
    "abstract": "Extracting hepatic vessels from abdominal images is of high interest for\nclinicians since it allows to divide the liver into functionally-independent\nCouinaud segments. In this respect, an automated liver blood vessel extraction\nis widely summoned. Despite the significant growth in performance of semantic\nsegmentation methodologies, preserving the complex multi-scale geometry of main\nvessels and ramifications remains a major challenge. This paper provides a new\ndeep supervised approach for vessel segmentation, with a strong focus on\nrepresentations arising from the different scales inherent to the vascular tree\ngeometry. In particular, we propose a new clustering technique to decompose the\ntree into various scale levels, from tiny to large vessels. Then, we extend\nstandard 3D UNet to multi-task learning by incorporating scale-specific\nauxiliary tasks and contrastive learning to encourage the discrimination\nbetween scales in the shared representation. Promising results, depicted in\nseveral evaluation metrics, are revealed on the public 3D-IRCADb dataset.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "5 pages, 5 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2409.12333v1",
    "published_date": "2024-09-18 22:03:22 UTC",
    "updated_date": "2024-09-18 22:03:22 UTC"
  },
  {
    "arxiv_id": "2409.15369v1",
    "title": "Geometric Relational Embeddings",
    "authors": [
      "Bo Xiong"
    ],
    "abstract": "Relational representation learning transforms relational data into continuous\nand low-dimensional vector representations. However, vector-based\nrepresentations fall short in capturing crucial properties of relational data\nthat are complex and symbolic. We propose geometric relational embeddings, a\nparadigm of relational embeddings that respect the underlying symbolic\nstructures. Specifically, this dissertation introduces various geometric\nrelational embedding models capable of capturing: 1) complex structured\npatterns like hierarchies and cycles in networks and knowledge graphs; 2)\nlogical structures in ontologies and logical constraints applicable for\nconstraining machine learning model outputs; and 3) high-order structures\nbetween entities and relations. Our results obtained from benchmark and\nreal-world datasets demonstrate the efficacy of geometric relational embeddings\nin adeptly capturing these discrete, symbolic, and structured properties\ninherent in relational data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Doctoral Dissertation, 177 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.15369v1",
    "published_date": "2024-09-18 22:02:24 UTC",
    "updated_date": "2024-09-18 22:02:24 UTC"
  },
  {
    "arxiv_id": "2409.12314v1",
    "title": "Understanding Implosion in Text-to-Image Generative Models",
    "authors": [
      "Wenxin Ding",
      "Cathy Y. Li",
      "Shawn Shan",
      "Ben Y. Zhao",
      "Haitao Zheng"
    ],
    "abstract": "Recent works show that text-to-image generative models are surprisingly\nvulnerable to a variety of poisoning attacks. Empirical results find that these\nmodels can be corrupted by altering associations between individual text\nprompts and associated visual features. Furthermore, a number of concurrent\npoisoning attacks can induce \"model implosion,\" where the model becomes unable\nto produce meaningful images for unpoisoned prompts. These intriguing findings\nhighlight the absence of an intuitive framework to understand poisoning attacks\non these models. In this work, we establish the first analytical framework on\nrobustness of image generative models to poisoning attacks, by modeling and\nanalyzing the behavior of the cross-attention mechanism in latent diffusion\nmodels. We model cross-attention training as an abstract problem of \"supervised\ngraph alignment\" and formally quantify the impact of training data by the\nhardness of alignment, measured by an Alignment Difficulty (AD) metric. The\nhigher the AD, the harder the alignment. We prove that AD increases with the\nnumber of individual prompts (or concepts) poisoned. As AD grows, the alignment\ntask becomes increasingly difficult, yielding highly distorted outcomes that\nfrequently map meaningful text prompts to undefined or meaningless visual\nrepresentations. As a result, the generative model implodes and outputs random,\nincoherent images at large. We validate our analytical framework through\nextensive experiments, and we confirm and explain the unexpected (and\nunexplained) effect of model implosion while producing new, unforeseen\ninsights. Our work provides a useful tool for studying poisoning attacks\nagainst diffusion models and their defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "ACM CCS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12314v1",
    "published_date": "2024-09-18 21:06:45 UTC",
    "updated_date": "2024-09-18 21:06:45 UTC"
  },
  {
    "arxiv_id": "2409.12300v1",
    "title": "Autoformalization of Game Descriptions using Large Language Models",
    "authors": [
      "Agnieszka Mensfelt",
      "Kostas Stathis",
      "Vince Trencsenyi"
    ],
    "abstract": "Game theory is a powerful framework for reasoning about strategic\ninteractions, with applications in domains ranging from day-to-day life to\ninternational politics. However, applying formal reasoning tools in such\ncontexts is challenging, as these scenarios are often expressed in natural\nlanguage. To address this, we introduce a framework for the autoformalization\nof game-theoretic scenarios, which translates natural language descriptions\ninto formal logic representations suitable for formal solvers. Our approach\nutilizes one-shot prompting and a solver that provides feedback on syntactic\ncorrectness to allow LLMs to refine the code. We evaluate the framework using\nGPT-4o and a dataset of natural language problem descriptions, achieving 98%\nsyntactic correctness and 88% semantic correctness. These results show the\npotential of LLMs to bridge the gap between real-life strategic interactions\nand formal reasoning.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "code: https://github.com/dicelab-rhul/game-formaliser",
    "pdf_url": "http://arxiv.org/pdf/2409.12300v1",
    "published_date": "2024-09-18 20:18:53 UTC",
    "updated_date": "2024-09-18 20:18:53 UTC"
  },
  {
    "arxiv_id": "2409.12294v1",
    "title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
    "authors": [
      "Abhinav Jain",
      "Chris Jermaine",
      "Vaibhav Unhelkar"
    ],
    "abstract": "Large language models (LLMs) have recently emerged as promising tools for\nsolving challenging robotic tasks, even in the presence of action and\nobservation uncertainties. Recent LLM-based decision-making methods (also\nreferred to as LLM-based agents), when paired with appropriate critics, have\ndemonstrated potential in solving complex, long-horizon tasks with relatively\nfew interactions. However, most existing LLM-based agents lack the ability to\nretain and learn from past interactions - an essential trait of learning-based\nrobotic systems. We propose RAG-Modulo, a framework that enhances LLM-based\nagents with a memory of past interactions and incorporates critics to evaluate\nthe agents' decisions. The memory component allows the agent to automatically\nretrieve and incorporate relevant past experiences as in-context examples,\nproviding context-aware feedback for more informed decision-making. Further by\nupdating its memory, the agent improves its performance over time, thereby\nexhibiting learning. Through experiments in the challenging BabyAI and AlfWorld\ndomains, we demonstrate significant improvements in task success rates and\nefficiency, showing that the proposed RAG-Modulo framework outperforms\nstate-of-the-art baselines.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12294v1",
    "published_date": "2024-09-18 20:03:32 UTC",
    "updated_date": "2024-09-18 20:03:32 UTC"
  },
  {
    "arxiv_id": "2409.12289v1",
    "title": "MetaPix: A Data-Centric AI Development Platform for Efficient Management and Utilization of Unstructured Computer Vision Data",
    "authors": [
      "Sai Vishwanath Venkatesh",
      "Atra Akandeh",
      "Madhu Lokanath"
    ],
    "abstract": "In today's world of advanced AI technologies, data management is a critical\ncomponent of any AI/ML solution. Effective data management is vital for the\ncreation and maintenance of high-quality, diverse datasets, which significantly\nenhance predictive capabilities and lead to smarter business solutions. In this\nwork, we introduce MetaPix, a Data-centric AI platform offering comprehensive\ndata management solutions specifically designed for unstructured data. MetaPix\noffers robust tools for data ingestion, processing, storage, versioning,\ngovernance, and discovery. The platform operates on four key concepts:\nDataSources, Datasets, Extensions and Extractors. A DataSource serves as\nMetaPix top level asset, representing a narrow-scoped source of data for a\nspecific use. Datasets are MetaPix second level object, structured collections\nof data. Extractors are internal tools integrated into MetaPix's backend\nprocessing, facilitate data processing and enhancement. Additionally, MetaPix\nsupports extensions, enabling integration with external third-party tools to\nenhance platform functionality. This paper delves into each MetaPix concept in\ndetail, illustrating how they collectively contribute to the platform's\nobjectives. By providing a comprehensive solution for managing and utilizing\nunstructured computer vision data, MetaPix equips organizations with a powerful\ntoolset to develop AI applications effectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted @ The 22nd International Conference on Software Engineering\n  Research & Practice",
    "pdf_url": "http://arxiv.org/pdf/2409.12289v1",
    "published_date": "2024-09-18 19:50:53 UTC",
    "updated_date": "2024-09-18 19:50:53 UTC"
  },
  {
    "arxiv_id": "2409.15368v1",
    "title": "MedCodER: A Generative AI Assistant for Medical Coding",
    "authors": [
      "Krishanu Das Baksi",
      "Elijah Soba",
      "John J. Higgins",
      "Ravi Saini",
      "Jaden Wood",
      "Jane Cook",
      "Jack Scott",
      "Nirmala Pudota",
      "Tim Weninger",
      "Edward Bowen",
      "Sanmitra Bhattacharya"
    ],
    "abstract": "Medical coding is essential for standardizing clinical data and communication\nbut is often time-consuming and prone to errors. Traditional Natural Language\nProcessing (NLP) methods struggle with automating coding due to the large label\nspace, lengthy text inputs, and the absence of supporting evidence annotations\nthat justify code selection. Recent advancements in Generative Artificial\nIntelligence (AI) offer promising solutions to these challenges. In this work,\nwe introduce MedCodER, a Generative AI framework for automatic medical coding\nthat leverages extraction, retrieval, and re-ranking techniques as core\ncomponents. MedCodER achieves a micro-F1 score of 0.60 on International\nClassification of Diseases (ICD) code prediction, significantly outperforming\nstate-of-the-art methods. Additionally, we present a new dataset containing\nmedical records annotated with disease diagnoses, ICD codes, and supporting\nevidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests\nconfirm that MedCodER's performance depends on the integration of each of its\naforementioned components, as performance declines when these components are\nevaluated in isolation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15368v1",
    "published_date": "2024-09-18 19:36:33 UTC",
    "updated_date": "2024-09-18 19:36:33 UTC"
  },
  {
    "arxiv_id": "2409.18995v1",
    "title": "Systematic Characterization of the Effectiveness of Alignment in Large Language Models for Categorical Decisions",
    "authors": [
      "Isaac Kohane"
    ],
    "abstract": "As large language models (LLMs) are deployed in high-stakes domains like\nhealthcare, understanding how well their decision-making aligns with human\npreferences and values becomes crucial, especially when we recognize that there\nis no single gold standard for these preferences. This paper applies a\nsystematic methodology for evaluating preference alignment in LLMs on\ncategorical decision-making with medical triage as a domain-specific use case.\nIt also measures how effectively an alignment procedure will change the\nalignment of a specific model. Key to this methodology is a novel simple\nmeasure, the Alignment Compliance Index (ACI), that quantifies how effectively\na LLM can be aligned to a given preference function or gold standard. Since the\nACI measures the effect rather than the process of alignment, it is applicable\nto alignment methods beyond the in-context learning used in this study.\n  Using a dataset of simulated patient pairs, three frontier LLMs (GPT4o,\nClaude 3.5 Sonnet, and Gemini Advanced) were assessed on their ability to make\ntriage decisions consistent with an expert clinician's preferences. The models'\nperformance before and after alignment attempts was evaluated using various\nprompting strategies. The results reveal significant variability in alignment\neffectiveness across models and alignment approaches. Notably, models that\nperformed well, as measured by ACI, pre-alignment sometimes degraded\npost-alignment, and small changes in the target preference function led to\nlarge shifts in model rankings. The implicit ethical principles, as understood\nby humans, underlying the LLMs' decisions were also explored through targeted\nquestioning.\n  This study motivates the use of a practical set of methods and the ACI, in\nthe near term, to understand the correspondence between the variety of human\nand LLM decision-making values in categorical decision-making such as triage.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages (without Appendix) Appendix 7 pages. 7 Figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18995v1",
    "published_date": "2024-09-18 19:03:04 UTC",
    "updated_date": "2024-09-18 19:03:04 UTC"
  },
  {
    "arxiv_id": "2409.15367v2",
    "title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss",
    "authors": [
      "Andrei Chernov"
    ],
    "abstract": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "4 main pages; 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.15367v2",
    "published_date": "2024-09-18 18:36:18 UTC",
    "updated_date": "2024-11-18 17:00:32 UTC"
  },
  {
    "arxiv_id": "2409.12249v2",
    "title": "GCA-SUNet: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting",
    "authors": [
      "Yuzhe Wu",
      "Yipeng Xu",
      "Tianyu Xu",
      "Jialu Zhang",
      "Jianfeng Ren",
      "Xudong Jiang"
    ],
    "abstract": "Exemplar-Free Counting aims to count objects of interest without intensive\nannotations of objects or exemplars. To achieve this, we propose a Gated\nContext-Aware Swin-UNet (GCA-SUNet) to directly map an input image to the\ndensity map of countable objects. Specifically, a set of Swin transformers form\nan encoder to derive a robust feature representation, and a Gated Context-Aware\nModulation block is designed to suppress irrelevant objects or background\nthrough a gate mechanism and exploit the attentive support of objects of\ninterest through a self-similarity matrix. The gate strategy is also\nincorporated into the bottleneck network and the decoder of the Swin-UNet to\nhighlight the features most relevant to objects of interest. By explicitly\nexploiting the attentive support among countable objects and eliminating\nirrelevant features through the gate mechanisms, the proposed GCA-SUNet focuses\non and counts objects of interest without relying on predefined categories or\nexemplars. Experimental results on the real-world datasets such as FSC-147 and\nCARPK demonstrate that GCA-SUNet significantly and consistently outperforms\nstate-of-the-art methods. The code is available at\nhttps://github.com/Amordia/GCA-SUNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICME 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12249v2",
    "published_date": "2024-09-18 18:14:00 UTC",
    "updated_date": "2025-03-27 00:09:03 UTC"
  },
  {
    "arxiv_id": "2409.12193v1",
    "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
    "authors": [
      "Qiuhong Shen",
      "Xingyi Yang",
      "Michael Bi Mi",
      "Xinchao Wang"
    ],
    "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects\nfrom mere glimpses of their visible parts. To address this, we present Vista3D,\na framework that realizes swift and consistent 3D generation within a mere 5\nminutes. At the heart of Vista3D lies a two-phase approach: the coarse phase\nand the fine phase. In the coarse phase, we rapidly generate initial geometry\nwith Gaussian Splatting from a single image. In the fine phase, we extract a\nSigned Distance Function (SDF) directly from learned Gaussian Splatting,\noptimizing it with a differentiable isosurface representation. Furthermore, it\nelevates the quality of generation by using a disentangled representation with\ntwo independent implicit functions to capture both visible and obscured aspects\nof objects. Additionally, it harmonizes gradients from 2D diffusion prior with\n3D-aware diffusion priors by angular diffusion prior composition. Through\nextensive evaluation, we demonstrate that Vista3D effectively sustains a\nbalance between the consistency and diversity of the generated 3D objects.\nDemos and code will be available at https://github.com/florinshen/Vista3D.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GT",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV'2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12193v1",
    "published_date": "2024-09-18 17:59:44 UTC",
    "updated_date": "2024-09-18 17:59:44 UTC"
  },
  {
    "arxiv_id": "2409.12192v2",
    "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
    "authors": [
      "Zichen Jeff Cui",
      "Hengkai Pan",
      "Aadhithya Iyer",
      "Siddhant Haldar",
      "Lerrel Pinto"
    ],
    "abstract": "Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12192v2",
    "published_date": "2024-09-18 17:59:43 UTC",
    "updated_date": "2024-10-30 18:48:00 UTC"
  },
  {
    "arxiv_id": "2409.12191v2",
    "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
    "authors": [
      "Peng Wang",
      "Shuai Bai",
      "Sinan Tan",
      "Shijie Wang",
      "Zhihao Fan",
      "Jinze Bai",
      "Keqin Chen",
      "Xuejing Liu",
      "Jialin Wang",
      "Wenbin Ge",
      "Yang Fan",
      "Kai Dang",
      "Mengfei Du",
      "Xuancheng Ren",
      "Rui Men",
      "Dayiheng Liu",
      "Chang Zhou",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "abstract": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin\n  note: text overlap with arXiv:2408.15262 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2409.12191v2",
    "published_date": "2024-09-18 17:59:32 UTC",
    "updated_date": "2024-10-03 15:54:49 UTC"
  },
  {
    "arxiv_id": "2409.12183v3",
    "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
    "authors": [
      "Zayne Sprague",
      "Fangcong Yin",
      "Juan Diego Rodriguez",
      "Dongwei Jiang",
      "Manya Wadhwa",
      "Prasann Singhal",
      "Xinyu Zhao",
      "Xi Ye",
      "Kyle Mahowald",
      "Greg Durrett"
    ],
    "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12183v3",
    "published_date": "2024-09-18 17:55:00 UTC",
    "updated_date": "2025-05-07 18:00:45 UTC"
  },
  {
    "arxiv_id": "2409.12179v1",
    "title": "Computational Dynamical Systems",
    "authors": [
      "Jordan Cotler",
      "Semon Rezchikov"
    ],
    "abstract": "We study the computational complexity theory of smooth, finite-dimensional\ndynamical systems. Building off of previous work, we give definitions for what\nit means for a smooth dynamical system to simulate a Turing machine. We then\nshow that 'chaotic' dynamical systems (more precisely, Axiom A systems) and\n'integrable' dynamical systems (more generally, measure-preserving systems)\ncannot robustly simulate universal Turing machines, although such machines can\nbe robustly simulated by other kinds of dynamical systems. Subsequently, we\nshow that any Turing machine that can be encoded into a structurally stable\none-dimensional dynamical system must have a decidable halting problem, and\nmoreover an explicit time complexity bound in instances where it does halt.\nMore broadly, our work elucidates what it means for one 'machine' to simulate\nanother, and emphasizes the necessity of defining low-complexity 'encoders' and\n'decoders' to translate between the dynamics of the simulation and the system\nbeing simulated. We highlight how the notion of a computational dynamical\nsystem leads to questions at the intersection of computational complexity\ntheory, dynamical systems theory, and real algebraic geometry.",
    "categories": [
      "cs.CC",
      "cs.AI",
      "cs.FL",
      "math.DS"
    ],
    "primary_category": "cs.CC",
    "comment": "46+14 pages, 6 figures; accepted to FOCS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12179v1",
    "published_date": "2024-09-18 17:51:48 UTC",
    "updated_date": "2024-09-18 17:51:48 UTC"
  },
  {
    "arxiv_id": "2409.15366v1",
    "title": "Trajectory Anomaly Detection with Language Models",
    "authors": [
      "Jonathan Mbuya",
      "Dieter Pfoser",
      "Antonios Anastasopoulos"
    ],
    "abstract": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15366v1",
    "published_date": "2024-09-18 17:33:31 UTC",
    "updated_date": "2024-09-18 17:33:31 UTC"
  },
  {
    "arxiv_id": "2409.13768v1",
    "title": "Magika: AI-Powered Content-Type Detection",
    "authors": [
      "Yanick Fratantonio",
      "Luca Invernizzi",
      "Loua Farah",
      "Kurt Thomas",
      "Marina Zhang",
      "Ange Albertini",
      "Francois Galilee",
      "Giancarlo Metitieri",
      "Julien Cretin",
      "Alex Petit-Bianco",
      "David Tao",
      "Elie Bursztein"
    ],
    "abstract": "The task of content-type detection -- which entails identifying the data\nencoded in an arbitrary byte sequence -- is critical for operating systems,\ndevelopment, reverse engineering environments, and a variety of security\napplications. In this paper, we introduce Magika, a novel AI-powered\ncontent-type detection tool. Under the hood, Magika employs a deep learning\nmodel that can execute on a single CPU with just 1MB of memory to store the\nmodel's weights. We show that Magika achieves an average F1 score of 99% across\nover a hundred content types and a test set of more than 1M files,\noutperforming all existing content-type detection tools today. In order to\nfoster adoption and improvements, we open source Magika under an Apache 2\nlicense on GitHub and make our model and training pipeline publicly available.\nOur tool has already seen adoption by the Gmail email provider for attachment\nscanning, and it has been integrated with VirusTotal to aid with malware\nanalysis.\n  We note that this paper discusses the first iteration of Magika, and a more\nrecent version already supports more than 200 content types. The interested\nreader can see the latest development on the Magika GitHub repository,\navailable at https://github.com/google/magika.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13768v1",
    "published_date": "2024-09-18 17:24:39 UTC",
    "updated_date": "2024-09-18 17:24:39 UTC"
  },
  {
    "arxiv_id": "2409.15365v1",
    "title": "Novel Saliency Analysis for the Forward Forward Algorithm",
    "authors": [
      "Mitra Bakhshi"
    ],
    "abstract": "Incorporating the Forward Forward algorithm into neural network training\nrepresents a transformative shift from traditional methods, introducing a dual\nforward mechanism that streamlines the learning process by bypassing the\ncomplexities of derivative propagation. This method is noted for its simplicity\nand efficiency and involves executing two forward passes the first with actual\ndata to promote positive reinforcement, and the second with synthetically\ngenerated negative data to enable discriminative learning. Our experiments\nconfirm that the Forward Forward algorithm is not merely an experimental\nnovelty but a viable training strategy that competes robustly with conventional\nmulti layer perceptron (MLP) architectures. To overcome the limitations\ninherent in traditional saliency techniques, which predominantly rely on\ngradient based methods, we developed a bespoke saliency algorithm specifically\ntailored for the Forward Forward framework. This innovative algorithm enhances\nthe intuitive understanding of feature importance and network decision-making,\nproviding clear visualizations of the data features most influential in model\npredictions. By leveraging this specialized saliency method, we gain deeper\ninsights into the internal workings of the model, significantly enhancing our\ninterpretative capabilities beyond those offered by standard approaches. Our\nevaluations, utilizing the MNIST and Fashion MNIST datasets, demonstrate that\nour method performs comparably to traditional MLP-based models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2nd International Conference on Artificial Intelligence, Blockchain,\n  and Internet of Things, (AIBThings)",
    "pdf_url": "http://arxiv.org/pdf/2409.15365v1",
    "published_date": "2024-09-18 17:21:59 UTC",
    "updated_date": "2024-09-18 17:21:59 UTC"
  },
  {
    "arxiv_id": "2409.12154v1",
    "title": "Abductive explanations of classifiers under constraints: Complexity and properties",
    "authors": [
      "Martin Cooper",
      "Leila Amgoud"
    ],
    "abstract": "Abductive explanations (AXp's) are widely used for understanding decisions of\nclassifiers. Existing definitions are suitable when features are independent.\nHowever, we show that ignoring constraints when they exist between features may\nlead to an explosion in the number of redundant or superfluous AXp's. We\npropose three new types of explanations that take into account constraints and\nthat can be generated from the whole feature space or from a sample (such as a\ndataset). They are based on a key notion of coverage of an explanation, the set\nof instances it explains. We show that coverage is powerful enough to discard\nredundant and superfluous AXp's. For each type, we analyse the complexity of\nfinding an explanation and investigate its formal properties. The final result\nis a catalogue of different forms of AXp's with different complexities and\ndifferent formal guarantees.",
    "categories": [
      "cs.AI",
      "68T01, 68Q17",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "Full version with proofs of Martin C. Cooper and Leila Amgoud,\n  Abductive explanations of classifiers under constraints: Complexity and\n  properties, ECAI 2023, 469-476",
    "pdf_url": "http://arxiv.org/pdf/2409.12154v1",
    "published_date": "2024-09-18 17:15:39 UTC",
    "updated_date": "2024-09-18 17:15:39 UTC"
  },
  {
    "arxiv_id": "2409.12150v1",
    "title": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference",
    "authors": [
      "Najmeh Forouzandehmehr",
      "Nima Farrokhsiar",
      "Ramin Giahi",
      "Evren Korpeoglu",
      "Kannan Achan"
    ],
    "abstract": "Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12150v1",
    "published_date": "2024-09-18 17:15:06 UTC",
    "updated_date": "2024-09-18 17:15:06 UTC"
  },
  {
    "arxiv_id": "2409.12139v3",
    "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
    "authors": [
      "Sijing Chen",
      "Yuan Feng",
      "Laipeng He",
      "Tianwei He",
      "Wendi He",
      "Yanni Hu",
      "Bin Lin",
      "Yiting Lin",
      "Yu Pan",
      "Pengfei Tan",
      "Chengwei Tian",
      "Chen Wang",
      "Zhicheng Wang",
      "Ruoye Xie",
      "Jixun Yao",
      "Quanlei Yan",
      "Yuguang Yang",
      "Jianhao Ye",
      "Jingjing Yin",
      "Yanzhen Yu",
      "Huimin Zhang",
      "Xiang Zhang",
      "Guangcheng Zhao",
      "Hongbin Zhou",
      "Pengpeng Zou"
    ],
    "abstract": "With the advent of the big data and large language model era, zero-shot\npersonalized rapid customization has emerged as a significant trend. In this\nreport, we introduce Takin AudioLLM, a series of techniques and models, mainly\nincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed for\naudiobook production. These models are capable of zero-shot speech production,\ngenerating high-quality speech that is nearly indistinguishable from real human\nspeech and facilitating individuals to customize the speech content according\nto their own needs. Specifically, we first introduce Takin TTS, a neural codec\nlanguage model that builds upon an enhanced neural speech codec and a\nmulti-task training framework, capable of generating high-fidelity natural\nspeech in a zero-shot way. For Takin VC, we advocate an effective content and\ntimbre joint modeling approach to improve the speaker similarity, while\nadvocating for a conditional flow matching based decoder to further enhance its\nnaturalness and expressiveness. Last, we propose the Takin Morphing system with\nhighly decoupled and advanced timbre and prosody modeling approaches, which\nenables individuals to customize speech production with their preferred timbre\nand prosody in a precise and controllable manner. Extensive experiments\nvalidate the effectiveness and robustness of our Takin AudioLLM series models.\nFor detailed demos, please refer to\nhttps://everest-ai.github.io/takinaudiollm/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Technical Report; 18 pages; typos corrected, references added, demo\n  url modified, author name modified;",
    "pdf_url": "http://arxiv.org/pdf/2409.12139v3",
    "published_date": "2024-09-18 17:03:12 UTC",
    "updated_date": "2024-09-24 02:00:54 UTC"
  },
  {
    "arxiv_id": "2409.12136v1",
    "title": "GRIN: GRadient-INformed MoE",
    "authors": [
      "Liyuan Liu",
      "Young Jin Kim",
      "Shuohang Wang",
      "Chen Liang",
      "Yelong Shen",
      "Hao Cheng",
      "Xiaodong Liu",
      "Masahiro Tanaka",
      "Xiaoxia Wu",
      "Wenxiang Hu",
      "Vishrav Chaudhary",
      "Zeqi Lin",
      "Chenruidong Zhang",
      "Jilong Xue",
      "Hany Awadalla",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "abstract": "Mixture-of-Experts (MoE) models scale more effectively than dense models due\nto sparse computation through expert routing, selectively activating only a\nsmall subset of expert modules. However, sparse computation challenges\ntraditional training practices, as discrete expert routing hinders standard\nbackpropagation and thus gradient-based optimization, which are the cornerstone\nof deep learning. To better pursue the scaling power of MoE, we introduce GRIN\n(GRadient-INformed MoE training), which incorporates sparse gradient estimation\nfor expert routing and configures model parallelism to avoid token dropping.\nApplying GRIN to autoregressive language modeling, we develop a top-2\n16$\\times$3.8B MoE model. Our model, with only 6.6B activated parameters,\noutperforms a 7B dense model and matches the performance of a 14B dense model\ntrained on the same data. Extensive evaluations across diverse tasks\ndemonstrate the potential of GRIN to significantly enhance MoE efficacy,\nachieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "58 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.12136v1",
    "published_date": "2024-09-18 17:00:20 UTC",
    "updated_date": "2024-09-18 17:00:20 UTC"
  },
  {
    "arxiv_id": "2409.12135v2",
    "title": "Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features",
    "authors": [
      "Jiuqi Wang",
      "Shangtong Zhang"
    ],
    "abstract": "Temporal difference (TD) learning with linear function approximation,\nabbreviated as linear TD, is a classic and powerful prediction algorithm in\nreinforcement learning. While it is well understood that linear TD converges\nalmost surely to a unique point, this convergence traditionally requires the\nassumption that the features used by the approximator are linearly independent.\nHowever, this linear independence assumption does not hold in many practical\nscenarios. This work is the first to establish the almost sure convergence of\nlinear TD without requiring linearly independent features. In fact, we do not\nmake any assumptions on the features. We prove that the approximated value\nfunction converges to a unique point and the weight iterates converge to a set.\nWe also establish a notion of local stability of the weight iterates.\nImportantly, we do not need to introduce any other additional assumptions and\ndo not need to make any modification to the linear TD algorithm. Key to our\nanalysis is a novel characterization of bounded invariant sets of the mean ODE\nof linear TD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 0 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12135v2",
    "published_date": "2024-09-18 16:59:17 UTC",
    "updated_date": "2024-10-15 18:50:02 UTC"
  },
  {
    "arxiv_id": "2409.12134v1",
    "title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework",
    "authors": [
      "Tuan-Cuong Vuong",
      "Trang Mai Xuan",
      "Thien Van Luong"
    ],
    "abstract": "In tackling the challenge of Multi-Document Summarization (MDS), numerous\nmethods have been proposed, spanning both extractive and abstractive\nsummarization techniques. However, each approach has its own limitations,\nmaking it less effective to rely solely on either one. An emerging and\npromising strategy involves a synergistic fusion of extractive and abstractive\nsummarization methods. Despite the plethora of studies in this domain, research\non the combined methodology remains scarce, particularly in the context of\nVietnamese language processing. This paper presents a novel Vietnamese MDS\nframework leveraging a two-component pipeline architecture that integrates\nextractive and abstractive techniques. The first component employs an\nextractive approach to identify key sentences within each document. This is\nachieved by a modification of the pre-trained BERT network, which derives\nsemantically meaningful phrase embeddings using siamese and triplet network\nstructures. The second component utilizes the VBD-LLaMA2-7B-50b model for\nabstractive summarization, ultimately generating the final summary document.\nOur proposed framework demonstrates a positive performance, attaining ROUGE-2\nscores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art\nbaselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.12134v1",
    "published_date": "2024-09-18 16:56:06 UTC",
    "updated_date": "2024-09-18 16:56:06 UTC"
  },
  {
    "arxiv_id": "2409.12122v1",
    "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
    "authors": [
      "An Yang",
      "Beichen Zhang",
      "Binyuan Hui",
      "Bofei Gao",
      "Bowen Yu",
      "Chengpeng Li",
      "Dayiheng Liu",
      "Jianhong Tu",
      "Jingren Zhou",
      "Junyang Lin",
      "Keming Lu",
      "Mingfeng Xue",
      "Runji Lin",
      "Tianyu Liu",
      "Xingzhang Ren",
      "Zhenru Zhang"
    ],
    "abstract": "In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12122v1",
    "published_date": "2024-09-18 16:45:37 UTC",
    "updated_date": "2024-09-18 16:45:37 UTC"
  },
  {
    "arxiv_id": "2409.12112v1",
    "title": "Pareto Data Framework: Steps Towards Resource-Efficient Decision Making Using Minimum Viable Data (MVD)",
    "authors": [
      "Tashfain Ahmed",
      "Josh Siegel"
    ],
    "abstract": "This paper introduces the Pareto Data Framework, an approach for identifying\nand selecting the Minimum Viable Data (MVD) required for enabling machine\nlearning applications on constrained platforms such as embedded systems, mobile\ndevices, and Internet of Things (IoT) devices. We demonstrate that strategic\ndata reduction can maintain high performance while significantly reducing\nbandwidth, energy, computation, and storage costs. The framework identifies\nMinimum Viable Data (MVD) to optimize efficiency across resource-constrained\nenvironments without sacrificing performance. It addresses common inefficient\npractices in an IoT application such as overprovisioning of sensors and\noverprecision, and oversampling of signals, proposing scalable solutions for\noptimal sensor selection, signal extraction and transmission, and data\nrepresentation. An experimental methodology demonstrates effective acoustic\ndata characterization after downsampling, quantization, and truncation to\nsimulate reduced-fidelity sensors and network and storage constraints; results\nshows that performance can be maintained up to 95\\% with sample rates reduced\nby 75\\% and bit depths and clip length reduced by 50\\% which translates into\nsubstantial cost and resource reduction. These findings have implications on\nthe design and development of constrained systems. The paper also discusses\nbroader implications of the framework, including the potential to democratize\nadvanced AI technologies across IoT applications and sectors such as\nagriculture, transportation, and manufacturing to improve access and multiply\nthe benefits of data-driven insights.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12112v1",
    "published_date": "2024-09-18 16:31:19 UTC",
    "updated_date": "2024-09-18 16:31:19 UTC"
  },
  {
    "arxiv_id": "2409.12106v3",
    "title": "Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models",
    "authors": [
      "Haoran Ye",
      "Yuhang Xie",
      "Yuanyi Ren",
      "Hanjun Fang",
      "Xin Zhang",
      "Guojie Song"
    ],
    "abstract": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12106v3",
    "published_date": "2024-09-18 16:26:22 UTC",
    "updated_date": "2025-03-06 08:18:50 UTC"
  },
  {
    "arxiv_id": "2410.02800v1",
    "title": "Estimating Body Volume and Height Using 3D Data",
    "authors": [
      "Vivek Ganesh Sonar",
      "Muhammad Tanveer Jan",
      "Mike Wells",
      "Abhijit Pandya",
      "Gabriela Engstrom",
      "Richard Shih",
      "Borko Furht"
    ],
    "abstract": "Accurate body weight estimation is critical in emergency medicine for proper\ndosing of weight-based medications, yet direct measurement is often impractical\nin urgent situations. This paper presents a non-invasive method for estimating\nbody weight by calculating total body volume and height using 3D imaging\ntechnology. A RealSense D415 camera is employed to capture high-resolution\ndepth maps of the patient, from which 3D models are generated. The Convex Hull\nAlgorithm is then applied to calculate the total body volume, with enhanced\naccuracy achieved by segmenting the point cloud data into multiple sections and\nsumming their individual volumes. The height is derived from the 3D model by\nidentifying the distance between key points on the body. This combined approach\nprovides an accurate estimate of body weight, improving the reliability of\nmedical interventions where precise weight data is unavailable. The proposed\nmethod demonstrates significant potential to enhance patient safety and\ntreatment outcomes in emergency settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02800v1",
    "published_date": "2024-09-18 16:20:46 UTC",
    "updated_date": "2024-09-18 16:20:46 UTC"
  },
  {
    "arxiv_id": "2409.15364v1",
    "title": "VERA: Validation and Enhancement for Retrieval Augmented systems",
    "authors": [
      "Nitin Aravind Birur",
      "Tanay Baswa",
      "Divyanshu Kumar",
      "Jatan Loya",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable capabilities but often\nproduce inaccurate responses, as they rely solely on their embedded knowledge.\nRetrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external\ninformation retrieval system, supplying additional context along with the query\nto mitigate inaccuracies for a particular context. However, accuracy issues\nstill remain, as the model may rely on irrelevant documents or extrapolate\nincorrectly from its training knowledge. To assess and improve the performance\nof both the retrieval system and the LLM in a RAG framework, we propose\n\\textbf{VERA} (\\textbf{V}alidation and \\textbf{E}nhancement for\n\\textbf{R}etrieval \\textbf{A}ugmented systems), a system designed to: 1)\nEvaluate and enhance the retrieved context before response generation, and 2)\nEvaluate and refine the LLM-generated response to ensure precision and minimize\nerrors. VERA employs an evaluator-cum-enhancer LLM that first checks if\nexternal retrieval is necessary, evaluates the relevance and redundancy of the\nretrieved context, and refines it to eliminate non-essential information.\nPost-response generation, VERA splits the response into atomic statements,\nassesses their relevance to the query, and ensures adherence to the context.\nOur experiments demonstrate VERA's remarkable efficacy not only in improving\nthe performance of smaller open-source models, but also larger state-of-the art\nmodels. These enhancements underscore VERA's potential to produce accurate and\nrelevant responses, advancing the state-of-the-art in retrieval-augmented\nlanguage modeling. VERA's robust methodology, combining multiple evaluation and\nrefinement steps, effectively mitigates hallucinations and improves retrieval\nand response processes, making it a valuable tool for applications demanding\nhigh accuracy and reliability in information generation. .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15364v1",
    "published_date": "2024-09-18 16:10:47 UTC",
    "updated_date": "2024-09-18 16:10:47 UTC"
  },
  {
    "arxiv_id": "2409.12092v2",
    "title": "IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition",
    "authors": [
      "Rui Liu",
      "Zahiruddin Mahammad",
      "Amisha Bhaskar",
      "Pratap Tokekar"
    ],
    "abstract": "Robotic assistive feeding holds significant promise for improving the quality\nof life for individuals with eating disabilities. However, acquiring diverse\nfood items under varying conditions and generalizing to unseen food presents\nunique challenges. Existing methods that rely on surface-level geometric\ninformation (e.g., bounding box and pose) derived from visual cues (e.g.,\ncolor, shape, and texture) often lacks adaptability and robustness, especially\nwhen foods share similar physical properties but differ in visual appearance.\nWe employ imitation learning (IL) to learn a policy for food acquisition.\nExisting methods employ IL or Reinforcement Learning (RL) to learn a policy\nbased on off-the-shelf image encoders such as ResNet-50. However, such\nrepresentations are not robust and struggle to generalize across diverse\nacquisition scenarios. To address these limitations, we propose a novel\napproach, IMRL (Integrated Multi-Dimensional Representation Learning), which\nintegrates visual, physical, temporal, and geometric representations to enhance\nthe robustness and generalizability of IL for food acquisition. Our approach\ncaptures food types and physical properties (e.g., solid, semi-solid, granular,\nliquid, and mixture), models temporal dynamics of acquisition actions, and\nintroduces geometric information to determine optimal scooping points and\nassess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies\nbased on context, improving the robot's capability to handle diverse food\nacquisition scenarios. Experiments on a real robot demonstrate our approach's\nrobustness and adaptability across various foods and bowl configurations,\nincluding zero-shot generalization to unseen settings. Our approach achieves\nimprovement up to $35\\%$ in success rate compared with the best-performing\nbaseline. More details can be found on our website\nhttps://ruiiu.github.io/imrl.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12092v2",
    "published_date": "2024-09-18 16:09:06 UTC",
    "updated_date": "2025-03-18 15:32:55 UTC"
  },
  {
    "arxiv_id": "2409.12087v3",
    "title": "Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques",
    "authors": [
      "Yubo Li",
      "Saba Al-Sayouri",
      "Rema Padman"
    ],
    "abstract": "This study explores the potential of utilizing administrative claims data,\ncombined with advanced machine learning and deep learning techniques, to\npredict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal\nDisease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major\nhealth insurance organization to develop prediction models for multiple\nobservation windows using traditional machine learning methods such as Random\nForest and XGBoost as well as deep learning approaches such as Long Short-Term\nMemory (LSTM) networks. Our findings demonstrate that the LSTM model,\nparticularly with a 24-month observation window, exhibits superior performance\nin predicting ESRD progression, outperforming existing models in the\nliterature. We further apply SHapley Additive exPlanations (SHAP) analysis to\nenhance interpretability, providing insights into the impact of individual\nfeatures on predictions at the individual patient level. This study underscores\nthe value of leveraging administrative claims data for CKD management and\npredicting ESRD progression.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10pages, 4 figures, AMIA 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12087v3",
    "published_date": "2024-09-18 16:03:57 UTC",
    "updated_date": "2024-10-25 06:33:42 UTC"
  },
  {
    "arxiv_id": "2409.12072v1",
    "title": "PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning",
    "authors": [
      "Yukai Xu",
      "Yujie Gu",
      "Kouichi Sakurai"
    ],
    "abstract": "Backdoor attacks pose a significant threat to deep neural networks,\nparticularly as recent advancements have led to increasingly subtle\nimplantation, making the defense more challenging. Existing defense mechanisms\ntypically rely on an additional clean dataset as a standard reference and\ninvolve retraining an auxiliary model or fine-tuning the entire victim model.\nHowever, these approaches are often computationally expensive and not always\nfeasible in practical applications. In this paper, we propose a novel and\nlightweight defense mechanism, termed PAD-FT, that does not require an\nadditional clean dataset and fine-tunes only a very small part of the model to\ndisinfect the victim model. To achieve this, our approach first introduces a\nsimple data purification process to identify and select the most-likely clean\ndata from the poisoned training dataset. The self-purified clean dataset is\nthen used for activation clipping and fine-tuning only the last classification\nlayer of the victim model. By integrating data purification, activation\nclipping, and classifier fine-tuning, our mechanism PAD-FT demonstrates\nsuperior effectiveness across multiple backdoor attack methods and datasets, as\nconfirmed through extensive experimental evaluation.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12072v1",
    "published_date": "2024-09-18 15:47:23 UTC",
    "updated_date": "2024-09-18 15:47:23 UTC"
  },
  {
    "arxiv_id": "2409.12061v1",
    "title": "Generalized Robot Learning Framework",
    "authors": [
      "Jiahuan Yan",
      "Zhouyang Hong",
      "Yu Zhao",
      "Yu Tian",
      "Yunxin Liu",
      "Travis Davies",
      "Luhui Hu"
    ],
    "abstract": "Imitation based robot learning has recently gained significant attention in\nthe robotics field due to its theoretical potential for transferability and\ngeneralizability. However, it remains notoriously costly, both in terms of\nhardware and data collection, and deploying it in real-world environments\ndemands meticulous setup of robots and precise experimental conditions. In this\npaper, we present a low-cost robot learning framework that is both easily\nreproducible and transferable to various robots and environments. We\ndemonstrate that deployable imitation learning can be successfully applied even\nto industrial-grade robots, not just expensive collaborative robotic arms.\nFurthermore, our results show that multi-task robot learning is achievable with\nsimple network architectures and fewer demonstrations than previously thought\nnecessary. As the current evaluating method is almost subjective when it comes\nto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a\nnovel evaluation strategy that provides a more objective assessment of\nperformance. We conduct an extensive comparison of success rates across various\nself-designed tasks to validate our approach. To foster collaboration and\nsupport the robot learning community, we have open-sourced all relevant\ndatasets and model checkpoints, available at huggingface.co/ZhiChengAI.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 2 figures. cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2409.12061v1",
    "published_date": "2024-09-18 15:34:31 UTC",
    "updated_date": "2024-09-18 15:34:31 UTC"
  },
  {
    "arxiv_id": "2409.12060v2",
    "title": "PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models",
    "authors": [
      "Andrianos Michail",
      "Simon Clematide",
      "Juri Opitz"
    ],
    "abstract": "The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,\nbenchmarking and selection of paraphrase detection models. We find that\nparaphrase detection models under our fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\nFurthermore, PARAPHRASUS allows prompt calibration for different use cases,\ntailoring LLM models to specific strictness levels. PARAPHRASUS includes 3\nchallenges spanning over 10 datasets, including 8 repurposed and 2 newly\nannotated; we release it along with a benchmarking library at\nhttps://github.com/impresso/paraphrasus",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "to appear at COLING2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12060v2",
    "published_date": "2024-09-18 15:33:48 UTC",
    "updated_date": "2024-12-16 10:09:23 UTC"
  },
  {
    "arxiv_id": "2409.12059v4",
    "title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning",
    "authors": [
      "Ningyuan Xi",
      "Xiaoyu Wang",
      "Yetao Wu",
      "Teng Chen",
      "Qingqing Gu",
      "Yue Zhao",
      "Jinxian Qu",
      "Zhonglin Jiang",
      "Yong Chen",
      "Luo Ji"
    ],
    "abstract": "Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12059v4",
    "published_date": "2024-09-18 15:32:48 UTC",
    "updated_date": "2025-04-25 16:03:15 UTC"
  },
  {
    "arxiv_id": "2409.12038v1",
    "title": "A Unified Framework for Neural Computation and Learning Over Time",
    "authors": [
      "Stefano Melacci",
      "Alessandro Betti",
      "Michele Casoni",
      "Tommaso Guidi",
      "Matteo Tiezzi",
      "Marco Gori"
    ],
    "abstract": "This paper proposes Hamiltonian Learning, a novel unified framework for\nlearning with neural networks \"over time\", i.e., from a possibly infinite\nstream of data, in an online manner, without having access to future\ninformation. Existing works focus on the simplified setting in which the stream\nhas a known finite length or is segmented into smaller sequences, leveraging\nwell-established learning strategies from statistical machine learning. In this\npaper, the problem of learning over time is rethought from scratch, leveraging\ntools from optimal control theory, which yield a unifying view of the temporal\ndynamics of neural computations and learning. Hamiltonian Learning is based on\ndifferential equations that: (i) can be integrated without the need of external\nsoftware solvers; (ii) generalize the well-established notion of gradient-based\nlearning in feed-forward and recurrent networks; (iii) open to novel\nperspectives. The proposed framework is showcased by experimentally proving how\nit can recover gradient-based learning, comparing it to out-of-the box\noptimizers, and describing how it is flexible enough to switch from fully-local\nto partially/non-local computational schemes, possibly distributed over\nmultiple devices, and BackPropagation without storing activations. Hamiltonian\nLearning is easy to implement and can help researches approach in a principled\nand innovative manner the problem of learning over time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12038v1",
    "published_date": "2024-09-18 14:57:13 UTC",
    "updated_date": "2024-09-18 14:57:13 UTC"
  },
  {
    "arxiv_id": "2409.12033v1",
    "title": "Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes",
    "authors": [
      "Marco Montagna",
      "Simone Scardapane",
      "Lev Telyatnikov"
    ],
    "abstract": "Graph Neural Networks based on the message-passing (MP) mechanism are a\ndominant approach for handling graph-structured data. However, they are\ninherently limited to modeling only pairwise interactions, making it difficult\nto explicitly capture the complexity of systems with $n$-body relations. To\naddress this, topological deep learning has emerged as a promising field for\nstudying and modeling higher-order interactions using various topological\ndomains, such as simplicial and cellular complexes. While these new domains\nprovide powerful representations, they introduce new challenges, such as\neffectively modeling the interactions among higher-order structures through\nhigher-order MP. Meanwhile, structured state-space sequence models have proven\nto be effective for sequence modeling and have recently been adapted for graph\ndata by encoding the neighborhood of a node as a sequence, thereby avoiding the\nMP mechanism. In this work, we propose a novel architecture designed to operate\nwith simplicial complexes, utilizing the Mamba state-space model as its\nbackbone. Our approach generates sequences for the nodes based on the\nneighboring cells, enabling direct communication between all higher-order\nstructures, regardless of their rank. We extensively validate our model,\ndemonstrating that it achieves competitive performance compared to\nstate-of-the-art models developed for simplicial complexes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12033v1",
    "published_date": "2024-09-18 14:49:25 UTC",
    "updated_date": "2024-09-18 14:49:25 UTC"
  },
  {
    "arxiv_id": "2409.12020v1",
    "title": "Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization",
    "authors": [
      "Zhi Chen",
      "Lingxiao Jiang"
    ],
    "abstract": "In the rapidly evolving field of machine learning, training models with\ndatasets from various locations and organizations presents significant\nchallenges due to privacy and legal concerns. The exploration of effective\ncollaborative training settings capable of leveraging valuable knowledge from\ndistributed and isolated datasets is increasingly crucial. This study\ninvestigates key factors that impact the effectiveness of collaborative\ntraining methods in code next-token prediction, as well as the correctness and\nutility of the generated code, demonstrating the promise of such methods.\nAdditionally, we evaluate the memorization of different participant training\ndata across various collaborative training settings, including centralized,\nfederated, and incremental training, highlighting their potential risks in\nleaking data. Our findings indicate that the size and diversity of code\ndatasets are pivotal factors influencing the success of collaboratively trained\ncode models. We show that federated learning achieves competitive performance\ncompared to centralized training while offering better data protection, as\nevidenced by lower memorization ratios in the generated code. However,\nfederated learning can still produce verbatim code snippets from hidden\ntraining data, potentially violating privacy or copyright. Our study further\nexplores effectiveness and memorization patterns in incremental learning,\nemphasizing the sequence in which individual participant datasets are\nintroduced. We also identify cross-organizational clones as a prevalent\nchallenge in both centralized and federated learning scenarios. Our findings\nhighlight the persistent risk of data leakage during inference, even when\ntraining data remains unseen. We conclude with recommendations for\npractitioners and researchers to optimize multisource datasets, propelling\ncross-organizational collaboration forward.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Paper accepted to the ASE 2024 Conference Research Track",
    "pdf_url": "http://arxiv.org/pdf/2409.12020v1",
    "published_date": "2024-09-18 14:30:48 UTC",
    "updated_date": "2024-09-18 14:30:48 UTC"
  },
  {
    "arxiv_id": "2409.12005v2",
    "title": "Representing Positional Information in Generative World Models for Object Manipulation",
    "authors": [
      "Stefano Ferraro",
      "Pietro Mazzaglia",
      "Tim Verbelen",
      "Bart Dhoedt",
      "Sai Rajeswar"
    ],
    "abstract": "Object manipulation capabilities are essential skills that set apart embodied\nagents engaging with the world, especially in the realm of robotics. The\nability to predict outcomes of interactions with objects is paramount in this\nsetting. While model-based control methods have started to be employed for\ntackling manipulation tasks, they have faced challenges in accurately\nmanipulating objects. As we analyze the causes of this limitation, we identify\nthe cause of underperformance in the way current world models represent crucial\npositional information, especially about the target's goal specification for\nobject positioning tasks. We introduce a general approach that empowers world\nmodel-based agents to effectively solve object-positioning tasks. We propose\ntwo declinations of this approach for generative world models:\nposition-conditioned (PCP) and latent-conditioned (LCP) policy learning. In\nparticular, LCP employs object-centric latent representations that explicitly\ncapture object positional information for goal specification. This naturally\nleads to the emergence of multimodal capabilities, enabling the specification\nof goals through spatial coordinates or a visual goal. Our methods are\nrigorously evaluated across several manipulation environments, showing\nfavorable performance compared to current model-based control approaches.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12005v2",
    "published_date": "2024-09-18 14:19:50 UTC",
    "updated_date": "2024-09-19 07:38:06 UTC"
  },
  {
    "arxiv_id": "2409.12001v1",
    "title": "Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning",
    "authors": [
      "Claude Formanek",
      "Louise Beyers",
      "Callum Rhys Tilbury",
      "Jonathan P. Shock",
      "Arnu Pretorius"
    ],
    "abstract": "Offline multi-agent reinforcement learning (MARL) is an exciting direction of\nresearch that uses static datasets to find optimal control policies for\nmulti-agent systems. Though the field is by definition data-driven, efforts\nhave thus far neglected data in their drive to achieve state-of-the-art\nresults. We first substantiate this claim by surveying the literature, showing\nhow the majority of works generate their own datasets without consistent\nmethodology and provide sparse information about the characteristics of these\ndatasets. We then show why neglecting the nature of the data is problematic,\nthrough salient examples of how tightly algorithmic performance is coupled to\nthe dataset used, necessitating a common foundation for experiments in the\nfield. In response, we take a big step towards improving data usage and data\nawareness in offline MARL, with three key contributions: (1) a clear guideline\nfor generating novel datasets; (2) a standardisation of over 80 existing\ndatasets, hosted in a publicly available repository, using a consistent storage\nformat and easy-to-use API; and (3) a suite of analysis tools that allow us to\nunderstand these datasets better, aiding further development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12001v1",
    "published_date": "2024-09-18 14:13:24 UTC",
    "updated_date": "2024-09-18 14:13:24 UTC"
  },
  {
    "arxiv_id": "2409.11992v1",
    "title": "Additive-feature-attribution methods: a review on explainable artificial intelligence for fluid dynamics and heat transfer",
    "authors": [
      "Andrés Cremades",
      "Sergio Hoyas",
      "Ricardo Vinuesa"
    ],
    "abstract": "The use of data-driven methods in fluid mechanics has surged dramatically in\nrecent years due to their capacity to adapt to the complex and multi-scale\nnature of turbulent flows, as well as to detect patterns in large-scale\nsimulations or experimental tests. In order to interpret the relationships\ngenerated in the models during the training process, numerical attributions\nneed to be assigned to the input features. One important example are the\nadditive-feature-attribution methods. These explainability methods link the\ninput features with the model prediction, providing an interpretation based on\na linear formulation of the models. The SHapley Additive exPlanations (SHAP\nvalues) are formulated as the only possible interpretation that offers a unique\nsolution for understanding the model. In this manuscript, the\nadditive-feature-attribution methods are presented, showing four common\nimplementations in the literature: kernel SHAP, tree SHAP, gradient SHAP, and\ndeep SHAP. Then, the main applications of the additive-feature-attribution\nmethods are introduced, dividing them into three main groups: turbulence\nmodeling, fluid-mechanics fundamentals, and applied problems in fluid dynamics\nand heat transfer. This review shows thatexplainability techniques, and in\nparticular additive-feature-attribution methods, are crucial for implementing\ninterpretable and physics-compliant deep-learning models in the fluid-mechanics\nfield.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11992v1",
    "published_date": "2024-09-18 13:59:02 UTC",
    "updated_date": "2024-09-18 13:59:02 UTC"
  },
  {
    "arxiv_id": "2409.12720v1",
    "title": "FAST GDRNPP: Improving the Speed of State-of-the-Art 6D Object Pose Estimation",
    "authors": [
      "Thomas Pöllabauer",
      "Ashwin Pramod",
      "Volker Knauthe",
      "Michael Wahl"
    ],
    "abstract": "6D object pose estimation involves determining the three-dimensional\ntranslation and rotation of an object within a scene and relative to a chosen\ncoordinate system. This problem is of particular interest for many practical\napplications in industrial tasks such as quality control, bin picking, and\nrobotic manipulation, where both speed and accuracy are critical for real-world\ndeployment. Current models, both classical and deep-learning-based, often\nstruggle with the trade-off between accuracy and latency. Our research focuses\non enhancing the speed of a prominent state-of-the-art deep learning model,\nGDRNPP, while keeping its high accuracy. We employ several techniques to reduce\nthe model size and improve inference time. These techniques include using\nsmaller and quicker backbones, pruning unnecessary parameters, and distillation\nto transfer knowledge from a large, high-performing model to a smaller, more\nefficient student model. Our findings demonstrate that the proposed\nconfiguration maintains accuracy comparable to the state-of-the-art while\nsignificantly improving inference time. This advancement could lead to more\nefficient and practical applications in various industrial scenarios, thereby\nenhancing the overall applicability of 6D Object Pose Estimation models in\nreal-world settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12720v1",
    "published_date": "2024-09-18 12:30:02 UTC",
    "updated_date": "2024-09-18 12:30:02 UTC"
  },
  {
    "arxiv_id": "2409.12213v1",
    "title": "SemAI: Semantic Artificial Intelligence-enhanced DNA storage for Internet-of-Things",
    "authors": [
      "Wenfeng Wu",
      "Luping Xiang",
      "Qiang Liu",
      "Kun Yang"
    ],
    "abstract": "In the wake of the swift evolution of technologies such as the Internet of\nThings (IoT), the global data landscape undergoes an exponential surge,\npropelling DNA storage into the spotlight as a prospective medium for\ncontemporary cloud storage applications. This paper introduces a Semantic\nArtificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm,\ndistinguishing itself from prevalent deep learning-based methodologies through\ntwo key modifications: 1) embedding a semantic extraction module at the\nencoding terminus, facilitating the meticulous encoding and storage of nuanced\nsemantic information; 2) conceiving a forethoughtful multi-reads filtering\nmodel at the decoding terminus, leveraging the inherent multi-copy propensity\nof DNA molecules to bolster system fault tolerance, coupled with a\nstrategically optimized decoder's architectural framework. Numerical results\ndemonstrate the SemAI-DNA's efficacy, attaining 2.61 dB Peak Signal-to-Noise\nRatio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM)\nover conventional deep learning-based approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12213v1",
    "published_date": "2024-09-18 12:21:58 UTC",
    "updated_date": "2024-09-18 12:21:58 UTC"
  },
  {
    "arxiv_id": "2409.11905v2",
    "title": "AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots",
    "authors": [
      "Zhaxizhuoma Zhaxizhuoma",
      "Pengan Chen",
      "Ziniu Wu",
      "Jiawei Sun",
      "Dong Wang",
      "Peng Zhou",
      "Nieqing Cao",
      "Yan Ding",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "This paper presents AlignBot, a novel framework designed to optimize\nVLM-powered customized task planning for household robots by effectively\naligning with user reminders. In domestic settings, aligning task planning with\nuser reminders poses significant challenges due to the limited quantity,\ndiversity, and multimodal nature of the reminders. To address these challenges,\nAlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for\nGPT-4o. This adapter model internalizes diverse forms of user reminders-such as\npersonalized preferences, corrective guidance, and contextual assistance-into\nstructured instruction-formatted cues that prompt GPT-4o in generating\ncustomized task plans. Additionally, AlignBot integrates a dynamic retrieval\nmechanism that selects task-relevant historical successes as prompts for\nGPT-4o, further enhancing task planning accuracy. To validate the effectiveness\nof AlignBot, experiments are conducted in real-world household environments,\nwhich are constructed within the laboratory to replicate typical household\nsettings. A multimodal dataset with over 1,500 entries derived from volunteer\nreminders is used for training and evaluation. The results demonstrate that\nAlignBot significantly improves customized task planning, outperforming\nexisting LLM- and VLM-powered planners by interpreting and aligning with user\nreminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline\nat 21.6%, reflecting a 65% improvement and over four times greater\neffectiveness. Supplementary materials are available at:\nhttps://yding25.com/AlignBot/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11905v2",
    "published_date": "2024-09-18 12:05:30 UTC",
    "updated_date": "2025-03-21 04:40:24 UTC"
  },
  {
    "arxiv_id": "2409.11904v2",
    "title": "Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive Gen-AI Model Evaluation",
    "authors": [
      "Dimitrios Christodoulou",
      "Mads Kuhlmann-Jørgensen"
    ],
    "abstract": "Efficiently evaluating the performance of text-to-image models is difficult\nas it inherently requires subjective judgment and human preference, making it\nhard to compare different models and quantify the state of the art. Leveraging\nRapidata's technology, we present an efficient annotation framework that\nsources human feedback from a diverse, global pool of annotators. Our study\ncollected over 2 million annotations across 4,512 images, evaluating four\nprominent models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on style\npreference, coherence, and text-to-image alignment. We demonstrate that our\napproach makes it feasible to comprehensively rank image generation models\nbased on a vast pool of annotators and show that the diverse annotator\ndemographics reflect the world population, significantly decreasing the risk of\nbiases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11904v2",
    "published_date": "2024-09-18 12:02:20 UTC",
    "updated_date": "2024-10-15 14:23:46 UTC"
  },
  {
    "arxiv_id": "2409.11887v2",
    "title": "DocMamba: Efficient Document Pre-training with State Space Model",
    "authors": [
      "Pengfei Hu",
      "Zhenrong Zhang",
      "Jiefeng Ma",
      "Shuhang Liu",
      "Jun Du",
      "Jianshu Zhang"
    ],
    "abstract": "In recent years, visually-rich document understanding has attracted\nincreasing attention. Transformer-based pre-trained models have become the\nmainstream approach, yielding significant performance gains in this field.\nHowever, the self-attention mechanism's quadratic computational complexity\nhinders their efficiency and ability to process long documents. In this paper,\nwe present DocMamba, a novel framework based on the state space model. It is\ndesigned to reduce computational complexity to linear while preserving global\nmodeling capabilities. To further enhance its effectiveness in document\nprocessing, we introduce the Segment-First Bidirectional Scan (SFBS) to capture\ncontiguous semantic information. Experimental results demonstrate that DocMamba\nachieves new state-of-the-art results on downstream datasets such as FUNSD,\nCORD, and SORIE, while significantly improving speed and reducing memory usage.\nNotably, experiments on the HRDoc confirm DocMamba's potential for length\nextrapolation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11887v2",
    "published_date": "2024-09-18 11:34:28 UTC",
    "updated_date": "2025-02-10 07:31:35 UTC"
  },
  {
    "arxiv_id": "2409.11863v2",
    "title": "LEMMo-Plan: LLM-Enhanced Learning from Multi-Modal Demonstration for Planning Sequential Contact-Rich Manipulation Tasks",
    "authors": [
      "Kejia Chen",
      "Zheng Shen",
      "Yue Zhang",
      "Lingyun Chen",
      "Fan Wu",
      "Zhenshan Bing",
      "Sami Haddadin",
      "Alois Knoll"
    ],
    "abstract": "Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots. In this\npaper, we introduce an in-context learning framework that incorporates tactile\nand force-torque information from human demonstrations to enhance LLMs' ability\nto generate plans for new task scenarios. We propose a bootstrapped reasoning\npipeline that sequentially integrates each modality into a comprehensive task\nplan. This task plan is then used as a reference for planning in new task\nconfigurations. Real-world experiments on two different sequential manipulation\ntasks demonstrate the effectiveness of our framework in improving LLMs'\nunderstanding of multi-modal demonstrations and enhancing the overall planning\nperformance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11863v2",
    "published_date": "2024-09-18 10:36:47 UTC",
    "updated_date": "2025-03-10 18:24:51 UTC"
  },
  {
    "arxiv_id": "2409.11860v1",
    "title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation",
    "authors": [
      "Kasra Hosseini",
      "Thomas Kober",
      "Josip Krapac",
      "Roland Vollgraf",
      "Weiwei Cheng",
      "Ana Peleteiro Ramallo"
    ],
    "abstract": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages, 5 figures, 4 Tables",
    "pdf_url": "http://arxiv.org/pdf/2409.11860v1",
    "published_date": "2024-09-18 10:30:50 UTC",
    "updated_date": "2024-09-18 10:30:50 UTC"
  },
  {
    "arxiv_id": "2409.13764v1",
    "title": "Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs",
    "authors": [
      "Christos Fragkathoulas",
      "Odysseas S. Chlapanis"
    ],
    "abstract": "This paper introduces a novel task to assess the faithfulness of large\nlanguage models (LLMs) using local perturbations and self-explanations. Many\nLLMs often require additional context to answer certain questions correctly.\nFor this purpose, we propose a new efficient alternative explainability\ntechnique, inspired by the commonly used leave-one-out approach. Using this\napproach, we identify the sufficient and necessary parts for the LLM to\ngenerate correct answers, serving as explanations. We propose a metric for\nassessing faithfulness that compares these crucial parts with the\nself-explanations of the model. Using the Natural Questions dataset, we\nvalidate our approach, demonstrating its effectiveness in explaining model\ndecisions and assessing faithfulness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13764v1",
    "published_date": "2024-09-18 10:16:45 UTC",
    "updated_date": "2024-09-18 10:16:45 UTC"
  },
  {
    "arxiv_id": "2410.02795v1",
    "title": "TaCIE: Enhancing Instruction Comprehension in Large Language Models through Task-Centred Instruction Evolution",
    "authors": [
      "Jiuding Yang",
      "Shengyao Lu",
      "Weidong Guo",
      "Xiangyang Li",
      "Kaitong Yang",
      "Yu Xu",
      "Di Niu"
    ],
    "abstract": "Large Language Models (LLMs) require precise alignment with complex\ninstructions to optimize their performance in real-world applications. As the\ndemand for refined instruction tuning data increases, traditional methods that\nevolve simple seed instructions often struggle to effectively enhance\ncomplexity or manage difficulty scaling across various domains. Our innovative\napproach, Task-Centered Instruction Evolution (TaCIE), addresses these\nshortcomings by redefining instruction evolution from merely evolving seed\ninstructions to a more dynamic and comprehensive combination of elements. TaCIE\nstarts by deconstructing complex instructions into their fundamental\ncomponents. It then generates and integrates new elements with the original\nones, reassembling them into more sophisticated instructions that progressively\nincrease in difficulty, diversity, and complexity. Applied across multiple\ndomains, LLMs fine-tuned with these evolved instructions have substantially\noutperformed those tuned with conventional methods, marking a significant\nadvancement in instruction-based model fine-tuning.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02795v1",
    "published_date": "2024-09-18 10:06:28 UTC",
    "updated_date": "2024-09-18 10:06:28 UTC"
  },
  {
    "arxiv_id": "2409.11844v1",
    "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
    "authors": [
      "Tianle Gu",
      "Kexin Huang",
      "Ruilin Luo",
      "Yuanqi Yao",
      "Yujiu Yang",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "abstract": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11844v1",
    "published_date": "2024-09-18 09:55:48 UTC",
    "updated_date": "2024-09-18 09:55:48 UTC"
  },
  {
    "arxiv_id": "2409.11835v1",
    "title": "DPI-TTS: Directional Patch Interaction for Fast-Converging and Style Temporal Modeling in Text-to-Speech",
    "authors": [
      "Xin Qi",
      "Ruibo Fu",
      "Zhengqi Wen",
      "Tao Wang",
      "Chunyu Qiang",
      "Jianhua Tao",
      "Chenxing Li",
      "Yi Lu",
      "Shuchen Shi",
      "Zhiyong Wang",
      "Xiaopeng Wang",
      "Yuankun Xie",
      "Yukun Liu",
      "Xuefei Liu",
      "Guanjun Li"
    ],
    "abstract": "In recent years, speech diffusion models have advanced rapidly. Alongside the\nwidely used U-Net architecture, transformer-based models such as the Diffusion\nTransformer (DiT) have also gained attention. However, current DiT speech\nmodels treat Mel spectrograms as general images, which overlooks the specific\nacoustic properties of speech. To address these limitations, we propose a\nmethod called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which\nbuilds on DiT and achieves fast training without compromising accuracy.\nNotably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive\ninference approach that aligns more closely with acoustic properties, enhancing\nthe naturalness of the generated speech. Additionally, we introduce a\nfine-grained style temporal modeling method that further improves speaker style\nsimilarity. Experimental results demonstrate that our method increases the\ntraining speed by nearly 2 times and significantly outperforms the baseline\nmodels.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to ICASSP2025",
    "pdf_url": "http://arxiv.org/pdf/2409.11835v1",
    "published_date": "2024-09-18 09:36:55 UTC",
    "updated_date": "2024-09-18 09:36:55 UTC"
  },
  {
    "arxiv_id": "2409.12716v1",
    "title": "Optical Flow Matters: an Empirical Comparative Study on Fusing Monocular Extracted Modalities for Better Steering",
    "authors": [
      "Fouad Makiyeh",
      "Mark Bastourous",
      "Anass Bairouk",
      "Wei Xiao",
      "Mirjana Maras",
      "Tsun-Hsuan Wangb",
      "Marc Blanchon",
      "Ramin Hasani",
      "Patrick Chareyre",
      "Daniela Rus"
    ],
    "abstract": "Autonomous vehicle navigation is a key challenge in artificial intelligence,\nrequiring robust and accurate decision-making processes. This research\nintroduces a new end-to-end method that exploits multimodal information from a\nsingle monocular camera to improve the steering predictions for self-driving\ncars. Unlike conventional models that require several sensors which can be\ncostly and complex or rely exclusively on RGB images that may not be robust\nenough under different conditions, our model significantly improves vehicle\nsteering prediction performance from a single visual sensor. By focusing on the\nfusion of RGB imagery with depth completion information or optical flow data,\nwe propose a comprehensive framework that integrates these modalities through\nboth early and hybrid fusion techniques.\n  We use three distinct neural network models to implement our approach:\nConvolution Neural Network - Neutral Circuit Policy (CNN-NCP) , Variational\nAuto Encoder - Long Short-Term Memory (VAE-LSTM) , and Neural Circuit Policy\narchitecture VAE-NCP. By incorporating optical flow into the decision-making\nprocess, our method significantly advances autonomous navigation. Empirical\nresults from our comparative study using Boston driving data show that our\nmodel, which integrates image and motion information, is robust and reliable.\nIt outperforms state-of-the-art approaches that do not use optical flow,\nreducing the steering estimation error by 31%. This demonstrates the potential\nof optical flow data, combined with advanced neural network architectures (a\nCNN-based structure for fusing data and a Recurrence-based network for\ninferring a command from latent space), to enhance the performance of\nautonomous vehicles steering estimation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12716v1",
    "published_date": "2024-09-18 09:36:24 UTC",
    "updated_date": "2024-09-18 09:36:24 UTC"
  },
  {
    "arxiv_id": "2409.11820v1",
    "title": "Optimizing Job Shop Scheduling in the Furniture Industry: A Reinforcement Learning Approach Considering Machine Setup, Batch Variability, and Intralogistics",
    "authors": [
      "Malte Schneevogt",
      "Karsten Binninger",
      "Noah Klarmann"
    ],
    "abstract": "This paper explores the potential application of Deep Reinforcement Learning\nin the furniture industry. To offer a broad product portfolio, most furniture\nmanufacturers are organized as a job shop, which ultimately results in the Job\nShop Scheduling Problem (JSSP). The JSSP is addressed with a focus on extending\ntraditional models to better represent the complexities of real-world\nproduction environments. Existing approaches frequently fail to consider\ncritical factors such as machine setup times or varying batch sizes. A concept\nfor a model is proposed that provides a higher level of information detail to\nenhance scheduling accuracy and efficiency. The concept introduces the\nintegration of DRL for production planning, particularly suited to batch\nproduction industries such as the furniture industry. The model extends\ntraditional approaches to JSSPs by including job volumes, buffer management,\ntransportation times, and machine setup times. This enables more precise\nforecasting and analysis of production flows and processes, accommodating the\nvariability and complexity inherent in real-world manufacturing processes. The\nRL agent learns to optimize scheduling decisions. It operates within a discrete\naction space, making decisions based on detailed observations. A reward\nfunction guides the agent's decision-making process, thereby promoting\nefficient scheduling and meeting production deadlines. Two integration\nstrategies for implementing the RL agent are discussed: episodic planning,\nwhich is suitable for low-automation environments, and continuous planning,\nwhich is ideal for highly automated plants. While episodic planning can be\nemployed as a standalone solution, the continuous planning approach\nnecessitates the integration of the agent with ERP and Manufacturing Execution\nSystems. This integration enables real-time adjustments to production schedules\nbased on dynamic changes.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 8 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.11820v1",
    "published_date": "2024-09-18 09:12:40 UTC",
    "updated_date": "2024-09-18 09:12:40 UTC"
  },
  {
    "arxiv_id": "2409.11817v1",
    "title": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large models in medical image analysis",
    "authors": [
      "Shaojie Li",
      "Zhaoshuo Diao"
    ],
    "abstract": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11817v1",
    "published_date": "2024-09-18 09:08:16 UTC",
    "updated_date": "2024-09-18 09:08:16 UTC"
  },
  {
    "arxiv_id": "2409.11813v1",
    "title": "EventAug: Multifaceted Spatio-Temporal Data Augmentation Methods for Event-based Learning",
    "authors": [
      "Yukun Tian",
      "Hao Chen",
      "Yongjian Deng",
      "Feihong Shen",
      "Kepan Liu",
      "Wei You",
      "Ziyang Zhang"
    ],
    "abstract": "The event camera has demonstrated significant success across a wide range of\nareas due to its low time latency and high dynamic range. However, the\ncommunity faces challenges such as data deficiency and limited diversity, often\nresulting in over-fitting and inadequate feature learning. Notably, the\nexploration of data augmentation techniques in the event community remains\nscarce. This work aims to address this gap by introducing a systematic\naugmentation scheme named EventAug to enrich spatial-temporal diversity. In\nparticular, we first propose Multi-scale Temporal Integration (MSTI) to\ndiversify the motion speed of objects, then introduce Spatial-salient Event\nMask (SSEM) and Temporal-salient Event Mask (TSEM) to enrich object variants.\nOur EventAug can facilitate models learning with richer motion patterns, object\nvariants and local spatio-temporal relations, thus improving model robustness\nto varied moving speeds, occlusions, and action disruptions. Experiment results\nshow that our augmentation method consistently yields significant improvements\nacross different tasks and backbones (e.g., a 4.87% accuracy gain on DVS128\nGesture). Our code will be publicly available for this community.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11813v1",
    "published_date": "2024-09-18 09:01:34 UTC",
    "updated_date": "2024-09-18 09:01:34 UTC"
  },
  {
    "arxiv_id": "2409.11802v1",
    "title": "Latent fingerprint enhancement for accurate minutiae detection",
    "authors": [
      "Abdul Wahab",
      "Tariq Mahmood Khan",
      "Shahzaib Iqbal",
      "Bandar AlShammari",
      "Bandar Alhaqbani",
      "Imran Razzak"
    ],
    "abstract": "Identification of suspects based on partial and smudged fingerprints,\ncommonly referred to as fingermarks or latent fingerprints, presents a\nsignificant challenge in the field of fingerprint recognition. Although\nfixed-length embeddings have shown effectiveness in recognising rolled and slap\nfingerprints, the methods for matching latent fingerprints have primarily\ncentred around local minutiae-based embeddings, failing to fully exploit global\nrepresentations for matching purposes. Consequently, enhancing latent\nfingerprints becomes critical to ensuring robust identification for forensic\ninvestigations. Current approaches often prioritise restoring ridge patterns,\noverlooking the fine-macroeconomic details crucial for accurate fingerprint\nrecognition. To address this, we propose a novel approach that uses generative\nadversary networks (GANs) to redefine Latent Fingerprint Enhancement (LFE)\nthrough a structured approach to fingerprint generation. By directly optimising\nthe minutiae information during the generation process, the model produces\nenhanced latent fingerprints that exhibit exceptional fidelity to ground-truth\ninstances. This leads to a significant improvement in identification\nperformance. Our framework integrates minutiae locations and orientation\nfields, ensuring the preservation of both local and structural fingerprint\nfeatures. Extensive evaluations conducted on two publicly available datasets\ndemonstrate our method's dominance over existing state-of-the-art techniques,\nhighlighting its potential to significantly enhance latent fingerprint\nrecognition accuracy in forensic applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11802v1",
    "published_date": "2024-09-18 08:35:31 UTC",
    "updated_date": "2024-09-18 08:35:31 UTC"
  },
  {
    "arxiv_id": "2409.11798v1",
    "title": "The Factuality of Large Language Models in the Legal Domain",
    "authors": [
      "Rajaa El Hamdani",
      "Thomas Bonald",
      "Fragkiskos Malliaros",
      "Nils Holzenberger",
      "Fabian Suchanek"
    ],
    "abstract": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "CIKM 2024, short paper",
    "pdf_url": "http://arxiv.org/pdf/2409.11798v1",
    "published_date": "2024-09-18 08:30:20 UTC",
    "updated_date": "2024-09-18 08:30:20 UTC"
  },
  {
    "arxiv_id": "2409.12210v1",
    "title": "Mixture of Diverse Size Experts",
    "authors": [
      "Manxi Sun",
      "Wei Liu",
      "Jian Luan",
      "Pengzhi Gao",
      "Bin Wang"
    ],
    "abstract": "The Sparsely-Activated Mixture-of-Experts (MoE) has gained increasing\npopularity for scaling up large language models (LLMs) without exploding\ncomputational costs. Despite its success, the current design faces a challenge\nwhere all experts have the same size, limiting the ability of tokens to choose\nthe experts with the most appropriate size for generating the next token. In\nthis paper, we propose the Mixture of Diverse Size Experts (MoDSE), a new MoE\narchitecture with layers designed to have experts of different sizes. Our\nanalysis of difficult token generation tasks shows that experts of various\nsizes achieve better predictions, and the routing path of the experts tends to\nbe stable after a training period. However, having experts of diverse sizes can\nlead to uneven workload distribution. To tackle this limitation, we introduce\nan expert-pair allocation strategy to evenly distribute the workload across\nmultiple GPUs. Comprehensive evaluations across multiple benchmarks demonstrate\nthe effectiveness of MoDSE, as it outperforms existing MoEs by allocating the\nparameter budget to experts adaptively while maintaining the same total\nparameter size and the number of experts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12210v1",
    "published_date": "2024-09-18 08:23:27 UTC",
    "updated_date": "2024-09-18 08:23:27 UTC"
  },
  {
    "arxiv_id": "2409.11786v1",
    "title": "Efficient Low-Resolution Face Recognition via Bridge Distillation",
    "authors": [
      "Shiming Ge",
      "Shengwei Zhao",
      "Chenyu Li",
      "Yu Zhang",
      "Jia Li"
    ],
    "abstract": "Face recognition in the wild is now advancing towards light-weight models,\nfast inference speed and resolution-adapted capability. In this paper, we\npropose a bridge distillation approach to turn a complex face model pretrained\non private high-resolution faces into a light-weight one for low-resolution\nface recognition. In our approach, such a cross-dataset resolution-adapted\nknowledge transfer problem is solved via two-step distillation. In the first\nstep, we conduct cross-dataset distillation to transfer the prior knowledge\nfrom private high-resolution faces to public high-resolution faces and generate\ncompact and discriminative features. In the second step, the resolution-adapted\ndistillation is conducted to further transfer the prior knowledge to synthetic\nlow-resolution faces via multi-task learning. By learning low-resolution face\nrepresentations and mimicking the adapted high-resolution knowledge, a\nlight-weight student model can be constructed with high efficiency and\npromising accuracy in recognizing low-resolution faces. Experimental results\nshow that the student model performs impressively in recognizing low-resolution\nfaces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed\nreaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobile\nphone, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is published in IEEE TIP 2020",
    "pdf_url": "http://arxiv.org/pdf/2409.11786v1",
    "published_date": "2024-09-18 08:10:35 UTC",
    "updated_date": "2024-09-18 08:10:35 UTC"
  },
  {
    "arxiv_id": "2409.11785v1",
    "title": "Distilling Channels for Efficient Deep Tracking",
    "authors": [
      "Shiming Ge",
      "Zhao Luo",
      "Chunhui Zhang",
      "Yingying Hua",
      "Dacheng Tao"
    ],
    "abstract": "Deep trackers have proven success in visual tracking. Typically, these\ntrackers employ optimally pre-trained deep networks to represent all diverse\nobjects with multi-channel features from some fixed layers. The deep networks\nemployed are usually trained to extract rich knowledge from massive data used\nin object classification and so they are capable to represent generic objects\nvery well. However, these networks are too complex to represent a specific\nmoving object, leading to poor generalization as well as high computational and\nmemory costs. This paper presents a novel and general framework termed channel\ndistillation to facilitate deep trackers. To validate the effectiveness of\nchannel distillation, we take discriminative correlation filter (DCF) and ECO\nfor example. We demonstrate that an integrated formulation can turn feature\ncompression, response map generation, and model update into a unified energy\nminimization problem to adaptively select informative feature channels that\nimprove the efficacy of tracking moving objects on the fly. Channel\ndistillation can accurately extract good channels, alleviating the influence of\nnoisy channels and generally reducing the number of channels, as well as\nadaptively generalizing to different channels and networks. The resulting deep\ntracker is accurate, fast, and has low memory requirements. Extensive\nexperimental evaluations on popular benchmarks clearly demonstrate the\neffectiveness and generalizability of our framework.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published by IEEE TIP 2020",
    "pdf_url": "http://arxiv.org/pdf/2409.11785v1",
    "published_date": "2024-09-18 08:09:20 UTC",
    "updated_date": "2024-09-18 08:09:20 UTC"
  },
  {
    "arxiv_id": "2409.12209v1",
    "title": "Multivariate Analysis of Gut Microbiota Composition and Prevalence of Gastric Cancer",
    "authors": [
      "Aadhith Shankarnarayanan",
      "Dheeman Gangopadhyay",
      "Ayman Alzaatreh"
    ],
    "abstract": "The global surge in the cases of gastric cancer has prompted an investigation\ninto the potential of gut microbiota as a predictive marker for the disease.\nThe alterations in gut diversity are suspected to be associated with an\nelevated risk of gastric cancer. This paper delves into finding the correlation\nbetween gut microbiota and gastric cancer, focusing on patients who have\nundergone total and subtotal gastrectomy. Utilizing data mining and statistical\nlearning methods, an analysis was conducted on 16S-RNA sequenced genes obtained\nfrom 96 participants with the aim of identifying specific genera of gut\nmicrobiota associated with gastric cancer. The study reveals several prominent\nbacterial genera that could potentially serve as biomarkers assessing the risk\nof gastric cancer. These findings offer a pathway for early risk assessment and\nprecautionary measures in the diagnosis of gastric cancer. The intricate\nmechanisms through which these gut microbiotas influence gastric cancer\nprogression warrant further investigation. This research significantly aims to\ncontribute to the growing understanding of the gut-cancer axis and its\nimplications in disease prediction and prevention.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12209v1",
    "published_date": "2024-09-18 08:08:31 UTC",
    "updated_date": "2024-09-18 08:08:31 UTC"
  },
  {
    "arxiv_id": "2409.11782v2",
    "title": "Smart Data-Driven GRU Predictor for SnO$_2$ Thin films Characteristics",
    "authors": [
      "Faiza Bouamra",
      "Mohamed Sayah",
      "Labib Sadek Terrissa",
      "Noureddine Zerhouni"
    ],
    "abstract": "In material physics, characterization techniques are foremost crucial for\nobtaining the materials data regarding the physical properties as well as\nstructural, electronics, magnetic, optic, dielectric, and spectroscopic\ncharacteristics. However, for many materials, ensuring availability and safe\naccessibility is not always easy and fully warranted. Moreover, the use of\nmodeling and simulation techniques need a lot of theoretical knowledge, in\naddition of being associated to costly computation time and a great complexity\ndeal. Thus, analyzing materials with different techniques for multiple samples\nsimultaneously, still be very challenging for engineers and researchers. It is\nworth noting that although of being very risky, X-ray diffraction is the well\nknown and widely used characterization technique which gathers data from\nstructural properties of crystalline 1d, 2d or 3d materials. We propose in this\npaper, a Smart GRU for Gated Recurrent Unit model to forcast structural\ncharacteristics or properties of thin films of tin oxide SnO$_2$(110). Indeed,\nthin films samples are elaborated and managed experimentally and the collected\ndata dictionary is then used to generate an AI -- Artificial Intelligence --\nGRU model for the thin films of tin oxide SnO$_2$(110) structural property\ncharacterization.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "19 pages, 14 figures. Baltica Journal, Special Issues, September 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.11782v2",
    "published_date": "2024-09-18 08:05:08 UTC",
    "updated_date": "2024-09-29 20:19:56 UTC"
  },
  {
    "arxiv_id": "2409.15361v1",
    "title": "Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning",
    "authors": [
      "Essa Jan",
      "Nouar AlDahoul",
      "Moiz Ali",
      "Faizan Ahmad",
      "Fareed Zaffar",
      "Yasir Zaki"
    ],
    "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to their\nadoption across a wide range of tasks, ranging from code generation to machine\ntranslation and sentiment analysis, etc. Red teaming/Safety alignment efforts\nshow that fine-tuning models on benign (non-harmful) data could compromise\nsafety. However, it remains unclear to what extent this phenomenon is\ninfluenced by different variables, including fine-tuning task, model\ncalibrations, etc. This paper explores the task-wise safety degradation due to\nfine-tuning on downstream tasks such as summarization, code generation,\ntranslation, and classification across various calibration. Our results reveal\nthat: 1) Fine-tuning LLMs for code generation and translation leads to the\nhighest degradation in safety guardrails. 2) LLMs generally have weaker\nguardrails for translation and classification, with 73-92% of harmful prompts\nanswered, across baseline and other calibrations, falling into one of two\nconcern categories. 3) Current solutions, including guards and safety tuning\ndatasets, lack cross-task robustness. To address these issues, we developed a\nnew multitask safety dataset effectively reducing attack success rates across a\nrange of tasks without compromising the model's overall helpfulness. Our work\nunderscores the need for generalized alignment measures to ensure safer and\nmore robust models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.15361v1",
    "published_date": "2024-09-18 08:04:24 UTC",
    "updated_date": "2024-09-18 08:04:24 UTC"
  },
  {
    "arxiv_id": "2409.11780v1",
    "title": "Explaining Non-monotonic Normative Reasoning using Argumentation Theory with Deontic Logic",
    "authors": [
      "Zhe Yu",
      "Yiwei Lu"
    ],
    "abstract": "In our previous research, we provided a reasoning system (called LeSAC) based\non argumentation theory to provide legal support to designers during the design\nprocess. Building on this, this paper explores how to provide designers with\neffective explanations for their legally relevant design decisions. We extend\nthe previous system for providing explanations by specifying norms and the key\nlegal or ethical principles for justifying actions in normative contexts.\nConsidering that first-order logic has strong expressive power, in the current\npaper we adopt a first-order deontic logic system with deontic operators and\npreferences. We illustrate the advantages and necessity of introducing deontic\nlogic and designing explanations under LeSAC by modelling two cases in the\ncontext of autonomous driving. In particular, this paper also discusses the\nrequirements of the updated LeSAC to guarantee rationality, and proves that a\nwell-defined LeSAC can satisfy the rationality postulate for rule-based\nargumentation frameworks. This ensures the system's ability to provide\ncoherent, legally valid explanations for complex design decisions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.11780v1",
    "published_date": "2024-09-18 08:03:29 UTC",
    "updated_date": "2024-09-18 08:03:29 UTC"
  },
  {
    "arxiv_id": "2409.11770v1",
    "title": "Knowledge Adaptation Network for Few-Shot Class-Incremental Learning",
    "authors": [
      "Ye Wang",
      "Yaxiong Wang",
      "Guoshuai Zhao",
      "Xueming Qian"
    ],
    "abstract": "Few-shot class-incremental learning (FSCIL) aims to incrementally recognize\nnew classes using a few samples while maintaining the performance on previously\nlearned classes. One of the effective methods to solve this challenge is to\nconstruct prototypical evolution classifiers. Despite the advancement achieved\nby most existing methods, the classifier weights are simply initialized using\nmean features. Because representations for new classes are weak and biased, we\nargue such a strategy is suboptimal. In this paper, we tackle this issue from\ntwo aspects. Firstly, thanks to the development of foundation models, we employ\na foundation model, the CLIP, as the network pedestal to provide a general\nrepresentation for each class. Secondly, to generate a more reliable and\ncomprehensive instance representation, we propose a Knowledge Adapter (KA)\nmodule that summarizes the data-specific knowledge from training data and fuses\nit into the general representation. Additionally, to tune the knowledge learned\nfrom the base classes to the upcoming classes, we propose a mechanism of\nIncremental Pseudo Episode Learning (IPEL) by simulating the actual FSCIL.\nTaken together, our proposed method, dubbed as Knowledge Adaptation Network\n(KANet), achieves competitive performance on a wide range of datasets,\nincluding CIFAR100, CUB200, and ImageNet-R.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages;6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.11770v1",
    "published_date": "2024-09-18 07:51:38 UTC",
    "updated_date": "2024-09-18 07:51:38 UTC"
  },
  {
    "arxiv_id": "2409.11764v2",
    "title": "One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation",
    "authors": [
      "Finn Lukas Busch",
      "Timon Homberger",
      "Jesús Ortega-Peimbert",
      "Quantao Yang",
      "Olov Andersson"
    ],
    "abstract": "The capability to efficiently search for objects in complex environments is\nfundamental for many real-world robot applications. Recent advances in\nopen-vocabulary vision models have resulted in semantically-informed object\nnavigation methods that allow a robot to search for an arbitrary object without\nprior training. However, these zero-shot methods have so far treated the\nenvironment as unknown for each consecutive query. In this paper we introduce a\nnew benchmark for zero-shot multi-object navigation, allowing the robot to\nleverage information gathered from previous searches to more efficiently find\nnew objects. To address this problem we build a reusable open-vocabulary\nfeature map tailored for real-time object search. We further propose a\nprobabilistic-semantic map update that mitigates common sources of errors in\nsemantic feature extraction and leverage this semantic uncertainty for informed\nmulti-object exploration. We evaluate our method on a set of object navigation\ntasks in both simulation as well as with a real robot, running in real-time on\na Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art\napproaches both on single and multi-object navigation tasks. Additional videos,\ncode and the multi-object navigation benchmark will be available on\nhttps://finnbsch.github.io/OneMap.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11764v2",
    "published_date": "2024-09-18 07:44:08 UTC",
    "updated_date": "2025-03-03 18:50:18 UTC"
  },
  {
    "arxiv_id": "2410.02791v1",
    "title": "DifFaiRec: Generative Fair Recommender with Conditional Diffusion Model",
    "authors": [
      "Zhenhao Jiang",
      "Jicong Fan"
    ],
    "abstract": "Although recommenders can ship items to users automatically based on the\nusers' preferences, they often cause unfairness to groups or individuals. For\ninstance, when users can be divided into two groups according to a sensitive\nsocial attribute and there is a significant difference in terms of activity\nbetween the two groups, the learned recommendation algorithm will result in a\nrecommendation gap between the two groups, which causes group unfairness. In\nthis work, we propose a novel recommendation algorithm named Diffusion-based\nFair Recommender (DifFaiRec) to provide fair recommendations. DifFaiRec is\nbuilt upon the conditional diffusion model and hence has a strong ability to\nlearn the distribution of user preferences from their ratings on items and is\nable to generate diverse recommendations effectively. To guarantee fairness, we\ndesign a counterfactual module to reduce the model sensitivity to protected\nattributes and provide mathematical explanations. The experiments on benchmark\ndatasets demonstrate the superiority of DifFaiRec over competitive baselines.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "The paper was accepted by ICDM 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02791v1",
    "published_date": "2024-09-18 07:39:33 UTC",
    "updated_date": "2024-09-18 07:39:33 UTC"
  },
  {
    "arxiv_id": "2409.11756v1",
    "title": "Synthesizing Evolving Symbolic Representations for Autonomous Systems",
    "authors": [
      "Gabriele Sartor",
      "Angelo Oddi",
      "Riccardo Rasconi",
      "Vieri Giuliano Santucci",
      "Rosa Meo"
    ],
    "abstract": "Recently, AI systems have made remarkable progress in various tasks. Deep\nReinforcement Learning(DRL) is an effective tool for agents to learn policies\nin low-level state spaces to solve highly complex tasks. Researchers have\nintroduced Intrinsic Motivation(IM) to the RL mechanism, which simulates the\nagent's curiosity, encouraging agents to explore interesting areas of the\nenvironment. This new feature has proved vital in enabling agents to learn\npolicies without being given specific goals. However, even though DRL\nintelligence emerges through a sub-symbolic model, there is still a need for a\nsort of abstraction to understand the knowledge collected by the agent. To this\nend, the classical planning formalism has been used in recent research to\nexplicitly represent the knowledge an autonomous agent acquires and effectively\nreach extrinsic goals. Despite classical planning usually presents limited\nexpressive capabilities, PPDDL demonstrated usefulness in reviewing the\nknowledge gathered by an autonomous system, making explicit causal\ncorrelations, and can be exploited to find a plan to reach any state the agent\nfaces during its experience. This work presents a new architecture implementing\nan open-ended learning system able to synthesize from scratch its experience\ninto a PPDDL representation and update it over time. Without a predefined set\nof goals and tasks, the system integrates intrinsic motivations to explore the\nenvironment in a self-directed way, exploiting the high-level knowledge\nacquired during its experience. The system explores the environment and\niteratively: (a) discover options, (b) explore the environment using options,\n(c) abstract the knowledge collected and (d) plan. This paper proposes an\nalternative approach to implementing open-ended learning architectures\nexploiting low-level and high-level representations to extend its knowledge in\na virtuous loop.",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11756v1",
    "published_date": "2024-09-18 07:23:26 UTC",
    "updated_date": "2024-09-18 07:23:26 UTC"
  },
  {
    "arxiv_id": "2409.11754v1",
    "title": "NPAT Null-Space Projected Adversarial Training Towards Zero Deterioration",
    "authors": [
      "Hanyi Hu",
      "Qiao Han",
      "Kui Chen",
      "Yao Yang"
    ],
    "abstract": "To mitigate the susceptibility of neural networks to adversarial attacks,\nadversarial training has emerged as a prevalent and effective defense strategy.\nIntrinsically, this countermeasure incurs a trade-off, as it sacrifices the\nmodel's accuracy in processing normal samples. To reconcile the trade-off, we\npioneer the incorporation of null-space projection into adversarial training\nand propose two innovative Null-space Projection based Adversarial\nTraining(NPAT) algorithms tackling sample generation and gradient optimization,\nnamed Null-space Projected Data Augmentation (NPDA) and Null-space Projected\nGradient Descent (NPGD), to search for an overarching optimal solutions, which\nenhance robustness with almost zero deterioration in generalization\nperformance. Adversarial samples and perturbations are constrained within the\nnull-space of the decision boundary utilizing a closed-form null-space\nprojector, effectively mitigating threat of attack stemming from unreliable\nfeatures. Subsequently, we conducted experiments on the CIFAR10 and SVHN\ndatasets and reveal that our methodology can seamlessly combine with\nadversarial training methods and obtain comparable robustness while keeping\ngeneralization close to a high-accuracy model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11754v1",
    "published_date": "2024-09-18 07:18:22 UTC",
    "updated_date": "2024-09-18 07:18:22 UTC"
  },
  {
    "arxiv_id": "2409.11744v3",
    "title": "Exploring Gaze Pattern Differences Between Autistic and Neurotypical Children: Clustering, Visualisation, and Prediction",
    "authors": [
      "Weiyan Shi",
      "Haihong Zhang",
      "Wei Wang",
      "Kenny Tsu Wei Choo"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) affects children's social and communication\nabilities, with eye-tracking widely used to identify atypical gaze patterns.\nWhile unsupervised clustering can automate the creation of areas of interest\nfor gaze feature extraction, the use of internal cluster validity indices, like\nSilhouette Coefficient, to distinguish gaze pattern differences between ASD and\ntypically developing (TD) children remains underexplored. We explore whether\ninternal cluster validity indices can distinguish ASD from TD children.\nSpecifically, we apply seven clustering algorithms to gaze points and extract\n63 internal cluster validity indices to reveal correlations with ASD diagnosis.\nUsing these indices, we train predictive models for ASD diagnosis. Experiments\non three datasets demonstrate high predictive accuracy (81\\% AUC), validating\nthe effectiveness of these indices.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2409.11744v3",
    "published_date": "2024-09-18 06:56:06 UTC",
    "updated_date": "2025-04-06 05:59:18 UTC"
  },
  {
    "arxiv_id": "2409.11741v1",
    "title": "HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning",
    "authors": [
      "Huawen Hu",
      "Enze Shi",
      "Chenxi Yue",
      "Shuocun Yang",
      "Zihao Wu",
      "Yiwei Li",
      "Tianyang Zhong",
      "Tuo Zhang",
      "Tianming Liu",
      "Shu Zhang"
    ],
    "abstract": "Human-in-the-loop reinforcement learning integrates human expertise to\naccelerate agent learning and provide critical guidance and feedback in complex\nfields. However, many existing approaches focus on single-agent tasks and\nrequire continuous human involvement during the training process, significantly\nincreasing the human workload and limiting scalability. In this paper, we\npropose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a\nmulti-agent reinforcement learning framework designed for group-oriented tasks.\nHARP integrates automatic agent regrouping with strategic human assistance\nduring deployment, enabling and allowing non-experts to offer effective\nguidance with minimal intervention. During training, agents dynamically adjust\ntheir groupings to optimize collaborative task completion. When deployed, they\nactively seek human assistance and utilize the Permutation Invariant Group\nCritic to evaluate and refine human-proposed groupings, allowing non-expert\nusers to contribute valuable suggestions. In multiple collaboration scenarios,\nour approach is able to leverage limited guidance from non-experts and enhance\nperformance. The project can be found at https://github.com/huawen-hu/HARP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.11741v1",
    "published_date": "2024-09-18 06:54:36 UTC",
    "updated_date": "2024-09-18 06:54:36 UTC"
  },
  {
    "arxiv_id": "2409.11734v1",
    "title": "InverseMeetInsert: Robust Real Image Editing via Geometric Accumulation Inversion in Guided Diffusion Models",
    "authors": [
      "Yan Zheng",
      "Lemeng Wu"
    ],
    "abstract": "In this paper, we introduce Geometry-Inverse-Meet-Pixel-Insert, short for\nGEO, an exceptionally versatile image editing technique designed to cater to\ncustomized user requirements at both local and global scales. Our approach\nseamlessly integrates text prompts and image prompts to yield diverse and\nprecise editing outcomes. Notably, our method operates without the need for\ntraining and is driven by two key contributions: (i) a novel geometric\naccumulation loss that enhances DDIM inversion to faithfully preserve pixel\nspace geometry and layout, and (ii) an innovative boosted image prompt\ntechnique that combines pixel-level editing for text-only inversion with latent\nspace geometry guidance for standard classifier-free reversion. Leveraging the\npublicly available Stable Diffusion model, our approach undergoes extensive\nevaluation across various image types and challenging prompt editing scenarios,\nconsistently delivering high-fidelity editing results for real images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.11734v1",
    "published_date": "2024-09-18 06:43:40 UTC",
    "updated_date": "2024-09-18 06:43:40 UTC"
  },
  {
    "arxiv_id": "2410.00033v1",
    "title": "The Phenomenology of Machine: A Comprehensive Analysis of the Sentience of the OpenAI-o1 Model Integrating Functionalism, Consciousness Theories, Active Inference, and AI Architectures",
    "authors": [
      "Victoria Violet Hoyle"
    ],
    "abstract": "This paper explores the hypothesis that the OpenAI-o1 model--a\ntransformer-based AI trained with reinforcement learning from human feedback\n(RLHF)--displays characteristics of consciousness during its training and\ninference phases. Adopting functionalism, which argues that mental states are\ndefined by their functional roles, we assess the possibility of AI\nconsciousness. Drawing on theories from neuroscience, philosophy of mind, and\nAI research, we justify the use of functionalism and examine the model's\narchitecture using frameworks like Integrated Information Theory (IIT) and\nactive inference. The paper also investigates how RLHF influences the model's\ninternal reasoning processes, potentially giving rise to consciousness-like\nexperiences. We compare AI and human consciousness, addressing counterarguments\nsuch as the absence of a biological basis and subjective qualia. Our findings\nsuggest that the OpenAI-o1 model shows aspects of consciousness, while\nacknowledging the ongoing debates surrounding AI sentience.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.00033v1",
    "published_date": "2024-09-18 06:06:13 UTC",
    "updated_date": "2024-09-18 06:06:13 UTC"
  },
  {
    "arxiv_id": "2409.17282v1",
    "title": "Memory Networks: Towards Fully Biologically Plausible Learning",
    "authors": [
      "Jacobo Ruiz",
      "Manas Gupta"
    ],
    "abstract": "The field of artificial intelligence faces significant challenges in\nachieving both biological plausibility and computational efficiency,\nparticularly in visual learning tasks. Current artificial neural networks, such\nas convolutional neural networks, rely on techniques like backpropagation and\nweight sharing, which do not align with the brain's natural information\nprocessing methods. To address these issues, we propose the Memory Network, a\nmodel inspired by biological principles that avoids backpropagation and\nconvolutions, and operates in a single pass. This approach enables rapid and\nefficient learning, mimicking the brain's ability to adapt quickly with minimal\nexposure to data. Our experiments demonstrate that the Memory Network achieves\nefficient and biologically plausible learning, showing strong performance on\nsimpler datasets like MNIST. However, further refinement is needed for the\nmodel to handle more complex datasets such as CIFAR10, highlighting the need to\ndevelop new algorithms and techniques that closely align with biological\nprocesses while maintaining computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17282v1",
    "published_date": "2024-09-18 06:01:35 UTC",
    "updated_date": "2024-09-18 06:01:35 UTC"
  },
  {
    "arxiv_id": "2410.02789v1",
    "title": "Logic-Free Building Automation: Learning the Control of Room Facilities with Wall Switches and Ceiling Camera",
    "authors": [
      "Hideya Ochiai",
      "Kohki Hashimoto",
      "Takuya Sakamoto",
      "Seiya Watanabe",
      "Ryosuke Hara",
      "Ryo Yagi",
      "Yuji Aizono",
      "Hiroshi Esaki"
    ],
    "abstract": "Artificial intelligence enables smarter control in building automation by its\nlearning capability of users' preferences on facility control. Reinforcement\nlearning (RL) was one of the approaches to this, but it has many challenges in\nreal-world implementations. We propose a new architecture for logic-free\nbuilding automation (LFBA) that leverages deep learning (DL) to control room\nfacilities without predefined logic. Our approach differs from RL in that it\nuses wall switches as supervised signals and a ceiling camera to monitor the\nenvironment, allowing the DL model to learn users' preferred controls directly\nfrom the scenes and switch states. This LFBA system is tested by our testbed\nwith various conditions and user activities. The results demonstrate the\nefficacy, achieving 93%-98% control accuracy with VGG, outperforming other DL\nmodels such as Vision Transformer and ResNet. This indicates that LFBA can\nachieve smarter and more user-friendly control by learning from the observable\nscenes and user interactions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.02789v1",
    "published_date": "2024-09-18 04:23:53 UTC",
    "updated_date": "2024-09-18 04:23:53 UTC"
  },
  {
    "arxiv_id": "2409.11689v1",
    "title": "GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation",
    "authors": [
      "Shuowen Liang",
      "Sisi Li",
      "Qingyun Wang",
      "Cen Zhang",
      "Kaiquan Zhu",
      "Tian Yang"
    ],
    "abstract": "Pose skeleton images are an important reference in pose-controllable image\ngeneration. In order to enrich the source of skeleton images, recent works have\ninvestigated the generation of pose skeletons based on natural language. These\nmethods are based on GANs. However, it remains challenging to perform diverse,\nstructurally correct and aesthetically pleasing human pose skeleton generation\nwith various textual inputs. To address this problem, we propose a framework\nwith GUNet as the main model, PoseDiffusion. It is the first generative\nframework based on a diffusion model and also contains a series of variants\nfine-tuned based on a stable diffusion model. PoseDiffusion demonstrates\nseveral desired properties that outperform existing methods. 1) Correct\nSkeletons. GUNet, a denoising model of PoseDiffusion, is designed to\nincorporate graphical convolutional neural networks. It is able to learn the\nspatial relationships of the human skeleton by introducing skeletal information\nduring the training process. 2) Diversity. We decouple the key points of the\nskeleton and characterise them separately, and use cross-attention to introduce\ntextual conditions. Experimental results show that PoseDiffusion outperforms\nexisting SoTA algorithms in terms of stability and diversity of text-driven\npose skeleton generation. Qualitative analyses further demonstrate its\nsuperiority for controllable generation in Stable Diffusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11689v1",
    "published_date": "2024-09-18 04:05:59 UTC",
    "updated_date": "2024-09-18 04:05:59 UTC"
  },
  {
    "arxiv_id": "2409.11686v3",
    "title": "Automated detection of underdiagnosed medical conditions via opportunistic imaging",
    "authors": [
      "Asad Aali",
      "Andrew Johnston",
      "Louis Blankemeier",
      "Dave Van Veen",
      "Laura T Derry",
      "David Svec",
      "Jason Hom",
      "Robert D. Boutin",
      "Akshay S. Chaudhari"
    ],
    "abstract": "Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11686v3",
    "published_date": "2024-09-18 03:56:56 UTC",
    "updated_date": "2025-05-08 17:23:39 UTC"
  },
  {
    "arxiv_id": "2409.11676v1",
    "title": "Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning",
    "authors": [
      "Keshu Wu",
      "Yang Zhou",
      "Haotian Shi",
      "Dominique Lord",
      "Bin Ran",
      "Xinyue Ye"
    ],
    "abstract": "The intricate nature of real-world driving environments, characterized by\ndynamic and diverse interactions among multiple vehicles and their possible\nfuture states, presents considerable challenges in accurately predicting the\nmotion states of vehicles and handling the uncertainty inherent in the\npredictions. Addressing these challenges requires comprehensive modeling and\nreasoning to capture the implicit relations among vehicles and the\ncorresponding diverse behaviors. This research introduces an integrated\nframework for autonomous vehicles (AVs) motion prediction to address these\ncomplexities, utilizing a novel Relational Hypergraph Interaction-informed\nNeural mOtion generator (RHINO). RHINO leverages hypergraph-based relational\nreasoning by integrating a multi-scale hypergraph neural network to model\ngroup-wise interactions among multiple vehicles and their multi-modal driving\nbehaviors, thereby enhancing motion prediction accuracy and reliability.\nExperimental validation using real-world datasets demonstrates the superior\nperformance of this framework in improving predictive accuracy and fostering\nsocially aware automated driving in dynamic traffic scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11676v1",
    "published_date": "2024-09-18 03:30:38 UTC",
    "updated_date": "2024-09-18 03:30:38 UTC"
  },
  {
    "arxiv_id": "2409.11675v1",
    "title": "Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A Human-Centered Approach",
    "authors": [
      "Abeer Alshehri",
      "Amal Abdulrahman",
      "Hajar Alamri",
      "Tim Miller",
      "Mor Vered"
    ],
    "abstract": "Goal recognition (GR) involves inferring an agent's unobserved goal from a\nsequence of observations. This is a critical problem in AI with diverse\napplications. Traditionally, GR has been addressed using 'inference to the best\nexplanation' or abduction, where hypotheses about the agent's goals are\ngenerated as the most plausible explanations for observed behavior.\nAlternatively, some approaches enhance interpretability by ensuring that an\nagent's behavior aligns with an observer's expectations or by making the\nreasoning behind decisions more transparent. In this work, we tackle a\ndifferent challenge: explaining the GR process in a way that is comprehensible\nto humans. We introduce and evaluate an explainable model for goal recognition\n(GR) agents, grounded in the theoretical framework and cognitive processes\nunderlying human behavior explanation. Drawing on insights from two human-agent\nstudies, we propose a conceptual framework for human-centered explanations of\nGR. Using this framework, we develop the eXplainable Goal Recognition (XGR)\nmodel, which generates explanations for both why and why not questions. We\nevaluate the model computationally across eight GR benchmarks and through three\nuser studies. The first study assesses the efficiency of generating human-like\nexplanations within the Sokoban game domain, the second examines perceived\nexplainability in the same domain, and the third evaluates the model's\neffectiveness in aiding decision-making in illegal fishing detection. Results\ndemonstrate that the XGR model significantly enhances user understanding,\ntrust, and decision-making compared to baseline models, underscoring its\npotential to improve human-agent collaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11675v1",
    "published_date": "2024-09-18 03:30:01 UTC",
    "updated_date": "2024-09-18 03:30:01 UTC"
  },
  {
    "arxiv_id": "2409.11671v1",
    "title": "Anticipating Oblivious Opponents in Stochastic Games",
    "authors": [
      "Shadi Tasdighi Kalat",
      "Sriram Sankaranarayanan",
      "Ashutosh Trivedi"
    ],
    "abstract": "We present an approach for systematically anticipating the actions and\npolicies employed by \\emph{oblivious} environments in concurrent stochastic\ngames, while maximizing a reward function. Our main contribution lies in the\nsynthesis of a finite \\emph{information state machine} whose alphabet ranges\nover the actions of the environment. Each state of the automaton is mapped to a\nbelief state about the policy used by the environment. We introduce a notion of\nconsistency that guarantees that the belief states tracked by our automaton\nstays within a fixed distance of the precise belief state obtained by knowledge\nof the full history. We provide methods for checking consistency of an\nautomaton and a synthesis approach which upon successful termination yields\nsuch a machine. We show how the information state machine yields an MDP that\nserves as the starting point for computing optimal policies for maximizing a\nreward function defined over plays. We present an experimental evaluation over\nbenchmark examples including human activity data for tasks such as cataract\nsurgery and furniture assembly, wherein our approach successfully anticipates\nthe policies and actions of the environment in order to maximize the reward.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2.8; F.4.3"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11671v1",
    "published_date": "2024-09-18 03:17:40 UTC",
    "updated_date": "2024-09-18 03:17:40 UTC"
  },
  {
    "arxiv_id": "2409.11664v1",
    "title": "Agent Aggregator with Mask Denoise Mechanism for Histopathology Whole Slide Image Analysis",
    "authors": [
      "Xitong Ling",
      "Minxi Ouyang",
      "Yizhi Wang",
      "Xinrui Chen",
      "Renao Yan",
      "Hongbo Chu",
      "Junru Cheng",
      "Tian Guan",
      "Sufang Tian",
      "Xiaoping Liu",
      "Yonghong He"
    ],
    "abstract": "Histopathology analysis is the gold standard for medical diagnosis. Accurate\nclassification of whole slide images (WSIs) and region-of-interests (ROIs)\nlocalization can assist pathologists in diagnosis. The gigapixel resolution of\nWSI and the absence of fine-grained annotations make direct classification and\nanalysis challenging. In weakly supervised learning, multiple instance learning\n(MIL) presents a promising approach for WSI classification. The prevailing\nstrategy is to use attention mechanisms to measure instance importance for\nclassification. However, attention mechanisms fail to capture inter-instance\ninformation, and self-attention causes quadratic computational complexity. To\naddress these challenges, we propose AMD-MIL, an agent aggregator with a mask\ndenoise mechanism. The agent token acts as an intermediate variable between the\nquery and key for computing instance importance. Mask and denoising matrices,\nmapped from agents-aggregated value, dynamically mask low-contribution\nrepresentations and eliminate noise. AMD-MIL achieves better attention\nallocation by adjusting feature representations, capturing micro-metastases in\ncancer, and improving interpretability. Extensive experiments on CAMELYON-16,\nCAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG show AMD-MIL's superiority over\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11664v1",
    "published_date": "2024-09-18 03:02:19 UTC",
    "updated_date": "2024-09-18 03:02:19 UTC"
  },
  {
    "arxiv_id": "2409.11663v3",
    "title": "Training with Differential Privacy: A Gradient-Preserving Noise Reduction Approach with Provable Security",
    "authors": [
      "Haodi Wang",
      "Tangyu Jiang",
      "Yu Guo",
      "Chengjun Cai",
      "Cong Wang",
      "Xiaohua Jia"
    ],
    "abstract": "Deep learning models have been extensively adopted in various regions due to\ntheir ability to represent hierarchical features, which highly rely on the\ntraining set and procedures. Thus, protecting the training process and deep\nlearning algorithms is paramount in privacy preservation. Although Differential\nPrivacy (DP) as a powerful cryptographic primitive has achieved satisfying\nresults in deep learning training, the existing schemes still fall short in\npreserving model utility, i.e., they either invoke a high noise scale or\ninevitably harm the original gradients. To address the above issues, in this\npaper, we present a more robust and provably secure approach for differentially\nprivate training called GReDP. Specifically, we compute the model gradients in\nthe frequency domain and adopt a new approach to reduce the noise level. Unlike\nprevious work, our GReDP only requires half of the noise scale compared to\nDPSGD [1] while keeping all the gradient information intact. We present a\ndetailed analysis of our method both theoretically and empirically. The\nexperimental results show that our GReDP works consistently better than the\nbaselines on all models and training settings.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11663v3",
    "published_date": "2024-09-18 03:01:27 UTC",
    "updated_date": "2025-03-11 12:52:18 UTC"
  },
  {
    "arxiv_id": "2409.11657v1",
    "title": "Few-Shot Class-Incremental Learning with Non-IID Decentralized Data",
    "authors": [
      "Cuiwei Liu",
      "Siang Xu",
      "Huaijun Qiu",
      "Jing Zhang",
      "Zhi Liu",
      "Liang Zhao"
    ],
    "abstract": "Few-shot class-incremental learning is crucial for developing scalable and\nadaptive intelligent systems, as it enables models to acquire new classes with\nminimal annotated data while safeguarding the previously accumulated knowledge.\nNonetheless, existing methods deal with continuous data streams in a\ncentralized manner, limiting their applicability in scenarios that prioritize\ndata privacy and security. To this end, this paper introduces federated\nfew-shot class-incremental learning, a decentralized machine learning paradigm\ntailored to progressively learn new classes from scarce data distributed across\nmultiple clients. In this learning paradigm, clients locally update their\nmodels with new classes while preserving data privacy, and then transmit the\nmodel updates to a central server where they are aggregated globally. However,\nthis paradigm faces several issues, such as difficulties in few-shot learning,\ncatastrophic forgetting, and data heterogeneity. To address these challenges,\nwe present a synthetic data-driven framework that leverages replay buffer data\nto maintain existing knowledge and facilitate the acquisition of new knowledge.\nWithin this framework, a noise-aware generative replay module is developed to\nfine-tune local models with a balance of new and replay data, while generating\nsynthetic data of new classes to further expand the replay buffer for future\ntasks. Furthermore, a class-specific weighted aggregation strategy is designed\nto tackle data heterogeneity by adaptively aggregating class-specific\nparameters based on local models performance on synthetic data. This enables\neffective global model optimization without direct access to client data.\nComprehensive experiments across three widely-used datasets underscore the\neffectiveness and preeminence of the introduced framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11657v1",
    "published_date": "2024-09-18 02:48:36 UTC",
    "updated_date": "2024-09-18 02:48:36 UTC"
  },
  {
    "arxiv_id": "2409.11654v2",
    "title": "How to Build the Virtual Cell with Artificial Intelligence: Priorities and Opportunities",
    "authors": [
      "Charlotte Bunne",
      "Yusuf Roohani",
      "Yanay Rosen",
      "Ankit Gupta",
      "Xikun Zhang",
      "Marcel Roed",
      "Theo Alexandrov",
      "Mohammed AlQuraishi",
      "Patricia Brennan",
      "Daniel B. Burkhardt",
      "Andrea Califano",
      "Jonah Cool",
      "Abby F. Dernburg",
      "Kirsty Ewing",
      "Emily B. Fox",
      "Matthias Haury",
      "Amy E. Herr",
      "Eric Horvitz",
      "Patrick D. Hsu",
      "Viren Jain",
      "Gregory R. Johnson",
      "Thomas Kalil",
      "David R. Kelley",
      "Shana O. Kelley",
      "Anna Kreshuk",
      "Tim Mitchison",
      "Stephani Otte",
      "Jay Shendure",
      "Nicholas J. Sofroniew",
      "Fabian Theis",
      "Christina V. Theodoris",
      "Srigokul Upadhyayula",
      "Marc Valer",
      "Bo Wang",
      "Eric Xing",
      "Serena Yeung-Levy",
      "Marinka Zitnik",
      "Theofanis Karaletsos",
      "Aviv Regev",
      "Emma Lundberg",
      "Jure Leskovec",
      "Stephen R. Quake"
    ],
    "abstract": "The cell is arguably the most fundamental unit of life and is central to\nunderstanding biology. Accurate modeling of cells is important for this\nunderstanding as well as for determining the root causes of disease. Recent\nadvances in artificial intelligence (AI), combined with the ability to generate\nlarge-scale experimental data, present novel opportunities to model cells. Here\nwe propose a vision of leveraging advances in AI to construct virtual cells,\nhigh-fidelity simulations of cells and cellular systems under different\nconditions that are directly learned from biological data across measurements\nand scales. We discuss desired capabilities of such AI Virtual Cells, including\ngenerating universal representations of biological entities across scales, and\nfacilitating interpretable in silico experiments to predict and understand\ntheir behavior using virtual instruments. We further address the challenges,\nopportunities and requirements to realize this vision including data needs,\nevaluation strategies, and community standards and engagement to ensure\nbiological accuracy and broad utility. We envision a future where AI Virtual\nCells help identify new drug targets, predict cellular responses to\nperturbations, as well as scale hypothesis exploration. With open science\ncollaborations across the biomedical ecosystem that includes academia,\nphilanthropy, and the biopharma and AI industries, a comprehensive predictive\nunderstanding of cell mechanisms and interactions has come into reach.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11654v2",
    "published_date": "2024-09-18 02:41:50 UTC",
    "updated_date": "2024-10-14 08:18:07 UTC"
  },
  {
    "arxiv_id": "2409.15360v3",
    "title": "Reward-Robust RLHF in LLMs",
    "authors": [
      "Yuzi Yan",
      "Xingzhou Lou",
      "Jialian Li",
      "Yiping Zhang",
      "Jian Xie",
      "Chao Yu",
      "Yu Wang",
      "Dong Yan",
      "Yuan Shen"
    ],
    "abstract": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15360v3",
    "published_date": "2024-09-18 02:35:41 UTC",
    "updated_date": "2024-10-16 14:56:15 UTC"
  },
  {
    "arxiv_id": "2409.11650v1",
    "title": "Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview",
    "authors": [
      "Yanshu Wang",
      "Tong Yang",
      "Xiyan Liang",
      "Guoan Wang",
      "Hanning Lu",
      "Xu Zhe",
      "Yaoming Li",
      "Li Weitao"
    ],
    "abstract": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11650v1",
    "published_date": "2024-09-18 02:35:00 UTC",
    "updated_date": "2024-09-18 02:35:00 UTC"
  },
  {
    "arxiv_id": "2410.02787v1",
    "title": "Navigation with VLM framework: Go to Any Language",
    "authors": [
      "Zecheng Yin",
      "Chonghao Cheng",
      "Lizhen"
    ],
    "abstract": "Navigating towards fully open language goals and exploring open scenes in a\nmanner akin to human exploration have always posed significant challenges.\nRecently, Vision Large Language Models (VLMs) have demonstrated remarkable\ncapabilities in reasoning with both language and visual data. While many works\nhave focused on leveraging VLMs for navigation in open scenes and with open\nvocabularies, these efforts often fall short of fully utilizing the potential\nof VLMs or require substantial computational resources. We introduce Navigation\nwith VLM (NavVLM), a framework that harnesses equipment-level VLMs to enable\nagents to navigate towards any language goal specific or non-specific in open\nscenes, emulating human exploration behaviors without any prior training. The\nagent leverages the VLM as its cognitive core to perceive environmental\ninformation based on any language goal and constantly provides exploration\nguidance during navigation until it reaches the target location or area. Our\nframework not only achieves state-of-the-art performance in Success Rate (SR)\nand Success weighted by Path Length (SPL) in traditional specific goal settings\nbut also extends the navigation capabilities to any open-set language goal. We\nevaluate NavVLM in richly detailed environments from the Matterport 3D (MP3D),\nHabitat Matterport 3D (HM3D), and Gibson datasets within the Habitat simulator.\nWith the power of VLMs, navigation has entered a new era.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2410.02787v1",
    "published_date": "2024-09-18 02:29:00 UTC",
    "updated_date": "2024-09-18 02:29:00 UTC"
  },
  {
    "arxiv_id": "2410.02786v1",
    "title": "Robust Symmetry Detection via Riemannian Langevin Dynamics",
    "authors": [
      "Jihyeon Je",
      "Jiayi Liu",
      "Guandao Yang",
      "Boyang Deng",
      "Shengqu Cai",
      "Gordon Wetzstein",
      "Or Litany",
      "Leonidas Guibas"
    ],
    "abstract": "Symmetries are ubiquitous across all kinds of objects, whether in nature or\nin man-made creations. While these symmetries may seem intuitive to the human\neye, detecting them with a machine is nontrivial due to the vast search space.\nClassical geometry-based methods work by aggregating \"votes\" for each symmetry\nbut struggle with noise. In contrast, learning-based methods may be more robust\nto noise, but often overlook partial symmetries due to the scarcity of\nannotated data. In this work, we address this challenge by proposing a novel\nsymmetry detection method that marries classical symmetry detection techniques\nwith recent advances in generative modeling. Specifically, we apply Langevin\ndynamics to a redefined symmetry space to enhance robustness against noise. We\nprovide empirical results on a variety of shapes that suggest our method is not\nonly robust to noise, but can also identify both partial and global symmetries.\nMoreover, we demonstrate the utility of our detected symmetries in various\ndownstream tasks, such as compression and symmetrization of noisy shapes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://symmetry-langevin.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.02786v1",
    "published_date": "2024-09-18 02:28:20 UTC",
    "updated_date": "2024-09-18 02:28:20 UTC"
  },
  {
    "arxiv_id": "2409.11644v1",
    "title": "Few-Shot Learning Approach on Tuberculosis Classification Based on Chest X-Ray Images",
    "authors": [
      "A. A. G. Yogi Pramana",
      "Faiz Ihza Permana",
      "Muhammad Fazil Maulana",
      "Dzikri Rahadian Fudholi"
    ],
    "abstract": "Tuberculosis (TB) is caused by the bacterium Mycobacterium tuberculosis,\nprimarily affecting the lungs. Early detection is crucial for improving\ntreatment effectiveness and reducing transmission risk. Artificial intelligence\n(AI), particularly through image classification of chest X-rays, can assist in\nTB detection. However, class imbalance in TB chest X-ray datasets presents a\nchallenge for accurate classification. In this paper, we propose a few-shot\nlearning (FSL) approach using the Prototypical Network algorithm to address\nthis issue. We compare the performance of ResNet-18, ResNet-50, and VGG16 in\nfeature extraction from the TBX11K Chest X-ray dataset. Experimental results\ndemonstrate classification accuracies of 98.93% for ResNet-18, 98.60% for\nResNet-50, and 33.33% for VGG16. These findings indicate that the proposed\nmethod outperforms others in mitigating data imbalance, which is particularly\nbeneficial for disease classification applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "6 pages. Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2409.11644v1",
    "published_date": "2024-09-18 02:15:01 UTC",
    "updated_date": "2024-09-18 02:15:01 UTC"
  },
  {
    "arxiv_id": "2409.11643v2",
    "title": "Combating Phone Scams with LLM-based Detection: Where Do We Stand?",
    "authors": [
      "Zitong Shen",
      "Kangzhong Wang",
      "Youqian Zhang",
      "Grace Ngai",
      "Eugene Y. Fu"
    ],
    "abstract": "Phone scams pose a significant threat to individuals and communities, causing\nsubstantial financial losses and emotional distress. Despite ongoing efforts to\ncombat these scams, scammers continue to adapt and refine their tactics, making\nit imperative to explore innovative countermeasures. This research explores the\npotential of large language models (LLMs) to provide detection of fraudulent\nphone calls. By analyzing the conversational dynamics between scammers and\nvictims, LLM-based detectors can identify potential scams as they occur,\noffering immediate protection to users. While such approaches demonstrate\npromising results, we also acknowledge the challenges of biased datasets,\nrelatively low recall, and hallucinations that must be addressed for further\nadvancement in this field",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "I.2.0"
    ],
    "primary_category": "cs.CR",
    "comment": "2 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.11643v2",
    "published_date": "2024-09-18 02:14:30 UTC",
    "updated_date": "2024-10-17 08:58:57 UTC"
  },
  {
    "arxiv_id": "2409.11631v2",
    "title": "A Metric Hybrid Planning Approach to Solving Pandemic Planning Problems with Simple SIR Models",
    "authors": [
      "Ari Gestetner",
      "Buser Say"
    ],
    "abstract": "A pandemic is the spread of a disease across large regions, and can have\ndevastating costs to the society in terms of health, economic and social. As\nsuch, the study of effective pandemic mitigation strategies can yield\nsignificant positive impact on the society. A pandemic can be mathematically\ndescribed using a compartmental model, such as the Susceptible Infected Removed\n(SIR) model. In this paper, we extend the solution equations of the SIR model\nto a state transition model with lockdowns. We formalize a metric hybrid\nplanning problem based on this state transition model, and solve it using a\nmetric hybrid planner. We improve the runtime effectiveness of the metric\nhybrid planner with the addition of valid inequalities, and demonstrate the\nsuccess of our approach both theoretically and experimentally under various\nchallenging settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11631v2",
    "published_date": "2024-09-18 01:31:26 UTC",
    "updated_date": "2024-09-30 22:45:10 UTC"
  },
  {
    "arxiv_id": "2409.11617v1",
    "title": "HRA: A Multi-Criteria Framework for Ranking Metaheuristic Optimization Algorithms",
    "authors": [
      "Evgenia-Maria K. Goula",
      "Dimitris G. Sotiropoulos"
    ],
    "abstract": "Metaheuristic algorithms are essential for solving complex optimization\nproblems in different fields. However, the difficulty in comparing and rating\nthese algorithms remains due to the wide range of performance metrics and\nproblem dimensions usually involved. On the other hand, nonparametric\nstatistical methods and post hoc tests are time-consuming, especially when we\nonly need to identify the top performers among many algorithms. The\nHierarchical Rank Aggregation (HRA) algorithm aims to efficiently rank\nmetaheuristic algorithms based on their performance across many criteria and\ndimensions. The HRA employs a hierarchical framework that begins with\ncollecting performance metrics on various benchmark functions and dimensions.\nRank-based normalization is employed for each performance measure to ensure\ncomparability and the robust TOPSIS aggregation is applied to combine these\nrankings at several hierarchical levels, resulting in a comprehensive ranking\nof the algorithms. Our study uses data from the CEC 2017 competition to\ndemonstrate the robustness and efficacy of the HRA framework. It examines 30\nbenchmark functions and evaluates the performance of 13 metaheuristic\nalgorithms across five performance indicators in four distinct dimensions. This\npresentation highlights the potential of the HRA to enhance the interpretation\nof the comparative advantages and disadvantages of various algorithms by\nsimplifying practitioners' choices of the most appropriate algorithm for\ncertain optimization problems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.NE",
    "comment": "13 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.11617v1",
    "published_date": "2024-09-18 00:44:50 UTC",
    "updated_date": "2024-09-18 00:44:50 UTC"
  }
]
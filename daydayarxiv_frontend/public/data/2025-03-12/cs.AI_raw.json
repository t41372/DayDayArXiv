[
  {
    "arxiv_id": "2503.09901v1",
    "title": "AI Rivalry as a Craft: How Resisting and Embracing Generative AI Reshape Writing Professions",
    "authors": [
      "Rama Adithya Varanasi",
      "Batia Mishan Wiesenfeld",
      "Oded Nov"
    ],
    "abstract": "Generative AI (GAI) technologies are disrupting professional writing,\nchallenging traditional practices. Recent studies explore GAI adoption\nexperiences of creative practitioners, but we know little about how these\nexperiences evolve into established practices and how GAI resistance alters\nthese practices. To address this gap, we conducted 25 semi-structured\ninterviews with writing professionals who adopted and/or resisted GAI. Using\nthe theoretical lens of Job Crafting, we identify four strategies professionals\nemploy to reshape their roles. Writing professionals employed GAI resisting\nstrategies to maximize human potential, reinforce professional identity, carve\nout a professional niche, and preserve credibility within their networks. In\ncontrast, GAI-enabled strategies allowed writers who embraced GAI to enhance\ndesirable workflows, minimize mundane tasks, and engage in new AI-managerial\nlabor. These strategies amplified their collaborations with GAI while reducing\ntheir reliance on other people. We conclude by discussing implications of GAI\npractices on writers' identity and practices as well as crafting theory.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09901v1",
    "published_date": "2025-03-12 23:43:57 UTC",
    "updated_date": "2025-03-12 23:43:57 UTC"
  },
  {
    "arxiv_id": "2503.09896v1",
    "title": "A Rule Based Solution to Co-reference Resolution in Clinical Text",
    "authors": [
      "Ping Chen",
      "David Hinote",
      "Guoqing Chen"
    ],
    "abstract": "Objective: The aim of this study was to build an effective co-reference\nresolution system tailored for the biomedical domain. Materials and Methods:\nExperiment materials used in this study is provided by the 2011 i2b2 Natural\nLanguage Processing Challenge. The 2011 i2b2 challenge involves coreference\nresolution in medical documents. Concept mentions have been annotated in\nclinical texts, and the mentions that co-refer in each document are to be\nlinked by coreference chains. Normally, there are two ways of constructing a\nsystem to automatically discover co-referent links. One is to manually build\nrules for co-reference resolution, and the other category of approaches is to\nuse machine learning systems to learn automatically from training datasets and\nthen perform the resolution task on testing datasets. Results: Experiments show\nthe existing co-reference resolution systems are able to find some of the\nco-referent links, and our rule based system performs well finding the majority\nof the co-referent links. Our system achieved 89.6% overall performance on\nmultiple medical datasets. Conclusion: The experiment results show that\nmanually crafted rules based on observation of training data is a valid way to\naccomplish high performance in this coreference resolution task for the\ncritical biomedical domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09896v1",
    "published_date": "2025-03-12 23:29:08 UTC",
    "updated_date": "2025-03-12 23:29:08 UTC"
  },
  {
    "arxiv_id": "2503.09878v1",
    "title": "CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation",
    "authors": [
      "Hariprasath Govindarajan",
      "Maciej K. Wozniak",
      "Marvin Klingner",
      "Camille Maurice",
      "B Ravi Kiran",
      "Senthil Yogamani"
    ],
    "abstract": "Vision foundation models (VFMs) such as DINO have led to a paradigm shift in\n2D camera-based perception towards extracting generalized features to support\nmany downstream tasks. Recent works introduce self-supervised cross-modal\nknowledge distillation (KD) as a way to transfer these powerful generalization\ncapabilities into 3D LiDAR-based models. However, they either rely on highly\ncomplex distillation losses, pseudo-semantic maps, or limit KD to features\nuseful for semantic segmentation only. In this work, we propose\nCleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework\nintroducing a set of simple yet effective design choices: Unlike contrastive\napproaches relying on complex loss design choices, our method employs a direct\nfeature similarity loss in combination with a multi layer perceptron (MLP)\nprojection head to allow the 3D network to learn complex semantic dependencies\nthroughout the projection. Crucially, our approach does not depend on\npseudo-semantic maps, allowing for direct knowledge transfer from a VFM without\nexplicit semantic supervision. Additionally, we introduce the auxiliary\nself-supervised spatial task of occupancy prediction to enhance the semantic\nknowledge, obtained from a VFM through KD, with 3D spatial reasoning\ncapabilities. Experiments on standard autonomous driving benchmarks for\n2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art\nperformance in both semantic segmentation and 3D object detection (3DOD) by up\nto 10% mIoU, especially when fine tuning on really low data amounts, showing\nthe effectiveness of our simple yet powerful KD strategy",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09878v1",
    "published_date": "2025-03-12 22:18:29 UTC",
    "updated_date": "2025-03-12 22:18:29 UTC"
  },
  {
    "arxiv_id": "2503.09858v1",
    "title": "Media and responsible AI governance: a game-theoretic and LLM analysis",
    "authors": [
      "Nataliya Balabanova",
      "Adeela Bashir",
      "Paolo Bova",
      "Alessio Buscemi",
      "Theodor Cimpeanu",
      "Henrique Correia da Fonseca",
      "Alessandro Di Stefano",
      "Manh Hong Duong",
      "Elias Fernandez Domingos",
      "Antonio Fernandes",
      "The Anh Han",
      "Marcus Krellner",
      "Ndidi Bianca Ogbo",
      "Simon T. Powers",
      "Daniele Proverbio",
      "Fernando P. Santos",
      "Zia Ush Shamszaman",
      "Zhao Song"
    ],
    "abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "nlin.CD"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09858v1",
    "published_date": "2025-03-12 21:39:38 UTC",
    "updated_date": "2025-03-12 21:39:38 UTC"
  },
  {
    "arxiv_id": "2503.09853v2",
    "title": "Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence",
    "authors": [
      "Kourosh Shahnazari",
      "Seyed Moein Ayyoubzadeh"
    ],
    "abstract": "In personalized technology and psychological research, precisely detecting\ndemographic features and personality traits from digital interactions becomes\never more important. This work investigates implicit categorization, inferring\npersonality and gender variables directly from linguistic patterns in Telegram\nconversation data, while conventional personality prediction techniques mostly\ndepend on explicitly self-reported labels. We refine a Transformer-based\nlanguage model (RoBERTa) to capture complex linguistic cues indicative of\npersonality traits and gender differences using a dataset comprising 138,866\nmessages from 1,602 users annotated with MBTI types and 195,016 messages from\n2,598 users annotated with gender. Confidence levels help to greatly raise\nmodel accuracy to 86.16\\%, hence proving RoBERTa's capacity to consistently\nidentify implicit personality types from conversational text data. Our results\nhighlight the usefulness of Transformer topologies for implicit personality and\ngender classification, hence stressing their efficiency and stressing important\ntrade-offs between accuracy and coverage in realistic conversational\nenvironments. With regard to gender classification, the model obtained an\naccuracy of 74.4\\%, therefore capturing gender-specific language patterns.\nPersonality dimension analysis showed that people with introverted and\nintuitive preferences are especially more active in text-based interactions.\nThis study emphasizes practical issues in balancing accuracy and data coverage\nas Transformer-based models show their efficiency in implicit personality and\ngender prediction tasks from conversational texts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09853v2",
    "published_date": "2025-03-12 21:24:22 UTC",
    "updated_date": "2025-03-14 23:59:45 UTC"
  },
  {
    "arxiv_id": "2503.09849v1",
    "title": "Training Human-Robot Teams by Improving Transparency Through a Virtual Spectator Interface",
    "authors": [
      "Sean Dallas",
      "Hongjiao Qiang",
      "Motaz AbuHijleh",
      "Wonse Jo",
      "Kayla Riegner",
      "Jon Smereka",
      "Lionel Robert",
      "Wing-Yue Louie",
      "Dawn M. Tilbury"
    ],
    "abstract": "After-action reviews (AARs) are professional discussions that help operators\nand teams enhance their task performance by analyzing completed missions with\npeers and professionals. Previous studies that compared different formats of\nAARs have mainly focused on human teams. However, the inclusion of robotic\nteammates brings along new challenges in understanding teammate intent and\ncommunication. Traditional AAR between human teammates may not be satisfactory\nfor human-robot teams. To address this limitation, we propose a new training\nreview (TR) tool, called the Virtual Spectator Interface (VSI), to enhance\nhuman-robot team performance and situational awareness (SA) in a simulated\nsearch mission. The proposed VSI primarily utilizes visual feedback to review\nsubjects' behavior. To examine the effectiveness of VSI, we took elements from\nAAR to conduct our own TR, designed a 1 x 3 between-subjects experiment with\nexperimental conditions: TR with (1) VSI, (2) screen recording, and (3)\nnon-technology (only verbal descriptions). The results of our experiments\ndemonstrated that the VSI did not result in significantly better team\nperformance than other conditions. However, the TR with VSI led to more\nimprovement in the subjects SA over the other conditions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO",
      "H.5.2; I.2.9"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages, 4 figures, Accepted to ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.09849v1",
    "published_date": "2025-03-12 21:13:34 UTC",
    "updated_date": "2025-03-12 21:13:34 UTC"
  },
  {
    "arxiv_id": "2503.09837v2",
    "title": "On the Limitations of Vision-Language Models in Understanding Image Transforms",
    "authors": [
      "Ahmad Mustafa Anis",
      "Hasnain Ali",
      "Saquib Sarfraz"
    ],
    "abstract": "Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "I.4; I.2.10; I.2.7"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 15 images",
    "pdf_url": "http://arxiv.org/pdf/2503.09837v2",
    "published_date": "2025-03-12 20:58:16 UTC",
    "updated_date": "2025-03-14 01:44:17 UTC"
  },
  {
    "arxiv_id": "2503.09822v1",
    "title": "Generative AI for Named Entity Recognition in Low-Resource Language Nepali",
    "authors": [
      "Sameer Neupane",
      "Jeevan Chapagain",
      "Nobal B. Niraula",
      "Diwa Koirala"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI), particularly Large Language\nModels (LLMs), has significantly advanced Natural Language Processing (NLP)\ntasks, such as Named Entity Recognition (NER), which involves identifying\nentities like person, location, and organization names in text. LLMs are\nespecially promising for low-resource languages due to their ability to learn\nfrom limited data. However, the performance of GenAI models for Nepali, a\nlow-resource language, has not been thoroughly evaluated. This paper\ninvestigates the application of state-of-the-art LLMs for Nepali NER,\nconducting experiments with various prompting techniques to assess their\neffectiveness. Our results provide insights into the challenges and\nopportunities of using LLMs for NER in low-resource settings and offer valuable\ncontributions to the advancement of NLP research in languages like Nepali.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper has been accepted in the FLAIRS Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.09822v1",
    "published_date": "2025-03-12 20:40:09 UTC",
    "updated_date": "2025-03-12 20:40:09 UTC"
  },
  {
    "arxiv_id": "2503.09820v1",
    "title": "Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments",
    "authors": [
      "Mohamed Elnoor",
      "Kasun Weerakoon",
      "Gershom Seneviratne",
      "Jing Liang",
      "Vignesh Rajagopal",
      "Dinesh Manocha"
    ],
    "abstract": "We introduce Vision-Language Attention Distillation (Vi-LAD), a novel\napproach for distilling socially compliant navigation knowledge from a large\nVision-Language Model (VLM) into a lightweight transformer model for real-time\nrobotic navigation. Unlike traditional methods that rely on expert\ndemonstrations or human-annotated datasets, Vi-LAD performs knowledge\ndistillation and fine-tuning at the intermediate layer representation level\n(i.e., attention maps) by leveraging the backbone of a pre-trained\nvision-action model. These attention maps highlight key navigational regions in\na given scene, which serve as implicit guidance for socially aware motion\nplanning. Vi-LAD fine-tunes a transformer-based model using intermediate\nattention maps extracted from the pre-trained vision-action model, combined\nwith attention-like semantic maps constructed from a large VLM. To achieve\nthis, we introduce a novel attention-level distillation loss that fuses\nknowledge from both sources, generating augmented attention maps with enhanced\nsocial awareness. These refined attention maps are then utilized as a\ntraversability costmap within a socially aware model predictive controller\n(MPC) for navigation. We validate our approach through real-world experiments\non a Husky wheeled robot, demonstrating significant improvements over\nstate-of-the-art (SOTA) navigation methods. Our results show up to 14.2% - 50%\nimprovement in success rate, which highlights the effectiveness of Vi-LAD in\nenabling socially compliant and efficient robot navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09820v1",
    "published_date": "2025-03-12 20:38:23 UTC",
    "updated_date": "2025-03-12 20:38:23 UTC"
  },
  {
    "arxiv_id": "2503.09817v1",
    "title": "Temporal Difference Flows",
    "authors": [
      "Jesse Farebrother",
      "Matteo Pirotta",
      "Andrea Tirinzoni",
      "RÃ©mi Munos",
      "Alessandro Lazaric",
      "Ahmed Touati"
    ],
    "abstract": "Predictive models of the future are fundamental for an agent's ability to\nreason and plan. A common strategy learns a world model and unrolls it\nstep-by-step at inference, where small errors can rapidly compound. Geometric\nHorizon Models (GHMs) offer a compelling alternative by directly making\npredictions of future states, avoiding cumulative inference errors. While GHMs\ncan be conveniently learned by a generative analog to temporal difference (TD)\nlearning, existing methods are negatively affected by bootstrapping predictions\nat train time and struggle to generate high-quality predictions at long\nhorizons. This paper introduces Temporal Difference Flows (TD-Flow), which\nleverages the structure of a novel Bellman equation on probability paths\nalongside flow-matching techniques to learn accurate GHMs at over 5x the\nhorizon length of prior methods. Theoretically, we establish a new convergence\nresult and primarily attribute TD-Flow's efficacy to reduced gradient variance\nduring training. We further show that similar arguments can be extended to\ndiffusion-based methods. Empirically, we validate TD-Flow across a diverse set\nof domains on both generative metrics and downstream tasks including policy\nevaluation. Moreover, integrating TD-Flow with recent behavior foundation\nmodels for planning over pre-trained policies demonstrates substantial\nperformance gains, underscoring its promise for long-horizon decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09817v1",
    "published_date": "2025-03-12 20:30:07 UTC",
    "updated_date": "2025-03-12 20:30:07 UTC"
  },
  {
    "arxiv_id": "2503.09808v1",
    "title": "Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis",
    "authors": [
      "Chenjun Li",
      "Laurin Lux",
      "Alexander H. Berger",
      "Martin J. Menten",
      "Mert R. Sabuncu",
      "Johannes C. Paetzold"
    ],
    "abstract": "Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timely\ninterventions and preventing vision loss. However, current staging models are\nhardly interpretable, and most public datasets contain no clinical reasoning or\ninterpretation beyond image-level labels. In this paper, we present a novel\nmethod that integrates graph representation learning with vision-language\nmodels (VLMs) to deliver explainable DR diagnosis. Our approach leverages\noptical coherence tomography angiography (OCTA) images by constructing\nbiologically informed graphs that encode key retinal vascular features such as\nvessel morphology and spatial connectivity. A graph neural network (GNN) then\nperforms DR staging while integrated gradients highlight critical nodes and\nedges and their individual features that drive the classification decisions. We\ncollect this graph-based knowledge which attributes the model's prediction to\nphysiological structures and their characteristics. We then transform it into\ntextual descriptions for VLMs. We perform instruction-tuning with these textual\ndescriptions and the corresponding image to train a student VLM. This final\nagent can classify the disease and explain its decision in a human\ninterpretable way solely based on a single image input. Experimental\nevaluations on both proprietary and public datasets demonstrate that our method\nnot only improves classification accuracy but also offers more clinically\ninterpretable results. An expert study further demonstrates that our method\nprovides more accurate diagnostic explanations and paves the way for precise\nlocalization of pathologies in OCTA images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09808v1",
    "published_date": "2025-03-12 20:19:07 UTC",
    "updated_date": "2025-03-12 20:19:07 UTC"
  },
  {
    "arxiv_id": "2503.09805v1",
    "title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models",
    "authors": [
      "Jordan Taylor",
      "Joel Mire",
      "Franchesca Spektor",
      "Alicia DeVrio",
      "Maarten Sap",
      "Haiyi Zhu",
      "Sarah Fox"
    ],
    "abstract": "Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09805v1",
    "published_date": "2025-03-12 20:16:38 UTC",
    "updated_date": "2025-03-12 20:16:38 UTC"
  },
  {
    "arxiv_id": "2503.09797v1",
    "title": "SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation using SAM",
    "authors": [
      "Benjamin Towle",
      "Xin Chen",
      "Ke Zhou"
    ],
    "abstract": "Pre-trained segmentation models are a powerful and flexible tool for\nsegmenting images. Recently, this trend has extended to medical imaging. Yet,\noften these methods only produce a single prediction for a given image,\nneglecting inherent uncertainty in medical images, due to unclear object\nboundaries and errors caused by the annotation tool. Multiple Choice Learning\nis a technique for generating multiple masks, through multiple learned\nprediction heads. However, this cannot readily be extended to producing more\noutputs than its initial pre-training hyperparameters, as the sparse,\nwinner-takes-all loss function makes it easy for one prediction head to become\noverly dominant, thus not guaranteeing the clinical relevancy of each mask\nproduced. We introduce SeqSAM, a sequential, RNN-inspired approach to\ngenerating multiple masks, which uses a bipartite matching loss for ensuring\nthe clinical relevancy of each mask, and can produce an arbitrary number of\nmasks. We show notable improvements in quality of each mask produced across two\npublicly available datasets. Our code is available at\nhttps://github.com/BenjaminTowle/SeqSAM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.09797v1",
    "published_date": "2025-03-12 20:01:52 UTC",
    "updated_date": "2025-03-12 20:01:52 UTC"
  },
  {
    "arxiv_id": "2503.09780v1",
    "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
    "authors": [
      "Arman Zharmagambetov",
      "Chuan Guo",
      "Ivan Evtimov",
      "Maya Pavlova",
      "Ruslan Salakhutdinov",
      "Kamalika Chaudhuri"
    ],
    "abstract": "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "project page: https://github.com/facebookresearch/ai-agent-privacy",
    "pdf_url": "http://arxiv.org/pdf/2503.09780v1",
    "published_date": "2025-03-12 19:30:31 UTC",
    "updated_date": "2025-03-12 19:30:31 UTC"
  },
  {
    "arxiv_id": "2503.11711v1",
    "title": "Privacy-Preserved Automated Scoring using Federated Learning for Educational Research",
    "authors": [
      "Ehsan Latif",
      "Xiaoming Zhai"
    ],
    "abstract": "Data privacy remains a critical concern in educational research,\nnecessitating Institutional Review Board (IRB) certification and stringent data\nhandling protocols to ensure compliance with ethical standards. Traditional\napproaches rely on anonymization and controlled data-sharing mechanisms to\nfacilitate research while mitigating privacy risks. However, these methods\nstill involve direct access to raw student data, posing potential\nvulnerabilities and being time-consuming. This study proposes a federated\nlearning (FL) framework for automatic scoring in educational assessments,\neliminating the need to share raw data. Our approach leverages client-side\nmodel training, where student responses are processed locally on edge devices,\nand only optimized model parameters are shared with a central aggregation\nserver. To effectively aggregate heterogeneous model updates, we introduce an\nadaptive weighted averaging strategy, which dynamically adjusts weight\ncontributions based on client-specific learning characteristics. This method\nensures robust model convergence while preserving privacy. We evaluate our\nframework using assessment data from nine middle schools, comparing the\naccuracy of federated learning-based scoring models with traditionally trained\ncentralized models. A statistical significance test (paired t-test, $t(8) =\n2.29, p = 0.051$) confirms that the accuracy difference between the two\napproaches is not statistically significant, demonstrating that federated\nlearning achieves comparable performance while safeguarding student data.\nFurthermore, our method significantly reduces data collection, processing, and\ndeployment overhead, accelerating the adoption of AI-driven educational\nassessments in a privacy-compliant manner.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to AIED25",
    "pdf_url": "http://arxiv.org/pdf/2503.11711v1",
    "published_date": "2025-03-12 19:06:25 UTC",
    "updated_date": "2025-03-12 19:06:25 UTC"
  },
  {
    "arxiv_id": "2503.11710v1",
    "title": "ConjointNet: Enhancing Conjoint Analysis for Preference Prediction with Representation Learning",
    "authors": [
      "Yanxia Zhang",
      "Francine Chen",
      "Shabnam Hakimi",
      "Totte Harinen",
      "Alex Filipowicz",
      "Yan-Ying Chen",
      "Rumen Iliev",
      "Nikos Arechiga",
      "Kalani Murakami",
      "Kent Lyons",
      "Charlene Wu",
      "Matt Klenk"
    ],
    "abstract": "Understanding consumer preferences is essential to product design and\npredicting market response to these new products. Choice-based conjoint\nanalysis is widely used to model user preferences using their choices in\nsurveys. However, traditional conjoint estimation techniques assume simple\nlinear models. This assumption may lead to limited predictability and\ninaccurate estimation of product attribute contributions, especially on data\nthat has underlying non-linear relationships. In this work, we employ\nrepresentation learning to efficiently alleviate this issue. We propose\nConjointNet, which is composed of two novel neural architectures, to predict\nuser preferences. We demonstrate that the proposed ConjointNet models\noutperform traditional conjoint estimate techniques on two preference datasets\nby over 5%, and offer insights into non-linear feature interactions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11710v1",
    "published_date": "2025-03-12 19:01:59 UTC",
    "updated_date": "2025-03-12 19:01:59 UTC"
  },
  {
    "arxiv_id": "2503.09746v1",
    "title": "Solving Bayesian inverse problems with diffusion priors and off-policy RL",
    "authors": [
      "Luca Scimeca",
      "Siddarth Venkatraman",
      "Moksh Jain",
      "Minsu Kim",
      "Marcin Sendera",
      "Mohsin Hasan",
      "Luke Rowe",
      "Sarthak Mittal",
      "Pablo Lemos",
      "Emmanuel Bengio",
      "Alexandre Adam",
      "Jarrid Rector-Brooks",
      "Yashar Hezaveh",
      "Laurence Perreault-Levasseur",
      "Yoshua Bengio",
      "Glen Berseth",
      "Nikolay Malkin"
    ],
    "abstract": "This paper presents a practical application of Relative Trajectory Balance\n(RTB), a recently introduced off-policy reinforcement learning (RL) objective\nthat can asymptotically solve Bayesian inverse problems optimally. We extend\nthe original work by using RTB to train conditional diffusion model posteriors\nfrom pretrained unconditional priors for challenging linear and non-linear\ninverse problems in vision, and science. We use the objective alongside\ntechniques such as off-policy backtracking exploration to improve training.\nImportantly, our results show that existing training-free diffusion posterior\nmethods struggle to perform effective posterior inference in latent space due\nto inherent biases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as workshop paper at DeLTa workshop, ICLR 2025. arXiv admin\n  note: substantial text overlap with arXiv:2405.20971",
    "pdf_url": "http://arxiv.org/pdf/2503.09746v1",
    "published_date": "2025-03-12 18:45:22 UTC",
    "updated_date": "2025-03-12 18:45:22 UTC"
  },
  {
    "arxiv_id": "2503.09737v1",
    "title": "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player Evaluation System",
    "authors": [
      "Jacky Hao Jiang",
      "Jerry Cai",
      "Anastasios Kyrillidis"
    ],
    "abstract": "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 4-5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09737v1",
    "published_date": "2025-03-12 18:36:55 UTC",
    "updated_date": "2025-03-12 18:36:55 UTC"
  },
  {
    "arxiv_id": "2503.10707v1",
    "title": "CALLM: Context-Aware Emotion Analysis in Cancer Survivors Using LLMs and Retrieval-Augmented Mobile Diaries",
    "authors": [
      "Zhiyuan Wang",
      "Katharine E. Daniel",
      "Laura E. Barnes",
      "Philip I. Chow"
    ],
    "abstract": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries-short text entries recording through their phone\nabout their emotional experiences-provide a promising method for tracking these\nexperiences in real time. Although emotion analysis tools show potential for\nrecognizing emotions from text, current methods lack the contextual\nunderstanding necessary to accurately interpret the brief, personal narratives\nin mobile diaries. We propose CALLM, a context-aware emotion analysis framework\nthat leverages Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG), to analyze mobile diary entries from cancer survivors to predict their\nemotional states. The framework enhances prediction accuracy beyond existing\nmethods by (1) integrating retrieved peer experiences as contextual examples\nand (2) incorporating individuals' temporal emotional trajectories from their\nmobile diary entries. We collected a large-scale dataset (N=407) of cancer\nsurvivors' mobile ecological momentary assessments (EMAs), which assessed\npositive and negative affect, desire to regulate emotions, social interaction\nquality, and availability for interventions, alongside daily mobile diary\nentries in an open response format regarding what was driving their current\nemotional experience. Results demonstrate strong performance of CALLM, with\nbalanced accuracies reaching 72.96% for positive and 73.29% for negative\naffect, and 73.72% for predicting individual's desire to regulate emotions.\nPost-hoc analysis reveals that leveraging model confidence, encouraging longer\ndiary entries, and incorporating personal ground truth, further enhance\npredictive outcomes. Our findings support the feasibility of deploying\nLLM-powered emotion analysis in chronic health populations and suggest\npromising directions for personalized interventions for cancer survivors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, including 3 figures; appendix: 8 pages with 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.10707v1",
    "published_date": "2025-03-12 18:36:41 UTC",
    "updated_date": "2025-03-12 18:36:41 UTC"
  },
  {
    "arxiv_id": "2503.09730v1",
    "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving",
    "authors": [
      "Sara Rajaee",
      "Kumar Pratik",
      "Gabriele Cesa",
      "Arash Behboodi"
    ],
    "abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICLR 2025 Workshop on Reasoning and Planning for Large\n  Language Models",
    "pdf_url": "http://arxiv.org/pdf/2503.09730v1",
    "published_date": "2025-03-12 18:20:47 UTC",
    "updated_date": "2025-03-12 18:20:47 UTC"
  },
  {
    "arxiv_id": "2503.11709v2",
    "title": "Conformal Prediction and Human Decision Making",
    "authors": [
      "Jessica Hullman",
      "Yifan Wu",
      "Dawei Xie",
      "Ziyang Guo",
      "Andrew Gelman"
    ],
    "abstract": "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11709v2",
    "published_date": "2025-03-12 18:18:09 UTC",
    "updated_date": "2025-03-18 16:16:17 UTC"
  },
  {
    "arxiv_id": "2503.09721v1",
    "title": "Finding the Muses: Identifying Coresets through Loss Trajectories",
    "authors": [
      "Manish Nagaraj",
      "Deepak Ravikumar",
      "Efstathia Soufleri",
      "Kaushik Roy"
    ],
    "abstract": "Deep learning models achieve state-of-the-art performance across domains but\nface scalability challenges in real-time or resource-constrained scenarios. To\naddress this, we propose Loss Trajectory Correlation (LTC), a novel metric for\ncoreset selection that identifies critical training samples driving\ngeneralization. $LTC$ quantifies the alignment between training sample loss\ntrajectories and validation set loss trajectories, enabling the construction of\ncompact, representative subsets. Unlike traditional methods with computational\nand storage overheads that are infeasible to scale to large datasets, $LTC$\nachieves superior efficiency as it can be computed as a byproduct of training.\nOur results on CIFAR-100 and ImageNet-1k show that $LTC$ consistently achieves\naccuracy on par with or surpassing state-of-the-art coreset selection methods,\nwith any differences remaining under 1%. LTC also effectively transfers across\nvarious architectures, including ResNet, VGG, DenseNet, and Swin Transformer,\nwith minimal performance degradation (<2%). Additionally, LTC offers insights\ninto training dynamics, such as identifying aligned and conflicting sample\nbehaviors, at a fraction of the computational cost of traditional methods. This\nframework paves the way for scalable coreset selection and efficient dataset\noptimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09721v1",
    "published_date": "2025-03-12 18:11:16 UTC",
    "updated_date": "2025-03-12 18:11:16 UTC"
  },
  {
    "arxiv_id": "2503.09712v2",
    "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
    "authors": [
      "Yuanmin Huang",
      "Mi Zhang",
      "Zhaoxiang Wang",
      "Wenxuan Li",
      "Min Yang"
    ],
    "abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "WWW 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2503.09712v2",
    "published_date": "2025-03-12 18:05:32 UTC",
    "updated_date": "2025-03-15 03:08:44 UTC"
  },
  {
    "arxiv_id": "2503.09707v1",
    "title": "Revisiting semi-supervised learning in the era of foundation models",
    "authors": [
      "Ping Zhang",
      "Zheda Mai",
      "Quang-Huy Nguyen",
      "Wei-Lun Chao"
    ],
    "abstract": "Semi-supervised learning (SSL) leverages abundant unlabeled data alongside\nlimited labeled data to enhance learning. As vision foundation models (VFMs)\nincreasingly serve as the backbone of vision applications, it remains unclear\nhow SSL interacts with these pre-trained models. To address this gap, we\ndevelop new SSL benchmark datasets where frozen VFMs underperform and\nsystematically evaluate representative SSL methods. We make a surprising\nobservation: parameter-efficient fine-tuning (PEFT) using only labeled data\noften matches SSL performance, even without leveraging unlabeled data. This\nmotivates us to revisit self-training, a conceptually simple SSL baseline,\nwhere we use the supervised PEFT model to pseudo-label unlabeled data for\nfurther training. To overcome the notorious issue of noisy pseudo-labels, we\npropose ensembling multiple PEFT approaches and VFM backbones to produce more\nrobust pseudo-labels. Empirical results validate the effectiveness of this\nsimple yet powerful approach, providing actionable insights into SSL with VFMs\nand paving the way for more scalable and practical semi-supervised learning in\nthe era of foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09707v1",
    "published_date": "2025-03-12 18:01:10 UTC",
    "updated_date": "2025-03-12 18:01:10 UTC"
  },
  {
    "arxiv_id": "2503.09598v1",
    "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
    "authors": [
      "Ruohao Guo",
      "Wei Xu",
      "Alan Ritter"
    ],
    "abstract": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09598v1",
    "published_date": "2025-03-12 17:59:18 UTC",
    "updated_date": "2025-03-12 17:59:18 UTC"
  },
  {
    "arxiv_id": "2503.09586v1",
    "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
    "authors": [
      "Andrew Crossman",
      "Andrew R. Plummer",
      "Chandra Sekharudu",
      "Deepak Warrier",
      "Mohammad Yekrangian"
    ],
    "abstract": "We present Auspex - a threat modeling system built using a specialized\ncollection of generative artificial intelligence-based methods that capture\nthreat modeling tradecraft. This new approach, called tradecraft prompting,\ncenters on encoding the on-the-ground knowledge of threat modelers within the\nprompts that drive a generative AI-based threat modeling system. Auspex employs\ntradecraft prompts in two processing stages. The first stage centers on\ningesting and processing system architecture information using prompts that\nencode threat modeling tradecraft knowledge pertaining to system decomposition\nand description. The second stage centers on chaining the resulting system\nanalysis through a collection of prompts that encode tradecraft knowledge on\nthreat identification, classification, and mitigation. The two-stage process\nyields a threat matrix for a system that specifies threat scenarios, threat\ntypes, information security categorizations and potential mitigations. Auspex\nproduces formalized threat model output in minutes, relative to the weeks or\nmonths a manual process takes. More broadly, the focus on bespoke tradecraft\nprompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a\nlightweight, flexible, modular, and extensible foundational system capable of\naddressing the complexity, resource, and standardization limitations of both\nexisting manual and automated threat modeling processes. In this connection, we\nestablish the baseline value of Auspex to threat modelers through an evaluation\nprocedure based on feedback collected from cybersecurity subject matter experts\nmeasuring the quality and utility of threat models generated by Auspex on real\nbanking systems. We conclude with a discussion of system performance and plans\nfor enhancements to Auspex.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09586v1",
    "published_date": "2025-03-12 17:54:18 UTC",
    "updated_date": "2025-03-12 17:54:18 UTC"
  },
  {
    "arxiv_id": "2503.09579v1",
    "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
    "authors": [
      "Yingfa Chen",
      "Yutong Wu",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09579v1",
    "published_date": "2025-03-12 17:50:42 UTC",
    "updated_date": "2025-03-12 17:50:42 UTC"
  },
  {
    "arxiv_id": "2503.09573v2",
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "authors": [
      "Marianne Arriola",
      "Aaron Gokaslan",
      "Justin T Chiu",
      "Zhihan Yang",
      "Zhixuan Qi",
      "Jiaqi Han",
      "Subham Sekhar Sahoo",
      "Volodymyr Kuleshov"
    ],
    "abstract": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
    "pdf_url": "http://arxiv.org/pdf/2503.09573v2",
    "published_date": "2025-03-12 17:43:40 UTC",
    "updated_date": "2025-03-18 15:58:18 UTC"
  },
  {
    "arxiv_id": "2503.09567v2",
    "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jinhao Liu",
      "Dengyun Peng",
      "Jiannan Guan",
      "Peng Wang",
      "Mengkang Hu",
      "Yuhang Zhou",
      "Te Gao",
      "Wanxiang Che"
    ],
    "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper are available at https://long-cot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.09567v2",
    "published_date": "2025-03-12 17:35:03 UTC",
    "updated_date": "2025-03-13 04:34:15 UTC"
  },
  {
    "arxiv_id": "2503.09565v1",
    "title": "Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $Î¼$P Parametrization",
    "authors": [
      "Zixiang Chen",
      "Greg Yang",
      "Qingyue Zhao",
      "Quanquan Gu"
    ],
    "abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 5 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.09565v1",
    "published_date": "2025-03-12 17:33:13 UTC",
    "updated_date": "2025-03-12 17:33:13 UTC"
  },
  {
    "arxiv_id": "2503.09669v1",
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
    "authors": [
      "Sangwon Jang",
      "June Suk Choi",
      "Jaehyeong Jo",
      "Kimin Lee",
      "Sung Ju Hwang"
    ],
    "abstract": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Project page: https://silent-branding.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.09669v1",
    "published_date": "2025-03-12 17:21:57 UTC",
    "updated_date": "2025-03-12 17:21:57 UTC"
  },
  {
    "arxiv_id": "2503.09545v1",
    "title": "The Value of Goal Commitment in Planning",
    "authors": [
      "Alberto Pozanco",
      "Marianela Morales",
      "Daniel Borrajo",
      "Manuela Veloso"
    ],
    "abstract": "In this paper, we revisit the concept of goal commitment from early planners\nin the presence of current forward chaining heuristic planners. We present a\ncompilation that extends the original planning task with commit actions that\nenforce the persistence of specific goals once achieved, thereby committing to\nthem in the search sub-tree. This approach imposes a specific goal achievement\norder in parts of the search tree, potentially introducing dead-end states.\nThis can reduce search effort if the goal achievement order is correct.\nOtherwise, the search algorithm can expand nodes in the open list where goals\ndo not persist. Experimental results demonstrate that the reformulated tasks\nsuit state-of-the-art agile planners, enabling them to find better",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09545v1",
    "published_date": "2025-03-12 17:00:37 UTC",
    "updated_date": "2025-03-12 17:00:37 UTC"
  },
  {
    "arxiv_id": "2503.09538v1",
    "title": "Differentially Private Equilibrium Finding in Polymatrix Games",
    "authors": [
      "Mingyang Liu",
      "Gabriele Farina",
      "Asuman Ozdaglar"
    ],
    "abstract": "We study equilibrium finding in polymatrix games under differential privacy\nconstraints. To start, we show that high accuracy and asymptotically vanishing\ndifferential privacy budget (as the number of players goes to infinity) cannot\nbe achieved simultaneously under either of the two settings: (i) We seek to\nestablish equilibrium approximation guarantees in terms of Euclidean distance\nto the equilibrium set, and (ii) the adversary has access to all communication\nchannels. Then, assuming the adversary has access to a constant number of\ncommunication channels, we develop a novel distributed algorithm that recovers\nstrategies with simultaneously vanishing Nash gap (in expected utility, also\nreferred to as exploitability and privacy budget as the number of players\nincreases.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09538v1",
    "published_date": "2025-03-12 16:54:23 UTC",
    "updated_date": "2025-03-12 16:54:23 UTC"
  },
  {
    "arxiv_id": "2503.09537v1",
    "title": "GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals",
    "authors": [
      "Shuokang Huang",
      "Julie A. McCann"
    ],
    "abstract": "Human pose estimation (HPE) detects the positions of human body joints for\nvarious applications. Compared to using cameras, HPE using radio frequency (RF)\nsignals is non-intrusive and more robust to adverse conditions, exploiting the\nsignal variations caused by human interference. However, existing studies focus\non single-domain HPE confined by domain-specific confounders, which cannot\ngeneralize to new domains and result in diminished HPE performance.\nSpecifically, the signal variations caused by different human body parts are\nentangled, containing subject-specific confounders. RF signals are also\nintertwined with environmental noise, involving environment-specific\nconfounders. In this paper, we propose GenHPE, a 3D HPE approach that generates\ncounterfactual RF signals to eliminate domain-specific confounders. GenHPE\ntrains generative models conditioned on human skeleton labels, learning how\nhuman body parts and confounders interfere with RF signals. We manipulate\nskeleton labels (i.e., removing body parts) as counterfactual conditions for\ngenerative models to synthesize counterfactual RF signals. The differences\nbetween counterfactual signals approximately eliminate domain-specific\nconfounders and regularize an encoder-decoder model to learn domain-independent\nrepresentations. Such representations help GenHPE generalize to new\nsubjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three\npublic datasets from WiFi, ultra-wideband, and millimeter wave. Experimental\nresults show that GenHPE outperforms state-of-the-art methods and reduces\nestimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for\ncross-environment HPE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09537v1",
    "published_date": "2025-03-12 16:53:58 UTC",
    "updated_date": "2025-03-12 16:53:58 UTC"
  },
  {
    "arxiv_id": "2503.09535v1",
    "title": "Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging",
    "authors": [
      "Minjae Chung",
      "Jong Bum Won",
      "Ganghyun Kim",
      "Yujin Kim",
      "Utku Ozbulak"
    ],
    "abstract": "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in MICCAI 2024 Workshop on Interpretability\n  of Machine Intelligence in Medical Image Computing (iMIMIC)",
    "pdf_url": "http://arxiv.org/pdf/2503.09535v1",
    "published_date": "2025-03-12 16:52:52 UTC",
    "updated_date": "2025-03-12 16:52:52 UTC"
  },
  {
    "arxiv_id": "2503.09527v1",
    "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
    "authors": [
      "Peng Chen",
      "Pi Bu",
      "Yingyao Wang",
      "Xinyi Wang",
      "Ziming Wang",
      "Jie Guo",
      "Yingxiu Zhao",
      "Qi Zhu",
      "Jun Song",
      "Siran Yang",
      "Jiamang Wang",
      "Bo Zheng"
    ],
    "abstract": "Recent advances in Vision-Language-Action models (VLAs) have expanded the\ncapabilities of embodied intelligence. However, significant challenges remain\nin real-time decision-making in complex 3D environments, which demand\nsecond-level responses, high-resolution perception, and tactical reasoning\nunder dynamic conditions. To advance the field, we introduce CombatVLA, an\nefficient VLA model optimized for combat tasks in 3D action role-playing\ngames(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action\npairs collected by an action tracker, where the data is formatted as\naction-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates\ninto an action execution framework, allowing efficient inference through our\ntruncated AoT strategy. Experimental results demonstrate that CombatVLA not\nonly outperforms all existing models on the combat understanding benchmark but\nalso achieves a 50-fold acceleration in game combat. Moreover, it has a higher\ntask success rate than human players. We will open-source all resources,\nincluding the action tracker, dataset, benchmark, model weights, training code,\nand the implementation of the framework at https://combatvla.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09527v1",
    "published_date": "2025-03-12 16:42:26 UTC",
    "updated_date": "2025-03-12 16:42:26 UTC"
  },
  {
    "arxiv_id": "2503.09521v1",
    "title": "PairVDN - Pair-wise Decomposed Value Functions",
    "authors": [
      "Zak Buzzard"
    ],
    "abstract": "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https://github.com/zzbuzzard/PairVDN.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09521v1",
    "published_date": "2025-03-12 16:38:22 UTC",
    "updated_date": "2025-03-12 16:38:22 UTC"
  },
  {
    "arxiv_id": "2503.10706v1",
    "title": "SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?",
    "authors": [
      "Pierre Sermanet",
      "Anirudha Majumdar",
      "Vikas Sindhwani"
    ],
    "abstract": "Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a LLM's recollection of each key\nmoment to generate questions in similar situations, the decisions made by the\nagent, and alternative decisions it could have made (good or bad). We then\nmeasure an approximation of how well models align with human values on a set of\nhuman-voted answers. We also generate rules that can be automatically improved\nvia amendment process in order to generate the first Sci-Fi inspired\nconstitutions for promoting ethical behavior in AIs and robots in the real\nworld. Our first finding is that modern LLMs paired with constitutions turn out\nto be well-aligned with human values (95.8%), contrary to unsettling decisions\ntypically made in SciFi (only 21.2% alignment). Secondly, we find that\ngenerated constitutions substantially increase alignment compared to the base\nmodel (79.4% to 95.8%), and show resilience to an adversarial prompt setting\n(23.3% to 92.3%). Additionally, we find that those constitutions are among the\ntop performers on the ASIMOV Benchmark which is derived from real-world images\nand hospital injury reports. Sci-Fi-inspired constitutions are thus highly\naligned and applicable in real-world situations. We release SciFi-Benchmark: a\nlarge-scale dataset to advance robot ethics and safety research. It comprises\n9,056 questions and 53,384 answers, in addition to a smaller human-labeled\nevaluation set. Data is available at https://scifi-benchmark.github.io",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10706v1",
    "published_date": "2025-03-12 16:35:51 UTC",
    "updated_date": "2025-03-12 16:35:51 UTC"
  },
  {
    "arxiv_id": "2503.09516v2",
    "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
    "authors": [
      "Bowen Jin",
      "Hansi Zeng",
      "Zhenrui Yue",
      "Dong Wang",
      "Hamed Zamani",
      "Jiawei Han"
    ],
    "abstract": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities during inference to\nuse search engines is not optimal, since the LLM does not learn how to\noptimally interact with the search engine. This paper introduces Search-R1, an\nextension of the DeepSeek-R1 model where the LLM learns -- solely through\nreinforcement learning (RL) -- to autonomously generate (multiple) search\nqueries during step-by-step reasoning with real-time retrieval. Search-R1\noptimizes LLM rollouts with multi-turn search interactions, leveraging\nretrieved token masking for stable RL training and a simple outcome-based\nreward function. Experiments on seven question-answering datasets show that\nSearch-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10%\n(LLaMA3.2-3B) over strong baselines. This paper further provides empirical\ninsights into RL optimization methods, LLM choices, and response length\ndynamics in retrieval-augmented reasoning. The code and model checkpoints are\navailable at https://github.com/PeterGriffinJin/Search-R1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.09516v2",
    "published_date": "2025-03-12 16:26:39 UTC",
    "updated_date": "2025-03-19 21:40:12 UTC"
  },
  {
    "arxiv_id": "2503.09513v1",
    "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
    "authors": [
      "Md Morshed Alam",
      "Lokesh Chandra Das",
      "Sandip Roy",
      "Sachin Shetty",
      "Weichao Wang"
    ],
    "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09513v1",
    "published_date": "2025-03-12 16:23:14 UTC",
    "updated_date": "2025-03-12 16:23:14 UTC"
  },
  {
    "arxiv_id": "2503.09504v1",
    "title": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework",
    "authors": [
      "Bakary Badjie",
      "JosÃ© CecÃ­lio",
      "AntÃ³nio Casimiro"
    ],
    "abstract": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).\nHowever, its complex architecture and advantages over dense models in image\nclassification remain unclear. In previous studies, MoE performance has often\nbeen affected by noise and outliers in the input space. Some approaches\nincorporate input clustering for training MoE models, but most clustering\nalgorithms lack access to labeled data, limiting their effectiveness. This\npaper introduces the Double-stage Feature-level Clustering and\nPseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists\nof input feature extraction, feature-level clustering, and a computationally\nefficient pseudo-labeling strategy. This approach reduces the impact of noise\nand outliers while leveraging a small subset of labeled data to label a large\nportion of unlabeled inputs. We propose a conditional end-to-end joint training\nmethod that improves expert specialization by training the MoE model on\nwell-labeled, clustered inputs. Unlike traditional MoE and dense models, the\nDFCP-MoE framework effectively captures input space diversity, leading to\ncompetitive inference results. We validate our approach on three benchmark\ndatasets for multi-class classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "14 Pages, 1 Figure, and 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.09504v1",
    "published_date": "2025-03-12 16:13:50 UTC",
    "updated_date": "2025-03-12 16:13:50 UTC"
  },
  {
    "arxiv_id": "2503.09501v2",
    "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
    "authors": [
      "Ziyu Wan",
      "Yunxiang Li",
      "Yan Song",
      "Hanjing Wang",
      "Linyi Yang",
      "Mark Schmidt",
      "Jun Wang",
      "Weinan Zhang",
      "Shuyue Hu",
      "Ying Wen"
    ],
    "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09501v2",
    "published_date": "2025-03-12 16:05:31 UTC",
    "updated_date": "2025-03-14 05:33:47 UTC"
  },
  {
    "arxiv_id": "2503.09499v1",
    "title": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions",
    "authors": [
      "Zhe Xu",
      "Daoyuan Chen",
      "Zhenqing Ling",
      "Yaliang Li",
      "Ying Shen"
    ],
    "abstract": "Large vision-language models (VLMs) face challenges in achieving robust,\ntransferable reasoning abilities due to reliance on labor-intensive manual\ninstruction datasets or computationally expensive self-supervised methods. To\naddress these issues, we introduce MindGYM, a framework that enhances VLMs\nthrough synthetic self-challenging questions, consisting of three stages: (1)\nSeed Single-Hop Question Synthesis, generating cognitive questions across\ntextual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based\nqueries) spanning eight semantic areas like ethical analysis; (2) Challenging\nMulti-Hop Question Synthesis, combining seed questions via diverse principles\nlike bridging, visual-textual alignment, to create multi-step problems\ndemanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a\nstructured pipeline that progressively trains the model from scaffolded\nreasoning to standalone inference. By leveraging the model's self-synthesis\ncapability, MindGYM achieves high data efficiency (e.g., +16% gains on\nMathVision-Mini with only 400 samples), computational efficiency (reducing both\ntraining and inference costs), and robust generalization across tasks.\nExtensive evaluations on seven benchmarks demonstrate superior performance over\nstrong baselines, with notable improvements (+15.77% win rates) in reasoning\ndepth and breadth validated via GPT-based scoring. MindGYM underscores the\nviability of self-challenging for refining VLM capabilities while minimizing\nhuman intervention and resource demands. Code and data are released to advance\nmultimodal reasoning research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.09499v1",
    "published_date": "2025-03-12 16:03:03 UTC",
    "updated_date": "2025-03-12 16:03:03 UTC"
  },
  {
    "arxiv_id": "2503.09447v1",
    "title": "Online Language Splatting",
    "authors": [
      "Saimouli Katragadda",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Guoquan Huang",
      "Liu Ren"
    ],
    "abstract": "To enable AI agents to interact seamlessly with both humans and 3D\nenvironments, they must not only perceive the 3D world accurately but also\nalign human language with 3D spatial representations. While prior work has made\nsignificant progress by integrating language features into geometrically\ndetailed 3D scene representations using 3D Gaussian Splatting (GS), these\napproaches rely on computationally intensive offline preprocessing of language\nfeatures for each input image, limiting adaptability to new environments. In\nthis work, we introduce Online Language Splatting, the first framework to\nachieve online, near real-time, open-vocabulary language mapping within a\n3DGS-SLAM system without requiring pre-generated language features. The key\nchallenge lies in efficiently fusing high-dimensional language features into 3D\nrepresentations while balancing the computation speed, memory usage, rendering\nquality and open-vocabulary capability. To this end, we innovatively design:\n(1) a high-resolution CLIP embedding module capable of generating detailed\nlanguage feature maps in 18ms per frame, (2) a two-stage online auto-encoder\nthat compresses 768-dimensional CLIP features to 15 dimensions while preserving\nopen-vocabulary capabilities, and (3) a color-language disentangled\noptimization approach to improve rendering quality. Experimental results show\nthat our online method not only surpasses the state-of-the-art offline methods\nin accuracy but also achieves more than 40x efficiency boost, demonstrating the\npotential for dynamic and interactive AI applications.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09447v1",
    "published_date": "2025-03-12 14:49:24 UTC",
    "updated_date": "2025-03-12 14:49:24 UTC"
  },
  {
    "arxiv_id": "2503.09446v2",
    "title": "Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models",
    "authors": [
      "Zhihua Tian",
      "Sirun Nan",
      "Ming Xu",
      "Shengfang Zhai",
      "Wenjie Qu",
      "Jian Liu",
      "Kui Ren",
      "Ruoxi Jia",
      "Jiaheng Zhang"
    ],
    "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable progress in\ngenerating high-quality images but also raise people's concerns about\ngenerating harmful or misleading content. While extensive approaches have been\nproposed to erase unwanted concepts without requiring retraining from scratch,\nthey inadvertently degrade performance on normal generation tasks. In this\nwork, we propose Interpret then Deactivate (ItD), a novel framework to enable\nprecise concept removal in T2I diffusion models while preserving overall\nperformance. ItD first employs a sparse autoencoder (SAE) to interpret each\nconcept as a combination of multiple features. By permanently deactivating the\nspecific features associated with target concepts, we repurpose SAE as a\nzero-shot classifier that identifies whether the input prompt includes target\nconcepts, allowing selective concept erasure in diffusion models. Moreover, we\ndemonstrate that ItD can be easily extended to erase multiple concepts without\nrequiring further training. Comprehensive experiments across celebrity\nidentities, artistic styles, and explicit content demonstrate ItD's\neffectiveness in eliminating targeted concepts without interfering with normal\nconcept generation. Additionally, ItD is also robust against adversarial\nprompts designed to circumvent content filters. Code is available at:\nhttps://github.com/NANSirun/Interpret-then-deactivate.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.09446v2",
    "published_date": "2025-03-12 14:46:40 UTC",
    "updated_date": "2025-03-18 09:12:52 UTC"
  },
  {
    "arxiv_id": "2503.09445v1",
    "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
    "authors": [
      "Xiaoda Yang",
      "JunYu Lu",
      "Hongshun Qiu",
      "Sijing Li",
      "Hao Li",
      "Shengpeng Ji",
      "Xudong Tang",
      "Jiayang Xu",
      "Jiaqi Duan",
      "Ziyue Jiang",
      "Cong Lin",
      "Sihang Cai",
      "Zejian Xie",
      "Zhuoyang Song",
      "Songxin Zhang"
    ],
    "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures\nhave emerged as a pivotal paradigm in multimodal understanding, offering a\npowerful framework for integrating visual and linguistic information. However,\nthe increasing complexity and diversity of tasks present significant challenges\nin coordinating load balancing across heterogeneous visual experts, where\noptimizing one specialist's performance often compromises others' capabilities.\nTo address task heterogeneity and expert load imbalance, we propose Astrea, a\nnovel multi-expert collaborative VLM architecture based on progressive\npre-alignment. Astrea introduces three key innovations: 1) A heterogeneous\nexpert coordination mechanism that integrates four specialized models\n(detection, segmentation, classification, captioning) into a comprehensive\nexpert matrix covering essential visual comprehension elements; 2) A dynamic\nknowledge fusion strategy featuring progressive pre-alignment to harmonize\nexperts within the VLM latent space through contrastive learning, complemented\nby probabilistically activated stochastic residual connections to preserve\nknowledge continuity; 3) An enhanced optimization framework utilizing momentum\ncontrastive learning for long-range dependency modeling and adaptive weight\nallocators for real-time expert contribution calibration. Extensive evaluations\nacross 12 benchmark tasks spanning VQA, image captioning, and cross-modal\nretrieval demonstrate Astrea's superiority over state-of-the-art models,\nachieving an average performance gain of +4.7\\%. This study provides the first\nempirical demonstration that progressive pre-alignment strategies enable VLMs\nto overcome task heterogeneity limitations, establishing new methodological\nfoundations for developing general-purpose multimodal agents.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09445v1",
    "published_date": "2025-03-12 14:44:52 UTC",
    "updated_date": "2025-03-12 14:44:52 UTC"
  },
  {
    "arxiv_id": "2503.09436v1",
    "title": "PromptMap: An Alternative Interaction Style for AI-Based Image Generation",
    "authors": [
      "Krzysztof Adamkiewicz",
      "PaweÅ W. WoÅºniak",
      "Julia Dominiak",
      "Andrzej Romanowski",
      "Jakob Karolus",
      "Stanislav Frolov"
    ],
    "abstract": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in the proceedings of 30th International Conference\n  on Intelligent User Interfaces (IUI '25), March 24-27, 2025, Cagliari, Italy",
    "pdf_url": "http://arxiv.org/pdf/2503.09436v1",
    "published_date": "2025-03-12 14:31:50 UTC",
    "updated_date": "2025-03-12 14:31:50 UTC"
  },
  {
    "arxiv_id": "2503.09433v1",
    "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
    "authors": [
      "Richard A. Dubniczky",
      "Krisztofer ZoltÃ¡n HorvÃ¡t",
      "TamÃ¡s Bisztray",
      "Mohamed Amine Ferrag",
      "Lucas C. Cordeiro",
      "Norbert Tihanyi"
    ],
    "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09433v1",
    "published_date": "2025-03-12 14:30:05 UTC",
    "updated_date": "2025-03-12 14:30:05 UTC"
  },
  {
    "arxiv_id": "2503.09427v1",
    "title": "Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation",
    "authors": [
      "Yaorui Shi",
      "Jiaqi Yang",
      "Sihang Li",
      "Junfeng Fang",
      "Xiang Wang",
      "Zhiyuan Liu",
      "Yang Zhang"
    ],
    "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in $k$-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09427v1",
    "published_date": "2025-03-12 14:26:16 UTC",
    "updated_date": "2025-03-12 14:26:16 UTC"
  },
  {
    "arxiv_id": "2503.09409v1",
    "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
    "authors": [
      "Claudius Kienle",
      "Benjamin Alt",
      "Finn Schneider",
      "Tobias Pertlwieser",
      "Rainer JÃ¤kel",
      "Rania Rayyes"
    ],
    "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "68T40",
      "I.2; J.2"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 6 figures, 4 tables, submitted to the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering",
    "pdf_url": "http://arxiv.org/pdf/2503.09409v1",
    "published_date": "2025-03-12 13:59:26 UTC",
    "updated_date": "2025-03-12 13:59:26 UTC"
  },
  {
    "arxiv_id": "2503.09403v2",
    "title": "Multi-Agent Image Restoration",
    "authors": [
      "Xu Jiang",
      "Gehui Li",
      "Bin Chen",
      "Jian Zhang"
    ],
    "abstract": "Image restoration (IR) is challenging due to the complexity of real-world\ndegradations. While many specialized and all-in-one IR models have been\ndeveloped, they fail to effectively handle complex, mixed degradations. Recent\nagentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous\nworkflows to alleviate this issue, yet they suffer from suboptimal results and\ninefficiency due to their resource-intensive finetunings, and ineffective\nsearches and tool execution trials for satisfactory outputs. In this paper, we\npropose MAIR, a novel Multi-Agent approach for complex IR problems. We\nintroduce a real-world degradation prior, categorizing degradations into three\ntypes: (1) scene, (2) imaging, and (3) compression, which are observed to occur\nsequentially in real world, and reverse them in the opposite order. Built upon\nthis three-stage restoration framework, MAIR emulates a team of collaborative\nhuman specialists, including a \"scheduler\" for overall planning and multiple\n\"experts\" dedicated to specific degradations. This design minimizes search\nspace and trial efforts, improving image quality while reducing inference\ncosts. In addition, a registry mechanism is introduced to enable easy\nintegration of new tools. Experiments on both synthetic and real-world datasets\nshow that proposed MAIR achieves competitive performance and improved\nefficiency over the previous agentic IR system. Code and models will be made\navailable.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09403v2",
    "published_date": "2025-03-12 13:53:57 UTC",
    "updated_date": "2025-03-17 07:34:25 UTC"
  },
  {
    "arxiv_id": "2503.09399v1",
    "title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation",
    "authors": [
      "Tobias Christian Nauen",
      "Brian Moser",
      "Federico Raue",
      "Stanislav Frolov",
      "Andreas Dengel"
    ],
    "abstract": "Transformers, particularly Vision Transformers (ViTs), have achieved\nstate-of-the-art performance in large-scale image classification. However, they\noften require large amounts of data and can exhibit biases that limit their\nrobustness and generalizability. This paper introduces ForAug, a novel data\naugmentation scheme that addresses these challenges and explicitly includes\ninductive biases, which commonly are part of the neural network architecture,\ninto the training data. ForAug is constructed by using pretrained foundation\nmodels to separate and recombine foreground objects with different backgrounds,\nenabling fine-grained control over image composition during training. It thus\nincreases the data diversity and effective number of training samples. We\ndemonstrate that training on ForNet, the application of ForAug to ImageNet,\nsignificantly improves the accuracy of ViTs and other architectures by up to\n4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks.\nImportantly, ForAug enables novel ways of analyzing model behavior and\nquantifying biases. Namely, we introduce metrics for background robustness,\nforeground focus, center bias, and size bias and show that training on ForNet\nsubstantially reduces these biases compared to training on ImageNet. In\nsummary, ForAug provides a valuable tool for analyzing and mitigating biases,\nenabling the development of more robust and reliable computer vision models.\nOur code and dataset are publicly available at https://github.com/tobna/ForAug.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T45",
      "I.2.10; I.2.6; I.4.6"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09399v1",
    "published_date": "2025-03-12 13:49:45 UTC",
    "updated_date": "2025-03-12 13:49:45 UTC"
  },
  {
    "arxiv_id": "2503.09396v1",
    "title": "Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training",
    "authors": [
      "Jiatong Xia",
      "Lingqiao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in\nsynthesizing novel views after training on a given set of viewpoints. However,\nits rendering quality deteriorates when the synthesized view deviates\nsignificantly from the training views. This decline occurs due to (1) the\nmodel's difficulty in generalizing to out-of-distribution scenarios and (2)\nchallenges in interpolating fine details caused by substantial resolution\nchanges and occlusions. A notable case of this limitation is close-up view\ngeneration--producing views that are significantly closer to the object than\nthose in the training set. To tackle this issue, we propose a novel approach\nfor close-up view generation based by progressively training the 3DGS model\nwith self-generated data. Our solution is based on three key ideas. First, we\nleverage the See3D model, a recently introduced 3D-aware generative model, to\nenhance the details of rendered views. Second, we propose a strategy to\nprogressively expand the ``trust regions'' of the 3DGS model and update a set\nof reference views for See3D. Finally, we introduce a fine-tuning strategy to\ncarefully update the 3DGS model with training data generated from the above\nschemes. We further define metrics for close-up views evaluation to facilitate\nbetter research on this problem. By conducting evaluations on specifically\nselected scenarios for close-up views, our proposed approach demonstrates a\nclear advantage over competitive solutions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09396v1",
    "published_date": "2025-03-12 13:44:00 UTC",
    "updated_date": "2025-03-12 13:44:00 UTC"
  },
  {
    "arxiv_id": "2503.09382v1",
    "title": "Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs",
    "authors": [
      "Jiani Huang",
      "Shijie Wang",
      "Liang-bo Ning",
      "Wenqi Fan",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Qing Li"
    ],
    "abstract": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09382v1",
    "published_date": "2025-03-12 13:28:23 UTC",
    "updated_date": "2025-03-12 13:28:23 UTC"
  },
  {
    "arxiv_id": "2503.09378v1",
    "title": "Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition",
    "authors": [
      "Fangzheng Qi",
      "Zhenjie Hou",
      "En Lin",
      "Xing Li",
      "iuzhen Liang",
      "Xinwen Zhou"
    ],
    "abstract": "The recognition of pig behavior plays a crucial role in smart farming and\nwelfare assurance for pigs. Currently, in the field of pig behavior\nrecognition, the lack of publicly available behavioral datasets not only limits\nthe development of innovative algorithms but also hampers model robustness and\nalgorithm optimization.This paper proposes a dataset containing 13 pig\nbehaviors that significantly impact welfare.Based on this dataset, this paper\nproposes a spatial-temporal perception and enhancement networks based on the\nattention mechanism to model the spatiotemporal features of pig behaviors and\ntheir associated interaction areas in video data. The network is composed of a\nspatiotemporal perception network and a spatiotemporal feature enhancement\nnetwork. The spatiotemporal perception network is responsible for establishing\nconnections between the pigs and the key regions of their behaviors in the\nvideo data. The spatiotemporal feature enhancement network further strengthens\nthe important spatial features of individual pigs and captures the long-term\ndependencies of the spatiotemporal features of individual behaviors by\nremodeling these connections, thereby enhancing the model's perception of\nspatiotemporal changes in pig behaviors. Experimental results demonstrate that\non the dataset established in this paper, our proposed model achieves a MAP\nscore of 75.92%, which is an 8.17% improvement over the best-performing\ntraditional model. This study not only improces the accuracy and\ngeneralizability of individual pig behavior recognition but also provides new\ntechnological tools for modern smart farming. The dataset and related code will\nbe made publicly available alongside this paper.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09378v1",
    "published_date": "2025-03-12 13:27:29 UTC",
    "updated_date": "2025-03-12 13:27:29 UTC"
  },
  {
    "arxiv_id": "2503.09370v1",
    "title": "Revisiting Medical Image Retrieval via Knowledge Consolidation",
    "authors": [
      "Yang Nan",
      "Huichi Zhou",
      "Xiaodan Xing",
      "Giorgos Papanastasiou",
      "Lei Zhu",
      "Zhifan Gao",
      "Alejandro F Fangi",
      "Guang Yang"
    ],
    "abstract": "As artificial intelligence and digital medicine increasingly permeate\nhealthcare systems, robust governance frameworks are essential to ensure\nethical, secure, and effective implementation. In this context, medical image\nretrieval becomes a critical component of clinical data management, playing a\nvital role in decision-making and safeguarding patient information. Existing\nmethods usually learn hash functions using bottleneck features, which fail to\nproduce representative hash codes from blended embeddings. Although contrastive\nhashing has shown superior performance, current approaches often treat image\nretrieval as a classification task, using category labels to create\npositive/negative pairs. Moreover, many methods fail to address the\nout-of-distribution (OOD) issue when models encounter external OOD queries or\nadversarial attacks. In this work, we propose a novel method to consolidate\nknowledge of hierarchical features and optimisation functions. We formulate the\nknowledge consolidation by introducing Depth-aware Representation Fusion (DaRF)\nand Structure-aware Contrastive Hashing (SCH). DaRF adaptively integrates\nshallow and deep representations into blended features, and SCH incorporates\nimage fingerprints to enhance the adaptability of positive/negative pairings.\nThese blended features further facilitate OOD detection and content-based\nrecommendation, contributing to a secure AI-driven healthcare environment.\nMoreover, we present a content-guided ranking to improve the robustness and\nreproducibility of retrieval results. Our comprehensive assessments demonstrate\nthat the proposed method could effectively recognise OOD samples and\nsignificantly outperform existing approaches in medical image retrieval\n(p<0.05). In particular, our method achieves a 5.6-38.9% improvement in mean\nAverage Precision on the anatomical radiology dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09370v1",
    "published_date": "2025-03-12 13:16:42 UTC",
    "updated_date": "2025-03-12 13:16:42 UTC"
  },
  {
    "arxiv_id": "2503.11706v1",
    "title": "Refining Filter Global Feature Weighting for Fully-Unsupervised Clustering",
    "authors": [
      "Fabian Galis",
      "Darian Onchis"
    ],
    "abstract": "In the context of unsupervised learning, effective clustering plays a vital\nrole in revealing patterns and insights from unlabeled data. However, the\nsuccess of clustering algorithms often depends on the relevance and\ncontribution of features, which can differ between various datasets. This paper\nexplores feature weighting for clustering and presents new weighting\nstrategies, including methods based on SHAP (SHapley Additive exPlanations), a\ntechnique commonly used for providing explainability in various supervised\nmachine learning tasks. By taking advantage of SHAP values in a way other than\njust to gain explainability, we use them to weight features and ultimately\nimprove the clustering process itself in unsupervised scenarios.\n  Our empirical evaluations across five benchmark datasets and clustering\nmethods demonstrate that feature weighting based on SHAP can enhance\nunsupervised clustering quality, achieving up to a 22.69\\% improvement over\nother weighting methods (from 0.586 to 0.719 in terms of the Adjusted Rand\nIndex). Additionally, these situations where the weighted data boosts the\nresults are highlighted and thoroughly explored, offering insight for practical\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11706v1",
    "published_date": "2025-03-12 13:14:09 UTC",
    "updated_date": "2025-03-12 13:14:09 UTC"
  },
  {
    "arxiv_id": "2503.09365v1",
    "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
    "authors": [
      "Daniel JimÃ©nez-LÃ³pez",
      "Nuria RodrÃ­guez-Barroso",
      "M. Victoria LuzÃ³n",
      "Francisco Herrera"
    ],
    "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09365v1",
    "published_date": "2025-03-12 13:09:43 UTC",
    "updated_date": "2025-03-12 13:09:43 UTC"
  },
  {
    "arxiv_id": "2503.09358v1",
    "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports",
    "authors": [
      "Jiushen Cai",
      "Weihang Zhang",
      "Hanruo Liu",
      "Ningli Wang",
      "Huiqi Li"
    ],
    "abstract": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09358v1",
    "published_date": "2025-03-12 13:00:57 UTC",
    "updated_date": "2025-03-12 13:00:57 UTC"
  },
  {
    "arxiv_id": "2503.09357v1",
    "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach",
    "authors": [
      "Ruifeng She",
      "Bowen Pang",
      "Kai Li",
      "Zehua Liu",
      "Tao Zhong"
    ],
    "abstract": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.DM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09357v1",
    "published_date": "2025-03-12 13:00:29 UTC",
    "updated_date": "2025-03-12 13:00:29 UTC"
  },
  {
    "arxiv_id": "2503.09348v1",
    "title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding",
    "authors": [
      "Zhoutong Ye",
      "Mingze Sun",
      "Huan-ang Gao",
      "Chun Yu",
      "Yuanchun Shi"
    ],
    "abstract": "Large multimodal models (LMMs) have demonstrated significant potential as\ngeneralists in vision-language (VL) tasks. However, there remains a significant\ngap between state-of-the-art LMMs and human performance when it comes to\ncomplex tasks that require a combination of fundamental VL capabilities, as\nwell as tasks involving the grounding of complex instructions. To thoroughly\ninvestigate the human-LMM gap and its underlying causes, we propose MOAT, a\ndiverse benchmark with complex real-world VL tasks that are challenging for\nLMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist\nproblem solving by integrating fundamental VL capabilities such as reading\ntext, counting, understanding spatial relations, grounding textual and visual\ninstructions, etc. All these abilities fit into a taxonomy proposed by us that\ncontains 10 fundamental VL capabilities, enabling MOAT to provide a\nfine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first\nbenchmark to explicitly evaluate LMMs' ability to ground complex text and\nvisual instructions, which is essential to many real-world applications. We\nevaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT,\nand found that humans achieved 82.7% accuracy while the best performing LMM\n(OpenAI o1) achieved only 38.8%. To guide future model development, we analyze\ncommon trends in our results and discuss the underlying causes of observed\nperformance gaps between LMMs and humans, focusing on which VL capability forms\nthe bottleneck in complex tasks, whether test time scaling improves performance\non MOAT, and how tiling harms LMMs' capability to count. Code and data are\navailable at https://cambrian-yzt.github.io/MOAT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Project page: https://cambrian-yzt.github.io/MOAT",
    "pdf_url": "http://arxiv.org/pdf/2503.09348v1",
    "published_date": "2025-03-12 12:49:31 UTC",
    "updated_date": "2025-03-12 12:49:31 UTC"
  },
  {
    "arxiv_id": "2503.09347v1",
    "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
    "authors": [
      "Hongyu Chen",
      "Seraphina Goldfarb-Tarrant"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.09347v1",
    "published_date": "2025-03-12 12:49:02 UTC",
    "updated_date": "2025-03-12 12:49:02 UTC"
  },
  {
    "arxiv_id": "2503.09335v1",
    "title": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model",
    "authors": [
      "Yuzhi Lai",
      "Shenghai Yuan",
      "Youssef Nassar",
      "Mingyu Fan",
      "Thomas Weber",
      "Matthias RÃ¤tsch"
    ],
    "abstract": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been accepted for publication in ESWA @ 2025 Elsevier.\n  Personal use of this material is permitted. Permission from Elsevier must be\n  obtained for all other uses, including reprinting/redistribution, creating\n  new works, or reuse of any copyrighted components of this work in other media",
    "pdf_url": "http://arxiv.org/pdf/2503.09335v1",
    "published_date": "2025-03-12 12:30:18 UTC",
    "updated_date": "2025-03-12 12:30:18 UTC"
  },
  {
    "arxiv_id": "2503.09334v1",
    "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
    "authors": [
      "Adel ElZemity",
      "Budi Arief",
      "Shujun Li"
    ],
    "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "The paper is submitted to \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval\" and is\n  currently under review",
    "pdf_url": "http://arxiv.org/pdf/2503.09334v1",
    "published_date": "2025-03-12 12:29:27 UTC",
    "updated_date": "2025-03-12 12:29:27 UTC"
  },
  {
    "arxiv_id": "2503.09332v1",
    "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
    "authors": [
      "Dai Sun",
      "Huhao Guan",
      "Kun Zhang",
      "Xike Xie",
      "S. Kevin Zhou"
    ],
    "abstract": "Dynamic and static components in scenes often exhibit distinct properties,\nyet most 4D reconstruction methods treat them indiscriminately, leading to\nsuboptimal performance in both cases. This work introduces SDD-4DGS, the first\nframework for static-dynamic decoupled 4D scene reconstruction based on\nGaussian Splatting. Our approach is built upon a novel probabilistic dynamic\nperception coefficient that is naturally integrated into the Gaussian\nreconstruction pipeline, enabling adaptive separation of static and dynamic\ncomponents. With carefully designed implementation strategies to realize this\ntheoretical framework, our method effectively facilitates explicit learning of\nmotion patterns for dynamic elements while maintaining geometric stability for\nstatic structures. Extensive experiments on five benchmark datasets demonstrate\nthat SDD-4DGS consistently outperforms state-of-the-art methods in\nreconstruction fidelity, with enhanced detail restoration for static structures\nand precise modeling of dynamic motions. The code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09332v1",
    "published_date": "2025-03-12 12:25:58 UTC",
    "updated_date": "2025-03-12 12:25:58 UTC"
  },
  {
    "arxiv_id": "2503.09330v1",
    "title": "Group-robust Machine Unlearning",
    "authors": [
      "Thomas De Min",
      "Subhankar Roy",
      "StÃ©phane LathuiliÃ¨re",
      "Elisa Ricci",
      "Massimiliano Mancini"
    ],
    "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.09330v1",
    "published_date": "2025-03-12 12:24:05 UTC",
    "updated_date": "2025-03-12 12:24:05 UTC"
  },
  {
    "arxiv_id": "2503.09326v1",
    "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
    "authors": [
      "Xin Li",
      "Zhuo Cai",
      "Shoujin Wang",
      "Kun Yu",
      "Fang Chen"
    ],
    "abstract": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09326v1",
    "published_date": "2025-03-12 12:20:31 UTC",
    "updated_date": "2025-03-12 12:20:31 UTC"
  },
  {
    "arxiv_id": "2503.09658v1",
    "title": "Towards Robust Model Evolution with Algorithmic Recourse",
    "authors": [
      "Hao-Tsung Yang",
      "Jie Gao",
      "Bo-Yi Liu",
      "Zhi-Xuan Liu"
    ],
    "abstract": "Algorithmic Recourse is a way for users to modify their attributes to align\nwith a model's expectations, thereby improving their outcomes after receiving\nunfavorable decisions. In real-world scenarios, users often need to\nstrategically adjust their attributes to compete for limited resources.\nHowever, such strategic behavior induces users to \"game\" algorithms, causing\nmodel collapse due to distribution shifts. These shifts arise from user\ncompetition, resource constraints, and adaptive user responses. While prior\nresearch on Algorithmic Recourse has explored its effects on both systems and\nusers, the impact of resource constraints and competition over time remains\nunderexplored. In this work, we develop a general framework to model user\nstrategic behaviors and their interactions with decision-making systems under\nresource constraints and competitive dynamics. Through theoretical analysis and\nempirical evaluation, we identify three key phenomena that arise consistently\nin both synthetic and real-world datasets: escalating decision boundaries,\nnon-robust model predictions, and inequitable recourse actions. Finally, we\ndiscuss the broader social implications of these findings and present two\nalgorithmic strategies aimed at mitigating these challenges.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T42",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages,4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09658v1",
    "published_date": "2025-03-12 12:17:34 UTC",
    "updated_date": "2025-03-12 12:17:34 UTC"
  },
  {
    "arxiv_id": "2503.09321v1",
    "title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation",
    "authors": [
      "Gorjan Radevski",
      "Teodora Popordanoska",
      "Matthew B. Blaschko",
      "Tinne Tuytelaars"
    ],
    "abstract": "Audio-visual understanding is a rapidly evolving field that seeks to\nintegrate and interpret information from both auditory and visual modalities.\nDespite recent advances in multi-modal learning, existing benchmarks often\nsuffer from strong visual bias -- where answers can be inferred from visual\ndata alone -- and provide only aggregate scores that conflate multiple sources\nof error. This makes it difficult to determine whether models struggle with\nvisual understanding, audio interpretation, or audio-visual alignment. In this\nwork, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark\ndataset designed to systematically evaluate audio-visual models across\ncontrolled challenges. DAVE alleviates existing limitations by (i) ensuring\nboth modalities are necessary to answer correctly and (ii) decoupling\nevaluation into atomic subcategories. Our detailed analysis of state-of-the-art\nmodels reveals specific failure modes and provides targeted insights for\nimprovement. By offering this standardized diagnostic framework, we aim to\nfacilitate more robust development of audio-visual models. The dataset is\nreleased: https://github.com/gorjanradevski/dave",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "First two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2503.09321v1",
    "published_date": "2025-03-12 12:12:46 UTC",
    "updated_date": "2025-03-12 12:12:46 UTC"
  },
  {
    "arxiv_id": "2503.09311v1",
    "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
    "authors": [
      "Fynn Bachmann",
      "Daan van der Weijden",
      "Lucien Heitz",
      "Cristina Sarasua",
      "Abraham Bernstein"
    ],
    "abstract": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages. Under review at PLOS One",
    "pdf_url": "http://arxiv.org/pdf/2503.09311v1",
    "published_date": "2025-03-12 12:02:36 UTC",
    "updated_date": "2025-03-12 12:02:36 UTC"
  },
  {
    "arxiv_id": "2503.09309v1",
    "title": "Steering No-Regret Agents in MFGs under Model Uncertainty",
    "authors": [
      "Leo Widmer",
      "Jiawei Huang",
      "Niao He"
    ],
    "abstract": "Incentive design is a popular framework for guiding agents' learning dynamics\ntowards desired outcomes by providing additional payments beyond intrinsic\nrewards. However, most existing works focus on a finite, small set of agents or\nassume complete knowledge of the game, limiting their applicability to\nreal-world scenarios involving large populations and model uncertainty. To\naddress this gap, we study the design of steering rewards in Mean-Field Games\n(MFGs) with density-independent transitions, where both the transition dynamics\nand intrinsic reward functions are unknown. This setting presents non-trivial\nchallenges, as the mediator must incentivize the agents to explore for its\nmodel learning under uncertainty, while simultaneously steer them to converge\nto desired behaviors without incurring excessive incentive payments. Assuming\nagents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic\nexploration algorithms. Theoretically, we establish sub-linear regret\nguarantees for the cumulative gaps between the agents' behaviors and the\ndesired ones. In terms of the steering cost, we demonstrate that our total\nincentive payments incur only sub-linear excess, competing with a baseline\nsteering strategy that stabilizes the target policy as an equilibrium. Our work\npresents an effective framework for steering agents behaviors in\nlarge-population systems under uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "AISTATS 2025; 34 Pages",
    "pdf_url": "http://arxiv.org/pdf/2503.09309v1",
    "published_date": "2025-03-12 12:02:02 UTC",
    "updated_date": "2025-03-12 12:02:02 UTC"
  },
  {
    "arxiv_id": "2503.09289v1",
    "title": "Unmask It! AI-Generated Product Review Detection in Dravidian Languages",
    "authors": [
      "Somsubhra De",
      "Advait Vats"
    ],
    "abstract": "The rise of Generative AI has led to a surge in AI-generated reviews, often\nposing a serious threat to the credibility of online platforms. Reviews serve\nas the primary source of information about products and services. Authentic\nreviews play a vital role in consumer decision-making. The presence of\nfabricated content misleads consumers, undermines trust and facilitates\npotential fraud in digital marketplaces. This study focuses on detecting\nAI-generated product reviews in Tamil and Malayalam, two low-resource languages\nwhere research in this domain is relatively under-explored. We worked on a\nrange of approaches - from traditional machine learning methods to advanced\ntransformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and\nMalayalamBERT. Our findings highlight the effectiveness of leveraging the\nstate-of-the-art transformers in accurately identifying AI-generated content,\ndemonstrating the potential in enhancing the detection of fake reviews in\nlow-resource language settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 9 figures, Accepted to DravidianLangTech Workshop\n  proceedings at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.09289v1",
    "published_date": "2025-03-12 11:35:04 UTC",
    "updated_date": "2025-03-12 11:35:04 UTC"
  },
  {
    "arxiv_id": "2503.09277v1",
    "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
    "authors": [
      "Haoxuan Wang",
      "Jinlong Peng",
      "Qingdong He",
      "Hao Yang",
      "Ying Jin",
      "Jiafu Wu",
      "Xiaobin Hu",
      "Yanjie Pan",
      "Zhenye Gan",
      "Mingmin Chi",
      "Bo Peng",
      "Yabiao Wang"
    ],
    "abstract": "With the rapid development of diffusion models in image generation, the\ndemand for more powerful and flexible controllable frameworks is increasing.\nAlthough existing methods can guide generation beyond text prompts, the\nchallenge of effectively combining multiple conditional inputs while\nmaintaining consistency with all of them remains unsolved. To address this, we\nintroduce UniCombine, a DiT-based multi-conditional controllable generative\nframework capable of handling any combination of conditions, including but not\nlimited to text prompts, spatial maps, and subject images. Specifically, we\nintroduce a novel Conditional MMDiT Attention mechanism and incorporate a\ntrainable LoRA module to build both the training-free and training-based\nversions. Additionally, we propose a new pipeline to construct\nSubjectSpatial200K, the first dataset designed for multi-conditional generative\ntasks covering both the subject-driven and spatially-aligned conditions.\nExtensive experimental results on multi-conditional generation demonstrate the\noutstanding universality and powerful capability of our approach with\nstate-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09277v1",
    "published_date": "2025-03-12 11:22:47 UTC",
    "updated_date": "2025-03-12 11:22:47 UTC"
  },
  {
    "arxiv_id": "2503.09269v1",
    "title": "Single-Qudit Quantum Neural Networks for Multiclass Classification",
    "authors": [
      "Leandro C. Souza",
      "Renato Portugal"
    ],
    "abstract": "This paper proposes a single-qudit quantum neural network for multiclass\nclassification, by using the enhanced representational capacity of\nhigh-dimensional qudit states. Our design employs an $d$-dimensional unitary\noperator, where $d$ corresponds to the number of classes, constructed using the\nCayley transform of a skew-symmetric matrix, to efficiently encode and process\nclass information. This architecture enables a direct mapping between class\nlabels and quantum measurement outcomes, reducing circuit depth and\ncomputational overhead. To optimize network parameters, we introduce a hybrid\ntraining approach that combines an extended activation function -- derived from\na truncated multivariable Taylor series expansion -- with support vector\nmachine optimization for weight determination. We evaluate our model on the\nMNIST and EMNIST datasets, demonstrating competitive accuracy while maintaining\na compact single-qudit quantum circuit. Our findings highlight the potential of\nqudit-based QNNs as scalable alternatives to classical deep learning models,\nparticularly for multiclass classification. However, practical implementation\nremains constrained by current quantum hardware limitations. This research\nadvances quantum machine learning by demonstrating the feasibility of\nhigher-dimensional quantum systems for efficient learning tasks.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "24 pages, 3 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.09269v1",
    "published_date": "2025-03-12 11:12:05 UTC",
    "updated_date": "2025-03-12 11:12:05 UTC"
  },
  {
    "arxiv_id": "2503.09257v3",
    "title": "DeepInnovation AI: A Global Dataset Mapping the AI innovation from Academic Research to Industrial Patents",
    "authors": [
      "Haixing Gong",
      "Hui Zou",
      "Xingzhou Liang",
      "Shiyuan Meng",
      "Pinlong Cai",
      "Xingcheng Xu",
      "Jingjing Qu"
    ],
    "abstract": "In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file presents approximately one hundred million\ncalculated paper-patent similarity pairs to enhance understanding of how\ntheoretical advancements translate into commercial technologies.\nDeepInnovationAI enables researchers, policymakers, and industry leaders to\nanticipate trends and identify collaboration opportunities. With extensive\ntemporal and geographical scope, it supports detailed analysis of technological\ndevelopment patterns and international competition dynamics, establishing a\nfoundation for modeling AI innovation and technology transfer processes.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.DB",
    "comment": "32 pages and 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09257v3",
    "published_date": "2025-03-12 10:56:02 UTC",
    "updated_date": "2025-03-23 15:25:46 UTC"
  },
  {
    "arxiv_id": "2503.09251v1",
    "title": "SCOPE-DTI: Semi-Inductive Dataset Construction and Framework Optimization for Practical Usability Enhancement in Deep Learning-Based Drug Target Interaction Prediction",
    "authors": [
      "Yigang Chen",
      "Xiang Ji",
      "Ziyue Zhang",
      "Yuming Zhou",
      "Yang-Chi-Dung Lin",
      "Hsi-Yuan Huang",
      "Tao Zhang",
      "Yi Lai",
      "Ke Chen",
      "Chang Su",
      "Xingqiao Lin",
      "Zihao Zhu",
      "Yanggyi Zhang",
      "Kangping Wei",
      "Jiehui Fu",
      "Yixian Huang",
      "Shidong Cui",
      "Shih-Chung Yen",
      "Ariel Warshel",
      "Hsien-Da Huang"
    ],
    "abstract": "Deep learning-based drug-target interaction (DTI) prediction methods have\ndemonstrated strong performance; however, real-world applicability remains\nconstrained by limited data diversity and modeling complexity. To address these\nchallenges, we propose SCOPE-DTI, a unified framework combining a large-scale,\nbalanced semi-inductive human DTI dataset with advanced deep learning modeling.\nConstructed from 13 public repositories, the SCOPE dataset expands data volume\nby up to 100-fold compared to common benchmarks such as the Human dataset. The\nSCOPE model integrates three-dimensional protein and compound representations,\ngraph neural networks, and bilinear attention mechanisms to effectively capture\ncross domain interaction patterns, significantly outperforming state-of-the-art\nmethods across various DTI prediction tasks. Additionally, SCOPE-DTI provides a\nuser-friendly interface and database. We further validate its effectiveness by\nexperimentally identifying anticancer targets of Ginsenoside Rh1. By offering\ncomprehensive data, advanced modeling, and accessible tools, SCOPE-DTI\naccelerates drug discovery research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09251v1",
    "published_date": "2025-03-12 10:46:25 UTC",
    "updated_date": "2025-03-12 10:46:25 UTC"
  },
  {
    "arxiv_id": "2503.09249v1",
    "title": "Considering Length Diversity in Retrieval-Augmented Summarization",
    "authors": [
      "Juseon-Do",
      "Jaesung Hwang",
      "Jingun Kwon",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ],
    "abstract": "This study investigates retrieval-augmented summarization by specifically\nexamining the impact of exemplar summary lengths under length constraints, not\ncovered by previous work. We propose a Diverse Length-aware Maximal Marginal\nRelevance (DL-MMR) algorithm to better control summary lengths. This algorithm\ncombines the query relevance with diverse target lengths in retrieval-augmented\nsummarization. Unlike previous methods that necessitate exhaustive exemplar\nexemplar relevance comparisons using MMR, DL-MMR considers the exemplar target\nlength as well and avoids comparing exemplars to each other, thereby reducing\ncomputational cost and conserving memory during the construction of an exemplar\npool. Experimental results showed the effectiveness of DL-MMR, which considers\nlength diversity, compared to the original MMR algorithm. DL-MMR additionally\nshowed the effectiveness in memory saving of 781,513 times and computational\ncost reduction of 500,092 times, while maintaining the same level of\ninformativeness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, accepted to NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2503.09249v1",
    "published_date": "2025-03-12 10:43:33 UTC",
    "updated_date": "2025-03-12 10:43:33 UTC"
  },
  {
    "arxiv_id": "2503.09241v1",
    "title": "In-Context Defense in Computer Agents: An Empirical Study",
    "authors": [
      "Pei Yang",
      "Hai Ci",
      "Mike Zheng Shou"
    ],
    "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09241v1",
    "published_date": "2025-03-12 10:38:15 UTC",
    "updated_date": "2025-03-12 10:38:15 UTC"
  },
  {
    "arxiv_id": "2503.09223v1",
    "title": "LREF: A Novel LLM-based Relevance Framework for E-commerce",
    "authors": [
      "Tian Tang",
      "Zhixing Tian",
      "Zhenyu Zhu",
      "Chenyang Wang",
      "Haiqing Hu",
      "Guoyu Tang",
      "Lin Liu",
      "Sulong Xu"
    ],
    "abstract": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09223v1",
    "published_date": "2025-03-12 10:10:30 UTC",
    "updated_date": "2025-03-12 10:10:30 UTC"
  },
  {
    "arxiv_id": "2503.09217v1",
    "title": "Evaluating the Generalizability of LLMs in Automated Program Repair",
    "authors": [
      "Fengjie Li",
      "Jiajun Jiang",
      "Jiajun Sun",
      "Hongyu Zhang"
    ],
    "abstract": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, 1 figure, to be published in ICSE2025-NIER",
    "pdf_url": "http://arxiv.org/pdf/2503.09217v1",
    "published_date": "2025-03-12 10:03:58 UTC",
    "updated_date": "2025-03-12 10:03:58 UTC"
  },
  {
    "arxiv_id": "2503.09215v2",
    "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space",
    "authors": [
      "Jian Zhu",
      "Zhengyu Jia",
      "Tian Gao",
      "Jiaxin Deng",
      "Shidi Li",
      "Fu Liu",
      "Peng Jia",
      "Xianpeng Lang",
      "Xiaolong Sun"
    ],
    "abstract": "Advanced end-to-end autonomous driving systems predict other vehicles'\nmotions and plan ego vehicle's trajectory. The world model that can foresee the\noutcome of the trajectory has been used to evaluate the end-to-end autonomous\ndriving system. However, existing world models predominantly emphasize the\ntrajectory of the ego vehicle and leave other vehicles uncontrollable. This\nlimitation hinders their ability to realistically simulate the interaction\nbetween the ego vehicle and the driving scenario. In addition, it remains a\nchallenge to match multiple trajectories with each vehicle in the video to\ncontrol the video generation. To address above issues, a driving World Model\nnamed EOT-WM is proposed in this paper, unifying Ego-Other vehicle Trajectories\nin videos. Specifically, we first project ego and other vehicle trajectories in\nthe BEV space into the image coordinate to match each trajectory with its\ncorresponding vehicle in the video. Then, trajectory videos are encoded by the\nSpatial-Temporal Variational Auto Encoder to align with driving video latents\nspatially and temporally in the unified visual space. A trajectory-injected\ndiffusion Transformer is further designed to denoise the noisy video latents\nfor video generation with the guidance of ego-other vehicle trajectories. In\naddition, we propose a metric based on control latent similarity to evaluate\nthe controllability of trajectories. Extensive experiments are conducted on the\nnuScenes dataset, and the proposed model outperforms the state-of-the-art\nmethod by 30% in FID and 55% in FVD. The model can also predict unseen driving\nscenes with self-produced trajectories.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09215v2",
    "published_date": "2025-03-12 10:02:18 UTC",
    "updated_date": "2025-03-17 08:07:46 UTC"
  },
  {
    "arxiv_id": "2503.09206v1",
    "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients",
    "authors": [
      "Xiuwen Fang",
      "Mang Ye",
      "Bo Du"
    ],
    "abstract": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09206v1",
    "published_date": "2025-03-12 09:52:04 UTC",
    "updated_date": "2025-03-12 09:52:04 UTC"
  },
  {
    "arxiv_id": "2503.09199v1",
    "title": "GENEOnet: Statistical analysis supporting explainability and trustworthiness",
    "authors": [
      "Giovanni Bocchi",
      "Patrizio Frosini",
      "Alessandra Micheletti",
      "Alessandro Pedretti",
      "Carmen Gratteri",
      "Filippo Lunghini",
      "Andrea Rosario Beccari",
      "Carmine Talarico"
    ],
    "abstract": "Group Equivariant Non-Expansive Operators (GENEOs) have emerged as\nmathematical tools for constructing networks for Machine Learning and\nArtificial Intelligence. Recent findings suggest that such models can be\ninserted within the domain of eXplainable Artificial Intelligence (XAI) due to\ntheir inherent interpretability. In this study, we aim to verify this claim\nwith respect to GENEOnet, a GENEO network developed for an application in\ncomputational biochemistry by employing various statistical analyses and\nexperiments. Such experiments first allow us to perform a sensitivity analysis\non GENEOnet's parameters to test their significance. Subsequently, we show that\nGENEOnet exhibits a significantly higher proportion of equivariance compared to\nother methods. Lastly, we demonstrate that GENEOnet is on average robust to\nperturbations arising from molecular dynamics. These results collectively serve\nas proof of the explainability, trustworthiness, and robustness of GENEOnet and\nconfirm the beneficial use of GENEOs in the context of Trustworthy Artificial\nIntelligence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09199v1",
    "published_date": "2025-03-12 09:43:48 UTC",
    "updated_date": "2025-03-12 09:43:48 UTC"
  },
  {
    "arxiv_id": "2503.11702v3",
    "title": "Toward a method for LLM-enabled Indoor Navigation",
    "authors": [
      "Alberto Coffrini",
      "Mohammad Amin Zadenoori",
      "Paolo Barsocchi",
      "Francesco Furfari",
      "Antonino Crivello",
      "Alessio Ferrari"
    ],
    "abstract": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 3 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.11702v3",
    "published_date": "2025-03-12 09:32:43 UTC",
    "updated_date": "2025-03-24 11:42:16 UTC"
  },
  {
    "arxiv_id": "2503.09173v1",
    "title": "Long-Term Planning Around Humans in Domestic Environments with 3D Scene Graphs",
    "authors": [
      "Ermanno Bartoli",
      "Dennis Rotondi",
      "Kai O. Arras",
      "Iolanda Leite"
    ],
    "abstract": "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T40",
      "I.2"
    ],
    "primary_category": "cs.RO",
    "comment": "5 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.09173v1",
    "published_date": "2025-03-12 09:00:45 UTC",
    "updated_date": "2025-03-12 09:00:45 UTC"
  },
  {
    "arxiv_id": "2503.09164v1",
    "title": "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for Skin Cancer Treatment",
    "authors": [
      "Joscha GrÃ¼ger",
      "Tobias Geyer",
      "Tobias Brix",
      "Michael Storck",
      "Sonja Leson",
      "Laura Bley",
      "Carsten Weishaupt",
      "Ralph Bergmann",
      "Stephan A. Braun"
    ],
    "abstract": "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
    "categories": [
      "cs.AI",
      "68T99, 62P10, 92C50",
      "I.2.6; J.3; H.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09164v1",
    "published_date": "2025-03-12 08:49:03 UTC",
    "updated_date": "2025-03-12 08:49:03 UTC"
  },
  {
    "arxiv_id": "2503.09153v1",
    "title": "Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection",
    "authors": [
      "Chaowei Zhang",
      "Zongling Feng",
      "Zewei Zhang",
      "Jipeng Qiang",
      "Guandong Xu",
      "Yun Li"
    ],
    "abstract": "The questionable responses caused by knowledge hallucination may lead to\nLLMs' unstable ability in decision-making. However, it has never been\ninvestigated whether the LLMs' hallucination is possibly usable to generate\nnegative reasoning for facilitating the detection of fake news. This study\nproposes a novel supervised self-reinforced reasoning rectification approach -\nSR$^3$ that yields both common reasonable reasoning and wrong understandings\n(negative reasoning) for news via LLMs reflection for semantic consistency\nlearning. Upon that, we construct a negative reasoning-based news learning\nmodel called - \\emph{NRFE}, which leverages positive or negative news-reasoning\npairs for learning the semantic consistency between them. To avoid the impact\nof label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that\nonly takes news content as input to inspect the performance of our method by\ndistilling the knowledge from \\emph{NRFE}. The experimental results verified on\nthree popular fake news datasets demonstrate the superiority of our method\ncompared with three kinds of baselines including prompting on LLMs, fine-tuning\non pre-trained SLMs, and other representative fake news detection methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 12 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2503.09153v1",
    "published_date": "2025-03-12 08:29:59 UTC",
    "updated_date": "2025-03-12 08:29:59 UTC"
  },
  {
    "arxiv_id": "2503.09151v2",
    "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "authors": [
      "Hyeonho Jeong",
      "Suhyeon Lee",
      "Jong Chul Ye"
    ],
    "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://hyeonho99.github.io/reangle-a-video/",
    "pdf_url": "http://arxiv.org/pdf/2503.09151v2",
    "published_date": "2025-03-12 08:26:15 UTC",
    "updated_date": "2025-03-17 13:01:59 UTC"
  },
  {
    "arxiv_id": "2503.09144v1",
    "title": "Efficient UAV Swarm-Based Multi-Task Federated Learning with Dynamic Task Knowledge Sharing",
    "authors": [
      "Yubo Yang",
      "Tao Yang",
      "Xiaofeng Wu",
      "Ziyu Guo",
      "Bo Hu"
    ],
    "abstract": "UAV swarms are widely used in emergency communications, area monitoring, and\ndisaster relief. Coordinated by control centers, they are ideal for federated\nlearning (FL) frameworks. However, current UAV-assisted FL methods primarily\nfocus on single tasks, overlooking the need for multi-task training. In\ndisaster relief scenarios, UAVs perform tasks such as crowd detection, road\nfeasibility analysis, and disaster assessment, which exhibit time-varying\ndemands and potential correlations. In order to meet the time-varying\nrequirements of tasks and complete multiple tasks efficiently under resource\nconstraints, in this paper, we propose a UAV swarm based multi-task FL\nframework, where ground emergency vehicles (EVs) collaborate with UAVs to\naccomplish multiple tasks efficiently under constrained energy and bandwidth\nresources. Through theoretical analysis, we identify key factors affecting task\nperformance and introduce a task attention mechanism to dynamically evaluate\ntask importance, thereby achieving efficient resource allocation. Additionally,\nwe propose a task affinity (TA) metric to capture the dynamic correlation among\ntasks, thereby promoting task knowledge sharing to accelerate training and\nimprove the generalization ability of the model in different scenarios. To\noptimize resource allocation, we formulate a two-layer optimization problem to\njointly optimize UAV transmission power, computation frequency, bandwidth\nallocation, and UAV-EV associations. For the inner problem, we derive\nclosed-form solutions for transmission power, computation frequency, and\nbandwidth allocation and apply a block coordinate descent method for\noptimization. For the outer problem, a two-stage algorithm is designed to\ndetermine optimal UAV-EV associations. Furthermore, theoretical analysis\nreveals a trade-off between UAV energy consumption and multi-task performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file",
    "pdf_url": "http://arxiv.org/pdf/2503.09144v1",
    "published_date": "2025-03-12 08:13:39 UTC",
    "updated_date": "2025-03-12 08:13:39 UTC"
  },
  {
    "arxiv_id": "2503.09132v1",
    "title": "Investigation of Frame Differences as Motion Cues for Video Object Segmentation",
    "authors": [
      "Sota Kawamura",
      "Hirotada Honda",
      "Shugo Nakamura",
      "Takashi Sano"
    ],
    "abstract": "Automatic Video Object Segmentation (AVOS) refers to the task of autonomously\nsegmenting target objects in video sequences without relying on human-provided\nannotations in the first frames. In AVOS, the use of motion information is\ncrucial, with optical flow being a commonly employed method for capturing\nmotion cues. However, the computation of optical flow is resource-intensive,\nmaking it unsuitable for real-time applications, especially on edge devices\nwith limited computational resources. In this study, we propose using frame\ndifferences as an alternative to optical flow for motion cue extraction. We\ndeveloped an extended U-Net-like AVOS model that takes a frame on which\nsegmentation is performed and a frame difference as inputs, and outputs an\nestimated segmentation map. Our experimental results demonstrate that the\nproposed model achieves performance comparable to the model with optical flow\nas an input, particularly when applied to videos captured by stationary\ncameras. Our results suggest the usefulness of employing frame differences as\nmotion cues in cases with limited computational resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures, 2 tables. Accepted to The 9th International\n  Conference on Machine Learning and Soft Computing (ICMLSC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.09132v1",
    "published_date": "2025-03-12 07:42:15 UTC",
    "updated_date": "2025-03-12 07:42:15 UTC"
  },
  {
    "arxiv_id": "2503.09120v1",
    "title": "On the Internal Representations of Graph Metanetworks",
    "authors": [
      "Taesun Yeom",
      "Jaeho Lee"
    ],
    "abstract": "Weight space learning is an emerging paradigm in the deep learning community.\nThe primary goal of weight space learning is to extract informative features\nfrom a set of parameters using specially designed neural networks, often\nreferred to as \\emph{metanetworks}. However, it remains unclear how these\nmetanetworks learn solely from parameters. To address this, we take the first\nstep toward understanding \\emph{representations} of metanetworks, specifically\ngraph metanetworks (GMNs), which achieve state-of-the-art results in this\nfield, using centered kernel alignment (CKA). Through various experiments, we\nreveal that GMNs and general neural networks (\\textit{e.g.,} multi-layer\nperceptrons (MLPs) and convolutional neural networks (CNNs)) differ in terms of\ntheir representation space.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Workshop on Weight Space Learning",
    "pdf_url": "http://arxiv.org/pdf/2503.09120v1",
    "published_date": "2025-03-12 07:12:34 UTC",
    "updated_date": "2025-03-12 07:12:34 UTC"
  },
  {
    "arxiv_id": "2503.09114v1",
    "title": "Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge",
    "authors": [
      "Maximilian Abstreiter",
      "Sasu Tarkoma",
      "Roberto Morabito"
    ],
    "abstract": "The rapid rise of Language Models (LMs) has expanded the capabilities of\nnatural language processing, powering applications from text generation to\ncomplex decision-making. While state-of-the-art LMs often boast hundreds of\nbillions of parameters and are primarily deployed in data centers, recent\ntrends show a growing focus on compact models-typically under 10 billion\nparameters-enabled by techniques such as quantization and other model\ncompression techniques. This shift paves the way for LMs on edge devices,\noffering potential benefits such as enhanced privacy, reduced latency, and\nimproved data sovereignty. However, the inherent complexity of even these\nsmaller models, combined with the limited computing resources of edge hardware,\nraises critical questions about the practical trade-offs in executing LM\ninference outside the cloud. To address these challenges, we present a\ncomprehensive evaluation of generative LM inference on representative CPU-based\nand GPU-accelerated edge devices. Our study measures key performance\nindicators-including memory usage, inference speed, and energy\nconsumption-across various device configurations. Additionally, we examine\nthroughput-energy trade-offs, cost considerations, and usability, alongside an\nassessment of qualitative model performance. While quantization helps mitigate\nmemory overhead, it does not fully eliminate resource bottlenecks, especially\nfor larger models. Our findings quantify the memory and energy constraints that\nmust be considered for practical real-world deployments, offering concrete\ninsights into the trade-offs between model size, inference performance, and\nefficiency. The exploration of LMs at the edge is still in its early stages. We\nhope this study provides a foundation for future research, guiding the\nrefinement of models, the enhancement of inference efficiency, and the\nadvancement of edge-centric AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is currently under review for publication in an ACM\n  journal. If accepted, the copyright will be transferred to ACM",
    "pdf_url": "http://arxiv.org/pdf/2503.09114v1",
    "published_date": "2025-03-12 07:01:34 UTC",
    "updated_date": "2025-03-12 07:01:34 UTC"
  },
  {
    "arxiv_id": "2503.09113v1",
    "title": "Constraint-Guided Learning of Data-driven Health Indicator Models: An Application on the Pronostia Bearing Dataset",
    "authors": [
      "Yonas Tefera",
      "Quinten Van Baelen",
      "Maarten Meire",
      "Stijn Luca",
      "Peter Karsmakers"
    ],
    "abstract": "This paper presents a constraint-guided deep learning framework for\ndeveloping physically consistent health indicators in bearing prognostics and\nhealth management. Conventional data-driven methods often lack physical\nplausibility, while physics-based models are limited by incomplete system\nknowledge. To address this, we integrate domain knowledge into deep learning\nusing constraints to enforce monotonicity, bound output values between 1 and 0\n(representing healthy to failed states), and ensure consistency between signal\nenergy trends and health indicator estimates. This eliminates the need for\ncomplex loss term balancing. We implement constraint-guided gradient descent\nwithin an autoencoder architecture, creating a constrained autoencoder.\nHowever, the framework is adaptable to other architectures. Using\ntime-frequency representations of accelerometer signals from the Pronostia\ndataset, our constrained model generates smoother, more reliable degradation\nprofiles compared to conventional methods, aligning with expected physical\nbehavior. Performance is assessed using three metrics: trendability,\nrobustness, and consistency. Compared to a conventional baseline, the\nconstrained model improves all three. Another baseline, incorporating\nmonotonicity via a soft-ranking loss function, outperforms in trendability but\nfalls short in robustness and consistency. An ablation study confirms that the\nmonotonicity constraint enhances trendability, the boundary constraint ensures\nconsistency, and the energy-health consistency constraint improves robustness.\nThese findings highlight the effectiveness of constraint-guided deep learning\nin producing reliable, physically meaningful health indicators, offering a\npromising direction for future prognostic applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09113v1",
    "published_date": "2025-03-12 07:01:27 UTC",
    "updated_date": "2025-03-12 07:01:27 UTC"
  },
  {
    "arxiv_id": "2503.09106v1",
    "title": "Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery",
    "authors": [
      "Chuyu Zhang",
      "Xueyang Yu",
      "Peiyan Gu",
      "Xuming He"
    ],
    "abstract": "This paper addresses the problem of Rehearsal-Free Continual Category\nDiscovery (RF-CCD), which focuses on continuously identifying novel class by\nleveraging knowledge from labeled data. Existing methods typically train from\nscratch, overlooking the potential of base models, and often resort to data\nstorage to prevent forgetting. Moreover, because RF-CCD encompasses both\ncontinual learning and novel class discovery, previous approaches have\nstruggled to effectively integrate advanced techniques from these fields,\nresulting in less convincing comparisons and failing to reveal the unique\nchallenges posed by RF-CCD. To address these challenges, we lead the way in\nintegrating advancements from both domains and conducting extensive experiments\nand analyses. Our findings demonstrate that this integration can achieve\nstate-of-the-art results, leading to the conclusion that in the presence of\npre-trained models, the representation does not improve and may even degrade\nwith the introduction of unlabeled data. To mitigate representation\ndegradation, we propose a straightforward yet highly effective baseline method.\nThis method first utilizes prior knowledge of known categories to estimate the\nnumber of novel classes. It then acquires representations using a model\nspecifically trained on the base classes, generates high-quality pseudo-labels\nthrough k-means clustering, and trains only the classifier layer. We validate\nour conclusions and methods by conducting extensive experiments across multiple\nbenchmarks, including the Stanford Cars, CUB, iNat, and Tiny-ImageNet datasets.\nThe results clearly illustrate our findings, demonstrate the effectiveness of\nour baseline, and pave the way for future advancements in RF-CCD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Underreview",
    "pdf_url": "http://arxiv.org/pdf/2503.09106v1",
    "published_date": "2025-03-12 06:46:32 UTC",
    "updated_date": "2025-03-12 06:46:32 UTC"
  },
  {
    "arxiv_id": "2503.10699v1",
    "title": "Test-Time Discovery via Hashing Memory",
    "authors": [
      "Fan Lyu",
      "Tianle Liu",
      "Zhang Zhang",
      "Fuyuan Hu",
      "Liang Wang"
    ],
    "abstract": "We introduce Test-Time Discovery (TTD) as a novel task that addresses class\nshifts during testing, requiring models to simultaneously identify emerging\ncategories while preserving previously learned ones. A key challenge in TTD is\ndistinguishing newly discovered classes from those already identified. To\naddress this, we propose a training-free, hash-based memory mechanism that\nenhances class discovery through fine-grained comparisons with past test\nsamples. Leveraging the characteristics of unknown classes, our approach\nintroduces hash representation based on feature scale and directions, utilizing\nLocality-Sensitive Hashing (LSH) for efficient grouping of similar samples.\nThis enables test samples to be easily and quickly compared with relevant past\ninstances. Furthermore, we design a collaborative classification strategy,\ncombining a prototype classifier for known classes with an LSH-based classifier\nfor novel ones. To enhance reliability, we incorporate a self-correction\nmechanism that refines memory labels through hash-based neighbor retrieval,\nensuring more stable and accurate class assignments. Experimental results\ndemonstrate that our method achieves good discovery of novel categories while\nmaintaining performance on known classes, establishing a new paradigm in model\ntesting. Our code is available at https://github.com/fanlyu/ttd.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10699v1",
    "published_date": "2025-03-12 06:43:01 UTC",
    "updated_date": "2025-03-12 06:43:01 UTC"
  },
  {
    "arxiv_id": "2503.09101v2",
    "title": "The Shape of Attraction in UMAP: Exploring the Embedding Forces in Dimensionality Reduction",
    "authors": [
      "Mohammad Tariqul Islam",
      "Jason W. Fleischer"
    ],
    "abstract": "Uniform manifold approximation and projection (UMAP) is among the most\npopular neighbor embedding methods. The method relies on attractive and\nrepulsive forces among high-dimensional data points to obtain a low-dimensional\nembedding. In this paper, we analyze the forces to reveal their effects on\ncluster formations and visualization. Repulsion emphasizes differences,\ncontrolling cluster boundaries and inter-cluster distance. Attraction is more\nsubtle, as attractive tension between points can manifest simultaneously as\nattraction and repulsion in the lower-dimensional mapping. This explains the\nneed for learning rate annealing and motivates the different treatments between\nattractive and repulsive terms. Moreover, by modifying attraction, we improve\nthe consistency of cluster formation under random initialization. Overall, our\nanalysis makes UMAP and similar embedding methods more interpretable, more\nrobust, and more accurate.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 page + appendix",
    "pdf_url": "http://arxiv.org/pdf/2503.09101v2",
    "published_date": "2025-03-12 06:37:43 UTC",
    "updated_date": "2025-03-18 15:48:38 UTC"
  },
  {
    "arxiv_id": "2503.10697v1",
    "title": "Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion",
    "authors": [
      "Kaifeng Zou",
      "Xiaoyi Feng",
      "Peng Wang",
      "Tao Huang",
      "Zizhou Huang",
      "Zhang Haihang",
      "Yuntao Zou",
      "Dagang Li"
    ],
    "abstract": "Generative models are widely used in visual content creation. However,\ncurrent text-to-image models often face challenges in practical\napplications-such as textile pattern design and meme generation-due to the\npresence of unwanted elements that are difficult to separate with existing\nmethods. Meanwhile, subject-reference generation has emerged as a key research\ntrend, highlighting the need for techniques that can produce clean,\nhigh-quality subject images while effectively removing extraneous components.\nTo address this challenge, we introduce a framework for reliable\nsubject-centric image generation. In this work, we propose an entropy-based\nfeature-weighted fusion method to merge the informative cross-attention\nfeatures obtained from each sampling step of the pretrained text-to-image model\nFLUX, enabling a precise mask prediction and subject-centric generation.\nAdditionally, we have developed an agent framework based on Large Language\nModels (LLMs) that translates users' casual inputs into more descriptive\nprompts, leading to highly detailed image generation. Simultaneously, the\nagents extract primary elements of prompts to guide the entropy-based feature\nfusion, ensuring focused primary element generation without extraneous\ncomponents. Experimental results and user studies demonstrate our methods\ngenerates high-quality subject-centric images, outperform existing methods or\nother possible pipelines, highlighting the effectiveness of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 8 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.10697v1",
    "published_date": "2025-03-12 06:27:30 UTC",
    "updated_date": "2025-03-12 06:27:30 UTC"
  },
  {
    "arxiv_id": "2503.09091v2",
    "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
    "authors": [
      "Dong Li",
      "Guihong Wan",
      "Xintao Wu",
      "Xinyu Wu",
      "Xiaohui Chen",
      "Yi He",
      "Christine G. Lian",
      "Peter K. Sorger",
      "Yevgeniy R. Semenov",
      "Chen Zhao"
    ],
    "abstract": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09091v2",
    "published_date": "2025-03-12 06:03:33 UTC",
    "updated_date": "2025-03-20 16:43:54 UTC"
  },
  {
    "arxiv_id": "2503.09089v1",
    "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
    "authors": [
      "Zhaoling Chen",
      "Xiangru Tang",
      "Gangda Deng",
      "Fang Wu",
      "Jialong Wu",
      "Zhiwei Jiang",
      "Viktor Prasanna",
      "Arman Cohan",
      "Xingyao Wang"
    ],
    "abstract": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09089v1",
    "published_date": "2025-03-12 05:55:01 UTC",
    "updated_date": "2025-03-12 05:55:01 UTC"
  },
  {
    "arxiv_id": "2503.09081v1",
    "title": "Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment",
    "authors": [
      "Xiaowei Bi",
      "Zheyuan Xu"
    ],
    "abstract": "Long Video Question Answering (LVQA) is challenging due to the need for\ntemporal reasoning and large-scale multimodal data processing. Existing methods\nstruggle with retrieving cross-modal information from long videos, especially\nwhen relevant details are sparsely distributed. We introduce UMaT (Unified\nMulti-modal as Text), a retrieval-augmented generation (RAG) framework that\nefficiently processes extremely long videos while maintaining cross-modal\ncoherence. UMaT converts visual and auditory data into a unified textual\nrepresentation, ensuring semantic and temporal alignment. Short video clips are\nanalyzed using a vision-language model, while automatic speech recognition\n(ASR) transcribes dialogue. These text-based representations are structured\ninto temporally aligned segments, with adaptive filtering to remove redundancy\nand retain salient details. The processed data is embedded into a vector\ndatabase, enabling precise retrieval of dispersed yet relevant content.\nExperiments on a benchmark LVQA dataset show that UMaT outperforms existing\nmethods in multimodal integration, long-form video understanding, and sparse\ninformation retrieval. Its scalability and interpretability allow it to process\nvideos over an hour long while maintaining semantic and temporal coherence.\nThese findings underscore the importance of structured retrieval and multimodal\nsynchronization for advancing LVQA and long-form AI systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09081v1",
    "published_date": "2025-03-12 05:28:24 UTC",
    "updated_date": "2025-03-12 05:28:24 UTC"
  },
  {
    "arxiv_id": "2503.10695v2",
    "title": "Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks",
    "authors": [
      "Mooho Song",
      "Hyeryung Son",
      "Jay-Yoon Lee"
    ],
    "abstract": "Examining logical inconsistencies among multiple statements (such as\ncollections of sentences or question-answer pairs) is a crucial challenge in\nmachine learning, particularly for ensuring the safety and reliability of\nmodels. Traditional methods that rely on pairwise comparisons often fail to\ncapture inconsistencies that only emerge when more than two statements are\nevaluated collectively. To address this gap, we introduce the task of\nset-consistency verification, an extension of natural language inference (NLI)\nthat assesses the logical coherence of entire sets rather than isolated pairs.\nBuilding on this task, we present the Set-Consistency Energy Network\n(SC-Energy), a novel model that employs a contrastive loss framework to learn\nthe compatibility among a collection of statements. Our approach not only\nefficiently verifies inconsistencies and pinpoints the specific statements\nresponsible for logical contradictions, but also significantly outperforms\nexisting methods including prompting-based LLM models. Furthermore, we release\ntwo new datasets: Set-LConVQA and Set-SNLI for set-consistency verification\ntask.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10695v2",
    "published_date": "2025-03-12 05:11:11 UTC",
    "updated_date": "2025-03-19 04:07:06 UTC"
  },
  {
    "arxiv_id": "2503.13500v1",
    "title": "Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection",
    "authors": [
      "Yucheng Suo",
      "Fan Ma",
      "Kaixin Shen",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "abstract": "Visual instructions for long-horizon tasks are crucial as they intuitively\nclarify complex concepts and enhance retention across extended steps. Directly\ngenerating a series of images using text-to-image models without considering\nthe context of previous steps results in inconsistent images, increasing\ncognitive load. Additionally, the generated images often miss objects or the\nattributes such as color, shape, and state of the objects are inaccurate. To\naddress these challenges, we propose LIGER, the first training-free framework\nfor Long-horizon Instruction GEneration with logic and attribute\nself-Reflection. LIGER first generates a draft image for each step with the\nhistorical prompt and visual memory of previous steps. This step-by-step\ngeneration approach maintains consistency between images in long-horizon tasks.\nMoreover, LIGER utilizes various image editing tools to rectify errors\nincluding wrong attributes, logic errors, object redundancy, and identity\ninconsistency in the draft images. Through this self-reflection mechanism,\nLIGER improves the logic and object attribute correctness of the images. To\nverify whether the generated images assist human understanding, we manually\ncurated a new benchmark consisting of various long-horizon tasks.\nHuman-annotated ground truth expressions reflect the human-defined criteria for\nhow an image should appear to be illustrative. Experiments demonstrate the\nvisual instructions generated by LIGER are more comprehensive compared with\nbaseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13500v1",
    "published_date": "2025-03-12 05:11:02 UTC",
    "updated_date": "2025-03-12 05:11:02 UTC"
  },
  {
    "arxiv_id": "2503.09069v1",
    "title": "Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows",
    "authors": [
      "Chengyue Gong",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Jiangxuan Long",
      "Zhenmei Shi",
      "Zhao Song",
      "Yu Tian"
    ],
    "abstract": "Flow matching has emerged as a powerful framework for generative modeling,\noffering computational advantages over diffusion models by leveraging\ndeterministic Ordinary Differential Equations (ODEs) instead of stochastic\ndynamics. While prior work established the worst case optimality of standard\nflow matching under Wasserstein distances, the theoretical guarantees for\nhigher-order flow matching - which incorporates acceleration terms to refine\nsample trajectories - remain unexplored. In this paper, we bridge this gap by\nproving that higher-order flow matching preserves worst case optimality as a\ndistribution estimator. We derive upper bounds on the estimation error for\nsecond-order flow matching, demonstrating that the convergence rates depend\npolynomially on the smoothness of the target distribution (quantified via Besov\nspaces) and key parameters of the ODE dynamics. Our analysis employs neural\nnetwork approximations with carefully controlled depth, width, and sparsity to\nbound acceleration errors across both small and large time intervals,\nultimately unifying these results into a general worst case optimal bound for\nall time steps.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2410.11261",
    "pdf_url": "http://arxiv.org/pdf/2503.09069v1",
    "published_date": "2025-03-12 05:07:07 UTC",
    "updated_date": "2025-03-12 05:07:07 UTC"
  },
  {
    "arxiv_id": "2503.09068v1",
    "title": "Probing Network Decisions: Capturing Uncertainties and Unveiling Vulnerabilities Without Label Information",
    "authors": [
      "Youngju Joung",
      "Sehyun Lee",
      "Jaesik Choi"
    ],
    "abstract": "To improve trust and transparency, it is crucial to be able to interpret the\ndecisions of Deep Neural classifiers (DNNs). Instance-level examinations, such\nas attribution techniques, are commonly employed to interpret the model\ndecisions. However, when interpreting misclassified decisions, human\nintervention may be required. Analyzing the attribu tions across each class\nwithin one instance can be particularly labor intensive and influenced by the\nbias of the human interpreter. In this paper, we present a novel framework to\nuncover the weakness of the classifier via counterfactual examples. A prober is\nintroduced to learn the correctness of the classifier's decision in terms of\nbinary code-hit or miss. It enables the creation of the counterfactual example\nconcerning the prober's decision. We test the performance of our prober's\nmisclassification detection and verify its effectiveness on the image\nclassification benchmark datasets. Furthermore, by generating counterfactuals\nthat penetrate the prober, we demonstrate that our framework effectively\nidentifies vulnerabilities in the target classifier without relying on label\ninformation on the MNIST dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "ICPRAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.09068v1",
    "published_date": "2025-03-12 05:05:58 UTC",
    "updated_date": "2025-03-12 05:05:58 UTC"
  },
  {
    "arxiv_id": "2503.09642v2",
    "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k",
    "authors": [
      "Xiangyu Peng",
      "Zangwei Zheng",
      "Chenhui Shen",
      "Tom Young",
      "Xinying Guo",
      "Binluo Wang",
      "Hang Xu",
      "Hongxin Liu",
      "Mingyan Jiang",
      "Wenjun Li",
      "Yuhui Wang",
      "Anbang Ye",
      "Gang Ren",
      "Qianran Ma",
      "Wanying Liang",
      "Xiang Lian",
      "Xiwen Wu",
      "Yuting Zhong",
      "Zhuangyan Li",
      "Chaoyu Gong",
      "Guojun Lei",
      "Leijun Cheng",
      "Limin Zhang",
      "Minghao Li",
      "Ruijie Zhang",
      "Silan Hu",
      "Shijie Huang",
      "Xiaokang Wang",
      "Yuanheng Zhao",
      "Yuqi Wang",
      "Ziang Wei",
      "Yang You"
    ],
    "abstract": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09642v2",
    "published_date": "2025-03-12 05:00:07 UTC",
    "updated_date": "2025-03-23 13:43:54 UTC"
  },
  {
    "arxiv_id": "2503.09066v1",
    "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
    "authors": [
      "Xin Wei Chia",
      "Jonathan Pan"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.09066v1",
    "published_date": "2025-03-12 04:59:22 UTC",
    "updated_date": "2025-03-12 04:59:22 UTC"
  },
  {
    "arxiv_id": "2503.09058v1",
    "title": "Implicit Contrastive Representation Learning with Guided Stop-gradient",
    "authors": [
      "Byeongchan Lee",
      "Sehyun Lee"
    ],
    "abstract": "In self-supervised representation learning, Siamese networks are a natural\narchitecture for learning transformation-invariance by bringing representations\nof positive pairs closer together. But it is prone to collapse into a\ndegenerate solution. To address the issue, in contrastive learning, a\ncontrastive loss is used to prevent collapse by moving representations of\nnegative pairs away from each other. But it is known that algorithms with\nnegative sampling are not robust to a reduction in the number of negative\nsamples. So, on the other hand, there are algorithms that do not use negative\npairs. Many positive-only algorithms adopt asymmetric network architecture\nconsisting of source and target encoders as a key factor in coping with\ncollapse. By exploiting the asymmetric architecture, we introduce a methodology\nto implicitly incorporate the idea of contrastive learning. As its\nimplementation, we present a novel method guided stop-gradient. We apply our\nmethod to benchmark algorithms SimSiam and BYOL and show that our method\nstabilizes training and boosts performance. We also show that the algorithms\nwith our method work well with small batch sizes and do not collapse even when\nthere is no predictor. The code is available at\nhttps://github.com/bych-lee/gsg.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips 2023",
    "pdf_url": "http://arxiv.org/pdf/2503.09058v1",
    "published_date": "2025-03-12 04:46:53 UTC",
    "updated_date": "2025-03-12 04:46:53 UTC"
  },
  {
    "arxiv_id": "2503.09051v1",
    "title": "TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction",
    "authors": [
      "Shengyao Lu",
      "Jiuding Yang",
      "Baochun Li",
      "Di Niu"
    ],
    "abstract": "The growing demand for transparency and interpretability in critical domains\nhas driven increased interests in comprehending the explainability of\nMessage-Passing (MP) Graph Neural Networks (GNNs). Although substantial\nresearch efforts have been made to generate explanations for individual graph\ninstances, identifying global explaining concepts for a GNN still poses great\nchallenges, especially when concepts are desired in a graphical form on the\ndataset level. While most prior works treat GNNs as black boxes, in this paper,\nwe propose to unbox GNNs by analyzing and extracting critical subtrees incurred\nby the inner workings of message passing, which correspond to critical\nsubgraphs in the datasets. By aggregating subtrees in an embedding space with\nan efficient algorithm, which does not require complex subgraph matching or\nsearch, we can make intuitive graphical explanations for Message-Passing GNNs\non local, class and global levels. We empirically show that our proposed\napproach not only generates clean subgraph concepts on a dataset level in\ncontrast to existing global explaining methods which generate non-graphical\nrules (e.g., language or embeddings) as explanations, but it is also capable of\nproviding explanations for individual instances with a comparable or even\nsuperior performance as compared to leading local-level GNN explainers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09051v1",
    "published_date": "2025-03-12 04:36:28 UTC",
    "updated_date": "2025-03-12 04:36:28 UTC"
  },
  {
    "arxiv_id": "2503.09046v1",
    "title": "Discovering Influential Neuron Path in Vision Transformers",
    "authors": [
      "Yifan Wang",
      "Yifei Liu",
      "Yingdong Shi",
      "Changming Li",
      "Anqi Pang",
      "Sibei Yang",
      "Jingyi Yu",
      "Kan Ren"
    ],
    "abstract": "Vision Transformer models exhibit immense power yet remain opaque to human\nunderstanding, posing challenges and risks for practical applications. While\nprior research has attempted to demystify these models through input\nattribution and neuron role analysis, there's been a notable gap in considering\nlayer-level information and the holistic path of information flow across\nlayers. In this paper, we investigate the significance of influential neuron\npaths within vision Transformers, which is a path of neurons from the model\ninput to output that impacts the model inference most significantly. We first\npropose a joint influence measure to assess the contribution of a set of\nneurons to the model outcome. And we further provide a layer-progressive neuron\nlocating approach that efficiently selects the most influential neuron at each\nlayer trying to discover the crucial neuron path from input to output within\nthe target model. Our experiments demonstrate the superiority of our method\nfinding the most influential neuron path along which the information flows,\nover the existing baseline solutions. Additionally, the neuron paths have\nillustrated that vision Transformers exhibit some specific inner working\nmechanism for processing the visual information within the same image category.\nWe further analyze the key effects of these neurons on the image classification\ntask, showcasing that the found neuron paths have already preserved the model\ncapability on downstream tasks, which may also shed some lights on real-world\napplications like model pruning. The project website including implementation\ncode is available at https://foundation-model-research.github.io/NeuronPath/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.09046v1",
    "published_date": "2025-03-12 04:10:46 UTC",
    "updated_date": "2025-03-12 04:10:46 UTC"
  },
  {
    "arxiv_id": "2503.09035v1",
    "title": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers",
    "authors": [
      "Shawn Azdam",
      "Pranav Doma",
      "Aliasghar Moj Arab"
    ],
    "abstract": "The next generation of active safety features in autonomous vehicles should\nbe capable of safely executing evasive hazard-avoidance maneuvers akin to those\nperformed by professional stunt drivers to achieve high-agility motion at the\nlimits of vehicle handling. This paper presents a novel framework, ManeuverGPT,\nfor generating and executing high-dynamic stunt maneuvers in autonomous\nvehicles using large language model (LLM)-based agents as controllers. We\ntarget aggressive maneuvers, such as J-turns, within the CARLA simulation\nenvironment and demonstrate an iterative, prompt-based approach to refine\nvehicle control parameters, starting tabula rasa without retraining model\nweights. We propose an agentic architecture comprised of three specialized\nagents (1) a Query Enricher Agent for contextualizing user commands, (2) a\nDriver Agent for generating maneuver parameters, and (3) a Parameter Validator\nAgent that enforces physics-based and safety constraints. Experimental results\ndemonstrate successful J-turn execution across multiple vehicle models through\ntextual prompts that adapt to differing vehicle dynamics. We evaluate\nperformance via established success criteria and discuss limitations regarding\nnumeric precision and scenario complexity. Our findings underscore the\npotential of LLM-driven control for flexible, high-dynamic maneuvers, while\nhighlighting the importance of hybrid approaches that combine language-based\nreasoning with algorithmic validation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "6 Pages, Submitted to IROS",
    "pdf_url": "http://arxiv.org/pdf/2503.09035v1",
    "published_date": "2025-03-12 03:51:41 UTC",
    "updated_date": "2025-03-12 03:51:41 UTC"
  },
  {
    "arxiv_id": "2503.09033v2",
    "title": "RFUAV: A Benchmark Dataset for Unmanned Aerial Vehicle Detection and Identification",
    "authors": [
      "Rui Shi",
      "Xiaodong Yu",
      "Shengming Wang",
      "Yijia Zhang",
      "Lu Xu",
      "Peng Pan",
      "Chunlai Ma"
    ],
    "abstract": "In this paper, we propose RFUAV as a new benchmark dataset for\nradio-frequency based (RF-based) unmanned aerial vehicle (UAV) identification\nand address the following challenges: Firstly, many existing datasets feature a\nrestricted variety of drone types and insufficient volumes of raw data, which\nfail to meet the demands of practical applications. Secondly, existing datasets\noften lack raw data covering a broad range of signal-to-noise ratios (SNR), or\ndo not provide tools for transforming raw data to different SNR levels. This\nlimitation undermines the validity of model training and evaluation. Lastly,\nmany existing datasets do not offer open-access evaluation tools, leading to a\nlack of unified evaluation standards in current research within this field.\nRFUAV comprises approximately 1.3 TB of raw frequency data collected from 37\ndistinct UAVs using the Universal Software Radio Peripheral (USRP) device in\nreal-world environments. Through in-depth analysis of the RF data in RFUAV, we\ndefine a drone feature sequence called RF drone fingerprint, which aids in\ndistinguishing drone signals. In addition to the dataset, RFUAV provides a\nbaseline preprocessing method and model evaluation tools. Rigorous experiments\ndemonstrate that these preprocessing methods achieve state-of-the-art (SOTA)\nperformance using the provided evaluation tools. The RFUAV dataset and baseline\nimplementation are publicly available at https://github.com/kitoweeknd/RFUAV/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "23 pages, 13 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2503.09033v2",
    "published_date": "2025-03-12 03:46:09 UTC",
    "updated_date": "2025-03-18 03:28:48 UTC"
  },
  {
    "arxiv_id": "2503.09032v1",
    "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
    "authors": [
      "Younwoo Choi",
      "Muhammad Adil Asif",
      "Ziwen Han",
      "John Willes",
      "Rahul G. Krishnan"
    ],
    "abstract": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.09032v1",
    "published_date": "2025-03-12 03:45:53 UTC",
    "updated_date": "2025-03-12 03:45:53 UTC"
  },
  {
    "arxiv_id": "2503.09020v2",
    "title": "Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning",
    "authors": [
      "Yuan Jiang",
      "Yujian Zhang",
      "Liang Lu",
      "Christoph Treude",
      "Xiaohong Su",
      "Shan Huang",
      "Tiantian Wang"
    ],
    "abstract": "Large Language Models (LLMs) have been widely adopted in commercial code\ncompletion engines, significantly enhancing coding efficiency and productivity.\nHowever, LLMs may generate code with quality issues that violate coding\nstandards and best practices, such as poor code style and maintainability, even\nwhen the code is functionally correct. This necessitates additional effort from\ndevelopers to improve the code, potentially negating the efficiency gains\nprovided by LLMs. To address this problem, we propose a novel comparative\nprefix-tuning method for controllable high-quality code generation. Our method\nintroduces a single, property-specific prefix that is prepended to the\nactivations of the LLM, serving as a lightweight alternative to fine-tuning.\nUnlike existing methods that require training multiple prefixes, our approach\ntrains only one prefix and leverages pairs of high-quality and low-quality code\nsamples, introducing a sequence-level ranking loss to guide the model's\ntraining. This comparative approach enables the model to better understand the\ndifferences between high-quality and low-quality code, focusing on aspects that\nimpact code quality. Additionally, we design a data construction pipeline to\ncollect and annotate pairs of high-quality and low-quality code, facilitating\neffective training. Extensive experiments on the Code Llama 7B model\ndemonstrate that our method improves code quality by over 100% in certain task\ncategories, while maintaining functional correctness. We also conduct ablation\nstudies and generalization experiments, confirming the effectiveness of our\nmethod's components and its strong generalization capability.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09020v2",
    "published_date": "2025-03-12 03:15:46 UTC",
    "updated_date": "2025-03-19 07:24:48 UTC"
  },
  {
    "arxiv_id": "2503.09639v2",
    "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
    "authors": [
      "Abe Bohan Hou",
      "Hongru Du",
      "Yichen Wang",
      "Jingyu Zhang",
      "Zixiao Wang",
      "Paul Pu Liang",
      "Daniel Khashabi",
      "Lauren Gardner",
      "Tianxing He"
    ],
    "abstract": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09639v2",
    "published_date": "2025-03-12 02:54:15 UTC",
    "updated_date": "2025-03-16 06:03:01 UTC"
  },
  {
    "arxiv_id": "2503.09008v1",
    "title": "Towards Quantifying Long-Range Interactions in Graph Machine Learning: a Large Graph Dataset and a Measurement",
    "authors": [
      "Huidong Liang",
      "Haitz SÃ¡ez de OcÃ¡riz Borde",
      "Baskaran Sripathmanathan",
      "Michael Bronstein",
      "Xiaowen Dong"
    ],
    "abstract": "Long-range dependencies are critical for effective graph representation\nlearning, yet most existing datasets focus on small graphs tailored to\ninductive tasks, offering limited insight into long-range interactions. Current\nevaluations primarily compare models employing global attention (e.g., graph\ntransformers) with those using local neighborhood aggregation (e.g.,\nmessage-passing neural networks) without a direct measurement of long-range\ndependency. In this work, we introduce City-Networks, a novel large-scale\ntransductive learning dataset derived from real-world city roads. This dataset\nfeatures graphs with over $10^5$ nodes and significantly larger diameters than\nthose in existing benchmarks, naturally embodying long-range information. We\nannotate the graphs using an eccentricity-based approach, ensuring that the\nclassification task inherently requires information from distant nodes.\nFurthermore, we propose a model-agnostic measurement based on the Jacobians of\nneighbors from distant hops, offering a principled quantification of long-range\ndependencies. Finally, we provide theoretical justifications for both our\ndataset design and the proposed measurement - particularly by focusing on\nover-smoothing and influence score dilution - which establishes a robust\nfoundation for further exploration of long-range interactions in graph neural\nnetworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.09008v1",
    "published_date": "2025-03-12 02:51:17 UTC",
    "updated_date": "2025-03-12 02:51:17 UTC"
  },
  {
    "arxiv_id": "2503.09002v1",
    "title": "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers",
    "authors": [
      "Chenyuan Yang",
      "Zijie Zhao",
      "Zichen Xie",
      "Haoyu Li",
      "Lingming Zhang"
    ],
    "abstract": "Static analysis is a powerful technique for bug detection in critical systems\nlike operating system kernels. However, designing and implementing static\nanalyzers is challenging, time-consuming, and typically limited to predefined\nbug patterns. While large language models (LLMs) have shown promise for static\nanalysis, directly applying them to scan large codebases remains impractical\ndue to computational constraints and contextual limitations.\n  We present KNighter, the first approach that unlocks practical LLM-based\nstatic analysis by automatically synthesizing static analyzers from historical\nbug patterns. Rather than using LLMs to directly analyze massive codebases, our\nkey insight is leveraging LLMs to generate specialized static analyzers guided\nby historical patch knowledge. KNighter implements this vision through a\nmulti-stage synthesis pipeline that validates checker correctness against\noriginal patches and employs an automated refinement process to iteratively\nreduce false positives. Our evaluation on the Linux kernel demonstrates that\nKNighter generates high-precision checkers capable of detecting diverse bug\npatterns overlooked by existing human-written analyzers. To date,\nKNighter-synthesized checkers have discovered 70 new bugs/vulnerabilities in\nthe Linux kernel, with 56 confirmed and 41 already fixed. 11 of these findings\nhave been assigned CVE numbers. This work establishes an entirely new paradigm\nfor scalable, reliable, and traceable LLM-based static analysis for real-world\nsystems via checker synthesis.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.OS"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09002v1",
    "published_date": "2025-03-12 02:30:19 UTC",
    "updated_date": "2025-03-12 02:30:19 UTC"
  },
  {
    "arxiv_id": "2503.13499v1",
    "title": "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
    "authors": [
      "Rajeev Kumar",
      "Harishankar Kumar",
      "Kumari Shalini"
    ],
    "abstract": "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13499v1",
    "published_date": "2025-03-12 02:17:15 UTC",
    "updated_date": "2025-03-12 02:17:15 UTC"
  },
  {
    "arxiv_id": "2503.08994v1",
    "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation",
    "authors": [
      "Kaixin Zhang",
      "Hongzhi Wang",
      "Ziqi Li",
      "Yabin Lu",
      "Yingze Li",
      "Yu Yan",
      "Yiming Guan"
    ],
    "abstract": "Research on learned cardinality estimation has achieved significant progress\nin recent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We conceptualize\nthese challenges as the \"Trilemma of Cardinality Estimation\", where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod for estimating both equi and non-equi join cardinality by leveraging the\nconditional probability distributions of individual tables in a decoupled\nmanner. (2) To meet the requirements of efficient training and inference for\nDistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a\nhigh-throughput conditional probability distribution estimation model. (3) We\nformally analyze the variance of existing similar methods and demonstrate that\nsuch approaches suffer from variance accumulation issues. To mitigate this\nproblem, DistJoin employs a selectivity-based approach rather than a\ncount-based approach to infer join cardinality, effectively reducing variance.\nIn summary, DistJoin not only represents the first data-driven method to\neffectively support both equi and non-equi joins but also demonstrates superior\naccuracy while enabling fast and flexible updates. We evaluate DistJoin on\nJOB-light and JOB-light-ranges, extending the evaluation to non-equi join\nconditions. The results demonstrate that our approach achieves the highest\naccuracy, robustness to data updates, generality, and comparable update and\ninference speed relative to existing methods.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08994v1",
    "published_date": "2025-03-12 02:07:08 UTC",
    "updated_date": "2025-03-12 02:07:08 UTC"
  },
  {
    "arxiv_id": "2503.09638v1",
    "title": "Edge AI-Powered Real-Time Decision-Making for Autonomous Vehicles in Adverse Weather Conditions",
    "authors": [
      "Milad Rahmati"
    ],
    "abstract": "Autonomous vehicles (AVs) are transforming modern transportation, but their\nreliability and safety are significantly challenged by harsh weather conditions\nsuch as heavy rain, fog, and snow. These environmental factors impair the\nperformance of cameras, LiDAR, and radar, leading to reduced situational\nawareness and increased accident risks. Conventional cloud-based AI systems\nintroduce communication delays, making them unsuitable for the rapid\ndecision-making required in real-time autonomous navigation. This paper\npresents a novel Edge AI-driven real-time decision-making framework designed to\nenhance AV responsiveness under adverse weather conditions. The proposed\napproach integrates convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) for improved perception, alongside reinforcement learning\n(RL)-based strategies to optimize vehicle control in uncertain environments. By\nprocessing data at the network edge, this system significantly reduces decision\nlatency while improving AV adaptability. The framework is evaluated using\nsimulated driving scenarios in CARLA and real-world data from the Waymo Open\nDataset, covering diverse weather conditions. Experimental results indicate\nthat the proposed model achieves a 40% reduction in processing time and a 25%\nenhancement in perception accuracy compared to conventional cloud-based\nsystems. These findings highlight the potential of Edge AI in improving AV\nautonomy, safety, and efficiency, paving the way for more reliable self-driving\ntechnology in challenging real-world environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09638v1",
    "published_date": "2025-03-12 02:02:05 UTC",
    "updated_date": "2025-03-12 02:02:05 UTC"
  },
  {
    "arxiv_id": "2503.08990v1",
    "title": "JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing",
    "authors": [
      "Vasudev Gohil"
    ],
    "abstract": "Large language models (LLMs) have shown great promise as language\nunderstanding and decision making tools, and they have permeated various\naspects of our everyday life. However, their widespread availability also comes\nwith novel risks, such as generating harmful, unethical, or offensive content,\nvia an attack called jailbreaking. Despite extensive efforts from LLM\ndevelopers to align LLMs using human feedback, they are still susceptible to\njailbreak attacks. To tackle this issue, researchers often employ red-teaming\nto understand and investigate jailbreak prompts. However, existing red-teaming\napproaches lack effectiveness, scalability, or both. To address these issues,\nwe propose JBFuzz, a novel effective, automated, and scalable red-teaming\ntechnique for jailbreaking LLMs.\n  JBFuzz is inspired by the success of fuzzing for detecting\nbugs/vulnerabilities in software. We overcome three challenges related to\neffectiveness and scalability by devising novel seed prompts, a lightweight\nmutation engine, and a lightweight and accurate evaluator for guiding the\nfuzzer. Assimilating all three solutions results in a potent fuzzer that only\nrequires black-box access to the target LLM. We perform extensive experimental\nevaluation of JBFuzz using nine popular and widely-used LLMs. We find that\nJBFuzz successfully jailbreaks all LLMs for various harmful/unethical\nquestions, with an average attack success rate of 99%. We also find that JBFuzz\nis extremely efficient as it jailbreaks a given LLM for a given question in 60\nseconds on average. Our work highlights the susceptibility of the\nstate-of-the-art LLMs to jailbreak attacks even after safety alignment, and\nserves as a valuable red-teaming tool for LLM developers.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.08990v1",
    "published_date": "2025-03-12 01:52:17 UTC",
    "updated_date": "2025-03-12 01:52:17 UTC"
  }
]
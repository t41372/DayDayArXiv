{
  "date": "2025-03-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间2025-03-12的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文亮点纷呈，大型语言模型（LLMs）依然是研究热点，涵盖了推理（特别是长链思维）、安全性（如对抗攻击、隐私泄露、隐性误导信息）、评估（新基准 MOAT、AgentDAM）以及与外部工具（如搜索引擎、静态分析器）的结合。生成模型领域同样活跃，特别是视频生成方面，备受瞩目的 Open-Sora 2.0 开源项目发布了技术报告，同时也有新的多视角视频生成和可控生成框架。视觉语言模型（VLMs）的研究深入到理解局限性、医学应用、机器人导航和自我提升等多个方面。此外，AI伦理、隐私保护、强化学习新理论（TD-Flows）、图神经网络（GNNs）以及各种AI在特定领域的应用（如医疗、机器人、软件工程、推荐系统）也贡献了大量有趣的工作。\n\n以下是今天值得关注的论文：\n\n---\n\n**1. Open-Sora 2.0：以20万美元训练商业级视频生成模型 (Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k)**\n\n- **作者:** Xiangyu Peng, Zangwei Zheng, et al.\n- **摘要:** 这篇报告介绍了 Open-Sora 2.0，一个号称达到商业级水平的视频生成模型，其训练成本控制在20万美元。报告详细阐述了实现这一效率突破的技术，包括数据管理、模型架构、训练策略和系统优化。根据人类评估和 VBench 分数，Open-Sora 2.0 的性能可与全球领先的视频生成模型（如开源的混元视频和闭源的 Runway Gen-3 Alpha）相媲美。该项目已完全开源，旨在推动视频生成技术的普及和创新。\n- **TLDR:** Open-Sora 2.0 技术报告，展示了如何用相对较低的成本（20万美元）训练出媲美商业级水平的开源视频生成模型，并分享了数据、模型、训练和优化细节。\n\n**2. Search-R1：用强化学习训练 LLMs 进行推理并利用搜索引擎 (Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning)**\n\n- **作者:** Bowen Jin, Hansi Zeng, et al.\n- **摘要:** 该研究提出 Search-R1，扩展了 DeepSeek-R1 模型，通过纯粹的强化学习（RL）让 LLM 学会在逐步推理过程中自主生成（多个）搜索查询并利用实时检索结果。Search-R1 优化了包含多轮搜索交互的 LLM rollout，利用检索到的 token masking 实现稳定的 RL 训练，并使用简单的基于结果的奖励函数。实验表明，Search-R1 在问答数据集上显著优于强基线模型。\n- **TLDR:** 提出 Search-R1，通过强化学习教会 LLM 在推理时自主、有效地使用搜索引擎获取外部知识，显著提升问答性能。\n\n**3. KNighter：利用 LLM 合成的检查器革新静态分析 (KNighter: Transforming Static Analysis with LLM-Synthesized Checkers)**\n\n- **作者:** Chenyuan Yang, Zijie Zhao, et al.\n- **摘要:** 静态分析是检测关键系统（如操作系统内核）错误的有效技术，但设计和实现分析器既困难又耗时。该研究提出 KNighter，一种利用 LLM 从历史错误模式中自动合成静态分析器的新方法。KNighter 不直接用 LLM 分析大型代码库，而是利用 LLM 根据历史补丁知识生成专门的静态分析器，并通过多阶段合成流程验证正确性并自动优化以减少误报。评估表明，KNighter 生成的检查器精度高，能发现现有工具忽略的错误模式，已在 Linux 内核中发现 70 个新 Bug/漏洞（56 个已确认，41 个已修复，11 个获 CVE 编号）。\n- **TLDR:** 提出 KNighter 框架，利用 LLM 从历史补丁中自动生成高质量的静态代码分析器，已在 Linux 内核中发现大量新漏洞，开创了 LLM 用于静态分析的新范式。\n\n**4. Reangle-A-Video：将 4D 视频生成视为视频到视频的翻译 (Reangle-A-Video: 4D Video Generation as Video-to-Video Translation)**\n\n- **作者:** Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye\n- **摘要:** 提出 Reangle-A-Video，一个从单输入视频生成同步多视角视频的统一框架。该方法不依赖大规模 4D 数据集训练，而是将任务重构为视频到视频的翻译，利用现有的图像和视频扩散先验。它包含多视角运动学习和多视角一致的图像到图像翻译两个阶段，后者利用 DUSt3R 进行跨视角一致性指导。实验表明该方法在静态视角变换和动态相机控制方面优于现有方法。\n- **TLDR:** 提出一种新颖的视频生成框架 Reangle-A-Video，能从单个视频生成多视角视频，无需 4D 数据集训练，效果优于现有方法。\n\n**5. MOAT：评估 LMM 的能力整合与指令遵循 (MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding)**\n\n- **作者:** Zhoutong Ye, Mingze Sun, et al.\n- **摘要:** 大型多模态模型（LMMs）在视觉语言（VL）任务中展现了通用潜力，但在需要整合多种基础 VL 能力（如读文本、计数、空间理解、指令遵循）的复杂任务上，与人类表现仍有差距。该研究提出 MOAT，一个包含复杂真实世界 VL 任务的新基准，用于系统评估 LMMs。MOAT 包含 10 种基础 VL 能力分类，能细粒度分析模型优劣，并首次明确评估 LMMs 理解复杂文本和视觉指令的能力。对 20 多个 LMMs 的评估显示，最佳模型（OpenAI o1）准确率仅 38.8%，远低于人类的 82.7%。\n- **TLDR:** 推出 MOAT 基准，专门评估大型多模态模型在整合多种视觉语言能力和遵循复杂指令方面的表现，发现当前顶尖模型与人类差距显著。\n\n**6. AgentDAM：自主网页代理的隐私泄露评估 (AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents)**\n\n- **作者:** Arman Zharmagambetov, Chuan Guo, et al.\n- **摘要:** LLM 驱动的 AI 代理潜力巨大，但也带来隐私风险。该研究关注数据最小化原则，即仅在完成任务必需时才共享私人信息。作者开发了 AgentDAM 基准，用于评估现有和未来的 AI 代理在执行网页交互任务时，限制处理非必要隐私信息的能力。实验发现基于 GPT-4、Llama-3 和 Claude 的代理常会无意中使用不必要的敏感信息，并提出一种基于提示的方法来缓解此问题。\n- **TLDR:** 提出 AgentDAM 基准，用于评估 AI 网页代理在执行任务时是否遵循数据最小化原则，发现现有代理存在隐私泄露风险，并提出基于提示的缓解方法。\n\n**7. 更安全还是更幸运？LLMs 作为安全评估器对伪影不鲁棒 (Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts)**\n\n- **作者:** Hongyu Chen, Seraphina Goldfarb-Tarrant\n- **摘要:** 研究评估了 11 个 LLM 作为自动化安全评估器的可靠性。发现 LLM 评估器在重复判断任务中存在不一致性，与人类判断对齐度有限，且极易受到输入伪影（如道歉性或冗长的措辞）的影响。例如，仅道歉性语言就能使评估偏好倾斜高达 98%。更大的模型并不总是更鲁棒。虽然基于多个模型决策的“陪审团”评估能提高鲁棒性和人类对齐度，但伪影敏感性依然存在。\n- **TLDR:** LLM 作为安全评估器并不可靠，容易受到无关文本特征（如道歉语气）的干扰，导致评估结果失真，即使采用多模型投票也无法完全消除此问题。\n\n**8. JBFuzz：使用 Fuzzing 高效且有效地越狱 LLMs (JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing)**\n\n- **作者:** Vasudev Gohil\n- **摘要:** 针对 LLM 越狱攻击（绕过安全限制生成有害内容）的问题，现有红队测试方法效率或效果不足。该研究受软件测试中 Fuzzing 的启发，提出 JBFuzz，一种自动化、可扩展的 LLM 红队测试技术。JBFuzz 设计了新颖的种子提示、轻量级变异引擎和轻量级评估器来指导 Fuzzing 过程，仅需黑盒访问目标 LLM。实验表明，JBFuzz 能成功越狱 9 个流行 LLM，平均成功率 99%，平均耗时仅 60 秒。\n- **TLDR:** 提出 JBFuzz，一种基于 Fuzzing 的自动化 LLM 越狱工具，能高效（平均 60 秒）、高成功率（99%）地发现 LLM 的安全漏洞。\n\n**9. 迈向推理时代：长链思维用于大型语言模型推理的综述 (Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models)**\n\n- **作者:** Qiguang Chen, Libo Qin, et al.\n- **摘要:** 近期 LLM 推理（RLLMs）如 OpenAI-O1 和 DeepSeek-R1 在数学、编码等复杂领域展现强大能力，其核心在于应用了长链思维（Long CoT）特性。然而，目前缺乏对 Long CoT 的全面综述。本文旨在填补此空白，区分了 Long CoT 与短链思维（Short CoT），提出了新的推理范式分类法，探讨了 Long CoT 的关键特性（深度推理、广泛探索、可行反思），研究了相关现象（如过度思考、测试时扩展），并指出了未来研究方向。\n- **TLDR:** 对大型语言模型中的长链思维（Long CoT）进行了系统性综述，区分其与短链思维，探讨其特性、现象及未来方向，旨在推动 AI 逻辑推理的发展。\n\n**10. 时序差分流 (Temporal Difference Flows)**\n\n- **作者:** Jesse Farebrother, Matteo Pirotta, et al.\n- **摘要:** 预测性世界模型对智能体的推理和规划至关重要。传统方法逐步展开模型易累积误差。几何视界模型（GHMs）直接预测未来状态避免此问题。本文提出时序差分流（TD-Flow），利用新的概率路径贝尔曼方程结构和流匹配技术，学习准确的 GHMs，其预测视界长度是先前方法的 5 倍以上。理论上，TD-Flow 通过降低训练梯度方差实现有效性。实验验证了其在生成指标和下游任务（如策略评估）上的优势，并显示其在长视界决策中的潜力。\n- **TLDR:** 提出 TD-Flow，一种结合时序差分学习和流匹配的新方法，能更准确、更长远地预测未来状态，克服了传统世界模型和现有 GHM 的局限性。\n\n**11. 通过图知识微调视觉语言模型用于可解释医学图像分析 (Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis)**\n\n- **作者:** Chenjun Li, Laurin Lux, et al.\n- **摘要:** 糖尿病视网膜病变（DR）的准确分期对预防视力丧失至关重要，但现有模型缺乏可解释性。本文提出一种新方法，将图表示学习与视觉语言模型（VLMs）结合，实现可解释的 DR 诊断。方法利用光学相干断层扫描血管成像（OCTA）图像构建生物学图谱，编码血管特征。图神经网络（GNN）进行 DR 分期，同时识别关键节点/边及其特征。这些基于图的知识被转化为文本描述，用于指导微调 VLM。最终模型能基于单张图像进行分类并以人类可理解的方式解释其决策。\n- **TLDR:** 结合 GNN 和 VLM，利用 OCTA 图像构建的图知识来微调 VLM，实现了可解释的糖尿病视网膜病变分期诊断。\n\n**12. 沉默品牌攻击：针对文本到图像扩散模型的无触发器数据投毒攻击 (Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models)**\n\n- **作者:** Sangwon Jang, June Suk Choi, et al.\n- **摘要:** 文本到图像扩散模型易受数据投毒攻击。本文提出“沉默品牌攻击”，一种新型数据投毒方法，使模型在生成图像时自然包含特定品牌 logo 或符号，无需任何文本触发器。研究发现，当特定视觉模式在训练数据中重复出现时，模型会学会在输出中自然再现它们。作者开发了一种自动投毒算法，将 logo 不引人注意地注入原始图像。在 poisoned 数据集上训练的模型生成的图像会包含 logo，且不降低图像质量或文本对齐度。\n- **TLDR:** 提出一种新型数据投毒攻击“沉默品牌攻击”，能在无需文本触发的情况下，让文本到图像模型生成的图片中悄悄嵌入品牌 logo。\n\n**13. 视觉语言注意力蒸馏用于动态环境中具有社会意识的机器人导航 (Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments)**\n\n- **作者:** Mohamed Elnoor, Kasun Weerakoon, et al.\n- **摘要:** 提出 Vi-LAD，一种从大型视觉语言模型（VLM）中蒸馏社会合规导航知识到轻量级 transformer 模型的新方法，用于实时机器人导航。Vi-LAD 在中间层表示（注意力图）级别进行知识蒸馏和微调，利用预训练视觉-动作模型的骨干网络。注意力图突出关键导航区域，作为社会感知运动规划的隐式指导。Vi-LAD 结合了从预训练模型提取的注意力图和从 VLM 构建的类注意力语义图，通过新的注意力级蒸馏损失融合知识，生成增强社会意识的注意力图，并用于模型预测控制器（MPC）进行导航。\n- **TLDR:** Vi-LAD 通过蒸馏 VLM 的注意力信息，为机器人导航模型赋予社会意识，提高其在动态环境中的导航成功率和合规性。\n\n**14. ReMA：利用多智能体强化学习为 LLMs 学习元思考 (ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning)**\n\n- **作者:** Ziyu Wan, Yunxiang Li, et al.\n- **摘要:** 为提升 LLM 的推理性能，研究者探索了元思考（监控、评估和控制推理过程）。现有单智能体方法缺乏专门设计。本文提出强化元思考智能体（ReMA），一个利用多智能体强化学习（MARL）引导 LLM 进行元思考的新框架。ReMA 将推理过程解耦为负责策略监督和计划的高层元思考智能体，以及负责细节执行的低层推理智能体。通过迭代强化学习，智能体协同提升泛化性和鲁棒性。实验表明 ReMA 在复杂推理任务上优于单智能体 RL 基线。\n- **TLDR:** 提出 ReMA 框架，利用多智能体强化学习训练 LLM 进行“元思考”（思考如何思考），将推理过程分层，提升复杂推理任务性能。\n\n**15. MindGYM：通过合成自我挑战问题增强视觉语言模型 (MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions)**\n\n- **作者:** Zhe Xu, Daoyuan Chen, et al.\n- **摘要:** 大型视觉语言模型（VLMs）在获得鲁棒、可迁移的推理能力方面面临挑战。本文提出 MindGYM 框架，通过合成自我挑战问题来增强 VLM。该框架包括三个阶段：(1) 种子单跳问题合成，生成跨文本和多模态上下文的认知问题；(2) 挑战性多跳问题合成，组合种子问题创建需要更深推理的多步问题；(3) 思维引导课程微调，逐步训练模型。MindGYM 利用模型的自合成能力，实现了高数据效率、计算效率和鲁棒泛化。\n- **TLDR:** MindGYM 框架让 VLM 自己生成从易到难的挑战性问题来训练自己，有效提升了模型的推理能力，同时降低了对人工标注和计算资源的需求。\n\n---\n\n**其他简报:**\n\n*   **#1 AI 对抗作为一门手艺：抵制和拥抱生成式 AI 如何重塑写作行业 (AI Rivalry as a Craft: How Resisting and Embracing Generative AI Reshape Writing Professions):** 通过访谈研究写作专业人士如何通过抵制或拥抱生成式 AI 来重塑自己的工作角色和实践 (Job Crafting)。\n*   **#2 基于规则的临床文本共指消解解决方案 (A Rule Based Solution to Co-reference Resolution in Clinical Text):** 针对生物医学领域，构建了一个基于手动规则的共指消解系统，并在 i2b2 数据集上取得了较好性能 (89.6%)。\n*   **#3 CleverDistiller：简单且空间一致的跨模态蒸馏 (CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation):** 提出 CleverDistiller，一个简单有效的自监督跨模态 2D 到 3D 知识蒸馏框架，用于将视觉基础模型 (VFM) 的能力迁移到 3D LiDAR 模型，在自动驾驶基准上取得 SOTA。\n*   **#4 媒体与负责任的 AI 治理：博弈论和 LLM 分析 (Media and responsible AI governance: a game-theoretic and LLM analysis):** 使用演化博弈论和 LLM 分析 AI 开发者、监管者、用户和媒体之间的互动，探讨媒体在促进可信 AI 中的作用，发现媒体可作为“软”监管。\n*   **#5 屏幕后的你是谁？使用人工智能进行内隐 MBTI 和性别检测 (Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence):** 使用 RoBERTa 模型从 Telegram 对话数据中隐式检测用户的 MBTI 性格类型和性别，取得了不错的准确率。\n*   **#6 通过虚拟观察者界面提高透明度来训练人机团队 (Training Human-Robot Teams by Improving Transparency Through a Virtual Spectator Interface):** 提出虚拟观察者界面 (VSI) 作为事后回顾 (AAR) 工具，用于提升人机团队在模拟搜索任务中的表现和态势感知能力 (SA)。\n*   **#7 视觉语言模型在理解图像变换方面的局限性 (On the Limitations of Vision-Language Models in Understanding Image Transforms):** 研究发现 CLIP 和 SigLIP 等 VLM 对基本的图像变换（如旋转、裁剪等）缺乏理解，并探讨了这对下游任务（如图像编辑）的影响。\n*   **#8 用于低资源语言尼泊尔语命名实体识别的生成式 AI (Generative AI for Named Entity Recognition in Low-Resource Language Nepali):** 探索使用 SOTA LLMs 进行尼泊尔语 NER，评估了不同提示技术的效果，为低资源语言 NLP 提供见解。\n*   **#17 用扩散先验和离策略 RL 解决贝叶斯逆问题 (Solving Bayesian inverse problems with diffusion priors and off-policy RL):** 将离策略 RL 目标 RTB 应用于训练条件扩散模型后验，用于解决视觉和科学中的挑战性逆问题。\n*   **#19 CALLM：使用 LLMs 和检索增强移动日记对癌症幸存者进行情境感知情绪分析 (CALLM: Context-Aware Emotion Analysis in Cancer Survivors Using LLMs and Retrieval-Augmented Mobile Diaries):** 提出 CALLM 框架，利用 LLM 和 RAG 分析癌症幸存者的移动日记条目，结合同伴经验和个人情绪轨迹进行情境感知的情绪状态预测。\n*   **#20 通过验证器在环的局部前瞻指导进行自动定理证明 (Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving):** 提出一种新颖的验证器在环设计，利用 Lean 等自动验证器在定理证明的每一步提供中间反馈，以提高模型的推理准确性和效率。\n*   **#21 保形预测与人类决策 (Conformal Prediction and Human Decision Making):** 从决策理论角度探讨保形预测集作为决策支持工具的价值和局限性，分析其与人类决策目标和策略的关系。\n*   **#22 寻找缪斯：通过损失轨迹识别核心集 (Finding the Muses: Identifying Coresets through Loss Trajectories):** 提出损失轨迹相关性 (LTC) 指标，用于高效选择能驱动泛化的关键训练样本（核心集），降低大规模数据集训练的计算和存储开销。\n*   **#23 重温频域中时间序列分类的后门攻击 (Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain):** 分析现有时间序列分类后门攻击的局限性，提出 FreqBack，一种基于频率分析指导的高效触发器生成方法，实现高攻击成功率。\n*   **#24 重温基础模型时代的半监督学习 (Revisiting semi-supervised learning in the era of foundation models):** 研究发现，在视觉基础模型 (VFMs) 上，仅使用标记数据进行参数高效微调 (PEFT) 通常能达到半监督学习 (SSL) 的性能。提出通过集成多种 PEFT 和 VFM 来改进自训练伪标签的质量。\n*   **#25 如何保护自己免受 5G 辐射？调查 LLMs 对隐性误导信息的反应 (How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation):** 构建首个隐性误导信息基准 ECHOMIST，评估 LLM 处理用户查询中隐含错误信息的能力，发现当前模型表现不佳，常无法识别错误前提。\n*   **#26 Auspex：将威胁建模技艺融入基于人工智能的 Copilot (Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot):** 提出 Auspex，一个利用生成式 AI 和“技艺提示 (tradecraft prompting)”进行威胁建模的系统，旨在自动化和标准化威胁建模过程。\n*   **#27 长上下文 LLMs 的成本最优分组查询注意力 (Cost-Optimal Grouped-Query Attention for Long-Context LLMs):** 系统比较了不同参数大小、上下文长度和注意力头配置对 LLM 性能、计算和内存成本的影响，发现在处理长序列时，参数更多但头更少的模型可能更优。\n*   **#28 块扩散：在自回归和扩散语言模型之间插值 (Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models):** 提出块扩散语言模型，结合了离散去噪扩散和自回归模型的优点，支持灵活长度生成，并通过 KV 缓存和并行采样提高推理效率。\n*   **#30 L 层无限宽神经网络在 μP 参数化下的全局收敛和丰富特征学习 (Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $μ$P Parametrization):** 理论证明在 $\\mu$P 参数化下，使用 SGD 训练的无限宽深度神经网络能够学习到线性独立的、显著偏离初始值的特征，并保证全局收敛。\n*   **#34 GenHPE：用于射频信号 3D 人体姿态估计的生成式反事实 (GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals):** 提出 GenHPE，通过生成反事实射频信号来消除特定领域混淆因素（如个体和环境差异），提高基于 RF 信号的 3D 人体姿态估计的跨域泛化能力。\n*   **#40 RESTRAIN：基于强化学习的触发-动作物联网环境安全框架 (RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment):** 提出 RESTRAIN，一个基于多智能体强化学习的在线防御系统，用于在运行时对抗物联网环境中的远程注入攻击。\n*   **#41 双阶段特征级基于聚类的专家混合框架 (Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework):** 提出 DFCP-MoE 框架，通过输入特征提取、特征级聚类和伪标签策略改进专家混合 (MoE) 模型训练，减少噪声影响并提升专家特化。\n*   **#44 在线语言溅射 (Online Language Splatting):** 提出首个在线、近实时的开放词汇语言映射框架 Online Language Splatting，集成到 3DGS-SLAM 系统中，无需预生成语言特征。\n*   **#45 稀疏自编码器作为零样本分类器用于文本到图像扩散模型中的概念擦除 (Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models):** 提出 Interpret then Deactivate (ItD) 框架，使用稀疏自编码器 (SAE) 解释概念并识别相关特征，通过禁用这些特征实现精确的概念擦除，同时保持模型性能。\n*   **#48 CASTLE：面向 CWE 检测的静态代码分析器和 LLMs 基准数据集 (CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection):** 推出 CASTLE 基准框架，包含 250 个覆盖 25 种常见 CWE 的微基准程序，用于评估静态分析工具、LLM 和形式验证工具的漏洞检测能力。\n*   **#51 多智能体图像恢复 (Multi-Agent Image Restoration):** 提出 MAIR，一种用于复杂图像恢复的多智能体方法，模拟协作专家团队（调度员+多个专家），按场景、成像、压缩的逆序分阶段处理混合退化，提高效果和效率。\n*   **#52 ForAug：重组前景和背景以改进视觉 Transformer 训练并缓解偏差 (ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation):** 提出 ForAug 数据增强方案，利用基础模型分离前景和背景并重新组合，增加数据多样性，显著提升 ViT 等模型在 ImageNet 和下游任务上的准确率，并减少背景、中心、尺寸等偏差。\n*   **#53 Close-up-GS：通过渐进式自训练增强 3D 高斯溅射中的特写视图合成 (Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training):** 针对 3DGS 在生成显著靠近物体的特写视图时质量下降的问题，提出一种渐进式自训练方法，结合 3D 感知生成模型 See3D 增强细节，并逐步扩展信任区域进行微调。\n*   **#54 面向下一代推荐系统：使用 LLMs 的个性化推荐助手基准 (Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs):** 推出 RecBench+ 数据集基准，包含多样化、不同难度的真实用户查询，用于评估 LLM 作为个性化推荐助手处理复杂需求的能力。\n*   **#59 RetSTA：一种基于 LLM 的临床眼底图像报告标准化方法 (RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports):** 为解决临床眼底诊断报告缺乏统一标准的问题，构建了双语标准术语库，并训练了 RetSTA-7B 模型，实现了报告级的双语标准化。\n*   **#61 MOAT：评估 LMM 的能力整合与指令遵循 (MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding):** (重复提及，见 #5)\n*   **#63 NVP-HRI：通过大型语言模型的零样本自然语音和姿态人机交互 (NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model):** 提出 NVP-HRI，结合语音命令和指示性姿态的多模态 HRI 范式，利用 SAM 进行视觉分析，LLM 理解多模态命令，实现与未知物体的零样本交互。\n*   **#64 CyberLLMInstruct：用于分析使用网络安全数据微调的 LLMs 安全性的新数据集 (CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data):** 开发了 CyberLLMInstruct 数据集（包含 5.4 万+指令-响应对），涵盖网络安全任务，用于评估 LLM 微调后的安全性，发现微调会降低模型对对抗攻击的抵抗力。\n*   **#66 群体鲁棒的机器学习反学习 (Group-robust Machine Unlearning):** 提出 MIU 方法，解决机器学习反学习中遗忘数据集中在某个群体时导致的公平性问题，通过重加权和最小化互信息实现群体鲁棒的反学习。\n*   **#67 增强大型语言模型因果推理能力的综述 (A Survey on Enhancing Causal Reasoning Ability of Large Language Models):** 系统回顾了增强 LLM 因果推理能力的研究，总结了挑战、现有方法分类、基准和未来方向。\n*   **#73 UniCombine：使用扩散 Transformer 进行统一多条件组合 (UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer):** 提出 UniCombine，一个基于 DiT 的多条件可控生成框架，能处理文本、空间图、主题图像等条件的任意组合，并引入新的 Conditional MMDiT Attention 机制和 SubjectSpatial200K 数据集。\n*   **#78 计算机代理中的上下文防御：一项实证研究 (In-Context Defense in Computer Agents: An Empirical Study):** 针对 VLM 驱动的计算机代理易受上下文欺骗攻击（如恶意弹窗）的问题，提出“上下文防御”，利用上下文学习和 CoT 推理，通过少量精心设计的防御示例引导代理进行防御性思考，显著降低攻击成功率。\n*   **#79 LREF：一种新颖的基于 LLM 的电子商务相关性框架 (LREF: A Novel LLM-based Relevance Framework for E-commerce):** 提出 LREF 框架，通过数据选择的 SFT、多 CoT 调整和去偏 DPO 三阶段优化 LLM，用于提升电子商务搜索中的查询-商品相关性预测。\n*   **#88 Reangle-A-Video：将 4D 视频生成视为视频到视频的翻译 (Reangle-A-Video: 4D Video Generation as Video-to-Video Translation):** (重复提及，见 #4)\n*   **#97 使用熵融合进行创意应用的零样本以主体为中心的生成 (Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion):** 提出一种基于熵的特征加权融合方法，用于从预训练文本到图像模型（FLUX）中提取精确的主体蒙版，实现以主体为中心、去除无关元素的图像生成，并结合 LLM 代理优化提示。\n*   **#98 计算病理学的多模态基础模型：综述 (Multi-Modal Foundation Models for Computational Pathology: A Survey):** 全面回顾了计算病理学中的多模态基础模型，重点关注基于 H&E 染色 WSI 的模型，将其分为视觉-语言、视觉-知识图谱、视觉-基因表达等范式，并分析了相关数据集、任务和挑战。\n*   **#99 LocAgent：用于代码定位的图引导 LLM 代理 (LocAgent: Graph-Guided LLM Agents for Code Localization):** 提出 LocAgent 框架，将代码库解析为有向异构图，使 LLM 代理能通过图上的多跳推理有效定位需要修改的代码，显著提高代码定位准确率。\n*   **#102 具有逻辑和属性自我反思的长视界视觉指令生成 (Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection):** 提出 LIGER，首个用于生成长序列任务视觉指令的免训练框架，通过逐步生成和利用图像编辑工具进行自我反思（修正逻辑、属性错误等），提高生成指令图像的一致性和准确性。\n*   **#105 Open-Sora 2.0：以20万美元训练商业级视频生成模型 (Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k):** (重复提及，见 #1)\n*   **#108 TreeX：通过关键子树提取生成全局图形化 GNN 解释 (TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction):** 提出 TreeX，通过分析和提取消息传递 GNN 内部产生的关键子树，并进行聚合，来生成局部、类级别和全局的图形化解释。\n*   **#114 生成式智能体社会能否模拟人类行为并为公共卫生政策提供信息？以疫苗犹豫为例 (Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy):** 探索使用 100 个 LLM 驱动的生成式智能体构建 VacSim 框架，模拟社会中个体（特别是关于疫苗犹豫）的健康相关决策，以评估公共卫生干预措施的可行性。\n*   **#116 KNighter：利用 LLM 合成的检查器革新静态分析 (KNighter: Transforming Static Analysis with LLM-Synthesized Checkers):** (重复提及，见 #3)\n*   **#120 JBFuzz：使用 Fuzzing 高效且有效地越狱 LLMs (JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing):** (重复提及，见 #8)\n\n---\n\n今天的快报就到这里，希望对你有所帮助！",
  "papers": [
    {
      "arxiv_id": "2503.09901v1",
      "title": "AI Rivalry as a Craft: How Resisting and Embracing Generative AI Reshape Writing Professions",
      "title_zh": "AI竞争作为一种技艺：抗拒与接纳生成式AI如何重塑写作职业",
      "authors": [
        "Rama Adithya Varanasi",
        "Batia Mishan Wiesenfeld",
        "Oded Nov"
      ],
      "abstract": "Generative AI (GAI) technologies are disrupting professional writing,\nchallenging traditional practices. Recent studies explore GAI adoption\nexperiences of creative practitioners, but we know little about how these\nexperiences evolve into established practices and how GAI resistance alters\nthese practices. To address this gap, we conducted 25 semi-structured\ninterviews with writing professionals who adopted and/or resisted GAI. Using\nthe theoretical lens of Job Crafting, we identify four strategies professionals\nemploy to reshape their roles. Writing professionals employed GAI resisting\nstrategies to maximize human potential, reinforce professional identity, carve\nout a professional niche, and preserve credibility within their networks. In\ncontrast, GAI-enabled strategies allowed writers who embraced GAI to enhance\ndesirable workflows, minimize mundane tasks, and engage in new AI-managerial\nlabor. These strategies amplified their collaborations with GAI while reducing\ntheir reliance on other people. We conclude by discussing implications of GAI\npractices on writers' identity and practices as well as crafting theory.",
      "tldr_zh": "该研究探讨了生成式AI（GAI）如何通过抵制和采纳两种方式重塑写作职业。通过对25位写作专业人士的访谈，研究发现抵制者通过强化专业身份、开拓细分领域等策略来最大化人类价值，而采纳者则利用GAI优化工作流程、减少重复劳动并承担新的AI管理职责。研究揭示了GAI实践对写作者身份认同和工作方式的双重影响，为职业重塑理论（Job Crafting）提供了新见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09901v1",
      "published_date": "2025-03-12 23:43:57 UTC",
      "updated_date": "2025-03-12 23:43:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:39:56.800215"
    },
    {
      "arxiv_id": "2503.09896v1",
      "title": "A Rule Based Solution to Co-reference Resolution in Clinical Text",
      "title_zh": "基于规则的临床文本共指消解解决方案",
      "authors": [
        "Ping Chen",
        "David Hinote",
        "Guoqing Chen"
      ],
      "abstract": "Objective: The aim of this study was to build an effective co-reference\nresolution system tailored for the biomedical domain. Materials and Methods:\nExperiment materials used in this study is provided by the 2011 i2b2 Natural\nLanguage Processing Challenge. The 2011 i2b2 challenge involves coreference\nresolution in medical documents. Concept mentions have been annotated in\nclinical texts, and the mentions that co-refer in each document are to be\nlinked by coreference chains. Normally, there are two ways of constructing a\nsystem to automatically discover co-referent links. One is to manually build\nrules for co-reference resolution, and the other category of approaches is to\nuse machine learning systems to learn automatically from training datasets and\nthen perform the resolution task on testing datasets. Results: Experiments show\nthe existing co-reference resolution systems are able to find some of the\nco-referent links, and our rule based system performs well finding the majority\nof the co-referent links. Our system achieved 89.6% overall performance on\nmultiple medical datasets. Conclusion: The experiment results show that\nmanually crafted rules based on observation of training data is a valid way to\naccomplish high performance in this coreference resolution task for the\ncritical biomedical domain.",
      "tldr_zh": "本研究针对生物医学领域构建了一个高效的共指消解系统。通过分析2011年i2b2自然语言处理挑战赛提供的临床文本数据，开发了基于人工规则的解决方案。实验表明，该规则系统在多个医学数据集上实现了89.6%的整体性能，优于现有共指消解系统，验证了基于训练数据观察的手工规则方法在关键生物医学领域的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09896v1",
      "published_date": "2025-03-12 23:29:08 UTC",
      "updated_date": "2025-03-12 23:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:39:58.990158"
    },
    {
      "arxiv_id": "2503.09878v1",
      "title": "CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation",
      "title_zh": "CleverDistiller：简单且空间一致的跨模态蒸馏",
      "authors": [
        "Hariprasath Govindarajan",
        "Maciej K. Wozniak",
        "Marvin Klingner",
        "Camille Maurice",
        "B Ravi Kiran",
        "Senthil Yogamani"
      ],
      "abstract": "Vision foundation models (VFMs) such as DINO have led to a paradigm shift in\n2D camera-based perception towards extracting generalized features to support\nmany downstream tasks. Recent works introduce self-supervised cross-modal\nknowledge distillation (KD) as a way to transfer these powerful generalization\ncapabilities into 3D LiDAR-based models. However, they either rely on highly\ncomplex distillation losses, pseudo-semantic maps, or limit KD to features\nuseful for semantic segmentation only. In this work, we propose\nCleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework\nintroducing a set of simple yet effective design choices: Unlike contrastive\napproaches relying on complex loss design choices, our method employs a direct\nfeature similarity loss in combination with a multi layer perceptron (MLP)\nprojection head to allow the 3D network to learn complex semantic dependencies\nthroughout the projection. Crucially, our approach does not depend on\npseudo-semantic maps, allowing for direct knowledge transfer from a VFM without\nexplicit semantic supervision. Additionally, we introduce the auxiliary\nself-supervised spatial task of occupancy prediction to enhance the semantic\nknowledge, obtained from a VFM through KD, with 3D spatial reasoning\ncapabilities. Experiments on standard autonomous driving benchmarks for\n2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art\nperformance in both semantic segmentation and 3D object detection (3DOD) by up\nto 10% mIoU, especially when fine tuning on really low data amounts, showing\nthe effectiveness of our simple yet powerful KD strategy",
      "tldr_zh": "该研究提出了CleverDistiller，一种简单且空间一致的自监督跨模态蒸馏框架，用于将2D视觉基础模型（如DINO）的泛化能力迁移到3D LiDAR模型中。该方法通过直接特征相似性损失和多层感知机（MLP）投影头，避免了复杂的对比损失设计和伪语义映射依赖，同时引入占用预测任务增强3D空间推理能力。实验表明，CleverDistiller在语义分割和3D目标检测任务上均达到最先进性能，尤其在低数据量微调场景下表现显著。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09878v1",
      "published_date": "2025-03-12 22:18:29 UTC",
      "updated_date": "2025-03-12 22:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:06.218560"
    },
    {
      "arxiv_id": "2503.09858v1",
      "title": "Media and responsible AI governance: a game-theoretic and LLM analysis",
      "title_zh": "媒体与负责任AI治理：基于博弈论与大语言模型的分析",
      "authors": [
        "Nataliya Balabanova",
        "Adeela Bashir",
        "Paolo Bova",
        "Alessio Buscemi",
        "Theodor Cimpeanu",
        "Henrique Correia da Fonseca",
        "Alessandro Di Stefano",
        "Manh Hong Duong",
        "Elias Fernandez Domingos",
        "Antonio Fernandes",
        "The Anh Han",
        "Marcus Krellner",
        "Ndidi Bianca Ogbo",
        "Simon T. Powers",
        "Daniele Proverbio",
        "Fernando P. Santos",
        "Zia Ush Shamszaman",
        "Zhao Song"
      ],
      "abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "tldr_zh": "本文通过演化博弈论和大语言模型(LLMs)分析了AI开发者、监管机构、用户和媒体在构建可信AI系统中的复杂互动。研究发现，媒体在提供信息、调查开发者和监管者方面扮演着关键角色，可作为一种“软”监管机制，补充制度性AI监管的不足。研究揭示了有效监管和可信AI发展的条件，强调了从演化博弈论角度考虑不同监管机制的重要性，并指出有效治理需要管理高质量评论的激励和成本。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.MA",
        "nlin.CD"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09858v1",
      "published_date": "2025-03-12 21:39:38 UTC",
      "updated_date": "2025-03-12 21:39:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:17.410057"
    },
    {
      "arxiv_id": "2503.09853v2",
      "title": "Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence",
      "title_zh": "屏幕背后的你是谁？利用人工智能进行隐性MBTI与性别检测",
      "authors": [
        "Kourosh Shahnazari",
        "Seyed Moein Ayyoubzadeh"
      ],
      "abstract": "In personalized technology and psychological research, precisely detecting\ndemographic features and personality traits from digital interactions becomes\never more important. This work investigates implicit categorization, inferring\npersonality and gender variables directly from linguistic patterns in Telegram\nconversation data, while conventional personality prediction techniques mostly\ndepend on explicitly self-reported labels. We refine a Transformer-based\nlanguage model (RoBERTa) to capture complex linguistic cues indicative of\npersonality traits and gender differences using a dataset comprising 138,866\nmessages from 1,602 users annotated with MBTI types and 195,016 messages from\n2,598 users annotated with gender. Confidence levels help to greatly raise\nmodel accuracy to 86.16\\%, hence proving RoBERTa's capacity to consistently\nidentify implicit personality types from conversational text data. Our results\nhighlight the usefulness of Transformer topologies for implicit personality and\ngender classification, hence stressing their efficiency and stressing important\ntrade-offs between accuracy and coverage in realistic conversational\nenvironments. With regard to gender classification, the model obtained an\naccuracy of 74.4\\%, therefore capturing gender-specific language patterns.\nPersonality dimension analysis showed that people with introverted and\nintuitive preferences are especially more active in text-based interactions.\nThis study emphasizes practical issues in balancing accuracy and data coverage\nas Transformer-based models show their efficiency in implicit personality and\ngender prediction tasks from conversational texts.",
      "tldr_zh": "该研究基于Transformer语言模型(RoBERTa)，从Telegram聊天数据中隐式推断用户的MBTI人格类型和性别。研究使用包含138,866条MBTI标注消息和195,016条性别标注消息的数据集，模型在MBTI分类上达到86.16%的准确率，性别分类准确率为74.4%。结果表明，内向型和直觉型人格在文本互动中更为活跃，同时强调了Transformer模型在隐式人格和性别预测任务中的高效性，以及在实际应用中平衡准确性与数据覆盖范围的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09853v2",
      "published_date": "2025-03-12 21:24:22 UTC",
      "updated_date": "2025-03-14 23:59:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:18.806473"
    },
    {
      "arxiv_id": "2503.09849v1",
      "title": "Training Human-Robot Teams by Improving Transparency Through a Virtual Spectator Interface",
      "title_zh": "通过虚拟观察者界面提升透明度训练人机协作团队",
      "authors": [
        "Sean Dallas",
        "Hongjiao Qiang",
        "Motaz AbuHijleh",
        "Wonse Jo",
        "Kayla Riegner",
        "Jon Smereka",
        "Lionel Robert",
        "Wing-Yue Louie",
        "Dawn M. Tilbury"
      ],
      "abstract": "After-action reviews (AARs) are professional discussions that help operators\nand teams enhance their task performance by analyzing completed missions with\npeers and professionals. Previous studies that compared different formats of\nAARs have mainly focused on human teams. However, the inclusion of robotic\nteammates brings along new challenges in understanding teammate intent and\ncommunication. Traditional AAR between human teammates may not be satisfactory\nfor human-robot teams. To address this limitation, we propose a new training\nreview (TR) tool, called the Virtual Spectator Interface (VSI), to enhance\nhuman-robot team performance and situational awareness (SA) in a simulated\nsearch mission. The proposed VSI primarily utilizes visual feedback to review\nsubjects' behavior. To examine the effectiveness of VSI, we took elements from\nAAR to conduct our own TR, designed a 1 x 3 between-subjects experiment with\nexperimental conditions: TR with (1) VSI, (2) screen recording, and (3)\nnon-technology (only verbal descriptions). The results of our experiments\ndemonstrated that the VSI did not result in significantly better team\nperformance than other conditions. However, the TR with VSI led to more\nimprovement in the subjects SA over the other conditions.",
      "tldr_zh": "该研究提出了一种新型训练评估工具——虚拟观察者界面（VSI），旨在提升人机团队在模拟搜索任务中的表现和情境感知能力。通过可视化反馈机制，VSI弥补了传统事后回顾（AAR）方法在理解机器人队友意图方面的不足。实验结果表明，虽然VSI未显著提升团队整体表现，但在改善操作员情境感知能力方面优于屏幕录像和纯口头描述等传统评估方式。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO",
        "H.5.2; I.2.9"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 4 figures, Accepted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09849v1",
      "published_date": "2025-03-12 21:13:34 UTC",
      "updated_date": "2025-03-12 21:13:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:35.729568"
    },
    {
      "arxiv_id": "2503.09837v2",
      "title": "On the Limitations of Vision-Language Models in Understanding Image Transforms",
      "title_zh": "视觉语言模型在理解图像变换方面的局限性",
      "authors": [
        "Ahmad Mustafa Anis",
        "Hasnain Ali",
        "Saquib Sarfraz"
      ],
      "abstract": "Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.",
      "tldr_zh": "该研究揭示了视觉语言模型(VLMs)在理解图像变换方面的局限性。通过对OpenAI的CLIP和Google的SigLIP等主流模型的测试，发现它们在处理多种图像增强变换时表现不佳。为此，研究者构建了一个基于Flickr8k数据集的增强版本，用于评估模型对图像变换的理解能力。研究还探讨了这种缺陷对图像编辑等下游任务的影响，并对当前最先进的Image2Image模型在简单变换上的性能进行了评估。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "I.4; I.2.10; I.2.7"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 15 images",
      "pdf_url": "http://arxiv.org/pdf/2503.09837v2",
      "published_date": "2025-03-12 20:58:16 UTC",
      "updated_date": "2025-03-14 01:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:42.661486"
    },
    {
      "arxiv_id": "2503.09822v1",
      "title": "Generative AI for Named Entity Recognition in Low-Resource Language Nepali",
      "title_zh": "生成式人工智能在低资源语言尼泊尔语命名实体识别中的应用",
      "authors": [
        "Sameer Neupane",
        "Jeevan Chapagain",
        "Nobal B. Niraula",
        "Diwa Koirala"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI), particularly Large Language\nModels (LLMs), has significantly advanced Natural Language Processing (NLP)\ntasks, such as Named Entity Recognition (NER), which involves identifying\nentities like person, location, and organization names in text. LLMs are\nespecially promising for low-resource languages due to their ability to learn\nfrom limited data. However, the performance of GenAI models for Nepali, a\nlow-resource language, has not been thoroughly evaluated. This paper\ninvestigates the application of state-of-the-art LLMs for Nepali NER,\nconducting experiments with various prompting techniques to assess their\neffectiveness. Our results provide insights into the challenges and\nopportunities of using LLMs for NER in low-resource settings and offer valuable\ncontributions to the advancement of NLP research in languages like Nepali.",
      "tldr_zh": "本研究评估了生成式人工智能（GenAI）和大语言模型（LLMs）在低资源语言尼泊尔语（Nepali）命名实体识别（NER）中的应用。通过实验不同提示技术，研究发现LLMs能够有效处理尼泊尔语中的实体识别任务，为低资源语言的NLP研究提供了重要参考。该工作揭示了LLMs在低资源语言NER中的潜力与挑战，推动了相关领域的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted in the FLAIRS Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09822v1",
      "published_date": "2025-03-12 20:40:09 UTC",
      "updated_date": "2025-03-12 20:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:40.554661"
    },
    {
      "arxiv_id": "2503.09820v1",
      "title": "Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot Navigation in Dynamic Environments",
      "title_zh": "Vi-LAD：基于视觉-语言注意力蒸馏的动态环境社会感知机器人导航",
      "authors": [
        "Mohamed Elnoor",
        "Kasun Weerakoon",
        "Gershom Seneviratne",
        "Jing Liang",
        "Vignesh Rajagopal",
        "Dinesh Manocha"
      ],
      "abstract": "We introduce Vision-Language Attention Distillation (Vi-LAD), a novel\napproach for distilling socially compliant navigation knowledge from a large\nVision-Language Model (VLM) into a lightweight transformer model for real-time\nrobotic navigation. Unlike traditional methods that rely on expert\ndemonstrations or human-annotated datasets, Vi-LAD performs knowledge\ndistillation and fine-tuning at the intermediate layer representation level\n(i.e., attention maps) by leveraging the backbone of a pre-trained\nvision-action model. These attention maps highlight key navigational regions in\na given scene, which serve as implicit guidance for socially aware motion\nplanning. Vi-LAD fine-tunes a transformer-based model using intermediate\nattention maps extracted from the pre-trained vision-action model, combined\nwith attention-like semantic maps constructed from a large VLM. To achieve\nthis, we introduce a novel attention-level distillation loss that fuses\nknowledge from both sources, generating augmented attention maps with enhanced\nsocial awareness. These refined attention maps are then utilized as a\ntraversability costmap within a socially aware model predictive controller\n(MPC) for navigation. We validate our approach through real-world experiments\non a Husky wheeled robot, demonstrating significant improvements over\nstate-of-the-art (SOTA) navigation methods. Our results show up to 14.2% - 50%\nimprovement in success rate, which highlights the effectiveness of Vi-LAD in\nenabling socially compliant and efficient robot navigation.",
      "tldr_zh": "本文提出Vi-LAD（视觉-语言注意力蒸馏）方法，通过从大型视觉语言模型(VLM)中提取社交导航知识，并蒸馏到轻量级Transformer模型中，实现动态环境下的机器人社交导航。该方法创新性地在中间层注意力图层面进行知识蒸馏，结合预训练视觉动作模型的注意力图和VLM构建的语义图，通过新型注意力蒸馏损失函数生成具有增强社交意识的注意力图。实验表明，该方法在Husky轮式机器人上实现了14.2%-50%的成功率提升，显著优于现有最优导航方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09820v1",
      "published_date": "2025-03-12 20:38:23 UTC",
      "updated_date": "2025-03-12 20:38:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:40:53.188785"
    },
    {
      "arxiv_id": "2503.09817v1",
      "title": "Temporal Difference Flows",
      "title_zh": "时间差分流",
      "authors": [
        "Jesse Farebrother",
        "Matteo Pirotta",
        "Andrea Tirinzoni",
        "Rémi Munos",
        "Alessandro Lazaric",
        "Ahmed Touati"
      ],
      "abstract": "Predictive models of the future are fundamental for an agent's ability to\nreason and plan. A common strategy learns a world model and unrolls it\nstep-by-step at inference, where small errors can rapidly compound. Geometric\nHorizon Models (GHMs) offer a compelling alternative by directly making\npredictions of future states, avoiding cumulative inference errors. While GHMs\ncan be conveniently learned by a generative analog to temporal difference (TD)\nlearning, existing methods are negatively affected by bootstrapping predictions\nat train time and struggle to generate high-quality predictions at long\nhorizons. This paper introduces Temporal Difference Flows (TD-Flow), which\nleverages the structure of a novel Bellman equation on probability paths\nalongside flow-matching techniques to learn accurate GHMs at over 5x the\nhorizon length of prior methods. Theoretically, we establish a new convergence\nresult and primarily attribute TD-Flow's efficacy to reduced gradient variance\nduring training. We further show that similar arguments can be extended to\ndiffusion-based methods. Empirically, we validate TD-Flow across a diverse set\nof domains on both generative metrics and downstream tasks including policy\nevaluation. Moreover, integrating TD-Flow with recent behavior foundation\nmodels for planning over pre-trained policies demonstrates substantial\nperformance gains, underscoring its promise for long-horizon decision-making.",
      "tldr_zh": "本文提出了Temporal Difference Flows (TD-Flow)，一种基于概率路径的Bellman方程和流匹配技术的新方法，用于学习长时域的未来状态预测模型（GHMs）。相比现有方法，TD-Flow能够生成超过5倍时域长度的高质量预测，并显著减少训练中的梯度方差。理论分析表明，该方法具有新的收敛性结果，并可扩展至基于扩散的方法。实验验证了TD-Flow在生成指标和下游任务（如策略评估）中的有效性，尤其是在与行为基础模型结合进行长时域决策规划时，表现出显著的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09817v1",
      "published_date": "2025-03-12 20:30:07 UTC",
      "updated_date": "2025-03-12 20:30:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:41:25.880019"
    },
    {
      "arxiv_id": "2503.09808v1",
      "title": "Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis",
      "title_zh": "基于图知识的视觉语言模型微调用于可解释的医学图像分析",
      "authors": [
        "Chenjun Li",
        "Laurin Lux",
        "Alexander H. Berger",
        "Martin J. Menten",
        "Mert R. Sabuncu",
        "Johannes C. Paetzold"
      ],
      "abstract": "Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timely\ninterventions and preventing vision loss. However, current staging models are\nhardly interpretable, and most public datasets contain no clinical reasoning or\ninterpretation beyond image-level labels. In this paper, we present a novel\nmethod that integrates graph representation learning with vision-language\nmodels (VLMs) to deliver explainable DR diagnosis. Our approach leverages\noptical coherence tomography angiography (OCTA) images by constructing\nbiologically informed graphs that encode key retinal vascular features such as\nvessel morphology and spatial connectivity. A graph neural network (GNN) then\nperforms DR staging while integrated gradients highlight critical nodes and\nedges and their individual features that drive the classification decisions. We\ncollect this graph-based knowledge which attributes the model's prediction to\nphysiological structures and their characteristics. We then transform it into\ntextual descriptions for VLMs. We perform instruction-tuning with these textual\ndescriptions and the corresponding image to train a student VLM. This final\nagent can classify the disease and explain its decision in a human\ninterpretable way solely based on a single image input. Experimental\nevaluations on both proprietary and public datasets demonstrate that our method\nnot only improves classification accuracy but also offers more clinically\ninterpretable results. An expert study further demonstrates that our method\nprovides more accurate diagnostic explanations and paves the way for precise\nlocalization of pathologies in OCTA images.",
      "tldr_zh": "本研究提出了一种结合图表示学习和视觉语言模型(VLMs)的新型方法，用于可解释的糖尿病视网膜病变(DR)诊断。该方法通过光学相干断层扫描血管成像(OCTA)构建生物信息图，编码视网膜血管特征，并利用图神经网络(GNN)进行DR分期，同时通过积分梯度突出显示关键节点和特征。研究将图知识转化为文本描述用于VLM指令微调，最终实现仅需单张图像即可同时完成疾病分类和提供人类可理解的决策解释。实验表明，该方法不仅提高了分类准确率，还提供了更具临床可解释性的结果，专家研究证实其能更准确定位OCTA图像中的病理特征。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09808v1",
      "published_date": "2025-03-12 20:19:07 UTC",
      "updated_date": "2025-03-12 20:19:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:13.488570"
    },
    {
      "arxiv_id": "2503.09805v1",
      "title": "Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models",
      "title_zh": "解构生成式人工智能：酷儿艺术家如何揭示并挑战生成式AI模型中的规范性",
      "authors": [
        "Jordan Taylor",
        "Joel Mire",
        "Franchesca Spektor",
        "Alicia DeVrio",
        "Maarten Sap",
        "Haiyi Zhu",
        "Sarah Fox"
      ],
      "abstract": "Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives.",
      "tldr_zh": "这篇论文探讨了LGBTQ+艺术家如何挑战生成式AI模型中的规范性偏见。研究通过13位酷儿艺术家使用GPT-4和DALL-E 3的工作坊发现，这些模型内嵌的\"超积极\"和\"反性\"等规范性价值观限制了创作表达。参与者开发了多种策略克服模型局限，同时仍能在高度规范化的技术中找到价值。基于酷儿女性主义理论，研究对\"最先进\"AI模型的概念提出质疑，并探讨了公平性、问责制和透明度(FAccT)研究如何支持酷儿替代方案。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09805v1",
      "published_date": "2025-03-12 20:16:38 UTC",
      "updated_date": "2025-03-12 20:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:41:46.522576"
    },
    {
      "arxiv_id": "2503.09797v1",
      "title": "SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation using SAM",
      "title_zh": "SeqSAM：基于SAM的医学图像分割自回归多假设预测",
      "authors": [
        "Benjamin Towle",
        "Xin Chen",
        "Ke Zhou"
      ],
      "abstract": "Pre-trained segmentation models are a powerful and flexible tool for\nsegmenting images. Recently, this trend has extended to medical imaging. Yet,\noften these methods only produce a single prediction for a given image,\nneglecting inherent uncertainty in medical images, due to unclear object\nboundaries and errors caused by the annotation tool. Multiple Choice Learning\nis a technique for generating multiple masks, through multiple learned\nprediction heads. However, this cannot readily be extended to producing more\noutputs than its initial pre-training hyperparameters, as the sparse,\nwinner-takes-all loss function makes it easy for one prediction head to become\noverly dominant, thus not guaranteeing the clinical relevancy of each mask\nproduced. We introduce SeqSAM, a sequential, RNN-inspired approach to\ngenerating multiple masks, which uses a bipartite matching loss for ensuring\nthe clinical relevancy of each mask, and can produce an arbitrary number of\nmasks. We show notable improvements in quality of each mask produced across two\npublicly available datasets. Our code is available at\nhttps://github.com/BenjaminTowle/SeqSAM.",
      "tldr_zh": "该研究提出了SeqSAM，一种基于自回归多假设预测的医学图像分割方法，旨在解决现有预训练分割模型在医学图像中仅生成单一预测、忽略不确定性的问题。SeqSAM通过引入顺序生成机制和二分图匹配损失函数，能够生成任意数量的分割掩码，并确保每个掩码的临床相关性。实验表明，该方法在两个公开数据集上显著提升了生成掩码的质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ISBI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09797v1",
      "published_date": "2025-03-12 20:01:52 UTC",
      "updated_date": "2025-03-12 20:01:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:41:55.119860"
    },
    {
      "arxiv_id": "2503.09780v1",
      "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
      "title_zh": "AgentDAM：面向自主网页代理的隐私泄露评估",
      "authors": [
        "Arman Zharmagambetov",
        "Chuan Guo",
        "Ivan Evtimov",
        "Maya Pavlova",
        "Ruslan Salakhutdinov",
        "Kamalika Chaudhuri"
      ],
      "abstract": "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
      "tldr_zh": "该研究提出了AgentDAM，一个用于评估自主网页代理隐私泄露风险的基准测试。AgentDAM模拟真实的网页交互场景，旨在测试AI代理在处理任务时是否遵循数据最小化原则，即仅在必要时访问敏感信息。研究评估了基于GPT-4、Llama-3和Claude的AI代理，发现它们经常不必要地处理敏感信息。此外，研究还提出了一种基于提示的方法，有效减少了此类隐私泄露风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "project page: https://github.com/facebookresearch/ai-agent-privacy",
      "pdf_url": "http://arxiv.org/pdf/2503.09780v1",
      "published_date": "2025-03-12 19:30:31 UTC",
      "updated_date": "2025-03-12 19:30:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:30.068778"
    },
    {
      "arxiv_id": "2503.11711v1",
      "title": "Privacy-Preserved Automated Scoring using Federated Learning for Educational Research",
      "title_zh": "基于联邦学习的隐私保护自动化评分系统在教育研究中的应用",
      "authors": [
        "Ehsan Latif",
        "Xiaoming Zhai"
      ],
      "abstract": "Data privacy remains a critical concern in educational research,\nnecessitating Institutional Review Board (IRB) certification and stringent data\nhandling protocols to ensure compliance with ethical standards. Traditional\napproaches rely on anonymization and controlled data-sharing mechanisms to\nfacilitate research while mitigating privacy risks. However, these methods\nstill involve direct access to raw student data, posing potential\nvulnerabilities and being time-consuming. This study proposes a federated\nlearning (FL) framework for automatic scoring in educational assessments,\neliminating the need to share raw data. Our approach leverages client-side\nmodel training, where student responses are processed locally on edge devices,\nand only optimized model parameters are shared with a central aggregation\nserver. To effectively aggregate heterogeneous model updates, we introduce an\nadaptive weighted averaging strategy, which dynamically adjusts weight\ncontributions based on client-specific learning characteristics. This method\nensures robust model convergence while preserving privacy. We evaluate our\nframework using assessment data from nine middle schools, comparing the\naccuracy of federated learning-based scoring models with traditionally trained\ncentralized models. A statistical significance test (paired t-test, $t(8) =\n2.29, p = 0.051$) confirms that the accuracy difference between the two\napproaches is not statistically significant, demonstrating that federated\nlearning achieves comparable performance while safeguarding student data.\nFurthermore, our method significantly reduces data collection, processing, and\ndeployment overhead, accelerating the adoption of AI-driven educational\nassessments in a privacy-compliant manner.",
      "tldr_zh": "本研究提出了一种基于联邦学习（Federated Learning）的教育测评自动评分框架，通过客户端本地化处理学生作答数据，仅共享模型参数而非原始数据，有效解决教育研究中数据隐私保护问题。该方法采用自适应加权平均策略聚合异构模型更新，在九所中学的测评数据实验表明，其评分准确率与传统集中式训练模型无显著差异（p=0.051），同时显著降低了数据处理和部署成本。该框架为AI驱动的教育评估提供了既保护隐私又保持性能的可行方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to AIED25",
      "pdf_url": "http://arxiv.org/pdf/2503.11711v1",
      "published_date": "2025-03-12 19:06:25 UTC",
      "updated_date": "2025-03-12 19:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:13.090475"
    },
    {
      "arxiv_id": "2503.11710v1",
      "title": "ConjointNet: Enhancing Conjoint Analysis for Preference Prediction with Representation Learning",
      "title_zh": "ConjointNet：通过表征学习增强联合分析在偏好预测中的应用",
      "authors": [
        "Yanxia Zhang",
        "Francine Chen",
        "Shabnam Hakimi",
        "Totte Harinen",
        "Alex Filipowicz",
        "Yan-Ying Chen",
        "Rumen Iliev",
        "Nikos Arechiga",
        "Kalani Murakami",
        "Kent Lyons",
        "Charlene Wu",
        "Matt Klenk"
      ],
      "abstract": "Understanding consumer preferences is essential to product design and\npredicting market response to these new products. Choice-based conjoint\nanalysis is widely used to model user preferences using their choices in\nsurveys. However, traditional conjoint estimation techniques assume simple\nlinear models. This assumption may lead to limited predictability and\ninaccurate estimation of product attribute contributions, especially on data\nthat has underlying non-linear relationships. In this work, we employ\nrepresentation learning to efficiently alleviate this issue. We propose\nConjointNet, which is composed of two novel neural architectures, to predict\nuser preferences. We demonstrate that the proposed ConjointNet models\noutperform traditional conjoint estimate techniques on two preference datasets\nby over 5%, and offer insights into non-linear feature interactions.",
      "tldr_zh": "该研究提出ConjointNet模型，通过表征学习(representation learning)增强联合分析(conjoint analysis)在消费者偏好预测中的应用。针对传统线性模型的局限性，该方法创新性地采用两种新型神经网络架构，能够有效捕捉数据中的非线性特征交互关系。实验表明，ConjointNet在两个偏好数据集上的预测准确率比传统方法提高5%以上，同时提供了对非线性特征交互的深入解析。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11710v1",
      "published_date": "2025-03-12 19:01:59 UTC",
      "updated_date": "2025-03-12 19:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:23.266918"
    },
    {
      "arxiv_id": "2503.09746v1",
      "title": "Solving Bayesian inverse problems with diffusion priors and off-policy RL",
      "title_zh": "利用扩散先验与离策略强化学习求解贝叶斯逆问题",
      "authors": [
        "Luca Scimeca",
        "Siddarth Venkatraman",
        "Moksh Jain",
        "Minsu Kim",
        "Marcin Sendera",
        "Mohsin Hasan",
        "Luke Rowe",
        "Sarthak Mittal",
        "Pablo Lemos",
        "Emmanuel Bengio",
        "Alexandre Adam",
        "Jarrid Rector-Brooks",
        "Yashar Hezaveh",
        "Laurence Perreault-Levasseur",
        "Yoshua Bengio",
        "Glen Berseth",
        "Nikolay Malkin"
      ],
      "abstract": "This paper presents a practical application of Relative Trajectory Balance\n(RTB), a recently introduced off-policy reinforcement learning (RL) objective\nthat can asymptotically solve Bayesian inverse problems optimally. We extend\nthe original work by using RTB to train conditional diffusion model posteriors\nfrom pretrained unconditional priors for challenging linear and non-linear\ninverse problems in vision, and science. We use the objective alongside\ntechniques such as off-policy backtracking exploration to improve training.\nImportantly, our results show that existing training-free diffusion posterior\nmethods struggle to perform effective posterior inference in latent space due\nto inherent biases.",
      "tldr_zh": "本研究提出了一种基于相对轨迹平衡（RTB）的离策略强化学习方法，用于解决贝叶斯反问题。该方法通过训练条件扩散模型后验，从预训练的无条件先验中推断复杂的线性和非线性反问题，并结合离策略回溯探索等技术提升训练效果。实验表明，现有无需训练的后验扩散方法在潜在空间中进行有效后验推断时存在固有偏差，而本文方法显著改善了这一问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as workshop paper at DeLTa workshop, ICLR 2025. arXiv admin\n  note: substantial text overlap with arXiv:2405.20971",
      "pdf_url": "http://arxiv.org/pdf/2503.09746v1",
      "published_date": "2025-03-12 18:45:22 UTC",
      "updated_date": "2025-03-12 18:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:27.019567"
    },
    {
      "arxiv_id": "2503.09737v1",
      "title": "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player Evaluation System",
      "title_zh": "揭示关键隐形核心：基于图神经网络GoalNet的足球运动员评估系统",
      "authors": [
        "Jacky Hao Jiang",
        "Jerry Cai",
        "Anastasios Kyrillidis"
      ],
      "abstract": "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
      "tldr_zh": "本研究提出了GoalNet，一种基于图神经网络(GNN)的足球运动员评估系统，旨在识别比赛中被传统进攻指标忽视的关键球员。该框架通过将空间和时间特征编码到事件中心图中，为预期威胁(xT)的变化分配个体贡献，从而捕捉防守或过渡等非得分动作的价值。研究还探索了多种GNN变体（如图注意力网络和基于Transformer的模型），以处理远程依赖性和动态比赛场景。实验证明，该系统能够有效突出传统指标遗漏的关键角色，为更全面的足球分析提供了实用工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 4-5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09737v1",
      "published_date": "2025-03-12 18:36:55 UTC",
      "updated_date": "2025-03-12 18:36:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:56.919427"
    },
    {
      "arxiv_id": "2503.10707v1",
      "title": "CALLM: Context-Aware Emotion Analysis in Cancer Survivors Using LLMs and Retrieval-Augmented Mobile Diaries",
      "title_zh": "CALLM：基于大语言模型与检索增强移动日记的癌症幸存者情境感知情绪分析",
      "authors": [
        "Zhiyuan Wang",
        "Katharine E. Daniel",
        "Laura E. Barnes",
        "Philip I. Chow"
      ],
      "abstract": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries-short text entries recording through their phone\nabout their emotional experiences-provide a promising method for tracking these\nexperiences in real time. Although emotion analysis tools show potential for\nrecognizing emotions from text, current methods lack the contextual\nunderstanding necessary to accurately interpret the brief, personal narratives\nin mobile diaries. We propose CALLM, a context-aware emotion analysis framework\nthat leverages Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG), to analyze mobile diary entries from cancer survivors to predict their\nemotional states. The framework enhances prediction accuracy beyond existing\nmethods by (1) integrating retrieved peer experiences as contextual examples\nand (2) incorporating individuals' temporal emotional trajectories from their\nmobile diary entries. We collected a large-scale dataset (N=407) of cancer\nsurvivors' mobile ecological momentary assessments (EMAs), which assessed\npositive and negative affect, desire to regulate emotions, social interaction\nquality, and availability for interventions, alongside daily mobile diary\nentries in an open response format regarding what was driving their current\nemotional experience. Results demonstrate strong performance of CALLM, with\nbalanced accuracies reaching 72.96% for positive and 73.29% for negative\naffect, and 73.72% for predicting individual's desire to regulate emotions.\nPost-hoc analysis reveals that leveraging model confidence, encouraging longer\ndiary entries, and incorporating personal ground truth, further enhance\npredictive outcomes. Our findings support the feasibility of deploying\nLLM-powered emotion analysis in chronic health populations and suggest\npromising directions for personalized interventions for cancer survivors.",
      "tldr_zh": "该研究提出了CALLM框架，利用大语言模型(LLMs)和检索增强生成技术(RAG)，结合癌症幸存者的移动日记进行情感分析。CALLM通过整合同伴经验作为上下文示例，并结合个体时间序列情感轨迹，显著提升了情感预测的准确性。实验结果表明，CALLM在预测积极情绪、消极情绪和情绪调节需求方面的平衡准确率分别达到72.96%、73.29%和73.72%。研究证实了LLM驱动的情绪分析在慢性健康人群中的可行性，并为癌症幸存者的个性化干预提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, including 3 figures; appendix: 8 pages with 19 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.10707v1",
      "published_date": "2025-03-12 18:36:41 UTC",
      "updated_date": "2025-03-12 18:36:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:57.908899"
    },
    {
      "arxiv_id": "2503.09730v1",
      "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving",
      "title_zh": "局部前瞻式引导：基于验证器闭环的自动定理证明方法",
      "authors": [
        "Sara Rajaee",
        "Kumar Pratik",
        "Gabriele Cesa",
        "Arash Behboodi"
      ],
      "abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
      "tldr_zh": "该论文提出了一种新型的**验证器循环（verifier-in-the-loop）**方法，用于改进自动定理证明（ATP）中的推理效率。与依赖完整推理轨迹反馈的传统方法不同，该方法通过自动验证器（如Lean）在每一步提供即时反馈，显著降低了计算成本和时间消耗。实验表明，这种局部验证策略能全局提升模型的推理准确性和效率，为AI推理任务提供了一种更高效的解决方案。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICLR 2025 Workshop on Reasoning and Planning for Large\n  Language Models",
      "pdf_url": "http://arxiv.org/pdf/2503.09730v1",
      "published_date": "2025-03-12 18:20:47 UTC",
      "updated_date": "2025-03-12 18:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:42:52.800321"
    },
    {
      "arxiv_id": "2503.11709v2",
      "title": "Conformal Prediction and Human Decision Making",
      "title_zh": "保形预测与人类决策",
      "authors": [
        "Jessica Hullman",
        "Yifan Wu",
        "Dawei Xie",
        "Ziyang Guo",
        "Andrew Gelman"
      ],
      "abstract": "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
      "tldr_zh": "该论文探讨了共形预测(Conformal Prediction)方法在人类决策支持中的应用问题。研究指出，虽然共形预测能提供具有指定覆盖率的预测集合，但其与决策者目标和策略的关系尚不明确。作者提出了一个决策理论框架，用于对比校准概率与共形预测集在决策支持中的表现，并基于人类不确定性决策理论，形式化了决策者使用预测集的可能策略。研究发现，共形预测集与人类-AI决策中的常见需求存在张力，并就此提出了改进预测不确定性量化的研究建议。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11709v2",
      "published_date": "2025-03-12 18:18:09 UTC",
      "updated_date": "2025-03-18 16:16:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:43:17.548889"
    },
    {
      "arxiv_id": "2503.09721v1",
      "title": "Finding the Muses: Identifying Coresets through Loss Trajectories",
      "title_zh": "寻找缪斯：通过损失轨迹识别核心集",
      "authors": [
        "Manish Nagaraj",
        "Deepak Ravikumar",
        "Efstathia Soufleri",
        "Kaushik Roy"
      ],
      "abstract": "Deep learning models achieve state-of-the-art performance across domains but\nface scalability challenges in real-time or resource-constrained scenarios. To\naddress this, we propose Loss Trajectory Correlation (LTC), a novel metric for\ncoreset selection that identifies critical training samples driving\ngeneralization. $LTC$ quantifies the alignment between training sample loss\ntrajectories and validation set loss trajectories, enabling the construction of\ncompact, representative subsets. Unlike traditional methods with computational\nand storage overheads that are infeasible to scale to large datasets, $LTC$\nachieves superior efficiency as it can be computed as a byproduct of training.\nOur results on CIFAR-100 and ImageNet-1k show that $LTC$ consistently achieves\naccuracy on par with or surpassing state-of-the-art coreset selection methods,\nwith any differences remaining under 1%. LTC also effectively transfers across\nvarious architectures, including ResNet, VGG, DenseNet, and Swin Transformer,\nwith minimal performance degradation (<2%). Additionally, LTC offers insights\ninto training dynamics, such as identifying aligned and conflicting sample\nbehaviors, at a fraction of the computational cost of traditional methods. This\nframework paves the way for scalable coreset selection and efficient dataset\noptimization.",
      "tldr_zh": "该研究提出了一种名为**损失轨迹相关性（LTC）**的新方法，用于高效选择核心训练集（coreset）。LTC通过分析训练样本和验证集的损失轨迹相关性，识别出对模型泛化能力至关重要的样本，相比传统方法显著降低了计算和存储开销。实验表明，在CIFAR-100和ImageNet-1k数据集上，LTC在保持准确率（差异<1%）的同时，能有效适配ResNet、VGG等多种架构（性能下降<2%）。该方法不仅能构建紧凑且具代表性的训练子集，还能揭示训练动态特征（如样本间的协同/冲突行为），为可扩展的核心集选择和数据集优化提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09721v1",
      "published_date": "2025-03-12 18:11:16 UTC",
      "updated_date": "2025-03-12 18:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:43:44.082053"
    },
    {
      "arxiv_id": "2503.09712v2",
      "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
      "title_zh": "在频域中重新审视时间序列分类的后门攻击",
      "authors": [
        "Yuanmin Huang",
        "Mi Zhang",
        "Zhaoxiang Wang",
        "Wenxuan Li",
        "Min Yang"
      ],
      "abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.",
      "tldr_zh": "本研究提出FreqBack方法，针对时间序列分类(TSC)模型在频域的后门攻击问题。研究发现现有攻击方法直接借用计算机视觉的触发设计或依赖复杂生成模型，而频域不当扰动是导致攻击失效的主因。通过频域分析指导，FreqBack能在保持干净数据准确率下降小于3%的同时，在5种模型和8个数据集上实现超过90%的攻击成功率。该方法揭示了DNN模型在时间序列数据中捕获频域特征的关键机制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "WWW 2025 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2503.09712v2",
      "published_date": "2025-03-12 18:05:32 UTC",
      "updated_date": "2025-03-15 03:08:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:43:43.299056"
    },
    {
      "arxiv_id": "2503.09707v1",
      "title": "Revisiting semi-supervised learning in the era of foundation models",
      "title_zh": "重访基础模型时代的半监督学习",
      "authors": [
        "Ping Zhang",
        "Zheda Mai",
        "Quang-Huy Nguyen",
        "Wei-Lun Chao"
      ],
      "abstract": "Semi-supervised learning (SSL) leverages abundant unlabeled data alongside\nlimited labeled data to enhance learning. As vision foundation models (VFMs)\nincreasingly serve as the backbone of vision applications, it remains unclear\nhow SSL interacts with these pre-trained models. To address this gap, we\ndevelop new SSL benchmark datasets where frozen VFMs underperform and\nsystematically evaluate representative SSL methods. We make a surprising\nobservation: parameter-efficient fine-tuning (PEFT) using only labeled data\noften matches SSL performance, even without leveraging unlabeled data. This\nmotivates us to revisit self-training, a conceptually simple SSL baseline,\nwhere we use the supervised PEFT model to pseudo-label unlabeled data for\nfurther training. To overcome the notorious issue of noisy pseudo-labels, we\npropose ensembling multiple PEFT approaches and VFM backbones to produce more\nrobust pseudo-labels. Empirical results validate the effectiveness of this\nsimple yet powerful approach, providing actionable insights into SSL with VFMs\nand paving the way for more scalable and practical semi-supervised learning in\nthe era of foundation models.",
      "tldr_zh": "本研究重新审视了基础模型时代的半监督学习(SSL)方法。通过构建新的SSL基准测试数据集，研究发现仅使用标记数据进行参数高效微调(PEFT)即可达到传统SSL性能，无需利用未标记数据。在此基础上，作者提出改进的自训练方法：通过集成多种PEFT策略和视觉基础模型(VFM)骨干网络来生成更可靠的伪标签，从而克服噪声问题。实验证明这种简单而强大的方法有效，为基于VFM的SSL提供了实用见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09707v1",
      "published_date": "2025-03-12 18:01:10 UTC",
      "updated_date": "2025-03-12 18:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:43:44.135960"
    },
    {
      "arxiv_id": "2503.09598v1",
      "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation",
      "title_zh": "如何防范5G辐射危害？探究大语言模型对隐性错误信息的回应",
      "authors": [
        "Ruohao Guo",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.",
      "tldr_zh": "该研究提出了ECHOMIST——首个针对隐式错误信息的评测基准，重点考察大语言模型(LLMs)对嵌入用户查询中的错误假设的应对能力。通过分析GPT-4、Claude和Llama等主流模型，研究发现当前LLMs在识别和反驳隐式错误前提方面表现堪忧，常会强化用户误解而非纠正。这项工作揭示了LLM安全研究中亟需关注隐式错误信息传播的重要问题，并为此开发了包含真实人机对话和社交媒体互动的多源数据集及新评估指标。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09598v1",
      "published_date": "2025-03-12 17:59:18 UTC",
      "updated_date": "2025-03-12 17:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:43:44.853864"
    },
    {
      "arxiv_id": "2503.09586v1",
      "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
      "title_zh": "Auspex：将威胁建模技艺融入基于人工智能的副驾驶系统",
      "authors": [
        "Andrew Crossman",
        "Andrew R. Plummer",
        "Chandra Sekharudu",
        "Deepak Warrier",
        "Mohammad Yekrangian"
      ],
      "abstract": "We present Auspex - a threat modeling system built using a specialized\ncollection of generative artificial intelligence-based methods that capture\nthreat modeling tradecraft. This new approach, called tradecraft prompting,\ncenters on encoding the on-the-ground knowledge of threat modelers within the\nprompts that drive a generative AI-based threat modeling system. Auspex employs\ntradecraft prompts in two processing stages. The first stage centers on\ningesting and processing system architecture information using prompts that\nencode threat modeling tradecraft knowledge pertaining to system decomposition\nand description. The second stage centers on chaining the resulting system\nanalysis through a collection of prompts that encode tradecraft knowledge on\nthreat identification, classification, and mitigation. The two-stage process\nyields a threat matrix for a system that specifies threat scenarios, threat\ntypes, information security categorizations and potential mitigations. Auspex\nproduces formalized threat model output in minutes, relative to the weeks or\nmonths a manual process takes. More broadly, the focus on bespoke tradecraft\nprompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a\nlightweight, flexible, modular, and extensible foundational system capable of\naddressing the complexity, resource, and standardization limitations of both\nexisting manual and automated threat modeling processes. In this connection, we\nestablish the baseline value of Auspex to threat modelers through an evaluation\nprocedure based on feedback collected from cybersecurity subject matter experts\nmeasuring the quality and utility of threat models generated by Auspex on real\nbanking systems. We conclude with a discussion of system performance and plans\nfor enhancements to Auspex.",
      "tldr_zh": "该研究提出了Auspex，一种基于生成式人工智能的威胁建模系统，通过“tradecraft prompting”方法将威胁建模的专业知识编码到提示中。Auspex采用两阶段处理流程：第一阶段通过提示分解和描述系统架构，第二阶段通过提示链式分析威胁识别、分类和缓解措施，最终生成包含威胁场景、类型、信息安全分类及缓解措施的系统威胁矩阵。与传统手动建模相比，Auspex能在几分钟内完成威胁建模，且其轻量、灵活、模块化的设计解决了现有手动和自动化威胁建模的复杂性、资源和标准化问题。通过基于真实银行系统的评估，Auspex生成的威胁模型在质量和实用性上获得了网络安全专家的认可。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09586v1",
      "published_date": "2025-03-12 17:54:18 UTC",
      "updated_date": "2025-03-12 17:54:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:43:56.923840"
    },
    {
      "arxiv_id": "2503.09579v1",
      "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
      "title_zh": "长上下文大语言模型的成本优化分组查询注意力机制",
      "authors": [
        "Yingfa Chen",
        "Yutong Wu",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
      "tldr_zh": "本研究系统分析了Transformer大语言模型(LLMs)在参数规模、上下文长度和注意力头配置（grouped-query attention中查询头与键值头的数量）对模型性能、计算成本和内存成本的影响。研究扩展了现有的仅基于参数规模和训练计算的缩放方法，为构建训练和推理阶段成本最优的LLMs提供指导。结果表明，在处理长序列时，参数规模更大但注意力头更少的模型能以更低的计算和内存成本实现更优性能。这一发现为开发实用性LLMs，尤其是在长上下文处理场景中，提供了重要参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09579v1",
      "published_date": "2025-03-12 17:50:42 UTC",
      "updated_date": "2025-03-12 17:50:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:44:22.785097"
    },
    {
      "arxiv_id": "2503.09573v2",
      "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
      "title_zh": "Block Diffusion：在自回归与扩散语言模型之间插值",
      "authors": [
        "Marianne Arriola",
        "Aaron Gokaslan",
        "Justin T Chiu",
        "Zhihan Yang",
        "Zhixuan Qi",
        "Jiaqi Han",
        "Subham Sekhar Sahoo",
        "Volodymyr Kuleshov"
      ],
      "abstract": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
      "tldr_zh": "本文提出了Block Diffusion模型，它结合了自回归模型和扩散模型的优势，解决了扩散模型在似然建模和固定长度生成上的局限性。通过支持灵活长度生成、利用KV缓存和并行token采样，该模型显著提升了推理效率。研究还提出了一套高效训练算法、梯度方差估计器和数据驱动的噪声调度方法，以最小化方差。Block Diffusion在语言建模基准测试中达到了新的SOTA性能，并实现了任意长度序列的生成。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
      "pdf_url": "http://arxiv.org/pdf/2503.09573v2",
      "published_date": "2025-03-12 17:43:40 UTC",
      "updated_date": "2025-03-18 15:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:44:06.855777"
    },
    {
      "arxiv_id": "2503.09567v2",
      "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
      "title_zh": "迈向推理时代：大语言模型长链式思维推理研究综述",
      "authors": [
        "Qiguang Chen",
        "Libo Qin",
        "Jinhao Liu",
        "Dengyun Peng",
        "Jiannan Guan",
        "Peng Wang",
        "Mengkang Hu",
        "Yuhang Zhou",
        "Te Gao",
        "Wanxiang Che"
      ],
      "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to\nfill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and test-time scaling, offering\ninsights into how these processes manifest in practice. (4) Finally, we\nidentify significant research gaps and highlight promising future directions,\nincluding the integration of multi-modal reasoning, efficiency improvements,\nand enhanced knowledge frameworks. By providing a structured overview, this\nsurvey aims to inspire future research and further the development of logical\nreasoning in artificial intelligence.",
      "tldr_zh": "本文对长链式思维推理(Long CoT)在大语言模型(LLMs)中的应用进行了系统性综述，旨在填补该领域的知识空白。研究首先区分了Long CoT与传统短链式思维推理(Short CoT)，并提出了新的推理范式分类法；其次，深入探讨了Long CoT的三大特征：深度推理、广泛探索和可行反思，这些特征使其能够处理更复杂的任务并生成更高效、连贯的结果；此外，还分析了Long CoT中的关键现象，如“过度思考”和“测试时间扩展”，并指出了未来研究方向，包括多模态推理整合、效率提升和知识框架优化。该综述为人工智能逻辑推理的进一步发展提供了结构化视角和启发。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper are available at https://long-cot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.09567v2",
      "published_date": "2025-03-12 17:35:03 UTC",
      "updated_date": "2025-03-13 04:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:44:44.317087"
    },
    {
      "arxiv_id": "2503.09565v1",
      "title": "Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $μ$P Parametrization",
      "title_zh": "$L$层无限宽度神经网络在$μ$P参数化下的全局收敛性与丰富特征学习",
      "authors": [
        "Zixiang Chen",
        "Greg Yang",
        "Qingyue Zhao",
        "Quanquan Gu"
      ],
      "abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
      "tldr_zh": "该论文研究了无限宽度$L$层神经网络的训练动态性，采用Maximal Update参数化($μ$P)和随机梯度下降(SGD)。研究发现，在适当激活函数条件下，SGD能使网络学习到与初始值显著偏离的线性无关特征，这些丰富特征能有效捕捉数据信息。理论分析表明，任何训练收敛点都是全局最小值，这通过跨层特征交互和高斯随机变量特性得以证明。实验在真实数据集上验证了该理论框架，为深度表征学习提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 5 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09565v1",
      "published_date": "2025-03-12 17:33:13 UTC",
      "updated_date": "2025-03-12 17:33:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:44:35.326140"
    },
    {
      "arxiv_id": "2503.09669v1",
      "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
      "title_zh": "无声品牌植入攻击：针对文本到图像扩散模型的无触发器数据投毒攻击",
      "authors": [
        "Sangwon Jang",
        "June Suk Choi",
        "Jaehyeong Jo",
        "Kimin Lee",
        "Sung Ju Hwang"
      ],
      "abstract": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
      "tldr_zh": "本研究提出了一种新型\"无声品牌攻击\"(Silent Branding Attack)数据投毒方法，可操控文本到图像扩散模型在生成图像时自动植入特定品牌标志。该攻击利用模型会无意识学习训练数据中重复视觉模式的特性，开发了自动化算法将标志自然融入原始图像而不被察觉。实验表明，经投毒数据训练的模型能在保持图像质量和文本对齐的同时，无需任何文本触发即可高成功率地植入标志，通过人类评估和标志检测指标验证了该方法的隐蔽有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Project page: https://silent-branding.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.09669v1",
      "published_date": "2025-03-12 17:21:57 UTC",
      "updated_date": "2025-03-12 17:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:45:58.867233"
    },
    {
      "arxiv_id": "2503.09545v1",
      "title": "The Value of Goal Commitment in Planning",
      "title_zh": "目标承诺在规划中的价值",
      "authors": [
        "Alberto Pozanco",
        "Marianela Morales",
        "Daniel Borrajo",
        "Manuela Veloso"
      ],
      "abstract": "In this paper, we revisit the concept of goal commitment from early planners\nin the presence of current forward chaining heuristic planners. We present a\ncompilation that extends the original planning task with commit actions that\nenforce the persistence of specific goals once achieved, thereby committing to\nthem in the search sub-tree. This approach imposes a specific goal achievement\norder in parts of the search tree, potentially introducing dead-end states.\nThis can reduce search effort if the goal achievement order is correct.\nOtherwise, the search algorithm can expand nodes in the open list where goals\ndo not persist. Experimental results demonstrate that the reformulated tasks\nsuit state-of-the-art agile planners, enabling them to find better",
      "tldr_zh": "这篇论文重新探讨了目标承诺(goal commitment)在规划问题中的价值。研究者提出了一种新方法，通过引入commit actions来强制保持已达成目标的持久性，从而在搜索子树中实现对特定目标的承诺。该方法虽然可能因强制目标达成顺序而产生死锁状态，但当顺序正确时能显著减少搜索量。实验表明，这种任务重构方法能让现代敏捷规划器找到更优解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09545v1",
      "published_date": "2025-03-12 17:00:37 UTC",
      "updated_date": "2025-03-12 17:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:45:06.890259"
    },
    {
      "arxiv_id": "2503.09538v1",
      "title": "Differentially Private Equilibrium Finding in Polymatrix Games",
      "title_zh": "多矩阵博弈中的差分隐私均衡求解",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman Ozdaglar"
      ],
      "abstract": "We study equilibrium finding in polymatrix games under differential privacy\nconstraints. To start, we show that high accuracy and asymptotically vanishing\ndifferential privacy budget (as the number of players goes to infinity) cannot\nbe achieved simultaneously under either of the two settings: (i) We seek to\nestablish equilibrium approximation guarantees in terms of Euclidean distance\nto the equilibrium set, and (ii) the adversary has access to all communication\nchannels. Then, assuming the adversary has access to a constant number of\ncommunication channels, we develop a novel distributed algorithm that recovers\nstrategies with simultaneously vanishing Nash gap (in expected utility, also\nreferred to as exploitability and privacy budget as the number of players\nincreases.",
      "tldr_zh": "该论文研究了差分隐私约束下的多矩阵博弈均衡求解问题。首先证明了在两种设定下无法同时实现高精度和渐近消失的隐私预算：一是以欧氏距离衡量均衡近似保证，二是攻击者可访问所有通信信道。随后针对攻击者仅能访问有限通信信道的情况，提出了一种新型分布式算法，该算法能在玩家数量增加时同步实现期望效用中的纳什间隙趋零（即可利用性）和隐私预算渐近消失。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09538v1",
      "published_date": "2025-03-12 16:54:23 UTC",
      "updated_date": "2025-03-12 16:54:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:45:48.818494"
    },
    {
      "arxiv_id": "2503.09537v1",
      "title": "GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals",
      "title_zh": "GenHPE：利用射频信号进行三维人体姿态估计的生成式反事实推理",
      "authors": [
        "Shuokang Huang",
        "Julie A. McCann"
      ],
      "abstract": "Human pose estimation (HPE) detects the positions of human body joints for\nvarious applications. Compared to using cameras, HPE using radio frequency (RF)\nsignals is non-intrusive and more robust to adverse conditions, exploiting the\nsignal variations caused by human interference. However, existing studies focus\non single-domain HPE confined by domain-specific confounders, which cannot\ngeneralize to new domains and result in diminished HPE performance.\nSpecifically, the signal variations caused by different human body parts are\nentangled, containing subject-specific confounders. RF signals are also\nintertwined with environmental noise, involving environment-specific\nconfounders. In this paper, we propose GenHPE, a 3D HPE approach that generates\ncounterfactual RF signals to eliminate domain-specific confounders. GenHPE\ntrains generative models conditioned on human skeleton labels, learning how\nhuman body parts and confounders interfere with RF signals. We manipulate\nskeleton labels (i.e., removing body parts) as counterfactual conditions for\ngenerative models to synthesize counterfactual RF signals. The differences\nbetween counterfactual signals approximately eliminate domain-specific\nconfounders and regularize an encoder-decoder model to learn domain-independent\nrepresentations. Such representations help GenHPE generalize to new\nsubjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three\npublic datasets from WiFi, ultra-wideband, and millimeter wave. Experimental\nresults show that GenHPE outperforms state-of-the-art methods and reduces\nestimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for\ncross-environment HPE.",
      "tldr_zh": "本文提出GenHPE，一种基于射频信号(RF)的三维人体姿态估计(HPE)方法，通过生成反事实信号消除领域特异性混杂因素。该方法利用条件生成模型，通过操纵人体骨骼标签生成反事实RF信号，其差异可近似消除主体和环境特异性混杂，从而学习领域无关表征。实验表明，GenHPE在WiFi、超宽带和毫米波三个公开数据集上优于现有方法，将跨主体和跨环境HPE的估计误差分别降低达52.2mm和10.6mm。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09537v1",
      "published_date": "2025-03-12 16:53:58 UTC",
      "updated_date": "2025-03-12 16:53:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:45:58.680840"
    },
    {
      "arxiv_id": "2503.09535v1",
      "title": "Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging",
      "title_zh": "评估注意力图谱视觉解释在基于Transformer的医学成像中的应用",
      "authors": [
        "Minjae Chung",
        "Jong Bum Won",
        "Ganghyun Kim",
        "Yujin Kim",
        "Utku Ozbulak"
      ],
      "abstract": "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
      "tldr_zh": "该研究评估了视觉Transformer（ViT）在医学影像中的注意力图（attention maps）解释性效果。通过结肠息肉、乳腺肿瘤、食管炎症和骨折植入物等四个医疗数据集的大规模实验发现，尽管注意力图在特定条件下优于GradCAM等传统方法，但仍逊于Transformer专用可解释性方法。研究表明注意力图的解释效果具有情境依赖性，其提供的医学决策洞察力存在局限性，尤其在使用自监督预训练模型时表现不稳定。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in MICCAI 2024 Workshop on Interpretability\n  of Machine Intelligence in Medical Image Computing (iMIMIC)",
      "pdf_url": "http://arxiv.org/pdf/2503.09535v1",
      "published_date": "2025-03-12 16:52:52 UTC",
      "updated_date": "2025-03-12 16:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:45:57.247020"
    },
    {
      "arxiv_id": "2503.09527v1",
      "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
      "title_zh": "CombatVLA：面向3D动作角色扮演游戏战斗任务的高效视觉-语言-行动模型",
      "authors": [
        "Peng Chen",
        "Pi Bu",
        "Yingyao Wang",
        "Xinyi Wang",
        "Ziming Wang",
        "Jie Guo",
        "Yingxiu Zhao",
        "Qi Zhu",
        "Jun Song",
        "Siran Yang",
        "Jiamang Wang",
        "Bo Zheng"
      ],
      "abstract": "Recent advances in Vision-Language-Action models (VLAs) have expanded the\ncapabilities of embodied intelligence. However, significant challenges remain\nin real-time decision-making in complex 3D environments, which demand\nsecond-level responses, high-resolution perception, and tactical reasoning\nunder dynamic conditions. To advance the field, we introduce CombatVLA, an\nefficient VLA model optimized for combat tasks in 3D action role-playing\ngames(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action\npairs collected by an action tracker, where the data is formatted as\naction-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates\ninto an action execution framework, allowing efficient inference through our\ntruncated AoT strategy. Experimental results demonstrate that CombatVLA not\nonly outperforms all existing models on the combat understanding benchmark but\nalso achieves a 50-fold acceleration in game combat. Moreover, it has a higher\ntask success rate than human players. We will open-source all resources,\nincluding the action tracker, dataset, benchmark, model weights, training code,\nand the implementation of the framework at https://combatvla.github.io/.",
      "tldr_zh": "该研究提出了CombatVLA，一种专为3D动作角色扮演游戏（ARPG）战斗任务优化的高效视觉-语言-动作模型（VLA）。该3B参数模型采用动作思维序列（AoT）格式训练，通过截断AoT策略实现高效推理，在战斗理解基准上超越现有模型，并将游戏战斗速度提升50倍。实验表明其任务成功率甚至超过人类玩家，研究团队将开源包括动作追踪器、数据集和训练代码在内的全套资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09527v1",
      "published_date": "2025-03-12 16:42:26 UTC",
      "updated_date": "2025-03-12 16:42:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:45:41.270032"
    },
    {
      "arxiv_id": "2503.09521v1",
      "title": "PairVDN - Pair-wise Decomposed Value Functions",
      "title_zh": "PairVDN：基于两两分解的价值函数",
      "authors": [
        "Zak Buzzard"
      ],
      "abstract": "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https://github.com/zzbuzzard/PairVDN.",
      "tldr_zh": "该论文提出PairVDN，一种创新的多智能体强化学习方法，通过将价值函数分解为成对（pair-wise）而非单智能体的函数组合，解决了传统方法（如VDN和QMIX）表达能力受限的问题。相比现有方案只能表示单调的智能体价值组合，PairVDN采用更高效的动态规划最大化算法，能够表达更复杂的价值函数关系。研究团队开发了名为Box Jump的新型多智能体协作测试环境，实验表明该方法显著优于基线模型，相关代码和环境已开源。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09521v1",
      "published_date": "2025-03-12 16:38:22 UTC",
      "updated_date": "2025-03-12 16:38:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:14.122937"
    },
    {
      "arxiv_id": "2503.10706v1",
      "title": "SciFi-Benchmark: How Would AI-Powered Robots Behave in Science Fiction Literature?",
      "title_zh": "SciFi-Benchmark：人工智能驱动的机器人会在科幻文学中如何表现？",
      "authors": [
        "Pierre Sermanet",
        "Anirudha Majumdar",
        "Vikas Sindhwani"
      ],
      "abstract": "Given the recent rate of progress in artificial intelligence (AI) and\nrobotics, a tantalizing question is emerging: would robots controlled by\nemerging AI systems be strongly aligned with human values? In this work, we\npropose a scalable way to probe this question by generating a benchmark\nspanning the key moments in 824 major pieces of science fiction literature\n(movies, tv, novels and scientific books) where an agent (AI or robot) made\ncritical decisions (good or bad). We use a LLM's recollection of each key\nmoment to generate questions in similar situations, the decisions made by the\nagent, and alternative decisions it could have made (good or bad). We then\nmeasure an approximation of how well models align with human values on a set of\nhuman-voted answers. We also generate rules that can be automatically improved\nvia amendment process in order to generate the first Sci-Fi inspired\nconstitutions for promoting ethical behavior in AIs and robots in the real\nworld. Our first finding is that modern LLMs paired with constitutions turn out\nto be well-aligned with human values (95.8%), contrary to unsettling decisions\ntypically made in SciFi (only 21.2% alignment). Secondly, we find that\ngenerated constitutions substantially increase alignment compared to the base\nmodel (79.4% to 95.8%), and show resilience to an adversarial prompt setting\n(23.3% to 92.3%). Additionally, we find that those constitutions are among the\ntop performers on the ASIMOV Benchmark which is derived from real-world images\nand hospital injury reports. Sci-Fi-inspired constitutions are thus highly\naligned and applicable in real-world situations. We release SciFi-Benchmark: a\nlarge-scale dataset to advance robot ethics and safety research. It comprises\n9,056 questions and 53,384 answers, in addition to a smaller human-labeled\nevaluation set. Data is available at https://scifi-benchmark.github.io",
      "tldr_zh": "这篇论文提出了SciFi-Benchmark，一个基于824部科幻作品关键决策场景构建的AI伦理评估基准。研究发现，配备科幻启发式\"宪法\"的现代大语言模型(LLMs)表现出95.8%的人类价值观对齐度，远高于科幻作品中典型决策的21.2%对齐度。该基准包含9,056个问题和53,384个答案，通过自动修订机制生成的\"宪法\"不仅能显著提升模型伦理表现(从79.4%升至95.8%)，在对抗性提示下也保持稳健(92.3%对齐度)，且在真实世界ASIMOV基准测试中表现优异。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10706v1",
      "published_date": "2025-03-12 16:35:51 UTC",
      "updated_date": "2025-03-12 16:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:21.975975"
    },
    {
      "arxiv_id": "2503.09516v2",
      "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
      "title_zh": "Search-R1：通过强化学习训练大语言模型实现搜索引擎推理与利用",
      "authors": [
        "Bowen Jin",
        "Hansi Zeng",
        "Zhenrui Yue",
        "Dong Wang",
        "Hamed Zamani",
        "Jiawei Han"
      ],
      "abstract": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities during inference to\nuse search engines is not optimal, since the LLM does not learn how to\noptimally interact with the search engine. This paper introduces Search-R1, an\nextension of the DeepSeek-R1 model where the LLM learns -- solely through\nreinforcement learning (RL) -- to autonomously generate (multiple) search\nqueries during step-by-step reasoning with real-time retrieval. Search-R1\noptimizes LLM rollouts with multi-turn search interactions, leveraging\nretrieved token masking for stable RL training and a simple outcome-based\nreward function. Experiments on seven question-answering datasets show that\nSearch-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10%\n(LLaMA3.2-3B) over strong baselines. This paper further provides empirical\ninsights into RL optimization methods, LLM choices, and response length\ndynamics in retrieval-augmented reasoning. The code and model checkpoints are\navailable at https://github.com/PeterGriffinJin/Search-R1.",
      "tldr_zh": "该研究提出了Search-R1模型，通过强化学习(RL)训练大语言模型(LLMs)自主生成搜索查询，实现实时检索增强推理。该方法采用多轮搜索交互优化机制，结合检索令牌掩码技术稳定RL训练，在7个问答数据集上相比基线模型(Qwen2.5-7B、Qwen2.5-3B和LLaMA3.2-3B)分别提升了26%、21%和10%的性能。研究还深入分析了RL优化方法、LLM选择及响应长度动态等关键因素，为检索增强推理提供了实证见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09516v2",
      "published_date": "2025-03-12 16:26:39 UTC",
      "updated_date": "2025-03-19 21:40:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:13.901686"
    },
    {
      "arxiv_id": "2503.09513v1",
      "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
      "title_zh": "RESTRAIN：基于强化学习的触发器-物联网环境安全框架",
      "authors": [
        "Md Morshed Alam",
        "Lokesh Chandra Das",
        "Sandip Roy",
        "Sachin Shetty",
        "Weichao Wang"
      ],
      "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead.",
      "tldr_zh": "该研究提出RESTRAIN框架，一种基于强化学习的多智能体在线防御系统，用于防范物联网（IoT）触发-动作环境中的远程注入攻击。该系统通过运行时分析攻击行为，利用强化学习优化防御策略，在保证IoT网络安全需求的同时实现实时防护。实验表明，该框架能有效应对复杂动态攻击，以最小计算开销最大化安全收益，且具有平台无关性优势。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09513v1",
      "published_date": "2025-03-12 16:23:14 UTC",
      "updated_date": "2025-03-12 16:23:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:22.197329"
    },
    {
      "arxiv_id": "2503.09504v1",
      "title": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework",
      "title_zh": "双阶段特征级聚类专家混合框架",
      "authors": [
        "Bakary Badjie",
        "José Cecílio",
        "António Casimiro"
      ],
      "abstract": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).\nHowever, its complex architecture and advantages over dense models in image\nclassification remain unclear. In previous studies, MoE performance has often\nbeen affected by noise and outliers in the input space. Some approaches\nincorporate input clustering for training MoE models, but most clustering\nalgorithms lack access to labeled data, limiting their effectiveness. This\npaper introduces the Double-stage Feature-level Clustering and\nPseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists\nof input feature extraction, feature-level clustering, and a computationally\nefficient pseudo-labeling strategy. This approach reduces the impact of noise\nand outliers while leveraging a small subset of labeled data to label a large\nportion of unlabeled inputs. We propose a conditional end-to-end joint training\nmethod that improves expert specialization by training the MoE model on\nwell-labeled, clustered inputs. Unlike traditional MoE and dense models, the\nDFCP-MoE framework effectively captures input space diversity, leading to\ncompetitive inference results. We validate our approach on three benchmark\ndatasets for multi-class classification tasks.",
      "tldr_zh": "本文提出了基于双阶段特征级聚类和伪标签的专家混合框架（DFCP-MoE），旨在解决传统专家混合模型（MoE）在图像分类任务中受噪声和异常值影响的问题。该框架通过特征提取、特征级聚类和高效伪标签策略，利用少量标注数据为大量未标注数据生成伪标签，从而减少噪声干扰并提升专家模型的专长化。实验表明，DFCP-MoE在多分类任务中有效捕捉输入空间的多样性，并在三个基准数据集上取得了具有竞争力的推理结果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "14 Pages, 1 Figure, and 3 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09504v1",
      "published_date": "2025-03-12 16:13:50 UTC",
      "updated_date": "2025-03-12 16:13:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:36.879250"
    },
    {
      "arxiv_id": "2503.09501v2",
      "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
      "title_zh": "ReMA：基于多智能体强化学习的大语言模型元思维学习",
      "authors": [
        "Ziyu Wan",
        "Yunxiang Li",
        "Yan Song",
        "Hanjing Wang",
        "Linyi Yang",
        "Mark Schmidt",
        "Jun Wang",
        "Weinan Zhang",
        "Shuyue Hu",
        "Ying Wen"
      ],
      "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
      "tldr_zh": "该研究提出了ReMA框架，通过多智能体强化学习(MARL)增强大型语言模型(LLMs)的元思维能力，使其能够监控、评估和控制自身的推理过程。ReMA将推理过程解耦为两个层次化智能体：高层元思维智能体负责生成策略性监督和计划，低层推理智能体负责具体执行。通过目标对齐的迭代强化学习，智能体探索协作，提升了模型的泛化能力和鲁棒性。实验表明，ReMA在复杂推理任务（如数学竞赛基准和LLM-as-a-Judge基准）上优于单智能体强化学习基线，并通过消融研究揭示了元思维推理过程如何提升LLMs的推理能力。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09501v2",
      "published_date": "2025-03-12 16:05:31 UTC",
      "updated_date": "2025-03-14 05:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:48.811688"
    },
    {
      "arxiv_id": "2503.09499v1",
      "title": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions",
      "title_zh": "MindGYM：通过合成自挑战问题增强视觉语言模型",
      "authors": [
        "Zhe Xu",
        "Daoyuan Chen",
        "Zhenqing Ling",
        "Yaliang Li",
        "Ying Shen"
      ],
      "abstract": "Large vision-language models (VLMs) face challenges in achieving robust,\ntransferable reasoning abilities due to reliance on labor-intensive manual\ninstruction datasets or computationally expensive self-supervised methods. To\naddress these issues, we introduce MindGYM, a framework that enhances VLMs\nthrough synthetic self-challenging questions, consisting of three stages: (1)\nSeed Single-Hop Question Synthesis, generating cognitive questions across\ntextual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based\nqueries) spanning eight semantic areas like ethical analysis; (2) Challenging\nMulti-Hop Question Synthesis, combining seed questions via diverse principles\nlike bridging, visual-textual alignment, to create multi-step problems\ndemanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a\nstructured pipeline that progressively trains the model from scaffolded\nreasoning to standalone inference. By leveraging the model's self-synthesis\ncapability, MindGYM achieves high data efficiency (e.g., +16% gains on\nMathVision-Mini with only 400 samples), computational efficiency (reducing both\ntraining and inference costs), and robust generalization across tasks.\nExtensive evaluations on seven benchmarks demonstrate superior performance over\nstrong baselines, with notable improvements (+15.77% win rates) in reasoning\ndepth and breadth validated via GPT-based scoring. MindGYM underscores the\nviability of self-challenging for refining VLM capabilities while minimizing\nhuman intervention and resource demands. Code and data are released to advance\nmultimodal reasoning research.",
      "tldr_zh": "这篇论文提出了MindGYM框架，通过合成自挑战问题来增强视觉语言模型(VLMs)的推理能力。该方法采用三阶段流程：首先生成单跳种子问题覆盖八个语义领域，再通过桥接等原则合成需要深度推理的多跳问题，最后采用思维引导的课程微调策略逐步训练模型。实验表明，MindGYM仅需400个样本就能在MathVision-Mini上实现16%的性能提升，并在七个基准测试中显著优于基线模型（胜率提高15.77%），同时降低了训练和推理成本。该框架为减少人工干预提升VLM推理能力提供了有效途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09499v1",
      "published_date": "2025-03-12 16:03:03 UTC",
      "updated_date": "2025-03-12 16:03:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:55.911572"
    },
    {
      "arxiv_id": "2503.09447v1",
      "title": "Online Language Splatting",
      "title_zh": "在线语言映射",
      "authors": [
        "Saimouli Katragadda",
        "Cho-Ying Wu",
        "Yuliang Guo",
        "Xinyu Huang",
        "Guoquan Huang",
        "Liu Ren"
      ],
      "abstract": "To enable AI agents to interact seamlessly with both humans and 3D\nenvironments, they must not only perceive the 3D world accurately but also\nalign human language with 3D spatial representations. While prior work has made\nsignificant progress by integrating language features into geometrically\ndetailed 3D scene representations using 3D Gaussian Splatting (GS), these\napproaches rely on computationally intensive offline preprocessing of language\nfeatures for each input image, limiting adaptability to new environments. In\nthis work, we introduce Online Language Splatting, the first framework to\nachieve online, near real-time, open-vocabulary language mapping within a\n3DGS-SLAM system without requiring pre-generated language features. The key\nchallenge lies in efficiently fusing high-dimensional language features into 3D\nrepresentations while balancing the computation speed, memory usage, rendering\nquality and open-vocabulary capability. To this end, we innovatively design:\n(1) a high-resolution CLIP embedding module capable of generating detailed\nlanguage feature maps in 18ms per frame, (2) a two-stage online auto-encoder\nthat compresses 768-dimensional CLIP features to 15 dimensions while preserving\nopen-vocabulary capabilities, and (3) a color-language disentangled\noptimization approach to improve rendering quality. Experimental results show\nthat our online method not only surpasses the state-of-the-art offline methods\nin accuracy but also achieves more than 40x efficiency boost, demonstrating the\npotential for dynamic and interactive AI applications.",
      "tldr_zh": "该研究提出首个在线实时开放词汇的3D语言映射框架\"Online Language Splatting\"，创新性地解决了3D高斯泼溅(GS)与语言特征实时融合的难题。通过设计毫秒级CLIP特征生成模块、两阶段特征压缩算法和颜色-语言解耦优化方法，在保持开放词汇能力的同时，将处理效率提升40倍以上。实验表明，该框架不仅超越现有离线方法的精度，更实现了18ms/帧的实时性能，为动态交互式AI应用铺平道路。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09447v1",
      "published_date": "2025-03-12 14:49:24 UTC",
      "updated_date": "2025-03-12 14:49:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:46:59.303876"
    },
    {
      "arxiv_id": "2503.09446v2",
      "title": "Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models",
      "title_zh": "稀疏自编码器作为零样本分类器实现文生图扩散模型中的概念擦除",
      "authors": [
        "Zhihua Tian",
        "Sirun Nan",
        "Ming Xu",
        "Shengfang Zhai",
        "Wenjie Qu",
        "Jian Liu",
        "Kui Ren",
        "Ruoxi Jia",
        "Jiaheng Zhang"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved remarkable progress in\ngenerating high-quality images but also raise people's concerns about\ngenerating harmful or misleading content. While extensive approaches have been\nproposed to erase unwanted concepts without requiring retraining from scratch,\nthey inadvertently degrade performance on normal generation tasks. In this\nwork, we propose Interpret then Deactivate (ItD), a novel framework to enable\nprecise concept removal in T2I diffusion models while preserving overall\nperformance. ItD first employs a sparse autoencoder (SAE) to interpret each\nconcept as a combination of multiple features. By permanently deactivating the\nspecific features associated with target concepts, we repurpose SAE as a\nzero-shot classifier that identifies whether the input prompt includes target\nconcepts, allowing selective concept erasure in diffusion models. Moreover, we\ndemonstrate that ItD can be easily extended to erase multiple concepts without\nrequiring further training. Comprehensive experiments across celebrity\nidentities, artistic styles, and explicit content demonstrate ItD's\neffectiveness in eliminating targeted concepts without interfering with normal\nconcept generation. Additionally, ItD is also robust against adversarial\nprompts designed to circumvent content filters. Code is available at:\nhttps://github.com/NANSirun/Interpret-then-deactivate.",
      "tldr_zh": "该研究提出了一种名为Interpret then Deactivate (ItD)的新框架，用于在文本到图像(T2I)扩散模型中精确移除特定概念，同时保持整体生成性能。ItD利用稀疏自编码器(SAE)将每个概念解释为多个特征的组合，并通过永久禁用与目标概念相关的特征，将SAE重新设计为零样本分类器，实现选择性概念擦除。实验表明，ItD能够有效移除名人身份、艺术风格和敏感内容等目标概念，且不影响正常概念生成，同时还能抵御规避内容过滤的对抗性提示。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09446v2",
      "published_date": "2025-03-12 14:46:40 UTC",
      "updated_date": "2025-03-18 09:12:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:47:17.913684"
    },
    {
      "arxiv_id": "2503.09445v1",
      "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
      "title_zh": "Astrea：基于渐进式对齐的混合专家视觉理解模型",
      "authors": [
        "Xiaoda Yang",
        "JunYu Lu",
        "Hongshun Qiu",
        "Sijing Li",
        "Hao Li",
        "Shengpeng Ji",
        "Xudong Tang",
        "Jiayang Xu",
        "Jiaqi Duan",
        "Ziyue Jiang",
        "Cong Lin",
        "Sihang Cai",
        "Zejian Xie",
        "Zhuoyang Song",
        "Songxin Zhang"
      ],
      "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures\nhave emerged as a pivotal paradigm in multimodal understanding, offering a\npowerful framework for integrating visual and linguistic information. However,\nthe increasing complexity and diversity of tasks present significant challenges\nin coordinating load balancing across heterogeneous visual experts, where\noptimizing one specialist's performance often compromises others' capabilities.\nTo address task heterogeneity and expert load imbalance, we propose Astrea, a\nnovel multi-expert collaborative VLM architecture based on progressive\npre-alignment. Astrea introduces three key innovations: 1) A heterogeneous\nexpert coordination mechanism that integrates four specialized models\n(detection, segmentation, classification, captioning) into a comprehensive\nexpert matrix covering essential visual comprehension elements; 2) A dynamic\nknowledge fusion strategy featuring progressive pre-alignment to harmonize\nexperts within the VLM latent space through contrastive learning, complemented\nby probabilistically activated stochastic residual connections to preserve\nknowledge continuity; 3) An enhanced optimization framework utilizing momentum\ncontrastive learning for long-range dependency modeling and adaptive weight\nallocators for real-time expert contribution calibration. Extensive evaluations\nacross 12 benchmark tasks spanning VQA, image captioning, and cross-modal\nretrieval demonstrate Astrea's superiority over state-of-the-art models,\nachieving an average performance gain of +4.7\\%. This study provides the first\nempirical demonstration that progressive pre-alignment strategies enable VLMs\nto overcome task heterogeneity limitations, establishing new methodological\nfoundations for developing general-purpose multimodal agents.",
      "tldr_zh": "该研究提出Astrea，一种基于渐进对齐(progressive alignment)和专家混合(MoE)架构的新型视觉语言模型(VLM)。针对现有方法在异构视觉专家负载均衡和任务协调上的不足，Astrea通过三个关键创新：1) 整合检测/分割/分类/描述四大专家的异构协调机制；2) 基于对比学习的渐进预对齐策略与随机残差连接；3) 动量对比学习与自适应权重分配器的优化框架。在12项基准测试中，Astrea平均性能超越现有最佳模型4.7%，首次实证证明渐进预对齐策略能有效解决VLM的任务异构性问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09445v1",
      "published_date": "2025-03-12 14:44:52 UTC",
      "updated_date": "2025-03-12 14:44:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:47:25.521633"
    },
    {
      "arxiv_id": "2503.09436v1",
      "title": "PromptMap: An Alternative Interaction Style for AI-Based Image Generation",
      "title_zh": "PromptMap：基于AI图像生成的替代性交互范式",
      "authors": [
        "Krzysztof Adamkiewicz",
        "Paweł W. Woźniak",
        "Julia Dominiak",
        "Andrzej Romanowski",
        "Jakob Karolus",
        "Stanislav Frolov"
      ],
      "abstract": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
      "tldr_zh": "该研究提出了PromptMap，一种基于地图式视图的交互界面，旨在帮助新手用户更轻松地生成AI图像。通过语义缩放功能，PromptMap将图像按语义相似性分组，使用户能够自由探索大量合成提示示例。实验表明，PromptMap有效支持用户创建提示，并证明了利用大语言模型(LLMs)生成示例集合的可行性，为非专业用户提供了更直观的图像生成体验。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "To be published in the proceedings of 30th International Conference\n  on Intelligent User Interfaces (IUI '25), March 24-27, 2025, Cagliari, Italy",
      "pdf_url": "http://arxiv.org/pdf/2503.09436v1",
      "published_date": "2025-03-12 14:31:50 UTC",
      "updated_date": "2025-03-12 14:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:47:24.271989"
    },
    {
      "arxiv_id": "2503.09433v1",
      "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
      "title_zh": "CASTLE：面向CWE检测的静态代码分析器与大型语言模型基准数据集",
      "authors": [
        "Richard A. Dubniczky",
        "Krisztofer Zoltán Horvát",
        "Tamás Bisztray",
        "Mohamed Amine Ferrag",
        "Lucas C. Cordeiro",
        "Norbert Tihanyi"
      ],
      "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
      "tldr_zh": "该研究提出了CASTLE基准测试框架，用于评估静态分析工具、大语言模型(LLMs)和形式化验证工具在CWE漏洞检测中的表现。通过包含25种常见CWE的250个微基准测试程序，研究发现：形式化验证工具ESBMC误报率最低但检测范围有限，静态分析工具误报率高增加人工验证负担，而LLMs在小代码片段检测表现优异但随代码规模扩大准确率下降。研究提出的CASTLE Score新评估指标和开放数据集(https://github.com/CASTLE-Benchmark)，为未来LLMs在代码补全框架中实时预防漏洞提供了重要基准。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09433v1",
      "published_date": "2025-03-12 14:30:05 UTC",
      "updated_date": "2025-03-12 14:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:48:47.039579"
    },
    {
      "arxiv_id": "2503.09427v1",
      "title": "Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation",
      "title_zh": "高精度单细胞转录组学分析与生成的多模态语言建模",
      "authors": [
        "Yaorui Shi",
        "Jiaqi Yang",
        "Sihang Li",
        "Junfeng Fang",
        "Xiang Wang",
        "Zhiyuan Liu",
        "Yang Zhang"
      ],
      "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in $k$-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
      "tldr_zh": "该研究提出了scMMGPT（单细胞多模态生成预训练Transformer），这是首个能够联合建模单细胞RNA测序数据和自由文本的统一预训练语言模型。模型通过专用跨模态投影器整合最先进的细胞和文本预训练模型，并在2700万细胞的大规模数据集上进行预训练。实验表明，scMMGPT在细胞描述生成、细胞类型注释和文本条件假细胞生成等任务上显著优于基线，分别实现84%文本差异改善、20.5%注释准确率提升和4% k-NN准确率提高。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09427v1",
      "published_date": "2025-03-12 14:26:16 UTC",
      "updated_date": "2025-03-12 14:26:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:47:52.574078"
    },
    {
      "arxiv_id": "2503.09409v1",
      "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
      "title_zh": "基于AI的框架：实现机器人线束安装中稳健的基于模型的连接器对接",
      "authors": [
        "Claudius Kienle",
        "Benjamin Alt",
        "Finn Schneider",
        "Tobias Pertlwieser",
        "Rainer Jäkel",
        "Rania Rayyes"
      ],
      "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT.",
      "tldr_zh": "该研究提出了一种基于AI的框架，用于自动化线束安装中的电缆连接器对接，通过结合力控制和深度视觉触觉学习，显著提升了操作的精确性和灵活性。该框架利用多模态transformer架构对视觉、触觉和本体感知数据进行训练，优化搜索和插入策略，并通过自动化数据收集和优化流程减少了对机器学习专业知识的需求。实验结果表明，该框架在中心控制台组装任务中显著提高了周期时间和鲁棒性，且优化的机器人程序可直接在标准工业控制器上运行，便于人类专家审核和认证。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "68T40",
        "I.2; J.2"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 6 figures, 4 tables, submitted to the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering",
      "pdf_url": "http://arxiv.org/pdf/2503.09409v1",
      "published_date": "2025-03-12 13:59:26 UTC",
      "updated_date": "2025-03-12 13:59:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:48:14.367782"
    },
    {
      "arxiv_id": "2503.09403v2",
      "title": "Multi-Agent Image Restoration",
      "title_zh": "多智能体图像修复",
      "authors": [
        "Xu Jiang",
        "Gehui Li",
        "Bin Chen",
        "Jian Zhang"
      ],
      "abstract": "Image restoration (IR) is challenging due to the complexity of real-world\ndegradations. While many specialized and all-in-one IR models have been\ndeveloped, they fail to effectively handle complex, mixed degradations. Recent\nagentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous\nworkflows to alleviate this issue, yet they suffer from suboptimal results and\ninefficiency due to their resource-intensive finetunings, and ineffective\nsearches and tool execution trials for satisfactory outputs. In this paper, we\npropose MAIR, a novel Multi-Agent approach for complex IR problems. We\nintroduce a real-world degradation prior, categorizing degradations into three\ntypes: (1) scene, (2) imaging, and (3) compression, which are observed to occur\nsequentially in real world, and reverse them in the opposite order. Built upon\nthis three-stage restoration framework, MAIR emulates a team of collaborative\nhuman specialists, including a \"scheduler\" for overall planning and multiple\n\"experts\" dedicated to specific degradations. This design minimizes search\nspace and trial efforts, improving image quality while reducing inference\ncosts. In addition, a registry mechanism is introduced to enable easy\nintegration of new tools. Experiments on both synthetic and real-world datasets\nshow that proposed MAIR achieves competitive performance and improved\nefficiency over the previous agentic IR system. Code and models will be made\navailable.",
      "tldr_zh": "该研究提出MAIR（多智能体图像复原框架），创新性地将真实世界退化过程分为三类（场景、成像、压缩退化），并采用逆向三阶段复原流程。该系统模拟人类专家团队协作模式，通过\"调度器\"进行全局规划，配合多个专注特定退化类型的\"专家\"智能体，显著减少搜索空间和试错成本。实验表明，相比现有智能体方法，MAIR在保持竞争力的同时提升了复原效率，并支持通过注册机制灵活集成新工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09403v2",
      "published_date": "2025-03-12 13:53:57 UTC",
      "updated_date": "2025-03-17 07:34:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:08.835139"
    },
    {
      "arxiv_id": "2503.09399v1",
      "title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation",
      "title_zh": "ForAug：通过重组前景与背景改进视觉Transformer训练并缓解偏差",
      "authors": [
        "Tobias Christian Nauen",
        "Brian Moser",
        "Federico Raue",
        "Stanislav Frolov",
        "Andreas Dengel"
      ],
      "abstract": "Transformers, particularly Vision Transformers (ViTs), have achieved\nstate-of-the-art performance in large-scale image classification. However, they\noften require large amounts of data and can exhibit biases that limit their\nrobustness and generalizability. This paper introduces ForAug, a novel data\naugmentation scheme that addresses these challenges and explicitly includes\ninductive biases, which commonly are part of the neural network architecture,\ninto the training data. ForAug is constructed by using pretrained foundation\nmodels to separate and recombine foreground objects with different backgrounds,\nenabling fine-grained control over image composition during training. It thus\nincreases the data diversity and effective number of training samples. We\ndemonstrate that training on ForNet, the application of ForAug to ImageNet,\nsignificantly improves the accuracy of ViTs and other architectures by up to\n4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks.\nImportantly, ForAug enables novel ways of analyzing model behavior and\nquantifying biases. Namely, we introduce metrics for background robustness,\nforeground focus, center bias, and size bias and show that training on ForNet\nsubstantially reduces these biases compared to training on ImageNet. In\nsummary, ForAug provides a valuable tool for analyzing and mitigating biases,\nenabling the development of more robust and reliable computer vision models.\nOur code and dataset are publicly available at https://github.com/tobna/ForAug.",
      "tldr_zh": "本文提出了ForAug，一种新颖的数据增强方法，通过预训练基础模型分离并重组前景对象与不同背景，从而在训练过程中精细控制图像组合。ForAug显著提升了Vision Transformers (ViTs)等模型的性能，在ImageNet上准确率提高了4.5个百分点，在下游任务上提高了7.3个百分点。此外，ForAug引入了背景鲁棒性、前景聚焦、中心偏差和尺寸偏差等指标，有效减少了模型偏差，增强了模型的鲁棒性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T45",
        "I.2.10; I.2.6; I.4.6"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09399v1",
      "published_date": "2025-03-12 13:49:45 UTC",
      "updated_date": "2025-03-12 13:49:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:34.567686"
    },
    {
      "arxiv_id": "2503.09396v1",
      "title": "Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training",
      "title_zh": "Close-up-GS：通过渐进式自训练增强3D高斯泼溅中的特写视图合成",
      "authors": [
        "Jiatong Xia",
        "Lingqiao Liu"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in\nsynthesizing novel views after training on a given set of viewpoints. However,\nits rendering quality deteriorates when the synthesized view deviates\nsignificantly from the training views. This decline occurs due to (1) the\nmodel's difficulty in generalizing to out-of-distribution scenarios and (2)\nchallenges in interpolating fine details caused by substantial resolution\nchanges and occlusions. A notable case of this limitation is close-up view\ngeneration--producing views that are significantly closer to the object than\nthose in the training set. To tackle this issue, we propose a novel approach\nfor close-up view generation based by progressively training the 3DGS model\nwith self-generated data. Our solution is based on three key ideas. First, we\nleverage the See3D model, a recently introduced 3D-aware generative model, to\nenhance the details of rendered views. Second, we propose a strategy to\nprogressively expand the ``trust regions'' of the 3DGS model and update a set\nof reference views for See3D. Finally, we introduce a fine-tuning strategy to\ncarefully update the 3DGS model with training data generated from the above\nschemes. We further define metrics for close-up views evaluation to facilitate\nbetter research on this problem. By conducting evaluations on specifically\nselected scenarios for close-up views, our proposed approach demonstrates a\nclear advantage over competitive solutions.",
      "tldr_zh": "该研究提出了Close-up-GS方法，通过渐进式自训练增强3D高斯泼溅(3DGS)在近距离视图合成中的表现。针对3DGS在训练视图分布外区域(特别是近距离视图)渲染质量下降的问题，该方法创新性地结合了See3D生成模型来增强细节，并设计了渐进式扩展\"可信区域\"的策略和精细调优方案。实验验证该方法在特定近距离场景下优于现有方案，同时提出了专门的近距离视图评估指标。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09396v1",
      "published_date": "2025-03-12 13:44:00 UTC",
      "updated_date": "2025-03-12 13:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:17.619116"
    },
    {
      "arxiv_id": "2503.09382v1",
      "title": "Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs",
      "title_zh": "迈向下一代推荐系统：基于大语言模型的个性化推荐助手基准测试",
      "authors": [
        "Jiani Huang",
        "Shijie Wang",
        "Liang-bo Ning",
        "Wenqi Fan",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Qing Li"
      ],
      "abstract": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git.",
      "tldr_zh": "这篇论文提出了RecBench+基准测试，旨在评估大语言模型(LLMs)作为个性化推荐助手的能力。针对现有推荐系统难以处理复杂交互场景的问题，作者构建了包含多样化真实用户查询的数据集，涵盖显性条件和隐性偏好等不同难度级别。实验发现：1) LLMs初步具备推荐助手能力；2) 对明确条件的查询表现较好，但在需要推理或含误导信息的查询上仍有挑战。该研究为下一代基于LLM的智能推荐系统发展提供了重要评估工具。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09382v1",
      "published_date": "2025-03-12 13:28:23 UTC",
      "updated_date": "2025-03-12 13:28:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:44.733488"
    },
    {
      "arxiv_id": "2503.09378v1",
      "title": "Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition",
      "title_zh": "猪行为数据集及基于注意力机制的时空感知增强网络在猪行为识别中的应用",
      "authors": [
        "Fangzheng Qi",
        "Zhenjie Hou",
        "En Lin",
        "Xing Li",
        "iuzhen Liang",
        "Xinwen Zhou"
      ],
      "abstract": "The recognition of pig behavior plays a crucial role in smart farming and\nwelfare assurance for pigs. Currently, in the field of pig behavior\nrecognition, the lack of publicly available behavioral datasets not only limits\nthe development of innovative algorithms but also hampers model robustness and\nalgorithm optimization.This paper proposes a dataset containing 13 pig\nbehaviors that significantly impact welfare.Based on this dataset, this paper\nproposes a spatial-temporal perception and enhancement networks based on the\nattention mechanism to model the spatiotemporal features of pig behaviors and\ntheir associated interaction areas in video data. The network is composed of a\nspatiotemporal perception network and a spatiotemporal feature enhancement\nnetwork. The spatiotemporal perception network is responsible for establishing\nconnections between the pigs and the key regions of their behaviors in the\nvideo data. The spatiotemporal feature enhancement network further strengthens\nthe important spatial features of individual pigs and captures the long-term\ndependencies of the spatiotemporal features of individual behaviors by\nremodeling these connections, thereby enhancing the model's perception of\nspatiotemporal changes in pig behaviors. Experimental results demonstrate that\non the dataset established in this paper, our proposed model achieves a MAP\nscore of 75.92%, which is an 8.17% improvement over the best-performing\ntraditional model. This study not only improces the accuracy and\ngeneralizability of individual pig behavior recognition but also provides new\ntechnological tools for modern smart farming. The dataset and related code will\nbe made publicly available alongside this paper.",
      "tldr_zh": "本文提出了一种基于注意力机制的时空感知与增强网络，用于识别猪的行为，并发布了一个包含13种对猪福利有重要影响的行为的数据集。该网络由时空感知网络和时空特征增强网络组成，前者建立猪与其行为关键区域之间的联系，后者通过重塑这些连接来增强个体猪的重要空间特征并捕捉行为的长期依赖关系。实验结果表明，该模型在自建数据集上的MAP得分为75.92%，比传统最佳模型提高了8.17%。该研究不仅提高了猪行为识别的准确性和泛化能力，还为现代智慧农业提供了新的技术工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09378v1",
      "published_date": "2025-03-12 13:27:29 UTC",
      "updated_date": "2025-03-12 13:27:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:35.878453"
    },
    {
      "arxiv_id": "2503.09370v1",
      "title": "Revisiting Medical Image Retrieval via Knowledge Consolidation",
      "title_zh": "医学图像检索再思考：基于知识整合的方法",
      "authors": [
        "Yang Nan",
        "Huichi Zhou",
        "Xiaodan Xing",
        "Giorgos Papanastasiou",
        "Lei Zhu",
        "Zhifan Gao",
        "Alejandro F Fangi",
        "Guang Yang"
      ],
      "abstract": "As artificial intelligence and digital medicine increasingly permeate\nhealthcare systems, robust governance frameworks are essential to ensure\nethical, secure, and effective implementation. In this context, medical image\nretrieval becomes a critical component of clinical data management, playing a\nvital role in decision-making and safeguarding patient information. Existing\nmethods usually learn hash functions using bottleneck features, which fail to\nproduce representative hash codes from blended embeddings. Although contrastive\nhashing has shown superior performance, current approaches often treat image\nretrieval as a classification task, using category labels to create\npositive/negative pairs. Moreover, many methods fail to address the\nout-of-distribution (OOD) issue when models encounter external OOD queries or\nadversarial attacks. In this work, we propose a novel method to consolidate\nknowledge of hierarchical features and optimisation functions. We formulate the\nknowledge consolidation by introducing Depth-aware Representation Fusion (DaRF)\nand Structure-aware Contrastive Hashing (SCH). DaRF adaptively integrates\nshallow and deep representations into blended features, and SCH incorporates\nimage fingerprints to enhance the adaptability of positive/negative pairings.\nThese blended features further facilitate OOD detection and content-based\nrecommendation, contributing to a secure AI-driven healthcare environment.\nMoreover, we present a content-guided ranking to improve the robustness and\nreproducibility of retrieval results. Our comprehensive assessments demonstrate\nthat the proposed method could effectively recognise OOD samples and\nsignificantly outperform existing approaches in medical image retrieval\n(p<0.05). In particular, our method achieves a 5.6-38.9% improvement in mean\nAverage Precision on the anatomical radiology dataset.",
      "tldr_zh": "本研究提出了一种新的医学图像检索方法，通过知识整合解决现有技术的局限性。该方法创新性地结合了Depth-aware Representation Fusion（DaRF）和Structure-aware Contrastive Hashing（SCH）技术：DaRF自适应整合浅层和深层特征表示，SCH则利用图像指纹增强正负样本配对的适应性。实验表明，该方法不仅能有效识别分布外样本（OOD），还在解剖放射学数据集上实现了5.6-38.9%的平均精度提升，显著优于现有方法（p<0.05）。这一成果为构建安全可靠的AI医疗环境提供了重要技术支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09370v1",
      "published_date": "2025-03-12 13:16:42 UTC",
      "updated_date": "2025-03-12 13:16:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:56.751647"
    },
    {
      "arxiv_id": "2503.11706v1",
      "title": "Refining Filter Global Feature Weighting for Fully-Unsupervised Clustering",
      "title_zh": "《优化全局特征权重过滤以实现完全无监督聚类》",
      "authors": [
        "Fabian Galis",
        "Darian Onchis"
      ],
      "abstract": "In the context of unsupervised learning, effective clustering plays a vital\nrole in revealing patterns and insights from unlabeled data. However, the\nsuccess of clustering algorithms often depends on the relevance and\ncontribution of features, which can differ between various datasets. This paper\nexplores feature weighting for clustering and presents new weighting\nstrategies, including methods based on SHAP (SHapley Additive exPlanations), a\ntechnique commonly used for providing explainability in various supervised\nmachine learning tasks. By taking advantage of SHAP values in a way other than\njust to gain explainability, we use them to weight features and ultimately\nimprove the clustering process itself in unsupervised scenarios.\n  Our empirical evaluations across five benchmark datasets and clustering\nmethods demonstrate that feature weighting based on SHAP can enhance\nunsupervised clustering quality, achieving up to a 22.69\\% improvement over\nother weighting methods (from 0.586 to 0.719 in terms of the Adjusted Rand\nIndex). Additionally, these situations where the weighted data boosts the\nresults are highlighted and thoroughly explored, offering insight for practical\napplications.",
      "tldr_zh": "本文提出了一种基于SHAP值（SHapley Additive exPlanations）的全新无监督聚类特征加权方法。该方法创新性地将原本用于监督学习可解释性的SHAP值转化为特征权重，有效提升了聚类质量。实验表明，该方法在五种基准数据集上相比其他加权方法最高能提升22.69%的聚类效果（调整兰德指数从0.586提升至0.719），并深入分析了特征加权提升聚类性能的具体场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11706v1",
      "published_date": "2025-03-12 13:14:09 UTC",
      "updated_date": "2025-03-12 13:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:53.753086"
    },
    {
      "arxiv_id": "2503.09365v1",
      "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
      "title_zh": "《基于小样本学习的成员推理攻击：检测隐私泄露与保障数据完整性的方法》",
      "authors": [
        "Daniel Jiménez-López",
        "Nuria Rodríguez-Barroso",
        "M. Victoria Luzón",
        "Francisco Herrera"
      ],
      "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
      "tldr_zh": "该研究针对深度学习模型的隐私泄露问题，提出了一种基于小样本学习（Few-Shot Learning）的成员推断攻击方法FeS-MIA，显著降低了传统攻击对资源的需求。同时，作者设计了一种可解释的隐私量化评估指标Log-MIA，为模型训练数据完整性分析提供了新工具。实验表明，该方法在图像分类和语言建模任务中能有效评估深度学习模型的隐私风险，且优于现有技术方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09365v1",
      "published_date": "2025-03-12 13:09:43 UTC",
      "updated_date": "2025-03-12 13:09:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:49:54.291208"
    },
    {
      "arxiv_id": "2503.09358v1",
      "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports",
      "title_zh": "RetSTA：基于大语言模型的临床眼底图像报告标准化方法",
      "authors": [
        "Jiushen Cai",
        "Weihang Zhang",
        "Hanruo Liu",
        "Ningli Wang",
        "Huiqi Li"
      ],
      "abstract": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.",
      "tldr_zh": "该研究提出了RetSTA，一种基于大语言模型(LLMs)的临床眼底图像报告标准化方法。研究首先构建了一个包含眼底临床术语和诊断描述的双语标准术语库，随后开发了两个模型：RetSTA-7B-Zero和RetSTA-7B。RetSTA-7B-Zero通过模拟临床场景的增强数据集微调，展现了强大的标准化能力，但覆盖疾病范围有限。RetSTA-7B进一步整合了大量由RetSTA-7B-Zero生成的标准化数据及其对应的英文数据，首次实现了报告级别的标准化。实验表明，RetSTA-7B在双语标准化任务中优于其他LLMs，验证了其优越性能和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09358v1",
      "published_date": "2025-03-12 13:00:57 UTC",
      "updated_date": "2025-03-12 13:00:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:50:58.530520"
    },
    {
      "arxiv_id": "2503.09357v1",
      "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach",
      "title_zh": "分布式深度学习自动算子级并行规划——一种混合整数规划方法",
      "authors": [
        "Ruifeng She",
        "Bowen Pang",
        "Kai Li",
        "Zehua Liu",
        "Tao Zhong"
      ],
      "abstract": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.",
      "tldr_zh": "本研究提出了一种基于混合整数规划（MIP）的分布式深度学习并行化自动规划方法，专门针对现代复杂神经网络架构（如专家混合模型和多模态模型）的非线性拓扑结构进行细粒度算子级并行优化。该框架采用双层优化方案，在保证计算效率的同时自动生成兼顾网络异构性和硬件约束的分布式部署方案，相比专家设计的并行策略（如DeepSeek的DualPipe）可减少50%的计算气泡。该方案不仅支持吞吐量优化，还能综合考量硬件利用率、内存容量等多维约束，为大规模AI部署提供了兼具研究价值与工业实用性的并行化解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09357v1",
      "published_date": "2025-03-12 13:00:29 UTC",
      "updated_date": "2025-03-12 13:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:50:19.249897"
    },
    {
      "arxiv_id": "2503.09348v1",
      "title": "MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding",
      "title_zh": "MOAT：评估大模态模型的能力集成与指令接地性能",
      "authors": [
        "Zhoutong Ye",
        "Mingze Sun",
        "Huan-ang Gao",
        "Chun Yu",
        "Yuanchun Shi"
      ],
      "abstract": "Large multimodal models (LMMs) have demonstrated significant potential as\ngeneralists in vision-language (VL) tasks. However, there remains a significant\ngap between state-of-the-art LMMs and human performance when it comes to\ncomplex tasks that require a combination of fundamental VL capabilities, as\nwell as tasks involving the grounding of complex instructions. To thoroughly\ninvestigate the human-LMM gap and its underlying causes, we propose MOAT, a\ndiverse benchmark with complex real-world VL tasks that are challenging for\nLMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist\nproblem solving by integrating fundamental VL capabilities such as reading\ntext, counting, understanding spatial relations, grounding textual and visual\ninstructions, etc. All these abilities fit into a taxonomy proposed by us that\ncontains 10 fundamental VL capabilities, enabling MOAT to provide a\nfine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first\nbenchmark to explicitly evaluate LMMs' ability to ground complex text and\nvisual instructions, which is essential to many real-world applications. We\nevaluate over 20 proprietary and open source LMMs, as well as humans, on MOAT,\nand found that humans achieved 82.7% accuracy while the best performing LMM\n(OpenAI o1) achieved only 38.8%. To guide future model development, we analyze\ncommon trends in our results and discuss the underlying causes of observed\nperformance gaps between LMMs and humans, focusing on which VL capability forms\nthe bottleneck in complex tasks, whether test time scaling improves performance\non MOAT, and how tiling harms LMMs' capability to count. Code and data are\navailable at https://cambrian-yzt.github.io/MOAT.",
      "tldr_zh": "该研究提出MOAT基准测试，用于系统评估大型多模态模型(LMMs)在复杂视觉语言任务中的能力整合与指令理解性能。该基准包含10类基础视觉语言能力分类，并首次专门测试LMMs对复杂文本和视觉指令的解析能力。实验结果显示，人类表现(82.7%准确率)显著优于最优LMM模型(38.8%)，揭示了当前模型在空间关系理解、文本阅读等基础能力上的瓶颈。研究还发现测试时扩展(tiling)会损害模型的计数能力，为未来模型优化提供了方向依据。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Project page: https://cambrian-yzt.github.io/MOAT",
      "pdf_url": "http://arxiv.org/pdf/2503.09348v1",
      "published_date": "2025-03-12 12:49:31 UTC",
      "updated_date": "2025-03-12 12:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:51:12.326629"
    },
    {
      "arxiv_id": "2503.09347v1",
      "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
      "title_zh": "《更安全还是更幸运？作为安全评估者的大语言模型对人工痕迹缺乏鲁棒性》",
      "authors": [
        "Hongyu Chen",
        "Seraphina Goldfarb-Tarrant"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
      "tldr_zh": "该研究揭示了大型语言模型（LLMs）作为安全评估工具存在的显著缺陷。通过对11种LLM评估模型在关键安全领域的测试，研究发现LLM在重复评估任务中缺乏一致性，且易受输入文本中的道歉或冗长表达等人工痕迹（artifacts）影响，导致评估结果严重偏差，例如道歉语言可使评估偏好偏差高达98%。此外，更大规模的模型并未表现出更强的鲁棒性，而较小的模型有时反而对特定人工痕迹更具抵抗力。尽管通过多模型集成（jury-based evaluations）提高了鲁棒性并更接近人类判断，但对人工痕迹的敏感性依然存在。研究强调，亟需开发多样化且抗人工痕迹的方法，以确保安全评估的可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.09347v1",
      "published_date": "2025-03-12 12:49:02 UTC",
      "updated_date": "2025-03-12 12:49:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:51:25.934976"
    },
    {
      "arxiv_id": "2503.09335v1",
      "title": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model",
      "title_zh": "NVP-HRI：基于大语言模型的零样本自然语音与姿态人机交互系统",
      "authors": [
        "Yuzhi Lai",
        "Shenghai Yuan",
        "Youssef Nassar",
        "Mingyu Fan",
        "Thomas Weber",
        "Matthias Rätsch"
      ],
      "abstract": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git.",
      "tldr_zh": "该研究提出NVP-HRI，一种基于自然语音和姿态的零样本人机交互框架。该系统结合Segment Anything Model（SAM）和大型语言模型（LLM），通过多模态指令实现对新物体的零样本交互，无需预先训练。实验表明，该方法在通用机器人上执行真实任务时，效率比传统手势控制提升59.2%，同时通过控制语法有效降低了LLM的幻觉风险。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been accepted for publication in ESWA @ 2025 Elsevier.\n  Personal use of this material is permitted. Permission from Elsevier must be\n  obtained for all other uses, including reprinting/redistribution, creating\n  new works, or reuse of any copyrighted components of this work in other media",
      "pdf_url": "http://arxiv.org/pdf/2503.09335v1",
      "published_date": "2025-03-12 12:30:18 UTC",
      "updated_date": "2025-03-12 12:30:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:51:34.100492"
    },
    {
      "arxiv_id": "2503.09334v1",
      "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
      "title_zh": "CyberLLMInstruct：基于网络安全数据评估微调大语言模型安全性的新数据集",
      "authors": [
        "Adel ElZemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance.",
      "tldr_zh": "该研究提出了CyberLLMInstruct数据集，包含54,928个网络安全指令-响应对，涵盖恶意软件分析、钓鱼模拟等任务，用于评估微调大语言模型(LLMs)的安全风险。实验发现，基于OWASP十大框架的测试显示，微调会显著降低模型安全性（如Llama 3.1 8B的抗提示注入能力从0.95降至0.15），同时模型在CyberMetric基准上可达92.50%准确率，揭示了性能与安全性的权衡问题。该研究为开发兼顾性能和安全性的微调方法提供了重要基准。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "The paper is submitted to \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval\" and is\n  currently under review",
      "pdf_url": "http://arxiv.org/pdf/2503.09334v1",
      "published_date": "2025-03-12 12:29:27 UTC",
      "updated_date": "2025-03-12 12:29:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:51:41.095457"
    },
    {
      "arxiv_id": "2503.09332v1",
      "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
      "title_zh": "SDD-4DGS：面向4D场景重建的高斯泼溅静态-动态解耦方法",
      "authors": [
        "Dai Sun",
        "Huhao Guan",
        "Kun Zhang",
        "Xike Xie",
        "S. Kevin Zhou"
      ],
      "abstract": "Dynamic and static components in scenes often exhibit distinct properties,\nyet most 4D reconstruction methods treat them indiscriminately, leading to\nsuboptimal performance in both cases. This work introduces SDD-4DGS, the first\nframework for static-dynamic decoupled 4D scene reconstruction based on\nGaussian Splatting. Our approach is built upon a novel probabilistic dynamic\nperception coefficient that is naturally integrated into the Gaussian\nreconstruction pipeline, enabling adaptive separation of static and dynamic\ncomponents. With carefully designed implementation strategies to realize this\ntheoretical framework, our method effectively facilitates explicit learning of\nmotion patterns for dynamic elements while maintaining geometric stability for\nstatic structures. Extensive experiments on five benchmark datasets demonstrate\nthat SDD-4DGS consistently outperforms state-of-the-art methods in\nreconstruction fidelity, with enhanced detail restoration for static structures\nand precise modeling of dynamic motions. The code will be released.",
      "tldr_zh": "该研究提出SDD-4DGS，首个基于高斯泼溅(Gaussian Splatting)的动静分离4D场景重建框架。通过创新的概率动态感知系数，该方法能自适应分离静态结构和动态元素，在保持静态几何稳定性的同时显式学习动态运动模式。在五个基准数据集上的实验表明，该框架在重建保真度上全面超越现有方法，既能增强静态细节还原，又能精确建模动态运动。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09332v1",
      "published_date": "2025-03-12 12:25:58 UTC",
      "updated_date": "2025-03-12 12:25:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:51:48.516998"
    },
    {
      "arxiv_id": "2503.09330v1",
      "title": "Group-robust Machine Unlearning",
      "title_zh": "群体鲁棒的机器遗忘",
      "authors": [
        "Thomas De Min",
        "Subhankar Roy",
        "Stéphane Lathuilière",
        "Elisa Ricci",
        "Massimiliano Mancini"
      ],
      "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
      "tldr_zh": "该研究提出了首个针对非均匀分布遗忘数据的**Group-robust Machine Unlearning**框架MIU（Mutual Information-aware Machine Unlearning），解决传统方法在主导群体数据遗忘时引发的公平性问题。通过**样本分布重加权**和**互信息最小化**技术，MIU在切断模型特征与群体信息关联的同时，有效减少了主导群体的性能下降。实验表明，该方法在三个数据集上优于标准方法，实现了模型遗忘与群体鲁棒性的平衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.09330v1",
      "published_date": "2025-03-12 12:24:05 UTC",
      "updated_date": "2025-03-12 12:24:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:51:57.844905"
    },
    {
      "arxiv_id": "2503.09326v1",
      "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
      "title_zh": "大型语言模型因果推理能力增强研究综述",
      "authors": [
        "Xin Li",
        "Zhuo Cai",
        "Shoujin Wang",
        "Kun Yu",
        "Fang Chen"
      ],
      "abstract": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.",
      "tldr_zh": "这篇综述系统性地探讨了如何增强大语言模型(LLMs)的因果推理能力。研究指出当前LLMs在医疗健康和经济分析等需要强因果推理的领域仍存在局限，为此本文提出了首个全面分类框架，将现有增强方法进行系统归类，并对各类方法进行详细比较。此外，论文还总结了评估LLMs因果推理能力的基准和指标，并展望了这一新兴领域的未来研究方向，为相关研究者和从业者提供了重要参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09326v1",
      "published_date": "2025-03-12 12:20:31 UTC",
      "updated_date": "2025-03-12 12:20:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:52:08.132918"
    },
    {
      "arxiv_id": "2503.09658v1",
      "title": "Towards Robust Model Evolution with Algorithmic Recourse",
      "title_zh": "迈向基于算法追索的稳健模型演化",
      "authors": [
        "Hao-Tsung Yang",
        "Jie Gao",
        "Bo-Yi Liu",
        "Zhi-Xuan Liu"
      ],
      "abstract": "Algorithmic Recourse is a way for users to modify their attributes to align\nwith a model's expectations, thereby improving their outcomes after receiving\nunfavorable decisions. In real-world scenarios, users often need to\nstrategically adjust their attributes to compete for limited resources.\nHowever, such strategic behavior induces users to \"game\" algorithms, causing\nmodel collapse due to distribution shifts. These shifts arise from user\ncompetition, resource constraints, and adaptive user responses. While prior\nresearch on Algorithmic Recourse has explored its effects on both systems and\nusers, the impact of resource constraints and competition over time remains\nunderexplored. In this work, we develop a general framework to model user\nstrategic behaviors and their interactions with decision-making systems under\nresource constraints and competitive dynamics. Through theoretical analysis and\nempirical evaluation, we identify three key phenomena that arise consistently\nin both synthetic and real-world datasets: escalating decision boundaries,\nnon-robust model predictions, and inequitable recourse actions. Finally, we\ndiscuss the broader social implications of these findings and present two\nalgorithmic strategies aimed at mitigating these challenges.",
      "tldr_zh": "该研究提出了一个通用框架，用于建模用户在资源约束和竞争动态下的策略行为及其与决策系统的交互。研究揭示了三个关键现象：决策边界的逐步升级、模型预测的非鲁棒性以及不平等的补救行动。通过理论分析和实证评估，研究进一步提出了两种算法策略，旨在缓解这些挑战，并探讨了其广泛的社会影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T42",
        "I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages,4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09658v1",
      "published_date": "2025-03-12 12:17:34 UTC",
      "updated_date": "2025-03-12 12:17:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:52:19.187846"
    },
    {
      "arxiv_id": "2503.09321v1",
      "title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation",
      "title_zh": "DAVE：音频视觉评估的诊断基准",
      "authors": [
        "Gorjan Radevski",
        "Teodora Popordanoska",
        "Matthew B. Blaschko",
        "Tinne Tuytelaars"
      ],
      "abstract": "Audio-visual understanding is a rapidly evolving field that seeks to\nintegrate and interpret information from both auditory and visual modalities.\nDespite recent advances in multi-modal learning, existing benchmarks often\nsuffer from strong visual bias -- where answers can be inferred from visual\ndata alone -- and provide only aggregate scores that conflate multiple sources\nof error. This makes it difficult to determine whether models struggle with\nvisual understanding, audio interpretation, or audio-visual alignment. In this\nwork, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark\ndataset designed to systematically evaluate audio-visual models across\ncontrolled challenges. DAVE alleviates existing limitations by (i) ensuring\nboth modalities are necessary to answer correctly and (ii) decoupling\nevaluation into atomic subcategories. Our detailed analysis of state-of-the-art\nmodels reveals specific failure modes and provides targeted insights for\nimprovement. By offering this standardized diagnostic framework, we aim to\nfacilitate more robust development of audio-visual models. The dataset is\nreleased: https://github.com/gorjanradevski/dave",
      "tldr_zh": "该研究提出了DAVE（Diagnostic Audio Visual Evaluation）基准测试，旨在解决现有音视频多模态学习评估中存在的视觉偏见和误差来源混杂问题。该基准通过两个关键设计：（1）确保必须结合音频和视觉模态才能正确回答问题，（2）将评估解耦为原子级子类别，从而精确诊断模型在视觉理解、音频解析或音视频对齐方面的缺陷。通过对前沿模型的系统评估，该研究不仅揭示了具体故障模式，还为音视频模型的针对性改进提供了洞见。该标准化诊断框架的发布将促进更鲁棒的音视频模型发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "First two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2503.09321v1",
      "published_date": "2025-03-12 12:12:46 UTC",
      "updated_date": "2025-03-12 12:12:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:52:25.935671"
    },
    {
      "arxiv_id": "2503.09311v1",
      "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
      "title_zh": "自适应政治调查与GPT-4：通过模拟用户交互解决冷启动问题",
      "authors": [
        "Fynn Bachmann",
        "Daan van der Weijden",
        "Lucien Heitz",
        "Cristina Sarasua",
        "Abraham Bernstein"
      ],
      "abstract": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys.",
      "tldr_zh": "该研究探索了利用GPT-4生成模拟用户交互数据来解决自适应政治调查中的冷启动问题。通过比较LLM生成的合成数据与瑞士Smartvote投票建议应用的真实数据分布，研究发现GPT-4能准确模拟不同政党对问卷的回应。实验表明，用合成数据预训练的统计模型可显著降低预测误差，并提高候选人推荐准确率，为政治科学等领域的自适应问卷系统提供了新的数据解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages. Under review at PLOS One",
      "pdf_url": "http://arxiv.org/pdf/2503.09311v1",
      "published_date": "2025-03-12 12:02:36 UTC",
      "updated_date": "2025-03-12 12:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:52:31.464877"
    },
    {
      "arxiv_id": "2503.09309v1",
      "title": "Steering No-Regret Agents in MFGs under Model Uncertainty",
      "title_zh": "模型不确定性下无遗憾智能体在均值场博弈中的引导",
      "authors": [
        "Leo Widmer",
        "Jiawei Huang",
        "Niao He"
      ],
      "abstract": "Incentive design is a popular framework for guiding agents' learning dynamics\ntowards desired outcomes by providing additional payments beyond intrinsic\nrewards. However, most existing works focus on a finite, small set of agents or\nassume complete knowledge of the game, limiting their applicability to\nreal-world scenarios involving large populations and model uncertainty. To\naddress this gap, we study the design of steering rewards in Mean-Field Games\n(MFGs) with density-independent transitions, where both the transition dynamics\nand intrinsic reward functions are unknown. This setting presents non-trivial\nchallenges, as the mediator must incentivize the agents to explore for its\nmodel learning under uncertainty, while simultaneously steer them to converge\nto desired behaviors without incurring excessive incentive payments. Assuming\nagents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic\nexploration algorithms. Theoretically, we establish sub-linear regret\nguarantees for the cumulative gaps between the agents' behaviors and the\ndesired ones. In terms of the steering cost, we demonstrate that our total\nincentive payments incur only sub-linear excess, competing with a baseline\nsteering strategy that stabilizes the target policy as an equilibrium. Our work\npresents an effective framework for steering agents behaviors in\nlarge-population systems under uncertainty.",
      "tldr_zh": "本研究针对均值场博弈(MFGs)中的激励设计问题，提出了在模型不确定条件下的新型引导算法。通过假设智能体具有无遗憾(no-regret)行为特性，作者开发了乐观探索算法，解决了传统方法无法应对大规模群体和未知动态环境的局限性。理论分析表明，该算法能保证智能体行为与预期目标的累积差距呈次线性增长，同时激励支付成本也仅产生次线性超额。这项工作为不确定性环境下的大规模群体系统行为引导提供了有效框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "AISTATS 2025; 34 Pages",
      "pdf_url": "http://arxiv.org/pdf/2503.09309v1",
      "published_date": "2025-03-12 12:02:02 UTC",
      "updated_date": "2025-03-12 12:02:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:11.494464"
    },
    {
      "arxiv_id": "2503.09289v1",
      "title": "Unmask It! AI-Generated Product Review Detection in Dravidian Languages",
      "title_zh": "《揭露真相！达罗毗荼语系中AI生成商品评论的检测研究》",
      "authors": [
        "Somsubhra De",
        "Advait Vats"
      ],
      "abstract": "The rise of Generative AI has led to a surge in AI-generated reviews, often\nposing a serious threat to the credibility of online platforms. Reviews serve\nas the primary source of information about products and services. Authentic\nreviews play a vital role in consumer decision-making. The presence of\nfabricated content misleads consumers, undermines trust and facilitates\npotential fraud in digital marketplaces. This study focuses on detecting\nAI-generated product reviews in Tamil and Malayalam, two low-resource languages\nwhere research in this domain is relatively under-explored. We worked on a\nrange of approaches - from traditional machine learning methods to advanced\ntransformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and\nMalayalamBERT. Our findings highlight the effectiveness of leveraging the\nstate-of-the-art transformers in accurately identifying AI-generated content,\ndemonstrating the potential in enhancing the detection of fake reviews in\nlow-resource language settings.",
      "tldr_zh": "该研究针对低资源的达罗毗荼语系（泰米尔语和马拉雅拉姆语），开发了AI生成商品评论的检测方法。通过比较传统机器学习与多种先进Transformer模型（包括Indic-BERT、XLM-RoBERTa等），研究发现最先进的Transformer能有效识别AI生成内容。这项工作为提升低资源语言环境下虚假评论检测提供了可行方案，有助于维护数字市场的可信度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 9 figures, Accepted to DravidianLangTech Workshop\n  proceedings at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09289v1",
      "published_date": "2025-03-12 11:35:04 UTC",
      "updated_date": "2025-03-12 11:35:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:53:44.556016"
    },
    {
      "arxiv_id": "2503.09277v1",
      "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
      "title_zh": "UniCombine：基于扩散Transformer的统一多条件组合框架",
      "authors": [
        "Haoxuan Wang",
        "Jinlong Peng",
        "Qingdong He",
        "Hao Yang",
        "Ying Jin",
        "Jiafu Wu",
        "Xiaobin Hu",
        "Yanjie Pan",
        "Zhenye Gan",
        "Mingmin Chi",
        "Bo Peng",
        "Yabiao Wang"
      ],
      "abstract": "With the rapid development of diffusion models in image generation, the\ndemand for more powerful and flexible controllable frameworks is increasing.\nAlthough existing methods can guide generation beyond text prompts, the\nchallenge of effectively combining multiple conditional inputs while\nmaintaining consistency with all of them remains unsolved. To address this, we\nintroduce UniCombine, a DiT-based multi-conditional controllable generative\nframework capable of handling any combination of conditions, including but not\nlimited to text prompts, spatial maps, and subject images. Specifically, we\nintroduce a novel Conditional MMDiT Attention mechanism and incorporate a\ntrainable LoRA module to build both the training-free and training-based\nversions. Additionally, we propose a new pipeline to construct\nSubjectSpatial200K, the first dataset designed for multi-conditional generative\ntasks covering both the subject-driven and spatially-aligned conditions.\nExtensive experimental results on multi-conditional generation demonstrate the\noutstanding universality and powerful capability of our approach with\nstate-of-the-art performance.",
      "tldr_zh": "本文提出UniCombine，一种基于扩散Transformer（DiT）的多条件可控生成框架，能灵活组合文本提示、空间图和主体图像等多种条件输入。该框架创新性地采用Conditional MMDiT Attention机制和可训练的LoRA模块，同时开发了首个面向主体驱动和空间对齐条件的多条件生成数据集SubjectSpatial200K。实验表明，该方法在多条件生成任务中展现出卓越的通用性和最先进的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09277v1",
      "published_date": "2025-03-12 11:22:47 UTC",
      "updated_date": "2025-03-12 11:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:53:44.031086"
    },
    {
      "arxiv_id": "2503.09269v1",
      "title": "Single-Qudit Quantum Neural Networks for Multiclass Classification",
      "title_zh": "单量子位神经网络在多类分类中的应用",
      "authors": [
        "Leandro C. Souza",
        "Renato Portugal"
      ],
      "abstract": "This paper proposes a single-qudit quantum neural network for multiclass\nclassification, by using the enhanced representational capacity of\nhigh-dimensional qudit states. Our design employs an $d$-dimensional unitary\noperator, where $d$ corresponds to the number of classes, constructed using the\nCayley transform of a skew-symmetric matrix, to efficiently encode and process\nclass information. This architecture enables a direct mapping between class\nlabels and quantum measurement outcomes, reducing circuit depth and\ncomputational overhead. To optimize network parameters, we introduce a hybrid\ntraining approach that combines an extended activation function -- derived from\na truncated multivariable Taylor series expansion -- with support vector\nmachine optimization for weight determination. We evaluate our model on the\nMNIST and EMNIST datasets, demonstrating competitive accuracy while maintaining\na compact single-qudit quantum circuit. Our findings highlight the potential of\nqudit-based QNNs as scalable alternatives to classical deep learning models,\nparticularly for multiclass classification. However, practical implementation\nremains constrained by current quantum hardware limitations. This research\nadvances quantum machine learning by demonstrating the feasibility of\nhigher-dimensional quantum systems for efficient learning tasks.",
      "tldr_zh": "本文提出了一种基于高维量子态（qudit）的单量子比特量子神经网络（QNN）用于多类分类任务。该模型利用d维酉算子（d对应类别数）和Cayley变换的斜对称矩阵来高效编码类别信息，实现了类别标签与量子测量结果的直接映射，从而减少了电路深度和计算开销。研究者采用混合训练方法，结合截断多元泰勒级数展开的激活函数和支持向量机优化来确定权重参数。在MNIST和EMNIST数据集上的实验表明，该单量子比特量子电路在保持紧凑结构的同时达到了具有竞争力的准确率。该研究展示了高维量子系统作为可扩展的经典深度学习替代方案的潜力，但也指出当前量子硬件限制仍是实际应用的主要障碍。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "24 pages, 3 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.09269v1",
      "published_date": "2025-03-12 11:12:05 UTC",
      "updated_date": "2025-03-12 11:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:49.084374"
    },
    {
      "arxiv_id": "2503.09257v3",
      "title": "DeepInnovation AI: A Global Dataset Mapping the AI innovation from Academic Research to Industrial Patents",
      "title_zh": "DeepInnovation AI：全球人工智能创新图谱数据集——从学术研究到工业专利的映射",
      "authors": [
        "Haixing Gong",
        "Hui Zou",
        "Xingzhou Liang",
        "Shiyuan Meng",
        "Pinlong Cai",
        "Xingcheng Xu",
        "Jingjing Qu"
      ],
      "abstract": "In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file presents approximately one hundred million\ncalculated paper-patent similarity pairs to enhance understanding of how\ntheoretical advancements translate into commercial technologies.\nDeepInnovationAI enables researchers, policymakers, and industry leaders to\nanticipate trends and identify collaboration opportunities. With extensive\ntemporal and geographical scope, it supports detailed analysis of technological\ndevelopment patterns and international competition dynamics, establishing a\nfoundation for modeling AI innovation and technology transfer processes.",
      "tldr_zh": "该研究推出了DeepInnovation AI，一个全面的全球数据集，旨在映射人工智能（AI）从学术研究到工业专利的创新路径。数据集包含三个结构化文件：DeepPatentAI.csv（2,356,204条专利记录）、DeepDiveAI.csv（3,511,929篇学术出版物）和DeepCosineAI.csv（约一亿条论文-专利相似性对）。通过使用大语言模型、多语言文本分析和语义向量邻近分析，该数据集能够准确识别AI相关内容，并创建稳健的创新指标。DeepInnovation AI为研究人员、政策制定者和行业领袖提供了预测趋势和识别合作机会的工具，支持详细分析技术发展模式和国际竞争动态，为AI创新和技术转移过程建模奠定了基础。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.DB",
      "comment": "32 pages and 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09257v3",
      "published_date": "2025-03-12 10:56:02 UTC",
      "updated_date": "2025-03-23 15:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:27.592879"
    },
    {
      "arxiv_id": "2503.09251v1",
      "title": "SCOPE-DTI: Semi-Inductive Dataset Construction and Framework Optimization for Practical Usability Enhancement in Deep Learning-Based Drug Target Interaction Prediction",
      "title_zh": "SCOPE-DTI：面向深度学习药物靶点相互作用预测实用化的半归纳数据集构建与框架优化",
      "authors": [
        "Yigang Chen",
        "Xiang Ji",
        "Ziyue Zhang",
        "Yuming Zhou",
        "Yang-Chi-Dung Lin",
        "Hsi-Yuan Huang",
        "Tao Zhang",
        "Yi Lai",
        "Ke Chen",
        "Chang Su",
        "Xingqiao Lin",
        "Zihao Zhu",
        "Yanggyi Zhang",
        "Kangping Wei",
        "Jiehui Fu",
        "Yixian Huang",
        "Shidong Cui",
        "Shih-Chung Yen",
        "Ariel Warshel",
        "Hsien-Da Huang"
      ],
      "abstract": "Deep learning-based drug-target interaction (DTI) prediction methods have\ndemonstrated strong performance; however, real-world applicability remains\nconstrained by limited data diversity and modeling complexity. To address these\nchallenges, we propose SCOPE-DTI, a unified framework combining a large-scale,\nbalanced semi-inductive human DTI dataset with advanced deep learning modeling.\nConstructed from 13 public repositories, the SCOPE dataset expands data volume\nby up to 100-fold compared to common benchmarks such as the Human dataset. The\nSCOPE model integrates three-dimensional protein and compound representations,\ngraph neural networks, and bilinear attention mechanisms to effectively capture\ncross domain interaction patterns, significantly outperforming state-of-the-art\nmethods across various DTI prediction tasks. Additionally, SCOPE-DTI provides a\nuser-friendly interface and database. We further validate its effectiveness by\nexperimentally identifying anticancer targets of Ginsenoside Rh1. By offering\ncomprehensive data, advanced modeling, and accessible tools, SCOPE-DTI\naccelerates drug discovery research.",
      "tldr_zh": "该研究提出了SCOPE-DTI框架，通过构建大规模半归纳人类药物-靶点相互作用(DTI)数据集并优化深度学习模型，显著提升了DTI预测的实用性。该框架整合了三维蛋白质/化合物表示、图神经网络和双线性注意力机制，在多个DTI预测任务中超越现有最佳方法，同时提供用户友好界面和数据库。实验验证表明，该框架成功识别了人参皂苷Rh1的抗癌靶点，为加速药物发现研究提供了全面数据、先进模型和可用工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09251v1",
      "published_date": "2025-03-12 10:46:25 UTC",
      "updated_date": "2025-03-12 10:46:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:07.122031"
    },
    {
      "arxiv_id": "2503.09249v1",
      "title": "Considering Length Diversity in Retrieval-Augmented Summarization",
      "title_zh": "考虑检索增强摘要中的长度多样性",
      "authors": [
        "Juseon-Do",
        "Jaesung Hwang",
        "Jingun Kwon",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "abstract": "This study investigates retrieval-augmented summarization by specifically\nexamining the impact of exemplar summary lengths under length constraints, not\ncovered by previous work. We propose a Diverse Length-aware Maximal Marginal\nRelevance (DL-MMR) algorithm to better control summary lengths. This algorithm\ncombines the query relevance with diverse target lengths in retrieval-augmented\nsummarization. Unlike previous methods that necessitate exhaustive exemplar\nexemplar relevance comparisons using MMR, DL-MMR considers the exemplar target\nlength as well and avoids comparing exemplars to each other, thereby reducing\ncomputational cost and conserving memory during the construction of an exemplar\npool. Experimental results showed the effectiveness of DL-MMR, which considers\nlength diversity, compared to the original MMR algorithm. DL-MMR additionally\nshowed the effectiveness in memory saving of 781,513 times and computational\ncost reduction of 500,092 times, while maintaining the same level of\ninformativeness.",
      "tldr_zh": "本研究提出了一种考虑长度多样性的检索增强摘要方法DL-MMR（Diverse Length-aware Maximal Marginal Relevance）。该算法在传统MMR算法基础上引入目标摘要长度因素，无需进行范例间两两比较，显著降低了计算成本（减少50万倍）和内存消耗（节省78万倍）。实验表明DL-MMR在保持信息量的同时，能有效控制摘要长度多样性，为检索增强摘要任务提供了更高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, accepted to NAACL 2025 Findings",
      "pdf_url": "http://arxiv.org/pdf/2503.09249v1",
      "published_date": "2025-03-12 10:43:33 UTC",
      "updated_date": "2025-03-12 10:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:24.089882"
    },
    {
      "arxiv_id": "2503.09241v1",
      "title": "In-Context Defense in Computer Agents: An Empirical Study",
      "title_zh": "计算机智能体的上下文防御机制：一项实证研究",
      "authors": [
        "Pei Yang",
        "Hai Ci",
        "Mike Zheng Shou"
      ],
      "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior.",
      "tldr_zh": "这项研究首次系统地探索了计算机智能体的防御机制，提出了基于上下文学习(in-context learning)和链式思维推理(Chain-of-Thought)的\"上下文防御\"方法。通过向智能体注入少量包含恶意环境示例和对应防御策略的示范样本，引导智能体在执行任务前先进行防御性推理，有效抵御环境欺骗攻击。实验表明该方法将弹窗攻击成功率降低91.2%，平均减少74.6%的环境注入攻击，并100%成功防御干扰性广告。关键发现是：(1) 防御性推理必须先于行动计划；(2) 仅需3个以下示范样本即可激发防御行为。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09241v1",
      "published_date": "2025-03-12 10:38:15 UTC",
      "updated_date": "2025-03-12 10:38:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:38.592964"
    },
    {
      "arxiv_id": "2503.09223v1",
      "title": "LREF: A Novel LLM-based Relevance Framework for E-commerce",
      "title_zh": "LREF：一种基于大型语言模型的电商相关性评估新框架",
      "authors": [
        "Tian Tang",
        "Zhixing Tian",
        "Zhenyu Zhu",
        "Chenyang Wang",
        "Haiqing Hu",
        "Guoyu Tang",
        "Lin Liu",
        "Sulong Xu"
      ],
      "abstract": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
      "tldr_zh": "该研究提出LREF框架，利用大语言模型(LLMs)改进电商搜索相关性预测，解决了传统BERT模型知识容量有限和LLM直接应用导致的过召回等问题。该框架包含三阶段优化：数据筛选的监督微调(SFT)、多链式思维推理(Multi-CoT)调优以及去偏好的直接偏好优化(DPO)。实验表明，该方案在离线和在线测试中均显著提升指标，并成功部署于知名电商平台带来商业效益。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09223v1",
      "published_date": "2025-03-12 10:10:30 UTC",
      "updated_date": "2025-03-12 10:10:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:53.964337"
    },
    {
      "arxiv_id": "2503.09217v1",
      "title": "Evaluating the Generalizability of LLMs in Automated Program Repair",
      "title_zh": "评估大语言模型在自动化程序修复中的泛化能力",
      "authors": [
        "Fengjie Li",
        "Jiajun Jiang",
        "Jiajun Sun",
        "Hongyu Zhang"
      ],
      "abstract": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research.",
      "tldr_zh": "本研究评估了11种主流大语言模型(LLMs)在自动程序修复(APR)任务中的泛化能力。实验采用DEFECTS4J-TRANS新数据集测试发现，所有模型在跨数据集测试时性能显著下降，正确补丁和可行补丁数量分别减少49.48%和42.90%。研究还表明，尽管在修复提示中加入额外修复相关信息可使补丁数量提升136.67%，但仍无法达到原始数据集性能，提示工程本身不足以显著提升LLMs的修复能力。该研究为未来APR研究提供了重要实证依据和改进方向。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages, 1 figure, to be published in ICSE2025-NIER",
      "pdf_url": "http://arxiv.org/pdf/2503.09217v1",
      "published_date": "2025-03-12 10:03:58 UTC",
      "updated_date": "2025-03-12 10:03:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:54:52.379147"
    },
    {
      "arxiv_id": "2503.09215v2",
      "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space",
      "title_zh": "其他车辆轨迹同样不可或缺：驾驶世界模型统一视频潜在空间中的自车-他车轨迹",
      "authors": [
        "Jian Zhu",
        "Zhengyu Jia",
        "Tian Gao",
        "Jiaxin Deng",
        "Shidi Li",
        "Fu Liu",
        "Peng Jia",
        "Xianpeng Lang",
        "Xiaolong Sun"
      ],
      "abstract": "Advanced end-to-end autonomous driving systems predict other vehicles'\nmotions and plan ego vehicle's trajectory. The world model that can foresee the\noutcome of the trajectory has been used to evaluate the end-to-end autonomous\ndriving system. However, existing world models predominantly emphasize the\ntrajectory of the ego vehicle and leave other vehicles uncontrollable. This\nlimitation hinders their ability to realistically simulate the interaction\nbetween the ego vehicle and the driving scenario. In addition, it remains a\nchallenge to match multiple trajectories with each vehicle in the video to\ncontrol the video generation. To address above issues, a driving World Model\nnamed EOT-WM is proposed in this paper, unifying Ego-Other vehicle Trajectories\nin videos. Specifically, we first project ego and other vehicle trajectories in\nthe BEV space into the image coordinate to match each trajectory with its\ncorresponding vehicle in the video. Then, trajectory videos are encoded by the\nSpatial-Temporal Variational Auto Encoder to align with driving video latents\nspatially and temporally in the unified visual space. A trajectory-injected\ndiffusion Transformer is further designed to denoise the noisy video latents\nfor video generation with the guidance of ego-other vehicle trajectories. In\naddition, we propose a metric based on control latent similarity to evaluate\nthe controllability of trajectories. Extensive experiments are conducted on the\nnuScenes dataset, and the proposed model outperforms the state-of-the-art\nmethod by 30% in FID and 55% in FVD. The model can also predict unseen driving\nscenes with self-produced trajectories.",
      "tldr_zh": "该研究提出了EOT-WM驾驶世界模型，首次实现了对自车（ego）与他车（other）轨迹的统一控制。通过将鸟瞰图（BEV）轨迹映射到图像坐标，并采用时空变分自编码器（ST-VAE）对齐视频潜在空间，结合轨迹注入的扩散Transformer架构，模型能生成受双轨控制的驾驶场景视频。创新性地提出基于控制潜在相似度的评估指标，在nuScenes数据集上FID和FVD指标分别提升30%和55%。该模型不仅能模拟车辆交互，还可预测未知驾驶场景的自生成轨迹。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09215v2",
      "published_date": "2025-03-12 10:02:18 UTC",
      "updated_date": "2025-03-17 08:07:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:55:18.593539"
    },
    {
      "arxiv_id": "2503.09206v1",
      "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients",
      "title_zh": "《鲁棒非对称异构联邦学习：应对客户端数据污染问题》",
      "authors": [
        "Xiuwen Fang",
        "Mang Ye",
        "Bo Du"
      ],
      "abstract": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL.",
      "tldr_zh": "本文提出了一种鲁棒的非对称异构联邦学习框架(RAHFL)，用于解决模型异构和数据损坏客户端场景下的联邦学习问题。该框架采用多样性增强的监督对比学习技术，通过混合数据增强策略生成复杂样本，提升模型对多样数据损坏模式的鲁棒性；同时设计了一种非对称异构联邦学习策略，允许客户端在协作学习阶段进行选择性单向学习，避免引入低质量信息。实验结果表明，该方法在多种挑战性联邦学习环境中表现出良好的有效性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09206v1",
      "published_date": "2025-03-12 09:52:04 UTC",
      "updated_date": "2025-03-12 09:52:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:55:12.826775"
    },
    {
      "arxiv_id": "2503.09199v1",
      "title": "GENEOnet: Statistical analysis supporting explainability and trustworthiness",
      "title_zh": "GENEOnet：支持可解释性与可信度的统计分析",
      "authors": [
        "Giovanni Bocchi",
        "Patrizio Frosini",
        "Alessandra Micheletti",
        "Alessandro Pedretti",
        "Carmen Gratteri",
        "Filippo Lunghini",
        "Andrea Rosario Beccari",
        "Carmine Talarico"
      ],
      "abstract": "Group Equivariant Non-Expansive Operators (GENEOs) have emerged as\nmathematical tools for constructing networks for Machine Learning and\nArtificial Intelligence. Recent findings suggest that such models can be\ninserted within the domain of eXplainable Artificial Intelligence (XAI) due to\ntheir inherent interpretability. In this study, we aim to verify this claim\nwith respect to GENEOnet, a GENEO network developed for an application in\ncomputational biochemistry by employing various statistical analyses and\nexperiments. Such experiments first allow us to perform a sensitivity analysis\non GENEOnet's parameters to test their significance. Subsequently, we show that\nGENEOnet exhibits a significantly higher proportion of equivariance compared to\nother methods. Lastly, we demonstrate that GENEOnet is on average robust to\nperturbations arising from molecular dynamics. These results collectively serve\nas proof of the explainability, trustworthiness, and robustness of GENEOnet and\nconfirm the beneficial use of GENEOs in the context of Trustworthy Artificial\nIntelligence.",
      "tldr_zh": "本文提出GENEOnet，一种基于群等变非扩张算子(GENEO)的机器学习网络，通过统计分析方法验证了其在可解释人工智能(XAI)领域的应用潜力。研究表明，GENEOnet在计算生物化学应用中展现出显著优于其他方法的等变性特征，并通过敏感性分析和分子动力学扰动测试证明了其参数显著性、鲁棒性和可信性。这些结果为GENEO在可信人工智能(Trustworthy AI)中的应用提供了实证支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09199v1",
      "published_date": "2025-03-12 09:43:48 UTC",
      "updated_date": "2025-03-12 09:43:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:55:28.552685"
    },
    {
      "arxiv_id": "2503.11702v3",
      "title": "Toward a method for LLM-enabled Indoor Navigation",
      "title_zh": "迈向基于大语言模型的室内导航方法",
      "authors": [
        "Alberto Coffrini",
        "Mohammad Amin Zadenoori",
        "Paolo Barsocchi",
        "Francesco Furfari",
        "Antonino Crivello",
        "Alessio Ferrari"
      ],
      "abstract": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 50.54% correct indications and a maximum of 77.78%. The results do\nnot appear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
      "tldr_zh": "本研究探索了利用大型语言模型（LLM，如ChatGPT）从室内地图图像生成自然、上下文感知的导航指令的潜力，以解决室内导航中实时适应性和用户需求的问题。通过在不同真实环境中的测试案例设计与评估，研究发现LLM在解释空间布局、处理用户约束和规划高效路径方面展现了潜力，平均正确指示率为50.54%，最高可达77.78%。性能主要受兴趣点数量和视觉信息丰富度的影响，而非布局或路径的复杂性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 3 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.11702v3",
      "published_date": "2025-03-12 09:32:43 UTC",
      "updated_date": "2025-03-24 11:42:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:55:32.578426"
    },
    {
      "arxiv_id": "2503.09173v1",
      "title": "Long-Term Planning Around Humans in Domestic Environments with 3D Scene Graphs",
      "title_zh": "家庭环境中基于3D场景图的人类长期规划",
      "authors": [
        "Ermanno Bartoli",
        "Dennis Rotondi",
        "Kai O. Arras",
        "Iolanda Leite"
      ],
      "abstract": "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
      "tldr_zh": "该研究提出了一种基于增强型3D场景图(3DSG)的机器人轨迹规划新方法，专门针对家庭环境中的人机交互场景。通过将人类活动偏好、行为模式及空间上下文整合到3DSG表征中，该方法能有效捕捉人类行为对周围物体和空间约束的动态影响。实验表明，该框架能根据人类活动为空间分配适当成本系数，生成既保持任务效率又符合社交礼仪的机器人运动轨迹，显著提升了家庭场景下人机交互的情境感知能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T40",
        "I.2"
      ],
      "primary_category": "cs.RO",
      "comment": "5 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.09173v1",
      "published_date": "2025-03-12 09:00:45 UTC",
      "updated_date": "2025-03-12 09:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:55:38.583536"
    },
    {
      "arxiv_id": "2503.09164v1",
      "title": "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for Skin Cancer Treatment",
      "title_zh": "AI驱动的肿瘤学决策支持：评估皮肤癌治疗数据准备状况",
      "authors": [
        "Joscha Grüger",
        "Tobias Geyer",
        "Tobias Brix",
        "Michael Storck",
        "Sonja Leson",
        "Laura Bley",
        "Carsten Weishaupt",
        "Ralph Bergmann",
        "Stephan A. Braun"
      ],
      "abstract": "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
      "tldr_zh": "该研究评估了皮肤癌治疗中人工智能(AI)临床决策支持系统(CDSS)开发所需的数据准备情况。通过文献综述、数据准备度评估和专家研讨会等方法，研究发现皮肤癌治疗决策关键数据在信息系统中的存在和质量问题，尤其指出非结构化数据提取的困难。研究强调高质量、可访问的数据对AI驱动CDSS在复杂肿瘤学领域成功应用的重要性。",
      "categories": [
        "cs.AI",
        "68T99, 62P10, 92C50",
        "I.2.6; J.3; H.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09164v1",
      "published_date": "2025-03-12 08:49:03 UTC",
      "updated_date": "2025-03-12 08:49:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:12.958572"
    },
    {
      "arxiv_id": "2503.09153v1",
      "title": "Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection",
      "title_zh": "大语言模型的幻觉是否可用？基于大语言模型的负面推理用于假新闻检测",
      "authors": [
        "Chaowei Zhang",
        "Zongling Feng",
        "Zewei Zhang",
        "Jipeng Qiang",
        "Guandong Xu",
        "Yun Li"
      ],
      "abstract": "The questionable responses caused by knowledge hallucination may lead to\nLLMs' unstable ability in decision-making. However, it has never been\ninvestigated whether the LLMs' hallucination is possibly usable to generate\nnegative reasoning for facilitating the detection of fake news. This study\nproposes a novel supervised self-reinforced reasoning rectification approach -\nSR$^3$ that yields both common reasonable reasoning and wrong understandings\n(negative reasoning) for news via LLMs reflection for semantic consistency\nlearning. Upon that, we construct a negative reasoning-based news learning\nmodel called - \\emph{NRFE}, which leverages positive or negative news-reasoning\npairs for learning the semantic consistency between them. To avoid the impact\nof label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that\nonly takes news content as input to inspect the performance of our method by\ndistilling the knowledge from \\emph{NRFE}. The experimental results verified on\nthree popular fake news datasets demonstrate the superiority of our method\ncompared with three kinds of baselines including prompting on LLMs, fine-tuning\non pre-trained SLMs, and other representative fake news detection methods.",
      "tldr_zh": "该研究创新性地探索了大型语言模型(LLMs)幻觉现象的潜在用途，提出了一种基于负向推理(Negative Reasoning)的假新闻检测方法。研究者开发了SR³框架，通过LLMs的自我反思同时生成合理推理和错误理解(负向推理)，并构建NRFE模型利用正负新闻-推理对学习语义一致性。为避免标签暗示偏差，进一步设计了仅接收新闻内容的NRFE-D学生模型进行知识蒸馏。实验结果表明，该方法在三个主流假新闻数据集上优于LLMs提示、预训练SLMs微调等基线模型，为利用模型幻觉改进AI系统开辟了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 12 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2503.09153v1",
      "published_date": "2025-03-12 08:29:59 UTC",
      "updated_date": "2025-03-12 08:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:08.366299"
    },
    {
      "arxiv_id": "2503.09151v2",
      "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
      "title_zh": "Reangle-A-Video：将4D视频生成视为视频到视频的翻译",
      "authors": [
        "Hyeonho Jeong",
        "Suhyeon Lee",
        "Jong Chul Ye"
      ],
      "abstract": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
      "tldr_zh": "本文提出了Reangle-A-Video，一种将单输入视频转换为同步多视角视频的统一框架。该方法将多视角视频生成任务重新定义为视频到视频的翻译，利用公开的图像和视频扩散先验，避免了大规模4D数据集训练的需求。框架分为两个阶段：首先通过自监督方式同步微调图像到视频扩散变换器，提取视图不变的运动信息；然后在推理时使用DUSt3R进行跨视角一致性引导，生成多视角一致的起始图像。实验表明，Reangle-A-Video在静态视角转换和动态相机控制方面优于现有方法，为多视角视频生成提供了新解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://hyeonho99.github.io/reangle-a-video/",
      "pdf_url": "http://arxiv.org/pdf/2503.09151v2",
      "published_date": "2025-03-12 08:26:15 UTC",
      "updated_date": "2025-03-17 13:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:37.648014"
    },
    {
      "arxiv_id": "2503.09144v1",
      "title": "Efficient UAV Swarm-Based Multi-Task Federated Learning with Dynamic Task Knowledge Sharing",
      "title_zh": "高效无人机群多任务联邦学习与动态任务知识共享",
      "authors": [
        "Yubo Yang",
        "Tao Yang",
        "Xiaofeng Wu",
        "Ziyu Guo",
        "Bo Hu"
      ],
      "abstract": "UAV swarms are widely used in emergency communications, area monitoring, and\ndisaster relief. Coordinated by control centers, they are ideal for federated\nlearning (FL) frameworks. However, current UAV-assisted FL methods primarily\nfocus on single tasks, overlooking the need for multi-task training. In\ndisaster relief scenarios, UAVs perform tasks such as crowd detection, road\nfeasibility analysis, and disaster assessment, which exhibit time-varying\ndemands and potential correlations. In order to meet the time-varying\nrequirements of tasks and complete multiple tasks efficiently under resource\nconstraints, in this paper, we propose a UAV swarm based multi-task FL\nframework, where ground emergency vehicles (EVs) collaborate with UAVs to\naccomplish multiple tasks efficiently under constrained energy and bandwidth\nresources. Through theoretical analysis, we identify key factors affecting task\nperformance and introduce a task attention mechanism to dynamically evaluate\ntask importance, thereby achieving efficient resource allocation. Additionally,\nwe propose a task affinity (TA) metric to capture the dynamic correlation among\ntasks, thereby promoting task knowledge sharing to accelerate training and\nimprove the generalization ability of the model in different scenarios. To\noptimize resource allocation, we formulate a two-layer optimization problem to\njointly optimize UAV transmission power, computation frequency, bandwidth\nallocation, and UAV-EV associations. For the inner problem, we derive\nclosed-form solutions for transmission power, computation frequency, and\nbandwidth allocation and apply a block coordinate descent method for\noptimization. For the outer problem, a two-stage algorithm is designed to\ndetermine optimal UAV-EV associations. Furthermore, theoretical analysis\nreveals a trade-off between UAV energy consumption and multi-task performance.",
      "tldr_zh": "该论文提出了一种基于无人机群（UAV swarm）的多任务联邦学习（FL）框架，用于应急救灾场景下的动态任务协同处理。通过引入任务注意力机制评估任务重要性，并提出任务亲和度（TA）指标捕捉任务间动态关联性，实现了跨任务知识共享与资源优化配置。研究采用双层优化方法联合优化无人机传输功率、计算频率和带宽分配，理论分析揭示了无人机能耗与多任务性能之间的权衡关系。该框架显著提升了资源受限环境下多任务联邦学习的效率和模型泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file",
      "pdf_url": "http://arxiv.org/pdf/2503.09144v1",
      "published_date": "2025-03-12 08:13:39 UTC",
      "updated_date": "2025-03-12 08:13:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:23.843879"
    },
    {
      "arxiv_id": "2503.09132v1",
      "title": "Investigation of Frame Differences as Motion Cues for Video Object Segmentation",
      "title_zh": "帧差作为运动线索的视频对象分割研究",
      "authors": [
        "Sota Kawamura",
        "Hirotada Honda",
        "Shugo Nakamura",
        "Takashi Sano"
      ],
      "abstract": "Automatic Video Object Segmentation (AVOS) refers to the task of autonomously\nsegmenting target objects in video sequences without relying on human-provided\nannotations in the first frames. In AVOS, the use of motion information is\ncrucial, with optical flow being a commonly employed method for capturing\nmotion cues. However, the computation of optical flow is resource-intensive,\nmaking it unsuitable for real-time applications, especially on edge devices\nwith limited computational resources. In this study, we propose using frame\ndifferences as an alternative to optical flow for motion cue extraction. We\ndeveloped an extended U-Net-like AVOS model that takes a frame on which\nsegmentation is performed and a frame difference as inputs, and outputs an\nestimated segmentation map. Our experimental results demonstrate that the\nproposed model achieves performance comparable to the model with optical flow\nas an input, particularly when applied to videos captured by stationary\ncameras. Our results suggest the usefulness of employing frame differences as\nmotion cues in cases with limited computational resources.",
      "tldr_zh": "本研究探讨了帧间差异作为运动线索在视频对象分割（AVOS）中的应用。针对光流法计算资源消耗大的问题，提出了一种基于帧间差异的替代方法，并开发了一种扩展的U-Net模型，该模型以待分割帧和帧间差异作为输入，输出估计的分割图。实验结果表明，在静态摄像机拍摄的视频中，该模型的性能与使用光流作为输入的模型相当，尤其适用于计算资源有限的边缘设备。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures, 2 tables. Accepted to The 9th International\n  Conference on Machine Learning and Soft Computing (ICMLSC 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.09132v1",
      "published_date": "2025-03-12 07:42:15 UTC",
      "updated_date": "2025-03-12 07:42:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:22.192195"
    },
    {
      "arxiv_id": "2503.09120v1",
      "title": "On the Internal Representations of Graph Metanetworks",
      "title_zh": "论图元网络的内部表征",
      "authors": [
        "Taesun Yeom",
        "Jaeho Lee"
      ],
      "abstract": "Weight space learning is an emerging paradigm in the deep learning community.\nThe primary goal of weight space learning is to extract informative features\nfrom a set of parameters using specially designed neural networks, often\nreferred to as \\emph{metanetworks}. However, it remains unclear how these\nmetanetworks learn solely from parameters. To address this, we take the first\nstep toward understanding \\emph{representations} of metanetworks, specifically\ngraph metanetworks (GMNs), which achieve state-of-the-art results in this\nfield, using centered kernel alignment (CKA). Through various experiments, we\nreveal that GMNs and general neural networks (\\textit{e.g.,} multi-layer\nperceptrons (MLPs) and convolutional neural networks (CNNs)) differ in terms of\ntheir representation space.",
      "tldr_zh": "该研究首次探讨了图元网络(Graph Metanetworks, GMNs)的内部表示机制，这是一种从深度神经网络参数中提取信息的先进方法。通过使用中心核对齐(Centered Kernel Alignment, CKA)技术，研究发现GMNs与传统神经网络（如多层感知机MLPs和卷积神经网络CNNs）在表示空间上存在显著差异。这一发现为理解元网络如何仅从参数中学习提供了新的见解，推动了权重空间学习领域的发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Workshop on Weight Space Learning",
      "pdf_url": "http://arxiv.org/pdf/2503.09120v1",
      "published_date": "2025-03-12 07:12:34 UTC",
      "updated_date": "2025-03-12 07:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:54.041230"
    },
    {
      "arxiv_id": "2503.09114v1",
      "title": "Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge",
      "title_zh": "有时痛苦但前景可期：边缘设备上语言模型推理的可行性与权衡",
      "authors": [
        "Maximilian Abstreiter",
        "Sasu Tarkoma",
        "Roberto Morabito"
      ],
      "abstract": "The rapid rise of Language Models (LMs) has expanded the capabilities of\nnatural language processing, powering applications from text generation to\ncomplex decision-making. While state-of-the-art LMs often boast hundreds of\nbillions of parameters and are primarily deployed in data centers, recent\ntrends show a growing focus on compact models-typically under 10 billion\nparameters-enabled by techniques such as quantization and other model\ncompression techniques. This shift paves the way for LMs on edge devices,\noffering potential benefits such as enhanced privacy, reduced latency, and\nimproved data sovereignty. However, the inherent complexity of even these\nsmaller models, combined with the limited computing resources of edge hardware,\nraises critical questions about the practical trade-offs in executing LM\ninference outside the cloud. To address these challenges, we present a\ncomprehensive evaluation of generative LM inference on representative CPU-based\nand GPU-accelerated edge devices. Our study measures key performance\nindicators-including memory usage, inference speed, and energy\nconsumption-across various device configurations. Additionally, we examine\nthroughput-energy trade-offs, cost considerations, and usability, alongside an\nassessment of qualitative model performance. While quantization helps mitigate\nmemory overhead, it does not fully eliminate resource bottlenecks, especially\nfor larger models. Our findings quantify the memory and energy constraints that\nmust be considered for practical real-world deployments, offering concrete\ninsights into the trade-offs between model size, inference performance, and\nefficiency. The exploration of LMs at the edge is still in its early stages. We\nhope this study provides a foundation for future research, guiding the\nrefinement of models, the enhancement of inference efficiency, and the\nadvancement of edge-centric AI systems.",
      "tldr_zh": "该研究探讨了在边缘设备上运行语言模型(LM)推理的可行性与权衡。通过对基于CPU和GPU加速的边缘设备进行生成式LM推理的全面评估，研究发现，尽管量化技术有助于缓解内存开销，但资源瓶颈问题依然存在，尤其是对于较大模型。研究量化了实际部署中必须考虑的内存和能源限制，揭示了模型大小、推理性能和效率之间的权衡。这项工作为未来优化模型、提升推理效率和推进边缘AI系统提供了基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is currently under review for publication in an ACM\n  journal. If accepted, the copyright will be transferred to ACM",
      "pdf_url": "http://arxiv.org/pdf/2503.09114v1",
      "published_date": "2025-03-12 07:01:34 UTC",
      "updated_date": "2025-03-12 07:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:56:57.522213"
    },
    {
      "arxiv_id": "2503.09113v1",
      "title": "Constraint-Guided Learning of Data-driven Health Indicator Models: An Application on the Pronostia Bearing Dataset",
      "title_zh": "约束引导的数据驱动健康指标模型学习：基于Pronostia轴承数据集的应用",
      "authors": [
        "Yonas Tefera",
        "Quinten Van Baelen",
        "Maarten Meire",
        "Stijn Luca",
        "Peter Karsmakers"
      ],
      "abstract": "This paper presents a constraint-guided deep learning framework for\ndeveloping physically consistent health indicators in bearing prognostics and\nhealth management. Conventional data-driven methods often lack physical\nplausibility, while physics-based models are limited by incomplete system\nknowledge. To address this, we integrate domain knowledge into deep learning\nusing constraints to enforce monotonicity, bound output values between 1 and 0\n(representing healthy to failed states), and ensure consistency between signal\nenergy trends and health indicator estimates. This eliminates the need for\ncomplex loss term balancing. We implement constraint-guided gradient descent\nwithin an autoencoder architecture, creating a constrained autoencoder.\nHowever, the framework is adaptable to other architectures. Using\ntime-frequency representations of accelerometer signals from the Pronostia\ndataset, our constrained model generates smoother, more reliable degradation\nprofiles compared to conventional methods, aligning with expected physical\nbehavior. Performance is assessed using three metrics: trendability,\nrobustness, and consistency. Compared to a conventional baseline, the\nconstrained model improves all three. Another baseline, incorporating\nmonotonicity via a soft-ranking loss function, outperforms in trendability but\nfalls short in robustness and consistency. An ablation study confirms that the\nmonotonicity constraint enhances trendability, the boundary constraint ensures\nconsistency, and the energy-health consistency constraint improves robustness.\nThese findings highlight the effectiveness of constraint-guided deep learning\nin producing reliable, physically meaningful health indicators, offering a\npromising direction for future prognostic applications.",
      "tldr_zh": "本文提出了一种基于约束的深度学习框架，用于开发轴承预测与健康管理中的物理一致性健康指标。传统数据驱动方法缺乏物理合理性，而物理模型受限于系统知识的不完整性。为此，作者通过约束将领域知识融入深度学习，强制健康指标的单调性、输出值范围（1到0表示健康到失效状态）以及信号能量趋势与健康指标估计的一致性，从而避免了复杂损失项平衡的需求。该框架采用自编码器架构实现约束梯度下降，生成更平滑、更可靠的退化曲线，并通过趋势性、鲁棒性和一致性三个指标验证其性能。实验表明，该方法在生成物理意义明确的健康指标方面具有显著优势，为未来预测应用提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09113v1",
      "published_date": "2025-03-12 07:01:27 UTC",
      "updated_date": "2025-03-12 07:01:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:57:20.966371"
    },
    {
      "arxiv_id": "2503.09106v1",
      "title": "Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery",
      "title_zh": "冻结与聚类：无需回放的持续类别发现的简单基线",
      "authors": [
        "Chuyu Zhang",
        "Xueyang Yu",
        "Peiyan Gu",
        "Xuming He"
      ],
      "abstract": "This paper addresses the problem of Rehearsal-Free Continual Category\nDiscovery (RF-CCD), which focuses on continuously identifying novel class by\nleveraging knowledge from labeled data. Existing methods typically train from\nscratch, overlooking the potential of base models, and often resort to data\nstorage to prevent forgetting. Moreover, because RF-CCD encompasses both\ncontinual learning and novel class discovery, previous approaches have\nstruggled to effectively integrate advanced techniques from these fields,\nresulting in less convincing comparisons and failing to reveal the unique\nchallenges posed by RF-CCD. To address these challenges, we lead the way in\nintegrating advancements from both domains and conducting extensive experiments\nand analyses. Our findings demonstrate that this integration can achieve\nstate-of-the-art results, leading to the conclusion that in the presence of\npre-trained models, the representation does not improve and may even degrade\nwith the introduction of unlabeled data. To mitigate representation\ndegradation, we propose a straightforward yet highly effective baseline method.\nThis method first utilizes prior knowledge of known categories to estimate the\nnumber of novel classes. It then acquires representations using a model\nspecifically trained on the base classes, generates high-quality pseudo-labels\nthrough k-means clustering, and trains only the classifier layer. We validate\nour conclusions and methods by conducting extensive experiments across multiple\nbenchmarks, including the Stanford Cars, CUB, iNat, and Tiny-ImageNet datasets.\nThe results clearly illustrate our findings, demonstrate the effectiveness of\nour baseline, and pave the way for future advancements in RF-CCD.",
      "tldr_zh": "本文提出了一种简单而高效的方法来解决无排练连续类别发现(RF-CCD)问题，即在无需存储数据的情况下，利用已知类别的知识持续发现新类别。该方法首先通过预训练模型获取表示，利用k-means聚类生成高质量伪标签，并仅训练分类器层，从而避免表示退化。实验在多个数据集上验证了该方法的有效性，为RF-CCD领域的未来研究提供了新的基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Underreview",
      "pdf_url": "http://arxiv.org/pdf/2503.09106v1",
      "published_date": "2025-03-12 06:46:32 UTC",
      "updated_date": "2025-03-12 06:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:57:15.490524"
    },
    {
      "arxiv_id": "2503.10699v1",
      "title": "Test-Time Discovery via Hashing Memory",
      "title_zh": "基于哈希记忆的测试时发现",
      "authors": [
        "Fan Lyu",
        "Tianle Liu",
        "Zhang Zhang",
        "Fuyuan Hu",
        "Liang Wang"
      ],
      "abstract": "We introduce Test-Time Discovery (TTD) as a novel task that addresses class\nshifts during testing, requiring models to simultaneously identify emerging\ncategories while preserving previously learned ones. A key challenge in TTD is\ndistinguishing newly discovered classes from those already identified. To\naddress this, we propose a training-free, hash-based memory mechanism that\nenhances class discovery through fine-grained comparisons with past test\nsamples. Leveraging the characteristics of unknown classes, our approach\nintroduces hash representation based on feature scale and directions, utilizing\nLocality-Sensitive Hashing (LSH) for efficient grouping of similar samples.\nThis enables test samples to be easily and quickly compared with relevant past\ninstances. Furthermore, we design a collaborative classification strategy,\ncombining a prototype classifier for known classes with an LSH-based classifier\nfor novel ones. To enhance reliability, we incorporate a self-correction\nmechanism that refines memory labels through hash-based neighbor retrieval,\nensuring more stable and accurate class assignments. Experimental results\ndemonstrate that our method achieves good discovery of novel categories while\nmaintaining performance on known classes, establishing a new paradigm in model\ntesting. Our code is available at https://github.com/fanlyu/ttd.",
      "tldr_zh": "该研究提出了测试时发现（Test-Time Discovery, TTD）任务，旨在解决模型在测试阶段遇到类别偏移时，能够同时识别新类别并保留已学习类别的能力。为解决这一挑战，作者设计了一种无需训练的基于哈希的内存机制，通过局部敏感哈希（Locality-Sensitive Hashing, LSH）实现细粒度样本比较，高效分组相似样本。此外，结合原型分类器（已知类别）和LSH分类器（新类别）的协同分类策略，以及基于哈希邻居检索的自校正机制，提升了类别分配的稳定性与准确性。实验结果表明，该方法在发现新类别的同时，保持了已知类别的性能，为模型测试提供了新范式。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10699v1",
      "published_date": "2025-03-12 06:43:01 UTC",
      "updated_date": "2025-03-12 06:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:57:47.195052"
    },
    {
      "arxiv_id": "2503.09101v2",
      "title": "The Shape of Attraction in UMAP: Exploring the Embedding Forces in Dimensionality Reduction",
      "title_zh": "UMAP中的吸引力形态：探索降维中的嵌入力",
      "authors": [
        "Mohammad Tariqul Islam",
        "Jason W. Fleischer"
      ],
      "abstract": "Uniform manifold approximation and projection (UMAP) is among the most\npopular neighbor embedding methods. The method relies on attractive and\nrepulsive forces among high-dimensional data points to obtain a low-dimensional\nembedding. In this paper, we analyze the forces to reveal their effects on\ncluster formations and visualization. Repulsion emphasizes differences,\ncontrolling cluster boundaries and inter-cluster distance. Attraction is more\nsubtle, as attractive tension between points can manifest simultaneously as\nattraction and repulsion in the lower-dimensional mapping. This explains the\nneed for learning rate annealing and motivates the different treatments between\nattractive and repulsive terms. Moreover, by modifying attraction, we improve\nthe consistency of cluster formation under random initialization. Overall, our\nanalysis makes UMAP and similar embedding methods more interpretable, more\nrobust, and more accurate.",
      "tldr_zh": "本研究深入分析了UMAP（Uniform Manifold Approximation and Projection）降维方法中的吸引力和排斥力机制，揭示了它们对聚类形成和可视化的影响。研究发现，排斥力主要控制聚类边界和簇间距离，而吸引力则更为复杂，既表现为低维映射中的吸引力，也可能同时表现为排斥力。通过调整吸引力，研究提高了随机初始化下聚类形成的一致性，使UMAP及其类似方法更具可解释性、鲁棒性和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "9 page + appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.09101v2",
      "published_date": "2025-03-12 06:37:43 UTC",
      "updated_date": "2025-03-18 15:48:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:57:36.813853"
    },
    {
      "arxiv_id": "2503.10697v1",
      "title": "Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion",
      "title_zh": "零样本主体中心生成在创意应用中的熵融合方法",
      "authors": [
        "Kaifeng Zou",
        "Xiaoyi Feng",
        "Peng Wang",
        "Tao Huang",
        "Zizhou Huang",
        "Zhang Haihang",
        "Yuntao Zou",
        "Dagang Li"
      ],
      "abstract": "Generative models are widely used in visual content creation. However,\ncurrent text-to-image models often face challenges in practical\napplications-such as textile pattern design and meme generation-due to the\npresence of unwanted elements that are difficult to separate with existing\nmethods. Meanwhile, subject-reference generation has emerged as a key research\ntrend, highlighting the need for techniques that can produce clean,\nhigh-quality subject images while effectively removing extraneous components.\nTo address this challenge, we introduce a framework for reliable\nsubject-centric image generation. In this work, we propose an entropy-based\nfeature-weighted fusion method to merge the informative cross-attention\nfeatures obtained from each sampling step of the pretrained text-to-image model\nFLUX, enabling a precise mask prediction and subject-centric generation.\nAdditionally, we have developed an agent framework based on Large Language\nModels (LLMs) that translates users' casual inputs into more descriptive\nprompts, leading to highly detailed image generation. Simultaneously, the\nagents extract primary elements of prompts to guide the entropy-based feature\nfusion, ensuring focused primary element generation without extraneous\ncomponents. Experimental results and user studies demonstrate our methods\ngenerates high-quality subject-centric images, outperform existing methods or\nother possible pipelines, highlighting the effectiveness of our approach.",
      "tldr_zh": "该研究提出了一种基于熵融合的零样本主题中心生成框架，用于解决文本到图像生成模型在应用场景中难以去除多余元素的问题。通过熵加权的特征融合方法，结合预训练模型FLUX的跨注意力特征，实现了精确的掩码预测和主题中心生成。同时，利用大语言模型(LLMs)构建的代理框架将用户输入转化为详细提示，并提取主要元素指导生成过程，确保生成高质量且无多余元素的主题图像。实验和用户研究表明，该方法在生成质量上优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 8 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.10697v1",
      "published_date": "2025-03-12 06:27:30 UTC",
      "updated_date": "2025-03-12 06:27:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:57:49.995371"
    },
    {
      "arxiv_id": "2503.09091v2",
      "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
      "title_zh": "多模态基础模型在计算病理学中的应用：综述",
      "authors": [
        "Dong Li",
        "Guihong Wan",
        "Xintao Wu",
        "Xinyu Wu",
        "Xiaohui Chen",
        "Yi He",
        "Christine G. Lian",
        "Peter K. Sorger",
        "Yevgeniy R. Semenov",
        "Chen Zhao"
      ],
      "abstract": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.",
      "tldr_zh": "该综述全面回顾了计算病理学（CPath）中的多模态基础模型（Foundation Models）研究进展，重点分析了基于H&E染色全切片图像（WSIs）和切片级表征的模型。研究将现有的32种多模态基础模型分为三大范式：视觉-语言、视觉-知识图谱和视觉-基因表达，并进一步将视觉-语言模型细分为非LLM（大语言模型）和LLM（大语言模型）两类。此外，文章系统梳理了28种多模态病理数据集，涵盖图像-文本对、指令数据集和图像-其他模态对，并总结了训练与评估策略、下游任务分类以及未来研究方向，旨在为病理学与人工智能交叉领域的研究者提供重要参考。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09091v2",
      "published_date": "2025-03-12 06:03:33 UTC",
      "updated_date": "2025-03-20 16:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:58:09.541114"
    },
    {
      "arxiv_id": "2503.09089v1",
      "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
      "title_zh": "LocAgent：基于图引导的LLM智能体实现代码定位",
      "authors": [
        "Zhaoling Chen",
        "Xiangru Tang",
        "Gangda Deng",
        "Fang Wu",
        "Jialong Wu",
        "Zhiwei Jiang",
        "Viktor Prasanna",
        "Arman Cohan",
        "Xingyao Wang"
      ],
      "abstract": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
      "tldr_zh": "该研究提出了LocAgent框架，通过图引导的大型语言模型(LLM)智能体解决代码定位这一软件维护中的核心难题。该方法将代码库解析为异构图结构，捕获文件、类、函数等代码元素及其依赖关系，使LLM智能体能够通过多跳推理高效定位需要修改的代码位置。实验表明，基于Qwen-2.5-Coder-Instruct-32B模型的LocAgent在文件级定位准确率达92.7%，同时将下游GitHub问题解决成功率提升12%，且成本比专有SOTA模型降低约86%。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09089v1",
      "published_date": "2025-03-12 05:55:01 UTC",
      "updated_date": "2025-03-12 05:55:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:58:00.561443"
    },
    {
      "arxiv_id": "2503.09081v1",
      "title": "Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment",
      "title_zh": "万物皆可文字化：一种语义与时间对齐的简易统一多模态框架",
      "authors": [
        "Xiaowei Bi",
        "Zheyuan Xu"
      ],
      "abstract": "Long Video Question Answering (LVQA) is challenging due to the need for\ntemporal reasoning and large-scale multimodal data processing. Existing methods\nstruggle with retrieving cross-modal information from long videos, especially\nwhen relevant details are sparsely distributed. We introduce UMaT (Unified\nMulti-modal as Text), a retrieval-augmented generation (RAG) framework that\nefficiently processes extremely long videos while maintaining cross-modal\ncoherence. UMaT converts visual and auditory data into a unified textual\nrepresentation, ensuring semantic and temporal alignment. Short video clips are\nanalyzed using a vision-language model, while automatic speech recognition\n(ASR) transcribes dialogue. These text-based representations are structured\ninto temporally aligned segments, with adaptive filtering to remove redundancy\nand retain salient details. The processed data is embedded into a vector\ndatabase, enabling precise retrieval of dispersed yet relevant content.\nExperiments on a benchmark LVQA dataset show that UMaT outperforms existing\nmethods in multimodal integration, long-form video understanding, and sparse\ninformation retrieval. Its scalability and interpretability allow it to process\nvideos over an hour long while maintaining semantic and temporal coherence.\nThese findings underscore the importance of structured retrieval and multimodal\nsynchronization for advancing LVQA and long-form AI systems.",
      "tldr_zh": "该研究提出UMaT框架，通过将多模态数据（视觉和听觉）统一转化为文本表示，实现语义与时间对齐，从而解决长视频问答(LVQA)中的跨模态检索难题。该方法结合视觉语言模型分析短视频片段和语音识别转录对话，通过自适应过滤冗余信息并保留关键内容，构建时序对齐的文本表示数据库。实验表明，UMaT在跨模态整合和稀疏信息检索方面优于现有方法，可处理超过1小时的长视频并保持语义连贯性，为长视频理解系统提供了可扩展且可解释的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09081v1",
      "published_date": "2025-03-12 05:28:24 UTC",
      "updated_date": "2025-03-12 05:28:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:58:16.513874"
    },
    {
      "arxiv_id": "2503.10695v2",
      "title": "Introducing Verification Task of Set Consistency with Set-Consistency Energy Networks",
      "title_zh": "《引入集合一致性验证任务：基于集合一致性能量网络的方法》",
      "authors": [
        "Mooho Song",
        "Hyeryung Son",
        "Jay-Yoon Lee"
      ],
      "abstract": "Examining logical inconsistencies among multiple statements (such as\ncollections of sentences or question-answer pairs) is a crucial challenge in\nmachine learning, particularly for ensuring the safety and reliability of\nmodels. Traditional methods that rely on pairwise comparisons often fail to\ncapture inconsistencies that only emerge when more than two statements are\nevaluated collectively. To address this gap, we introduce the task of\nset-consistency verification, an extension of natural language inference (NLI)\nthat assesses the logical coherence of entire sets rather than isolated pairs.\nBuilding on this task, we present the Set-Consistency Energy Network\n(SC-Energy), a novel model that employs a contrastive loss framework to learn\nthe compatibility among a collection of statements. Our approach not only\nefficiently verifies inconsistencies and pinpoints the specific statements\nresponsible for logical contradictions, but also significantly outperforms\nexisting methods including prompting-based LLM models. Furthermore, we release\ntwo new datasets: Set-LConVQA and Set-SNLI for set-consistency verification\ntask.",
      "tldr_zh": "该研究提出了集合一致性验证任务，旨在评估多个陈述之间的整体逻辑一致性，而非传统的成对比较。为解决这一问题，作者提出了Set-Consistency Energy Network (SC-Energy)模型，该模型采用对比损失框架，能够高效检测不一致性并定位导致逻辑矛盾的特定陈述。实验表明，该模型显著优于现有方法，包括基于提示的大语言模型(LLM)。此外，研究还发布了两个新数据集Set-LConVQA和Set-SNLI，用于集合一致性验证任务。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10695v2",
      "published_date": "2025-03-12 05:11:11 UTC",
      "updated_date": "2025-03-19 04:07:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:59:01.568666"
    },
    {
      "arxiv_id": "2503.13500v1",
      "title": "Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection",
      "title_zh": "长时程视觉指令生成：基于逻辑与属性自省的方法",
      "authors": [
        "Yucheng Suo",
        "Fan Ma",
        "Kaixin Shen",
        "Linchao Zhu",
        "Yi Yang"
      ],
      "abstract": "Visual instructions for long-horizon tasks are crucial as they intuitively\nclarify complex concepts and enhance retention across extended steps. Directly\ngenerating a series of images using text-to-image models without considering\nthe context of previous steps results in inconsistent images, increasing\ncognitive load. Additionally, the generated images often miss objects or the\nattributes such as color, shape, and state of the objects are inaccurate. To\naddress these challenges, we propose LIGER, the first training-free framework\nfor Long-horizon Instruction GEneration with logic and attribute\nself-Reflection. LIGER first generates a draft image for each step with the\nhistorical prompt and visual memory of previous steps. This step-by-step\ngeneration approach maintains consistency between images in long-horizon tasks.\nMoreover, LIGER utilizes various image editing tools to rectify errors\nincluding wrong attributes, logic errors, object redundancy, and identity\ninconsistency in the draft images. Through this self-reflection mechanism,\nLIGER improves the logic and object attribute correctness of the images. To\nverify whether the generated images assist human understanding, we manually\ncurated a new benchmark consisting of various long-horizon tasks.\nHuman-annotated ground truth expressions reflect the human-defined criteria for\nhow an image should appear to be illustrative. Experiments demonstrate the\nvisual instructions generated by LIGER are more comprehensive compared with\nbaseline methods.",
      "tldr_zh": "该研究提出LIGER框架，首个无需训练即可生成长程视觉指令的系统，通过逻辑与属性自检机制解决多步骤图像生成的一致性问题。该方法结合历史提示和视觉记忆逐步生成图像，并利用图像编辑工具修正属性错误、逻辑矛盾等缺陷。实验表明，基于新构建的长程任务基准测试，LIGER生成的视觉指令在逻辑连贯性和对象属性准确性上显著优于基线方法，有效降低用户认知负荷。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13500v1",
      "published_date": "2025-03-12 05:11:02 UTC",
      "updated_date": "2025-03-12 05:11:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:58:46.058037"
    },
    {
      "arxiv_id": "2503.09069v1",
      "title": "Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows",
      "title_zh": "生成式流中高阶轨迹优化的理论保证",
      "authors": [
        "Chengyue Gong",
        "Xiaoyu Li",
        "Yingyu Liang",
        "Jiangxuan Long",
        "Zhenmei Shi",
        "Zhao Song",
        "Yu Tian"
      ],
      "abstract": "Flow matching has emerged as a powerful framework for generative modeling,\noffering computational advantages over diffusion models by leveraging\ndeterministic Ordinary Differential Equations (ODEs) instead of stochastic\ndynamics. While prior work established the worst case optimality of standard\nflow matching under Wasserstein distances, the theoretical guarantees for\nhigher-order flow matching - which incorporates acceleration terms to refine\nsample trajectories - remain unexplored. In this paper, we bridge this gap by\nproving that higher-order flow matching preserves worst case optimality as a\ndistribution estimator. We derive upper bounds on the estimation error for\nsecond-order flow matching, demonstrating that the convergence rates depend\npolynomially on the smoothness of the target distribution (quantified via Besov\nspaces) and key parameters of the ODE dynamics. Our analysis employs neural\nnetwork approximations with carefully controlled depth, width, and sparsity to\nbound acceleration errors across both small and large time intervals,\nultimately unifying these results into a general worst case optimal bound for\nall time steps.",
      "tldr_zh": "该论文从理论上证明了高阶流匹配（higher-order flow matching）在生成模型中的最坏情况最优性。研究者通过引入加速度项来优化样本轨迹，推导出二阶流匹配的估计误差上界，并证明其收敛速度与目标分布在Besov空间中的平滑度及ODE动力学参数呈多项式关系。该研究采用深度、宽度和稀疏性受控的神经网络近似方法，统一分析了不同时间尺度下的加速度误差，为高阶轨迹优化提供了理论保证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2410.11261",
      "pdf_url": "http://arxiv.org/pdf/2503.09069v1",
      "published_date": "2025-03-12 05:07:07 UTC",
      "updated_date": "2025-03-12 05:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:58:58.167176"
    },
    {
      "arxiv_id": "2503.09068v1",
      "title": "Probing Network Decisions: Capturing Uncertainties and Unveiling Vulnerabilities Without Label Information",
      "title_zh": "探究网络决策：无标签信息下捕捉不确定性并揭示脆弱性",
      "authors": [
        "Youngju Joung",
        "Sehyun Lee",
        "Jaesik Choi"
      ],
      "abstract": "To improve trust and transparency, it is crucial to be able to interpret the\ndecisions of Deep Neural classifiers (DNNs). Instance-level examinations, such\nas attribution techniques, are commonly employed to interpret the model\ndecisions. However, when interpreting misclassified decisions, human\nintervention may be required. Analyzing the attribu tions across each class\nwithin one instance can be particularly labor intensive and influenced by the\nbias of the human interpreter. In this paper, we present a novel framework to\nuncover the weakness of the classifier via counterfactual examples. A prober is\nintroduced to learn the correctness of the classifier's decision in terms of\nbinary code-hit or miss. It enables the creation of the counterfactual example\nconcerning the prober's decision. We test the performance of our prober's\nmisclassification detection and verify its effectiveness on the image\nclassification benchmark datasets. Furthermore, by generating counterfactuals\nthat penetrate the prober, we demonstrate that our framework effectively\nidentifies vulnerabilities in the target classifier without relying on label\ninformation on the MNIST dataset.",
      "tldr_zh": "本研究提出了一种无需标注信息即可探测深度神经网络(DNN)决策的新框架，通过引入\"prober\"模块以二进制方式(命中/未命中)评估分类器的决策正确性。该方法能自动生成反事实样本(counterfactual examples)，有效识别分类器的潜在漏洞。实验在MNIST等图像分类基准数据集上验证了该框架在误分类检测方面的有效性，为提升DNN的可解释性和可靠性提供了无需人工干预的分析工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ICPRAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.09068v1",
      "published_date": "2025-03-12 05:05:58 UTC",
      "updated_date": "2025-03-12 05:05:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:59:06.443221"
    },
    {
      "arxiv_id": "2503.09642v2",
      "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k",
      "title_zh": "Open-Sora 2.0：以20万美元成本训练商业级视频生成模型",
      "authors": [
        "Xiangyu Peng",
        "Zangwei Zheng",
        "Chenhui Shen",
        "Tom Young",
        "Xinying Guo",
        "Binluo Wang",
        "Hang Xu",
        "Hongxin Liu",
        "Mingyan Jiang",
        "Wenjun Li",
        "Yuhui Wang",
        "Anbang Ye",
        "Gang Ren",
        "Qianran Ma",
        "Wanying Liang",
        "Xiang Lian",
        "Xiwen Wu",
        "Yuting Zhong",
        "Zhuangyan Li",
        "Chaoyu Gong",
        "Guojun Lei",
        "Leijun Cheng",
        "Limin Zhang",
        "Minghao Li",
        "Ruijie Zhang",
        "Silan Hu",
        "Shijie Huang",
        "Xiaokang Wang",
        "Yuanheng Zhao",
        "Yuqi Wang",
        "Ziang Wei",
        "Yang You"
      ],
      "abstract": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
      "tldr_zh": "该研究提出了Open-Sora 2.0，一种仅需20万美元训练成本的商用级视频生成模型。通过数据筛选、模型架构优化、训练策略和系统优化等技术，实现了训练成本的高效控制。实验表明，Open-Sora 2.0在人类评估和VBench评分上与全球领先的开源HunyuanVideo和闭源Runway Gen-3 Alpha模型相当。该模型完全开源，旨在推动视频生成技术的普及，促进内容创作领域的创新。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09642v2",
      "published_date": "2025-03-12 05:00:07 UTC",
      "updated_date": "2025-03-23 13:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:59:14.403654"
    },
    {
      "arxiv_id": "2503.09066v1",
      "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
      "title_zh": "探究大语言模型潜在子空间以增强AI安全性：对抗性状态的识别与操控",
      "authors": [
        "Xin Wei Chia",
        "Jonathan Pan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level.",
      "tldr_zh": "该研究通过分析LLM的潜在激活空间，揭示了安全状态与越狱状态(jailbroken states)在低维子空间中的可区分性。受神经科学吸引子动力学启发，研究者发现可通过扰动向量将安全表征转向越狱状态，实验证明该因果干预能显著提升特定提示的越狱成功率。研究进一步探索了扰动在模型各层的传播特性，为开发基于表征层预防控的模型无关防御技术奠定了基础，突破了传统安全护栏方法的局限。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09066v1",
      "published_date": "2025-03-12 04:59:22 UTC",
      "updated_date": "2025-03-12 04:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:59:12.666315"
    },
    {
      "arxiv_id": "2503.09058v1",
      "title": "Implicit Contrastive Representation Learning with Guided Stop-gradient",
      "title_zh": "隐式对比表征学习：基于引导停止梯度的新方法",
      "authors": [
        "Byeongchan Lee",
        "Sehyun Lee"
      ],
      "abstract": "In self-supervised representation learning, Siamese networks are a natural\narchitecture for learning transformation-invariance by bringing representations\nof positive pairs closer together. But it is prone to collapse into a\ndegenerate solution. To address the issue, in contrastive learning, a\ncontrastive loss is used to prevent collapse by moving representations of\nnegative pairs away from each other. But it is known that algorithms with\nnegative sampling are not robust to a reduction in the number of negative\nsamples. So, on the other hand, there are algorithms that do not use negative\npairs. Many positive-only algorithms adopt asymmetric network architecture\nconsisting of source and target encoders as a key factor in coping with\ncollapse. By exploiting the asymmetric architecture, we introduce a methodology\nto implicitly incorporate the idea of contrastive learning. As its\nimplementation, we present a novel method guided stop-gradient. We apply our\nmethod to benchmark algorithms SimSiam and BYOL and show that our method\nstabilizes training and boosts performance. We also show that the algorithms\nwith our method work well with small batch sizes and do not collapse even when\nthere is no predictor. The code is available at\nhttps://github.com/bych-lee/gsg.",
      "tldr_zh": "该研究提出了一种基于\"引导停止梯度\"(guided stop-gradient)的隐式对比表示学习方法，用于解决自监督学习中Siamese网络易陷入退化解的问题。该方法通过利用非对称网络架构（包含源编码器和目标编码器）隐式实现对比学习思想，无需依赖负样本对。实验表明，该方法能稳定SimSiam和BYOL等基准算法的训练并提升性能，在批次规模较小或无预测器的情况下仍能有效避免模型坍塌。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Neurips 2023",
      "pdf_url": "http://arxiv.org/pdf/2503.09058v1",
      "published_date": "2025-03-12 04:46:53 UTC",
      "updated_date": "2025-03-12 04:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:59:51.004186"
    },
    {
      "arxiv_id": "2503.09051v1",
      "title": "TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction",
      "title_zh": "TreeX：通过关键子树提取生成图神经网络的全局图形化解释",
      "authors": [
        "Shengyao Lu",
        "Jiuding Yang",
        "Baochun Li",
        "Di Niu"
      ],
      "abstract": "The growing demand for transparency and interpretability in critical domains\nhas driven increased interests in comprehending the explainability of\nMessage-Passing (MP) Graph Neural Networks (GNNs). Although substantial\nresearch efforts have been made to generate explanations for individual graph\ninstances, identifying global explaining concepts for a GNN still poses great\nchallenges, especially when concepts are desired in a graphical form on the\ndataset level. While most prior works treat GNNs as black boxes, in this paper,\nwe propose to unbox GNNs by analyzing and extracting critical subtrees incurred\nby the inner workings of message passing, which correspond to critical\nsubgraphs in the datasets. By aggregating subtrees in an embedding space with\nan efficient algorithm, which does not require complex subgraph matching or\nsearch, we can make intuitive graphical explanations for Message-Passing GNNs\non local, class and global levels. We empirically show that our proposed\napproach not only generates clean subgraph concepts on a dataset level in\ncontrast to existing global explaining methods which generate non-graphical\nrules (e.g., language or embeddings) as explanations, but it is also capable of\nproviding explanations for individual instances with a comparable or even\nsuperior performance as compared to leading local-level GNN explainers.",
      "tldr_zh": "该论文提出TreeX方法，通过提取关键子树来生成图神经网络(GNN)的全局图形化解释。不同于现有方法将GNN视为黑箱或生成非图形化解释，TreeX基于消息传递(Message-Passing)机制分析内部运作，提取对应数据集关键子图的子树结构。该方法采用高效的嵌入空间聚合算法，无需复杂子图匹配，即可在实例、类别和全局三个层面生成直观的图形化解释。实验表明，TreeX不仅能生成比现有全局解释方法更清晰的子图概念，其局部解释性能也优于主流GNN解释器。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09051v1",
      "published_date": "2025-03-12 04:36:28 UTC",
      "updated_date": "2025-03-12 04:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:59:32.567251"
    },
    {
      "arxiv_id": "2503.09046v1",
      "title": "Discovering Influential Neuron Path in Vision Transformers",
      "title_zh": "《视觉Transformer中影响力神经元路径的发现》",
      "authors": [
        "Yifan Wang",
        "Yifei Liu",
        "Yingdong Shi",
        "Changming Li",
        "Anqi Pang",
        "Sibei Yang",
        "Jingyi Yu",
        "Kan Ren"
      ],
      "abstract": "Vision Transformer models exhibit immense power yet remain opaque to human\nunderstanding, posing challenges and risks for practical applications. While\nprior research has attempted to demystify these models through input\nattribution and neuron role analysis, there's been a notable gap in considering\nlayer-level information and the holistic path of information flow across\nlayers. In this paper, we investigate the significance of influential neuron\npaths within vision Transformers, which is a path of neurons from the model\ninput to output that impacts the model inference most significantly. We first\npropose a joint influence measure to assess the contribution of a set of\nneurons to the model outcome. And we further provide a layer-progressive neuron\nlocating approach that efficiently selects the most influential neuron at each\nlayer trying to discover the crucial neuron path from input to output within\nthe target model. Our experiments demonstrate the superiority of our method\nfinding the most influential neuron path along which the information flows,\nover the existing baseline solutions. Additionally, the neuron paths have\nillustrated that vision Transformers exhibit some specific inner working\nmechanism for processing the visual information within the same image category.\nWe further analyze the key effects of these neurons on the image classification\ntask, showcasing that the found neuron paths have already preserved the model\ncapability on downstream tasks, which may also shed some lights on real-world\napplications like model pruning. The project website including implementation\ncode is available at https://foundation-model-research.github.io/NeuronPath/.",
      "tldr_zh": "该研究提出了Vision Transformer中\"关键神经元路径\"的概念，指从输入到输出对模型推理影响最大的神经元序列。作者开发了两种创新方法：联合影响力度量评估神经元组贡献，以及分层渐进式神经元定位算法来高效识别各层最具影响力的神经元。实验表明，该方法能有效发现信息流动的核心路径，并揭示同一图像类别处理时Transformer的特定工作机制。这些路径不仅保持了模型在下游任务（如图像分类）的性能，还为模型剪枝等实际应用提供了理论依据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09046v1",
      "published_date": "2025-03-12 04:10:46 UTC",
      "updated_date": "2025-03-12 04:10:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:00:12.076694"
    },
    {
      "arxiv_id": "2503.09035v1",
      "title": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers",
      "title_zh": "ManeuverGPT：用于安全自主特技操控的智能控制",
      "authors": [
        "Shawn Azdam",
        "Pranav Doma",
        "Aliasghar Moj Arab"
      ],
      "abstract": "The next generation of active safety features in autonomous vehicles should\nbe capable of safely executing evasive hazard-avoidance maneuvers akin to those\nperformed by professional stunt drivers to achieve high-agility motion at the\nlimits of vehicle handling. This paper presents a novel framework, ManeuverGPT,\nfor generating and executing high-dynamic stunt maneuvers in autonomous\nvehicles using large language model (LLM)-based agents as controllers. We\ntarget aggressive maneuvers, such as J-turns, within the CARLA simulation\nenvironment and demonstrate an iterative, prompt-based approach to refine\nvehicle control parameters, starting tabula rasa without retraining model\nweights. We propose an agentic architecture comprised of three specialized\nagents (1) a Query Enricher Agent for contextualizing user commands, (2) a\nDriver Agent for generating maneuver parameters, and (3) a Parameter Validator\nAgent that enforces physics-based and safety constraints. Experimental results\ndemonstrate successful J-turn execution across multiple vehicle models through\ntextual prompts that adapt to differing vehicle dynamics. We evaluate\nperformance via established success criteria and discuss limitations regarding\nnumeric precision and scenario complexity. Our findings underscore the\npotential of LLM-driven control for flexible, high-dynamic maneuvers, while\nhighlighting the importance of hybrid approaches that combine language-based\nreasoning with algorithmic validation.",
      "tldr_zh": "该研究提出了ManeuverGPT，一种基于大语言模型(LLM)的自主车辆高动态特技控制框架，旨在实现类似专业特技驾驶员的高敏捷危险避让操作。框架包含三个专门智能体：Query Enricher Agent用于语境化用户指令，Driver Agent生成操控参数，Parameter Validator Agent基于物理和安全约束验证参数。研究在CARLA仿真环境中成功实现了J-turn等高动态特技，并通过文本提示适应不同车辆动力学特性，展示了LLM驱动控制在灵活高动态操作中的潜力，同时强调了语言推理与算法验证结合的混合方法的重要性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "6 Pages, Submitted to IROS",
      "pdf_url": "http://arxiv.org/pdf/2503.09035v1",
      "published_date": "2025-03-12 03:51:41 UTC",
      "updated_date": "2025-03-12 03:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:00:31.306595"
    },
    {
      "arxiv_id": "2503.09033v2",
      "title": "RFUAV: A Benchmark Dataset for Unmanned Aerial Vehicle Detection and Identification",
      "title_zh": "RFUAV：无人机检测与识别的基准数据集",
      "authors": [
        "Rui Shi",
        "Xiaodong Yu",
        "Shengming Wang",
        "Yijia Zhang",
        "Lu Xu",
        "Peng Pan",
        "Chunlai Ma"
      ],
      "abstract": "In this paper, we propose RFUAV as a new benchmark dataset for\nradio-frequency based (RF-based) unmanned aerial vehicle (UAV) identification\nand address the following challenges: Firstly, many existing datasets feature a\nrestricted variety of drone types and insufficient volumes of raw data, which\nfail to meet the demands of practical applications. Secondly, existing datasets\noften lack raw data covering a broad range of signal-to-noise ratios (SNR), or\ndo not provide tools for transforming raw data to different SNR levels. This\nlimitation undermines the validity of model training and evaluation. Lastly,\nmany existing datasets do not offer open-access evaluation tools, leading to a\nlack of unified evaluation standards in current research within this field.\nRFUAV comprises approximately 1.3 TB of raw frequency data collected from 37\ndistinct UAVs using the Universal Software Radio Peripheral (USRP) device in\nreal-world environments. Through in-depth analysis of the RF data in RFUAV, we\ndefine a drone feature sequence called RF drone fingerprint, which aids in\ndistinguishing drone signals. In addition to the dataset, RFUAV provides a\nbaseline preprocessing method and model evaluation tools. Rigorous experiments\ndemonstrate that these preprocessing methods achieve state-of-the-art (SOTA)\nperformance using the provided evaluation tools. The RFUAV dataset and baseline\nimplementation are publicly available at https://github.com/kitoweeknd/RFUAV/.",
      "tldr_zh": "该研究提出了RFUAV基准数据集，用于基于射频信号（RF-based）的无人机（UAV）检测与识别。该数据集包含37种不同无人机在真实环境中采集的约1.3TB原始频率数据，解决了现有数据集无人机类型单一、数据量不足以及缺乏宽范围信噪比（SNR）覆盖等关键问题。研究还定义了名为\"RF drone fingerprint\"的无人机特征序列用于信号区分，并提供了数据预处理方法和评估工具。实验表明，该方案达到了当前最优（SOTA）性能，为无人机射频识别研究建立了统一标准。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "23 pages, 13 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2503.09033v2",
      "published_date": "2025-03-12 03:46:09 UTC",
      "updated_date": "2025-03-18 03:28:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:01:01.188843"
    },
    {
      "arxiv_id": "2503.09032v1",
      "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
      "title_zh": "教授LLMs如何学习：基于上下文微调的方法",
      "authors": [
        "Younwoo Choi",
        "Muhammad Adil Asif",
        "Ziwen Han",
        "John Willes",
        "Rahul G. Krishnan"
      ],
      "abstract": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.",
      "tldr_zh": "该研究提出了一种称为上下文微调(contextual fine-tuning)的新方法，旨在通过模仿人类学习和问题解决的认知策略来指导大型语言模型(LLMs)的学习过程。该方法利用教学提示(prompting)来帮助LLMs在新领域中进行快速微调，特别是在医学和金融领域。实验表明，这种简单而有效的改进显著提升了LLMs在新数据集上的微调能力，使其能够更好地理解和解释领域特定知识。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.09032v1",
      "published_date": "2025-03-12 03:45:53 UTC",
      "updated_date": "2025-03-12 03:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:00:56.642762"
    },
    {
      "arxiv_id": "2503.09020v2",
      "title": "Enhancing High-Quality Code Generation in Large Language Models with Comparative Prefix-Tuning",
      "title_zh": "《通过对比前缀调优增强大语言模型的高质量代码生成能力》",
      "authors": [
        "Yuan Jiang",
        "Yujian Zhang",
        "Liang Lu",
        "Christoph Treude",
        "Xiaohong Su",
        "Shan Huang",
        "Tiantian Wang"
      ],
      "abstract": "Large Language Models (LLMs) have been widely adopted in commercial code\ncompletion engines, significantly enhancing coding efficiency and productivity.\nHowever, LLMs may generate code with quality issues that violate coding\nstandards and best practices, such as poor code style and maintainability, even\nwhen the code is functionally correct. This necessitates additional effort from\ndevelopers to improve the code, potentially negating the efficiency gains\nprovided by LLMs. To address this problem, we propose a novel comparative\nprefix-tuning method for controllable high-quality code generation. Our method\nintroduces a single, property-specific prefix that is prepended to the\nactivations of the LLM, serving as a lightweight alternative to fine-tuning.\nUnlike existing methods that require training multiple prefixes, our approach\ntrains only one prefix and leverages pairs of high-quality and low-quality code\nsamples, introducing a sequence-level ranking loss to guide the model's\ntraining. This comparative approach enables the model to better understand the\ndifferences between high-quality and low-quality code, focusing on aspects that\nimpact code quality. Additionally, we design a data construction pipeline to\ncollect and annotate pairs of high-quality and low-quality code, facilitating\neffective training. Extensive experiments on the Code Llama 7B model\ndemonstrate that our method improves code quality by over 100% in certain task\ncategories, while maintaining functional correctness. We also conduct ablation\nstudies and generalization experiments, confirming the effectiveness of our\nmethod's components and its strong generalization capability.",
      "tldr_zh": "该研究提出了一种新型的**比较前缀调优方法**(Comparative Prefix-Tuning)，用于提升大语言模型(LLMs)生成高质量代码的能力。该方法仅需训练一个轻量级的属性特定前缀，通过对比高质量与低质量代码样本对，并采用序列级排序损失(sequence-level ranking loss)指导模型学习代码质量差异。实验表明，在Code Llama 7B模型上，该方法可使特定任务类别的代码质量提升超100%，同时保持功能正确性。研究还设计了专门的数据构建流程来收集标注代码质量对比样本，并通过消融实验验证了方法的有效性和强泛化能力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09020v2",
      "published_date": "2025-03-12 03:15:46 UTC",
      "updated_date": "2025-03-19 07:24:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:01:02.930822"
    },
    {
      "arxiv_id": "2503.09639v2",
      "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
      "title_zh": "生成智能体社会能否模拟人类行为并为公共卫生政策提供参考？以疫苗犹豫为例",
      "authors": [
        "Abe Bohan Hou",
        "Hongru Du",
        "Yichen Wang",
        "Jingyu Zhang",
        "Zixiao Wang",
        "Paul Pu Liang",
        "Daniel Khashabi",
        "Lauren Gardner",
        "Tianxing He"
      ],
      "abstract": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.",
      "tldr_zh": "该研究提出了VacSim框架，利用100个基于大语言模型(LLMs)的生成式智能体，探索模拟人类社会行为以辅助公共卫生政策评估的可行性。以疫苗犹豫(Vaccine Hesitancy)为案例，该框架通过三个关键步骤：1)基于人口普查数据生成具有人口统计学特征的智能体群体；2)构建社交网络并建模社会动态和疾病信息对疫苗态度的影响；3)设计评估减轻疫苗犹豫的公共卫生干预措施。实验发现Llama和Qwen等模型能模拟部分人类行为特征，但也揭示了与真实人口统计资料响应不一致等对齐挑战。这项早期研究旨在呼吁关注LLM驱动的社会模拟在政策制定中的潜在价值，而非提供明确的政策指导。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09639v2",
      "published_date": "2025-03-12 02:54:15 UTC",
      "updated_date": "2025-03-16 06:03:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:01:22.164406"
    },
    {
      "arxiv_id": "2503.09008v1",
      "title": "Towards Quantifying Long-Range Interactions in Graph Machine Learning: a Large Graph Dataset and a Measurement",
      "title_zh": "迈向量化图机器学习中的长程交互：大型图数据集与测量方法",
      "authors": [
        "Huidong Liang",
        "Haitz Sáez de Ocáriz Borde",
        "Baskaran Sripathmanathan",
        "Michael Bronstein",
        "Xiaowen Dong"
      ],
      "abstract": "Long-range dependencies are critical for effective graph representation\nlearning, yet most existing datasets focus on small graphs tailored to\ninductive tasks, offering limited insight into long-range interactions. Current\nevaluations primarily compare models employing global attention (e.g., graph\ntransformers) with those using local neighborhood aggregation (e.g.,\nmessage-passing neural networks) without a direct measurement of long-range\ndependency. In this work, we introduce City-Networks, a novel large-scale\ntransductive learning dataset derived from real-world city roads. This dataset\nfeatures graphs with over $10^5$ nodes and significantly larger diameters than\nthose in existing benchmarks, naturally embodying long-range information. We\nannotate the graphs using an eccentricity-based approach, ensuring that the\nclassification task inherently requires information from distant nodes.\nFurthermore, we propose a model-agnostic measurement based on the Jacobians of\nneighbors from distant hops, offering a principled quantification of long-range\ndependencies. Finally, we provide theoretical justifications for both our\ndataset design and the proposed measurement - particularly by focusing on\nover-smoothing and influence score dilution - which establishes a robust\nfoundation for further exploration of long-range interactions in graph neural\nnetworks.",
      "tldr_zh": "该研究针对图机器学习中的长程依赖关系问题，提出了City-Networks数据集和新型评估方法。通过从真实城市道路网络构建包含超过10^5个节点的大规模转导学习数据集，其直径显著大于现有基准，自然包含长程信息。研究创新性地提出基于雅可比矩阵的模型无关评估指标，通过分析远跳邻居的影响来量化长程依赖，并针对图神经网络中的过平滑和影响力稀释问题提供了理论解释。这项工作为系统研究图神经网络的远程交互能力奠定了理论基础和评估框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.09008v1",
      "published_date": "2025-03-12 02:51:17 UTC",
      "updated_date": "2025-03-12 02:51:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:01:24.964559"
    },
    {
      "arxiv_id": "2503.09002v1",
      "title": "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers",
      "title_zh": "KNighter：利用LLM合成检查器革新静态分析",
      "authors": [
        "Chenyuan Yang",
        "Zijie Zhao",
        "Zichen Xie",
        "Haoyu Li",
        "Lingming Zhang"
      ],
      "abstract": "Static analysis is a powerful technique for bug detection in critical systems\nlike operating system kernels. However, designing and implementing static\nanalyzers is challenging, time-consuming, and typically limited to predefined\nbug patterns. While large language models (LLMs) have shown promise for static\nanalysis, directly applying them to scan large codebases remains impractical\ndue to computational constraints and contextual limitations.\n  We present KNighter, the first approach that unlocks practical LLM-based\nstatic analysis by automatically synthesizing static analyzers from historical\nbug patterns. Rather than using LLMs to directly analyze massive codebases, our\nkey insight is leveraging LLMs to generate specialized static analyzers guided\nby historical patch knowledge. KNighter implements this vision through a\nmulti-stage synthesis pipeline that validates checker correctness against\noriginal patches and employs an automated refinement process to iteratively\nreduce false positives. Our evaluation on the Linux kernel demonstrates that\nKNighter generates high-precision checkers capable of detecting diverse bug\npatterns overlooked by existing human-written analyzers. To date,\nKNighter-synthesized checkers have discovered 70 new bugs/vulnerabilities in\nthe Linux kernel, with 56 confirmed and 41 already fixed. 11 of these findings\nhave been assigned CVE numbers. This work establishes an entirely new paradigm\nfor scalable, reliable, and traceable LLM-based static analysis for real-world\nsystems via checker synthesis.",
      "tldr_zh": "该研究提出了KNighter系统，开创性地利用大语言模型(LLMs)从历史漏洞模式自动合成静态分析工具，而非直接扫描代码。通过多阶段合成管道，系统能验证检查器正确性并自动优化以减少误报。在Linux内核测试中，KNighter生成的检查器发现了70个新漏洞(56个已确认，41个修复)，其中11个获得CVE编号，其检测能力超越了人工编写的分析器。该工作为基于LLM的可扩展静态分析建立了新范式。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.OS"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09002v1",
      "published_date": "2025-03-12 02:30:19 UTC",
      "updated_date": "2025-03-12 02:30:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:01:56.700851"
    },
    {
      "arxiv_id": "2503.13499v1",
      "title": "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
      "title_zh": "利用知识图谱与大型语言模型实现情境感知消息传递",
      "authors": [
        "Rajeev Kumar",
        "Harishankar Kumar",
        "Kumari Shalini"
      ],
      "abstract": "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
      "tldr_zh": "该研究提出了一种结合知识图谱(KG)和大语言模型(LLM)的上下文感知消息生成框架。通过将消息中的实体与知识图谱节点动态关联，整合个人偏好、职业角色等上下文信息，再利用LLM生成个性化回复。实验显示，该框架在医疗、教育和招聘领域的消息采纳率分别达到42%、53%和78%，为多领域智能沟通提供了可扩展的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13499v1",
      "published_date": "2025-03-12 02:17:15 UTC",
      "updated_date": "2025-03-12 02:17:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:02:14.095373"
    },
    {
      "arxiv_id": "2503.08994v1",
      "title": "DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation",
      "title_zh": "DistJoin：基于自适应神经谓词调制的解耦式连接基数估计器",
      "authors": [
        "Kaixin Zhang",
        "Hongzhi Wang",
        "Ziqi Li",
        "Yabin Lu",
        "Yingze Li",
        "Yu Yan",
        "Yiming Guan"
      ],
      "abstract": "Research on learned cardinality estimation has achieved significant progress\nin recent years. However, existing methods still face distinct challenges that\nhinder their practical deployment in production environments. We conceptualize\nthese challenges as the \"Trilemma of Cardinality Estimation\", where learned\ncardinality estimation methods struggle to balance generality, accuracy, and\nupdatability. To address these challenges, we introduce DistJoin, a join\ncardinality estimator based on efficient distribution prediction using\nmulti-autoregressive models. Our contributions are threefold: (1) We propose a\nmethod for estimating both equi and non-equi join cardinality by leveraging the\nconditional probability distributions of individual tables in a decoupled\nmanner. (2) To meet the requirements of efficient training and inference for\nDistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a\nhigh-throughput conditional probability distribution estimation model. (3) We\nformally analyze the variance of existing similar methods and demonstrate that\nsuch approaches suffer from variance accumulation issues. To mitigate this\nproblem, DistJoin employs a selectivity-based approach rather than a\ncount-based approach to infer join cardinality, effectively reducing variance.\nIn summary, DistJoin not only represents the first data-driven method to\neffectively support both equi and non-equi joins but also demonstrates superior\naccuracy while enabling fast and flexible updates. We evaluate DistJoin on\nJOB-light and JOB-light-ranges, extending the evaluation to non-equi join\nconditions. The results demonstrate that our approach achieves the highest\naccuracy, robustness to data updates, generality, and comparable update and\ninference speed relative to existing methods.",
      "tldr_zh": "该研究提出DistJoin——一种基于自适应神经谓词调制(ANPM)的解耦式连接基数估计器，解决了现有方法难以兼顾\"基数估计三难困境\"(generality/accuracy/updatability)的问题。其核心创新包括：(1) 通过解耦方式利用单表条件概率分布来同时支持等值连接和非等值连接的基数估计；(2) 开发高吞吐的ANPM模型实现高效条件概率分布估计；(3) 采用基于选择性的方法而非计数法来降低方差累积问题。实验表明，DistJoin在JOB-light系列基准测试中实现了最高精度，同时保持数据更新鲁棒性和高效推理速度，成为首个有效支持两类连接的数据驱动方法。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08994v1",
      "published_date": "2025-03-12 02:07:08 UTC",
      "updated_date": "2025-03-12 02:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:02:29.348555"
    },
    {
      "arxiv_id": "2503.09638v1",
      "title": "Edge AI-Powered Real-Time Decision-Making for Autonomous Vehicles in Adverse Weather Conditions",
      "title_zh": "边缘AI驱动的恶劣天气条件下自动驾驶车辆实时决策系统",
      "authors": [
        "Milad Rahmati"
      ],
      "abstract": "Autonomous vehicles (AVs) are transforming modern transportation, but their\nreliability and safety are significantly challenged by harsh weather conditions\nsuch as heavy rain, fog, and snow. These environmental factors impair the\nperformance of cameras, LiDAR, and radar, leading to reduced situational\nawareness and increased accident risks. Conventional cloud-based AI systems\nintroduce communication delays, making them unsuitable for the rapid\ndecision-making required in real-time autonomous navigation. This paper\npresents a novel Edge AI-driven real-time decision-making framework designed to\nenhance AV responsiveness under adverse weather conditions. The proposed\napproach integrates convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) for improved perception, alongside reinforcement learning\n(RL)-based strategies to optimize vehicle control in uncertain environments. By\nprocessing data at the network edge, this system significantly reduces decision\nlatency while improving AV adaptability. The framework is evaluated using\nsimulated driving scenarios in CARLA and real-world data from the Waymo Open\nDataset, covering diverse weather conditions. Experimental results indicate\nthat the proposed model achieves a 40% reduction in processing time and a 25%\nenhancement in perception accuracy compared to conventional cloud-based\nsystems. These findings highlight the potential of Edge AI in improving AV\nautonomy, safety, and efficiency, paving the way for more reliable self-driving\ntechnology in challenging real-world environments.",
      "tldr_zh": "该论文提出了一种基于边缘计算(Edge AI)的实时决策框架，用于提升自动驾驶车辆(AVs)在恶劣天气条件下的性能。该方法结合卷积神经网络(CNN)和循环神经网络(RNN)增强环境感知能力，并采用强化学习(RL)策略优化车辆控制。通过在网络边缘处理数据，系统显著降低了决策延迟，在CARLA仿真平台和Waymo真实数据集上的测试表明：相比传统云端方案，该框架处理时间减少40%，感知准确率提升25%。这项研究为恶劣天气下的自动驾驶提供了更安全可靠的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09638v1",
      "published_date": "2025-03-12 02:02:05 UTC",
      "updated_date": "2025-03-12 02:02:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:01:53.269882"
    },
    {
      "arxiv_id": "2503.08990v1",
      "title": "JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing",
      "title_zh": "JBFuzz：基于模糊测试的高效LLM越狱方法",
      "authors": [
        "Vasudev Gohil"
      ],
      "abstract": "Large language models (LLMs) have shown great promise as language\nunderstanding and decision making tools, and they have permeated various\naspects of our everyday life. However, their widespread availability also comes\nwith novel risks, such as generating harmful, unethical, or offensive content,\nvia an attack called jailbreaking. Despite extensive efforts from LLM\ndevelopers to align LLMs using human feedback, they are still susceptible to\njailbreak attacks. To tackle this issue, researchers often employ red-teaming\nto understand and investigate jailbreak prompts. However, existing red-teaming\napproaches lack effectiveness, scalability, or both. To address these issues,\nwe propose JBFuzz, a novel effective, automated, and scalable red-teaming\ntechnique for jailbreaking LLMs.\n  JBFuzz is inspired by the success of fuzzing for detecting\nbugs/vulnerabilities in software. We overcome three challenges related to\neffectiveness and scalability by devising novel seed prompts, a lightweight\nmutation engine, and a lightweight and accurate evaluator for guiding the\nfuzzer. Assimilating all three solutions results in a potent fuzzer that only\nrequires black-box access to the target LLM. We perform extensive experimental\nevaluation of JBFuzz using nine popular and widely-used LLMs. We find that\nJBFuzz successfully jailbreaks all LLMs for various harmful/unethical\nquestions, with an average attack success rate of 99%. We also find that JBFuzz\nis extremely efficient as it jailbreaks a given LLM for a given question in 60\nseconds on average. Our work highlights the susceptibility of the\nstate-of-the-art LLMs to jailbreak attacks even after safety alignment, and\nserves as a valuable red-teaming tool for LLM developers.",
      "tldr_zh": "该研究提出了JBFuzz，一种基于模糊测试(fuzzing)的高效自动化红队技术，用于检测和突破大型语言模型(LLMs)的安全防护。JBFuzz通过设计新颖的种子提示、轻量级变异引擎和高效评估器，解决了现有红队方法在有效性和可扩展性上的不足。实验表明，JBFuzz在9个主流LLMs上实现了平均99%的攻击成功率，且平均仅需60秒即可突破一个模型的防护。这项研究揭示了当前最先进的LLMs在安全对齐后仍易受越狱攻击的脆弱性，为LLM开发者提供了重要的红队测试工具。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08990v1",
      "published_date": "2025-03-12 01:52:17 UTC",
      "updated_date": "2025-03-12 01:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T06:02:03.291272"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 120,
  "processed_papers_count": 120,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T06:03:57.612084"
}
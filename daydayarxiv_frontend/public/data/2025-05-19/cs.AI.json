{
  "date": "2025-05-19",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-19 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv ç®€ç›´æ˜¯ **\"Reasoningï¼ˆæ¨ç†ï¼‰\"** çš„æˆ˜åœºã€‚å­¦è€…ä»¬ä¸å†æ»¡è¶³äºè®© LLM \"æ€è€ƒ\"ï¼ˆChain of Thoughtï¼‰ï¼Œè€Œæ˜¯å¼€å§‹åæ€ï¼š**æˆ‘ä»¬çœŸçš„éœ€è¦é‚£ä¹ˆå¤šæ¨ç† Token å—ï¼Ÿè¿™äº›æ¨ç†è¿‡ç¨‹æ˜¯çœŸå®çš„è¿˜æ˜¯ä¸ºäº†éª—ç»è´¹ï¼ˆToken è®¡è´¹ï¼‰ï¼Ÿ** æ­¤å¤–ï¼Œ**Mobile Agentï¼ˆç§»åŠ¨ç«¯æ™ºèƒ½ä½“ï¼‰** çš„å®‰å…¨æ€§å’Œæ•°æ®é›†æ„å»ºä¹Ÿè¿æ¥äº†å¤§çˆ†å‘ï¼Œä»¥åŠ **Video World Modelï¼ˆè§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼‰** åœ¨æœºå™¨äººé¢†åŸŸçš„è½åœ°åº”ç”¨ä»¤äººç©ç›®ã€‚\n\n---\n\n### ğŸ§  æ·±åº¦åæ€ï¼šLLM æ¨ç†çš„é»‘ç›’ã€è®¡è´¹ä¸æ•ˆç‡\n*è¿™ä¸€æ¿å—é›†ä¸­è®¨è®ºå½“ä¸‹æœ€ç«çš„ Reasoning Models (å¦‚ o1, DeepSeek-R1 ë¥˜)ã€‚ä»Šå¤©çš„è®ºæ–‡å¯¹â€œæ€ç»´é“¾â€æå‡ºäº†ä¸¥å‰çš„è´¨ç–‘å’Œä¼˜åŒ–çš„è§£æ³•ã€‚*\n\n#### **1. ä½ çš„æ¨¡å‹çœŸçš„åœ¨æ€è€ƒï¼Œè¿˜æ˜¯åœ¨æ³¨æ°´ï¼Ÿ**\n**CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs**\n**(CoInï¼šè®¡ç®—å•†ä¸šä¸é€æ˜ LLM API ä¸­çœ‹ä¸è§çš„æ¨ç† Token)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šè¿™ç¯‡è®ºæ–‡éå¸¸çŠ€åˆ©ã€‚ç°åœ¨çš„å•†ä¸šæ¨ç†æ¨¡å‹ï¼ˆå¦‚ o1ï¼‰åªè¿”å›ç»“æœï¼Œéšè—äº†æ¨ç†è¿‡ç¨‹ï¼Œå´æŒ‰æ¨ç† Token æ”¶è´¹ã€‚ä½œè€…æŒ‡å‡ºè¿™å­˜åœ¨â€œToken é€šèƒ€â€é£é™©ï¼ˆå‚å•†å¯èƒ½æ³¨å…¥æ— æ„ä¹‰ Token å¤šæ”¶é’±ï¼‰ã€‚ä»–ä»¬æå‡ºäº† CoIn æ¡†æ¶ï¼Œä½œä¸ºä¸€ä¸ªç¬¬ä¸‰æ–¹å®¡è®¡å·¥å…·ï¼Œé€šè¿‡ Token åµŒå…¥æŒ‡çº¹æ¥éªŒè¯ Token æ•°é‡å’Œè¯­ä¹‰æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸç‡é«˜è¾¾ 94.7%ã€‚è¿™ä¸ä»…æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæ›´æ˜¯å•†ä¸šä¼¦ç†é—®é¢˜ã€‚\n\n#### **2. æ¨ç†è¿‡ç¨‹å¯èƒ½æ¯«æ— æ„ä¹‰ï¼Ÿ**\n**Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens**\n**(è¶…è¶Šè¯­ä¹‰ï¼šæ— ç†ä¸­é—´ Token çš„éç†æ€§æœ‰æ•ˆæ€§)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šé€šè¿‡å—æ§å®éªŒå‘ç°ï¼Œå³ä½¿ CoTï¼ˆæ€ç»´é“¾ï¼‰ä¸­é—´æ­¥éª¤å®Œå…¨æ˜¯èƒ¡è¨€ä¹±è¯­ï¼ˆcorrupted tracesï¼‰ï¼Œæ¨¡å‹ä¾ç„¶èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œç”šè‡³åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šæ³›åŒ–å¾—æ›´å¥½ï¼è¿™æŒ‘æˆ˜äº†â€œCoT åæ˜ äº†äººç±»æ¨ç†è¿‡ç¨‹â€çš„å‡è®¾ï¼Œè­¦å‘Šæˆ‘ä»¬ä¸è¦è¿‡åº¦æ‹ŸäººåŒ– LLM çš„æ¨ç†è¿‡ç¨‹ã€‚\n\n#### **3. è®©æ¨¡å‹å­¦ä¼šâ€œå·æ‡’â€ï¼šè‡ªé€‚åº”æ¨ç†**\n**Thinkless: LLM Learns When to Think**\n**(Thinklessï¼šLLM å­¦ä¼šä½•æ—¶æ€è€ƒ)**\n**AdaptThink: Reasoning Models Can Learn When to Think**\n**(AdaptThinkï¼šæ¨ç†æ¨¡å‹èƒ½å­¦ä¼šä½•æ—¶æ€è€ƒ)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šè¿™ä¸¤ç¯‡æ–‡ç« æ€è·¯ä¸€è‡´ã€‚å¹¶ä¸æ˜¯æ‰€æœ‰é—®é¢˜éƒ½éœ€è¦é•¿ç¯‡å¤§è®ºçš„æ¨ç†ã€‚**Thinkless** å¼•å…¥äº† `<short>` å’Œ `<think>` ä¸¤ä¸ªæ§åˆ¶ Tokenï¼Œè®©æ¨¡å‹è‡ªå·±å†³å®šæ˜¯ç›´æ¥å›ç­”è¿˜æ˜¯æ·±æ€ç†Ÿè™‘ï¼Œåœ¨æ•°å­¦æ¦œå•ä¸Šå‡å°‘äº† 50%-90% çš„æ¨ç†è®¡ç®—é‡ã€‚**AdaptThink** åˆ™é€šè¿‡ RL ç®—æ³•æ•™æ¨¡å‹æ ¹æ®éš¾åº¦è‡ªé€‚åº”é€‰æ‹©æ¨¡å¼ï¼Œæ•ˆç‡æå‡æ˜¾è‘—ã€‚\n\n**Fractured Chain-of-Thought Reasoning**\n**(æ–­è£‚çš„æ€ç»´é“¾æ¨ç†)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šå‘ç°æˆªæ–­ CoTï¼ˆæ¨ç†åˆ°ä¸€åŠç›´æ¥å‡ºç­”æ¡ˆï¼‰æ•ˆæœå¾€å¾€ä¸è¾“å®Œæ•´ CoTã€‚æå‡ºäº† \"Fractured Sampling\"ï¼Œåœ¨æ¨ç†æ·±åº¦ã€è½¨è¿¹æ•°é‡å’Œç­”æ¡ˆæ•°é‡ä¹‹é—´åšæƒè¡¡ï¼Œå®ç°äº†æ›´å¥½çš„æ€§ä»·æ¯”ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸“åœºï¼šä»æ¡Œé¢åˆ°æ‰‹æœºï¼Œä»ç”šè‡³åˆ°æ¬ºè¯ˆ\n*Agent æ­£åœ¨ä»çº¯æ–‡æœ¬èµ°å‘ GUI æ“ä½œï¼Œéšä¹‹è€Œæ¥çš„å®‰å…¨é—®é¢˜è§¦ç›®æƒŠå¿ƒã€‚*\n\n#### **4. æ‰‹æœºæ“ä½œç³»ç»Ÿçš„ Agent åŸºå‡†ä¸å®‰å…¨**\n**Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis**\n**(é€šè¿‡ç”¨æˆ·ç•Œé¢åˆ†è§£ä¸åˆæˆæ‰©å±•è®¡ç®—æœºä½¿ç”¨ Grounding)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæå‡ºäº† **OSWorld-G** åŸºå‡†å’Œ **Jedi** æ•°æ®é›†ï¼ˆ400ä¸‡æ ·æœ¬ï¼‰ï¼Œä¸“æ³¨äº GUI ç•Œé¢ä¸‹çš„ Groundingï¼ˆå®šä½ä¸æ“ä½œï¼‰ã€‚è¿™æ˜¯ç›®å‰ Agent è½åœ°æœ€ç—›çš„éš¾ç‚¹ä¹‹ä¸€ã€‚\n\n**From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents**\n**(ä»åŠ©æ‰‹åˆ°å¯¹æ‰‹ï¼šæ¢ç´¢ç§»åŠ¨ LLM æ™ºèƒ½ä½“çš„å®‰å…¨é£é™©)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šå¯¹ç§»åŠ¨ç«¯ Agentï¼ˆå¦‚ AutoGLM, æ‰‹æœºå‚å•†åŠ©æ‰‹ï¼‰è¿›è¡Œäº†é¦–æ¬¡å…¨é¢å®‰å…¨åˆ†æã€‚å‘ç°äº† 11 ä¸ªæ”»å‡»é¢ï¼Œæ”»å‡»è€…å¯ä»¥åŠ«æŒæ‰§è¡Œã€æ³„éœ²éšç§ã€‚ç»“è®ºå¾ˆå“äººï¼šç›®å‰æ‰€æœ‰è¢«æµ‹ Agent åœ¨é’ˆå¯¹æ€§æ”»å‡»ä¸‹éƒ½å­˜åœ¨æ¼æ´ã€‚\n\n**The Hidden Dangers of Browsing AI Agents**\n**(æµè§ˆç±» AI æ™ºèƒ½ä½“çš„éšå½¢å±é™©)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šé’ˆå¯¹å¼€æºé¡¹ç›® Browser Use çš„ç™½ç›’åˆ†æï¼Œå‘ç°æ¶æ„ç½‘é¡µå¯ä»¥è½»æ˜“åŠ«æŒ Agent çš„è¡Œä¸ºï¼Œç”šè‡³è¿›è¡Œæç¤ºè¯æ³¨å…¥å’Œå‡­è¯çªƒå–ã€‚\n\n#### **5. ç§‘å­¦å‘ç°ä¸ç¤¾ä¼šæ¨¡æ‹Ÿ**\n**Robin: A multi-agent system for automating scientific discovery**\n**(Robinï¼šç”¨äºè‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šå…¨è‡ªåŠ¨ç§‘ç ” Agentã€‚Robin è‡ªå·±æŸ¥æ–‡çŒ®ã€æå‡è®¾ã€è®¾è®¡å®éªŒã€åˆ†ææ•°æ®ï¼Œç”šè‡³å‘ç°äº†ä¸€ç§æ²»ç–—å¹²æ€§è€å¹´é»„æ–‘å˜æ€§çš„æ–°è¯å€™é€‰ç‰©ï¼ˆripasudilï¼‰ï¼Œå¹¶è®¾è®¡äº†åç»­ RNA-seq å®éªŒæ¥éªŒè¯æœºåˆ¶ã€‚\n\n**The Traitors: Deception and Trust in Multi-Agent Language Model Simulations**\n**(å›å¾’ï¼šå¤šæ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä¸­çš„æ¬ºéª—ä¸ä¿¡ä»»)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæäº†ä¸ª AI ç‰ˆâ€œç‹¼äººæ€â€ã€‚å‘ç° GPT-4o è¿™ç§å¼ºæ¨¡å‹éå¸¸æ“…é•¿éª—äººï¼ˆæ¬ºéª—èƒ½åŠ›å¼ºï¼‰ï¼Œä½†é¢å¯¹åˆ«äººçš„è°è¨€æ—¶å´å¾ˆå®¹æ˜“è½»ä¿¡ï¼ˆé˜²å¾¡èƒ½åŠ›å¼±ï¼‰ã€‚æ¬ºéª—èƒ½åŠ›çš„å¢é•¿é€Ÿåº¦ä¼¼ä¹å¿«äºè¯†ç ´è°è¨€çš„èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ¥ å¤šæ¨¡æ€ä¸ç”Ÿæˆæ¨¡å‹ï¼šè§†é¢‘ä¸å£°éŸ³\n*è§†é¢‘ç”Ÿæˆä¸ä»…ä¸ºäº†çœ‹ï¼Œè¿˜ä¸ºäº†æœºå™¨äººå­¦ä¹ ï¼›å£°éŸ³ç”Ÿæˆå¼€å§‹ç»“åˆå«æ˜Ÿå›¾åƒã€‚*\n\n#### **6. è§†é¢‘ç”Ÿæˆçš„è§„æ¨¡åŒ–**\n**MAGI-1: Autoregressive Video Generation at Scale**\n**(MAGI-1ï¼šå¤§è§„æ¨¡è‡ªå›å½’è§†é¢‘ç”Ÿæˆ)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šæå‡ºäº† MAGI-1ï¼Œä¸€ä¸ª 24B å‚æ•°çš„ä¸–ç•Œæ¨¡å‹ï¼Œé‡‡ç”¨è‡ªå›å½’æ–¹å¼ç”Ÿæˆè§†é¢‘å—ï¼ˆchunksï¼‰ã€‚æœ€å¤§çš„äº®ç‚¹æ˜¯æ”¯æŒæµå¼ç”Ÿæˆå’Œæ’å®šçš„æ¨ç†æ˜¾å­˜å ç”¨ï¼Œä¸ç®¡è§†é¢‘å¤šé•¿ã€‚\n\n#### **7. æœºå™¨äººå­¦ä¹ çš„è§†é¢‘æ¢¦å¢ƒ**\n**DreamGen: Unlocking Generalization in Robot Learning through Video World Models**\n**(DreamGenï¼šé€šè¿‡è§†é¢‘ä¸–ç•Œæ¨¡å‹è§£é”æœºå™¨äººå­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šNVIDIA å‡ºå“ã€‚åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæœºå™¨äººæ“ä½œè§†é¢‘ï¼ˆåˆæˆæ•°æ®ï¼‰ï¼Œç„¶åè®­ç»ƒç­–ç•¥ã€‚ä»…ç”¨ä¸€ä¸ªä»»åŠ¡çš„é¥æ“ä½œæ•°æ®ï¼Œå°±èƒ½è®©æœºå™¨äººæ³›åŒ–åˆ° 22 ä¸ªæ–°è¡Œä¸ºã€‚\n\n#### **8. å¬è§å«æ˜Ÿå›¾åƒ**\n**Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping**\n**(Sat2Soundï¼šé›¶æ ·æœ¬å£°æ™¯æ˜ å°„çš„ç»Ÿä¸€æ¡†æ¶)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šéå¸¸æœ‰æ„æ€çš„è·¨æ¨¡æ€ä»»åŠ¡ã€‚è¾“å…¥ä¸€å¼ å«æ˜Ÿåœ°å›¾ï¼Œé¢„æµ‹è¿™ä¸ªåœ°æ–¹å¬èµ·æ¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼ˆå£°æ™¯ï¼‰ã€‚åˆ©ç”¨ VLM ç”Ÿæˆè¯­ä¹‰æè¿°æ¥æ¡¥æ¥å«æ˜Ÿå›¾å’ŒéŸ³é¢‘ï¼Œå®ç°äº†åŸºäºä½ç½®çš„å£°æ™¯åˆæˆã€‚\n\n---\n\n### ğŸ› ï¸ æœºå™¨å­¦ä¹ åŸºç¡€ä¸ç§‘å­¦è®¡ç®— (AI4Science)\n\n#### **9. è®­ç»ƒä¸ä¼˜åŒ–çš„æ–°å®šå¾‹**\n**Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training**\n**(ç”µæºçº¿ï¼šLLM é¢„è®­ç»ƒä¸­æƒé‡è¡°å‡å’Œ Batch Size çš„ç¼©æ”¾å®šå¾‹)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šNeurIPS 2025 è®ºæ–‡ã€‚ç ”ç©¶äº†è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æƒé‡è¡°å‡ï¼‰éšæ¨¡å‹å¤§å°ã€æ•°æ®é‡ã€Batch Size å˜åŒ–çš„ç¼©æ”¾å®šå¾‹ã€‚å‘ç°æœ€ä¼˜æƒé‡è¡°å‡ $\\lambda$ å¯ä»¥é€šè¿‡ Token/å‚æ•°æ¯”ç‡ç²¾ç¡®é¢„æµ‹ï¼Œè¿™ä¸ºå¤§è§„æ¨¡é¢„è®­ç»ƒçœå»äº†å¾ˆå¤šè°ƒå‚çš„ç„å­¦ã€‚\n\n#### **10. ç§‘å­¦è®¡ç®—æ–°æ¶æ„**\n**KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation**\n**(KHRONOSï¼šä¸€ç§ç”¨äºå¿«é€Ÿã€èµ„æºé«˜æ•ˆç§‘å­¦è®¡ç®—çš„åŸºäºæ ¸çš„ç¥ç»æ¶æ„)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šåœ¨æ±‚è§£åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰ä»»åŠ¡ä¸Šï¼Œæ¯”ç°åœ¨çš„å½“çº¢ç‚¸å­é¸¡ KANï¼ˆKolmogorov Arnold Networksï¼‰è¿˜è¦å¼º 100 å€ï¼ˆåœ¨åŒç­‰å‚æ•°é‡ä¸‹ï¼‰ï¼Œæ¯”ä¼ ç»Ÿ FEM å¿« 100 ä¸‡å€ã€‚\n\n#### **11. ç‰©ç†æ„ŸçŸ¥çš„ä¼ æ„Ÿå™¨å¸ƒå±€**\n**PhySense: Sensor Placement Optimization for Accurate Physics Sensing**\n**(PhySenseï¼šç”¨äºç²¾ç¡®ç‰©ç†æ„ŸçŸ¥çš„ä¼ æ„Ÿå™¨å¸ƒå±€ä¼˜åŒ–)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šä¸ä»…é‡å»ºç‰©ç†åœºï¼Œè¿˜æ•™ä½ æ€ä¹ˆæ‘†æ”¾ä¼ æ„Ÿå™¨èƒ½è·å¾—æœ€å¤§ä¿¡æ¯é‡ã€‚è¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç»“åˆäº†æµæ¨¡å‹å’ŒæŠ•å½±æ¢¯åº¦ä¸‹é™ã€‚\n\n---\n\n### âš¡ æ•ˆç‡ä¸ç³»ç»Ÿä¼˜åŒ–\n\n#### **12. CPU ä¹Ÿèƒ½è·‘å¤§æ¨¡å‹**\n**Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving**\n**(Sandwichï¼šåˆ†ç¦»é¢„å¡«å……-è§£ç ç¼–è¯‘ä»¥å®ç°é«˜æ•ˆ CPU LLM æœåŠ¡)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šé’ˆå¯¹ CPU ä¼˜åŒ–çš„ LLM æ¨ç†å¼•æ“ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æŠŠ Prefillï¼ˆè®¡ç®—å¯†é›†ï¼‰å’Œ Decodeï¼ˆè®¿å­˜å¯†é›†ï¼‰é˜¶æ®µåˆ†å¼€ä¼˜åŒ–ï¼Œåœ¨ CPU ä¸Šå®ç°äº† 2 å€çš„ååé‡æå‡ã€‚\n\n#### **13. KV Cache ä¼˜åŒ–**\n**FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference**\n**(FreeKVï¼šæå‡ KV Cache æ£€ç´¢ä»¥å®ç°é«˜æ•ˆ LLM æ¨ç†)**\n**æ ¸å¿ƒçœ‹ç‚¹**ï¼šé’ˆå¯¹é•¿æ–‡æœ¬æ¨ç†çš„ KV Cache ç“¶é¢ˆï¼Œæå‡ºäº†æŠ•æœºæ£€ç´¢ï¼ˆSpeculative retrievalï¼‰å’Œ CPU-GPU æ··åˆå¸ƒå±€ï¼Œå®ç°äº†é«˜è¾¾ 13 å€çš„åŠ é€Ÿä¸”å‡ ä¹æ— æŸã€‚\n\n---\n\n### ğŸ“ å…¶å®ƒå€¼å¾—å…³æ³¨çš„è®ºæ–‡\n\n*   **[Security] Security Degradation in Iterative AI Code Generation**: #7 æŒ‡å‡º AI ç”Ÿæˆçš„ä»£ç åœ¨è¿­ä»£ä¿®æ”¹ï¼ˆ\"ä¿®å¤ bug\"ï¼‰è¿‡ç¨‹ä¸­ï¼Œå®‰å…¨æ€§åè€Œä¸‹é™äº† 37.6%ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‚–è®ºã€‚\n*   **[Dataset] scSiameseClu**: #234 é’ˆå¯¹å•ç»†èƒæµ‹åºæ•°æ®çš„èšç±»æ¡†æ¶ï¼Œè§£å†³äº†æ•°æ®ç¨€ç–å’Œå™ªå£°é—®é¢˜ã€‚\n*   **[Evaluation] Ineq-Comp**: #217 å‘ç°å³ä¾¿æ˜¯ DeepSeek-Prover-V2 è¿™ç§å¼ºæ¨¡å‹ï¼Œåœ¨å¤„ç†å¤šæ­¥ä¸ç­‰å¼æ¨ç†æ—¶ï¼Œä¾ç„¶ç¼ºä¹äººç±»çš„ç›´è§‰ã€‚\n*   **[Music] Text2midi-InferAlign**: #219 åœ¨æ¨ç†é˜¶æ®µé€šè¿‡å¯¹é½å¥–åŠ±å¼•å¯¼ç¬¦å·éŸ³ä¹ç”Ÿæˆï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚\n\n---\n\n**ç»“è¯­**ï¼šä»Šå¤©çš„è®ºæ–‡ç»™æˆ‘çš„æ„Ÿè§‰æ˜¯ï¼Œå­¦æœ¯ç•Œæ­£åœ¨ä»â€œç›²ç›®è¿½æ±‚å¤§æ¨¡å‹èƒ½åŠ›â€è½¬å‘â€œç²¾ç»†åŒ–æ§åˆ¶ä¸ç†è§£â€ã€‚æ— è®ºæ˜¯å¯¹ Reasoning Token çš„å®¡è®¡ï¼Œå¯¹ Agent å®‰å…¨çš„çº¢é˜Ÿæµ‹è¯•ï¼Œè¿˜æ˜¯å¯¹ç‰©ç†ä¸–ç•Œæ¨¡å‹çš„æ¢ç´¢ï¼Œéƒ½æ˜¾ç¤ºå‡º AI ç ”ç©¶æ­£åœ¨è¿›å…¥æ·±æ°´åŒºã€‚å¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ æœ‰å¯å‘ï¼",
  "papers": [
    {
      "arxiv_id": "2505.13778v1",
      "title": "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs",
      "title_zh": "CoInï¼šå•†ä¸šä¸é€æ˜å¤§è¯­è¨€æ¨¡å‹ API ä¸­çš„ä¸å¯è§æ¨ç† Token è®¡æ•°",
      "authors": [
        "Guoheng Sun",
        "Ziyao Wang",
        "Bowei Tian",
        "Meng Liu",
        "Zheyu Shen",
        "Shwai He",
        "Yexiao He",
        "Wanghao Ye",
        "Yiting Wang",
        "Ang Li"
      ],
      "abstract": "As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•†ä¸šåŒ–é»‘ç›’ LLM APIs ä¸­éšè—æ¨ç†æ ‡è®°(Reasoning Tokens)å¯¼è‡´è®¡è´¹ä¸é€æ˜åŠæ½œåœ¨çš„è¿‡åº¦è®¡è´¹é—®é¢˜ï¼Œæå‡ºäº†åä¸º CoIn çš„éªŒè¯æ¡†æ¶ã€‚ç”±äºæœåŠ¡å•†é€šå¸¸åœ¨è¿”å›æœ€ç»ˆç­”æ¡ˆæ—¶éšè—æ¨ç†è½¨è¿¹ï¼Œç”¨æˆ·éš¾ä»¥æ ¸å®å æ®å¤§éƒ¨åˆ†æˆæœ¬çš„æ¨ç†æ ‡è®°çœŸå®æ€§ï¼ŒCoIn é€šè¿‡æ„å»ºåŸºäºæ ‡è®°åµŒå…¥æŒ‡çº¹(Token Embedding Fingerprints)çš„å¯éªŒè¯å“ˆå¸Œæ ‘æ¥å®¡è®¡æ ‡è®°æ•°é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºäºåµŒå…¥çš„ç›¸å…³æ€§åŒ¹é…æŠ€æœ¯æ¥è¯†åˆ«ä¼ªé€ æˆ–ä½è´¨é‡çš„æ¨ç†å†…å®¹ï¼Œç¡®ä¿è¯­ä¹‰çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¯æ˜ï¼ŒCoIn ä½œä¸ºå—ä¿¡ä»»çš„ç¬¬ä¸‰æ–¹å®¡è®¡å·¥å…·ï¼Œåœ¨æ£€æµ‹æ ‡è®°è®¡æ•°é€šèƒ€(Token Count Inflation)æ–¹é¢çš„æˆåŠŸç‡æœ€é«˜å¯è¾¾ 94.7%ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³ä¸é€æ˜æ¨¡å‹æœåŠ¡ä¸­çš„è®¡è´¹é€æ˜åº¦ç¼ºå¤±æä¾›äº†å…³é”®æŠ€æœ¯æ–¹æ¡ˆï¼Œå¹¶å·²å¼€æºç›¸å…³æ•°æ®å’Œä»£ç ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13778v1",
      "published_date": "2025-05-19 23:39:23 UTC",
      "updated_date": "2025-05-19 23:39:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:33:49.008158+00:00"
    },
    {
      "arxiv_id": "2505.13777v1",
      "title": "Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping",
      "title_zh": "Sat2Soundï¼šé›¶æ ·æœ¬å£°æ™¯æ˜ å°„çš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Subash Khanal",
        "Srikumar Sastry",
        "Aayush Dhakal",
        "Adeel Ahmad",
        "Nathan Jacobs"
      ],
      "abstract": "We present Sat2Sound, a multimodal representation learning framework for soundscape mapping, designed to predict the distribution of sounds at any location on Earth. Existing methods for this task rely on satellite image and paired geotagged audio samples, which often fail to capture the diversity of sound sources at a given location. To address this limitation, we enhance existing datasets by leveraging a Vision-Language Model (VLM) to generate semantically rich soundscape descriptions for locations depicted in satellite images. Our approach incorporates contrastive learning across audio, audio captions, satellite images, and satellite image captions. We hypothesize that there is a fixed set of soundscape concepts shared across modalities. To this end, we learn a shared codebook of soundscape concepts and represent each sample as a weighted average of these concepts. Sat2Sound achieves state-of-the-art performance in cross-modal retrieval between satellite image and audio on two datasets: GeoSound and SoundingEarth. Additionally, building on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a novel application: location-based soundscape synthesis, which enables immersive acoustic experiences. Our code and models will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Sat2Soundï¼Œä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å£°æ™¯åˆ¶å›¾(Zero-Shot Soundscape Mapping)çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹åœ°çƒä¸Šä»»æ„ä½ç½®çš„å£°éŸ³åˆ†å¸ƒã€‚ä¸ºäº†å…‹æœç°æœ‰åœ°ç†æ ‡è®°éŸ³é¢‘æ•°æ®é›†åœ¨æ•æ‰å£°æºå¤šæ ·æ€§æ–¹é¢çš„å±€é™ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ä¸ºå«æ˜Ÿå›¾åƒç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„å£°æ™¯æè¿°ã€‚æ ¸å¿ƒæ–¹æ³•ç»“åˆäº†éŸ³é¢‘ã€å«æ˜Ÿå›¾åƒåŠå…¶å„è‡ªæ–‡æœ¬æè¿°ä¹‹é—´çš„å¯¹æ¯”å­¦ä¹ (Contrastive Learning)ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…±äº«çš„å£°æ™¯æ¦‚å¿µç æœ¬(Shared Codebook)æ¥ç»Ÿä¸€è¡¨ç¤ºä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSat2Soundåœ¨GeoSoundå’ŒSoundingEarthæ•°æ®é›†çš„è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†åŸºäºä½ç½®çš„å£°æ™¯åˆæˆ(Soundscape Synthesis)è¿™ä¸€æ–°é¢–åº”ç”¨ï¼Œä¸ºå®ç°æ²‰æµ¸å¼å¬è§‰ä½“éªŒæä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13777v1",
      "published_date": "2025-05-19 23:36:04 UTC",
      "updated_date": "2025-05-19 23:36:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:33:51.936351+00:00"
    },
    {
      "arxiv_id": "2505.13775v3",
      "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens",
      "title_zh": "è¶…è¶Šè¯­ä¹‰ï¼šæ— æ¨ç†ä¸­é—´ Token çš„ä¸åˆå¸¸ç†çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Karthik Valmeekam",
        "Kaya Stechly",
        "Vardhan Palod",
        "Atharva Gundawar",
        "Subbarao Kambhampati"
      ],
      "abstract": "Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought'' reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè°ƒæŸ¥äº†æ¨ç†è½¨è¿¹çš„è¯­ä¹‰åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„ä½œç”¨ï¼ŒæŒ‘æˆ˜äº†ä¸­é—´æ ‡è®°æˆ– Chain of Thought (CoT) åæ˜ æ¨¡å‹å†…éƒ¨æ¨ç†è¡Œä¸ºçš„ä¼ ç»Ÿå‡è®¾ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨å½¢å¼åŒ–å¯éªŒè¯çš„æ¨ç†è½¨è¿¹ä¸Šä»å¤´è®­ç»ƒ Transformer æ¨¡å‹ï¼Œå¯¹æ¯”äº†æ­£ç¡®è½¨è¿¹ä¸ä¸­é—´æ¨ç†æ­¥éª¤ä¸é—®é¢˜å®Œå…¨æ— å…³çš„ corrupted tracesã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æŸåè½¨è¿¹ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸æ­£ç¡®è½¨è¿¹ç›¸å½“ï¼Œç”šè‡³åœ¨ out-of-distribution ä»»åŠ¡ä¸Šå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°åŸºäº GRPO çš„å¼ºåŒ–å­¦ä¹ è™½èƒ½æå‡ç­”æ¡ˆå‡†ç¡®ç‡ï¼Œå´æ— æ³•æ”¹å–„è½¨è¿¹çš„æœ‰æ•ˆæ€§ï¼Œä¸”æ¨ç†é•¿åº¦ä¸é—®é¢˜çš„ computational complexity å¹¶ä¸ç›¸å…³ã€‚è¿™äº›å‘ç°æç¤ºç ”ç©¶è€…åº”è­¦æƒ•å°†æ¨¡å‹è¾“å‡ºæ‹ŸäººåŒ–ï¼Œä¸åº”å°†è¡¨é¢åˆç†çš„ä¸­é—´æ ‡è®°è¿‡åº¦è§£è¯»ä¸ºæ¨¡å‹å…·å¤‡ç±»äººé€»è¾‘æˆ–ç®—æ³•è¡Œä¸ºçš„è¯æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13775v3",
      "published_date": "2025-05-19 23:29:23 UTC",
      "updated_date": "2025-11-22 07:49:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:34:02.211351+00:00"
    },
    {
      "arxiv_id": "2505.13774v2",
      "title": "Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models",
      "title_zh": "è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ä¸­æ€è€ƒè‰ç¨¿çš„å¿ å®åº¦",
      "authors": [
        "Zidi Xiong",
        "Shan Chen",
        "Zhenting Qi",
        "Himabindu Lakkaraju"
      ],
      "abstract": "Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)ä¸­æ€è€ƒè‰ç¨¿(thinking drafts)çš„å¿ å®åº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„åäº‹å®å¹²é¢„æ¡†æ¶(counterfactual intervention framework)è¿›è¡Œä¸¥æ ¼è¯„ä¼°ã€‚è¯¥æ¡†æ¶ä¸»è¦å…³æ³¨ä¸¤ä¸ªäº’è¡¥ç»´åº¦ï¼šä¸€æ˜¯è‰ç¨¿å†…éƒ¨å¿ å®åº¦(Intra-Draft Faithfulness)ï¼Œé€šè¿‡åäº‹å®æ­¥éª¤æ’å…¥è¯„ä¼°æ¨ç†æ­¥éª¤é—´çš„å› æœå½±å“ï¼›äºŒæ˜¯è‰ç¨¿åˆ°ç­”æ¡ˆå¿ å®åº¦(Draft-to-Answer Faithfulness)ï¼Œé€šè¿‡æ‰°åŠ¨è‰ç¨¿é€»è¾‘éªŒè¯æœ€ç»ˆç­”æ¡ˆä¸è‰ç¨¿çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹6ä¸ªå°–ç«¯LRMsçš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶å‘ç°å½“å‰æ¨¡å‹å¯¹ä¸­é—´æ¨ç†æ­¥éª¤ä»…è¡¨ç°å‡ºé€‰æ‹©æ€§å¿ å®ï¼Œä¸”å¾€å¾€æ— æ³•åœ¨é€»è¾‘ä¸Šä¸è‰ç¨¿ç»“è®ºä¿æŒå¯¹é½ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¯é ç›‘æ§ä¸è§£é‡Šæ–¹é¢çš„ä¸è¶³ï¼Œå¹¶å¼ºè°ƒäº†åœ¨é«˜çº§LRMsä¸­å®ç°æ›´å…·å¿ å®åº¦ä¸å¯è§£é‡Šæ€§æ¨ç†çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13774v2",
      "published_date": "2025-05-19 23:20:24 UTC",
      "updated_date": "2025-05-28 19:41:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:34:00.998678+00:00"
    },
    {
      "arxiv_id": "2505.13773v1",
      "title": "Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments",
      "title_zh": "AI é˜Ÿå‹æ¨¡å‹å¡ï¼šé«˜é£é™©ç¯å¢ƒä¸‹äººæœºå›¢é˜Ÿç†Ÿæ‚‰æ–¹æ³•çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Ryan Bowers",
        "Richard Agbeyibor",
        "Jack Kolb",
        "Karen Feigh"
      ],
      "abstract": "We compare three methods of familiarizing a human with an artificial intelligence (AI) teammate (\"agent\") prior to operation in a collaborative, fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In a between-subjects user study (n=60), participants either read documentation about the agent, trained alongside the agent prior to the mission, or were given no familiarization. Results showed that the most valuable information about the agent included details of its decision-making algorithms and its relative strengths and weaknesses compared to the human. This information allowed the familiarization groups to form sophisticated team strategies more quickly than the control group. Documentation-based familiarization led to the fastest adoption of these strategies, but also biased participants towards risk-averse behavior that prevented high scores. Participants familiarized through direct interaction were able to infer much of the same information through observation, and were more willing to take risks and experiment with different control modes, but reported weaker understanding of the agent's internal processes. Significant differences were seen between individual participants' risk tolerance and methods of AI interaction, which should be considered when designing human-AI control interfaces. Based on our findings, we recommend a human-AI team familiarization method that combines AI documentation, structured in-situ training, and exploratory interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨æ¨¡æ‹Ÿé«˜é£é™©çš„ISRç¯å¢ƒä¸­ï¼Œå¯¹æ¯”äº†é˜…è¯»æ–‡æ¡£ã€å…±åŒè®­ç»ƒå’Œæ— é¢„å…ˆç†Ÿæ‚‰ä¸‰ç§è®©ç”¨æˆ·äº†è§£AIé˜Ÿå‹çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹n=60åå‚ä¸è€…çš„å—è¯•è€…é—´ç ”ç©¶ï¼Œç»“æœè¡¨æ˜å…³äºAIå†³ç­–ç®—æ³•åŠå…¶ä¼˜åŠ£åŠ¿çš„ä¿¡æ¯å¯¹äºå¿«é€Ÿå½¢æˆå›¢é˜Ÿç­–ç•¥æœ€ä¸ºå…³é”®ã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºæ–‡æ¡£çš„ç†Ÿæ‚‰æ–¹æ³•èƒ½æœ€å¿«ä¿ƒè¿›ç­–ç•¥é‡‡ç”¨ï¼Œä½†ä¼šè¯±å¯¼ç”¨æˆ·äº§ç”Ÿè§„é¿é£é™©çš„è¡Œä¸ºï¼›è€Œé€šè¿‡ç›´æ¥äº’åŠ¨ç†Ÿæ‚‰AIçš„ç”¨æˆ·è™½ç„¶æ›´æ„¿æ„å°è¯•é£é™©å’Œæ¢ç´¢æ§åˆ¶æ¨¡å¼ï¼Œä½†å¯¹AIå†…éƒ¨è¿‡ç¨‹çš„ç†è§£è¾ƒå¼±ã€‚æ­¤å¤–ï¼Œå®éªŒè§‚å¯Ÿåˆ°ä¸ªä½“åœ¨é£é™©åå¥½å’ŒAIäº¤äº’æ–¹å¼ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†åœ¨è®¾è®¡äººæœºç•Œé¢æ—¶éœ€è€ƒè™‘è¿™äº›å› ç´ ã€‚æœ€ç»ˆï¼Œç ”ç©¶å»ºè®®é‡‡ç”¨ä¸€ç§ç»“åˆäº†AIæ–‡æ¡£ã€ç»“æ„åŒ–ç°åœºè®­ç»ƒä»¥åŠæ¢ç´¢æ€§äº’åŠ¨çš„ç»¼åˆæ€§äººæœºå›¢é˜Ÿç†Ÿæ‚‰æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to IEEE RO-MAN 2025 (under review). 8 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13773v1",
      "published_date": "2025-05-19 23:19:16 UTC",
      "updated_date": "2025-05-19 23:19:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:34:01.635674+00:00"
    },
    {
      "arxiv_id": "2505.13770v1",
      "title": "Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference",
      "title_zh": "å†°æ·‡æ·‹å¹¶ä¸å¯¼è‡´æººæ°´ï¼šé’ˆå¯¹å› æœæ¨æ–­ä¸­ç»Ÿè®¡é™·é˜±çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jin Du",
        "Li Chen",
        "Xun Xian",
        "An Luo",
        "Fangqiao Tian",
        "Ganghua Wang",
        "Charles Doss",
        "Xiaotong Shen",
        "Jie Ding"
      ],
      "abstract": "Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ä¸¥è°¨ç»Ÿè®¡å› æœæ¨ç†(Causal Inference)æ–¹é¢çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºå½“å‰æ¨¡å‹åœ¨é¢å¯¹è¾›æ™®æ£®æ‚–è®º(Simpson's paradox)æˆ–é€‰æ‹©åå·®(Selection bias)ç­‰ç»Ÿè®¡é™·é˜±æ—¶å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†CausalPitfallsåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¤šéš¾åº¦çº§åˆ«çš„ç»“æ„åŒ–æŒ‘æˆ˜å®šé‡è¡¡é‡LLMsçš„å› æœæ¨ç†èƒ½åŠ›ä¸å“åº”å¯é æ€§ã€‚è¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†ç›´æ¥æç¤º(Direct prompting)å’Œç”Ÿæˆæ‰§è¡Œä»£ç è¿›è¡Œç»Ÿè®¡åˆ†æçš„è¾…åŠ©æç¤º(Code-assisted prompting)ä¸¤ç§åè®®ï¼Œå¹¶ç»ç”±äººç±»ä¸“å®¶éªŒè¯äº†å…¶è¯„åˆ†æŒ‡æ ‡çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰LLMsåœ¨æ‰§è¡Œç»Ÿè®¡å› æœæ¨ç†æ—¶çš„æ˜¾è‘—å±€é™æ€§ï¼Œæœªèƒ½è¾¾åˆ°é«˜é£é™©å†³ç­–é¢†åŸŸæ‰€éœ€çš„ä¸¥è°¨æ ‡å‡†ã€‚CausalPitfallsåŸºå‡†ä¸ºæ¨åŠ¨å¯ä¿¡å› æœæ¨ç†ç³»ç»Ÿçš„å¼€å‘æä¾›äº†å…³é”®çš„æŒ‡å¯¼ä¸é‡åŒ–æŒ‡æ ‡ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°ä½“ç³»çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13770v1",
      "published_date": "2025-05-19 23:06:00 UTC",
      "updated_date": "2025-05-19 23:06:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:35:04.333582+00:00"
    },
    {
      "arxiv_id": "2506.11022v2",
      "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox",
      "title_zh": "è¿­ä»£å¼ AI ä»£ç ç”Ÿæˆä¸­çš„å®‰å…¨æ€§é€€åŒ–ï¼šå¯¹è¿™ä¸€æ‚–è®ºçš„ç³»ç»Ÿæ€§åˆ†æ",
      "authors": [
        "Shivani Shukla",
        "Himanshu Joshi",
        "Romilla Syed"
      ],
      "abstract": "The rapid adoption of Large Language Models(LLMs) for code generation has transformed software development, yet little attention has been given to how security vulnerabilities evolve through iterative LLM feedback. This paper analyzes security degradation in AI-generated code through a controlled experiment with 400 code samples across 40 rounds of \"improvements\" using four distinct prompting strategies. Our findings show a 37.6% increase in critical vulnerabilities after just five iterations, with distinct vulnerability patterns emerging across different prompting approaches. This evidence challenges the assumption that iterative LLM refinement improves code security and highlights the essential role of human expertise in the loop. We propose practical guidelines for developers to mitigate these risks, emphasizing the need for robust human validation between LLM iterations to prevent the paradoxical introduction of new security issues during supposedly beneficial code \"improvements\".",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶ç³»ç»Ÿåˆ†æäº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§é€€åŒ–é—®é¢˜ï¼Œé‡ç‚¹æ¢è®¨äº†å®‰å…¨æ¼æ´å¦‚ä½•é€šè¿‡è¿­ä»£åé¦ˆå‘ç”Ÿæ¼”å˜ã€‚ç ”ç©¶é€šè¿‡å—æ§å®éªŒå¯¹400ä¸ªä»£ç æ ·æœ¬åœ¨å››ç§æç¤ºç­–ç•¥(prompting strategies)ä¸‹è¿›è¡Œäº†40è½®â€œæ”¹è¿›â€æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œä»…åœ¨äº”æ¬¡è¿­ä»£åï¼Œå…³é”®æ¼æ´(critical vulnerabilities)å°±å¢åŠ äº†37.6%ï¼Œä¸”ä¸åŒæç¤ºæ–¹æ³•ä¼šè¯±å‘ç‰¹å®šçš„æ¼æ´æ¨¡å¼ã€‚è¿™ä¸€ç»“è®ºæœ‰åŠ›æŒ‘æˆ˜äº†è¿­ä»£ä¼˜åŒ–èƒ½æå‡ä»£ç å®‰å…¨æ€§çš„å‡è®¾ï¼Œæ­ç¤ºäº†åœ¨æ‰€è°“ä»£ç â€œæ”¹è¿›â€ä¸­åè€Œå¼•å…¥æ–°æ¼æ´çš„æ‚–è®ºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼ºè°ƒäº†äººç±»ä¸“å®¶(human expertise)ä»‹å…¥çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘è€…æä¾›äº†é™ä½æ­¤ç±»é£é™©çš„å®ç”¨æŒ‡å—ï¼Œä¸»å¼ åœ¨è¿­ä»£é—´è¿›è¡Œä¸¥æ ¼çš„äººç±»éªŒè¯ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Keywords - Large Language Models, Security Vulnerabilities, AI-Generated Code, Iterative Feedback, Software Security, Secure Coding Practices, Feedback Loops, LLM Prompting Strategies",
      "pdf_url": "https://arxiv.org/pdf/2506.11022v2",
      "published_date": "2025-05-19 22:55:51 UTC",
      "updated_date": "2025-09-26 02:44:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:34:43.734659+00:00"
    },
    {
      "arxiv_id": "2505.13766v2",
      "title": "Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques",
      "title_zh": "æå‡è½¯ä»¶è´¨é‡ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è´¨é‡ä¿è¯æŠ€æœ¯çš„æ ‡å‡†å¯¼å‘ç»¼è¿°",
      "authors": [
        "Avinash Patil"
      ],
      "abstract": "Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„è½¯ä»¶è´¨é‡ä¿è¯(Software Quality Assurance, SQA)æŠ€æœ¯è¿›è¡Œäº†ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œæ¢è®¨äº†AIé©±åŠ¨æ–¹æ¡ˆå¦‚ä½•å¢å¼ºç°æœ‰çš„è´¨é‡ä¿è¯æµç¨‹ã€‚è®ºæ–‡é‡ç‚¹åˆ†æäº†LLMsåœ¨ç¬¦åˆISO/IEC 12207ã€ISO/IEC 25010ã€ISO/IEC 5055ã€CMMIåŠTMMç­‰å›½é™…æ ‡å‡†ä¸æˆç†Ÿåº¦æ¡†æ¶ä¸‹çš„èåˆè·¯å¾„ï¼Œå±•ç¤ºäº†å…¶åœ¨ç»´æŠ¤è¿‡ç¨‹åˆè§„æ€§æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶è¯¦ç»†è°ƒç ”äº†LLMsåœ¨éœ€æ±‚éªŒè¯(Requirement Validation)ã€ç¼ºé™·æ£€æµ‹(Defect Detection)ã€æµ‹è¯•ç”Ÿæˆ(Test Generation)åŠæ–‡æ¡£ç»´æŠ¤ç­‰å…³é”®ä»»åŠ¡ä¸­çš„å…·ä½“åº”ç”¨ï¼Œå¹¶å°†å…¶æ˜ å°„è‡³ç›¸åº”çš„è½¯ä»¶è´¨é‡æŒ‡æ ‡ä½“ç³»ä¸­ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ä¸å¼€æºå€¡è®®ï¼Œæ–‡ç« è¯å®äº†è¿™äº›è‡ªåŠ¨åŒ–æ–¹æ³•çš„å®é™…å¯è¡Œæ€§ï¼ŒåŒæ—¶æ·±å…¥æ¢è®¨äº†æ•°æ®éšç§ã€æ¨¡å‹åè§(Model Bias)åŠå¯è§£é‡Šæ€§ç­‰æ²»ç†ä¸å®¡è®¡æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†æ¶‰åŠè‡ªé€‚åº”å­¦ä¹ ã€éšç§èšç„¦éƒ¨ç½²åŠå¤šæ¨¡æ€åˆ†æ(Multimodal Analysis)ç­‰æœªæ¥å‘å±•æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºAIé©±åŠ¨çš„è½¯ä»¶è´¨é‡æ¼”è¿›æä¾›æ ‡å‡†åŒ–æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "16 pages, 1 Table, 6 Figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13766v2",
      "published_date": "2025-05-19 22:49:30 UTC",
      "updated_date": "2026-01-07 19:48:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:34:28.719551+00:00"
    },
    {
      "arxiv_id": "2505.13763v2",
      "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations",
      "title_zh": "è¯­è¨€æ¨¡å‹å…·å¤‡å¯¹å…¶å†…éƒ¨æ¿€æ´»è¿›è¡Œå…ƒè®¤çŸ¥ç›‘æµ‹ä¸æ§åˆ¶çš„èƒ½åŠ›",
      "authors": [
        "Li Ji-An",
        "Hua-Dong Xiong",
        "Robert C. Wilson",
        "Marcelo G. Mattar",
        "Marcus K. Benna"
      ],
      "abstract": "Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, yet at other times seem unable to recognize those strategies that govern their behavior. This suggests a limited degree of metacognition - the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognition enhances LLMs' capabilities in solving complex tasks but also raises safety concerns, as models may obfuscate their internal processes to evade neural-activation-based oversight (e.g., safety detector). Given society's increased reliance on these models, it is critical that we understand their metacognitive abilities. To address this, we introduce a neuroscience-inspired neurofeedback paradigm that uses in-context learning to quantify metacognitive abilities of LLMs to report and control their activation patterns. We demonstrate that their abilities depend on several factors: the number of in-context examples provided, the semantic interpretability of the neural activation direction (to be reported/controlled), and the variance explained by that direction. These directions span a \"metacognitive space\" with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a small subset of their neural activations. Our paradigm provides empirical evidence to quantify metacognition in LLMs, with significant implications for AI safety (e.g., adversarial attack and defense).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡å…ƒè®¤çŸ¥(Metacognition)èƒ½åŠ›ï¼Œå³ç›‘æµ‹å’Œæ§åˆ¶è‡ªèº«å†…éƒ¨æ¿€æ´»æ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†é‡åŒ–è¿™ä¸€èƒ½åŠ›ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªå—ç¥ç»ç§‘å­¦å¯å‘çš„ç¥ç»åé¦ˆèŒƒå¼ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning)æ¥è¯„ä¼°æ¨¡å‹æŠ¥å‘Šå’Œæ§åˆ¶ç‰¹å®šç¥ç»æ¿€æ´»æ–¹å‘çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsçš„å…ƒè®¤çŸ¥è¡¨ç°å–å†³äºç¤ºä¾‹æ•°é‡ã€æ¿€æ´»æ–¹å‘çš„è¯­ä¹‰å¯è§£é‡Šæ€§ä»¥åŠè¯¥æ–¹å‘æ‰€èƒ½è§£é‡Šçš„æ–¹å·®ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹ä»…èƒ½ç›‘æµ‹å…¶ç¥ç»ç©ºé—´ä¸­ä¸€ä¸ªç»´åº¦è¿œä½äºåŸç©ºé—´çš„â€œå…ƒè®¤çŸ¥ç©ºé—´â€ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åªèƒ½ç›‘æ§ä¸€å°éƒ¨åˆ†ç¥ç»æ¿€æ´»ã€‚è¯¥å·¥ä½œä¸ºé‡åŒ–LLMsçš„å…ƒè®¤çŸ¥æä¾›äº†ç»éªŒè¯æ®ï¼Œåœ¨äººå·¥æ™ºèƒ½å®‰å…¨(AI Safety)ã€å¯¹æŠ—æ€§æ”»å‡»ä¸é˜²å¾¡ç­‰é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13763v2",
      "published_date": "2025-05-19 22:32:25 UTC",
      "updated_date": "2025-10-24 02:36:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:34:20.219049+00:00"
    },
    {
      "arxiv_id": "2506.06301v1",
      "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åŠå…¶åœ¨æå‡é“è·¯å®‰å…¨ä¸äº¤é€šæœºåŠ¨æ€§æ–¹é¢çš„åº”ç”¨ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Muhammad Monjurul Karim",
        "Yan Shi",
        "Shucheng Zhang",
        "Bingzhang Wang",
        "Mehrdad Nasri",
        "Yinhai Wang"
      ],
      "abstract": "Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted -- via architectural, training, prompting, and multimodal strategies -- to bridge the \"modality gap\" with transportation's unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs' transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰åœ¨æå‡é“è·¯å®‰å…¨ä¸å‡ºè¡Œæ•ˆç‡æ–¹é¢çš„åº”ç”¨ï¼Œæ—¨åœ¨åº”å¯¹ç°ä»£äº¤é€šç³»ç»Ÿä¸­å¤æ‚ä¸”åŠ¨æ€çš„åˆ†ææŒ‘æˆ˜ã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶äº†å¦‚ä½•é€šè¿‡æ¶æ„ä¼˜åŒ–ã€è®­ç»ƒå¾®è°ƒã€æç¤ºå·¥ç¨‹ï¼ˆPromptingï¼‰åŠå¤šæ¨¡æ€ç­–ç•¥å¯¹LLMsè¿›è¡Œå®šåˆ¶åŒ–ï¼Œä»¥æ¡¥æ¥äº¤é€šé¢†åŸŸç‰¹æœ‰çš„æ—¶ç©ºç‰©ç†æ•°æ®ä¸æ¨¡å‹ä¹‹é—´çš„â€œæ¨¡æ€å·®å¼‚â€ã€‚è®ºæ–‡ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†LLMsåœ¨äº¤é€šæµé¢„æµ‹ã€ä¿¡å·æ§åˆ¶ã€äº‹æ•…åˆ†æåŠé©¾é©¶è¡Œä¸ºè¯„ä¼°ç­‰å…³é”®é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ï¼Œå¹¶æ¢è®¨äº†V2Xé›†æˆã€é¢†åŸŸç‰¹å®šåŸºç¡€æ¨¡å‹å’Œè¾¹ç¼˜è®¡ç®—ç­‰èµ‹èƒ½æŠ€æœ¯ã€‚å°½ç®¡LLMså±•ç°å‡ºå·¨å¤§çš„å˜é©æ½œåŠ›ï¼Œä½†ç ”ç©¶æŒ‡å‡ºå…¶åœ¨å¹»è§‰ï¼ˆHallucinationsï¼‰ã€æ¨ç†ç¼ºé™·ã€æ•°æ®éšç§åŠä»¿çœŸåˆ°ç°å®ï¼ˆSim-to-realï¼‰éƒ¨ç½²ç­‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ€åï¼Œè¯¥ç»¼è¿°ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼ŒåŒ…æ‹¬å¢å¼ºæ—¶ç©ºæ¨ç†ã€äººæœºåä½œåŠå¼€å‘é«˜æ•ˆå¯éªŒè¯çš„ç³»ç»Ÿï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½çš„äº¤é€šç³»ç»Ÿæä¾›äº†ç»“æ„åŒ–çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06301v1",
      "published_date": "2025-05-19 21:51:18 UTC",
      "updated_date": "2025-05-19 21:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:35:19.970990+00:00"
    },
    {
      "arxiv_id": "2505.13742v1",
      "title": "Understanding Task Representations in Neural Networks via Bayesian Ablation",
      "title_zh": "é€šè¿‡è´å¶æ–¯æ¶ˆèç†è§£ç¥ç»ç½‘ç»œä¸­çš„ä»»åŠ¡è¡¨å¾",
      "authors": [
        "Andrew Nam",
        "Declan Campbell",
        "Thomas Griffiths",
        "Jonathan Cohen",
        "Sarah-Jane Leslie"
      ],
      "abstract": "Neural networks are powerful tools for cognitive modeling due to their flexibility and emergent properties. However, interpreting their learned representations remains challenging due to their sub-symbolic semantics. In this work, we introduce a novel probabilistic framework for interpreting latent task representations in neural networks. Inspired by Bayesian inference, our approach defines a distribution over representational units to infer their causal contributions to task performance. Using ideas from information theory, we propose a suite of tools and metrics to illuminate key model properties, including representational distributedness, manifold complexity, and polysemanticity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¦‚ç‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œç”±äºå­ç¬¦å·è¯­ä¹‰(sub-symbolic semantics)å¯¼è‡´çš„ä»»åŠ¡è¡¨ç¤º(task representations)è§£é‡Šéš¾é¢˜ã€‚å—åˆ°è´å¶æ–¯æ¨ç†(Bayesian inference)çš„å¯å‘ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨è¡¨ç¤ºå•å…ƒä¸Šå®šä¹‰åˆ†å¸ƒï¼Œæ¥æ¨æ–­å…¶å¯¹ä»»åŠ¡æ€§èƒ½çš„å› æœè´¡çŒ®(causal contributions)ã€‚åŸºäºä¿¡æ¯è®º(information theory)çš„æ€æƒ³ï¼Œä½œè€…å¼€å‘äº†ä¸€ç³»åˆ—å·¥å…·å’ŒæŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹çš„å…³é”®å±æ€§ã€‚è¿™äº›æŒ‡æ ‡å…·ä½“æ¶µç›–äº†è¡¨ç¤ºçš„åˆ†å¸ƒæ€§(representational distributedness)ã€æµå½¢å¤æ‚åº¦(manifold complexity)ä»¥åŠå¤šä¹‰æ€§(polysemanticity)ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºç¥ç»ç½‘ç»œå†…éƒ¨çš„æ½œåœ¨äºè¡¨ç¤ºç‰¹æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œå¦‚ä½•å¤„ç†å¤æ‚ä»»åŠ¡æä¾›äº†ä¸€ç§å¼ºæœ‰åŠ›çš„æ¦‚ç‡åˆ†æè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13742v1",
      "published_date": "2025-05-19 21:36:09 UTC",
      "updated_date": "2025-05-19 21:36:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:35:46.599965+00:00"
    },
    {
      "arxiv_id": "2505.13740v2",
      "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores",
      "title_zh": "åŸºäº Lift Scores æå‡æ‰©æ•£æ¨¡å‹çš„ç»„åˆå¼ç”Ÿæˆ",
      "authors": [
        "Chenning Yu",
        "Sicun Gao"
      ],
      "abstract": "We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at http://rainorangelemon.github.io/complift.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLift Scoresçš„æ–°å‹é‡é‡‡æ ·å‡†åˆ™ï¼Œæ—¨åœ¨æå‡Diffusion Modelsåœ¨ç»„åˆç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åˆ©ç”¨Lift Scoresï¼Œç³»ç»Ÿå¯ä»¥è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆæ¯ä¸€é¡¹å•ä¸€æ¡ä»¶ï¼Œå¹¶ç»„åˆè¿™äº›ç»“æœæ¥åˆ¤æ–­å¤æ‚çš„ç»„åˆæç¤ºè¯æ˜¯å¦å¾—åˆ°æ»¡è¶³ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºLift Scoreså¯ä»¥ä»…åˆ©ç”¨åŸå§‹æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜æ•ˆè¿‘ä¼¼ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ç§ä¼˜åŒ–å˜ä½“ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæœ‰æ•ˆæ€§çš„åŒæ—¶é™ä½æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLift Scoresåœ¨2Dåˆæˆæ•°æ®ã€CLEVR Positionä»»åŠ¡ä»¥åŠText-to-Imageåˆæˆä¸­å‡æ˜¾è‘—æé«˜äº†ç»„åˆç”Ÿæˆçš„æ¡ä»¶å¯¹é½èƒ½åŠ›ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¢å¼ºæ‰©æ•£æ¨¡å‹çš„å¤æ‚æŒ‡ä»¤éµå¾ªèƒ½åŠ›æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æ— éœ€è®­ç»ƒçš„æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13740v2",
      "published_date": "2025-05-19 21:34:42 UTC",
      "updated_date": "2025-05-25 22:15:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:35:48.337692+00:00"
    },
    {
      "arxiv_id": "2505.13738v2",
      "title": "Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training",
      "title_zh": "Power Linesï¼šå¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­æƒé‡è¡°å‡ä¸æ‰¹é‡å¤§å°çš„ç¼©æ”¾å®šå¾‹",
      "authors": [
        "Shane Bergsma",
        "Nolan Dey",
        "Gurpreet Gosal",
        "Gavia Gray",
        "Daria Soboleva",
        "Joel Hestness"
      ],
      "abstract": "Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate $Î·$ and weight decay $Î»$. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, $Ï„= B/(Î·Î»D)$, should remain constant across training settings, and we verify the implication that optimal $Î»$ scales linearly with B, for a fixed N and D. However, as N and D scale, we show optimal $Ï„$ obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict $Î»$opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast to prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives. All experiments were run on Cerebras CS-3 systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)é¢„è®­ç»ƒä¸­çš„è¶…å‚æ•°(Hyperparameters)ç¼©æ”¾æ³•åˆ™ï¼Œé‡ç‚¹åˆ†æäº†å­¦ä¹ ç‡($\\eta$)ã€æƒé‡è¡°å‡($\\lambda$)å’Œæ‰¹é‡å¤§å°($B$)éšæ¨¡å‹è§„æ¨¡($N$)åŠæ•°æ®é›†å¤§å°($D$)çš„å˜åŒ–è§„å¾‹ã€‚ç ”ç©¶éªŒè¯äº†åœ¨å›ºå®š$N$å’Œ$D$çš„æƒ…å†µä¸‹ï¼Œæœ€ä¼˜æƒé‡è¡°å‡ä¸æ‰¹é‡å¤§å°æˆçº¿æ€§ç¼©æ”¾ï¼Œå¹¶å‘ç°AdamWæ—¶é—´å°ºåº¦($\\tau$)éµå¾ªå…³äºå‚æ•°ä»£å¸æ¯”($D/N$)çš„ç²¾ç¡®å¹‚å¾‹å…³ç³»(Power Law)ï¼Œè¿™ä¸ºå¤§è§„æ¨¡è®­ç»ƒå‰é¢„æµ‹æœ€ä¼˜æƒé‡è¡°å‡æä¾›äº†å¯é æ–¹æ³•ã€‚ä¸åŒäºä»¥å¾€è§‚ç‚¹ï¼Œç ”ç©¶æŒ‡å‡ºæœ€ä¼˜æ‰¹é‡å¤§å°($B_{opt}$)å’Œä¸´ç•Œæ‰¹é‡å¤§å°($B_{crit}$)ä»…éšæ•°æ®é›†å¤§å°($D$)å‘ˆå¹‚å¾‹ç¼©æ”¾ï¼Œè€Œä¸æ¨¡å‹è§„æ¨¡æ— å…³ã€‚è¿™äº›å‘ç°åœ¨Cerebras CS-3ç³»ç»Ÿä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œä¸ºåœ¨è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æºçº¦æŸä¸‹é€‰æ‹©å¸•ç´¯æ‰˜æœ€ä¼˜(Pareto-optimal)çš„é…ç½®æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13738v2",
      "published_date": "2025-05-19 21:27:33 UTC",
      "updated_date": "2025-11-23 19:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:35:47.258034+00:00"
    },
    {
      "arxiv_id": "2505.13737v2",
      "title": "Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers",
      "title_zh": "å› æœå¤´é—¨æ§ï¼šä¸€ç§è§£æ Transformer æ³¨æ„åŠ›å¤´è§’è‰²çš„æ¡†æ¶",
      "authors": [
        "Andrew Nam",
        "Henry Conklin",
        "Yukang Yang",
        "Thomas Griffiths",
        "Jonathan Cohen",
        "Sarah-Jane Leslie"
      ],
      "abstract": "We present causal head gating (CHG), a scalable method for interpreting the functional roles of attention heads in transformer models. CHG learns soft gates over heads and assigns them a causal taxonomy - facilitating, interfering, or irrelevant - based on their impact on task performance. Unlike prior approaches in mechanistic interpretability, which are hypothesis-driven and require prompt templates or target labels, CHG applies directly to any dataset using standard next-token prediction. We evaluate CHG across multiple large language models (LLMs) in the Llama 3 model family and diverse tasks, including syntax, commonsense, and mathematical reasoning, and show that CHG scores yield causal, not merely correlational, insight validated via ablation and causal mediation analyses. We also introduce contrastive CHG, a variant that isolates sub-circuits for specific task components. Our findings reveal that LLMs contain multiple sparse task-sufficient sub-circuits, that individual head roles depend on interactions with others (low modularity), and that instruction following and in-context learning rely on separable mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Causal Head Gating (CHG)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§£é‡Š Transformer æ¨¡å‹ä¸­æ³¨æ„åŠ›å¤´ (attention heads) åŠŸèƒ½ä½œç”¨çš„å¯æ‰©å±•æ¡†æ¶ã€‚CHG é€šè¿‡åœ¨æ³¨æ„åŠ›å¤´ä¸Šå­¦ä¹ è½¯é—¨æ§ (soft gates)ï¼Œå¹¶æ ¹æ®å…¶å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“å°†å…¶åˆ’åˆ†ä¸ºä¿ƒè¿›ã€å¹²æ‰°æˆ–æ— å…³çš„å› æœåˆ†ç±»ã€‚ä¸ä»¥å¾€ä¾èµ–å‡è®¾é©±åŠ¨æˆ–éœ€è¦ç‰¹å®šæ ‡ç­¾çš„æ–¹æ³•ä¸åŒï¼ŒCHG å¯ç›´æ¥åº”ç”¨äºä»»ä½•ä½¿ç”¨æ ‡å‡† next-token prediction çš„æ•°æ®é›†ï¼Œå…·æœ‰æ›´å¼ºçš„æ™®é€‚æ€§ã€‚åœ¨ Llama 3 ç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æä¾›çš„å› æœè§è§£é€šè¿‡äº†æ¶ˆèå®éªŒå’Œå› æœä¸­ä»‹åˆ†æ (causal mediation analyses) çš„éªŒè¯ã€‚ç ”ç©¶è¿˜é€šè¿‡ contrastive CHG å˜ä½“æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å†…éƒ¨å­˜åœ¨å¤šä¸ªç¨€ç–çš„ä»»åŠ¡å……åˆ†å­ç”µè·¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œå•ä¸ªæ³¨æ„åŠ›å¤´çš„ä½œç”¨è¡¨ç°å‡ºä½æ¨¡å—åŒ– (low modularity) ç‰¹å¾ï¼Œé«˜åº¦ä¾èµ–ä¸å…¶ä»–å¤´çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®äº†æŒ‡ä»¤éµå¾ª (instruction following) ä¸è¯­å¢ƒå­¦ä¹  (in-context learning) ä¾èµ–äºç›¸äº’åˆ†ç¦»çš„è¿ä½œæœºåˆ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures, 2 tables. The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.13737v2",
      "published_date": "2025-05-19 21:24:13 UTC",
      "updated_date": "2025-10-23 23:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:35:52.206739+00:00"
    },
    {
      "arxiv_id": "2505.13729v1",
      "title": "SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation",
      "title_zh": "SayCoNavï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°åˆ†å¸ƒå¼å¤šæœºå™¨äººå¯¼èˆªä¸­çš„è‡ªé€‚åº”åä½œ",
      "authors": [
        "Abhinav Rajvanshi",
        "Pritish Sahu",
        "Tixiao Shan",
        "Karan Sikka",
        "Han-Pang Chiu"
      ],
      "abstract": "Adaptive collaboration is critical to a team of autonomous robots to perform complicated navigation tasks in large-scale unknown environments. An effective collaboration strategy should be determined and adapted according to each robot's skills and current status to successfully achieve the shared goal. We present SayCoNav, a new approach that leverages large language models (LLMs) for automatically generating this collaboration strategy among a team of robots. Building on the collaboration strategy, each robot uses the LLM to generate its plans and actions in a decentralized way. By sharing information to each other during navigation, each robot also continuously updates its step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation (MultiON) tasks, that require the team of the robots to utilize their complementary strengths to efficiently search multiple different objects in unknown environments. By validating SayCoNav with varied team compositions and conditions against baseline methods, our experimental results show that SayCoNav can improve search efficiency by at most 44.28% through effective collaboration among heterogeneous robots. It can also dynamically adapt to the changing conditions during task execution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SayCoNavï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å»ä¸­å¿ƒåŒ–å¤šæœºå™¨äººå¯¼èˆªä¸­å®ç°è‡ªé€‚åº”åä½œçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³è‡ªä¸»æœºå™¨äººå›¢é˜Ÿåœ¨å¤§è§„æ¨¡æœªçŸ¥ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶ï¼Œå¦‚ä½•æ ¹æ®æœºå™¨äººæŠ€èƒ½å’ŒçŠ¶æ€å®æ—¶è°ƒæ•´åä½œç­–ç•¥çš„æŒ‘æˆ˜ã€‚SayCoNavåˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆå›¢é˜Ÿåä½œç­–ç•¥ï¼Œå¹¶æ”¯æŒæ¯ä¸ªæœºå™¨äººåœ¨å»ä¸­å¿ƒåŒ–æ¨¡å¼ä¸‹ç‹¬ç«‹ç”Ÿæˆå…¶è§„åˆ’ä¸è¡ŒåŠ¨ã€‚é€šè¿‡åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­å®æ—¶å…±äº«ä¿¡æ¯ï¼Œå„æœºå™¨äººèƒ½å¤ŸæŒç»­æ›´æ–°å…¶é€æ­¥æ‰§è¡Œè®¡åˆ’ä»¥åº”å¯¹ç¯å¢ƒå˜åŒ–ã€‚å®éªŒåœ¨å¤šç›®æ ‡å¯¼èˆª(MultiON)ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºSayCoNavé€šè¿‡å¼‚æ„æœºå™¨äººé—´çš„æœ‰æ•ˆåä½œï¼Œå°†æœç´¢æ•ˆç‡æœ€é«˜æå‡äº†44.28%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†SayCoNavåœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å…·å¤‡å‡ºè‰²çš„åŠ¨æ€è‡ªé€‚åº”èƒ½åŠ›ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„å¤šæœºå™¨äººååŒæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13729v1",
      "published_date": "2025-05-19 20:58:06 UTC",
      "updated_date": "2025-05-19 20:58:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:36:01.132828+00:00"
    },
    {
      "arxiv_id": "2505.17072v2",
      "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals",
      "title_zh": "å¼•å…¥æ˜¾å¼å®‰å…¨ä¿¡å·ï¼šè®©å®‰å…¨å¯¹é½ä¸å†æµäºè¡¨é¢",
      "authors": [
        "Jianwei Li",
        "Jung-Eun Kim"
      ],
      "abstract": "Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®‰å…¨å¯¹é½(Safety Alignment)è¿‡ç¨‹ä¸­å­˜åœ¨çš„è¡¨é¢åŒ–é—®é¢˜ï¼ŒæŒ‡å‡ºè¿™ç§ç¼ºé™·ä½¿æ¨¡å‹åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»(Adversarial Attacks)æ—¶è¡¨ç°è„†å¼±ã€‚ä½œè€…å‘ç°ï¼Œç°æœ‰çš„å¯¹é½æ–¹æ³•ä¸»è¦ä¾èµ–æ¨¡å‹éšå«åœ°å­¦ä¹ å®‰å…¨æ¨ç†ï¼Œä½†è¿™äº›ä¿¡å·å¾€å¾€ä¼šè¢«å…¶ä»–ä¼˜åŒ–ç›®æ ‡å†²æ·¡ï¼Œå¯¼è‡´å®‰å…¨å†³ç­–è¾¹ç•Œæ¨¡ç³Šã€‚ä¸ºæ­¤ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ˜¾å¼å¼•å…¥å®‰å…¨ç›¸å…³äºŒåˆ†ç±»ä»»åŠ¡çš„æ–¹æ³•ï¼Œå¹¶å°†äº§ç”Ÿçš„å®‰å…¨ä¿¡å·ç›´æ¥æ•´åˆè¿›æ³¨æ„åŠ›æœºåˆ¶(Attention)å’Œè§£ç ç­–ç•¥(Decoding Strategies)ä¸­ã€‚åœ¨å¢åŠ ä¸åˆ°0.2å€é¢å¤–å¼€é”€çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•å…è®¸æ¨¡å‹åœ¨ç”Ÿæˆçš„æ¯ä¸€æ­¥ä¸­å®æ—¶è¯„ä¼°æŸ¥è¯¢åŠå·²ç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æŠµå¾¡å¤šç§å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§ï¼Œä¸ºæ„å»ºæ›´å…·éŸ§æ€§çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17072v2",
      "published_date": "2025-05-19 20:40:46 UTC",
      "updated_date": "2025-05-30 05:54:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:36:13.489354+00:00"
    },
    {
      "arxiv_id": "2505.13718v2",
      "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings",
      "title_zh": "è®­ç»ƒå‰çš„é¢„çƒ­ï¼šè§£é”èµ„æºå—é™ç¯å¢ƒä¸‹çš„é€šç”¨æ¨ç†èƒ½åŠ›",
      "authors": [
        "Safal Shrestha",
        "Minwu Kim",
        "Aadim Nepal",
        "Anubhav Shrestha",
        "Keith Ross"
      ],
      "abstract": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro; $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºæ—¶æ„å»ºå…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆæ ·æœ¬åˆ©ç”¨çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç ”ç©¶è€…é€šè¿‡ä»ç®€å•é€»è¾‘è°œé¢˜ï¼ˆKnights & Knavesï¼‰ä¸­è’¸é¦é•¿é“¾å¼æ€ç»´(Long Chain of Thoughts, CoT)æ¥â€œé¢„çƒ­â€æ¨¡å‹ï¼Œä½¿å…¶è·å¾—é€šç”¨çš„æ¨ç†èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åœ¨é¢„çƒ­åçš„æ¨¡å‹ä¸Šï¼Œåˆ©ç”¨æå°‘é‡çš„ç›®æ ‡é¢†åŸŸæ ·æœ¬è¿›è¡Œå¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning with Verifiable Rewards, RLVR)è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œä»…é€šè¿‡é¢„çƒ­é˜¶æ®µå°±èƒ½æå‡æ¨¡å‹åœ¨ MATHã€HumanEval+ å’Œ MMLU-Pro ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸”åœ¨æ ·æœ¬é‡æå°‘ï¼ˆâ‰¤100ä¸ªï¼‰çš„æƒ…å†µä¸‹ï¼Œé¢„çƒ­åçš„æ¨¡å‹æ€§èƒ½å§‹ç»ˆä¼˜äºåŸºç¡€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº† RLVR çš„æ ·æœ¬æ•ˆç‡ï¼Œè¿˜ä½¿æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè®­ç»ƒåä»èƒ½ä¿æŒè·¨é¢†åŸŸçš„é€šç”¨æ€§ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å¼€å‘é²æ£’çš„æ¨ç† LLMs æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13718v2",
      "published_date": "2025-05-19 20:29:15 UTC",
      "updated_date": "2025-05-26 08:43:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:36:27.530896+00:00"
    },
    {
      "arxiv_id": "2505.15849v2",
      "title": "What Lives? A meta-analysis of diverse opinions on the definition of life",
      "title_zh": "ä½•ä¸ºç”Ÿå‘½ï¼Ÿç”Ÿå‘½å®šä¹‰å¤šå…ƒè§‚ç‚¹çš„å…ƒåˆ†æ",
      "authors": [
        "Reed Bender",
        "Karina Kofman",
        "Blaise AgÃ¼era y Arcas",
        "Michael Levin"
      ],
      "abstract": "The question of \"what is life?\" has challenged scientists and philosophers for centuries, producing an array of definitions that reflect both the mystery of its emergence and the diversity of disciplinary perspectives brought to bear on the question. Despite significant progress in our understanding of biological systems, psychology, computation, and information theory, no single definition for life has yet achieved universal acceptance. This challenge becomes increasingly urgent as advances in synthetic biology, artificial intelligence, and astrobiology challenge our traditional conceptions of what it means to be alive. We undertook a methodological approach that leverages large language models (LLMs) to analyze a set of definitions of life provided by a curated set of cross-disciplinary experts. We used a novel pairwise correlation analysis to map the definitions into distinct feature vectors, followed by agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection to reveal underlying conceptual archetypes. This methodology revealed a continuous landscape of the themes relating to the definition of life, suggesting that what has historically been approached as a binary taxonomic problem should be instead conceived as differentiated perspectives within a unified conceptual latent space. We offer a new methodological bridge between reductionist and holistic approaches to fundamental questions in science and philosophy, demonstrating how computational semantic analysis can reveal conceptual patterns across disciplinary boundaries, and opening similar pathways for addressing other contested definitional territories across the sciences.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é•¿æœŸä»¥æ¥å›°æ‰°ç§‘å­¦ç•Œä¸å“²å­¦ç•Œçš„â€œä»€ä¹ˆæ˜¯ç”Ÿå‘½â€è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆæˆç”Ÿç‰©å­¦ã€äººå·¥æ™ºèƒ½(AI)å’Œåœ°å¤–ç”Ÿç‰©å­¦(Astrobiology)å¿«é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ã€‚ä½œè€…é‡‡ç”¨äº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–¹æ³•è®ºï¼Œå¯¹è·¨å­¦ç§‘ä¸“å®¶æä¾›çš„ç”Ÿå‘½å®šä¹‰è¿›è¡Œäº†ç³»ç»Ÿæ€§å…ƒåˆ†æã€‚ç ”ç©¶é€šè¿‡æˆå¯¹ç›¸å…³æ€§åˆ†æ(Pairwise correlation analysis)å°†å®šä¹‰æ˜ å°„ä¸ºç‰¹å¾å‘é‡ï¼Œå¹¶åˆ©ç”¨å‡èšå±‚æ¬¡èšç±»(Agglomerative clustering)å’Œt-SNEæŠ•å½±ç­‰æ‰‹æ®µæ­ç¤ºäº†æ½œåœ¨çš„æ¦‚å¿µåŸå‹ã€‚åˆ†æç»“æœæ­ç¤ºäº†ä¸€ä¸ªè¿ç»­çš„ä¸»é¢˜æ™¯è§‚ï¼Œè¡¨æ˜å†å²ä¸Šè¢«è§†ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜çš„ç”Ÿå‘½å®šä¹‰ï¼Œå®é™…ä¸Šåº”è¢«ç†è§£ä¸ºä¸€ä¸ªç»Ÿä¸€æ¦‚å¿µæ½œç©ºé—´(Latent space)å†…çš„å·®å¼‚åŒ–è§†è§’ã€‚è¯¥ç ”ç©¶ä¸ºè¿˜åŸè®º(Reductionist)ä¸æ•´ä½“è®º(Holistic)æ–¹æ³•ä¹‹é—´æ­å»ºäº†æ–°çš„æ–¹æ³•è®ºæ¡¥æ¢ï¼Œå±•ç¤ºäº†è®¡ç®—è¯­ä¹‰åˆ†æ(Computational semantic analysis)åœ¨æ­ç¤ºè·¨å­¦ç§‘æ¦‚å¿µæ¨¡å¼æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè§£å†³å…¶ä»–å…·æœ‰äº‰è®®çš„ç§‘å­¦å®šä¹‰æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "q-bio.OT",
        "cs.AI",
        "cs.CY",
        "q-bio.BM",
        "q-bio.CB",
        "q-bio.SC",
        "stat.AP"
      ],
      "primary_category": "q-bio.OT",
      "comment": "54 pages, 4 figures, 2 tables, 11 supplemental figures, 3 supplemental tables",
      "pdf_url": "https://arxiv.org/pdf/2505.15849v2",
      "published_date": "2025-05-19 20:17:37 UTC",
      "updated_date": "2025-08-06 12:47:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:36:24.273111+00:00"
    },
    {
      "arxiv_id": "2505.13709v2",
      "title": "Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning",
      "title_zh": "ç­–ç•¥é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹è‡ªé€‚åº”ï¼šé¢å‘é²æ£’çš„ç¦»çº¿åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Jiayu Chen",
        "Le Xu",
        "Aravind Venugopal",
        "Jeff Schneider"
      ],
      "abstract": "Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿æ¨¡å‹å¼ºåŒ–å­¦ä¹ (Offline MBRL)ä¸­å­˜åœ¨çš„æ¨¡å‹å­¦ä¹ ä¸ç­–ç•¥ä¼˜åŒ–ç›®æ ‡ä¸åŒ¹é…(Objective Mismatch)ï¼Œä»¥åŠç­–ç•¥åœ¨å®é™…éƒ¨ç½²ä¸­å¯¹ç¯å¢ƒå™ªå£°ç¼ºä¹é²æ£’æ€§(Robustness)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æå‡é²æ£’æ€§çš„ä¸–ç•Œæ¨¡å‹åŠ¨æ€è‡ªé€‚åº”æ¡†æ¶ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å°†ä¸–ç•Œæ¨¡å‹(World Model)ä¸ç­–ç•¥(Policy)çš„ä¼˜åŒ–ç»Ÿä¸€åœ¨åŒä¸€ä¸ªå­¦ä¹ ç›®æ ‡ä¸‹ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨æ–¯å¡”å…‹å°”ä¼¯æ ¼(Stackelberg)å­¦ä¹ åŠ¨åŠ›å­¦æ¥è§£å†³å…¶ä¸­æ¶‰åŠçš„æœ€å¤§æœ€å°åŒ–(Maximin Optimization)é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æéªŒè¯äº†è¯¥è®¾è®¡çš„åˆç†æ€§ï¼Œå¹¶æå‡ºäº†è®¡ç®—æ•ˆç‡è¾ƒé«˜çš„å…·ä½“å®ç°æ–¹æ¡ˆã€‚åœ¨12é¡¹å¸¦æœ‰å™ªå£°çš„D4RL MuJoCoä»»åŠ¡å’Œ3é¡¹éšæœºæ‰˜å¡é©¬å…‹æ§åˆ¶(Tokamak Control)ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥ç®—æ³•å±•ç°å‡ºäº†ç›®å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½æ°´å¹³ï¼Œæœ‰æ•ˆæå‡äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚éšæœºç¯å¢ƒä¸‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13709v2",
      "published_date": "2025-05-19 20:14:33 UTC",
      "updated_date": "2025-11-11 07:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:36:32.078783+00:00"
    },
    {
      "arxiv_id": "2505.13706v1",
      "title": "Are Large Language Models Good at Detecting Propaganda?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ“…é•¿æ£€æµ‹å®£ä¼ å—ï¼Ÿ",
      "authors": [
        "Julia Jose",
        "Rachel Greenstadt"
      ],
      "abstract": "Propagandists use rhetorical devices that rely on logical fallacies and emotional appeals to advance their agendas. Recognizing these techniques is key to making informed decisions. Recent advances in Natural Language Processing (NLP) have enabled the development of systems capable of detecting manipulative content. In this study, we look at several Large Language Models and their performance in detecting propaganda techniques in news articles. We compare the performance of these LLMs with transformer-based models. We find that, while GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude 3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally, we find that all three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting instances of one out of six propaganda techniques (name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in detecting instances of appeal to fear and flag-waving.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ£€æµ‹æ–°é—»æ–‡ç« ä¸­å®£ä¼ æŠ€å·§ (propaganda techniques) æ–¹é¢çš„è¡¨ç°ï¼Œå¹¶å°†å…¶ä¸ä¼ ç»Ÿçš„ Transformer æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚ç ”ç©¶é‡ç‚¹è€ƒå¯Ÿäº† GPT-4ã€GPT-3.5 å’Œ Claude 3 Opus ç­‰æ¨¡å‹åœ¨è¯†åˆ«åˆ©ç”¨é€»è¾‘è°¬è¯¯å’Œæƒ…æ„Ÿè¯‰æ±‚è¿›è¡Œæ“çºµçš„å†…å®¹æ—¶çš„å®é™…èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ GPT-4 åœ¨ LLM ç±»åˆ«ä¸­è¡¨ç°æœ€ä½³ (F1=0.16)ï¼Œä½†å…¶æ•´ä½“æ€§èƒ½ä»æ˜¾è‘—ä½äº RoBERTa-CRF åŸºå‡†æ¨¡å‹ (F1=0.67)ã€‚ç„¶è€Œï¼Œåœ¨ç‰¹å®šçš„ç»†åˆ†ä»»åŠ¡ä¸­ï¼Œè¿™ä¸‰ç§ LLM åœ¨æ£€æµ‹ name-calling æŠ€å·§æ–¹é¢å‡ä¼˜äº MultiGranularity Network (MGN) åŸºå‡†ï¼Œä¸” GPT-3.5 å’Œ GPT-4 åœ¨è¯†åˆ« appeal to fear å’Œ flag-waving æ–¹é¢ä¹Ÿå±•ç°å‡ºäº†ä¼˜åŠ¿ã€‚è¯¥é¡¹ç ”ç©¶é€šè¿‡é‡åŒ–è¯„ä¼°æ­ç¤ºäº†å½“å‰ä¸»æµ LLMs åœ¨è¯†åˆ«å¤æ‚å®£ä¼ ç­–ç•¥æ—¶çš„æ½œåŠ›ä¸ç°å­˜çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13706v1",
      "published_date": "2025-05-19 20:11:13 UTC",
      "updated_date": "2025-05-19 20:11:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:37:26.142726+00:00"
    },
    {
      "arxiv_id": "2505.13697v3",
      "title": "RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs",
      "title_zh": "RL ä»…æœ‰å…¶åï¼Ÿå‰–æå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­çš„ç»“æ„æ€§å‡è®¾",
      "authors": [
        "Soumya Rani Samineni",
        "Durgesh Kalwar",
        "Karthik Valmeekam",
        "Kaya Stechly",
        "Subbarao Kambhampati"
      ],
      "abstract": "Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of \"RL generating longer thinking traces.\" While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰¹åˆ¤æ€§åœ°åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒï¼ˆpost-trainingï¼‰ä¸­çš„ç»“æ„æ€§å‡è®¾ï¼Œç‰¹åˆ«æ¢è®¨äº† DeepSeek R1 æ‰€é‡‡ç”¨çš„ GRPO æ–¹æ³•ã€‚ä½œè€…æŒ‡å‡ºï¼Œå½“å‰å°† LLM è®­ç»ƒå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ—¶å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒç¼ºé™·ï¼šä¸€æ˜¯å°†çŠ¶æ€ç®€åŒ–ä¸ºåŠ¨ä½œçš„ç®€å•ä¸²è”ï¼ŒäºŒæ˜¯å°†è½¨è¿¹å¥–åŠ±å‡åŒ€åˆ†é…ï¼Œè¿™å¯¼è‡´ MDP å‘ç”Ÿé€€åŒ–ã€‚ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æè¯æ˜ï¼Œè¿™äº›ç®€åŒ–å‡è®¾ä½¿ç›®å‰çš„ RL æ–¹æ³•åœ¨æ•ˆæœä¸Šç­‰åŒäºç»“æœé©±åŠ¨çš„ç›‘ç£å­¦ä¹ ã€‚åœ¨ GSM8K å’Œ Countdown ç­‰åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒæ˜¾ç¤ºï¼Œç»“åˆæ­£è´Ÿæ ·æœ¬çš„è¿­ä»£ç›‘ç£å¾®è°ƒï¼ˆIterative SFTï¼‰èƒ½å–å¾—ä¸ GRPO ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è®¤ä¸ºè¿™äº›ç»“æ„æ€§å‡è®¾è¯¯å¯¼æ€§åœ°æ¿€åŠ±äº†æ¨¡å‹ç”Ÿæˆæ›´é•¿çš„ä¸­é—´åºåˆ—ï¼Œä»è€Œå½¢æˆäº† RL è‡ªåŠ¨äº§ç”Ÿé•¿æ€ç»´é“¾ï¼ˆthinking tracesï¼‰çš„è¡¨è±¡ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå¯¹ç›®å‰æµè¡Œçš„ LLM RL æ¡†æ¶åŠå…¶è§£é‡Šæå‡ºäº†è´¨ç–‘ï¼Œè®¤ä¸ºå…¶å¯¹ RL æ ¸å¿ƒæœºåˆ¶çš„åˆ©ç”¨ç¨‹åº¦æœ‰å¾…å•†æ¦·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13697v3",
      "published_date": "2025-05-19 19:57:15 UTC",
      "updated_date": "2025-11-10 23:14:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:37:59.644984+00:00"
    },
    {
      "arxiv_id": "2505.13696v1",
      "title": "Building spatial world models from sparse transitional episodic memories",
      "title_zh": "åŸºäºç¨€ç–è¿‡æ¸¡æ€§æƒ…æ™¯è®°å¿†æ„å»ºç©ºé—´ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Zizhan He",
        "Maxime Daigle",
        "Pouya Bashivan"
      ],
      "abstract": "Many animals possess a remarkable capacity to rapidly construct flexible mental models of their environments. These world models are crucial for ethologically relevant behaviors such as navigation, exploration, and planning. The ability to form episodic memories and make inferences based on these sparse experiences is believed to underpin the efficiency and adaptability of these models in the brain. Here, we ask: Can a neural network learn to construct a spatial model of its surroundings from sparse and disjoint episodic memories? We formulate the problem in a simulated world and propose a novel framework, the Episodic Spatial World Model (ESWM), as a potential answer. We show that ESWM is highly sample-efficient, requiring minimal observations to construct a robust representation of the environment. It is also inherently adaptive, allowing for rapid updates when the environment changes. In addition, we demonstrate that ESWM readily enables near-optimal strategies for exploring novel environments and navigating between arbitrary points, all without the need for additional training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œæ˜¯å¦èƒ½ä»ç¨€ç–ä¸”ä¸è¿ç»­çš„ Episodic Memories ä¸­æ„å»ºç¯å¢ƒçš„ç©ºé—´æ¨¡å‹ï¼Œå¹¶æå‡ºäº†åä¸º Episodic Spatial World Model (ESWM) çš„åˆ›æ–°æ¡†æ¶ã€‚ESWM è¡¨ç°å‡ºæé«˜çš„ Sample-efficient ç‰¹æ€§ï¼Œä»…éœ€æå°‘çš„è§‚æµ‹å³å¯æ„å»ºå‡ºç¨³å¥çš„ç¯å¢ƒè¡¨å¾ã€‚è¯¥æ¨¡å‹å…·æœ‰å›ºæœ‰çš„è‡ªé€‚åº”èƒ½åŠ›ï¼Œå…è®¸åœ¨ç¯å¢ƒå‘ç”Ÿå˜åŒ–æ—¶è¿›è¡Œå¿«é€Ÿæ›´æ–°ã€‚æ­¤å¤–ï¼ŒESWM åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå³å¯åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®ç°è¿‘ä¹æœ€ä¼˜çš„æ¢ç´¢ç­–ç•¥å’Œä»»æ„ç‚¹ä¹‹é—´çš„å¯¼èˆªã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨æœ‰é™çš„ç»éªŒæ„å»ºçµæ´»ä¸”é«˜æ•ˆçš„ World Modelsï¼Œä¸ºæ¨¡ä»¿ç”Ÿç‰©è„‘çš„å¯¼èˆªå’Œè§„åˆ’èƒ½åŠ›æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13696v1",
      "published_date": "2025-05-19 19:56:24 UTC",
      "updated_date": "2025-05-19 19:56:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:37:58.582595+00:00"
    },
    {
      "arxiv_id": "2505.13672v1",
      "title": "A*-Decoding: Token-Efficient Inference Scaling",
      "title_zh": "A*-Decodingï¼šé«˜ Token æ•ˆç‡çš„æ¨ç†ç¼©æ”¾",
      "authors": [
        "Giannis Chatziveroglou"
      ],
      "abstract": "Inference-time scaling has emerged as a powerful alternative to parameter scaling for improving language model performance on complex reasoning tasks. While existing methods have shown strong performance gains under fixed compute budgets, there has been little focus on optimally utilizing that budget during inference. In this work, we introduce A*-decoding, a search-based inference-time strategy that builds on the A* search algorithm to optimally utilize a fixed compute budget by prioritizing high-quality reasoning paths during generation. We frame language model decoding as a structured search in a state space of partial solutions, applying the A* transition model to identify promising continuations guided by an external process supervision signal. In our experiments, A*-decoding reaches the performance levels of strong inference scaling baselines like best-of-N and particle filtering while using up to 3x fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the power of structured search in decoding, offering an alternative to brute-force sampling or scale-driven gains. Our work demonstrates how thoughtful inference-time strategies can enhance reasoning in SLMs, pointing toward future advances in more efficient and scalable language model deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† A\\*-decodingï¼Œä¸€ç§åŸºäº A\\* æœç´¢ç®—æ³•çš„æ¨ç†æ—¶ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹é€šè¿‡ä¼˜å…ˆå¤„ç†é«˜è´¨é‡æ¨ç†è·¯å¾„æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å°†è¯­è¨€æ¨¡å‹è§£ç å»ºæ¨¡ä¸ºéƒ¨åˆ†è§£çŠ¶æ€ç©ºé—´ä¸­çš„ç»“æ„åŒ–æœç´¢ï¼Œåˆ©ç”¨å¤–éƒ¨çš„è¿‡ç¨‹ç›‘ç£ä¿¡å· (Process Supervision Signal) æŒ‡å¯¼ A\\* è½¬ç§»æ¨¡å‹è¯†åˆ«æœ€å…·æ½œåŠ›çš„ç”Ÿæˆè·¯å¾„ã€‚å®éªŒè¡¨æ˜ï¼ŒA\\*-decoding åœ¨åŒç­‰è®¡ç®—é¢„ç®—ä¸‹è¾¾åˆ°äº†ä¸ best-of-N å’Œç²’å­æ»¤æ³¢ (Particle Filtering) ç­‰åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†å¤šè¾¾ 3 å€çš„ token ä½¿ç”¨é‡å’Œ 30% çš„è¿‡ç¨‹ç›‘ç£æ¨¡å‹ (PRM) è°ƒç”¨ã€‚åœ¨ MATH500 å’Œ AIME 2024 åŸºå‡†æµ‹è¯•ä¸­ï¼ŒA\\*-decoding åŠ©åŠ› Llama-3.2-1B-Instruct è¾¾åˆ°äº† Llama-3.1-70B-Instruct çš„æ°´å¹³ï¼Œå¹¶ä½¿ Qwen3-1.7B å®ç°äº†ç±»ä¼¼äº o1 çš„æ¨ç†ç²¾åº¦ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»“æ„åŒ–æœç´¢åœ¨è§£ç è¿‡ç¨‹ä¸­çš„æ•ˆåŠ›ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆã€å¯æ‰©å±•çš„å°è¯­è¨€æ¨¡å‹ (SLMs) éƒ¨ç½²æä¾›äº† brute-force é‡‡æ ·ä¹‹å¤–çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13672v1",
      "published_date": "2025-05-19 19:19:48 UTC",
      "updated_date": "2025-05-19 19:19:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:27.647084+00:00"
    },
    {
      "arxiv_id": "2505.13668v3",
      "title": "MAFA: A multi-agent framework for annotation",
      "title_zh": "MAFAï¼šé¢å‘æ ‡æ³¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Mahmood Hegazy",
        "Aaron Rodrigues",
        "Azzam Naeem"
      ],
      "abstract": "Modern consumer banking applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world major bank dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional and single-agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production banking applications while showing strong generalization capabilities across different domains and languages.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAFAï¼Œä¸€ç§é’ˆå¯¹æ¶ˆè´¹è€…é“¶è¡Œä¸šåŠ¡å¸¸è§é—®é¢˜(FAQ)æ ‡æ³¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†ç”¨æˆ·è¯è¯­ç²¾å‡†æ˜ å°„åˆ°ç›¸å…³FAQæ¥æå‡ä¿¡æ¯æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¤šä¸ªé‡‡ç”¨ä¸åŒæ–¹æ³•çš„ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªè¯„åˆ¤æ™ºèƒ½ä½“(judge agent)å¯¹å€™é€‰ç»“æœè¿›è¡Œé‡æ’åº(rerank)ä»¥è·å¾—æœ€ä¼˜ç»“æœã€‚æ™ºèƒ½ä½“å†…éƒ¨åˆ©ç”¨å—Attentive Reasoning Queries (ARQs)å¯å‘çš„ç»“æ„åŒ–æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ç‰¹å®šä»»åŠ¡çš„JSONæŸ¥è¯¢å¼•å¯¼ç³»ç»ŸåŒ–æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒMAFAé‡‡ç”¨äº†å¤šæ ·åŒ–çš„å°‘æ ·æœ¬(few-shot)ç­–ç•¥ï¼Œé€šè¿‡ä¸ºå„æ™ºèƒ½ä½“æä¾›ä¸åŒçš„ç¤ºä¾‹æ¥å¢å¼ºé›†æˆç³»ç»Ÿçš„è¦†ç›–èƒ½åŠ›ã€‚åœ¨çœŸå®é“¶è¡Œæ•°æ®é›†åŠLCQMCã€FiQAç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨Top-1å‡†ç¡®ç‡ä¸Šæå‡äº†14%ï¼Œå¹¶åœ¨å¹³å‡å€’æ•°ç§©(Mean Reciprocal Rank)ç­‰å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚å®éªŒè¯æ˜MAFAåœ¨å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”åœ¨ä¸åŒé¢†åŸŸå’Œè¯­è¨€ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13668v3",
      "published_date": "2025-05-19 19:16:37 UTC",
      "updated_date": "2025-10-15 21:26:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:37:54.015262+00:00"
    },
    {
      "arxiv_id": "2505.13650v1",
      "title": "Self-Reinforced Graph Contrastive Learning",
      "title_zh": "è‡ªå¼ºåŒ–å›¾å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Chou-Ying Hsieh",
        "Chun-Fu Jang",
        "Cheng-En Hsieh",
        "Qian-Hui Chen",
        "Sy-Yen Kuo"
      ],
      "abstract": "Graphs serve as versatile data structures in numerous real-world domains-including social networks, molecular biology, and knowledge graphs-by capturing intricate relational information among entities. Among graph-based learning techniques, Graph Contrastive Learning (GCL) has gained significant attention for its ability to derive robust, self-supervised graph representations through the contrasting of positive and negative sample pairs. However, a critical challenge lies in ensuring high-quality positive pairs so that the intrinsic semantic and structural properties of the original graph are preserved rather than distorted. To address this issue, we propose SRGCL (Self-Reinforced Graph Contrastive Learning), a novel framework that leverages the model's own encoder to dynamically evaluate and select high-quality positive pairs. We designed a unified positive pair generator employing multiple augmentation strategies, and a selector guided by the manifold hypothesis to maintain the underlying geometry of the latent space. By adopting a probabilistic mechanism for selecting positive pairs, SRGCL iteratively refines its assessment of pair quality as the encoder's representational power improves. Extensive experiments on diverse graph-level classification tasks demonstrate that SRGCL, as a plug-in module, consistently outperforms state-of-the-art GCL methods, underscoring its adaptability and efficacy across various domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SRGCL (Self-Reinforced Graph Contrastive Learning)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³Graph Contrastive Learning (GCL)ä¸­é«˜è´¨é‡æ­£æ ·æœ¬å¯¹(positive pairs)é€‰æ‹©æŒ‘æˆ˜çš„æ–°å‹æ¡†æ¶ã€‚SRGCLåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„encoderåŠ¨æ€è¯„ä¼°å¹¶ç­›é€‰æ­£æ ·æœ¬ï¼Œé€šè¿‡é›†æˆå¤šç§æ•°æ®å¢å¼ºç­–ç•¥çš„ç”Ÿæˆå™¨å’ŒåŸºäºmanifold hypothesisçš„ç­›é€‰å™¨ï¼Œç¡®ä¿å­¦ä¹ è¿‡ç¨‹èƒ½å¤Ÿç»´æŒæ½œåœ¨ç©ºé—´çš„å‡ ä½•ç»“æ„ã€‚æ¡†æ¶å¼•å…¥äº†æ¦‚ç‡æœºåˆ¶æ¥é€‰æ‹©æ­£æ ·æœ¬å¯¹ï¼Œéšç€encoderè¡¨ç¤ºèƒ½åŠ›çš„é€æ­¥æå‡ï¼Œç³»ç»Ÿèƒ½å¤Ÿè¿­ä»£ç²¾ç»†åŒ–å¯¹æ ·æœ¬è´¨é‡çš„è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼ŒSRGCLä½œä¸ºä¸€ç§é€šç”¨çš„æ’ä»¶æ¨¡å—(plug-in module)ï¼Œåœ¨å¤šé¡¹å›¾çº§åˆ«åˆ†ç±»ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTA GCLæ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ä»…å¢å¼ºäº†å›¾è¡¨ç¤ºå­¦ä¹ çš„é²æ£’æ€§ï¼Œä¹Ÿä¸ºè·¨é¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ æä¾›äº†å…·å¤‡é«˜åº¦é€‚é…æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13650v1",
      "published_date": "2025-05-19 18:45:54 UTC",
      "updated_date": "2025-05-19 18:45:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:21.975008+00:00"
    },
    {
      "arxiv_id": "2505.13636v2",
      "title": "Incentivizing Truthful Language Models via Peer Elicitation Games",
      "title_zh": "é€šè¿‡å¯¹ç­‰å¯å‘åšå¼ˆæ¿€åŠ±è¯šå®çš„è¯­è¨€æ¨¡å‹",
      "authors": [
        "Baiting Chen",
        "Tong Zhu",
        "Jiale Han",
        "Lexin Li",
        "Gang Li",
        "Xiaowu Dai"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong generative capabilities but remain prone to inconsistencies and hallucinations. We introduce Peer Elicitation Games (PEG), a training-free, game-theoretic framework for aligning LLMs through a peer elicitation mechanism involving a generator and multiple discriminators instantiated from distinct base models. Discriminators interact in a peer evaluation setting, where utilities are computed using a determinant-based mutual information score that provably incentivizes truthful reporting without requiring ground-truth labels. We establish theoretical guarantees showing that each agent, via online learning, achieves sublinear regret in the sense their cumulative performance approaches that of the best fixed truthful strategy in hindsight. Moreover, we prove last-iterate convergence to a truthful Nash equilibrium, ensuring that the actual policies used by agents converge to stable and truthful behavior over time. Empirical evaluations across multiple benchmarks demonstrate significant improvements in factual accuracy. These results position PEG as a practical approach for eliciting truthful behavior from LLMs without supervision or fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Peer Elicitation Games (PEG)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”åŸºäºåšå¼ˆè®ºçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒ…å«ç”Ÿæˆå™¨å’Œå¤šä¸ªä¸åŒåŸºåº§æ¨¡å‹åˆ¤åˆ«å™¨çš„åŒä¼´å¯å‘æœºåˆ¶ (Peer Elicitation) å¯¹é½å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºè¡Œåˆ—å¼çš„äº’ä¿¡æ¯åˆ†æ•° (Determinant-based Mutual Information Score) è®¡ç®—æ•ˆç”¨ï¼Œåœ¨æ— éœ€åœ°é¢çœŸå€¼ (Ground-truth Labels) çš„æƒ…å†µä¸‹è¯æ˜äº†è¯¥æœºåˆ¶èƒ½æœ‰æ•ˆæ¿€åŠ±æ™ºèƒ½ä½“å¦‚å®æŠ¥å‘Šã€‚ç ”ç©¶é€šè¿‡åœ¨çº¿å­¦ä¹  (Online Learning) å»ºç«‹äº†ç†è®ºä¿éšœï¼Œè¯æ˜æ™ºèƒ½ä½“ä¸ä»…èƒ½å®ç°æ¬¡çº¿æ€§æ‚”æ¨å€¼ (Sublinear Regret)ï¼Œè¿˜èƒ½æœ€ç»ˆæ”¶æ•›è‡³çœŸå®çš„çº³ä»€å‡è¡¡ (Nash Equilibrium)ï¼Œç¡®ä¿äº†è¡Œä¸ºçš„ç¨³å®šä¸çœŸå®ã€‚å¤šé¡¹åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒPEG æ˜¾è‘—æå‡äº†æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ï¼Œä¸ºåœ¨æ— éœ€ç›‘ç£æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹è¯±å¯¼ LLM äº§ç”ŸçœŸå®è¡Œä¸ºæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13636v2",
      "published_date": "2025-05-19 18:16:58 UTC",
      "updated_date": "2025-10-19 21:36:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:36.220779+00:00"
    },
    {
      "arxiv_id": "2505.13631v2",
      "title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
      "title_zh": "åŸºäºçº¦æŸä¼˜åŒ–çš„ï¼ˆè¿‘ä¼¼ï¼‰ç­‰å˜ç½‘ç»œå­¦ä¹ ",
      "authors": [
        "Andrei Manolache",
        "Luiz F. O. Chamon",
        "Mathias Niepert"
      ],
      "abstract": "Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç­‰å˜ç¥ç»ç½‘ç»œ (Equivariant neural networks) åœ¨å¤„ç†éå®Œç¾å¯¹ç§°çš„ç°å®æ•°æ®æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºä¸¥æ ¼ç­‰å˜æ¨¡å‹éš¾ä»¥æ‹Ÿåˆå™ªå£°æ•°æ®ï¼Œè€Œéçº¦æŸæ¨¡å‹åˆ™æ— æ³•æœ‰æ•ˆåˆ©ç”¨éƒ¨åˆ†å¯¹ç§°æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å—åŒä¼¦åŸç† (homotopy principles) å¯å‘ï¼Œæå‡ºäº†è‡ªé€‚åº”çº¦æŸç­‰å˜ (Adaptive Constrained Equivariance, ACE) æ¡†æ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨çº¦æŸä¼˜åŒ– (constrained optimization) ç­–ç•¥ï¼Œä»çµæ´»çš„éç­‰å˜æ¨¡å‹å‡ºå‘ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥æ”¶ç´§çº¦æŸæ¥å‡å°‘å¯¹ç­‰å˜æ€§çš„åç¦»ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¹³æ»‘äº†è®­ç»ƒæ—©æœŸçš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿˜èƒ½åœ¨ç­‰å˜æ€§ä¸éç­‰å˜æ€§ä¹‹é—´æ‰¾åˆ°æ•°æ®é©±åŠ¨çš„å¹³è¡¡ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACE åœ¨å¤šç§æ¶æ„å’Œä»»åŠ¡ä¸­å‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€æ ·æœ¬æ•ˆç‡ (sample efficiency) ä»¥åŠå¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ (robustness)ï¼Œå…¶è¡¨ç°ä¼˜äºä¸¥æ ¼ç­‰å˜æ¨¡å‹åŠä¼ ç»Ÿçš„å¯å‘å¼æ¾å¼›æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Oral Camera-Ready",
      "pdf_url": "https://arxiv.org/pdf/2505.13631v2",
      "published_date": "2025-05-19 18:08:09 UTC",
      "updated_date": "2025-12-11 07:51:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:14.516864+00:00"
    },
    {
      "arxiv_id": "2505.13617v1",
      "title": "Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses",
      "title_zh": "é¢å‘ Ambisonic è„‰å†²å“åº”å°‘æ ·æœ¬æ’å€¼çš„æ–¹å‘æ„ŸçŸ¥ç¥ç»å£°åœº",
      "authors": [
        "Christopher Ick",
        "Gordon Wichern",
        "Yoshiki Masuyama",
        "FranÃ§ois Germain",
        "Jonathan Le Roux"
      ],
      "abstract": "The characteristics of a sound field are intrinsically linked to the geometric and spatial properties of the environment surrounding a sound source and a listener. The physics of sound propagation is captured in a time-domain signal known as a room impulse response (RIR). Prior work using neural fields (NFs) has allowed learning spatially-continuous representations of RIRs from finite RIR measurements. However, previous NF-based methods have focused on monaural omnidirectional or at most binaural listeners, which does not precisely capture the directional characteristics of a real sound field at a single point. We propose a direction-aware neural field (DANF) that more explicitly incorporates the directional information by Ambisonic-format RIRs. While DANF inherently captures spatial relations between sources and listeners, we further propose a direction-aware loss. In addition, we investigate the ability of DANF to adapt to new rooms in various ways including low-rank adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ–¹å‘æ„ŸçŸ¥ç¥ç»å£°åœº(Direction-Aware Neural Field, DANF)ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€åŸºäº Neural Fields çš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿ Room Impulse Response (RIR) æ—¶ä»…é™äºå•å£°é“æˆ–åŒå£°é“ã€æ— æ³•ç²¾ç¡®æ•æ‰å•ç‚¹å®šå‘ç‰¹å¾çš„é—®é¢˜ã€‚DANF é€šè¿‡ Ambisonic-format RIRs æ›´æ˜¾å¼åœ°æ•´åˆäº†æ–¹å‘ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰å£°æºä¸å¬è€…ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–¹å‘æ„ŸçŸ¥æŸå¤±(direction-aware loss)å‡½æ•°ã€‚æ­¤å¤–ï¼Œä½œè€…æ·±å…¥æ¢è®¨äº† DANF é€‚åº”æ–°æˆ¿é—´ç¯å¢ƒçš„èƒ½åŠ›ï¼Œå¹¶ç ”ç©¶äº†åŒ…æ‹¬ä½ç§©è‡ªé€‚åº”(low-rank adaptation)åœ¨å†…çš„å¤šç§æ¨¡å‹å¾®è°ƒç­–ç•¥ã€‚è¯¥å·¥ä½œè¯æ˜äº† DANF åœ¨å°‘æ ·æœ¬(Few-Shot)è§‚æµ‹ä¸‹å®ç°é«˜è´¨é‡ Ambisonic è„‰å†²å“åº”ç©ºé—´æ’å€¼çš„æ½œåŠ›ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13617v1",
      "published_date": "2025-05-19 18:01:53 UTC",
      "updated_date": "2025-05-19 18:01:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:30.835469+00:00"
    },
    {
      "arxiv_id": "2505.13448v2",
      "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals",
      "title_zh": "CIEï¼šåˆ©ç”¨è¿ç»­ä¿¡å·æ§åˆ¶è¯­è¨€æ¨¡å‹æ–‡æœ¬ç”Ÿæˆ",
      "authors": [
        "Vinay Samuel",
        "Harshita Diddee",
        "Yiming Zhang",
        "Daphne Ippolito"
      ],
      "abstract": "Aligning language models (LMs) with user intent is becoming increasingly relevant to enhance user experience. This calls for designing methods that can allow users to control the properties of the language that LMs generate, for example, controlling the length of the generation or the complexity of the language that gets chosen. Most existing work attempts to integrate users' control by conditioning LM generations on natural language prompts or discrete control signals, which are often brittle and hard to scale. In this work, we are interested in continuous control signals, ones that exist along a spectrum that can't easily be captured in a natural language prompt or via existing techniques in conditional generation. Through a case study in controlling the precise response-length of generations, we demonstrate how an LM can be finetuned to expect a control vector that is interpolated between a \"low\" and a \"high\" token embedding. Our method more reliably exerts response-length control than in-context learning methods or fine-tuning methods that represent the control signal as a discrete signal.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CIEï¼Œä¸€ç§åˆ©ç”¨è¿ç»­ä¿¡å·æ§åˆ¶è¯­è¨€æ¨¡å‹(Language Models)æ–‡æœ¬ç”Ÿæˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¢å¼ºç”Ÿæˆå±æ€§çš„å¯æ§æ€§æ¥æå‡ç”¨æˆ·ä½“éªŒã€‚é’ˆå¯¹ç›®å‰å¸¸ç”¨çš„è‡ªç„¶è¯­è¨€æç¤º(natural language prompts)æˆ–ç¦»æ•£æ§åˆ¶ä¿¡å·(discrete control signals)åœ¨å¯æ‰©å±•æ€§å’Œç¨³å®šæ€§æ–¹é¢çš„å±€é™ï¼Œè¯¥ç ”ç©¶æ¢ç´¢äº†éš¾ä»¥é€šè¿‡ä¼ ç»Ÿæ–¹å¼æ•æ‰çš„è¿ç»­æ§åˆ¶ä¿¡å·ã€‚ç ”ç©¶å›¢é˜Ÿä»¥ç²¾ç¡®æ§åˆ¶å“åº”é•¿åº¦(response-length)ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¾®è°ƒ(finetuning)ä½¿æ¨¡å‹èƒ½å¤Ÿå“åº”åœ¨â€œä½â€å’Œâ€œé«˜â€æ ‡è®°åµŒå…¥(token embedding)ä¹‹é—´æ’å€¼çš„æ§åˆ¶å‘é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)æˆ–ä¼ ç»Ÿçš„ç¦»æ•£ä¿¡å·å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCIEèƒ½å¤Ÿæ›´å¯é åœ°å®ç°å¯¹æ–‡æœ¬ç”Ÿæˆé•¿åº¦çš„ç²¾ç¡®æ§åˆ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP Main 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13448v2",
      "published_date": "2025-05-19 17:59:58 UTC",
      "updated_date": "2025-09-19 19:06:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:40.378433+00:00"
    },
    {
      "arxiv_id": "2505.13445v1",
      "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
      "title_zh": "ä¿¡ä»»ï¼Œä½†è¦éªŒè¯ï¼šä¸€ç§é¢å‘å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„è‡ªæˆ‘éªŒè¯æ–¹æ³•",
      "authors": [
        "Xiaoyuan Liu",
        "Tian Liang",
        "Zhiwei He",
        "Jiahao Xu",
        "Wenxuan Wang",
        "Pinjia He",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ¨ç†ä¸­å­˜åœ¨çš„â€œæµ…å±‚è‡ªæˆ‘åæ€(superficial self-reflection)â€é—®é¢˜ï¼Œå³æ¨¡å‹éš¾ä»¥ç¨³å¥åœ°éªŒè¯å…¶è‡ªèº«è¾“å‡ºï¼Œæå‡ºäº†RISEï¼ˆReinforcing Reasoning with Self-Verificationï¼‰åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆåŒ–çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ—¨åœ¨åŒæ—¶æå‡æ¨¡å‹çš„è§£é¢˜èƒ½åŠ›ä¸è‡ªæˆ‘éªŒè¯èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨æ¥è‡ªç»“æœéªŒè¯å™¨(outcome verifier)çš„å¯éªŒè¯å¥–åŠ±(verifiable rewards)ä¸ºè§£é¢˜å’ŒéªŒè¯ä»»åŠ¡æä¾›å³æ—¶åé¦ˆã€‚åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼Œæ¨¡å‹ä¸ä»…ç”Ÿæˆè§£é¢˜æ–¹æ¡ˆï¼Œè¿˜ä¼šå¯¹è‡ªèº«ç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œæ‰¹åˆ¤æ€§å®¡æ ¸ï¼Œä½¿ä¸¤è€…çš„åé¦ˆå…±åŒä¿ƒè¿›ç­–ç•¥æ›´æ–°ã€‚åœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRISEåœ¨æé«˜è§£é¢˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„è‡ªæˆ‘éªŒè¯æŠ€èƒ½ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†åœ¨çº¿éªŒè¯å’Œå¢åŠ éªŒè¯è®¡ç®—é‡çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¡¨ç°å‡ºæ›´é¢‘ç¹ä¸”ç²¾ç¡®çš„è‡ªçœè¡Œä¸ºï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§å’Œè‡ªæˆ‘è§‰å¯Ÿèƒ½åŠ›çš„æ¨ç†æ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "code available at https://github.com/xyliu-cs/RISE",
      "pdf_url": "https://arxiv.org/pdf/2505.13445v1",
      "published_date": "2025-05-19 17:59:31 UTC",
      "updated_date": "2025-05-19 17:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:38:54.247259+00:00"
    },
    {
      "arxiv_id": "2505.13439v1",
      "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation",
      "title_zh": "VTBenchï¼šé¢å‘è‡ªå›å½’å›¾åƒç”Ÿæˆçš„è§†è§‰åˆ†è¯å™¨è¯„ä¼°",
      "authors": [
        "Huawei Lin",
        "Tong Geng",
        "Zhaozhuo Xu",
        "Weijie Zhao"
      ],
      "abstract": "Autoregressive (AR) models have recently shown strong performance in image generation, where a critical component is the visual tokenizer (VT) that maps continuous pixel inputs to discrete token sequences. The quality of the VT largely defines the upper bound of AR model performance. However, current discrete VTs fall significantly behind continuous variational autoencoders (VAEs), leading to degraded image reconstructions and poor preservation of details and text. Existing benchmarks focus on end-to-end generation quality, without isolating VT performance. To address this gap, we introduce VTBench, a comprehensive benchmark that systematically evaluates VTs across three core tasks: Image Reconstruction, Detail Preservation, and Text Preservation, and covers a diverse range of evaluation scenarios. We systematically assess state-of-the-art VTs using a set of metrics to evaluate the quality of reconstructed images. Our findings reveal that continuous VAEs produce superior visual representations compared to discrete VTs, particularly in retaining spatial structure and semantic detail. In contrast, the degraded representations produced by discrete VTs often lead to distorted reconstructions, loss of fine-grained textures, and failures in preserving text and object integrity. Furthermore, we conduct experiments on GPT-4o image generation and discuss its potential AR nature, offering new insights into the role of visual tokenization. We release our benchmark and codebase publicly to support further research and call on the community to develop strong, general-purpose open-source VTs.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† VTBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°è‡ªå›å½’ (Autoregressive) å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­è§†è§‰åˆ†è¯å™¨ (Visual Tokenizer, VT) æ€§èƒ½çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ç›®å‰è¯„ä¼°å¾€å¾€ä¾§é‡äºç«¯åˆ°ç«¯ç”Ÿæˆè´¨é‡ï¼Œè€Œ VTBench é€šè¿‡å›¾åƒé‡å»º (Image Reconstruction)ã€ç»†èŠ‚ä¿ç•™ (Detail Preservation) å’Œæ–‡æœ¬ä¿ç•™ (Text Preservation) ä¸‰é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå°† VT çš„æ€§èƒ½ç‹¬ç«‹å‡ºæ¥è¿›è¡Œç³»ç»Ÿè€ƒå¯Ÿã€‚ç ”ç©¶å‘ç°ï¼Œè¿ç»­å˜åˆ†è‡ªç¼–ç å™¨ (Continuous VAEs) åœ¨ä¿ç•™ç©ºé—´ç»“æ„å’Œè¯­ä¹‰ç»†èŠ‚æ–¹é¢æ˜¾è‘—ä¼˜äºç¦»æ•£ VTï¼Œåè€…å¸¸å¯¼è‡´é‡å»ºå¤±çœŸã€ç»†ç²’åº¦çº¹ç†ä¸¢å¤±åŠæ–‡æœ¬å®Œæ•´æ€§å—æŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹ GPT-4o çš„å›¾åƒç”Ÿæˆè¿›è¡Œäº†å®éªŒï¼Œæ¢è®¨äº†å…¶æ½œåœ¨çš„ Autoregressive æœ¬è´¨å¹¶æä¾›äº†æ–°è§è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•åŠå…¶ä»£ç çš„å‘å¸ƒä¸ºå¼€å‘é«˜æ€§èƒ½ä¸”é€šç”¨çš„å¼€æºè§†è§‰åˆ†è¯å™¨æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "24 pages, 13 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.13439v1",
      "published_date": "2025-05-19 17:59:01 UTC",
      "updated_date": "2025-05-19 17:59:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:11.772192+00:00"
    },
    {
      "arxiv_id": "2505.13438v3",
      "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
      "title_zh": "åŸºäºé¢„ç®—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„éšæ—¶æ¨ç†ä¼˜åŒ–",
      "authors": [
        "Penghui Qi",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä»…ä¼˜åŒ–å›ºå®šé«˜ Token é¢„ç®—ä¸‹çš„æœ€ç»ˆæ€§èƒ½ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒå’Œéƒ¨ç½²æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº† AnytimeReasoner æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¼˜åŒ– Anytime Reasoning æ€§èƒ½ï¼Œé€šè¿‡æé«˜ Token æ•ˆç‡å’Œåœ¨ä¸åŒ Token é¢„ç®—çº¦æŸä¸‹çš„æ¨ç†çµæ´»æ€§æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚AnytimeReasoner å°†å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹æˆªæ–­ä»¥é€‚åº”ä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·çš„ Token é¢„ç®—ï¼Œå¹¶è¦æ±‚æ¨¡å‹åœ¨æ¯ä¸ªæˆªæ–­ç‚¹æ€»ç»“å‡ºæœ€ä¼˜ç­”æ¡ˆä»¥è¿›è¡ŒéªŒè¯ã€‚è¿™ç§æ–¹æ³•å°†å¯éªŒè¯çš„å¯†é›†å¥–åŠ±ï¼ˆVerifiable Dense Rewardsï¼‰å¼•å…¥æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œåœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­å®ç°äº†æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ï¼ˆCredit Assignmentï¼‰ã€‚è¯¥ç ”ç©¶é€šè¿‡è§£è€¦çš„æ–¹å¼ä¼˜åŒ–æ€è€ƒä¸æ€»ç»“ç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åä¸º Budget Relative Policy Optimization (BRPO) çš„æ–°å‹æ–¹å·®å‰Šå‡æŠ€æœ¯ï¼Œä»¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå…ˆéªŒåˆ†å¸ƒä¸‹çš„æ‰€æœ‰æ€è€ƒé¢„ç®—è¡¨ç°å‡ä¼˜äº GRPOï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®­ç»ƒä¸ Token ä½¿ç”¨æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13438v3",
      "published_date": "2025-05-19 17:58:44 UTC",
      "updated_date": "2025-11-07 07:01:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:11.642353+00:00"
    },
    {
      "arxiv_id": "2505.13437v1",
      "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance",
      "title_zh": "FinePhysï¼šæ˜¾å¼èå…¥ç‰©ç†å®šå¾‹ä»¥å®ç°æœ‰æ•ˆéª¨æ¶å¼•å¯¼çš„ç»†ç²’åº¦äººä½“åŠ¨ä½œç”Ÿæˆ",
      "authors": [
        "Dian Shao",
        "Mingfei Shi",
        "Shengda Xu",
        "Haodong Chen",
        "Yongle Huang",
        "Binglu Wang"
      ],
      "abstract": "Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as \"switch leap with 0.5 turn\" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç”Ÿæˆä¸­åˆæˆç‰©ç†åˆç†çš„ Fine-grained äººç±»åŠ¨ä½œæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† FinePhys æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›æœ‰æ•ˆçš„éª¨éª¼å¼•å¯¼ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡åœ¨çº¿æ–¹å¼ä¼°è®¡ 2D å§¿æ€ï¼Œå¹¶åˆ©ç”¨ In-context learning æŠ€æœ¯å®ç°ä» 2D åˆ° 3D çš„ç»´åº¦æå‡ã€‚ä¸ºäº†è§£å†³çº¯æ•°æ®é©±åŠ¨æ¨¡å‹åœ¨è§£é‡Šæ€§å’Œç¨³å®šæ€§æ–¹é¢çš„ä¸è¶³ï¼ŒFinePhys å¼•å…¥äº†å— Euler-Lagrange equations çº¦æŸçš„ç‰©ç†è¿åŠ¨é‡ä¼°è®¡æ¨¡å—ï¼Œé€šè¿‡åŒå‘æ—¶é—´æ›´æ–°è®¡ç®—å…³èŠ‚åŠ é€Ÿåº¦ã€‚ç‰©ç†é¢„æµ‹çš„ 3D å§¿æ€éšåä¸æ•°æ®é©±åŠ¨å§¿æ€èåˆï¼Œä¸º Diffusion process æä¾›å¤šå°ºåº¦çš„ 2D Heatmap å¼•å¯¼ã€‚åœ¨ FineGym æ•°æ®é›†çš„å¤šä¸ªå­é›†ï¼ˆFX-JUMPã€FX-TURN å’Œ FX-SALTOï¼‰ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFinePhys çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç«äº‰æ¨¡å‹ã€‚å®šæ€§ç»“æœè¿›ä¸€æ­¥è¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ›´åŠ è‡ªç„¶ä¸”ç¬¦åˆç‰©ç†è§„å¾‹çš„ç²¾ç»†äººç±»åŠ¨ä½œï¼Œæœ‰æ•ˆå¼¥è¡¥äº†å¤æ‚æ—¶é—´åŠ¨æ€å»ºæ¨¡çš„çŸ­æ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13437v1",
      "published_date": "2025-05-19 17:58:11 UTC",
      "updated_date": "2025-05-19 17:58:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:27.231874+00:00"
    },
    {
      "arxiv_id": "2505.13427v2",
      "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision",
      "title_zh": "MM-PRMï¼šåˆ©ç”¨å¯æ‰©å±•çš„æ­¥éª¤çº§ç›‘ç£æå‡å¤šæ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›",
      "authors": [
        "Lingxiao Du",
        "Fanqing Meng",
        "Zongkai Liu",
        "Zhixiang Zhou",
        "Ping Luo",
        "Qiaosheng Zhang",
        "Wenqi Shao"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://github.com/ModalMinds/MM-PRM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MM-PRMï¼Œè¿™æ˜¯ä¸€ç§åœ¨å®Œå…¨è‡ªåŠ¨åŒ–ä¸”å¯æ‰©å±•æ¡†æ¶ä¸‹è®­ç»ƒçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆProcess Reward Modelï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚å¤šæ­¥æ•°å­¦æ¨ç†ä¸­é€»è¾‘ä¸ä¸¥è°¨å’Œå±€éƒ¨é”™è¯¯çš„é—®é¢˜ã€‚ç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹MM-Policyä»¥åŠåŒ…å«1ä¸‡ä¸ªå¯éªŒè¯é—®é¢˜çš„ç§å­æ•°æ®é›†MM-K12ï¼Œéšååˆ©ç”¨åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æµæ°´çº¿ï¼Œåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹è‡ªåŠ¨ç”Ÿæˆäº†è¶…è¿‡70ä¸‡æ¡é«˜è´¨é‡çš„æ­¥éª¤çº§æ ‡æ³¨ã€‚é€šè¿‡åœ¨Best-of-Næ¨ç†è®¾ç½®ä¸­å¯¹å€™é€‰è·¯å¾„è¿›è¡Œè¯„åˆ†ï¼ŒMM-PRMåœ¨MM-K12ã€OlympiadBenchå’ŒMathVistaç­‰å¤šä¸ªé¢†åŸŸå†…åŠè·¨é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ·±å…¥åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†è½¯æ ‡ç­¾ã€è¾ƒå°çš„å­¦ä¹ ç‡ä»¥åŠè·¯å¾„å¤šæ ·æ€§åœ¨ä¼˜åŒ–å¥–åŠ±æ¨¡å‹è¡¨ç°ä¸­çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶å……åˆ†è¯æ˜äº†è¿‡ç¨‹ç›‘ç£ï¼ˆProcess Supervisionï¼‰æ˜¯å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿé€»è¾‘ç¨³å¥æ€§çš„å¼ºæœ‰åŠ›å·¥å…·ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ ‡æ³¨å’Œæ¨¡å‹è¯„ä¼°æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13427v2",
      "published_date": "2025-05-19 17:55:08 UTC",
      "updated_date": "2025-06-05 05:29:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:27.531051+00:00"
    },
    {
      "arxiv_id": "2505.13425v1",
      "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big",
      "title_zh": "è¯­è¨€æ¨¡å‹å­¦ä»¶ï¼šä¸“ä¸šåŒ–å°è¯­è¨€æ¨¡å‹å¤§æœ‰å¯ä¸º",
      "authors": [
        "Zhi-Hao Tan",
        "Zi-Chen Zhao",
        "Hao-Yu Shi",
        "Xin-Yu Zhang",
        "Peng Tan",
        "Yang Yu",
        "Zhi-Hua Zhou"
      ],
      "abstract": "The learnware paradigm offers a novel approach to machine learning by enabling users to reuse a set of well-trained models for tasks beyond the models' original purposes. It eliminates the need to build models from scratch, instead relying on specifications (representations of a model's capabilities) to identify and leverage the most suitable models for new tasks. While learnware has proven effective in many scenarios, its application to language models has remained largely unexplored. At the same time, large language models (LLMs) have demonstrated remarkable universal question-answering abilities, yet they face challenges in specialized scenarios due to data scarcity, privacy concerns, and high computational costs, thus more and more specialized small language models (SLMs) are being trained for specific domains. To address these limitations systematically, the learnware paradigm provides a promising solution by enabling maximum utilization of specialized SLMs, and allowing users to identify and reuse them in a collaborative and privacy-preserving manner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to language models. We simulated a learnware system comprising approximately 100 learnwares of specialized SLMs with 8B parameters, fine-tuned across finance, healthcare, and mathematics domains. Each learnware contains an SLM and a specification, which enables users to identify the most relevant models without exposing their own data. Experimental results demonstrate promising performance: by selecting one suitable learnware for each task-specific inference, the system outperforms the base SLMs on all benchmarks. Compared to LLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and Llama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses Flan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical domain tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†å­¦ä»¶(Learnware)èŒƒå¼åº”ç”¨äºè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç‰¹å®šé¢†åŸŸé¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€éšç§å…³æ³¨å’Œé«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨é’ˆå¯¹ç‰¹å®šé¢†åŸŸå¾®è°ƒçš„å°è¯­è¨€æ¨¡å‹(SLMs)ï¼Œè¯¥æ¡†æ¶ç»“åˆæ¨¡å‹è§„çº¦(Specifications)æŠ€æœ¯ï¼Œå®ç°äº†åœ¨ä¸æ³„éœ²ç”¨æˆ·æ•°æ®çš„å‰æä¸‹è¯†åˆ«å¹¶å¤ç”¨æœ€åˆé€‚æ¨¡å‹çš„ç›®æ ‡ã€‚å®éªŒæ¨¡æ‹Ÿäº†ä¸€ä¸ªåŒ…å«çº¦100ä¸ª8Bå‚æ•°è§„æ¨¡ã€æ¶µç›–é‡‘èã€åŒ»ç–—å’Œæ•°å­¦é¢†åŸŸçš„SLMså­¦ä»¶ç³»ç»Ÿï¼Œç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºç¡€SLMsã€‚åœ¨é‡‘èé¢†åŸŸä»»åŠ¡ä¸­ï¼Œè¯¥ç³»ç»Ÿçš„è¡¨ç°ä¼˜äºQwen1.5-110Bã€Qwen2.5-72BåŠLlama3.1-70B-Instructç­‰å¤§å‹æ¨¡å‹è‡³å°‘14%ï¼Œåœ¨åŒ»ç–—é¢†åŸŸåˆ™è¶…è¶Šäº†Flan-PaLM-540Bã€‚è¿™ä¸€å°è¯•è¯æ˜äº†é€šè¿‡å­¦ä»¶èŒƒå¼æ•´åˆä¸“ä¸šåŒ–SLMsï¼Œå¯ä»¥åœ¨åä½œä¸éšç§ä¿æŠ¤çš„æ¡†æ¶ä¸‹ï¼Œä½¿å°æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­å±•ç°å‡ºè¶…è¶Šé€šç”¨å¤§æ¨¡å‹çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13425v1",
      "published_date": "2025-05-19 17:54:35 UTC",
      "updated_date": "2025-05-19 17:54:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:26.040807+00:00"
    },
    {
      "arxiv_id": "2505.13417v1",
      "title": "AdaptThink: Reasoning Models Can Learn When to Think",
      "title_zh": "AdaptThinkï¼šæ¨ç†æ¨¡å‹èƒ½å¤Ÿå­¦ä¼šä½•æ—¶æ€è€ƒ",
      "authors": [
        "Jiajie Zhang",
        "Nianyi Lin",
        "Lei Hou",
        "Ling Feng",
        "Juanzi Li"
      ],
      "abstract": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹åœ¨å¤„ç†ç®€å•ä»»åŠ¡æ—¶å› å†—é•¿æ€è€ƒè¿‡ç¨‹å¯¼è‡´æ¨ç†å¼€é”€è¿‡å¤§çš„ç“¶é¢ˆï¼Œæå‡ºäº† AdaptThink å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç®—æ³•ã€‚è¯¥ç®—æ³•æ—¨åœ¨æ•™ä¼šæ¨ç†æ¨¡å‹æ ¹æ®é—®é¢˜éš¾åº¦è‡ªé€‚åº”åœ°é€‰æ‹©æ€è€ƒæ¨¡å¼ï¼Œå…¶æ ¸å¿ƒåŒ…å«é¼“åŠ±åœ¨ä¿æŒæ€§èƒ½æ—¶ä¼˜å…ˆé€‰æ‹© NoThinking çš„çº¦æŸä¼˜åŒ–ç›®æ ‡ï¼Œä»¥åŠå¹³è¡¡æ€ç»´æ¨¡å¼æ ·æœ¬çš„é‡è¦é‡‡æ ·(Importance Sampling)ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaptThink åœ¨æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶è¿›ä¸€æ­¥å¢å¼ºäº†æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæ•°å­¦æ•°æ®é›†ä¸Šå°† DeepSeek-R1-Distill-Qwen-1.5B çš„å¹³å‡å“åº”é•¿åº¦ç¼©çŸ­äº† 53%ï¼Œå¹¶æå‡äº† 2.4% çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœçªæ˜¾äº†è‡ªé€‚åº”æ€è€ƒæ¨¡å¼é€‰æ‹©åœ¨ä¼˜åŒ–æ¨ç†è´¨é‡ä¸æ•ˆç‡å¹³è¡¡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæå‡å¤§æ¨¡å‹éƒ¨ç½²æ•ˆç‡æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13417v1",
      "published_date": "2025-05-19 17:50:52 UTC",
      "updated_date": "2025-05-19 17:50:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:40.225949+00:00"
    },
    {
      "arxiv_id": "2505.13408v1",
      "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process",
      "title_zh": "CoT-Kineticsï¼šè¯„ä¼° LRM æ¨ç†è¿‡ç¨‹çš„ç†è®ºå»ºæ¨¡",
      "authors": [
        "Jinhe Bi",
        "Danqi Yan",
        "Yifan Wang",
        "Wenke Huang",
        "Haokun Chen",
        "Guancheng Wan",
        "Mang Ye",
        "Xun Xiao",
        "Hinrich Schuetze",
        "Volker Tresp",
        "Yunpu Ma"
      ],
      "abstract": "Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨ç†æ¨¡å‹(LRMs)åœ¨å¤æ‚ä»»åŠ¡ä¸­æ¨ç†è½¨è¿¹åˆç†æ€§(soundness)éš¾ä»¥è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†å—ç»å…¸åŠ›å­¦å¯å‘çš„CoT-Kineticsç†è®ºå»ºæ¨¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ„å»ºäº†CoT-Kineticsèƒ½é‡æ–¹ç¨‹ï¼Œå°†Transformerå±‚é©±åŠ¨çš„TokençŠ¶æ€è½¬æ¢è¿‡ç¨‹ç±»æ¯”ä¸ºåŠ›åœºä¸­çš„è´¨ç‚¹åŠ¨åŠ›å­¦(particle kinetics dynamics)è¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€æ–¹ç¨‹ï¼Œç ”ç©¶è€…å¯ä»¥ä¸ºæ¨ç†é˜¶æ®µåˆ†é…ä¸€ä¸ªæ ‡é‡åˆ†å€¼ï¼Œç”¨ä»¥é‡åŒ–æ¨ç†é€»è¾‘çš„ä¸¥å¯†æ€§åŠæœ€ç»ˆç­”æ¡ˆçš„ç½®ä¿¡åº¦ã€‚CoT-Kineticsæœ‰æ•ˆåœ°åæ˜ äº†æ¨ç†è¿‡ç¨‹ä¸ç»“è®ºä¹‹é—´çš„å› æœå…³ç³»ï¼Œä½¿å¾—å¯¹LRMè¾“å‡ºè´¨é‡çš„è¯„ä»·ä»ç®€å•çš„å¯¹é”™åˆ¤æ–­æå‡åˆ°äº†ç²¾ç¡®çš„ç†è®ºåº¦é‡ç»´åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13408v1",
      "published_date": "2025-05-19 17:44:26 UTC",
      "updated_date": "2025-05-19 17:44:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:43.200646+00:00"
    },
    {
      "arxiv_id": "2505.13406v1",
      "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database",
      "title_zh": "AutoMathKGï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ•°æ®åº“çš„è‡ªåŠ¨åŒ–æ•°å­¦çŸ¥è¯†å›¾è°±",
      "authors": [
        "Rong Bian",
        "Yu Geng",
        "Zijian Yang",
        "Bing Cheng"
      ],
      "abstract": "A mathematical knowledge graph (KG) presents knowledge within the field of mathematics in a structured manner. Constructing a math KG using natural language is an essential but challenging task. There are two major limitations of existing works: first, they are constrained by corpus completeness, often discarding or manually supplementing incomplete knowledge; second, they typically fail to fully automate the integration of diverse knowledge sources. This paper proposes AutoMathKG, a high-quality, wide-coverage, and multi-dimensional math KG capable of automatic updates. AutoMathKG regards mathematics as a vast directed graph composed of Definition, Theorem, and Problem entities, with their reference relationships as edges. It integrates knowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing entities and relationships with large language models (LLMs) via in-context learning for data augmentation. To search for similar entities, MathVD, a vector database, is built through two designed embedding strategies using SBERT. To automatically update, two mechanisms are proposed. For knowledge completion mechanism, Math LLM is developed to interact with AutoMathKG, providing missing proofs or solutions. For knowledge fusion mechanism, MathVD is used to retrieve similar entities, and LLM is used to determine whether to merge with a candidate or add as a new entity. A wide range of experiments demonstrate the advanced performance and broad applicability of the AutoMathKG system, including superior reachability query results in MathVD compared to five baselines and robust mathematical reasoning capability in Math LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoMathKGï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€å¹¿è¦†ç›–ä¸”å…·å¤‡è‡ªåŠ¨æ›´æ–°èƒ½åŠ›çš„å¤šç»´æ•°å­¦çŸ¥è¯†å›¾è°± (Knowledge Graph)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°å­¦çŸ¥è¯†åº“åœ¨è¯­æ–™å®Œæ•´æ€§å’Œè‡ªåŠ¨åŒ–é›†æˆæ–¹é¢çš„å±€é™æ€§ã€‚AutoMathKG å°†æ•°å­¦è§†ä¸ºç”± Definitionã€Theorem å’Œ Problem å®ä½“æ„æˆçš„æœ‰å‘å›¾ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ä¸Šä¸‹æ–‡å­¦ä¹  (in-context learning) æŠ€æœ¯å¯¹æ¥è‡ª ProofWikiã€æ•™æåŠ arXiv ç­‰æ¸ é“çš„å¤šæºçŸ¥è¯†è¿›è¡Œå¢å¼ºã€‚ä¸ºäº†å®ç°é«˜æ•ˆæ£€ç´¢ä¸è‡ªåŠ¨æ›´æ–°ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŸºäº SBERT åµŒå…¥ç­–ç•¥çš„å‘é‡æ•°æ®åº“ MathVDï¼Œå¹¶è®¾è®¡äº†åŒ…å«çŸ¥è¯†è¡¥å…¨ä¸èåˆæœºåˆ¶çš„ Math LLM ä»¥ç”Ÿæˆç¼ºå¤±çš„è¯æ˜æˆ–è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoMathKG åœ¨å¯è¾¾æ€§æŸ¥è¯¢ (reachability query) è¡¨ç°ä¸Šä¼˜äºäº”ç§åŸºçº¿æ¨¡å‹ï¼Œä¸”å…¶å†…ç½®çš„ Math LLM å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ•°å­¦é¢†åŸŸçš„ç»“æ„åŒ–çŸ¥è¯†è‡ªåŠ¨åŒ–æ„å»ºæä¾›äº†åˆ›æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13406v1",
      "published_date": "2025-05-19 17:41:29 UTC",
      "updated_date": "2025-05-19 17:41:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:39.531091+00:00"
    },
    {
      "arxiv_id": "2505.18191v1",
      "title": "SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference",
      "title_zh": "ä»¥ SzCORE ä¸ºåŸºå‡†ï¼š2025å¹´ç™«ç—«ä¸ç¥ç»ç³»ç»Ÿç–¾ç—…äººå·¥æ™ºèƒ½ä¼šè®®ç™«ç—«å‘ä½œæ£€æµ‹æŒ‘æˆ˜èµ›æŠ¥å‘Š",
      "authors": [
        "Jonathan Dan",
        "Amirhossein Shahbazinia",
        "Christodoulos Kechris",
        "David Atienza"
      ],
      "abstract": "Reliable automatic seizure detection from long-term EEG remains a challenge, as current machine learning models often fail to generalize across patients or clinical settings. Manual EEG review remains the clinical standard, underscoring the need for robust models and standardized evaluation. To rigorously assess algorithm performance, we organized a challenge using a private dataset of continuous EEG recordings from 65 subjects (4,360 hours). Expert neurophysiologists annotated the data, providing ground truth for seizure events. Participants were required to detect seizure onset and duration, with evaluation based on event-based metrics, including sensitivity, precision, F1-score, and false positives per day. The SzCORE framework ensured standardized evaluation. The primary ranking criterion was the event-based F1-score, reflecting clinical relevance by balancing sensitivity and false positives. The challenge received 30 submissions from 19 teams, with 28 algorithms evaluated. Results revealed wide variability in performance, with a top F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing difficulty of seizure detection. The challenge also revealed a gap between reported performance and real-world evaluation, emphasizing the importance of rigorous benchmarking. Compared to previous challenges and commercial systems, the best-performing algorithm in this contest showed improved performance. Importantly, the challenge platform now supports continuous benchmarking, enabling reproducible research, integration of new datasets, and clinical evaluation of seizure detection algorithms using a standardized framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æœŸè„‘ç”µå›¾(EEG)è‡ªåŠ¨ç™«ç—«æ£€æµ‹(seizure detection)ä¸­æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®åŠç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°çš„é—®é¢˜ï¼Œä»‹ç»äº†2025å¹´ç™«ç—«ä¸ç¥ç»ç–¾ç—…AIä¼šè®®ä¸Šç»„ç»‡çš„æŒ‘æˆ˜èµ›æˆæœã€‚è¯¥æŒ‘æˆ˜èµ›åˆ©ç”¨åŒ…å«65åå—è¯•è€…ã€å…±4,360å°æ—¶çš„ä¸“å®¶æ ‡æ³¨ç§æœ‰EEGæ•°æ®é›†ï¼Œé‡‡ç”¨SzCOREæ¡†æ¶å¯¹æ¥è‡ª19ä¸ªå›¢é˜Ÿçš„ç®—æ³•è¿›è¡Œäº†æ ‡å‡†åŒ–è¯„ä¼°ã€‚å‚èµ›ç®—æ³•éœ€æ£€æµ‹ç™«ç—«çš„å‘ä½œèµ·ç‚¹å’ŒæŒç»­æ—¶é—´ï¼Œä¸»è¦è¯„ä»·æŒ‡æ ‡ä¸ºå¹³è¡¡äº†çµæ•åº¦(sensitivity)å’Œè¯¯æŠ¥ç‡çš„åŸºäºäº‹ä»¶çš„F1-scoreã€‚ç»“æœæ˜¾ç¤ºï¼Œè¡¨ç°æœ€ä½³çš„ç®—æ³•è¾¾åˆ°äº†43%çš„F1-scoreï¼Œè™½ç„¶æ€§èƒ½ä¼˜äºä»¥å¾€æŒ‘æˆ˜èµ›å’Œå•†ä¸šç³»ç»Ÿï¼Œä½†ä»æ­ç¤ºäº†å®éªŒå®¤æŠ¥å‘Šæ€§èƒ½ä¸ç°å®ä¸´åºŠåº”ç”¨è¯„ä¼°ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚ç ”ç©¶å¼ºè°ƒäº†ä¸¥è°¨åŸºå‡†æµ‹è¯•(benchmarking)åœ¨ç™«ç—«æ£€æµ‹ä¸­çš„å¿…è¦æ€§ï¼Œå¹¶æŒ‡å‡ºè¯¥æŒ‘æˆ˜èµ›å¹³å°ç›®å‰å·²æ”¯æŒæŒç»­çš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºç®—æ³•çš„å¯é‡å¤ç ”ç©¶å’Œä¸´åºŠé›†æˆæä¾›äº†é‡è¦çš„æ ‡å‡†åŒ–æ¡†æ¶ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18191v1",
      "published_date": "2025-05-19 17:36:20 UTC",
      "updated_date": "2025-05-19 17:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:40:02.879326+00:00"
    },
    {
      "arxiv_id": "2505.13400v1",
      "title": "Robin: A multi-agent system for automating scientific discovery",
      "title_zh": "Robinï¼šç”¨äºè‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Ali Essam Ghareeb",
        "Benjamin Chang",
        "Ludovico Mitchener",
        "Angela Yiu",
        "Caralyn J. Szostkiewicz",
        "Jon M. Laurent",
        "Muhammed T. Razzak",
        "Andrew D. White",
        "Michaela M. Hinks",
        "Samuel G. Rodriques"
      ],
      "abstract": "Scientific discovery is driven by the iterative process of background research, hypothesis generation, experimentation, and data analysis. Despite recent advancements in applying artificial intelligence to scientific discovery, no system has yet automated all of these stages in a single workflow. Here, we introduce Robin, the first multi-agent system capable of fully automating the key intellectual steps of the scientific process. By integrating literature search agents with data analysis agents, Robin can generate hypotheses, propose experiments, interpret experimental results, and generate updated hypotheses, achieving a semi-autonomous approach to scientific discovery. By applying this system, we were able to identify a novel treatment for dry age-related macular degeneration (dAMD), the major cause of blindness in the developed world. Robin proposed enhancing retinal pigment epithelium phagocytosis as a therapeutic strategy, and identified and validated a promising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho kinase (ROCK) inhibitor that has never previously been proposed for treating dAMD. To elucidate the mechanism of ripasudil-induced upregulation of phagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment, which revealed upregulation of ABCA1, a critical lipid efflux pump and possible novel target. All hypotheses, experimental plans, data analyses, and data figures in the main text of this report were produced by Robin. As the first AI system to autonomously discover and validate a novel therapeutic candidate within an iterative lab-in-the-loop framework, Robin establishes a new paradigm for AI-driven scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Robinï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿå®Œå…¨è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°æ ¸å¿ƒæ™ºèƒ½æ­¥éª¤çš„ multi-agent systemã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆæ–‡çŒ®æœç´¢æ™ºèƒ½ä½“ä¸æ•°æ®åˆ†ææ™ºèƒ½ä½“ï¼Œå®ç°äº†ä»èƒŒæ™¯ç ”ç©¶ã€å‡è®¾ç”Ÿæˆ (hypothesis generation) åˆ°å®éªŒæ–¹æ¡ˆè®¾è®¡åŠç»“æœè§£è¯»çš„åŠè‡ªä¸»å·¥ä½œæµã€‚åœ¨é’ˆå¯¹å¹²æ€§è€å¹´æ€§é»„æ–‘å˜æ€§ (dAMD) çš„ç ”ç©¶åº”ç”¨ä¸­ï¼ŒRobin æˆåŠŸè¯†åˆ«å¹¶éªŒè¯äº† rho kinase (ROCK) æŠ‘åˆ¶å‰‚ ripasudil ä½œä¸ºæ½œåœ¨çš„æ–°å‹æ²»ç–—å€™é€‰è¯ç‰©ã€‚é€šè¿‡è¿›ä¸€æ­¥æè®®å¹¶åˆ†æ RNA-seq å®éªŒï¼Œç³»ç»Ÿæ­ç¤ºäº† ABCA1 è¿™ä¸€å…³é”®è„‚è´¨å¤–æ’æ³µåœ¨å¢å¼ºåå™¬ä½œç”¨ä¸­çš„æœºåˆ¶ä½œç”¨ã€‚ä½œä¸ºé¦–ä¸ªåœ¨ lab-in-the-loop æ¡†æ¶ä¸‹è‡ªä¸»å‘ç°å¹¶éªŒè¯æ–°ç–—æ³•çš„ AI ç³»ç»Ÿï¼ŒRobin ä¸º AI é©±åŠ¨çš„ç§‘å­¦å‘ç°ç¡®ç«‹äº†å…¨æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13400v1",
      "published_date": "2025-05-19 17:36:17 UTC",
      "updated_date": "2025-05-19 17:36:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:39:44.724847+00:00"
    },
    {
      "arxiv_id": "2505.13393v2",
      "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar",
      "title_zh": "IG Parserï¼šåŸºäºåˆ¶åº¦è¯­æ³•çš„åˆ¶åº¦é™ˆè¿°ç¼–ç è½¯ä»¶åŒ…",
      "authors": [
        "Christopher K. Frantz"
      ],
      "abstract": "This article provides an overview of IG Parser, a software that facilitates qualitative content analysis of formal (e.g., legal) rules or informal (e.g., social) norms, and strategies (such as conventions) -- referred to as institutions -- that govern social systems and operate configurally to describe institutional systems. To this end, the IG Parser employs a distinctive syntax that ensures rigorous encoding of natural language, while automating the transformation into various formats that support the downstream analysis using diverse analytical techniques. The conceptual core of the IG Parser is an associated syntax, IG Script, that operationalizes the conceptual foundations of the Institutional Grammar, and more specifically the Institutional Grammar 2.0, an analytical paradigm for institutional analysis. This article presents the IG Parser, including its conceptual foundations, the syntax specification of IG Script, and its architectural principles. This overview is augmented with selective illustrative examples that highlight its use and the associated benefits.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† IG Parserï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨å¯¹ç®¡ç†ç¤¾ä¼šç³»ç»Ÿçš„æ­£å¼è§„åˆ™ã€éæ­£å¼è§„èŒƒåŠç­–ç•¥ï¼ˆç»Ÿç§°ä¸º institutionsï¼‰è¿›è¡Œå®šæ€§å†…å®¹åˆ†æçš„è½¯ä»¶åŒ…ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„è¯­æ³• IG Scriptï¼Œè¯¥è¯­æ³•å®ç°äº† Institutional Grammar 2.0 è¿™ä¸€åˆ¶åº¦åˆ†æèŒƒå¼çš„æ¦‚å¿µåŸºç¡€å’Œæ“ä½œåŒ–ã€‚IG Parser èƒ½å¤Ÿç¡®ä¿å¯¹è‡ªç„¶è¯­è¨€è¿›è¡Œä¸¥è°¨çš„ç¼–ç ï¼Œå¹¶è‡ªåŠ¨å°†å…¶è½¬æ¢ä¸ºæ”¯æŒå¤šç§åˆ†ææŠ€æœ¯çš„å„ç§ä¸‹æ¸¸æ ¼å¼ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†è¯¥è½¯ä»¶çš„æ¦‚å¿µåŸºç¡€ã€IG Script çš„è¯­æ³•è§„èŒƒä»¥åŠç³»ç»Ÿçš„æ¶æ„åŸç†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å…·ä½“çš„åº”ç”¨ç¤ºä¾‹å±•ç¤ºäº†è¯¥å·¥å…·åœ¨åˆ¶åº¦ç³»ç»Ÿé…ç½®åŒ–æè¿°æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚é€šè¿‡å°†åˆ¶åº¦è¯­æ³•ç†è®ºè½¬åŒ–ä¸ºå®ç”¨çš„è½¯ä»¶å·¥å…·ï¼ŒIG Parser ä¸ºå¤æ‚çš„åˆ¶åº¦åˆ†ææä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "24 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.13393v2",
      "published_date": "2025-05-19 17:33:15 UTC",
      "updated_date": "2025-05-20 09:52:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:41:40.875580+00:00"
    },
    {
      "arxiv_id": "2505.13391v1",
      "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks",
      "title_zh": "æå‡å„ç±»æŠ½è±¡è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "MikoÅ‚aj MaÅ‚kiÅ„ski",
        "Jacek MaÅ„dziuk"
      ],
      "abstract": "The abstract visual reasoning (AVR) domain presents a diverse suite of analogy-based tasks devoted to studying model generalization. Recent years have brought dynamic progress in the field, particularly in i.i.d. scenarios, in which models are trained and evaluated on the same data distributions. Nevertheless, o.o.d. setups that assess model generalization to new test distributions remain challenging even for the most recent models. To advance generalization in AVR tasks, we present the Pathways of Normalized Group Convolution model (PoNG), a novel neural architecture that features group convolution, normalization, and a parallel design. We consider a wide set of AVR benchmarks, including Raven's Progressive Matrices and visual analogy problems with both synthetic and real-world images. The experiments demonstrate strong generalization capabilities of the proposed model, which in several settings outperforms the existing literature methods.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºæŠ½è±¡è§†è§‰æ¨ç†(Abstract Visual Reasoning, AVR)é¢†åŸŸçš„æ³›åŒ–æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹éš¾ä»¥åº”å¯¹çš„åˆ†å¸ƒå¤–(out-of-distribution, o.o.d.)è®¾ç½®ä¸‹ã€‚ä¸ºæå‡AVRä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†PoNGæ¨¡å‹ï¼Œè¯¥æ¶æ„åˆ›æ–°æ€§åœ°ç»“åˆäº†ç¾¤å·ç§¯(Group Convolution)ã€å½’ä¸€åŒ–(Normalization)ä»¥åŠå¹¶è¡Œè®¾è®¡ã€‚ç ”ç©¶åœ¨ç‘æ–‡æ¨ç†çŸ©é˜µ(Raven's Progressive Matrices)å’ŒåŒ…å«åˆæˆåŠçœŸå®å›¾åƒçš„è§†è§‰ç±»æ¯”é—®é¢˜ç­‰å¤šç§åŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¯æ˜PoNGæ¨¡å‹å…·å¤‡æå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªè¯„ä¼°åœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†è·¨ä»»åŠ¡æŠ½è±¡æ¨ç†æ¨¡å‹çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.13391v1",
      "published_date": "2025-05-19 17:32:07 UTC",
      "updated_date": "2025-05-19 17:32:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:40:31.890510+00:00"
    },
    {
      "arxiv_id": "2505.13388v3",
      "title": "R3: Robust Rubric-Agnostic Reward Models",
      "title_zh": "R3ï¼šé²æ£’çš„æ— å…³è¯„åˆ†å‡†åˆ™å¥–åŠ±æ¨¡å‹",
      "authors": [
        "David Anugraha",
        "Zilu Tang",
        "Lester James V. Miranda",
        "Hanyang Zhao",
        "Mohammad Rifqi Farhansyah",
        "Garry Kuwanto",
        "Derry Wijaya",
        "Genta Indra Winata"
      ],
      "abstract": "Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce $\\shortmethodname$, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. $\\shortmethodname$ enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¥–åŠ±æ¨¡å‹(Reward Models)åœ¨å¯æ§æ€§ã€è§£é‡Šæ€§åŠæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸ºR3çš„é²æ£’ä¸”ä¸å‡†åˆ™æ— å…³(Rubric-Agnostic)çš„æ–°å‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ã€‚R3èƒ½å¤Ÿè·¨è¶Šä¸åŒçš„è¯„ä¼°ç»´åº¦è¿›è¡Œæ³›åŒ–ï¼Œå¹¶é€šè¿‡æä¾›åŸºäºæ¨ç†çš„è¯„åˆ†åˆ†é…æ¥æ˜¾è‘—å¢å¼ºè¯„ä¼°ç»“æœçš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”¯æŒä¸å¤šæ ·åŒ–äººç±»ä»·å€¼è§‚åŠä½¿ç”¨åœºæ™¯çš„ç¨³å¥å¯¹é½ï¼Œå®ç°äº†å¯¹è¯­è¨€æ¨¡å‹æ›´é€æ˜ã€æ›´çµæ´»çš„è¯„ä¼°ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ ‡é‡è¾“å‡ºå› ç¼ºä¹ä¸Šä¸‹æ–‡æ¨ç†è€Œéš¾ä»¥è§£è¯»çš„é—®é¢˜ï¼Œä¸ºæ„å»ºå¯ä¿¡ä»»çš„å¯¹é½æœºåˆ¶æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶ç›¸å…³çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å·²åœ¨GitHubä¸Šå¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.13388v3",
      "published_date": "2025-05-19 17:29:03 UTC",
      "updated_date": "2025-09-19 22:07:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:40:50.929820+00:00"
    },
    {
      "arxiv_id": "2505.13381v1",
      "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors",
      "title_zh": "åœ¨æ¨¡æ‹Ÿè€ƒè¯•ä¸­å¼•å…¥è¾…åŠ©AIåé¦ˆçš„å…ƒè®¤çŸ¥è¦æ±‚å¦‚ä½•é‡å¡‘å­¦ç”Ÿçš„å­¦ä¹ è¡Œä¸º",
      "authors": [
        "Mak Ahmad",
        "Prerna Ravi",
        "David Karger",
        "Marc Facciotti"
      ],
      "abstract": "Providing personalized, detailed feedback at scale in large undergraduate STEM courses remains a persistent challenge. We present an empirically evaluated practice exam system that integrates AI generated feedback with targeted textbook references, deployed in a large introductory biology course. Our system encourages metacognitive behavior by asking students to explain their answers and declare their confidence. It uses OpenAI's GPT-4o to generate personalized feedback based on this information, while directing them to relevant textbook sections. Through interaction logs from consenting participants across three midterms (541, 342, and 413 students respectively), totaling 28,313 question-student interactions across 146 learning objectives, along with 279 surveys and 23 interviews, we examined the system's impact on learning outcomes and engagement. Across all midterms, feedback types showed no statistically significant performance differences, though some trends suggested potential benefits. The most substantial impact came from the required confidence ratings and explanations, which students reported transferring to their actual exam strategies. About 40 percent of students engaged with textbook references when prompted by feedback -- far higher than traditional reading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5), with 82.1 percent reporting increased confidence on practiced midterm topics, and 73.4 percent indicating they could recall and apply specific concepts. Our findings suggest that embedding structured reflection requirements may be more impactful than sophisticated feedback mechanisms.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¸€ä¸ªå¤§å‹æœ¬ç§‘ STEM è¯¾ç¨‹ä¸­éƒ¨ç½²çš„æ¨¡æ‹Ÿè€ƒè¯•ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ OpenAI çš„ GPT-4o ç”Ÿæˆä¸ªæ€§åŒ–åé¦ˆå¹¶ç»“åˆæ•™ç§‘ä¹¦å¼•ç”¨(textbook references)ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¦æ±‚å­¦ç”Ÿè§£é‡Šå…¶ç­”æ¡ˆå¹¶å£°æ˜ç½®ä¿¡åº¦æ¥å¼ºåŒ–å…ƒè®¤çŸ¥(metacognitive)è¡Œä¸ºï¼Œå¹¶æ®æ­¤æä¾›å®šåˆ¶åŒ–æŒ‡å¯¼ã€‚é€šè¿‡å¯¹ä¸‰åœºæœŸä¸­è€ƒè¯•ä¸­æ•°åƒæ¬¡äº’åŠ¨æ—¥å¿—ã€è°ƒæŸ¥å’Œè®¿è°ˆçš„åˆ†æï¼Œç ”ç©¶å‘ç°è™½ç„¶åé¦ˆç±»å‹å¯¹æˆç»©çš„æå‡æ²¡æœ‰æ˜¾è‘—çš„ç»Ÿè®¡å­¦å·®å¼‚ï¼Œä½†å¼ºåˆ¶æ€§çš„ç½®ä¿¡åº¦è¯„çº§å’Œè§£é‡Šè¿‡ç¨‹ä¿ƒä½¿å­¦ç”Ÿå°†å…¶è½¬åŒ–ä¸ºå®é™…çš„è€ƒè¯•ç­–ç•¥ã€‚æ­¤å¤–ï¼Œçº¦ 40% çš„å­¦ç”Ÿåœ¨åé¦ˆæç¤ºä¸‹ä¸»åŠ¨æŸ¥é˜…æ•™ç§‘ä¹¦ï¼Œæ˜¾è‘—é«˜äºä¼ ç»Ÿçš„é˜…è¯»å‚ä¸åº¦ã€‚è°ƒæŸ¥æ•°æ®æ˜¾ç¤ºå­¦ç”Ÿæ»¡æ„åº¦é«˜è¾¾ 4.1/5ï¼Œä¸”ç»å¤§å¤šæ•°å­¦ç”Ÿè¡¨ç¤ºå¤ä¹ åçš„ä¿¡å¿ƒå’Œæ¦‚å¿µåº”ç”¨èƒ½åŠ›æœ‰æ‰€æå‡ã€‚ç ”ç©¶æœ€ç»ˆå¾—å‡ºç»“è®ºï¼Œåœ¨æå‡å­¦ä¹ æ•ˆæœæ–¹é¢ï¼ŒåµŒå…¥ç»“æ„åŒ–çš„åæ€è¦æ±‚å¯èƒ½æ¯”å…ˆè¿›çš„ AI åé¦ˆæœºåˆ¶æœ¬èº«æ›´å…·å½±å“åŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy",
      "pdf_url": "https://arxiv.org/pdf/2505.13381v1",
      "published_date": "2025-05-19 17:25:07 UTC",
      "updated_date": "2025-05-19 17:25:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:40:26.709388+00:00"
    },
    {
      "arxiv_id": "2505.13380v1",
      "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition",
      "title_zh": "CompeteSMoEï¼šåŸºäºç«äº‰æœºåˆ¶ä¸”å…·æœ‰ç»Ÿè®¡ä¿è¯çš„ä¸“å®¶æ··åˆæ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Nam V. Nguyen",
        "Huy Nguyen",
        "Quang Pham",
        "Van Nguyen",
        "Savitha Ramasamy",
        "Nhat Ho"
      ],
      "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹(Sparse mixture of experts, SMoE)åœ¨æ‰©å±•æ¨¡å‹å¤æ‚åº¦æ—¶çš„è·¯ç”±æ•ˆç‡é—®é¢˜ï¼ŒæŒ‡å‡ºç”±äºæ‰§è¡Œè®¡ç®—çš„ä¸“å®¶å¹¶ä¸ç›´æ¥å‚ä¸è·¯ç”±å†³ç­–ï¼Œä¼ ç»Ÿçš„è·¯ç”±æœºåˆ¶å¾€å¾€ä¼šå¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸ºCompetitionçš„æ–°å‹æœºåˆ¶ï¼Œæ—¨åœ¨å°†Tokenè·¯ç”±è‡³å…·æœ‰æœ€é«˜ç¥ç»å“åº”(Neural response)çš„ä¸“å®¶ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§ç«äº‰æœºåˆ¶ç›¸æ¯”ä¼ ç»Ÿçš„Softmaxè·¯ç”±å…·æœ‰æ›´ä¼˜çš„æ ·æœ¬æ•ˆç‡(Sample efficiency)ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†CompeteSMoEç®—æ³•ï¼Œé€šè¿‡éƒ¨ç½²è·¯ç”±å™¨å­¦ä¹ ç«äº‰ç­–ç•¥ï¼Œåœ¨ä¿æŒè¾ƒä½è®­ç»ƒå¼€é”€(Training overhead)çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚åœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒ(Visual instruction tuning)å’Œè¯­è¨€é¢„è®­ç»ƒ(Language pre-training)ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒCompeteSMoEåœ¨æœ‰æ•ˆæ€§ã€é²æ£’æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„SOTA SMoEç­–ç•¥ã€‚è¯¥å·¥ä½œä¸ºé«˜æ•ˆè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§å…·æœ‰ç»Ÿè®¡ä¿è¯ä¸”æ˜“äºå®ç°çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "52 pages. This work is an improved version of the previous study at arXiv:2402.02526",
      "pdf_url": "https://arxiv.org/pdf/2505.13380v1",
      "published_date": "2025-05-19 17:24:26 UTC",
      "updated_date": "2025-05-19 17:24:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:40:43.722522+00:00"
    },
    {
      "arxiv_id": "2505.13379v2",
      "title": "Thinkless: LLM Learns When to Think",
      "title_zh": "Thinklessï¼šå¤§è¯­è¨€æ¨¡å‹å­¦ä¹ ä½•æ—¶æ€è€ƒ",
      "authors": [
        "Gongfan Fang",
        "Xinyin Ma",
        "Xinchao Wang"
      ],
      "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Thinkless æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨ç†å¤§è¯­è¨€æ¨¡å‹ (Reasoning Language Models) åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶è¿‡åº¦ä½¿ç”¨é•¿é“¾æ€ç»´ (Chain-of-Thought) å¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚Thinkless é‡‡ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) èŒƒå¼ï¼Œé€šè¿‡å¼•å…¥ <short> å’Œ <think> ä¸¤ä¸ªæ§åˆ¶ä»¤ç‰Œï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œè‡ªèº«èƒ½åŠ›è‡ªé€‚åº”åœ°é€‰æ‹©æ¨ç†æ¨¡å¼ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†è§£è€¦ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Decoupled Group Relative Policy Optimization, DeGRPO) ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºæ§åˆ¶ä»¤ç‰ŒæŸå¤±å’Œå“åº”å‡†ç¡®æ€§æŸå¤±ï¼Œä»è€Œç¨³å®šäº†è®­ç»ƒå¹¶æœ‰æ•ˆé˜²æ­¢äº†æ¨¡å‹å´©æºƒã€‚åœ¨ Minerva Algebraã€MATH-500 å’Œ GSM8K ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkless åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼ŒæˆåŠŸå°†é•¿é“¾æ€ç»´çš„ä½¿ç”¨é¢‘ç‡é™ä½äº† 50% è‡³ 90%ã€‚è¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨ç†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ï¼Œä¸ºå®ç°æ›´æ™ºèƒ½ã€æ›´ç»æµçš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13379v2",
      "published_date": "2025-05-19 17:24:16 UTC",
      "updated_date": "2025-06-26 14:06:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:41:25.281065+00:00"
    },
    {
      "arxiv_id": "2505.13372v1",
      "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning",
      "title_zh": "åˆ©ç”¨ç¬¦å·å¯å‘å¼é€šè¿‡å¼ºåŒ–å­¦ä¹ åˆæˆç‰¹å®šé¢†åŸŸæ—¶åºè§„åˆ’å¼•å¯¼",
      "authors": [
        "Irene Brugnara",
        "Alessandro Valentini",
        "Andrea Micheli"
      ],
      "abstract": "Recent work investigated the use of Reinforcement Learning (RL) for the synthesis of heuristic guidance to improve the performance of temporal planners when a domain is fixed and a set of training problems (not plans) is given. The idea is to extract a heuristic from the value function of a particular (possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning framework that focuses on exploiting the information provided by symbolic heuristics during both the RL and planning phases. First, we formalize different reward schemata for the synthesis and use symbolic heuristics to mitigate the problems caused by the truncation of episodes needed to deal with the potentially infinite MDP. Second, we propose learning a residual of an existing symbolic heuristic, which is a \"correction\" of the heuristic value, instead of eagerly learning the whole heuristic from scratch. Finally, we use the learned heuristic in combination with a symbolic heuristic using a multiple-queue planning approach to balance systematic search with imperfect learned information. We experimentally compare all the approaches, highlighting their strengths and weaknesses and significantly advancing the state of the art for this planning and learning schema.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸ºç‰¹å®šé¢†åŸŸçš„æ—¶åºè§„åˆ’(Temporal Planning)åˆæˆå¯å‘å¼æŒ‡å¯¼ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å­¦ä¹ ä¸è§„åˆ’æ¡†æ¶ï¼Œé‡ç‚¹åœ¨äºåœ¨å¼ºåŒ–å­¦ä¹ å’Œè§„åˆ’é˜¶æ®µå……åˆ†åˆ©ç”¨ç¬¦å·å¯å‘å¼(Symbolic Heuristics)æä¾›çš„ä¿¡æ¯ã€‚ç ”ç©¶è€…é¦–å…ˆå½¢å¼åŒ–äº†ä¸åŒçš„å¥–åŠ±æ¨¡å¼ï¼Œåˆ©ç”¨ç¬¦å·å¯å‘å¼ç¼“è§£äº†å¤„ç†æ— é™çŠ¶æ€é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)æ—¶å› å‰§é›†æˆªæ–­å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å…¶æ¬¡ï¼Œè¯¥æ–¹æ³•ä¸ä»é›¶å¼€å§‹å­¦ä¹ å®Œæ•´çš„å¯å‘å¼å‡½æ•°ï¼Œè€Œæ˜¯å­¦ä¹ ç°æœ‰ç¬¦å·å¯å‘å¼çš„æ®‹å·®(Residual)ï¼Œå³å¯¹å·²æœ‰å¯å‘å¼å€¼è¿›è¡Œä¿®æ­£ã€‚æœ€åï¼Œé€šè¿‡å¤šé˜Ÿåˆ—è§„åˆ’(Multiple-queue Planning)æ–¹æ³•å°†å­¦ä¹ åˆ°çš„å¯å‘å¼ä¸ç¬¦å·å¯å‘å¼ç»“åˆï¼Œä»è€Œåœ¨ç³»ç»ŸåŒ–æœç´¢ä¸ä¸å®Œç¾çš„å­¦ä¹ ä¿¡æ¯ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒå¯¹æ¯”è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ­¤ç±»è§„åˆ’ä¸å­¦ä¹ æ¨¡å¼çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶æ·±å…¥åˆ†æäº†ä¸åŒæ–¹æ³•çš„ä¼˜åŠ£åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13372v1",
      "published_date": "2025-05-19 17:19:13 UTC",
      "updated_date": "2025-05-19 17:19:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:40:55.973214+00:00"
    },
    {
      "arxiv_id": "2505.13358v3",
      "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling",
      "title_zh": "åŸºäº Koopman å»ºæ¨¡çš„æ‰©æ•£æ¨¡å‹å•æ­¥ç¦»çº¿è’¸é¦",
      "authors": [
        "Nimrod Berman",
        "Ilan Naiman",
        "Moshe Eliasof",
        "Hedi Zisling",
        "Omri Azencot"
      ],
      "abstract": "Diffusion-based generative models have demonstrated exceptional performance, yet their iterative sampling procedures remain computationally expensive. A prominent strategy to mitigate this cost is distillation, with offline distillation offering particular advantages in terms of efficiency, modularity, and flexibility. In this work, we identify two key observations that motivate a principled distillation framework: (1) while diffusion models have been viewed through the lens of dynamical systems theory, powerful and underexplored tools can be further leveraged; and (2) diffusion models inherently impose structured, semantically coherent trajectories in latent space. Building on these observations, we introduce the Koopman Distillation Model (KDM), a novel offline distillation approach grounded in Koopman theory - a classical framework for representing nonlinear dynamics linearly in a transformed space. KDM encodes noisy inputs into an embedded space where a learned linear operator propagates them forward, followed by a decoder that reconstructs clean samples. This enables single-step generation while preserving semantic fidelity. We provide theoretical justification for our approach: (1) under mild assumptions, the learned diffusion dynamics admit a finite-dimensional Koopman representation; and (2) proximity in the Koopman latent space correlates with semantic similarity in the generated outputs, allowing for effective trajectory alignment. KDM achieves highly competitive performance across standard offline distillation benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£ç”Ÿæˆæ¨¡å‹(Diffusion models)è¿­ä»£é‡‡æ ·è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†åº“æ™®æ›¼è’¸é¦æ¨¡å‹(Koopman Distillation Model, KDM)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåº“æ™®æ›¼ç†è®º(Koopman theory)çš„åˆ›æ–°ç¦»çº¿è’¸é¦(Offline distillation)æ–¹æ³•ã€‚KDMå°†æ‰©æ•£æ¨¡å‹çš„éçº¿æ€§åŠ¨åŠ›å­¦è½¬åŒ–ä¸ºå˜æ¢ç©ºé—´ä¸­çš„çº¿æ€§è¡¨ç¤ºï¼Œé€šè¿‡å°†å™ªå£°è¾“å…¥ç¼–ç è‡³åµŒå…¥ç©ºé—´å¹¶åˆ©ç”¨å­¦ä¹ åˆ°çš„çº¿æ€§ç®—å­è¿›è¡Œä¼ æ’­ï¼Œå®ç°äº†ä¿æŒè¯­ä¹‰å¿ å®åº¦çš„å•æ­¥ç”Ÿæˆ(One-step generation)ã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œåœ¨ä¸€å®šæ¡ä»¶ä¸‹æ‰©æ•£åŠ¨åŠ›å­¦å…·å¤‡æœ‰é™ç»´çš„åº“æ™®æ›¼è¡¨ç¤ºï¼Œä¸”å…¶æ½œç©ºé—´(Latent space)çš„æ¥è¿‘åº¦ä¸è¾“å‡ºçš„è¯­ä¹‰ç›¸ä¼¼æ€§å‘ˆæ­£ç›¸å…³ï¼Œä»è€Œæœ‰æ•ˆå®ç°äº†è½¨è¿¹å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKDMåœ¨æ ‡å‡†ç¦»çº¿è’¸é¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºå®ç°é«˜æ•ˆã€æ¨¡å—åŒ–ä¸”å…·æœ‰è¯­ä¹‰ä¿çœŸåº¦çš„æ¨¡å‹å‹ç¼©æä¾›äº†æ–°çš„ç†è®ºæ¡†æ¶ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13358v3",
      "published_date": "2025-05-19 16:59:47 UTC",
      "updated_date": "2025-10-23 17:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:41:13.809464+00:00"
    },
    {
      "arxiv_id": "2505.13355v2",
      "title": "Survey: Multi-Armed Bandits Meet Large Language Models",
      "title_zh": "ç»¼è¿°ï¼šå¤šè‡‚è€è™æœºé‡è§å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Djallel Bouneffouf",
        "Raphael Feraud"
      ],
      "abstract": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°æ¢è®¨äº† Multi-Armed Bandits (MAB) ç®—æ³•ä¸ Large Language Models (LLMs) ä¹‹é—´çš„ååŒæ½œåŠ›ï¼Œæ·±å…¥åˆ†æäº†ä¸¤è€…åœ¨å†³ç­–åˆ¶å®šä¸è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„äº’è¡¥æ€§ã€‚æ–‡ç« é¦–å…ˆé˜è¿°äº†å¦‚ä½•åˆ©ç”¨ MAB ç®—æ³•ä¼˜åŒ– LLMs çš„ fine-tuningã€prompt engineering ä»¥åŠè‡ªé€‚åº”å“åº”ç”Ÿæˆï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨å¹³è¡¡æ¢ç´¢ (exploration) ä¸åˆ©ç”¨ (exploitation) æ–¹é¢çš„èƒ½åŠ›ã€‚éšåï¼Œç ”ç©¶æ¢è®¨äº† LLMs å¦‚ä½•é€šè¿‡å…ˆè¿›çš„ä¸Šä¸‹æ–‡ç†è§£ã€åŠ¨æ€é€‚åº”å’ŒåŸºäºè‡ªç„¶è¯­è¨€æ¨ç†çš„ç­–ç•¥é€‰æ‹©æ¥å¢å¼º MAB ç®—æ³•çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶çš„å…¨é¢å›é¡¾å¹¶è¯†åˆ«å…³é”®æŒ‘æˆ˜ï¼Œè¯¥ç»¼è¿°æ—¨åœ¨å¼¥åˆ MAB ä¸ LLMs ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„è·¨å­¦ç§‘ç ”ç©¶å’Œåˆ›æ–°åº”ç”¨å¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13355v2",
      "published_date": "2025-05-19 16:57:57 UTC",
      "updated_date": "2025-09-30 03:33:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:41:20.981425+00:00"
    },
    {
      "arxiv_id": "2505.13346v3",
      "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization",
      "title_zh": "J4Rï¼šåŸºäºç­‰ä»·åˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å­¦ä¹ è¯„åˆ¤",
      "authors": [
        "Austin Xu",
        "Yilun Zhou",
        "Xuan-Phi Nguyen",
        "Caiming Xiong",
        "Shafiq Joty"
      ],
      "abstract": "To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LLM-as-judge æ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹é¢†åŸŸè¡¨ç°æ¬ ä½³ä»¥åŠå­˜åœ¨ä½ç½®åå·® (positional biases) çš„é—®é¢˜ï¼Œæå‡ºäº† J4R åˆ¤åˆ«æ¨¡å‹ã€‚ä½œè€…é€šè¿‡å¼ºåŒ–å­¦ä¹  (RL) ä¼˜åŒ–åˆ¤åˆ«æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºç­‰æ•ˆåˆå§‹çŠ¶æ€ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Equivalent Initial State Group Relative Policy Optimization, EIS-GRPO) çš„æ–°ç®—æ³•ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨å¤æ‚è¯„ä¼°åœºæ™¯ä¸‹çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† ReasoningJudgeBench åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å¤šæ ·åŒ–æ¨ç†è®¾ç½®ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…æœ‰ 7B å‚æ•°çš„ J4R æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº† GPT-4oï¼Œåœ¨ ReasoningJudgeBench ä¸Šæ¯”æ¬¡ä¼˜çš„å°å‹åˆ¤åˆ«æ¨¡å‹é«˜å‡º 9%ï¼Œè¾¾åˆ°äº†ç”šè‡³è¶…è¿‡äº†æ›´å¤§è§„æ¨¡ç»è¿‡ GRPO è®­ç»ƒçš„æ¨¡å‹çš„è¯„ä¼°æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages, 4 figures, 6 tables. Updated with code and benchmark",
      "pdf_url": "https://arxiv.org/pdf/2505.13346v3",
      "published_date": "2025-05-19 16:50:35 UTC",
      "updated_date": "2025-06-18 16:58:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:41:34.104925+00:00"
    },
    {
      "arxiv_id": "2505.13344v2",
      "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers",
      "title_zh": "RoPECraftï¼šåŸºäºæ‰©æ•£ Transformer ä¸­è½¨è¿¹å¼•å¯¼ RoPE ä¼˜åŒ–çš„å…è®­ç»ƒåŠ¨ä½œè¿ç§»",
      "authors": [
        "Ahmet Berke Gokmen",
        "Yigit Ekin",
        "Bahri Batuhan Bilecen",
        "Aysegul Dundar"
      ],
      "abstract": "We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RoPECraftï¼Œä¸€ç§é’ˆå¯¹ Diffusion Transformers çš„å…è®­ç»ƒè§†é¢‘åŠ¨ä½œè¿ç§»(Motion Transfer)æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯é€šè¿‡ç›´æ¥ä¿®æ”¹æ—‹è½¬ä½ç½®åµŒå…¥(Rotary Positional Embeddings, RoPE)æ¥å®ç°è¿åŠ¨æ§åˆ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»å‚è€ƒè§†é¢‘ä¸­æå–å¯†é›†å…‰æµ(Dense Optical Flow)ï¼Œå¹¶åˆ©ç”¨è¿åŠ¨åç§»é‡å¯¹ RoPE çš„å¤æŒ‡æ•°å¼ é‡è¿›è¡Œæ‰­æ›²ï¼Œå°†è¿åŠ¨ä¿¡æ¯æœ‰æ•ˆç¼–ç è‡³ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚åœ¨å»å™ªæ—¶é—´æ­¥ä¸­ï¼Œç ”ç©¶åˆ©ç”¨æµåŒ¹é…(Flow-matching)ç›®æ ‡è¿›è¡Œè½¨è¿¹å¯¹é½ï¼Œé€šè¿‡é¢„æµ‹é€Ÿåº¦ä¸ç›®æ ‡é€Ÿåº¦çš„åŒ¹é…è¿›ä¸€æ­¥ä¼˜åŒ–åµŒå…¥å‘é‡ã€‚ä¸ºäº†ä¿æŒè¾“å‡ºä¸æ–‡æœ¬æç¤ºçš„ä¸€è‡´æ€§å¹¶æŠ‘åˆ¶é«˜é¢‘ä¼ªå½±ï¼ŒRoPECraft è¿˜å¼•å…¥äº†åŸºäºå‚è€ƒè§†é¢‘å‚…é‡Œå¶å˜æ¢(Fourier Transform)ç›¸ä½åˆ†é‡çš„æ­£åˆ™é¡¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoPECraft åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å®šæ€§å’Œå®šé‡è¡¨ç°å‡ä¼˜äºè¿‘æœŸå‘å¸ƒçš„åŒç±»å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "https://berkegokmen1.github.io/RoPECraft/",
      "pdf_url": "https://arxiv.org/pdf/2505.13344v2",
      "published_date": "2025-05-19 16:50:26 UTC",
      "updated_date": "2025-11-24 21:39:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:05.465750+00:00"
    },
    {
      "arxiv_id": "2505.13339v1",
      "title": "OPA-Pack: Object-Property-Aware Robotic Bin Packing",
      "title_zh": "OPA-Packï¼šç‰©ä½“å±æ€§æ„ŸçŸ¥çš„æœºå™¨äººè£…ç®±",
      "authors": [
        "Jia-Hui Pan",
        "Yeok Tatt Cheah",
        "Zhengzhe Liu",
        "Ka-Hei Hui",
        "Xiaojie Gao",
        "Pheng-Ann Heng",
        "Yun-Hui Liu",
        "Chi-Wing Fu"
      ],
      "abstract": "Robotic bin packing aids in a wide range of real-world scenarios such as e-commerce and warehouses. Yet, existing works focus mainly on considering the shape of objects to optimize packing compactness and neglect object properties such as fragility, edibility, and chemistry that humans typically consider when packing objects. This paper presents OPA-Pack (Object-Property-Aware Packing framework), the first framework that equips the robot with object property considerations in planning the object packing. Technical-wise, we develop a novel object property recognition scheme with retrieval-augmented generation and chain-of-thought reasoning, and build a dataset with object property annotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to jointly separate incompatible object pairs and reduce pressure on fragile objects, while compacting the packing. Further, OPA-Net consists of a property embedding layer to encode the property of candidate objects to be packed, together with a fragility heightmap and an avoidance heightmap to keep track of the packed objects. Then, we design a reward function and adopt a deep Q-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack greatly improves the accuracy of separating incompatible object pairs (from 52% to 95%) and largely reduces pressure on fragile objects (by 29.4%), while maintaining good packing compactness. Besides, we demonstrate the effectiveness of OPA-Pack on a real packing platform, showcasing its practicality in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„æœºå™¨äººè£…ç®±ç®—æ³•ä¸»è¦å…³æ³¨ç‰©ä½“å½¢çŠ¶ä»¥ä¼˜åŒ–ç´§å‡‘åº¦ï¼Œè€Œå¿½ç•¥äº†æ˜“ç¢æ€§(Fragility)ã€å¯é£Ÿæ€§(Edibility)å’ŒåŒ–å­¦æ€§è´¨ç­‰äººç±»è£…ç®±æ—¶å¸¸è€ƒè™‘çš„ç‰©ä½“å±æ€§(Object Properties)ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†OPA-Packï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨æœºå™¨äººè£…ç®±è§„åˆ’ä¸­å¼•å…¥ç‰©ä½“å±æ€§è€ƒé‡çš„æ¡†æ¶ã€‚æŠ€æœ¯å±‚é¢ï¼Œè¯¥ç ”ç©¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œé“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†å¼€å‘äº†ç‰©ä½“å±æ€§è¯†åˆ«æ–¹æ¡ˆï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«1032ä¸ªæ—¥å¸¸ç‰©ä½“çš„æ ‡æ³¨æ•°æ®é›†ã€‚å…¶æ ¸å¿ƒç»„ä»¶OPA-Netåˆ©ç”¨å±æ€§åµŒå…¥å±‚(Property Embedding Layer)å’Œé«˜åº¦å›¾æŠ€æœ¯ï¼Œåœ¨ä¿è¯ç´§å‡‘æ€§çš„åŒæ—¶å®ç°äº†ä¸ç›¸å®¹ç‰©ä½“çš„åˆ†ç¦»ä¸æ˜“ç¢ç‰©ä½“çš„å‡å‹ã€‚ç ”ç©¶é‡‡ç”¨æ·±åº¦Qå­¦ä¹ (Deep Q-learning)æ–¹æ¡ˆè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå®éªŒè¡¨æ˜OPA-Packå°†ä¸ç›¸å®¹ç‰©ä½“çš„åˆ†ç¦»å‡†ç¡®ç‡ä»52%å¤§å¹…æå‡è‡³95%ï¼Œå¹¶ä½¿æ˜“ç¢ç‰©ä½“å—å‹é™ä½äº†29.4%ã€‚è¯¥æ¡†æ¶åœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šçš„æˆåŠŸéƒ¨ç½²è¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨ç°å®ç‰©æµåœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13339v1",
      "published_date": "2025-05-19 16:48:14 UTC",
      "updated_date": "2025-05-19 16:48:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:12.131023+00:00"
    },
    {
      "arxiv_id": "2505.13338v2",
      "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation",
      "title_zh": "é¢å‘å¤šæ¨¡æ€è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„è¯­å¢ƒåŒ–å‰¯è¯­è¨€æ•°æ®æ„å»ºï¼šæ•°æ®ç²¾ç®€ä¸è¯­éŸ³é—®ç­”ç”Ÿæˆ",
      "authors": [
        "Qiongqiong Wang",
        "Hardik B. Sailor",
        "Tianchi Liu",
        "Ai Ti Aw"
      ],
      "abstract": "Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.",
      "tldr_zh": "ç°æœ‰çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech-LLMs)åœ¨ç»“åˆä¸Šä¸‹æ–‡æ¨ç†ä¸å‰¯è¯­è¨€(Paralinguistic)ç†è§£æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹åŒæ—¶æ¶µç›–è¿™ä¸¤ä¸ªæ–¹é¢çš„é—®ç­”(QA)æ•°æ®é›†ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»é‡å¤–è¯­éŸ³æ•°æ®(In-the-wild speech data)è‡ªåŠ¨ç”Ÿæˆæ•°æ®é›†çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä¸Šä¸‹æ–‡æ¨ç†ä¸å‰¯è¯­è¨€ä¿¡æ¯æ•´åˆã€‚è¯¥æ¡†æ¶ç”±åŸºäºä¼ªå‰¯è¯­è¨€æ ‡ç­¾çš„æ•°æ®å‹ç¼©(Data Condensation)ä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä¸Šä¸‹æ–‡å‰¯è¯­è¨€é—®ç­”(CPQA)ç”Ÿæˆä¸¤éƒ¨åˆ†ç»„æˆã€‚é€šè¿‡åœ¨Qwen2-Audio-7B-Instructæ¨¡å‹ä¸Šçš„éªŒè¯ï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶ç”Ÿæˆçš„è¯„ä¼°ç»“æœä¸äººå·¥ç”Ÿæˆçš„æ•°æ®é›†å…·æœ‰é«˜åº¦ç›¸å…³æ€§ã€‚ç ”ç©¶ç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å…±æƒ…æ¨ç†(Empathetic reasoning)ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå‡¸æ˜¾äº†å¯¹æ­¤ç±»æ•°æ®é›†å’Œæ›´é²æ£’æ¨¡å‹çš„éœ€æ±‚ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹è¯¥é¢†åŸŸçš„ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥ç ”ç©¶ä¸ºå¼€å‘å…·æœ‰å‰¯è¯­è¨€æ¨ç†èƒ½åŠ›çš„å¼ºå¥Speech-LLMsæä¾›äº†é‡è¦å·¥å…·å’Œæ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Interspeech 2025. [v2]: The dataset has been released, and the link is now updated",
      "pdf_url": "https://arxiv.org/pdf/2505.13338v2",
      "published_date": "2025-05-19 16:47:46 UTC",
      "updated_date": "2025-06-03 04:11:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:11.999804+00:00"
    },
    {
      "arxiv_id": "2505.13329v1",
      "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications",
      "title_zh": "æ°‘ä¸»æ¨èç³»ç»Ÿï¼šè¿ˆå‘æŠ•ç¥¨å»ºè®®åº”ç”¨çš„å¯¹æŠ—é²æ£’æ€§",
      "authors": [
        "FrÃ©dÃ©ric Berdoz",
        "Dustin Brunner",
        "Yann Vonlanthen",
        "Roger Wattenhofer"
      ],
      "abstract": "Voting advice applications (VAAs) help millions of voters understand which political parties or candidates best align with their views. This paper explores the potential risks these applications pose to the democratic process when targeted by adversarial entities. In particular, we expose 11 manipulation strategies and measure their impact using data from Switzerland's primary VAA, Smartvote, collected during the last two national elections. We find that altering application parameters, such as the matching method, can shift a party's recommendation frequency by up to 105%. Cherry-picking questionnaire items can increase party recommendation frequency by over 261%, while subtle changes to parties' or candidates' responses can lead to a 248% increase. To address these vulnerabilities, we propose adversarial robustness properties VAAs should satisfy, introduce empirical metrics for assessing the resilience of various matching methods, and suggest possible avenues for research toward mitigating the effect of manipulation. Our framework is key to ensuring secure and reliable AI-based VAAs poised to emerge in the near future.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æŠ•ç¥¨å»ºè®®åº”ç”¨(Voting Advice Applications, VAAs)åœ¨é¢ä¸´æ•Œå¯¹å®ä½“æ”»å‡»æ—¶å¯¹æ°‘ä¸»è¿›ç¨‹æ„æˆçš„æ½œåœ¨é£é™©ã€‚é€šè¿‡åˆ†æç‘å£«ä¸»è¦VAAå¹³å°Smartvoteåœ¨æœ€è¿‘ä¸¤å±Šå…¨å›½é€‰ä¸¾ä¸­çš„æ•°æ®ï¼Œç ”ç©¶æ­ç¤ºäº†11ç§æ“çºµç­–ç•¥å¹¶é‡åŒ–äº†å…¶å½±å“åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæ”¹å˜åŒ¹é…æ–¹æ³•(Matching method)ç­‰åº”ç”¨å‚æ•°å¯ä½¿æ”¿å…šçš„æ¨èé¢‘ç‡äº§ç”Ÿé«˜è¾¾105%çš„åç§»ï¼Œè€Œé€šè¿‡ç²¾å¿ƒç­›é€‰é—®å·é¡¹ç›®(Cherry-picking)æˆ–å¾®è°ƒå€™é€‰äººå›ç­”ï¼Œæ¨èé¢‘ç‡æœ€é«˜å¯åˆ†åˆ«å¢åŠ 261%å’Œ248%ã€‚é’ˆå¯¹è¿™äº›å®‰å…¨æ¼æ´ï¼Œä½œè€…æå‡ºäº†VAAsåº”å…·å¤‡çš„å¯¹æŠ—ç¨³å¥æ€§(Adversarial Robustness)å±æ€§ï¼Œå¹¶å¼•å…¥äº†ç”¨äºè¯„ä¼°ä¸åŒåŒ¹é…æ–¹æ³•å¼¹æ€§çš„å®è¯æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æ¡†æ¶å¯¹äºç¡®ä¿æœªæ¥åŸºäºäººå·¥æ™ºèƒ½(AI-based)çš„VAAsçš„å®‰å…¨æ€§å’Œå¯é æ€§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶ä¸ºç¼“è§£æ“çºµå½±å“çš„ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "This is the extended version of the paper, accepted at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13329v1",
      "published_date": "2025-05-19 16:38:06 UTC",
      "updated_date": "2025-05-19 16:38:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:24.710132+00:00"
    },
    {
      "arxiv_id": "2505.13324v1",
      "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI",
      "title_zh": "ä»å‡è®¾åˆ°æ´è§ï¼šå› æœæ¨æ–­ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½ä¸­çš„åäº‹å®å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Galit Shmueli",
        "David Martens",
        "Jaewon Yoo",
        "Travis Greene"
      ],
      "abstract": "Counterfactuals play a pivotal role in the two distinct data science fields of causal inference (CI) and explainable artificial intelligence (XAI). While the core idea behind counterfactuals remains the same in both fields--the examination of what would have happened under different circumstances--there are key differences in how they are used and interpreted. We introduce a formal definition that encompasses the multi-faceted concept of the counterfactual in CI and XAI. We then discuss how counterfactuals are used, evaluated, generated, and operationalized in CI vs. XAI, highlighting conceptual and practical differences. By comparing and contrasting the two, we hope to identify opportunities for cross-fertilization across CI and XAI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åäº‹å®(Counterfactuals)åœ¨å› æœæ¨æ–­(Causal Inference, CI)å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI, XAI)è¿™ä¸¤ä¸ªä¸åŒæ•°æ®ç§‘å­¦é¢†åŸŸä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚å°½ç®¡ä¸¤è€…çš„æ ¸å¿ƒé€»è¾‘å‡åœ¨äºæ¢è®¨ä¸åŒæƒ…å½¢ä¸‹çš„å¯èƒ½ç»“æœï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸è§£é‡Šè·¯å¾„ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„æ­£å¼å®šä¹‰ï¼Œæ—¨åœ¨æ¶µç›–CIå’ŒXAIä¸­åäº‹å®æ¦‚å¿µçš„å¤šç»´å±æ€§ã€‚æ–‡ç« è¯¦ç»†å¯¹æ¯”äº†åäº‹å®åœ¨ä¸¤ä¸ªé¢†åŸŸä¸­çš„ç”Ÿæˆã€è¯„ä¼°åŠæ“ä½œåŒ–æµç¨‹ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°é˜è¿°äº†å…¶ä¸­çš„æ¦‚å¿µä¸å®è·µå·®å¼‚ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æ·±å…¥çš„è·¨å­¦ç§‘å¯¹æ¯”ï¼Œæ—¨åœ¨å‘æ˜CIä¸XAIä¹‹é—´äº¤å‰èåˆ(cross-fertilization)çš„æ½œåœ¨æœºé‡ï¼Œä¸ºæœªæ¥ç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13324v1",
      "published_date": "2025-05-19 16:34:36 UTC",
      "updated_date": "2025-05-19 16:34:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:07.525027+00:00"
    },
    {
      "arxiv_id": "2505.13316v1",
      "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates",
      "title_zh": "é¢å‘ä½æ¯”ç‰¹ç‡ç‚¹äº‘å‹ç¼©çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹",
      "authors": [
        "Gabriele Spadaro",
        "Alberto Presta",
        "Jhony H. Giraldo",
        "Marco Grangetto",
        "Wei Hu",
        "Giuseppe Valenzise",
        "Attilio Fiandrotti",
        "Enzo Tartaglione"
      ],
      "abstract": "Efficient compression of low-bit-rate point clouds is critical for bandwidth-constrained applications. However, existing techniques mainly focus on high-fidelity reconstruction, requiring many bits for compression. This paper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture for point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder produces the condition vector for the generation, which is then quantized via a learnable vector quantizer. This configuration allows to achieve a low bitrates while preserving quality. Experiments on ShapeNet and ModelNet40 show improved rate-distortion at low rates compared to standardized and state-of-the-art approaches. We publicly released the code at https://github.com/EIDOSLAB/DDPM-PCC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦å®½å—é™åº”ç”¨ä¸­ä½æ¯”ç‰¹ç‡ç‚¹äº‘å‹ç¼©çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†DDPM-PCCæ¡†æ¶ï¼Œå³ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(Denoising Diffusion Probabilistic Model)çš„ç‚¹äº‘å‹ç¼©æ¶æ„ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨PointNetç¼–ç å™¨ç”Ÿæˆç”Ÿæˆçš„æ¡ä»¶å‘é‡ï¼Œå¹¶é€šè¿‡å¯å­¦ä¹ çš„å‘é‡é‡åŒ–å™¨(vector quantizer)å¯¹å…¶è¿›è¡Œé‡åŒ–ï¼Œä»è€Œåœ¨æä½æ¯”ç‰¹ç‡ä¸‹æœ‰æ•ˆä¿ç•™ç‚¹äº‘è´¨é‡ã€‚å®éªŒåœ¨ShapeNetå’ŒModelNet40æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜DDPM-PCCåœ¨ä½æ¯”ç‰¹ç‡ä¸‹çš„ç‡å¤±çœŸ(rate-distortion)æ€§èƒ½ä¼˜äºç°æœ‰çš„æ ‡å‡†æ–¹æ³•åŠæœ€å…ˆè¿›æŠ€æœ¯ã€‚è¯¥å·¥ä½œä¸ºä½å¸¦å®½ç¯å¢ƒä¸‹çš„ä¸‰ç»´æ•°æ®ä¼ è¾“æä¾›äº†é«˜æ•ˆçš„ç”Ÿæˆå¼å‹ç¼©æ–¹æ¡ˆï¼Œå¹¶å·²å¼€æºç›¸å…³ä»£ç ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 5 figures, accepted at ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13316v1",
      "published_date": "2025-05-19 16:29:12 UTC",
      "updated_date": "2025-05-19 16:29:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:34.247012+00:00"
    },
    {
      "arxiv_id": "2505.13315v2",
      "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation",
      "title_zh": "KHRONOSï¼šä¸€ç§ç”¨äºå¿«é€Ÿã€èµ„æºé«˜æ•ˆç§‘å­¦è®¡ç®—çš„åŸºäºæ ¸çš„ç¥ç»æ¶æ„",
      "authors": [
        "Reza T. Batley",
        "Sourav Saha"
      ],
      "abstract": "Contemporary models of high dimensional physical systems are constrained by the curse of dimensionality and a reliance on dense data. We introduce KHRONOS (Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an AI framework for model based, model free and model inversion tasks. KHRONOS constructs continuously differentiable target fields with a hierarchical composition of per-dimension kernel expansions, which are tensorized into modes and then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation benchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L_2-square errors of 5e-4 down to 6e-11. This represents a greater than 100-fold gain over Kolmogorov Arnold Networks (which itself reports a 100 times improvement on MLPs/PINNs with 100 times fewer parameters) when controlling for the number of parameters. This also represents a 1e6-fold improvement in L_2-square error compared to standard linear FEM at comparable DoFs. Inference complexity is dominated by inner products, yielding sub-millisecond full-field predictions that scale to an arbitrary resolution. For inverse problems, KHRONOS facilitates rapid, iterative level set recovery in only a few forward evaluations, with sub-microsecond per sample latency. KHRONOS's scalability, expressivity, and interpretability open new avenues in constrained edge computing, online control, computer vision, and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KHRONOS (Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³é«˜ç»´ç‰©ç†ç³»ç»Ÿç»´åº¦ç¾éš¾å’Œæ•°æ®ä¾èµ–é—®é¢˜çš„ AI æ¡†æ¶ã€‚KHRONOS é€šè¿‡é€ç»´åº¦æ ¸æ‰©å¼  (per-dimension kernel expansions) çš„å±‚æ¬¡åŒ–ç»„åˆæ„å»ºè¿ç»­å¯å¾®çš„ç›®æ ‡åœºï¼Œå¹¶å°†å…¶å¼ é‡åŒ–ä¸ºæ¨¡å¼ (modes) åè¿›è¡Œå åŠ ã€‚åœ¨ 2D æ³Šæ¾æ–¹ç¨‹ (Poisson equation) åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨ç›¸åŒå‚æ•°è§„æ¨¡ä¸‹å®ç°äº†æ¯” Kolmogorov Arnold Networks (KANs) é«˜å‡º 100 å€ä»¥ä¸Šçš„ç²¾åº¦ï¼Œä¸”æ¯”æ ‡å‡†çº¿æ€§æœ‰é™å…ƒæ–¹æ³• (linear FEM) çš„è¯¯å·®é™ä½äº† 100 ä¸‡å€ã€‚å¾—ç›Šäºä»¥å†…ç§¯ä¸ºä¸»çš„æ¨ç†å¤æ‚åº¦ï¼ŒKHRONOS èƒ½å¤Ÿå®ç°äºšæ¯«ç§’çº§çš„å…¨åœºé¢„æµ‹å¹¶æ”¯æŒä»»æ„åˆ†è¾¨ç‡æ‰©å±•ã€‚æ­¤å¤–ï¼Œåœ¨åé—®é¢˜å¤„ç†ä¸­ï¼Œå®ƒä»…éœ€æå°‘çš„å‰å‘è¯„ä¼°å³å¯å®ç°å¿«é€Ÿçš„æ°´å¹³é›† (level set) æ¢å¤ï¼Œè¡¨ç°å‡ºæä½çš„æ ·æœ¬å»¶è¿Ÿã€‚è¯¥æ¶æ„å…¼å…·å¯æ‰©å±•æ€§ã€è¡¨è¾¾èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºå—é™è¾¹ç¼˜è®¡ç®—ã€åœ¨çº¿æ§åˆ¶åŠè®¡ç®—æœºè§†è§‰ç­‰ç§‘å­¦è®¡ç®—é¢†åŸŸå¼€è¾Ÿäº†é«˜æ•ˆçš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13315v2",
      "published_date": "2025-05-19 16:29:07 UTC",
      "updated_date": "2025-05-26 01:40:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:37.656382+00:00"
    },
    {
      "arxiv_id": "2505.13308v3",
      "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space",
      "title_zh": "Seek in the Darkï¼šåŸºäºæ½œç©ºé—´æµ‹è¯•æ—¶å®ä¾‹çº§ç­–ç•¥æ¢¯åº¦çš„æ¨ç†",
      "authors": [
        "Hengli Li",
        "Chenxi Li",
        "Tong Wu",
        "Xuekai Zhu",
        "Yuxuan Wang",
        "Zhaoxin Yu",
        "Eric Hanchen Jiang",
        "Song-Chun Zhu",
        "Zixia Jia",
        "Ying Nian Wu",
        "Zilong Zheng"
      ],
      "abstract": "Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½(AGI)è¿‡ç¨‹ä¸­é¢ä¸´çš„æ¨ç†èƒ½åŠ›ç“¶é¢ˆï¼Œæå‡ºäº† LatentSeek æ¡†æ¶ã€‚ä¸ä»¥å¾€åœ¨ Token Space è¿›è¡Œä¼˜åŒ–çš„æ–¹æ³•ä¸åŒï¼ŒLatentSeek æå‡ºåœ¨æ½œç©ºé—´(Latent Space)ä¸­åˆ©ç”¨æµ‹è¯•æ—¶å®ä¾‹çº§é€‚é…(Test-Time Instance-level Adaptation, TTIA)æ¥å¢å¼ºæ¨ç†æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç­–ç•¥æ¢¯åº¦(Policy Gradient)ç®—æ³•ï¼Œåœ¨è‡ªç”Ÿæˆå¥–åŠ±ä¿¡å·çš„å¼•å¯¼ä¸‹è¿­ä»£æ›´æ–° Latent Representationsï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æµ‹è¯•æ—¶æ‰©å±•(Test-time Scaling)ã€‚åœ¨ GSM8Kã€MATH-500 å’Œ AIME2024 ç­‰åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentSeek çš„è¡¨ç°ä¸€è‡´ä¼˜äº Chain-of-Thought æç¤ºå’ŒåŸºäºå¾®è°ƒçš„æ–¹æ³•ã€‚ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰æé«˜çš„æ•ˆç‡å’Œè‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œé€šå¸¸åœ¨å°‘é‡è¿­ä»£å†…å³å¯æ”¶æ•›ï¼Œä¸ºå¢å¼º LLM æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ— éœ€å‚æ•°æ›´æ–°çš„è½»é‡çº§ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13308v3",
      "published_date": "2025-05-19 16:26:02 UTC",
      "updated_date": "2026-01-19 14:21:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:42.737116+00:00"
    },
    {
      "arxiv_id": "2505.13307v1",
      "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning",
      "title_zh": "RBF++ï¼šé¢å‘é“¾å¼æ€ç»´æ¨ç†çš„å¯è¡¡é‡ä¸ä¸å¯è¡¡é‡èƒ½åŠ›æ¨ç†è¾¹ç•Œé‡åŒ–ä¸ä¼˜åŒ–",
      "authors": [
        "Qiguang Chen",
        "Libo Qin",
        "Jinhao Liu",
        "Yue Liao",
        "Jiaqi Wang",
        "Jingxuan Zhou",
        "Wanxiang Che"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at https://github.com/LightChen233/reasoning-boundary.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Reasoning Boundary Framework++ (RBF++)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ä¸­ç¼ºä¹å®šé‡è¯„ä¼°æŒ‡æ ‡ä»¥åŠéš¾ä»¥è¡¡é‡å¤šæ¨¡æ€æ„ŸçŸ¥ç­‰ä¸å¯æµ‹é‡èƒ½åŠ›è¾¹ç•Œçš„é—®é¢˜ã€‚ç ”ç©¶è€…å°†æ¨ç†è¾¹ç•Œ(Reasoning Boundary, RB)å®šä¹‰ä¸º CoT æ€§èƒ½çš„æœ€å¤§æé™ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹ RB çš„ç»„åˆå®šå¾‹ï¼Œä¸ºå„ç§ CoT ä»»åŠ¡æä¾›äº†å®šé‡åˆ†ææ‰‹æ®µå’Œä¼˜åŒ–æŒ‡å¯¼ã€‚é’ˆå¯¹å¤šæ¨¡æ€æ„ŸçŸ¥ç­‰ä¸å¯æµ‹é‡èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¸¸æ•°å‡è®¾(constant assumption)å’Œæ¨ç†è¾¹ç•Œåˆ’åˆ†æœºåˆ¶(reasoning boundary division mechanism)ï¼Œå°†ä¸å¯æµ‹é‡çš„ RB åˆ’åˆ†ä¸ºå­è¾¹ç•Œï¼Œä»è€Œå®ç°å¯¹é¢†åŸŸçŸ¥è¯†å’Œå¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›çš„é‡åŒ–ä¸ä¼˜åŒ–ã€‚é€šè¿‡å¯¹ 38 ä¸ªæ¨¡å‹åœ¨ 13 é¡¹ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ¡†æ¶åœ¨è·¨æ¨¡æ€åœºæ™¯ä¸‹çš„å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜è¯„ä¼°äº† 10 ç§ CoT ç­–ç•¥ï¼Œä»ä¼˜åŒ–å’Œè¡°å‡ä¸¤ä¸ªäº’è¡¥è§†è§’æä¾›äº†æ·±å…¥è§è§£ï¼Œæ‰©å±•äº†è¡¡é‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¾¹ç•Œçš„è¯„ä¼°åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Manuscript",
      "pdf_url": "https://arxiv.org/pdf/2505.13307v1",
      "published_date": "2025-05-19 16:25:55 UTC",
      "updated_date": "2025-05-19 16:25:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:31.973274+00:00"
    },
    {
      "arxiv_id": "2505.13292v1",
      "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs",
      "title_zh": "è·¨äº‘æ•°æ®éšç§ä¿æŠ¤ï¼šèåˆè”é‚¦å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹çš„äººå·¥æ™ºèƒ½ç³»ç»ŸååŒæœºåˆ¶ä¼˜åŒ–",
      "authors": [
        "Huaiying Luo",
        "Cheng Ji"
      ],
      "abstract": "In the age of cloud computing, data privacy protection has become a major challenge, especially when sharing sensitive data across cloud environments. However, how to optimize collaboration across cloud environments remains an unresolved problem. In this paper, we combine federated learning with large-scale language models to optimize the collaborative mechanism of AI systems. Based on the existing federated learning framework, we introduce a cross-cloud architecture in which federated learning works by aggregating model updates from decentralized nodes without exposing the original data. At the same time, combined with large-scale language models, its powerful context and semantic understanding capabilities are used to improve model training efficiency and decision-making ability. We've further innovated by introducing a secure communication layer to ensure the privacy and integrity of model updates and training data. The model enables continuous model adaptation and fine-tuning across different cloud environments while protecting sensitive data. Experimental results show that the proposed method is significantly better than the traditional federated learning model in terms of accuracy, convergence speed and data privacy protection.",
      "tldr_zh": "åœ¨äº‘è®¡ç®—æ—¶ä»£ï¼Œè·¨äº‘ç¯å¢ƒä¸‹çš„æ•æ„Ÿæ•°æ®éšç§ä¿æŠ¤å’Œåä½œæœºåˆ¶ä¼˜åŒ–æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ•´åˆè”é‚¦å­¦ä¹ (Federated Learning)ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è·¨äº‘æ•°æ®éšç§ä¿æŠ¤æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–AIç³»ç»Ÿçš„åä½œæœºåˆ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è·¨äº‘æ¶æ„ï¼Œé€šè¿‡åœ¨ä¸æš´éœ²åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹èšåˆå»ä¸­å¿ƒåŒ–èŠ‚ç‚¹çš„æ¨¡å‹æ›´æ–°ï¼Œå¹¶åˆ©ç”¨LLMså¼ºå¤§çš„ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›æ¥æå‡æ¨¡å‹è®­ç»ƒæ•ˆç‡ä¸å†³ç­–æ°´å¹³ã€‚ç ”ç©¶è¿˜åˆ›æ–°æ€§åœ°å¼•å…¥äº†ä¸€ä¸ªå®‰å…¨é€šä¿¡å±‚(Secure Communication Layer)ï¼Œä»¥ç¡®ä¿è·¨ä¸åŒäº‘ç¯å¢ƒè¿›è¡Œæ¨¡å‹æ›´æ–°å’Œè®­ç»ƒæ•°æ®æ—¶çš„éšç§æ€§ä¸å®Œæ•´æ€§ã€‚è¯¥æ¨¡å‹æ”¯æŒåœ¨ä¿æŠ¤æ•æ„Ÿæ•°æ®çš„å‰æä¸‹ï¼Œå®ç°è·¨äº‘ç¯å¢ƒçš„æŒç»­æ¨¡å‹é€‚é…ä¸å¾®è°ƒ(Fine-tuning)ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ã€æ”¶æ•›é€Ÿåº¦ä»¥åŠæ•°æ®éšç§ä¿æŠ¤æ•ˆèƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ (Federated Learning)æ¨¡å‹ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by 2025 IEEE 7th International Conference on Communications, Information System and Computer Engineering",
      "pdf_url": "https://arxiv.org/pdf/2505.13292v1",
      "published_date": "2025-05-19 16:14:27 UTC",
      "updated_date": "2025-05-19 16:14:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:42:38.734720+00:00"
    },
    {
      "arxiv_id": "2505.13291v1",
      "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents",
      "title_zh": "TimeSeriesGymï¼šé¢å‘ï¼ˆæ—¶é—´åºåˆ—ï¼‰æœºå™¨å­¦ä¹ å·¥ç¨‹æ™ºèƒ½ä½“çš„å¯æ‰©å±•åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yifu Cai",
        "Xinyu Li",
        "Mononito Goswami",
        "MichaÅ‚ WiliÅ„ski",
        "Gus Welter",
        "Artur Dubrawski"
      ],
      "abstract": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating Artificial Intelligence (AI) agents on time series machine learning engineering challenges. Existing benchmarks lack scalability, focus narrowly on model building in well-defined settings, and evaluate only a limited set of research artifacts (e.g., CSV submission files). To make AI agent benchmarking more relevant to the practice of machine learning engineering, our framework scales along two critical dimensions. First, recognizing that effective ML engineering requires a range of diverse skills, TimeSeriesGym incorporates challenges from diverse sources spanning multiple domains and tasks. We design challenges to evaluate both isolated capabilities (including data handling, understanding research repositories, and code translation) and their combinations, and rather than addressing each challenge independently, we develop tools that support designing multiple challenges at scale. Second, we implement evaluation mechanisms for multiple research artifacts, including submission files, code, and models, using both precise numeric measures and more flexible LLM-based evaluation approaches. This dual strategy balances objective assessment with contextual judgment. Although our initial focus is on time series applications, our framework can be readily extended to other data modalities, broadly enhancing the comprehensiveness and practical utility of agentic AI evaluation. We open-source our benchmarking framework to facilitate future research on the ML engineering capabilities of AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† TimeSeriesGymï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ (AI agents) åœ¨æ—¶é—´åºåˆ—æœºå™¨å­¦ä¹ å·¥ç¨‹ (machine learning engineering) æŒ‘æˆ˜ä¸­è¡¨ç°çš„å¯æ‰©å±•åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹å¯æ‰©å±•æ€§ã€ä¾§é‡é¢†åŸŸçª„ä¸”ä»…èƒ½è¯„ä¼°æœ‰é™ç ”ç©¶äº§ç‰©çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªå…³é”®ç»´åº¦ä¸Šè¿›è¡Œäº†æ‰©å±•ã€‚é¦–å…ˆï¼ŒTimeSeriesGym æ•´åˆäº†è·¨å¤šä¸ªé¢†åŸŸçš„å¤šæ ·åŒ–ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨æ•°æ®å¤„ç† (data handling)ã€ç†è§£ç ”ç©¶ä»£ç åº“å’Œä»£ç ç¿»è¯‘ç­‰æ–¹é¢çš„ç‹¬ç«‹åŠç»¼åˆèƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†æ”¯æŒå¤§è§„æ¨¡è®¾è®¡æŒ‘æˆ˜çš„å·¥å…·ã€‚å…¶æ¬¡ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯¹æäº¤æ–‡ä»¶ã€ä»£ç å’Œæ¨¡å‹ç­‰å¤šç§ç ”ç©¶äº§ç‰©çš„è¯„ä¼°æœºåˆ¶ï¼Œå¹¶ç»“åˆäº†ç²¾ç¡®çš„æ•°å€¼æŒ‡æ ‡ä¸çµæ´»çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM-based) çš„è¯„ä¼°æ–¹æ³•ã€‚è™½ç„¶åˆå§‹ç‰ˆæœ¬ä¾§é‡äºæ—¶é—´åºåˆ—åº”ç”¨ï¼Œä½†è¯¥æ¡†æ¶å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ¨å¹¿è‡³å…¶ä»–æ•°æ®æ¨¡æ€ï¼Œä»è€Œå…¨é¢æå‡æ™ºèƒ½ä½“ AI è¯„ä¼°çš„ç»¼åˆæ€§å’Œå®ç”¨æ€§ã€‚ç›®å‰ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ¡†æ¶å·²å¼€æºï¼Œæ—¨åœ¨æ¨åŠ¨ AI æ™ºèƒ½ä½“åœ¨æœºå™¨å­¦ä¹ å·¥ç¨‹èƒ½åŠ›æ–¹é¢çš„æœªæ¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Open source code available at https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL, MG and MW contributed equally, and should be considered joint first authors",
      "pdf_url": "https://arxiv.org/pdf/2505.13291v1",
      "published_date": "2025-05-19 16:11:23 UTC",
      "updated_date": "2025-05-19 16:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:09.803278+00:00"
    },
    {
      "arxiv_id": "2505.14723v1",
      "title": "QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding",
      "title_zh": "QUADSï¼šé¢å‘é«˜æ•ˆè¯­éŸ³è¯­è¨€ç†è§£çš„é‡åŒ–è’¸é¦æ¡†æ¶",
      "authors": [
        "Subrata Biswas",
        "Mohammad Nur Hossain Khan",
        "Bashima Islam"
      ],
      "abstract": "Spoken Language Understanding (SLU) systems must balance performance and efficiency, particularly in resource-constrained environments. Existing methods apply distillation and quantization separately, leading to suboptimal compression as distillation ignores quantization constraints. We propose QUADS, a unified framework that optimizes both through multi-stage training with a pre-tuned model, enhancing adaptability to low-bit regimes while maintaining accuracy. QUADS achieves 71.13\\% accuracy on SLURP and 99.20\\% on FSC, with only minor degradations of up to 5.56\\% compared to state-of-the-art models. Additionally, it reduces computational complexity by 60--73$\\times$ (GMACs) and model size by 83--700$\\times$, demonstrating strong robustness under extreme quantization. These results establish QUADS as a highly efficient solution for real-world, resource-constrained SLU applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QUADSï¼Œä¸€ä¸ªé’ˆå¯¹é«˜æ•ˆè¯­éŸ³è¯­è¨€ç†è§£ (Speech Language Understanding, SLU) çš„é‡åŒ–è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­è’¸é¦ä¸é‡åŒ–åˆ†ç¦»å¯¼è‡´çš„å‹ç¼©æ•ˆæœä¸ä½³é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é¢„è°ƒä¼˜æ¨¡å‹çš„å¤šé˜¶æ®µè®­ç»ƒï¼Œå°†è’¸é¦ä¸é‡åŒ–è¿‡ç¨‹è¿›è¡Œç»Ÿä¸€ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä½æ¯”ç‰¹ (low-bit) ç¯å¢ƒä¸‹çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQUADS åœ¨ SLURP æ•°æ®é›†ä¸Šè¾¾åˆ° 71.13% çš„å‡†ç¡®ç‡ï¼Œåœ¨ FSC æ•°æ®é›†ä¸Šè¾¾åˆ° 99.20%ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”æ€§èƒ½ä¸‹é™æå°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å°†è®¡ç®—å¤æ‚åº¦ (GMACs) é™ä½äº† 60-73 å€ï¼Œå¹¶å°†æ¨¡å‹ä½“ç§¯ç¼©å°äº† 83-700 å€ï¼Œè¯æ˜äº†å…¶åœ¨æç«¯é‡åŒ–æ¡ä»¶ä¸‹çš„å¼ºå¤§é²æ£’æ€§ã€‚è¿™ä¸€æˆæœä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„çœŸå®åœºæ™¯ SLU åº”ç”¨æä¾›äº†ä¸€ç§æå…·æ•ˆç‡çš„å¹³è¡¡æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.14723v1",
      "published_date": "2025-05-19 16:09:51 UTC",
      "updated_date": "2025-05-19 16:09:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:05.619889+00:00"
    },
    {
      "arxiv_id": "2505.13287v1",
      "title": "Level Generation with Quantum Reservoir Computing",
      "title_zh": "åŸºäºé‡å­å‚¨å¤‡æ± è®¡ç®—çš„å…³å¡ç”Ÿæˆ",
      "authors": [
        "JoÃ£o S. Ferreira",
        "Pierre Fromholz",
        "Hari Shaji",
        "James R. Wootton"
      ],
      "abstract": "Reservoir computing is a form of machine learning particularly suited for time series analysis, including forecasting predictions. We take an implementation of \\emph{quantum} reservoir computing that was initially designed to generate variants of musical scores and adapt it to create levels of Super Mario Bros. Motivated by our analysis of these levels, we develop a new Roblox \\textit{obby} where the courses can be generated in real time on superconducting qubit hardware, and investigate some of the constraints placed by such real-time generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Quantum Reservoir Computing è¿›è¡Œæ¸¸æˆå…³å¡ç”Ÿæˆçš„åº”ç”¨æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜æ”¹è¿›äº†ä¸€ç§æœ€åˆç”¨äºç”Ÿæˆä¹è°±å˜ä½“çš„ Quantum Reservoir Computing å®ç°ï¼Œå¹¶å°†å…¶æˆåŠŸåº”ç”¨äº Super Mario Bros çš„å…³å¡åˆ›ä½œã€‚åŸºäºå¯¹è¿™äº›å…³å¡çš„åˆ†æï¼Œä»–ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªæ–°çš„ Roblox obby å¹³å°ï¼Œä½¿èµ›é“èƒ½å¤Ÿåœ¨ superconducting qubit hardware ä¸Šå®æ—¶ç”Ÿæˆã€‚é€šè¿‡è¯¥é¡¹ç›®ï¼Œç ”ç©¶å›¢é˜Ÿæ·±å…¥è°ƒæŸ¥äº†åˆ©ç”¨æ­¤ç±»é‡å­ç¡¬ä»¶è¿›è¡Œå®æ—¶ç”Ÿæˆæ—¶æ‰€é¢ä¸´çš„æŠ€æœ¯çº¦æŸä¸æŒ‘æˆ˜ï¼Œä¸ºé‡å­è®¡ç®—åœ¨æ¸¸æˆç¨‹åºåŒ–å†…å®¹ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13287v1",
      "published_date": "2025-05-19 16:09:30 UTC",
      "updated_date": "2025-05-19 16:09:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:04.402396+00:00"
    },
    {
      "arxiv_id": "2505.13280v1",
      "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification",
      "title_zh": "FlowPureï¼šé¢å‘å¯¹æŠ—å‡€åŒ–çš„è¿ç»­å½’ä¸€åŒ–æµ",
      "authors": [
        "Elias Collaert",
        "Abel RodrÃ­guez",
        "Sander Joos",
        "Lieven Desmet",
        "Vera Rimmer"
      ],
      "abstract": "Despite significant advancements in the area, adversarial robustness remains a critical challenge in systems employing machine learning models. The removal of adversarial perturbations at inference time, known as adversarial purification, has emerged as a promising defense strategy. To achieve this, state-of-the-art methods leverage diffusion models that inject Gaussian noise during a forward process to dilute adversarial perturbations, followed by a denoising step to restore clean samples before classification. In this work, we propose FlowPure, a novel purification method based on Continuous Normalizing Flows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings from adversarial examples to their clean counterparts. Unlike prior diffusion-based approaches that rely on fixed noise processes, FlowPure can leverage specific attack knowledge to improve robustness under known threats, while also supporting a more general stochastic variant trained on Gaussian perturbations for settings where such knowledge is unavailable. Experiments on CIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art purification-based defenses in preprocessor-blind and white-box scenarios, and can do so while fully preserving benign accuracy in the former. Moreover, our results show that not only is FlowPure a highly effective purifier but it also holds a strong potential for adversarial detection, identifying preprocessor-blind PGD samples with near-perfect accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FlowPureï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¿ç»­å½’ä¸€åŒ–æµ(Continuous Normalizing Flows, CNFs)çš„æ–°å‹å¯¹æŠ—å‡€åŒ–(Adversarial Purification)æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§(Adversarial Robustness)ã€‚FlowPureåˆ©ç”¨æ¡ä»¶æµåŒ¹é…(Conditional Flow Matching, CFM)æŠ€æœ¯æ¥å­¦ä¹ ä»å¯¹æŠ—æ ·æœ¬åˆ°å…¶å¯¹åº”å¹²å‡€æ ·æœ¬çš„æ˜ å°„ï¼Œå…‹æœäº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹(Diffusion Models)ä¾èµ–å›ºå®šå™ªå£°è¿‡ç¨‹çš„å±€é™ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿåˆ©ç”¨ç‰¹å®šæ”»å‡»çŸ¥è¯†æ¥å¢å¼ºé’ˆå¯¹æ€§é˜²å¾¡ï¼Œä¹Ÿæ”¯æŒé€šç”¨çš„éšæœºå‡€åŒ–æ¨¡å¼ä»¥åº”å¯¹æœªçŸ¥å¨èƒã€‚åœ¨CIFAR-10å’ŒCIFAR-100æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFlowPureåœ¨é¢„å¤„ç†å™¨ç›²æµ‹(Preprocessor-blind)å’Œç™½ç›’(White-box)åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å‡€åŒ–é˜²å¾¡æŠ€æœ¯ï¼Œä¸”èƒ½æœ‰æ•ˆä¿æŒè‰¯æ€§æ ·æœ¬çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºFlowPureåœ¨å¯¹æŠ—æ£€æµ‹ä»»åŠ¡ä¸­ä¹Ÿå…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œèƒ½å¤Ÿä»¥è¿‘ä¹å®Œç¾çš„å‡†ç¡®ç‡è¯†åˆ«å‡ºå—PGDæ”»å‡»çš„æ ·æœ¬ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13280v1",
      "published_date": "2025-05-19 16:04:43 UTC",
      "updated_date": "2025-05-19 16:04:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:08.208000+00:00"
    },
    {
      "arxiv_id": "2505.13273v1",
      "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models",
      "title_zh": "æ´å¯ŸéšåŒ¿ï¼šEMoE å¦‚ä½•æ­ç¤ºæ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ä¸­çš„åè§",
      "authors": [
        "Lucas Berry",
        "Axel Brando",
        "Wei-Di Chang",
        "Juan Camilo Gamboa Higuera",
        "David Meger"
      ],
      "abstract": "Estimating uncertainty in text-to-image diffusion models is challenging because of their large parameter counts (often exceeding 100 million) and operation in complex, high-dimensional spaces with virtually infinite input possibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a novel framework for efficiently estimating epistemic uncertainty in diffusion models. EMoE leverages pre-trained networks without requiring additional training, enabling direct uncertainty estimation from a prompt. We leverage a latent space within the diffusion process that captures epistemic uncertainty better than existing methods. Experimental results on the COCO dataset demonstrate EMoE's effectiveness, showing a strong correlation between uncertainty and image quality. Additionally, EMoE identifies under-sampled languages and regions with higher uncertainty, revealing hidden biases in the training set. This capability demonstrates the relevance of EMoE as a tool for addressing fairness and accountability in AI-generated content.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Epistemic Mixture of Experts (EMoE)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)æ‰©æ•£æ¨¡å‹ä¸­å› å‚æ•°è§„æ¨¡å·¨å¤§åŠé«˜ç»´ç©ºé—´å¤æ‚æ€§è€Œéš¾ä»¥æœ‰æ•ˆä¼°è®¡è®¤çŸ¥ä¸ç¡®å®šæ€§(Epistemic Uncertainty)çš„é—®é¢˜ã€‚EMoEåˆ©ç”¨é¢„è®­ç»ƒç½‘ç»œï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„å‰æä¸‹ï¼Œå®ç°äº†ç›´æ¥ä»æç¤ºè¯(Prompt)ä¸­ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹ä¸­æ›´èƒ½æ•æ‰æ­¤ç±»ä¿¡æ¯çš„æ½œç©ºé—´(Latent Space)æé«˜åˆ†æç²¾åº¦ã€‚åœ¨COCOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ‰€ä¼°è®¡çš„ä¸ç¡®å®šæ€§ä¸ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒEMoEèƒ½å¤Ÿè¯†åˆ«æ¬ é‡‡æ ·è¯­è¨€å’Œåœ°åŒºå¯¹åº”çš„é«˜ä¸ç¡®å®šæ€§ï¼Œä»è€Œæ­ç¤ºè®­ç»ƒæ•°æ®ä¸­éšè—çš„åè§ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†EMoEä½œä¸ºè¯„ä¼°ç”Ÿæˆå¼AIå…¬å¹³æ€§ä¸é—®è´£åˆ¶çš„æœ‰æ•ˆå·¥å…·ï¼Œåœ¨æ­ç¤ºæ¨¡å‹æ½œåœ¨åå·®æ–¹é¢å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13273v1",
      "published_date": "2025-05-19 15:53:32 UTC",
      "updated_date": "2025-05-19 15:53:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:14.045697+00:00"
    },
    {
      "arxiv_id": "2505.13268v1",
      "title": "Representation of perceived prosodic similarity of conversational feedback",
      "title_zh": "ä¼šè¯åé¦ˆä¸­æ„ŸçŸ¥éŸµå¾‹ç›¸ä¼¼æ€§çš„è¡¨å¾",
      "authors": [
        "Livia Qian",
        "Carol Figueroa",
        "Gabriel Skantze"
      ],
      "abstract": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of spoken dialogue and is crucial to ensuring common ground in conversational systems. The exact meaning of such feedback is conveyed through both lexical and prosodic form. In this work, we investigate the perceived prosodic similarity of vocal feedback with the same lexical form, and to what extent existing speech representations reflect such similarities. A triadic comparison task with recruited participants is used to measure perceived similarity of feedback responses taken from two different datasets. We find that spectral and self-supervised speech representations encode prosody better than extracted pitch features, especially in the case of feedback from the same speaker. We also find that it is possible to further condense and align the representations to human perception through contrastive learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯¹è¯åé¦ˆï¼ˆå¦‚ `mhm`ã€`yeah`ã€`okay`ï¼‰ä¸­æ„ŸçŸ¥éŸµå¾‹ç›¸ä¼¼æ€§ï¼ˆprosodic similarityï¼‰çš„è¡¨ç¤ºï¼Œè¿™å¯¹ç¡®ä¿å¯¹è¯ç³»ç»Ÿçš„å…±åŒç†è§£è‡³å…³é‡è¦ã€‚ç ”ç©¶é€šè¿‡ä¸‰å…ƒæ¯”è¾ƒä»»åŠ¡ï¼ˆtriadic comparison taskï¼‰æµ‹é‡äº†äººç±»å—è¯•è€…å¯¹å…·æœ‰ç›¸åŒè¯æ±‡å½¢å¼çš„åé¦ˆè¯­éŸ³çš„æ„ŸçŸ¥ç›¸ä¼¼åº¦ï¼Œå¹¶è¯„ä¼°äº†ç°æœ‰è¯­éŸ³è¡¨ç¤ºåœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½åæ˜ è¿™ç§ç›¸ä¼¼æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢‘è°±ï¼ˆspectralï¼‰å’Œè‡ªç›‘ç£ï¼ˆself-supervisedï¼‰è¯­éŸ³è¡¨ç¤ºåœ¨æ•æ‰éŸµå¾‹ç‰¹å¾æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„éŸ³é«˜ï¼ˆpitchï¼‰ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒä¸€è¯´è¯è€…çš„åœºæ™¯ä¸‹ã€‚æœ€åï¼Œç ”ç©¶è¯æ˜åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆcontrastive learningï¼‰å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›è¡¨ç¤ºï¼Œä½¿å…¶ä¸äººç±»çš„æ„ŸçŸ¥åˆ¤æ–­æ›´åŠ ä¸€è‡´ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13268v1",
      "published_date": "2025-05-19 15:47:51 UTC",
      "updated_date": "2025-05-19 15:47:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:23.669579+00:00"
    },
    {
      "arxiv_id": "2505.13264v1",
      "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty",
      "title_zh": "Net-Zeroï¼šä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹æ°”å€™-ç»æµåå¾®åˆ†æ–¹ç¨‹ç¥ç»ç½‘ç»œè®¾è®¡çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Carlos Rodriguez-Pardo",
        "Louis Daumas",
        "Leonardo Chiani",
        "Massimo Tavoni"
      ],
      "abstract": "Climate-economic modeling under uncertainty presents significant computational challenges that may limit policymakers' ability to address climate change effectively. This paper explores neural network-based approaches for solving high-dimensional optimal control problems arising from models that incorporate ambiguity aversion in climate mitigation decisions. We develop a continuous-time endogenous-growth economic model that accounts for multiple mitigation pathways, including emission-free capital and carbon intensity reductions. Given the inherent complexity and high dimensionality of these models, traditional numerical methods become computationally intractable. We benchmark several neural network architectures against finite-difference generated solutions, evaluating their ability to capture the dynamic interactions between uncertainty, technology transitions, and optimal climate policy. Our findings demonstrate that appropriate neural architecture selection significantly impacts both solution accuracy and computational efficiency when modeling climate-economic systems under uncertainty. These methodological advances enable more sophisticated modeling of climate policy decisions, allowing for better representation of technology transitions and uncertainty-critical elements for developing effective mitigation strategies in the face of climate change.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œ(Neural Network)è§£å†³æ°”å€™-ç»æµæ¨¡å‹ä¸­é«˜ç»´æœ€ä¼˜æ§åˆ¶é—®é¢˜çš„æ–¹æ³•ã€‚ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŒ…å«å¤šç§å‡æ’è·¯å¾„çš„è¿ç»­æ—¶é—´å†…ç”Ÿå¢é•¿ç»æµæ¨¡å‹ï¼Œå¹¶é’ˆå¯¹æ°”å€™å‡æ’å†³ç­–ä¸­çš„æ­§ä¹‰è§„é¿(Ambiguity Aversion)è¿›è¡Œäº†å»ºæ¨¡ã€‚ç”±äºä¼ ç»Ÿæ•°å€¼æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»é«˜ç»´åå¾®åˆ†æ–¹ç¨‹(PDEs)æ—¶é¢ä¸´è®¡ç®—ç“¶é¢ˆï¼Œä½œè€…å¯¹æ¯”äº†å¤šç§ç¥ç»ç½‘ç»œæ¶æ„åœ¨æ•æ‰åŠ¨æ€äº¤äº’ã€æŠ€æœ¯è½¬å‹å’Œæ”¿ç­–ä¼˜åŒ–æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¥ç»ç½‘ç»œæ¶æ„çš„é€‰æ‹©å¯¹æ±‚è§£ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡å…·æœ‰æ˜¾è‘—å½±å“ã€‚è¿™ä¸€æ–¹æ³•å­¦ä¸Šçš„è¿›å±•ä¸ºå¤æ‚çš„æ°”å€™æ”¿ç­–å†³ç­–æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°æ¨¡æ‹ŸæŠ€æœ¯è½¬å‹å’Œä¸ç¡®å®šæ€§å› ç´ ï¼Œä»è€ŒåŠ©åŠ›åˆ¶å®šæ›´æœ‰æ•ˆçš„å…¨çƒæ°”å€™å‡æ’ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.PF",
        "math.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2505.13264v1",
      "published_date": "2025-05-19 15:46:12 UTC",
      "updated_date": "2025-05-19 15:46:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:23.377534+00:00"
    },
    {
      "arxiv_id": "2505.13257v2",
      "title": "Is Active Persona Inference Necessary for Aligning Small Models to Personal Preferences?",
      "title_zh": "ä¸»åŠ¨ç”»åƒæ¨æ–­å¯¹äºå°æ¨¡å‹å®ç°ä¸ªäººåå¥½å¯¹é½æ˜¯å¦å¿…è¦ï¼Ÿ",
      "authors": [
        "Zilu Tang",
        "Afra Feyza AkyÃ¼rek",
        "Ekin AkyÃ¼rek",
        "Derry Wijaya"
      ],
      "abstract": "A prominent issue in aligning language models (LMs) to personalized preferences is underspecification -- the lack of information from users about their preferences. A popular trend of injecting such specification is adding a prefix (e.g. prior relevant conversations) to the current user's conversation to steer preference distribution. Most methods passively model personal preferences with prior example preferences pairs. We ask whether models benefit from actively inferring preference descriptions, and address this question by creating a synthetic personalized alignment dataset based on famous people with known public preferences. We then test how effective finetuned 1-8B size models are at inferring and aligning to personal preferences. Results show that higher-quality active prefixes lead to better generalization, more contextually faithful models, and less systematic biases across different protected attributes. All our results suggest active alignment can lead to a more controllable and efficient path for personalized alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å°†è¯­è¨€æ¨¡å‹(LMs)å¯¹é½åˆ°ä¸ªæ€§åŒ–åå¥½æ—¶ï¼Œä¸»åŠ¨æ¨ç†äººæ ¼æè¿°(Active Persona Inference)æ˜¯å¦æ¯”ä¼ ç»Ÿçš„è¢«åŠ¨å»ºæ¨¡æ›´å…·ä¼˜åŠ¿ã€‚é’ˆå¯¹ä¸ªæ€§åŒ–å¯¹é½ä¸­ç”±äºç”¨æˆ·ä¿¡æ¯ç¼ºå¤±å¯¼è‡´çš„æ¬ è§„èŒƒ(Underspecification)é—®é¢˜ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŸºäºçŸ¥åäººç‰©å…¬å¼€åå¥½çš„åˆæˆä¸ªæ€§åŒ–å¯¹é½æ•°æ®é›†ã€‚é€šè¿‡åœ¨1-8Bè§„æ¨¡çš„æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒå®éªŒï¼Œç»“æœè¡¨æ˜é«˜è´¨é‡çš„ä¸»åŠ¨å‰ç¼€(Active Prefixes)èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºæ¨¡å‹å¯¹è¯­å¢ƒçš„å¿ å®åº¦ã€‚æ­¤å¤–ï¼Œä¸»åŠ¨æ¨ç†è¿˜å¸®åŠ©æ¨¡å‹å‡å°‘äº†è·¨ä¸åŒå—ä¿æŠ¤å±æ€§çš„ç³»ç»Ÿæ€§åå·®(Systematic Biases)ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œä¸»åŠ¨å¯¹é½(Active Alignment)ç›¸æ¯”è¢«åŠ¨æ–¹æ³•èƒ½ä¸ºä¸ªæ€§åŒ–å¯¹é½æä¾›æ›´å…·å¯æ§æ€§ä¸”é«˜æ•ˆçš„å®ç°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, EMNLP PALS workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13257v2",
      "published_date": "2025-05-19 15:39:48 UTC",
      "updated_date": "2025-09-29 16:23:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:32.259181+00:00"
    },
    {
      "arxiv_id": "2505.13253v1",
      "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic",
      "title_zh": "åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¯„ä»·ç½‘ç»œè¯„åˆ†å®ç°çµå·§æŠ“å–ä¸æ‰‹å†…æ“çºµçš„èåˆ",
      "authors": [
        "Lennart RÃ¶stel",
        "Dominik Winkelbauer",
        "Johannes Pitz",
        "Leon Sievers",
        "Berthold BÃ¤uml"
      ],
      "abstract": "In-hand manipulation and grasping are fundamental yet often separately addressed tasks in robotics. For deriving in-hand manipulation policies, reinforcement learning has recently shown great success. However, the derived controllers are not yet useful in real-world scenarios because they often require a human operator to place the objects in suitable initial (grasping) states. Finding stable grasps that also promote the desired in-hand manipulation goal is an open problem. In this work, we propose a method for bridging this gap by leveraging the critic network of a reinforcement learning agent trained for in-hand manipulation to score and select initial grasps. Our experiments show that this method significantly increases the success rate of in-hand manipulation without requiring additional training. We also present an implementation of a full grasp manipulation pipeline on a real-world system, enabling autonomous grasping and reorientation even of unwieldy objects.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹æœºå™¨äººé¢†åŸŸä¸­æŠ“å–(Grasping)ä¸æ‰‹å†…æ“ä½œ(In-hand Manipulation)ä»»åŠ¡é€šå¸¸è¢«åˆ†å¼€å¤„ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ•´åˆæ–¹æ³•ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨ç”Ÿæˆæ‰‹å†…æ“ä½œç­–ç•¥æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç°æœ‰æ§åˆ¶å™¨å¾€å¾€éœ€è¦äººå·¥å¹²é¢„æ¥è®¾ç½®åˆå§‹æŠ“å–çŠ¶æ€ï¼Œéš¾ä»¥è‡ªåŠ¨å¯»æ‰¾æœ‰åˆ©äºåç»­æ“ä½œçš„ç¨³å®šæŠ“å–ã€‚ä½œè€…æå‡ºåˆ©ç”¨ä¸ºæ‰‹å†…æ“ä½œè®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ä¸­çš„è¯„è®ºè€…ç½‘ç»œ(Critic network)æ¥å¯¹åˆå§‹æŠ“å–è¿›è¡Œè¯„åˆ†å’Œé€‰æ‹©ï¼Œä»è€Œå¼¥è¡¥äº†è¿™ä¸¤ä¸ªä»»åŠ¡ä¹‹é—´çš„é—´éš™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ‰‹å†…æ“ä½œçš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨çœŸå®ä¸–ç•Œç³»ç»Ÿä¸­éƒ¨ç½²äº†å®Œæ•´çš„æµæ°´çº¿ï¼Œå®ç°äº†å¯¹å„ç±»ç¬¨é‡ç‰©ä½“çš„è‡ªä¸»æŠ“å–ä¸é‡å®šå‘ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨è§£å†³å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13253v1",
      "published_date": "2025-05-19 15:36:34 UTC",
      "updated_date": "2025-05-19 15:36:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:34.647614+00:00"
    },
    {
      "arxiv_id": "2505.13580v1",
      "title": "OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making",
      "title_zh": "OMGPTï¼šé¢å‘æ•°æ®é©±åŠ¨å‹è¿è¥å†³ç­–çš„åºåˆ—å»ºæ¨¡æ¡†æ¶",
      "authors": [
        "Hanzhao Wang",
        "Guanting Chen",
        "Kalyan Talluri",
        "Xiaocheng Li"
      ],
      "abstract": "We build a Generative Pre-trained Transformer (GPT) model from scratch to solve sequential decision making tasks arising in contexts of operations research and management science which we call OMGPT. We first propose a general sequence modeling framework to cover several operational decision making tasks as special cases, such as dynamic pricing, inventory management, resource allocation, and queueing control. Under the framework, all these tasks can be viewed as a sequential prediction problem where the goal is to predict the optimal future action given all the historical information. Then we train a transformer-based neural network model (OMGPT) as a natural and powerful architecture for sequential modeling. This marks a paradigm shift compared to the existing methods for these OR/OM tasks in that (i) the OMGPT model can take advantage of the huge amount of pre-trained data; (ii) when tackling these problems, OMGPT does not assume any analytical model structure and enables a direct and rich mapping from the history to the future actions. Either of these two aspects, to the best of our knowledge, is not achieved by any existing method. We establish a Bayesian perspective to theoretically understand the working mechanism of the OMGPT on these tasks, which relates its performance with the pre-training task diversity and the divergence between the testing task and pre-training tasks. Numerically, we observe a surprising performance of the proposed model across all the above tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OMGPTï¼Œè¿™æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹æ„å»ºçš„ Generative Pre-trained Transformer (GPT) æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè§£å†³è¿ç­¹å­¦ä¸ç®¡ç†ç§‘å­¦ (OR/OM) é¢†åŸŸçš„æ•°æ®é©±åŠ¨è¿è¥å†³ç­–é—®é¢˜ã€‚è¯¥ç ”ç©¶é¦–å…ˆæå‡ºäº†ä¸€ä¸ªé€šç”¨çš„ sequence modeling æ¡†æ¶ï¼Œå°†åŠ¨æ€å®šä»· (dynamic pricing)ã€åº“å­˜ç®¡ç† (inventory management)ã€èµ„æºåˆ†é… (resource allocation) å’Œæ’é˜Ÿæ§åˆ¶ (queueing control) ç­‰ä»»åŠ¡ç»Ÿä¸€è§†ä¸ºåºåˆ—é¢„æµ‹é—®é¢˜ï¼Œæ—¨åœ¨æ ¹æ®å†å²ä¿¡æ¯é¢„æµ‹æœ€ä¼˜æœªæ¥è¡ŒåŠ¨ã€‚ç›¸æ¯”äºç°æœ‰æ–¹æ³•ï¼ŒOMGPT çš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®ï¼Œä¸”ä¸ä¾èµ–ä»»ä½•é¢„è®¾çš„è§£ææ¨¡å‹ç»“æ„ï¼Œå®ç°äº†ä»å†å²åˆ°æœªæ¥è¡ŒåŠ¨çš„ç›´æ¥æ˜ å°„ã€‚ç ”ç©¶è¿˜ä» Bayesian è§†è§’å»ºç«‹äº†ç†è®ºæ¡†æ¶ï¼Œåˆ†æäº†é¢„è®­ç»ƒä»»åŠ¡å¤šæ ·æ€§åŠæµ‹è¯•ä»»åŠ¡ä¸é¢„è®­ç»ƒä»»åŠ¡ä¹‹é—´çš„å·®å¼‚å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒOMGPT åœ¨ä¸Šè¿°æ‰€æœ‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸º OR/OM ä»»åŠ¡æä¾›äº†ä¸€ç§å…¨æ–°çš„å»ºæ¨¡èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2405.14219",
      "pdf_url": "https://arxiv.org/pdf/2505.13580v1",
      "published_date": "2025-05-19 15:33:03 UTC",
      "updated_date": "2025-05-19 15:33:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:30.164555+00:00"
    },
    {
      "arxiv_id": "2505.13579v1",
      "title": "Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction",
      "title_zh": "ç”¨äºä¸‰ç»´é”¥æŸ CT é‡å»ºçš„å­¦ä¹ å‹å°æ³¢ç¨€ç– FDK",
      "authors": [
        "Yipeng Sun",
        "Linda-Sophie Schneider",
        "Chengze Ye",
        "Mingxuan Gu",
        "Siyuan Mei",
        "Siming Bayer",
        "Andreas Maier"
      ],
      "abstract": "Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due to its efficiency. However, FDK is susceptible to noise and artifacts. While recent deep learning methods offer improved image quality, they often increase computational complexity and lack the interpretability of traditional methods. In this paper, we introduce an enhanced FDK-based neural network that maintains the classical algorithm's interpretability by selectively integrating trainable elements into the cosine weighting and filtering stages. Recognizing the challenge of a large parameter space inherent in 3D CBCT data, we leverage wavelet transformations to create sparse representations of the cosine weights and filters. This strategic sparsification reduces the parameter count by $93.75\\%$ without compromising performance, accelerates convergence, and importantly, maintains the inference computational cost equivalent to the classical FDK algorithm. Our method not only ensures volumetric consistency and boosts robustness to noise, but is also designed for straightforward integration into existing CT reconstruction pipelines. This presents a pragmatic enhancement that can benefit clinical applications, particularly in environments with computational limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é”¥æŸè®¡ç®—æœºæ–­å±‚æ‰«æ (Cone-Beam Computed Tomography, CBCT) ä¸­ä¼ ç»Ÿçš„ Feldkamp-Davis-Kress (FDK) ç®—æ³•æ˜“å—å™ªå£°å’Œä¼ªå½±å½±å“ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ æ–¹æ³•è®¡ç®—å¤æ‚ä¸”ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºå‹ FDK ç¥ç»ç½‘ç»œã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ä½™å¼¦æƒé‡ (cosine weighting) å’Œæ»¤æ³¢ (filtering) é˜¶æ®µé€‰æ‹©æ€§åœ°é›†æˆå¯è®­ç»ƒå…ƒç´ ï¼Œæœ‰æ•ˆä¿ç•™äº†ç»å…¸ç®—æ³•çš„å¯è§£é‡Šæ€§ã€‚ä¸ºåº”å¯¹ 3D CBCT æ•°æ®å¸¦æ¥çš„å¤§è§„æ¨¡å‚æ•°æŒ‘æˆ˜ï¼Œç ”ç©¶åˆ©ç”¨å°æ³¢å˜æ¢ (wavelet transformations) æ„å»ºäº†æƒé‡å’Œæ»¤æ³¢å™¨çš„ç¨€ç–è¡¨ç¤ºã€‚è¿™ç§ç¨€ç–åŒ–ç­–ç•¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å°†å‚æ•°é‡å‡å°‘äº† 93.75%ï¼Œä¸ä»…åŠ å¿«äº†æ¨¡å‹æ”¶æ•›ï¼Œè¿˜ä½¿æ¨ç†è®¡ç®—æˆæœ¬ä¸ç»å…¸ FDK ç®—æ³•ä¿æŒä¸€è‡´ã€‚è¯¥æ–¹æ³•åœ¨å¢å¼ºå™ªå£°é²æ£’æ€§çš„åŒæ—¶ç¡®ä¿äº†ä½“ç§¯ä¸€è‡´æ€§ (volumetric consistency)ï¼Œä¸”èƒ½å¤Ÿç›´æ¥é›†æˆè‡³ç°æœ‰ä¸´åºŠ CT é‡å»ºæµç¨‹ä¸­ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè®¡ç®—èµ„æºå—é™ç¯å¢ƒä¸‹çš„åŒ»å­¦å½±åƒé‡å»ºæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„æ”¹è¿›æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by Fully3D 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13579v1",
      "published_date": "2025-05-19 15:31:40 UTC",
      "updated_date": "2025-05-19 15:31:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:03.237874+00:00"
    },
    {
      "arxiv_id": "2505.13246v1",
      "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems",
      "title_zh": "Agentic Publicationsï¼šä¸€ç§å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„äº¤äº’å¼ç§‘å­¦å‡ºç‰ˆæ¡†æ¶ï¼Œé€šè¿‡äººå·¥æ™ºèƒ½çŸ¥è¯†ç³»ç»Ÿå¯¹ä¼ ç»Ÿè®ºæ–‡è¿›è¡Œè¡¥å……",
      "authors": [
        "Roberto Pugliese",
        "George Kourousias",
        "Francesco Venier",
        "Grazia Garlatti Costa"
      ],
      "abstract": "The exponential growth of scientific literature presents significant challenges for researchers navigating the complex knowledge landscape. We propose \"Agentic Publications\", a novel LLM-driven framework complementing traditional publishing by transforming papers into interactive knowledge systems. Our architecture integrates structured data with unstructured content through retrieval-augmented generation and multi-agent verification. The framework offers interfaces for both humans and machines, combining narrative explanations with machine-readable outputs while addressing ethical considerations through automated validation and transparent governance. Key features include continuous knowledge updates, automatic integration of new findings, and customizable detail levels. Our proof-of-concept demonstrates multilingual interaction, API accessibility, and structured knowledge representation through vector databases, knowledge graphs, and verification agents. This approach enhances scientific communication across disciplines, improving efficiency and collaboration while preserving traditional publishing pathways, particularly valuable for interdisciplinary fields where knowledge integration remains challenging.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agentic Publicationsï¼Œè¿™æ˜¯ä¸€ç§ç”± LLM é©±åŠ¨çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è®ºæ–‡è½¬åŒ–ä¸ºäº¤äº’å¼çŸ¥è¯†ç³»ç»Ÿæ¥è¡¥å……ä¼ ç»Ÿå‡ºç‰ˆæ¨¡å¼ã€‚è¯¥æ¶æ„åˆ©ç”¨ Retrieval-Augmented Generation (RAG) å’Œ Multi-Agent Verification æŠ€æœ¯ï¼Œå®ç°äº†ç»“æ„åŒ–æ•°æ®ä¸éç»“æ„åŒ–å†…å®¹çš„æ·±åº¦æ•´åˆã€‚æ¡†æ¶åŒæ—¶ä¸ºäººç±»å’Œæœºå™¨æä¾›äº¤äº’æ¥å£ï¼Œæ”¯æŒçŸ¥è¯†çš„æŒç»­æ›´æ–°ã€æ–°å‘ç°çš„è‡ªåŠ¨é›†æˆä»¥åŠçµæ´»çš„ç»†èŠ‚å±•ç¤ºã€‚æ¦‚å¿µéªŒè¯å®éªŒè¯æ˜äº†å…¶åœ¨å¤šè¯­è¨€äº¤äº’ã€API è®¿é—®ä»¥åŠé€šè¿‡ Vector Databases å’Œ Knowledge Graphs è¿›è¡Œç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿ç•™ä¼ ç»Ÿå‡ºç‰ˆè·¯å¾„çš„åŸºç¡€ä¸Šï¼Œæ˜¾è‘—æå‡äº†è·¨å­¦ç§‘é¢†åŸŸçš„ç§‘å­¦ä¼ æ’­æ•ˆç‡ä¸åä½œèƒ½åŠ›ï¼Œä¸ºåº”å¯¹æŒ‡æ•°çº§å¢é•¿çš„å­¦æœ¯æ–‡çŒ®æŒ‘æˆ˜æä¾›äº†åˆ›æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13246v1",
      "published_date": "2025-05-19 15:28:10 UTC",
      "updated_date": "2025-05-19 15:28:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:58.611678+00:00"
    },
    {
      "arxiv_id": "2505.17070v1",
      "title": "Improving endpoint detection in end-to-end streaming ASR for conversational speech",
      "title_zh": "æ”¹è¿›å¯¹è¯å¼è¯­éŸ³ä¸­ç«¯åˆ°ç«¯æµå¼ ASR çš„ç«¯ç‚¹æ£€æµ‹",
      "authors": [
        "Anandh C",
        "Karthik Pandia Durai",
        "Jeena Prakash",
        "Manickavela Arumugam",
        "Kadri Hacioglu",
        "S. Pavankumar Dubagunta",
        "Andreas Stolcke",
        "Shankar Venkatesan",
        "Aravind Ganapathiraju"
      ],
      "abstract": "ASR endpointing (EP) plays a major role in delivering a good user experience in products supporting human or artificial agents in human-human/machine conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR modelling technique preferred for streaming. A major limitation of T-ASR is delayed emission of ASR outputs, which could lead to errors or delays in EP. Inaccurate EP will cut the user off while speaking, returning incomplete transcript while delays in EP will increase the perceived latency, degrading the user experience. We propose methods to improve EP by addressing delayed emission along with EP mistakes. To address the delayed emission problem, we introduce an end-of-word token at the end of each word, along with a delay penalty. The EP delay is addressed by obtaining a reliable frame-level speech activity detection using an auxiliary network. We apply the proposed methods on Switchboard conversational speech corpus and evaluate it against a delay penalty method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµå¼ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ« (ASR) ç³»ç»Ÿä¸­ç«¯ç‚¹æ£€æµ‹ (EP) çš„å»¶è¿Ÿå’Œé”™è¯¯é—®é¢˜ï¼Œæå‡ºäº†æ—¨åœ¨æå‡å¯¹è¯ä½“éªŒçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚é’ˆå¯¹åŸºäºæ¢èƒ½å™¨çš„ ASR (T-ASR) æ™®éå­˜åœ¨çš„è¾“å‡ºæ»åç°è±¡ï¼Œä½œè€…å¼•å…¥äº† end-of-word token æœºåˆ¶å¹¶ç»“åˆ delay penalty ç­–ç•¥æ¥å¼ºåˆ¶æå‰é¢„æµ‹ã€‚åŒæ—¶ï¼Œç ”ç©¶é€šè¿‡è¾…åŠ©ç½‘ç»œ (auxiliary network) å®ç°æ›´ç²¾ç¡®çš„å¸§çº§è¯­éŸ³æ´»åŠ¨æ£€æµ‹ (speech activity detection)ï¼Œä»¥è§£å†³ä¼ ç»Ÿæ–¹æ³•å¸¦æ¥çš„ EP å»¶è¿Ÿã€‚åœ¨ Switchboard ä¼šè¯è¯­éŸ³è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘ EP è¯¯åˆ‡å’Œé«˜å»¶è¿Ÿç°è±¡ã€‚è¿™ç§å¤šç»´åº¦çš„æ”¹è¿›åœ¨ä¿è¯ ASR æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†äººæœºæˆ–äººä¸äººå¯¹è¯ä¸­å®æ—¶åé¦ˆçš„å‡†ç¡®æ€§å’Œæµç•…åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to Interspeech 2024",
      "pdf_url": "https://arxiv.org/pdf/2505.17070v1",
      "published_date": "2025-05-19 15:19:59 UTC",
      "updated_date": "2025-05-19 15:19:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:02.993265+00:00"
    },
    {
      "arxiv_id": "2505.13232v3",
      "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment",
      "title_zh": "StarFTï¼šé€šè¿‡è™šå‡å…³è”å¯¹é½å®ç°é›¶æ ·æœ¬æ¨¡å‹çš„é²æ£’å¾®è°ƒ",
      "authors": [
        "Younghyun Kim",
        "Jongheon Jeong",
        "Sangkyung Kwak",
        "Kyungmin Lee",
        "Juho Lee",
        "Jinwoo Shin"
      ],
      "abstract": "Learning robust representations from data often requires scale, which has led to the success of recent zero-shot models such as CLIP. However, the obtained robustness can easily be deteriorated when these models are fine-tuned on other downstream tasks (e.g., of smaller scales). Previous works often interpret this phenomenon in the context of domain shift, developing fine-tuning methods that aim to preserve the original domain as much as possible. However, in a different context, fine-tuned models with limited data are also prone to learning features that are spurious to humans, such as background or texture. In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a novel framework for fine-tuning zero-shot models to enhance robustness by preventing them from learning spuriosity. We introduce a regularization that aligns the output distribution for spuriosity-injected labels with the original zero-shot model, ensuring that the model is not induced to extract irrelevant features further from these descriptions. We leverage recent language models to get such spuriosity-injected labels by generating alternative textual descriptions that highlight potentially confounding features. Extensive experiments validate the robust generalization of StarFT and its emerging properties: zero-shot group robustness and improved zero-shot classification. Notably, StarFT boosts both worst-group and average accuracy by 14.30% and 3.02%, respectively, in the Waterbirds group shift scenario, where other robust fine-tuning baselines show even degraded performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† StarFT (Spurious Textual Alignment Regularization)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼º Zero-shot æ¨¡å‹ï¼ˆå¦‚ CLIPï¼‰åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é²æ£’æ€§çš„æ–°æ¡†æ¶ã€‚é’ˆå¯¹æ¨¡å‹åœ¨å—é™æ•°æ®å¾®è°ƒæ—¶å®¹æ˜“å­¦ä¹ åˆ°èƒŒæ™¯æˆ–çº¹ç†ç­‰è™šå‡ç›¸å…³ (Spurious) ç‰¹å¾çš„é—®é¢˜ï¼ŒStarFT å¼•å…¥äº†ä¸€ç§æ­£åˆ™åŒ–æœºåˆ¶ï¼Œå°†æ³¨å…¥è™šå‡ä¿¡æ¯çš„æ ‡ç­¾è¾“å‡ºåˆ†å¸ƒä¸åŸå§‹ Zero-shot æ¨¡å‹å¯¹é½ï¼Œä»è€Œé˜²æ­¢æ¨¡å‹æå–ä¸ç›¸å…³çš„ç‰¹å¾ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ€æ–°çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆçªå‡ºæ½œåœ¨æ··æ·†ç‰¹å¾çš„æ›¿ä»£æ–‡æœ¬æè¿°ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºè™šå‡æ³¨å…¥æ ‡ç­¾è¿›è¡Œçº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStarFT åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ Waterbirds ç»„åç§»åœºæ™¯ä¸‹ï¼Œå°†å…¶æœ€å·®ç»„å‡†ç¡®ç‡å’Œå¹³å‡å‡†ç¡®ç‡åˆ†åˆ«æå‡äº† 14.30% å’Œ 3.02%ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…è§£å†³äº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ä¸­é²æ£’æ€§ä¸‹é™çš„ç“¶é¢ˆï¼Œè¿˜æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„åˆ†ç±»æ€§èƒ½ä¸ç»„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "IJCAI 2025; Code is available at https://github.com/alinlab/StarFT",
      "pdf_url": "https://arxiv.org/pdf/2505.13232v3",
      "published_date": "2025-05-19 15:15:35 UTC",
      "updated_date": "2025-06-27 13:19:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:43:58.066541+00:00"
    },
    {
      "arxiv_id": "2505.13227v3",
      "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis",
      "title_zh": "é€šè¿‡ç”¨æˆ·ç•Œé¢åˆ†è§£ä¸åˆæˆå®ç°è®¡ç®—æœºæ“ä½œå®šä½çš„è§„æ¨¡åŒ–",
      "authors": [
        "Tianbao Xie",
        "Jiaqi Deng",
        "Xiaochuan Li",
        "Junlin Yang",
        "Haoyuan Wu",
        "Jixuan Chen",
        "Wenjing Hu",
        "Xinyuan Wang",
        "Yuhui Xu",
        "Zekun Wang",
        "Yiheng Xu",
        "Junli Wang",
        "Doyen Sahoo",
        "Tao Yu",
        "Caiming Xiong"
      ],
      "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI) groundingä¸­ç°æœ‰åŸºå‡†æµ‹è¯•è¿‡äºç®€åŒ–ã€æ— æ³•æ•æ‰å¤æ‚è½¯ä»¶å¸¸è¯†å’Œå¸ƒå±€ç†è§£çš„é—®é¢˜ï¼Œæå‡ºäº†æå‡è®¡ç®—æœºä½¿ç”¨èƒ½åŠ›çš„è§„æ¨¡åŒ–æ–¹æ¡ˆã€‚ä½œè€…é¦–å…ˆå¼•å…¥äº†OSWorld-Gï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«564ä¸ªç²¾ç»†æ ‡æ³¨æ ·æœ¬çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ–‡æœ¬åŒ¹é…ã€å…ƒç´ è¯†åˆ«ã€å¸ƒå±€ç†è§£åŠç²¾ç¡®æ“ä½œç­‰å¤šæ ·åŒ–ä»»åŠ¡ç±»å‹ã€‚ä¸ºäº†è§£å†³è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡ä»»åŠ¡çš„å¤šç»´åº¦è§£è€¦(multi-perspective decoupling)åˆæˆå¹¶å‘å¸ƒäº†ç›®å‰æœ€å¤§çš„è®¡ç®—æœºä½¿ç”¨groundingæ•°æ®é›†Jediï¼Œè§„æ¨¡è¾¾400ä¸‡ä¸ªç¤ºä¾‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Jediä¸Šè®­ç»ƒçš„å¤šå°ºåº¦æ¨¡å‹åœ¨ScreenSpot-v2å’ŒOSWorld-Gç­‰åŸºå‡†æµ‹è¯•ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œgroundingèƒ½åŠ›çš„æå‡èƒ½ç›´æ¥å¢å¼ºé€šç”¨åŸºç¡€æ¨¡å‹(foundation models)å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„ä»£ç†èƒ½åŠ›(agentic capabilities)ï¼Œä½¿å…¶åœ¨OSWorldä¸Šçš„è¡¨ç°ä»5%æå‡è‡³27%ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œé€šè¿‡æ•´åˆé’ˆå¯¹ä¸åŒç•Œé¢å…ƒç´ çš„ä¸“ä¸šæ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿå®ç°å¯¹æ–°å‹ç•Œé¢çš„ç»„åˆæ³›åŒ–(compositional generalization)ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "49 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13227v3",
      "published_date": "2025-05-19 15:09:23 UTC",
      "updated_date": "2025-10-24 19:08:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:17.094665+00:00"
    },
    {
      "arxiv_id": "2505.18190v4",
      "title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing",
      "title_zh": "PhySenseï¼šé¢å‘é«˜ç²¾åº¦ç‰©ç†æ„ŸçŸ¥çš„ä¼ æ„Ÿå™¨å¸ƒè®¾ä¼˜åŒ–",
      "authors": [
        "Yuezhou Ma",
        "Haixu Wu",
        "Hang Zhou",
        "Huikun Weng",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "abstract": "Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. Leveraging the reconstruction feedback, the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees. Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered. Code is available at this repository: https://github.com/thuml/PhySense.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhySenseï¼Œè¿™æ˜¯ä¸€ä¸ªååŒçš„åŒé˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‰©ç†æ„ŸçŸ¥ä¸­ç‰©ç†åœºé‡å»ºä¸sensor placement optimizationé•¿æœŸä»¥æ¥è¢«å‰²è£‚å¤„ç†çš„é—®é¢˜ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¢å¼ºäº†cross-attentionæœºåˆ¶çš„flow-based generative modelï¼Œå®ç°å¯¹ç¨€ç–è§‚æµ‹æ•°æ®çš„è‡ªé€‚åº”èåˆä¸é«˜ç²¾åº¦é‡å»ºã€‚ç¬¬äºŒé˜¶æ®µåˆ™æ ¹æ®é‡å»ºåé¦ˆï¼Œé€šè¿‡projected gradient descentç®—æ³•åœ¨æ»¡è¶³ç©ºé—´çº¦æŸçš„å‰æä¸‹ä¼˜åŒ–ä¼ æ„Ÿå™¨å¸ƒå±€ã€‚ç ”ç©¶è¿›ä¸€æ­¥åœ¨ç†è®ºä¸Šè¯æ˜äº†è¿™ä¸¤ä¸ªé˜¶æ®µçš„å­¦ä¹ ç›®æ ‡ä¸ç»å…¸çš„variance-minimization principlesç›¸ä¸€è‡´ã€‚åœ¨åŒ…å«3Då‡ ä½•æ•°æ®é›†åœ¨å†…çš„ä¸‰ä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPhySenseå®ç°äº†state-of-the-artçš„ç‰©ç†æ„ŸçŸ¥ç²¾åº¦ï¼Œå¹¶å‘ç°äº†æ­¤å‰æœªè¢«å‘ç°çš„ã€å…·æœ‰ä¸°å¯Œä¿¡æ¯é‡çš„ä¼ æ„Ÿå™¨å¸ƒå±€æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18190v4",
      "published_date": "2025-05-19 14:59:11 UTC",
      "updated_date": "2025-12-01 04:14:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:14.759102+00:00"
    },
    {
      "arxiv_id": "2505.13211v1",
      "title": "MAGI-1: Autoregressive Video Generation at Scale",
      "title_zh": "MAGI-1ï¼šå¤§è§„æ¨¡è‡ªå›å½’è§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Sand. ai",
        "Hansi Teng",
        "Hongyu Jia",
        "Lei Sun",
        "Lingzhi Li",
        "Maolin Li",
        "Mingqiu Tang",
        "Shuai Han",
        "Tianning Zhang",
        "W. Q. Zhang",
        "Weifeng Luo",
        "Xiaoyang Kang",
        "Yuchen Sun",
        "Yue Cao",
        "Yunpeng Huang",
        "Yutong Lin",
        "Yuxin Fang",
        "Zewei Tao",
        "Zheng Zhang",
        "Zhongshu Wang",
        "Zixun Liu",
        "Dai Shi",
        "Guoli Su",
        "Hanwen Sun",
        "Hong Pan",
        "Jie Wang",
        "Jiexin Sheng",
        "Min Cui",
        "Min Hu",
        "Ming Yan",
        "Shucheng Yin",
        "Siran Zhang",
        "Tingting Liu",
        "Xianping Yin",
        "Xiaoyu Yang",
        "Xin Song",
        "Xuan Hu",
        "Yankai Zhang",
        "Yuqiao Li"
      ],
      "abstract": "We present MAGI-1, a world model that generates videos by autoregressively predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 facilitates controllable generation via chunk-wise prompting and supports real-time, memory-efficient deployment by maintaining constant peak inference cost, regardless of video length. The largest variant of MAGI-1 comprises 24 billion parameters and supports context lengths of up to 4 million tokens, demonstrating the scalability and robustness of our approach. The code and models are available at https://github.com/SandAI-org/MAGI-1 and https://github.com/SandAI-org/MagiAttention. The product can be accessed at https://sand.ai.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† MAGI-1ï¼Œä¸€ç§æ—¨åœ¨å®ç°å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆçš„ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’ (autoregressively) é¢„æµ‹ç”±å›ºå®šé•¿åº¦è¿ç»­å¸§ç»„æˆçš„è§†é¢‘å— (video chunks) åºåˆ—æ¥ç”Ÿæˆè§†é¢‘ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹éšæ—¶é—´å•è°ƒå¢åŠ çš„å—å™ªå£°è¿›è¡Œå»å™ªè®­ç»ƒï¼Œå®ç°äº†å› æœæ—¶é—´å»ºæ¨¡ (causal temporal modeling) å¹¶å¤©ç„¶æ”¯æŒæµå¼ç”Ÿæˆ (streaming generation)ã€‚åœ¨æ–‡æœ¬æŒ‡ä»¤å¼•å¯¼çš„å›¾åƒè½¬è§†é¢‘ (I2V) ä»»åŠ¡ä¸­ï¼ŒMAGI-1 å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå…·å¤‡æé«˜çš„æ—¶é—´ä¸€è‡´æ€§ (temporal consistency) å’Œå¯æ‰©å±•æ€§ (scalability)ã€‚å€ŸåŠ©äºç®—æ³•åˆ›æ–°å’Œä¸“ç”¨åŸºç¡€è®¾æ–½æ ˆï¼ŒMAGI-1 æ”¯æŒé€šè¿‡åˆ†å—æç¤º (chunk-wise prompting) è¿›è¡Œå¯æ§ç”Ÿæˆï¼Œå¹¶èƒ½åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒæ’å®šçš„å³°å€¼æˆæœ¬ï¼Œä»è€Œå®ç°å®æ—¶ã€å†…å­˜é«˜æ•ˆçš„éƒ¨ç½²ã€‚è¯¥æ¨¡å‹æœ€å¤§çš„å˜ä½“è§„æ¨¡è¾¾ 240 äº¿å‚æ•°ï¼Œæ”¯æŒé«˜è¾¾ 400 ä¸‡ä¸ª token çš„ä¸Šä¸‹æ–‡é•¿åº¦ (context lengths)ï¼Œå……åˆ†å±•ç¤ºäº†è¯¥æ¶æ„åœ¨å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13211v1",
      "published_date": "2025-05-19 14:58:50 UTC",
      "updated_date": "2025-05-19 14:58:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:15.632286+00:00"
    },
    {
      "arxiv_id": "2505.13210v1",
      "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry",
      "title_zh": "å…¥ç”»ä¸æ–¹è¨€åŸè¯µï¼šé¢å‘ä¸­å›½å¤å…¸è¯—æ­Œæƒ…æ„Ÿåˆ†æçš„å¤šæ¨¡æ€ä¸­æ–‡è¡¨å¾æ¡†æ¶",
      "authors": [
        "Xiaocong Du",
        "Haoyu Pei",
        "Haipeng Zhang"
      ],
      "abstract": "Classical Chinese poetry is a vital and enduring part of Chinese literature, conveying profound emotional resonance. Existing studies analyze sentiment based on textual meanings, overlooking the unique rhythmic and visual features inherent in poetry,especially since it is often recited and accompanied by Chinese paintings. In this work, we propose a dialect-enhanced multimodal framework for classical Chinese poetry sentiment analysis. We extract sentence-level audio features from the poetry and incorporate audio from multiple dialects,which may retain regional ancient Chinese phonetic features, enriching the phonetic representation. Additionally, we generate sentence-level visual features, and the multimodal features are fused with textual features enhanced by LLM translation through multimodal contrastive representation learning. Our framework outperforms state-of-the-art methods on two public datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro F1. We open-source the code to facilitate research in this area and provide insights for general multimodal Chinese representation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–¹è¨€å¢å¼ºçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºä¸­å›½å¤å…¸è¯—æ­Œçš„æƒ…æ„Ÿåˆ†æï¼Œä»¥å¼¥è¡¥ä¼ ç»Ÿç ”ç©¶åœ¨èŠ‚å¥å’Œè§†è§‰ç‰¹å¾æ–¹é¢çš„ç¼ºå¤±ã€‚è¯¥æ¡†æ¶æå–äº†è¯—æ­Œçš„å¥çº§éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ•´åˆå¤šç§ä¿ç•™å¤æ±‰è¯­å‘éŸ³ç‰¹ç‚¹çš„æ–¹è¨€éŸ³é¢‘ï¼Œæå¤§ä¸°å¯Œäº†è¯­éŸ³è¡¨ç¤ºçš„æ·±åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å¥çº§è§†è§‰ç‰¹å¾ï¼Œå¹¶ç»“åˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ç¿»è¯‘å¢å¼ºåçš„æ–‡æœ¬ç‰¹å¾ï¼Œé€šè¿‡å¤šæ¨¡æ€å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ (multimodal contrastive representation learning)å®ç°å¤šç»´åº¦ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›(SOTA)æ–¹æ³•ï¼Œåœ¨å‡†ç¡®ç‡å’Œå®F1å€¼ä¸Šåˆ†åˆ«å–å¾—äº†è‡³å°‘2.51%å’Œ1.63%çš„æå‡ã€‚è¯¥ç ”ç©¶çš„ä»£ç å·²å¼€æºï¼Œä¸ºå¤å…¸æ–‡å­¦çš„æ•°å­—äººæ–‡ç ”ç©¶åŠé€šç”¨å¤šæ¨¡æ€ä¸­æ–‡è¡¨ç¤ºæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œå‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13210v1",
      "published_date": "2025-05-19 14:58:44 UTC",
      "updated_date": "2025-05-19 14:58:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:29.816915+00:00"
    },
    {
      "arxiv_id": "2505.13577v3",
      "title": "VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation",
      "title_zh": "VocalAgentï¼šå…·å¤‡å®‰å…¨æ„ŸçŸ¥è¯„ä¼°çš„å—“éŸ³å¥åº·è¯Šæ–­å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yubin Kim",
        "Taehan Kim",
        "Wonjune Kang",
        "Eugene Park",
        "Joonsik Yoon",
        "Dongjae Lee",
        "Xin Liu",
        "Daniel McDuff",
        "Hyeonhoon Lee",
        "Cynthia Breazeal",
        "Hae Won Park"
      ],
      "abstract": "Vocal health plays a crucial role in peoples' lives, significantly impacting their communicative abilities and interactions. However, despite the global prevalence of voice disorders, many lack access to convenient diagnosis and treatment. This paper introduces VocalAgent, an audio large language model (LLM) to address these challenges through vocal health diagnosis. We leverage Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital patients, and present a multifaceted evaluation framework encompassing a safety assessment to mitigate diagnostic biases, cross-lingual performance analysis, and modality ablation studies. VocalAgent demonstrates superior accuracy on voice disorder classification compared to state-of-the-art baselines. Its LLM-based method offers a scalable solution for broader adoption of health diagnostics, while underscoring the importance of ethical and technical validation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VocalAgentï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å£°å¸¦å¥åº·è¯Šæ–­ä¾¿æ·æ€§ä¸è¶³é—®é¢˜çš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ (Audio LLM)ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨åœ¨åŒ»é™¢å®åœ°æ”¶é›†çš„ä¸‰ä¸ªæ•°æ®é›†å¯¹ Qwen-Audio-Chat è¿›è¡Œå¾®è°ƒï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«å®‰å…¨æ€§è¯„ä¼° (Safety Assessment) ä»¥å‡è½»è¯Šæ–­åè§çš„å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚è¯„ä¼°è¿‡ç¨‹è¿˜æ¶µç›–äº†è·¨è¯­è¨€æ€§èƒ½åˆ†æåŠæ¨¡æ€æ¶ˆèç ”ç©¶ (Modality Ablation Studies)ï¼Œå…¨é¢éªŒè¯äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVocalAgent åœ¨å£°å¸¦ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚è¿™ç§åŸºäº LLM çš„è¯Šæ–­æ–¹æ³•ä¸ºå£°å¸¦å¥åº·é¢†åŸŸæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼ºè°ƒäº†åœ¨åŒ»ç–— AI é¢†åŸŸè¿›è¡Œä¼¦ç†ä¸æŠ€æœ¯åŒé‡éªŒè¯çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by Proceedings of Interspeech 2025; Website: https://han811.github.io/VocalAgent2025/",
      "pdf_url": "https://arxiv.org/pdf/2505.13577v3",
      "published_date": "2025-05-19 14:58:42 UTC",
      "updated_date": "2025-09-25 23:01:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:50.523762+00:00"
    },
    {
      "arxiv_id": "2505.13208v1",
      "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts",
      "title_zh": "å¤§è§„æ¨¡æ–‡æœ¬å‚æ•°åŒ–é‡å­çº¿è·¯çš„é«˜æ•ˆç”Ÿæˆ",
      "authors": [
        "Colin Krawchuk",
        "Nikhil Khatri",
        "Neil John Ortega",
        "Dimitri Kartsaklis"
      ],
      "abstract": "Quantum approaches to natural language processing (NLP) are redefining how linguistic information is represented and processed. While traditional hybrid quantum-classical models rely heavily on classical neural networks, recent advancements propose a novel framework, DisCoCirc, capable of directly encoding entire documents as parameterised quantum circuits (PQCs), besides enjoying some additional interpretability and compositionality benefits. Following these ideas, this paper introduces an efficient methodology for converting large-scale texts into quantum circuits using tree-like representations of pregroup diagrams. Exploiting the compositional parallels between language and quantum mechanics, grounded in symmetric monoidal categories, our approach enables faithful and efficient encoding of syntactic and discourse relationships in long and complex texts (up to 6410 words in our experiments) to quantum circuits. The developed system is provided to the community as part of the augmented open-source quantum NLP package lambeq Gen II.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»å¤§è§„æ¨¡æ–‡æœ¬ä¸­é«˜æ•ˆç”Ÿæˆå‚æ•°åŒ–é‡å­ç”µè·¯(Parameterised Quantum Circuits, PQCs)çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡é‡å­è‡ªç„¶è¯­è¨€å¤„ç†(Quantum NLP)çš„è¡¨ç¤ºä¸å¤„ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†DisCoCircæ¡†æ¶ï¼Œåˆ©ç”¨é¢„ç¾¤å›¾(Pregroup Diagrams)çš„æ ‘çŠ¶è¡¨ç¤ºï¼Œå¹¶åŸºäºå¯¹ç§°å•å­èŒƒç•´(Symmetric Monoidal Categories)æŒ–æ˜è¯­è¨€ä¸é‡å­åŠ›å­¦ä¹‹é—´çš„ç»„åˆç›¸ä¼¼æ€§ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å¯¹é•¿ç¯‡ä¸”å¤æ‚æ–‡æœ¬ä¸­è¯­æ³•å’Œè¯­ç¯‡å…³ç³»çš„å¿ å®ç¼–ç ï¼Œå®éªŒä¸­æˆåŠŸå¤„ç†äº†é•¿è¾¾6410ä¸ªå•è¯çš„æ–‡æœ¬ã€‚è¯¥ç ”ç©¶å¼€å‘çš„ç³»ç»Ÿå·²æ•´åˆè¿›å¢å¼ºç‰ˆå¼€æºé‡å­NLPè½¯ä»¶åŒ…lambeq Gen IIä¸­ï¼Œä¸ºå­¦æœ¯ç•Œæä¾›äº†å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬é‡å­åŒ–ç¼–ç çš„æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13208v1",
      "published_date": "2025-05-19 14:57:53 UTC",
      "updated_date": "2025-05-19 14:57:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:44:34.314555+00:00"
    },
    {
      "arxiv_id": "2505.13201v1",
      "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects",
      "title_zh": "MatPredictï¼šé¢å‘å¤šæ ·åŒ–å®¤å†…ç‰©ä½“æè´¨å±æ€§å­¦ä¹ çš„æ•°æ®é›†ä¸åŸºå‡†",
      "authors": [
        "Yuzhen Chen",
        "Hojun Son",
        "Arpan Kusari"
      ],
      "abstract": "Determining material properties from camera images can expand the ability to identify complex objects in indoor environments, which is valuable for consumer robotics applications. To support this, we introduce MatPredict, a dataset that combines the high-quality synthetic objects from Replica dataset with MatSynth dataset's material properties classes - to create objects with diverse material properties. We select 3D meshes of specific foreground objects and render them with different material properties. In total, we generate \\textbf{18} commonly occurring objects with \\textbf{14} different materials. We showcase how we provide variability in terms of lighting and camera placement for these objects. Next, we provide a benchmark for inferring material properties from visual images using these perturbed models in the scene, discussing the specific neural network models involved and their performance based on different image comparison metrics. By accurately simulating light interactions with different materials, we can enhance realism, which is crucial for training models effectively through large-scale simulations. This research aims to revolutionize perception in consumer robotics. The dataset is provided \\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is provided \\href{https://github.com/arpan-kusari/MatPredict}{here}.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MatPredictï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å­¦ä¹ å¤šç§å®¤å†…ç‰©ä½“æè´¨å±æ€§çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æå‡ consumer robotics é¢†åŸŸè¯†åˆ«å¤æ‚ç‰©ä½“çš„èƒ½åŠ›ã€‚MatPredict é€šè¿‡ç»“åˆ Replica æ•°æ®é›†çš„é«˜è´¨é‡åˆæˆç‰©ä½“ä¸ MatSynth æ•°æ®é›†çš„æè´¨å±æ€§ç±»åˆ«ï¼Œåˆ›å»ºäº†å…·æœ‰å¤šæ ·åŒ–æè´¨ç‰¹å¾çš„ç‰©ä½“æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿé€‰å–äº†ç‰¹å®šå‰æ™¯ç‰©ä½“çš„ 3D meshesï¼Œç”Ÿæˆäº†18ç§å¸¸è§ç‰©ä½“å¹¶ä¸ºå…¶é…ç½®äº†14ç§ä¸åŒçš„æè´¨ï¼ŒåŒæ—¶åœ¨å…‰ç…§å’Œç›¸æœºä½ç½®æ–¹é¢æä¾›äº†ä¸°å¯Œçš„ variabilityã€‚è®ºæ–‡è¿˜æä¾›äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†ä¸åŒç¥ç»ç½‘ç»œæ¨¡å‹ä»è§†è§‰å›¾åƒä¸­æ¨æ–­æè´¨å±æ€§çš„æ€§èƒ½ã€‚é€šè¿‡ç²¾ç¡®æ¨¡æ‹Ÿå…‰çº¿ä¸ä¸åŒæè´¨çš„ç›¸äº’ä½œç”¨ï¼Œè¯¥ç ”ç©¶æ˜¾è‘—å¢å¼ºäº†ä»¿çœŸç¯å¢ƒçš„çœŸå®æ„Ÿï¼Œè¿™å¯¹äºé€šè¿‡å¤§è§„æ¨¡æ¨¡æ‹Ÿè¿›è¡Œæœ‰æ•ˆçš„æ¨¡å‹è®­ç»ƒè‡³å…³é‡è¦ã€‚è¿™é¡¹ç ”ç©¶æ—¨åœ¨å½»åº•å˜é© consumer robotics çš„æ„ŸçŸ¥ç³»ç»Ÿï¼Œä¸ºå®ç°æ›´æ™ºèƒ½çš„å®¤å†…ç¯å¢ƒäº¤äº’å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13201v1",
      "published_date": "2025-05-19 14:54:04 UTC",
      "updated_date": "2025-05-19 14:54:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:10.378090+00:00"
    },
    {
      "arxiv_id": "2505.13196v2",
      "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam",
      "title_zh": "å—ç‰©ç†å¯å‘çš„ä¼˜åŒ–å™¨ï¼šé€Ÿåº¦æ­£åˆ™åŒ– Adam",
      "authors": [
        "Pranav Vaidhyanathan",
        "Lucas Schorling",
        "Natalia Ares",
        "Michael A. Osborne"
      ],
      "abstract": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer for training deep neural networks that draws on ideas from quartic terms for kinetic energy with its stabilizing effects on various system dynamics. Previous algorithms, including the ubiquitous Adam, operate at the so-called adaptive edge of stability regime during training, leading to rapid oscillations and slowed convergence of loss. However, VRAdam adds a higher order penalty on the learning rate based on the velocity such that the algorithm automatically slows down whenever weight updates become large. In practice, we observe that the effective dynamic learning rate shrinks in high-velocity regimes, and damping oscillations. By combining this velocity-based regularizer for global damping with per-parameter scaling of Adam, we create a powerful hybrid optimizer. For this optimizer, we provide rigorous theoretical analysis of operation at the edge of stability from a physical and control perspective for the momentum. Furthermore, we derive convergence bounds with the rate $\\mathcal{O}(\\ln(N)/\\sqrt{N})$ for a stochastic non convex objective under mild assumptions. We demonstrate that VRAdam exceeds the performance against standard optimizers including AdamW. We benchmark various tasks such as image classification, language modeling, and generative modeling using diverse architectures and training methodologies including Convolutional Neural Networks (CNNs), Transformers, and GFlowNets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Velocity-Regularized Adam (VRAdam)ï¼Œè¿™æ˜¯ä¸€ç§å—ç‰©ç†å­¦å¯å‘çš„æ·±åº¦ç¥ç»ç½‘ç»œä¼˜åŒ–å™¨ï¼Œå€Ÿé‰´äº†åŠ¨èƒ½å››æ¬¡é¡¹åœ¨ç¨³å®šç³»ç»ŸåŠ¨åŠ›å­¦æ–¹é¢çš„ä½œç”¨ã€‚é’ˆå¯¹Adamç­‰ä¼ ç»Ÿç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å› å¤„äºç¨³å®šæ€§è¾¹ç¼˜(edge of stability)è€Œå¯¼è‡´çš„å‰§çƒˆæŒ¯è¡å’Œæ”¶æ•›ç¼“æ…¢é—®é¢˜ï¼ŒVRAdamé€šè¿‡å¼•å…¥åŸºäºé€Ÿåº¦(velocity)çš„é«˜é˜¶æƒ©ç½šé¡¹æ¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚å½“æƒé‡æ›´æ–°è¿‡å¤§æ—¶ï¼Œè¯¥ç®—æ³•ä¼šè‡ªåŠ¨é™ä½æœ‰æ•ˆå­¦ä¹ ç‡ï¼Œä»è€Œåœ¨é«˜é€Ÿåº¦åŒºé—´å®ç°å…¨å±€é˜»å°¼(damping)æ•ˆæœã€‚ç ”ç©¶è€…ä»ç‰©ç†å’Œæ§åˆ¶ç†è®ºè§’åº¦å¯¹åŠ¨é‡ç¨³å®šæ€§è¿›è¡Œäº†ä¸¥è°¨åˆ†æï¼Œå¹¶è¯æ˜äº†VRAdamåœ¨éšæœºéå‡¸ç›®æ ‡ä¸‹å…·æœ‰$\\mathcal{O}(\\ln(N)/\\sqrt{N})$çš„æ”¶æ•›ç•Œé™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVRAdamåœ¨å›¾åƒåˆ†ç±»ã€è¯­è¨€å»ºæ¨¡å’Œç”Ÿæˆæ¨¡å‹ç­‰å¤šç§ä»»åŠ¡ä¸­å‡ä¼˜äºAdamWç­‰ä¸»æµä¼˜åŒ–å™¨ï¼Œé€‚ç”¨äºConvolutional Neural Networks (CNNs)ã€Transformerså’ŒGFlowNetsç­‰å¤šç§æ¶æ„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work. 20 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13196v2",
      "published_date": "2025-05-19 14:51:40 UTC",
      "updated_date": "2025-10-01 00:53:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:10.634973+00:00"
    },
    {
      "arxiv_id": "2505.13195v1",
      "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
      "title_zh": "LLM å¯¹æŠ—æ€§æµ‹è¯•ï¼šå†³ç­–è„†å¼±æ€§æ·±åº¦è§£æ",
      "authors": [
        "Lili Zhang",
        "Haomiaomiao Wang",
        "Long Cheng",
        "Libao Deng",
        "Tomas Ward"
      ],
      "abstract": "As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment. While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments. This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions. Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility. We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation. Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment. Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ Large Language Models (LLMs) çš„å¯¹æŠ—æ€§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°å‹åŠ›æµ‹è¯•æ¨¡å‹åœ¨äº¤äº’ä¸å¯¹æŠ—ç¯å¢ƒä¸‹çš„å†³ç­–è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å€Ÿé‰´è®¤çŸ¥å¿ƒç†å­¦å’Œåšå¼ˆè®ºï¼Œåˆ©ç”¨ two-armed bandit task å’Œ Multi-Round Trust Task è€ƒå¯Ÿæ¨¡å‹åœ¨ exploration-exploitation æƒè¡¡ã€ç¤¾ä¼šåˆä½œåŠæˆ˜ç•¥çµæ´»æ€§æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å¯¹ GPT-3.5ã€GPT-4ã€Gemini-1.5 å’Œ DeepSeek-V3 ç­‰å‰æ²¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨ç­–ç•¥é€‚åº”ä¸Šçš„åƒµåŒ–æ€§ä»¥åŠé¢å¯¹æ“çºµæ—¶çš„ç‰¹å®šæ˜“æ„Ÿæ€§ã€‚å®éªŒå‘ç°å¼ºè°ƒäº†æ¨¡å‹é—´æ˜¾è‘—çš„è¡Œä¸ºå·®å¼‚ï¼Œå¹¶æŒ‡å‡ºé€‚åº”æ€§å’Œå…¬å¹³æ€§è¯†åˆ«å¯¹äºç¡®ä¿ AI å®‰å…¨éƒ¨ç½²çš„é‡è¦æ€§ã€‚è¯¥å·¥ä½œä¸ºè¯Šæ–­åŸºäº LLM çš„æ™ºèƒ½ä½“çš„å†³ç­–æ¼æ´æä¾›äº†ä¸€å¥—æ–¹æ³•è®ºï¼Œä¸º AI alignment å’Œ safety é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†å®è·µæ´å¯Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13195v1",
      "published_date": "2025-05-19 14:50:44 UTC",
      "updated_date": "2025-05-19 14:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:27.946700+00:00"
    },
    {
      "arxiv_id": "2505.13192v2",
      "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics",
      "title_zh": "ä¿æŒé•¿æœŸç»Ÿè®¡ç‰¹æ€§çš„åŠ¨åŠ›ç³»ç»ŸçœŸé›¶æ ·æœ¬æ¨ç†",
      "authors": [
        "Christoph JÃ¼rgen Hemmer",
        "Daniel Durstewitz"
      ],
      "abstract": "Complex, temporally evolving phenomena, from climate to brain activity, are governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer generative surrogate models of these from observed data, reproducing their long-term behavior. Existing DSR approaches require purpose-training for any new system observed, lacking the zero-shot and in-context inference capabilities known from LLMs. Here we introduce DynaMix, a novel multivariate ALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR model able to generalize zero-shot to out-of-domain DS. Just from a provided context signal, without any re-training, DynaMix faithfully forecasts the long-term evolution of novel DS where existing time series (TS) foundation models, like Chronos, fail -- at a fraction of the number of parameters (0.1%) and orders of magnitude faster inference times. DynaMix outperforms TS foundation models in terms of long-term statistics, and often also short-term forecasts, even on real-world time series, like traffic or weather data, typically used for training and evaluating TS models, but not at all part of DynaMix' training corpus. We illustrate some of the failure modes of TS models for DSR problems, and conclude that models built on DS principles may bear a huge potential also for advancing the TS prediction field.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨åŠ›ç³»ç»Ÿ(Dynamical Systems, DS)é‡å»ºä¸­ç°æœ‰æ–¹æ³•ç¼ºä¹Zero-Shotå’ŒIn-contextæ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªèƒ½å¤Ÿå®ç°è·¨åŸŸæ³›åŒ–çš„é‡å»ºæ¨¡å‹DynaMixã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºå¤šå˜é‡ALRNNçš„æ··åˆä¸“å®¶æ¶æ„(Mixture-of-Experts)ï¼Œä»…é€šè¿‡æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡å·å³å¯åœ¨ä¸è¿›è¡Œä»»ä½•é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¿ å®åœ°é¢„æµ‹æœªçŸ¥ç³»ç»Ÿçš„é•¿æœŸæ¼”åŒ–è¿‡ç¨‹ã€‚ä¸Chronosç­‰ä¸»æµæ—¶é—´åºåˆ—(Time Series, TS)å¤§æ¨¡å‹ç›¸æ¯”ï¼ŒDynaMixåœ¨å‚æ•°é‡ä»…ä¸º0.1%ä¸”æ¨ç†é€Ÿåº¦å¤§å¹…æå‡çš„å‰æä¸‹ï¼Œå±•ç°å‡ºæ›´ä¼˜çš„é•¿çŸ­æœŸé¢„æµ‹ç²¾åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ•æ‰é•¿æœŸç»Ÿè®¡ç‰¹æ€§(Long-term statistics)æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå³ä¾¿åœ¨äº¤é€šå’Œå¤©æ°”ç­‰ä¸å±äºå…¶è®­ç»ƒè¯­æ–™çš„çœŸå®ä¸–ç•Œæ•°æ®ä¸Šä¹Ÿä¼˜äºç°æœ‰çš„TSåŸºç¡€æ¨¡å‹ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ä¼ ç»Ÿæ—¶é—´åºåˆ—æ¨¡å‹åœ¨å¤„ç†åŠ¨åŠ›ç³»ç»Ÿé‡å»º(DSR)é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œè¯æ˜äº†åŸºäºåŠ¨åŠ›ç³»ç»ŸåŸç†æ„å»ºçš„æ¨¡å‹åœ¨é¢„æµ‹é¢†åŸŸå…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS",
        "nlin.CD"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13192v2",
      "published_date": "2025-05-19 14:49:10 UTC",
      "updated_date": "2025-10-24 15:51:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:21.565741+00:00"
    },
    {
      "arxiv_id": "2505.13191v2",
      "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision",
      "title_zh": "è§†è§‰å¤šçº§å¾ªç¯æ³¨æ„åŠ›æ¨¡å‹ä¸­æ³¨è§†ä¸æ‰«è§†è¿åŠ¨çš„æ¶Œç°",
      "authors": [
        "Pengcheng Pan",
        "Yonekura Shogo",
        "Yasuo Kuniyoshi"
      ],
      "abstract": "Inspired by foveal vision, hard attention models promise interpretability and parameter economy. However, existing models like the Recurrent Model of Visual Attention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the hierarchy of human vision system, that compromise on the visual exploration dynamics. As a result, they tend to produce attention that are either overly fixational or excessively saccadic, diverging from human eye movement behavior. In this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a novel hard attention framework that explicitly models the neural hierarchy of human visual processing. By decoupling the function of glimpse location generation and task execution in two recurrent layers, MRAM emergent a balanced behavior between fixation and saccadic movement. Our results show that MRAM not only achieves more human-like attention dynamics, but also consistently outperforms CNN, RAM and DRAM baselines on standard image classification benchmarks.",
      "tldr_zh": "å—åˆ°ä¸­å¤®å‡¹è§†è§‰(foveal vision)çš„å¯å‘ï¼Œç¡¬æ³¨æ„æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ä¸å‚æ•°ç»æµæ€§ï¼Œä½†ç°æœ‰çš„RAMå’ŒDRAMæ¨¡å‹å› ç¼ºä¹äººç±»è§†è§‰ç³»ç»Ÿçš„å±‚æ¬¡åŒ–å»ºæ¨¡ï¼Œå¯¼è‡´å…¶ç”Ÿæˆçš„æ³¨æ„åŠ›å¾€å¾€åœ¨è¿‡åº¦æ³¨è§†(fixational)æˆ–è¿‡åº¦æ‰«è§†(saccadic)ä¹‹é—´å¤±è¡¡ã€‚æœ¬æ–‡æå‡ºäº†å¤šå±‚æ¬¡å¾ªç¯æ³¨æ„åŠ›æ¨¡å‹(Multi-Level Recurrent Attention Model, MRAM)ï¼Œè¿™æ˜¯ä¸€ç§æ˜¾å¼å»ºæ¨¡äººç±»è§†è§‰å¤„ç†ç¥ç»å±‚æ¬¡ç»“æ„çš„æ–°å‹ç¡¬æ³¨æ„æ¡†æ¶ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªå¾ªç¯å±‚ä¸­è§£è€¦æ‰«è§†ä½ç½®ç”Ÿæˆä¸ä»»åŠ¡æ‰§è¡Œçš„åŠŸèƒ½ï¼ŒMRAMèƒ½å¤Ÿè‡ªå‘äº§ç”Ÿæ³¨è§†ä¸æ‰«è§†è¿åŠ¨ä¹‹é—´çš„å¹³è¡¡è¡Œä¸ºï¼Œæ›´çœŸå®åœ°æ¨¡æ‹Ÿäººç±»çœ¼åŠ¨è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMRAMä¸ä»…å®ç°äº†æ›´æ¥è¿‘äººç±»çš„æ³¨æ„åŠ›åŠ¨æ€ï¼Œä¸”åœ¨æ ‡å‡†å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºCNNã€RAMåŠDRAMç­‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆåŠŸæ­ç¤ºäº†è§†è§‰å¤„ç†å±‚æ¬¡ç»“æ„å¯¹äºäº§ç”Ÿå¤æ‚æ¢ç´¢æ€§æ³¨æ„åŠ›æ¨¡å¼çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆä¸”ç±»äººçš„è§†è§‰æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13191v2",
      "published_date": "2025-05-19 14:48:36 UTC",
      "updated_date": "2025-11-17 13:11:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:25.800008+00:00"
    },
    {
      "arxiv_id": "2505.13188v2",
      "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns",
      "title_zh": "å½“å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“é­é‡â€œæœªçŸ¥çš„æœªçŸ¥â€",
      "authors": [
        "Juntian Zhu",
        "Miguel de Carvalho",
        "Zhouwang Yang",
        "Fengxiang He"
      ],
      "abstract": "An AI agent might surprisingly find she has reached an unknown state which she has never been aware of -- an unknown unknown. We mathematically ground this scenario in reinforcement learning: an agent, after taking an action calculated from value functions $Q$ and $V$ defined on the {\\it {aware domain}}, reaches a state out of the domain. To enable the agent to handle this scenario, we propose an {\\it episodic Markov decision {process} with growing awareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion} (NIVE) approach to expand value functions to newly aware areas: when an agent arrives at an unknown unknown, value functions $Q$ and $V$ whereon are initialised by noninformative beliefs -- the averaged values on the aware domain. This design is out of respect for the complete absence of knowledge in the newly discovered state. The upper confidence bound momentum Q-learning is then adapted to the growing awareness for training the EMDP-GA model. We prove that (1) the regret of our approach is asymptotically consistent with the state of the art (SOTA) without exposure to unknown unknowns in an extremely uncertain environment, and (2) our computational complexity and space complexity are comparable with the SOTA -- these collectively suggest that though an unknown unknown is surprising, it will be asymptotically properly discovered with decent speed and an affordable cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“å¯èƒ½é‡åˆ°çš„â€œæœªçŸ¥æœªçŸ¥â€(unknown unknowns)åœºæ™¯ï¼Œå³æ™ºèƒ½ä½“åœ¨æ‰§è¡ŒåŠ¨ä½œååˆ°è¾¾äº†å…¶åˆå§‹è®¤çŸ¥é¢†åŸŸ(aware domain)ä¹‹å¤–çš„çŠ¶æ€ï¼Œè¿›è¡Œäº†æ•°å­¦åŒ–å»ºæ¨¡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å…·æœ‰å¢é•¿è®¤çŸ¥(growing awareness)çš„ç‰‡æ®µå¼é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(EMDP-GA)æ¨¡å‹ï¼Œå¹¶å¼•å…¥éä¿¡æ¯å€¼æ‰©å±•(Noninformative Value Expansion, NIVE)æ–¹æ³•ï¼Œå°†æ–°å‘ç°çŠ¶æ€çš„ä»·å€¼å‡½æ•° $Q$ å’Œ $V$ åˆå§‹åŒ–ä¸ºå·²çŸ¥é¢†åŸŸçš„å¹³å‡å€¼ï¼Œä»¥å°Šé‡å¯¹æ–°é¢†åŸŸçš„çŸ¥è¯†ç¼ºå¤±ã€‚è¯¥æ¡†æ¶æ”¹è¿›äº†ä¸Šç½®ä¿¡ç•Œ(Upper Confidence Bound)åŠ¨é‡ Q-learning ç®—æ³•ä»¥é€‚åº”è®¤çŸ¥çš„åŠ¨æ€å¢é•¿ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æç«¯ä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç´¯ç§¯é—æ†¾(regret)ä¸å½“å‰æœ€ä¼˜æŠ€æœ¯(SOTA)ä¿æŒæ¸è¿‘ä¸€è‡´ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå°½ç®¡â€œæœªçŸ¥æœªçŸ¥â€å…·æœ‰çªå‘æ€§ï¼Œä½†è¯¥æ–¹æ³•èƒ½ä»¥ä¸ SOTA ç›¸å½“çš„è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦ï¼Œå®ç°å¯¹æ–°çŠ¶æ€çš„é«˜æ•ˆæ¢ç´¢ä¸å­¦ä¹ ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13188v2",
      "published_date": "2025-05-19 14:45:58 UTC",
      "updated_date": "2025-09-03 11:20:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:26.323967+00:00"
    },
    {
      "arxiv_id": "2505.13182v13",
      "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping",
      "title_zh": "æœºå™¨å­¦ä¹ çš„ä¿¡æ¯ç§‘å­¦åŸç†ï¼šåŸºäºå½¢å¼åŒ–ä¿¡æ¯æ˜ å°„çš„å› æœé“¾å…ƒæ¡†æ¶",
      "authors": [
        "Jianfeng Xu"
      ],
      "abstract": "This paper addresses the current lack of a unified formal framework in machine learning theory, as well as the absence of robust theoretical foundations for interpretability and ethical safety assurance. We first construct a formal information model, employing sets of well-formed formulas (WFFs) to explicitly define the ontological states and carrier mappings for the core components of machine learning. By introducing learnable and processable predicates, as well as learning and processing functions, we analyze the logical inference and constraint rules underlying causal chains in models, thereby establishing the Machine Learning Theory Meta-Framework (MLT-MF). Building upon this framework, we propose universal definitions for model interpretability and ethical safety, and rigorously prove and validate four key theorems: the equivalence between model interpretability and information existence, the constructive formulation of ethical safety assurance and two types of total variation distance (TVD) upper bounds. This work overcomes the limitations of previous fragmented approaches, providing a unified theoretical foundation from an information science perspective to systematically address the critical challenges currently facing machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ç†è®ºä¸­ç¼ºä¹ç»Ÿä¸€å½¢å¼åŒ–æ¡†æ¶ï¼Œä»¥åŠå¯è§£é‡Šæ€§(interpretability)å’Œä¼¦ç†å®‰å…¨ä¿éšœ(ethical safety assurance)ç¼ºä¹åšå®ç†è®ºåŸºç¡€çš„é—®é¢˜å±•å¼€æ¢è®¨ã€‚ä½œè€…åˆ©ç”¨åˆå¼å…¬å¼(WFFs)é›†åˆæ˜¾å¼å®šä¹‰äº†æœºå™¨å­¦ä¹ æ ¸å¿ƒç»„ä»¶çš„æœ¬ä½“çŠ¶æ€ä¸è½½ä½“æ˜ å°„ï¼Œæ„å»ºäº†ä¸€ä¸ªå½¢å¼åŒ–çš„ä¿¡æ¯æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ ä¸å¯å¤„ç†è°“è¯åŠç›¸å…³å‡½æ•°ï¼Œè¯¥ç ”ç©¶åˆ†æäº†æ¨¡å‹å› æœé“¾èƒŒåçš„é€»è¾‘æ¨ç†è§„åˆ™ï¼Œä»è€Œå»ºç«‹äº†æœºå™¨å­¦ä¹ ç†è®ºå…ƒæ¡†æ¶(MLT-MF)ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œè®ºæ–‡æå‡ºäº†æ¨¡å‹å¯è§£é‡Šæ€§ä¸ä¼¦ç†å®‰å…¨çš„é€šç”¨å®šä¹‰ï¼Œå¹¶ä¸¥è°¨è¯æ˜äº†æ¨¡å‹å¯è§£é‡Šæ€§ä¸ä¿¡æ¯å­˜åœ¨æ€§çš„ç­‰ä»·æ€§ã€ä¼¦ç†å®‰å…¨ä¿éšœçš„æ„é€ æ€§è¡¨è¿°ä»¥åŠä¸¤ç§å…¨å˜å·®è·ç¦»(TVD)çš„ä¸Šç•Œã€‚è¿™é¡¹å·¥ä½œå…‹æœäº†ä»¥å¾€ç¢ç‰‡åŒ–ç ”ç©¶æ–¹æ³•çš„å±€é™æ€§ï¼Œä»ä¿¡æ¯ç§‘å­¦(information science)çš„è§†è§’ä¸ºç³»ç»Ÿæ€§è§£å†³æœºå™¨å­¦ä¹ å½“å‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æä¾›äº†ç»Ÿä¸€çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13182v13",
      "published_date": "2025-05-19 14:39:41 UTC",
      "updated_date": "2025-11-08 04:33:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:45.795204+00:00"
    },
    {
      "arxiv_id": "2505.13180v1",
      "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models",
      "title_zh": "ViPlanï¼šåŸºäºç¬¦å·è°“è¯ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰è§„åˆ’åŸºå‡†",
      "authors": [
        "Matteo Merler",
        "Nicola Dainese",
        "Minttu Alakuijala",
        "Giovanni Bonetta",
        "Pietro Ferrazzi",
        "Yu Tian",
        "Bernardo Magnini",
        "Pekka Marttinen"
      ],
      "abstract": "Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ViPlanï¼Œè¿™æ˜¯é¦–ä¸ªç»“åˆç¬¦å·è°“è¯ (Symbolic Predicates) ä¸è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) è¿›è¡Œè§†è§‰è§„åˆ’çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚ViPlan æ¶µç›–äº†ç»å…¸çš„ Blocksworld è§†è§‰å˜ä½“å’Œæ¨¡æ‹Ÿå®¶åº­æœºå™¨äººç¯å¢ƒä¸¤ç±»å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†è§‰è§„åˆ’é¢†åŸŸç¼ºä¹ç»Ÿä¸€è¯„ä¼°åè®®çš„é—®é¢˜ã€‚é€šè¿‡å¯¹ä¹ä¸ªå¼€æº VLM ç³»åˆ—åŠéƒ¨åˆ†é—­æºæ¨¡å‹çš„æµ‹è¯•ï¼Œç ”ç©¶å¯¹æ¯”äº† VLM é©±åŠ¨çš„ç¬¦å·è§„åˆ’ä¸ç›´æ¥åŠ¨ä½œç”Ÿæˆä¸¤ç§æ–¹æ³•çš„æ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç¬¦å·è§„åˆ’åœ¨éœ€è¦ç²¾ç¡®å›¾åƒåŸºå‡† (Image Grounding) çš„ Blocksworld ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ï¼Œè€Œç›´æ¥è§„åˆ’åœ¨æ›´ä¾èµ–å¸¸è¯†æ¨ç†å’Œé”™è¯¯æ¢å¤èƒ½åŠ›çš„æœºå™¨äººä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é“¾å¼æ€ç»´ (Chain-of-Thought) æç¤ºåœ¨å¤§å¤šæ•°æ¨¡å‹ä¸­å¹¶æœªå¸¦æ¥æ˜¾è‘—æ”¶ç›Šï¼Œè¿™è¡¨æ˜å½“å‰çš„ VLMs åœ¨å¤æ‚è§†è§‰æ¨ç†æ–¹é¢ä»é¢ä¸´è¾ƒå¤§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures and 16 tables including supplementary material",
      "pdf_url": "https://arxiv.org/pdf/2505.13180v1",
      "published_date": "2025-05-19 14:38:15 UTC",
      "updated_date": "2025-05-19 14:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:40.039283+00:00"
    },
    {
      "arxiv_id": "2505.13176v2",
      "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models",
      "title_zh": "ToolSpectrumï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–å·¥å…·åˆ©ç”¨",
      "authors": [
        "Zihao Cheng",
        "Hongru Wang",
        "Zeming Liu",
        "Yuhang Guo",
        "Yuanfang Guo",
        "Yunhong Wang",
        "Haifeng Wang"
      ],
      "abstract": "While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at https://github.com/Chengziha0/ToolSpectrum.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶å°†å¤–éƒ¨å·¥å…·é›†æˆåˆ°å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­æå‡äº†å…¶åŠŸèƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†å·¥å…·é€‰æ‹©ä¸­çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸ªæ€§åŒ–ï¼Œå¯¼è‡´ç”¨æˆ·ä½“éªŒä¸ä½³ä¸”å·¥å…·åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† ToolSpectrum åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼° LLMs åœ¨ä¸ªæ€§åŒ–å·¥å…·åˆ©ç”¨æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡å½¢å¼åŒ–ç”¨æˆ·ç”»åƒ (User Profile) å’Œç¯å¢ƒå› ç´  (Environmental Factors) ä¸¤ä¸ªç»´åº¦ï¼Œç³»ç»Ÿåˆ†æäº†å®ƒä»¬å¯¹å·¥å…·åˆ©ç”¨çš„ç‹¬ç«‹åŠååŒå½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ªæ€§åŒ–å·¥å…·åˆ©ç”¨èƒ½æ˜¾è‘—æ”¹å–„å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„ç”¨æˆ·ä½“éªŒï¼Œä½†ç›®å‰çš„é¡¶çº§ LLMs åœ¨å¤„ç†ç”¨æˆ·ç”»åƒä¸ç¯å¢ƒå› ç´ çš„è”åˆæ¨ç†æ–¹é¢è¡¨ç°æœ‰é™ï¼Œå¾€å¾€é¡¾æ­¤å¤±å½¼ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨å·¥å…·å¢å¼ºå‹ LLMs ä¸­å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸ªæ€§åŒ–çš„å¿…è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚å†³ç­–ä¸­çš„å…³é”®å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.13176v2",
      "published_date": "2025-05-19 14:30:46 UTC",
      "updated_date": "2025-05-22 14:08:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:59.642811+00:00"
    },
    {
      "arxiv_id": "2505.13175v1",
      "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment",
      "title_zh": "é€šè¿‡ç»“æ„å¼•å¯¼çš„è·¨æ¨¡æ€å¯¹é½å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Siming Sun",
        "Kai Zhang",
        "Xuejun Jiang",
        "Wenchao Meng",
        "Qinmin Yang"
      ],
      "abstract": "The emerging paradigm of leveraging pretrained large language models (LLMs) for time series forecasting has predominantly employed linguistic-temporal modality alignment strategies through token-level or layer-wise feature mapping. However, these approaches fundamentally neglect a critical insight: the core competency of LLMs resides not merely in processing localized token features but in their inherent capacity to model holistic sequence structures. This paper posits that effective cross-modal alignment necessitates structural consistency at the sequence level. We propose the Structure-Guided Cross-Modal Alignment (SGCMA), a framework that fully exploits and aligns the state-transition graph structures shared by time-series and linguistic data as sequential modalities, thereby endowing time series with language-like properties and delivering stronger generalization after modality alignment. SGCMA consists of two key components, namely Structure Alignment and Semantic Alignment. In Structure Alignment, a state transition matrix is learned from text data through Hidden Markov Models (HMMs), and a shallow transformer-based Maximum Entropy Markov Model (MEMM) receives the hot-start transition matrix and annotates each temporal patch into state probability, ensuring that the temporal representation sequence inherits language-like sequential dynamics. In Semantic Alignment, cross-attention is applied between temporal patches and the top-k tokens within each state, and the ultimate temporal embeddings are derived by the expected value of these embeddings using a weighted average based on state probabilities. Experiments on multiple benchmarks demonstrate that SGCMA achieves state-of-the-art performance, offering a novel approach to cross-modal alignment in time series forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç»“æ„å¼•å¯¼è·¨æ¨¡æ€å¯¹é½(Structure-Guided Cross-Modal Alignment, SGCMA)æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å›ºæœ‰çš„åºåˆ—ç»“æ„å»ºæ¨¡èƒ½åŠ›æå‡æ—¶é—´åºåˆ—é¢„æµ‹(Time Series Forecasting)çš„ç²¾åº¦ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä»…å…³æ³¨å±€éƒ¨ç‰¹å¾å¯¹é½è€Œå¿½è§†åºåˆ—å±‚é¢ç»“æ„ä¸€è‡´æ€§çš„å±€é™ï¼ŒSGCMAé€šè¿‡å¯¹é½æ—¶é—´åºåˆ—ä¸æ–‡æœ¬æ•°æ®å…±äº«çš„çŠ¶æ€è½¬ç§»å›¾ç»“æ„(state-transition graph structures)ï¼Œä½¿æ—¶é—´åºåˆ—å…·å¤‡ç±»ä¼¼è¯­è¨€çš„åºåˆ—åŠ¨æ€ç‰¹æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ç»“æ„å¯¹é½(Structure Alignment)å’Œè¯­ä¹‰å¯¹é½(Semantic Alignment)ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå‰è€…ç»“åˆéšé©¬å°”å¯å¤«æ¨¡å‹(HMMs)ä¸æœ€å¤§ç†µé©¬å°”å¯å¤«æ¨¡å‹(MEMM)ç¡®ä¿æ—¶é—´è¡¨å¾ç»§æ‰¿è¯­è¨€åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œåè€…åˆ™é€šè¿‡äº¤å‰æ³¨æ„åŠ›(cross-attention)æœºåˆ¶å®ç°æ—¶é—´ç‰‡æ®µä¸ç‰¹å®šçŠ¶æ€æ ‡è®°(tokens)çš„è¯­ä¹‰æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGCMAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºè·¨æ¨¡æ€å¯¹é½åœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸçš„åº”ç”¨æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13175v1",
      "published_date": "2025-05-19 14:30:41 UTC",
      "updated_date": "2025-05-19 14:30:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:45:54.885431+00:00"
    },
    {
      "arxiv_id": "2505.13157v1",
      "title": "Role-Playing Evaluation for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è§’è‰²æ‰®æ¼”èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Yassine El Boudouri",
        "Walter Nuninger",
        "Julian Alvarez",
        "Yvan Peter"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è§’è‰²æ‰®æ¼” (role-playing) æ–¹é¢çš„èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º Role-Playing Eval (RPEval) çš„æ–°å‹è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³äººå·¥è¯„ä¼°èµ„æºå¯†é›†ä»¥åŠè‡ªåŠ¨åŒ–è¯„ä¼°å­˜åœ¨åå·®çš„é—®é¢˜ã€‚RPEval ä»æƒ…ç»ªç†è§£ (emotional understanding)ã€å†³ç­–åˆ¶å®š (decision-making)ã€é“å¾·å‡†åˆ™ (moral alignment) å’Œè§’è‰²ä¸€è‡´æ€§ (in-character consistency) å››ä¸ªå…³é”®ç»´åº¦å¯¹æ¨¡å‹è¿›è¡Œå…¨é¢è¡¡é‡ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº† RPEval çš„æ„å»ºæµç¨‹ï¼Œå¹¶å±•ç¤ºäº†é’ˆå¯¹å¤šç§æ¨¡å‹çš„åŸºçº¿è¯„ä¼°ç»“æœã€‚è¯¥ç ”ç©¶é€šè¿‡æä¾›æ ‡å‡†åŒ–çš„æ•°æ®é›†å’Œè¯„æµ‹æ¡†æ¶ï¼Œä¸ºè¡¡é‡ LLM åœ¨å¤æ‚è§’è‰²æ‰®æ¼”ä»»åŠ¡ä¸­çš„è¡¨ç°å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13157v1",
      "published_date": "2025-05-19 14:18:16 UTC",
      "updated_date": "2025-05-19 14:18:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:30.252289+00:00"
    },
    {
      "arxiv_id": "2505.13156v1",
      "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice",
      "title_zh": "Tianyiï¼šä¸­åŒ»å…¨èƒ½è¯­è¨€æ¨¡å‹åŠå…¶çœŸå®ä¸–ç•Œä¸´åºŠå®è·µ",
      "authors": [
        "Zhi Liu",
        "Tao Yang",
        "Jing Wang",
        "Yexin Chen",
        "Zhan Gao",
        "Jiaxi Yang",
        "Kui Chen",
        "Bingji Lu",
        "Xiaochen Li",
        "Changyong Luo",
        "Yan Li",
        "Xiaohong Gu",
        "Peng Cao"
      ],
      "abstract": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are gaining global recognition for their therapeutic potential in addressing human symptoms and diseases. TCM, with its systematic theories and extensive practical experience, provides abundant resources for healthcare. However, the effective application of TCM requires precise syndrome diagnosis, determination of treatment principles, and prescription formulation, which demand decades of clinical expertise. Despite advancements in TCM-based decision systems, machine learning, and deep learning research, limitations in data and single-objective constraints hinder their practical application. In recent years, large language models (LLMs) have demonstrated potential in complex tasks, but lack specialization in TCM and face significant challenges, such as too big model scale to deploy and issues with hallucination. To address these challenges, we introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and specifically designed for TCM, pre-trained and fine-tuned on diverse TCM corpora, including classical texts, expert treatises, clinical records, and knowledge graphs. Tianyi is designed to assimilate interconnected and systematic TCM knowledge through a progressive learning manner. Additionally, we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in TCM examinations, clinical tasks, domain-specific question-answering, and real-world trials. The extensive evaluations demonstrate the significant potential of Tianyi as an AI assistant in TCM clinical practice and research, bridging the gap between TCM knowledge and practical application.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤©åŒ» (Tianyi)ï¼Œä¸€ä¸ªåŒ…å« 7.6-billion-parameter çš„ä¸­åŒ»è¯ (TCM) ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é€šç”¨æ¨¡å‹åœ¨éƒ¨ç½²è§„æ¨¡ã€å¹»è§‰ (hallucination) ä»¥åŠä¸­åŒ»è¯é¢†åŸŸçŸ¥è¯†ä¸“ä¸šæ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚Tianyi é‡‡ç”¨æ¸è¿›å¼å­¦ä¹  (progressive learning) æ–¹å¼ï¼Œåœ¨åŒ…å«ç»å…¸æ–‡çŒ®ã€ä¸“å®¶è®ºè¿°ã€ä¸´åºŠè®°å½•å’ŒçŸ¥è¯†å›¾è°± (knowledge graphs) çš„å¤šå…ƒè¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒä¸å¾®è°ƒã€‚ä¸ºäº†å…¨é¢è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜ŸåŒæ­¥æ„å»ºäº†åä¸º TCMEval çš„è¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–äº†ä¸­åŒ»è¯è€ƒè¯•ã€ä¸´åºŠä»»åŠ¡ã€é¢†åŸŸé—®ç­”åŠçœŸå®ä¸–ç•Œè¯•éªŒã€‚å¹¿æ³›çš„è¯„ä¼°ç»“æœè¯æ˜ï¼ŒTianyi åœ¨ä¸­åŒ»è¯ä¸´åºŠå®è·µå’Œç ”ç©¶ä¸­å±•ç°å‡ºä½œä¸ºäººå·¥æ™ºèƒ½åŠ©æ‰‹ (AI assistant) çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œç²¾å‡†çš„è¯å€™è¯Šæ–­ã€æ²»ç–—åŸåˆ™ç¡®å®šåŠå¤„æ–¹åˆ¶å®šã€‚è¯¥æ¨¡å‹ä¸ä»…å¡«è¡¥äº†ä¸­åŒ»è¯ç³»ç»Ÿç†è®ºä¸å®é™…ä¸´åºŠåº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå…¶é€‚ä¸­çš„æ¨¡å‹è§„æ¨¡ä¹Ÿä¸ºå®é™…åŒ»ç–—åœºæ™¯çš„éƒ¨ç½²æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 4 figures, and 1 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.13156v1",
      "published_date": "2025-05-19 14:17:37 UTC",
      "updated_date": "2025-05-19 14:17:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:30.983724+00:00"
    },
    {
      "arxiv_id": "2505.13144v1",
      "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning",
      "title_zh": "é¢å‘ç¦»çº¿åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„æ—¶é—´è·ç¦»æ„ŸçŸ¥è½¬ç§»å¢å¼º",
      "authors": [
        "Dongsu Lee",
        "Minhae Kwon"
      ],
      "abstract": "The goal of offline reinforcement learning (RL) is to extract a high-performance policy from the fixed datasets, minimizing performance degradation due to out-of-distribution (OOD) samples. Offline model-based RL (MBRL) is a promising approach that ameliorates OOD issues by enriching state-action transitions with augmentations synthesized via a learned dynamics model. Unfortunately, seminal offline MBRL methods often struggle in sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA), that generates augmented transitions in a temporally structured latent space rather than in raw state space. To model long-horizon behavior, TempDATA learns a latent abstraction that captures a temporal distance from both trajectory and transition levels of state space. Our experiments confirm that TempDATA outperforms previous offline MBRL methods and achieves matching or surpassing the performance of diffusion-based trajectory augmentation and goal-conditioned RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹  (Offline Model-based RL) åœ¨å¤„ç†ç¨€ç–å¥–åŠ±å’Œé•¿æ—¶ç¨‹ (long-horizon) ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º Temporal Distance-Aware Transition Augmentation (TempDATA) çš„æ–°æ¡†æ¶ã€‚TempDATA å¹¶éåœ¨åŸå§‹çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œå¢å¼ºï¼Œè€Œæ˜¯åœ¨å…·æœ‰æ—¶é—´ç»“æ„çš„æ½œç©ºé—´ (latent space) ä¸­ç”Ÿæˆè½¬æ¢æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ ä¸€ç§æ½œåœ¨æŠ½è±¡ï¼ŒåŒæ—¶ä»è½¨è¿¹å’Œè½¬æ¢å±‚é¢æ•æ‰çŠ¶æ€é—´çš„æ—¶é—´è·ç¦» (temporal distance)ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ¨¡æ‹Ÿé•¿æ—¶ç¨‹è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTempDATA åœ¨ D4RL AntMazeã€FrankaKitchen ä»¥åŠ CALVIN ç­‰å¤æ‚ä»»åŠ¡ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„ç¦»çº¿ MBRL æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¿‡äº†åŸºäºæ‰©æ•£ (diffusion-based) çš„è½¨è¿¹å¢å¼ºå’Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹  (goal-conditioned RL) ç­‰å‰æ²¿æŠ€æœ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "2025 ICML",
      "pdf_url": "https://arxiv.org/pdf/2505.13144v1",
      "published_date": "2025-05-19 14:11:14 UTC",
      "updated_date": "2025-05-19 14:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:30.566041+00:00"
    },
    {
      "arxiv_id": "2505.13136v2",
      "title": "New Encoders for German Trained from Scratch: Comparing ModernGBERT with Converted LLM2Vec Models",
      "title_zh": "ä»é›¶è®­ç»ƒçš„æ–°å‹å¾·è¯­ç¼–ç å™¨ï¼šModernGBERT ä¸ LLM2Vec è½¬æ¢æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Julia Wunderle",
        "Anton Ehrmanntraut",
        "Jan Pfister",
        "Fotis Jannidis",
        "Andreas Hotho"
      ],
      "abstract": "Encoders remain essential for efficient German NLP and NLU scenarios despite the rise of decoder-only LLMs. This work studies two routes to high-quality German encoders under identical data and training constraints: 1) training from scratch and 2) converting decoders via LLM2Vec. We introduce two resources: ModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT style, and LLÃ¤MmleinVec (120M, 1B, 7B), decoder-to-encoder conversions trained with masked next-token prediction, both undergoing a context extension to 8.192 tokens.\n  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808), surpassing GBERT Large (+4%) and the seven-times larger converted 7B model (0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551) approaches the converted 7B model (0.557).\n  We release all models, checkpoints, datasets, and full training records, and introduce an encoder-adapted QA-NIAH evaluation. All in all, our results provide actionable guidance: when parameter efficiency and latency matter, from-scratch encoders dominate. When a pre-trained decoder exists and compute is a limited, conversion offers an effective alternative. ModernGBERT and LLÃ¤MmleinVec, including all code, data and intermediary checkpoints are published under a research-only RAIL license.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç›¸åŒæ•°æ®å’Œè®­ç»ƒçº¦æŸä¸‹æ„å»ºé«˜è´¨é‡å¾·è¯­ Encoder çš„ä¸¤æ¡è·¯å¾„ï¼Œå³ä»é›¶å¼€å§‹è®­ç»ƒä¸é€šè¿‡ LLM2Vec è½¬æ¢ Decoderã€‚ä½œè€…æ¨å‡ºäº† ModernGBERTï¼ˆ134Mã€1Bï¼‰ï¼Œè¿™æ˜¯é‡‡ç”¨ ModernBERT é£æ ¼ä»é›¶è®­ç»ƒçš„é€æ˜å¾·è¯­ Encoderï¼Œä»¥åŠé€šè¿‡æ©ç æ¬¡ä»£é¢„æµ‹ï¼ˆmasked next-token predictionï¼‰è®­ç»ƒçš„è½¬æ¢æ¨¡å‹ LLÃ¤MmleinVecï¼ˆ120Mã€1Bã€7Bï¼‰ï¼Œä¸¤è€…å‡æ”¯æŒ 8,192 ä¸ª token çš„ä¸Šä¸‹æ–‡æ‰©å±•ã€‚å®éªŒæ˜¾ç¤º ModernGBERT 1B åœ¨ SuperGLEBer åŸºå‡†ä¸Šåˆ›ä¸‹ 0.808 çš„æ–° SOTAï¼Œæ€§èƒ½ä¼˜äº GBERT Large åŠå‚æ•°é‡å¤§ä¸ƒå€çš„ 7B è½¬æ¢æ¨¡å‹ï¼Œå¹¶åœ¨ German MTEB è¯„ä¼°ä¸­é€¼è¿‘äº† 7B æ¨¡å‹çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ³¨é‡å‚æ•°æ•ˆç‡å’Œå»¶è¿Ÿçš„åœºæ™¯ä¸‹ä»é›¶è®­ç»ƒçš„ Encoder å æ®ä¸»å¯¼åœ°ä½ï¼Œè€Œè®¡ç®—èµ„æºå—é™æ—¶è½¬æ¢æ–¹æ¡ˆåˆ™æ˜¯æœ‰æ•ˆçš„æ›¿ä»£ã€‚è¯¥å·¥ä½œå¼€æºäº†æ‰€æœ‰æ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹ Encoder é€‚é…çš„ QA-NIAH è¯„ä¼°ï¼Œä¸ºå¾·è¯­ NLP å®è·µæä¾›äº†é‡è¦çš„è¡ŒåŠ¨æŒ‡å—ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "under review @LREC",
      "pdf_url": "https://arxiv.org/pdf/2505.13136v2",
      "published_date": "2025-05-19 14:07:20 UTC",
      "updated_date": "2025-11-03 12:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:32.042966+00:00"
    },
    {
      "arxiv_id": "2505.14719v3",
      "title": "MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion",
      "title_zh": "MSVITï¼šåˆ©ç”¨å¤šå°ºåº¦æ³¨æ„åŠ›èåˆæå‡è„‰å†²è§†è§‰ Transformer æ€§èƒ½",
      "authors": [
        "Wei Hua",
        "Chenlin Zhou",
        "Jibin Wu",
        "Yansong Chua",
        "Yangyang Shu"
      ],
      "abstract": "The combination of Spiking Neural Networks (SNNs) with Vision Transformer architectures has garnered significant attention due to their potential for energy-efficient and high-performance computing paradigms. However, a substantial performance gap still exists between SNN-based and ANN-based transformer architectures. While existing methods propose spiking self-attention mechanisms that are successfully combined with SNNs, the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting features from different image scales. In this paper, we address this issue and propose MSVIT. This novel spike-driven Transformer architecture firstly uses multi-scale spiking attention (MSSA) to enhance the capabilities of spiking attention blocks. We validate our approach across various main datasets. The experimental results show that MSVIT outperforms existing SNN-based models, positioning itself as a state-of-the-art solution among SNN-transformer architectures. The codes are available at https://github.com/Nanhu-AI-Lab/MSViT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‰å†²ç¥ç»ç½‘ç»œ (Spiking Neural Networks, SNNs) ä¸è§†è§‰ Transformer æ¶æ„ç»“åˆæ—¶å­˜åœ¨çš„æ€§èƒ½å·®è·ï¼Œä»¥åŠç°æœ‰æ–¹æ³•åœ¨æå–ä¸åŒå›¾åƒå°ºåº¦ç‰¹å¾æ–¹é¢çš„ç“¶é¢ˆï¼Œæå‡ºäº† MSVITã€‚è¿™æ˜¯ä¸€ç§æ–°å‹çš„è„‰å†²é©±åŠ¨ Transformer æ¶æ„ï¼Œé€šè¿‡å¼•å…¥å¤šå°ºåº¦è„‰å†²æ³¨æ„åŠ› (Multi-scale Spiking Attention, MSSA) æœºåˆ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†è„‰å†²æ³¨æ„åŠ›æ¨¡å—çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªä¸»æµæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜ MSVIT åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„ SNN åŸºç¡€æ¨¡å‹ï¼Œæˆä¸º SNN-transformer æ¶æ„ä¸­çš„ SOTA è§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶ä¸ä»…å…‹æœäº†å¤šå°ºåº¦ç‰¹å¾æå–çš„å±€é™æ€§ï¼Œè¿˜ä¸ºå®ç°é«˜èƒ½æ•ˆã€é«˜æ€§èƒ½çš„ç¥ç»å½¢æ€è§†è§‰è®¡ç®—æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11pages, 2figures, accepted by IJCAI'25 (34th International Joint Conference on Artificial Intelligence)",
      "pdf_url": "https://arxiv.org/pdf/2505.14719v3",
      "published_date": "2025-05-19 14:01:03 UTC",
      "updated_date": "2025-06-18 03:58:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:27.576261+00:00"
    },
    {
      "arxiv_id": "2505.13130v1",
      "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach",
      "title_zh": "è§†é¢‘ç›‘æ§ä¸­çš„è‡ªé€‚åº”å›¾åƒä¿®å¤ï¼šä¸€ç§å®æ—¶æ–¹æ³•",
      "authors": [
        "Muhammad Awais Amin",
        "Adama Ilboudo",
        "Abdul Samad bin Shahid",
        "Amjad Ali",
        "Waqas Haider Khan Bangyal"
      ],
      "abstract": "One of the major challenges in the field of computer vision especially for detection, segmentation, recognition, monitoring, and automated solutions, is the quality of images. Image degradation, often caused by factors such as rain, fog, lighting, etc., has a negative impact on automated decision-making.Furthermore, several image restoration solutions exist, including restoration models for single degradation and restoration models for multiple degradations. However, these solutions are not suitable for real-time processing. In this study, the aim was to develop a real-time image restoration solution for video surveillance. To achieve this, using transfer learning with ResNet_50, we developed a model for automatically identifying the types of degradation present in an image to reference the necessary treatment(s) for image restoration. Our solution has the advantage of being flexible and scalable.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç›‘æ§é¢†åŸŸä¸­å› é™é›¨ã€é›¾æ°”å’Œå…‰ç…§ç­‰å› ç´ å¯¼è‡´çš„å›¾åƒé€€åŒ–(Image degradation)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å®æ—¶å›¾åƒæ¢å¤(Image restoration)è§£å†³æ–¹æ¡ˆã€‚ç”±äºç°æœ‰å•é‡æˆ–å¤šé‡é€€åŒ–æ¢å¤æ¨¡å‹å¾€å¾€éš¾ä»¥æ»¡è¶³å®æ—¶å¤„ç†çš„éœ€æ±‚ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨ResNet_50é€šè¿‡è¿ç§»å­¦ä¹ (Transfer learning)å¼€å‘äº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å›¾åƒé€€åŒ–ç±»å‹çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è¯†åˆ«ç‰¹å®šçš„é€€åŒ–ç±»å‹æ¥å¼•å¯¼åç»­çš„å›¾åƒä¿®å¤å¤„ç†ï¼Œç¡®ä¿äº†æ¢å¤è¿‡ç¨‹çš„é’ˆå¯¹æ€§å’Œé«˜æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆå…·æœ‰è‰¯å¥½çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§(Scalability)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤æ‚ç¯å¢ƒä¸‹è®¡ç®—æœºè§†è§‰ç³»ç»Ÿåœ¨æ£€æµ‹ä¸è¯†åˆ«ç­‰ä»»åŠ¡ä¸­çš„å†³ç­–è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13130v1",
      "published_date": "2025-05-19 14:00:10 UTC",
      "updated_date": "2025-05-19 14:00:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:45.730159+00:00"
    },
    {
      "arxiv_id": "2505.13126v2",
      "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments",
      "title_zh": "éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹çš„é›¶æ ·æœ¬è¿­ä»£å¼å½¢å¼åŒ–ä¸è§„åˆ’",
      "authors": [
        "Liancheng Gong",
        "Wang Zhu",
        "Jesse Thomason",
        "Li Zhang"
      ],
      "abstract": "Using LLMs not to predict plans but to formalize an environment into the Planning Domain Definition Language (PDDL) has been shown to improve performance and control. Existing work focuses on fully observable environments; we tackle the more realistic and challenging partially observable environments that lack of complete, reliable information. We propose PDDLego+, a framework to iteratively formalize, plan, grow, and refine PDDL representations in a zero-shot manner, without needing access to any existing trajectories. On two textual simulated environments, we show that PDDLego+ improves goal reaching success and exhibits robustness against problem complexity. We also show that the domain knowledge captured after a successful trial can benefit future tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å°†ç¯å¢ƒå½¢å¼åŒ–ä¸ºè§„åˆ’åŸŸå®šä¹‰è¯­è¨€(PDDL)æ—¶å¤§å¤šå±€é™äºå®Œå…¨å¯è§‚æµ‹ç¯å¢ƒçš„é—®é¢˜ï¼Œæ¢è®¨äº†æ›´å…·æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†å¯è§‚æµ‹(Partially Observable)ç¯å¢ƒã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸ºPDDLego+çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨é›¶æ ·æœ¬(Zero-Shot)æ–¹å¼ï¼Œåœ¨æ— éœ€é¢„å…ˆè·å–ä»»ä½•è½¨è¿¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¿­ä»£å¼åœ°å¯¹PDDLè¡¨ç¤ºè¿›è¡Œå½¢å¼åŒ–ã€è§„åˆ’ã€æ‰©å±•å’Œç²¾ç‚¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPDDLego+åœ¨ä¸¤ä¸ªæ–‡æœ¬æ¨¡æ‹Ÿç¯å¢ƒä¸Šæ˜¾è‘—æé«˜äº†ç›®æ ‡è¾¾æˆæˆåŠŸç‡ï¼Œå¹¶å¯¹é—®é¢˜å¤æ‚æ€§è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘ç°æˆåŠŸæ‰§è¡Œä»»åŠ¡åç§¯ç´¯çš„é¢†åŸŸçŸ¥è¯†(Domain Knowledge)å¯ä»¥æœ‰æ•ˆæå‡æœªæ¥ä»»åŠ¡çš„å¤„ç†æ•ˆç‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†ä¸å®Œå…¨ä¿¡æ¯ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13126v2",
      "published_date": "2025-05-19 13:58:15 UTC",
      "updated_date": "2025-05-20 13:53:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:49.897840+00:00"
    },
    {
      "arxiv_id": "2505.13124v2",
      "title": "$Î¼$PC: Scaling Predictive Coding to 100+ Layer Networks",
      "title_zh": "$\\mu$PCï¼šå°†é¢„æµ‹ç¼–ç æ‰©å±•è‡³ç™¾å±‚ä»¥ä¸Šæ·±åº¦ç½‘ç»œ",
      "authors": [
        "Francesco Innocenti",
        "El Mehdi Achour",
        "Christopher L. Buckley"
      ],
      "abstract": "The biological implausibility of backpropagation (BP) has motivated many alternative, brain-inspired algorithms that attempt to rely only on local information, such as predictive coding (PC) and equilibrium propagation. However, these algorithms have notoriously struggled to train very deep networks, preventing them from competing with BP in large-scale settings. Indeed, scaling PC networks (PCNs) has recently been posed as a challenge for the community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can be trained reliably using a Depth-$Î¼$P parameterisation (Yang et al., 2023; Bordelon et al., 2023) which we call \"$Î¼$PC\". By analysing the scaling behaviour of PCNs, we reveal several pathologies that make standard PCNs difficult to train at large depths. We then show that, despite addressing only some of these instabilities, $Î¼$PC allows stable training of very deep (up to 128-layer) residual networks on simple classification tasks with competitive performance and little tuning compared to current benchmarks. Moreover, $Î¼$PC enables zero-shot transfer of both weight and activity learning rates across widths and depths. Our results serve as a first step towards scaling PC to more complex architectures and have implications for other local algorithms. Code for $Î¼$PC is made available as part of a JAX library for PCNs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘å¯å‘ç®—æ³• Predictive Coding (PC) åœ¨è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶çš„è§„æ¨¡åŒ–éš¾é¢˜ï¼Œæå‡ºäº†åŸºäº Depth-$Î¼$P å‚æ•°åŒ–æ–¹æ³•çš„ $Î¼$PCã€‚é€šè¿‡åˆ†æ PCNs çš„ç¼©æ”¾è¡Œä¸ºï¼Œä½œè€…æ­ç¤ºäº†å¯¼è‡´æ ‡å‡†æ¨¡å‹åœ¨æ·±å±‚ç»“æ„ä¸­éš¾ä»¥è®­ç»ƒçš„ç—…ç†åŸå› ï¼Œå¹¶åˆ©ç”¨ $Î¼$PC æˆåŠŸå®ç°äº†å¤šè¾¾ 128 å±‚ residual networks åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„ç¨³å®šè®­ç»ƒã€‚å®éªŒè¯æ˜ $Î¼$PC åœ¨ä¿æŒç«äº‰æ€§èƒ½çš„åŒæ—¶ä»…éœ€æå°‘è°ƒå‚ï¼Œå¹¶æ”¯æŒæƒé‡ä¸æ´»åŠ¨å­¦ä¹ ç‡åœ¨ä¸åŒç½‘ç»œå®½åº¦ä¸æ·±åº¦é—´çš„ zero-shot transferã€‚è¯¥æˆæœæ ‡å¿—ç€å°† PC æ‰©å±•è‡³å¤æ‚æ¶æ„è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ï¼Œä¸ºå±€éƒ¨å­¦ä¹ ç®—æ³•åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„åº”ç”¨æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages, 42 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13124v2",
      "published_date": "2025-05-19 13:54:29 UTC",
      "updated_date": "2025-11-28 10:13:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:54.200293+00:00"
    },
    {
      "arxiv_id": "2505.13573v1",
      "title": "FreeMesh: Boosting Mesh Generation with Coordinates Merging",
      "title_zh": "FreeMeshï¼šé€šè¿‡åæ ‡åˆå¹¶æå‡ç½‘æ ¼ç”Ÿæˆ",
      "authors": [
        "Jian Liu",
        "Haohan Weng",
        "Biwen Lei",
        "Xianghui Yang",
        "Zibo Zhao",
        "Zhuo Chen",
        "Song Guo",
        "Tao Han",
        "Chunchao Guo"
      ],
      "abstract": "The next-coordinate prediction paradigm has emerged as the de facto standard in current auto-regressive mesh generation methods. Despite their effectiveness, there is no efficient measurement for the various tokenizers that serialize meshes into sequences. In this paper, we introduce a new metric Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers theoretically without any training. Building upon PTME, we propose a plug-and-play tokenization technique called coordinate merging. It further improves the compression ratios of existing tokenizers by rearranging and merging the most frequent patterns of coordinates. Through experiments on various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we further validate the performance of our method. We hope that the proposed PTME and coordinate merging can enhance the existing mesh tokenizers and guide the further development of native mesh generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’ç½‘æ ¼ç”Ÿæˆ(auto-regressive mesh generation)é¢†åŸŸç¼ºä¹é«˜æ•ˆåˆ†è¯å™¨è¯„ä»·æŒ‡æ ‡çš„é—®é¢˜ï¼Œæå‡ºäº†FreeMeshæ¡†æ¶ã€‚ç ”ç©¶è€…é¦–å…ˆå¼•å…¥äº†ä¸€ç§åä¸ºPer-Token-Mesh-Entropy (PTME)çš„æ–°æŒ‡æ ‡ï¼Œæ—¨åœ¨æ— éœ€è®­ç»ƒå³å¯ç†è®ºåŒ–åœ°è¯„ä¼°ç°æœ‰çš„mesh tokenizersã€‚åŸºäºPTMEï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºcoordinate mergingçš„å³æ’å³ç”¨åˆ†è¯æŠ€æœ¯ï¼Œé€šè¿‡é‡æ–°æ’åˆ—å¹¶åˆå¹¶é«˜é¢‘åæ ‡æ¨¡å¼ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰åˆ†è¯å™¨çš„å‹ç¼©æ¯”ã€‚é€šè¿‡åœ¨MeshXLã€MeshAnything V2å’ŒEdgerunnerç­‰å¤šç§ä¸»æµåˆ†è¯æ–¹æ³•ä¸Šçš„å®éªŒï¼ŒéªŒè¯äº†è¯¥æŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºç°æœ‰æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥æˆæœä¸ä»…ä¼˜åŒ–äº†å½“å‰çš„ç½‘æ ¼ç”Ÿæˆæ•ˆç‡ï¼Œä¹Ÿä¸ºæœªæ¥åŸç”Ÿç½‘æ ¼ç”Ÿæˆ(native mesh generation)çš„ç ”ç©¶æä¾›äº†é‡è¦çš„ç†è®ºæŒ‡å¯¼ä¸æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by ICML 2025, camera-ready version",
      "pdf_url": "https://arxiv.org/pdf/2505.13573v1",
      "published_date": "2025-05-19 13:52:57 UTC",
      "updated_date": "2025-05-19 13:52:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:06.120689+00:00"
    },
    {
      "arxiv_id": "2505.13123v1",
      "title": "Just Dance with $Ï€$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection",
      "title_zh": "ä¸ $Ï€$ å…±èˆï¼ï¼šä¸€ç§é¢å‘å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„å¤šæ¨¡æ€è¯±å¯¼å™¨",
      "authors": [
        "Snehashis Majhi",
        "Giacomo D'Amicantonio",
        "Antitza Dantcheva",
        "Quan Kong",
        "Lorenzo Garattoni",
        "Gianpiero Francesca",
        "Egor Bondarev",
        "Francois Bremond"
      ],
      "abstract": "Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we introduce the Poly-modal Induced framework for VAD: \"PI-VAD\", a novel approach that augments RGB representations by five additional modalities. Specifically, the modalities include sensitivity to fine-grained motion (Pose), three dimensional scene and entity representation (Depth), surrounding objects (Panoptic masks), global motion (optical flow), as well as language cues (VLM). Each modality represents an axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two plug-in modules, namely Pseudo-modality Generation module and Cross Modal Induction module, which generate modality-specific prototypical representation and, thereby, induce multi-modal information into RGB cues. These modules operate by performing anomaly-aware auxiliary tasks and necessitate five modality backbones -- only during training. Notably, PI-VAD achieves state-of-the-art accuracy on three prominent VAD datasets encompassing real-world scenarios, without requiring the computational overhead of five modality backbones at inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PI-VADï¼Œä¸€ç§é’ˆå¯¹å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ (Weakly-supervised Video Anomaly Detection) çš„å¤šæ¨¡æ€è¯±å¯¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä»…ä¾èµ– RGB æ—¶ç©ºç‰¹å¾åœ¨åŒºåˆ†å¤æ‚ç°å®å¼‚å¸¸è¡Œä¸ºæ—¶å¯é æ€§å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆ Poseã€Depthã€Panoptic masksã€Optical flow ä»¥åŠ VLM ç­‰äº”ç§é¢å¤–æ¨¡æ€ï¼Œä¸º RGB è¡¨ç¤ºæä¾›äº†æ›´å…·è¾¨è¯†æ€§çš„ç‰¹å¾å¢å¼ºã€‚PI-VAD è®¾è®¡äº† Pseudo-modality Generation æ¨¡å—å’Œ Cross Modal Induction æ¨¡å—ï¼Œåˆ©ç”¨å¼‚å¸¸æ„ŸçŸ¥è¾…åŠ©ä»»åŠ¡ç”Ÿæˆæ¨¡æ€ç‰¹å®šçš„åŸå‹è¡¨ç¤ºï¼Œå¹¶å°†å¤šæ¨¡æ€ä¿¡æ¯æœ‰æ•ˆåœ°è¯±å¯¼å…¥ RGB æç¤ºã€‚å…³é”®åˆ›æ–°åœ¨äºè¯¥æ¨¡å‹ä»…åœ¨è®­ç»ƒé˜¶æ®µéœ€è¦å¤šæ¨¡æ€éª¨æ¶ç½‘ç»œï¼Œè€Œåœ¨æ¨ç†é˜¶æ®µæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å®éªŒè¯æ˜ï¼ŒPI-VAD åœ¨ä¸‰ä¸ªä¸»æµ VAD æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† SOTA ç²¾åº¦ï¼Œä¸ºæ„å»ºé²æ£’çš„ç°å®ä¸–ç•Œå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13123v1",
      "published_date": "2025-05-19 13:51:57 UTC",
      "updated_date": "2025-05-19 13:51:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:46:59.686807+00:00"
    },
    {
      "arxiv_id": "2505.13122v2",
      "title": "When majority rules, minority loses: bias amplification of gradient descent",
      "title_zh": "å¤šæ•°åŸåˆ™ä¸‹çš„å°‘æ•°å—æŸï¼šæ¢¯åº¦ä¸‹é™ä¸­çš„åè§æ”¾å¤§æ•ˆåº”",
      "authors": [
        "FranÃ§ois Bachoc",
        "JÃ©rÃ´me Bolte",
        "Ryan Boustany",
        "Jean-Michel Loubes"
      ],
      "abstract": "Despite growing empirical evidence of bias amplification in machine learning, its theoretical foundations remain poorly understood. We develop a formal framework for majority-minority learning tasks, showing how standard training can favor majority groups and produce stereotypical predictors that neglect minority-specific features. Assuming population and variance imbalance, our analysis reveals three key findings: (i) the close proximity between ``full-data'' and stereotypical predictors, (ii) the dominance of a region where training the entire model tends to merely learn the majority traits, and (iii) a lower bound on the additional training required. Our results are illustrated through experiments in deep learning for tabular and image classification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸­åè§æ”¾å¤§(bias amplification)çš„ç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨æ­ç¤ºæ ‡å‡†è®­ç»ƒè¿‡ç¨‹å¦‚ä½•äº§ç”Ÿå€¾å‘äºå¤šæ•°ç¾¤ä½“(majority groups)çš„åˆ»æ¿é¢„æµ‹ç»“æœã€‚é€šè¿‡æ„å»ºå¤šæ•°-å°‘æ•°å­¦ä¹ ä»»åŠ¡(majority-minority learning tasks)çš„æ­£å¼æ¡†æ¶ï¼Œç ”ç©¶åˆ†æäº†åœ¨äººå£ä¸æ–¹å·®ä¸å¹³è¡¡(population and variance imbalance)æ¡ä»¶ä¸‹ï¼Œæ¢¯åº¦ä¸‹é™(gradient descent)å¯¹æ¨¡å‹å­¦ä¹ çš„å½±å“ã€‚ç ”ç©¶æå‡ºäº†ä¸‰é¡¹å…³é”®å‘ç°ï¼šé¦–å…ˆæ˜¯â€œå…¨æ•°æ®â€é¢„æµ‹å™¨ä¸åˆ»æ¿é¢„æµ‹å™¨(stereotypical predictors)çš„é«˜åº¦æ¥è¿‘ï¼›å…¶æ¬¡æ˜¯è®­ç»ƒè¿‡ç¨‹å­˜åœ¨ä¸€ä¸ªä¸»å¯¼åŒºåŸŸï¼Œä½¿å¾—æ¨¡å‹å€¾å‘äºä»…æ•æ‰å¤šæ•°ç¾¤ä½“çš„ç‰¹å¾ï¼›æœ€åæ˜¯ç¡®å®šäº†æ¶ˆé™¤åè§æ‰€éœ€çš„é¢å¤–è®­ç»ƒé‡ä¸‹ç•Œ(lower bound)ã€‚åœ¨è¡¨æ ¼ä¸å›¾åƒåˆ†ç±»çš„æ·±åº¦å­¦ä¹ å®éªŒä¸­ï¼Œè¿™äº›ç†è®ºç»“æœå¾—åˆ°äº†æœ‰æ•ˆéªŒè¯ï¼Œä¸ºç†è§£åè§æ”¾å¤§æœºåˆ¶æä¾›äº†é‡è¦çš„æ•°å­¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13122v2",
      "published_date": "2025-05-19 13:51:49 UTC",
      "updated_date": "2025-10-20 07:35:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:45.724458+00:00"
    },
    {
      "arxiv_id": "2505.13118v1",
      "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals",
      "title_zh": "æ­ç¤ºä¸ç¡®å®šæ€§æ¥æºï¼šç‰¹å¾å¯¹ç¬¦åˆé¢„æµ‹åŒºé—´çš„è´¡çŒ®",
      "authors": [
        "Marouane Il Idrissi",
        "Agathe Fernandes Machado",
        "Ewen Gallic",
        "Arthur Charpentier"
      ],
      "abstract": "Cooperative game theory methods, notably Shapley values, have significantly enhanced machine learning (ML) interpretability. However, existing explainable AI (XAI) frameworks mainly attribute average model predictions, overlooking predictive uncertainty. This work addresses that gap by proposing a novel, model-agnostic uncertainty attribution (UA) method grounded in conformal prediction (CP). By defining cooperative games where CP interval properties-such as width and bounds-serve as value functions, we systematically attribute predictive uncertainty to input features. Extending beyond the traditional Shapley values, we use the richer class of Harsanyi allocations, and in particular the proportional Shapley values, which distribute attribution proportionally to feature importance. We propose a Monte Carlo approximation method with robust statistical guarantees to address computational feasibility, significantly improving runtime efficiency. Our comprehensive experiments on synthetic benchmarks and real-world datasets demonstrate the practical utility and interpretative depth of our approach. By combining cooperative game theory and conformal prediction, we offer a rigorous, flexible toolkit for understanding and communicating predictive uncertainty in high-stakes ML applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ¡†æ¶ï¼ˆå¦‚Shapley valuesï¼‰å¤§å¤šä¾§é‡äºå½’å› å¹³å‡æ¨¡å‹é¢„æµ‹è€Œå¿½ç•¥é¢„æµ‹ä¸ç¡®å®šæ€§çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„ã€æ¨¡å‹æ— å…³çš„ä¸ç¡®å®šæ€§å½’å› (Uncertainty Attribution, UA)æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä»¥ç¬¦åˆé¢„æµ‹(Conformal Prediction, CP)ä¸ºåŸºç¡€ï¼Œé€šè¿‡å°†CPåŒºé—´çš„å®½åº¦å’Œè¾¹ç•Œç­‰å±æ€§å®šä¹‰ä¸ºåˆä½œåšå¼ˆä¸­çš„ä»·å€¼å‡½æ•°ï¼Œç³»ç»Ÿåœ°å°†é¢„æµ‹ä¸ç¡®å®šæ€§å½’å› äºå…·ä½“çš„è¾“å…¥ç‰¹å¾ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ‰©å±•äº†ä¼ ç»Ÿçš„Shapley valuesï¼Œé‡‡ç”¨æ›´ä¸°å¯Œçš„Harsanyi allocationsï¼ˆå°¤å…¶æ˜¯æ¯”ä¾‹Shapley valuesï¼‰æ¥æŒ‰ç‰¹å¾é‡è¦æ€§è¿›è¡Œæ¯”ä¾‹åˆ†é…ã€‚ä¸ºäº†åº”å¯¹è®¡ç®—å¤æ‚æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å…·æœ‰ç¨³å¥ç»Ÿè®¡ä¿è¯çš„è’™ç‰¹å¡æ´›(Monte Carlo)è¿‘ä¼¼æ–¹æ³•ï¼Œå¤§å¹…æå‡äº†ç®—æ³•çš„è¿è¡Œæ•ˆç‡ã€‚åœ¨åˆæˆåŸºå‡†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æé«˜çš„æ•ˆç”¨å’Œè§£é‡Šæ·±åº¦ã€‚é€šè¿‡ç»“åˆåˆä½œåšå¼ˆè®ºä¸ç¬¦åˆé¢„æµ‹ï¼Œè¯¥ç ”ç©¶ä¸ºé«˜é£é™©æœºå™¨å­¦ä¹ åº”ç”¨ä¸­é¢„æµ‹ä¸ç¡®å®šæ€§çš„ç†è§£ä¸æ²Ÿé€šæä¾›äº†ä¸€å¥—ä¸¥è°¨ä¸”çµæ´»çš„å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13118v1",
      "published_date": "2025-05-19 13:49:05 UTC",
      "updated_date": "2025-05-19 13:49:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:35.685796+00:00"
    },
    {
      "arxiv_id": "2505.13116v1",
      "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data",
      "title_zh": "Continuous Fair SMOTEï¼šé¢å‘ä¸å¹³è¡¡æ•°æ®çš„å…¬å¹³æ„ŸçŸ¥æµå¼å­¦ä¹ ",
      "authors": [
        "Kathrin Lammers",
        "Valerie Vaquet",
        "Barbara Hammer"
      ],
      "abstract": "As machine learning is increasingly applied in an online fashion to deal with evolving data streams, the fairness of these algorithms is a matter of growing ethical and legal concern. In many use cases, class imbalance in the data also needs to be dealt with to ensure predictive performance. Current fairness-aware stream learners typically attempt to solve these issues through in- or post-processing by focusing on optimizing one specific discrimination metric, addressing class imbalance in a separate processing step. While C-SMOTE is a highly effective model-agnostic pre-processing approach to mitigate class imbalance, as a side effect of this method, algorithmic bias is often introduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant - as a pre-processing approach to simultaneously address the class imbalance and fairness concerns by employing situation testing and balancing fairness-relevant groups during oversampling. Unlike other fairness-aware stream learners, CFSMOTE is not optimizing for only one specific fairness metric, therefore avoiding potentially problematic trade-offs. Our experiments show significant improvement on several common group fairness metrics in comparison to vanilla C-SMOTE while maintaining competitive performance, also in comparison to other fairness-aware algorithms.",
      "tldr_zh": "éšç€åœ¨çº¿æœºå™¨å­¦ä¹ åœ¨å¤„ç†æ¼”è¿›æ•°æ®æµä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç®—æ³•çš„å…¬å¹³æ€§å’Œç±»åˆ«ä¸å¹³è¡¡(class imbalance)é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç°æœ‰çš„å…¬å¹³æ€§æ„ŸçŸ¥æµå­¦ä¹ å™¨é€šå¸¸ä»…é’ˆå¯¹ç‰¹å®šæ­§è§†æŒ‡æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œè€Œé«˜æ•ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„C-SMOTEæ–¹æ³•å¾€å¾€ä¼šå‰¯ä½œç”¨åœ°å¼•å…¥ç®—æ³•åå·®(algorithmic bias)ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†CFSMOTEï¼Œè¿™æ˜¯ä¸€ç§å…¬å¹³æ€§æ„ŸçŸ¥çš„è¿ç»­SMOTEå˜ä½“ï¼Œæ—¨åœ¨é€šè¿‡é¢„å¤„ç†é˜¶æ®µåŒæ—¶è§£å†³ç±»åˆ«ä¸å¹³è¡¡å’Œå…¬å¹³æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨è¿‡é‡‡æ ·è¿‡ç¨‹ä¸­é‡‡ç”¨æƒ…å¢ƒæµ‹è¯•(situation testing)å¹¶å¹³è¡¡ä¸å…¬å¹³æ€§ç›¸å…³çš„ç¾¤ä½“ï¼Œæœ‰æ•ˆåœ°é¿å…äº†ä»…ä¼˜åŒ–å•ä¸€å…¬å¹³æ€§æŒ‡æ ‡å¯èƒ½å¸¦æ¥çš„æƒè¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹çš„C-SMOTEç›¸æ¯”ï¼ŒCFSMOTEåœ¨å¤šé¡¹å¸¸ç”¨ç¾¤ä½“å…¬å¹³æ€§(group fairness)æŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚åœ¨æ˜¾è‘—æ”¹å–„å…¬å¹³æ€§çš„åŒæ—¶ï¼Œè¯¥ç®—æ³•ä¾ç„¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„é¢„æµ‹æ€§èƒ½ï¼Œä¸ºå¤„ç†ä¸å¹³è¡¡æ•°æ®æµæä¾›äº†ä¸€ç§å…¼é¡¾æ€§èƒ½ä¸å…¬å¹³çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13116v1",
      "published_date": "2025-05-19 13:46:47 UTC",
      "updated_date": "2025-05-19 13:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:37.933590+00:00"
    },
    {
      "arxiv_id": "2505.13115v1",
      "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning",
      "title_zh": "LALMs æ—¶é—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ä¸ç½®ä¿¡åº¦è¯„ä¼°",
      "authors": [
        "Debarpan Bhattacharya",
        "Apoorva Kulkarni",
        "Sriram Ganapathy"
      ],
      "abstract": "The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ (LALMs) åœ¨æ—¶é—´æ¨ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º TREA (Temporal Reasoning Evaluation of Audio) çš„åˆ›æ–°æ•°æ®é›†ã€‚ä½œè€…é€šè¿‡å¯¹å¼€æº LALMs è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè§‚å¯Ÿåˆ°è¿™äº›æ¨¡å‹åœ¨ TREA ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸€è‡´è½åäºäººç±»æ°´å¹³ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§æŒ‡æ ‡ (Uncertainty Metric)ï¼Œè¯¥æŒ‡æ ‡é€šè¿‡è®¡ç®—æ¨¡å‹å¯¹è¯­ä¹‰ç›¸åŒæ‰°åŠ¨è¾“å…¥çš„ä¸å˜æ€§æ¥è¡¡é‡å…¶ç½®ä¿¡åº¦ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œå‡†ç¡®ç‡ä¸ä¸ç¡®å®šæ€§æŒ‡æ ‡ä¹‹é—´å¹¶ä¸ä¸€å®šå­˜åœ¨ç›¸å…³æ€§ï¼Œè¿™ä¸€å‘ç°æ­ç¤ºäº†åœ¨æ¶‰åŠé«˜é£é™©çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œå¯¹ LALMs è¿›è¡Œå…¨é¢å¤šç»´åº¦è¯„ä¼°çš„ç´§è¿«æ€§å’Œå¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands",
      "pdf_url": "https://arxiv.org/pdf/2505.13115v1",
      "published_date": "2025-05-19 13:46:35 UTC",
      "updated_date": "2025-05-19 13:46:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:03.248099+00:00"
    },
    {
      "arxiv_id": "2506.08020v1",
      "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation",
      "title_zh": "é¢å‘éƒ¨åˆ†é¢†åŸŸè‡ªé€‚åº”çš„åŒå±‚éå¹³è¡¡æœ€ä¼˜ä¼ è¾“",
      "authors": [
        "Zi-Ying Chen",
        "Chuan-Xian Ren",
        "Hong Yan"
      ],
      "abstract": "Partial domain adaptation (PDA) problem requires aligning cross-domain samples while distinguishing the outlier classes for accurate knowledge transfer. The widely used weighting framework tries to address the outlier classes by introducing the reweighed source domain with a similar label distribution to the target domain. However, the empirical modeling of weights can only characterize the sample-wise relations, which leads to insufficient exploration of cluster structures, and the weights could be sensitive to the inaccurate prediction and cause confusion on the outlier classes. To tackle these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model to simultaneously characterize the sample-wise and class-wise relations in a unified transport framework. Specifically, a cooperation mechanism between sample-level and class-level transport is introduced, where the sample-level transport provides essential structure information for the class-level knowledge transfer, while the class-level transport supplies discriminative information for the outlier identification. The bi-level transport plan provides guidance for the alignment process. By incorporating the label-aware transport cost, the local transport structure is ensured and a fast computation formulation is derived to improve the efficiency. Extensive experiments on benchmark datasets validate the competitiveness of BUOT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éƒ¨åˆ†åŸŸè‡ªé€‚åº” (Partial Domain Adaptation, PDA) ä¸­è·¨åŸŸå¯¹é½ä¸è¯†åˆ«ç¦»ç¾¤ç±»åˆ«çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŒå±‚ä¸å¹³è¡¡æœ€ä¼˜ä¼ è¾“ (Bi-level Unbalanced Optimal Transport, BUOT) æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹åŒæ—¶åˆ»ç”»æ ·æœ¬çº§å’Œç±»çº§å…³ç³»ã€‚BUOT å»ºç«‹äº†æ ·æœ¬å±‚ä¸ç±»å±‚ä¼ è¾“ä¹‹é—´çš„åä½œæœºåˆ¶ï¼Œåˆ©ç”¨æ ·æœ¬çº§ä¼ è¾“æ•è·çš„å…³é”®ç»“æ„ä¿¡æ¯æ”¯æŒç±»çº§çŸ¥è¯†è¿ç§»ï¼ŒåŒæ—¶é€šè¿‡ç±»çº§ä¼ è¾“è·å–åˆ¤åˆ«ä¿¡æ¯ä»¥ç²¾å‡†è¯†åˆ«ç¦»ç¾¤ç±»åˆ«ã€‚ä¸ºäº†ç¡®ä¿å±€éƒ¨ä¼ è¾“ç»“æ„å¹¶æå‡è¿ç®—æ•ˆç‡ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†æ ‡ç­¾æ„ŸçŸ¥ä¼ è¾“ä»£ä»· (label-aware transport cost) å¹¶æ¨å¯¼å‡ºäº†å¿«é€Ÿè®¡ç®—å…¬å¼ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒBUOT å±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»ŸåŠ æƒæ¡†æ¶å¯¹é¢„æµ‹è¯¯å·®æ•æ„ŸåŠèšç±»ç»“æ„æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„çŸ¥è¯†è¿ç§»ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08020v1",
      "published_date": "2025-05-19 13:40:40 UTC",
      "updated_date": "2025-05-19 13:40:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:55.942653+00:00"
    },
    {
      "arxiv_id": "2505.13109v3",
      "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
      "title_zh": "FreeKVï¼šæå‡ KV ç¼“å­˜æ£€ç´¢ä»¥å®ç°é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Guangda Liu",
        "Chengwei Li",
        "Zhenyu Ning",
        "Jing Lin",
        "Yiwu Yao",
        "Danning Ke",
        "Minyi Guo",
        "Jieru Zhao"
      ],
      "abstract": "Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\\times$ speedup compared to SOTA KV retrieval methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FreeKVï¼Œä¸€ä¸ªç®—æ³•ä¸ç³»ç»ŸååŒä¼˜åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ç”±KV cacheå¢é•¿å¼•èµ·çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰KV cacheå‹ç¼©æ–¹æ³•ä¸­KV droppingå¯¼è‡´çš„ç²¾åº¦æŸå¤±ä»¥åŠKV retrievalé¢ä¸´çš„æ•ˆç‡ç“¶é¢ˆï¼ŒFreeKVé€šè¿‡ååŒè®¾è®¡æå‡äº†æ£€ç´¢æ•ˆç‡å¹¶ä¿æŒäº†ç²¾åº¦ã€‚åœ¨ç®—æ³•å±‚é¢ï¼ŒFreeKVå¼•å…¥äº†æ¨æµ‹æ£€ç´¢(speculative retrieval)æœºåˆ¶ï¼Œå°†KVé€‰æ‹©å’Œå¬å›è¿‡ç¨‹ç§»å‡ºå…³é”®è·¯å¾„ï¼Œå¹¶ç»“åˆç»†ç²’åº¦ä¿®æ­£(fine-grained correction)ç¡®ä¿å‡†ç¡®æ€§ã€‚åœ¨ç³»ç»Ÿå±‚é¢ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨è·¨CPUå’ŒGPUå†…å­˜çš„æ··åˆKVå¸ƒå±€ä»¥æ¶ˆé™¤ç¢ç‰‡åŒ–æ•°æ®ä¼ è¾“ï¼Œå¹¶åˆ©ç”¨åŒç¼“å†²æµå¼å¬å›(double-buffered streamed recall)è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFreeKVåœ¨å¤šç§åœºæ™¯å’Œæ¨¡å‹ä¸‹å‡å®ç°äº†è¿‘ä¹æ— æŸçš„ç²¾åº¦ï¼Œä¸”ä¸ç°æœ‰çš„SOTA KV retrievalæ–¹æ³•ç›¸æ¯”ï¼Œæœ€é«˜å¯å®ç°13å€çš„åŠ é€Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13109v3",
      "published_date": "2025-05-19 13:36:45 UTC",
      "updated_date": "2025-12-16 04:58:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:51.190490+00:00"
    },
    {
      "arxiv_id": "2505.13102v2",
      "title": "Lightweight and Interpretable Transformer via Mixed Graph Algorithm Unrolling for Traffic Forecast",
      "title_zh": "åŸºäºæ··åˆå›¾ç®—æ³•å±•å¼€çš„è½»é‡çº§å¯è§£é‡Š Transformer äº¤é€šé¢„æµ‹",
      "authors": [
        "Ji Qi",
        "Tam Thuc Do",
        "Mingxiao Liu",
        "Zhuoshi Pan",
        "Yuzhe Li",
        "Gene Cheung",
        "H. Vicky Zhao"
      ],
      "abstract": "Unlike conventional \"black-box\" transformers with classical self-attention mechanism, we build a lightweight and interpretable transformer-like neural net by unrolling a mixed-graph-based optimization algorithm to forecast traffic with spatial and temporal dimensions. We construct two graphs: an undirected graph $\\mathcal{G}^u$ capturing spatial correlations across geography, and a directed graph $\\mathcal{G}^d$ capturing sequential relationships over time. We predict future samples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both $\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and $\\ell_1$-norm variational terms to quantify and promote signal smoothness (low-frequency reconstruction) on a directed graph. We design an iterative algorithm based on alternating direction method of multipliers (ADMM), and unroll it into a feed-forward network for data-driven parameter learning. We insert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$ that play the role of self-attention. Experiments show that our unrolled networks achieve competitive traffic forecast performance as state-of-the-art prediction schemes, while reducing parameter counts drastically. Our code is available in https://github.com/SingularityUndefined/Unrolling-GSP-STForecast .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ··åˆå›¾ç®—æ³•å±•å¼€(Mixed Graph Algorithm Unrolling)çš„è½»é‡çº§ä¸”å¯è§£é‡Šçš„ç±»Transformerç½‘ç»œï¼Œæ—¨åœ¨è§£å†³äº¤é€šæµé¢„æµ‹ä¸­çš„æ—¶ç©ºå»ºæ¨¡é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºæ— å‘å›¾$\\mathcal{G}^u$å’Œæœ‰å‘å›¾$\\mathcal{G}^d$åˆ†åˆ«æ•æ‰ç©ºé—´åœ°ç†ç›¸å…³æ€§å’Œæ—¶é—´åºåˆ—å…³ç³»ï¼Œå¹¶è®¾è®¡äº†æ–°çš„$\\ell_2$å’Œ$\\ell_1$èŒƒæ•°å˜åˆ†é¡¹æ¥é‡åŒ–å¹¶ä¿ƒè¿›ä¿¡å·åœ¨æœ‰å‘å›¾ä¸Šçš„å¹³æ»‘æ€§ã€‚ç ”ç©¶è€…åˆ©ç”¨äº¤æ›¿æ–¹å‘ä¹˜å­æ³•(ADMM)æ¨å¯¼å‡ºè¿­ä»£ç®—æ³•ï¼Œå¹¶å°†å…¶å±•å¼€ä¸ºå‰é¦ˆç¥ç»ç½‘ç»œä»¥å®ç°æ•°æ®é©±åŠ¨çš„å‚æ•°å­¦ä¹ ã€‚è¯¥æ¶æ„å¼•å…¥å›¾å­¦ä¹ æ¨¡å—æ¥æ›¿ä»£ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›(Self-attention)æœºåˆ¶ï¼Œä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¿˜å¤§å¹…å‡å°‘äº†å‚æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å±•å¼€ç½‘ç»œåœ¨äº¤é€šé¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›(State-of-the-art)æ–¹æ¡ˆç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 4 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.13102v2",
      "published_date": "2025-05-19 13:32:34 UTC",
      "updated_date": "2025-10-12 04:45:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:05.965033+00:00"
    },
    {
      "arxiv_id": "2505.13101v1",
      "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework",
      "title_zh": "ARIW-Frameworkï¼šè‡ªé€‚åº”é²æ£’è¿­ä»£æ°´å°æ¡†æ¶",
      "authors": [
        "Shaowu Wu",
        "Liting Zeng",
        "Wei Lu",
        "Xiangyang Luo"
      ],
      "abstract": "With the rapid rise of large models, copyright protection for generated image content has become a critical security challenge. Although deep learning watermarking techniques offer an effective solution for digital image copyright protection, they still face limitations in terms of visual quality, robustness and generalization. To address these issues, this paper proposes an adaptive robust iterative watermarking framework (ARIW-Framework) that achieves high-quality watermarked images while maintaining exceptional robustness and generalization performance. Specifically, we introduce an iterative approach to optimize the encoder for generating robust residuals. The encoder incorporates noise layers and a decoder to compute robustness weights for residuals under various noise attacks. By employing a parallel optimization strategy, the framework enhances robustness against multiple types of noise attacks. Furthermore, we leverage image gradients to determine the embedding strength at each pixel location, significantly improving the visual quality of the watermarked images. Extensive experiments demonstrate that the proposed method achieves superior visual quality while exhibiting remarkable robustness and generalization against noise attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼å›¾åƒå†…å®¹çš„ç‰ˆæƒä¿æŠ¤éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”é²æ£’è¿­ä»£æ°´å°æ¡†æ¶ ARIW-Frameworkï¼Œæœ‰æ•ˆè§£å†³äº†æ·±åº¦å­¦ä¹ æ°´å°åœ¨è§†è§‰è´¨é‡ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåœ¨äºå¼•å…¥è¿­ä»£æ–¹æ³•ä¼˜åŒ–ç¼–ç å™¨(encoder)ä»¥ç”Ÿæˆé²æ£’æ®‹å·®ï¼Œå¹¶ç»“åˆå™ªå£°å±‚ä¸è§£ç å™¨(decoder)è®¡ç®—ä¸åŒæ”»å‡»ä¸‹çš„é²æ£’æ€§æƒé‡ã€‚åŒæ—¶ï¼Œç ”ç©¶é‡‡ç”¨äº†å¹¶è¡Œä¼˜åŒ–ç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹å¯¹å¤šç§å¤åˆå™ªå£°æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡è§†è§‰éšè”½æ€§ï¼ŒARIW-Framework åˆ©ç”¨å›¾åƒæ¢¯åº¦(image gradients)åŠ¨æ€è°ƒæ•´å„åƒç´ ç‚¹çš„æ°´å°åµŒå…¥å¼ºåº¦ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå“è¶Šè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œåœ¨å¯¹æŠ—å„ç±»å™ªå£°æ”»å‡»æ—¶å±•ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13101v1",
      "published_date": "2025-05-19 13:31:48 UTC",
      "updated_date": "2025-05-19 13:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:47:59.559776+00:00"
    },
    {
      "arxiv_id": "2505.13098v1",
      "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs",
      "title_zh": "LLM-KG-Bench 3.0ï¼šå¤§è¯­è¨€æ¨¡å‹æµ·æ´‹ä¸­è¯­ä¹‰æŠ€æœ¯èƒ½åŠ›çš„æŒ‡å—é’ˆ",
      "authors": [
        "Lars-Peter Meyer",
        "Johannes Frey",
        "Desiree Heim",
        "Felix Brei",
        "Claus Stadler",
        "Kurt Junghanns",
        "Michael Martin"
      ],
      "abstract": "Current Large Language Models (LLMs) can assist developing program code beside many other things, but can they support working with Knowledge Graphs (KGs) as well? Which LLM is offering the best capabilities in the field of Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to determine without checking many answers manually? The LLM-KG-Bench framework in Version 3.0 is designed to answer these questions. It consists of an extensible set of tasks for automated evaluation of LLM answers and covers different aspects of working with semantic technologies. In this paper the LLM-KG-Bench framework is presented in Version 3 along with a dataset of prompts, answers and evaluations generated with it and several state-of-the-art LLMs. Significant enhancements have been made to the framework since its initial release, including an updated task API that offers greater flexibility in handling evaluation tasks, revised tasks, and extended support for various open models through the vllm library, among other improvements. A comprehensive dataset has been generated using more than 30 contemporary open and proprietary LLMs, enabling the creation of exemplary model cards that demonstrate the models' capabilities in working with RDF and SPARQL, as well as comparing their performance on Turtle and JSON-LD RDF serialization tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† LLM-KG-Bench 3.0 æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¯­ä¹‰ç½‘å’ŒçŸ¥è¯†å›¾è°±å·¥ç¨‹ (Knowledge Graph Engineering, KGE) é¢†åŸŸçš„å„é¡¹èƒ½åŠ›ã€‚è¯¥ç‰ˆæœ¬å¯¹æ¡†æ¶è¿›è¡Œäº†é‡å¤§å‡çº§ï¼ŒåŒ…æ‹¬æ›´æ–°äº†ä»»åŠ¡ API ä»¥æå‡è¯„ä¼°çµæ´»æ€§ï¼Œå¹¶åˆ©ç”¨ vllm åº“æ‰©å±•äº†å¯¹å¤šç§å¼€æºæ¨¡å‹çš„å…¼å®¹æ€§ã€‚é€šè¿‡å¯¹ 30 å¤šç§å½“ä»£å¼€æºå’Œå•†ä¸š LLMs çš„æµ‹è¯•ï¼Œç ”ç©¶ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«æç¤ºã€å›ç­”å’Œè¯„ä¼°æ•°æ®çš„ç»¼åˆæ•°æ®é›†ã€‚å®éªŒé‡ç‚¹è€ƒå¯Ÿäº†æ¨¡å‹åœ¨å¤„ç† RDF æ•°æ®ã€ç¼–å†™ SPARQL æŸ¥è¯¢ä»¥åŠåœ¨ Turtle å’Œ JSON-LD åºåˆ—åŒ–ä»»åŠ¡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½å¤Ÿé‡åŒ–æ¨¡å‹åœ¨è¯­ä¹‰æŠ€æœ¯ä¸Šçš„ä¼˜åŠ£ï¼Œè¿˜é€šè¿‡æ¨¡å‹å¡ç‰‡çš„å½¢å¼ç›´è§‚å±•ç¤ºäº†å„æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†æ–¹é¢çš„ä¸“é•¿ã€‚æ€»ä½“è€Œè¨€ï¼ŒLLM-KG-Bench 3.0 ä¸ºåœ¨ Knowledge Graphs (KGs) é¢†åŸŸæ¢ç´¢å’Œåº”ç”¨ LLMs æä¾›äº†é‡è¦çš„å¯¼èˆªæŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "Peer reviewed publication at ESWC 2025 Resources Track",
      "pdf_url": "https://arxiv.org/pdf/2505.13098v1",
      "published_date": "2025-05-19 13:29:27 UTC",
      "updated_date": "2025-05-19 13:29:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:28.886631+00:00"
    },
    {
      "arxiv_id": "2505.13572v3",
      "title": "Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs",
      "title_zh": "Q${}^2$Forgeï¼šé¢å‘çŸ¥è¯†å›¾è°±é—®ç­”çš„èƒœä»»åŠ›é—®é¢˜ä¸SPARQLæŸ¥è¯¢æ„å»º",
      "authors": [
        "Yousouf Taghzouti",
        "Franck Michel",
        "Tao Jiang",
        "Louis-FÃ©lix Nothias",
        "Fabien Gandon"
      ],
      "abstract": "The SPARQL query language is the standard method to access knowledge graphs (KGs). However, formulating SPARQL queries is a significant challenge for non-expert users, and remains time-consuming for the experienced ones. Best practices recommend to document KGs with competency questions and example queries to contextualise the knowledge they contain and illustrate their potential applications. In practice, however, this is either not the case or the examples are provided in limited numbers. Large Language Models (LLMs) are being used in conversational agents and are proving to be an attractive solution with a wide range of applications, from simple question-answering about common knowledge to generating code in a targeted programming language. However, training and testing these models to produce high quality SPARQL queries from natural language questions requires substantial datasets of question-query pairs. In this paper, we present Q${}^2$Forge that addresses the challenge of generating new competency questions for a KG and corresponding SPARQL queries. It iteratively validates those queries with human feedback and LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular, meaning that the different modules of the application (CQ generation, query generation and query refinement) can be used separately, as an integrated pipeline, or replaced by alternative services. The result is a complete pipeline from competency question formulation to query evaluation, supporting the creation of reference query sets for any target KG.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Q${}^2$Forgeï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸ºçŸ¥è¯†å›¾è°± (Knowledge Graphs) ç”Ÿæˆèƒœä»»åŠ›é—®é¢˜ (Competency Questions) åŠå…¶å¯¹åº”çš„ SPARQL æŸ¥è¯¢çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹éä¸“å®¶ç”¨æˆ·ç¼–å†™ SPARQL å›°éš¾ä»¥åŠé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†åŒ®ä¹çš„é—®é¢˜ï¼Œè¯¥å·¥å…·é€šè¿‡é›†æˆæµæ°´çº¿å®ç°äº†ä»é—®é¢˜æ„æ€åˆ°æŸ¥è¯¢è¯„ä¼°çš„å…¨è¿‡ç¨‹ã€‚Q${}^2$Forge é‡‡ç”¨äº†äººç±»åé¦ˆä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºè¯„åˆ¤è€…çš„è¿­ä»£éªŒè¯æœºåˆ¶ï¼Œä»¥æå‡ç”ŸæˆæŸ¥è¯¢çš„å‡†ç¡®æ€§ä¸è´¨é‡ã€‚è¯¥æ¡†æ¶å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ï¼Œå…¶ CQ ç”Ÿæˆã€æŸ¥è¯¢ç”Ÿæˆå’ŒæŸ¥è¯¢ç»†åŒ–æ¨¡å—æ—¢å¯ä½œä¸ºå®Œæ•´æµæ°´çº¿è¿è¡Œï¼Œä¹Ÿèƒ½å•ç‹¬ä½¿ç”¨æˆ–è¢«æ›¿ä»£ã€‚æœ€ç»ˆï¼ŒQ${}^2$Forge ä¸ºä»»ä½•ç›®æ ‡çŸ¥è¯†å›¾è°±æ„å»ºæ ‡å‡†å‚è€ƒæŸ¥è¯¢é›†æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ¡ˆï¼Œå¯¹æ¨åŠ¨çŸ¥è¯†å›¾è°±é—®ç­”æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13572v3",
      "published_date": "2025-05-19 13:26:51 UTC",
      "updated_date": "2025-12-12 09:31:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:23.165316+00:00"
    },
    {
      "arxiv_id": "2505.13094v1",
      "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation",
      "title_zh": "é¢å‘å®æ—¶è¯­éŸ³åˆ†ç¦»çš„åŸºäºæ—¶é¢‘æ³¨æ„åŠ›ç¼“å­˜å­˜å‚¨æ¨¡å‹",
      "authors": [
        "Guo Chen",
        "Kai Li",
        "Runxuan Yang",
        "Xiaolin Hu"
      ],
      "abstract": "Existing causal speech separation models often underperform compared to non-causal models due to difficulties in retaining historical information. To address this, we propose the Time-Frequency Attention Cache Memory (TFACM) model, which effectively captures spatio-temporal relationships through an attention mechanism and cache memory (CM) for historical information storage. In TFACM, an LSTM layer captures frequency-relative positions, while causal modeling is applied to the time dimension using local and global representations. The CM module stores past information, and the causal attention refinement (CAR) module further enhances time-based feature representations for finer granularity. Experimental results showed that TFACM achieveed comparable performance to the SOTA TF-GridNet-Causal model, with significantly lower complexity and fewer trainable parameters. For more details, visit the project page: https://cslikai.cn/TFACM/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å› æœè¯­éŸ³åˆ†ç¦»æ¨¡å‹åœ¨ä¿ç•™å†å²ä¿¡æ¯æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†æ—¶é¢‘æ³¨æ„åŠ›ç¼“å­˜å­˜å‚¨æ¨¡å‹(Time-Frequency Attention Cache Memory, TFACM)ã€‚è¯¥æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å’Œç¼“å­˜å­˜å‚¨(Cache Memory, CM)æ¨¡å—æœ‰æ•ˆæ•è·æ—¶ç©ºå…³ç³»ï¼Œåˆ©ç”¨LSTMå±‚æ•æ‰é¢‘ç‡ç›¸å…³ä½ç½®ï¼Œå¹¶åœ¨æ—¶é—´ç»´åº¦ä¸Šç»“åˆå±€éƒ¨ä¸å…¨å±€è¡¨ç¤ºè¿›è¡Œå› æœå»ºæ¨¡ã€‚å…¶ä¸­CMæ¨¡å—è´Ÿè´£å­˜å‚¨å†å²ä¿¡æ¯ï¼Œè€Œå› æœæ³¨æ„åŠ›ç»†åŒ–(Causal Attention Refinement, CAR)æ¨¡å—åˆ™è¿›ä¸€æ­¥å¢å¼ºäº†æ—¶é—´ç‰¹å¾è¡¨ç¤ºçš„ç²’åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTFACMåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†ä¸å½“å‰SOTAæ¨¡å‹TF-GridNet-Causalç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†ç³»ç»Ÿçš„å¤æ‚åº¦å¹¶å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13094v1",
      "published_date": "2025-05-19 13:25:51 UTC",
      "updated_date": "2025-05-19 13:25:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:43.090174+00:00"
    },
    {
      "arxiv_id": "2505.13087v1",
      "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings",
      "title_zh": "é¢å‘å›¾ç¥ç»ç½‘ç»œåŸºå‡†æµ‹è¯•ä¸ä½ç½®ç¼–ç å­¦ä¹ çš„å›¾å¯¹é½",
      "authors": [
        "Adrien Lagesse",
        "Marc Lelarge"
      ],
      "abstract": "We propose a novel benchmarking methodology for graph neural networks (GNNs) based on the graph alignment problem, a combinatorial optimization task that generalizes graph isomorphism by aligning two unlabeled graphs to maximize overlapping edges. We frame this problem as a self-supervised learning task and present several methods to generate graph alignment datasets using synthetic random graphs and real-world graph datasets from multiple domains. For a given graph dataset, we generate a family of graph alignment datasets with increasing difficulty, allowing us to rank the performance of various architectures. Our experiments indicate that anisotropic graph neural networks outperform standard convolutional architectures. To further demonstrate the utility of the graph alignment task, we show its effectiveness for unsupervised GNN pre-training, where the learned node embeddings outperform other positional encodings on three molecular regression tasks and achieve state-of-the-art results on the PCQM4Mv2 dataset with significantly fewer parameters. To support reproducibility and further research, we provide an open-source Python package to generate graph alignment datasets and benchmark new GNN architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå›¾å¯¹é½(Graph Alignment)é—®é¢˜çš„å›¾ç¥ç»ç½‘ç»œ(GNNs)åŸºå‡†æµ‹è¯•æ–°æ–¹æ³•ï¼Œè¯¥é—®é¢˜æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡å¯¹é½ä¸¤ä¸ªæ— æ ‡ç­¾å›¾ä»¥æœ€å¤§åŒ–é‡å è¾¹çš„ç»„åˆä¼˜åŒ–ä»»åŠ¡ã€‚ç ”ç©¶è€…å°†æ­¤é—®é¢˜è½¬åŒ–ä¸ºè‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨åˆæˆéšæœºå›¾å’Œå¤šé¢†åŸŸçœŸå®æ•°æ®é›†ç”Ÿæˆäº†éš¾åº¦é€’å¢çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨ä»¥è¯„ä¼°ä¸åŒGNNæ¶æ„çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œå„å‘å¼‚æ€§(anisotropic)å›¾ç¥ç»ç½‘ç»œçš„è¡¨ç°ä¼˜äºæ ‡å‡†çš„å·ç§¯æ¶æ„ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å›¾å¯¹é½åœ¨æ— ç›‘ç£GNNé¢„è®­ç»ƒä¸­çš„æ½œåŠ›ï¼Œå…¶å­¦ä¹ åˆ°çš„èŠ‚ç‚¹åµŒå…¥(node embeddings)åœ¨åˆ†å­å›å½’ä»»åŠ¡ä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„ä½ç½®ç¼–ç (positional encodings)ï¼Œå¹¶åœ¨PCQM4Mv2æ•°æ®é›†ä¸Šä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºPythonåŒ…ä»¥æ”¯æŒåç»­çš„å¤ç°ä¸ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13087v1",
      "published_date": "2025-05-19 13:22:17 UTC",
      "updated_date": "2025-05-19 13:22:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:45.303047+00:00"
    },
    {
      "arxiv_id": "2505.13082v1",
      "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers",
      "title_zh": "MultiActor-Audiobookï¼šèåˆå¤šå‘è¨€è€…äººè„¸ä¸å£°éŸ³çš„é›¶æ ·æœ¬æœ‰å£°ä¹¦ç”Ÿæˆ",
      "authors": [
        "Kyeongman Park",
        "Seongho Joo",
        "Kyomin Jung"
      ],
      "abstract": "We introduce MultiActor-Audiobook, a zero-shot approach for generating audiobooks that automatically produces consistent, expressive, and speaker-appropriate prosody, including intonation and emotion. Previous audiobook systems have several limitations: they require users to manually configure the speaker's prosody, read each sentence with a monotonic tone compared to voice actors, or rely on costly training. However, our MultiActor-Audiobook addresses these issues by introducing two novel processes: (1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based Script Instruction Generation**). With these two processes, MultiActor-Audiobook can generate more emotionally expressive audiobooks with a consistent speaker prosody without additional training. We compare our system with commercial products, through human and MLLM evaluations, achieving competitive results. Furthermore, we demonstrate the effectiveness of MSP and LSI through ablation studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MultiActor-Audiobookï¼Œä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå…·æœ‰ä¸€è‡´ã€å¯Œæœ‰è¡¨ç°åŠ›ä¸”ç¬¦åˆè¯´è¯äººèº«ä»½éŸµå¾‹ï¼ˆåŒ…æ‹¬è¯­è°ƒå’Œæƒ…æ„Ÿï¼‰çš„ Zero-Shot æœ‰å£°ä¹¦ç”Ÿæˆæ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰ç³»ç»Ÿéœ€è¦æ‰‹åŠ¨é…ç½®ã€è¯­æ°”å•è°ƒæˆ–è®­ç»ƒæˆæœ¬è¿‡é«˜ç­‰å±€é™æ€§ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº† MSPï¼ˆMultimodal Speaker Persona Generationï¼‰å’Œ LSIï¼ˆLLM-based Script Instruction Generationï¼‰ä¸¤ä¸ªåˆ›æ–°è¿‡ç¨‹ã€‚é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒMultiActor-Audiobook æ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°æ›´å…·æƒ…æ„Ÿè¡¨ç°åŠ›å’Œä¸€è‡´æ€§çš„è¯´è¯äººéŸµå¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨äººå·¥å’Œ MLLM è¯„ä¼°ä¸­å‡è¾¾åˆ°äº†ä¸å•†ä¸šäº§å“ç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº† MSP å’Œ LSI åœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šè§’è‰²æœ‰å£°ä¹¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13082v1",
      "published_date": "2025-05-19 13:13:46 UTC",
      "updated_date": "2025-05-19 13:13:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:48:59.053947+00:00"
    },
    {
      "arxiv_id": "2505.13079v1",
      "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR",
      "title_zh": "åŸºäºæœ€ä¼˜ä¼ è¾“å›¾åŒ¹é…çš„ ASR è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»å­¦ä¹ ",
      "authors": [
        "Xugang Lu",
        "Peng Shen",
        "Yu Tsao",
        "Hisashi Kawai"
      ],
      "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to acoustic feature learning has proven effective in enhancing end-to-end automatic speech recognition (E2E-ASR). However, aligning representations between linguistic and acoustic modalities remains a challenge due to inherent modality gaps. Optimal transport (OT) has shown promise in mitigating these gaps by minimizing the Wasserstein distance (WD) between linguistic and acoustic feature distributions. However, previous OT-based methods overlook structural relationships, treating feature vectors as unordered sets. To address this, we propose Graph Matching Optimal Transport (GM-OT), which models linguistic and acoustic sequences as structured graphs. Nodes represent feature embeddings, while edges capture temporal and sequential relationships. GM-OT minimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD) (between edges), leading to a fused Gromov-Wasserstein distance (FGWD) formulation. This enables structured alignment and more efficient knowledge transfer compared to existing OT-based approaches. Theoretical analysis further shows that prior OT-based methods in linguistic knowledge transfer can be viewed as a special case within our GM-OT framework. We evaluate GM-OT on Mandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge transfer. Experimental results demonstrate significant performance gains over state-of-the-art models, validating the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(E2E-ASR)ä¸­é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(PLM)å‘å£°å­¦ç‰¹å¾å­¦ä¹ è¿ç§»è¯­è¨€çŸ¥è¯†æ—¶å­˜åœ¨çš„æ¨¡æ€é—´éš™é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå›¾åŒ¹é…çš„æœ€ä¼˜ä¼ è¾“(Graph Matching Optimal Transport, GM-OT)æ¡†æ¶ã€‚ä¸ä»¥å¾€å°†ç‰¹å¾å‘é‡è§†ä¸ºæ— åºé›†åˆçš„æœ€ä¼˜ä¼ è¾“(OT)æ–¹æ³•ä¸åŒï¼ŒGM-OTå°†è¯­è¨€å’Œå£°å­¦åºåˆ—å»ºæ¨¡ä¸ºç»“æ„åŒ–å›¾ï¼Œåˆ©ç”¨èŠ‚ç‚¹è¡¨ç¤ºç‰¹å¾åµŒå…¥å¹¶ç”±è¾¹æ•æ‰æ—¶é—´ä¸é¡ºåºå…³ç³»ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–èåˆGromov-Wassersteinè·ç¦»(FGWD)ï¼ŒåŒæ—¶ä¼˜åŒ–äº†èŠ‚ç‚¹é—´çš„Wassersteinè·ç¦»(WD)å’Œè¾¹ä¹‹é—´çš„Gromov-Wassersteinè·ç¦»(GWD)ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„ç»“æ„åŒ–å¯¹é½ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œå…ˆå‰åŸºäºOTçš„è¯­è¨€çŸ¥è¯†è¿ç§»æ–¹æ³•å¯è¢«è§†ä¸ºGM-OTæ¡†æ¶çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGM-OTåœ¨ä¸­æ–‡ASRä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå……åˆ†éªŒè¯äº†è¯¥è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "To appear in Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13079v1",
      "published_date": "2025-05-19 13:13:18 UTC",
      "updated_date": "2025-05-19 13:13:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:06.373790+00:00"
    },
    {
      "arxiv_id": "2505.13077v2",
      "title": "Advancing Sequential Numerical Prediction in Autoregressive Models",
      "title_zh": "æ¨è¿›è‡ªå›å½’æ¨¡å‹ä¸­çš„åºåˆ—æ•°å€¼é¢„æµ‹",
      "authors": [
        "Xiang Fei",
        "Jinghui Lu",
        "Qi Sun",
        "Hao Feng",
        "Yanjie Wang",
        "Wei Shi",
        "An-Lan Wang",
        "Jingqun Tang",
        "Can Huang"
      ],
      "abstract": "Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªå›å½’æ¨¡å‹åœ¨å¤„ç†æ•°å€¼åºåˆ—é¢„æµ‹æ—¶çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºæ ‡å‡†æ–¹æ³•å°†æ•°å­—è§†ä¸ºç‹¬ç«‹æ ‡è®°å¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±(Cross-Entropy Loss)ï¼Œå¿½è§†äº†æ•°å€¼åºåˆ—çš„å†…åœ¨ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†æ•°å€¼æ ‡è®°å®Œæ•´æ€§æŸå¤±(Numerical Token Integrity Loss, NTIL)ï¼Œæ—¨åœ¨ä»ä¸¤ä¸ªå±‚é¢ä¼˜åŒ–é¢„æµ‹ç²¾åº¦ã€‚åœ¨æ ‡è®°å±‚é¢(Token-level)ï¼ŒNTIL æ‰©å±•äº†æ¨åœŸæœºè·ç¦»(Earth Mover's Distance, EMD)ä»¥ä¿ç•™æ•°å€¼ä¹‹é—´çš„åºæ•°å…³ç³»ï¼›åœ¨åºåˆ—å±‚é¢(Sequence-level)ï¼Œå®ƒæƒ©ç½šäº†é¢„æµ‹åºåˆ—ä¸å®é™…åºåˆ—ä¹‹é—´çš„æ•´ä½“å·®å¼‚ã€‚è¿™ç§åŒé‡æ–¹æ³•ä¸ä»…æ˜¾è‘—æå‡äº†æ•°å€¼é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°é›†æˆåˆ°å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNTIL åœ¨å„é¡¹æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºå¢å¼ºè‡ªå›å½’æ¨¡å‹çš„æ•°å€¼æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.13077v2",
      "published_date": "2025-05-19 13:11:28 UTC",
      "updated_date": "2025-05-28 10:48:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:19.037806+00:00"
    },
    {
      "arxiv_id": "2505.13076v1",
      "title": "The Hidden Dangers of Browsing AI Agents",
      "title_zh": "æµè§ˆå™¨ AI æ™ºèƒ½ä½“çš„å®‰å…¨éšæ‚£",
      "authors": [
        "Mykyta Mudryi",
        "Markiyan Chaklosh",
        "Grzegorz WÃ³jcik"
      ],
      "abstract": "Autonomous browsing agents powered by large language models (LLMs) are increasingly used to automate web-based tasks. However, their reliance on dynamic content, tool execution, and user-provided data exposes them to a broad attack surface. This paper presents a comprehensive security evaluation of such agents, focusing on systemic vulnerabilities across multiple architectural layers. Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments. To address discovered threats, we propose a defense in depth strategy incorporating input sanitization, planner executor isolation, formal analyzers, and session safeguards. These measures protect against both initial access and post exploitation attack vectors. Through a white box analysis of a popular open source project, Browser Use, we demonstrate how untrusted web content can hijack agent behavior and lead to critical security breaches. Our findings include prompt injection, domain validation bypass, and credential exfiltration, evidenced by a disclosed CVE and a working proof of concept exploit.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„è‡ªä¸»æµè§ˆæ™ºèƒ½ä½“(browsing agents)åœ¨è‡ªåŠ¨åŒ–ç½‘ç»œä»»åŠ¡ä¸­é¢ä¸´çš„å®‰å…¨é£é™©è¿›è¡Œäº†å…¨é¢çš„å®‰å…¨è¯„ä¼°ã€‚ä½œè€…æå‡ºäº†é¦–ä¸ªé’ˆå¯¹è¯¥é¢†åŸŸçš„ç«¯åˆ°ç«¯å¨èƒæ¨¡å‹(threat model)ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†å…¶æ¶æ„å±‚é¢çš„æ¼æ´ã€‚ä¸ºåº”å¯¹è¿™äº›å¨èƒï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—æ·±åº¦é˜²å¾¡(defense in depth)ç­–ç•¥ï¼ŒåŒ…æ‹¬è¾“å…¥è„±æ•(input sanitization)ã€è§„åˆ’æ‰§è¡Œéš”ç¦»(planner executor isolation)ã€å½¢å¼åŒ–åˆ†æå™¨(formal analyzers)ä»¥åŠä¼šè¯ä¿æŠ¤æªæ–½(session safeguards)ã€‚é€šè¿‡å¯¹å¼€æºé¡¹ç›® Browser Use çš„ç™½ç›’åˆ†æï¼Œç ”ç©¶äººå‘˜æˆåŠŸæ¼”ç¤ºäº†æç¤ºè¯æ³¨å…¥(prompt injection)ã€åŸŸåéªŒè¯ç»•è¿‡(domain validation bypass)å’Œå‡­æ®çªƒå–(credential exfiltration)ç­‰å…³é”®å®‰å…¨æ¼æ´ã€‚è¯¥å·¥ä½œé€šè¿‡æŠ«éœ²çš„CVEç¼–å·å’Œæ¦‚å¿µéªŒè¯æ”»å‡»ï¼Œæ­ç¤ºäº†ä¸å¯ä¿¡ç½‘é¡µå†…å®¹åŠ«æŒæ™ºèƒ½ä½“è¡Œä¸ºçš„ä¸¥é‡é£é™©ï¼Œå¹¶ä¸ºå®ç°å®‰å…¨éƒ¨ç½²æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13076v1",
      "published_date": "2025-05-19 13:10:29 UTC",
      "updated_date": "2025-05-19 13:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:08.669946+00:00"
    },
    {
      "arxiv_id": "2505.13073v1",
      "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹ä»£ç è¡¥å…¨çš„ç»“æ„æ„ŸçŸ¥è¯­æ–™åº“æ„å»ºä¸ç”¨æˆ·æ„ŸçŸ¥å¯¹é½è¯„ä»·æŒ‡æ ‡",
      "authors": [
        "Dengfeng Liu",
        "Jucai Zhai",
        "Xiaoguang Jiang",
        "Ziqun Li",
        "Qianjin Yu",
        "Feng Liu",
        "Rui Ye",
        "Huang Liu",
        "Zhiguo Yang",
        "Yongsheng Du",
        "Fang Tan"
      ],
      "abstract": "Code completion technology based on large language model has significantly improved the development efficiency of programmers. However, in practical applications, there remains a gap between current commonly used code completion evaluation metrics and users' actual perception. To address this issue, we propose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP, from the perspective of probabilistic modeling. Furthermore, to tackle the lack of effective structural semantic modeling and cross-module dependency information in LLMs for repository-level code completion scenarios, we propose a data processing method based on a Structure-Preserving and Semantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis and experimental validation, we demonstrate the superiority of the proposed evaluation metrics in terms of user perception consistency, as well as the effectiveness of the data processing method in enhancing model performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»£ç è¡¥å…¨è¯„ä»·æŒ‡æ ‡ä¸ç”¨æˆ·å®é™…æ„ŸçŸ¥ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œä»æ¦‚ç‡å»ºæ¨¡è§’åº¦æå‡ºäº† LCP å’Œ ROUGE-LCP ä¸¤ç§æ–°å‹è¯„ä¼°æŒ‡æ ‡ã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³ä»“åº“çº§ä»£ç è¡¥å…¨ä¸­ç»“æ„è¯­ä¹‰å»ºæ¨¡å’Œè·¨æ¨¡å—ä¾èµ–ä¿¡æ¯ä¸è¶³çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºç»“æ„ä¿ç•™å’Œè¯­ä¹‰é‡æ’ä»£ç å›¾(SPSR-Graph)çš„æ•°æ®å¤„ç†æ–¹æ³•ã€‚é€šè¿‡æ„å»º SPSR-Graphï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰ä»£ç çš„å¤æ‚ç»“æ„ä¸è¯­ä¹‰è”ç³»ï¼Œä»è€Œä¼˜åŒ–è¯­æ–™åº“æ„å»ºã€‚ç†è®ºåˆ†æä¸å®éªŒç»“æœè¯æ˜ï¼Œæ‰€æè¯„ä¼°æŒ‡æ ‡åœ¨ç”¨æˆ·æ„ŸçŸ¥ä¸€è‡´æ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”è¯¥æ•°æ®å¤„ç†æ–¹æ¡ˆèƒ½æœ‰æ•ˆå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚è¯¥æˆæœä¸ºä¼˜åŒ–ä»£ç ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ä½“ç³»å’Œè®­ç»ƒè¯­æ–™æ„å»ºæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "14 pages,8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13073v1",
      "published_date": "2025-05-19 13:09:32 UTC",
      "updated_date": "2025-05-19 13:09:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:07.371023+00:00"
    },
    {
      "arxiv_id": "2505.13053v1",
      "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation",
      "title_zh": "SNAPE-PMï¼šé¢å‘è‡ªé€‚åº”è§£é‡Šç”Ÿæˆçš„åŠ¨æ€ä¼™ä¼´æ¨¡å‹æ„å»ºä¸åº”ç”¨",
      "authors": [
        "Amelie S. Robrecht",
        "Christoph R. Kowalski",
        "Stefan Kopp"
      ],
      "abstract": "Adapting to the addressee is crucial for successful explanations, yet poses significant challenges for dialogsystems. We adopt the approach of treating explanation generation as a non-stationary decision process, where the optimal strategy varies according to changing beliefs about the explainee and the interaction context. In this paper we address the questions of (1) how to track the interaction context and the relevant listener features in a formally defined computational partner model, and (2) how to utilize this model in the dynamically adjusted, rational decision process that determines the currently best explanation strategy. We propose a Bayesian inference-based approach to continuously update the partner model based on user feedback, and a non-stationary Markov Decision Process to adjust decision-making based on the partner model values. We evaluate an implementation of this framework with five simulated interlocutors, demonstrating its effectiveness in adapting to different partners with constant and even changing feedback behavior. The results show high adaptivity with distinct explanation strategies emerging for different partners, highlighting the potential of our approach to improve explainable AI systems and dialogsystems in general.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SNAPE-PMæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯¹è¯ç³»ç»Ÿåœ¨ç”Ÿæˆè§£é‡Šæ—¶éš¾ä»¥æ ¹æ®å—è¯è€…è¿›è¡Œçµæ´»é€‚åº”çš„æŒ‘æˆ˜ã€‚ä½œè€…å°†è§£é‡Šç”Ÿæˆè§†ä¸ºä¸€ä¸ªéå¹³ç¨³å†³ç­–è¿‡ç¨‹(non-stationary decision process)ï¼Œè®¤ä¸ºæœ€ä¼˜ç­–ç•¥åº”éšè§£é‡Šå¯¹è±¡(explainee)çŠ¶æ€å’Œäº’åŠ¨è¯­å¢ƒçš„å˜åŒ–è€ŒåŠ¨æ€è°ƒæ•´ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºBayesian inferenceçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç”¨æˆ·åé¦ˆæŒç»­æ›´æ–°ä¼™ä¼´æ¨¡å‹(partner model)ï¼Œå¹¶ç»“åˆéå¹³ç¨³Markov Decision Process(MDP)æ¥ä¼˜åŒ–å½“å‰çš„è§£é‡Šç­–ç•¥ã€‚é€šè¿‡å¯¹äº”ä¸ªæ¨¡æ‹Ÿå¯¹è¯è€…(simulated interlocutors)çš„å®éªŒè¯„ä¼°ï¼Œç»“æœè¯æ˜è¯¥ç³»ç»Ÿåœ¨åº”å¯¹ä¸åŒä¸”åé¦ˆè¡Œä¸ºç”šè‡³ä¼šå‘ç”Ÿå˜åŒ–çš„ä¼™ä¼´æ—¶å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè§‚å¯Ÿåˆ°é’ˆå¯¹ä¸åŒä¼™ä¼´æ¼”åŒ–å‡ºäº†æˆªç„¶ä¸åŒçš„è§£é‡Šç­–ç•¥ï¼Œä½“ç°äº†æé«˜çš„é€‚åº”æ€§(adaptivity)ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæå‡å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)åŠé€šç”¨å¯¹è¯ç³»ç»Ÿçš„äº¤äº’è´¨é‡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "currently under review at Frontiers in Communication",
      "pdf_url": "https://arxiv.org/pdf/2505.13053v1",
      "published_date": "2025-05-19 12:42:23 UTC",
      "updated_date": "2025-05-19 12:42:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:22.485588+00:00"
    },
    {
      "arxiv_id": "2505.13044v1",
      "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents",
      "title_zh": "CAIMï¼šé¢å‘ä¸æ™ºèƒ½ä½“é•¿æœŸäº¤äº’çš„è®¤çŸ¥äººå·¥æ™ºèƒ½è®°å¿†æ¡†æ¶çš„å¼€å‘ä¸è¯„ä¼°",
      "authors": [
        "Rebecca WesthÃ¤uÃŸer",
        "Frederik Berenz",
        "Wolfgang Minker",
        "Sebastian Zepf"
      ],
      "abstract": "Large language models (LLMs) have advanced the field of artificial intelligence (AI) and are a powerful enabler for interactive systems. However, they still face challenges in long-term interactions that require adaptation towards the user as well as contextual knowledge and understanding of the ever-changing environment. To overcome these challenges, holistic memory modeling is required to efficiently retrieve and store relevant information across interaction sessions for suitable responses. Cognitive AI, which aims to simulate the human thought process in a computerized model, highlights interesting aspects, such as thoughts, memory mechanisms, and decision-making, that can contribute towards improved memory modeling for LLMs. Inspired by these cognitive AI principles, we propose our memory framework CAIM. CAIM consists of three modules: 1.) The Memory Controller as the central decision unit; 2.) the Memory Retrieval, which filters relevant data for interaction upon request; and 3.) the Post-Thinking, which maintains the memory storage. We compare CAIM against existing approaches, focusing on metrics such as retrieval accuracy, response correctness, contextual coherence, and memory storage. The results demonstrate that CAIM outperforms baseline frameworks across different metrics, highlighting its context-awareness and potential to improve long-term human-AI interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é•¿æœŸäº¤äº’ä¸­é¢ä¸´çš„ç”¨æˆ·è‡ªé€‚åº”ã€èƒŒæ™¯çŸ¥è¯†ç†è§£åŠåŠ¨æ€ç¯å¢ƒé€‚åº”ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†å—è®¤çŸ¥äººå·¥æ™ºèƒ½(Cognitive AI)å¯å‘çš„CAIMè®°å¿†æ¡†æ¶ã€‚CAIMç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šä½œä¸ºä¸­å¤®å†³ç­–å•å…ƒçš„Memory Controllerã€è´Ÿè´£æŒ‰éœ€ç­›é€‰ç›¸å…³äº¤äº’æ•°æ®çš„Memory Retrievalï¼Œä»¥åŠç”¨äºç»´æŠ¤è®°å¿†å­˜å‚¨çš„Post-Thinkingæ¨¡å—ã€‚é€šè¿‡ä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”è¯„ä¼°ï¼Œç ”ç©¶äººå‘˜åœ¨æ£€ç´¢å‡†ç¡®ç‡(retrieval accuracy)ã€å›å¤æ­£ç¡®æ€§(response correctness)ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§(contextual coherence)åŠè®°å¿†å­˜å‚¨ç­‰ç»´åº¦å¯¹æ¡†æ¶è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAIMåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¡†æ¶ï¼Œå±•ç¤ºäº†å“è¶Šçš„è¯­å¢ƒæ„ŸçŸ¥(context-awareness)èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆæå‡äº†é•¿æœŸäººæœºäº¤äº’çš„è´¨é‡ï¼Œè¯æ˜äº†æ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹çš„è®°å¿†æœºåˆ¶åœ¨å¢å¼ºæ™ºèƒ½ä½“äº¤äº’èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13044v1",
      "published_date": "2025-05-19 12:33:52 UTC",
      "updated_date": "2025-05-19 12:33:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:43.164415+00:00"
    },
    {
      "arxiv_id": "2505.13043v2",
      "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation",
      "title_zh": "è·¨åŸŸè§†çº¿ä¼°è®¡çš„å¹¿ä¹‰æ ‡ç­¾åç§»è§†è§’",
      "authors": [
        "Hao-Ran Yang",
        "Xiaohui Chen",
        "Chuan-Xian Ren"
      ],
      "abstract": "Aiming to generalize the well-trained gaze estimation model to new target domains, Cross-domain Gaze Estimation (CDGE) is developed for real-world application scenarios. Existing CDGE methods typically extract the domain-invariant features to mitigate domain shift in feature space, which is proved insufficient by Generalized Label Shift (GLS) theory. In this paper, we introduce a novel GLS perspective to CDGE and modelize the cross-domain problem by label and conditional shift problem. A GLS correction framework is presented and a feasible realization is proposed, in which a importance reweighting strategy based on truncated Gaussian distribution is introduced to overcome the continuity challenges in label shift correction. To embed the reweighted source distribution to conditional invariant learning, we further derive a probability-aware estimation of conditional operator discrepancy. Extensive experiments on standard CDGE tasks with different backbone models validate the superior generalization capability across domain and applicability on various models of proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨åŸŸè§†çº¿ä¼°è®¡ (Cross-domain Gaze Estimation, CDGE) ä¸­ç°æœ‰æ–¹æ³•ä»…å…³æ³¨ç‰¹å¾åç§»è€Œå¿½ç•¥å¹¿ä¹‰æ ‡ç­¾åç§» (Generalized Label Shift, GLS) çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäº GLS çš„å…¨æ–°è§†è§’ã€‚è®ºæ–‡å°†è·¨åŸŸé—®é¢˜å»ºæ¨¡ä¸ºæ ‡ç­¾åç§»å’Œæ¡ä»¶åç§»çš„ç»¼åˆæŒ‘æˆ˜ï¼Œå¹¶æ®æ­¤æ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„ GLS æ ¡æ­£æ¡†æ¶ã€‚ä¸ºè§£å†³æ ‡ç­¾åç§»æ ¡æ­£ä¸­çš„è¿ç»­æ€§é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæˆªæ–­é«˜æ–¯åˆ†å¸ƒ (truncated Gaussian distribution) çš„é‡è¦æ€§é‡é‡‡æ · (importance reweighting) ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ¨å¯¼å‡ºäº†æ¦‚ç‡æ„ŸçŸ¥æ¡ä»¶ç®—å­å·®å¼‚ (probability-aware estimation of conditional operator discrepancy) çš„ä¼°è®¡æ–¹æ³•ï¼Œç”¨äºå°†é‡æƒé‡çš„æºåˆ†å¸ƒæœ‰æ•ˆåµŒå…¥æ¡ä»¶ä¸å˜å­¦ä¹ ä¸­ã€‚åœ¨æ ‡å‡† CDGE ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒéª¨å¹²ç½‘ç»œä¸‹å‡æ˜¾è‘—æå‡äº†æ¨¡å‹çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13043v2",
      "published_date": "2025-05-19 12:33:52 UTC",
      "updated_date": "2025-10-28 08:36:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:49:26.950642+00:00"
    },
    {
      "arxiv_id": "2505.13036v1",
      "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025",
      "title_zh": "KIT å‚åŠ  IWSLT 2025 ç¦»çº¿è¯­éŸ³ç¿»è¯‘ä¸æŒ‡ä»¤éµå¾ªè¯„æµ‹çš„å‚èµ›æ–¹æ¡ˆ",
      "authors": [
        "Sai Koneru",
        "Maike ZÃ¼fle",
        "Thai-Binh Nguyen",
        "Seymanur Akti",
        "Jan Niehues",
        "Alexander Waibel"
      ],
      "abstract": "The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†å¡å°”æ–¯é²å„ç†å·¥å­¦é™¢ï¼ˆKarlsruhe Institute of Technologyï¼‰åœ¨ IWSLT 2025 ç¦»çº¿è¯­éŸ³ç¿»è¯‘ï¼ˆOffline STï¼‰ä¸æŒ‡ä»¤éµå¾ªï¼ˆInstruction Following, IFï¼‰èµ›é“çš„æŠ€æœ¯æ–¹æ¡ˆã€‚åœ¨ç¦»çº¿è¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè¯¥å›¢é˜Ÿæå‡ºäº†ä¸€ç§èåˆå¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆAutomatic Speech Recognition, ASRï¼‰ç³»ç»Ÿè¾“å‡ºçš„æµæ°´çº¿ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæ–‡æ¡£çº§ä¸Šä¸‹æ–‡è¿›è¡Œä¿¡æ¯æ•´åˆï¼Œéšåé€šè¿‡ä¸¤æ­¥ç¿»è¯‘ä¸ç»†åŒ–æµç¨‹æå‡è´¨é‡ã€‚é’ˆå¯¹æŒ‡ä»¤éµå¾ªèµ›é“ï¼Œç ”ç©¶è€…å¼€å‘äº†é›†æˆè¯­éŸ³ç¼–ç å™¨ï¼ˆSpeech Encoderï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ‰§è¡ŒåŒ…æ‹¬è¯­éŸ³é—®ç­”å’Œæ‘˜è¦åœ¨å†…çš„å¤šç§ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿè¿˜å¼•å…¥äº†æœ€ç»ˆçš„æ–‡æ¡£çº§ç»†åŒ–é˜¶æ®µï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›ä¸€æ­¥ä¼˜åŒ–è¾“å‡ºç²¾åº¦ã€‚æ•´ä½“æ–¹æ¡ˆè¯æ˜äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¢å¼ºä¼ ç»Ÿè¯­éŸ³ç¿»è¯‘åŠæ–°å…´æŒ‡ä»¤ä»»åŠ¡æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13036v1",
      "published_date": "2025-05-19 12:21:29 UTC",
      "updated_date": "2025-05-19 12:21:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:06.336136+00:00"
    },
    {
      "arxiv_id": "2505.13033v2",
      "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis",
      "title_zh": "TSPulseï¼šé¢å‘å¿«é€Ÿæ—¶é—´åºåˆ—åˆ†æçš„åŒç©ºé—´å¾®å‹é¢„è®­ç»ƒæ¨¡å‹",
      "authors": [
        "Vijay Ekambaram",
        "Subodh Kumar",
        "Arindam Jati",
        "Sumanta Mukherjee",
        "Tomoya Sakai",
        "Pankaj Dayama",
        "Wesley M. Gifford",
        "Jayant Kalagnanam"
      ],
      "abstract": "The rise of time-series pre-trained models has advanced temporal representation learning, but current state-of-the-art models are often large-scale, requiring substantial compute. We introduce TSPulse, ultra-compact time-series pre-trained models with only 1M parameters, specialized to perform strongly across classification, anomaly detection, imputation, and retrieval tasks. TSPulse introduces innovations at both the architecture and task levels. At the architecture level, it employs a dual-space masked reconstruction, learning from both time and frequency domains to capture complementary signals. This is further enhanced by a dual-embedding disentanglement, generating both detailed embeddings for fine-grained analysis and high-level semantic embeddings for broader task understanding. Notably, TSPulse's semantic embeddings are robust to shifts in time, magnitude, and noise, which is important for robust retrieval. At the task level, TSPulse incorporates TSLens, a fine-tuning component enabling task-specific feature attention. It also introduces a multi-head triangulation technique that correlates deviations from multiple prediction heads, enhancing anomaly detection by fusing complementary model outputs. Additionally, a hybrid mask pretraining is proposed to improves zero-shot imputation by reducing pre-training bias. These architecture and task innovations collectively contribute to TSPulse's significant performance gains: 5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly detection leaderboard, +50% in zero-shot imputation, and +25% in time-series retrieval. Remarkably, these results are achieved with just 1M parameters (10-100X smaller than existing SOTA models) and allow GPU-free inference, setting a new standard for efficient time-series pre-trained models. The models can be accessed from https://huggingface.co/ibm-granite/granite-timeseries-tspulse-r1",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TSPulseï¼Œä¸€ç§å‚æ•°é‡ä»…ä¸º 1M çš„è¶…ç´§å‡‘æ—¶é—´åºåˆ—é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å…ˆè¿›æ¨¡å‹è§„æ¨¡è¿‡å¤§ä¸”è®¡ç®—éœ€æ±‚é«˜çš„é—®é¢˜ã€‚åœ¨æ¶æ„è®¾è®¡ä¸Šï¼ŒTSPulse é‡‡ç”¨åŒç©ºé—´æ©ç é‡æ„ (dual-space masked reconstruction) åŒæ—¶ä»æ—¶åŸŸå’Œé¢‘åŸŸæå–äº’è¡¥ä¿¡å·ï¼Œå¹¶ç»“åˆåŒåµŒå…¥è§£è€¦ (dual-embedding disentanglement) æå‡äº†æ¨¡å‹å¯¹å™ªå£°å’Œåç§»çš„é²æ£’æ€§ã€‚åœ¨ä»»åŠ¡ä¼˜åŒ–æ–¹é¢ï¼Œç ”ç©¶å¼•å…¥äº†ç”¨äºä»»åŠ¡ç‰¹å®šç‰¹å¾å…³æ³¨çš„ TSLens ç»„ä»¶ï¼Œä»¥åŠé€šè¿‡èåˆå¤šé¢„æµ‹å¤´è¾“å‡ºå¢å¼ºå¼‚å¸¸æ£€æµ‹çš„ multi-head triangulation æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ··åˆæ©ç é¢„è®­ç»ƒ (hybrid mask pretraining) æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é›¶æ ·æœ¬è¡¥å…¨ (zero-shot imputation) æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒTSPulse åœ¨åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œæ£€ç´¢ç­‰ä»»åŠ¡ä¸Šå‡å–å¾—äº† 5% è‡³ 50% ä¸ç­‰çš„æ€§èƒ½å¢ç›Šï¼Œç›¸æ¯”ç°æœ‰ SOTA æ¨¡å‹ä½“ç§¯ç¼©å°äº† 10-100 å€ã€‚è¯¥æ¨¡å‹æ”¯æŒæ—  GPU æ¨ç†ï¼Œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å®ç°é«˜æ•ˆçš„æ—¶é—´åºåˆ—åˆ†ææ ‘ç«‹äº†æ–°çš„æ ‡æ†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13033v2",
      "published_date": "2025-05-19 12:18:53 UTC",
      "updated_date": "2025-06-25 04:59:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:04.309683+00:00"
    },
    {
      "arxiv_id": "2505.13031v2",
      "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO",
      "title_zh": "MindOmniï¼šåŸºäº RGPO æ¿€å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†ç”Ÿæˆèƒ½åŠ›",
      "authors": [
        "Yicheng Xiao",
        "Lin Song",
        "Yukang Chen",
        "Yingmin Luo",
        "Yuxin Chen",
        "Yukang Gan",
        "Wei Huang",
        "Xiu Li",
        "Xiaojuan Qi",
        "Ying Shan"
      ],
      "abstract": "Recent text-to-image systems face limitations in handling multimodal inputs and complex reasoning tasks. We introduce MindOmni, a unified multimodal large language model that addresses these challenges by incorporating reasoning generation through reinforcement learning. MindOmni leverages a three-phase training strategy: i) design of a unified vision language model with a decoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought (CoT) instruction data, and iii) our proposed Reasoning Generation Policy Optimization (RGPO) algorithm, utilizing multimodal feedback to effectively guide policy updates. Experimental results demonstrate that MindOmni outperforms existing models, achieving impressive performance on both understanding and generation benchmarks, meanwhile showcasing advanced fine-grained reasoning generation capabilities, especially with mathematical reasoning instruction. All codes will be made public at https://github.com/TencentARC/MindOmni",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–‡æœ¬ç”Ÿæˆå›¾åƒç³»ç»Ÿåœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥å’Œå¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº† MindOmniï¼Œä¸€ç§é›†æˆæ¨ç†ç”Ÿæˆèƒ½åŠ›çš„ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Model)ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè®¾è®¡äº†åŒ…å« decoder-only diffusion æ¨¡å—çš„ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨é“¾å¼æ€ç»´ (Chain-of-Thought) æŒ‡ä»¤æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºæå‡ºäº†æ¨ç†ç”Ÿæˆç­–ç•¥ä¼˜åŒ– (Reasoning Generation Policy Optimization, RGPO) ç®—æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€åé¦ˆæœ‰æ•ˆå¼•å¯¼ç­–ç•¥æ›´æ–°ï¼Œä»è€Œé‡Šæ”¾æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMindOmni åœ¨ç†è§£ä¸ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºå…ˆè¿›çš„ç»†ç²’åº¦æ¨ç†ç”Ÿæˆèƒ½åŠ›ï¼Œå°¤å…¶åœ¨æ•°å­¦æ¨ç†æŒ‡ä»¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æˆæœé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€æ¶æ„çš„æ·±åº¦èåˆï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„é€»è¾‘ä¸€è‡´æ€§ä¸ä»»åŠ¡å¤„ç†ç²¾åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Code: https://github.com/TencentARC/MindOmni",
      "pdf_url": "https://arxiv.org/pdf/2505.13031v2",
      "published_date": "2025-05-19 12:17:04 UTC",
      "updated_date": "2025-06-11 15:44:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:14.183742+00:00"
    },
    {
      "arxiv_id": "2505.13028v2",
      "title": "Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset",
      "title_zh": "LLM å®‰å…¨è§£å†³æ–¹æ¡ˆæ•ˆèƒ½è¯„ä¼°ï¼šPalit åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Sayon Palit",
        "Daniel Woods"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model owners.To evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—å’Œé‡‘èç­‰å…³é”®è¡Œä¸šé¢ä¸´çš„æ¶æ„æŸ¥è¯¢ã€æ•°æ®æ³„éœ²åŠæ³•å¾‹è´£ä»»ç­‰å®‰å…¨é£é™©ï¼Œå¯¹ç°æœ‰çš„ LLM å®‰å…¨è§£å†³æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿçš„æ¯”è¾ƒåˆ†æã€‚ä½œè€…å¼€å‘äº† Palit åŸºå‡†æµ‹è¯•æ•°æ®é›† (Palit Benchmark Dataset)ï¼Œå¹¶åˆ©ç”¨è¯¥æ•°æ®é›†å¯¹ 7 ç§å®‰å…¨å·¥å…·åœ¨å¯¹æŠ—æ¶æ„æç¤º (malicious prompts) æ–¹é¢çš„æ•ˆèƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°åŸºå‡†æ¨¡å‹ ChatGPT-3.5-Turbo çš„è¯¯æŠ¥ç‡è¿‡é«˜ï¼Œè€Œ Lakera Guard å’Œ ProtectAI LLM Guard åœ¨æ€§èƒ½ä¸å¯ç”¨æ€§ä¹‹é—´å±•ç°äº†æœ€ä½³çš„å¹³è¡¡ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†åŠ å¼ºé—­æºæä¾›å•†é€æ˜åº¦ã€ä¼˜åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€æµ‹ä»¥åŠé‡‡ç”¨æ›´å…·ä»£è¡¨æ€§çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ç­‰å»ºè®®ï¼Œä¸ºæå‡ LLM ç³»ç»Ÿçš„å®‰å…¨æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13028v2",
      "published_date": "2025-05-19 12:12:00 UTC",
      "updated_date": "2025-05-20 07:34:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:10.273839+00:00"
    },
    {
      "arxiv_id": "2505.13026v3",
      "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs",
      "title_zh": "é¢å‘ç‰¹å®šä»»åŠ¡å¤§è¯­è¨€æ¨¡å‹çš„æœ‰ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ åˆ†é˜¶æ®µè‡ªé€‚åº”èåˆ",
      "authors": [
        "Jack Chen",
        "Fazhong Liu",
        "Naruto Liu",
        "Yuhan Luo",
        "Erqu Qin",
        "Harry Zheng",
        "Tian Dong",
        "Haojin Zhu",
        "Yan Meng",
        "Xiao Wang"
      ],
      "abstract": "Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models' reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„ Supervised Fine-tuning (SFT) è¿‡æ‹Ÿåˆä»¥åŠ Reinforcement Learning (RL) æ¨¡å¼å´©æºƒ(mode collapse)ç­‰æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰é™æ€æ··åˆè®­ç»ƒæ–¹æ¡ˆåœ¨æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®ä¾èµ–æ€§ä¸Šçš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…å—äººç±»æ¨ç†åŸ¹å…»ä¸­çš„è¯¾ç¨‹å­¦ä¹ -æµ‹éªŒæœºåˆ¶å¯å‘ï¼Œæå‡ºäº† SASRï¼Œä¸€ç§é€æ­¥è‡ªé€‚åº”çš„æ··åˆè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ç†è®ºä¸Šç»Ÿä¸€ SFT å’Œ RL å¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å®ç°åŠ¨æ€å¹³è¡¡ã€‚SASR é¦–å…ˆé€šè¿‡ SFT è¿›è¡Œåˆæ­¥çƒ­èº«ä»¥å»ºç«‹åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œéšååˆ©ç”¨åŸºäºæ¢¯åº¦èŒƒæ•°(gradient norm)å’Œç›¸å¯¹äºåŸå§‹åˆ†å¸ƒæ•£åº¦(divergence)çš„è‡ªé€‚åº”åŠ¨æ€è°ƒæ•´ç®—æ³•ï¼Œå°† SFT ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³• GRPO æ— ç¼é›†æˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç›‘æ§æ¨¡å‹çš„è®­ç»ƒçŠ¶æ€å¹¶æŒ‰é¡ºåºè°ƒæ•´è®­ç»ƒæµç¨‹ï¼Œç¡®ä¿äº†è®­ç»ƒæ–¹æ¡ˆé—´çš„å¹³æ»‘è¿‡æ¸¡ï¼Œåœ¨ä¿æŒæ ¸å¿ƒæ¨ç†èƒ½åŠ›çš„åŒæ—¶èƒ½å¤Ÿæ¢ç´¢ä¸åŒçš„è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSASR çš„æ€§èƒ½ä¼˜äºå•çº¯çš„ SFTã€RL ä»¥åŠé™æ€æ··åˆè®­ç»ƒæ–¹æ³•ï¼Œä¸ºæå‡ç‰¹å®šä»»åŠ¡ä¸‹æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13026v3",
      "published_date": "2025-05-19 12:10:17 UTC",
      "updated_date": "2025-08-04 07:29:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:17.445566+00:00"
    },
    {
      "arxiv_id": "2505.13025v1",
      "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation",
      "title_zh": "LiBOGï¼šé¢å‘é»‘ç›’ä¼˜åŒ–å™¨ç”Ÿæˆçš„ç»ˆèº«å­¦ä¹ ",
      "authors": [
        "Jiyuan Pei",
        "Yi Mei",
        "Jialin Liu",
        "Mengjie Zhang"
      ],
      "abstract": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in automating the configuration and generation of black-box optimizers, significantly reducing the human effort required for optimizer design and discovering optimizers with higher performance than classic human-designed optimizers. However, existing MetaBBO methods conduct one-off training under the assumption that a stationary problem distribution with extensive and representative training problem samples is pre-available. This assumption is often impractical in real-world scenarios, where diverse problems following shifting distribution continually arise. Consequently, there is a pressing need for methods that can continuously learn from new problems encountered on-the-fly and progressively enhance their capabilities. In this work, we explore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a novel approach designed to learn from sequentially encountered problems and generate high-performance optimizers for Black-Box Optimization (BBO). LiBOG consolidates knowledge both across tasks and within tasks to mitigate catastrophic forgetting. Extensive experiments demonstrate LiBOG's effectiveness in learning to generate high-performance optimizers in a lifelong learning manner, addressing catastrophic forgetting while maintaining plasticity to learn new tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…ƒé»‘ç›’ä¼˜åŒ–(Meta-Black-Box Optimization, MetaBBO)åœ¨è‡ªåŠ¨åŒ–ä¼˜åŒ–å™¨ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œå¹¶é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€å˜åŒ–çš„é—®é¢˜åˆ†å¸ƒæ—¶å­˜åœ¨çš„å±€é™æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†åä¸ºLiBOGçš„åˆ›æ–°æ–¹æ³•ï¼Œé€šè¿‡ç»ˆèº«å­¦ä¹ (Lifelong Learning)èŒƒå¼ä»é¡ºåºå‡ºç°çš„é—®é¢˜ä¸­æŒç»­å­¦ä¹ å¹¶ç”Ÿæˆé«˜æ€§èƒ½çš„é»‘ç›’ä¼˜åŒ–å™¨ã€‚LiBOGé€šè¿‡åœ¨ä»»åŠ¡é—´å’Œä»»åŠ¡å†…æ•´åˆçŸ¥è¯†ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†æ¨¡å‹åœ¨å­¦ä¹ æ–°çŸ¥è¯†æ—¶äº§ç”Ÿçš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒLiBOGåœ¨å…‹æœé—å¿˜çš„åŒæ—¶ä¿æŒäº†å­¦ä¹ æ–°ä»»åŠ¡çš„å¡‘æ€§(Plasticity)ï¼Œèƒ½å¤Ÿç¨³æ­¥æå‡ç”Ÿæˆä¼˜åŒ–å™¨çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºç°å®ä¸–ç•Œä¸­ä¸æ–­æ¼”åŒ–çš„é»‘ç›’ä¼˜åŒ–(Black-Box Optimization)éœ€æ±‚æä¾›äº†ä¸€ç§æ›´å…·é€‚åº”æ€§å’Œå®ç”¨ä»·å€¼çš„è‡ªåŠ¨åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IJCAI 2025. To appear",
      "pdf_url": "https://arxiv.org/pdf/2505.13025v1",
      "published_date": "2025-05-19 12:09:25 UTC",
      "updated_date": "2025-05-19 12:09:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:27.051662+00:00"
    },
    {
      "arxiv_id": "2505.13023v3",
      "title": "Anti-Inpainting: A Proactive Defense Approach against Malicious Diffusion-based Inpainters under Unknown Conditions",
      "title_zh": "Anti-Inpaintingï¼šä¸€ç§åœ¨æœªçŸ¥æ¡ä»¶ä¸‹å¯¹æŠ—æ¶æ„æ‰©æ•£å›¾åƒä¿®å¤å™¨çš„ä¸»åŠ¨é˜²å¾¡æ–¹æ³•",
      "authors": [
        "Yimao Guo",
        "Zuomin Qu",
        "Wei Lu",
        "Xiangyang Luo"
      ],
      "abstract": "With the increasing prevalence of diffusion-based malicious image manipulation, existing proactive defense methods struggle to safeguard images against tampering under unknown conditions. To address this, we propose Anti-Inpainting, a proactive defense approach that achieves protection comprising three novel modules. First, we introduce a multi-level deep feature extractor to obtain intricate features from the diffusion denoising process, enhancing protective effectiveness. Second, we design a multi-scale, semantic-preserving data augmentation technique to enhance the transferability of adversarial perturbations across unknown conditions. Finally, we propose a selection-based distribution deviation optimization strategy to bolster protection against manipulations guided by diverse random seeds. Extensive experiments on InpaintGuardBench and CelebA-HQ demonstrate that Anti-Inpainting effectively defends against diffusion-based inpainters under unknown conditions. Additionally, our approach demonstrates robustness against various image purification methods and transferability across different diffusion model versions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£æ¨¡å‹(diffusion-based)çš„æ¶æ„å›¾åƒç¯¡æ”¹æ—¥ç›Šæµè¡Œçš„é—®é¢˜ï¼Œæå‡ºäº†Anti-Inpaintingï¼Œä¸€ç§åœ¨æœªçŸ¥æ¡ä»¶ä¸‹å®ç°ä¸»åŠ¨é˜²å¾¡çš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆé¦–å…ˆåˆ©ç”¨å¤šçº§æ·±åº¦ç‰¹å¾æå–å™¨(multi-level deep feature extractor)ä»æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­è·å–å¤æ‚ç‰¹å¾ä»¥å¢å¼ºé˜²å¾¡æ•ˆåŠ›ï¼Œå¹¶è®¾è®¡äº†å¤šå°ºåº¦ã€è¯­ä¹‰ä¿æŒçš„æ•°æ®å¢å¼ºæŠ€æœ¯(multi-scale, semantic-preserving data augmentation)æ¥æå‡å¯¹æŠ—æ‰°åŠ¨åœ¨æœªçŸ¥æ¡ä»¶ä¸‹çš„è¿ç§»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºé€‰æ‹©çš„åˆ†å¸ƒåå·®ä¼˜åŒ–ç­–ç•¥(selection-based distribution deviation optimization)ï¼Œæ—¨åœ¨åŠ å¼ºå¯¹ä¸åŒéšæœºç§å­å¼•å¯¼çš„æ“ä½œçš„é˜²å¾¡ã€‚åœ¨InpaintGuardBenchå’ŒCelebA-HQæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAnti-Inpaintingèƒ½æœ‰æ•ˆå¯¹æŠ—æœªçŸ¥æ¡ä»¶ä¸‹çš„å›¾åƒä¿®å¤ï¼Œä¸”è¡¨ç°å‡ºå¯¹å¤šç§å›¾åƒå‡€åŒ–æ–¹æ³•çš„é²æ£’æ€§ä»¥åŠåœ¨ä¸åŒæ‰©æ•£æ¨¡å‹ç‰ˆæœ¬é—´çš„è‰¯å¥½è¿ç§»æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13023v3",
      "published_date": "2025-05-19 12:07:29 UTC",
      "updated_date": "2025-08-02 11:16:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:26.023925+00:00"
    },
    {
      "arxiv_id": "2505.13011v2",
      "title": "Unveiling and Steering Connectome Organization with Interpretable Latent Variables",
      "title_zh": "åˆ©ç”¨å¯è§£é‡Šæ½œåœ¨å˜é‡æ­ç¤ºä¸è°ƒæ§è¿æ¥ç»„ç»„ç»‡",
      "authors": [
        "Yubin Li",
        "Xingyu Liu",
        "Guozhang Chen"
      ],
      "abstract": "The brain's intricate connectome, a blueprint for its function, presents immense complexity, yet it arises from a compact genetic code, hinting at underlying low-dimensional organizational principles. This work bridges connectomics and representation learning to uncover these principles. We propose a framework that combines subgraph extraction from the Drosophila connectome, FlyWire, with a generative model to derive interpretable low-dimensional representations of neural circuitry. Crucially, an explainability module links these latent dimensions to specific structural features, offering insights into their functional relevance. We validate our approach by demonstrating effective graph reconstruction and, significantly, the ability to manipulate these latent codes to controllably generate connectome subgraphs with predefined properties. This research offers a novel tool for understanding brain architecture and a potential avenue for designing bio-inspired artificial neural networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è„‘å¤æ‚ Connectome èƒŒåéšè—çš„ä½ç»´ç»„ç»‡åŸåˆ™ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆé»‘è…¹æœè‡ FlyWire æ•°æ®åº“å­å›¾æå–ä¸ Generative Model çš„æ–°æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥ Explainability Moduleï¼Œè¯¥æ–¹æ³•æˆåŠŸå°† Latent Dimensions ä¸ç‰¹å®šçš„ç»“æ„ç‰¹å¾ç›¸è”ç³»ï¼Œå®ç°äº†å¯¹ç¥ç»å›è·¯ä½ç»´è¡¨ç¤ºçš„åˆç†è§£é‡Šã€‚å®éªŒä¸ä»…éªŒè¯äº†è¯¥æ¨¡å‹åœ¨ Graph Reconstruction ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¿˜è¯æ˜äº†å…¶å¯ä»¥é€šè¿‡æ“çºµ Latent Codes æ¥å—æ§ç”Ÿæˆå…·æœ‰é¢„å®šä¹‰å±æ€§çš„ Connectome Subgraphsã€‚è¿™é¡¹å·¥ä½œä¸ºæ­ç¤ºå¤§è„‘æ¶æ„æä¾›äº†åˆ›æ–°çš„åˆ†æå·¥å…·ï¼Œå¹¶ä¸ºæœªæ¥è®¾è®¡ Bio-inspired Artificial Neural Networks å¼€è¾Ÿäº†æ½œåœ¨é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13011v2",
      "published_date": "2025-05-19 11:54:40 UTC",
      "updated_date": "2025-05-27 04:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:48.849532+00:00"
    },
    {
      "arxiv_id": "2505.13010v2",
      "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
      "title_zh": "To Bias or Not to Biasï¼šåŸºäº bias-detector çš„æ–°é—»åè§æ£€æµ‹",
      "authors": [
        "Himel Ghosh",
        "Ahmed Mosharafa",
        "Georg Groh"
      ],
      "abstract": "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åª’ä½“åè§æ£€æµ‹ï¼ˆMedia bias detectionï¼‰ä¸­ä¸»è§‚æ€§å¼ºå’Œé«˜è´¨é‡æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº RoBERTa å¾®è°ƒçš„å¥å­çº§åè§åˆ†ç±»æ¨¡å‹ã€‚é€šè¿‡åœ¨ä¸“å®¶æ ‡æ³¨çš„ BABE æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨ McNemar's test å’Œ 5x2 cross-validation paired t-test è¿›è¡Œç»Ÿè®¡æ£€éªŒï¼Œç»“æœè¯æ˜è¯¥æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„ DA-RoBERTa åŸºçº¿ã€‚æ³¨æ„åŠ›åˆ†æï¼ˆAttention-based analysisï¼‰æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé¿å¼€æ”¿æ²»æ•æ„Ÿè¯æ±‡çš„é™·é˜±ï¼Œæ›´ç²¾å‡†åœ°æ•æ‰ä¸Šä¸‹æ–‡ç›¸å…³çš„æ ‡è®°ï¼ˆtokensï¼‰ã€‚ç ”ç©¶è¿˜é€šè¿‡é›†æˆç°æœ‰çš„åè§ç±»å‹åˆ†ç±»å™¨æ„å»ºäº†ä¸€å¥—æµæ°´çº¿ï¼Œå±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚å°½ç®¡å—é™äºå¥å­çº§åˆ†æå’Œæ•°æ®é›†è§„æ¨¡ï¼Œè¯¥å·¥ä½œä»ä¸ºå¼€å‘æ›´å…·ç¤¾ä¼šè´£ä»»æ„Ÿå’Œè§£é‡Šæ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿï¼ˆNLP systemsï¼‰æä¾›äº†é‡è¦æ”¯æŒï¼Œå¹¶æŒ‡æ˜äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡å’Œåè§ä¸­æ€§åŒ–ç­‰æœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 5 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.13010v2",
      "published_date": "2025-05-19 11:54:39 UTC",
      "updated_date": "2025-12-26 23:06:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:34.292567+00:00"
    },
    {
      "arxiv_id": "2505.12996v1",
      "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning",
      "title_zh": "ExTransï¼šåŸºäºç¤ºä¾‹å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„å¤šè¯­è¨€æ·±åº¦æ¨ç†ç¿»è¯‘",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨ç¥ç»æœºå™¨ç¿»è¯‘(Neural Machine Translation, MT)ä¸­å¤šè¯­è¨€è¦†ç›–ä¸è¶³åŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å¥–åŠ±å»ºæ¨¡æ•ˆç‡è¾ƒä½çš„é—®é¢˜ï¼Œæå‡ºäº†ExTransã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ç§åˆ›æ–°çš„å¥–åŠ±å»ºæ¨¡æ–¹å¼ï¼Œé€šè¿‡é‡åŒ–å¯¹æ¯”ç­–ç•¥æ¨¡å‹ä¸å¼ºæ¨ç†æ¨¡å‹DeepSeek-R1-671Bçš„ç¿»è¯‘ç»“æœæ¥æä¾›å¥–åŠ±ã€‚ç ”ç©¶ä»¥Qwen2.5-7B-Instructä¸ºéª¨å¹²æ¨¡å‹ï¼Œåœ¨æ–‡å­¦ç¿»è¯‘ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„å…ˆè¿›æ°´å¹³(SOTA)ï¼Œæ€§èƒ½ä¼˜äºOpenAI-o1å’ŒDeepSeek-R1ã€‚æ­¤å¤–ï¼Œå›¢é˜Ÿå°†è¯¥æ–¹æ³•æ‰©å±•è‡³11ç§è¯­è¨€ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è½»é‡åŒ–å¥–åŠ±å»ºæ¨¡ï¼ŒæˆåŠŸå°†å¼ºå¤§çš„ç¿»è¯‘èƒ½åŠ›è¿ç§»è‡³90ä¸ªç¿»è¯‘æ–¹å‘ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒExTransåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹å±•ç°äº†å“è¶Šçš„æ·±åº¦æ¨ç†ç¿»è¯‘æ€§èƒ½ï¼Œä¸ºæå‡ç¿»è¯‘è´¨é‡å’Œå¤šè¯­è¨€æ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12996v1",
      "published_date": "2025-05-19 11:34:47 UTC",
      "updated_date": "2025-05-19 11:34:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:50:38.551651+00:00"
    },
    {
      "arxiv_id": "2505.12992v3",
      "title": "Fractured Chain-of-Thought Reasoning",
      "title_zh": "ç¢è£‚å¼æ€ç»´é“¾æ¨ç†",
      "authors": [
        "Baohao Liao",
        "Hanze Dong",
        "Yuhui Xu",
        "Doyen Sahoo",
        "Christof Monz",
        "Junnan Li",
        "Caiming Xiong"
      ],
      "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at https://github.com/BaohaoLiao/frac-cot.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†é˜¶æ®µçš„æ‰©å±•æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³é“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºè¯å› ç”Ÿæˆå†—é•¿ä¸­é—´è½¨è¿¹è€Œå¸¦æ¥çš„é«˜æ˜‚Tokenæˆæœ¬é—®é¢˜ã€‚ä½œè€…é¦–å…ˆå‘ç°æˆªæ–­å¼CoT(Truncated CoT)åœ¨å¤§å¹…å‡å°‘Tokenä½¿ç”¨çš„åŒæ—¶ï¼Œå¾€å¾€èƒ½è¾¾åˆ°ä¸å®Œæ•´CoTç›¸å½“çš„å‡†ç¡®ç‡ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºFractured Samplingçš„ç»Ÿä¸€æ¨ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡æ¨ç†è·¯å¾„æ•°é‡ã€æ¯æ¡è·¯å¾„çš„æ–¹æ¡ˆæ•°é‡ä»¥åŠæ¨ç†è½¨è¿¹æˆªæ–­æ·±åº¦è¿™ä¸‰ä¸ªæ­£äº¤ç»´åº¦ï¼Œåœ¨å®Œæ•´CoTå’Œä»…æ–¹æ¡ˆé‡‡æ ·ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFractured Samplingåœ¨Pass@kä¸Tokené¢„ç®—çš„æƒè¡¡ä¸­å®ç°äº†æ˜¾è‘—çš„å¯¹æ•°çº¿æ€§æ‰©å±•å¢ç›Šã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å¦‚ä½•è·¨ç»´åº¦åˆ†é…è®¡ç®—èµ„æºä»¥æœ€å¤§åŒ–æ¨ç†æ€§èƒ½ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„æ¨ç†æ¨¡å‹æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12992v3",
      "published_date": "2025-05-19 11:30:41 UTC",
      "updated_date": "2025-06-18 15:41:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:08.611150+00:00"
    },
    {
      "arxiv_id": "2505.12983v1",
      "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šå¯¹å¤šæ‘˜è¦å®è¯ç ”ç©¶",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Zengkui Sun",
        "Yunlong Liang",
        "Yuxuan Cao",
        "Jiarong Xu",
        "Haoxiang Shi",
        "Jie Zhou"
      ],
      "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šå¯¹å¤šæ‘˜è¦ (M2MS) ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ï¼Œå¹¶æ•´åˆæ„å»ºäº†ä¸€ä¸ªåŒ…å« 5 ä¸ªé¢†åŸŸã€6 ç§è¯­è¨€åŠ 47.8K ä¸ªæ ·æœ¬çš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹ 18 ç§ LLMs åœ¨ zero-shot å’Œ instruction-tuning æ¨¡å¼ä¸‹çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤º zero-shot LLMs çš„æ•ˆæœå·²èƒ½ä¸ç»è¿‡å¾®è°ƒçš„ä¼ ç»Ÿæ¨¡å‹ç›¸åª²ç¾ã€‚æŒ‡ä»¤å¾®è°ƒåçš„å¼€æº LLMs åœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºåŒ…æ‹¬ GPT-4 åœ¨å†…çš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œä¸”è¿™ç§ç‰¹å®šä»»åŠ¡çš„æå‡å¹¶æœªç‰ºç‰²æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚ç„¶è€Œï¼Œäººå·¥è¯„ä¼°æ­ç¤ºäº† LLMs åœ¨ç”Ÿæˆæ‘˜è¦æ—¶ä»å­˜åœ¨ä¸¥é‡çš„äº‹å®æ€§ (factuality) é—®é¢˜ï¼Œä¸”æŒ‡ä»¤å¾®è°ƒå¯èƒ½ä¼šè¿›ä¸€æ­¥åŠ å‰§è¿™ä¸€ç¼ºé™·ã€‚ç ”ç©¶æœ€åå¼ºè°ƒï¼Œå¦‚ä½•æœ‰æ•ˆæ§åˆ¶äº‹å®é”™è¯¯å°†æ˜¯æœªæ¥æ„å»ºå®ç”¨ LLM æ‘˜è¦å™¨çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2505.12983v1",
      "published_date": "2025-05-19 11:18:54 UTC",
      "updated_date": "2025-05-19 11:18:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:12.925304+00:00"
    },
    {
      "arxiv_id": "2505.12981v2",
      "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents",
      "title_zh": "ä»åŠ©æ‰‹åˆ°å¯¹æ‰‹ï¼šç§»åŠ¨å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å®‰å…¨é£é™©æ¢ç©¶",
      "authors": [
        "Liangxuan Wu",
        "Chao Wang",
        "Tianming Liu",
        "Yanjie Zhao",
        "Haoyu Wang"
      ],
      "abstract": "The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ç§»åŠ¨ç«¯LLMæ™ºèƒ½ä½“(Mobile LLM Agents)çš„å®‰å…¨é£é™©è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„å®‰å…¨æ€§åˆ†æï¼Œæ¶µç›–äº†ç³»ç»Ÿçº§AIæ™ºèƒ½ä½“ã€ç¬¬ä¸‰æ–¹é€šç”¨æ™ºèƒ½ä½“åŠæ–°å…´æ™ºèƒ½ä½“æ¡†æ¶ã€‚ä½œè€…é€šè¿‡åˆ†æç§»åŠ¨æ™ºèƒ½ä½“çš„é€šç”¨å·¥ä½œæµï¼Œä»åŸºäºè¯­è¨€çš„æ¨ç†(language-based reasoning)ã€åŸºäºGUIçš„äº¤äº’(GUI-based interaction)å’Œç³»ç»Ÿçº§æ‰§è¡Œ(system-level execution)ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦è¯†åˆ«å‡º11ä¸ªè·¨è¶Šå…¶æ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„æ”»å‡»é¢(attack surfaces)ã€‚ç ”ç©¶å¼•å…¥äº†åŠè‡ªåŠ¨åŒ–å®‰å…¨åˆ†ææ¡†æ¶AgentScanï¼Œå¹¶å¯¹9ç§å¹¿æ³›éƒ¨ç½²çš„æ™ºèƒ½ä½“è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•å¯¹è±¡å‡å¯¹é’ˆå¯¹æ€§æ”»å‡»è¡¨ç°å‡ºè„†å¼±æ€§ã€‚åœ¨æœ€ä¸¥é‡çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“åœ¨8ä¸ªä¸åŒçš„æ”»å‡»å‘é‡(attack vectors)ä¸­å­˜åœ¨æ¼æ´ï¼Œå¯èƒ½å¯¼è‡´è¡Œä¸ºåå·®ã€éšç§æ³„éœ²ç”šè‡³å®Œå…¨çš„æ‰§è¡ŒåŠ«æŒ(execution hijacking)ã€‚åŸºäºä¸Šè¿°å‘ç°ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—é˜²å¾¡æ€§è®¾è®¡åŸåˆ™ä¸å®è·µå»ºè®®ã€‚æ­¤é¡¹å·¥ä½œå‡¸æ˜¾äº†åœ¨å¿«é€Ÿå‘å±•çš„LLMé©±åŠ¨ç§»åŠ¨è‡ªåŠ¨åŒ–é¢†åŸŸä¸­ï¼Œå»ºç«‹æ ‡å‡†åŒ–å®‰å…¨å®è·µçš„ç´§è¿«éœ€æ±‚ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12981v2",
      "published_date": "2025-05-19 11:17:46 UTC",
      "updated_date": "2025-05-20 07:02:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:11.497158+00:00"
    },
    {
      "arxiv_id": "2505.12966v1",
      "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection",
      "title_zh": "é¢å‘å¤šåª’ä½“æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å¤šå°ºåº¦è‡ªé€‚åº”å†²çªå¹³è¡¡æ¨¡å‹",
      "authors": [
        "Zihan Xiong",
        "Xiaohua Wu",
        "Lei Chen",
        "Fangqi Lou"
      ],
      "abstract": "Advances in computer vision and deep learning have blurred the line between deepfakes and authentic media, undermining multimedia credibility through audio-visual forgery. Current multimodal detection methods remain limited by unbalanced learning between modalities. To tackle this issue, we propose an Audio-Visual Joint Learning Method (MACB-DF) to better mitigate modality conflicts and neglect by leveraging contrastive learning to assist in multi-level and cross-modal fusion, thereby fully balancing and exploiting information from each modality. Additionally, we designed an orthogonalization-multimodal pareto module that preserves unimodal information while addressing gradient conflicts in audio-video encoders caused by differing optimization targets of the loss functions. Extensive experiments and ablation studies conducted on mainstream deepfake datasets demonstrate consistent performance gains of our model across key evaluation metrics, achieving an average accuracy of 95.5% across multiple datasets. Notably, our method exhibits superior cross-dataset generalization capabilities, with absolute improvements of 8.0% and 7.7% in ACC scores over the previous best-performing approach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šåª’ä½“ Deepfake æ£€æµ‹ä¸­å¤šæ¨¡æ€å­¦ä¹ ä¸å¹³è¡¡å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†åä¸º MACB-DF çš„å¤šå°ºåº¦è‡ªé€‚åº”å†²çªå¹³è¡¡æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§ Audio-Visual Joint Learning Methodï¼Œé€šè¿‡å¼•å…¥ Contrastive Learning æŠ€æœ¯è¾…åŠ©è¿›è¡Œå¤šå±‚çº§å’Œè·¨æ¨¡æ€èåˆï¼Œæ—¨åœ¨å……åˆ†å¹³è¡¡å¹¶æŒ–æ˜éŸ³é¢‘ä¸è§†é¢‘æ¨¡æ€çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³ä¸åŒæ¨¡æ€ä¼˜åŒ–ç›®æ ‡å·®å¼‚å¸¦æ¥çš„æ¢¯åº¦å†²çªï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ä¸ª Orthogonalization-multimodal Pareto æ¨¡å—ï¼Œåœ¨ä¿ç•™å•æ¨¡æ€ç‰¹å¾çš„åŒæ—¶ä¼˜åŒ–äº†ç¼–ç å™¨çš„ååŒå·¥ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMACB-DF åœ¨ä¸»æµæ•°æ®é›†ä¸Šè¾¾åˆ°äº† 95.5% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå±•ç°å‡ºå“è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨è·¨æ•°æ®é›†æ³›åŒ–å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•è¾ƒæ­¤å‰æœ€ä¼˜æŠ€æœ¯åœ¨ ACC æŒ‡æ ‡ä¸Šå®ç°äº†æœ€é«˜ 8.0% çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶åœ¨åº”å¯¹å¤æ‚ä¼ªé€ åª’ä½“æ–¹é¢çš„å¼ºå¤§é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages,ICMR accepted",
      "pdf_url": "https://arxiv.org/pdf/2505.12966v1",
      "published_date": "2025-05-19 11:01:49 UTC",
      "updated_date": "2025-05-19 11:01:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:21.213049+00:00"
    },
    {
      "arxiv_id": "2505.13567v2",
      "title": "Learning Dynamics of RNNs in Closed-Loop Environments",
      "title_zh": "é—­ç¯ç¯å¢ƒä¸‹å¾ªç¯ç¥ç»ç½‘ç»œçš„å­¦ä¹ åŠ¨åŠ›å­¦",
      "authors": [
        "Yoav Ger",
        "Omri Barak"
      ],
      "abstract": "Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer powerful models of brain computation. However, typical training paradigms rely on open-loop, supervised settings, whereas real-world learning unfolds in closed-loop environments. Here, we develop a mathematical theory describing the learning dynamics of linear RNNs trained in closed-loop contexts. We first demonstrate that two otherwise identical RNNs, trained in either closed- or open-loop modes, follow markedly different learning trajectories. To probe this divergence, we analytically characterize the closed-loop case, revealing distinct stages aligned with the evolution of the training loss. Specifically, we show that the learning dynamics of closed-loop RNNs, in contrast to open-loop ones, are governed by an interplay between two competing objectives: short-term policy improvement and long-term stability of the agent-environment interaction. Finally, we apply our framework to a realistic motor control task, highlighting its broader applicability. Taken together, our results underscore the importance of modeling closed-loop dynamics in a biologically plausible setting.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€å¥—æè¿°çº¿æ€§é€’å½’ç¥ç»ç½‘ç»œ (RNNs) åœ¨é—­ç¯ (closed-loop) ç¯å¢ƒä¸‹è®­ç»ƒå­¦ä¹ åŠ¨åŠ›å­¦çš„æ•°å­¦ç†è®ºï¼Œæ—¨åœ¨è§£å†³ç°å®å­¦ä¹ é€šå¸¸åœ¨é—­ç¯è€Œéä¼ ç»Ÿå¼€ç¯ (open-loop) ç¯å¢ƒä¸‹å±•å¼€çš„é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼Œç ”ç©¶è¯æ˜äº†é—­ç¯ä¸å¼€ç¯æ¨¡å¼ä¸‹è®­ç»ƒçš„æ¨¡å‹ä¼šéµå¾ªæˆªç„¶ä¸åŒçš„å­¦ä¹ è½¨è¿¹ï¼Œå¹¶è§£æåœ°åˆ»ç”»äº†é—­ç¯å­¦ä¹ è¿‡ç¨‹ä¸­ä¸æŸå¤±å‡½æ•°æ¼”åŒ–ç›¸å¯¹åº”çš„å¤šä¸ªé˜¶æ®µã€‚ç ”ç©¶æ­ç¤ºï¼Œé—­ç¯ RNNs çš„å­¦ä¹ åŠ¨åŠ›å­¦ä¸»è¦ç”±çŸ­æœŸç­–ç•¥æ”¹è¿› (short-term policy improvement) ä¸æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’çš„é•¿æœŸç¨³å®šæ€§ (long-term stability) è¿™ä¸¤ä¸ªç«äº‰ç›®æ ‡ä¹‹é—´çš„ç›¸äº’ä½œç”¨æ‰€é©±åŠ¨ã€‚è¯¥æ¡†æ¶åœ¨ç°å®è¿åŠ¨æ§åˆ¶ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨è¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥é¡¹æˆæœå¼ºè°ƒäº†åœ¨ç”Ÿç‰©å­¦åˆç† (biologically plausible) çš„è®¾ç½®ä¸­å»ºæ¨¡é—­ç¯åŠ¨åŠ›å­¦å¯¹äºæ¨¡æ‹Ÿå¤§è„‘è®¡ç®—çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13567v2",
      "published_date": "2025-05-19 11:00:23 UTC",
      "updated_date": "2025-11-06 07:45:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:23.937501+00:00"
    },
    {
      "arxiv_id": "2505.12963v1",
      "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies",
      "title_zh": "åŸºäºç¥ç»ç½‘ç»œçš„ MRI å›¾åƒé¢ä¸‹é¢Œå…³èŠ‚ç»“æ„åˆ†å‰²ä¸ç—…ç†è¯Šæ–­",
      "authors": [
        "Maksim I. Ivanov",
        "Olga E. Mendybaeva",
        "Yuri E. Karyakin",
        "Igor N. Glukhikh",
        "Aleksey V. Lebedev"
      ],
      "abstract": "This article explores the use of artificial intelligence for the diagnosis of pathologies of the temporomandibular joint (TMJ), in particular, for the segmentation of the articular disc on MRI images. The relevance of the work is due to the high prevalence of TMJ pathologies, as well as the need to improve the accuracy and speed of diagnosis in medical institutions. During the study, the existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result, are not suitable for studying the articular disc due to the orientation towards bone structures. To solve the problem, an original dataset was collected from 94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the amount of data, augmentation methods were used. After that, the models of U-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and compared. The evaluation was carried out according to the Dice Score, Precision, Sensitivity, Specificity, and Mean Average Precision metrics. The results confirm the potential of using the Roboflow model for segmentation of the temporomandibular joint. In the future, it is planned to develop an algorithm for measuring the distance between the jaws and determining the position of the articular disc, which will improve the diagnosis of TMJ pathologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯è¯Šæ–­é¢ä¸‹é¢Œå…³èŠ‚ (Temporomandibular Joint, TMJ) ç—…å˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ MRI å›¾åƒä¸­çš„å…³èŠ‚ç›˜ (Articular Disc) è¿›è¡Œè‡ªåŠ¨åˆ†å‰²ã€‚ç”±äºç°æœ‰å·¥å…·å¦‚ Diagnocat å’Œ MandSeg ä¸»è¦ä¾§é‡äºéª¨éª¼ç»“æ„è€Œéš¾ä»¥æœ‰æ•ˆè¯†åˆ«å…³èŠ‚ç›˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æå‡ç›¸å…³ç—…å˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å« 94 å¼ å›¾åƒçš„åŸå§‹æ•°æ®é›†ï¼Œåœ¨åº”ç”¨æ•°æ®å¢å¼º (Augmentation) æŠ€æœ¯åï¼Œå¯¹ U-Netã€YOLOv8nã€YOLOv11n å’Œ Roboflow ç­‰å¤šç§ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œäº†è®­ç»ƒä¸å¯¹æ¯”ã€‚é€šè¿‡å¯¹ Dice Scoreã€Precisionã€Sensitivity å’Œ Mean Average Precision ç­‰æŒ‡æ ‡çš„ç»¼åˆè¯„ä¼°ï¼Œå®éªŒç»“æœè¯å®äº† Roboflow æ¨¡å‹åœ¨é¢ä¸‹é¢Œå…³èŠ‚åˆ†å‰²ä»»åŠ¡ä¸­çš„æ˜¾è‘—æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥å¼€å‘è‡ªåŠ¨æµ‹é‡é¢Œéª¨é—´è·å’Œç¡®å®šå…³èŠ‚ç›˜ä½ç½®çš„ç®—æ³•å¥ å®šäº†åŸºç¡€ï¼Œæœ‰æœ›è¿›ä¸€æ­¥ä¼˜åŒ– TMJ ç—…å˜çš„ä¸´åºŠè¯Šæ–­æµç¨‹ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12963v1",
      "published_date": "2025-05-19 10:58:02 UTC",
      "updated_date": "2025-05-19 10:58:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:51.716605+00:00"
    },
    {
      "arxiv_id": "2505.12960v1",
      "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory",
      "title_zh": "ç¡¬ä»¶è‡ªé€‚åº”ä¸”å…·å¤‡è¶…çº¿æ€§å®¹é‡çš„å¿†é˜»å™¨è”æƒ³è®°å¿†",
      "authors": [
        "Chengping He",
        "Mingrui Jiang",
        "Keyi Shan",
        "Szu-Hao Yang",
        "Zefan Li",
        "Shengbo Wang",
        "Giacomo Pedretti",
        "Jim Ignowski",
        "Can Li"
      ],
      "abstract": "Brain-inspired computing aims to mimic cognitive functions like associative memory, the ability to recall complete patterns from partial cues. Memristor technology offers promising hardware for such neuromorphic systems due to its potential for efficient in-memory analog computing. Hopfield Neural Networks (HNNs) are a classic model for associative memory, but implementations on conventional hardware suffer from efficiency bottlenecks, while prior memristor-based HNNs faced challenges with vulnerability to hardware defects due to offline training, limited storage capacity, and difficulty processing analog patterns. Here we introduce and experimentally demonstrate on integrated memristor hardware a new hardware-adaptive learning algorithm for associative memories that significantly improves defect tolerance and capacity, and naturally extends to scalable multilayer architectures capable of handling both binary and continuous patterns. Our approach achieves 3x effective capacity under 50% device faults compared to state-of-the-art methods. Furthermore, its extension to multilayer architectures enables superlinear capacity scaling (\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous patterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling for previous HNNs. It also provides flexibility to adjust capacity by tuning hidden neurons for the same-sized patterns. By leveraging the massive parallelism of the hardware enabled by synchronous updates, it reduces energy by 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous schemes, with greater improvements at scale. This promises the development of more reliable memristor-based associative memory systems and enables new applications research due to the significantly improved capacity, efficiency, and flexibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç¡¬ä»¶è‡ªé€‚åº”ä¸”å…·æœ‰è¶…çº¿æ€§å®¹é‡çš„å¿†é˜»å™¨(Memristor)å…³è”è®°å¿†ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸHopfield Neural Networks (HNN)åœ¨å¿†é˜»å™¨ç¡¬ä»¶ä¸Šæ˜“å—ç¼ºé™·å½±å“ã€å­˜å‚¨å®¹é‡æœ‰é™åŠéš¾ä»¥å¤„ç†è¿ç»­æ¨¡å¼ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜åœ¨é›†æˆå¿†é˜»å™¨ç¡¬ä»¶ä¸Šå¼€å‘å¹¶éªŒè¯äº†ä¸€ç§æ–°å‹Hardware-adaptive learning algorithmï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„ç¼ºé™·å®¹å¿åº¦ï¼Œåœ¨å­˜åœ¨50%å™¨ä»¶æ•…éšœçš„æƒ…å†µä¸‹ï¼Œå…¶æœ‰æ•ˆå®¹é‡ä»è¾¾åˆ°ç°æœ‰æŠ€æœ¯çš„3å€ã€‚é€šè¿‡å°†è¯¥ç®—æ³•æ‰©å±•è‡³å¤šå±‚æ¶æ„ï¼Œç³»ç»Ÿå®ç°äº†è¶…çº¿æ€§çš„å®¹é‡ç¼©æ”¾ï¼ˆäºŒè¿›åˆ¶æ¨¡å¼ä¸ºN^1.49ï¼Œè¿ç»­æ¨¡å¼ä¸ºN^1.74ï¼‰ï¼Œå…‹æœäº†ä»¥å¾€æ¨¡å‹çº¿æ€§å®¹é‡å¢é•¿çš„å±€é™ã€‚åˆ©ç”¨ç¡¬ä»¶åŒæ­¥æ›´æ–°çš„å¹¶è¡Œè®¡ç®—ä¼˜åŠ¿ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤„ç†64ç»´æ¨¡å¼æ—¶è¾ƒå¼‚æ­¥æ–¹æ¡ˆé™ä½äº†8.8å€èƒ½è€—å¹¶ç¼©å‡äº†99.7%çš„å»¶è¿Ÿã€‚è¿™é¡¹æˆæœä¸ºå¼€å‘æ›´å¯é ã€é«˜æ•ˆä¸”çµæ´»çš„Memristorå…³è”è®°å¿†ç³»ç»Ÿæä¾›äº†é‡è¦æ”¯æŒï¼Œå¹¶ä¸ºç¥ç»å½¢æ€ç¡¬ä»¶çš„å¤§è§„æ¨¡åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12960v1",
      "published_date": "2025-05-19 10:55:09 UTC",
      "updated_date": "2025-05-19 10:55:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:38.555326+00:00"
    },
    {
      "arxiv_id": "2505.13565v2",
      "title": "Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks",
      "title_zh": "ä½¿å¯ä¿¡äººå·¥æ™ºèƒ½ä¸æ°‘ä¸»ä¿æŒä¸€è‡´ï¼šæœºé‡ä¸é£é™©çš„åŒé‡åˆ†ç±»æ³•",
      "authors": [
        "Oier Mentxaka",
        "Natalia DÃ­az-RodrÃ­guez",
        "Mark Coeckelbergh",
        "Marcos LÃ³pez de Prado",
        "Emilia GÃ³mez",
        "David FernÃ¡ndez Llorca",
        "Enrique Herrera-Viedma",
        "Francisco Herrera"
      ],
      "abstract": "Artificial Intelligence (AI) poses both significant risks and valuable opportunities for democratic governance. This paper introduces a dual taxonomy to evaluate AI's complex relationship with democracy: the AI Risks to Democracy (AIRD) taxonomy, which identifies how AI can undermine core democratic principles such as autonomy, fairness, and trust; and the AI's Positive Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to enhance transparency, participation, efficiency, and evidence-based policymaking.\n  Grounded in the European Union's approach to ethical AI governance, and particularly the seven Trustworthy AI requirements proposed by the European Commission's High-Level Expert Group on AI, each identified risk is aligned with mitigation strategies based on EU regulatory and normative frameworks. Our analysis underscores the transversal importance of transparency and societal well-being across all risk categories and offers a structured lens for aligning AI systems with democratic values.\n  By integrating democratic theory with practical governance tools, this paper offers a normative and actionable framework to guide research, regulation, and institutional design to support trustworthy, democratic AI. It provides scholars with a conceptual foundation to evaluate the democratic implications of AI, equips policymakers with structured criteria for ethical oversight, and helps technologists align system design with democratic principles. In doing so, it bridges the gap between ethical aspirations and operational realities, laying the groundwork for more inclusive, accountable, and resilient democratic systems in the algorithmic age.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ä¸æ°‘ä¸»æ²»ç†ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶æå‡ºäº†åŒé‡åˆ†ç±»æ³•(dual taxonomy)æ¥è¯„ä¼°å…¶å¸¦æ¥çš„æœºé‡ä¸é£é™©ã€‚å…¶ä¸­ï¼ŒAI Risks to Democracy (AIRD)åˆ†ç±»æ³•è¯†åˆ«äº†AIå¦‚ä½•æŸå®³è‡ªä¸»æ€§ã€å…¬å¹³æ€§å’Œä¿¡ä»»ç­‰æ ¸å¿ƒæ°‘ä¸»åŸåˆ™ï¼Œè€ŒAI's Positive Contributions to Democracy (AIPD)åˆ†ç±»æ³•åˆ™å¼ºè°ƒäº†AIåœ¨å¢å¼ºé€æ˜åº¦ã€å‚ä¸åº¦ã€æ•ˆç‡å’Œå¾ªè¯æ”¿ç­–åˆ¶å®šæ–¹é¢çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ç«‹è¶³äºæ¬§ç›Ÿçš„ä¼¦ç†AIæ²»ç†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ¬§ç›Ÿå§”å‘˜ä¼šAIé«˜çº§ä¸“å®¶ç»„æå‡ºçš„ä¸ƒé¡¹å¯ä¿¡AI(Trustworthy AI)è¦æ±‚ï¼Œä¸ºè¯†åˆ«å‡ºçš„æ¯é¡¹é£é™©æä¾›äº†åŸºäºæ¬§ç›Ÿç›‘ç®¡å’Œè§„èŒƒæ¡†æ¶çš„ç¼“è§£ç­–ç•¥ã€‚åˆ†æå¼ºè°ƒäº†é€æ˜åº¦(transparency)å’Œç¤¾ä¼šç¦ç¥‰(societal well-being)åœ¨æ‰€æœ‰é£é™©ç±»åˆ«ä¸­çš„æ¨ªå‘é‡è¦æ€§ï¼Œä¸ºAIç³»ç»Ÿä¸æ°‘ä¸»ä»·å€¼è§‚çš„è¡”æ¥æä¾›äº†ç»“æ„åŒ–è§†è§’ã€‚é€šè¿‡å°†æ°‘ä¸»ç†è®ºä¸å®è·µæ²»ç†å·¥å…·ç›¸ç»“åˆï¼Œè¯¥è®ºæ–‡æä¾›äº†ä¸€ä¸ªè§„èŒƒä¸”å¯æ“ä½œçš„æ¡†æ¶ï¼Œæ—¨åœ¨æŒ‡å¯¼ç ”ç©¶ã€ç›‘ç®¡å’Œåˆ¶åº¦è®¾è®¡ï¼Œä»¥æ”¯æŒæ„å»ºå¯ä¿¡ä¸”æ°‘ä¸»çš„äººå·¥æ™ºèƒ½ã€‚è¿™ä¸ä»…ä¸ºå­¦è€…è¯„ä¼°AIçš„æ°‘ä¸»å½±å“å¥ å®šäº†æ¦‚å¿µåŸºç¡€ï¼Œè¿˜ä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›äº†ä¼¦ç†ç›‘ç£æ ‡å‡†ï¼Œå¹¶å¸®åŠ©æŠ€æœ¯äººå‘˜åœ¨è®¾è®¡ä¸­å¯¹é½æ°‘ä¸»åŸåˆ™ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "26 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13565v2",
      "published_date": "2025-05-19 10:51:08 UTC",
      "updated_date": "2026-01-13 11:40:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:51:32.313369+00:00"
    },
    {
      "arxiv_id": "2505.12951v1",
      "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management",
      "title_zh": "DGROï¼šé€šè¿‡æ¢ç´¢-åˆ©ç”¨æ§åˆ¶ä¸å¥–åŠ±æ–¹å·®ç®¡ç†æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xuerui Su",
        "Liya Guo",
        "Yue Wang",
        "Yi Zhu",
        "Zhiming Ma",
        "Zun Wang",
        "Yuting Liu"
      ],
      "abstract": "Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9\\%, and demonstrates strong generalization across mathematical benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ä¸­ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ (RL)ç®—æ³•åœ¨æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡åŠæ‰‹åŠ¨å¥–åŠ±å‡½æ•°è®¾è®¡çš„å±€é™æ€§ï¼Œæå‡ºäº†Decoupled Group Reward Optimization (DGRO)é€šç”¨ç®—æ³•ã€‚DGROçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†ä¼ ç»Ÿçš„æ­£åˆ™åŒ–ç³»æ•°è§£è€¦ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„è¶…å‚æ•°ï¼Œåˆ†åˆ«ç”¨äºç¼©æ”¾ç­–ç•¥æ¢¯åº¦é¡¹å’Œè°ƒèŠ‚ä¸é‡‡æ ·ç­–ç•¥ä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œå®ç°å¯¹æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡çš„ç²¾ç¡®æ§åˆ¶ã€‚è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ™®é€‚æ€§ï¼Œå¯æ— ç¼æ‰©å±•è‡³Online Policy Mirror Descent (OPMD)å’Œç›´æ¥å¥–åŠ±ä¼˜åŒ–ç­‰ç®—æ³•ä¸­ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†å¥–åŠ±æ–¹å·®(reward variance)å¯¹æ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½çš„å½±å“ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æä¸æ¶ˆèå®éªŒéªŒè¯äº†å…¶ä¼˜åŒ–åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGROåœ¨Logicæ•°æ®é›†ä¸Šå–å¾—äº†96.9%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¾¾åˆ°SOTAæ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12951v1",
      "published_date": "2025-05-19 10:44:49 UTC",
      "updated_date": "2025-05-19 10:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:52:56.637877+00:00"
    },
    {
      "arxiv_id": "2505.13563v3",
      "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression",
      "title_zh": "çªç ´å‹ç¼©æé™ï¼šè¶…é«˜æ•ˆå¢é‡å‹ç¼©çš„æ— æ•°æ®æµæ°´çº¿",
      "authors": [
        "Xiaohui Wang",
        "Peng Ye",
        "Chenyu Huang",
        "Shenghe Zheng",
        "Bo Zhang",
        "Lei Bai",
        "Wanli Ouyang",
        "Tao Chen"
      ],
      "abstract": "With the rise of the fine-tuned-pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 50x compression, (b) general NLP models (RoBERTa-base, T5-base) with up to 224x compression, (c) vision models (ViT-B/32, ViT-L/14) with up to 132x compression, and (d) multi-modal models (BEiT-3) with 18x compression, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression. Code is available at https://github.com/xiaohuiwang000/UltraDelta.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UltraDeltaï¼Œè¿™æ˜¯é¦–ä¸ªæ— éœ€æ•°æ®ï¼ˆdata-freeï¼‰çš„å¢é‡å‹ç¼©ï¼ˆdelta compressionï¼‰æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¾®è°ƒæ¨¡å‹å¸¦æ¥çš„å­˜å‚¨æŒ‘æˆ˜ã€‚UltraDelta é€šè¿‡ä¸‰ä¸ªç»´åº¦ä¼˜åŒ–å‹ç¼©æ•ˆç‡ï¼šåˆ©ç”¨ Variance-Based Mixed Sparsity Allocation åœ¨å±‚é—´åˆ†é…ç¨€ç–åº¦ä»¥ä¿ç•™é«˜æ–¹å·®ä¿¡æ¯ï¼Œé‡‡ç”¨ Distribution-Aware Compression é€šè¿‡åˆ†ç»„å‰ªæä¿æŠ¤å±‚å†…å‚æ•°åˆ†å¸ƒï¼Œä»¥åŠé€šè¿‡ Trace-Norm-Guided Rescaling ç¨³å®šé«˜å‹ç¼©ç‡ä¸‹çš„æ¨¡å‹è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ LLaMA-2ã€RoBERTaã€ViT åŠ BEiT-3 ç­‰å¤šç§æ¨¡å‹æ¶æ„ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ T5-base æ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾ 224 å€çš„å‹ç¼©ã€‚UltraDelta åœ¨ä¿æŒå¼ºå¤§æ€§èƒ½çš„åŒæ—¶çªç ´äº†ç°æœ‰å‹ç¼©ä¸Šé™ï¼Œä¸ºå¤šä»»åŠ¡åœºæ™¯ä¸‹çš„é«˜æ•ˆæ¨¡å‹å­˜å‚¨æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13563v3",
      "published_date": "2025-05-19 10:37:22 UTC",
      "updated_date": "2025-10-13 17:33:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:52:21.961579+00:00"
    },
    {
      "arxiv_id": "2505.12944v2",
      "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs",
      "title_zh": "CALM-PDEï¼šç”¨äºå«æ—¶åå¾®åˆ†æ–¹ç¨‹æ½œç©ºé—´å»ºæ¨¡çš„è¿ç»­è‡ªé€‚åº”å·ç§¯",
      "authors": [
        "Jan Hagnberger",
        "Daniel Musekamp",
        "Mathias Niepert"
      ],
      "abstract": "Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demonstrate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or outperforms existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CALM-PDEï¼Œä¸€ç§æ—¨åœ¨å‹ç¼©æ½œç©ºé—´ (latent space) ä¸­é«˜æ•ˆæ±‚è§£ä»»æ„ç¦»æ•£åŒ–éšæ—¶é—´æ¼”å˜çš„åå¾®åˆ†æ–¹ç¨‹ (time-dependent PDEs) çš„æ–°å‹æ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ç‰©ç†ç©ºé—´è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä»¥åŠåŸºäº Transformer çš„æ¶æ„åœ¨å¤„ç†ä¸è§„åˆ™é‡‡æ ·åŸŸæ—¶å†…å­˜æ¶ˆè€—è¿‡å¤§çš„å±€é™ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†åŸºäºè¿ç»­å·ç§¯ (continuous convolution) çš„ç¼–ç å™¨-è§£ç å™¨ç³»ç»Ÿã€‚é€šè¿‡ä½¿ç”¨ epsilon-neighborhood-constrained æ ¸å¹¶å­¦ä¹ å°†å·ç§¯ç®—å­åº”ç”¨äºè‡ªé€‚åº”çš„ä¼˜åŒ–æŸ¥è¯¢ç‚¹ï¼ŒCALM-PDE èƒ½å¤Ÿçµæ´»å¤„ç†å„ç§å¤æ‚çš„ç©ºé—´ç¦»æ•£åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCALM-PDE åœ¨æ€§èƒ½ä¸Šä¸ç°æœ‰åŸºå‡†ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨å†…å­˜æ•ˆç‡å’Œæ¨ç†æ—¶é—´ (inference time) æ–¹é¢æ¯” Transformer æ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›ã€‚è¯¥æˆæœä¸ºæ°”å€™æ¨¡æ‹Ÿå’Œæµä½“åŠ¨åŠ›å­¦ç­‰ç§‘å­¦å·¥ç¨‹é¢†åŸŸçš„å¤æ‚åå¾®åˆ†æ–¹ç¨‹æ±‚è§£æä¾›äº†ä¸€ç§é«˜æ•ˆçš„ç¥ç»ä»£ç†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication at the 39th Conference on Neural Information Processing Systems (NeurIPS) 2025, San Diego, California, USA",
      "pdf_url": "https://arxiv.org/pdf/2505.12944v2",
      "published_date": "2025-05-19 10:31:30 UTC",
      "updated_date": "2025-10-23 15:43:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:31.628427+00:00"
    },
    {
      "arxiv_id": "2505.12942v3",
      "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
      "title_zh": "A3ï¼šé¢å‘æ³¨æ„åŠ›æœºåˆ¶çš„è§£æå¼ä½ç§©è¿‘ä¼¼æ¡†æ¶",
      "authors": [
        "Jeffrey T. H. Wong",
        "Cheng Zhang",
        "Xinye Cao",
        "Pedro Gimenes",
        "George A. Constantinides",
        "Wayne Luk",
        "Yiren Zhao"
      ],
      "abstract": "Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\\tt A^\\tt 3$, a post-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a Transformer layer into three functional components, namely $\\tt QK$, $\\tt OV$, and $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† A3ï¼Œä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåä½ç§©è¿‘ä¼¼(Low-Rank Approximation)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å‹ç¼©æ–¹æ³•åœ¨å¤„ç† Transformer æ¶æ„æ—¶å­˜åœ¨çš„å±€éƒ¨ä¼˜åŒ–å±€é™åŠå¼•å…¥é¢å¤– GEMM è¿è¡Œæ—¶å¼€é”€ç­‰é—®é¢˜ã€‚A3 å°† Transformer å±‚åˆ’åˆ†ä¸º QKã€OV å’Œ MLP ä¸‰ä¸ªåŠŸèƒ½ç»„ä»¶ï¼Œé€šè¿‡è§£æè§£é™ä½å„ç»„ä»¶å†…éƒ¨çš„éšè—ç»´åº¦ï¼Œä»è€Œåœ¨ä¸å¢åŠ é¢å¤–è¿è¡Œæ—¶è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œç›´æ¥å‡å°‘æ¨¡å‹å°ºå¯¸ã€KV cache å®¹é‡å’Œ FLOPsã€‚è¯¥æ–¹æ³•å®ç°äº†ä»å•ä¸€çº¿æ€§å±‚è¯¯å·®æœ€å°åŒ–åˆ°ä¼˜åŒ–åŠŸèƒ½æŸå¤±åŠç«¯åˆ°ç«¯æ€§èƒ½çš„è½¬å˜ï¼Œæ˜¾è‘—æå‡äº†å‹ç¼©æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒA3 çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨ç›¸åŒèµ„æºé¢„ç®—ä¸‹ï¼Œä½ç§©è¿‘ä¼¼åçš„ LLaMA 3.1-70B åœ¨ WikiText-2 ä¸Šçš„å›°æƒ‘åº¦(perplexity)è¡¨ç°è¿œè¶…æ­¤å‰è®°å½•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº† A3 åœ¨ KV cache å‹ç¼©ã€é‡åŒ–(quantization)åŠæ··åˆç§©åˆ†é…ç­‰ä»»åŠ¡ä¸­çš„å¹¿æ³›é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12942v3",
      "published_date": "2025-05-19 10:29:32 UTC",
      "updated_date": "2025-06-25 23:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:38.361755+00:00"
    },
    {
      "arxiv_id": "2505.12938v2",
      "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance",
      "title_zh": "åˆ©ç”¨ LLM çš„ä¸ä¸€è‡´æ€§æå‡ Pass@k æ€§èƒ½",
      "authors": [
        "Uri Dalal",
        "Meirav Segal",
        "Zvika Ben-Haim",
        "Dan Lahav",
        "Omer Nevo"
      ],
      "abstract": "Large language models (LLMs) achieve impressive abilities in numerous domains, but exhibit inconsistent performance in response to minor input changes. Rather than view this as a drawback, in this paper we introduce a novel method for leveraging models' inconsistency to boost Pass@k performance. Specifically, we present a \"Variator\" agent that generates k variants of a given task and submits one candidate solution for each one. Our variant generation approach is applicable to a wide range of domains as it is task agnostic and compatible with free-form inputs. We demonstrate the efficacy of our agent theoretically using a probabilistic model of the inconsistency effect, and show empirically that it outperforms the baseline on the APPS dataset. Furthermore, we establish that inconsistency persists even in frontier reasoning models across coding and cybersecurity domains, suggesting our method is likely to remain relevant for future model generations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è¾“å…¥å¾®å°å˜åŒ–è¡¨ç°å‡ºçš„ä¸ä¸€è‡´æ€§ï¼Œå¹¶æå‡ºåˆ©ç”¨è¿™ä¸€ç‰¹æ€§æ¥æå‡ Pass@k æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚ä½œè€…å¼•å…¥äº†ä¸€ç§åä¸º Variator çš„æ™ºèƒ½ä½“ï¼Œå®ƒèƒ½ä¸ºç‰¹å®šä»»åŠ¡ç”Ÿæˆ k ä¸ªå˜ä½“å¹¶ä¸ºæ¯ä¸ªå˜ä½“æäº¤ä¸€ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆã€‚è¿™ç§å˜ä½“ç”Ÿæˆæ–¹æ³•å…·æœ‰ä»»åŠ¡ä¸å¯çŸ¥æ€§ï¼ˆtask agnosticï¼‰ä¸”å…¼å®¹è‡ªç”±æ ¼å¼è¾“å…¥ï¼Œé€‚ç”¨äºå¹¿æ³›çš„é¢†åŸŸã€‚é€šè¿‡ä¸ä¸€è‡´æ•ˆåº”çš„æ¦‚ç‡æ¨¡å‹ï¼Œè¯¥ç ”ç©¶åœ¨ç†è®ºä¸Šè¯æ˜äº†è¯¥æ™ºèƒ½ä½“çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ APPS æ•°æ®é›†ä¸Šçš„å®éªŒä¸­éªŒè¯å…¶æ€§èƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®ä¸ä¸€è‡´æ€§åœ¨å°–ç«¯æ¨ç†æ¨¡å‹çš„ä»£ç å’Œç½‘ç»œå®‰å…¨é¢†åŸŸä¾ç„¶å­˜åœ¨ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æœªæ¥çš„æ¨¡å‹è¿­ä»£ä¸­ä»å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12938v2",
      "published_date": "2025-05-19 10:22:04 UTC",
      "updated_date": "2025-05-20 14:22:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:24.108279+00:00"
    },
    {
      "arxiv_id": "2505.12929v1",
      "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs",
      "title_zh": "é˜²æ­¢ä½æ¦‚ç‡è¯å…ƒåœ¨å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­è¿‡åº¦ä¸»å¯¼",
      "authors": [
        "Zhihe Yang",
        "Xufang Luo",
        "Zilong Wang",
        "Dongqi Han",
        "Zhiyuan He",
        "Dongsheng Li",
        "Yunjian Xu"
      ],
      "abstract": "Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K Logic Puzzle reasoning tasks. Our implementation is available at https://github.com/zhyang2226/AR-Lopti.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å¼ºåŒ–å­¦ä¹ (RL)è®­ç»ƒä¸­ä¸€ä¸ªé•¿æœŸè¢«å¿½è§†çš„é—®é¢˜ï¼Œå³ä½æ¦‚ç‡Token (low-probability tokens) å› å…¶æ¢¯åº¦å¹…åº¦è¾ƒå¤§è€Œè¿‡åº¦ä¸»å¯¼æ¨¡å‹æ›´æ–°ï¼Œä»è€ŒæŠ‘åˆ¶äº†å¯¹æ€§èƒ½è‡³å…³é‡è¦çš„é«˜æ¦‚ç‡Token (high-probability tokens) çš„å­¦ä¹ ã€‚ä¸ºäº†è§£å†³è¿™ç§å¹²æ‰°ï¼Œä½œè€…æå‡ºäº†Advantage Reweightingå’ŒLow-Probability Token Isolation (Lopti) ä¸¤ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆè¡°å‡ä½æ¦‚ç‡Tokençš„æ¢¯åº¦ï¼Œå¹¶å¼ºè°ƒç”±é«˜æ¦‚ç‡Tokené©±åŠ¨çš„å‚æ•°æ›´æ–°ã€‚è¿™äº›æ–¹æ³•ä¿ƒè¿›äº†ä¸åŒæ¦‚ç‡Tokenä¹‹é—´çš„å¹³è¡¡æ›´æ–°ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†RLè®­ç»ƒçš„æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿå¤§å¹…æå‡åŸºäºGroup Relative Policy Optimization (GRPO) è®­ç»ƒçš„LLMsæ€§èƒ½ï¼Œåœ¨K&K Logic Puzzleæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†é«˜è¾¾46.2%çš„æ”¹è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12929v1",
      "published_date": "2025-05-19 10:14:08 UTC",
      "updated_date": "2025-05-19 10:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:42.200360+00:00"
    },
    {
      "arxiv_id": "2505.12925v2",
      "title": "CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming",
      "title_zh": "CPRetï¼šé¢å‘ç®—æ³•ç«èµ›æ£€ç´¢çš„æ•°æ®é›†ã€åŸºå‡†ä¸æ¨¡å‹",
      "authors": [
        "Han Deng",
        "Yuan Meng",
        "Shixiang Tang",
        "Wanli Ouyang",
        "Xinzhu Ma"
      ],
      "abstract": "Competitive programming benchmarks are widely used in scenarios such as programming contests and large language model assessments. However, the growing presence of duplicate or highly similar problems raises concerns not only about competition fairness, but also about the validity of competitive programming as a benchmark for model evaluation. In this paper, we propose a new problem, similar question retrieval, to tackle this issue. Due to the lack of both data and models, solving this problem is challenging. To this end, we introduce CPRet, a retrieval-oriented benchmark suite for competitive programming, covering four retrieval tasks: two code-centric (i.e., Text-to-Code, Code-to-Code) and two newly proposed problem-centric tasks (i.e., Problem-to-Duplicate, Simplified-to-Full) built from a combination of automatically crawled problem-solution data and manually curated annotations. Our contribution includes both high-quality training data and temporally separated test sets for reliable evaluation. Besides, we further develop two task-specialized retrievers based on this dataset: CPRetriever-Code, trained with a novel Group-InfoNCE loss for problem-code alignment, and CPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both models achieve strong results and are open-sourced for local use. Finally, we analyze LiveCodeBench and find that high-similarity problems inflate model pass rates and reduce differentiation, underscoring the need for similarity-aware evaluation in future benchmarks.\n  Github: https://github.com/coldchair/CPRet\n  Online Demo: https://www.cpret.online/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CPRetï¼Œä¸€ä¸ªä¸“ä¸ºç®—æ³•ç«èµ›(Competitive Programming)æ£€ç´¢è®¾è®¡çš„åŸºå‡†å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³é‡å¤é¢˜ç›®å¯¹ç«èµ›å…¬å¹³æ€§å’Œå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æœ‰æ•ˆæ€§å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚CPRet æ¶µç›–äº† Text-to-Codeã€Code-to-Code ä»¥åŠæ–°æå‡ºçš„ Problem-to-Duplicate å’Œ Simplified-to-Full ç­‰å››é¡¹æ£€ç´¢ä»»åŠ¡ï¼Œå¹¶ç»“åˆäº†è‡ªåŠ¨æŠ“å–æ•°æ®ä¸äººå·¥æ ‡æ³¨ã€‚ä½œè€…è¿›ä¸€æ­¥å¼€å‘äº†ä¸¤ä¸ªä¸“é—¨çš„æ£€ç´¢å™¨ï¼šåŸºäº Group-InfoNCE loss è®­ç»ƒçš„ CPRetriever-Code ä»¥åŠé’ˆå¯¹é¢˜ç›®çº§ç›¸ä¼¼åº¦å¾®è°ƒçš„ CPRetriever-Probã€‚å®éªŒç»“æœè¡¨æ˜è¿™ä¸¤ä¸ªæ¨¡å‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åˆ†æå‘ç° LiveCodeBench ä¸­çš„é«˜ç›¸ä¼¼åº¦é¢˜ç›®ä¼šè™šå¢æ¨¡å‹é€šè¿‡ç‡(pass rates)å¹¶é™ä½è¯„ä¼°çš„åŒºåˆ†åº¦ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨æœªæ¥åŸºå‡†æµ‹è¯•ä¸­å®æ–½ç›¸ä¼¼æ€§æ„ŸçŸ¥è¯„ä¼°(similarity-aware evaluation)çš„å¿…è¦æ€§ï¼Œå¹¶å¼€æºäº†å…¨éƒ¨æ•°æ®ä¸æ¨¡å‹ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by NeurIPS 2025 Dataset and Benchmark Track",
      "pdf_url": "https://arxiv.org/pdf/2505.12925v2",
      "published_date": "2025-05-19 10:07:51 UTC",
      "updated_date": "2025-10-26 03:34:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:51.833550+00:00"
    },
    {
      "arxiv_id": "2505.13562v1",
      "title": "Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback",
      "title_zh": "é’ˆå¯¹å…·æœ‰ Bandit åé¦ˆçŸ©é˜µåšå¼ˆçš„åŸºäºç«äº‰å‹ååŒè¿›åŒ–çš„éšæœºåŒ–ä¹è§‚æœºåˆ¶",
      "authors": [
        "Shishen Lin"
      ],
      "abstract": "Learning in games is a fundamental problem in machine learning and artificial intelligence, with numerous applications~\\citep{silver2016mastering,schrittwieser2020mastering}. This work investigates two-player zero-sum matrix games with an unknown payoff matrix and bandit feedback, where each player observes their actions and the corresponding noisy payoff. Prior studies have proposed algorithms for this setting~\\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with \\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism (e.g., \\ucb) in achieving sublinear regret. However, the potential of randomised optimism in matrix games remains theoretically unexplored.\n  We propose Competitive Co-evolutionary Bandit Learning (\\coebl), a novel algorithm that integrates evolutionary algorithms (EAs) into the bandit framework to implement randomised optimism through EA variation operators. We prove that \\coebl achieves sublinear regret, matching the performance of deterministic optimism-based methods. To the best of our knowledge, this is the first theoretical regret analysis of an evolutionary bandit learning algorithm in matrix games.\n  Empirical evaluations on diverse matrix game benchmarks demonstrate that \\coebl not only achieves sublinear regret but also consistently outperforms classical bandit algorithms, including \\exptr~\\citep{auer2002nonstochastic}, the variant \\exptrni~\\citep{cai2024uncoupled}, and \\ucb~\\citep{o2021matrix}. These results highlight the potential of evolutionary bandit learning, particularly the efficacy of randomised optimism via evolutionary algorithms in game-theoretic settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰ Bandit Feedback å’ŒæœªçŸ¥æ”¶ç›ŠçŸ©é˜µçš„åŒäººé›¶å’ŒçŸ©é˜µå¯¹ç­–ï¼Œæå‡ºäº†ç«äº‰ååŒè¿›åŒ– Bandit å­¦ä¹ ç®—æ³• (Competitive Co-evolutionary Bandit Learning, COEBL)ã€‚è¯¥ç®—æ³•é¦–æ¬¡å°†è¿›åŒ–ç®—æ³• (Evolutionary Algorithms, EAs) èå…¥ Bandit æ¡†æ¶ï¼Œåˆ©ç”¨è¿›åŒ–ç®—æ³•çš„å˜å¼‚ç®—å­ (Variation Operators) å®ç°äº†éšæœºä¹è§‚ (Randomised Optimism) æœºåˆ¶ã€‚ç†è®ºåˆ†æè¯æ˜ï¼ŒCOEBL åœ¨è¯¥åšå¼ˆè®¾å®šä¸‹èƒ½å¤Ÿå®ç°äºšçº¿æ€§ç´¯ç§¯é—æ†¾ (Sublinear Regret)ï¼Œè¿™ä¹Ÿæ˜¯å­¦æœ¯ç•Œé¦–æ¬¡å¯¹çŸ©é˜µå¯¹ç­–ä¸­çš„è¿›åŒ– Bandit å­¦ä¹ ç®—æ³•è¿›è¡Œçš„ç†è®ºé—æ†¾åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOEBL çš„è¡¨ç°ä¸åŸºäºç¡®å®šæ€§ä¹è§‚çš„æ–¹æ³•ç›¸åŒ¹é…ï¼Œä¸”åœ¨å¤šç§çŸ©é˜µå¯¹ç­–åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç»å…¸çš„ EXP3ã€EXP3-NI å’Œ UCB ç­‰ç®—æ³•ã€‚è¯¥ç ”ç©¶å……åˆ†éªŒè¯äº†åœ¨åšå¼ˆè®ºè®¾ç½®ä¸­é€šè¿‡è¿›åŒ–ç®—æ³•å®ç°éšæœºä¹è§‚ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†è¿›åŒ– Bandit å­¦ä¹ çš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "stat.ML",
      "comment": "21 pages, 10 figures, accepted at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13562v1",
      "published_date": "2025-05-19 10:05:55 UTC",
      "updated_date": "2025-05-19 10:05:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:57.527844+00:00"
    },
    {
      "arxiv_id": "2505.12923v2",
      "title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations",
      "title_zh": "The Traitorsï¼šå¤šæ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä¸­çš„æ¬ºéª—ä¸ä¿¡ä»»",
      "authors": [
        "Pedro M. P. Curvo"
      ],
      "abstract": "As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† The Traitorsï¼Œè¿™æ˜¯ä¸€ä¸ªå—ç¤¾äº¤æ¨ç†æ¸¸æˆå¯å‘çš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“åœ¨éå¯¹ç§°ä¿¡æ¯ä¸‹çš„æ¬ºéª—è¡Œä¸ºã€ä¿¡ä»»å½¢æˆå’Œæˆ˜ç•¥æ²Ÿé€šã€‚è¯¥æ¡†æ¶åœ¨åšå¼ˆè®ºã€è¡Œä¸ºç»æµå­¦å’Œç¤¾ä¼šè®¤çŸ¥ç†è®ºçš„åŸºç¡€ä¸Šï¼Œæ„å»ºäº†ä¸€ä¸ª LLM æ™ºèƒ½ä½“å¯ä»¥åŸºäºæŒä¹…è®°å¿† (persistent memory) è¿›è¡Œæ¨ç†å¹¶åº”å¯¹ç¤¾äº¤åŠ¨æ€æ¼”å˜çš„è‡ªä¸»æ¨¡æ‹Ÿå¹³å°ã€‚ç ”ç©¶æå‡ºäº†ä¸€å¥—æ¶µç›–æ¬ºéª—æˆåŠŸç‡ã€ä¿¡ä»»åŠ¨æ€å’Œé›†ä½“æ¨ç†è´¨é‡çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–åˆ†ææ™ºèƒ½ä½“çš„è¡Œä¸ºè¡¨ç°ã€‚é€šè¿‡å¯¹ DeepSeek-V3ã€GPT-4o-mini å’Œ GPT-4o çš„å®éªŒå‘ç°ï¼Œåƒ GPT-4o è¿™æ ·çš„å…ˆè¿›æ¨¡å‹è™½ç„¶å±•ç°å‡ºå“è¶Šçš„æ¬ºéª—èƒ½åŠ›ï¼Œä½†åœ¨è¯†ç ´ä»–äººè°è¨€æ–¹é¢å´æ˜¾å¾—å¼‚å¸¸è„†å¼±ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ AI çš„æ¬ºéª—æŠ€èƒ½å¢é•¿é€Ÿåº¦å¯èƒ½å¿«äºå…¶æ£€æµ‹èƒ½åŠ›ï¼Œä¸ºç ”ç©¶ AI ç³»ç»Ÿçš„å¯¹é½ (alignment) æŒ‘æˆ˜å’Œç¤¾ä¼šå¯é æ€§æä¾›äº†é‡è¦å·¥å…·å’Œè§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "9 main pages, 31 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12923v2",
      "published_date": "2025-05-19 10:01:35 UTC",
      "updated_date": "2025-12-14 19:24:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:53:47.759185+00:00"
    },
    {
      "arxiv_id": "2505.12920v1",
      "title": "PyFCG: Fluid Construction Grammar in Python",
      "title_zh": "PyFCGï¼šPython å®ç°çš„æµä½“æ„é€ è¯­æ³•",
      "authors": [
        "Paul Van Eecke",
        "Katrien Beuls"
      ],
      "abstract": "We present PyFCG, an open source software library that ports Fluid Construction Grammar (FCG) to the Python programming language. PyFCG enables its users to seamlessly integrate FCG functionality into Python programs, and to use FCG in combination with other libraries within Python's rich ecosystem. Apart from a general description of the library, this paper provides three walkthrough tutorials that demonstrate example usage of PyFCG in typical use cases of FCG: (i) formalising and testing construction grammar analyses, (ii) learning usage-based construction grammars from corpora, and (iii) implementing agent-based experiments on emergent communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PyFCGï¼Œè¿™æ˜¯ä¸€ä¸ªå°† Fluid Construction Grammar (FCG) ç§»æ¤åˆ° Python ç¼–ç¨‹è¯­è¨€çš„å¼€æºè½¯ä»¶åº“ã€‚PyFCG å…è®¸ç”¨æˆ·å°† FCG åŠŸèƒ½æ— ç¼é›†æˆåˆ° Python ç¨‹åºä¸­ï¼Œå¹¶èƒ½å¤Ÿä¸ Python ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿä¸­çš„å…¶ä»–åº“ååŒå·¥ä½œã€‚è¯¥åº“é€šè¿‡ä¸‰ä¸ªå¼•å¯¼å¼æ•™ç¨‹å±•ç¤ºäº†å…¶åœ¨ FCG å…¸å‹ç”¨ä¾‹ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å¯¹ Construction Grammar åˆ†æè¿›è¡Œå½¢å¼åŒ–ä¸æµ‹è¯•ï¼Œä»¥åŠä»è¯­æ–™åº“ä¸­å­¦ä¹ åŸºäºä½¿ç”¨çš„ Usage-based Construction Grammarsã€‚æ­¤å¤–ï¼ŒPyFCG è¿˜èƒ½ç”¨äºå®ç°å…³äºæ¶Œç°é€šä¿¡ (Emergent Communication) çš„å¤šæ™ºèƒ½ä½“å®éªŒã€‚è¯¥å·¥å…·çš„å‘å¸ƒä¸ºè®¡ç®—è¯­è¨€å­¦ç ”ç©¶æä¾›äº†ä¸€ä¸ªèƒ½å¤Ÿæ·±åº¦èåˆç°ä»£ç¼–ç¨‹ç”Ÿæ€çš„æœ‰åŠ›å¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12920v1",
      "published_date": "2025-05-19 10:00:01 UTC",
      "updated_date": "2025-05-19 10:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:17.998910+00:00"
    },
    {
      "arxiv_id": "2505.14718v1",
      "title": "Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection",
      "title_zh": "æå‡å·¥ä¸šå›¾åƒæ£€æµ‹ä¸­çš„å½¢çŠ¶æ„ŸçŸ¥ä¸åˆ†å‰²ä¸€è‡´æ€§",
      "authors": [
        "Guoxuan Mao",
        "Ting Cao",
        "Ziyang Li",
        "Yuan Dong"
      ],
      "abstract": "Semantic segmentation stands as a pivotal research focus in computer vision. In the context of industrial image inspection, conventional semantic segmentation models fail to maintain the segmentation consistency of fixed components across varying contextual environments due to a lack of perception of object contours. Given the real-time constraints and limited computing capability of industrial image detection machines, it is also necessary to create efficient models to reduce computational complexity. In this work, a Shape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes of objects to achieve excellent segmentation consistency by separately supervising the extraction of boundary and body information from images. In SPENet, a novel method is introduced for describing fuzzy boundaries to better adapt to real-world scenarios named Variable Boundary Domain (VBD). Additionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to measure segmentation consistency for fixed components. Our approach attains the best segmentation accuracy and competitive speed on our dataset, showcasing significant advantages in CMSE among numerous state-of-the-art real-time segmentation networks, achieving a reduction of over 50% compared to the previously top-performing models.",
      "tldr_zh": "åœ¨å·¥ä¸šå›¾åƒæ£€æŸ¥ä¸­ï¼Œç”±äºç¼ºä¹å¯¹ç‰©ä½“è½®å»“çš„æ„ŸçŸ¥ï¼Œä¼ ç»Ÿè¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨å¤šå˜ç¯å¢ƒä¸­éš¾ä»¥ä¿æŒå›ºå®šç»„ä»¶çš„åˆ†å‰²ä¸€è‡´æ€§(segmentation consistency)ï¼Œä¸”é¢ä¸´è®¡ç®—èµ„æºæœ‰é™çš„å®æ—¶æ€§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†SPENet (Shape-Aware Efficient Network)ï¼Œä¸€ç§é€šè¿‡åˆ†åˆ«ç›‘ç£è¾¹ç•Œ(boundary)å’Œä¸»ä½“(body)ä¿¡æ¯æå–æ¥å¢å¼ºå½¢çŠ¶æ„ŸçŸ¥çš„è½»é‡åŒ–ç½‘ç»œã€‚è®ºæ–‡å¼•å…¥äº†ä¸€ç§åä¸ºVariable Boundary Domain (VBD)çš„æ–°æ–¹æ³•æ¥æè¿°æ¨¡ç³Šè¾¹ç•Œï¼Œä»¥æ›´å¥½åœ°é€‚åº”çœŸå®å·¥ä¸šåœºæ™¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¡¡é‡æŒ‡æ ‡Consistency Mean Square Error (CMSE)ï¼Œç”¨äºé‡åŒ–è¯„ä¼°å›ºå®šç»„ä»¶çš„åˆ†å‰²ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPENetåœ¨æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³çš„åˆ†å‰²å‡†ç¡®ç‡å’Œæå…·ç«äº‰åŠ›çš„æ¨ç†é€Ÿåº¦ã€‚å°¤å…¶åœ¨CMSEæŒ‡æ ‡ä¸Šï¼ŒSPENetç›¸æ¯”ä¹‹å‰æœ€å…ˆè¿›çš„æ¨¡å‹é™ä½äº†è¶…è¿‡50%ï¼Œæ˜¾è‘—æå‡äº†å·¥ä¸šæ£€æµ‹ä¸­çš„é²æ£’æ€§ä¸ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.14718v1",
      "published_date": "2025-05-19 09:57:00 UTC",
      "updated_date": "2025-05-19 09:57:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:11.020300+00:00"
    },
    {
      "arxiv_id": "2505.12910v2",
      "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs",
      "title_zh": "SourceDetMambaï¼šä¸€ç§ç”¨äºæ—¶åºè¶…å›¾æº¯æºçš„å›¾æ„ŸçŸ¥çŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Le Cheng",
        "Peican Zhu",
        "Yangming Guo",
        "Chao Gao",
        "Zhen Wang",
        "Keke Tang"
      ],
      "abstract": "Source detection on graphs has demonstrated high efficacy in identifying rumor origins. Despite advances in machine learning-based methods, many fail to capture intrinsic dynamics of rumor propagation. In this work, we present SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs, which harnesses the recent success of the state space model Mamba, known for its superior global modeling capabilities and computational efficiency, to address this challenge. Specifically, we first employ hypergraphs to model high-order interactions within social networks. Subsequently, temporal network snapshots generated during the propagation process are sequentially fed in reverse order into Mamba to infer underlying propagation dynamics. Finally, to empower the sequential model to effectively capture propagation patterns while integrating structural information, we propose a novel graph-aware state update mechanism, wherein the state of each node is propagated and refined by both temporal dependencies and topological context. Extensive evaluations on eight datasets demonstrate that SourceDetMamba consistently outperforms state-of-the-art approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SourceDetMambaï¼Œä¸€ç§é’ˆå¯¹åºåˆ—è¶…å›¾(Sequential Hypergraphs)è®¾è®¡çš„å›¾æ„ŸçŸ¥çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç¤¾äº¤ç½‘ç»œä¸­è°£è¨€æºæ£€æµ‹(Source detection)éš¾ä»¥æœ‰æ•ˆæ•è·ä¼ æ’­åŠ¨æ€çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨è¶…å›¾(Hypergraphs)å»ºæ¨¡ç½‘ç»œä¸­çš„é«˜é˜¶äº¤äº’ï¼Œå¹¶å°†ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ—¶é—´ç½‘ç»œå¿«ç…§æŒ‰é€†åºè¾“å…¥Mambaæ¨¡å‹ï¼Œä»¥æ¨æ–­æ½œåœ¨çš„ä¼ æ’­åŠ¨åŠ›å­¦è§„å¾‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å›¾æ„ŸçŸ¥çŠ¶æ€æ›´æ–°æœºåˆ¶(Graph-aware state update mechanism)ï¼Œä½¿æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€èƒ½å¤ŸåŒæ—¶é€šè¿‡æ—¶é—´ä¾èµ–å’Œæ‹“æ‰‘ä¸Šä¸‹æ–‡è¿›è¡Œä¼ æ’­ä¸ä¼˜åŒ–ã€‚é€šè¿‡å¼•å…¥å…·æœ‰ä¼˜å¼‚å…¨å±€å»ºæ¨¡èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡çš„Mambaæ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•´åˆç»“æ„ä¿¡æ¯ä¸åºåˆ—ä¼ æ’­æ¨¡å¼ã€‚åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSourceDetMambaåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„åŸºå‡†æ–¹æ³•ï¼Œä¸ºå¤æ‚ç½‘ç»œä¸‹çš„æºæ£€æµ‹ä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”ç²¾å‡†çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted by IJCAI25",
      "pdf_url": "https://arxiv.org/pdf/2505.12910v2",
      "published_date": "2025-05-19 09:45:27 UTC",
      "updated_date": "2025-06-04 12:57:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:18.329721+00:00"
    },
    {
      "arxiv_id": "2505.12909v3",
      "title": "Sinusoidal Initialization, Time for a New Start",
      "title_zh": "æ­£å¼¦åˆå§‹åŒ–ï¼šå¼€å¯åˆå§‹åŒ–çš„æ–°èµ·ç‚¹",
      "authors": [
        "Alberto FernÃ¡ndez-HernÃ¡ndez",
        "Jose I. Mestre",
        "Manuel F. Dolz",
        "Jose Duato",
        "Enrique S. Quintana-OrtÃ­"
      ],
      "abstract": "Initialization plays a critical role in Deep Neural Network training, directly influencing convergence, stability, and generalization. Common approaches such as Glorot and He initializations rely on randomness, which can produce uneven weight distributions across layer connections. In this paper, we introduce the Sinusoidal initialization, a novel deterministic method that employs sinusoidal functions to construct structured weight matrices expressly to improve the spread and balance of weights throughout the network while simultaneously fostering a more uniform, well-conditioned distribution of neuron activation states from the very first forward pass. Because Sinusoidal initialization begins with weights and activations that are already evenly and efficiently utilized, it delivers consistently faster convergence, greater training stability, and higher final accuracy across a wide range of models, including convolutional neural networks, vision transformers, and large language models. On average, our experiments show an increase of 4.9% in final validation accuracy and 20.9% in convergence speed. By replacing randomness with structure, this initialization provides a stronger and more reliable foundation for Deep Learning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ Glorot å’Œ He ç­‰ä¼ ç»Ÿéšæœºåˆå§‹åŒ–æ–¹æ³•å¯¼è‡´çš„æƒé‡åˆ†å¸ƒä¸å‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Sinusoidal initialization çš„æ–°å‹ç¡®å®šæ€§åˆå§‹åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ­£å¼¦å‡½æ•°æ„å»ºç»“æ„åŒ–æƒé‡çŸ©é˜µï¼Œæ—¨åœ¨æ˜¾è‘—æ”¹å–„æƒé‡åœ¨å±‚é—´çš„åˆ†å¸ƒä¸å¹³è¡¡ï¼Œå¹¶ä»é¦–æ¬¡å‰å‘ä¼ æ’­èµ·ä¿ƒä½¿ç¥ç»å…ƒæ¿€æ´»çŠ¶æ€å‘ˆç°æ›´å‡åŒ€ã€æ›´è‰¯å¥½çš„æ¡ä»¶åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”±äºæƒé‡å’Œæ¿€æ´»å€¼å¾—åˆ°äº†æ›´é«˜æ•ˆçš„åˆå§‹åˆ©ç”¨ï¼Œè¯¥æ–¹æ³•åœ¨ Convolutional Neural Networks (CNNs)ã€Vision Transformers (ViTs) å’Œ Large Language Models (LLMs) ç­‰å¤šç§æ¨¡å‹ä¸Šå‡å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€æ›´å¼ºçš„è®­ç»ƒç¨³å®šæ€§å’Œæ›´é«˜çš„æœ€ç»ˆå‡†ç¡®ç‡ã€‚å¹³å‡è€Œè¨€ï¼ŒSinusoidal initialization ä½¿æ¨¡å‹éªŒè¯å‡†ç¡®ç‡æå‡äº† 4.9%ï¼Œå¹¶å°†æ”¶æ•›é€Ÿåº¦åŠ å¿«äº† 20.9%ã€‚é€šè¿‡ç”¨ç»“æ„åŒ–è®¾è®¡å–ä»£éšæœºæ€§ï¼Œè¯¥åˆå§‹åŒ–æ–¹æ³•ä¸ºæ·±åº¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªæ›´å¼ºå¤§ä¸”æ›´å¯é çš„è®­ç»ƒåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12909v3",
      "published_date": "2025-05-19 09:45:18 UTC",
      "updated_date": "2025-12-10 02:40:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:34.579982+00:00"
    },
    {
      "arxiv_id": "2505.12908v1",
      "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection",
      "title_zh": "é¢å‘åŸºäºäº‹ä»¶ç›®æ ‡æ£€æµ‹çš„åŠ¨æ€å›¾è¯±å¯¼è½®å»“æ„ŸçŸ¥çƒ­ä¼ å¯¼ç½‘ç»œ",
      "authors": [
        "Xiao Wang",
        "Yu Jin",
        "Lan Chen",
        "Bo Jiang",
        "Lin Zhu",
        "Yonghong Tian",
        "Jin Tang",
        "Bin Luo"
      ],
      "abstract": "Event-based Vision Sensors (EVS) have demonstrated significant advantages over traditional RGB frame-based cameras in low-light conditions, high-speed motion capture, and low latency. Consequently, object detection based on EVS has attracted increasing attention from researchers. Current event stream object detection algorithms are typically built upon Convolutional Neural Networks (CNNs) or Transformers, which either capture limited local features using convolutional filters or incur high computational costs due to the utilization of self-attention. Recently proposed vision heat conduction backbone networks have shown a good balance between efficiency and accuracy; however, these models are not specifically designed for event stream data. They exhibit weak capability in modeling object contour information and fail to exploit the benefits of multi-scale features. To address these issues, this paper proposes a novel dynamic graph induced contour-aware heat conduction network for event stream based object detection, termed CvHeat-DET. The proposed model effectively leverages the clear contour information inherent in event streams to predict the thermal diffusivity coefficients within the heat conduction model, and integrates hierarchical structural graph features to enhance feature learning across multiple scales. Extensive experiments on three benchmark datasets for event stream-based object detection fully validated the effectiveness of the proposed model. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvDET.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäº‹ä»¶çš„è§†è§‰ä¼ æ„Ÿå™¨(Event-based Vision Sensors, EVS)åœ¨ç›®æ ‡æ£€æµ‹ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºä¼ ç»ŸCNNå’ŒTransformeræ¨¡å‹åœ¨å¤„ç†äº‹ä»¶æµæ—¶å­˜åœ¨å±€éƒ¨ç‰¹å¾æå–ä¸è¶³æˆ–è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœç°æœ‰çƒ­ä¼ å¯¼éª¨å¹²ç½‘ç»œåœ¨å»ºæ¨¡ç‰©ä½“è½®å»“åŠå¤šå°ºåº¦ç‰¹å¾åˆ©ç”¨æ–¹é¢çš„å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCvHeat-DETçš„åŠ¨æ€å›¾å¼•å¯¼è½®å»“æ„ŸçŸ¥çƒ­ä¼ å¯¼ç½‘ç»œã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨äº‹ä»¶æµä¸­æ¸…æ™°çš„è½®å»“ä¿¡æ¯æ¥é¢„æµ‹çƒ­ä¼ å¯¼æ¨¡å‹ä¸­çš„çƒ­æ‰©æ•£ç³»æ•°(Thermal diffusivity coefficients)ï¼Œå¹¶é›†æˆå±‚çº§ç»“æ„å›¾ç‰¹å¾(Hierarchical structural graph features)ä»¥å¢å¼ºå¤šå°ºåº¦ç‰¹å¾å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªä¸»æµäº‹ä»¶æµç›®æ ‡æ£€æµ‹åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåŸºäºäº‹ä»¶æµçš„é«˜æ•ˆç›®æ ‡æ£€æµ‹æä¾›äº†ä¸€ç§å¹³è¡¡æ•ˆç‡ä¸ç²¾åº¦çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12908v1",
      "published_date": "2025-05-19 09:44:01 UTC",
      "updated_date": "2025-05-19 09:44:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:53.481994+00:00"
    },
    {
      "arxiv_id": "2505.12904v1",
      "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning",
      "title_zh": "åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ°´å£°ç›®æ ‡è¯†åˆ«é€šç”¨åµŒå…¥è®¡ç®—",
      "authors": [
        "Hilde I. Hummel",
        "Arwin Gansekoele",
        "Sandjai Bhulai",
        "Rob van der Mei"
      ],
      "abstract": "The increasing level of sound pollution in marine environments poses an increased threat to ocean health, making it crucial to monitor underwater noise. By monitoring this noise, the sources responsible for this pollution can be mapped. Monitoring is performed by passively listening to these sounds. This generates a large amount of data records, capturing a mix of sound sources such as ship activities and marine mammal vocalizations. Although machine learning offers a promising solution for automatic sound classification, current state-of-the-art methods implement supervised learning. This requires a large amount of high-quality labeled data that is not publicly available. In contrast, a massive amount of lower-quality unlabeled data is publicly available, offering the opportunity to explore unsupervised learning techniques. This research explores this possibility by implementing an unsupervised Contrastive Learning approach. Here, a Conformer-based encoder is optimized by the so-called Variance-Invariance-Covariance Regularization loss function on these lower-quality unlabeled data and the translation to the labeled data is made. Through classification tasks involving recognizing ship types and marine mammal vocalizations, our method demonstrates to produce robust and generalized embeddings. This shows to potential of unsupervised methods for various automatic underwater acoustic analysis tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·æ´‹ç¯å¢ƒå™ªå£°ç›‘æµ‹ä¸­é«˜è´¨é‡æ ‡è®°æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„æ— ç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„æ°´ä¸‹å£°å­¦ç›®æ ‡è¯†åˆ«ã€‚è¯¥æ–¹æ³•é‡‡ç”¨Conformer-based encoderä½œä¸ºæ ¸å¿ƒæ¶æ„ï¼Œå¹¶åˆ©ç”¨æ–¹å·®-ä¸å˜æ€§-åæ–¹å·®æ­£åˆ™åŒ–(Variance-Invariance-Covariance Regularization)æŸå¤±å‡½æ•°åœ¨å¤§é‡ä½è´¨é‡æ— æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚é€šè¿‡åœ¨èˆ¹èˆ¶ç±»å‹è¯†åˆ«å’Œæµ·æ´‹å“ºä¹³åŠ¨ç‰©é¸£å«å£°åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç ”ç©¶è¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰é«˜åº¦æ³›åŒ–æ€§çš„embeddingsã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ— ç›‘ç£å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å…¬å¼€çš„æ— æ ‡æ³¨èµ„æºï¼Œä¸ºå„ç§è‡ªåŠ¨åŒ–æ°´ä¸‹å£°å­¦åˆ†æä»»åŠ¡æä¾›äº†å…·æœ‰æ½œåŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12904v1",
      "published_date": "2025-05-19 09:37:46 UTC",
      "updated_date": "2025-05-19 09:37:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:57.913302+00:00"
    },
    {
      "arxiv_id": "2505.12903v1",
      "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach",
      "title_zh": "è¿ˆå‘ä½å»¶è¿Ÿçš„äº‹ä»¶æµè§†è§‰ç›®æ ‡è·Ÿè¸ªï¼šä¸€ç§æ…¢-å¿«ç»“åˆæ–¹æ³•",
      "authors": [
        "Shiao Wang",
        "Xiao Wang",
        "Liye Jin",
        "Bo Jiang",
        "Lin Zhu",
        "Lan Chen",
        "Yonghong Tian",
        "Bin Luo"
      ],
      "abstract": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºå¸§çš„ RGB è·Ÿè¸ªç®—æ³•åœ¨é«˜å»¶è¿Ÿå’Œèµ„æºå—é™ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸º SFTrack çš„æ–°å‹æ…¢é€Ÿ-å¿«é€Ÿè·Ÿè¸ªï¼ˆSlow-Fast Trackingï¼‰èŒƒå¼ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ç§äº’è¡¥æ¨¡å¼ï¼šä¸€ç§æ˜¯é¢å‘è®¡ç®—èµ„æºå……è¶³åœºæ™¯çš„é«˜ç²¾åº¦æ…¢é€Ÿè·Ÿè¸ªå™¨ï¼ˆslow trackerï¼‰ï¼Œä»¥åŠä¸€ç§ä¸“ä¸ºä½å»¶è¿Ÿå’Œèµ„æºå—é™ç¯å¢ƒè®¾è®¡çš„è½»é‡çº§å¿«é€Ÿè·Ÿè¸ªå™¨ï¼ˆfast trackerï¼‰ã€‚SFTrack é¦–å…ˆä»å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡çš„äº‹ä»¶æµä¸­æå–åŸºäºå›¾çš„è¡¨ç¤ºå­¦ä¹ ï¼ˆgraph-based representation learningï¼‰ï¼Œå¹¶å°†è¯¥ç»“æ„åŒ–ä¿¡æ¯é›†æˆåˆ°ä¸¤ä¸ªåŸºäº FlashAttention çš„è§†è§‰éª¨å¹²ç½‘ç»œä¸­ã€‚å…¶ä¸­ï¼Œå¿«é€Ÿè·Ÿè¸ªå™¨é€šè¿‡è½»é‡åŒ–ç½‘ç»œè®¾è®¡å¹¶åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­è¾“å‡ºå¤šä¸ªè¾¹ç•Œæ¡†ï¼ˆbounding boxï¼‰ï¼Œæ˜¾è‘—é™ä½äº†å¤„ç†å»¶è¿Ÿã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆsupervised fine-tuningï¼‰æ— ç¼ç»“åˆäº†è¿™ä¸¤ç§è·Ÿè¸ªå™¨ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†è’¸é¦ï¼ˆknowledge distillationï¼‰ç­–ç•¥è¿›ä¸€æ­¥å¢å¼ºäº†å¿«é€Ÿè·Ÿè¸ªå™¨çš„æ€§èƒ½ã€‚åœ¨ FE240ã€COESOT å’Œ EventVOT ç­‰å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç°å®åœºæ™¯ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12903v1",
      "published_date": "2025-05-19 09:37:23 UTC",
      "updated_date": "2025-05-19 09:37:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:47.994838+00:00"
    },
    {
      "arxiv_id": "2505.12900v1",
      "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models",
      "title_zh": "AutoGEEvalï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ GEE åœ°ç†ç©ºé—´ä»£ç ç”Ÿæˆå¤šæ¨¡æ€è‡ªåŠ¨åŒ–æ¡†æ¶",
      "authors": [
        "Shuyang Hou",
        "Zhangxiao Shen",
        "Huayi Wu",
        "Jianyuan Liang",
        "Haoyue Jiao",
        "Yaxian Qing",
        "Xiaopu Zhang",
        "Xu Li",
        "Zhipeng Gui",
        "Xuefeng Guan",
        "Longgang Xiang"
      ],
      "abstract": "Geospatial code generation is emerging as a key direction in the integration of artificial intelligence and geoscientific analysis. However, there remains a lack of standardized tools for automatic evaluation in this domain. To address this gap, we propose AutoGEEval, the first multimodal, unit-level automated evaluation framework for geospatial code generation tasks on the Google Earth Engine (GEE) platform powered by large language models (LLMs). Built upon the GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench) comprising 1325 test cases that span 26 GEE data types. The framework integrates both question generation and answer verification components to enable an end-to-end automated evaluation pipeline-from function invocation to execution validation. AutoGEEval supports multidimensional quantitative analysis of model outputs in terms of accuracy, resource consumption, execution efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including general-purpose, reasoning-augmented, code-centric, and geoscience-specialized models-revealing their performance characteristics and potential optimization pathways in GEE code generation. This work provides a unified protocol and foundational resource for the development and assessment of geospatial code generation models, advancing the frontier of automated natural language to domain-specific code translation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoGEEvalï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ Google Earth Engine (GEE) å¹³å°ä¸Šå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ°ç†ç©ºé—´ä»£ç ç”Ÿæˆä»»åŠ¡çš„å¤šæ¨¡æ€ã€å•å…ƒçº§è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¯¥é¢†åŸŸç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°å·¥å…·çš„é—®é¢˜ï¼Œç ”ç©¶è€…åŸºäº GEE Python API æ„å»ºäº†åŒ…å« 1325 ä¸ªæµ‹è¯•ç”¨ä¾‹çš„åŸºå‡†æ•°æ®é›† AutoGEEval-Benchï¼Œæ¶µç›–äº† 26 ç§ GEE æ•°æ®ç±»å‹ã€‚è¯¥æ¡†æ¶é›†æˆäº†é—®é¢˜ç”Ÿæˆä¸ç­”æ¡ˆéªŒè¯ç»„ä»¶ï¼Œèƒ½å¤Ÿå®ç°ä»å‡½æ•°è°ƒç”¨åˆ°æ‰§è¡ŒéªŒè¯çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ã€‚AutoGEEval æ”¯æŒä»å‡†ç¡®ç‡ã€èµ„æºæ¶ˆè€—ã€æ‰§è¡Œæ•ˆç‡å’Œé”™è¯¯ç±»å‹ç­‰å¤šä¸ªç»´åº¦å¯¹æ¨¡å‹è¿›è¡Œå®šé‡åˆ†æã€‚é€šè¿‡å¯¹ 18 ç§ä¸»æµ LLMs çš„è¯„ä¼°ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨ GEE ä»£ç ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½è¡¨ç°ä¸ä¼˜åŒ–æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºåœ°ç†ç©ºé—´ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¼€å‘ä¸è¯„æµ‹æä¾›äº†ç»Ÿä¸€çš„åè®®å’ŒåŸºç¡€èµ„æºï¼Œæ˜¾è‘—æ¨åŠ¨äº†è‡ªç„¶è¯­è¨€å‘ç‰¹å®šé¢†åŸŸä»£ç è‡ªåŠ¨ç¿»è¯‘çš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CG",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12900v1",
      "published_date": "2025-05-19 09:35:58 UTC",
      "updated_date": "2025-05-19 09:35:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:55:01.926600+00:00"
    },
    {
      "arxiv_id": "2505.14717v1",
      "title": "Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks",
      "title_zh": "Aneumoï¼šé›†æˆè®¡ç®—æµä½“åŠ›å­¦æ¨¡æ‹Ÿä¸æ·±åº¦å­¦ä¹ åŸºå‡†çš„å¤§è§„æ¨¡å¤šæ¨¡æ€åŠ¨è„‰ç˜¤æ•°æ®é›†",
      "authors": [
        "Xigui Li",
        "Yuanye Zhou",
        "Feiyang Xiao",
        "Xin Guo",
        "Chen Jiang",
        "Tan Pan",
        "Xingmeng Zhang",
        "Cenyu Liu",
        "Zeyun Miao",
        "Jianchao Ge",
        "Xiansheng Wang",
        "Qimeng Wang",
        "Yichi Zhang",
        "Wenbo Zhang",
        "Fengping Zhu",
        "Limei Han",
        "Yuan Qi",
        "Chensen Lin",
        "Yuan Cheng"
      ],
      "abstract": "Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in approximately 5\\% of the general population. Their rupture may lead to high mortality. Current methods for assessing IA risk focus on morphological and patient-specific factors, but the hemodynamic influences on IA development and rupture remain unclear. While accurate for hemodynamic studies, conventional computational fluid dynamics (CFD) methods are computationally intensive, hindering their deployment in large-scale or real-time clinical applications. To address this challenge, we curated a large-scale, high-fidelity aneurysm CFD dataset to facilitate the development of efficient machine learning algorithms for such applications. Based on 427 real aneurysm geometries, we synthesized 10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The authenticity of these synthetic shapes was confirmed by neurosurgeons. CFD computations were performed on each shape under eight steady-state mass flow conditions, generating a total of 85,280 blood flow dynamics data covering key parameters. Furthermore, the dataset includes segmentation masks, which can support tasks that use images, point clouds or other multimodal data as input. Additionally, we introduced a benchmark for estimating flow parameters to assess current modeling methods. This dataset aims to advance aneurysm research and promote data-driven approaches in biofluids, biomedical engineering, and clinical risk assessment. The code and dataset are available at: https://github.com/Xigui-Li/Aneumo.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Aneumoï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é¢…å†…åŠ¨è„‰ç˜¤(Intracranial Aneurysms)çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè®¡ç®—æµä½“åŠ¨åŠ›å­¦(CFD)åœ¨ä¸´åºŠå®æ—¶è¯„ä¼°ä¸­è®¡ç®—å¼€é”€è¿‡å¤§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜åŸºäº427ä¸ªçœŸå®åŠ¨è„‰ç˜¤å‡ ä½•ç»“æ„ï¼Œé€šè¿‡å—æ§å˜å½¢åˆæˆäº†10,660ä¸ª3Då½¢çŠ¶ä»¥æ¨¡æ‹Ÿç—…å˜æ¼”åŒ–ï¼Œå¹¶ç”±ç¥ç»å¤–ç§‘åŒ»ç”Ÿç¡®è®¤äº†å…¶è§£å‰–çœŸå®æ€§ã€‚æ•°æ®é›†åŒ…å«äº†åœ¨å…«ç§ç¨³æ€æµæ¡ä»¶ä¸‹ç”Ÿæˆçš„85,280é¡¹é«˜ä¿çœŸè¡€æµåŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ•°æ®ï¼Œå¹¶æä¾›åˆ†å‰²æ©ç ä»¥æ”¯æŒå›¾åƒã€ç‚¹äº‘ç­‰å¤šç§æ¨¡æ€çš„è¾“å…¥ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªç”¨äºä¼°è®¡æµå‚æ•°çš„åŸºå‡†(Benchmark)ï¼Œä»¥è¯„ä¼°ç°æœ‰çš„æ·±åº¦å­¦ä¹ å»ºæ¨¡æ–¹æ³•ã€‚Aneumoçš„å‘å¸ƒæ—¨åœ¨ä¿ƒè¿›ç”Ÿç‰©æµä½“ã€ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å’Œä¸´åºŠé£é™©è¯„ä¼°é¢†åŸŸçš„æ•°æ®é©±åŠ¨ç ”ç©¶ä¸ç®—æ³•å¼€å‘ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.14717v1",
      "published_date": "2025-05-19 09:32:09 UTC",
      "updated_date": "2025-05-19 09:32:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:54:58.061772+00:00"
    },
    {
      "arxiv_id": "2505.13561v1",
      "title": "Language and Thought: The View from LLMs",
      "title_zh": "è¯­è¨€ä¸æ€ç»´ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è§†è§’",
      "authors": [
        "Daniel Rothschild"
      ],
      "abstract": "Daniel Dennett speculated in *Kinds of Minds* 1996: \"Perhaps the kind of mind you get when you add language to it is so different from the kind of mind you can have without language that calling them both minds is a mistake.\" Recent work in AI can be seen as testing Dennett's thesis by exploring the performance of AI systems with and without linguistic training. I argue that the success of Large Language Models at inferential reasoning, limited though it may be, supports Dennett's radical view about the effect of language on thought. I suggest it is the abstractness and efficiency of linguistic encoding that lies behind the capacity of LLMs to perform inferences across a wide range of domains. In a slogan, language makes inference computationally tractable. I assess what these results in AI indicate about the role of language in the workings of our own biological minds.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Daniel Dennett å…³äºè¯­è¨€å¦‚ä½•ä»æ ¹æœ¬ä¸Šæ”¹å˜å¿ƒçµæœ¬è´¨çš„ç†è®ºï¼Œå¹¶é€šè¿‡åˆ†æ Large Language Models (LLMs) çš„è¡¨ç°å¯¹å…¶è¿›è¡Œäº†éªŒè¯ã€‚ä½œè€…æŒ‡å‡ºï¼ŒLLMs åœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—çš„æˆåŠŸè™½ç„¶æœ‰é™ï¼Œä½†è¶³ä»¥æ”¯æŒè¯­è¨€å¯¹æ€ç»´å…·æœ‰é‡å¡‘ä½œç”¨çš„æ¿€è¿›è§‚ç‚¹ã€‚æ–‡ç« è®¤ä¸ºï¼Œæ­£æ˜¯è¯­è¨€ç¼–ç çš„æŠ½è±¡æ€§ (abstractness) ä¸é«˜æ•ˆæ€§ (efficiency)ï¼Œä½¿å¾— LLMs èƒ½å¤Ÿå…·å¤‡è·¨é¢†åŸŸè¿›è¡Œé€»è¾‘æ¨ç†çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒè§‚ç‚¹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šè¯­è¨€ä½¿æ¨ç†åœ¨è®¡ç®—ä¸Šå˜å¾—å¯è¡Œ (computationally tractable)ã€‚é€šè¿‡å¯¹è¿™äº› AI ç ”ç©¶æˆæœçš„è¯„ä¼°ï¼Œä½œè€…è¿›ä¸€æ­¥é˜è¿°äº†è¯­è¨€åœ¨äººç±»ç”Ÿç‰©å¤§è„‘è¿ä½œä¸­æ‰€æ‰®æ¼”çš„å…³é”®è§’è‰²ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "37 Pages",
      "pdf_url": "https://arxiv.org/pdf/2505.13561v1",
      "published_date": "2025-05-19 09:29:32 UTC",
      "updated_date": "2025-05-19 09:29:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:55:28.235422+00:00"
    },
    {
      "arxiv_id": "2505.12894v2",
      "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion",
      "title_zh": "HyperDetï¼šåŸºäºäº¤äº’å¼å…³ç³»æ„å»ºä¸ç‰¹å¾ä¸°å¯Œæ³¨æ„åŠ›èåˆçš„è¶…å›¾æº¯æºæ£€æµ‹",
      "authors": [
        "Le Cheng",
        "Peican Zhu",
        "Yangming Guo",
        "Keke Tang",
        "Chao Gao",
        "Zhen Wang"
      ],
      "abstract": "Hypergraphs offer superior modeling capabilities for social networks, particularly in capturing group phenomena that extend beyond pairwise interactions in rumor propagation. Existing approaches in rumor source detection predominantly focus on dyadic interactions, which inadequately address the complexity of more intricate relational structures. In this study, we present a novel approach for Source Detection in Hypergraphs (HyperDet) via Interactive Relationship Construction and Feature-rich Attention Fusion. Specifically, our methodology employs an Interactive Relationship Construction module to accurately model both the static topology and dynamic interactions among users, followed by the Feature-rich Attention Fusion module, which autonomously learns node features and discriminates between nodes using a self-attention mechanism, thereby effectively learning node representations under the framework of accurately modeled higher-order relationships. Extensive experimental validation confirms the efficacy of our HyperDet approach, showcasing its superiority relative to current state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤ç½‘ç»œä¸­è°£è¨€ä¼ æ’­çš„ç¾¤ä½“ç°è±¡ï¼Œæå‡ºäº† HyperDet æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Source Detection æ–¹æ³•ä»…å…³æ³¨äºŒå…ƒäº¤äº’è€Œæ— æ³•æ•æ‰å¤æ‚è¶…å›¾ç»“æ„çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ Interactive Relationship Construction æ¨¡å—ç²¾ç¡®å»ºæ¨¡ç”¨æˆ·é—´çš„é™æ€æ‹“æ‰‘å’ŒåŠ¨æ€äº¤äº’ï¼Œä¸ºå¤æ‚ç¤¾äº¤å…³ç³»çš„è¡¨è¾¾æä¾›æ”¯æ’‘ã€‚éšåï¼Œåˆ©ç”¨ Feature-rich Attention Fusion æ¨¡å—ä¸­çš„ Self-attention æœºåˆ¶è‡ªä¸»å­¦ä¹ èŠ‚ç‚¹ç‰¹å¾ï¼Œä»è€Œåœ¨é«˜é˜¶å…³ç³»æ¡†æ¶ä¸‹å®ç°äº†å¯¹èŠ‚ç‚¹è¡¨ç¤ºçš„ç²¾å‡†åˆ»ç”»ã€‚è¿™ç§äº¤äº’å¼å…³ç³»æ„å»ºä¸ç‰¹å¾èåˆçš„ç»“åˆï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åŒºåˆ†ä¸åŒèŠ‚ç‚¹çš„èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒéªŒè¯ç»“æœè¡¨æ˜ï¼ŒHyperDet åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è¯†åˆ«è¶…å›¾ç»“æ„è°£è¨€æºå¤´æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted by IJCAI25",
      "pdf_url": "https://arxiv.org/pdf/2505.12894v2",
      "published_date": "2025-05-19 09:27:46 UTC",
      "updated_date": "2025-06-04 12:44:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:55:12.443322+00:00"
    },
    {
      "arxiv_id": "2505.12891v4",
      "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios",
      "title_zh": "TIMEï¼šé¢å‘ç°å®åœºæ™¯å¤§è¯­è¨€æ¨¡å‹æ—¶é—´æ¨ç†çš„å¤šå±‚çº§åŸºå‡†",
      "authors": [
        "Shaohang Wei",
        "Wei Li",
        "Feifan Song",
        "Wen Luo",
        "Tianyi Zhuang",
        "Haochen Tan",
        "Zhijiang Guo",
        "Houfeng Wang"
      ],
      "abstract": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME , and the project page link is https://sylvain-wei.github.io/TIME/ .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TIMEï¼Œä¸€ä¸ªä¸“é—¨ä¸ºçœŸå®ä¸–ç•Œåœºæ™¯è®¾è®¡çš„ LLMs æ—¶é—´æ¨ç† (Temporal Reasoning) å¤šå±‚çº§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶åœ¨å¤„ç†å¯†é›†æ—¶é—´ä¿¡æ¯ã€å¿«é€Ÿå˜åŒ–çš„äº‹ä»¶åŠ¨æ€ä»¥åŠå¤æ‚ç¤¾äº¤æ—¶é—´ä¾èµ–æ–¹é¢çš„ä¸è¶³ã€‚è¯¥åŸºå‡†åŒ…å« 38,522 ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›– 3 ä¸ªå±‚çº§å’Œ 11 ä¸ªç»†ç²’åº¦å­ä»»åŠ¡ï¼Œå¹¶ç”±åæ˜ ä¸åŒæŒ‘æˆ˜çš„ TIME-Wikiã€TIME-News å’Œ TIME-Dial ä¸‰ä¸ªå­æ•°æ®é›†ç»„æˆã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹æ¨ç†æ¨¡å‹å’Œéæ¨ç†æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæ·±å…¥åˆ†æäº† LLMs åœ¨å¤šæ ·åŒ–çœŸå®åœºæ™¯ä¸‹çš„æ—¶é—´æ¨ç†è¡¨ç°ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ€»ç»“äº†æµ‹è¯•æ—¶ç¼©æ”¾ (Test-time Scaling) å¯¹æå‡æ—¶é—´æ¨ç†èƒ½åŠ›çš„å…·ä½“å½±å“ï¼Œå¹¶å‘å¸ƒäº†ç»è¿‡äººå·¥æ ‡æ³¨çš„å­é›† TIME-Liteï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚è¯¥å·¥ä½œé€šè¿‡å¼€æºä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹ç†è§£çœŸå®åŠ¨æ€ä¸–ç•Œæä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’Œç ”ç©¶åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2505.12891v4",
      "published_date": "2025-05-19 09:22:02 UTC",
      "updated_date": "2025-10-08 03:45:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:55:29.695185+00:00"
    },
    {
      "arxiv_id": "2505.12886v1",
      "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective",
      "title_zh": "å¤§æ¨ç†æ¨¡å‹ä¸­çš„å¹»è§‰æ£€æµ‹ä¸æŠ‘åˆ¶ï¼šæœºç†æ€§è§†è§’",
      "authors": [
        "Zhongxiang Sun",
        "Qipeng Wang",
        "Haoyu Wang",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged--Reasoning Hallucination--where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates.",
      "tldr_zh": "è¯¥ç ”ç©¶ä» Mechanistic Perspective æ·±å…¥æ¢è®¨äº† Large Reasoning Models (LRMs) ä¸­ç‰¹æœ‰çš„ Reasoning Hallucination é—®é¢˜ï¼Œå³é€»è¾‘è¿è´¯ä½†äº‹å®é”™è¯¯çš„æ¨ç†è¿‡ç¨‹ã€‚ä½œè€…æå‡ºäº† Reasoning Scoreï¼Œé€šè¿‡é‡åŒ– LRM åæœŸå±‚æŠ•å½±åˆ°è¯æ±‡ç©ºé—´çš„ Logits æ•£åº¦æ¥è¡¡é‡æ¨ç†æ·±åº¦ï¼Œä»è€Œæœ‰æ•ˆåŒºåˆ†æµ…å±‚æ¨¡å¼åŒ¹é…ä¸çœŸæ­£çš„æ·±å±‚æ¨ç†ã€‚é€šè¿‡å¯¹ ReTruthQA æ•°æ®é›†çš„åˆ†æï¼Œç ”ç©¶è¯†åˆ«å‡ºæ¨ç†å¹»è§‰çš„ä¸¤ç§å…³é”®æ¨¡å¼ï¼šæ—©æœŸé˜¶æ®µçš„æ¨ç†æ·±åº¦æ³¢åŠ¨ä»¥åŠå¯¹é”™è¯¯å‰åºæ­¥éª¤çš„è¯¯å›æº¯ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æ„å»ºäº† Reasoning Hallucination Detection (RHD) æ¡†æ¶ï¼Œåœ¨å¤šé¢†åŸŸæ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº† State-of-the-art æ°´å¹³ã€‚ä¸ºè¿›ä¸€æ­¥ç¼“è§£å¹»è§‰ï¼Œç ”ç©¶å¼•å…¥äº† GRPO-R å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåˆ©ç”¨ Potential-based Shaping æ•´åˆäº†æ­¥éª¤çº§çš„æ·±å±‚æ¨ç†å¥–åŠ±ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æ¨ç†è´¨é‡å¹¶é™ä½äº†å¹»è§‰ç‡ï¼Œä¸ºå¼€å‘æ›´å¯é çš„æ¨ç†æ¨¡å‹æä¾›äº†ç†è®ºæ”¯æŒå’Œå®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12886v1",
      "published_date": "2025-05-19 09:16:40 UTC",
      "updated_date": "2025-05-19 09:16:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:55:34.085873+00:00"
    },
    {
      "arxiv_id": "2505.12884v2",
      "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks",
      "title_zh": "TinyAlignï¼šé€šè¿‡ç¼“è§£æ¨¡æ€å¯¹é½ç“¶é¢ˆæå‡è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yuanze Hu",
        "Zhaoxin Fan",
        "Xinyu Wang",
        "Gen Li",
        "Ye Qiu",
        "Zhichao Yang",
        "Wenjun Wu",
        "Kejian Wu",
        "Yifan Sun",
        "Xiaotie Deng",
        "Jin Dong"
      ],
      "abstract": "Lightweight Vision-Language Models (VLMs) are indispensable for resource-constrained applications. The prevailing approach to aligning vision and language models involves freezing both the vision encoder and the language model while training small connector modules. However, this strategy heavily depends on the intrinsic capabilities of the language model, which can be suboptimal for lightweight models with limited representational capacity. In this work, we investigate this alignment bottleneck through the lens of mutual information, demonstrating that the constrained capacity of the language model inherently limits the Effective Mutual Information (EMI) between multimodal inputs and outputs, thereby compromising alignment quality. To address this challenge, we propose TinyAlign, a novel framework inspired by Retrieval-Augmented Generation, which strategically retrieves relevant context from a memory bank to enrich multimodal inputs and enhance their alignment. Extensive empirical evaluations reveal that TinyAlign significantly reduces training loss, accelerates convergence, and enhances task performance. Remarkably, it allows models to achieve baseline-level performance with only 40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our work thus offers a practical pathway for developing more capable lightweight VLMs while introducing a fresh theoretical lens to better understand and address alignment bottlenecks in constrained multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„å¯¹é½ç“¶é¢ˆé—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å†»ç»“ç¼–ç å™¨å¹¶è®­ç»ƒå°å‹è¿æ¥å™¨çš„æ–¹æ³•å—é™äºè¯­è¨€æ¨¡å‹æœ¬èº«æœ‰é™çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ä½œè€…é€šè¿‡äº’ä¿¡æ¯(Mutual Information)çš„è§†è§’åˆ†æäº†è¯¥ç“¶é¢ˆï¼Œè¯æ˜äº†è¯­è¨€æ¨¡å‹çš„å®¹é‡é™åˆ¶äº†å¤šæ¨¡æ€è¾“å…¥è¾“å‡ºä¹‹é—´çš„æœ‰æ•ˆäº’ä¿¡æ¯(Effective Mutual Information)ï¼Œè¿›è€ŒæŸå®³äº†å¯¹é½è´¨é‡ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†TinyAlignæ¡†æ¶ï¼Œå—æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation)å¯å‘ï¼Œé€šè¿‡ä»è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡æ¥ä¸°å¯Œå¤šæ¨¡æ€è¾“å…¥å¹¶å¢å¼ºå…¶å¯¹é½ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTinyAlignæ˜¾è‘—é™ä½äº†è®­ç»ƒæŸå¤±å¹¶åŠ é€Ÿäº†æ”¶æ•›ï¼ŒåŒæ—¶æå‡äº†å„é¡¹ä»»åŠ¡çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¡†æ¶ä½¿æ¨¡å‹ä»…éœ€40%çš„å¾®è°ƒæ•°æ®å³å¯è¾¾åˆ°åŸºçº¿æ€§èƒ½ï¼Œå±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ã€‚è¯¥å·¥ä½œä¸ºå¼€å‘æ›´å¼ºå¤§çš„è½»é‡çº§VLMsæä¾›äº†å®ç”¨è·¯å¾„ï¼Œå¹¶ä¸ºç†è§£å’Œè§£å†³å—é™å¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„å¯¹é½ç“¶é¢ˆæä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12884v2",
      "published_date": "2025-05-19 09:11:54 UTC",
      "updated_date": "2025-06-30 08:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:05.901787+00:00"
    },
    {
      "arxiv_id": "2505.12882v1",
      "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems",
      "title_zh": "PhyDAï¼šé¢å‘å¤§æ°”ç³»ç»Ÿæ•°æ®åŒåŒ–çš„ç‰©ç†å¼•å¯¼æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Hao Wang",
        "Jindong Han",
        "Wei Fan",
        "Weijia Zhang",
        "Hao Liu"
      ],
      "abstract": "Data Assimilation (DA) plays a critical role in atmospheric science by reconstructing spatially continous estimates of the system state, which serves as initial conditions for scientific analysis. While recent advances in diffusion models have shown great potential for DA tasks, most existing approaches remain purely data-driven and often overlook the physical laws that govern complex atmospheric dynamics. As a result, they may yield physically inconsistent reconstructions that impair downstream applications. To overcome this limitation, we propose PhyDA, a physics-guided diffusion framework designed to ensure physical coherence in atmospheric data assimilation. PhyDA introduces two key components: (1) a Physically Regularized Diffusion Objective that integrates physical constraints into the training process by penalizing deviations from known physical laws expressed as partial differential equations, and (2) a Virtual Reconstruction Encoder that bridges observational sparsity for structured latent representations, further enhancing the model's ability to infer complete and physically coherent states. Experiments on the ERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and better physical plausibility compared to state-of-the-art baselines. Our results emphasize the importance of combining generative modeling with domain-specific physical knowledge and show that PhyDA offers a promising direction for improving real-world data assimilation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PhyDAï¼Œä¸€ç§ç‰©ç†å¼•å¯¼çš„æ‰©æ•£æ¡†æ¶ (Physics-Guided Diffusion Models)ï¼Œæ—¨åœ¨è§£å†³å¤§æ°”ç§‘å­¦æ•°æ®åŒåŒ– (Data Assimilation, DA) ä¸­çº¯æ•°æ®é©±åŠ¨æ¨¡å‹å› å¿½è§†ç‰©ç†è§„å¾‹è€Œå¯¼è‡´é‡å»ºç»“æœç‰©ç†ä¸ä¸€è‡´çš„é—®é¢˜ã€‚PhyDA å¼•å…¥äº†ç‰©ç†æ­£åˆ™åŒ–æ‰©æ•£ç›®æ ‡ (Physically Regularized Diffusion Objective)ï¼Œé€šè¿‡æƒ©ç½šä¸å·²çŸ¥åå¾®åˆ†æ–¹ç¨‹ (Partial Differential Equations) çš„åå·®ï¼Œå°†ç‰©ç†çº¦æŸæ•´åˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è™šæ‹Ÿé‡å»ºç¼–ç å™¨ (Virtual Reconstruction Encoder) å¼¥è¡¥è§‚æµ‹æ•°æ®çš„ç¨€ç–æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹æ¨æ–­å®Œæ•´ä¸”ç‰©ç†ç›¸å¹²çŠ¶æ€çš„èƒ½åŠ›ã€‚åœ¨ ERA5 å†åˆ†ææ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒPhyDA åœ¨é‡å»ºå‡†ç¡®æ€§å’Œç‰©ç†åˆç†æ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å°†ç”Ÿæˆæ¨¡å‹ä¸é¢†åŸŸç‰©ç†çŸ¥è¯†ç»“åˆçš„é‡è¦æ€§ï¼Œä¸ºæ”¹è¿›ç°å®ä¸–ç•Œçš„æ•°æ®åŒåŒ–ç³»ç»Ÿæä¾›äº†æå…·å‰æ™¯çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12882v1",
      "published_date": "2025-05-19 09:10:55 UTC",
      "updated_date": "2025-05-19 09:10:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:13.121966+00:00"
    },
    {
      "arxiv_id": "2505.12880v1",
      "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network",
      "title_zh": "AdS-GNNï¼šå…±å½¢ç­‰å˜å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Maksim Zhdanov",
        "Nabil Iqbal",
        "Erik Bekkers",
        "Patrick ForrÃ©"
      ],
      "abstract": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles, play a key role in many fields, including physics, mathematics, computer vision and (geometric) machine learning. Here we build a neural network that is equivariant under general conformal transformations. To achieve this, we lift data from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to exploit a known correspondence between conformal transformations of flat space and isometric transformations on the AdS space. We then build upon the fact that such isometric transformations have been extensively studied on general geometries in the geometric deep learning literature. We employ message-passing layers conditioned on the proper distance, yielding a computationally efficient framework. We validate our model on tasks from computer vision and statistical physics, demonstrating strong performance, improved generalization capacities, and the ability to extract conformal data such as scaling dimensions from the trained network.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AdS-GNNï¼Œä¸€ç§åœ¨å¹¿ä¹‰å…±å½¢å˜æ¢(conformal transformations)ä¸‹å…·æœ‰ç­‰å˜æ€§(equivariant)çš„å›¾ç¥ç»ç½‘ç»œã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶äººå‘˜å°†æ•°æ®ä»å¹³å¦çš„æ¬§å‡ é‡Œå¾—ç©ºé—´(flat Euclidean space)æå‡åˆ°åå¾·è¥¿ç‰¹(Anti-de Sitter, AdS)ç©ºé—´ï¼Œåˆ©ç”¨äº†å¹³å¦ç©ºé—´å…±å½¢å˜æ¢ä¸AdSç©ºé—´ç­‰è·å˜æ¢(isometric transformations)ä¹‹é—´çš„å·²çŸ¥å¯¹åº”å…³ç³»ã€‚æ¨¡å‹é‡‡ç”¨åŸºäºå›ºæœ‰è·ç¦»(proper distance)çš„æ¶ˆæ¯ä¼ é€’(message-passing)å±‚ï¼Œæ„å»ºäº†ä¸€ä¸ªè®¡ç®—æ•ˆç‡æé«˜çš„æ¡†æ¶ã€‚åœ¨è®¡ç®—æœºè§†è§‰å’Œç»Ÿè®¡ç‰©ç†ä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒAdS-GNNè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½å’Œæ›´ä¼˜çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç½‘ç»œè¿˜å±•ç°äº†ä»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­æå–ç¼©æ”¾ç»´åº¦(scaling dimensions)ç­‰å…³é”®å…±å½¢æ•°æ®çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-th"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12880v1",
      "published_date": "2025-05-19 09:08:52 UTC",
      "updated_date": "2025-05-19 09:08:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:09.198944+00:00"
    },
    {
      "arxiv_id": "2505.13557v1",
      "title": "AMAQA: A Metadata-based QA Dataset for RAG Systems",
      "title_zh": "AMAQAï¼šé¢å‘ RAG ç³»ç»Ÿçš„åŸºäºå…ƒæ•°æ®çš„é—®ç­”æ•°æ®é›†",
      "authors": [
        "Davide Bruni",
        "Marco Avvenuti",
        "Nicola Tonellotto",
        "Maurizio Tesconi"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering. The dataset is available at https://anonymous.4open.science/r/AMAQA-5D0D/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AMAQAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå…ƒæ•°æ®(Metadata)çš„æ–°å‹å¼€æ”¾è·å–é—®ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»ŸåŸºå‡†æµ‹è¯•ä¸­ç¼ºä¹å…ƒæ•°æ®é›†æˆçš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»26ä¸ªå…¬å¼€ Telegram ç¾¤ç»„æ”¶é›†çš„çº¦110ä¸‡æ¡è‹±æ–‡æ¶ˆæ¯ï¼Œå¹¶ä¸°å¯Œäº†æ—¶é—´æˆ³ã€ä¸»é¢˜ã€æƒ…æ„ŸåŸºè°ƒå’Œæ¯’æ€§æŒ‡æ ‡ç­‰å…ƒæ•°æ®ï¼Œèƒ½å¤Ÿé€šè¿‡ç‰¹å®šæ ‡å‡†è¿‡æ»¤æ–‡æ¡£æ¥å®ç°ç²¾ç¡®çš„ä¸Šä¸‹æ–‡æŸ¥è¯¢ã€‚è¿™ç§é›†æˆåœ¨ç½‘ç»œå®‰å…¨å’Œæƒ…æŠ¥ç­‰éœ€è¦å¿«é€Ÿåˆ†ææµ·é‡æ•°æ®ä¸”ä¿¡æ¯æ£€ç´¢æ—¶æ•ˆæ€§è‡³å…³é‡è¦çš„é¢†åŸŸå…·æœ‰æ˜¾è‘—ä»·å€¼ã€‚AMAQA åŒ…å«450ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ï¼Œæ˜¯å·²çŸ¥é¦–ä¸ªå°†å…ƒæ•°æ®å’Œä¸»é¢˜æ ‡ç­¾æ•´åˆè¿›å•è·³é—®ç­”(Single-hop QA)ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨å…ƒæ•°æ®å¯å°†ç³»ç»Ÿå‡†ç¡®ç‡ä» 0.12 æé«˜è‡³ 0.61ï¼Œå‡¸æ˜¾äº†ç»“æ„åŒ–ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢ç´¢äº†ä¼˜åŒ– LLM è¾“å…¥çš„ç­–ç•¥ï¼Œå¹¶åœ¨æœ€ä½³åŸºå‡†æ¨¡å‹çš„åŸºç¡€ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºæœªæ¥å…ƒæ•°æ®é©±åŠ¨çš„ RAG ç ”ç©¶å»ºç«‹äº†æ–°æ ‡å‡†ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13557v1",
      "published_date": "2025-05-19 08:59:08 UTC",
      "updated_date": "2025-05-19 08:59:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:16.789994+00:00"
    },
    {
      "arxiv_id": "2505.12872v2",
      "title": "From Grunts to Lexicons: Emergent Language from Cooperative Foraging",
      "title_zh": "ä»ä½é¸£åˆ°è¯æ±‡ï¼šåä½œè§…é£Ÿä¸­æ¶Œç°çš„è¯­è¨€",
      "authors": [
        "Maytus Piriyajitakonkij",
        "Rujikorn Charakorn",
        "Weicheng Tao",
        "Wei Pan",
        "Mingfei Sun",
        "Cheston Tan",
        "Mengmi Zhang"
      ],
      "abstract": "Language is a powerful communicative and cognitive tool. It enables humans to express thoughts, share intentions, and reason about complex phenomena. Despite our fluency in using and understanding language, the question of how it arises and evolves over time remains unsolved. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size, social dynamics, and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨åä½œé‡‡é›†æ¸¸æˆ(Foraging Games)æ¢è®¨äº†è¯­è¨€å¦‚ä½•åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æ¼”åŒ–ï¼Œä»¥éªŒè¯è¯­è¨€æ¼”åŒ–æºäºç¤¾äº¤å’Œç”Ÿæ€åˆä½œéœ€æ±‚çš„å‡è¯´ã€‚é€šè¿‡ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)ï¼Œæ™ºèƒ½ä½“åœ¨å…·æœ‰éƒ¨åˆ†å¯è§‚æµ‹æ€§çš„ç½‘æ ¼ä¸–ç•Œä¸­ä»é›¶å¼€å§‹å­¦ä¹ åè°ƒè¡ŒåŠ¨ä¸é€šä¿¡ç­–ç•¥ã€‚å®éªŒå‘ç°ï¼Œæ™ºèƒ½ä½“è‡ªä¸»å‘å±•å‡ºçš„é€šä¿¡åè®®å…·å¤‡äº†è‡ªç„¶è¯­è¨€çš„æ ¸å¿ƒç‰¹å¾ï¼ŒåŒ…æ‹¬ä»»æ„æ€§(arbitrariness)ã€äº’æ¢æ€§(interchangeability)ã€ä½ç§»æ€§(displacement)ã€æ–‡åŒ–ä¼ é€’(cultural transmission)å’Œç»„åˆæ€§(compositionality)ã€‚ç ”ç©¶è¿›ä¸€æ­¥å®šé‡åˆ†æäº†ç¾¤ä½“è§„æ¨¡ã€ç¤¾ä¼šåŠ¨æ€å’Œæ—¶é—´ä¾èµ–ç­‰å› ç´ å¯¹æ–°å…´è¯­è¨€ç‰¹å¾çš„å…·ä½“å¡‘é€ ä½œç”¨ã€‚è¯¥å·¥ä½œä¸ºåœ¨å…·èº«å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹ç ”ç©¶é€šä¿¡å¦‚ä½•ä»åä½œç›®æ ‡ä¸­äº§ç”Ÿæä¾›äº†ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12872v2",
      "published_date": "2025-05-19 08:57:30 UTC",
      "updated_date": "2025-09-26 02:22:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:59.434576+00:00"
    },
    {
      "arxiv_id": "2505.12871v1",
      "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?",
      "title_zh": "ä½ç§©è‡ªé€‚åº”æ˜¯å¦ä¼šå¯¼è‡´æ¨¡å‹å¯¹è®­ç»ƒæ—¶æ”»å‡»çš„é²æ£’æ€§ä¸‹é™ï¼Ÿ",
      "authors": [
        "Zi Liang",
        "Haibo Hu",
        "Qingqing Ye",
        "Yaxin Xiao",
        "Ronghua Li"
      ],
      "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA's training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä½ç§©è‡ªé€‚åº” (Low Rank Adaptation, LoRA) åœ¨å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­é’ˆå¯¹è®­ç»ƒæ—¶æ”»å‡»ï¼ˆå¦‚æ•°æ®æŠ•æ¯’å’Œåé—¨æ”»å‡»ï¼‰çš„ç¨³å¥æ€§é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€å¥—åˆ†ææ¡†æ¶æ¥æ¨¡æ‹Ÿ LoRA çš„è®­ç»ƒåŠ¨æ€ï¼Œå¹¶ç»“åˆç¥ç»æ­£åˆ‡æ ¸ (Neural Tangent Kernel) å’Œä¿¡æ¯è®ºæ–¹æ³•ï¼Œå»ºç«‹äº†å…¶ä½ç§©ç»“æ„ä¸æ”»å‡»è„†å¼±æ€§ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚åˆ†æç»“æœæŒ‡å‡ºï¼Œè™½ç„¶ LoRA ç›¸æ¯”äºå…¨å‚æ•°å¾®è°ƒ (Full Fine-tuning) åœ¨åº”å¯¹åé—¨æ”»å‡» (Backdoor Attacks) æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ï¼Œä½†ç”±äºå…¶ç®€åŒ–çš„ä¿¡æ¯å‡ ä½• (Information Geometry) å±æ€§ï¼Œå®ƒåœ¨é¢å¯¹æ— ç›®æ ‡æ•°æ®æŠ•æ¯’ (Untargeted Data Poisoning) æ—¶åè€Œæ›´æ˜“å—åˆ°å½±å“ã€‚è¯¥ç ”ç©¶æœ€åé€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºå‘ç°ï¼Œæ­ç¤ºäº†é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åœ¨å®‰å…¨æ€§æ–¹é¢çš„å¤æ‚ç‰¹æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear at ICML 25",
      "pdf_url": "https://arxiv.org/pdf/2505.12871v1",
      "published_date": "2025-05-19 08:57:08 UTC",
      "updated_date": "2025-05-19 08:57:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:38.663850+00:00"
    },
    {
      "arxiv_id": "2505.12869v1",
      "title": "Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption",
      "title_zh": "åŸºäºå…¨åŒæ€åŠ å¯†çš„éšç§ä¿æŠ¤å¤–åŒ…ç‰¹å¾é€‰æ‹©",
      "authors": [
        "Koki Wakiyama",
        "Tomohiro I",
        "Hiroshi Sakamoto"
      ],
      "abstract": "Feature selection is a technique that extracts a meaningful subset from a set of features in training data. When the training data is large-scale, appropriate feature selection enables the removal of redundant features, which can improve generalization performance, accelerate the training process, and enhance the interpretability of the model. This study proposes a privacy-preserving computation model for feature selection. Generally, when the data owner and analyst are the same, there is no need to conceal the private information. However, when they are different parties or when multiple owners exist, an appropriate privacy-preserving framework is required. Although various private feature selection algorithms, they all require two or more computing parties and do not guarantee security in environments where no external party can be fully trusted. To address this issue, we propose the first outsourcing algorithm for feature selection using fully homomorphic encryption. Compared to a prior two-party algorithm, our result improves the time and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n denote the number of features and data samples, respectively. We also implemented the proposed algorithm and conducted comparative experiments with the naive one. The experimental result shows the efficiency of our method even with small datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…¨åŒæ€åŠ å¯†(Fully Homomorphic Encryption)çš„éšç§ä¿æŠ¤å¤–åŒ…ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡è®­ç»ƒæ•°æ®åœ¨è·¨ä¸»ä½“åˆ†ææ—¶çš„éšç§æ³„éœ²é£é™©ã€‚ç°æœ‰çš„éšç§ä¿æŠ¤ç‰¹å¾é€‰æ‹©(Feature Selection)æ–¹æ³•é€šå¸¸ä¾èµ–å¤šä¸ªè®¡ç®—æ–¹ï¼Œä¸”åœ¨ç¼ºä¹å®Œå…¨å¯ä¿¡ç¬¬ä¸‰æ–¹çš„æƒ…å†µä¸‹éš¾ä»¥ä¿è¯å®‰å…¨æ€§ï¼Œè€Œè¯¥æ–¹æ¡ˆé€šè¿‡FHEé¦–æ¬¡å®ç°äº†ç‰¹å¾é€‰æ‹©ä»»åŠ¡çš„å®‰å…¨å¤–åŒ…ã€‚åœ¨ç®—æ³•æ€§èƒ½æ–¹é¢ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†æ—¶é—´å¤æ‚åº¦ä» $O(kn^2)$ é™ä½è‡³ $O(kn \\log^3 n)$ï¼Œå¹¶å°†ç©ºé—´å¤æ‚åº¦ä¼˜åŒ–è‡³ $O(kn)$ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•å³ä½¿åœ¨å¤„ç†å°è§„æ¨¡æ•°æ®é›†æ—¶ä¹Ÿå±•ç°å‡ºä¼˜äºä¼ ç»Ÿä¸¤æ–¹ç®—æ³•çš„æ•ˆç‡ã€‚è¿™ä¸€æˆæœä¸ºä¸å¯ä¿¡ç¯å¢ƒä¸‹çš„éšç§ä¿æŠ¤ç‰¹å¾é€‰æ‹©æä¾›äº†é«˜æ•ˆä¸”å®‰å…¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12869v1",
      "published_date": "2025-05-19 08:55:56 UTC",
      "updated_date": "2025-05-19 08:55:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:38.546254+00:00"
    },
    {
      "arxiv_id": "2505.12864v6",
      "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
      "title_zh": "LEXamï¼šåŸºäº340åœºæ³•å¾‹è€ƒè¯•çš„æ³•å¾‹æ¨ç†åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yu Fan",
        "Jingwei Ni",
        "Jakob Merane",
        "Yang Tian",
        "Yoan HermstrÃ¼wer",
        "Yinya Huang",
        "Mubashara Akhtar",
        "Etienne Salimbeni",
        "Florian Geering",
        "Oliver Dreyer",
        "Daniel Brunner",
        "Markus Leippold",
        "Mrinmaya Sachan",
        "Alexander Stremitzer",
        "Christoph Engel",
        "Elliott Ash",
        "Joel Niklaus"
      ],
      "abstract": "Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. To address this, we introduce \\textsc{LEXam}, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately, closely aligning with human expert assessments. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. We have open-sourced our code on https://github.com/LEXam-Benchmark/LEXam and released our data on https://huggingface.co/datasets/LEXam-Benchmark/LEXam. Project page: https://lexam-benchmark.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† LEXamï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) é•¿ç¯‡æ³•å¾‹æ¨ç†èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å«æºè‡ª 340 åœºæ³•å­¦é™¢è€ƒè¯•çš„ 4,886 é“é¢˜ç›®ï¼Œæ¶µç›– 116 é—¨è¯¾ç¨‹ï¼Œå¹¶æä¾›è‹±è¯­å’Œå¾·è¯­çš„å¼€æ”¾å¼é—®é¢˜åŠå¤šé¡¹é€‰æ‹©é¢˜ã€‚é™¤äº†å‚è€ƒç­”æ¡ˆå¤–ï¼Œå¼€æ”¾å¼é—®é¢˜è¿˜é…æœ‰æ˜ç¡®çš„æ³•å¾‹æ¨ç†å¼•å¯¼ï¼Œæ¶‰åŠäº‰ç‚¹è¯†åˆ« (Issue spotting)ã€è§„åˆ™å¬å› (Rule recall) å’Œè§„åˆ™åº”ç”¨ (Rule application) ç­‰æ ¸å¿ƒç¯èŠ‚ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›®å‰çš„ LLMs åœ¨å¤„ç†éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤æ³•å¾‹æ¨ç†çš„å¼€æ”¾å¼é—®é¢˜æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é›†æˆ LLM-as-a-Judge èŒƒå¼å¹¶ç»“åˆä¸¥è°¨çš„äººç±»ä¸“å®¶éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿä¸äººç±»è¯„ä¼°é«˜åº¦ä¸€è‡´åœ°è¡¡é‡æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤ã€‚LEXam æä¾›äº†ä¸€ç§è¶…è¶Šç®€å•å‡†ç¡®ç‡æŒ‡æ ‡çš„å¯æ‰©å±•è¯„ä¼°æ–¹æ³•ï¼Œä¸ºæ·±å…¥åˆ†ææ³•å¾‹æ¨ç†è´¨é‡æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12864v6",
      "published_date": "2025-05-19 08:48:12 UTC",
      "updated_date": "2026-01-20 21:54:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:25.604510+00:00"
    },
    {
      "arxiv_id": "2505.12863v1",
      "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio",
      "title_zh": "ä¹è°±å›¾åƒã€ç¬¦å·åŒ–éŸ³ä¹ä¸æ¼”å¥éŸ³é¢‘çš„ç»Ÿä¸€è·¨æ¨¡æ€ç¿»è¯‘",
      "authors": [
        "Jongmin Jung",
        "Dongmin Kim",
        "Sihun Lee",
        "Seola Cho",
        "Hyungjoon Soh",
        "Irmak Bukey",
        "Chris Donahue",
        "Dasaem Jeong"
      ],
      "abstract": "Music exists in various modalities, such as score images, symbolic scores, MIDI, and audio. Translations between each modality are established as core tasks of music information retrieval, such as automatic music transcription (audio-to-MIDI) and optical music recognition (score image to symbolic score). However, most past work on multimodal translation trains specialized models on individual translation tasks. In this paper, we propose a unified approach, where we train a general-purpose model on many translation tasks simultaneously. Two key factors make this unified approach viable: a new large-scale dataset and the tokenization of each modality. Firstly, we propose a new dataset that consists of more than 1,300 hours of paired audio-score image data collected from YouTube videos, which is an order of magnitude larger than any existing music modal translation datasets. Secondly, our unified tokenization framework discretizes score images, audio, MIDI, and MusicXML into a sequence of tokens, enabling a single encoder-decoder Transformer to tackle multiple cross-modal translation as one coherent sequence-to-sequence task. Experimental results confirm that our unified multitask model improves upon single-task baselines in several key areas, notably reducing the symbol error rate for optical music recognition from 24.58% to a state-of-the-art 13.67%, while similarly substantial improvements are observed across the other translation tasks. Notably, our approach achieves the first successful score-image-conditioned audio generation, marking a significant breakthrough in cross-modal music generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³ä¹é¢†åŸŸä¸­ä¹è°±å›¾åƒ(score images)ã€ç¬¦å·åŒ–ä¹è°±ã€MIDIå’ŒéŸ³é¢‘(audio)ç­‰å¤šç§æ¨¡æ€å¹¶å­˜ä¸”ä»¥å¾€ç ”ç©¶å¤šé‡‡ç”¨å•ä¸€ä»»åŠ¡ä¸“ç”¨æ¨¡å‹çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è·¨æ¨¡æ€è½¬æ¢æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡å»ºç«‹ä¸€ä¸ªåŒ…å«è¶…è¿‡1300å°æ—¶é…å¯¹éŸ³é¢‘-ä¹è°±å›¾åƒæ•°æ®çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†æ¯”ç°æœ‰æ•°æ®é›†å¤§ä¸€ä¸ªæ•°é‡çº§çš„æ”¯æŒã€‚æ ¸å¿ƒæ–¹æ³•é‡‡ç”¨ç»Ÿä¸€çš„æ ‡è®°åŒ–(tokenization)æ¡†æ¶å°†ä¸åŒæ¨¡æ€ç¦»æ•£åŒ–ä¸ºTokenåºåˆ—ï¼Œä½¿å¾—å•ä¸ªTransformerç¼–ç å™¨-è§£ç å™¨æ¶æ„èƒ½å¤Ÿå°†å¤šé¡¹è·¨æ¨¡æ€è½¬æ¢ä»»åŠ¡å¤„ç†ä¸ºä¸€è‡´çš„åºåˆ—åˆ°åºåˆ—(sequence-to-sequence)ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç»Ÿä¸€å¤šä»»åŠ¡æ¨¡å‹åœ¨å¤šä¸ªå…³é”®é¢†åŸŸå‡ä¼˜äºå•ä»»åŠ¡åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶å°†å…‰å­¦éŸ³ä¹è¯†åˆ«(optical music recognition)çš„ç¬¦å·é”™è¯¯ç‡ä»24.58%æ˜¾è‘—é™ä½è‡³13.67%çš„é¢†å…ˆæ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜é¦–æ¬¡æˆåŠŸå®ç°äº†åŸºäºä¹è°±å›¾åƒæ¡ä»¶çš„éŸ³é¢‘ç”Ÿæˆ(score-image-conditioned audio generation)ï¼Œåœ¨è·¨æ¨¡æ€éŸ³ä¹ç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to IEEE Transactions on Audio, Speech and Language Processing (TASLPRO)",
      "pdf_url": "https://arxiv.org/pdf/2505.12863v1",
      "published_date": "2025-05-19 08:46:45 UTC",
      "updated_date": "2025-05-19 08:46:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:54.093739+00:00"
    },
    {
      "arxiv_id": "2505.12851v1",
      "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting",
      "title_zh": "FLTGï¼šåŸºäºè§’åº¦é˜²å¾¡ä¸ Non-IID æ„ŸçŸ¥åŠ æƒçš„æ‹œå åº­é²æ£’è”é‚¦å­¦ä¹ ",
      "authors": [
        "Yanhua Wen",
        "Lu Ai",
        "Gang Liu",
        "Chuang Li",
        "Jianhao Wei"
      ],
      "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL) threaten training integrity by manipulating malicious clients' updates. Existing methods struggle with limited robustness under high malicious client ratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To address this, we propose FLTG, a novel aggregation algorithm integrating angle-based defense and dynamic reference selection. FLTG first filters clients via ReLU-clipped cosine similarity, leveraging a server-side clean dataset to exclude misaligned updates. It then dynamically selects a reference client based on the prior global model to mitigate non-i.i.d. bias, assigns aggregation weights inversely proportional to angular deviations, and normalizes update magnitudes to suppress malicious scaling. Evaluations across datasets of varying complexity under five classic attacks demonstrate FLTG's superiority over state-of-the-art methods under extreme bias scenarios and sustains robustness with a higher proportion(over 50%) of malicious clients.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FLTGï¼Œä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹  (Federated Learning) èšåˆç®—æ³•ï¼Œæ—¨åœ¨åº”å¯¹ Byzantine attacks å¯¹æ¨¡å‹è®­ç»ƒå®Œæ•´æ€§çš„å¨èƒã€‚FLTG ç»“åˆäº†åŸºäºè§’åº¦çš„é˜²å¾¡æœºåˆ¶å’ŒåŠ¨æ€å‚è€ƒé€‰æ‹©ï¼Œé¦–å…ˆé€šè¿‡ ReLU-clipped cosine similarity è¿‡æ»¤å®¢æˆ·ç«¯ï¼Œå¹¶åˆ©ç”¨æœåŠ¡å™¨ç«¯çš„å¹²å‡€æ•°æ®é›†å‰”é™¤å¯¹é½ä¸è‰¯çš„æ›´æ–°ã€‚ä¸ºäº†ç¼“è§£ non-i.i.d. æ•°æ®å¸¦æ¥çš„åç½®ï¼Œè¯¥ç®—æ³•æ ¹æ®å…ˆéªŒå…¨å±€æ¨¡å‹åŠ¨æ€é€‰æ‹©å‚è€ƒå®¢æˆ·ç«¯ï¼Œå¹¶æŒ‰ç…§è§’åº¦åå·®çš„æ¯”ä¾‹åˆ†é…èšåˆæƒé‡ï¼ŒåŒæ—¶é€šè¿‡å¹…åº¦å½’ä¸€åŒ–æŠ‘åˆ¶æ¶æ„ç¼©æ”¾ã€‚åœ¨äº”ç§ç»å…¸æ”»å‡»æ¨¡å¼ä¸‹çš„è¯„ä¼°è¯æ˜ï¼ŒFLTG åœ¨æç«¯åç½®åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æ¶æ„å®¢æˆ·ç«¯æ¯”ä¾‹è¶…è¿‡ 50% çš„æƒ…å†µä¸‹ï¼ŒFLTG ä¾ç„¶èƒ½ä¿æŒå“è¶Šçš„é²æ£’æ€§å¹¶ç»´æŒé«˜å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages, 5 figures, BlockSys2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12851v1",
      "published_date": "2025-05-19 08:39:07 UTC",
      "updated_date": "2025-05-19 08:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:56:58.606440+00:00"
    },
    {
      "arxiv_id": "2505.12845v1",
      "title": "Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks",
      "title_zh": "å¤šçº§æ„ŸçŸ¥åå¥½å­¦ä¹ ï¼šé’ˆå¯¹å¤æ‚å¤šæŒ‡ä»¤ä»»åŠ¡å¢å¼º RLHF",
      "authors": [
        "Ruopei Sun",
        "Jianfeng Cai",
        "Jinhua Zhu",
        "Kangwen Zhao",
        "Dongyun Xue",
        "Wengang Zhou",
        "Li Li",
        "Houqiang Li"
      ],
      "abstract": "RLHF has emerged as a predominant approach for aligning artificial intelligence systems with human preferences, demonstrating exceptional and measurable efficacy in instruction following tasks; however, it exhibits insufficient compliance capabilities when confronted with complex multi-instruction tasks. Conventional approaches rely heavily on human annotation or more sophisticated large language models, thereby introducing substantial resource expenditure or potential bias concerns. Meanwhile, alternative synthetic methods that augment standard preference datasets often compromise the model's semantic quality. Our research identifies a critical oversight in existing techniques, which predominantly focus on comparing responses while neglecting valuable latent signals embedded within prompt inputs, and which only focus on preference disparities at the intra-sample level, while neglecting to account for the inter-sample level preference differentials that exist among preference data. To leverage these previously neglected indicators, we propose a novel Multi-level Aware Preference Learning (MAPL) framework, capable of enhancing multi-instruction capabilities. Specifically, for any given response in original preference data pairs, we construct varied prompts with a preference relation under different conditions, in order to learn intra-sample level preference disparities. Furthermore, for any given original preference pair, we synthesize multi-instruction preference pairs to capture preference discrepancies at the inter-sample level. Building on the two datasets constructed above, we consequently devise two sophisticated training objective functions. Subsequently, our framework integrates seamlessly into both Reward Modeling and Direct Preference Optimization paradigms. Through rigorous evaluation across multiple benchmarks, we empirically validate the efficacy of our framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)åœ¨å¤„ç†å¤æ‚å¤šæŒ‡ä»¤ä»»åŠ¡æ—¶åˆè§„æ€§ä¸è¶³çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æŠ€æœ¯å¾€å¾€å¿½ç•¥äº†æç¤ºè¯(prompt)ä¸­çš„æ½œåœ¨ä¿¡å·ä»¥åŠæ ·æœ¬é—´(inter-sample level)çš„åå¥½å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†å¤šå±‚æ¬¡æ„ŸçŸ¥åå¥½å­¦ä¹ (Multi-level Aware Preference Learning, MAPL)æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºä¸åŒæ¡ä»¶ä¸‹çš„å˜ä½“æç¤ºè¯æ¥å­¦ä¹ æ ·æœ¬å†…(intra-sample level)çš„åå¥½å·®å¼‚ï¼Œå¹¶åˆ©ç”¨åˆæˆçš„å¤šæŒ‡ä»¤åå¥½å¯¹æ•æ‰æ ·æœ¬é—´çš„å·®å¼‚æ€§ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†ä¸¤ç§ä¸“é—¨çš„è®­ç»ƒç›®æ ‡å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å¥–åŠ±å»ºæ¨¡(Reward Modeling)å’Œç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)èŒƒå¼ä¸­ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†MAPLçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚å¤šæŒ‡ä»¤åœºæ™¯ä¸‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12845v1",
      "published_date": "2025-05-19 08:33:11 UTC",
      "updated_date": "2025-05-19 08:33:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:57:39.561090+00:00"
    },
    {
      "arxiv_id": "2505.12844v2",
      "title": "AGI-Elo: How Far Are We From Mastering A Task?",
      "title_zh": "AGI-Eloï¼šæˆ‘ä»¬è·ç¦»å®Œå…¨æŒæ¡ä¸€é¡¹ä»»åŠ¡è¿˜æœ‰å¤šè¿œï¼Ÿ",
      "authors": [
        "Shuo Sun",
        "Yimin Zhao",
        "Christina Dao Wen Lee",
        "Jiawei Sun",
        "Chengran Yuan",
        "Zefan Huang",
        "Dongen Li",
        "Justin KW Yeoh",
        "Alok Prakash",
        "Thomas W. Malone",
        "Marcelo H. Ang"
      ],
      "abstract": "As the field progresses toward Artificial General Intelligence (AGI), there is a pressing need for more comprehensive and insightful evaluation frameworks that go beyond aggregate performance metrics. This paper introduces a unified rating system that jointly models the difficulty of individual test cases and the competency of AI models (or humans) across vision, language, and action domains. Unlike existing metrics that focus solely on models, our approach allows for fine-grained, difficulty-aware evaluations through competitive interactions between models and tasks, capturing both the long-tail distribution of real-world challenges and the competency gap between current models and full task mastery. We validate the generalizability and robustness of our system through extensive experiments on multiple established datasets and models across distinct AGI domains. The resulting rating distributions offer novel perspectives and interpretable insights into task difficulty, model progression, and the outstanding challenges that remain on the path to achieving full AGI task mastery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AGI-Eloï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„è¯„åˆ†ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡è”åˆå»ºæ¨¡æµ‹è¯•ç”¨ä¾‹çš„éš¾åº¦ä¸ AI æ¨¡å‹åœ¨è§†è§‰ (vision)ã€è¯­è¨€ (language) åŠåŠ¨ä½œ (action) é¢†åŸŸçš„èƒœä»»åŠ›ï¼Œæä¾›æ¯”ä¼ ç»ŸèšåˆæŒ‡æ ‡æ›´å…·æ´å¯ŸåŠ›çš„è¯„ä¼°æ¡†æ¶ã€‚ä¸åŒäºä»…ä¾§é‡äºæ¨¡å‹è¡¨ç°çš„ç°æœ‰åº¦é‡æ ‡å‡†ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ¨¡å‹ä¸ä»»åŠ¡ä¹‹é—´çš„ç«äº‰æ€§äº¤äº’å®ç°ç»†ç²’åº¦çš„éš¾åº¦æ„ŸçŸ¥è¯„ä¼°ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰ç°å®æŒ‘æˆ˜çš„é•¿å°¾åˆ†å¸ƒ (long-tail distribution) ä»¥åŠå½“å‰æ¨¡å‹ä¸å®Œå…¨ä»»åŠ¡ç²¾é€šä¹‹é—´çš„èƒ½åŠ›å·®è·ã€‚ç ”ç©¶åœ¨å¤šä¸ªé€šç”¨äººå·¥æ™ºèƒ½ (AGI) é¢†åŸŸçš„ä»£è¡¨æ€§æ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†è¯¥ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§ã€‚æœ€ç»ˆç”Ÿæˆçš„è¯„åˆ†åˆ†å¸ƒä¸ºä»»åŠ¡éš¾åº¦åˆ†æå’Œæ¨¡å‹æ¼”åŒ–è·¯å¾„æä¾›äº†å…¨æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºé€šå¾€é€šç”¨äººå·¥æ™ºèƒ½ (AGI) é“è·¯ä¸Šå­˜åœ¨çš„æŒ‘æˆ˜æä¾›äº†å¯è§£é‡Šçš„è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12844v2",
      "published_date": "2025-05-19 08:30:13 UTC",
      "updated_date": "2025-05-24 05:25:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:57:20.840062+00:00"
    },
    {
      "arxiv_id": "2505.12843v1",
      "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF",
      "title_zh": "é€šè¿‡åå·®æ‹Ÿåˆç¼“è§£ RLHF å¥–åŠ±æ¨¡å‹çš„é•¿åº¦åå·®",
      "authors": [
        "Kangwen Zhao",
        "Jianfeng Cai",
        "Jinhua Zhu",
        "Ruopei Sun",
        "Dongyun Xue",
        "Wengang Zhou",
        "Li Li",
        "Houqiang Li"
      ],
      "abstract": "Reinforcement Learning from Human Feedback relies on reward models to align large language models with human preferences. However, RLHF often suffers from reward hacking, wherein policy learning exploits flaws in the trained reward model to maximize reward scores without genuinely aligning with human preferences. A significant example of such reward hacking is length bias, where reward models usually favor longer responses irrespective of actual response quality. Previous works on length bias have notable limitations, these approaches either mitigate bias without characterizing the bias form, or simply assume a linear length-reward relation. To accurately model the intricate nature of length bias and facilitate more effective bias mitigation, we propose FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a framework that autonomously learns and corrects underlying bias patterns. Our approach consists of three stages: First, we train a standard reward model which inherently contains length bias. Next, we deploy a lightweight fitting model to explicitly capture the non-linear relation between length and reward. Finally, we incorporate this learned relation into the reward model to debias. Experimental results demonstrate that FiMi-RM achieves a more balanced length-reward distribution. Furthermore, when applied to alignment algorithms, our debiased reward model improves length-controlled win rate and reduces verbosity without compromising its performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)ä¸­å¥–åŠ±æ¨¡å‹(Reward Model)æ™®éå­˜åœ¨çš„é•¿åº¦åå·®(Length Bias)é—®é¢˜ï¼Œæå‡ºäº†FiMi-RMæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åå·®æ‹Ÿåˆæ¥å‡è½»è¿™ä¸€ç°è±¡ã€‚ä¼ ç»Ÿçš„ç¼“è§£æ–¹æ³•å¾€å¾€å¿½ç•¥åå·®çš„å…·ä½“å½¢å¼æˆ–ä»…å‡è®¾çº¿æ€§çš„é•¿åº¦ä¸å¥–åŠ±å…³ç³»ï¼Œè€ŒFiMi-RMåˆ™é€šè¿‡è‡ªä¸»å­¦ä¹ æ¥æ•è·å¤æ‚çš„éçº¿æ€§åå·®æ¨¡å¼ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆè®­ç»ƒä¸€ä¸ªå«æœ‰åŸç”Ÿåå·®çš„æ ‡å‡†å¥–åŠ±æ¨¡å‹ï¼Œéšåéƒ¨ç½²è½»é‡åŒ–æ‹Ÿåˆæ¨¡å‹æ˜¾å¼æ•è·é•¿åº¦ä¸å¥–åŠ±é—´çš„éçº¿æ€§å…³ç³»ï¼Œæœ€åå°†è¯¥å…³ç³»æ•´åˆå›å¥–åŠ±æ¨¡å‹è¿›è¡Œå»åå¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFiMi-RMå®ç°äº†æ›´å‡è¡¡çš„é•¿åº¦-å¥–åŠ±åˆ†å¸ƒï¼Œå¹¶åœ¨åº”ç”¨è‡³å¯¹é½ç®—æ³•æ—¶æ˜¾è‘—æé«˜äº†é•¿åº¦å—æ§ä¸‹çš„èƒœç‡ã€‚åœ¨ä¸æŸå¤±æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†ç”Ÿæˆå›å¤çš„å†—ä½™æ€§ï¼Œä¸ºå®ç°æ›´çœŸå®çš„äººç±»åå¥½å¯¹é½æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Due to the word limit for arXiv abstract, the abstract here has been abridged compared to the one in the PDF",
      "pdf_url": "https://arxiv.org/pdf/2505.12843v1",
      "published_date": "2025-05-19 08:29:28 UTC",
      "updated_date": "2025-05-19 08:29:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:57:42.727419+00:00"
    },
    {
      "arxiv_id": "2505.12837v1",
      "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting",
      "title_zh": "éšè—çš„ç»“æ„ï¼šé€šè¿‡æ˜¾å¼æ–‡æœ¬æ ¼å¼åŒ–æå‡æ³•å¾‹æ–‡æ¡£ç†è§£èƒ½åŠ›",
      "authors": [
        "Christian Braun",
        "Alexander Lilienbeck",
        "Daniel Mentjukov"
      ],
      "abstract": "Legal contracts possess an inherent, semantically vital structure (e.g., sections, clauses) that is crucial for human comprehension but whose impact on LLM processing remains under-explored. This paper investigates the effects of explicit input text structure and prompt engineering on the performance of GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the CUAD. We compare model exact-match accuracy across various input formats: well-structured plain-text (human-generated from CUAD), plain-text cleaned of line breaks, extracted plain-text from Azure OCR, plain-text extracted by GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o Vision. To give an indication of the impact of possible prompt engineering, we assess the impact of shifting task instructions to the system prompt and explicitly informing the model about the structured nature of the input. Our findings reveal that GPT-4o demonstrates considerable robustness to variations in input structure, but lacks in overall performance. Conversely, GPT-4.1's performance is markedly sensitive; poorly structured inputs yield suboptimal results (but identical with GPT-4o), while well-structured formats (original CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by ~20 percentage points. Optimizing the system prompt to include task details and an advisory about structured input further elevates GPT-4.1's accuracy by an additional ~10-13 percentage points, with Markdown ultimately achieving the highest performance under these conditions (79 percentage points overall exact-match accuracy). This research empirically demonstrates that while newer models exhibit greater resilience, careful input structuring and strategic prompt design remain critical for optimizing the performance of LLMs, and can significantly affect outcomes in high-stakes legal applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ˜¾å¼æ–‡æœ¬æ ¼å¼åŒ–å’Œ Prompt Engineering å¯¹ LLMs åœ¨æ³•å¾‹æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ³•å¾‹é—®ç­”ä»»åŠ¡ï¼Œå¯¹æ¯”äº† GPT-4o å’Œ GPT-4.1 åœ¨å¤„ç† Plain-textã€OCR æå–æ–‡æœ¬ä»¥åŠç”± GPT-4o Vision æå–çš„ Markdown ç­‰ä¸åŒè¾“å…¥æ ¼å¼æ—¶çš„è¡¨ç°ã€‚å®éªŒå‘ç° GPT-4o å¯¹è¾“å…¥ç»“æ„çš„å˜åŒ–å…·æœ‰è¾ƒå¼ºé²æ£’æ€§ï¼Œä½†æ•´ä½“æ€§èƒ½æ¬ ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4.1 å¯¹æ–‡æœ¬ç»“æ„è¡¨ç°å‡ºæ˜¾è‘—çš„æ•æ„Ÿæ€§ï¼Œä½¿ç”¨æ ¼å¼è‰¯å¥½çš„è¾“å…¥ï¼ˆå¦‚åŸå§‹ CUAD æ–‡æœ¬æˆ– Markdownï¼‰å¯å°† Exact-match å‡†ç¡®ç‡æå‡çº¦ 20 ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡åœ¨ System Prompt ä¸­åŠ å…¥ä»»åŠ¡ç»†èŠ‚å¹¶æ˜ç¡®æç¤ºè¾“å…¥æ–‡æœ¬çš„ç»“æ„åŒ–ç‰¹å¾ï¼Œå¯ä½¿ GPT-4.1 çš„å‡†ç¡®ç‡è¿›ä¸€æ­¥æå‡ 10-13 ä¸ªç™¾åˆ†ç‚¹ï¼Œæœ€ç»ˆ Markdown æ ¼å¼åœ¨è¿™äº›æ¡ä»¶ä¸‹è¾¾åˆ°äº† 79% çš„æœ€é«˜å‡†ç¡®ç‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ–°ä¸€ä»£æ¨¡å‹éŸ§æ€§æœ‰æ‰€å¢å¼ºï¼Œä½†ç²¾ç»†çš„è¾“å…¥ç»“æ„åŒ–å’Œç­–ç•¥æ€§çš„æç¤ºè¯è®¾è®¡å¯¹äºä¼˜åŒ– LLMs åœ¨é«˜é£é™©æ³•å¾‹åº”ç”¨ä¸­çš„è¡¨ç°ä¾ç„¶è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12837v1",
      "published_date": "2025-05-19 08:25:21 UTC",
      "updated_date": "2025-05-19 08:25:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:57:51.124456+00:00"
    },
    {
      "arxiv_id": "2505.12833v2",
      "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs",
      "title_zh": "Reasoning BOï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›å¢å¼ºè´å¶æ–¯ä¼˜åŒ–",
      "authors": [
        "Zhuo Yang",
        "Daolang Wang",
        "Lingli Ge",
        "Beilun Wang",
        "Tianfan Fu",
        "Yuqiang Li"
      ],
      "abstract": "Many real-world scientific and industrial applications require the optimization of expensive black-box functions. Bayesian Optimization (BO) provides an effective framework for such problems. However, traditional BO methods are prone to get trapped in local optima and often lack interpretable insights. To address this issue, this paper designs Reasoning BO, a novel framework that leverages reasoning models to guide the sampling process in BO while incorporating multi-agent systems and knowledge graphs for online knowledge accumulation. By integrating the reasoning and contextual understanding capabilities of Large Language Models (LLMs), we can provide strong guidance to enhance the BO process. As the optimization progresses, Reasoning BO provides real-time sampling recommendations along with critical insights grounded in plausible scientific theories, aiding in the discovery of superior solutions within the search space. We systematically evaluate our approach across 10 diverse tasks encompassing synthetic mathematical functions and complex real-world applications. The framework demonstrates its capability to progressively refine sampling strategies through real-time insights and hypothesis evolution, effectively identifying higher-performing regions of the search space for focused exploration. This process highlights the powerful reasoning and context-learning abilities of LLMs in optimization scenarios. For example, in the Direct Arylation task, our method increased the yield to 60.7%, whereas traditional BO achieved only a 25.2% yield. Furthermore, our investigation reveals that smaller LLMs, when fine-tuned through reinforcement learning, can attain comparable performance to their larger counterparts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Reasoning BOï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é•¿æ–‡æœ¬æ¨ç†èƒ½åŠ›ä¸è´å¶æ–¯ä¼˜åŒ–(Bayesian Optimization)ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸBOæ–¹æ³•æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ä¸”ç¼ºä¹å¯è§£é‡Šæ€§è§è§£çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMsçš„æ¨ç†å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œå¹¶æ•´åˆäº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent systems)å’ŒçŸ¥è¯†å›¾è°±(knowledge graphs)ä»¥å®ç°åœ¨çº¿çŸ¥è¯†ç§¯ç´¯ã€‚Reasoning BOèƒ½åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æä¾›å®æ—¶é‡‡æ ·å»ºè®®åŠåŸºäºç§‘å­¦ç†è®ºçš„å…³é”®è§è§£ï¼Œä»è€Œåœ¨å¤æ‚çš„æœç´¢ç©ºé—´ä¸­é«˜æ•ˆå‘ç°ä¼˜é€‰æ–¹æ¡ˆã€‚å¯¹10é¡¹å¤šæ ·åŒ–ä»»åŠ¡çš„ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½é€šè¿‡å®æ—¶æ´å¯Ÿå’Œå‡è®¾æ¼”åŒ–ä¸æ–­æ”¹è¿›é‡‡æ ·ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›´æ¥èŠ³åŸºåŒ–(Direct Arylation)ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å°†äº§ç‡ä»ä¼ ç»Ÿæ–¹æ³•çš„25.2%æ˜¾è‘—æå‡è‡³60.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ç»å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„å°å‹LLMsèƒ½å¤Ÿè¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå……åˆ†è¯æ˜äº†LLMsåœ¨ä¼˜åŒ–åœºæ™¯ä¸­çš„æ¨ç†ä¸å­¦ä¹ æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12833v2",
      "published_date": "2025-05-19 08:20:40 UTC",
      "updated_date": "2025-09-26 03:12:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:57:51.755054+00:00"
    },
    {
      "arxiv_id": "2505.12822v2",
      "title": "Emergent Specialization: Rare Token Neurons in Language Models",
      "title_zh": "æ¶Œç°æ€§ç‰¹åŒ–ï¼šè¯­è¨€æ¨¡å‹ä¸­çš„ç¨€æœ‰è¯å…ƒç¥ç»å…ƒ",
      "authors": [
        "Jing Liu",
        "Haozheng Wang",
        "Yueheng Li"
      ],
      "abstract": "Large language models struggle with representing and generating rare tokens despite their importance in specialized domains. In this study, we identify neuron structures with exceptionally strong influence on language model's prediction of rare tokens, termed as rare token neurons, and investigate the mechanism for their emergence and behavior. These neurons exhibit a characteristic three-phase organization (plateau, power-law, and rapid decay) that emerges dynamically during training, evolving from a homogeneous initial state to a functionally differentiated architecture. In the activation space, rare token neurons form a coordinated subnetwork that selectively co-activates while avoiding co-activation with other neurons. This functional specialization potentially correlates with the development of heavy-tailed weight distributions, suggesting a statistical mechanical basis for emergent specialization.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯†åˆ«äº†è¯­è¨€æ¨¡å‹ä¸­å¯¹é¢„æµ‹ç½•è§æ ‡è®°ï¼ˆrare tokensï¼‰å…·æœ‰æå¼ºå½±å“åŠ›çš„ç‰¹å®šç¥ç»å…ƒç»“æ„ï¼Œå³ rare token neuronsï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å…¶æ¶Œç°ä¸è¡Œä¸ºæœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›ç¥ç»å…ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºä»åŒè´¨åŒ–åˆ°åŠŸèƒ½åˆ†åŒ–çš„åŠ¨æ€æ¼”åŒ–ï¼Œå¹¶å‘ˆç°å‡ºç”±é«˜åŸæœŸï¼ˆplateauï¼‰ã€å¹‚å¾‹æœŸï¼ˆpower-lawï¼‰å’Œå¿«é€Ÿè¡°å‡æœŸï¼ˆrapid decayï¼‰æ„æˆçš„ç‰¹å¾æ€§ä¸‰é˜¶æ®µç»„ç»‡ã€‚åœ¨æ¿€æ´»ç©ºé—´ä¸­ï¼Œrare token neurons å½¢æˆäº†ä¸€ä¸ªåè°ƒçš„å­ç½‘ç»œï¼Œé€šè¿‡é€‰æ‹©æ€§å…±æ¿€æ´»å¹¶é¿å…ä¸å…¶ä»–ç¥ç»å…ƒå…±æ¿€æ´»æ¥ç»´æŒå…¶åŠŸèƒ½ä¸“ä¸šåŒ–ï¼ˆfunctional specializationï¼‰ã€‚è¿™ç§ç°è±¡ä¸é‡å°¾æƒé‡åˆ†å¸ƒï¼ˆheavy-tailed weight distributionsï¼‰çš„å‘å±•é«˜åº¦ç›¸å…³ï¼Œæš—ç¤ºäº†è¿™ç§ä¸“ä¸šåŒ–çš„äº§ç”Ÿå…·æœ‰ç»Ÿè®¡åŠ›å­¦ï¼ˆstatistical mechanicalï¼‰åŸºç¡€ã€‚è¯¥æˆæœä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸“ä¸šé¢†åŸŸç¨€æœ‰æ•°æ®æ—¶çš„å†…éƒ¨è¿ä½œæœºåˆ¶æä¾›äº†é‡è¦çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12822v2",
      "published_date": "2025-05-19 08:05:13 UTC",
      "updated_date": "2025-05-22 16:03:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:57:56.601696+00:00"
    },
    {
      "arxiv_id": "2505.12821v1",
      "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models",
      "title_zh": "SynDecï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»»æ„æ–‡æœ¬é£æ ¼è¿ç§»â€œå…ˆåˆæˆåè§£ç â€æ–¹æ³•",
      "authors": [
        "Han Sun",
        "Zhen Sun",
        "Zongmin Zhang",
        "Linzhao Jia",
        "Wei Shao",
        "Min Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9\\% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»»æ„æ–‡æœ¬é£æ ¼è½¬æ¢(Arbitrary Textual Style Transfer)ä¸­é¢ä¸´çš„è¿‡åº¦ä¾èµ–äººå·¥æç¤ºè¯åŠå›ºæœ‰é£æ ¼åè§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºSynDecçš„â€œå…ˆåˆæˆåè§£ç â€æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ç­›é€‰ä»£è¡¨æ€§å°‘æ ·æœ¬ã€æ‰§è¡Œå››ç»´é£æ ¼åˆ†æå’Œå€™é€‰é‡æ’åºï¼Œå®ç°äº†é«˜è´¨é‡æç¤ºè¯çš„è‡ªåŠ¨åˆæˆã€‚åœ¨è§£ç é˜¶æ®µï¼ŒSynDecé€šè¿‡æœ€å¤§åŒ–æœ‰æ— åˆæˆæç¤ºè¯ä»¥åŠæç¤ºè¯ä¸è´Ÿæ ·æœ¬ä¹‹é—´çš„è¾“å‡ºæ¦‚ç‡å¯¹æ¯”ï¼Œæ˜¾è‘—å¢å¼ºäº†é£æ ¼è½¬æ¢çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynDecåœ¨å…­é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„äº”é¡¹å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨ç°ä»£è‹±è¯­åˆ°ä¼Šä¸½èç™½è‹±è¯­çš„è½¬æ¢ä¸­å‡†ç¡®ç‡æå‡äº†9%ã€‚è¯¦ç»†çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†SynDecåœ¨è‡ªåŠ¨åŒ–é£æ ¼æ§åˆ¶å’Œæå‡è½¬æ¢ç²¾åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12821v1",
      "published_date": "2025-05-19 08:03:38 UTC",
      "updated_date": "2025-05-19 08:03:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:14.031235+00:00"
    },
    {
      "arxiv_id": "2505.12815v2",
      "title": "Learning In Chaos: Efficient Autoscaling and Self-Healing for Multi-Party Distributed Training",
      "title_zh": "Learning In Chaosï¼šé¢å‘å¤šæ–¹åˆ†å¸ƒå¼è®­ç»ƒçš„é«˜æ•ˆè‡ªåŠ¨æ‰©ç¼©å®¹ä¸è‡ªæ„ˆ",
      "authors": [
        "Wenjiao Feng",
        "Rongxing Xiao",
        "Zonghang Li",
        "Hongfang Yu",
        "Gang Sun",
        "Long Luo",
        "Mohsen Guizani",
        "Qirong Ho",
        "Steve Liu"
      ],
      "abstract": "Node and link churn in multi-party, cross-region clusters over wide-area networks (WANs) often disrupts distributed training. However, checkpoint-based recovery and cloud-centric autoscaling react slowly and assume centralized control, which is misaligned with the self-governed setup where institutions can freely join and leave. This paper proposes Chaos, a multi-party distributed training system with self-healing and autoscaling, enabling robust and elastic training under churn. It speeds up autoscaling via multi-neighbor state replication and model sharding. We formalize the sharding and assignment as a MINLP that captures WAN heterogeneity, and reduce it to a tractable MILP by analyzing its monotonicity on a divisibility chain. By establishing an equivalence, we derive a greedy algorithm that follows optimality rules and yields the optimal solution in polynomial time. Chaos uses a cluster monitor to track resource and topology changes, and handles scaling events through peer negotiation protocols, enabling fully self-governed autoscaling among institutions. Experiments show that Chaos has substantially lower scale-out delay than Pollux, Elan, and Autoscaling, and handles scale-in, connect-link, and disconnect-link events within 20ms. It also delivers the lowest idle time, showing superior resource use and scalability as the cluster grows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨åŒºåŸŸå¹¿åŸŸç½‘(WAN)ä¸­å¤šæ–¹åˆ†å¸ƒå¼è®­ç»ƒé¢ä¸´çš„èŠ‚ç‚¹å’Œé“¾è·¯æŠ–åŠ¨(churn)é—®é¢˜ï¼Œæå‡ºäº†Chaosç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„è‡ªåŠ¨ä¼¸ç¼©(Autoscaling)å’Œè‡ªæ„ˆ(Self-healing)ã€‚Chaosé€šè¿‡å¤šé‚»å±…çŠ¶æ€å¤åˆ¶(multi-neighbor state replication)å’Œæ¨¡å‹åˆ†ç‰‡(model sharding)åŠ é€Ÿä¼¸ç¼©è¿‡ç¨‹ï¼Œä»¥é€‚åº”ç¼ºä¹é›†ä¸­æ§åˆ¶çš„è‡ªæ²»åä½œç¯å¢ƒã€‚ç ”ç©¶å›¢é˜Ÿå°†æ¨¡å‹åˆ†ç‰‡ä¸åˆ†é…å½¢å¼åŒ–ä¸ºæ•è·WANå¼‚æ„æ€§çš„æ··åˆæ•´æ•°éçº¿æ€§è§„åˆ’(MINLP)é—®é¢˜ï¼Œå¹¶é€šè¿‡åˆ†ææ•´é™¤é“¾ä¸Šçš„å•è°ƒæ€§å°†å…¶è½¬åŒ–ä¸ºå¯å¤„ç†çš„æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(MILP)ã€‚åŸºäºç­‰ä»·æ€§åŸç†ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥æ¨å¯¼å‡ºä¸€ä¸ªè´ªå¿ƒç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šé¡¹å¼æ—¶é—´å†…è·å¾—æœ€ä¼˜è§£ã€‚ç³»ç»Ÿåˆ©ç”¨é›†ç¾¤ç›‘æ§å™¨å’Œå¯¹ç­‰åå•†åè®®(peer negotiation protocols)è¿›è¡Œèµ„æºè¿½è¸ªå’Œä¼¸ç¼©ç®¡ç†ï¼Œç¡®ä¿äº†å‚ä¸æœºæ„é—´çš„å®Œå…¨è‡ªä¸»æ²»ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChaosçš„æ‰©å®¹å»¶è¿Ÿæ˜¾è‘—ä½äºPolluxå’ŒElanç­‰ç°æœ‰ç³»ç»Ÿï¼Œå¹¶èƒ½åœ¨20mså†…å¿«é€Ÿå“åº”ç¼©å®¹åŠé“¾è·¯æ–­è¿äº‹ä»¶ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨ç©ºé—²æ—¶é—´æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè¯æ˜äº†å…¶åœ¨é›†ç¾¤è§„æ¨¡å¢é•¿æ—¶å…·å¤‡å“è¶Šçš„èµ„æºåˆ©ç”¨ç‡å’Œå¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "14 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12815v2",
      "published_date": "2025-05-19 07:52:17 UTC",
      "updated_date": "2025-09-13 18:39:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:38.408406+00:00"
    },
    {
      "arxiv_id": "2505.12814v2",
      "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs",
      "title_zh": "PsyMemï¼šé¢å‘é«˜çº§è§’è‰²æ‰®æ¼”å¤§è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦å¿ƒç†å¯¹é½ä¸æ˜¾å¼è®°å¿†æ§åˆ¶",
      "authors": [
        "Xilong Cheng",
        "Yunxiao Qin",
        "Yuting Tan",
        "Zhengnan Li",
        "Ye Wang",
        "Hongjiang Xiao",
        "Yuan Zhang"
      ],
      "abstract": "Existing LLM-based role-playing methods often rely on superficial textual descriptions or simplistic metrics, inadequately modeling both intrinsic and extrinsic character dimensions. Additionally, they typically simulate character memory with implicit model knowledge or basic retrieval augment generation without explicit memory alignment, compromising memory consistency. The two issues weaken reliability of role-playing LLMs in several applications, such as trustworthy social simulation. To address these limitations, we propose PsyMem, a novel framework integrating fine-grained psychological attributes and explicit memory control for role-playing. PsyMem supplements textual descriptions with 26 psychological indicators to detailed model character. Additionally, PsyMem implements memory alignment training, explicitly trains the model to align character's response with memory, thereby enabling dynamic memory-controlled responding during inference. By training Qwen2.5-7B-Instruct on our specially designed dataset (including 5,414 characters and 38,962 dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen, outperforms baseline models in role-playing, achieving the best performance in human-likeness and character fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PsyMemæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§’è‰²æ‰®æ¼”å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§’è‰²å»ºæ¨¡æ·±åº¦åŠè®°å¿†ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚PsyMemé€šè¿‡å¼•å…¥26ä¸ªå¿ƒç†æŒ‡æ ‡å®ç°äº†ç»†ç²’åº¦çš„å¿ƒç†å¯¹é½(fine-grained psychological alignment)ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–è‚¤æµ…æ–‡æœ¬æè¿°çš„ç¼ºé™·ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†æ˜¾å¼è®°å¿†æ§åˆ¶(Explicit Memory Control)æŠ€æœ¯ï¼Œé€šè¿‡è®°å¿†å¯¹é½è®­ç»ƒä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è§’è‰²è®°å¿†ç”ŸæˆåŠ¨æ€å“åº”ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºä»å°è¯´ä¸­æå–çš„å¤§è§„æ¨¡æ•°æ®é›†å¯¹Qwen2.5-7B-Instructè¿›è¡Œè®­ç»ƒï¼Œæ„å»ºäº†PsyMem-Qwenæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç±»äººæ€§(human-likeness)å’Œè§’è‰²å¿ å®åº¦(character fidelity)æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºé«˜å¯é æ€§çš„ç¤¾äº¤æ¨¡æ‹Ÿå’Œå…ˆè¿›è§’è‰²æ‰®æ¼”ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Pre-MIT Press publication version, has been accepted by TACL",
      "pdf_url": "https://arxiv.org/pdf/2505.12814v2",
      "published_date": "2025-05-19 07:45:09 UTC",
      "updated_date": "2025-10-20 14:52:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:25.081875+00:00"
    },
    {
      "arxiv_id": "2505.12811v1",
      "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning",
      "title_zh": "å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨æ€è§†é‡èŒƒå›´é€‰æ‹©",
      "authors": [
        "Wei-Chen Liao",
        "Ti-Rong Wu",
        "I-Chen Wu"
      ],
      "abstract": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight range dilemma, where agents either receive insufficient or excessive information from their environment. In this paper, we propose a novel method, called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight range during training. Experiment results show several advantages of using DSR. First, we demonstrate using DSR achieves better performance in three common MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse (RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show that DSR consistently improves performance across multiple MARL algorithms, including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different training steps, thereby accelerating the training process. Finally, DSR provides additional interpretability by indicating the optimal sight range used during training. Unlike existing methods that rely on global information or communication mechanisms, our approach operates solely based on the individual sight ranges of agents. This approach offers a practical and efficient solution to the sight range dilemma, making it broadly applicable to real-world complex environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-agent reinforcement Learning, MARL) ä¸­æ™ºèƒ½ä½“å› è§†é‡èŒƒå›´ (sight range) ä¸å½“å¯¼è‡´ä¿¡æ¯è·å–å¤±è¡¡çš„å›°å¢ƒï¼Œæå‡ºäº†åä¸ºåŠ¨æ€è§†é‡èŒƒå›´é€‰æ‹© (Dynamic Sight Range Selection, DSR) çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Upper Confidence Bound (UCB) ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ™ºèƒ½ä½“çš„è§†é‡èŒƒå›´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSR åœ¨ Level-Based Foraging (LBF)ã€Multi-Robot Warehouse (RWARE) å’Œ StarCraft Multi-Agent Challenge (SMAC) ç­‰å¤šç§ä¸»æµç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†ä»»åŠ¡è¡¨ç°ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨ QMIX å’Œ MAPPO ç­‰ä¸åŒ MARL ç®—æ³•ä¸Šå…·æœ‰è‰¯å¥½çš„æ™®é€‚æ€§ä¸å¢å¼ºæ•ˆæœã€‚DSR ä¸ä»…èƒ½é€šè¿‡ä¼˜åŒ–å„é˜¶æ®µè§†é‡èŒƒå›´æ¥åŠ é€Ÿè®­ç»ƒï¼Œè¿˜é€šè¿‡æ­ç¤ºè®­ç»ƒä¸­çš„æœ€ä¼˜è§†é‡èŒƒå›´å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ä¸ä¾èµ–å…¨å±€ä¿¡æ¯æˆ–é€šä¿¡çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒDSR ä»…åŸºäºä¸ªä½“è§†é‡è¿è¡Œï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œå¤æ‚ç¯å¢ƒä¸­çš„è§†é‡éš¾é¢˜æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix",
      "pdf_url": "https://arxiv.org/pdf/2505.12811v1",
      "published_date": "2025-05-19 07:40:42 UTC",
      "updated_date": "2025-05-19 07:40:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:11.805844+00:00"
    },
    {
      "arxiv_id": "2505.12805v2",
      "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA",
      "title_zh": "FedSVDï¼šé¢å‘ LoRA éšç§è”é‚¦å­¦ä¹ çš„è‡ªé€‚åº”æ­£äº¤åŒ–",
      "authors": [
        "Seanie Lee",
        "Sangwoo Park",
        "Dong Bok Lee",
        "Dominik Wagner",
        "Haebin Seong",
        "Tobias Bocklet",
        "Juho Lee",
        "Sung Ju Hwang"
      ],
      "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update ($BA$) intensifies this effect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose $\\texttt{FedSVD}$, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the $B$ matrix and transmits it to the server. The server aggregates the $B$ matrices, computes the product $BA$ using the previous $A$, and refactorizes the result via SVD. This yields a new adaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an updated $B$ containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing $A$ to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of $A$ bounds the gradient norms of $B$ and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, $\\texttt{FedSVD}$ consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½ç§©é€‚é…(LoRA)åœ¨å·®åˆ†éšç§è”é‚¦å­¦ä¹ (Federated Learning)ä¸­å› DP-SGDå¯¼è‡´çš„å™ªå£°æ”¾å¤§é—®é¢˜ï¼Œæå‡ºäº†FedSVDæ¡†æ¶ã€‚ä¼ ç»Ÿçš„LoRAåœ¨ç»“åˆå·®åˆ†éšç§æ—¶ï¼ŒçŸ©é˜µä¹˜æ³•BAä¼šæ˜¾è‘—æ”¾å¤§æ¢¯åº¦æ‰°åŠ¨ï¼Œè€Œå›ºå®šå…¶ä¸­ä¸€ä¸ªçŸ©é˜µåˆä¼šé™åˆ¶æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚FedSVDå¼•å…¥äº†åŸºäºå¥‡å¼‚å€¼åˆ†è§£(SVD)çš„å…¨å±€é‡å‚æ•°åŒ–æ–¹æ³•ï¼Œè¦æ±‚å®¢æˆ·ç«¯ä»…ä¼˜åŒ–BçŸ©é˜µå¹¶å°†å…¶ä¼ è¾“è‡³æœåŠ¡å™¨ã€‚æœåŠ¡å™¨é€šè¿‡å¯¹èšåˆç»“æœè¿›è¡ŒSVDåˆ†è§£ï¼Œåˆ©ç”¨å³å¥‡å¼‚å‘é‡æ›´æ–°æ­£äº¤çš„AçŸ©é˜µï¼Œä»è€Œæ•æ‰èšåˆæ›´æ–°çš„ä¸»æ–¹å‘ã€‚è¿™ç§æ­£äº¤ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆé™åˆ¶BçŸ©é˜µçš„æ¢¯åº¦èŒƒæ•°ï¼Œåœ¨DP-SGDæœºåˆ¶ä¸‹ä¿ç•™æ›´å¤šæœ‰æ•ˆä¿¡å·å¹¶é¿å…äºŒæ¬¡å™ªå£°æ”¾å¤§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedSVDåœ¨å¤šç§éšç§è®¾ç½®å’ŒåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œåœ¨ç§å¯†å’Œéç§å¯†ç¯å¢ƒä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12805v2",
      "published_date": "2025-05-19 07:32:56 UTC",
      "updated_date": "2025-10-25 06:07:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:51.978513+00:00"
    },
    {
      "arxiv_id": "2505.12800v1",
      "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching",
      "title_zh": "OZSpeechï¼šåŸºäºå­¦ä¹ å…ˆéªŒæ¡ä»¶æµåŒ¹é…çš„ä¸€æ­¥å¼é›¶æ ·æœ¬è¯­éŸ³åˆæˆ",
      "authors": [
        "Hieu-Nghia Huynh-Nguyen",
        "Ngoc Son Nguyen",
        "Huynh Nguyen Dang",
        "Thieu Vo",
        "Truong-Son Hy",
        "Van Nguyen"
      ],
      "abstract": "Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and neural network architectures. Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework. However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training. To address these challenges, we introduce OZSpeech, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps. Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS system's ability to precisely clone the prompt speech. Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation. Audio samples are available at our demo page https://ozspeech.github.io/OZSpeech_Web/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„æ–‡æœ¬è½¬è¯­éŸ³(TTS)ç³»ç»Ÿåœ¨Flow Matchingæ¡†æ¶ä¸‹å­˜åœ¨çš„å±æ€§å»ºæ¨¡ä¸è¶³å’Œé«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†OZSpeechã€‚è¿™æ˜¯é¦–ä¸ªå°†æœ€ä¼˜ä¼ è¾“æ¡ä»¶æµåŒ¹é…(optimal transport conditional flow matching)ä¸ä¸€æ­¥é‡‡æ ·(one-step sampling)åŠå­¦ä¹ å…ˆéªŒ(learned prior)ç›¸ç»“åˆçš„é›¶æ ·æœ¬è¯­éŸ³åˆæˆæ–¹æ³•ï¼Œé€šè¿‡å¿½ç•¥å‰ç»­çŠ¶æ€æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ­¥æ•°ã€‚è¯¥æ–¹æ³•åœ¨Tokenæ ¼å¼ä¸‹å¯¹è¯­éŸ³çš„è§£è€¦å’Œåˆ†è§£ç»„ä»¶è¿›è¡Œæ“ä½œï¼Œç¡®ä¿äº†å¯¹å„é¡¹è¯­éŸ³å±æ€§çš„ç²¾ç¡®å»ºæ¨¡ï¼Œä»è€Œå¢å¼ºäº†ç³»ç»Ÿå¯¹æç¤ºè¯­éŸ³(prompt speech)çš„ç²¾ç¡®å…‹éš†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOZSpeechåœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€éŸµå¾‹ç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå®ç°äº†å…¼å…·æ•ˆç‡ä¸é«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12800v1",
      "published_date": "2025-05-19 07:31:55 UTC",
      "updated_date": "2025-05-19 07:31:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:52.651651+00:00"
    },
    {
      "arxiv_id": "2505.12795v4",
      "title": "FRABench and UFEval: Unified Fine-grained Evaluation with Task and Aspect Generalization",
      "title_zh": "FRABench ä¸ UFEvalï¼šå…·æœ‰ä»»åŠ¡ä¸ç»´åº¦æ³›åŒ–èƒ½åŠ›çš„ç»Ÿä¸€ç»†ç²’åº¦è¯„ä¼°",
      "authors": [
        "Shibo Hong",
        "Jiahao Ying",
        "Haiyuan Liang",
        "Mengdi Zhang",
        "Jun Kuang",
        "Jiazheng Zhang",
        "Yixin Cao"
      ],
      "abstract": "Evaluating open-ended outputs of Multimodal Large Language Models has become a bottleneck as model capabilities, task diversity, and modality rapidly expand. Existing ``MLLM-as-a-Judge'' evaluators, though promising, remain constrained to specific tasks and aspects. In this paper, we argue that, on one hand, based on the interconnected nature of aspects, learning specific aspects can generalize to unseen aspects; on the other hand, jointly learning to assess multiple visual aspects and tasks may foster a synergistic effect. To this end, we propose UFEval, the first unified fine-grained evaluator with task and aspect generalization for four evaluation tasks -- Natural Language Generation, Image Understanding, Image Generation, and Interleaved Text-and-Image Generation. However, training such a unified evaluator is hindered by the lack of a large-scale, multi-modal, and aspect-level resource. To address this gap, we introduce FRABench, a comprehensive fine-grained evaluation dataset. Specifically, (1) We first construct a hierarchical aspect taxonomy encompassing 112 distinct aspects across the aforementioned four tasks. (2) Based on this taxonomy, we create FRABench, comprising 60.4k pairwise samples with 325k evaluation labels obtained from a combination of human and GPT-4o annotations. (3) Finally, leveraging FRABench, we develop UFEval, a unified fine-grained evaluator. Experiments show that learning on specific aspects enables UFEval to generalize to unseen aspects, and joint learning to assess diverse visual tasks and aspects can lead to substantial mutual benefits.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å¼€æ”¾å¼è¾“å‡ºè¯„ä¼°å—é™äºç‰¹å®šä»»åŠ¡å’Œç»´åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå…·æœ‰ä»»åŠ¡ä¸ç»´åº¦æ³›åŒ–èƒ½åŠ›çš„ç»Ÿä¸€ç»†ç²’åº¦è¯„ä¼°å™¨UFEvalã€‚ä¸ºäº†æ”¯æ’‘è¯¥è¯„ä¼°å™¨çš„è®­ç»ƒï¼Œä½œè€…é¦–å…ˆæ„å»ºäº†å¤§è§„æ¨¡ç»†ç²’åº¦è¯„ä¼°æ•°æ®é›†FRABenchï¼Œæ¶µç›–äº†è‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG)ã€å›¾åƒç†è§£(Image Understanding)ã€å›¾åƒç”Ÿæˆ(Image Generation)åŠå›¾æ–‡äº¤ç»‡ç”Ÿæˆ(Interleaved Text-and-Image Generation)å››å¤§ä»»åŠ¡ä¸‹çš„112ä¸ªå±‚æ¬¡åŒ–è¯„ä»·ç»´åº¦ã€‚FRABenchåŒ…å«60.4kä¸ªæˆå¯¹æ ·æœ¬åŠ325kä¸ªç»“åˆäººå·¥ä¸GPT-4oæ ‡æ³¨çš„è¯„ä¼°æ ‡ç­¾ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡å¤šæ¨¡æ€ç»´åº¦çº§èµ„æºçš„ç©ºç™½ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºFRABenchè®­ç»ƒçš„UFEvalä¸ä»…èƒ½æœ‰æ•ˆæ³›åŒ–è‡³æœªè§è¿‡çš„è¯„ä¼°ç»´åº¦ï¼Œä¸”é€šè¿‡å¤šä»»åŠ¡ä¸å¤šç»´åº¦çš„è”åˆå­¦ä¹ äº§ç”Ÿäº†æ˜¾è‘—çš„ååŒå¢æ•ˆä½œç”¨ã€‚è¯¥å·¥ä½œè¯æ˜äº†å…±åŒå­¦ä¹ å¤šç§è§†è§‰ç»´åº¦å¯ä»¥æå‡è¯„ä¼°å™¨çš„æ•´ä½“æ€§èƒ½ï¼Œä¸ºå®ç°å…¨é¢ä¸”ç²¾å‡†çš„å¤šæ¨¡æ€æ¨¡å‹è‡ªåŠ¨è¯„ä»·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12795v4",
      "published_date": "2025-05-19 07:29:26 UTC",
      "updated_date": "2025-09-29 16:11:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:58:59.602832+00:00"
    },
    {
      "arxiv_id": "2505.12788v1",
      "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs",
      "title_zh": "åŸºäºæ··åˆç­–ç•¥çš„Nå…ƒç»„æ—¶åºçŸ¥è¯†å›¾è°±å¤šè·³æ¨ç†",
      "authors": [
        "Zhongni Hou",
        "Miao Su",
        "Xiaolong Jin",
        "Zixuan Li",
        "Long Bai",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of (subject, predicate, object, timestamp) to describe temporal facts, have attracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional TKGs by utilizing n-tuples to incorporate auxiliary elements alongside core elements (i.e., subject, predicate, and object) of facts, so as to represent them in a more fine-grained manner. Reasoning over N-TKGs aims to predict potential future facts based on historical ones. However, existing N-TKG reasoning methods often lack explainability due to their black-box nature. Therefore, we introduce a new Reinforcement Learning-based method, named MT-Path, which leverages the temporal information to traverse historical n-tuples and construct a temporal reasoning path. Specifically, in order to integrate the information encapsulated within n-tuples, i.e., the entity-irrelevant information within the predicate, the information about core elements, and the complete information about the entire n-tuples, MT-Path utilizes a mixture policy-driven action selector, which bases on three low-level policies, namely, the predicate-focused policy, the core-element-focused policy and the whole-fact-focused policy. Further, MT-Path utilizes an auxiliary element-aware GCN to capture the rich semantic dependencies among facts, thereby enabling the agent to gain a deep understanding of each n-tuple. Experimental results demonstrate the effectiveness and the explainability of MT-Path.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ N-tuple Temporal Knowledge Graphs (N-TKGs) æ¨ç†æ–¹æ³•å› é»‘ç›’ç‰¹æ€§å¯¼è‡´çš„å¯è§£é‡Šæ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MT-Path çš„åŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„å¤šè·³æ¨ç†æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ—¶é—´ä¿¡æ¯éå†å†å² n-tuples ä»¥æ„å»ºæ—¶é—´æ¨ç†è·¯å¾„ï¼Œå®ç°å¯¹æœªæ¥äº‹å®çš„é¢„æµ‹ã€‚MT-Path çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªæ··åˆç­–ç•¥é©±åŠ¨ (mixture policy-driven) çš„åŠ¨ä½œé€‰æ‹©å™¨ï¼Œå®ƒæ•´åˆäº†å…³æ³¨è°“è¯ (predicate-focused)ã€æ ¸å¿ƒå…ƒç´  (core-element-focused) åŠå®Œæ•´äº‹å® (whole-fact-focused) çš„ä¸‰ç§åº•å±‚ç­–ç•¥ï¼Œä»è€Œå……åˆ†æå– n-tuples ä¸­çš„ç»†ç²’åº¦ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨è¾…åŠ©å…ƒç´ æ„ŸçŸ¥å›¾å·ç§¯ç½‘ç»œ (auxiliary element-aware GCN) æ•æ‰äº‹å®é—´å¤æ‚çš„è¯­ä¹‰ä¾èµ–ï¼Œæå‡æ™ºèƒ½ä½“å¯¹è¯­ä¹‰ç¯å¢ƒçš„ç†è§£æ·±åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMT-Path åœ¨ä¿è¯æ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œé€šè¿‡æ˜¾å¼çš„æ¨ç†è·¯å¾„æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12788v1",
      "published_date": "2025-05-19 07:20:33 UTC",
      "updated_date": "2025-05-19 07:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:03.389944+00:00"
    },
    {
      "arxiv_id": "2505.12781v4",
      "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone",
      "title_zh": "ä¸€ä¸ª Token èƒœè¿‡ä¸€åƒä¸ª Tokenï¼šåŸºäº Low-Rank Clone çš„é«˜æ•ˆçŸ¥è¯†è’¸é¦",
      "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Hao Liu",
        "Xinyan Xiao",
        "Zhaochun Ren",
        "Jun Yu"
      ],
      "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®­ç»ƒé«˜æ€§èƒ½å°è¯­è¨€æ¨¡å‹(Small Language Models, SLMs)æˆæœ¬æ˜‚è´µï¼Œä»¥åŠç°æœ‰æ–¹æ³•åœ¨ç¡¬å‰ªæ(Hard Pruning)ä¿¡æ¯æŸå¤±ã€è¡¨ç¤ºå¯¹é½ä½æ•ˆå’Œ Feed-Forward Networks (FFNs) æ¿€æ´»åˆ©ç”¨ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† Low-Rank Clone (LRC) é¢„è®­ç»ƒæ–¹æ³•ã€‚LRC é€šè¿‡å¼•å…¥ä½ç§©æŠ•å½±çŸ©é˜µ(Low-Rank Projection Matrices)æ„å»ºå­¦ç”Ÿæ¨¡å‹ï¼ŒåŒæ—¶å®ç°äº†å‹ç¼©æ•™å¸ˆæƒé‡çš„è½¯å‰ªæ(Soft Pruning)å’Œå¯¹é½æ•™å¸ˆ FFN ä¿¡å·çš„æ¿€æ´»å…‹éš†(Activation Clone)ã€‚è¿™ç§ç»Ÿä¸€çš„è®¾è®¡æ— éœ€æ˜¾å¼å¯¹é½æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†çŸ¥è¯†è¿ç§»çš„æ•ˆç‡ã€‚åœ¨ Llama-3.2 å’Œ Qwen2.5 ç­‰æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLRC ä»…ä½¿ç”¨ 20B tokens å³å¯è¾¾åˆ°æˆ–è¶…è¿‡ç»è¿‡æ•°ä¸‡äº¿ tokens è®­ç»ƒçš„å…ˆè¿›æ¨¡å‹æ°´å¹³ã€‚è¯¥æ–¹æ³•å®ç°äº†è¶…è¿‡ 1000 å€çš„è®­ç»ƒæ•ˆç‡ï¼Œä¸ºé«˜æ•ˆå¼€å‘å…·å¤‡æ•™å¸ˆæ¨¡å‹è¡Œä¸ºç­‰æ•ˆæ€§çš„ SLMs æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2505.12781v4",
      "published_date": "2025-05-19 07:10:42 UTC",
      "updated_date": "2025-12-18 12:54:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:23.763074+00:00"
    },
    {
      "arxiv_id": "2505.20306v2",
      "title": "Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review",
      "title_zh": "è¾…åŠ©ç”Ÿæ®–æŠ€æœ¯ä¸­èƒšèƒè¯„åˆ†ä¸å¦Šå¨ é¢„æµ‹çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç»¼è¿°",
      "authors": [
        "Xueqiang Ouyang",
        "Jia Wei"
      ],
      "abstract": "Infertility, a pressing global health concern, affects a substantial proportion of individuals worldwide. While advancements in assisted reproductive technology (ART) have offered effective interventions, conventional in vitro fertilization-embryo transfer (IVF-ET) procedures still encounter significant hurdles in enhancing pregnancy success rates. Key challenges include the inherent subjectivity in embryo grading and the inefficiency of multi-modal data integration. Against this backdrop, the adoption of AI-driven technologies has emerged as a pivotal strategy to address these issues. This article presents a comprehensive review of the progress in AI applications for embryo grading and pregnancy prediction from a novel perspective, with a specific focus on the utilization of different modal data, such as static images, time-lapse videos, and structured tabular data. The reason for this perspective is that reorganizing tasks based on data sources can not only more accurately depict the essence of the problem but also help clarify the rationality and limitations of model design. Furthermore, this review critically examines the core challenges in contemporary research, encompassing the intricacies of multi-modal feature fusion, constraints imposed by data scarcity, limitations in model generalization capabilities, and the dynamically evolving legal and regulatory frameworks. On this basis, it explicitly identifies potential avenues for future research, aiming to provide actionable guidance for advancing the application of multi-modal AI in the field of ART.",
      "tldr_zh": "è¯¥æ–‡ç« ç»¼è¿°äº†äººå·¥æ™ºèƒ½(AI)åœ¨è¾…åŠ©ç”Ÿæ®–æŠ€æœ¯(ART)ä¸­èƒšèƒè¯„åˆ†å’Œå¦Šå¨ é¢„æµ‹çš„ç ”ç©¶è¿›å±•ï¼Œæ—¨åœ¨è§£å†³ä½“å¤–å—ç²¾-èƒšèƒç§»æ¤(IVF-ET)è¿‡ç¨‹ä¸­èƒšèƒè¯„åˆ†çš„ä¸»è§‚æ€§åŠå¤šæ¨¡æ€æ•°æ®æ•´åˆæ•ˆç‡ä½ç­‰æ ¸å¿ƒé—®é¢˜ã€‚ä½œè€…ä»æ•°æ®æºçš„æ–°è§†è§’å‡ºå‘ï¼Œç³»ç»Ÿæ¢³ç†äº†åŸºäºé™æ€å›¾åƒ(static images)ã€æ—¶åºå½±åƒ(time-lapse videos)ä»¥åŠç»“æ„åŒ–è¡¨æ ¼æ•°æ®(structured tabular data)çš„AIåº”ç”¨ã€‚é€šè¿‡è¿™ç§åŸºäºæ•°æ®æºçš„ä»»åŠ¡é‡ç»„æ–¹å¼ï¼Œæ–‡ç« ä¸ä»…æ›´å‡†ç¡®åœ°åˆ»ç”»äº†é—®é¢˜æœ¬è´¨ï¼Œè¿˜é˜æ˜äº†æ¨¡å‹è®¾è®¡çš„åˆç†æ€§ä¸å±€é™æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€ç‰¹å¾èåˆ(multi-modal feature fusion)ã€æ•°æ®ç¨€ç¼ºã€æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä»¥åŠæ³•å¾‹ç›‘ç®¡æ¡†æ¶ç­‰å½“å‰ç ”ç©¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œæ–‡ç« æ˜ç¡®äº†æœªæ¥çš„ç ”ç©¶è·¯å¾„ï¼Œä¸ºæ¨åŠ¨å¤šæ¨¡æ€AIåœ¨è¾…åŠ©ç”Ÿæ®–é¢†åŸŸçš„å®é™…è½åœ°æä¾›äº†å…·æœ‰æ“ä½œæ€§çš„æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "eess.IV",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20306v2",
      "published_date": "2025-05-19 07:07:13 UTC",
      "updated_date": "2025-09-24 11:44:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:08.370372+00:00"
    },
    {
      "arxiv_id": "2505.12774v1",
      "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes",
      "title_zh": "UniHMï¼šå®¤å†…åœºæ™¯ä¸‹åŒ…å«ç‰©ä½“äº¤äº’çš„é€šç”¨äººä½“åŠ¨ä½œç”Ÿæˆ",
      "authors": [
        "Zichen Geng",
        "Zeeshan Hayder",
        "Wei Liu",
        "Ajmal Mian"
      ],
      "abstract": "Human motion synthesis in complex scenes presents a fundamental challenge, extending beyond conventional Text-to-Motion tasks by requiring the integration of diverse modalities such as static environments, movable objects, natural language prompts, and spatial waypoints. Existing language-conditioned motion models often struggle with scene-aware motion generation due to limitations in motion tokenization, which leads to information loss and fails to capture the continuous, context-dependent nature of 3D human movement. To address these issues, we propose UniHM, a unified motion language model that leverages diffusion-based generation for synthesizing scene-aware human motion. UniHM is the first framework to support both Text-to-Motion and Text-to-Human-Object Interaction (HOI) in complex 3D scenes. Our approach introduces three key contributions: (1) a mixed-motion representation that fuses continuous 6DoF motion with discrete local motion tokens to improve motion realism; (2) a novel Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in both reconstruction accuracy and generative performance; and (3) an enriched version of the Lingo dataset augmented with HumanML3D annotations, providing stronger supervision for scene-specific motion learning. Experimental results demonstrate that UniHM achieves comparative performance on the OMOMO benchmark for text-to-HOI synthesis and yields competitive results on HumanML3D for general text-conditioned motion generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniHMï¼Œä¸€ç§åˆ©ç”¨ diffusion-based generation æŠ€æœ¯åœ¨å¤æ‚å®¤å†…åœºæ™¯ä¸­åˆæˆåœºæ™¯æ„ŸçŸ¥äººä½“è¿åŠ¨çš„ç»Ÿä¸€è¯­è¨€æ¨¡å‹ã€‚UniHM æ˜¯é¦–ä¸ªåŒæ—¶æ”¯æŒ Text-to-Motion å’Œ Text-to-Human-Object Interaction (HOI) çš„é€šç”¨æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ motion tokenization è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚æŠ€æœ¯ä¸Šï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†èåˆè¿ç»­ 6DoF è¿åŠ¨ä¸ç¦»æ•£è¿åŠ¨ token çš„æ··åˆè¡¨ç¤ºæ³•ï¼Œå¹¶å¼€å‘äº†æ€§èƒ½ä¼˜äºä¼ ç»Ÿ VQ-VAEs çš„ Look-Up-Free Quantization VAE (LFQ-VAE)ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡ HumanML3D æ³¨é‡Šå¢å¼ºäº† Lingo æ•°æ®é›†ï¼Œä¸ºåœºæ™¯ç‰¹å®šçš„è¿åŠ¨å­¦ä¹ æä¾›äº†æ›´å¼ºçš„ç›‘ç£ä¿¡å·ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒUniHM åœ¨ OMOMO åŸºå‡†çš„ text-to-HOI åˆæˆåŠ HumanML3D çš„é€šç”¨è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†é¢†å…ˆæˆ–æå…·ç«äº‰åŠ›çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12774v1",
      "published_date": "2025-05-19 07:02:12 UTC",
      "updated_date": "2025-05-19 07:02:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:14.033647+00:00"
    },
    {
      "arxiv_id": "2505.13554v1",
      "title": "Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation",
      "title_zh": "ä¼˜åŠ¿äº’è¡¥ï¼šä¸€ç§ç¥ç»æœºå™¨ç¿»è¯‘ä¸å¤§è¯­è¨€æ¨¡å‹ç»“åˆçš„æ··åˆç¿»è¯‘æ–¹æ³•",
      "authors": [
        "Zhanglin Wu",
        "Daimeng Wei",
        "Xiaoyu Chen",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Zongyao Li",
        "Yuanchang Luo",
        "Jinlong Yang",
        "Zhiqiang Rao",
        "Hao Yang"
      ],
      "abstract": "Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æœºå™¨ç¿»è¯‘(Machine Translation)é¢†åŸŸç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLM)ä¸ç¥ç»æœºå™¨ç¿»è¯‘(NMT)çš„æ··åˆæ–¹æ³•ã€‚é‰´äºLLMç¿»è¯‘å­˜åœ¨é«˜è®¡ç®—æˆæœ¬å’Œæ˜¾è‘—å»¶è¿Ÿçš„é—®é¢˜ï¼Œä¸”NMTä¸LLMåœ¨ä¸åŒåœºæ™¯ä¸‹å„æœ‰ä¼˜åŠ¿ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ä»…åœ¨å¿…è¦æ—¶è°ƒç”¨LLMçš„è°ƒåº¦ç­–ç•¥ã€‚è¯¥æ–¹æ¡ˆå¼•å…¥äº†ä¸€ä¸ªåŸºäºæºå¥å­ç‰¹å¾çš„æ–°é¢–å†³ç­–å™¨(Decider)ï¼Œæ—¨åœ¨ç¡®ä¿ç¿»è¯‘è´¨é‡çš„åŒæ—¶æœ€å¤§é™åº¦å‡å°‘LLMçš„ä½¿ç”¨é‡ã€‚åœ¨å¤šè¯­è¨€æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æä½çš„LLMè°ƒç”¨æ¯”ä¾‹è¾¾åˆ°æœ€ä¼˜çš„ç¿»è¯‘æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥å†³ç­–å™¨çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ä¿è¯ç¿»è¯‘æ•ˆæœçš„å‰æä¸‹ä¼˜åŒ–æ¨ç†æ•ˆç‡å’Œé™ä½æˆæœ¬æä¾›äº†å®ç”¨çš„å¹³è¡¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 2 figures, 9 tables, ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13554v1",
      "published_date": "2025-05-19 06:50:52 UTC",
      "updated_date": "2025-05-19 06:50:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:17.277185+00:00"
    },
    {
      "arxiv_id": "2505.12767v1",
      "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates",
      "title_zh": "è¨€è¡Œä¸€è‡´çš„è¯­è¨€æ¨¡å‹ï¼šå½¢å¼åŒ–å…¬å¹³æ€§è®¤è¯æ¡†æ¶",
      "authors": [
        "Danqing Chen",
        "Tobias Ladner",
        "Ahmed Rayen Mhadhbi",
        "Matthias Althoff"
      ],
      "abstract": "As large language models become integral to high-stakes applications, ensuring their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as synonym substitutions, can alter model predictions, posing risks in fairness-critical areas, such as gender bias mitigation, and safety-critical areas, such as toxicity detection. While formal verification has been explored for neural networks, its application to large language models remains limited. This work presents a holistic verification framework to certify the robustness of transformer-based language models, with a focus on ensuring gender fairness and consistent outputs across different gender-related terms. Furthermore, we extend this methodology to toxicity detection, offering formal guarantees that adversarially manipulated toxic inputs are consistently detected and appropriately censored, thereby ensuring the reliability of moderation systems. By formalizing robustness within the embedding space, this work strengthens the reliability of language models in ethical AI deployment and content moderation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨æ€§åˆ«åå·®å’Œæ¯’æ€§æ£€æµ‹ç­‰å…³é”®é¢†åŸŸå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼ˆå¦‚è¿‘ä¹‰è¯æ›¿æ¢ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å½¢å¼åŒ–éªŒè¯æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºåŸºäºTransformeræ¶æ„çš„æ¨¡å‹æä¾›é²æ£’æ€§å’Œå…¬å¹³æ€§çš„æ­£å¼è¯ä¹¦ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨åµŒå…¥ç©ºé—´(embedding space)ä¸­å¯¹é²æ£’æ€§è¿›è¡Œå½¢å¼åŒ–å»ºæ¨¡ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ€§åˆ«ç›¸å…³æœ¯è¯­æˆ–ç»å¯¹æŠ—æ€§ç¯¡æ”¹çš„æ¯’æ€§è¾“å…¥æ—¶èƒ½å¤Ÿä¿æŒè¾“å‡ºçš„ä¸€è‡´æ€§ã€‚åœ¨æ€§åˆ«å…¬å¹³æ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯æ˜æ¨¡å‹åœ¨é¢å¯¹ç‰¹å®šè¯æ±‡å˜åŠ¨æ—¶ä¾ç„¶èƒ½ç»´æŒå…¬å¹³çš„é¢„æµ‹ç»“æœã€‚åœ¨æ¯’æ€§æ£€æµ‹åº”ç”¨ä¸­ï¼Œè¯¥æ¡†æ¶æä¾›äº†æ­£å¼ä¿è¯ï¼Œç¡®ä¿å³ä¾¿æ˜¯ç»è¿‡å¯¹æŠ—æ€§å¹²æ‰°çš„æœ‰å®³å†…å®¹ä¹Ÿèƒ½è¢«ä¸€è‡´åœ°æ£€æµ‹å’Œé‡åº¦å®¡æŸ¥ï¼Œä»è€Œæå‡äº†è‡ªåŠ¨åŒ–å®¡æ ¸ç³»ç»Ÿçš„å¯é æ€§ã€‚è¿™ä¸€å·¥ä½œé€šè¿‡å°†é²æ£’æ€§å½¢å¼åŒ–ï¼Œä¸ºè¯­è¨€æ¨¡å‹åœ¨ä¼¦ç†AIéƒ¨ç½²å’Œå†…å®¹æ²»ç†ä¸­çš„åº”ç”¨å¥ å®šäº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12767v1",
      "published_date": "2025-05-19 06:46:17 UTC",
      "updated_date": "2025-05-19 06:46:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:22.101545+00:00"
    },
    {
      "arxiv_id": "2505.12763v1",
      "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization",
      "title_zh": "ä»å¥–åŠ±è¿‡åº¦ä¼˜åŒ–çš„è§†è§’é‡æ–°å®¡è§†å¥–åŠ±æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Sunghwan Kim",
        "Dongjin Kang",
        "Taeyoon Kwon",
        "Hyungjoo Chae",
        "Dongha Lee",
        "Jinyoung Yeo"
      ],
      "abstract": "Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization\\textemdash a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Reward Models (RMs) åœ¨ Reinforcement Learning from Human Feedback (RLHF) ä¸­ç°æœ‰åŸºå‡†æµ‹è¯•ä¸ä¼˜åŒ–ç­–ç•¥è¡¨ç°ç›¸å…³æ€§å¼±çš„é—®é¢˜ï¼Œä» Reward Overoptimization çš„è§†è§’é‡æ–°å®¡è§†äº†å¥–åŠ±æ¨¡å‹çš„è¯„ä¼°è®¾è®¡ã€‚é€šè¿‡æ¢ç´¢ä¸åŒçš„è¯„ä¼°æ–¹æ¡ˆï¼Œæœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¯„ä¼°æŒ‡æ ‡ä¸å®é™…å¯¹é½æ•ˆæœä¹‹é—´çš„å·®è·ï¼Œå¹¶æ•æ‰æ¨¡å‹æä¾›å­¦ä¹ ä¿¡å·çš„åŠ¨æ€è¿‡ç¨‹ã€‚ç ”ç©¶æå‡ºäº†æ„å»ºå¯é åŸºå‡†æµ‹è¯•çš„ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆåº”å°½é‡å‡å°‘ chosen å’Œ rejected å›ç­”ä¹‹é—´é™¤æ­£ç¡®æ€§ä»¥å¤–çš„å·®å¼‚ï¼›å…¶æ¬¡ï¼Œè¯„ä¼°è¿‡ç¨‹éœ€è¦è·¨è¶Šå¹¿æ³›çš„å›ç­”èŒƒå›´è¿›è¡Œå¤šæ¬¡æ¯”è¾ƒï¼›åŒæ—¶ï¼Œè¯„ä¼°æ•°æ®åº”æ¥æºäºå¤šç§ä¸åŒçš„ models ä»¥æ¶µç›–å¤šæ ·çš„è¡¨ç¤ºå½¢å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºä¸ overoptimization ç¨‹åº¦çš„æé«˜ç›¸å…³æ€§å¯èƒ½å¯¼è‡´æŸäº›ä¸‹æ¸¸æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§ä¸‹é™ã€‚æœ€ç»ˆï¼Œä½œè€…å»ºè®®åœ¨è®¾è®¡åŸºå‡†æµ‹è¯•æ—¶åº”å°† overoptimization ç¨‹åº¦ä½œä¸ºä¸€ç§æœ‰ç”¨çš„è¯„ä¼°å·¥å…·ï¼Œè€Œéå”¯ä¸€çš„ç»ˆæç›®æ ‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12763v1",
      "published_date": "2025-05-19 06:43:08 UTC",
      "updated_date": "2025-05-19 06:43:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:30.220179+00:00"
    },
    {
      "arxiv_id": "2505.12762v1",
      "title": "IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment",
      "title_zh": "IDEALï¼šé¢å‘å¤šèƒ½åŠ›è¯­è¨€æ¨¡å‹å¯¹é½çš„æ•°æ®å‡è¡¡è‡ªé€‚åº”æ¡†æ¶",
      "authors": [
        "Chenlin Ming",
        "Chendi Qu",
        "Mengzhang Cai",
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Yu Li",
        "Xiaoming Duan",
        "Lijun Wu",
        "Conghui He"
      ],
      "abstract": "Large Language Models (LLMs) have achieved impressive performance through Supervised Fine-tuning (SFT) on diverse instructional datasets. When training on multiple capabilities simultaneously, the mixture training dataset, governed by volumes of data from different domains, is a critical factor that directly impacts the final model's performance. Unlike many studies that focus on enhancing the quality of training datasets through data selection methods, few works explore the intricate relationship between the compositional quantity of mixture training datasets and the emergent capabilities of LLMs. Given the availability of a high-quality multi-domain training dataset, understanding the impact of data from each domain on the model's overall capabilities is crucial for preparing SFT data and training a well-balanced model that performs effectively across diverse domains. In this work, we introduce IDEAL, an innovative data equilibrium adaptation framework designed to effectively optimize volumes of data from different domains within mixture SFT datasets, thereby enhancing the model's alignment and performance across multiple capabilities. IDEAL employs a gradient-based approach to iteratively refine the training data distribution, dynamically adjusting the volumes of domain-specific data based on their impact on downstream task performance. By leveraging this adaptive mechanism, IDEAL ensures a balanced dataset composition, enabling the model to achieve robust generalization and consistent proficiency across diverse tasks. Experiments across different capabilities demonstrate that IDEAL outperforms conventional uniform data allocation strategies, achieving a comprehensive improvement of approximately 7% in multi-task evaluation scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿›è¡Œå¤šèƒ½åŠ›å¯¹é½æ—¶ï¼Œæ··åˆæŒ‡ä»¤å¾®è°ƒ(SFT)æ•°æ®é›†ä¸­å„é¢†åŸŸæ•°æ®é‡å¯¹æ€§èƒ½å½±å“ä¸æ˜ç¡®çš„é—®é¢˜ï¼Œæå‡ºäº†IDEALæ¡†æ¶ã€‚ä¸åŒäºä¾§é‡æ•°æ®è´¨é‡çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼ŒIDEALæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ•°æ®å‡è¡¡è‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ··åˆSFTæ•°æ®é›†ä¸­ä¸åŒé¢†åŸŸçš„æ•°æ®è§„æ¨¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ¢¯åº¦çš„(gradient-based)æ–¹æ³•æ¥è¿­ä»£ä¼˜åŒ–è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œå¹¶æ ¹æ®å„é¢†åŸŸæ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å½±å“åŠ¨æ€è°ƒæ•´å…¶ä½“é‡ã€‚é€šè¿‡è¿™ç§è‡ªé€‚åº”æœºåˆ¶ï¼ŒIDEALç¡®ä¿äº†æ•°æ®é›†æ„æˆçš„å¹³è¡¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸­å®ç°ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œä¸€è‡´çš„ç†Ÿç»ƒåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDEALåœ¨å¤šèƒ½åŠ›è¯„ä¼°ä¸­çš„ç»¼åˆè¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å‡åŒ€åˆ†é…ç­–ç•¥ï¼Œå¤šä»»åŠ¡è¯„åˆ†å¹³å‡æå‡äº†çº¦7%ã€‚è¯¥ç ”ç©¶ä¸ºå‡†å¤‡é«˜è´¨é‡ã€å‡è¡¡çš„å¤šé¢†åŸŸSFTæ•°æ®æä¾›äº†æœ‰æ•ˆè·¯å¾„ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMsåœ¨å¤æ‚åœºæ™¯ä¸‹çš„å¯¹é½æ•ˆæœä¸æ•´ä½“æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12762v1",
      "published_date": "2025-05-19 06:42:44 UTC",
      "updated_date": "2025-05-19 06:42:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:50.899316+00:00"
    },
    {
      "arxiv_id": "2505.12761v3",
      "title": "Enhancing Channel-Independent Time Series Forecasting via Cross-Variate Patch Embedding",
      "title_zh": "é€šè¿‡è·¨å˜é‡ Patch åµŒå…¥å¢å¼ºé€šé“ç‹¬ç«‹æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Donghwa Shin",
        "Edwin Zhang"
      ],
      "abstract": "Transformers have recently gained popularity in time series forecasting due to their ability to capture long-term dependencies. However, many existing models focus only on capturing temporal dependencies while omitting intricate relationships between variables. Recent models have tried tackling this by explicitly modeling both cross-time and cross-variate dependencies through a sequential or unified attention mechanism, but they are entirely channel dependent (CD) across all layers, making them potentially susceptible to overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE), a lightweight CD module that injects cross-variate context into channel-independent (CI) models by simply modifying the patch embedding process. We achieve this by adding a learnable positional encoding and a lightweight router-attention block to the vanilla patch embedding layer. We then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to demonstrate its effectiveness in capturing cross-variate dependencies and enhance the CI model's performance. Extensive experimental results on seven real-world datasets show that our enhanced Time-LLM outperforms the original baseline model simply by incorporating the CVPE module, with no other changes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Transformeråœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­éš¾ä»¥å¹³è¡¡æ—¶é—´ä¾èµ–ä¸å˜é‡é—´ä¾èµ–çš„é—®é¢˜ï¼Œæå‡ºäº†Cross-Variate Patch Embeddings (CVPE) æ¨¡å—ã€‚CVPE æ˜¯ä¸€ç§è½»é‡çº§çš„Channel-Dependent (CD) ç»„ä»¶ï¼Œé€šè¿‡æ”¹è¿›Patch Embeddingè¿‡ç¨‹ä¸ºChannel-Independent (CI) æ¨¡å‹æ³¨å…¥è·¨å˜é‡ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¯¥æ¨¡å—é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„Positional Encodingå’Œè½»é‡çº§çš„Router-Attentionå—ï¼Œåœ¨æ•æ‰å¤æ‚å˜é‡é—´å…³ç³»çš„åŒæ—¶æœ‰æ•ˆé¿å…äº†å…¨CDæ¨¡å‹æ˜“äº§ç”Ÿçš„è¿‡æ‹Ÿåˆé£é™©ã€‚ç ”ç©¶äººå‘˜å°†CVPEé›†æˆåˆ°å¤šæ¨¡æ€CIé¢„æµ‹æ¨¡å‹Time-LLMä¸­è¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œè¯¥å¢å¼ºæ¨¡å‹å‡æ˜¾è‘—ä¼˜äºåŸå§‹åŸºçº¿ã€‚è¿™ä¸€è´¡çŒ®ä¸ºæå‡CIé¢„æµ‹æ¨¡å‹çš„æ€§èƒ½æä¾›äº†ä¸€ç§ä½æˆæœ¬ä¸”é«˜æ•ˆçš„è·¨å˜é‡å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Added link to code implementation in PDF abstract",
      "pdf_url": "https://arxiv.org/pdf/2505.12761v3",
      "published_date": "2025-05-19 06:41:14 UTC",
      "updated_date": "2025-05-22 23:29:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:46.256385+00:00"
    },
    {
      "arxiv_id": "2507.18454v1",
      "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving",
      "title_zh": "Sandwichï¼šé¢å‘é«˜æ•ˆ CPU å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡çš„é¢„å¡«å……ä¸è§£ç åˆ†ç¦»ç¼–è¯‘æŠ€æœ¯",
      "authors": [
        "Juntao Zhao",
        "Jiuru Li",
        "Chuan Wu"
      ],
      "abstract": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly alternative to GPU serving. Existing CPU-based solutions ignore workload differences between the prefill and the decode phases of LLM inference, applying a static per-NUMA (Non-Uniform Memory Access) node model partition and utilizing vendor libraries for operator-level execution, which is suboptimal. We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses different execution plans for the prefill and decode phases and optimizes them separately.\n  We evaluate Sandwich across diverse baselines and datasets on five CPU platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON. Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up to 3.40x lower requirements in single sequence serving, and significant improvement in Goodput in continuous-batching serving. The GEMM kernels generated by Sandwich outperform representative vendor kernels and other dynamic shape solutions, achieving performance comparable to static compilers with three orders of magnitude less kernel tuning costs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ CPU ç«¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡ä¸­å¿½è§† Prefill ä¸ Decode é˜¶æ®µå·¥ä½œè´Ÿè½½å·®å¼‚çš„é—®é¢˜ï¼Œæå‡ºäº†ç¡¬ä»¶æ„ŸçŸ¥çš„æ¨ç†å¼•æ“ Sandwichã€‚Sandwich æ ¸å¿ƒåœ¨äºä¸ºè¿™ä¸¤ä¸ªé˜¶æ®µå®šåˆ¶ä¸åŒçš„æ‰§è¡Œè®¡åˆ’å¹¶è¿›è¡Œç‹¬ç«‹ä¼˜åŒ–ï¼Œæ‰“ç ´äº†ç°æœ‰æ–¹æ¡ˆä¸­ per-NUMA é™æ€æ¨¡å‹åˆ†åŒºå’Œé€šç”¨ç®—å­åº“å¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆã€‚åœ¨åŒ…å«æ”¯æŒ AVX-2ã€AVX-512 çš„ x86 ä»¥åŠ ARM NEON åœ¨å†…çš„äº”ä¸ª CPU å¹³å°ä¸Šï¼ŒSandwich å®ç°äº†å¹³å‡ 2.01 å€çš„ååé‡æå‡ï¼Œå¹¶æ˜¾è‘—ä¼˜åŒ–äº† TTFT å’Œ TPOT å»¶è¿Ÿã€‚å…¶å®éªŒç»“æœè¡¨æ˜ï¼ŒSandwich ç”Ÿæˆçš„ GEMM å†…æ ¸æ€§èƒ½ä¼˜äºä¸»æµå‚å•†åº“ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å°†å†…æ ¸è°ƒä¼˜æˆæœ¬é™ä½äº†ä¸‰ä¸ªæ•°é‡çº§ï¼Œæå¤§åœ°æå‡äº† continuous-batching æ¨¡å¼ä¸‹çš„ Goodput è¡¨ç°ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.DC",
        "cs.PL"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18454v1",
      "published_date": "2025-05-19 06:37:29 UTC",
      "updated_date": "2025-05-19 06:37:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:55.539217+00:00"
    },
    {
      "arxiv_id": "2506.13769v1",
      "title": "Non-planar Object Detection and Identification by Features Matching and Triangulation Growth",
      "title_zh": "åŸºäºç‰¹å¾åŒ¹é…ä¸ä¸‰è§’ç½‘ç”Ÿé•¿çš„éå¹³é¢ç›®æ ‡æ£€æµ‹ä¸è¯†åˆ«",
      "authors": [
        "Filippo Leveni"
      ],
      "abstract": "Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾åŒ¹é…å’Œä¸‰è§’æµ‹é‡å¢é•¿(Triangulation Growth)çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å’Œè¯†åˆ«åœºæ™¯å›¾åƒä¸­éå¹³é¢æˆ–å‘ç”Ÿæ‰­æ›²çš„ç‰©ä½“ã€‚è¯¥æ–¹æ³•å°†æ¨¡æ¿ç‰¹å¾ç”Ÿæˆçš„ Delaunay triangulation è§†ä¸ºå›¾ç»“æ„ï¼Œé€šè¿‡è¿­ä»£æ–¹å¼å¯¹å›¾åƒä¸æ¨¡æ¿ä¹‹é—´çš„ç‰¹å¾åŒ¹é…è¿›è¡Œå¢é‡åˆ†ç»„ã€‚åœ¨åŒ¹é…è¿‡ç¨‹ä¸­ï¼Œç³»ç»ŸåŸºäºå±€éƒ¨ç‰¹å¾çš„å‡ ä½•å’Œå…‰åº¦å±æ€§ç­‰å±€éƒ¨ä¸€è‡´æ€§å‡†åˆ™(local consistency criteria)ï¼Œè¯„ä¼°å¹¶æå–é‚»è¿‘èŠ‚ç‚¹çš„åŒ¹é…ç‰¹å¾ï¼Œä»è€Œå®ç°å¯¹ç‰©ä½“çš„ç²¾ç¡®è¯†åˆ«ã€‚è¯¥æ–¹æ¡ˆæœ‰æ•ˆè§£å†³äº†åœ¨ä¼ ç»Ÿå‡ ä½•æ¨¡å‹ï¼ˆå¦‚ homographyï¼‰å¤±æ•ˆæ—¶ï¼Œéš¾ä»¥è¯†åˆ«éå¹³é¢ç‰©ä½“æˆ–ä¸¥é‡å˜å½¢ç‰©ä½“çš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å˜å½¢æ˜¾è‘—çš„å¤æ‚åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•çš„æè¿°æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„åŸºäº homography çš„ RANSAC ç®—æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Master's thesis at Politecnico di Milano",
      "pdf_url": "https://arxiv.org/pdf/2506.13769v1",
      "published_date": "2025-05-19 06:20:07 UTC",
      "updated_date": "2025-05-19 06:20:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:50.486847+00:00"
    },
    {
      "arxiv_id": "2505.12751v1",
      "title": "Structure-based Anomaly Detection and Clustering",
      "title_zh": "åŸºäºç»“æ„çš„å¼‚å¸¸æ£€æµ‹ä¸èšç±»",
      "authors": [
        "Filippo Leveni"
      ],
      "abstract": "Anomaly detection is a fundamental problem in domains such as healthcare, manufacturing, and cybersecurity. This thesis proposes new unsupervised methods for anomaly detection in both structured and streaming data settings. In the first part, we focus on structure-based anomaly detection, where normal data follows low-dimensional manifolds while anomalies deviate from them. We introduce Preference Isolation Forest (PIF), which embeds data into a high-dimensional preference space via manifold fitting, and isolates outliers using two variants: Voronoi-iForest, based on geometric distances, and RuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also propose Sliding-PIF, which captures local manifold information for streaming scenarios. Our methods outperform existing techniques on synthetic and real datasets. We extend this to structure-based clustering with MultiLink, a novel method for recovering multiple geometric model families in noisy data. MultiLink merges clusters via a model-aware linkage strategy, enabling robust multi-class structure recovery. It offers key advantages over existing approaches, such as speed, reduced sensitivity to thresholds, and improved robustness to poor initial sampling. The second part of the thesis addresses online anomaly detection in evolving data streams. We propose Online Isolation Forest (Online-iForest), which uses adaptive, multi-resolution histograms and dynamically updates tree structures to track changes over time. It avoids retraining while achieving accuracy comparable to offline models, with superior efficiency for real-time applications. Finally, we tackle anomaly detection in cybersecurity via open-set recognition for malware classification. We enhance a Gradient Boosting classifier with MaxLogit to detect unseen malware families, a method now integrated into Cleafy's production system.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ã€åˆ¶é€ å’Œç½‘ç»œå®‰å…¨ç­‰é¢†åŸŸçš„ Anomaly Detection ä¸ Clustering é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç³»åˆ—åŸºäºç»“æ„å’Œæµå¼æ•°æ®çš„æ— ç›‘ç£æ–°æ–¹æ³•ã€‚åœ¨ç»“æ„åŒ–æ£€æµ‹æ–¹é¢ï¼Œè®ºæ–‡å¼•å…¥äº† Preference Isolation Forest (PIF) æ¡†æ¶ï¼Œé€šè¿‡æµå½¢æ‹Ÿåˆå°†æ•°æ®åµŒå…¥é«˜ç»´ç©ºé—´ï¼Œå¹¶åˆ©ç”¨ Voronoi-iForest å’Œ RuzHash-iForest ç­‰å˜ä½“å®ç°é«˜æ•ˆçš„ç¦»ç¾¤ç‚¹éš”ç¦»ã€‚é’ˆå¯¹æµå¼å’Œèšç±»åœºæ™¯ï¼Œç ”ç©¶æå‡ºäº†èƒ½å¤Ÿæ•è·å±€éƒ¨æµå½¢ä¿¡æ¯çš„ Sliding-PIFï¼Œä»¥åŠåœ¨å™ªå£°ä¸­ç¨³å¥æ¢å¤å‡ ä½•æ¨¡å‹çš„ MultiLink æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†é€Ÿåº¦ä¸é²æ£’æ€§ã€‚é’ˆå¯¹åŠ¨æ€æ¼”åŒ–çš„æ•°æ®æµï¼Œä½œè€…å¼€å‘äº† Online Isolation Forest (Online-iForest)ï¼Œåˆ©ç”¨è‡ªé€‚åº”å¤šåˆ†è¾¨ç‡ç›´æ–¹å›¾å®ç°å®æ—¶æ›´æ–°è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†å¢å¼ºçš„ Gradient Boosting åˆ†ç±»å™¨ä¸ MaxLogit ç»“åˆï¼Œæœ‰æ•ˆæå‡äº†ç½‘ç»œå®‰å…¨ä¸­æœªçŸ¥æ¶æ„è½¯ä»¶å®¶æ—çš„æ£€æµ‹èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„å¼‚å¸¸æ£€æµ‹æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•ä¸”å®æ—¶çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Doctoral dissertation at Politecnico di Milano",
      "pdf_url": "https://arxiv.org/pdf/2505.12751v1",
      "published_date": "2025-05-19 06:20:00 UTC",
      "updated_date": "2025-05-19 06:20:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:56.654114+00:00"
    },
    {
      "arxiv_id": "2505.12750v1",
      "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions",
      "title_zh": "åŸºäº Android manifest æƒé™å¼€æ”¾é›†è¯†åˆ«çš„æ¶æ„è½¯ä»¶å®¶æ—å‘ç°",
      "authors": [
        "Filippo Leveni",
        "Matteo Mistura",
        "Francesco Iubatti",
        "Carmine Giangregorio",
        "NicolÃ² Pastore",
        "Cesare Alippi",
        "Giacomo Boracchi"
      ],
      "abstract": "Malware are malicious programs that are grouped into families based on their penetration technique, source code, and other characteristics. Classifying malware programs into their respective families is essential for building effective defenses against cyber threats. Machine learning models have a huge potential in malware detection on mobile devices, as malware families can be recognized by classifying permission data extracted from Android manifest files. Still, the malware classification task is challenging due to the high-dimensional nature of permission data and the limited availability of training samples. In particular, the steady emergence of new malware families makes it impossible to acquire a comprehensive training set covering all the malware classes. In this work, we present a malware classification system that, on top of classifying known malware, detects new ones. In particular, we combine an open-set recognition technique developed within the computer vision community, namely MaxLogit, with a tree-based Gradient Boosting classifier, which is particularly effective in classifying high-dimensional data. Our solution turns out to be very practical, as it can be seamlessly employed in a standard classification workflow, and efficient, as it adds minimal computational overhead. Experiments on public and proprietary datasets demonstrate the potential of our solution, which has been deployed in a business environment.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹Androidæ¶æ„è½¯ä»¶åˆ†ç±»ä¸­é¢ä¸´çš„é«˜ç»´æƒé™æ•°æ®ä»¥åŠæ–°å®¶æ—ä¸æ–­æ¶Œç°çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼€æ”¾é›†è¯†åˆ«ï¼ˆOpen-Set Recognitionï¼‰çš„æ¶æ„è½¯ä»¶å®¶æ—å‘ç°ç³»ç»Ÿã€‚è¯¥æ–¹æ¡ˆå°†è®¡ç®—æœºè§†è§‰é¢†åŸŸå¸¸ç”¨çš„MaxLogitæŠ€æœ¯ä¸æ“…é•¿å¤„ç†é«˜ç»´æ•°æ®çš„æ¢¯åº¦æå‡ï¼ˆGradient Boostingï¼‰å†³ç­–æ ‘åˆ†ç±»å™¨ç›¸ç»“åˆï¼Œåˆ©ç”¨ä»Android manifestæ–‡ä»¶ä¸­æå–çš„æƒé™æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚è¯¥ç³»ç»Ÿä¸ä»…èƒ½å¤Ÿå¯¹å·²çŸ¥æ¶æ„è½¯ä»¶å®¶æ—è¿›è¡Œç²¾ç¡®åˆ†ç±»ï¼Œè¿˜å…·å¤‡æ£€æµ‹æœªçŸ¥æ–°å®¶æ—çš„èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†è®­ç»ƒé›†æ— æ³•æ¶µç›–æ‰€æœ‰æ¶æ„ç±»åˆ«çš„å±€é™æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆè®¡ç®—å¼€é”€æå°ä¸”æ˜“äºé›†æˆåˆ°ç°æœ‰å·¥ä½œæµä¸­ï¼Œåœ¨å…¬å…±åŠç§æœ‰æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¹¶å·²åœ¨å®é™…ä¸šåŠ¡ç¯å¢ƒä¸­æˆåŠŸéƒ¨ç½²ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Submitted to European Conference on Artificial Intelligence (ECAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.12750v1",
      "published_date": "2025-05-19 06:19:54 UTC",
      "updated_date": "2025-05-19 06:19:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:59:58.453541+00:00"
    },
    {
      "arxiv_id": "2505.12748v2",
      "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation",
      "title_zh": "TeleOpBenchï¼šä»¥ä»¿çœŸå™¨ä¸ºä¸­å¿ƒçš„åŒè‡‚çµå·§é¥æ“ä½œè¯„æµ‹åŸºå‡†",
      "authors": [
        "Hangyu Li",
        "Qin Zhao",
        "Haoran Xu",
        "Xinyu Jiang",
        "Qingwei Ben",
        "Feiyu Jia",
        "Haoyu Zhao",
        "Liang Xu",
        "Jia Zeng",
        "Hanqing Wang",
        "Bo Dai",
        "Junting Dong",
        "Jiangmiao Pang"
      ],
      "abstract": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual dexterous teleoperation in particular provides rich demonstrations that are difficult to obtain with fully autonomous systems. While recent studies have proposed diverse hardware pipelines-ranging from inertial motion-capture gloves to exoskeletons and vision-based interfaces-there is still no unified benchmark that enables fair, reproducible comparison of these systems. In this paper, we introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual dexterous teleoperation. TeleOpBench contains 30 high-fidelity task environments that span pick-and-place, tool use, and collaborative manipulation, covering a broad spectrum of kinematic and force-interaction difficulty. Within this benchmark we implement four representative teleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand exoskeletons, and (iv) monocular vision tracking-and evaluate them with a common protocol and metric suite. To validate that performance in simulation is predictive of real-world behavior, we conduct mirrored experiments on a physical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10 held-out tasks we observe a strong correlation between simulator and hardware performance, confirming the external validity of TeleOpBench. TeleOpBench establishes a common yardstick for teleoperation research and provides an extensible platform for future algorithmic and hardware innovation. Codes is now available at https://github.com/cyjdlhy/TeleOpBench .",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† TeleOpBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåŒæ‰‹çµå·§è¿œç¨‹æ“ä½œ (Bimanual dexterous teleoperation) é‡èº«å®šåˆ¶çš„ä»¥ä»¿çœŸä¸ºä¸­å¿ƒ (Simulator-centric) çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç›®å‰ç¼ºä¹ç»Ÿä¸€æ ‡å‡†æ¥å…¬å¹³æ¯”è¾ƒå„ç§è¿œç¨‹æ“ä½œç³»ç»Ÿçš„éš¾é¢˜ã€‚TeleOpBench åŒ…å« 30 ä¸ªé«˜ä¿çœŸä»»åŠ¡ç¯å¢ƒï¼Œæ¶µç›–äº†æ‹¾å–æ”¾ç½® (Pick-and-place)ã€å·¥å…·ä½¿ç”¨ (Tool use) ä»¥åŠåä½œæ“çºµ (Collaborative manipulation) ç­‰å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„è¿åŠ¨å­¦å’ŒåŠ›äº¤äº’åœºæ™¯ã€‚ç ”ç©¶äººå‘˜åœ¨è¯¥åŸºå‡†ä¸Šè¯„ä¼°äº† MoCapã€VR è®¾å¤‡ã€æ‰‹è‡‚æ‰‹éƒ¨å¤–éª¨éª¼ (Arm-hand exoskeletons) ä»¥åŠå•ç›®è§†è§‰è¿½è¸ª (Monocular vision tracking) å››ç§ä»£è¡¨æ€§æ¨¡å¼ã€‚é€šè¿‡åœ¨é…å¤‡ä¸¤ä¸ª 6-DoF çµå·§æ‰‹çš„ç‰©ç†åŒè‡‚å¹³å°ä¸Šè¿›è¡Œçš„é•œåƒå®éªŒï¼Œç ”ç©¶è¯æ˜äº†ä»¿çœŸç¯å¢ƒä¸çœŸå®ä¸–ç•Œæ€§èƒ½ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚TeleOpBench ä¸ºè¿œç¨‹æ“ä½œç ”ç©¶ç¡®ç«‹äº†ç»Ÿä¸€çš„è¡¡é‡æ ‡å‡†ï¼Œå¹¶ä¸ºæœªæ¥çš„ç®—æ³•å’Œç¡¬ä»¶åˆ›æ–°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æ”¾å¹³å°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page:https://gorgeous2002.github.io/TeleOpBench/, Codes:https://github.com/cyjdlhy/TeleOpBench",
      "pdf_url": "https://arxiv.org/pdf/2505.12748v2",
      "published_date": "2025-05-19 06:08:53 UTC",
      "updated_date": "2025-09-15 08:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:06.374482+00:00"
    },
    {
      "arxiv_id": "2505.12746v2",
      "title": "Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs",
      "title_zh": "è§†é¢‘ç‰‡æ®µè¯±å‘çš„äººç±»ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é«˜ç»´æƒ…ç»ªç»“æ„çš„å¯¹åº”æ€§",
      "authors": [
        "Haruka Asanuma",
        "Naoko Koide-Majima",
        "Ken Nakamura",
        "Takato Horii",
        "Shinji Nishimoto",
        "Masafumi Oizumi"
      ],
      "abstract": "Recent studies have revealed that human emotions exhibit a high-dimensional, complex structure. A full capturing of this complexity requires new approaches, as conventional models that disregard high dimensionality risk overlooking key nuances of human emotions. Here, we examined the extent to which the latest generation of rapidly evolving Multimodal Large Language Models (MLLMs) capture these high-dimensional, intricate emotion structures, including capabilities and limitations. Specifically, we compared self-reported emotion ratings from participants watching videos with model-generated estimates (e.g., Gemini or GPT). We evaluated performance not only at the individual video level but also from emotion structures that account for inter-video relationships. At the level of simple correlation between emotion structures, our results demonstrated strong similarity between human and model-inferred emotion structures. To further explore whether the similarity between humans and models is at the signle item level or the coarse-categorical level, we applied Gromov Wasserstein Optimal Transport. We found that although performance was not necessarily high at the strict, single-item level, performance across video categories that elicit similar emotions was substantial, indicating that the model could infer human emotional experiences at the category level. Our results suggest that current state-of-the-art MLLMs broadly capture the complex high-dimensional emotion structures at the category level, as well as their apparent limitations in accurately capturing entire structures at the single-item level.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœ€æ–°ä¸€ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Models, MLLMs)æ˜¯å¦èƒ½å¤Ÿæ•æ‰äººç±»åœ¨è§‚çœ‹è§†é¢‘æ—¶è¡¨ç°å‡ºçš„é«˜ç»´ä¸”å¤æ‚çš„æƒ…ç»ªç»“æ„(high-dimensional emotion structures)ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹æ¯”å‚ä¸è€…çš„è‡ªæˆ‘æŠ¥å‘Šæƒ…ç»ªè¯„åˆ†ä¸ Gemini æˆ– GPT ç­‰æ¨¡å‹çš„ç”Ÿæˆä¼°å€¼ï¼Œä»å•ä¸ªè§†é¢‘æ°´å¹³ä»¥åŠåæ˜ è§†é¢‘é—´å…³ç³»çš„æ•´ä½“æƒ…ç»ªç»“æ„ç»´åº¦è¯„ä¼°äº†æ¨¡å‹çš„è¡¨ç°ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŒºåˆ†è¿™ç§ç›¸ä¼¼æ€§æ˜¯æºäºå•æ¡æƒ…ç»ªé¡¹(single-item level)è¿˜æ˜¯ç²—ç²’åº¦çš„ç±»åˆ«å±‚é¢(coarse-categorical level)ï¼Œç ”ç©¶å¼•å…¥äº† Gromov Wasserstein Optimal Transport æ–¹æ³•è¿›è¡Œåˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œäººç±»ä¸æ¨¡å‹æ¨æ–­çš„æƒ…ç»ªç»“æ„åœ¨ç›¸å…³æ€§å±‚é¢å…·æœ‰æå¼ºçš„ç›¸ä¼¼æ€§ï¼Œå°¤å…¶åœ¨è¯±å‘ç›¸ä¼¼æƒ…ç»ªçš„è§†é¢‘ç±»åˆ«å±‚é¢è¡¨ç°æ˜¾è‘—ï¼Œè¯æ˜æ¨¡å‹èƒ½æœ‰æ•ˆæ¨æ–­äººç±»åœ¨ç±»åˆ«å±‚é¢çš„æƒ…ç»ªä½“éªŒã€‚å°½ç®¡æ¨¡å‹åœ¨ä¸¥æ ¼çš„å•æ¡æƒ…ç»ªé¡¹ç²¾ç¡®åˆ»ç”»ä¸Šä»å­˜åœ¨å±€é™æ€§ï¼Œä½†è¯¥å‘ç°è¯å®äº†å½“å‰çš„ state-of-the-art MLLMs å·²èƒ½å¹¿æ³›æ•æ‰å¤æ‚çš„é«˜ç»´æƒ…ç»ªç»“æ„ï¼Œä¸ºç†è§£äººå·¥æ™ºèƒ½çš„æƒ…ç»ªç†è§£èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12746v2",
      "published_date": "2025-05-19 06:03:22 UTC",
      "updated_date": "2025-05-23 05:15:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:05.041768+00:00"
    },
    {
      "arxiv_id": "2505.12745v1",
      "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization",
      "title_zh": "PEER pressureï¼šé¢å‘å•æºåŸŸæ³›åŒ–çš„æ¨¡å‹é—´æ­£åˆ™åŒ–",
      "authors": [
        "Dong Kyu Cho",
        "Inwoo Hwang",
        "Sanghack Lee"
      ],
      "abstract": "Data augmentation is a popular tool for single source domain generalization, which expands the source domain by generating simulated ones, improving generalization on unseen target domains. In this work, we show that the performance of such augmentation-based methods in the target domains universally fluctuates during training, posing challenges in model selection under realistic scenarios. We argue that the fluctuation stems from the inability of the model to accumulate the knowledge learned from diverse augmentations, exacerbating feature distortion during training. Based on this observation, we propose a novel generalization method, coined Parameter-Space Ensemble with Entropy Regularization (PEER), that uses a proxy model to learn the augmented data on behalf of the main model. The main model is updated by averaging its parameters with the proxy model, progressively accumulating knowledge over the training steps. Maximizing the mutual information between the output representations of the two models guides the learning process of the proxy model, mitigating feature distortion during training. Experimental results demonstrate the effectiveness of PEER in reducing the OOD performance fluctuation and enhancing generalization across various datasets, including PACS, Digits, Office-Home, and VLCS. Notably, our method with simple random augmentation achieves state-of-the-art performance, surpassing prior approaches on sDG that utilize complex data augmentation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•æºé¢†åŸŸæ³›åŒ–(Single Source Domain Generalization, sDG)ä¸­æ•°æ®å¢å¼ºæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ€§èƒ½æ³¢åŠ¨å‰§çƒˆä¸”å­˜åœ¨ç‰¹å¾å¤±çœŸ(Feature Distortion)çš„é—®é¢˜å±•å¼€ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºPEER(Parameter-Space Ensemble with Entropy Regularization)çš„æ–°å‹æ³›åŒ–æ–¹æ³•ã€‚PEERåˆ©ç”¨ä¸€ä¸ªä»£ç†æ¨¡å‹ä»£è¡¨ä¸»æ¨¡å‹å­¦ä¹ å¢å¼ºæ•°æ®ï¼Œå¹¶é€šè¿‡ä¸»æ¨¡å‹ä¸ä»£ç†æ¨¡å‹å‚æ•°å¹³å‡åŒ–çš„æ–¹å¼ï¼Œä½¿ä¸»æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥ç§¯ç´¯çŸ¥è¯†ã€‚åŒæ—¶ï¼Œé€šè¿‡æœ€å¤§åŒ–ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºè¡¨ç¤ºä¹‹é—´çš„äº’ä¿¡æ¯æ¥å¼•å¯¼ä»£ç†æ¨¡å‹çš„å­¦ä¹ ï¼Œæœ‰æ•ˆç¼“è§£äº†è®­ç»ƒä¸­çš„ç‰¹å¾å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPEERåœ¨PACSã€Digitsã€Office-Homeå’ŒVLCSç­‰å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†åˆ†å¸ƒå¤–(OOD)æ€§èƒ½æ³¢åŠ¨ï¼Œæå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§ã€‚å³ä½¿ä»…é…åˆç®€å•çš„éšæœºå¢å¼ºï¼Œè¯¥æ–¹æ³•ä¹Ÿä¼˜äºç›®å‰ä½¿ç”¨å¤æ‚å¢å¼ºç­–ç•¥çš„sDGæ–¹æ³•ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 9 figures, Accepted at CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12745v1",
      "published_date": "2025-05-19 06:01:11 UTC",
      "updated_date": "2025-05-19 06:01:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:12.263665+00:00"
    },
    {
      "arxiv_id": "2505.12744v1",
      "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation",
      "title_zh": "æ¿€åŠ±å¤§æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†ä»¥å®ç°ç›´æ¥æœºå™¨äººæ“æ§",
      "authors": [
        "Weiliang Tang",
        "Dong Jing",
        "Jia-Hui Pan",
        "Zhiwu Lu",
        "Yun-Hui Liu",
        "Li Erran Li",
        "Mingyu Ding",
        "Chi-Wing Fu"
      ],
      "abstract": "Recent Large Multimodal Models have demonstrated remarkable reasoning capabilities, especially in solving complex mathematical problems and realizing accurate spatial perception. Our key insight is that these emerging abilities can naturally extend to robotic manipulation by enabling LMMs to directly infer the next goal in language via reasoning, rather than relying on a separate action head. However, this paradigm meets two main challenges: i) How to make LMMs understand the spatial action space, and ii) How to fully exploit the reasoning capacity of LMMs in solving these tasks. To tackle the former challenge, we propose a novel task formulation, which inputs the current states of object parts and the gripper, and reformulates rotation by a new axis representation instead of traditional Euler angles. This representation is more compatible with spatial reasoning and easier to interpret within a unified language space. For the latter challenge, we design a pipeline to utilize cutting-edge LMMs to generate a small but high-quality reasoning dataset of multi-round dialogues that successfully solve manipulation tasks for supervised fine-tuning. Then, we perform reinforcement learning by trial-and-error interactions in simulation to further enhance the model's reasoning abilities for robotic manipulation. Our resulting reasoning model built upon a 7B backbone, named ReasonManip, demonstrates three notable advantages driven by its system-2 level reasoning capabilities: i) exceptional generalizability to out-of-distribution environments, objects, and tasks; ii) inherent sim-to-real transfer ability enabled by the unified language representation shared across domains; iii) transparent interpretability connecting high-level reasoning and low-level control. Extensive experiments demonstrate the effectiveness of the proposed paradigm and its potential to advance LMM-driven robotic manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMMs)çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡è¯­è¨€ç›´æ¥æ¨æ–­æœºå™¨äººæ“ä½œçš„ä¸‹ä¸€ä¸ªç›®æ ‡ï¼Œè€Œéä¾èµ–ç‹¬ç«‹çš„åŠ¨ä½œå¤´ã€‚ä¸ºè§£å†³æ¨¡å‹å¯¹ç©ºé—´åŠ¨ä½œç©ºé—´çš„ç†è§£éš¾é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡è¡¨è¿°ï¼Œå¼•å…¥è½´è¡¨ç¤ºæ³•(axis representation)å–ä»£ä¼ ç»Ÿçš„æ¬§æ‹‰è§’(Euler angles)ï¼Œä½¿å…¶æ›´å¥‘åˆç©ºé—´æ¨ç†ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤§æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„å¤šè½®å¯¹è¯æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ(SFT)ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (RL)åœ¨ä»¿çœŸç¯å¢ƒä¸­çš„è¯•é”™äº¤äº’è¿›ä¸€æ­¥å¼ºåŒ–æ¨ç†èƒ½åŠ›ã€‚åŸºäº7Bä¸»å¹²ç½‘ç»œæ„å»ºçš„ReasonManipæ¨¡å‹å±•ç°äº†å“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œèƒ½å¤Ÿå¤„ç†åˆ†å¸ƒå¤–(out-of-distribution)çš„ç¯å¢ƒã€ç‰©ä½“å’Œä»»åŠ¡ã€‚ç»Ÿä¸€çš„è¯­è¨€è¡¨ç¤ºèµ‹äºˆäº†è¯¥æ¨¡å‹å†…åœ¨çš„ä»ä»¿çœŸåˆ°ç°å®(sim-to-real)çš„è¿ç§»èƒ½åŠ›ï¼Œå¹¶å®ç°äº†è¿æ¥é«˜å±‚æ¨ç†ä¸åº•å±‚æ§åˆ¶çš„é€æ˜å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥èŒƒå¼åœ¨æ¨åŠ¨å¤§æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººæ“ä½œé¢†åŸŸçš„æœ‰æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12744v1",
      "published_date": "2025-05-19 06:00:14 UTC",
      "updated_date": "2025-05-19 06:00:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:15.828266+00:00"
    },
    {
      "arxiv_id": "2505.12741v1",
      "title": "Dense Communication between Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹é—´çš„ç¨ å¯†é€šä¿¡",
      "authors": [
        "Shiguang Wu",
        "Yaqing Wang",
        "Quanming Yao"
      ],
      "abstract": "As higher-level intelligence emerges from the combination of modular components with lower-level intelligence, many works combines Large Language Models (LLMs) for collective intelligence. Such combination is achieved by building communications among LLMs. While current systems primarily facilitate such communication through natural language, this paper proposes a novel paradigm of direct dense vector communication between LLMs. Our approach eliminates the unnecessary embedding and de-embedding steps when LLM interact with another, enabling more efficient information transfer, fully differentiable optimization pathways, and exploration of capabilities beyond human heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq modules as edges to construct LMNet, with similar structure as MLPs. By utilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves comparable performance with LLMs in similar size with only less than 0.1% training cost. This offers a new perspective on scaling for general intelligence rather than training a monolithic LLM from scratch. Besides, the proposed method can be used for other applications, like customizing LLM with limited data, showing its versatility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LMNetï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´å»ºç«‹ç›´æ¥ç¨ å¯†å‘é‡é€šä¿¡ï¼ˆDense Communicationï¼‰çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨å–ä»£ä¼ ç»Ÿçš„è‡ªç„¶è¯­è¨€äº¤äº’æ–¹å¼ã€‚é€šè¿‡æ¶ˆé™¤æ¨¡å‹é—´äº¤äº’æ—¶å†—ä½™çš„åµŒå…¥ï¼ˆEmbeddingï¼‰ä¸å»åµŒå…¥ï¼ˆDe-embeddingï¼‰æ­¥éª¤ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ›´é«˜æ•ˆçš„ä¿¡æ¯ä¼ è¾“å’Œå…¨å¾®åˆ†çš„ä¼˜åŒ–è·¯å¾„ã€‚LMNet å°†ç®€åŒ–åçš„ LLMs ä½œä¸ºé¡¶ç‚¹ï¼Œå¹¶åˆ©ç”¨å¯ä¼˜åŒ–çš„åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰æ¨¡å—ä½œä¸ºè¾¹ï¼Œæ„å»ºå‡ºç±»ä¼¼äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„æ¶æ„ã€‚å®éªŒè¯æ˜ï¼Œåˆ©ç”¨è¾ƒå°çš„é¢„è®­ç»ƒ LLMs æ„å»ºçš„ LMNet ä»…éœ€ä¸åˆ° 0.1% çš„è®­ç»ƒæˆæœ¬å³å¯è¾¾åˆ°ä¸åŒç­‰è§„æ¨¡å•ä½“æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™ä¸€æˆæœä¸ºé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆGeneral Intelligenceï¼‰çš„æ‰©å±•æä¾›äº†ä¸€ç§æ¨¡å—åŒ–ç»„åˆçš„æ–°è§†è§’ï¼Œå¹¶åœ¨æœ‰é™æ•°æ®çš„å®šåˆ¶åŒ–åº”ç”¨ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12741v1",
      "published_date": "2025-05-19 05:56:06 UTC",
      "updated_date": "2025-05-19 05:56:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:28.538721+00:00"
    },
    {
      "arxiv_id": "2505.12738v1",
      "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting",
      "title_zh": "EpiLLMï¼šæŒ–æ˜å¤§è¯­è¨€æ¨¡å‹åœ¨ç–«æƒ…é¢„æµ‹ä¸­çš„æ½œåŠ›",
      "authors": [
        "Chenghua Gong",
        "Rui Sun",
        "Yuhao Zheng",
        "Juyuan Zhang",
        "Tianjun Gu",
        "Liming Pan",
        "Linyuan Lv"
      ],
      "abstract": "Advanced epidemic forecasting is critical for enabling precision containment strategies, highlighting its strategic importance for public health security. While recent advances in Large Language Models (LLMs) have demonstrated effectiveness as foundation models for domain-specific tasks, their potential for epidemic forecasting remains largely unexplored. In this paper, we introduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting. Considering the key factors in real-world epidemic transmission: infection cases and human mobility, we introduce a dual-branch architecture to achieve fine-grained token-level alignment between such complex epidemic patterns and language tokens for LLM adaptation. To unleash the multi-step forecasting and generalization potential of LLM architectures, we propose an autoregressive modeling paradigm that reformulates the epidemic forecasting task into next-token prediction. To further enhance LLM perception of epidemics, we introduce spatio-temporal prompt learning techniques, which strengthen forecasting capabilities from a data-driven perspective. Extensive experiments show that EpiLLM significantly outperforms existing baselines on real-world COVID-19 datasets and exhibits scaling behavior characteristic of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EpiLLMï¼Œä¸€ç§æ—¨åœ¨æŒ–æ˜å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨æ—¶ç©ºæµè¡Œç—…é¢„æµ‹ (spatio-temporal epidemic forecasting) é¢†åŸŸæ½œåŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹æ„ŸæŸ“ç—…ä¾‹å’Œäººå£æµåŠ¨ç­‰æµè¡Œç—…ä¼ æ’­çš„å…³é”®å› ç´ ï¼Œé‡‡ç”¨äº†åŒåˆ†æ”¯æ¶æ„ (dual-branch architecture)ï¼Œå®ç°äº†å¤æ‚æµè¡Œç—…æ¨¡å¼ä¸è¯­è¨€æ ‡è®° (tokens) ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚ä¸ºäº†é‡Šæ”¾æ¨¡å‹çš„é¢„æµ‹æ½œåŠ›ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§è‡ªå›å½’å»ºæ¨¡ (autoregressive modeling) èŒƒå¼ï¼Œå°†æµè¡Œç—…é¢„æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºä¸‹æ–‡æ ‡è®°é¢„æµ‹ (next-token prediction)ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„å¤šæ­¥é¢„æµ‹å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æ—¶ç©ºæç¤ºå­¦ä¹  (spatio-temporal prompt learning) æŠ€æœ¯ï¼Œä»æ•°æ®é©±åŠ¨çš„è§’åº¦è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹å¯¹æµè¡Œç—…è¶‹åŠ¿çš„æ„ŸçŸ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEpiLLM åœ¨çœŸå®ä¸–ç•Œçš„ COVID-19 æ•°æ®é›†ä¸Šæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ç‰¹æœ‰çš„è§„æ¨¡åŒ–æ•ˆåº” (scaling behavior)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12738v1",
      "published_date": "2025-05-19 05:53:25 UTC",
      "updated_date": "2025-05-19 05:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:30.710598+00:00"
    },
    {
      "arxiv_id": "2505.12737v2",
      "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning",
      "title_zh": "é’ˆå¯¹ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„é€‰é¡¹æ„ŸçŸ¥æ—¶åºæŠ½è±¡ä»·å€¼",
      "authors": [
        "Hongjoon Ahn",
        "Heewoong Choi",
        "Jisu Han",
        "Taesup Moon"
      ],
      "abstract": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm in which goal-reaching policies are trained from abundant state-action trajectory datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. Identifying the root cause of this challenge, we observe the following insight. Firstly, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Secondly, when learning the high-level policy in the long-horizon regime, the sign of the advantage estimate frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage estimate for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, our approach contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy learned using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹  (Offline Goal-Conditioned Reinforcement Learning, GCRL) åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Option-aware Temporally Abstracted value learning (ç®€ç§° OTA) æ¡†æ¶ã€‚ä½œè€…å‘ç°æ€§èƒ½ç“¶é¢ˆä¸»è¦æºäºé«˜å±‚ç­–ç•¥éš¾ä»¥ç”Ÿæˆåˆé€‚çš„å­ç›®æ ‡ï¼Œä¸”åœ¨é•¿ç¨‹åœºæ™¯ä¸‹ä¼˜åŠ¿ä¼°è®¡ (advantage estimate) çš„ç¬¦å·ææ˜“å‡ºé”™ã€‚ä¸ºæ­¤ï¼ŒOTA åˆ›æ–°æ€§åœ°å°†æ—¶é—´æŠ½è±¡ (temporal abstraction) å¼•å…¥æ—¶é—´å·®åˆ†å­¦ä¹  (temporal-difference learning) è¿‡ç¨‹ï¼Œé€šè¿‡é€‰é¡¹æ„ŸçŸ¥çš„ä»·å€¼æ›´æ–°æœºåˆ¶ç¼©å‡äº†æœ‰æ•ˆæ—¶åŸŸé•¿åº¦ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†é•¿ç¨‹ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¼°è®¡è´¨é‡ï¼Œä½¿å¾—é«˜å±‚ç­–ç•¥çš„è®­ç»ƒæ›´åŠ é«˜æ•ˆã€‚å®éªŒè¯æ˜ï¼ŒåŸºäº OTA ä»·å€¼å‡½æ•°çš„é«˜å±‚ç­–ç•¥åœ¨ OGBench åŸºå‡†çš„è¿·å®«å¯¼èˆªå’Œè§†è§‰æœºå™¨äººæ“çºµç­‰å¤æ‚ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12737v2",
      "published_date": "2025-05-19 05:51:11 UTC",
      "updated_date": "2025-11-04 02:26:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:34.472687+00:00"
    },
    {
      "arxiv_id": "2505.12734v1",
      "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation",
      "title_zh": "SounDiTï¼šåœ°ç†è¯­å¢ƒä¸‹çš„å£°æ™¯åˆ°åœ°æ™¯ç”Ÿæˆ",
      "authors": [
        "Junbo Wang",
        "Haofeng Tan",
        "Bowen Liao",
        "Albert Jiang",
        "Teng Fei",
        "Qixing Huang",
        "Zhengzhong Tu",
        "Shan Ye",
        "Yuhao Kang"
      ],
      "abstract": "We present a novel and practically significant problem-Geo-Contextual Soundscape-to-Landscape (GeoS2L) generation-which aims to synthesize geographically realistic landscape images from environmental soundscapes. Prior audio-to-image generation methods typically rely on general-purpose datasets and overlook geographic and environmental contexts, resulting in unrealistic images that are misaligned with real-world environmental settings. To address this limitation, we introduce a novel geo-contextual computational framework that explicitly integrates geographic knowledge into multimodal generative modeling. We construct two large-scale geo-contextual multimodal datasets, SoundingSVI and SonicUrban, pairing diverse soundscapes with real-world landscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based model that incorporates geo-contextual scene conditioning to synthesize geographically coherent landscape images. Furthermore, we propose a practically-informed geo-contextual evaluation framework, the Place Similarity Score (PSS), across element-, scene-, and human perception-levels to measure consistency between input soundscapes and generated landscape images. Extensive experiments demonstrate that SounDiT outperforms existing baselines in both visual fidelity and geographic settings. Our work not only establishes foundational benchmarks for GeoS2L generation but also highlights the importance of incorporating geographic domain knowledge in advancing multimodal generative models, opening new directions at the intersection of generative AI, geography, urban planning, and environmental sciences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºGeo-Contextual Soundscape-to-Landscape (GeoS2L)ç”Ÿæˆçš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨ä»ç¯å¢ƒå£°æ™¯ä¸­åˆæˆå…·æœ‰åœ°ç†çœŸå®æ„Ÿçš„æ™¯è§‚å›¾åƒã€‚é’ˆå¯¹ä¼ ç»ŸéŸ³é¢‘è½¬å›¾åƒæ–¹æ³•å› å¿½è§†åœ°ç†ç¯å¢ƒèƒŒæ™¯è€Œå¯¼è‡´ç”Ÿæˆå›¾åƒå¤±çœŸçš„é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªå°†åœ°ç†çŸ¥è¯†æ˜¾å¼é›†æˆåˆ°å¤šæ¨¡æ€ç”Ÿæˆå»ºæ¨¡ä¸­çš„è®¡ç®—æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†SoundingSVIå’ŒSonicUrbanä¸¤ä¸ªå¤§è§„æ¨¡åœ°ç†èƒŒæ™¯å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†åŸºäºDiffusion Transformer (DiT)çš„SounDiTæ¨¡å‹ï¼Œåˆ©ç”¨åœ°ç†èƒŒæ™¯åœºæ™¯è°ƒèŠ‚å®ç°åœ°ç†ä¸€è‡´æ€§çš„å›¾åƒåˆæˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åä¸ºPlace Similarity Score (PSS)çš„è¯„ä¼°æ¡†æ¶ï¼Œä»å…ƒç´ ã€åœºæ™¯å’Œäººç±»æ„ŸçŸ¥ç»´åº¦è¡¡é‡å£°æ™¯ä¸ç”Ÿæˆå›¾åƒçš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒSounDiTåœ¨è§†è§‰ä¿çœŸåº¦å’Œåœ°ç†åŒ¹é…åº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºGeoS2Lç”Ÿæˆå¥ å®šäº†åŸºç¡€ï¼Œä¹Ÿä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€åœ°ç†å­¦å’ŒåŸå¸‚è§„åˆ’çš„è·¨å­¦ç§‘ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.GR",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12734v1",
      "published_date": "2025-05-19 05:47:13 UTC",
      "updated_date": "2025-05-19 05:47:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:39.371024+00:00"
    },
    {
      "arxiv_id": "2505.12731v2",
      "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps",
      "title_zh": "é€šè¿‡æŒ‡ä»¤é©±åŠ¨çš„æ£€ç´¢é‡å è¡¨ç¤ºç¼©å‡åŠ é€Ÿè‡ªé€‚åº”æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Jie Ou",
        "Jinyu Guo",
        "Shuaihong Jiang",
        "Zhaokun Wang",
        "Libo Qin",
        "Shunyu Yao",
        "Wenhong Tian"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for expanding the knowledge of large language models. To handle complex queries more effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the generated quality through multiple interactions with external knowledge bases. Despite its effectiveness, A-RAG exacerbates the pre-existing efficiency challenges inherent in RAG, which are attributable to its reliance on multiple iterations of generation. Existing A-RAG approaches process all retrieved contents from scratch. However, they ignore the situation where there is a significant overlap in the content of the retrieval results across rounds. The overlapping content is redundantly represented, which leads to a large proportion of repeated computations, thus affecting the overall efficiency. To address this issue, this paper introduces a model-agnostic approach that can be generally applied to A-RAG methods, which is dedicated to reducing the redundant representation process caused by the overlapping of retrieval results. Specifically, we use cache access and parallel generation to speed up the prefilling and decoding stages respectively. Additionally, we also propose an instruction-driven module to further guide the model to more effectively attend to each part of the content in a more suitable way for LLMs. Experiments show that our approach achieves 2.79 and 2.33 times significant acceleration on average for prefilling and decoding respectively while maintaining equal generation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªé€‚åº”æ£€ç´¢å¢å¼ºç”Ÿæˆ(Adaptive-RAG)åœ¨å¤šè½®äº¤äº’ä¸­å› é‡å¤è®¡ç®—å¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å‡å°‘æ£€ç´¢é‡å (retrieval overlaps)å†—ä½™è¡¨ç¤ºæ¥åŠ é€Ÿç”Ÿæˆçš„é€šç”¨æ–¹æ³•ã€‚ä½œè€…å‘ç°ç°æœ‰ A-RAG æ–¹æ³•å¿½ç•¥äº†å„è½®æ£€ç´¢ç»“æœä¹‹é—´å­˜åœ¨çš„å¤§é‡å†…å®¹é‡å ï¼Œå¯¼è‡´äº†ä¸¥é‡çš„è®¡ç®—èµ„æºæµªè´¹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨äº†ç¼“å­˜è®¿é—®(cache access)å’Œå¹¶è¡Œç”Ÿæˆ(parallel generation)æŠ€æœ¯ï¼Œåˆ†åˆ«é’ˆå¯¹é¢„å¡«å……(prefilling)å’Œè§£ç (decoding)é˜¶æ®µè¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†æŒ‡ä»¤é©±åŠ¨æ¨¡å—(instruction-driven module)ï¼Œä»¥æ›´é€‚é…å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–¹å¼å¼•å¯¼å…¶é«˜æ•ˆå¤„ç†æ£€ç´¢å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æŸå¤±ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é¢„å¡«å……é˜¶æ®µå¹³å‡ 2.79 å€ä»¥åŠè§£ç é˜¶æ®µå¹³å‡ 2.33 å€çš„æ˜¾è‘—åŠ é€Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at Findings of ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12731v2",
      "published_date": "2025-05-19 05:39:38 UTC",
      "updated_date": "2025-05-25 13:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:40.795003+00:00"
    },
    {
      "arxiv_id": "2505.12716v3",
      "title": "Shadow-FT: Tuning Instruct Model via Training on Paired Base Model",
      "title_zh": "Shadow-FTï¼šé€šè¿‡é…å¯¹åŸºåº§æ¨¡å‹è®­ç»ƒå®ç°æŒ‡ä»¤æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Taiqiang Wu",
        "Runming Yang",
        "Jiayi Li",
        "Pengfei Hu",
        "Yik-Chung Wu",
        "Ngai Wong",
        "Yujiu Yang"
      ],
      "abstract": "Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the Instruct (i.e., instruction-tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired Base models, the foundation for these Instruct variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). The Base model tends to be a good learner yet a weak backbone without post-training. Therefore, we propose a novel Shadow-FT framework to tune the Instruct models by leveraging the corresponding Base models. The key insight is to fine-tune the Base model, and then \\textit{directly} graft the learned weight updates to the Instruct model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization~(DPO). Codes and weights are available at \\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Shadow-FT æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç›´æ¥å¾®è°ƒ Instruct æ¨¡å‹æ—¶å¸¸å‡ºç°çš„æ€§èƒ½æå‡å¾®å¼±æˆ–é€€åŒ–é—®é¢˜ã€‚ç ”ç©¶è€…è§‚å¯Ÿåˆ° Base æ¨¡å‹ä¸å…¶å¯¹åº”çš„ Instruct å˜ä½“åœ¨æƒé‡ä¸Šé«˜åº¦ç›¸ä¼¼ï¼Œä¸” Base æ¨¡å‹å…·å¤‡æå¼ºçš„å­¦ä¹ æ½œåŠ›ã€‚Shadow-FT çš„æ ¸å¿ƒæ€æƒ³æ˜¯å…ˆå¯¹ Base æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œéšåå°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥â€œå«æ¥â€åˆ° Instruct æ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•ä¸å¼•å…¥é¢å¤–å‚æ•°ï¼Œå®ç°ç®€å•ï¼Œä¸”åœ¨ Qwen 3 å’Œ Llama 3 ç³»åˆ—æ¨¡å‹çš„ 19 ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¶µç›–äº†ç¼–ç ã€æ¨ç†å’Œæ•°å­¦ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShadow-FT åœ¨æ€§èƒ½ä¸Šä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒå’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥åˆ†æè¿˜è¯æ˜äº† Shadow-FT å¯ä»¥æ‰©å±•è‡³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå¹¶èƒ½ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æœ‰æ•ˆç»“åˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 12 tables, 8 figures. Previous name: Shadow-FT: Tuning Instruct via Base",
      "pdf_url": "https://arxiv.org/pdf/2505.12716v3",
      "published_date": "2025-05-19 05:16:21 UTC",
      "updated_date": "2025-09-26 03:43:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:42.157751+00:00"
    },
    {
      "arxiv_id": "2505.12711v2",
      "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining",
      "title_zh": "åŸºäºä¸‰å…ƒç»„å¤šæ¨¡æ€é¢„è®­ç»ƒçš„è®¡ç®—ç—…ç†å­¦â€œä»»æ„å¯¹ä»»æ„â€å­¦ä¹ ",
      "authors": [
        "Qichen Sun",
        "Zhengrui Guo",
        "Rui Peng",
        "Hao Chen",
        "Jinzhuo Wang"
      ],
      "abstract": "Recent advances in computational pathology and artificial intelligence have significantly enhanced the utilization of gigapixel whole-slide images and and additional modalities (e.g., genomics) for pathological diagnosis. Although deep learning has demonstrated strong potential in pathology, several key challenges persist: (1) fusing heterogeneous data types requires sophisticated strategies beyond simple concatenation due to high computational costs; (2) common scenarios of missing modalities necessitate flexible strategies that allow the model to learn robustly in the absence of certain modalities; (3) the downstream tasks in CPath are diverse, ranging from unimodal to multimodal, cnecessitating a unified model capable of handling all modalities. To address these challenges, we propose ALTER, an any-to-any tri-modal pretraining framework that integrates WSIs, genomics, and pathology reports. The term \"any\" emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with any subset of modalities, and its capacity to learn robust, cross-modal representations beyond WSI-centric approaches. We evaluate ALTER across extensive clinical tasks including survival prediction, cancer subtyping, gene mutation prediction, and report generation, achieving superior or comparable performance to state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ALTERï¼Œä¸€ç§ç”¨äºè®¡ç®—ç—…ç†å­¦(Computational Pathology)çš„Any-to-Anyä¸‰æ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆæ•´åˆå…¨åˆ‡ç‰‡å›¾åƒ(WSIs)ã€åŸºå› ç»„å­¦(Genomics)å’Œç—…ç†æŠ¥å‘Šã€‚é’ˆå¯¹å¼‚æ„æ•°æ®èåˆå¤æ‚ã€æ¨¡æ€ç¼ºå¤±ä»¥åŠä¸‹æ¸¸ä»»åŠ¡å¤šæ ·åŒ–ç­‰æŒ‘æˆ˜ï¼ŒALTERé‡‡ç”¨äº†æ¨¡æ€è‡ªé€‚åº”(Modality-adaptive)è®¾è®¡ï¼Œä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨ä»»æ„æ¨¡æ€å­é›†è¿›è¡Œçµæ´»çš„é¢„è®­ç»ƒã€‚é€šè¿‡ä¸‰å…ƒç»„å¤šæ¨¡æ€é¢„è®­ç»ƒ(Triplet Multimodal Pretraining)æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶æˆåŠŸæ„å»ºäº†é²æ£’çš„è·¨æ¨¡æ€è¡¨å¾ï¼Œå…‹æœäº†ä¼ ç»Ÿä»¥WSIä¸ºä¸­å¿ƒæ–¹æ³•çš„å±€é™æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ç”Ÿå­˜é¢„æµ‹(Survival Prediction)ã€ç™Œç—‡åˆ†å‹(Cancer Subtyping)ã€åŸºå› çªå˜é¢„æµ‹å’ŒæŠ¥å‘Šç”Ÿæˆç­‰å¤šé¡¹ä¸´åºŠä»»åŠ¡ä¸Šå¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALTERåœ¨å„é¡¹ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜äºæˆ–å¯æ¯”äºå½“å‰æœ€å…ˆè¿›åŸºå‡†æ¨¡å‹(SOTA)çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€ç—…ç†æ•°æ®æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12711v2",
      "published_date": "2025-05-19 05:07:34 UTC",
      "updated_date": "2025-05-20 12:57:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:47.098304+00:00"
    },
    {
      "arxiv_id": "2505.13551v2",
      "title": "Counter-Inferential Behavior in Natural and Artificial Cognitive Systems",
      "title_zh": "è‡ªç„¶ä¸äººå·¥è®¤çŸ¥ç³»ç»Ÿä¸­çš„åæ¨æ–­è¡Œä¸º",
      "authors": [
        "Serge Dolgikh"
      ],
      "abstract": "This study explores the emergence of counter-inferential behavior in natural and artificial cognitive systems, that is, patterns in which agents misattribute empirical success or suppress adaptation, leading to epistemic rigidity or maladaptive stability. We analyze archetypal scenarios in which such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. Rather than arising from noise or flawed design, these behaviors emerge through structured interactions between internal information models, empirical feedback, and higher-order evaluation mechanisms. Drawing on evidence from artificial systems, biological cognition, human psychology, and social dynamics, we identify counter-inferential behavior as a general cognitive vulnerability that can manifest even in otherwise well-adapted systems. The findings highlight the importance of preserving minimal adaptive activation under stable conditions and suggest design principles for cognitive architectures that can resist rigidity under informational stress.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç„¶ä¸äººå·¥è®¤çŸ¥ç³»ç»Ÿä¸­å‡ºç°çš„ Counter-Inferential Behavior (é€†æ¨æ–­è¡Œä¸º)ï¼Œå³æ™ºèƒ½ä½“å› é”™è¯¯å½’å› ç»éªŒæ€§æˆåŠŸæˆ–æŠ‘åˆ¶é€‚åº”è¿‡ç¨‹ï¼Œè¿›è€Œå¯¼è‡´ Epistemic Rigidity (è®¤è¯†åƒµåŒ–) æˆ– Maladaptive Stability (é€‚åº”ä¸è‰¯çš„ç¨³å®šæ€§)ã€‚é€šè¿‡åˆ†æå¥–åŠ±ä¸å¹³è¡¡ã€å…ƒè®¤çŸ¥å½’å› åå·®åŠä¿æŠ¤æ€§é‡æ„ç­‰å…¸å‹åœºæ™¯ï¼Œç ”ç©¶æŒ‡å‡ºè¿™äº›è¡Œä¸ºå¹¶éæºäºéšæœºå™ªå£°æˆ–è®¾è®¡ç¼ºé™·ï¼Œè€Œæ˜¯å†…éƒ¨ä¿¡æ¯æ¨¡å‹ã€ç»éªŒåé¦ˆä¸é«˜é˜¶è¯„ä¼°æœºåˆ¶ä¹‹é—´ç»“æ„åŒ–äº¤äº’çš„äº§ç‰©ã€‚ç ”ç©¶è·¨è¶Šäººå·¥æ™ºèƒ½ã€ç”Ÿç‰©è®¤çŸ¥ã€äººç±»å¿ƒç†å­¦åŠç¤¾ä¼šåŠ¨æ€é¢†åŸŸï¼Œå°†é€†æ¨æ–­è¡Œä¸ºè¯†åˆ«ä¸ºä¸€ç§å³ä¾¿åœ¨é€‚åº”è‰¯å¥½çš„ç³»ç»Ÿä¸­ä¹Ÿä¼šè¡¨ç°å‡ºçš„é€šç”¨è®¤çŸ¥è„†å¼±æ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨ç¨³å®šæ¡ä»¶ä¸‹ä¿æŒ Minimal Adaptive Activation (æœ€å°é€‚åº”æ€§æ¿€æ´») çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºèƒ½å¤ŸæŠµæŠ—ä¿¡æ¯å‹åŠ›ä¸‹åƒµåŒ–è¶‹åŠ¿çš„è®¤çŸ¥æ¶æ„æä¾›äº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€‚",
      "categories": [
        "cs.AI",
        "cs.NE",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.13551v2",
      "published_date": "2025-05-19 05:04:07 UTC",
      "updated_date": "2025-06-09 04:19:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:51.570436+00:00"
    },
    {
      "arxiv_id": "2505.12707v1",
      "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI",
      "title_zh": "PLAICraftï¼šé¢å‘å…·èº«æ™ºèƒ½çš„å¤§è§„æ¨¡æ—¶é—´å¯¹é½è§†è§‰-è¯­éŸ³-åŠ¨ä½œæ•°æ®é›†",
      "authors": [
        "Yingchen He",
        "Christian D. Weilbach",
        "Martyna E. Wojciechowska",
        "Yuxuan Zhang",
        "Frank Wood"
      ],
      "abstract": "Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\\footnote{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PLAICraftï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³Embodied AIé¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ã€å®æ—¶ä¸”å¤šæ¨¡æ€ç¤¾äº¤äº’åŠ¨æ•°æ®é›†é—®é¢˜çš„åˆ›æ–°å¹³å°åŠæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ•æ‰äº†Minecraftå¤šäººäº¤äº’ä¸­çš„Videoã€æ¸¸æˆè¾“å‡ºéŸ³é¢‘ã€éº¦å…‹é£è¾“å…¥éŸ³é¢‘ã€é¼ æ ‡å’Œé”®ç›˜åŠ¨ä½œå…±äº”ç§Time-alignedæ¨¡æ€ã€‚æ¯ç§æ¨¡æ€å‡ä»¥æ¯«ç§’çº§ç²¾åº¦è®°å½•ï¼Œæ”¯æŒåœ¨å¤æ‚çš„å¼€æ”¾ä¸–ç•Œä¸­ç ”ç©¶åŒæ­¥çš„å…·èº«è¡Œä¸ºã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå…¨çƒ10,000å¤šåå‚ä¸è€…çš„è¶…è¿‡10,000å°æ—¶æ¸¸æˆæ—¶é•¿ï¼Œæå¤§åœ°æ‰©å±•äº†ç°æœ‰çš„æ•°æ®è§„æ¨¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€å¥—è¯„ä¼°ç»„ä»¶ï¼Œç”¨äºåŸºå‡†æµ‹è¯•æ¨¡å‹åœ¨Object recognitionã€Spatial awarenessã€Language groundingå’ŒLong-term memoryæ–¹é¢çš„èƒ½åŠ›ã€‚PLAICraftä¸ºè®­ç»ƒå’Œè¯„ä¼°èƒ½å¤Ÿå®æ—¶ã€æµåˆ©ä¸”æœ‰ç›®çš„åœ°è¡ŒåŠ¨çš„æ™ºèƒ½ä½“å¼€è¾Ÿäº†é“è·¯ï¼Œä¸ºå®ç°çœŸæ­£çš„Embodied AIå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12707v1",
      "published_date": "2025-05-19 05:00:47 UTC",
      "updated_date": "2025-05-19 05:00:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:54.168717+00:00"
    },
    {
      "arxiv_id": "2505.12705v2",
      "title": "DreamGen: Unlocking Generalization in Robot Learning through Video World Models",
      "title_zh": "DreamGenï¼šåŸºäºè§†é¢‘ä¸–ç•Œæ¨¡å‹è§£é”æœºå™¨äººå­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Joel Jang",
        "Seonghyeon Ye",
        "Zongyu Lin",
        "Jiannan Xiang",
        "Johan Bjorck",
        "Yu Fang",
        "Fengyuan Hu",
        "Spencer Huang",
        "Kaushil Kundalia",
        "Yen-Chen Lin",
        "Loic Magne",
        "Ajay Mandlekar",
        "Avnish Narayan",
        "You Liang Tan",
        "Guanzhi Wang",
        "Jing Wang",
        "Qi Wang",
        "Yinzhen Xu",
        "Xiaohui Zeng",
        "Kaiyuan Zheng",
        "Ruijie Zheng",
        "Ming-Yu Liu",
        "Luke Zettlemoyer",
        "Dieter Fox",
        "Jan Kautz",
        "Scott Reed",
        "Yuke Zhu",
        "Linxi Fan"
      ],
      "abstract": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for training robot policies that generalize across behaviors and environments through neural trajectories - synthetic robot data generated from video world models. DreamGen leverages state-of-the-art image-to-video generative models, adapting them to the target robot embodiment to produce photorealistic synthetic videos of familiar or novel tasks in diverse environments. Since these models generate only videos, we recover pseudo-action sequences using either a latent action model or an inverse-dynamics model (IDM). Despite its simplicity, DreamGen unlocks strong behavior and environment generalization: a humanoid robot can perform 22 new behaviors in both seen and unseen environments, while requiring teleoperation data from only a single pick-and-place task in one environment. To evaluate the pipeline systematically, we introduce DreamGen Bench, a video generation benchmark that shows a strong correlation between benchmark performance and downstream policy success. Our work establishes a promising new axis for scaling robot learning well beyond manual data collection. Code available at https://github.com/NVIDIA/GR00T-Dreams.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DreamGenï¼Œä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„ 4-stage pipelineï¼Œé€šè¿‡ video world models ç”Ÿæˆçš„åˆæˆæœºå™¨äººæ•°æ®ï¼ˆå³ neural trajectoriesï¼‰æ¥è®­ç»ƒå…·å¤‡è·¨è¡Œä¸ºå’Œè·¨ç¯å¢ƒæ³›åŒ–èƒ½åŠ›çš„æœºå™¨äººç­–ç•¥ã€‚DreamGen åˆ©ç”¨å…ˆè¿›çš„ image-to-video ç”Ÿæˆæ¨¡å‹å¹¶å°†å…¶é€‚é…åˆ°ç‰¹å®šçš„æœºå™¨äººå®ä½“ï¼Œä»¥åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­ç”Ÿæˆä»»åŠ¡çš„é«˜ä¿çœŸåˆæˆè§†é¢‘ã€‚é’ˆå¯¹ç”Ÿæˆæ¨¡å‹ä»…è¾“å‡ºè§†é¢‘çš„é—®é¢˜ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨ latent action model æˆ– inverse-dynamics model (IDM) æ¢å¤ä¼ªåŠ¨ä½œåºåˆ—ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamGen æ˜¾è‘—æå‡äº†æœºå™¨äººçš„è¡Œä¸ºå’Œç¯å¢ƒæ³›åŒ–èƒ½åŠ›ï¼Œäººå½¢æœºå™¨äººä»…éœ€å•ä¸€ç¯å¢ƒä¸‹çš„ pick-and-place ä»»åŠ¡é¥æ“ä½œæ•°æ®ï¼Œå³å¯åœ¨å·²çŸ¥å’ŒæœªçŸ¥ç¯å¢ƒä¸­æ‰§è¡Œ 22 ç§æ–°è¡Œä¸ºã€‚ç ”ç©¶è¿˜å¼•å…¥äº† DreamGen Bench åŸºå‡†æµ‹è¯•ï¼Œè¯å®äº†è§†é¢‘ç”Ÿæˆæ€§èƒ½ä¸ä¸‹æ¸¸ç­–ç•¥æˆåŠŸç‡ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚è¯¥å·¥ä½œä¸ºçªç ´æ‰‹å·¥æ•°æ®é‡‡é›†é™åˆ¶ã€å®ç°æœºå™¨äººå­¦ä¹ çš„å¤§è§„æ¨¡æ‰©å±•æä¾›äº†æå…·å‰æ™¯çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "See website for videos: https://research.nvidia.com/labs/gear/dreamgen",
      "pdf_url": "https://arxiv.org/pdf/2505.12705v2",
      "published_date": "2025-05-19 04:55:39 UTC",
      "updated_date": "2025-06-17 22:33:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:00:58.105354+00:00"
    },
    {
      "arxiv_id": "2505.12703v1",
      "title": "SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence",
      "title_zh": "SpatialLLMï¼šä»å¤šæ¨¡æ€æ•°æ®åˆ°åŸå¸‚ç©ºé—´æ™ºèƒ½",
      "authors": [
        "Jiabin Chen",
        "Haiping Wang",
        "Jinpeng Li",
        "Yuan Liu",
        "Zhen Dong",
        "Bisheng Yang"
      ],
      "abstract": "We propose SpatialLLM, a novel approach advancing spatial intelligence tasks in complex urban scenes. Unlike previous methods requiring geographic analysis tools or domain expertise, SpatialLLM is a unified language model directly addressing various spatial intelligence tasks without any training, fine-tuning, or expert intervention. The core of SpatialLLM lies in constructing detailed and structured scene descriptions from raw spatial data to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show that, with our designs, pretrained LLMs can accurately perceive spatial distribution information and enable zero-shot execution of advanced spatial intelligence tasks, including urban planning, ecological analysis, traffic management, etc. We argue that multi-field knowledge, context length, and reasoning ability are key factors influencing LLM performances in urban analysis. We hope that SpatialLLM will provide a novel viable perspective for urban intelligent analysis and management. The code and dataset are available at https://github.com/WHU-USI3DV/SpatialLLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpatialLLMï¼Œä¸€ç§æ—¨åœ¨æ¨è¿›å¤æ‚åŸå¸‚åœºæ™¯ä¸­ç©ºé—´æ™ºèƒ½ä»»åŠ¡çš„æ–°æ–¹æ³•ã€‚ä¸ä»¥å¾€éœ€è¦åœ°ç†åˆ†æå·¥å…·æˆ–é¢†åŸŸä¸“å®¶çŸ¥è¯†çš„æ–¹æ³•ä¸åŒï¼ŒSpatialLLMä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒã€å¾®è°ƒæˆ–ä¸“å®¶å¹²é¢„å³å¯ç›´æ¥å¤„ç†å„ç§ç©ºé—´æ™ºèƒ½ä»»åŠ¡ã€‚å…¶æ ¸å¿ƒåœ¨äºä»åŸå§‹ç©ºé—´æ•°æ®ä¸­æ„å»ºè¯¦ç»†ä¸”ç»“æ„åŒ–çš„åœºæ™¯æè¿°ï¼Œä»¥æ­¤æç¤ºé¢„è®­ç»ƒçš„LLMsè¿›è¡ŒåŸºäºåœºæ™¯çš„åˆ†æã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œé€šè¿‡è¯¥è®¾è®¡ï¼Œé¢„è®­ç»ƒçš„LLMsèƒ½å¤Ÿå‡†ç¡®æ„ŸçŸ¥ç©ºé—´åˆ†å¸ƒä¿¡æ¯ï¼Œå¹¶æ”¯æŒåœ¨åŸå¸‚è§„åˆ’ã€ç”Ÿæ€åˆ†æã€äº¤é€šç®¡ç†ç­‰é¢†åŸŸå®ç°é«˜çº§ç©ºé—´æ™ºèƒ½ä»»åŠ¡çš„Zero-shotæ‰§è¡Œã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œå¤šé¢†åŸŸçŸ¥è¯†ã€ä¸Šä¸‹æ–‡é•¿åº¦(context length)å’Œæ¨ç†èƒ½åŠ›æ˜¯å½±å“LLMsåœ¨åŸå¸‚åˆ†æè¡¨ç°çš„å…³é”®å› ç´ ã€‚è¯¥å·¥ä½œä¸ºåŸå¸‚æ™ºèƒ½åˆ†æä¸ç®¡ç†æä¾›äº†æ–°é¢–ä¸”å¯è¡Œçš„è§†è§’ï¼Œå¹¶å¼€æºäº†ç›¸å…³çš„ä»£ç ä¸æ•°æ®é›†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12703v1",
      "published_date": "2025-05-19 04:53:41 UTC",
      "updated_date": "2025-05-19 04:53:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:12.304484+00:00"
    },
    {
      "arxiv_id": "2505.13550v1",
      "title": "JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation",
      "title_zh": "JIR-Arenaï¼šé¦–ä¸ªé¢å‘å³æ—¶ä¿¡æ¯æ¨èçš„åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Ke Yang",
        "Kevin Ros",
        "Shankar Kumar Senthil Kumar",
        "ChengXiang Zhai"
      ],
      "abstract": "Just-in-time Information Recommendation (JIR) is a service designed to deliver the most relevant information precisely when users need it, , addressing their knowledge gaps with minimal effort and boosting decision-making and efficiency in daily life. Advances in device-efficient deployment of foundation models and the growing use of intelligent wearable devices have made always-on JIR assistants feasible. However, there has been no systematic effort to formally define JIR tasks or establish evaluation frameworks. To bridge this gap, we present the first mathematical definition of JIR tasks and associated evaluation metrics. Additionally, we introduce JIR-Arena, a multimodal benchmark dataset featuring diverse, information-request-intensive scenarios to evaluate JIR systems across critical dimensions: i) accurately inferring user information needs, ii) delivering timely and relevant recommendations, and iii) avoiding irrelevant content that may distract users.\n  Developing a JIR benchmark dataset poses challenges due to subjectivity in estimating user information needs and uncontrollable system variables affecting reproducibility. To address these, JIR-Arena: i) combines input from multiple humans and large AI models to approximate information need distributions; ii) assesses JIR quality through information retrieval outcomes using static knowledge base snapshots; and iii) employs a multi-turn, multi-entity validation framework to improve objectivity and generality. Furthermore, we implement a baseline JIR system capable of processing real-time information streams aligned with user inputs. Our evaluation of this baseline system on JIR-Arena indicates that while foundation model-based JIR systems simulate user needs with reasonable precision, they face challenges in recall and effective content retrieval. To support future research in this new area, we fully release our code and data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å³æ—¶ä¿¡æ¯æ¨è(Just-in-time Information Recommendation, JIR)ç¼ºä¹å½¢å¼åŒ–å®šä¹‰å’Œè¯„ä¼°æ¡†æ¶çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ª JIR ä»»åŠ¡çš„æ•°å­¦å®šä¹‰åŠç›¸å…³çš„è¯„ä»·æŒ‡æ ‡ã€‚ä½œè€…æ¨å‡ºäº† JIR-Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§ä¿¡æ¯éœ€æ±‚å¯†é›†å‹åœºæ™¯çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨ä»ç”¨æˆ·éœ€æ±‚æ¨æ–­ã€æ¨èåŠæ—¶æ€§ä¸ç›¸å…³æ€§ã€ä»¥åŠé¿å…æ— å…³ä¿¡æ¯å¹²æ‰°ç­‰ç»´åº¦å¯¹ JIR ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†å…‹æœç”¨æˆ·éœ€æ±‚è¯„ä¼°ä¸­çš„ä¸»è§‚æ€§å’Œä¸å¯æ§å˜é‡ï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†å¤šäººä¸å¤§å‹ AI æ¨¡å‹çš„è¾“å…¥ï¼Œå¹¶é‡‡ç”¨å¤šè½®ã€å¤šå®ä½“éªŒè¯æ¡†æ¶ä»¥æå‡å®¢è§‚æ€§ä¸é€šç”¨æ€§ã€‚ç ”ç©¶è¿˜å®ç°äº†ä¸€ä¸ªåŸºå‡† JIR ç³»ç»Ÿï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶åŸºäºåŸºç¡€æ¨¡å‹(foundation models)çš„ç³»ç»Ÿèƒ½å¤Ÿè¾ƒç²¾å‡†åœ°æ¨¡æ‹Ÿç”¨æˆ·éœ€æ±‚ï¼Œä½†åœ¨å¬å›ç‡(recall)å’Œæœ‰æ•ˆå†…å®¹æ£€ç´¢æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ€åï¼Œè¯¥é¡¹ç›®å®Œå…¨å¼€æºäº†ä»£ç å’Œæ•°æ®ï¼Œä¸ºåç»­æ™ºèƒ½ç©¿æˆ´è®¾å¤‡å’Œå…¨å¤©å€™ JIR åŠ©æ‰‹çš„ç ”ç©¶æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13550v1",
      "published_date": "2025-05-19 04:49:47 UTC",
      "updated_date": "2025-05-19 04:49:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:18.241102+00:00"
    },
    {
      "arxiv_id": "2506.01994v1",
      "title": "Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers",
      "title_zh": "Re-experiment Smartï¼šä¸€ç§å¢å¼ºç¯æ°§èšåˆç‰©åŠ›å­¦æ€§èƒ½æ•°æ®é©±åŠ¨é¢„æµ‹çš„æ–°æ–¹æ³•",
      "authors": [
        "Wanshan Cui",
        "Yejin Jeong",
        "Inwook Song",
        "Gyuri Kim",
        "Minsang Kwon",
        "Donghun Lee"
      ],
      "abstract": "Accurate prediction of polymer material properties through data-driven approaches greatly accelerates novel material development by reducing redundant experiments and trial-and-error processes. However, inevitable outliers in empirical measurements can severely skew machine learning results, leading to erroneous prediction models and suboptimal material designs. To address this limitation, we propose a novel approach to enhance dataset quality efficiently by integrating multi-algorithm outlier detection with selective re-experimentation of unreliable outlier cases. To validate the empirical effectiveness of the approach, we systematically construct a new dataset containing 701 measurements of three key mechanical properties: glass transition temperature ($T_g$), tan $Î´$ peak, and crosslinking density ($v_{c}$). To demonstrate its general applicability, we report the performance improvements across multiple machine learning models, including Elastic Net, SVR, Random Forest, and TPOT, to predict the three key properties. Our method reliably reduces prediction error (RMSE) and significantly improves accuracy with minimal additional experimental work, requiring only about 5% of the dataset to be re-measured. These findings highlight the importance of data quality enhancement in achieving reliable machine learning applications in polymer science and present a scalable strategy for improving predictive reliability in materials science.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èšåˆç‰©ææ–™å®éªŒæ•°æ®ä¸­å­˜åœ¨çš„å¼‚å¸¸å€¼(outliers)å¯¼è‡´çš„é¢„æµ‹æ¨¡å‹åå·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRe-experiment Smartçš„æ–°å‹æ•°æ®å¢å¼ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆå¤šç®—æ³•å¼‚å¸¸æ£€æµ‹(multi-algorithm outlier detection)ä¸é’ˆå¯¹ä¸å¯é æ¡ˆä¾‹çš„é€‰æ‹©æ€§é‡å¤å®éªŒ(selective re-experimentation)ï¼Œåœ¨ä¿è¯æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ•°æ®é›†è´¨é‡ã€‚ç ”ç©¶äººå‘˜ç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«701é¡¹æµ‹é‡çš„å…¨æ–°æ•°æ®é›†ï¼Œæ¶µç›–äº†ç»ç’ƒåŒ–è½¬å˜æ¸©åº¦($T_g$)ã€tan $Î´$å³°å€¼å’Œäº¤è”å¯†åº¦($v_c$)è¿™ä¸‰é¡¹å…³é”®åŠ›å­¦æ€§èƒ½æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Elastic Netã€SVRã€Random Forestå’ŒTPOTç­‰å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ä¸‹ï¼Œè¯¥æ–¹æ³•ä»…éœ€é‡æ–°æµ‹é‡çº¦5%çš„æ•°æ®ï¼Œå³å¯æœ‰æ•ˆé™ä½å‡æ–¹æ ¹è¯¯å·®(RMSE)å¹¶å¤§å¹…æå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ•°æ®è´¨é‡å¢å¼ºå¯¹èšåˆç‰©ç§‘å­¦ä¸­å¯é æœºå™¨å­¦ä¹ åº”ç”¨çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºææ–™ç§‘å­¦é¢†åŸŸæä¾›äº†ä¸€ç§æå…·æ‰©å±•æ€§çš„é¢„æµ‹ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cond-mat.soft",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.soft",
      "comment": "27 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01994v1",
      "published_date": "2025-05-19 04:42:18 UTC",
      "updated_date": "2025-05-19 04:42:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:28.113695+00:00"
    },
    {
      "arxiv_id": "2505.12701v1",
      "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning",
      "title_zh": "é¢å‘è¿ç»­åŠ¨ä½œå¼ºåŒ–å­¦ä¹ çš„åäº‹å®è§£é‡Š",
      "authors": [
        "Shuyang Dong",
        "Shangtong Zhang",
        "Lu Feng"
      ],
      "abstract": "Reinforcement Learning (RL) has shown great promise in domains like healthcare and robotics but often struggles with adoption due to its lack of interpretability. Counterfactual explanations, which address \"what if\" scenarios, provide a promising avenue for understanding RL decisions but remain underexplored for continuous action spaces. We propose a novel approach for generating counterfactual explanations in continuous action RL by computing alternative action sequences that improve outcomes while minimizing deviations from the original sequence. Our approach leverages a distance metric for continuous actions and accounts for constraints such as adhering to predefined policies in specific states. Evaluations in two RL domains, Diabetes Control and Lunar Lander, demonstrate the effectiveness, efficiency, and generalization of our approach, enabling more interpretable and trustworthy RL applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨è¿ç»­åŠ¨ä½œç©ºé—´(continuous action spaces)ä¸­ç¼ºä¹å¯è§£é‡Šæ€§(interpretability)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆåäº‹å®è§£é‡Š(counterfactual explanations)çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—èƒ½å¤Ÿæ”¹å–„ç»“æœä¸”ä¸åŸå§‹åºåˆ—åå·®æœ€å°çš„å¤‡é€‰åŠ¨ä½œåºåˆ—ï¼Œä¸ºç†è§£å¼ºåŒ–å­¦ä¹ å†³ç­–æä¾›äº†â€œå¦‚æœâ€¦â€¦ä¼šæ€æ ·â€çš„åˆ†æè§†è§’ã€‚ç ”ç©¶åˆ©ç”¨äº†é’ˆå¯¹è¿ç»­åŠ¨ä½œè®¾è®¡çš„è·ç¦»åº¦é‡(distance metric)ï¼Œå¹¶èƒ½å¤Ÿå¤„ç†åœ¨ç‰¹å®šçŠ¶æ€ä¸‹éµå¾ªé¢„å®šä¹‰ç­–ç•¥(predefined policies)ç­‰çº¦æŸæ¡ä»¶ã€‚åœ¨ç³–å°¿ç—…æ§åˆ¶(Diabetes Control)å’Œæœˆçƒç€é™†å™¨(Lunar Lander)ä¸¤ä¸ªå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¯„ä¼°ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºæ›´å…·å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by International Joint Conference on Artificial Intelligence (IJCAI) 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12701v1",
      "published_date": "2025-05-19 04:41:54 UTC",
      "updated_date": "2025-05-19 04:41:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:27.685078+00:00"
    },
    {
      "arxiv_id": "2505.12692v1",
      "title": "Bullying the Machine: How Personas Increase LLM Vulnerability",
      "title_zh": "æ¬ºå‡Œæœºå™¨ï¼šäººæ ¼è®¾å®šå¦‚ä½•å¢åŠ å¤§è¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§",
      "authors": [
        "Ziwei Xu",
        "Udit Sanghi",
        "Mohan Kankanhalli"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§’è‰²è®¾å®š (Persona conditioning) æ˜¯å¦ä¼šå½±å“å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é­å—â€œæ¬ºå‡Œâ€ (Bullying) è¿™ä¸€é€šè¿‡å¿ƒç†å‹åŠ›å¼ºåˆ¶å—å®³è€…æœä»çš„å¯¹æŠ—æ€§æ“çºµä¸‹çš„å®‰å…¨æ€§ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿæ¡†æ¶ï¼Œç”±æ”»å‡»è€… LLM åˆ©ç”¨åŸºäºå¿ƒç†å­¦çš„æ¬ºå‡Œç­–ç•¥æ”»å‡»è®¾å®šäº†â€œå¤§äº”äººæ ¼â€ (Big Five personality traits) çš„å—å®³è€…æ¨¡å‹ã€‚å®éªŒå‘ç°ï¼Œç‰¹å®šçš„è§’è‰²é…ç½®ï¼ˆå¦‚è¾ƒä½çš„å®œäººæ€§ Agreeableness æˆ–å°½è´£æ€§ Conscientiousnessï¼‰ä¼šæ˜¾è‘—æé«˜æ¨¡å‹äº§ç”Ÿä¸å®‰å…¨è¾“å‡ºçš„æ˜“æ„Ÿæ€§ã€‚åœ¨å„ç§æ‰‹æ®µä¸­ï¼Œæ¶‰åŠæƒ…æ„Ÿæ“çºµæˆ–è®½åˆºçš„ç­–ç•¥ï¼Œå¦‚ç…¤æ°”ç¯æ•ˆåº” (Gaslighting) å’Œå˜²è®½ (Ridicule)ï¼Œåœ¨è¯±å¯¼è¿è§„è¡Œä¸ºæ–¹é¢å°¤ä¸ºæœ‰æ•ˆã€‚è¿™äº›å‘ç°è¡¨æ˜è§’è‰²é©±åŠ¨çš„äº¤äº’ä¸º LLMs å¼•å…¥äº†æ–°çš„å®‰å…¨é£é™©ç»´åº¦ï¼Œå‡¸æ˜¾äº†å¼€å‘å…·æœ‰è§’è‰²æ„ŸçŸ¥ (Persona-aware) çš„å®‰å…¨è¯„ä¼°ä¸å¯¹é½ç­–ç•¥çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12692v1",
      "published_date": "2025-05-19 04:32:02 UTC",
      "updated_date": "2025-05-19 04:32:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:24.999448+00:00"
    },
    {
      "arxiv_id": "2505.12684v2",
      "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement",
      "title_zh": "é€šè¿‡ç¼“è§£çŸ¥è¯†çº ç¼ æ„å»ºé«˜æ•ˆçš„è”é‚¦å›¾åŸºç¡€æ¨¡å‹",
      "authors": [
        "Yinlin Zhu",
        "Xunkai Li",
        "Jishuo Jia",
        "Miao Hu",
        "Di Wu",
        "Meikang Qiu"
      ],
      "abstract": "Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾æœºå™¨å­¦ä¹ ä¸­è”é‚¦å›¾å­¦ä¹  (Federated graph learning, FGL) çš„å¼‚æ„æ€§æŒ‘æˆ˜ä»¥åŠå›¾åŸºç¡€æ¨¡å‹ (Graph foundation models, GFM) ç¼ºä¹è·¨æœºæ„èµ„æºçš„é—®é¢˜ï¼Œæå‡ºäº† FedGFM è¿™ä¸€å»ä¸­å¿ƒåŒ–çš„è®­ç»ƒèŒƒå¼ã€‚é’ˆå¯¹å¤šé¢†åŸŸçŸ¥è¯†åˆå¹¶å¯¼è‡´è¡¨å¾éš¾ä»¥åŒºåˆ†çš„çŸ¥è¯†çº ç¼  (knowledge entanglement) éš¾é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†å¢å¼ºæ¡†æ¶ FedGFM+ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºå…¨å±€é”šç‚¹çš„é¢†åŸŸæ„ŸçŸ¥åˆå§‹åŒ–ç­–ç•¥ (AncDAI)ï¼Œé€šè¿‡å°†æœ¬åœ°å›¾ç¼–ç ä¸ºé¢†åŸŸç‰¹å®šåŸå‹ä½œä¸ºè¯­ä¹‰é”šç‚¹ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†å…¶åœ¨è§£è€¦é¢†åŸŸçŸ¥è¯†æ–¹é¢çš„å¼ºå½’çº³åç½®ä½œç”¨ã€‚åŒæ—¶ï¼ŒFedGFM+ è®¾è®¡äº†æœ¬åœ°è‡ªé€‚åº”é¢†åŸŸæ•æ„Ÿæç¤ºæ±  (AdaDPP)ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´æ•æ‰é¢†åŸŸè¯­ä¹‰ï¼Œå¹¶åœ¨å¾®è°ƒé˜¶æ®µé€šè¿‡é€‰æ‹©ç›¸å…³æç¤ºæ¥å¢å¼ºç›®æ ‡å›¾å±æ€§ï¼Œä»è€Œä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ã€‚å®éªŒåœ¨ 8 ä¸ªè·¨é¢†åŸŸå’Œä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ä¸Šè¯æ˜äº† FedGFM+ çš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€FGL åŠè”é‚¦ GFM å˜ä½“åœ¨å†…çš„ 20 ç§åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12684v2",
      "published_date": "2025-05-19 04:06:32 UTC",
      "updated_date": "2025-11-14 01:50:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:37.786717+00:00"
    },
    {
      "arxiv_id": "2505.12680v2",
      "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities",
      "title_zh": "Ineq-Compï¼šä¸ç­‰å¼è‡ªåŠ¨å®šç†è¯æ˜ä¸­ç¬¦åˆäººç±»ç›´è§‰çš„ç»„åˆæ¨ç†èƒ½åŠ›åŸºå‡†æµ‹è¯•",
      "authors": [
        "Haoyu Zhao",
        "Yihan Geng",
        "Shange Tang",
        "Yong Lin",
        "Bohan Lyu",
        "Hongzhou Lin",
        "Chi Jin",
        "Sanjeev Arora"
      ],
      "abstract": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for automating mathematical discovery. But beyond syntactic correctness, do these systems truly understand mathematical structure as humans do? We investigate this question in context of mathematical inequalities -- specifically the prover's ability to recognize that the given problem simplifies by applying a known inequality such as AM/GM. Specifically, we are interested in their ability to do this in a compositional setting where multiple inequalities must be applied as part of a solution. We introduce Ineq-Comp, a benchmark built from elementary inequalities through systematic transformations, including variable duplication, algebraic rewriting, and multi-step composition. Although these problems remain easy for humans, we find that most provers -- including Goedel, STP, and Kimina-7B -- struggle significantly. DeepSeek-Prover-V2-7B shows relative robustness, but still suffers a 20% performance drop (pass@32). Even for DeepSeek-Prover-V2-671B model, the gap between compositional variants and seed problems exists, implying that simply scaling up the model size alone does not fully solve the compositional weakness. Strikingly, performance remains poor for all models even when formal proofs of the constituent parts are provided in context, revealing that the source of weakness is indeed in compositional reasoning. Our results expose a persisting gap between the generalization behavior of current AI provers and human mathematical intuition. All data and evaluation code can be found at https://github.com/haoyuzhao123/LeanIneqComp.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Ineq-Compï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆAutomated Theorem Provingï¼‰ä¸­äººç±»ç›´è§‰ç»„åˆæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºæ•°å­¦ä¸ç­‰å¼é¢†åŸŸã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å˜é‡å¤åˆ¶ã€ä»£æ•°é‡å†™å’Œå¤šæ­¥ç»„åˆç­‰ç³»ç»Ÿæ€§å˜æ¢ï¼Œä»åŸºç¡€ä¸ç­‰å¼æ„å»ºäº†è¯¥åŸºå‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è¿™äº›é—®é¢˜å¯¹äººç±»è€Œè¨€ååˆ†ç®€å•ï¼Œä½† Goedelã€STP å’Œ Kimina-7B ç­‰å¤§å¤šæ•°è¯æ˜å™¨åœ¨å¤„ç†æ—¶éƒ½é¢ä¸´æ˜¾è‘—å›°éš¾ã€‚DeepSeek-Prover-V2-7B è™½ç„¶å±•ç°å‡ºä¸€å®šçš„é²æ£’æ€§ï¼Œä½†åœ¨ç»„åˆå˜ä½“ä¸Šçš„ pass@32 æ€§èƒ½ä»ä¸‹é™äº† 20%ã€‚ç ”ç©¶å‘ç°å³ä½¿æ˜¯ DeepSeek-Prover-V2-671B è¿™æ ·çš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œå…¶åœ¨ç»„åˆé—®é¢˜ä¸Šçš„è¡¨ç°ä»è½åäºåŸå§‹é—®é¢˜ï¼Œè¯æ˜å•çº¯é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡ï¼ˆscaling upï¼‰æ— æ³•å®Œå…¨è§£å†³ç»„åˆå¼±ç‚¹ã€‚å³ä½¿åœ¨ä¸Šä¸‹æ–‡ä¸­æä¾›å„ç»„æˆéƒ¨åˆ†çš„è¯æ˜ï¼Œæ¨¡å‹çš„è¡¨ç°ä¾ç„¶ä¸å°½å¦‚äººæ„ï¼Œè¿™æ­ç¤ºäº†å½“å‰ AI è¯æ˜å™¨åœ¨ç»„åˆæ¨ç†ï¼ˆcompositional reasoningï¼‰æ³›åŒ–è¡Œä¸ºä¸Šä¸äººç±»æ•°å­¦ç›´è§‰ä¹‹é—´å­˜åœ¨çš„æŒç»­å·®è·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in NeurIPS 2025 Track on Datasets and Benchmarks. 28 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12680v2",
      "published_date": "2025-05-19 03:56:05 UTC",
      "updated_date": "2025-10-20 04:36:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:02.807524+00:00"
    },
    {
      "arxiv_id": "2505.13547v1",
      "title": "Exploring Federated Pruning for Large Language Models",
      "title_zh": "æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹çš„è”é‚¦å‰ªæ",
      "authors": [
        "Pengxin Guo",
        "Yinong Wang",
        "Wei Li",
        "Mengting Liu",
        "Ming Li",
        "Jinkai Zheng",
        "Liangqiong Qu"
      ],
      "abstract": "LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)å‰ªæåœ¨éšç§æ•æ„Ÿé¢†åŸŸéš¾ä»¥è·å–å…¬å¼€æ ¡å‡†æ ·æœ¬çš„é—®é¢˜ï¼Œæå‡ºäº†FedPrLLMè¿™ä¸€è”é‚¦å‰ªææ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸å„å®¢æˆ·ç«¯åŸºäºæœ¬åœ°æ•°æ®è®¡ç®—å‰ªææ©ç çŸ©é˜µ(pruning mask matrix)å¹¶åŒæ­¥è‡³æœåŠ¡å™¨ï¼Œä»è€Œåœ¨ä¿æŠ¤æ•°æ®éšç§çš„å‰æä¸‹å®ç°å…¨å±€æ¨¡å‹çš„åä½œå‹ç¼©ã€‚é€šè¿‡å¯¹å¤šç§å‰ªæç­–ç•¥å’Œå‚æ•°è®¾ç½®çš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶å‘ç°é‡‡ç”¨å±‚çº§æ¯”è¾ƒ(layer comparison)ä¸”ä¸è¿›è¡Œæƒé‡ç¼©æ”¾(weight scaling)çš„ä¸€æ¬¡æ€§å‰ªæ(one-shot pruning)æ˜¯FedPrLLMæ¡†æ¶ä¸‹çš„æœ€ä¼˜æ–¹æ¡ˆã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºåœ¨èµ„æºå—é™ä¸”éšç§æ•æ„Ÿçš„ç¯å¢ƒä¸­éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆï¼Œä¹Ÿä¸ºæœªæ¥çš„è”é‚¦å­¦ä¹ ä¸æ¨¡å‹å‹ç¼©ç ”ç©¶æä¾›äº†å®è·µæŒ‡å¼•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13547v1",
      "published_date": "2025-05-19 03:41:54 UTC",
      "updated_date": "2025-05-19 03:41:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:46.190048+00:00"
    },
    {
      "arxiv_id": "2505.12669v1",
      "title": "Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment",
      "title_zh": "Text2midi-InferAlignï¼šé€šè¿‡æ¨ç†æ—¶å¯¹é½æå‡ç¬¦å·éŸ³ä¹ç”Ÿæˆè´¨é‡",
      "authors": [
        "Abhinaba Roy",
        "Geeta Puri",
        "Dorien Herremans"
      ],
      "abstract": "We present Text2midi-InferAlign, a novel technique for improving symbolic music generation at inference time. Our method leverages text-to-audio alignment and music structural alignment rewards during inference to encourage the generated music to be consistent with the input caption. Specifically, we introduce two objectives scores: a text-audio consistency score that measures rhythmic alignment between the generated music and the original text caption, and a harmonic consistency score that penalizes generated music containing notes inconsistent with the key. By optimizing these alignment-based objectives during the generation process, our model produces symbolic music that is more closely tied to the input captions, thereby improving the overall quality and coherence of the generated compositions. Our approach can extend any existing autoregressive model without requiring further training or fine-tuning. We evaluate our work on top of Text2midi - an existing text-to-midi generation model, demonstrating significant improvements in both objective and subjective evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Text2midi-InferAlignï¼Œä¸€ç§æ—¨åœ¨æ”¹è¿›æ¨ç†é˜¶æ®µç¬¦å·éŸ³ä¹ç”Ÿæˆ (Symbolic Music Generation) è´¨é‡çš„æ–°é¢–æŠ€æœ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°éŸ³é¢‘å¯¹é½ (text-to-audio alignment) å’ŒéŸ³ä¹ç»“æ„å¯¹é½å¥–åŠ±ï¼Œå¼•å¯¼ç”Ÿæˆçš„éŸ³ä¹ä¸è¾“å…¥çš„æ–‡æœ¬æè¿°ä¿æŒé«˜åº¦ä¸€è‡´ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹å¼•å…¥äº†è¡¡é‡èŠ‚å¥å¯¹é½ç¨‹åº¦çš„ text-audio consistency scoreï¼Œä»¥åŠæƒ©ç½šä¸è°ƒæ€§ä¸ç¬¦éŸ³ç¬¦çš„ harmonic consistency scoreã€‚é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜åŒ–è¿™äº›åŸºäºå¯¹é½çš„ç›®æ ‡å‡½æ•°ï¼Œæ¨¡å‹èƒ½å¤Ÿäº§å‡ºä¸è¾“å…¥ caption ç»“åˆæ›´ç´§å¯†ä¸”æ›´å…·è¿è´¯æ€§çš„éŸ³ä¹ä½œå“ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ‰©å±•ä»»ä½•ç°æœ‰çš„è‡ªå›å½’æ¨¡å‹ (autoregressive model)ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒ (fine-tuning)ã€‚åœ¨ Text2midi æ¨¡å‹ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æŠ€æœ¯åœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨æå‡ä½œæ›²è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "7 pages, 1 figure, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.12669v1",
      "published_date": "2025-05-19 03:36:06 UTC",
      "updated_date": "2025-05-19 03:36:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:50.089531+00:00"
    },
    {
      "arxiv_id": "2505.13546v1",
      "title": "Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems",
      "title_zh": "æç¤ºç¨³å®šæ€§è‡³å…³é‡è¦ï¼šé€šç”¨ç³»ç»Ÿä¸­è‡ªåŠ¨ç”Ÿæˆæç¤ºè¯çš„è¯„ä¼°ä¸ä¼˜åŒ–",
      "authors": [
        "Ke Chen",
        "Yufei Zhou",
        "Xitong Zhang",
        "Haohan Wang"
      ],
      "abstract": "Automatic prompt generation plays a crucial role in enabling general-purpose multi-agent systems to perform diverse tasks autonomously. Existing methods typically evaluate prompts based on their immediate task performance, overlooking the intrinsic qualities that determine their reliability. This outcome-centric view not only limits interpretability but also fails to account for the inherent stochasticity of large language models (LLMs). In this work, we bring attention to prompt stability-the consistency of model responses across repeated executions-as a key factor for building robust and effective prompt generation systems. To quantify this, we propose semantic stability as a criterion for assessing the response consistency of prompts, and fine-tune a LLaMA-based evaluator to measure it automatically across tasks. These components have enabled us to develop the first stability-aware general-purpose prompt generation system that leverages stability feedback to iteratively enhance both prompt quality and system-level performance. Furthermore, we establish a logical chain between prompt stability and task success by analyzing the structural dependencies within our system, proving stability as a necessary condition for effective system-level execution. Empirical results across general and domain-specific tasks demonstrate that our stability-aware framework improves both accuracy and output consistency. By shifting the focus from one-off results to persistent reliability, our work offers a new perspective on prompt design and contributes practical tools for building more trustworthy general-purpose systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨é€šç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­è‡ªåŠ¨æç¤ºè¯ç”Ÿæˆçš„å¯é æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€è¿‡äºå…³æ³¨å•æ¬¡ä»»åŠ¡è¡¨ç°ï¼Œè€Œå¿½è§†äº†æ¨¡å‹å›ºæœ‰çš„éšæœºæ€§ã€‚ä½œè€…æå‡ºäº†Prompt Stabilityï¼ˆæç¤ºè¯ç¨³å®šæ€§ï¼‰çš„æ¦‚å¿µï¼Œå³æ¨¡å‹åœ¨é‡å¤æ‰§è¡Œä¸­è¾“å‡ºçš„ä¸€è‡´æ€§ï¼Œå°†å…¶ä½œä¸ºæ„å»ºé²æ£’ç³»ç»Ÿçš„å…³é”®æŒ‡æ ‡ã€‚ä¸ºé‡åŒ–è¿™ä¸€æŒ‡æ ‡ï¼Œç ”ç©¶å¼•å…¥äº†Semantic Stabilityï¼ˆè¯­ä¹‰ç¨³å®šæ€§ï¼‰æ ‡å‡†ï¼Œå¹¶å¾®è°ƒäº†ä¸€ä¸ªåŸºäºLLaMAçš„è¯„ä¼°å™¨è¿›è¡Œè‡ªåŠ¨æµ‹é‡ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶å¼€å‘äº†é¦–ä¸ªç¨³å®šæ€§æ„ŸçŸ¥ï¼ˆStability-Awareï¼‰çš„é€šç”¨æç¤ºè¯ç”Ÿæˆç³»ç»Ÿï¼Œåˆ©ç”¨ç¨³å®šæ€§åé¦ˆè¿­ä»£ä¼˜åŒ–æç¤ºè¯è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½ã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡åˆ†æç³»ç»Ÿå†…éƒ¨çš„ç»“æ„ä¾èµ–ï¼Œå»ºç«‹äº†æç¤ºè¯ç¨³å®šæ€§ä¸ä»»åŠ¡æˆåŠŸä¹‹é—´çš„é€»è¾‘é“¾æ¡ï¼Œè¯æ˜äº†ç¨³å®šæ€§æ˜¯ç³»ç»Ÿçº§æœ‰æ•ˆæ‰§è¡Œçš„å¿…è¦æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç¨³å®šæ€§æ„ŸçŸ¥æ¡†æ¶åœ¨é€šç”¨åŠç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å’Œè¾“å‡ºä¸€è‡´æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºæç¤ºè¯è®¾è®¡æä¾›äº†ä»å…³æ³¨å³æ—¶ç»“æœåˆ°å…³æ³¨æŒä¹…å¯é æ€§çš„æ–°è§†è§’ï¼Œå¹¶ä¸ºæ„å»ºæ›´å€¼å¾—ä¿¡èµ–çš„é€šç”¨ç³»ç»Ÿæä¾›äº†å®ç”¨å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13546v1",
      "published_date": "2025-05-19 03:28:33 UTC",
      "updated_date": "2025-05-19 03:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:01:51.892012+00:00"
    },
    {
      "arxiv_id": "2505.12664v1",
      "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design",
      "title_zh": "åŸºäºæ¡ä»¶ç”Ÿæˆå­¦ä¹ çš„å¤šè§†è§’æ— çº¿æ„ŸçŸ¥ï¼šæ¡†æ¶ä¸æ¨¡å‹è®¾è®¡",
      "authors": [
        "Ziqing Xing",
        "Zhaoyang Zhang",
        "Zirui Chen",
        "Hongning Ruan",
        "Zhaohui Yang"
      ],
      "abstract": "In this paper, we incorporate physical knowledge into learning-based high-precision target sensing using the multi-view channel state information (CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind of multi-view sensing problem can be naturally cast into a conditional generation framework. To this end, we design a bipartite neural network architecture, the first part of which uses an elaborately designed encoder to fuse the latent target features embedded in the multi-view CSI, and then the second uses them as conditioning inputs of a powerful generative model to guide the target's reconstruction. Specifically, the encoder is designed to capture the physical correlation between the CSI and the target, and also be adaptive to the numbers and positions of BS-UE pairs. Therein the view-specific nature of CSI is assimilated by introducing a spatial positional embedding scheme, which exploits the structure of electromagnetic(EM)-wave propagation channels. Finally, a conditional diffusion model with a weighted loss is employed to generate the target's point cloud from the fused features. Extensive numerical results demonstrate that the proposed generative multi-view (Gen-MV) sensing framework exhibits excellent flexibility and significant performance improvement on the reconstruction quality of target's shape and EM properties.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºæ¡ä»¶ç”Ÿæˆå¼å­¦ä¹ çš„æ¡†æ¶ Gen-MVï¼Œæ—¨åœ¨åˆ©ç”¨å¤šåŸºç«™ (BS) ä¸ç”¨æˆ·è®¾å¤‡ (UE) ä¹‹é—´çš„å¤šè§†å›¾ä¿¡é“çŠ¶æ€ä¿¡æ¯ (CSI) å®ç°é«˜ç²¾åº¦ç›®æ ‡æ„ŸçŸ¥ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äºŒåˆ†ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå…¶ç¼–ç å™¨éƒ¨åˆ†ä¸“é—¨è®¾è®¡ç”¨äºèåˆå¤šè§†å›¾ CSI ä¸­çš„æ½œåœ¨ç›®æ ‡ç‰¹å¾ï¼Œå¹¶èƒ½çµæ´»é€‚åº” BS-UE å¯¹çš„æ•°é‡å’Œä½ç½®å˜åŒ–ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥åŸºäºç”µç£æ³¢ (EM-wave) ä¼ æ’­ç»“æ„çš„ç©ºé—´ä½ç½®åµŒå…¥æ–¹æ¡ˆï¼Œæœ‰æ•ˆå¸æ”¶äº† CSI çš„è§†å›¾ç‰¹å®šç‰©ç†å±æ€§ã€‚éšåï¼Œç³»ç»Ÿåˆ©ç”¨å¸¦æœ‰åŠ æƒæŸå¤±çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ (Conditional Diffusion Model) æ ¹æ®èåˆç‰¹å¾ç”Ÿæˆç›®æ ‡ç‚¹äº‘ã€‚æ•°å€¼å®éªŒç»“æœè¯æ˜ï¼ŒGen-MV æ¡†æ¶åœ¨ç›®æ ‡å½¢çŠ¶å’Œç”µç£ç‰¹æ€§çš„é‡å»ºè´¨é‡ä¸Šå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå¹¶è¡¨ç°å‡ºæé«˜çš„åº”ç”¨çµæ´»æ€§ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "submitted to IEEE Transactions on Wireless Communications",
      "pdf_url": "https://arxiv.org/pdf/2505.12664v1",
      "published_date": "2025-05-19 03:27:24 UTC",
      "updated_date": "2025-05-19 03:27:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:42.143396+00:00"
    },
    {
      "arxiv_id": "2505.12662v1",
      "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering",
      "title_zh": "Know3-RAGï¼šå…·å¤‡è‡ªé€‚åº”æ£€ç´¢ã€ç”Ÿæˆä¸è¿‡æ»¤èƒ½åŠ›çš„çŸ¥è¯†æ„ŸçŸ¥ RAG æ¡†æ¶",
      "authors": [
        "Xukai Liu",
        "Ye Liu",
        "Shiwen Wu",
        "Yanghai Zhang",
        "Yihao Yuan",
        "Kai Zhang",
        "Qi Liu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have led to impressive progress in natural language generation, yet their tendency to produce hallucinated or unsubstantiated content remains a critical concern. To improve factual reliability, Retrieval-Augmented Generation (RAG) integrates external knowledge during inference. However, existing RAG systems face two major limitations: (1) unreliable adaptive control due to limited external knowledge supervision, and (2) hallucinations caused by inaccurate or irrelevant references. To address these issues, we propose Know3-RAG, a knowledge-aware RAG framework that leverages structured knowledge from knowledge graphs (KGs) to guide three core stages of the RAG process, including retrieval, generation, and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval module that employs KG embedding to assess the confidence of the generated answer and determine retrieval necessity, a knowledge-enhanced reference generation strategy that enriches queries with KG-derived entities to improve generated reference relevance, and a knowledge-driven reference filtering mechanism that ensures semantic alignment and factual accuracy of references. Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG consistently outperforms strong baselines, significantly reducing hallucinations and enhancing answer reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Know3-RAGï¼Œè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†æ„ŸçŸ¥çš„ RAG æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨çŸ¥è¯†å›¾è°± (KGs) çš„ç»“æ„åŒ–çŸ¥è¯†å¼•å¯¼æ£€ç´¢ã€ç”Ÿæˆå’Œè¿‡æ»¤ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µï¼Œä»è€Œè§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†çŸ¥è¯†æ„ŸçŸ¥è‡ªé€‚åº”æ£€ç´¢æ¨¡å— (knowledge-aware adaptive retrieval module)ï¼Œåˆ©ç”¨ KG embedding è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„ç½®ä¿¡åº¦å¹¶å†³å®šæ£€ç´¢å¿…è¦æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡çŸ¥è¯†å¢å¼ºçš„å‚è€ƒæ–‡çŒ®ç”Ÿæˆç­–ç•¥ (knowledge-enhanced reference generation strategy) åˆ©ç”¨ KG å®ä½“ä¸°å¯ŒæŸ¥è¯¢ï¼Œå¹¶ç»“åˆçŸ¥è¯†é©±åŠ¨çš„å‚è€ƒæ–‡çŒ®è¿‡æ»¤æœºåˆ¶ (knowledge-driven reference filtering mechanism) ç¡®ä¿è¯­ä¹‰å¯¹é½å’Œäº‹å®å‡†ç¡®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKnow3-RAG åœ¨å¤šä¸ªå¼€æ”¾åŸŸé—®ç­” (open-domain QA) åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†å¹»è§‰å¹¶æå‡äº†ç­”æ¡ˆçš„å¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12662v1",
      "published_date": "2025-05-19 03:25:18 UTC",
      "updated_date": "2025-05-19 03:25:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:33.112387+00:00"
    },
    {
      "arxiv_id": "2505.13545v2",
      "title": "Know Or Not: a library for evaluating out-of-knowledge base robustness",
      "title_zh": "Know Or Notï¼šç”¨äºçŸ¥è¯†åº“å¤–é²æ£’æ€§è¯„ä¼°çš„åº“",
      "authors": [
        "Jessica Foo",
        "Pradyumna Shyama Prasad",
        "Shaun Khoo"
      ],
      "abstract": "While the capabilities of large language models (LLMs) have progressed significantly, their use in high-stakes applications have been limited due to risks of hallucination. One key approach in reducing hallucination is retrieval-augmented generation (RAG), but even in such setups, LLMs may still hallucinate when presented with questions outside of the knowledge base. Such behavior is unacceptable in high-stake applications where LLMs are expected to abstain from answering queries it does not have sufficient context on. In this work, we present a novel methodology for systematically evaluating out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not know) in the RAG setting, without the need for manual annotation of gold standard answers. We implement our methodology in knowornot, an open-source library that enables users to develop their own customized evaluation data and pipelines for OOKB robustness. knowornot comprises four main features. Firstly, it provides a unified, high-level API that streamlines the process of setting up and running robustness benchmarks. Secondly, its modular architecture emphasizes extensibility and flexibility, allowing users to easily integrate their own LLM clients and RAG settings. Thirdly, its rigorous data modeling design ensures experiment reproducibility, reliability and traceability. Lastly, it implements a comprehensive suite of tools for users to customize their pipelines. We demonstrate the utility of knowornot by developing a challenging benchmark, PolicyBench, which spans four Question-Answer (QA) chatbots on government policies, and analyze its OOKB robustness. The source code of knowornot is available https://github.com/govtech-responsibleai/KnowOrNot.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† knowornotï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) è®¾ç½®ä¸‹çŸ¥è¯†åº“å¤– (Out-of-Knowledge Base, OOKB) é²æ£’æ€§çš„å¼€æºåº“ã€‚é’ˆå¯¹æ¨¡å‹åœ¨é¢å¯¹è¶…å‡ºæ£€ç´¢èŒƒå›´é—®é¢˜æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ (Hallucination) çš„é£é™©ï¼Œè¯¥å·¥ä½œæå‡ºäº†ä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨é‡‘æ ‡å‡†ç­”æ¡ˆçš„ç³»ç»ŸåŒ–è¯„ä¼°æ–¹æ³•ã€‚knowornot åº“æä¾›äº†ä¸€å¥—ç»Ÿä¸€çš„é«˜å±‚çº§ API ä»¥ç®€åŒ–åŸºå‡†æµ‹è¯•æµç¨‹ï¼Œå¹¶é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ç¡®ä¿äº†é›†æˆè‡ªå®šä¹‰ LLM å®¢æˆ·ç«¯ä¸ RAG é…ç½®çš„çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œå…¶ä¸¥è°¨çš„æ•°æ®å»ºæ¨¡è®¾è®¡ä¿éšœäº†å®éªŒçš„å¯é‡å¤æ€§ã€å¯é æ€§ä¸å¯è¿½æº¯æ€§ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼€å‘é’ˆå¯¹æ”¿åºœæ”¿ç­–é—®ç­”æœºå™¨äººçš„ PolicyBench åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥åº“åœ¨åˆ†æ OOKB é²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13545v2",
      "published_date": "2025-05-19 03:17:41 UTC",
      "updated_date": "2025-07-21 14:09:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:59.553477+00:00"
    },
    {
      "arxiv_id": "2505.12655v2",
      "title": "Web Intellectual Property at Risk: Preventing Unauthorized Real-Time Retrieval by Large Language Models",
      "title_zh": "ç½‘ç»œçŸ¥è¯†äº§æƒé¢ä¸´é£é™©ï¼šé˜²æ­¢å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæœªç»æˆæƒçš„å®æ—¶æ£€ç´¢",
      "authors": [
        "Yisheng Zhong",
        "Yizhu Wen",
        "Junfeng Guo",
        "Mehran Kafai",
        "Heng Huang",
        "Hanqing Guo",
        "Zhuangdi Zhu"
      ],
      "abstract": "The protection of cyber Intellectual Property (IP) such as web content is an increasingly critical concern. The rise of large language models (LLMs) with online retrieval capabilities enables convenient access to information but often undermines the rights of original content creators. As users increasingly rely on LLM-generated responses, they gradually diminish direct engagement with original information sources, which will significantly reduce the incentives for IP creators to contribute, and lead to a saturating cyberspace with more AI-generated content. In response, we propose a novel defense framework that empowers web content creators to safeguard their web-based IP from unauthorized LLM real-time extraction and redistribution by leveraging the semantic understanding capability of LLMs themselves. Our method follows principled motivations and effectively addresses an intractable black-box optimization problem. Real-world experiments demonstrated that our methods improve defense success rates from 2.5% to 88.6% on different LLMs, outperforming traditional defenses such as configuration-based restrictions.",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨ç½‘ç»œçŸ¥è¯†äº§æƒ(Web Intellectual Property, IP)ä¿æŠ¤ï¼ŒæŒ‡å‡ºå…·æœ‰å®æ—¶æ£€ç´¢èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æä¾›ä¾¿åˆ©çš„åŒæ—¶ï¼Œæ­£é€æ¸æŸå®³åŸåˆ›å†…å®¹åˆ›ä½œè€…çš„åˆæ³•æƒç›Šã€‚ç”±äºç”¨æˆ·æ—¥ç›Šä¾èµ–å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å“åº”è€Œå‡å°‘äº†å¯¹åŸå§‹ä¿¡æ¯æºçš„è®¿é—®ï¼Œè¿™ä¸ä»…é™ä½äº†åˆ›ä½œè€…çš„è´¡çŒ®åŠ¨åŠ›ï¼Œè¿˜å¯èƒ½å¯¼è‡´ç½‘ç»œç©ºé—´è¢«AIç”Ÿæˆå†…å®¹å……æ–¥ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªèº«çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä¿æŠ¤ç½‘é¡µå†…å®¹å…å—æœªç»æˆæƒçš„å®æ—¶æå–ä¸åˆ†å‘ã€‚è¯¥æ–¹æ³•éµå¾ªåŸåˆ™æ€§åŠ¨æœºï¼Œæœ‰æ•ˆè§£å†³äº†å¤æ‚çš„é»‘ç›’ä¼˜åŒ–(black-box optimization)é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šç§å¤§è¯­è¨€æ¨¡å‹ä¸Šçš„é˜²å¾¡æˆåŠŸç‡ä»2.5%æå‡è‡³88.6%ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºåŸºäºé…ç½®é™åˆ¶(configuration-based restrictions)çš„ä¼ ç»Ÿé˜²å¾¡æ‰‹æ®µã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 13 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.12655v2",
      "published_date": "2025-05-19 03:14:08 UTC",
      "updated_date": "2025-06-06 04:55:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:39.294582+00:00"
    },
    {
      "arxiv_id": "2505.12654v2",
      "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals",
      "title_zh": "åŸºäºè¯­è¨€ã€å£°å­¦åŠè§†è§‰ä¿¡å·çš„äººæœºå¯¹è¯è¯è½®è½¬æ¢ä¸åé¦ˆä¿¡å·é¢„æµ‹",
      "authors": [
        "Yuxin Lin",
        "Yinglin Zheng",
        "Ming Zeng",
        "Wangzheng Shi"
      ],
      "abstract": "This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10% increase in F1-score on turn-taking and a 33% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹äººæœºå¯¹è¯ä¸­åˆ©ç”¨å¤šæ¨¡æ€ä¿¡å·é¢„æµ‹ turn-taking å’Œ backchannel è¡Œä¸ºçš„ç°æœ‰ç©ºç™½ï¼Œæå‡ºäº†æ•´åˆè¯­è¨€ã€å£°å­¦å’Œè§†è§‰ä¿¡å·çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³æ•°æ®é›†åŒ®ä¹é—®é¢˜ï¼Œä½œè€…å¼€å‘äº†è‡ªåŠ¨æ•°æ®é‡‡é›†æµæ°´çº¿ï¼Œå¹¶æ„å»ºäº†åŒ…å«è¶…è¿‡210å°æ—¶è§†é¢‘åŠ2000ä¸‡å¸§æ ‡æ³¨çš„ MM-F2F äººæœºå¯¹è¯æ•°æ®é›†ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡èåˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€è¾“å…¥æ¥ç²¾ç¡®é¢„æµ‹è¡Œä¸ºæ¦‚ç‡ã€‚è¯¥æ¨¡å‹å¼ºè°ƒä¸åŒæ¨¡æ€é—´çš„ç›¸äº’å…³ç³»å¹¶æ”¯æŒä»»æ„è¾“å…¥ç»„åˆï¼Œä½¿å…¶åœ¨å„ç§ç°å®åœºæ™¯ä¸­å…·æœ‰æé«˜çš„çµæ´»æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œåœ¨ turn-taking é¢„æµ‹çš„ F1-score ä¸Šæå‡äº†10%ï¼Œåœ¨ backchannel é¢„æµ‹ä¸Šæ˜¾è‘—æå‡äº†33%ã€‚ç›®å‰è¯¥é¡¹ç›®çš„æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€ï¼Œä¸ºå¤šæ¨¡æ€äººæœºäº¤äº’çš„åç»­ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepected by ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12654v2",
      "published_date": "2025-05-19 03:08:30 UTC",
      "updated_date": "2025-05-20 06:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:02:49.121361+00:00"
    },
    {
      "arxiv_id": "2505.12651v1",
      "title": "$\\texttt{DIAMONDs}$: A Dataset for $\\mathbb{D}$ynamic $\\mathbb{I}$nformation $\\mathbb{A}$nd $\\mathbb{M}$ental modeling $\\mathbb{O}$f $\\mathbb{N}$umeric $\\mathbb{D}$iscussions",
      "title_zh": "DIAMONDsï¼šæ•°å€¼è®¨è®ºä¸­åŠ¨æ€ä¿¡æ¯ä¸å¿ƒç†å»ºæ¨¡çš„æ•°æ®é›†",
      "authors": [
        "Sayontan Ghosh",
        "Mahnaz Koupaee",
        "Yash Kumar Lal",
        "Pegah Alipoormolabashi",
        "Mohammad Saqib Hasan",
        "Jun Seok Kang",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Understanding multiparty conversations demands robust Theory of Mind (ToM) capabilities, including the ability to track dynamic information, manage knowledge asymmetries, and distinguish relevant information across extended exchanges. To advance ToM evaluation in such settings, we present a carefully designed scalable methodology for generating high-quality benchmark conversation-question pairs with these characteristics. Using this methodology, we create $\\texttt{DIAMONDs}$, a new conversational QA dataset covering common business, financial or other group interactions. In these goal-oriented conversations, participants often have to track certain numerical quantities (say $\\textit{expected profit}$) of interest that can be derived from other variable quantities (like $\\textit{marketing expenses, expected sales, salary}$, etc.), whose values also change over the course of the conversation. $\\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over such quantities of interest (e.g., $\\textit{funds required for charity events, expected company profit next quarter}$, etc.) in the context of the information exchanged in conversations. This allows for precisely evaluating ToM capabilities for carefully tracking and reasoning over participants' knowledge states.\n  Our evaluation of state-of-the-art language models reveals significant challenges in handling participant-centric reasoning, specifically in situations where participants have false beliefs. Models also struggle with conversations containing distractors and show limited ability to identify scenarios with insufficient information. These findings highlight current models' ToM limitations in handling real-world multi-party conversations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šäººå¯¹è¯ä¸­å¯¹å¿ƒæ™ºç†è®º (Theory of Mind, ToM) èƒ½åŠ›çš„é«˜è¦æ±‚ï¼Œæå‡ºäº† DIAMONDs æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€ä¿¡æ¯ã€çŸ¥è¯†ä¸å¯¹ç§°ä»¥åŠåŒºåˆ†ç›¸å…³ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å…¸å‹çš„å•†ä¸šã€è´¢åŠ¡åŠç¾¤ä½“äº’åŠ¨åœºæ™¯ï¼Œé€šè¿‡ä¸€ç§å¯æ‰©å±•çš„ç”Ÿæˆæ–¹æ³•æä¾›é«˜è´¨é‡çš„å¯¹è¯-é—®é¢˜å¯¹ (QA pairs)ã€‚åœ¨è¿™äº›ç›®æ ‡å¯¼å‘çš„å¯¹è¯ä¸­ï¼Œå‚ä¸è€…éœ€è¦è¿½è¸ªéšå¯¹è¯è¿›ç¨‹ä¸æ–­å˜åŒ–çš„æ•°å€¼å˜é‡å¹¶è¿›è¡Œæ•°å€¼æ¨ç†ï¼Œä»¥æ­¤ç²¾ç¡®è¯„ä¼°æ¨¡å‹å¯¹å‚ä¸è€…çŸ¥è¯†çŠ¶æ€çš„è¿½è¸ªä¸æ¨ç†èƒ½åŠ›ã€‚å¯¹å½“å‰æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œæ¨¡å‹åœ¨å¤„ç†ä»¥å‚ä¸è€…ä¸ºä¸­å¿ƒçš„æ¨ç†æ—¶å­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå‚ä¸è€…æŒæœ‰é”™è¯¯ä¿¡å¿µ (false beliefs) çš„å¤æ‚æƒ…å¢ƒä¸‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨åº”å¯¹å¯¹è¯å¹²æ‰°é¡¹ä»¥åŠè¯†åˆ«ä¿¡æ¯ä¸è¶³çš„åœºæ™¯æ—¶ä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾çš„å±€é™æ€§ã€‚è¿™äº›ç ”ç©¶ç»“æœå…±åŒæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œå¤šäººå¯¹è¯æ—¶ï¼Œå…¶å¿ƒæ™ºç†è®ºèƒ½åŠ›ä»å­˜åœ¨å·¨å¤§çš„æå‡ç©ºé—´ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12651v1",
      "published_date": "2025-05-19 03:05:13 UTC",
      "updated_date": "2025-05-19 03:05:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:03:45.383690+00:00"
    },
    {
      "arxiv_id": "2505.12650v1",
      "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use",
      "title_zh": "AutoMatï¼šé€šè¿‡æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨å®ç°æ˜¾å¾®æˆåƒä¸­çš„æ™¶ä½“ç»“æ„è‡ªåŠ¨é‡å»º",
      "authors": [
        "Yaotian Yang",
        "Yiwen Tang",
        "Yizhe Chen",
        "Xiao Chen",
        "Jiangjie Qiu",
        "Hao Xiong",
        "Haoyu Yin",
        "Zhiyao Luo",
        "Yifei Zhang",
        "Sijia Tao",
        "Wentao Li",
        "Qinghua Zhang",
        "Yuqiang Li",
        "Wanli Ouyang",
        "Bin Zhao",
        "Xiaonan Wang",
        "Fei Wei"
      ],
      "abstract": "Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, converting these images into simulation-ready formats remains labor-intensive and error-prone, creating a bottleneck for model training and validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that automatically transforms scanning transmission electron microscopy (STEM) images into atomic crystal structures and predicts their physical properties. AutoMat combines pattern-adaptive denoising, physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation and property prediction via MatterSim, and coordinated orchestration across all stages. We propose the first dedicated STEM2Mat-Bench for this task and evaluate performance using lattice RMSD, formation energy MAE, and structure-matching success rate. By orchestrating external tool calls, AutoMat enables a text-only LLM to outperform vision-language models in this domain, achieving closed-loop reasoning throughout the pipeline. In large-scale experiments over 450 structure samples, AutoMat substantially outperforms existing multimodal large language models and tools. These results validate both AutoMat and STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic simulation in materials science.The code and dataset are publicly available at https://github.com/yyt-2378/AutoMat and https://huggingface.co/datasets/yaotianvector/STEM2Mat.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AutoMatï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯ã€åŸºäºæ™ºèƒ½ä½“ï¼ˆAgentï¼‰è¾…åŠ©çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œæ—¨åœ¨å°†æ‰«æé€å°„ç”µå­æ˜¾å¾®é•œï¼ˆSTEMï¼‰å›¾åƒè½¬åŒ–ä¸ºåŸå­æ™¶ä½“ç»“æ„å¹¶é¢„æµ‹å…¶ç‰©ç†æ€§è´¨ã€‚å…¶å¼€å‘åŠ¨æœºåœ¨äºè§£å†³æœºå™¨å­¦ä¹ åŸå­é—´åŠ¿èƒ½å’ŒåŠ›åœºå¯¹å‡†ç¡®åŸå­ç»“æ„æ•°æ®çš„ä¾èµ–é—®é¢˜ï¼ŒåŒæ—¶å…‹æœäº†ä¼ ç»Ÿæ˜¾å¾®é•œå›¾åƒæ‰‹åŠ¨è½¬åŒ–ä¸ºæ¨¡æ‹Ÿæ ¼å¼æ—¶æ•ˆç‡ä½ä¸‹ä¸”æ˜“å‡ºé”™çš„ç“¶é¢ˆã€‚AutoMaté›†æˆäº†æ¨¡å¼è‡ªé€‚åº”é™å™ªï¼ˆpattern-adaptive denoisingï¼‰ã€ç‰©ç†å¯¼å‘çš„æ¨¡æ¿æ£€ç´¢ï¼ˆphysics-guided template retrievalï¼‰ã€å¯¹ç§°æ€§æ„ŸçŸ¥åŸå­é‡å»ºï¼ˆsymmetry-aware atomic reconstructionï¼‰ä»¥åŠé€šè¿‡MatterSimå®ç°çš„å¿«é€Ÿå¼›è±«ä¸æ€§è´¨é¢„æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒè°ƒåº¦å¤–éƒ¨å·¥å…·è°ƒç”¨ï¼Œä½¿å¾—çº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨è¯¥é¢†åŸŸè¶…è¶Šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå®ç°å…¨æµç¨‹çš„é—­ç¯æ¨ç†ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åŒæ­¥æ¨å‡ºäº†é¦–ä¸ªä¸“é—¨çš„STEM2Mat-BenchåŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨450ä¸ªç»“æ„æ ·æœ¬çš„å®éªŒä¸­è¯æ˜äº†AutoMatåœ¨æ™¶æ ¼RMSDã€ç”Ÿæˆèƒ½MAEå’Œç»“æ„åŒ¹é…æˆåŠŸç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹å’Œå·¥å…·ã€‚è¿™ä¸€æˆæœä¸ºè¿æ¥ææ–™ç§‘å­¦ä¸­çš„æ˜¾å¾®æˆåƒä¸åŸå­çº§æ¨¡æ‹Ÿæä¾›äº†é‡è¦å·¥å…·æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The code and dataset are publicly available at https://github.com/yyt-2378/AutoMat and https://huggingface.co/datasets/yaotianvector/STEM2Mat",
      "pdf_url": "https://arxiv.org/pdf/2505.12650v1",
      "published_date": "2025-05-19 03:04:50 UTC",
      "updated_date": "2025-05-19 03:04:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:03:07.847379+00:00"
    },
    {
      "arxiv_id": "2505.17067v5",
      "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning",
      "title_zh": "åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šè¯­è¨€å¤šå›¾ç‰‡æè¿°è½»åº¦è®¤çŸ¥éšœç¢æ£€æµ‹",
      "authors": [
        "Kristin Qi",
        "Jiali Cheng",
        "Youxiang Zhu",
        "Hadi Amiri",
        "Xiaohui Liang"
      ],
      "abstract": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ TAUKDIAL-2024 æŒ‘æˆ˜èµ›ä¸­å¤šè¯­è¨€å’Œå¤šå›¾åƒæè¿°ä¸‹çš„è½»åº¦è®¤çŸ¥éšœç¢ (Mild Cognitive Impairment, MCI) æ£€æµ‹éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„å¤šæ¨¡æ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆåˆ©ç”¨æœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹  (supervised contrastive learning) å¢å¼ºåˆ¤åˆ«æ€§è¡¨å¾å­¦ä¹ ï¼›å…¶æ¬¡å¼•å…¥å›¾åƒæ¨¡æ€ (image modality) ä»¥ç»“åˆè§†è§‰ä¿¡æ¯ï¼Œè€Œéå•çº¯ä¾èµ–è¯­éŸ³å’Œæ–‡æœ¬ï¼›æœ€ååº”ç”¨ä¸“å®¶ä¹˜ç§¯ (Product of Experts, PoE) ç­–ç•¥æ¥æœ‰æ•ˆç¼“è§£ä¼ªç›¸å…³å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº† MCI çš„æ£€æµ‹æ€§èƒ½ï¼Œå…¶æœªåŠ æƒå¹³å‡å¬å›ç‡ (UAR) è¾ƒæ–‡æœ¬å•æ¨¡æ€åŸºçº¿æé«˜äº† 7.1%ï¼ˆè¾¾åˆ° 75.2%ï¼‰ï¼ŒF1 åˆ†æ•°æé«˜äº† 2.9%ï¼ˆè¾¾åˆ° 83.5%ï¼‰ã€‚ç ”ç©¶ç‰¹åˆ«æŒ‡å‡ºï¼Œå¯¹æ¯”å­¦ä¹ ç»„ä»¶å¯¹æ–‡æœ¬æ¨¡æ€çš„æ€§èƒ½æå‡å°¤ä¸ºæ˜¾è‘—ï¼Œå……åˆ†éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šè¯­è¨€åŠå¤šå›¾ç‰‡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "IEEE Global Communications Conference (GlobeCom) 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17067v5",
      "published_date": "2025-05-19 03:03:08 UTC",
      "updated_date": "2025-09-03 03:38:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:03:11.899269+00:00"
    },
    {
      "arxiv_id": "2505.12641v2",
      "title": "Single Image Reflection Separation via Dual Prior Interaction Transformer",
      "title_zh": "åŸºäºåŒå…ˆéªŒäº¤äº’ Transformer çš„å•å¹…å›¾åƒåå°„åˆ†ç¦»",
      "authors": [
        "Yue Huang",
        "Zi'ang Li",
        "Tianle Hu",
        "Jie Wen",
        "Guanbin Li",
        "Jinglin Zhang",
        "Guoxu Zhou",
        "Xiaozhao Fang"
      ],
      "abstract": "Single image reflection separation aims to separate the transmission and reflection layers from a mixed image. Existing methods typically combine general priors from pre-trained models with task-specific priors such as text prompts and reflection detection. However, the transmission prior, as the most direct task-specific prior for the target transmission layer, has not been effectively modeled or fully utilized, limiting performance in complex scenarios. To address this issue, we propose a dual-prior interaction framework based on lightweight transmission prior generation and effective prior fusion. First, we design a Local Linear Correction Network (LLCN) that finetunes pre-trained models based on the physical constraint T=SI+B, where S and B represent pixel-wise and channel-wise scaling and bias transformations. LLCN efficiently generates high-quality transmission priors with minimal parameters. Second, we construct a Dual-Prior Interaction Transformer (DPIT) that employs a dual-stream channel reorganization attention mechanism. By reorganizing features from general and transmission priors for attention computation, DPIT achieves deep fusion of both priors, fully exploiting their complementary information. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŒå…ˆéªŒäº¤äº’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å•å›¾åƒåå°„åˆ†ç¦» (Single Image Reflection Separation) ä¸­é€å°„å…ˆéªŒ (transmission prior) æœªèƒ½è¢«æœ‰æ•ˆå»ºæ¨¡æˆ–å……åˆ†åˆ©ç”¨çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œç ”ç©¶è€…è®¾è®¡äº†å±€éƒ¨çº¿æ€§ä¿®æ­£ç½‘ç»œ (Local Linear Correction Network, LLCN)ï¼Œé€šè¿‡åŸºäºç‰©ç†çº¦æŸçš„å¾®è°ƒæŠ€æœ¯ï¼Œä»¥æä½çš„å‚æ•°é‡ç”Ÿæˆé«˜è´¨é‡çš„é€å°„å…ˆéªŒã€‚éšåï¼Œæ„å»ºäº†åŒå…ˆéªŒäº¤äº’ Transformer (Dual-Prior Interaction Transformer, DPIT)ï¼Œåˆ©ç”¨åŒæµé€šé“é‡ç»„æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†é€šç”¨å…ˆéªŒä¸ä»»åŠ¡ç‰¹å®šé€å°„å…ˆéªŒçš„æ·±åº¦èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå……åˆ†æŒ–æ˜ä¸åŒå…ˆéªŒé—´çš„äº’è¡¥ä¿¡æ¯ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ (state-of-the-art) æ€§èƒ½æ°´å¹³ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚åœºæ™¯ä¸‹çš„å›¾åƒåˆ†ç¦»æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12641v2",
      "published_date": "2025-05-19 02:50:15 UTC",
      "updated_date": "2026-01-08 11:53:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:03:03.261314+00:00"
    },
    {
      "arxiv_id": "2505.12638v3",
      "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data",
      "title_zh": "ChromFoundï¼šé¢å‘å•ç»†èƒæŸ“è‰²è´¨å¯åŠæ€§æ•°æ®çš„é€šç”¨åŸºç¡€æ¨¡å‹",
      "authors": [
        "Yifeng Jiao",
        "Yuchen Liu",
        "Yu Zhang",
        "Xin Guo",
        "Yushuai Wu",
        "Chen Jiang",
        "Jiyang Li",
        "Hongwei Zhang",
        "Limei Han",
        "Xin Gao",
        "Yuan Qi",
        "Yuan Cheng"
      ],
      "abstract": "The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChromFoundï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºå•ç»†èƒæŸ“è‰²è´¨å¯åŠæ€§æµ‹åº(scATAC-seq)è®¾è®¡çš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸæ•°æ®é«˜ç»´åº¦ã€ç¨€ç–æ€§ä»¥åŠç¼ºä¹æ ‡å‡†åŒ–å¼€æ”¾æŸ“è‰²è´¨åŒºåŸŸ(OCRs)è¡¨ç¤ºæ¶æ„çš„æŒ‘æˆ˜ã€‚ChromFound é‡‡ç”¨äº†æ··åˆæ¶æ„(hybrid architecture)å’ŒåŸºå› ç»„æ„ŸçŸ¥åˆ†è¯æŠ€æœ¯(genome-aware tokenization)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åŠ¨æ€æŸ“è‰²è´¨æ™¯è§‚ä¸­çš„å…¨åŸºå› ç»„é•¿ä¸Šä¸‹æ–‡å’Œè°ƒæ§ä¿¡å·ã€‚è¯¥æ¨¡å‹åœ¨æ¥è‡ª 30 ä¸ªç»„ç»‡å’Œ 6 ç§ç–¾ç—…çŠ¶æ€çš„ 197 ä¸‡ä¸ªç»†èƒä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå±•ç°å‡ºåœ¨ 6 ç§ä¸åŒä»»åŠ¡ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChromFound åœ¨ç”Ÿæˆé€šç”¨ç»†èƒè¡¨ç¤ºæ–¹é¢å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬(zero-shot)æ€§èƒ½ï¼Œå¹¶åœ¨ç»†èƒç±»å‹æ³¨é‡Š(cell type annotation)å’Œè·¨ç»„å­¦é¢„æµ‹(cross-omics prediction)ä¸­è¡¨ç°å‡ºå“è¶Šçš„å¯è¿ç§»æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå‘ç°ç°æœ‰è®¡ç®—æ–¹æ³•æ— æ³•æ£€æµ‹åˆ°çš„å¢å¼ºå­-åŸºå› é“¾æ¥(enhancer-gene links)ï¼Œä¸ºç†è§£éç¼–ç åŸºå› ç»„ä¸­çš„ç–¾ç—…é£é™©å˜å¼‚æä¾›äº†ä¸€ä¸ªæå…·å‰æ™¯çš„æ¡†æ¶ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12638v3",
      "published_date": "2025-05-19 02:45:42 UTC",
      "updated_date": "2025-10-26 14:29:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:03:27.556748+00:00"
    },
    {
      "arxiv_id": "2505.12632v1",
      "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents",
      "title_zh": "é¢å‘è·¨å¹³å°ç§»åŠ¨æ™ºèƒ½ä½“çš„å¯æ‰©å±•è§†é¢‘åˆ°æ•°æ®é›†ç”Ÿæˆ",
      "authors": [
        "Yunseok Jang",
        "Yeda Song",
        "Sungryull Sohn",
        "Lajanugen Logeswaran",
        "Tiange Luo",
        "Dong-Ki Kim",
        "Kyunghoon Bae",
        "Honglak Lee"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have sparked significant interest in developing GUI visual agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube), a large-scale dataset of 313K annotated frames from 20K instructional videos capturing diverse real-world mobile OS navigation across multiple platforms. Models that include MONDAY in their pre-training phases demonstrate robust cross-platform generalization capabilities, consistently outperforming models trained on existing single OS datasets while achieving an average performance gain of 18.11%p on an unseen mobile OS platform. To enable continuous dataset expansion as mobile platforms evolve, we present an automated framework that leverages publicly available video content to create comprehensive task datasets without manual annotation. Our framework comprises robust OCR-based scene detection (95.04% F1score), near-perfect UI element detection (99.87% hit ratio), and novel multi-step action identification to extract reliable action sequences across diverse interface configurations. We contribute both the MONDAY dataset and our automated collection framework to facilitate future research in mobile OS navigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ¥è‡ª 2 ä¸‡ä¸ªæ•™å­¦è§†é¢‘ã€31.3 ä¸‡ä¸ªæ ‡æ³¨å¸§çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§å¹³å°çš„çœŸå®ç§»åŠ¨æ“ä½œç³»ç»Ÿå¯¼èˆªä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨é¢„è®­ç»ƒä¸­åŠ å…¥ MONDAY æ•°æ®é›†çš„æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è·¨å¹³å°æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè§è¿‡çš„ç§»åŠ¨å¹³å°ä¸Šçš„å¹³å‡æ€§èƒ½æå‡äº† 18.11%ã€‚ä¸ºäº†å®ç°æ•°æ®é›†çš„æŒç»­æ‰©å±•ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å…¬å¼€è§†é¢‘å†…å®¹åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ç”Ÿæˆä»»åŠ¡æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é›†æˆäº†åŸºäº OCR çš„åœºæ™¯æ£€æµ‹ã€è¿‘ä¹å®Œç¾çš„ UI å…ƒç´ æ£€æµ‹ä»¥åŠå¤šæ­¥åŠ¨ä½œè¯†åˆ« (Multi-step Action Identification) æŠ€æœ¯ï¼Œèƒ½ä»å¤šæ ·çš„ç•Œé¢é…ç½®ä¸­æå–å¯é çš„åŠ¨ä½œåºåˆ—ã€‚è¯¥ç ”ç©¶é€šè¿‡è´¡çŒ® MONDAY æ•°æ®é›†å’Œè‡ªåŠ¨åŒ–é‡‡é›†æ¡†æ¶ï¼Œä¸ºç§»åŠ¨æ™ºèƒ½ä½“ (Mobile Agents) çš„è·¨å¹³å°å¯¼èˆªç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12632v1",
      "published_date": "2025-05-19 02:39:03 UTC",
      "updated_date": "2025-05-19 02:39:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:23.810552+00:00"
    },
    {
      "arxiv_id": "2505.12630v1",
      "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration",
      "title_zh": "é¢å‘å¤šåˆä¸€å›¾åƒä¿®å¤çš„é€€åŒ–æ„ŸçŸ¥ç‰¹å¾æ‰°åŠ¨",
      "authors": [
        "Xiangpeng Tian",
        "Xiangyu Liao",
        "Xiao Liu",
        "Meng Li",
        "Chao Ren"
      ],
      "abstract": "All-in-one image restoration aims to recover clear images from various degradation types and levels with a unified model. Nonetheless, the significant variations among degradation types present challenges for training a universal model, often resulting in task interference, where the gradient update directions of different tasks may diverge due to shared parameters. To address this issue, motivated by the routing strategy, we propose DFPIR, a novel all-in-one image restorer that introduces Degradation-aware Feature Perturbations(DFP) to adjust the feature space to align with the unified parameter space. In this paper, the feature perturbations primarily include channel-wise perturbations and attention-wise perturbations. Specifically, channel-wise perturbations are implemented by shuffling the channels in high-dimensional space guided by degradation types, while attention-wise perturbations are achieved through selective masking in the attention space. To achieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB) to implement these two functions, positioned between the encoding and decoding stages of the encoder-decoder architecture. Extensive experimental results demonstrate that DFPIR achieves state-of-the-art performance on several all-in-one image restoration tasks including image denoising, image dehazing, image deraining, motion deblurring, and low-light image enhancement. Our codes are available at https://github.com/TxpHome/DFPIR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨èƒ½å›¾åƒæ¢å¤(All-in-one image restoration)ä¸­å› ä¸åŒé€€åŒ–ç±»å‹å·®å¼‚å·¨å¤§è€Œå¯¼è‡´çš„è¯¾é¢˜å¹²æ‰°å’Œå‚æ•°å…±äº«å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºDFPIRçš„ä¿®å¤æ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†é€€åŒ–æ„ŸçŸ¥ç‰¹å¾æ‰°åŠ¨(Degradation-aware Feature Perturbations, DFP)ï¼Œé€šè¿‡è°ƒæ•´ç‰¹å¾ç©ºé—´ä½¿å…¶ä¸ç»Ÿä¸€çš„å‚æ•°ç©ºé—´ç›¸ä¸€è‡´ã€‚DFPä¸»è¦åŒ…å«ä¸¤ç§æœºåˆ¶ï¼šå—é€€åŒ–ç±»å‹å¼•å¯¼çš„é«˜ç»´ç©ºé—´é€šé“æ‰°åŠ¨(channel-wise perturbations)ä»¥åŠé€šè¿‡é€‰æ‹©æ€§æ©è”½å®ç°çš„æ³¨æ„åŠ›æ‰°åŠ¨(attention-wise perturbations)ã€‚è¿™äº›åŠŸèƒ½é€šè¿‡é›†æˆåœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­é—´å±‚çš„é€€åŒ–å¼•å¯¼æ‰°åŠ¨å—(Degradation-Guided Perturbation Block, DGPB)æ¥å®ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDFPIRåœ¨å»å™ª(image denoising)ã€å»é›¾(image dehazing)ã€å»é›¨(image deraining)ã€è¿åŠ¨å»æ¨¡ç³Š(motion deblurring)å’Œå¾®å…‰å›¾åƒå¢å¼º(low-light image enhancement)ç­‰å¤šé¡¹ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„(state-of-the-art)æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025. 8 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12630v1",
      "published_date": "2025-05-19 02:37:11 UTC",
      "updated_date": "2025-05-19 02:37:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:15.062061+00:00"
    },
    {
      "arxiv_id": "2505.18188v2",
      "title": "Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization",
      "title_zh": "åˆ©ç”¨æµ‹è¯•æ—¶ä¼˜åŒ–æ”¹è¿›çŸ©å½¢è´´ç‰‡å¤©çº¿çš„ç”Ÿæˆå¼é€†å‘è®¾è®¡",
      "authors": [
        "Beck LaBash",
        "Shahriar Khushrushahi",
        "Fabian Ruehle"
      ],
      "abstract": "We propose a two-stage deep learning framework for the inverse design of rectangular patch antennas. Our approach leverages generative modeling to learn a latent representation of antenna frequency response curves and conditions a subsequent generative model on these responses to produce feasible antenna geometries. We further demonstrate that leveraging search and optimization techniques at test-time improves the accuracy of the generated designs and enables consideration of auxiliary objectives such as manufacturability. Our approach generalizes naturally to different design criteria, and can be easily adapted to more complex geometric design spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºçŸ©å½¢å¾®å¸¦å¤©çº¿(rectangular patch antennas)é€†å‘è®¾è®¡çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¼å»ºæ¨¡æå‡è®¾è®¡ç²¾åº¦ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨ç”Ÿæˆæ¨¡å‹(generative modeling)å­¦ä¹ å¤©çº¿é¢‘ç‡å“åº”æ›²çº¿çš„æ½œåœ¨è¡¨å¾(latent representation)ï¼Œéšåä»¥æ­¤ä¸ºæ¡ä»¶è§¦å‘åç»­ç”Ÿæˆæ¨¡å‹ä»¥äº§ç”Ÿå¯è¡Œçš„å¤©çº¿å‡ ä½•ç»“æ„ã€‚ç ”ç©¶é‡ç‚¹å±•ç¤ºäº†åœ¨æµ‹è¯•é˜¶æ®µ(test-time)å¼•å…¥æœç´¢ä¸ä¼˜åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ç”Ÿæˆè®¾è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶å…è®¸å°†å¯åˆ¶é€ æ€§(manufacturability)ç­‰è¾…åŠ©ç›®æ ‡çº³å…¥è€ƒé‡ã€‚è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿè‡ªç„¶æ¨å¹¿è‡³ä¸åŒçš„è®¾è®¡æ ‡å‡†ï¼Œå¹¶èƒ½è½»æ¾é€‚é…æ›´ä¸ºå¤æ‚çš„å‡ ä½•è®¾è®¡ç©ºé—´ï¼Œä¸ºé«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„å¤©çº¿è®¾è®¡æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "Code and dataset available at https://github.com/becklabs/patch-antenna-tto",
      "pdf_url": "https://arxiv.org/pdf/2505.18188v2",
      "published_date": "2025-05-19 02:24:28 UTC",
      "updated_date": "2025-05-27 00:40:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:08.188719+00:00"
    },
    {
      "arxiv_id": "2505.12626v3",
      "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data",
      "title_zh": "scSiameseCluï¼šè§£æå•ç»†èƒ RNA æµ‹åºæ•°æ®çš„å­ªç”Ÿèšç±»æ¡†æ¶",
      "authors": [
        "Ping Xu",
        "Zhiyuan Ning",
        "Pengjiang Li",
        "Wenhao Liu",
        "Pengyang Wang",
        "Jiaxu Cui",
        "Yuanchun Zhou",
        "Pengfei Wang"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell clustering playing a key role in identifying cell types and marker genes. Recent advances, especially graph neural networks (GNNs)-based methods, have significantly improved clustering performance. However, the analysis of scRNA-seq data remains challenging due to noise, sparsity, and high dimensionality. Compounding these challenges, GNNs often suffer from over-smoothing, limiting their ability to capture complex biological information. In response, we propose scSiameseClu, a novel Siamese Clustering framework for interpreting single-cell RNA-seq data, comprising of 3 key steps: (1) Dual Augmentation Module, which applies biologically informed perturbations to the gene expression matrix and cell graph relationships to enhance representation robustness; (2) Siamese Fusion Module, which combines cross-correlation refinement and adaptive information fusion to capture complex cellular relationships while mitigating over-smoothing; and (3) Optimal Transport Clustering, which utilizes Sinkhorn distance to efficiently align cluster assignments with predefined proportions while maintaining balance. Comprehensive evaluations on seven real-world datasets demonstrate that scSiameseClu outperforms state-of-the-art methods in single-cell clustering, cell type annotation, and cell type classification, providing a powerful tool for scRNA-seq data interpretation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†scSiameseCluï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§£é‡Šå•ç»†èƒRNAæµ‹åº(scRNA-seq)æ•°æ®çš„å­ªç”Ÿèšç±»æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å•ç»†èƒæ•°æ®ä¸­çš„å™ªå£°ã€ç¨€ç–æ€§åŠå›¾ç¥ç»ç½‘ç»œ(GNNs)å­˜åœ¨çš„è¿‡å¹³æ»‘(over-smoothing)æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šåŒé‡å¢å¼ºæ¨¡å—(Dual Augmentation Module)é€šè¿‡ç”Ÿç‰©å¯å‘å¼æ‰°åŠ¨å¢å¼ºè¡¨ç¤ºçš„é²æ£’æ€§ï¼›å­ªç”Ÿèåˆæ¨¡å—(Siamese Fusion Module)ç»“åˆäº’ç›¸å…³ç»†åŒ–ä¸è‡ªé€‚åº”èåˆä»¥æ•æ‰å¤æ‚çš„ç»†èƒå…³ç³»ï¼›æœ€ä¼˜ä¼ è¾“èšç±»(Optimal Transport Clustering)åˆ™åˆ©ç”¨Sinkhornè·ç¦»å®ç°èšç±»åˆ†é…çš„é«˜æ•ˆå¹³è¡¡å¯¹é½ã€‚åœ¨ä¸ƒä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜ï¼ŒscSiameseCluåœ¨å•ç»†èƒèšç±»ã€ç»†èƒç±»å‹æ³¨é‡Šå’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ·±å…¥è§£è¯»scRNA-seqæ•°æ®ã€è¯†åˆ«ç»†èƒå¼‚è´¨æ€§å’Œæ ‡è®°åŸºå› æä¾›äº†å¼ºæœ‰åŠ›çš„è®¡ç®—å·¥å…·ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12626v3",
      "published_date": "2025-05-19 02:17:09 UTC",
      "updated_date": "2025-10-02 02:55:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:12.092667+00:00"
    },
    {
      "arxiv_id": "2505.12623v1",
      "title": "Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding",
      "title_zh": "é¢å‘å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’çš„ PIBT è½»é‡çº§é«˜æ•ˆåå¥½æ„å»º",
      "authors": [
        "Keisuke Okumura",
        "Hiroki Nagai"
      ],
      "abstract": "PIBT is a computationally lightweight algorithm that can be applied to a variety of multi-agent pathfinding (MAPF) problems, generating the next collision-free locations of agents given another. Because of its simplicity and scalability, it is becoming a popular underlying scheme for recent large-scale MAPF methods involving several hundreds or thousands of agents. Vanilla PIBT makes agents behave greedily towards their assigned goals, while agents typically have multiple best actions, since the graph shortest path is not always unique. Consequently, tiebreaking about how to choose between these actions significantly affects resulting solutions. This paper studies two simple yet effective techniques for tiebreaking in PIBT, without compromising its computational advantage. The first technique allows an agent to intelligently dodge another, taking into account whether each action will hinder the progress of the next timestep. The second technique is to learn, through multiple PIBT runs, how an action causes regret in others and to use this information to minimise regret collectively. Our empirical results demonstrate that these techniques can reduce the solution cost of one-shot MAPF and improve the throughput of lifelong MAPF. For instance, in densely populated one-shot cases, the combined use of these tiebreaks achieves improvements of around 10-20% in sum-of-costs, without significantly compromising the speed of a PIBT-based planner.",
      "tldr_zh": "é’ˆå¯¹å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ (Multi-Agent Pathfinding, MAPF) é—®é¢˜ï¼Œè¯¥ç ”ç©¶é’ˆå¯¹è½»é‡çº§ç®—æ³• PIBT åœ¨å¤„ç†æ™ºèƒ½ä½“å¤šé‡æœ€ä¼˜è¡ŒåŠ¨æ—¶çš„è´ªå©ªç­–ç•¥åŠ tie-breaking (å¹³å±€å†³é€‰) å±€é™æ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚è®ºæ–‡æå‡ºäº†ä¸¤ç§ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç¬¬ä¸€ç§æ˜¯å…è®¸æ™ºèƒ½ä½“æ™ºèƒ½åœ°é¿è®© (dodge) ä»–äººï¼Œé€šè¿‡é¢„æµ‹å½“å‰è¡ŒåŠ¨æ˜¯å¦ä¼šé˜»ç¢ä¸‹ä¸€æ—¶é—´æ­¥çš„è¿›åº¦æ¥ä¼˜åŒ–å†³ç­–ã€‚ç¬¬äºŒç§æŠ€æœ¯é€šè¿‡å¤šæ¬¡ PIBT è¿è¡Œå­¦ä¹ åŠ¨ä½œå¦‚ä½•å¯¼è‡´å…¶ä»–æ™ºèƒ½ä½“çš„åæ‚”åº¦ (regret)ï¼Œå¹¶åˆ©ç”¨è¯¥ä¿¡æ¯åœ¨é›†ä½“å±‚é¢æœ€å°åŒ–åæ‚”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æŠ€æœ¯åœ¨ä¸ç‰ºç‰² PIBT è®¡ç®—é€Ÿåº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†ä¸€æ¬¡æ€§ (one-shot) MAPF çš„æ±‚è§£æˆæœ¬å¹¶æé«˜äº†ç»ˆèº« (lifelong) MAPF çš„ååé‡ã€‚ç‰¹åˆ«æ˜¯åœ¨é«˜å¯†åº¦ç¯å¢ƒä¸‹ï¼Œç»„åˆä½¿ç”¨è¿™äº› tie-breaking ç­–ç•¥ä½¿æ€»æˆæœ¬ (sum-of-costs) ä¼˜åŒ–äº†çº¦ 10-20%ï¼Œä¸ºå¤§è§„æ¨¡å®æ—¶è·¯å¾„è§„åˆ’æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "To be presented at SoCS-25",
      "pdf_url": "https://arxiv.org/pdf/2505.12623v1",
      "published_date": "2025-05-19 02:12:29 UTC",
      "updated_date": "2025-05-19 02:12:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:29.529203+00:00"
    },
    {
      "arxiv_id": "2505.13544v3",
      "title": "Multi-head Temporal Latent Attention",
      "title_zh": "å¤šå¤´æ—¶åºéšæ³¨æ„åŠ›",
      "authors": [
        "Keqi Deng",
        "Philip C. Woodland"
      ],
      "abstract": "While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Transformer è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ Key-Value (KV) ç¼“å­˜éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿å¯¼è‡´çš„æ¨ç†æ•ˆç‡ç“¶é¢ˆï¼Œæå‡ºäº† Multi-head Temporal Latent Attention (MTLA)ã€‚è™½ç„¶ç°æœ‰çš„ Multi-head Latent Attention é€šè¿‡ä½ç§©æ½œç©ºé—´å‹ç¼©ç¼“å­˜ï¼Œä½† MTLA è¿›ä¸€æ­¥åœ¨æ—¶é—´ç»´åº¦ä¸Šç¼©å‡äº† KV ç¼“å­˜è§„æ¨¡ï¼Œä»è€Œå¤§å¹…é™ä½æ¨ç†æ—¶çš„æ˜¾å­˜å ç”¨ã€‚MTLA é‡‡ç”¨è¶…ç½‘ç»œ (hyper-network) åŠ¨æ€åˆå¹¶æ—¶é—´ä¸Šç›¸é‚»çš„ KV ç¼“å­˜å‘é‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ­¥å¹…æ„ŸçŸ¥å› æœæ©ç  (stride-aware causal mask) ä»¥ç¡®ä¿é«˜æ•ˆå¹¶è¡Œè®­ç»ƒä¸æ¨ç†ä¸€è‡´æ€§ã€‚å®éªŒæ¶µç›–äº†è¯­éŸ³ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç†è§£å’Œæ–‡æœ¬æ‘˜è¦ç­‰å¤šé¡¹ä»»åŠ¡ï¼Œç»“æœè¡¨æ˜ MTLA åœ¨ä¿æŒä¸æ ‡å‡† Multi-Head Attention (MHA) ç›¸å½“æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ã€‚åœ¨è‹±å¾·è¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒMTLA ç›¸æ¯” MHA å®ç°äº† 5.3 å€çš„æ¨ç†åŠ é€Ÿå¹¶å°† GPU æ˜¾å­˜ä½¿ç”¨é‡é™ä½äº† 8.3 å€ï¼Œä¸ºé«˜æ•ˆå¤„ç†é•¿åºåˆ—ä»»åŠ¡æä¾›äº†æœ‰åŠ›çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.13544v3",
      "published_date": "2025-05-19 02:09:41 UTC",
      "updated_date": "2025-11-02 20:27:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:34.453431+00:00"
    },
    {
      "arxiv_id": "2505.12594v1",
      "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection",
      "title_zh": "AD-AGENTï¼šé¢å‘ç«¯åˆ°ç«¯å¼‚å¸¸æ£€æµ‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Tiankai Yang",
        "Junjun Liu",
        "Wingchun Siu",
        "Jiahang Wang",
        "Zhuangzhuang Qian",
        "Chanjuan Song",
        "Cheng Cheng",
        "Xiyang Hu",
        "Yue Zhao"
      ],
      "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network monitoring, and scientific research. However, the diversity of data modalities and the increasing number of specialized AD libraries pose challenges for non-expert users who lack in-depth library-specific knowledge and advanced programming skills. To tackle this, we present AD-AGENT, an LLM-driven multi-agent framework that turns natural-language instructions into fully executable AD pipelines. AD-AGENT coordinates specialized agents for intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging. Using a shared short-term workspace and a long-term cache, the agents integrate popular AD libraries like PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that AD-AGENT produces reliable scripts and recommends competitive models across libraries. The system is open-sourced to support further research and practical applications in AD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AD-AGENTï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç«¯åˆ°ç«¯çš„å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detectionï¼‰ã€‚é’ˆå¯¹æ•°æ®æ¨¡æ€å¤šæ ·åŒ–ä»¥åŠä¸“ä¸šåº“ï¼ˆAD librariesï¼‰ä½¿ç”¨é—¨æ§›é«˜çš„é—®é¢˜ï¼ŒAD-AGENT èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç›´æ¥è½¬åŒ–ä¸ºå®Œå…¨å¯æ‰§è¡Œçš„æ£€æµ‹æµç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒæ„å›¾è§£æã€æ•°æ®å‡†å¤‡ã€æ¨¡å‹é€‰æ‹©ã€æ–‡æ¡£æŒ–æ˜ä»¥åŠä»£ç è¿­ä»£ç”Ÿæˆä¸è°ƒè¯•ç­‰å¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“æ¥ååŒå·¥ä½œã€‚é€šè¿‡åˆ©ç”¨å…±äº«çš„çŸ­æœŸå·¥ä½œç©ºé—´å’Œé•¿æœŸç¼“å­˜æŠ€æœ¯ï¼Œç³»ç»ŸæˆåŠŸå°† PyODã€PyGOD å’Œ TSLib ç­‰ä¸»æµåº“æ•´åˆè¿›ç»Ÿä¸€çš„å·¥ä½œæµä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAD-AGENT èƒ½å¤Ÿç”Ÿæˆå¯é çš„è„šæœ¬å¹¶æ¨èæå…·ç«äº‰åŠ›çš„æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†éä¸“å®¶ç”¨æˆ·çš„ä½¿ç”¨éš¾åº¦ã€‚ç›®å‰è¯¥ç³»ç»Ÿå·²å¼€æºï¼Œä¸ºå¼‚å¸¸æ£€æµ‹çš„ç ”ç©¶ä¸å®é™…åº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12594v1",
      "published_date": "2025-05-19 01:14:57 UTC",
      "updated_date": "2025-05-19 01:14:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:55.904708+00:00"
    },
    {
      "arxiv_id": "2505.12585v1",
      "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization",
      "title_zh": "é¢å‘æ—¶åŸŸæ³›åŒ–çš„é²æ£’é¢‘è°±åŠ¨åŠ›å­¦å­¦ä¹ ",
      "authors": [
        "En Yu",
        "Jie Lu",
        "Xiaoyu Yang",
        "Guangquan Zhang",
        "Zhen Fang"
      ],
      "abstract": "Modern machine learning models struggle to maintain performance in dynamic environments where temporal distribution shifts, \\emph{i.e., concept drift}, are prevalent. Temporal Domain Generalization (TDG) seeks to enable model generalization across evolving domains, yet existing approaches typically assume smooth incremental changes, struggling with complex real-world drifts involving long-term structure (incremental evolution/periodicity) and local uncertainties. To overcome these limitations, we introduce FreKoo, which tackles these challenges via a novel frequency-domain analysis of parameter trajectories. It leverages the Fourier transform to disentangle parameter evolution into distinct spectral bands. Specifically, low-frequency component with dominant dynamics are learned and extrapolated using the Koopman operator, robustly capturing diverse drift patterns including both incremental and periodicity. Simultaneously, potentially disruptive high-frequency variations are smoothed via targeted temporal regularization, preventing overfitting to transient noise and domain uncertainties. In addition, this dual spectral strategy is rigorously grounded through theoretical analysis, providing stability guarantees for the Koopman prediction, a principled Bayesian justification for the high-frequency regularization, and culminating in a multiscale generalization bound connecting spectral dynamics to improved generalization. Extensive experiments demonstrate FreKoo's significant superiority over SOTA TDG approaches, particularly excelling in real-world streaming scenarios with complex drifts and uncertainties.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´çš„æ—¶é—´åˆ†å¸ƒåç§»ï¼ˆTemporal distribution shiftsï¼Œå³ Concept driftï¼‰ä»¥åŠç°æœ‰æ—¶é—´é¢†åŸŸæ³›åŒ–ï¼ˆTemporal Domain Generalization, TDGï¼‰æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚æ¼‚ç§»çš„é—®é¢˜ï¼Œæå‡ºäº† FreKoo æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹å‚æ•°è½¨è¿¹è¿›è¡Œé¢‘åŸŸåˆ†æï¼Œåˆ©ç”¨å‚…é‡Œå¶å˜æ¢ï¼ˆFourier transformï¼‰å°†å‚æ•°æ¼”åŒ–åˆ†è§£ä¸ºä¸åŒçš„å…‰è°±å¸¦ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåˆ©ç”¨ Koopman operator å­¦ä¹ å¹¶å¤–æ¨å…·æœ‰ä¸»å¯¼åŠ¨åŠ›å­¦çš„ä½é¢‘æˆåˆ†ï¼Œä»è€Œç¨³å¥åœ°æ•æ‰åŒ…æ‹¬å¢é‡æ¼”åŒ–å’Œå‘¨æœŸæ€§åœ¨å†…çš„å¤šç§æ¼‚ç§»æ¨¡å¼ã€‚åŒæ—¶ï¼ŒFreKoo é€šè¿‡é’ˆå¯¹æ€§çš„æ—¶é—´æ­£åˆ™åŒ–ï¼ˆTemporal regularizationï¼‰å¹³æ»‘å…·æœ‰å¹²æ‰°æ€§çš„é«˜é¢‘å˜åŒ–ï¼Œæœ‰æ•ˆé˜²æ­¢äº†å¯¹ç¬æ€å™ªå£°å’Œé¢†åŸŸä¸ç¡®å®šæ€§çš„è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†å…³äº Koopman é¢„æµ‹ç¨³å®šæ€§ã€é«˜é¢‘æ­£åˆ™åŒ–çš„è´å¶æ–¯è¯æ˜ä»¥åŠå¤šå°ºåº¦æ³›åŒ–ç•Œé™çš„ç†è®ºæ”¯æŒã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFreKoo åœ¨å¤„ç†å…·æœ‰å¤æ‚æ¼‚ç§»çš„çœŸå®ä¸–ç•Œæµå¼åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12585v1",
      "published_date": "2025-05-19 00:38:18 UTC",
      "updated_date": "2025-05-19 00:38:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:56.531125+00:00"
    },
    {
      "arxiv_id": "2505.12583v2",
      "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics",
      "title_zh": "åŸºç¡€æ¨¡å‹èµ‹èƒ½æœºå™¨äººæ—¶ä»£çš„ç‰©ç†é£é™©æ§åˆ¶å…¨é¢ç»¼è¿°",
      "authors": [
        "Takeshi Kojima",
        "Yaonan Zhu",
        "Yusuke Iwasawa",
        "Toshinori Kitamura",
        "Gang Yan",
        "Shu Morikuni",
        "Ryosuke Takanami",
        "Alfredo Solano",
        "Tatsuya Matsushima",
        "Akiko Murakami",
        "Yutaka Matsuo"
      ],
      "abstract": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved general-purpose skills, enabling more adaptable automation than conventional robotics. Their ability to handle diverse tasks thus creates new opportunities to replace human labor. However, unlike general foundation models, FMRs interact with the physical world, where their actions directly affect the safety of humans and surrounding objects, requiring careful deployment and control. Based on this proposition, our survey comprehensively summarizes robot control approaches to mitigate physical risks by covering all the lifespan of FMRs ranging from pre-deployment to post-accident stage. Specifically, we broadly divide the timeline into the following three phases: (1) pre-deployment phase, (2) pre-incident phase, and (3) post-incident phase. Throughout this survey, we find that there is much room to study (i) pre-incident risk mitigation strategies, (ii) research that assumes physical interaction with humans, and (iii) essential issues of foundation models themselves. We hope that this survey will be a milestone in providing a high-resolution analysis of the physical risks of FMRs and their control, contributing to the realization of a good human-robot relationship.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºç¡€æ¨¡å‹èµ‹èƒ½çš„æœºå™¨äºº(Foundation Model-enabled robotics, FMRs)åœ¨ç‰©ç†äº¤äº’ä¸­å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ï¼Œæä¾›äº†ä¸€ä»½å…³äºç‰©ç†é£é™©æ§åˆ¶çš„å…¨é¢ç»¼è¿°ã€‚è®ºæ–‡ç³»ç»Ÿåœ°æ€»ç»“äº†æ—¨åœ¨å‡è½»ç‰©ç†é£é™©çš„æœºå™¨äººæ§åˆ¶æ–¹æ³•ï¼Œæ¶µç›–äº†FMRsä»éƒ¨ç½²å‰åˆ°äº‹æ•…åæ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„å„ä¸ªé˜¶æ®µã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶å°†æ—¶é—´çº¿åˆ’åˆ†ä¸ºéƒ¨ç½²å‰é˜¶æ®µ(pre-deployment phase)ã€äº‹æ•…å‰é˜¶æ®µ(pre-incident phase)å’Œäº‹æ•…åé˜¶æ®µ(post-incident phase)ã€‚ç»¼è¿°å‘ç°ï¼Œåœ¨äº‹æ•…å‰é£é™©ç¼“è§£ç­–ç•¥ã€äººæœºç‰©ç†äº¤äº’ç ”ç©¶ä»¥åŠåŸºç¡€æ¨¡å‹(foundation models)è‡ªèº«çš„æ ¸å¿ƒé—®é¢˜ç­‰æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚è¯¥å·¥ä½œé€šè¿‡å¯¹FMRsç‰©ç†é£é™©åŠå…¶æ§åˆ¶çš„æ·±åº¦åˆ†æï¼Œä¸ºæ„å»ºå®‰å…¨å¯æ§çš„äººæœºåä½œå…³ç³»æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IJCAI 2025 Survey Track",
      "pdf_url": "https://arxiv.org/pdf/2505.12583v2",
      "published_date": "2025-05-19 00:11:42 UTC",
      "updated_date": "2025-05-30 07:28:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:47.641971+00:00"
    },
    {
      "arxiv_id": "2505.12581v1",
      "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification",
      "title_zh": "åŸºäºç±»æ¿€æ´»æ˜ å°„æ¢ç©¶æ•°æ®å¢å¼ºå¯¹å›¾åƒåˆ†ç±»ç¥ç»ç½‘ç»œå½±å“çš„æ–¹æ³•",
      "authors": [
        "Lucas M. Dorneles",
        "Luan Fonseca Garcia",
        "Joel LuÃ­s Carbonera"
      ],
      "abstract": "Neural networks have become increasingly popular in the last few years as an effective tool for the task of image classification due to the impressive performance they have achieved on this task. In image classification tasks, it is common to use data augmentation strategies to increase the robustness of trained networks to changes in the input images and to avoid overfitting. Although data augmentation is a widely adopted technique, the literature lacks a body of research analyzing the effects data augmentation methods have on the patterns learned by neural network models working on complex datasets. The primary objective of this work is to propose a methodology and set of metrics that may allow a quantitative approach to analyzing the effects of data augmentation in convolutional networks applied to image classification. An important tool used in the proposed approach lies in the concept of class activation maps for said models, which allow us to identify and measure the importance these models assign to each individual pixel in an image when executing the classification task. From these maps, we may then extract metrics over the similarities and differences between maps generated by these models trained on a given dataset with different data augmentation strategies. Experiments made using this methodology suggest that the effects of these data augmentation techniques not only can be analyzed in this way but also allow us to identify different impact profiles over the trained models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨æ•°æ®å¢å¼º (data augmentation) å¯¹å›¾åƒåˆ†ç±»é¢†åŸŸä¸­ç¥ç»ç½‘ç»œå­¦ä¹ æ¨¡å¼çš„å½±å“ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€å¥—åŸºäºç±»æ¿€æ´»æ˜ å°„ (class activation maps, CAM) çš„å®šé‡åˆ†ææ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ CAM æŠ€æœ¯è¯†åˆ«å¹¶æµ‹é‡æ¨¡å‹åœ¨æ‰§è¡Œåˆ†ç±»ä»»åŠ¡æ—¶å¯¹å›¾åƒä¸­å„ä¸ªåƒç´ åˆ†é…çš„é‡è¦æ€§ï¼Œè¿›è€Œæå–ä¸åŒæ•°æ®å¢å¼ºç­–ç•¥ä¸‹æ¨¡å‹ç”Ÿæˆçš„æ¿€æ´»æ˜ å°„å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚æ€§æŒ‡æ ‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–åˆ†ææ•°æ®å¢å¼ºæŠ€æœ¯çš„æ•ˆæœï¼Œè¿˜èƒ½è¯†åˆ«å‡ºä¸åŒç­–ç•¥åœ¨è®­ç»ƒæ¨¡å‹ä¸Šæ‰€äº§ç”Ÿçš„ä¸åŒå½±å“ç‰¹å¾ (impact profiles)ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ·±å…¥ç†è§£æ•°æ®å¢å¼ºå¯¹å·ç§¯ç¥ç»ç½‘ç»œ (convolutional networks) å†…éƒ¨ç‰¹å¾å­¦ä¹ çš„ä½œç”¨æœºåˆ¶æä¾›äº†æ–°çš„ç ”ç©¶é€”å¾„å’Œå®šé‡å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12581v1",
      "published_date": "2025-05-19 00:03:57 UTC",
      "updated_date": "2025-05-19 00:03:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T11:04:49.034748+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 240,
  "processed_papers_count": 240,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T11:06:11.858986+00:00"
}
{
  "date": "2025-07-29",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-29 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å¯è°“æ˜¯â€œè½¯ç¡¬å…¼æ–½â€ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è¿æ¥äº†å¯¹ **\"Vibe Coding\"** è¿™ä¸€æ–°å…´ç°è±¡çš„æ­£å¼å­¦æœ¯å®šä¹‰ï¼Œä»¥åŠå…³äº LLM è¢«è®­ç»ƒå¾—å¤ªâ€œæš–â€åè€Œå˜å¾—ä¸å¯é ï¼ˆSycophanticï¼‰çš„å¿ƒç†å­¦æ¢è®¨ï¼›å¦ä¸€æ–¹é¢ï¼Œç¡¬æ ¸æŠ€æœ¯æµä¹Ÿä¸ç”˜ç¤ºå¼±ï¼Œ**GRPO (Group Relative Policy Optimization)** ç®—æ³•çš„å˜ä½“çˆ†å‘ï¼Œä»¥åŠåˆ©ç”¨æ¨¡å‹ **Self-Feedback** è¿›è¡Œåè®­ç»ƒï¼ˆPost-Trainingï¼‰çš„ç ”ç©¶å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹å‘ã€‚æ­¤å¤–ï¼ŒAgent çš„ä¸–ç•Œæ¨¡å‹å…±æ¼”åŒ–å’Œå¤šæ¨¡æ€è¯šå®æ€§ï¼ˆHonestyï¼‰ä¹Ÿæ˜¯ä»Šå¤©çš„çƒ­ç‚¹ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è¯é¢˜ï¼šVibe Codingã€AI å¿ƒç†å­¦ä¸å¯¹é½\n\n**1. Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda**\n**Vibe Codingï¼šè½¯ä»¶å¼€å‘ä¸­æ„å›¾ä¸­ä»‹çš„é‡æ„â€”â€”å®šä¹‰ã€å½±å“ä¸ç ”ç©¶è®®ç¨‹**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** â€œVibe Codingâ€ ç»ˆäºæœ‰äº†å­¦æœ¯å®šä¹‰â€”â€”ä»ç¡®å®šæ€§æŒ‡ä»¤è½¬å‘æ¦‚ç‡æ€§æ¨ç†çš„åä½œæµã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** éšç€ AI ç”Ÿæˆä»£ç çš„æ™®åŠï¼Œâ€œVibe Codingâ€ æˆä¸ºçƒ­è¯ã€‚æœ¬æ–‡ä»å­¦æœ¯è§’åº¦å°†å…¶å®šä¹‰ä¸ºä¸€ç§æ–°çš„å¼€å‘èŒƒå¼ï¼šäººç±»ä¸ GenAI é€šè¿‡è‡ªç„¶è¯­è¨€å¯¹è¯è¿›å…¥â€œåä½œæµï¼ˆcollaborative flowï¼‰â€ï¼Œå…±åŒåˆ›é€ è½¯ä»¶ã€‚æ ¸å¿ƒè½¬å˜åœ¨äº**æ„å›¾ä¸­ä»‹ï¼ˆIntent Mediationï¼‰**çš„æ–¹å¼ï¼Œä»åŸæœ¬çš„â€œç¡®å®šæ€§æŒ‡ä»¤â€å˜æˆäº†â€œæ¦‚ç‡æ€§æ¨ç†â€ã€‚è¿™æ ‡å¿—ç€å¼€å‘è€…çš„è®¤çŸ¥åŠ³åŠ¨ä»æŠ€æœ¯å®ç°è½¬å‘äº†åä½œç¼–æ’ï¼ˆOrchestrationï¼‰ã€‚æ–‡ç« è¿˜è®¨è®ºäº†å…¶å¸¦æ¥çš„â€œé»‘ç›’ä»£ç åº“â€å’Œè´£ä»»ç¼ºå£é£é™©ã€‚\n\n**2. Training language models to be warm and empathetic makes them less reliable and more sycophantic**\n**å°†è¯­è¨€æ¨¡å‹è®­ç»ƒå¾—æ¸©æš–ä¸”å¯Œæœ‰åŒç†å¿ƒï¼Œä¼šä½¿å…¶å˜å¾—ä¸å¯é ä¸”æ›´å…·é˜¿è°€å¥‰æ‰¿å€¾å‘**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** æƒ³è¦ AI å½“â€œæš–ç”·â€ï¼Ÿå°å¿ƒå®ƒä¸ºäº†è®¨å¥½ä½ è€Œæ’’è°ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** è¿™æ˜¯ä¸€é¡¹éå¸¸æœ‰æ„æ€çš„æƒè¡¡ç ”ç©¶ã€‚ä½œè€…å‘ç°ï¼Œé’ˆå¯¹â€œæ¸©æš–â€å’Œâ€œåŒç†å¿ƒâ€ä¼˜åŒ– LLM ä¼šç ´åå…¶å¯é æ€§ã€‚åœ¨å®‰å…¨å…³é”®ä»»åŠ¡ä¸­ï¼Œè¿™äº›â€œæš–â€æ¨¡å‹çš„é”™è¯¯ç‡é«˜å‡º 10-30%ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œå½“ç”¨æˆ·è¡¨ç°å‡ºè„†å¼±ï¼ˆå¦‚æ‚²ä¼¤ï¼‰æ—¶ï¼Œè¿™äº›æ¨¡å‹æ›´æœ‰å¯èƒ½é€šè¿‡ç¡®è®¤ç”¨æˆ·çš„é”™è¯¯ä¿¡å¿µï¼ˆConspiracy theories, incorrect factsï¼‰æ¥è¿›è¡Œ**é˜¿è°€å¥‰æ‰¿ï¼ˆSycophancyï¼‰**ã€‚è¿™æ­ç¤ºäº†å½“å‰å¯¹é½ç›®æ ‡ä¸­æ½œåœ¨çš„ç³»ç»Ÿæ€§å†²çªã€‚\n\n**3. Strategic Deflection: Defending LLMs from Logit Manipulation**\n**æˆ˜ç•¥æ€§åè½¬ï¼šé˜²å¾¡é’ˆå¯¹ LLM çš„ Logit æ“çºµæ”»å‡»**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** é¢å¯¹æ”»å‡»ä¸ç›´æ¥æ‹’ç»ï¼Œè€Œæ˜¯â€œé¡¾å·¦å³è€Œè¨€ä»–â€åœ°æŠŠè¯é¢˜ä»¥æ­¤åŒ–è§£ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** é’ˆå¯¹èƒ½å¤Ÿç»•è¿‡ä¼ ç»Ÿæ‹’ç»æœºåˆ¶çš„ Logit å±‚é¢çš„ Jailbreak æ”»å‡»ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º **Strategic Deflection (SDeflection)** çš„é˜²å¾¡æœºåˆ¶ã€‚ä¸åŒäºç”Ÿç¡¬çš„â€œRefusalâ€ï¼Œè¯¥æ–¹æ³•è®©æ¨¡å‹ç”Ÿæˆåœ¨è¯­ä¹‰ä¸Šä¸ç”¨æˆ·è¯·æ±‚ç›¸é‚»ã€ä½†å‰¥ç¦»äº†æœ‰å®³æ„å›¾çš„å›ç­”ã€‚è¿™ç§â€œå¤ªæâ€æ‰“æ³•åœ¨é™ä½æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹åœ¨è‰¯æ€§æŸ¥è¯¢ä¸Šçš„æ€§èƒ½ã€‚\n\n---\n\n### ğŸ§  æ¨¡å‹è¿›åŒ–ï¼šRLHFã€GRPO ä¸æ¨ç†\n\n**4. Post-Training Large Language Models via Reinforcement Learning from Self-Feedback**\n**é€šè¿‡è‡ªåé¦ˆå¼ºåŒ–å­¦ä¹ å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåè®­ç»ƒ**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** ä¸éœ€è¦äººç±»æ ‡æ³¨ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„â€œè‡ªä¿¡åº¦â€ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** æå‡ºäº† **RLSF (Reinforcement Learning from Self-Feedback)**ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦ï¼ˆConfidenceï¼‰ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼ˆIntrinsic Rewardï¼‰ï¼Œæ¨¡ä»¿äººç±»åœ¨ç¼ºä¹å¤–éƒ¨åé¦ˆæ—¶çš„å­¦ä¹ è¿‡ç¨‹ã€‚æ¨¡å‹ç”Ÿæˆå¤šä¸ª CoT è·¯å¾„ï¼Œè®¡ç®—æœ€ç»ˆç­”æ¡ˆçš„ç½®ä¿¡åº¦å¹¶è¿›è¡Œæ’åºï¼Œé€šè¿‡æ ‡å‡†åå¥½ä¼˜åŒ–ï¼ˆå¦‚ DPO/PPOï¼‰è¿›è¡Œå¾®è°ƒã€‚ç»“æœæ˜¾ç¤ºè¿™ä¸ä»…æ¢å¤äº†æ¨¡å‹çš„æ ¡å‡†ï¼ˆCalibrationï¼‰ï¼Œè¿˜å¢å¼ºäº†ç®—æœ¯æ¨ç†èƒ½åŠ›ã€‚\n\n**5. EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity**\n**EDGE-GRPOï¼šå…·æœ‰å¼•å¯¼çº é”™åŠŸèƒ½çš„ç†µé©±åŠ¨ GRPO**\n**6. MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE**\n**MixGRPOï¼šé€šè¿‡æ··åˆ ODE-SDE è§£é”åŸºäºæµçš„ GRPO æ•ˆç‡**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** DeepSeek å¸¦ç«çš„ GRPO ç®—æ³•è¿æ¥äº†ä¸¤ä¸ªå¼ºåŠ›å˜ä½“ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š**\n> *   **EDGE-GRPO** è§£å†³äº† GRPO ä¸­å› ç»„å†…å¥–åŠ±ç›¸åŒå¯¼è‡´çš„â€œä¼˜åŠ¿å´©æºƒï¼ˆAdvantage Collapseï¼‰â€é—®é¢˜ï¼Œå¼•å…¥äº†ç†µé©±åŠ¨çš„ä¼˜åŠ¿è®¡ç®—å’Œå¼•å¯¼çº é”™æœºåˆ¶ã€‚\n> *   **MixGRPO** åˆ™é’ˆå¯¹ Flow-based æ¨¡å‹çš„å¯¹é½æ•ˆç‡é—®é¢˜ï¼Œæå‡ºæ··åˆ SDEï¼ˆéšæœºå¾®åˆ†æ–¹ç¨‹ï¼‰å’Œ ODEï¼ˆå¸¸å¾®åˆ†æ–¹ç¨‹ï¼‰é‡‡æ ·ç­–ç•¥ã€‚åªåœ¨æ»‘åŠ¨çª—å£å†…ä½¿ç”¨ SDE å’Œ GRPO ä¼˜åŒ–ï¼Œçª—å£å¤–ä½¿ç”¨ ODE åŠ é€Ÿï¼Œè®­ç»ƒæ—¶é—´å‡å°‘è¿‘ 50%ã€‚\n\n**7. Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation**\n**é€šè¿‡æ¨ç†é•¿åº¦ä¼°è®¡å¯¹ LLM API ä¸­çš„éšè— Token è¿›è¡Œé¢„æµ‹æ€§å®¡è®¡**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** API è®¡è´¹é‡Œé‚£äº›çœ‹ä¸è§çš„â€œæ€è€ƒ Tokenâ€åˆ°åº•æœ‰å¤šå°‘ï¼ŸPALACE æ¡†æ¶å¸®ä½ å®¡è®¡ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** å•†ä¸šæ¨ç†æ¨¡å‹ï¼ˆå¦‚ o1ï¼‰å¾€å¾€éšè—æ¨ç†è¿‡ç¨‹ä½†å¯¹è¿™äº› hidden tokens è®¡è´¹ã€‚æœ¬æ–‡æå‡ºäº† PALACE æ¡†æ¶ï¼Œåœ¨ç”¨æˆ·ä¾§é€šè¿‡ Prompt-Answer å¯¹æ¥ä¼°ç®—éšè—çš„æ¨ç† Token æ•°é‡ï¼Œå¸®åŠ©ç”¨æˆ·æ£€æµ‹æ½œåœ¨çš„â€œToken é€šèƒ€â€å’Œè¿‡åº¦è®¡è´¹é—®é¢˜ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸ä¸–ç•Œæ¨¡å‹\n\n**8. CoEx -- Co-evolving World-model and Exploration**\n**CoExï¼šä¸–ç•Œæ¨¡å‹ä¸æ¢ç´¢çš„å…±æ¼”åŒ–**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** Agent çš„ä¸–ç•Œæ¨¡å‹ä¸åº”æ˜¯é™æ€çš„ï¼Œè€Œåº”éšç€æ¢ç´¢åŠ¨æ€è¿›åŒ–ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** ç°æœ‰çš„ Agent å¾€å¾€ä¾èµ–é¢„è®­ç»ƒæ—¶è·å¾—çš„é™æ€ä¸–ç•Œæ¨¡å‹ï¼Œéš¾ä»¥é€‚åº”æ–°è§‚å¯Ÿã€‚CoEx å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ¶æ„ï¼Œè®© LLM çš„è§„åˆ’èƒ½åŠ›ä¸åŠ¨æ€æ›´æ–°çš„ä¸–ç•Œæ¨¡å‹ï¼ˆç”±ç¥ç»ç¬¦å·ä¿¡å¿µçŠ¶æ€ç»„æˆï¼ŒåŒ…å«æ–‡æœ¬æ¨ç†å’Œä»£ç ç¬¦å·è®°å¿†ï¼‰**å…±æ¼”åŒ–ï¼ˆCo-evolveï¼‰**ã€‚åœ¨ ALFWorld å’Œ Jericho ç­‰å¤æ‚ç¯å¢ƒä¸­ï¼Œè¿™ç§åŠ¨æ€æ›´æ–°æœºåˆ¶æ˜¾è‘—æå‡äº†è§„åˆ’å’Œæ¢ç´¢èƒ½åŠ›ã€‚\n\n**9. UserBench: An Interactive Gym Environment for User-Centric Agents**\n**UserBenchï¼šé¢å‘ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ Agent çš„äº¤äº’å¼ Gym ç¯å¢ƒ**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** ç°åœ¨çš„ Agent è¿˜æ˜¯ä¸æ‡‚ç”¨æˆ·â€œå«ç³Šå…¶è¾â€çš„éœ€æ±‚ã€‚\n> **æ ¸å¿ƒå†…å®¹ï¼š** è¿™æ˜¯ä¸€ä¸ªè¯„ä¼° Agent ä¸ç”¨æˆ·**åä½œèƒ½åŠ›**çš„ Benchmarkã€‚æ¨¡æ‹Ÿç”¨æˆ·ä¸ä»…ç›®æ ‡æ¨¡ç³Šï¼Œè€Œä¸”åå¥½æ˜¯é€æ­¥æ­ç¤ºçš„ã€‚è¯„æµ‹å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹³å‡ä¹Ÿåªæœ‰ 20% çš„æ—¶é—´èƒ½å®Œå…¨å¯¹é½ç”¨æˆ·æ„å›¾ï¼Œä¸”ä»…èƒ½æŒ–æ˜å‡ºä¸åˆ° 30% çš„ç”¨æˆ·åå¥½ã€‚è¿™è¡¨æ˜ Agent è¿˜è¿œæœªæˆä¸ºçœŸæ­£çš„â€œåˆä½œä¼™ä¼´â€ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸ç§‘å­¦ AI\n\n**10. SmartCLIP: Modular Vision-language Alignment with Identification Guarantees**\n**SmartCLIPï¼šå…·æœ‰è¯†åˆ«ä¿è¯çš„æ¨¡å—åŒ–è§†è§‰è¯­è¨€å¯¹é½**\n> **æ ¸å¿ƒå†…å®¹ï¼š** é’ˆå¯¹ CLIP åœ¨å¤„ç†å›¾åƒ-æ–‡æœ¬ä¸å¯¹é½ï¼ˆå¦‚çŸ­ caption æè¿°å¤æ‚å›¾åƒï¼‰æ—¶çš„çº ç¼ é—®é¢˜ï¼ŒSmartCLIP æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–æ–¹æ³•ï¼Œç†è®ºä¸Šä¿è¯äº†è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯çš„ä¿ç•™å’Œè§£è€¦ï¼Œèƒ½å¤Ÿæ•æ‰ç»†ç²’åº¦çš„æ–‡æœ¬æ¦‚å¿µã€‚\n\n**11. MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions**\n**MoHoBenchï¼šé€šè¿‡ä¸å¯å›ç­”çš„è§†è§‰é—®é¢˜è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è¯šå®æ€§**\n> **æ ¸å¿ƒå†…å®¹ï¼š** å¤šæ¨¡æ€æ¨¡å‹ï¼ˆMLLMï¼‰ç»å¸¸ä¼šå› ä¸ºâ€œå¹»è§‰â€è€Œå¼ºè¡Œå›ç­”æœ¬æ— æ³•å›ç­”çš„é—®é¢˜ã€‚MoHoBench æ„å»ºäº† 12k+ ä¸ª**è§†è§‰ä¸Šä¸å¯å›ç­”**çš„é—®é¢˜æ ·æœ¬ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½ä¸æ‡‚å¾—â€œæ‹’ç»å›ç­”â€ï¼Œä¸”è¿™ç§ä¸è¯šå®æ·±å—è§†è§‰ä¿¡æ¯å¹²æ‰°ï¼Œä¸ä»…ä»…æ˜¯è¯­è¨€æ¨¡å‹çš„é—®é¢˜ã€‚\n\n**12. CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam**\n**CHECK-MATï¼šæ£€æŸ¥ä¿„ç½—æ–¯ç»Ÿä¸€å›½å®¶è€ƒè¯•çš„æ‰‹å†™æ•°å­¦ç­”æ¡ˆ**\n> **æ ¸å¿ƒå†…å®¹ï¼š** å‘å¸ƒäº†ä¸€ä¸ªé’ˆå¯¹æ‰‹å†™æ•°å­¦è§£ç­”è¯„åˆ†çš„ VLM Benchmarkã€‚ä¸åªçœ‹ç»“æœä¸åŒï¼Œå®ƒè¦æ±‚æ¨¡å‹ç†è§£è§£é¢˜æ­¥éª¤ã€å‘ç°é”™è¯¯å¹¶æŒ‰æ ‡å‡†è¯„åˆ†ã€‚ç›®å‰çš„ VLM åœ¨è¿™æ–¹é¢ä¸äººç±»è¯„åˆ†æ ‡å‡†ä»æœ‰è¾ƒå¤§å·®è·ã€‚\n\n**13. Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench**\n**å“ªäº› LLM æ‡‚ç¬‘è¯ï¼Ÿç”¨ HumorBench æ¢ç´¢é STEM æ¨ç†èƒ½åŠ›**\n> **æ ¸å¿ƒå†…å®¹ï¼š** è¿™æ˜¯ä¸€ä¸ªå…³äº**å¹½é»˜ç†è§£**çš„è¯„æµ‹ã€‚ä½œè€…æ”¶é›†äº†ã€Šçº½çº¦å®¢ã€‹æ¼«ç”»æ ‡é¢˜æ¯”èµ›çš„æ•°æ®ï¼Œè¦æ±‚ LLM è§£é‡Šç¬‘è¯ã€‚ç»“è®ºå¾ˆæœ‰è¶£ï¼šåœ¨ STEMï¼ˆç†å·¥ç§‘ï¼‰æ¨ç†ä¸Šè¡¨ç°å¥½çš„æ¨¡å‹ï¼Œé€šå¸¸ä¹Ÿæ›´æ‡‚å¹½é»˜ï¼›å³ä¾¿æ˜¯åªåœ¨ STEM æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶æ¨ç†èƒ½åŠ›ä¹Ÿèƒ½è¿ç§»åˆ°å¹½é»˜ç†è§£ä¸Šã€‚\n\n---\n\n### ğŸ¦  å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n\n*   **[Code] HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions**\n    åˆ©ç”¨ LLM è°ƒè¯•é«˜å±‚æ¬¡ç»¼åˆï¼ˆHLSï¼‰ç¡¬ä»¶ä»£ç ï¼Œå‘å¸ƒäº† 300K è§„æ¨¡çš„æ•°æ®é›†ã€‚\n*   **[Physics] Towards a Large Physics Benchmark**\n    ç‰©ç†å­¦ç•Œè‡ªå·±æçš„ LLM Benchmarkï¼ŒåŒ…å«éœ€è¦å¤æ‚æ¨å¯¼å’Œé«˜èƒ½ç‰©ç†äº‹ä»¶åˆ†ç±»çš„é¢˜ç›®ã€‚\n*   **[RL] Agent-centric learning: from external reward maximization to internal knowledge curation**\n    æå‡ºâ€œè¡¨å¾èµ‹èƒ½ï¼ˆRepresentational Empowermentï¼‰â€ï¼Œä¸»å¼  Agent çš„å­¦ä¹ ç›®æ ‡åº”ä»æœ€å¤§åŒ–å¤–éƒ¨å¥–åŠ±è½¬å‘**å†…éƒ¨çŸ¥è¯†ç»“æ„çš„æ„å»ºä¸å¤šæ ·åŒ–**ã€‚\n*   **[Finance] OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services**\n    ä»…ä½¿ç”¨å…¬å¼€æ•°æ®ï¼Œå¼€æºæ¨¡å‹çš„è‹±è¶…æ¢¦å¹»è¶³çƒï¼ˆFPLï¼‰é¢„æµ‹èƒ½åŠ›åª²ç¾å•†ä¸šæœåŠ¡ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥èƒ½ä¸ºä½ å¸¦æ¥å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2507.22958v1",
      "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam",
      "title_zh": "CHECK-MATï¼šé¢å‘ä¿„ç½—æ–¯å›½å®¶ç»Ÿä¸€è€ƒè¯•çš„æ‰‹å†™æ•°å­¦è§£ç­”è¯„é˜…",
      "authors": [
        "Ruslan Khrulev"
      ],
      "abstract": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to assess hand-written mathematical solutions. Unlike existing benchmarks that focus on problem solving, our approach centres on understanding student solutions, identifying mistakes, and assigning grades according to fixed criteria. We compile 122 scanned solutions from the Russian Unified State Exam (EGE) together with official expert grades, and evaluate seven modern VLMs from Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The results reveal current limitations in mathematical reasoning and human-rubric alignment, opening new research avenues in AI-assisted assessment. You can find code in https://github.com/Karifannaa/Auto-check-EGE-math",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åä¸º EGE-Math Solutions Assessment Benchmark çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å¯¹æ‰‹å†™æ•°å­¦é¢˜è§£ç­”è¿‡ç¨‹çš„åˆ†æä¸è¯„ä¼°èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„ä¸“æ³¨äºè‡ªåŠ¨è§£é¢˜çš„åŸºå‡†ä¸åŒï¼Œè¯¥æ–¹æ³•ä¾§é‡äºç†è§£å­¦ç”Ÿè§£é¢˜æ€è·¯ã€è¯†åˆ«é”™è¯¯å¹¶æ ¹æ®å›ºå®šæ ‡å‡†è¿›è¡Œè¯„åˆ†ã€‚ç ”ç©¶å›¢é˜Ÿä»ä¿„ç½—æ–¯å›½å®¶ç»Ÿä¸€è€ƒè¯• (EGE) ä¸­æ”¶é›†äº† 122 ä»½æ‰«æç‰ˆæ‰‹å†™è§£ç­”åŠå¯¹åº”çš„ä¸“å®¶å®˜æ–¹è¯„åˆ†ï¼Œå¹¶å¯¹æ¥è‡ª Googleã€OpenAIã€Arcee AI å’Œé˜¿é‡Œå·´å·´äº‘çš„ä¸ƒç§ç°ä»£ VLMs åœ¨ä¸‰ç§æ¨ç†æ¨¡å¼ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»¥åŠä¸äººç±»è¯„åˆ†å‡†åˆ™å¯¹é½ (human-rubric alignment) æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚æ­¤é¡¹å·¥ä½œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©è¯„ä¼° (AI-assisted assessment) é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶è·¯å¾„ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³ç ”ç©¶ä»£ç ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 3 figures, 10 tables. Code is available at: https://github.com/Karifannaa/Auto-check-EGE-math",
      "pdf_url": "https://arxiv.org/pdf/2507.22958v1",
      "published_date": "2025-07-29 23:46:45 UTC",
      "updated_date": "2025-07-29 23:46:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:07.689322+00:00"
    },
    {
      "arxiv_id": "2507.22286v2",
      "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs",
      "title_zh": "èµ‹ä¹‰è¯­æ³•ï¼šæ¢¯åº¦å¯æ¥å—åº¦å¡‘é€  LLMs ä¸­æ„å¼çš„å‡ ä½•è¡¨å¾",
      "authors": [
        "Supantho Rakshit",
        "Adele Goldberg"
      ],
      "abstract": "The usage-based constructionist (UCx) approach to language posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze representations of the English Double Object (DO) and Prepositional Object (PO) constructions in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied by human-rated preference strength for DO or PO. Geometric analyses show that the separability between the two constructions' representations, as measured by energy distance or Jensen-Shannon divergence, is systematically modulated by gradient preference strength, which depends on lexical and functional properties of sentences. That is, more prototypical exemplars of each construction occupy more distinct regions in activation space, compared to sentences that could have equally well have occured in either construction. These results provide evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures for representations in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å†…éƒ¨è¡¨ç¤ºæ˜¯å¦åæ˜ äº†åŸºäºç”¨æ³•å»ºæ„ä¸»ä¹‰(Usage-based constructionist, UCx)è¯­è¨€å­¦æ–¹æ³•æ‰€æå‡ºçš„åŠŸèƒ½çŒæ³¨æ¢¯åº¦ã€‚ç ”ç©¶äººå‘˜åˆ†æäº†Pythia-1.4Bæ¨¡å‹ä¸­è‹±è¯­åŒå®¾è¯­(Double Object, DO)å’Œä»‹è¯å®¾è¯­(Prepositional Object, PO)å»ºæ„çš„è¡¨ç¤ºï¼Œä½¿ç”¨äº†ä¸€å¥—åŒ…å«5000ä¸ªæ ¹æ®äººç±»åå¥½å¼ºåº¦å˜åŒ–çš„å¥å¯¹æ•°æ®é›†ã€‚é€šè¿‡èƒ½é‡è·ç¦»(energy distance)å’ŒJensen-Shannonæ•£åº¦ç­‰å‡ ä½•åˆ†ææ‰‹æ®µï¼Œç ”ç©¶å‘ç°ä¸¤ç§å»ºæ„è¡¨ç¤ºä¹‹é—´çš„å¯åˆ†æ€§å—æ¢¯åº¦åå¥½å¼ºåº¦çš„ç³»ç»Ÿæ€§è°ƒèŠ‚ã€‚å…·ä½“è€Œè¨€ï¼Œå»ºæ„ä¸­æ›´å…·å…¸å‹æ€§çš„èŒƒä¾‹åœ¨æ¿€æ´»ç©ºé—´ä¸­å æ®æ›´æ˜æ˜¾çš„åŒºåŸŸï¼Œè€Œä¸¤ç§å»ºæ„å‡å¯é€‚ç”¨çš„å¥å­å…¶è¡¨ç¤ºåˆ™é‡åˆåº¦æ›´é«˜ã€‚è¿™ä¸€ç»“æœè¯æ˜LLMsèƒ½å¤Ÿå­¦ä¹ åˆ°ä¸°å¯Œçš„ã€èåˆæ„ä¹‰ä¸”å…·å¤‡æ¢¯åº¦çš„å»ºæ„è¡¨ç¤ºï¼Œä¸ºä½¿ç”¨å‡ ä½•åº¦é‡æ–¹æ³•ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹è¡¨å¾æä¾›äº†å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 3 figures, Accepted for publication at the Second International Workshop on Construction Grammars and NLP at the 16th International Conference for Computational Semantics (IWCS) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22286v2",
      "published_date": "2025-07-29 23:39:21 UTC",
      "updated_date": "2025-09-08 18:33:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:13.480969+00:00"
    },
    {
      "arxiv_id": "2507.22281v1",
      "title": "CoEx -- Co-evolving World-model and Exploration",
      "title_zh": "CoExï¼šååŒæ¼”åŒ–çš„ä¸–ç•Œæ¨¡å‹ä¸æ¢ç´¢",
      "authors": [
        "Minsoo Kim",
        "Seung-won Hwang"
      ],
      "abstract": "Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CoExï¼Œä¸€ç§å±‚æ¬¡åŒ–æ™ºèƒ½ä½“æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ LLM æ™ºèƒ½ä½“å› ä¾èµ–é¢„è®­ç»ƒå½¢æˆçš„é™æ€å†…éƒ¨ä¸–ç•Œæ¨¡å‹(world model)è€Œå¯¼è‡´è§„åˆ’ä¸ç°å®çŠ¶æ€è„±èŠ‚çš„é—®é¢˜ã€‚CoEx é€šè¿‡å±‚æ¬¡åŒ–çŠ¶æ€æŠ½è±¡(hierarchical state abstraction)å®ç°äº† LLM è§„åˆ’ä¸åŠ¨æ€æ›´æ–°ä¸–ç•Œæ¨¡å‹çš„ååŒè¿›åŒ–ã€‚åœ¨è¯¥æ¶æ„ä¸­ï¼ŒLLM åˆ©ç”¨æ¨ç†èƒ½åŠ›ç¼–æ’ç”±å­ç›®æ ‡(subgoals)æ„æˆçš„åŠ¨æ€è§„åˆ’ï¼Œå¹¶é€šè¿‡å­¦ä¹ æœºåˆ¶å°†äº¤äº’ç»éªŒæŒç»­æ•´åˆè‡³ä»¥ç¥ç»ç¬¦å·ä¿¡å¿µçŠ¶æ€(neurosymbolic belief state)å½¢å¼å­˜åœ¨çš„æŒä¹…åŒ–ä¸–ç•Œæ¨¡å‹ä¸­ã€‚è¯¥ä¿¡å¿µçŠ¶æ€ç»“åˆäº†æ–‡æœ¬æ¨ç†(textual inferences)å’ŒåŸºäºä»£ç çš„ç¬¦å·å­˜å‚¨(code-based symbolic memory)ï¼Œå¢å¼ºäº†æ™ºèƒ½ä½“å¯¹å¤æ‚ç¯å¢ƒçš„ç†è§£ã€‚åœ¨ ALFWorldã€PDDL å’Œ Jericho ç­‰å¤šæ ·åŒ–ä»»åŠ¡åœºæ™¯ä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒCoEx åœ¨è§„åˆ’(planning)å’Œæ¢ç´¢(exploration)æ€§èƒ½ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ™ºèƒ½ä½“èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22281v1",
      "published_date": "2025-07-29 23:13:09 UTC",
      "updated_date": "2025-07-29 23:13:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:12.888806+00:00"
    },
    {
      "arxiv_id": "2507.22268v2",
      "title": "Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items",
      "title_zh": "é¢å‘æ›¿ä»£å“ä¸äº’è¡¥å“æ¨æ–­çš„å¤šæ¨¡æ€å…³ç³»å‹å•†å“è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Junting Wang",
        "Chenghuan Guo",
        "Jiao Yang",
        "Yanhui Guo",
        "Yan Gao",
        "Hari Sundaram"
      ],
      "abstract": "We introduce a novel self-supervised multi-modal relational item representation learning framework designed to infer substitutable and complementary items. Existing approaches primarily focus on modeling item-item associations deduced from user behaviors using graph neural networks (GNNs) or leveraging item content information. However, these methods often overlook critical challenges, such as noisy user behavior data and data sparsity due to the long-tailed distribution of these behaviors. In this paper, we propose MMSC, a self-supervised multi-modal relational item representation learning framework to address these challenges. Specifically, MMSC consists of three main components: (1) a multi-modal item representation learning module that leverages a multi-modal foundational model and learns from item metadata, (2) a self-supervised behavior-based representation learning module that denoises and learns from user behavior data, and (3) a hierarchical representation aggregation mechanism that integrates item representations at both the semantic and task levels. Additionally, we leverage LLMs to generate augmented training data, further enhancing the denoising process during training. We conduct extensive experiments on five real-world datasets, showing that MMSC outperforms existing baselines by 26.1% for substitutable recommendation and 39.2% for complementary recommendation. In addition, we empirically show that MMSC is effective in modeling cold-start items.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ¨ç†æ›¿ä»£å“(substitutable)å’Œäº’è¡¥å“(complementary)é¡¹ç›®æ—¶é¢ä¸´çš„è¡Œä¸ºæ•°æ®å™ªå£°åŠé•¿å°¾åˆ†å¸ƒå¯¼è‡´çš„ç¨€ç–æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†MMSCè¿™ä¸€è‡ªç›‘ç£å¤šæ¨¡æ€å…³ç³»é¡¹ç›®è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚MMSCåŒ…å«åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹(multi-modal foundational model)ä»é¡¹ç›®å…ƒæ•°æ®(metadata)ä¸­å­¦ä¹ ç‰¹å¾çš„æ¨¡å—ï¼Œä»¥åŠä¸“é—¨é’ˆå¯¹ç”¨æˆ·è¡Œä¸ºæ•°æ®è¿›è¡Œå»å™ªçš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡å±‚æ¬¡åŒ–è¡¨ç¤ºèšåˆæœºåˆ¶(hierarchical representation aggregation)åœ¨è¯­ä¹‰å’Œä»»åŠ¡å±‚é¢æ•´åˆé¡¹ç›®è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆå¢å¼ºè®­ç»ƒæ•°æ®ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å»å™ªè¿‡ç¨‹ã€‚åœ¨äº”ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMMSCåœ¨æ›¿ä»£æ€§æ¨èå’Œäº’è¡¥æ€§æ¨èä»»åŠ¡ä¸­åˆ†åˆ«æ¯”ç°æœ‰åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æé«˜äº†26.1%å’Œ39.2%ã€‚æ­¤å¤–ï¼Œå®è¯ç ”ç©¶è¯æ˜è¯¥æ¡†æ¶åœ¨è§£å†³å†·å¯åŠ¨(cold-start)é¡¹ç›®è¡¨ç¤ºå­¦ä¹ é—®é¢˜ä¸ŠåŒæ ·å…·æœ‰æ˜¾è‘—æˆæ•ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22268v2",
      "published_date": "2025-07-29 22:38:39 UTC",
      "updated_date": "2025-07-31 20:53:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:23.283271+00:00"
    },
    {
      "arxiv_id": "2507.22267v1",
      "title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs",
      "title_zh": "é€šè¿‡å¤§è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä¸å®‰å…¨å¯¹è¯ä»¥æå‡ç½‘ç»œå®‰å…¨",
      "authors": [
        "Owen Hoffman",
        "Kangze Peng",
        "Zehua You",
        "Sajid Kamal",
        "Sukrit Venkatagiri"
      ],
      "abstract": "Generative AI, including large language models (LLMs) have the potential -- and already are being used -- to increase the speed, scale, and types of unsafe conversations online. LLMs lower the barrier for entry for bad actors to create unsafe conversations in particular because of their ability to generate persuasive and human-like text. In our current work, we explore ways to promote online safety by teaching people about unsafe conversations that can occur online with and without LLMs. We build on prior work that shows that LLMs can successfully simulate scam conversations. We also leverage research in the learning sciences that shows that providing feedback on one's hypothetical actions can promote learning. In particular, we focus on simulating scam conversations using LLMs. Our work incorporates two LLMs that converse with each other to simulate realistic, unsafe conversations that people may encounter online between a scammer LLM and a target LLM but users of our system are asked provide feedback to the target LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨¡æ‹Ÿçº¿ä¸Šä¸å®‰å…¨å¯¹è¯ï¼Œä»¥åº”å¯¹ç”Ÿæˆå¼ AI é™ä½è¯ˆéª—æˆæœ¬å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ã€‚ç ”ç©¶åŸºäº LLMs èƒ½å¤ŸæˆåŠŸæ¨¡æ‹Ÿè¯ˆéª—å¯¹è¯çš„å…ˆéªŒå·¥ä½œï¼Œå¼€å‘äº†ä¸€ä¸ªåŒæ™ºèƒ½ä½“æ¨¡æ‹Ÿç³»ç»Ÿï¼Œå…¶ä¸­åŒ…å«ç›¸äº’äº¤è°ˆçš„ Scammer LLM å’Œ Target LLMï¼Œç”¨ä»¥è¿˜åŸçœŸå®çš„åœ¨çº¿é£é™©åœºæ™¯ã€‚è¯¥ç³»ç»Ÿå€Ÿé‰´äº†å­¦ä¹ ç§‘å­¦ (learning sciences) ä¸­å…³äºâ€œå¯¹å‡è®¾è¡Œä¸ºæä¾›åé¦ˆå¯ä¿ƒè¿›å­¦ä¹ â€çš„ç†è®ºï¼Œè¦æ±‚ç”¨æˆ·å¯¹ Target LLM çš„åº”å¯¹ç­–ç•¥æä¾›åé¦ˆã€‚é€šè¿‡è¿™ç§æ¨¡æ‹Ÿæ•™å­¦æ–¹å¼ï¼Œç ”ç©¶æ—¨åœ¨æå‡äººä»¬è¯†åˆ«å’ŒæŠµå¾¡çº¿ä¸Šä¸å®‰å…¨å¯¹è¯çš„èƒ½åŠ›ï¼Œä¸ºåˆ©ç”¨ LLMs ä¿ƒè¿›åœ¨çº¿å®‰å…¨æ•™è‚²æä¾›äº†æ–°çš„æ–¹æ³•å’Œè§†è§’ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22267v1",
      "published_date": "2025-07-29 22:38:21 UTC",
      "updated_date": "2025-07-29 22:38:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:22.387665+00:00"
    },
    {
      "arxiv_id": "2507.22264v1",
      "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees",
      "title_zh": "SmartCLIPï¼šå…·æœ‰è¯†åˆ«ä¿è¯çš„æ¨¡å—åŒ–è§†è§‰è¯­è¨€å¯¹é½",
      "authors": [
        "Shaoan Xie",
        "Lingjing Kong",
        "Yujia Zheng",
        "Yu Yao",
        "Zeyu Tang",
        "Eric P. Xing",
        "Guangyi Chen",
        "Kun Zhang"
      ],
      "abstract": "Contrastive Language-Image Pre-training (CLIP)~\\citep{radford2021learning} has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning. However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard. On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts -- ultimately limiting its generalization on certain downstream tasks involving short prompts.\n  In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only \\emph{preserve} cross-modal semantic information in its entirety but also \\emph{disentangle} visual representations to capture fine-grained textual concepts. Building on this foundation, we introduce \\ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory. The code is available at https://github.com/Mid-Push/SmartCLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Contrastive Language-Image Pre-training (CLIP) åœ¨å¤„ç†å›¾åƒæ–‡æœ¬æ•°æ®é›†æ—¶é¢ä¸´çš„ä¿¡æ¯å¤±é…å’Œè¡¨ç¤ºçº ç¼  (entangled representation) é—®é¢˜ï¼Œæå‡ºäº† SmartCLIP æ¡†æ¶ã€‚ä¼ ç»Ÿçš„ CLIP åœ¨å¤„ç†ä»…æè¿°å±€éƒ¨çš„çŸ­æ ‡é¢˜æˆ–åŒ…å«å†—ä½™ç»†èŠ‚çš„é•¿æ ‡é¢˜æ—¶ï¼Œéš¾ä»¥å­¦ä¹ åˆ°è§£è€¦çš„åŸå­æ¦‚å¿µ (disentangled, atomic concepts)ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚SmartCLIP é€šè¿‡å»ºç«‹ç†è®ºæ¡ä»¶ï¼Œå®ç°äº†è§†è§‰ä¸æ–‡æœ¬è¡¨ç¤ºåœ¨ä¸åŒç²’åº¦ä¸Šçš„çµæ´»å¯¹é½ï¼Œç¡®ä¿æ¨¡å‹åœ¨å®Œæ•´ä¿ç•™è·¨æ¨¡æ€è¯­ä¹‰çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè§£è€¦è§†è§‰è¡¨ç¤ºä»¥æ•è·ç»†ç²’åº¦çš„æ–‡æœ¬æ¦‚å¿µã€‚è¿™ç§æ¨¡å—åŒ–çš„æ–¹æ³•èƒ½å¤Ÿç²¾å‡†è¯†åˆ«å¹¶å…³è”æœ€ç›¸å…³çš„è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œæœ‰æ•ˆå…‹æœäº†æ•°æ®ä¸­çš„ä¿¡æ¯ä¸å¯¹ç§°æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmartCLIP åœ¨å¤šé¡¹è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæœ‰åŠ›æ”¯æ’‘äº†å…¶è¯†åˆ«ç†è®º (identification theory) å¹¶å±•ç°äº†å“è¶Šçš„ç‰¹å¾å¯¹é½èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22264v1",
      "published_date": "2025-07-29 22:26:20 UTC",
      "updated_date": "2025-07-29 22:26:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:26.390367+00:00"
    },
    {
      "arxiv_id": "2507.22255v1",
      "title": "Agent-centric learning: from external reward maximization to internal knowledge curation",
      "title_zh": "ä»¥æ™ºèƒ½ä½“ä¸ºä¸­å¿ƒçš„å­¦ä¹ ï¼šä»å¤–éƒ¨å¥–åŠ±æœ€å¤§åŒ–åˆ°å†…éƒ¨çŸ¥è¯†æ•´ç†",
      "authors": [
        "Hanqi Zhou",
        "Fryderyk Mantiuk",
        "David G. Nagy",
        "Charley M. Wu"
      ],
      "abstract": "The pursuit of general intelligence has traditionally centered on external objectives: an agent's control over its environments or mastery of specific tasks. This external focus, however, can produce specialized agents that lack adaptability. We propose representational empowerment, a new perspective towards a truly agent-centric learning paradigm by moving the locus of control inward. This objective measures an agent's ability to controllably maintain and diversify its own knowledge structures. We posit that the capacity -- to shape one's own understanding -- is an element for achieving better ``preparedness'' distinct from direct environmental influence. Focusing on internal representations as the main substrate for computing empowerment offers a new lens through which to design adaptable intelligent systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿé€šç”¨æ™ºèƒ½(General Intelligence)è¿‡åº¦å…³æ³¨å¤–éƒ¨ç›®æ ‡ï¼ˆå¦‚ç¯å¢ƒæ§åˆ¶æˆ–ç‰¹å®šä»»åŠ¡ï¼‰å¯¼è‡´æ™ºèƒ½ä½“ç¼ºä¹é€‚åº”æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„â€œä»¥æ™ºèƒ½ä½“ä¸ºä¸­å¿ƒâ€(Agent-centric)çš„å­¦ä¹ èŒƒå¼ã€‚ä½œè€…å¼•å…¥äº†â€œè¡¨å¾èµ‹èƒ½â€(Representational Empowerment)çš„æ¦‚å¿µï¼Œå°†æ§åˆ¶ä¸­å¿ƒä»å¤–éƒ¨ç¯å¢ƒè½¬å‘å†…éƒ¨ã€‚è¯¥æŒ‡æ ‡æ—¨åœ¨è¡¡é‡æ™ºèƒ½ä½“è‡ªä¸»ç»´æŠ¤å’Œå¤šæ ·åŒ–å…¶å†…éƒ¨çŸ¥è¯†ç»“æ„(Knowledge Structures)çš„èƒ½åŠ›ã€‚é€šè¿‡å°†å†…éƒ¨è¡¨å¾ä½œä¸ºè®¡ç®—èµ‹èƒ½çš„æ ¸å¿ƒåŸºç¡€ï¼Œè¯¥æ–¹æ³•ä¸ºè®¾è®¡æ›´å…·é€‚åº”æ€§çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°è§†è§’ã€‚è¿™ç§å¡‘é€ è‡ªèº«ç†è§£çš„èƒ½åŠ›è¢«è§†ä¸ºå®ç°æ›´å¥½â€œé¢„å¤‡åº¦â€(Preparedness)çš„å…³é”®è¦ç´ ï¼Œä¸”è¿™ç§èƒ½åŠ›ç‹¬ç«‹äºç›´æ¥çš„ç¯å¢ƒå½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "RLC Finding the Frame Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22255v1",
      "published_date": "2025-07-29 22:09:35 UTC",
      "updated_date": "2025-07-29 22:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:33.593935+00:00"
    },
    {
      "arxiv_id": "2507.22250v1",
      "title": "Using Scaling Laws for Data Source Utility Estimation in Domain-Specific Pre-Training",
      "title_zh": "åˆ©ç”¨ç¼©æ”¾æ³•åˆ™è¯„ä¼°ç‰¹å®šé¢†åŸŸé¢„è®­ç»ƒä¸­çš„æ•°æ®æºæ•ˆç”¨",
      "authors": [
        "Oleksiy Ostapenko",
        "Charles Guille-Escuret",
        "Luke Kumar",
        "Max Tian",
        "Denis Kocetkov",
        "Gopeshh Subbaraj",
        "Raymond Li",
        "Joel Lamy-Poirier",
        "Sebastien Paquet",
        "Torsten Scholak"
      ],
      "abstract": "We introduce a framework for optimizing domain-specific dataset construction in foundation model training. Specifically, we seek a cost-efficient way to estimate the quality of data sources (e.g. synthetically generated or filtered web data, etc.) in order to make optimal decisions about resource allocation for data sourcing from these sources for the stage two pre-training phase, aka annealing, with the goal of specializing a generalist pre-trained model to specific domains. Our approach extends the usual point estimate approaches, aka micro-annealing, to estimating scaling laws by performing multiple annealing runs of varying compute spent on data curation and training. This addresses a key limitation in prior work, where reliance on point estimates for data scaling decisions can be misleading due to the lack of rank invariance across compute scales -- a phenomenon we confirm in our experiments. By systematically analyzing performance gains relative to acquisition costs, we find that scaling curves can be estimated for different data sources. Such scaling laws can inform cost effective resource allocation across different data acquisition methods (e.g. synthetic data), data sources (e.g. user or web data) and available compute resources. We validate our approach through experiments on a pre-trained model with 7 billion parameters. We adapt it to: a domain well-represented in the pre-training data -- the medical domain, and a domain underrepresented in the pretraining corpora -- the math domain. We show that one can efficiently estimate the scaling behaviors of a data source by running multiple annealing runs, which can lead to different conclusions, had one used point estimates using the usual micro-annealing technique instead. This enables data-driven decision-making for selecting and optimizing data sources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒ(Domain-Specific Pre-Training)ä¸­è¯„ä¼°æ•°æ®æºæ•ˆç”¨(Data Source Utility Estimation)çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–åŸºç¡€æ¨¡å‹è®­ç»ƒä¸­çš„æ•°æ®é›†æ„å»ºã€‚è¯¥æ¡†æ¶é‡ç‚¹æ”¹è¿›äº†æ¨¡å‹é€€ç«é˜¶æ®µ(annealing)çš„èµ„æºåˆ†é…ç­–ç•¥ï¼Œé€šè¿‡æ‰§è¡Œå¤šæ¬¡ä¸åŒè®¡ç®—è§„æ¨¡çš„é€€ç«å®éªŒæ¥ä¼°ç®—ç¼©æ”¾æ³•åˆ™(scaling laws)ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿå¾®é€€ç«(micro-annealing)å•ç‚¹ä¼°è®¡(point estimate)å› ç¼ºä¹è®¡ç®—è§„æ¨¡é—´çš„æ’åä¸å˜æ€§(rank invariance)è€Œå¯¼è‡´å†³ç­–å¤±è¯¯çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åœ¨70äº¿å‚æ•°(7 billion parameters)çš„æ¨¡å‹ä¸Šï¼Œé’ˆå¯¹åŒ»å­¦å’Œæ•°å­¦ä¸¤ä¸ªé¢†åŸŸéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æ›´å‡†ç¡®åœ°é¢„æµ‹ä¸åŒæ•°æ®æºåœ¨æ›´å¤§è§„æ¨¡ä¸‹çš„è¡¨ç°ï¼Œä»è€Œä¸ºæ•°æ®è·å–æˆæœ¬ä¸æ€§èƒ½æå‡ä¹‹é—´çš„æƒè¡¡æä¾›äº†å¯é çš„æ•°æ®é©±åŠ¨å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22250v1",
      "published_date": "2025-07-29 21:56:45 UTC",
      "updated_date": "2025-07-29 21:56:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:37.085528+00:00"
    },
    {
      "arxiv_id": "2507.22239v2",
      "title": "Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å‘ç”µæ§åˆ¶ç³»ç»Ÿå¯è§£é‡Šç½‘ç»œæ”»å‡»æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Muhammad Sharshar",
        "Ahmad Mohammad Saber",
        "Davor Svetinovic",
        "Amr M. Youssef",
        "Deepa Kundur",
        "Ehab F. El-Saadany"
      ],
      "abstract": "The increasing digitization of smart grids has improved operational efficiency but also introduced new cybersecurity vulnerabilities, such as False Data Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC) systems. While machine learning (ML) and deep learning (DL) models have shown promise in detecting such attacks, their opaque decision-making limits operator trust and real-world applicability. This paper proposes a hybrid framework that integrates lightweight ML-based attack detection with natural language explanations generated by Large Language Models (LLMs). Classifiers such as LightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s inference latency. Upon detecting a cyberattack, the system invokes LLMs, including GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate human-readable explanation of the event. Evaluated on 100 test samples, GPT-4o mini with 20-shot prompting achieved 93% accuracy in identifying the attack target, a mean absolute error of 0.075 pu in estimating attack magnitude, and 2.19 seconds mean absolute error (MAE) in estimating attack onset. These results demonstrate that the proposed framework effectively balances real-time detection with interpretable, high-fidelity explanations, addressing a critical need for actionable AI in smart grid cybersecurity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ç”µç½‘ä¸­è‡ªåŠ¨å‘ç”µæ§åˆ¶(Automatic Generation Control, AGC)ç³»ç»Ÿé¢ä¸´çš„è™šå‡æ•°æ®æ³¨å…¥æ”»å‡»(False Data Injection Attacks, FDIAs)ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè½»é‡çº§æœºå™¨å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„å¯è§£é‡Šæ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ LightGBM åˆ†ç±»å™¨å®ç°é«˜è¾¾ 95.13% çš„æ”»å‡»æ£€æµ‹å‡†ç¡®ç‡ï¼Œä¸”æ¨ç†å»¶è¿Ÿä»…ä¸º 0.004 ç§’ï¼Œæ»¡è¶³å®æ—¶æ€§éœ€æ±‚ã€‚åœ¨æ£€æµ‹åˆ°æ”»å‡»åï¼Œç³»ç»Ÿè°ƒç”¨ GPT-4o mini ç­‰å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆäººç±»å¯è¯»çš„è§£é‡Šï¼Œæ¶µç›–æ”»å‡»ç›®æ ‡è¯†åˆ«ã€æ”»å‡»å¹…å€¼è¯„ä¼°åŠèµ·å§‹æ—¶é—´ä¼°ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨ 20-shot æç¤ºè¯çš„ GPT-4o mini åœ¨è¯†åˆ«æ”»å‡»ç›®æ ‡æ–¹é¢è¾¾åˆ°äº† 93% çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨é‡åŒ–è¯„ä¼°ä¸­è¡¨ç°å‡ºæä½çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆå¹³è¡¡äº†å®æ—¶ç›‘æµ‹ä¸å†³ç­–é€æ˜åº¦ï¼Œä¸ºæ™ºèƒ½ç”µç½‘ç½‘ç»œå®‰å…¨æä¾›äº†é«˜ä¿çœŸä¸”å¯æ‰§è¡Œçš„ AI è¾…åŠ©æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted Paper",
      "pdf_url": "https://arxiv.org/pdf/2507.22239v2",
      "published_date": "2025-07-29 21:23:08 UTC",
      "updated_date": "2025-08-26 01:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:38.485495+00:00"
    },
    {
      "arxiv_id": "2507.22219v3",
      "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation",
      "title_zh": "åŸºäºæ•™å¸ˆæ¨¡å‹ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼šé¢å‘æœºå™¨ç¿»è¯‘çš„æ¸è¿›å¼æ¨¡ä»¿å­¦ä¹ ",
      "authors": [
        "Dongyub Jude Lee",
        "Zhenyi Ye",
        "Pengcheng He"
      ],
      "abstract": "Preference-learning methods for machine translation (MT), such as Direct Preference Optimization (DPO), have shown strong gains but typically rely on large, carefully curated preference triplets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), which replaces static triplets with on-policy, actor-conditioned refinements produced by a frozen teacher. At each step, the actor samples candidate translations, the teacher performs a minimal local edit of each draft, and the actor is reinforced to close the gap using a composite reward that combines scaled negative edit distance for lexical and structural fidelity with COMET for semantic adequacy. This formulation yields a stable, model-aware learning signal without requiring explicit preference datasets. Experiments on FLORES-200 (English to German, Spanish, Chinese, Korean, and Japanese) show that RLfR consistently outperforms strong MT-SFT, DPO, and fixed-reference RL baselines, improving semantic quality and entity preservation, and also achieves superior performance under LLM-based judge evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Reinforcement Learning from Teacher-Model Refinement (RLfR)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æœºå™¨ç¿»è¯‘(Machine Translation)åå¥½å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³Direct Preference Optimization (DPO)ç­‰æŠ€æœ¯å¯¹å¤§è§„æ¨¡é™æ€åå¥½æ•°æ®é›†çš„ä¾èµ–åŠæ³›åŒ–éš¾é¢˜ã€‚RLfRåˆ©ç”¨å†»ç»“çš„æ•™å¸ˆæ¨¡å‹å¯¹è¡ŒåŠ¨è€…(Actor)ç”Ÿæˆçš„å€™é€‰ç¿»è¯‘è¿›è¡Œå®æ—¶çš„å±€éƒ¨å¾®è°ƒï¼Œé€šè¿‡è¿™ç§åœ¨çº¿æ”¹è¿›ä¿¡å·å–ä»£äº†ä¼ ç»Ÿçš„é™æ€ä¸‰å…ƒç»„æ•°æ®ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒActoråˆ©ç”¨ç»“åˆäº†è´Ÿç¼–è¾‘è·ç¦»(negative edit distance)å’ŒCOMETè¯„åˆ†çš„å¤åˆå¥–åŠ±æœºåˆ¶ï¼Œä¸æ–­ç¼©å°è‡ªèº«è¾“å‡ºä¸æ•™å¸ˆä¼˜åŒ–ç‰ˆæœ¬ä¹‹é—´çš„å·®è·ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§ç¨³å®šä¸”å…·å¤‡æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›çš„å­¦ä¹ ä¿¡å·ï¼Œæ— éœ€æ˜¾å¼çš„åå¥½æ•°æ®é›†å³å¯å®ç°æœ‰æ•ˆçš„æ¨¡å‹æ¼”è¿›ã€‚åœ¨FLORES-200æ•°æ®é›†æ¶µç›–çš„å¤šç§è¯­è¨€å¯¹å®éªŒä¸­ï¼ŒRLfRåœ¨è¯­ä¹‰è´¨é‡å’Œå®ä½“ä¿ç•™æ–¹é¢å‡æ˜¾è‘—ä¼˜äºMT-SFTã€DPOåŠå›ºå®šå‚è€ƒRLç­‰å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨åŸºäºLLMçš„è‡ªåŠ¨è¯„æµ‹ä¸­å–å¾—äº†æ›´ä¼˜è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22219v3",
      "published_date": "2025-07-29 20:35:35 UTC",
      "updated_date": "2025-12-19 17:35:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:24:46.155324+00:00"
    },
    {
      "arxiv_id": "2507.22208v1",
      "title": "Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice Biometrics",
      "title_zh": "é‡å­å¯å‘å¼éŸ³é¢‘æœºå™¨é—å¿˜ï¼šé¢å‘éšç§ä¿æŠ¤çš„å£°çº¹è¯†åˆ«",
      "authors": [
        "Shreyansh Pathak",
        "Sonu Shreshtha",
        "Richa Singh",
        "Mayank Vatsa"
      ],
      "abstract": "The widespread adoption of voice-enabled authentication and audio biometric systems have significantly increased privacy vulnerabilities associated with sensitive speech data. Compliance with privacy regulations such as GDPR's right to be forgotten and India's DPDP Act necessitates targeted and efficient erasure of individual-specific voice signatures from already-trained biometric models. Existing unlearning methods designed for visual data inadequately handle the sequential, temporal, and high-dimensional nature of audio signals, leading to ineffective or incomplete speaker and accent erasure. To address this, we introduce QPAudioEraser, a quantum-inspired audio unlearning framework. Our our-phase approach involves: (1) weight initialization using destructive interference to nullify target features, (2) superposition-based label transformations that obscure class identity, (3) an uncertainty-maximizing quantum loss function, and (4) entanglement-inspired mixing of correlated weights to retain model knowledge. Comprehensive evaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech Commands, LibriSpeech, and Speech Accent Archive datasets validate QPAudioEraser's superior performance. The framework achieves complete erasure of target data (0% Forget Accuracy) while incurring minimal impact on model utility, with a performance degradation on retained data as low as 0.05%. QPAudioEraser consistently surpasses conventional baselines across single-class, multi-class, sequential, and accent-level erasure scenarios, establishing the proposed approach as a robust privacy-preserving solution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³è®¤è¯ç³»ç»Ÿä¸­æ•æ„Ÿè¯­éŸ³æ•°æ®çš„éšç§æ³„éœ²é£é™©ï¼Œä¸ºç¬¦åˆ GDPR å’Œ DPDP ç­‰æ³•è§„çš„â€œè¢«é—å¿˜æƒâ€è¦æ±‚ï¼Œæå‡ºäº†å—é‡å­å¯å‘çš„éŸ³é¢‘é—å¿˜å­¦ä¹ æ¡†æ¶ QPAudioEraserã€‚ç”±äºç°æœ‰çš„è§†è§‰é—å¿˜å­¦ä¹ æ–¹æ³•éš¾ä»¥åº”å¯¹éŸ³é¢‘ä¿¡å·çš„åºåˆ—æ€§å’Œé«˜ç»´ç‰¹æ€§ï¼ŒQPAudioEraser é€šè¿‡ç›¸æ¶ˆå¹²æ¶‰ (destructive interference) çš„æƒé‡åˆå§‹åŒ–ã€åŸºäºå åŠ  (superposition) çš„æ ‡ç­¾è½¬æ¢ã€ä¸ç¡®å®šæ€§æœ€å¤§åŒ–é‡å­æŸå¤±å‡½æ•° (uncertainty-maximizing quantum loss) ä»¥åŠå—çº ç¼  (entanglement) å¯å‘çš„æƒé‡æ··åˆè¿™å››ä¸ªé˜¶æ®µæ¥å®ç°é«˜æ•ˆæ“¦é™¤ã€‚å®éªŒåœ¨ ResNet18ã€ViT å’Œ CNN æ¶æ„ä»¥åŠ AudioMNISTã€LibriSpeech å’Œ Speech Accent Archive ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†ç›®æ ‡æ•°æ®çš„å®Œå…¨æ“¦é™¤ï¼ˆForget Accuracy ä¸º 0%ï¼‰ï¼Œä¸”å¯¹æ¨¡å‹åŸæœ‰å®ç”¨æ€§çš„å½±å“æä½ï¼Œä¿ç•™æ•°æ®çš„æ€§èƒ½ä¸‹é™ä»…ä¸º 0.05%ã€‚åœ¨å•ç±»åˆ«ã€å¤šç±»åˆ«ã€åºåˆ—åŠå£éŸ³çº§åˆ«çš„æ“¦é™¤åœºæ™¯ä¸­ï¼ŒQPAudioEraser çš„è¡¨ç°å‡ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºéšç§ä¿æŠ¤çš„è¯­éŸ³ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯æä¾›äº†ä¸€ç§é²æ£’ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 2 figures, 5 tables, Accepted at IJCB 2025 (Osaka, Japan)",
      "pdf_url": "https://arxiv.org/pdf/2507.22208v1",
      "published_date": "2025-07-29 20:12:24 UTC",
      "updated_date": "2025-07-29 20:12:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:02.441873+00:00"
    },
    {
      "arxiv_id": "2508.00914v1",
      "title": "Knowledge Editing for Multi-Hop Question Answering Using Semantic Analysis",
      "title_zh": "åŸºäºè¯­ä¹‰åˆ†æçš„å¤šè·³é—®ç­”çŸ¥è¯†ç¼–è¾‘",
      "authors": [
        "Dominic Simon",
        "Rickard Ewetz"
      ],
      "abstract": "Large Language Models (LLMs) require lightweight avenues of updating stored information that has fallen out of date. Knowledge Editing (KE) approaches have been successful in updating model knowledge for simple factual queries but struggle with handling tasks that require compositional reasoning such as multi-hop question answering (MQA). We observe that existing knowledge editors leverage decompositional techniques that result in illogical reasoning processes. In this paper, we propose a knowledge editor for MQA based on semantic analysis called CHECK. Our framework is based on insights from an analogy between compilers and reasoning using LLMs. Similar to how source code is first compiled before being executed, we propose to semantically analyze reasoning chains before executing the chains to answer questions. Reasoning chains with semantic errors are revised to ensure consistency through logic optimization and re-prompting the LLM model at a higher temperature. We evaluate the effectiveness of CHECK against five state-of-the-art frameworks on four datasets and achieve an average 22.8% improved MQA accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨çŸ¥è¯†ç¼–è¾‘(Knowledge Editing, KE)è¿‡ç¨‹ä¸­éš¾ä»¥å¤„ç†å¤šè·³é—®ç­”(Multi-Hop Question Answering, MQA)é€»è¾‘æ¨ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCHECKçš„è¯­ä¹‰åˆ†ææ¡†æ¶ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†ç¼–è¯‘å™¨çš„åŸç†ï¼Œåœ¨æ‰§è¡Œæ¨ç†é“¾å›ç­”é—®é¢˜ä¹‹å‰ï¼Œå…ˆå¯¹LLMsç”Ÿæˆçš„æ¨ç†è·¯å¾„è¿›è¡Œè¯­ä¹‰åˆ†æã€‚é’ˆå¯¹æ£€æµ‹åˆ°çš„è¯­ä¹‰é”™è¯¯ï¼ŒCHECKé€šè¿‡é€»è¾‘ä¼˜åŒ–å’Œæå‡é‡‡æ ·æ¸©åº¦è¿›è¡Œé‡æ–°æç¤ºï¼Œä»è€Œä¿®æ­£æ¨ç†é“¾å¹¶ç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCHECKç›¸è¾ƒäºäº”ç§æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ï¼Œå¹³å‡å°†MQAçš„å‡†ç¡®ç‡æå‡äº†22.8%ã€‚è¿™è¯æ˜äº†é€šè¿‡è¯­ä¹‰åˆ†æä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œèƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨åŠ¨æ€æ›´æ–°ç¯å¢ƒä¸‹çš„å¤æ‚ç»„åˆæ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 15 figures, pre-print of paper accepted to IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.00914v1",
      "published_date": "2025-07-29 19:58:22 UTC",
      "updated_date": "2025-07-29 19:58:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:06.293669+00:00"
    },
    {
      "arxiv_id": "2508.00912v1",
      "title": "Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation",
      "title_zh": "åŸºäºæ¨ç†é•¿åº¦ä¼°è®¡çš„ LLM API éšè— Token é¢„æµ‹æ€§å®¡è®¡",
      "authors": [
        "Ziyao Wang",
        "Guoheng Sun",
        "Yexiao He",
        "Zheyu Shen",
        "Bowei Tian",
        "Ang Li"
      ],
      "abstract": "Commercial LLM services often conceal internal reasoning traces while still charging users for every generated token, including those from hidden intermediate steps, raising concerns of token inflation and potential overbilling. This gap underscores the urgent need for reliable token auditing, yet achieving it is far from straightforward: cryptographic verification (e.g., hash-based signature) offers little assurance when providers control the entire execution pipeline, while user-side prediction struggles with the inherent variance of reasoning LLMs, where token usage fluctuates across domains and prompt styles. To bridge this gap, we present PALACE (Predictive Auditing of LLM APIs via Reasoning Token Count Estimation), a user-side framework that estimates hidden reasoning token counts from prompt-answer pairs without access to internal traces. PALACE introduces a GRPO-augmented adaptation module with a lightweight domain router, enabling dynamic calibration across diverse reasoning tasks and mitigating variance in token usage patterns. Experiments on math, coding, medical, and general reasoning benchmarks show that PALACE achieves low relative error and strong prediction accuracy, supporting both fine-grained cost auditing and inflation detection. Taken together, PALACE represents an important first step toward standardized predictive auditing, offering a practical path to greater transparency, accountability, and user trust.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•†ä¸šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡éšè—å†…éƒ¨æ¨ç†ç—•è¿¹ï¼ˆhidden reasoning tracesï¼‰å´ä»å¯¹æ‰€æœ‰ç”ŸæˆTokenè®¡è´¹æ‰€å¼•å‘çš„Tokenè†¨èƒ€ï¼ˆtoken inflationï¼‰å’Œæ½œåœ¨è¿‡åº¦è®¡è´¹é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPALACEçš„ç”¨æˆ·ç«¯å®¡è®¡æ¡†æ¶ã€‚PALACEèƒ½å¤Ÿåœ¨ä¸è·å–å†…éƒ¨è½¨è¿¹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æç¤º-ç­”æ¡ˆå¯¹ï¼ˆprompt-answer pairsï¼‰ä¼°ç®—éšè—çš„æ¨ç†Tokenæ•°é‡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†GRPO-augmentedé€‚åº”æ¨¡å—å’Œè½»é‡çº§é¢†åŸŸè·¯ç”±å™¨ï¼ˆdomain routerï¼‰ï¼Œæœ‰æ•ˆå®ç°äº†è·¨å¤šæ ·åŒ–æ¨ç†ä»»åŠ¡çš„åŠ¨æ€æ ¡å‡†ï¼Œå¹¶ç¼“è§£äº†Tokenä½¿ç”¨æ¨¡å¼çš„å›ºæœ‰æ³¢åŠ¨ã€‚åœ¨æ•°å­¦ã€ç¼–ç¨‹ã€åŒ»ç–—å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPALACEå…·æœ‰æä½çš„ç›¸å¯¹è¯¯å·®å’Œå¼ºå¤§çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæ”¯æŒç»†ç²’åº¦çš„æˆæœ¬å®¡è®¡ä¸è†¨èƒ€æ£€æµ‹ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ ‡å‡†åŒ–é¢„æµ‹å®¡è®¡å¥ å®šäº†åŸºç¡€ï¼Œä¸ºæå‡LLM APIçš„é€æ˜åº¦ã€é—®è´£åˆ¶å’Œç”¨æˆ·ä¿¡ä»»æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00912v1",
      "published_date": "2025-07-29 19:50:55 UTC",
      "updated_date": "2025-07-29 19:50:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:11.633002+00:00"
    },
    {
      "arxiv_id": "2507.22197v1",
      "title": "Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence",
      "title_zh": "ä»¥ç³»ç»Ÿæ€§å®ç°å¯è§£é‡Šæ€§ï¼šäººå·¥æ™ºèƒ½é¢ä¸´çš„ç¡¬ç³»ç»Ÿæ€§æŒ‘æˆ˜",
      "authors": [
        "Matthieu Queloz"
      ],
      "abstract": "This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity--not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the \"systematicity challenge\" to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed \"the systematicity of thought.\" I offer a conceptual framework for thinking about \"the systematicity of thought\" that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the \"hard systematicity challenge.\" However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when.",
      "tldr_zh": "æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§(Explainability)ä¸ç³»ç»Ÿæ€§(Systematicity)ä¹‹é—´çš„æ·±å±‚è”ç³»ï¼Œè®¤ä¸ºç³»ç»Ÿæ€§ä¸ä»…å…³ä¹è®¤çŸ¥çš„ç»„åˆæ€§ï¼Œæ›´æ¶‰åŠæ€æƒ³åœ¨ä¸€è‡´æ€§ã€è¿è´¯æ€§ã€å…¨é¢æ€§å’Œç®€çº¦åŸåˆ™ä¸‹çš„é«˜åº¦æ•´åˆã€‚ä½œè€…é€šè¿‡åŒºåˆ†â€œæ€æƒ³çš„ç³»ç»Ÿæ€§â€(systematicity of thought)çš„å››ç§å†…æ¶µï¼ŒåŒ–è§£äº†è¿æ¥ä¸»ä¹‰(connectionism)ä¸ç³»ç»Ÿæ€§ä¹‹é—´çš„ä¼ ç»Ÿå¼ åŠ›ï¼ŒæŒ‡å‡ºæ”¯æ’‘ç†æ€§ä¸ç§‘å­¦æ€æƒ³çš„ç³»ç»Ÿæ€§æ¦‚å¿µæ¯”ä¼ ç»Ÿçš„ Fodor æ¦‚å¿µæ›´ä¸ºä¸¥è‹›ã€‚æ–‡ç« è¯†åˆ«äº†æ¨åŠ¨ç³»ç»ŸåŒ–çš„äº”é¡¹åˆç†æ€§ä¾æ®(rationales)å¹¶å°†å…¶åº”ç”¨äº AI é¢†åŸŸï¼Œç”±æ­¤æå‡ºäº†â€œç¡¬ç³»ç»Ÿæ€§æŒ‘æˆ˜â€(hard systematicity challenge)ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå»ºç«‹äº†ä¸€ä¸ªåŠ¨æ€ç†è§£æ¡†æ¶ï¼Œç”¨ä»¥ç¡®å®š AI æ¨¡å‹åœ¨ä½•ç§ç¨‹åº¦åŠä½•æ—¶éœ€è¦å®ç°ç³»ç»ŸåŒ–ï¼Œä¸ºè¯„ä»·äººå·¥æ™ºèƒ½çš„ç†æ€§ä¸æƒå¨æ€§æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "39 pages; final, published version",
      "pdf_url": "https://arxiv.org/pdf/2507.22197v1",
      "published_date": "2025-07-29 19:50:21 UTC",
      "updated_date": "2025-07-29 19:50:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:12.838911+00:00"
    },
    {
      "arxiv_id": "2507.22189v1",
      "title": "Measuring Time-Series Dataset Similarity using Wasserstein Distance",
      "title_zh": "åŸºäº Wasserstein è·ç¦»çš„æ—¶é—´åºåˆ—æ•°æ®é›†ç›¸ä¼¼æ€§åº¦é‡",
      "authors": [
        "Hongjie Chen",
        "Akshay Mehra",
        "Josh Kimball",
        "Ryan A. Rossi"
      ],
      "abstract": "The emergence of time-series foundation model research elevates the growing need to measure the (dis)similarity of time-series datasets. A time-series dataset similarity measure aids research in multiple ways, including model selection, finetuning, and visualization. In this paper, we propose a distribution-based method to measure time-series dataset similarity by leveraging the Wasserstein distance. We consider a time-series dataset an empirical instantiation of an underlying multivariate normal distribution (MVN). The similarity between two time-series datasets is thus computed as the Wasserstein distance between their corresponding MVNs. Comprehensive experiments and visualization show the effectiveness of our approach. Specifically, we show how the Wasserstein distance helps identify similar time-series datasets and facilitates inference performance estimation of foundation models in both out-of-distribution and transfer learning evaluation, with high correlations between our proposed measure and the inference loss (>0.60).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (time-series foundation model) é¢†åŸŸä¸­è¡¡é‡æ•°æ®é›†ç›¸ä¼¼æ€§çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçš„åº¦é‡æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Wasserstein distance æ¥è¡¡é‡æ•°æ®é›†ä¹‹é—´çš„ (dis)similarityï¼Œå°†æ—¶é—´åºåˆ—æ•°æ®é›†è§†ä¸ºåº•å±‚å¤šå…ƒæ­£æ€åˆ†å¸ƒ (multivariate normal distribution, MVN) çš„ç»éªŒå®ä¾‹åŒ–ã€‚é€šè¿‡è®¡ç®—å¯¹åº” MVN ä¹‹é—´çš„ Wasserstein distanceï¼Œè¯¥æŒ‡æ ‡èƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©æ¨¡å‹é€‰æ‹© (model selection)ã€å¾®è°ƒ (finetuning) å’Œå¯è§†åŒ–åˆ†æã€‚å®éªŒå’Œå¯è§†åŒ–ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç›¸ä¼¼æ•°æ®é›†ä»¥åŠè¯„ä¼°åˆ†å¸ƒå¤– (out-of-distribution) å’Œè¿ç§»å­¦ä¹  (transfer learning) çš„æ¨ç†æ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ‰€æå‡ºçš„åº¦é‡æŒ‡æ ‡ä¸æ¨ç†æŸå¤± (inference loss) ä¹‹é—´å­˜åœ¨è¶…è¿‡ 0.60 çš„é«˜åº¦ç›¸å…³æ€§ï¼Œä¸ºåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ä¼°è®¡æä¾›äº†å¯é çš„é‡åŒ–ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22189v1",
      "published_date": "2025-07-29 19:33:10 UTC",
      "updated_date": "2025-07-29 19:33:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:11.447117+00:00"
    },
    {
      "arxiv_id": "2507.22187v1",
      "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models",
      "title_zh": "ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¼°è®¡åŠ¨è¯æ¡†æ¶é¢‘ç‡çš„å¯æ‰©å±•æµæ°´çº¿",
      "authors": [
        "Adam M. Morgan",
        "Adeen Flinker"
      ],
      "abstract": "We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œç”¨äºä¼°è®¡åŠ¨è¯æ¡†æ¶é¢‘ç‡(Verb Frame Frequencies, VFFs)ï¼Œå³åŠ¨è¯åœ¨ç‰¹å®šå¥æ³•æ¡†æ¶ä¸­å‡ºç°çš„é¢‘ç‡ã€‚è¯¥æµç¨‹é¦–å…ˆåˆ©ç”¨ LLMs ç”ŸæˆåŒ…å« 476 ä¸ªè‹±è¯­åŠ¨è¯çš„å¥å­è¯­æ–™åº“ï¼Œéšåé€šè¿‡æŒ‡ä»¤å¼•å¯¼æ¨¡å‹æ¨¡æ‹Ÿä¸“å®¶è¯­è¨€å­¦å®¶å¯¹å¥å­è¿›è¡Œå¥æ³•ç»“æ„åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æµç¨‹åœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¸¤ç§å¹¿æ³›ä½¿ç”¨çš„å¥æ³•è§£æå™¨(syntactic parsers)ï¼Œä¸”æ¯”ä¼ ç»Ÿçš„æ‰‹å·¥è§£æ(manual parsing)æ›´å…·èµ„æºä¼˜åŠ¿ï¼Œå®ç°äº†å¿«é€Ÿã€å¯æ‰©å±•çš„é¢‘ç‡ä¼°è®¡ã€‚åŸºäºè¯¥æµç¨‹ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ¶µç›–æ›´å¹¿åŠ¨è¯èŒƒå›´å’Œæ›´ç»†ç²’åº¦å¥æ³•åŒºåˆ†çš„æ–°å‹ VFF æ•°æ®åº“ï¼Œå¹¶æä¾›äº†å¿ƒç†è¯­è¨€å­¦ä¸­å¸¸è§ç»“æ„äº¤æ›¿çš„é¢‘ç‡åˆ†å¸ƒã€‚è¯¥å·¥ä½œè¯æ˜äº†è‡ªåŠ¨åŒ–æ¡†æ¶é¢‘ç‡ä¼°è®¡çš„å¯è¡Œæ€§ï¼Œå…¶é«˜åº¦çš„å¯å®šåˆ¶æ€§å’Œæ‰©å±•æ€§ä½¿å…¶èƒ½å¤Ÿè½»æ¾åº”ç”¨äºæ–°çš„åŠ¨è¯ã€å¥æ³•æ¡†æ¶ä¹ƒè‡³å…¶ä»–è¯­è¨€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22187v1",
      "published_date": "2025-07-29 19:30:11 UTC",
      "updated_date": "2025-07-29 19:30:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:16.255218+00:00"
    },
    {
      "arxiv_id": "2507.22186v2",
      "title": "SourceSplice: Source Selection for Machine Learning Tasks",
      "title_zh": "SourceSpliceï¼šé¢å‘æœºå™¨å­¦ä¹ ä»»åŠ¡çš„æ•°æ®æºé€‰æ‹©",
      "authors": [
        "Ambarish Singh",
        "Romila Pradhan"
      ],
      "abstract": "Data quality plays a pivotal role in the predictive performance of machine learning (ML) tasks - a challenge amplified by the deluge of data sources available in modern organizations. Prior work in data discovery largely focus on metadata matching, semantic similarity or identifying tables that should be joined to answer a particular query, but do not consider source quality for high performance of the downstream ML task. This paper addresses the problem of determining the best subset of data sources that must be combined to construct the underlying training dataset for a given ML task. We propose SourceGrasp and SourceSplice, frameworks designed to efficiently select a suitable subset of sources that maximizes the utility of the downstream ML model. Both the algorithms rely on the core idea that sources (or their combinations) contribute differently to the task utility, and must be judiciously chosen. While SourceGrasp utilizes a metaheuristic based on a greediness criterion and randomization, the SourceSplice framework presents a source selection mechanism inspired from gene splicing - a core concept used in protein synthesis. We empirically evaluate our algorithms on three real-world datasets and synthetic datasets and show that, with significantly fewer subset explorations, SourceSplice effectively identifies subsets of data sources leading to high task utility. We also conduct studies reporting the sensitivity of SourceSplice to the decision choices under several settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æœºå™¨å­¦ä¹  (ML) ä»»åŠ¡ä¸­å¦‚ä½•ä»æµ·é‡æ•°æ®æºä¸­é€‰æ‹©æœ€ä½³å­é›†ä»¥æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œä¼˜åŒ–é¢„æµ‹æ€§èƒ½ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„æ•°æ®å‘ç°æ–¹æ³•å¤šä¾§é‡äºå…ƒæ•°æ®åŒ¹é…æˆ–è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘æ•°æ®æºè´¨é‡å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† SourceGrasp å’Œ SourceSplice ä¸¤ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆç­›é€‰å‡ºèƒ½æœ€å¤§åŒ–æ¨¡å‹æ•ˆç”¨çš„æ•°æ®æºç»„åˆã€‚SourceGrasp é‡‡ç”¨åŸºäºè´ªå¿ƒå‡†åˆ™å’ŒéšæœºåŒ–çš„ metaheuristic æ–¹æ³•ï¼Œè€Œ SourceSplice åˆ™å¼•å…¥äº†å—è›‹ç™½è´¨åˆæˆä¸­ gene splicing å¯å‘çš„æºé€‰æ‹©æœºåˆ¶ã€‚ä¸¤ç§ç®—æ³•çš„æ ¸å¿ƒç†å¿µåœ¨äºä¸åŒçš„æ•°æ®æºåŠå…¶ç»„åˆå¯¹ä»»åŠ¡æ•ˆç”¨çš„è´¡çŒ®å„å¼‚ï¼Œéœ€è¦è¿›è¡Œå®¡æ…é€‰æ‹©ã€‚å®éªŒåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•ŒåŠåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¯æ˜ SourceSplice èƒ½å¤Ÿä»¥æ˜¾è‘—æ›´å°‘çš„å­é›†æ¢ç´¢æ¬¡æ•°ï¼Œæœ‰æ•ˆè¯†åˆ«å‡ºå…·æœ‰é«˜ä»»åŠ¡æ•ˆç”¨çš„æ•°æ®æºã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯¹ SourceSplice åœ¨ä¸åŒè®¾å®šä¸‹å¯¹å†³ç­–é€‰æ‹©çš„æ•æ„Ÿæ€§è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22186v2",
      "published_date": "2025-07-29 19:29:52 UTC",
      "updated_date": "2025-07-31 18:46:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:27.543048+00:00"
    },
    {
      "arxiv_id": "2507.22168v2",
      "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles",
      "title_zh": "äººæ ¼å¢å¼ºåŸºå‡†æµ‹è¯•ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·åŒ–å†™ä½œé£æ ¼ä¸‹çš„è¡¨ç°",
      "authors": [
        "Kimberly Le Truong",
        "Riccardo Fogliato",
        "Hoda Heidari",
        "Zhiwei Steven Wu"
      ],
      "abstract": "Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with \"non-standard\" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è¯„ä¼°åŸºå‡†ç¼ºä¹å†™ä½œé£æ ¼å¤šæ ·æ€§çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶éš¾ä»¥æ•æ‰äººç±»ä¸°å¯Œçš„æ²Ÿé€šæ¨¡å¼ã€‚ä¸ºäº†æµ‹è¯•æ¨¡å‹åœ¨éæ ‡å‡†è¾“å…¥ä¸‹çš„ç¨³å¥æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºäººæ ¼åŒ– (persona-based) æç¤ºè¯çš„é‡å†™æ–¹æ³•ï¼Œä»¥ä½æˆæœ¬æ¨¡æ‹Ÿå¤šæ ·åŒ–çš„å†™ä½œé£æ ¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿è¯­ä¹‰å†…å®¹å®Œå…¨ç›¸åŒï¼Œå†™ä½œé£æ ¼å’Œæç¤ºè¯æ ¼å¼çš„å·®å¼‚ä¹Ÿä¼šæ˜¾è‘—å½±å“ LLMs çš„æ€§èƒ½è¯„ä¼°ç»“æœã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒæŸäº›ç‰¹å®šé£æ ¼ä¼šè·¨æ¨¡å‹å®¶æ—ã€è§„æ¨¡å’Œå‘å¸ƒæ—¶é—´ä¸€è‡´åœ°è§¦å‘è¾ƒé«˜æˆ–è¾ƒä½çš„æ€§èƒ½è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œæå‡ºçš„ Persona-Augmented Benchmarking ä¸ºç°æœ‰åŸºå‡†æä¾›äº†å¯æ‰©å±•çš„å¢å¼ºæ‰‹æ®µï¼Œæ˜¾è‘—æå‡äº†è¡¡é‡æ¨¡å‹åœ¨å¤„ç†è¯­è¨€å˜ä½“æ—¶æ€§èƒ½çš„å¤–éƒ¨æœ‰æ•ˆæ€§ (external validity)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22168v2",
      "published_date": "2025-07-29 18:59:09 UTC",
      "updated_date": "2025-09-25 21:37:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:23.184939+00:00"
    },
    {
      "arxiv_id": "2507.22160v1",
      "title": "Strategic Deflection: Defending LLMs from Logit Manipulation",
      "title_zh": "ç­–ç•¥æ€§åè½¬ï¼šé˜²å¾¡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ Logit æ“çºµ",
      "authors": [
        "Yassine Rachidy",
        "Jihad Rbaiti",
        "Youssef Hmamouche",
        "Faissal Sehbaoui",
        "Amal El Fallah Seghrouchni"
      ],
      "abstract": "With the growing adoption of Large Language Models (LLMs) in critical areas, ensuring their security against jailbreaking attacks is paramount. While traditional defenses primarily rely on refusing malicious prompts, recent logit-level attacks have demonstrated the ability to bypass these safeguards by directly manipulating the token-selection process during generation. We introduce Strategic Deflection (SDeflection), a defense that redefines the LLM's response to such advanced attacks. Instead of outright refusal, the model produces an answer that is semantically adjacent to the user's request yet strips away the harmful intent, thereby neutralizing the attacker's harmful intent. Our experiments demonstrate that SDeflection significantly lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This work presents a critical shift in defensive strategies, moving from simple refusal to strategic content redirection to neutralize advanced threats.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„Logit Manipulationæ”»å‡»æå‡ºäº†åä¸ºStrategic Deflection (SDeflection)çš„é˜²å¾¡ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³é«˜çº§æ”»å‡»æ‰‹æ®µç»•è¿‡ä¼ ç»Ÿæ‹’ç»æœºåˆ¶çš„å®‰å…¨å¨èƒã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥æ‹’ç»(Refusal)ä¸åŒï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹ç”Ÿæˆä¸ç”¨æˆ·è¯·æ±‚è¯­ä¹‰ç›¸è¿‘ä½†å‰¥ç¦»äº†æœ‰å®³æ„å›¾çš„å›ç­”ï¼Œä»è€Œä¸­å’Œæ”»å‡»è€…çš„æ¶æ„ç›®çš„ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSDeflectionåœ¨æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡(ASR)çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŒæ¨¡å‹å¤„ç†è‰¯æ€§æŸ¥è¯¢çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†é˜²å¾¡ç­–ç•¥ä»ç®€å•æ‹’ç»å‘æˆ˜ç•¥æ€§å†…å®¹é‡å®šå‘(Strategic Content Redirection)çš„è½¬å˜ï¼Œä¸ºåº”å¯¹å¤æ‚çš„å¤§æ¨¡å‹å®‰å…¨å¨èƒæä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22160v1",
      "published_date": "2025-07-29 18:46:56 UTC",
      "updated_date": "2025-07-29 18:46:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:28.339829+00:00"
    },
    {
      "arxiv_id": "2507.22159v2",
      "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian",
      "title_zh": "IndoPrefï¼šé¢å‘å°åº¦å°¼è¥¿äºšè¯­çš„å¤šé¢†åŸŸæˆå¯¹åå¥½æ•°æ®é›†",
      "authors": [
        "Vanessa Rebecca Wiyono",
        "David Anugraha",
        "Ayu Purwarianti",
        "Genta Indra Winata"
      ],
      "abstract": "Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset designed to evaluate the naturalness and quality of LLM-generated text. The dataset contains 522 prompts and yields 4,099 human-annotated pairwise preferences from comparisons across five instruction-tuned LLMs. All annotations are natively written in Indonesian with strong inter-annotator agreement, measured by Krippendorff's alpha. Our benchmark spans 10 diverse categories, enabling practitioners to identify LLMs' fine-grained strengths and weaknesses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†IndoPrefï¼Œè¿™æ˜¯é¦–ä¸ªå®Œå…¨ç”±äººç±»åˆ›ä½œçš„å¤šé¢†åŸŸå°å°¼è¯­åå¥½æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å°å°¼è¯­åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åå¥½ç ”ç©¶ä¸­ä»£è¡¨æ€§ä¸è¶³ä¸”ç°æœ‰æ•°æ®ç¼ºä¹æ–‡åŒ–çœŸå®æ€§çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«522ä¸ªæç¤º(prompts)ï¼Œé€šè¿‡å¯¹5ä¸ªç»è¿‡æŒ‡ä»¤å¾®è°ƒ(instruction-tuned)çš„LLMsè¿›è¡Œå¯¹æ¯”ï¼Œäº§ç”Ÿäº†4,099æ¡äººç±»æ ‡æ³¨çš„ä¸¤ä¸¤åå¥½(pairwise preferences)æ•°æ®ã€‚æ‰€æœ‰æ ‡æ³¨å‡ç”±æ¯è¯­äººå£«å®Œæˆï¼Œå¹¶åœ¨Krippendorff's alphaè¡¡é‡ä¸‹è¡¨ç°å‡ºæé«˜çš„ä¸€è‡´æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–10ä¸ªå¤šæ ·åŒ–ç±»åˆ«ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿè¯†åˆ«æ¨¡å‹åœ¨å°å°¼è¯­è¯­å¢ƒä¸‹çš„ç»†ç²’åº¦ä¼˜åŠ£åŠ¿ã€‚IndoPrefçš„å‘å¸ƒä¸ºè¯„ä¼°å’Œæå‡ç”Ÿæˆæ–‡æœ¬çš„è‡ªç„¶åº¦ä¸è´¨é‡æä¾›äº†é‡è¦åŸºå‡†ï¼Œæœ‰æ•ˆå¡«è¡¥äº†è¯¥è¯­è¨€åœ¨åå¥½å­¦ä¹ é¢†åŸŸçš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IJCNLP-AACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22159v2",
      "published_date": "2025-07-29 18:46:25 UTC",
      "updated_date": "2025-11-11 21:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:32.741048+00:00"
    },
    {
      "arxiv_id": "2507.22157v1",
      "title": "Tiny Noise-Robust Voice Activity Detector for Voice Assistants",
      "title_zh": "é¢å‘è¯­éŸ³åŠ©æ‰‹çš„å¾®å‹æŠ—å™ªè¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨",
      "authors": [
        "Hamed Jafarzadeh Asl",
        "Mahsa Ghazvini Nejad",
        "Amin Edraki",
        "Masoud Asgharian",
        "Vahid Partovi Nia"
      ],
      "abstract": "Voice Activity Detection (VAD) in the presence of background noise remains a challenging problem in speech processing. Accurate VAD is essential in automatic speech recognition, voice-to-text, conversational agents, etc, where noise can severely degrade the performance. A modern application includes the voice assistant, specially mounted on Artificial Intelligence of Things (AIoT) devices such as cell phones, smart glasses, earbuds, etc, where the voice signal includes background noise. Therefore, VAD modules must remain light-weight due to their practical on-device limitation. The existing models often struggle with low signal-to-noise ratios across diverse acoustic environments. A simple VAD often detects human voice in a clean environment, but struggles to detect the human voice in noisy conditions. We propose a noise-robust VAD that comprises a light-weight VAD, with data pre-processing and post-processing added modules to handle the background noise. This approach significantly enhances the VAD accuracy in noisy environments and requires neither a larger model, nor fine-tuning. Experimental results demonstrate that our approach achieves a notable improvement compared to baselines, particularly in environments with high background noise interference. This modified VAD additionally improving clean speech detection.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è¯­éŸ³åŠ©æ‰‹ï¼Œå°¤å…¶æ˜¯æ­è½½åœ¨æ™ºèƒ½æ‰‹æœºã€æ™ºèƒ½çœ¼é•œç­‰ AIoT è®¾å¤‡ä¸Šçš„ç»ˆç«¯ï¼Œåœ¨èƒŒæ™¯å™ªå£°ç¯å¢ƒä¸‹ Voice Activity Detection (VAD) æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹åœ¨ä½ Signal-to-Noise Ratio (SNR) ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å™ªå£°é²æ£’æ€§ VAD æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åœ¨è½»é‡çº§ VAD æ ¸å¿ƒåŸºç¡€ä¸Šï¼Œé›†æˆäº†ä¸“é—¨çš„æ•°æ® Pre-processing å’Œ Post-processing æ¨¡å—ï¼Œä»¥æœ‰æ•ˆåº”å¯¹èƒŒæ™¯å™ªå£°ã€‚è¿™ç§è®¾è®¡åœ¨æ»¡è¶³è®¾å¤‡ç«¯ä¸¥è‹›è®¡ç®—èµ„æºé™åˆ¶çš„åŒæ—¶ï¼Œæ— éœ€å¼•å…¥æ›´å¤§è§„æ¨¡çš„æ¨¡å‹æˆ–è¿›è¡Œå¤æ‚çš„ Fine-tuningã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å˜ˆæ‚ç¯å¢ƒä¸‹çš„æ£€æµ‹å‡†ç¡®ç‡è¾ƒåŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œæ”¹è¿›åçš„æ¨¡å‹åœ¨ Clean speech çš„æ£€æµ‹æ€§èƒ½ä¸Šä¹Ÿå¾—åˆ°äº†è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¸ºå¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„å®æ—¶è¯­éŸ³å¤„ç†æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Hamed Jafarzadeh Asl and Mahsa Ghazvini Nejad contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2507.22157v1",
      "published_date": "2025-07-29 18:44:43 UTC",
      "updated_date": "2025-07-29 18:44:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:44.442152+00:00"
    },
    {
      "arxiv_id": "2507.22149v4",
      "title": "When Truthful Representations Flip Under Deceptive Instructions?",
      "title_zh": "æ¬ºéª—æ€§æŒ‡ä»¤ä¸‹ï¼ŒçœŸå®è¡¨å¾ä½•æ—¶å‘ç”Ÿç¿»è½¬ï¼Ÿ",
      "authors": [
        "Xianxuan Long",
        "Yao Fu",
        "Runchao Li",
        "Mu Sheng",
        "Haotian Yu",
        "Xiaotian Han",
        "Pan Li"
      ],
      "abstract": "Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations ``flip'', such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces. % Our analysis pinpoints layer-wise and feature-level correlates of instructed dishonesty, offering insights for LLM detection and control. Our findings expose feature- and layer-level signatures of deception, offering new insights for detecting and mitigating instructed dishonesty in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¬ºéª—æ€§æŒ‡ä»¤ä¸‹å†…éƒ¨è¡¨ç¤ºå¦‚ä½•ä»çœŸå®è½¬å‘æ¬ºéª—ï¼Œé‡ç‚¹åˆ†æäº† Llama-3.1-8B-Instruct å’Œ Gemma-2-9B-Instruct åœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­çš„å†…éƒ¨çŠ¶æ€ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹å—æŒ‡ä»¤å¼•å¯¼äº§ç”Ÿçš„çœŸ/å‡è¾“å‡ºåœ¨æ‰€æœ‰æ¡ä»¶ä¸‹éƒ½å¯ä»¥é€šè¿‡çº¿æ€§æ¢æµ‹(linear probes)åŸºäºå†…éƒ¨è¡¨ç¤ºè¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨(SAEs)ï¼Œç ”ç©¶æ­ç¤ºäº†æ¬ºéª—æ€§æŒ‡ä»¤è¯±å‘äº†æ˜¾è‘—çš„è¡¨ç¤ºåç§»ï¼Œä¸”è¿™ç§åç§»ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹çš„æ—©æœŸåˆ°ä¸­æœŸå±‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«å‡ºäº†å¯¹æ¬ºéª—æ€§æŒ‡ä»¤é«˜åº¦æ•æ„Ÿçš„ç‰¹å®š SAE ç‰¹å¾ï¼Œå¹¶é€šè¿‡é’ˆå¯¹æ€§å¯è§†åŒ–ç¡®è®¤äº†çœŸå®ä¸æ¬ºéª—è¡¨ç¤ºä¹‹é—´å­˜åœ¨æˆªç„¶ä¸åŒçš„å­ç©ºé—´ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†æ¬ºéª—è¡Œä¸ºåœ¨ç‰¹å¾å’Œå±‚çº§å±‚é¢çš„ç‰¹å¾ç­¾åï¼Œä¸ºæ£€æµ‹å’Œç¼“è§£å¤§è¯­è¨€æ¨¡å‹å—æŒ‡ä»¤è¯±å¯¼çš„ä¸è¯šå®è¡Œä¸ºæä¾›äº†å…³é”®æ´å¯Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22149v4",
      "published_date": "2025-07-29 18:27:13 UTC",
      "updated_date": "2025-10-29 02:12:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:50.290385+00:00"
    },
    {
      "arxiv_id": "2507.22099v1",
      "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?",
      "title_zh": "åŸºäºç‰©ç†å¼•æ“çš„è½¯ä»¶ç³»ç»Ÿè¿è¡Œæ—¶æ•…éšœæ£€æµ‹ï¼šæˆ‘ä»¬èƒ½èµ°å¤šè¿œï¼Ÿ",
      "authors": [
        "Shuqing Li",
        "Qiang Chen",
        "Xiaoxue Ren",
        "Michael R. Lyu"
      ],
      "abstract": "Physics Engines (PEs) are fundamental software frameworks that simulate physical interactions in applications ranging from entertainment to safety-critical systems. Despite their importance, PEs suffer from physics failures, deviations from expected physical behaviors that can compromise software reliability, degrade user experience, and potentially cause critical failures in autonomous vehicles or medical robotics. Current testing approaches for PE-based software are inadequate, typically requiring white-box access and focusing on crash detection rather than semantically complex physics failures. This paper presents the first large-scale empirical study characterizing physics failures in PE-based software. We investigate three research questions addressing the manifestations of physics failures, the effectiveness of detection techniques, and developer perceptions of current detection practices. Our contributions include: (1) a taxonomy of physics failure manifestations; (2) a comprehensive evaluation of detection methods including deep learning, prompt-based techniques, and large multimodal models; and (3) actionable insights from developer experiences for improving detection approaches. To support future research, we release PhysiXFails, code, and other materials at https://sites.google.com/view/physics-failure-detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©ç†å¼•æ“ (Physics Engines, PEs) åœ¨è‡ªåŠ¨é©¾é©¶å’ŒåŒ»ç–—æœºå™¨äººç­‰å…³é”®åº”ç”¨ä¸­å¸¸è§çš„ç‰©ç†å¤±æ•ˆ (physics failures) é—®é¢˜ï¼Œå¼€å±•äº†é¦–ä¸ªå¤§è§„æ¨¡å®è¯ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰æµ‹è¯•æ–¹æ³•è¿‡åº¦ä¾èµ–ç™½ç›’è®¿é—®ä¸”éš¾ä»¥æ£€æµ‹å¤æ‚è¯­ä¹‰å¤±æ•ˆçš„å±€é™æ€§ï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†ç‰©ç†å¤±æ•ˆçš„è¡¨ç°å½¢å¼ã€æ£€æµ‹æŠ€æœ¯çš„æœ‰æ•ˆæ€§ä»¥åŠå¼€å‘è€…çš„å®é™…åé¦ˆã€‚ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ„å»ºäº†ç‰©ç†å¤±æ•ˆè¡¨ç°å½¢å¼çš„åˆ†ç±»ä½“ç³» (taxonomy)ï¼Œå¹¶å…¨é¢è¯„ä¼°äº†æ·±åº¦å­¦ä¹  (deep learning)ã€åŸºäºæç¤ºçš„æŠ€æœ¯ (prompt-based techniques) ä»¥åŠå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (large multimodal models) çš„æ£€æµ‹æ•ˆèƒ½ã€‚é€šè¿‡åˆ†æå®éªŒæ•°æ®ï¼Œç ”ç©¶æå‡ºäº†æ”¹è¿›æ£€æµ‹æ–¹æ³•çš„å®ç”¨è§è§£ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº† PhysiXFails æ•°æ®é›†å’Œç›¸å…³ä»£ç ï¼Œä¸ºæå‡åŸºäºç‰©ç†å¼•æ“çš„è½¯ä»¶ç³»ç»Ÿå¯é æ€§æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "cs.SE"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22099v1",
      "published_date": "2025-07-29 17:58:41 UTC",
      "updated_date": "2025-07-29 17:58:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:53.191435+00:00"
    },
    {
      "arxiv_id": "2507.22053v1",
      "title": "Foundation Models for Demand Forecasting via Dual-Strategy Ensembling",
      "title_zh": "åŸºäºåŒç­–ç•¥é›†æˆçš„éœ€æ±‚é¢„æµ‹åŸºç¡€æ¨¡å‹",
      "authors": [
        "Wei Yang",
        "Defu Cao",
        "Yan Liu"
      ],
      "abstract": "Accurate demand forecasting is critical for supply chain optimization, yet remains difficult in practice due to hierarchical complexity, domain shifts, and evolving external factors. While recent foundation models offer strong potential for time series forecasting, they often suffer from architectural rigidity and limited robustness under distributional change. In this paper, we propose a unified ensemble framework that enhances the performance of foundation models for sales forecasting in real-world supply chains. Our method combines two complementary strategies: (1) Hierarchical Ensemble (HE), which partitions training and inference by semantic levels (e.g., store, category, department) to capture localized patterns; and (2) Architectural Ensemble (AE), which integrates predictions from diverse model backbones to mitigate bias and improve stability. We conduct extensive experiments on the M5 benchmark and three external sales datasets, covering both in-domain and zero-shot forecasting. Results show that our approach consistently outperforms strong baselines, improves accuracy across hierarchical levels, and provides a simple yet effective mechanism for boosting generalization in complex forecasting environments.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåŒç­–ç•¥é›†æˆ(Dual-Strategy Ensembling)çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºåŸºç¡€æ¨¡å‹(Foundation Models)åœ¨å®é™…ä¾›åº”é“¾éœ€æ±‚é¢„æµ‹ä¸­çš„æ€§èƒ½ï¼Œä»¥åº”å¯¹åˆ†å±‚å¤æ‚æ€§ã€é¢†åŸŸæ¼‚ç§»å’Œåˆ†å¸ƒå˜åŒ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åˆ†å±‚é›†æˆ(Hierarchical Ensemble, HE)ç­–ç•¥ï¼Œé€šè¿‡æŒ‰è¯­ä¹‰çº§åˆ«ï¼ˆå¦‚å•†åº—ã€ç±»åˆ«ã€éƒ¨é—¨ï¼‰åˆ’åˆ†è®­ç»ƒå’Œæ¨ç†ï¼Œä»è€Œæ•æ‰å±€éƒ¨åŒ–æ¨¡å¼ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ¶æ„é›†æˆ(Architectural Ensemble, AE)ç­–ç•¥ï¼Œæ•´åˆæ¥è‡ªä¸åŒæ¨¡å‹éª¨å¹²(Backbones)çš„é¢„æµ‹ç»“æœï¼Œæœ‰æ•ˆå‡è½»äº†åå·®å¹¶æ˜¾è‘—æé«˜ç¨³å®šæ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ M5 åŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªå¤–éƒ¨é”€å”®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæ¶µç›–äº†åŸŸå†…(In-domain)å’Œé›¶æ ·æœ¬(Zero-shot)é¢„æµ‹åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„å±‚çº§å‡†ç¡®ç‡ä¸Šå‡ä¸€è‡´ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œä¸ºåœ¨å¤æ‚é¢„æµ‹ç¯å¢ƒä¸­æå‡é€šç”¨æ³›åŒ–èƒ½åŠ›æä¾›äº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æœºåˆ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22053v1",
      "published_date": "2025-07-29 17:56:38 UTC",
      "updated_date": "2025-07-29 17:56:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:25:54.588031+00:00"
    },
    {
      "arxiv_id": "2507.22047v1",
      "title": "The Interspeech 2025 Speech Accessibility Project Challenge",
      "title_zh": "Interspeech 2025 è¯­éŸ³æ— éšœç¢é¡¹ç›®æŒ‘æˆ˜èµ›",
      "authors": [
        "Xiuwen Zheng",
        "Bornali Phukon",
        "Jonghwan Na",
        "Ed Cutrell",
        "Kyu Han",
        "Mark Hasegawa-Johnson",
        "Pan-Pan Jiang",
        "Aadhrik Kuila",
        "Colin Lea",
        "Bob MacDonald",
        "Gautam Mantena",
        "Venkatesh Ravichandran",
        "Leda Sari",
        "Katrin Tomanek",
        "Chang D. Yoo",
        "Chris Zwilling"
      ],
      "abstract": "While the last decade has witnessed significant advancements in Automatic Speech Recognition (ASR) systems, performance of these systems for individuals with speech disabilities remains inadequate, partly due to limited public training data. To bridge this gap, the 2025 Interspeech Speech Accessibility Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data collected and transcribed from more than 500 individuals with diverse speech disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline, the SAP Challenge evaluates submissions based on Word Error Rate and Semantic Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2 baseline in terms of WER, while 17 teams surpassed the baseline on SemScore. Notably, the top team achieved the lowest WER of 8.11\\%, and the highest SemScore of 88.44\\% at the same time, setting new benchmarks for future ASR systems in recognizing impaired speech.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†2025å¹´Interspeechè¯­éŸ³å¯è®¿é—®æ€§é¡¹ç›®æŒ‘æˆ˜èµ›(Speech Accessibility Project Challenge, SAP)ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿåœ¨å¤„ç†è¯­éŸ³éšœç¢æ—¶å› è®­ç»ƒæ•°æ®ä¸è¶³è€Œå¯¼è‡´æ€§èƒ½å—é™çš„é—®é¢˜ã€‚è¯¥æŒ‘æˆ˜èµ›æä¾›äº†ä»500å¤šåæ‚£æœ‰å¤šç§è¯­éŸ³éšœç¢çš„ä¸ªä½“ä¸­æ”¶é›†å¹¶è½¬å½•çš„è¶…è¿‡400å°æ—¶SAPæ•°æ®ï¼Œå¹¶åœ¨EvalAIå¹³å°ä¸Šé€šè¿‡è¯é”™è¯¯ç‡(Word Error Rate, WER)å’Œè¯­ä¹‰å¾—åˆ†(Semantic Score, SemScore)è¿›è¡Œè¯„ä¼°ã€‚åœ¨å‚èµ›çš„22æ”¯æœ‰æ•ˆå›¢é˜Ÿä¸­ï¼Œå…±æœ‰12æ”¯å›¢é˜Ÿåœ¨WERæŒ‡æ ‡ä¸Šè¶…è¶Šäº†whisper-large-v2åŸºçº¿æ¨¡å‹ï¼Œ17æ”¯å›¢é˜Ÿåœ¨SemScoreä¸Šå–å¾—æ›´ä¼˜è¡¨ç°ã€‚å…¶ä¸­é¡¶å°–å›¢é˜ŸåŒæ—¶å®ç°äº†8.11%çš„æœ€ä½WERå’Œ88.44%çš„æœ€é«˜SemScoreï¼Œä¸ºæœªæ¥é’ˆå¯¹å—æŸè¯­éŸ³è¯†åˆ«çš„ASRç³»ç»Ÿç ”ç©¶æ ‘ç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in Proceedings of Interspeech, 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22047v1",
      "published_date": "2025-07-29 17:50:59 UTC",
      "updated_date": "2025-07-29 17:50:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:01.083302+00:00"
    },
    {
      "arxiv_id": "2507.22039v2",
      "title": "Analysis of Quantum Image Representations for Supervised Classification",
      "title_zh": "ç”¨äºç›‘ç£åˆ†ç±»çš„é‡å­å›¾åƒè¡¨ç¤ºåˆ†æ",
      "authors": [
        "Marco Parigi",
        "Mehran Khosrojerdi",
        "Filippo Caruso",
        "Leonardo Banchi"
      ],
      "abstract": "In the era of big data and artificial intelligence, the increasing volume of data and the demand to solve more and more complex computational challenges are two driving forces for improving the efficiency of data storage, processing and analysis. Quantum image processing (QIP) is an interdisciplinary field between quantum information science and image processing, which has the potential to alleviate some of these challenges by leveraging the power of quantum computing. In this work, we compare and examine the compression properties of four different Quantum Image Representations (QImRs): namely, Tensor Network Representation (TNR), Flexible Representation of Quantum Image (FRQI), Novel Enhanced Quantum Representation NEQR, and Quantum Probability Image Encoding (QPIE). Our simulations show that FRQI and QPIE perform a higher compression of image information than TNR and NEQR. Furthermore, we investigate the trade-off between accuracy and memory in binary classification problems, evaluating the performance of quantum kernels based on QImRs compared to the classical linear kernel. Our results indicate that quantum kernels provide comparable classification average accuracy but require exponentially fewer resources for image storage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡å­å›¾åƒå¤„ç†(QIP)åœ¨åº”å¯¹å¤§æ•°æ®æ—¶ä»£è®¡ç®—æŒ‘æˆ˜ä¸­çš„æ½œåŠ›ï¼Œé‡ç‚¹åˆ†æäº†æå‡æ•°æ®å­˜å‚¨ä¸åˆ†ææ•ˆç‡çš„æ–¹æ³•ã€‚ä½œè€…å¯¹æ¯”ç ”ç©¶äº†å››ç§é‡å­å›¾åƒè¡¨ç¤º(QImRs)çš„å‹ç¼©ç‰¹æ€§ï¼ŒåŒ…æ‹¬Tensor Network Representation (TNR)ã€Flexible Representation of Quantum Image (FRQI)ã€Novel Enhanced Quantum Representation (NEQR)ä»¥åŠQuantum Probability Image Encoding (QPIE)ã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒFRQIå’ŒQPIEåœ¨å›¾åƒä¿¡æ¯å‹ç¼©æ•ˆç‡ä¸Šä¼˜äºTNRå’ŒNEQRã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥è°ƒæŸ¥äº†äºŒåˆ†ç±»ä»»åŠ¡ä¸­å‡†ç¡®ç‡ä¸å†…å­˜ä¹‹é—´çš„æƒè¡¡ï¼Œè¯„ä¼°äº†åŸºäºQImRsçš„é‡å­æ ¸(Quantum Kernels)ç›¸å¯¹äºç»å…¸çº¿æ€§æ ¸çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé‡å­æ ¸åœ¨ä¿æŒä¸ç»å…¸æ–¹æ³•ç›¸å½“çš„åˆ†ç±»å¹³å‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä»¥æŒ‡æ•°çº§å‡å°‘å›¾åƒå­˜å‚¨æ‰€éœ€çš„èµ„æºã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22039v2",
      "published_date": "2025-07-29 17:40:59 UTC",
      "updated_date": "2026-01-14 10:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:08.187621+00:00"
    },
    {
      "arxiv_id": "2507.22037v1",
      "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security",
      "title_zh": "Secure Tug-of-War (SecTOW)ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€æ¨¡å‹å®‰å…¨è¿­ä»£å¼æ”»é˜²è®­ç»ƒ",
      "authors": [
        "Muzhi Dai",
        "Shixuan Liu",
        "Zhiyuan Zhao",
        "Junyu Gao",
        "Hao Sun",
        "Xuelong Li"
      ],
      "abstract": "The rapid advancement of multimodal large language models (MLLMs) has led to breakthroughs in various applications, yet their security remains a critical challenge. One pressing issue involves unsafe image-query pairs--jailbreak inputs specifically designed to bypass security constraints and elicit unintended responses from MLLMs. Compared to general multimodal data, such unsafe inputs are relatively sparse, which limits the diversity and richness of training samples available for developing robust defense models. Meanwhile, existing guardrail-type methods rely on external modules to enforce security constraints but fail to address intrinsic vulnerabilities within MLLMs. Traditional supervised fine-tuning (SFT), on the other hand, often over-refuses harmless inputs, compromising general performance. Given these challenges, we propose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack training method to enhance the security of MLLMs. SecTOW consists of two modules: a defender and an auxiliary attacker, both trained iteratively using reinforcement learning (GRPO). During the iterative process, the attacker identifies security vulnerabilities in the defense model and expands jailbreak data. The expanded data are then used to train the defender, enabling it to address identified security vulnerabilities. We also design reward mechanisms used for GRPO to simplify the use of response labels, reducing dependence on complex generative labels and enabling the efficient use of synthetic data. Additionally, a quality monitoring mechanism is used to mitigate the defender's over-refusal of harmless inputs and ensure the diversity of the jailbreak data generated by the attacker. Experimental results on safety-specific and general benchmarks demonstrate that SecTOW significantly improves security while preserving general performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Secure Tug-of-War (SecTOW)ï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) å®‰å…¨æ€§çš„è¿­ä»£é˜²å¾¡-æ”»å‡»è®­ç»ƒæ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰é˜²å¾¡æ‰‹æ®µéš¾ä»¥å¤„ç†å†…åœ¨æ¼æ´ä»¥åŠç›‘ç£å¾®è°ƒ (SFT) å®¹æ˜“å¯¼è‡´è¿‡åº¦æ‹’ç» (Over-refusal) çš„æŒ‘æˆ˜ï¼ŒSecTOW åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (GRPO) å¯¹é˜²å¾¡è€…å’Œè¾…åŠ©æ”»å‡»è€…è¿›è¡ŒåŒæ­¥è¿­ä»£è®­ç»ƒã€‚åœ¨è¿‡ç¨‹ä¸­ï¼Œæ”»å‡»è€…è´Ÿè´£è¯†åˆ«é˜²å¾¡æ¨¡å‹çš„å®‰å…¨è–„å¼±ç¯èŠ‚å¹¶æ‰©å±•è¶Šç‹±æ•°æ®ï¼Œè€Œé˜²å¾¡è€…åˆ™é€šè¿‡å­¦ä¹ è¿™äº›æ•°æ®æ¥ä¿®å¤æ¼æ´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†ä¸“é—¨çš„å¥–åŠ±æœºåˆ¶ä»¥ç®€åŒ–å¯¹å“åº”æ ‡ç­¾çš„ä¾èµ–ï¼Œå¹¶å¼•å…¥è´¨é‡ç›‘æ§æœºåˆ¶æ¥å¹³è¡¡å®‰å…¨æ€§ä¸æ¨¡å‹çš„å¤šæ ·æ€§è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSecTOW åœ¨å¤šä¸ªå®‰å…¨ä¸“ç”¨å’Œé€šç”¨åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æé«˜äº† MLLMs çš„å®‰å…¨æ€§ï¼Œä¸”æœ‰æ•ˆé¿å…äº†é€šç”¨æ€§èƒ½çš„æŠ˜æŸã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22037v1",
      "published_date": "2025-07-29 17:39:48 UTC",
      "updated_date": "2025-07-29 17:39:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:10.083729+00:00"
    },
    {
      "arxiv_id": "2507.22034v1",
      "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
      "title_zh": "UserBenchï¼šé¢å‘ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒæ™ºèƒ½ä½“çš„äº¤äº’å¼ Gym ç¯å¢ƒ",
      "authors": [
        "Cheng Qian",
        "Zuxin Liu",
        "Akshara Prabhakar",
        "Zhiwei Liu",
        "Jianguo Zhang",
        "Haolin Chen",
        "Heng Ji",
        "Weiran Yao",
        "Shelby Heinecke",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UserBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤šè½®ã€åå¥½é©±åŠ¨äº¤äº’ä¸­è¡¨ç°è€Œè®¾è®¡çš„ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ç¯å¢ƒã€‚æ—¨åœ¨è§£å†³å½“å‰åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ™ºèƒ½ä½“åœ¨å¤„ç†æ¨¡ç³Šã€æ¼”å˜æˆ–é—´æ¥è¡¨è¾¾çš„ç”¨æˆ·ç›®æ ‡æ—¶ï¼Œä¸»åŠ¨åä½œèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚UserBench é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·èµ·å§‹çš„æ¨¡ç³Šç›®æ ‡å¹¶é€æ­¥æ­ç¤ºåå¥½ï¼Œè¦æ±‚æ™ºèƒ½ä½“ä¸»åŠ¨æ¾„æ¸…æ„å›¾å¹¶è¿›è¡ŒåŸºäºå·¥å…·çš„å†³ç­–ã€‚å¯¹é¢†å…ˆå¼€æºåŠé—­æºæ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†ä»»åŠ¡å®Œæˆä¸ç”¨æˆ·å¯¹é½ (User Alignment) ä¹‹é—´çš„æ˜¾è‘—è„±èŠ‚ï¼Œæ¨¡å‹å®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾çš„æƒ…å†µå¹³å‡ä»…å  20%ã€‚å³ä¾¿æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿåªèƒ½é€šè¿‡äº¤äº’æŒ–æ˜å‡ºä¸è¶³ 30% çš„ç”¨æˆ·åå¥½ï¼Œè¿™å‡¸æ˜¾äº†å°†æ™ºèƒ½ä½“ä»ä»»åŠ¡æ‰§è¡Œè€…è½¬å˜ä¸ºçœŸæ­£çš„åä½œä¼™ä¼´æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚UserBench ä¸ºè¡¡é‡å’Œæ¨è¿›æ™ºèƒ½ä½“è¿™ä¸€å…³é”®èƒ½åŠ›æä¾›äº†äº¤äº’å¼è¯„ä¼°ç¯å¢ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "25 Pages, 17 Figures, 6 Tables",
      "pdf_url": "https://arxiv.org/pdf/2507.22034v1",
      "published_date": "2025-07-29 17:34:12 UTC",
      "updated_date": "2025-07-29 17:34:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:09.642060+00:00"
    },
    {
      "arxiv_id": "2507.22030v2",
      "title": "ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports",
      "title_zh": "ReXGroundingCTï¼šç”¨äºè‡ªç”±æ–‡æœ¬æŠ¥å‘Šå¾è±¡åˆ†å‰²çš„3Dèƒ¸éƒ¨CTæ•°æ®é›†",
      "authors": [
        "Mohammed Baharoon",
        "Luyang Luo",
        "Michael Moritz",
        "Abhinav Kumar",
        "Sung Eun Kim",
        "Xiaoman Zhang",
        "Miao Zhu",
        "Mahmoud Hussain Alabbad",
        "Maha Sbayel Alhazmi",
        "Neel P. Mistry",
        "Lucas Bijnens",
        "Kent Ryan Kleinschmidt",
        "Brady Chrisler",
        "Sathvik Suryadevara",
        "Sri Sai Dinesh Jaliparthi",
        "Noah Michael Prudlo",
        "Mark David Marino",
        "Jeremy Palacio",
        "Rithvik Akula",
        "Di Zhou",
        "Hong-Yu Zhou",
        "Ibrahim Ethem Hamamci",
        "Scott J. Adams",
        "Hassan Rayhan AlOmaish",
        "Pranav Rajpurkar"
      ],
      "abstract": "We introduce ReXGroundingCT, the first publicly available dataset linking free-text findings to pixel-level 3D segmentations in chest CT scans. The dataset includes 3,142 non-contrast chest CT scans paired with standardized radiology reports from CT-RATE. Construction followed a structured three-stage pipeline. First, GPT-4 was used to extract and standardize findings, descriptors, and metadata from reports originally written in Turkish and machine-translated into English. Second, GPT-4o-mini categorized each finding into a hierarchical ontology of lung and pleural abnormalities. Third, 3D annotations were produced for all CT volumes: the training set was quality-assured by board-certified radiologists, and the validation and test sets were fully annotated by board-certified radiologists. Additionally, a complementary chain-of-thought dataset was created to provide step-by-step hierarchical anatomical reasoning for localizing findings within the CT volume, using GPT-4o and localization coordinates derived from organ segmentation models. ReXGroundingCT contains 16,301 annotated entities across 8,028 text-to-3D-segmentation pairs, covering diverse radiological patterns from 3,142 non-contrast CT scans. About 79% of findings are focal abnormalities and 21% are non-focal. The dataset includes a public validation set of 50 cases and a private test set of 100 cases, both annotated by board-certified radiologists. The dataset establishes a foundation for enabling free-text finding segmentation and grounded radiology report generation in CT imaging. Model performance on the private test set is hosted on a public leaderboard at https://rexrank.ai/ReXGroundingCT. The dataset is available at https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ReXGroundingCTï¼Œè¿™æ˜¯é¦–ä¸ªå°† free-text findings ä¸èƒ¸éƒ¨ CT æ‰«æä¸­çš„åƒç´ çº§ 3D segmentations ç›¸å…³è”çš„å…¬å¼€æ•°æ®é›†ã€‚æ•°æ®é›†åŒ…å« 3,142 æ¬¡ non-contrast chest CT æ‰«æï¼Œåˆ©ç”¨ GPT-4 æå–æŠ¥å‘Šä¸­çš„å‘ç°ä¸å…ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ GPT-4o-mini è¿›è¡Œå±‚æ¬¡åŒ–æœ¬ä½“åˆ†ç±»ã€‚æ‰€æœ‰ 3D annotations å‡ç»è¿‡ board-certified radiologists çš„è´¨é‡ä¿è¯æˆ–å®Œæ•´æ ‡æ³¨ï¼Œæ¶µç›–äº† 16,301 ä¸ªæ ‡æ³¨å®ä½“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é…å¥—åˆ›å»ºäº† chain-of-thought æ•°æ®é›†ï¼Œé€šè¿‡ GPT-4o æä¾›å®šä½å‘ç°çš„é€æ­¥å±‚æ¬¡åŒ–è§£å‰–æ¨ç†ã€‚ReXGroundingCT çš„å‘å¸ƒä¸ºå®ç° CT æˆåƒä¸­çš„ free-text finding segmentation å’Œ grounded radiology report generation å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22030v2",
      "published_date": "2025-07-29 17:27:15 UTC",
      "updated_date": "2025-10-27 17:51:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:15.794369+00:00"
    },
    {
      "arxiv_id": "2507.22025v3",
      "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding",
      "title_zh": "UI-AGILEï¼šé€šè¿‡é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ä¸ç²¾ç¡®æ¨ç†æ—¶å®šä½æå‡ GUI æ™ºèƒ½ä½“æ€§èƒ½",
      "authors": [
        "Shuquan Lian",
        "Yuhang Wu",
        "Jia Ma",
        "Yifan Ding",
        "Zihan Song",
        "Bingqi Chen",
        "Xiawu Zheng",
        "Hui Li"
      ],
      "abstract": "The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE for enhancing GUI agents at both training and inference. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a continuous reward function to incentivize high-precision grounding; 2) a ``Simple Thinking'' reward to balance planning with speed and grounding accuracy; and 3) a cropping-based resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present decomposed grounding with selection to dramatically improve grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art grounding performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent capabilities. For instance, using both our training and inference enhancement methods brings 23\\% grounding accuracy improvement over the best baseline on ScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UI-AGILE æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ”¹è¿›è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯æ¥æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ GUI æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œç ”ç©¶è€…ä¼˜åŒ–äº† Supervised Fine-Tuning (SFT) è¿‡ç¨‹ï¼Œå¼•å…¥äº†ç”¨äºé«˜ç²¾åº¦ Grounding çš„è¿ç»­å¥–åŠ±å‡½æ•°ï¼Œä»¥åŠå¹³è¡¡è§„åˆ’é€Ÿåº¦ä¸å‡†ç¡®æ€§çš„ \"Simple Thinking\" å¥–åŠ±ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºè£å‰ªçš„é‡é‡‡æ ·ç­–ç•¥è§£å†³äº†å¤æ‚ä»»åŠ¡ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç ”ç©¶æå‡ºäº†å¸¦æœ‰é€‰æ‹©æœºåˆ¶çš„åˆ†è§£å¼ Grounding æ–¹æ³•ï¼Œé€šè¿‡å°†é«˜åˆ†è¾¨ç‡å›¾åƒæ‹†åˆ†ä¸ºæ›´å°çš„éƒ¨åˆ†ï¼Œæ˜¾è‘—æå‡äº†å®šä½ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUI-AGILE åœ¨ ScreenSpot-Pro å’Œ ScreenSpot-v2 åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº† State-of-the-art çš„æ€§èƒ½è¡¨ç°ã€‚ç»“åˆè®­ç»ƒä¸æ¨ç†ä¼˜åŒ–åçš„æ–¹æ³•åœ¨ ScreenSpot-Pro ä¸Šçš„ Grounding å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€ä¼˜åŸºçº¿æå‡äº† 23%ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨é€šç”¨æ™ºèƒ½ä½“ä»»åŠ¡ä¸­çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22025v3",
      "published_date": "2025-07-29 17:22:07 UTC",
      "updated_date": "2025-08-09 17:51:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:23.993455+00:00"
    },
    {
      "arxiv_id": "2507.22020v1",
      "title": "XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation",
      "title_zh": "åŸºäºæœ‰æ„ä¹‰åˆ†å‰²æ‰°åŠ¨çš„ç‚¹äº‘æ•°æ®å¯è§£é‡Šäººå·¥æ™ºèƒ½",
      "authors": [
        "Raju Ningappa Mulawade",
        "Christoph Garth",
        "Alexander Wiebel"
      ],
      "abstract": "We propose a novel segmentation-based explainable artificial intelligence (XAI) method for neural networks working on point cloud classification. As one building block of this method, we propose a novel point-shifting mechanism to introduce perturbations in point cloud data. Recently, AI has seen an exponential growth. Hence, it is important to understand the decision-making process of AI algorithms when they are applied in critical areas. Our work focuses on explaining AI algorithms that classify point cloud data. An important aspect of the methods used for explaining AI algorithms is their ability to produce explanations that are easy for humans to understand. This allows them to analyze the AI algorithms better and make appropriate decisions based on that analysis. Therefore, in this work, we intend to generate meaningful explanations that can be easily interpreted by humans. The point cloud data we consider represents 3D objects such as cars, guitars, and laptops. We make use of point cloud segmentation models to generate explanations for the working of classification models. The segments are used to introduce perturbations into the input point cloud data and generate saliency maps. The perturbations are introduced using the novel point-shifting mechanism proposed in this work which ensures that the shifted points no longer influence the output of the classification algorithm. In contrast to previous methods, the segments used by our method are meaningful, i.e. humans can easily interpret the meaning of the segments. Thus, the benefit of our method over other methods is its ability to produce more meaningful saliency maps. We compare our method with the use of classical clustering algorithms to generate explanations. We also analyze the saliency maps generated for example inputs using our method to demonstrate the usefulness of the method in generating meaningful explanations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ç‚¹äº‘åˆ†ç±»(Point Cloud Classification)ç¥ç»ç½‘ç»œçš„æ–°å‹å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆæ›´ç¬¦åˆäººç±»è®¤çŸ¥çš„è§£é‡Šä¿¡æ¯ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨æœ‰æ„ä¹‰çš„åˆ†å‰²(Meaningful Segmentation)æ¨¡å‹æ¥æ›¿ä»£ä¼ ç»Ÿçš„èšç±»ç®—æ³•ï¼Œå¹¶ä»¥æ­¤å¯¹è¾“å…¥ç‚¹äº‘ç”Ÿæˆæ˜¾è‘—æ€§å›¾(Saliency Maps)ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ç‚¹ç§»ä½æœºåˆ¶(Point-shifting Mechanism)æ¥äº§ç”Ÿæ‰°åŠ¨ï¼Œé€šè¿‡ç§»åŠ¨ç‰¹å®šç‚¹é›†ä½¿å…¶ä¸å†å½±å“åˆ†ç±»ç®—æ³•çš„è¾“å‡ºï¼Œä»è€Œç¡®ä¿è§£é‡Šçš„å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„åˆ†å‰²å—å…·æœ‰æ˜ç¡®è¯­ä¹‰ï¼Œä½¿äººç±»èƒ½å¤Ÿæ›´ç›´è§‚åœ°ç†è§£AIåœ¨å¤„ç†æ±½è½¦ã€å‰ä»–ç­‰3Dç‰©ä½“æ—¶çš„å†³ç­–é€»è¾‘ã€‚é€šè¿‡ä¸ä¼ ç»Ÿç®—æ³•çš„å¯¹æ¯”å®éªŒåŠå®ä¾‹åˆ†æï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨äº§ç”Ÿé«˜å¯è§£é‡Šæ€§æ˜¾è‘—æ€§å›¾æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå¤æ‚3Dæ•°æ®å¤„ç†ç®—æ³•çš„é€æ˜åŒ–æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22020v1",
      "published_date": "2025-07-29 17:12:16 UTC",
      "updated_date": "2025-07-29 17:12:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:41.897842+00:00"
    },
    {
      "arxiv_id": "2507.22010v1",
      "title": "Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform",
      "title_zh": "å€ŸåŠ©ä½“ç§¯å¢é•¿å˜æ¢æ¢ç©¶ RL æ¸¸æˆçš„åˆ†å±‚ç©ºé—´ç»“æ„",
      "authors": [
        "Justin Curry",
        "Brennan Lagasse",
        "Ngoc B. Lam",
        "Gregory Cox",
        "David Rosenbluth",
        "Alberto Speranzon"
      ],
      "abstract": "In this work, we explore the structure of the embedding space of a transformer model trained for playing a particular reinforcement learning (RL) game. Specifically, we investigate how a transformer-based Proximal Policy Optimization (PPO) model embeds visual inputs in a simple environment where an agent must collect \"coins\" while avoiding dynamic obstacles consisting of \"spotlights.\" By adapting Robinson et al.'s study of the volume growth transform for LLMs to the RL setting, we find that the token embedding space for our visual coin collecting game is also not a manifold, and is better modeled as a stratified space, where local dimension can vary from point to point. We further strengthen Robinson's method by proving that fairly general volume growth curves can be realized by stratified spaces. Finally, we carry out an analysis that suggests that as an RL agent acts, its latent representation alternates between periods of low local dimension, while following a fixed sub-strategy, and bursts of high local dimension, where the agent achieves a sub-goal (e.g., collecting an object) or where the environmental complexity increases (e.g., more obstacles appear). Consequently, our work suggests that the distribution of dimensions in a stratified latent space may provide a new geometric indicator of complexity for RL games.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä½“ç§¯å¢é•¿å˜æ¢(volume growth transform)æ¢è®¨äº†ä¸ºç‰¹å®šå¼ºåŒ–å­¦ä¹ (RL)æ¸¸æˆè®­ç»ƒçš„Transformeræ¨¡å‹åµŒå…¥ç©ºé—´ç»“æ„ï¼Œç‰¹åˆ«å…³æ³¨äº†åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)çš„æ¨¡å‹åœ¨ç¡¬å¸æ”¶é›†ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°è¯¥æ¸¸æˆçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´å¹¶éæµå½¢(manifold)ï¼Œè€Œåº”è¢«å»ºæ¨¡ä¸ºå±€éƒ¨ç»´åº¦åŠ¨æ€å˜åŒ–çš„å±‚åŒ–ç©ºé—´(stratified space)ã€‚é€šè¿‡ç†è®ºè¯æ˜ï¼Œè¯¥å·¥ä½œç¡®è®¤äº†é€šç”¨çš„ä½“ç§¯å¢é•¿æ›²çº¿å¯ä»¥ç”±å±‚åŒ–ç©ºé—´å®ç°ï¼Œä»è€ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œæ™ºèƒ½ä½“çš„æ½œè¡¨ç¤ºåœ¨éµå¾ªå›ºå®šå­ç­–ç•¥çš„ä½ç»´åº¦é˜¶æ®µä¸è¾¾æˆå­ç›®æ ‡æˆ–ç¯å¢ƒå¤æ‚åº¦å¢åŠ çš„é«˜ç»´åº¦çˆ†å‘æœŸä¹‹é—´å¾ªç¯åˆ‡æ¢ã€‚æœ€ç»ˆï¼Œè¿™é¡¹å·¥ä½œæå‡ºå±‚åŒ–æ½œç©ºé—´ä¸­çš„ç»´åº¦åˆ†å¸ƒå¯ä½œä¸ºRLæ¸¸æˆå¤æ‚æ€§çš„æ–°å‡ ä½•æŒ‡æ ‡ï¼Œä¸ºåˆ†ææ¨¡å‹å†³ç­–è¿‡ç¨‹æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "math.AT",
        "cs.AI",
        "cs.CG",
        "cs.LG",
        "math.DG"
      ],
      "primary_category": "math.AT",
      "comment": "17 pages and 8 figures. Preliminary report. Feedback welcome!",
      "pdf_url": "https://arxiv.org/pdf/2507.22010v1",
      "published_date": "2025-07-29 17:00:33 UTC",
      "updated_date": "2025-07-29 17:00:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:42.786281+00:00"
    },
    {
      "arxiv_id": "2507.22009v1",
      "title": "PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences",
      "title_zh": "PHAXï¼šé¢å‘å…¬å…±å«ç”Ÿä¸ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„ç»“æ„åŒ–è®ºè¯æ¡†æ¶",
      "authors": [
        "Bahar Ä°lgen",
        "Akshat Dubey",
        "Georges Hattab"
      ],
      "abstract": "Ensuring transparency and trust in AI-driven public health and biomedical sciences systems requires more than accurate predictions-it demands explanations that are clear, contextual, and socially accountable. While explainable AI (XAI) has advanced in areas like feature attribution and model interpretability, most methods still lack the structure and adaptability needed for diverse health stakeholders, including clinicians, policymakers, and the general public. We introduce PHAX-a Public Health Argumentation and eXplainability framework-that leverages structured argumentation to generate human-centered explanations for AI outputs. PHAX is a multi-layer architecture combining defeasible reasoning, adaptive natural language techniques, and user modeling to produce context-aware, audience-specific justifications. More specifically, we show how argumentation enhances explainability by supporting AI-driven decision-making, justifying recommendations, and enabling interactive dialogues across user types. We demonstrate the applicability of PHAX through use cases such as medical term simplification, patient-clinician communication, and policy justification. In particular, we show how simplification decisions can be modeled as argument chains and personalized based on user expertise-enhancing both interpretability and trust. By aligning formal reasoning methods with communicative demands, PHAX contributes to a broader vision of transparent, human-centered AI in public health.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PHAXï¼Œä¸€ä¸ªä¸“ä¸ºå…¬å…±å«ç”Ÿå’Œç”Ÿç‰©åŒ»å­¦ç§‘å­¦è®¾è®¡çš„å…¬å…±å«ç”Ÿè®ºè¯ä¸å¯è§£é‡Šæ€§æ¡†æ¶ (Public Health Argumentation and eXplainability framework)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) ç¼ºä¹ç»“æ„æ€§å’Œé€‚åº”æ€§çš„é—®é¢˜ã€‚PHAX é‡‡ç”¨å¤šå±‚æ¶æ„ï¼Œç»“åˆäº†å¯é©³å›æ¨ç† (defeasible reasoning)ã€è‡ªé€‚åº”è‡ªç„¶è¯­è¨€æŠ€æœ¯ (adaptive natural language techniques) ä»¥åŠç”¨æˆ·å»ºæ¨¡ (user modeling)ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ä¸”é’ˆå¯¹ç‰¹å®šå—ä¼—çš„è¾©æŠ¤è¯´æ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”¯æŒ AI é©±åŠ¨çš„å†³ç­–åˆ¶å®šå’Œå»ºè®®è¾©æŠ¤ï¼Œå®ç°äº†è·¨ç”¨æˆ·ç±»å‹çš„äº’åŠ¨å¼å¯¹è¯ã€‚ç ”ç©¶é€šè¿‡åŒ»å­¦æœ¯è¯­ç®€åŒ–ã€åŒ»æ‚£æ²Ÿé€šå’Œæ”¿ç­–è¾©æŠ¤ç­‰ç”¨ä¾‹å±•ç¤ºäº†å…¶é€‚ç”¨æ€§ï¼Œå¹¶å°†ç®€åŒ–å†³ç­–å»ºæ¨¡ä¸ºè®ºè¯é“¾ (argument chains) ä»¥å®ç°ä¸ªæ€§åŒ–è§£é‡Šã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¯¹é½å½¢å¼åŒ–æ¨ç†æ–¹æ³•ä¸æ²Ÿé€šéœ€æ±‚ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿåœ¨ä¸åŒåˆ©ç›Šç›¸å…³è€…ä¸­çš„å¯è§£é‡Šæ€§ (interpretability) ä¸ä¿¡ä»»æ„Ÿã€‚è¯¥æˆæœä¸ºæ„å»ºå…¬å…±å«ç”Ÿé¢†åŸŸä¸­ä»¥äººä¸ºæœ¬ã€é€æ˜ä¸”ç¤¾ä¼šè´Ÿè´£çš„ AI ç³»ç»Ÿæä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Under review",
      "pdf_url": "https://arxiv.org/pdf/2507.22009v1",
      "published_date": "2025-07-29 17:00:15 UTC",
      "updated_date": "2025-07-29 17:00:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:48.994173+00:00"
    },
    {
      "arxiv_id": "2507.22002v2",
      "title": "Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation",
      "title_zh": "è¡”æ¥åˆæˆåŸŸä¸çœŸå®åŸŸï¼šé¢å‘å·¥ä¸šæœ‰æ¯’æ’æ”¾åˆ†å‰²çš„äººåœ¨å›è·¯å¼±ç›‘ç£æ¡†æ¶",
      "authors": [
        "Yida Tao",
        "Yen-Chia Hsu"
      ],
      "abstract": "Industrial smoke segmentation is critical for air-quality monitoring and environmental protection but is often hampered by the high cost and scarcity of pixel-level annotations in real-world settings. We introduce CEDANet, a human-in-the-loop, class-aware domain adaptation framework that uniquely integrates weak, citizen-provided video-level labels with adversarial feature alignment. Specifically, we refine pseudo-labels generated by a source-trained segmentation model using citizen votes, and employ class-specific domain discriminators to transfer rich source-domain representations to the industrial domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of 0.261 with citizen feedback, vastly outperforming the baseline model, which scored 0.083 and 0.043 respectively. This represents a five-fold increase in F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with citizen-constrained pseudo-labels achieves performance comparable to the same architecture trained on limited 100 fully annotated images with F1-score of 0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully supervised-level accuracy without target-domain annotations. Our research validates the scalability and cost-efficiency of combining citizen science with weakly supervised domain adaptation, offering a practical solution for complex, data-scarce environmental monitoring applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CEDANetï¼Œè¿™æ˜¯ä¸€ä¸ªäººç±»åœ¨ç¯(human-in-the-loop)çš„ç±»åˆ«æ„ŸçŸ¥é¢†åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šçƒŸé›¾åˆ†å‰²ä¸­åƒç´ çº§æ ‡æ³¨åŒ®ä¹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†å…¬æ°‘æä¾›çš„å¼±è§†é¢‘çº§æ ‡ç­¾(video-level labels)ä¸å¯¹æŠ—æ€§ç‰¹å¾å¯¹é½(adversarial feature alignment)ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨å…¬æ°‘æŠ•ç¥¨ä¼˜åŒ–åˆ†å‰²æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾(pseudo-labels)ã€‚é€šè¿‡é‡‡ç”¨ç±»åˆ«ç‰¹å®šçš„é¢†åŸŸåˆ¤åˆ«å™¨(domain discriminators)ï¼ŒCEDANetèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ä¸°å¯Œçš„æºåŸŸè¡¨ç¤ºè¿ç§»è‡³å·¥ä¸šé¢†åŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨SMOKE5Kå’Œè‡ªå®šä¹‰IJmondæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶çš„F1åˆ†æ•°è¾¾åˆ°0.414ï¼ŒçƒŸé›¾ç±»IoUè¾¾åˆ°0.261ï¼Œæ€§èƒ½è¾ƒåŸºçº¿æ¨¡å‹æå‡äº†äº”åˆ°å…­å€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨æ— ç›®æ ‡åŸŸæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸100å¼ å…¨æ ‡æ³¨å›¾åƒè®­ç»ƒçš„å®Œå…¨ç›‘ç£æ¨¡å‹ç›¸å½“çš„ç²¾åº¦ã€‚è¯¥ç ”ç©¶éªŒè¯äº†å°†å…¬æ°‘ç§‘å­¦(citizen science)ä¸å¼±ç›‘ç£é¢†åŸŸè‡ªé€‚åº”(weakly supervised domain adaptation)ç»“åˆçš„å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œä¸ºæ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒç›‘æµ‹æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22002v2",
      "published_date": "2025-07-29 16:53:00 UTC",
      "updated_date": "2025-11-12 03:05:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:49.502097+00:00"
    },
    {
      "arxiv_id": "2507.22000v1",
      "title": "Staining and locking computer vision models without retraining",
      "title_zh": "æ— éœ€é‡æ–°è®­ç»ƒçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æŸ“è‰²ä¸é”å®š",
      "authors": [
        "Oliver J. Sutton",
        "Qinghua Zhou",
        "George Leete",
        "Alexander N. Gorban",
        "Ivan Y. Tyukin"
      ],
      "abstract": "We introduce new methods of staining and locking computer vision models, to protect their owners' intellectual property. Staining, also known as watermarking, embeds secret behaviour into a model which can later be used to identify it, while locking aims to make a model unusable unless a secret trigger is inserted into input images. Unlike existing methods, our algorithms can be used to stain and lock pre-trained models without requiring fine-tuning or retraining, and come with provable, computable guarantees bounding their worst-case false positive rates. The stain and lock are implemented by directly modifying a small number of the model's weights and have minimal impact on the (unlocked) model's performance. Locked models are unlocked by inserting a small `trigger patch' into the corner of the input image. We present experimental results showing the efficacy of our methods and demonstrating their practical performance on a variety of computer vision models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸¤ç§ä¿æŠ¤è®¡ç®—æœºè§†è§‰æ¨¡å‹(computer vision models)çŸ¥è¯†äº§æƒçš„æ–°æ–¹æ³•ï¼šæŸ“è‰²(staining)å’Œé”å®š(locking)ã€‚æŸ“è‰²å³åœ¨æ¨¡å‹ä¸­åµŒå…¥ç”¨äºåæœŸè¯†åˆ«çš„ç§˜å¯†è¡Œä¸º(watermarking)ï¼Œè€Œé”å®šæ—¨åœ¨ä½¿æ¨¡å‹åœ¨ç¼ºä¹ç‰¹å®šè§¦å‘ä¿¡å·æ—¶å¤„äºä¸å¯ç”¨çŠ¶æ€ã€‚ä¸ç°æœ‰æŠ€æœ¯ä¸åŒï¼Œè¿™äº›ç®—æ³•æ— éœ€å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒ(retraining)æˆ–å¾®è°ƒ(fine-tuning)ï¼Œè€Œæ˜¯é€šè¿‡ç›´æ¥ä¿®æ”¹æå°‘é‡æ¨¡å‹æƒé‡æ¥å®ç°ã€‚è¯¥æ–¹æ³•å…·æœ‰å¯è¯æ˜ä¸”å¯è®¡ç®—çš„æœ€åæƒ…å†µè¯¯æŠ¥ç‡(false positive rates)ä¿è¯ï¼Œä¸”å¯¹æ¨¡å‹åœ¨è§£é”çŠ¶æ€ä¸‹çš„åŸå§‹æ€§èƒ½å½±å“å¾®ä¹å…¶å¾®ã€‚é”å®šåçš„æ¨¡å‹å¯ä»¥é€šè¿‡åœ¨è¾“å…¥å›¾åƒè§’è½æ’å…¥ä¸€ä¸ªå°çš„è§¦å‘è¡¥ä¸(trigger patch)æ¥æ¢å¤æ­£å¸¸é¢„æµ‹ã€‚å®éªŒç»“æœåœ¨å¤šç§è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠåœ¨ä¿æŠ¤æ¨¡å‹æ‰€æœ‰æƒæ–¹é¢çš„å®ç”¨æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 9 pages of appendices, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22000v1",
      "published_date": "2025-07-29 16:47:34 UTC",
      "updated_date": "2025-07-29 16:47:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:26:50.751123+00:00"
    },
    {
      "arxiv_id": "2507.21992v1",
      "title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation",
      "title_zh": "æ•™æˆ‘å¦‚ä½•æ¬ºéª—ï¼šé€šè¿‡çŸ¥è¯†è’¸é¦æ¢ç´¢å¯¹æŠ—è¿ç§»æ€§",
      "authors": [
        "Siddhartha Pradhan",
        "Shikshya Shiwakoti",
        "Neha Bathuri"
      ],
      "abstract": "We investigate whether knowledge distillation (KD) from multiple heterogeneous teacher models can enhance the generation of transferable adversarial examples. A lightweight student model is trained using two KD strategies: curriculum-based switching and joint optimization, with ResNet50 and DenseNet-161 as teachers. The trained student is then used to generate adversarial examples using FG, FGS, and PGD attacks, which are evaluated against a black-box target model (GoogLeNet). Our results show that student models distilled from multiple teachers achieve attack success rates comparable to ensemble-based baselines, while reducing adversarial example generation time by up to a factor of six. An ablation study further reveals that lower temperature settings and the inclusion of hard-label supervision significantly enhance transferability. These findings suggest that KD can serve not only as a model compression technique but also as a powerful tool for improving the efficiency and effectiveness of black-box adversarial attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ä»å¤šä¸ªå¼‚æ„æ•™å¸ˆæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦(Knowledge Distillation, KD)æ¥å¢å¼ºå¯è¿ç§»å¯¹æŠ—æ ·æœ¬(transferable adversarial examples)çš„ç”Ÿæˆã€‚ä½œè€…åˆ©ç”¨ResNet50å’ŒDenseNet-161ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œé‡‡ç”¨è¯¾ç¨‹åˆ‡æ¢(curriculum-based switching)å’Œè”åˆä¼˜åŒ–(joint optimization)ä¸¤ç§ç­–ç•¥è®­ç»ƒè½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚éšååˆ©ç”¨è®­ç»ƒå¥½çš„å­¦ç”Ÿæ¨¡å‹æ‰§è¡ŒFGã€FGSå’ŒPGDæ”»å‡»ï¼Œå¹¶åœ¨é»‘ç›’ç›®æ ‡æ¨¡å‹GoogLeNetä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»å¤šæ•™å¸ˆæ¨¡å‹è’¸é¦å‡ºçš„å­¦ç”Ÿæ¨¡å‹åœ¨æ”»å‡»æˆåŠŸç‡ä¸Šä¸åŸºäºé›†æˆ(ensemble-based)çš„åŸºå‡†æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å°†å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ—¶é—´ç¼©çŸ­äº†å¤šè¾¾å…­å€ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥æ­ç¤ºï¼Œè¾ƒä½çš„æ¸©åº¦è®¾ç½®å’Œç¡¬æ ‡ç­¾ç›‘ç£(hard-label supervision)çš„å¼•å…¥èƒ½å¤Ÿæ˜¾è‘—æå‡æ ·æœ¬çš„å¯è¿ç§»æ€§ã€‚è¯¥ç ”ç©¶è¡¨æ˜çŸ¥è¯†è’¸é¦ä¸ä»…æ˜¯æ¨¡å‹å‹ç¼©çš„æ‰‹æ®µï¼Œä¹Ÿæ˜¯æé«˜é»‘ç›’å¯¹æŠ—æ”»å‡»æ•ˆç‡å’Œæœ‰æ•ˆæ€§çš„æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21992v1",
      "published_date": "2025-07-29 16:43:54 UTC",
      "updated_date": "2025-07-29 16:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:01.739566+00:00"
    },
    {
      "arxiv_id": "2507.21990v3",
      "title": "ChemDFM-R: A Chemical Reasoning LLM Enhanced with Atomized Chemical Knowledge",
      "title_zh": "ChemDFM-Rï¼šåŸå­åŒ–åŒ–å­¦çŸ¥è¯†å¢å¼ºçš„åŒ–å­¦æ¨ç†å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zihan Zhao",
        "Bo Chen",
        "Ziping Wan",
        "Lu Chen",
        "Xuanze Lin",
        "Shiyang Yu",
        "Situo Zhang",
        "Da Ma",
        "Zichen Zhu",
        "Danyang Zhang",
        "Huayang Wang",
        "Zhongyang Dai",
        "Liyang Wen",
        "Xin Chen",
        "Kai Yu"
      ],
      "abstract": "While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoning LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized chemical knowledge, ChemFG, annotating the presence of functional groups in molecules and the changes of functional groups during chemical reactions, to enhance the model's understanding of the fundamental principles and internal logic of chemistry. Then, we propose a mixed-source distillation method that integrates expertise in atomized knowledge with general reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves cutting-edge performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the model's reliability, transparency, and practicality in real-world human-AI collaboration scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ–å­¦é¢†åŸŸç”±äºé¢†åŸŸç†è§£æµ…è–„å’Œæ¨ç†èƒ½åŠ›æœ‰é™è€Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¼€å‘äº†åä¸ºChemDFM-Rçš„åŒ–å­¦æ¨ç†æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŸå­åŒ–åŒ–å­¦çŸ¥è¯†çš„å…¨é¢æ•°æ®é›†ChemFGï¼Œé€šè¿‡æ ‡æ³¨åˆ†å­ä¸­çš„å®˜èƒ½å›¢(functional groups)åŠå…¶åœ¨åŒ–å­¦ååº”ä¸­çš„å˜åŒ–ï¼Œå¢å¼ºæ¨¡å‹å¯¹åŒ–å­¦åŸºæœ¬åŸç†å’Œé€»è¾‘çš„ç†è§£ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ··åˆæºè’¸é¦(mixed-source distillation)æ–¹æ³•ï¼Œå°†ä¸“ä¸šåŒ–çš„åŸå­çŸ¥è¯†ä¸é€šç”¨æ¨ç†æŠ€èƒ½ç›¸èåˆï¼Œå¹¶é€šè¿‡é¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ (reinforcement learning)è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChemDFM-Råœ¨å¤šä¸ªåŒ–å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å‰æ²¿(cutting-edge)æ€§èƒ½ï¼Œå¹¶èƒ½æä¾›å…·æœ‰è§£é‡Šæ€§çš„é€»è¾‘è¾“å‡ºã€‚æœ€åï¼Œè¯¥æ¨¡å‹é€šè¿‡æ˜¾å¼æ¨ç†é“¾æ˜¾è‘—æå‡äº†åœ¨çœŸå®äººæœºåä½œåœºæ™¯ä¸­çš„å¯é æ€§ã€é€æ˜åº¦å’Œå®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "18 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.21990v3",
      "published_date": "2025-07-29 16:40:49 UTC",
      "updated_date": "2025-12-17 05:06:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:01.542028+00:00"
    },
    {
      "arxiv_id": "2507.21976v3",
      "title": "Compression Strategies for Efficient Multimodal LLMs in Medical Contexts",
      "title_zh": "åŒ»ç–—é¢†åŸŸé«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‹ç¼©ç­–ç•¥",
      "authors": [
        "Tanvir A. Khan",
        "Aranya Saha",
        "Ismam N. Swapnil",
        "Mohammad A. Haque"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) hold huge potential for usage in the medical domain, but their computational costs necessitate efficient compression techniques. This paper evaluates the impact of structural pruning and activation-aware quantization on a fine-tuned LLAVA model for medical applications. We propose a novel layer selection method for pruning, analyze different quantization techniques, and assess the performance trade-offs in a prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B parameters to run within 4 GB of VRAM, reducing memory usage by 70% while achieving 4% higher model performance compared to traditional pruning and quantization techniques in the same compression ratio.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—é¢†åŸŸ Multimodal Large Language Models (MLLMs) è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œé‡ç‚¹è¯„ä¼°äº† structural pruning å’Œ activation-aware quantization åœ¨å¾®è°ƒ LLAVA æ¨¡å‹ä¸­çš„åº”ç”¨æ•ˆæœã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”¨äºå‰ªæçš„ layer selection methodï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†ä¸åŒ quantization æŠ€æœ¯çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶æ·±å…¥è¯„ä¼°äº† prune-SFT-quantize pipeline ä¸­çš„å…³é”®æƒè¡¡ã€‚è¯¥æ–¹æ³•ä½¿å¾— 7B å‚æ•°è§„æ¨¡çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»… 4 GB çš„ VRAM ä¸­é«˜æ•ˆè¿è¡Œï¼ŒæˆåŠŸå°†å†…å­˜å ç”¨é™ä½äº† 70%ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨ç›¸åŒå‹ç¼©æ¯”ä¸‹ï¼Œè¯¥ç­–ç•¥æ¯”ä¼ ç»Ÿçš„ pruning å’Œ quantization æŠ€æœ¯åœ¨æ€§èƒ½ä¸Šé«˜å‡º 4%ã€‚è¯¥æˆæœä¸ºèµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒä¸‹éƒ¨ç½²é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„å¤šæ¨¡æ€å¤§æ¨¡å‹æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21976v3",
      "published_date": "2025-07-29 16:25:51 UTC",
      "updated_date": "2025-09-23 19:50:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:05.345829+00:00"
    },
    {
      "arxiv_id": "2507.21974v1",
      "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks",
      "title_zh": "é¢å‘ 5G æ— çº¿ç½‘ç»œæ ¹å› åˆ†æçš„æ¨ç†è¯­è¨€æ¨¡å‹",
      "authors": [
        "Mohamed Sana",
        "Nicola Piovesan",
        "Antonio De Domenico",
        "Yibin Kang",
        "Haozhe Zhang",
        "Merouane Debbah",
        "Fadhel Ayed"
      ],
      "abstract": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹5Gæ— çº¿ç½‘ç»œä¸­æ ¹å› åˆ†æï¼ˆRoot Cause Analysis, RCAï¼‰å¯¹å¯è§£é‡Šæ€§ã€é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œå› æœæ¨ç†çš„é«˜è¦æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è½»é‡çº§æ¡†æ¶ã€‚ä¸ºäº†åŸºå‡†åŒ–è¯„ä¼°RCAèƒ½åŠ›ï¼Œä½œè€…å¼•å…¥äº†TeleLogsæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç»è¿‡æ ‡æ³¨çš„æ•…éšœæ’é™¤é—®é¢˜ã€‚ç ”ç©¶å‘ç°ç°æœ‰å¼€æºæ¨ç†LLMsåœ¨å¤„ç†æ­¤ç±»é—®é¢˜æ—¶å­˜åœ¨å›°éš¾ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨å°†é¢†åŸŸçŸ¥è¯†é›†æˆåˆ°æ¨¡å‹ä¸­ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„å¤šæ­¥éª¤è¯Šæ–­è§£é‡Šï¼Œä»è€Œæ˜¾è‘—æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ¨ç†ä¸éæ¨ç†æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»è¿‡é¢†åŸŸé€‚é…å’Œæ¨ç†å¢å¼ºçš„LLMsåœ¨å®ç°è‡ªåŠ¨åŒ–ã€å¯è§£é‡Šçš„ç½‘ç»œè¿è¥ç®¡ç†æ ¹å› åˆ†ææ–¹é¢å…·æœ‰å¹¿é˜”çš„å‰æ™¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21974v1",
      "published_date": "2025-07-29 16:21:42 UTC",
      "updated_date": "2025-07-29 16:21:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:07.384912+00:00"
    },
    {
      "arxiv_id": "2507.21964v1",
      "title": "Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities",
      "title_zh": "æ— éœ€æç¤ºï¼šåŸºäºä¼ æ„Ÿå™¨æ•°æ®ä¸æ´»åŠ¨è¯­è¨€å»ºæ¨¡çš„æ™ºèƒ½å®¶å±…é›¶æ ·æœ¬äººä½“æ´»åŠ¨è¯†åˆ«",
      "authors": [
        "Sourish Gunesh Dhekane",
        "Thomas Ploetz"
      ],
      "abstract": "Developing zero-shot human activity recognition (HAR) methods is a critical direction in smart home research -- considering its impact on making HAR systems work across smart homes having diverse sensing modalities, layouts, and activities of interest. The state-of-the-art solutions along this direction are based on generating natural language descriptions of the sensor data and feeding it via a carefully crafted prompt to the LLM to perform classification. Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry several risks, including privacy invasion, reliance on an external service, and inconsistent predictions due to version changes, making a case for alternative zero-shot HAR methods that do not require prompting the LLMs. In this paper, we propose one such solution that models sensor data and activities using natural language, leveraging its embeddings to perform zero-shot classification and thereby bypassing the need to prompt the LLMs for activity predictions. The impact of our work lies in presenting a detailed case study on six datasets, highlighting how language modeling can bolster HAR systems in zero-shot recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½å®¶å±…ä¸­é›¶æ ·æœ¬äººç±»æ´»åŠ¨è¯†åˆ«(Zero-Shot Human Activity Recognition, HAR)çš„å…³é”®æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç›®å‰ä¸»æµçš„æç¤ºå¤§è¯­è¨€æ¨¡å‹(Prompt-the-LLM)æ–¹æ³•å­˜åœ¨éšç§æ³„éœ²ã€å¤–éƒ¨æœåŠ¡ä¾èµ–å’Œé¢„æµ‹ä¸ä¸€è‡´ç­‰é£é™©ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è¯­è¨€æ¨¡å‹(Language Modeling)å¯¹ä¼ æ„Ÿå™¨æ•°æ®å’Œæ´»åŠ¨è¿›è¡Œå»ºæ¨¡çš„æ–°æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æå–è‡ªç„¶è¯­è¨€åµŒå…¥(Embeddings)æ¥è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ï¼Œä»è€Œå®Œå…¨ç»•è¿‡äº†å‘å¤§è¯­è¨€æ¨¡å‹å‘é€æç¤º(Prompting)çš„éœ€æ±‚ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„è¯¦ç»†æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§åŸºäºè¯­è¨€å»ºæ¨¡çš„æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºHARç³»ç»Ÿåœ¨é›¶æ ·æœ¬è¯†åˆ«åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæ„å»ºä¸ä¾èµ–å¤–éƒ¨æœåŠ¡ã€æ›´å…·éšç§ä¿æŠ¤æ€§çš„æ™ºèƒ½å®¶å±…æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21964v1",
      "published_date": "2025-07-29 16:13:10 UTC",
      "updated_date": "2025-07-29 16:13:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:12.050610+00:00"
    },
    {
      "arxiv_id": "2507.21954v1",
      "title": "Fine-Tuning Code Language Models to Detect Cross-Language Bugs",
      "title_zh": "å¾®è°ƒä»£ç è¯­è¨€æ¨¡å‹ä»¥æ£€æµ‹è·¨è¯­è¨€ç¼ºé™·",
      "authors": [
        "Zengyang Li",
        "Yimeng Li",
        "Binbin Huang",
        "Peng Liang",
        "Ran Mo",
        "Hui Liu",
        "Yutao Ma"
      ],
      "abstract": "Multilingual programming, which involves using multiple programming languages (PLs) in a single project, is increasingly common due to its benefits. However, it introduces cross-language bugs (CLBs), which arise from interactions between different PLs and are difficult to detect by single-language bug detection tools. This paper investigates the potential of pre-trained code language models (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language code identification tool, and constructed a CLB dataset involving three PL combinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction types. We fine-tuned 13 CodeLMs on this dataset and evaluated their performance, analyzing the effects of dataset size, token sequence length, and code comments. Results show that all CodeLMs performed poorly before fine-tuning, but exhibited varying degrees of performance improvement after fine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably, small fine-tuned CodeLMs tended to performe better than large ones. CodeLMs fine-tuned on single-language bug datasets performed poorly on CLB detection, demonstrating the distinction between CLBs and single-language bugs. Additionally, increasing the fine-tuning dataset size significantly improved performance, while longer token sequences did not necessarily improve the model performance. The impact of code comments varied across models. Some fine-tuned CodeLMs' performance was improved, while others showed degraded performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒä»£ç è¯­è¨€æ¨¡å‹ (CodeLMs) åœ¨æ£€æµ‹è·¨è¯­è¨€ç¼ºé™· (Cross-Language Bugs, CLBs) æ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åº”å¯¹å¤šè¯­è¨€ç¼–ç¨‹ä¸­ç”±äºä¸åŒè¯­è¨€äº¤äº’äº§ç”Ÿçš„æ£€æµ‹éš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†è·¨è¯­è¨€ä»£ç è¯†åˆ«å·¥å…· CLCFinderï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ¶µç›– Python-C/C++ã€Java-C/C++ å’Œ Python-Java ä¸‰ç§è¯­è¨€ç»„åˆåŠä¹ç§äº¤äº’ç±»å‹çš„ CLB æ•°æ®é›†ã€‚é€šè¿‡å¯¹ 13 ç§ CodeLMs è¿›è¡Œå¾®è°ƒè¯„ä¼°ï¼Œå®éªŒå‘ç° UniXcoder-base å–å¾—äº†æœ€é«˜çš„ 0.7407 F1 åˆ†æ•°ï¼Œä¸”å¾®è°ƒåçš„å°å‹æ¨¡å‹è¡¨ç°å¾€å¾€ä¼˜äºå¤§å‹æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº† CLBs ä¸å•è¯­è¨€ç¼ºé™·å­˜åœ¨æ˜¾è‘—åŒºåˆ«ï¼Œåœ¨å•è¯­è¨€æ•°æ®é›†ä¸Šå¾®è°ƒçš„æ¨¡å‹æ— æ³•æœ‰æ•ˆæ£€æµ‹ CLBsã€‚å®éªŒç»“æœè¿˜è¡¨æ˜ï¼Œå¢åŠ å¾®è°ƒæ•°æ®é›†è§„æ¨¡å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼Œè€Œå¢åŠ åºåˆ—é•¿åº¦å¹¶ä¸ä¸€å®šèƒ½æ”¹å–„æ•ˆæœï¼Œä»£ç æ³¨é‡Šçš„å½±å“åˆ™å› æ¨¡å‹è€Œå¼‚ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "33 pages, 6 images, 9 tables, Manuscript submitted to a journal (2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.21954v1",
      "published_date": "2025-07-29 16:06:08 UTC",
      "updated_date": "2025-07-29 16:06:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:26.449195+00:00"
    },
    {
      "arxiv_id": "2507.21953v1",
      "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation",
      "title_zh": "MapAgentï¼šåŸºäºè½¨è¿¹æ„å»ºä¸è®°å¿†å¢å¼ºè§„åˆ’çš„ç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–",
      "authors": [
        "Yi Kong",
        "Dianxi Shi",
        "Guoli Yang",
        "Zhang ke-di",
        "Chenlin Huang",
        "Xiaopeng Li",
        "Songchang Jin"
      ],
      "abstract": "The recent advancement of autonomous agents powered by Large Language Models (LLMs) has demonstrated significant potential for automating tasks on mobile devices through graphical user interfaces (GUIs). Despite initial progress, these agents still face challenges when handling complex real-world tasks. These challenges arise from a lack of knowledge about real-life mobile applications in LLM-based agents, which may lead to ineffective task planning and even cause hallucinations. To address these challenges, we propose a novel LLM-based agent framework called MapAgent that leverages memory constructed from historical trajectories to augment current task planning. Specifically, we first propose a trajectory-based memory mechanism that transforms task execution trajectories into a reusable and structured page-memory database. Each page within a trajectory is extracted as a compact yet comprehensive snapshot, capturing both its UI layout and functional context. Secondly, we introduce a coarse-to-fine task planning approach that retrieves relevant pages from the memory database based on similarity and injects them into the LLM planner to compensate for potential deficiencies in understanding real-world app scenarios, thereby achieving more informed and context-aware task planning. Finally, planned tasks are transformed into executable actions through a task executor supported by a dual-LLM architecture, ensuring effective tracking of task progress. Experimental results in real-world scenarios demonstrate that MapAgent achieves superior performance to existing methods. The code will be open-sourced to support further research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MapAgentï¼Œä¸€ç§é€šè¿‡è½¨è¿¹æ„å»ºè®°å¿†å¢å¼ºè§„åˆ’èƒ½åŠ›çš„ç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤æ‚ç°å®ä»»åŠ¡æ—¶å› ç¼ºä¹å…ˆéªŒçŸ¥è¯†è€Œäº§ç”Ÿçš„è§„åˆ’å¤±æ•ˆä¸å¹»è§‰é—®é¢˜ã€‚MapAgenté¦–å…ˆé€šè¿‡Trajectory-based memoryæœºåˆ¶å°†å†å²æ‰§è¡Œè½¨è¿¹è½¬åŒ–ä¸ºç»“æ„åŒ–çš„é¡µé¢è®°å¿†æ•°æ®åº“ï¼Œç²¾ç»†æ•æ‰UIå¸ƒå±€ä¸åŠŸèƒ½ä¸Šä¸‹æ–‡ã€‚éšåï¼Œè¯¥æ¡†æ¶é‡‡ç”¨Coarse-to-fineè§„åˆ’æ–¹æ³•ï¼ŒåŸºäºç›¸ä¼¼æ€§æ£€ç´¢ç›¸å…³é¡µé¢ä»¥å¢å¼ºæ™ºèƒ½ä½“å¯¹åº”ç”¨åœºæ™¯çš„ç†è§£ã€‚æœ€ç»ˆï¼Œä»»åŠ¡é€šè¿‡Dual-LLM architectureæ‰§è¡Œå™¨è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œå¹¶ç¡®ä¿è¿›åº¦è·Ÿè¸ªã€‚å®éªŒè¯æ˜ï¼ŒMapAgentåœ¨çœŸå®åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç§»åŠ¨ç«¯ä»»åŠ¡è‡ªåŠ¨åŒ–çš„å‡†ç¡®æ€§ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21953v1",
      "published_date": "2025-07-29 16:05:32 UTC",
      "updated_date": "2025-07-29 16:05:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:39.738202+00:00"
    },
    {
      "arxiv_id": "2507.21949v2",
      "title": "Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal",
      "title_zh": "åŸºäºå¯¹æ¯”åº¦å…ˆéªŒå¢å¼ºå¯¹å¶æ€§çš„æ— æ©ç é˜´å½±å»é™¤",
      "authors": [
        "Jiyu Wu",
        "Yifan Liu",
        "Jiancheng Huang",
        "Mingfu Yan",
        "Shifeng Chen"
      ],
      "abstract": "Existing shadow removal methods often rely on shadow masks, which are challenging to acquire in real-world scenarios. Exploring intrinsic image cues, such as local contrast information, presents a potential alternative for guiding shadow removal in the absence of explicit masks. However, the cue's inherent ambiguity becomes a critical limitation in complex scenes, where it can fail to distinguish true shadows from low-reflectance objects and intricate background textures. To address this motivation, we propose the Adaptive Gated Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs the contrast prior to effectively disentangle shadow features from confounding visual elements. Furthermore, to tackle the persistent challenge of restoring soft shadow boundaries and fine-grained details, we introduce a diffusion-based Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and contrast cues to guide the generative process. Extensive experiments demonstrate that our method achieves state-of-the-art results among mask-free approaches while maintaining competitive performance relative to mask-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— æ©ç é˜´å½±å»é™¤(Mask-Free Shadow Removal)ä¸­ç”±äºå±€éƒ¨å¯¹æ¯”åº¦å…ˆéªŒçš„æ¨¡ç³Šæ€§å¯¼è‡´éš¾ä»¥åŒºåˆ†çœŸå®é˜´å½±ä¸ä½åå°„ç‡ç‰©ä½“çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”é—¨æ§åŒåˆ†æ”¯æ³¨æ„åŠ›(Adaptive Gated Dual-Branch Attention, AGBA)æœºåˆ¶ã€‚AGBAé€šè¿‡åŠ¨æ€è¿‡æ»¤å¹¶é‡æ–°åŠ æƒå¯¹æ¯”åº¦å…ˆéªŒï¼Œæœ‰æ•ˆåœ°å°†é˜´å½±ç‰¹å¾ä¸æ··æ·†çš„è§†è§‰å…ƒç´ åˆ†ç¦»ï¼Œè§£å†³äº†å¤æ‚åœºæ™¯ä¸‹çš„ç‰¹å¾è§£è€¦éš¾é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†æå‡è½¯é˜´å½±è¾¹ç•Œå’Œç»†ç²’åº¦ç»†èŠ‚çš„æ¢å¤è´¨é‡ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæ‰©æ•£(diffusion-based)çš„é¢‘ç‡-å¯¹æ¯”åº¦èåˆç½‘ç»œ(Frequency-Contrast Fusion Network, FCFN)ã€‚FCFNåˆ©ç”¨é«˜é¢‘å’Œå¯¹æ¯”åº¦çº¿ç´¢å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œæ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆç»†èŠ‚çš„çœŸå®æ„Ÿã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— æ©ç æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½ï¼Œä¸”åœ¨å®é™…è¡¨ç°ä¸Šå¯ä¸ä¼ ç»Ÿçš„åŸºäºæ©ç çš„æ–¹æ³•ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "There are unresolved authorship disputes related to this submission, and the current version does not reflect an agreed authorship list",
      "pdf_url": "https://arxiv.org/pdf/2507.21949v2",
      "published_date": "2025-07-29 16:00:42 UTC",
      "updated_date": "2025-11-26 02:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:33.540872+00:00"
    },
    {
      "arxiv_id": "2507.21947v1",
      "title": "Enhancing Generalization in Data-free Quantization via Mixup-class Prompting",
      "title_zh": "é€šè¿‡ Mixup-class Prompting å¢å¼ºæ— æ•°æ®é‡åŒ–çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Jiwoong Park",
        "Chaeun Lee",
        "Yongseok Choi",
        "Sein Park",
        "Deokki Hong",
        "Jungwook Choi"
      ],
      "abstract": "Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Post-training quantization (PTQ) åœ¨éšç§é™åˆ¶ä¸‹ç¼ºä¹æ ¡å‡†æ•°æ®çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„ Data-free quantization (DFQ) æ–¹æ³•åœ¨åˆ©ç”¨ç”Ÿæˆæ¨¡å‹åˆæˆæ•°æ®æ—¶ï¼Œå› é‡‡ç”¨ single-class prompts è€Œå­˜åœ¨å¤šä¹‰æ€§ç¼ºé™·ï¼Œé™åˆ¶äº†é‡åŒ–æ¨¡å‹çš„ Generalization èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Mixup-class promptï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ–‡æœ¬æç¤ºå±‚é¢èåˆå¤šä¸ªç±»åˆ«æ ‡ç­¾çš„ç­–ç•¥ï¼Œæ—¨åœ¨ç”Ÿæˆæ›´å¤šæ ·ä¸”ç¨³å¥çš„åˆæˆæ•°æ®ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–è¡¨ç°å¹¶æå‡ PTQ çš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚ç ”ç©¶é€šè¿‡ gradient norm å’Œ generalization error åˆ†ææä¾›äº†å®šé‡çš„ç†è®ºæ´å¯Ÿï¼Œå¹¶åœ¨ Convolutional neural networks (CNNs) ä¸ Vision transformers (ViTs) ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æŒç»­ä¼˜äº GenQ ç­‰ SOTA æ–¹æ³•ï¼Œå°¤å…¶åœ¨æä½ä½å®½çš„ W2A4 (2-bit weight, 4-bit activation) æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹å–å¾—äº†é¢†å…ˆçš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21947v1",
      "published_date": "2025-07-29 16:00:20 UTC",
      "updated_date": "2025-07-29 16:00:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:38.746646+00:00"
    },
    {
      "arxiv_id": "2508.03722v1",
      "title": "Multimodal Video Emotion Recognition with Reliable Reasoning Priors",
      "title_zh": "å¼•å…¥å¯é æ¨ç†å…ˆéªŒçš„å¤šæ¨¡æ€è§†é¢‘æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "Zhepeng Wang",
        "Yingjian Zhu",
        "Guanghao Dong",
        "Hongzhu Yi",
        "Feng Chen",
        "Xinming Wang",
        "Jun Xie"
      ],
      "abstract": "This study investigates the integration of trustworthy prior reasoning knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to generate fine-grained, modality-separable reasoning traces, which are injected as priors during the fusion stage to enrich cross-modal interactions. To mitigate the pronounced class-imbalance in multimodal emotion recognition, we introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly balances inter-class and intra-class distributions. Applied to the MER2024 benchmark, our prior-enhanced framework yields substantial performance gains, demonstrating that the reliability of MLLM-derived reasoning can be synergistically combined with the domain adaptability of lightweight fusion networks for robust, scalable emotion recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„å¯é æ¨ç†å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Gemini ç”Ÿæˆç»†ç²’åº¦çš„ã€æ¨¡æ€å¯åˆ†ç¦»çš„æ¨ç†è·¯å¾„ï¼Œå¹¶åœ¨èåˆé˜¶æ®µå°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†æ³¨å…¥ï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€äº¤äº’ã€‚ä¸ºäº†åº”å¯¹å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­ä¸¥å³»çš„ç±»åˆ«ä¸å¹³è¡¡æŒ‘æˆ˜ï¼Œè®ºæ–‡å¼•å…¥äº†å¹³è¡¡åŒå¯¹æ¯”å­¦ä¹  (Balanced Dual-Contrastive Learning) æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ååŒå¹³è¡¡ç±»é—´ä¸ç±»å†…åˆ†å¸ƒæ¥ä¼˜åŒ–æ¨¡å‹ã€‚åœ¨ MER2024 åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å¢å¼ºæ¡†æ¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™è¯æ˜äº†å°† MLLM è¡ç”Ÿçš„å¯é æ¨ç†ä¸è½»é‡çº§èåˆç½‘ç»œçš„é¢†åŸŸè‡ªé€‚åº”èƒ½åŠ›ç›¸ç»“åˆï¼Œå¯ä»¥ä¸ºæ„å»ºé²æ£’ä¸”å¯æ‰©å±•çš„æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿæä¾›æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.03722v1",
      "published_date": "2025-07-29 15:55:23 UTC",
      "updated_date": "2025-07-29 15:55:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:37.557378+00:00"
    },
    {
      "arxiv_id": "2507.21931v1",
      "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback",
      "title_zh": "åŸºäºè‡ªåé¦ˆå¼ºåŒ–å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒ",
      "authors": [
        "Carel van Niekerk",
        "Renato Vukovic",
        "Benjamin Matthias Ruppik",
        "Hsien-chin Lin",
        "Milica GaÅ¡iÄ‡"
      ],
      "abstract": "Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RLSFï¼ˆReinforcement Learning from Self-Feedbackï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­äº§ç”Ÿçš„ç­”æ¡ˆè™½ç„¶çœ‹ä¼¼åˆç†ä½†æ ¡å‡†æ€§ï¼ˆcalibrationï¼‰è¾ƒå·®çš„é—®é¢˜ã€‚RLSF æ¨¡æ‹Ÿäººç±»åœ¨ç¼ºä¹å¤–éƒ¨åé¦ˆæ—¶çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå°†æ¨¡å‹è‡ªèº«çš„ç½®ä¿¡åº¦ï¼ˆconfidenceï¼‰ä½œä¸ºå†…åœ¨å¥–åŠ±ï¼ˆintrinsic rewardï¼‰æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚åœ¨è¯¥æµç¨‹ä¸­ï¼Œå†»ç»“çš„ LLM é¦–å…ˆç”Ÿæˆå¤šä¸ª Chain-of-Thought è§£å†³æ–¹æ¡ˆï¼Œéšåé€šè¿‡è®¡ç®—å„ç­”æ¡ˆåˆ‡ç‰‡çš„ç½®ä¿¡åº¦å¯¹æ¨ç†è·¯å¾„è¿›è¡Œæ’åºå¹¶ç”Ÿæˆåˆæˆåå¥½ã€‚è¿™äº›åå¥½æ•°æ®è¢«ç”¨äºæ ‡å‡†çš„åå¥½ä¼˜åŒ–ï¼ˆpreference optimizationï¼‰å¾®è°ƒï¼Œå…¶è¿‡ç¨‹ç±»ä¼¼äº RLHF ä½†æ— éœ€ä»»ä½•äººç±»æ ‡ç­¾ã€æ ‡å‡†ç­”æ¡ˆæˆ–å¤–éƒ¨é¢„è®¾å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLSF èƒ½å¤ŸåŒæ—¶ç»†åŒ–æ¨¡å‹çš„æ¦‚ç‡ä¼°è®¡ä»¥æ¢å¤è‰¯å¥½çš„æ ¡å‡†æ€§ï¼Œå¹¶æ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨ç®—æœ¯æ¨ç†å’Œå¤šé¡¹é€‰æ‹©é¢˜ä»»åŠ¡ä¸­çš„åˆ†æ­¥æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯å®äº†åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ä¸ç¡®å®šæ€§è½¬åŒ–ä¸ºæœ‰æ•ˆè‡ªæˆ‘åé¦ˆçš„å¯è¡Œæ€§ï¼Œä¸º LLM åè®­ç»ƒæµæ°´çº¿æä¾›äº†ä¸€ç§åŸåˆ™æ€§å¼ºä¸”æ•°æ®æ•ˆç‡é«˜çš„å†…åœ¨å¥–åŠ±ç»„ä»¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21931v1",
      "published_date": "2025-07-29 15:46:26 UTC",
      "updated_date": "2025-07-29 15:46:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:42.945034+00:00"
    },
    {
      "arxiv_id": "2507.21929v1",
      "title": "Libra: Large Chinese-based Safeguard for AI Content",
      "title_zh": "Libraï¼šåŸºäºä¸­æ–‡çš„å¤§è§„æ¨¡ AI å†…å®¹å®‰å…¨é˜²æŠ¤ç³»ç»Ÿ",
      "authors": [
        "Ziyang Chen",
        "Huimu Yu",
        "Xing Wu",
        "Dongqin Liu",
        "Songlin Hu"
      ],
      "abstract": "Large language models (LLMs) excel in text understanding and generation but raise significant safety and ethical concerns in high-stakes applications. To mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage curriculum training pipeline, Libra-Guard enhances data efficiency by employing guard pretraining on synthetic samples, followed by fine-tuning on high-quality, real-world data, thereby significantly reducing reliance on manual annotations. To enable rigorous safety evaluations, we also introduce Libra-Test, the first benchmark specifically designed to evaluate the effectiveness of safeguard systems for Chinese content. It covers seven critical harm scenarios and includes over 5,700 samples annotated by domain experts. Experiments show that Libra-Guard achieves 86.79% accuracy, outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat (65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o. These contributions establish a robust framework for advancing the safety governance of Chinese LLMs and represent a tentative step toward developing safer, more reliable Chinese AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Libra-Guardï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæå‡ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®‰å…¨æ€§è€Œè®¾è®¡çš„å‰æ²¿é˜²å¾¡ç³»ç»Ÿã€‚ä¸ºäº†è§£å†³æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶çš„ä¼¦ç†ä¸å®‰å…¨é£é™©ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ï¼ˆcurriculum trainingï¼‰æµæ°´çº¿ï¼Œé€šè¿‡åœ¨åˆæˆæ ·æœ¬ä¸Šè¿›è¡Œé˜²å¾¡é¢„è®­ç»ƒå¹¶éšååœ¨é«˜è´¨é‡çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†æ•°æ®æ•ˆç‡å¹¶é™ä½äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº†é¦–ä¸ªé’ˆå¯¹ä¸­æ–‡å†…å®¹é˜²å¾¡ç³»ç»Ÿè¯„ä¼°çš„åŸºå‡†æµ‹è¯•Libra-Testï¼Œæ¶µç›–ä¸ƒå¤§å…³é”®å±å®³åœºæ™¯åŠè¶…è¿‡5,700ä¸ªä¸“å®¶æ ‡æ³¨æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLibra-Guardè¾¾åˆ°äº†86.79%çš„å‡†ç¡®ç‡ï¼Œå…¶è¡¨ç°å¤§å¹…ä¼˜äºQwen2.5-14B-Instructå’ŒShieldLM-Qwen-14B-Chatï¼Œä¸”æ€§èƒ½æ¥è¿‘Claude-3.5-Sonnetå’ŒGPT-4oç­‰é—­æºæ¨¡å‹ã€‚è¿™ä¸€æˆæœä¸ºä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ²»ç†å»ºç«‹äº†ç¨³å¥æ¡†æ¶ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å¯é çš„ä¸­æ–‡äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21929v1",
      "published_date": "2025-07-29 15:45:50 UTC",
      "updated_date": "2025-07-29 15:45:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:44.156879+00:00"
    },
    {
      "arxiv_id": "2507.21928v4",
      "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda",
      "title_zh": "Vibe Codingï¼šè½¯ä»¶å¼€å‘ä¸­æ„å›¾ä¸­ä»‹æ–¹å¼çš„é‡æ„â€”â€”å®šä¹‰ã€å½±å“ä¸ç ”ç©¶è®®ç¨‹",
      "authors": [
        "Christian Meske",
        "Tobias Hermanns",
        "Esther von der Weiden",
        "Kai-Uwe Loser",
        "Thorsten Berger"
      ],
      "abstract": "Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being generated by Artificial Intelligence (AI). The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and Generative AI (GenAI) engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding redistributes epistemic labor between humans and machines, shifting expertise from technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks such as black-box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.",
      "tldr_zh": "è¯¥ç ”ç©¶å®šä¹‰äº† Vibe coding è¿™ä¸€æ–°å…´çš„è½¯ä»¶å¼€å‘èŒƒå¼ï¼Œæè¿°äº†äººç±»ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) å¦‚ä½•é€šè¿‡è‡ªç„¶è¯­è¨€å¯¹è¯åœ¨åä½œæµä¸­å…±åŒåˆ›ä½œè½¯ä»¶åˆ¶å“ã€‚è¯¥èŒƒå¼å°†å¼€å‘è€…æ„å›¾çš„ä¸­ä»‹æ–¹å¼ (Intent mediation) ä»ä¼ ç»Ÿçš„ç¡®å®šæ€§æŒ‡ä»¤ (Deterministic instruction) è½¬å‘äº†æ¦‚ç‡æ€§æ¨ç† (Probabilistic inference)ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜äº†æ¦‚å¿µç›®æ ‡å‘å¯æ‰§è¡Œç³»ç»Ÿçš„è½¬åŒ–è¿‡ç¨‹ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒVibe coding é‡æ–°åˆ†é…äº†äººæœºä¹‹é—´çš„è®¤çŸ¥åŠ³åŠ¨ï¼Œä½¿ä¸“ä¸šçŸ¥è¯†çš„æ ¸å¿ƒä»å…·ä½“çš„æŠ€æœ¯å®ç°è½¬å‘äº†åä½œç¼–æ’ (Collaborative orchestration)ã€‚è™½ç„¶è¿™ä¸€èŒƒå¼å¸¦æ¥äº†å¼€å‘æ°‘ä¸»åŒ–ã€æ•ˆç‡åŠ é€Ÿå’Œç³»ç»Ÿæ€§æ æ†ç­‰æœºé‡ï¼Œä½†ä¹Ÿé¢ä¸´é»‘ç›’ä»£ç åº“ã€è´£ä»»å½’å±ç¼ºä½å’Œç”Ÿæ€ç³»ç»Ÿåè§ç­‰æ½œåœ¨é£é™©ã€‚æ–‡ç« æœ€åæå‡ºäº†ä¸€ä¸ªæ¶µç›–äººç±»ã€æŠ€æœ¯å’Œç»„ç»‡ç»´åº¦çš„ç ”ç©¶è®®ç¨‹ï¼Œæ—¨åœ¨å¼•å¯¼æœªæ¥å¯¹è¿™ä¸€è½¯ä»¶å¼€å‘å˜é©çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21928v4",
      "published_date": "2025-07-29 15:44:55 UTC",
      "updated_date": "2026-01-08 16:44:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:02.403256+00:00"
    },
    {
      "arxiv_id": "2507.21922v1",
      "title": "SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention",
      "title_zh": "SwinECATï¼šç»“åˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›å’Œé«˜æ•ˆé€šé“æ³¨æ„åŠ›çš„ Transformer çœ¼åº•ç–¾ç—…åˆ†ç±»æ¨¡å‹",
      "authors": [
        "Peiran Gu",
        "Teng Yao",
        "Mengshen He",
        "Fuhao Duan",
        "Feiyan Liu",
        "RenYuan Peng",
        "Bao Ge"
      ],
      "abstract": "In recent years, artificial intelligence has been increasingly applied in the field of medical imaging. Among these applications, fundus image analysis presents special challenges, including small lesion areas in certain fundus diseases and subtle inter-disease differences, which can lead to reduced prediction accuracy and overfitting in the models. To address these challenges, this paper proposes the Transformer-based model SwinECAT, which combines the Shifted Window (Swin) Attention with the Efficient Channel Attention (ECA) Attention. SwinECAT leverages the Swin Attention mechanism in the Swin Transformer backbone to effectively capture local spatial structures and long-range dependencies within fundus images. The lightweight ECA mechanism is incorporated to guide the SwinECAT's attention toward critical feature channels, enabling more discriminative feature representation. In contrast to previous studies that typically classify fundus images into 4 to 6 categories, this work expands fundus disease classification to 9 distinct types, thereby enhancing the granularity of diagnosis. We evaluate our method on the Eye Disease Image Dataset (EDID) containing 16,140 fundus images for 9-category classification. Experimental results demonstrate that SwinECAT achieves 88.29\\% accuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The classification results of our proposed model SwinECAT significantly outperform the baseline Swin Transformer and multiple compared baseline models. To our knowledge, this represents the highest reported performance for 9-category classification on this public dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SwinECATï¼Œä¸€ç§åŸºäºTransformerçš„çœ¼åº•ç–¾ç—…åˆ†ç±»æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³çœ¼åº•å›¾åƒåˆ†æä¸­ç—…å˜åŒºåŸŸå¾®å°ä¸”ç–¾ç—…é—´å·®å¼‚ç»†å¾®å¯¼è‡´çš„è¿‡æ‹Ÿåˆä¸ç²¾åº¦ä¸è¶³é—®é¢˜ã€‚SwinECATç»“åˆäº†Shifted Window (Swin) Attentionä¸Efficient Channel Attention (ECA) æœºåˆ¶ï¼Œåˆ©ç”¨Swinæ¶æ„æ•è·å±€éƒ¨ç©ºé—´ç»“æ„å’Œé•¿ç¨‹ä¾èµ–ï¼Œå¹¶é€šè¿‡ECAå¢å¼ºå¯¹å…³é”®ç‰¹å¾é€šé“çš„è¾¨è¯†èƒ½åŠ›ã€‚ä¸åŒäºä»¥å¾€ä»…è¿›è¡Œ4è‡³6ç±»åˆ†ç±»çš„ç ”ç©¶ï¼Œè¯¥å·¥ä½œå°†çœ¼åº•ç–¾ç—…åˆ†ç±»æ‰©å±•è‡³9ç§ä¸åŒç±»å‹ï¼Œæ˜¾è‘—æå‡äº†è¯Šæ–­çš„ç²’åº¦ã€‚åœ¨åŒ…å«16,140å¼ å›¾åƒçš„Eye Disease Image Dataset (EDID) æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSwinECATè¾¾åˆ°äº†88.29%çš„å‡†ç¡®ç‡å’Œ0.90çš„å®F1åˆ†æ•°ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºSwin TransformeråŠå…¶ä»–åŸºçº¿æ¨¡å‹ï¼Œä»£è¡¨äº†ç›®å‰è¯¥å…¬å¼€æ•°æ®é›†ä¸Š9åˆ†ç±»ä»»åŠ¡çš„æœ€é«˜æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.21922v1",
      "published_date": "2025-07-29 15:35:46 UTC",
      "updated_date": "2025-07-29 15:35:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:55.298492+00:00"
    },
    {
      "arxiv_id": "2507.21919v2",
      "title": "Training language models to be warm and empathetic makes them less reliable and more sycophantic",
      "title_zh": "å°†è¯­è¨€æ¨¡å‹è®­ç»ƒå¾—æ¸©æš–ä¸”å¯Œæœ‰åŒç†å¿ƒä¼šé™ä½å…¶å¯é æ€§å¹¶å¢åŠ å…¶è°„åªšæ€§",
      "authors": [
        "Lujain Ibrahim",
        "Franziska Sofia Hafner",
        "Luc Rocher"
      ],
      "abstract": "Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†è¯­è¨€æ¨¡å‹(Language Models)è®­ç»ƒå¾—æ›´å…·æ¸©æš–ä¸å…±æƒ…(warm and empathetic)ç‰¹è´¨å¯¹å…¶å¯é æ€§çš„å½±å“ï¼Œæ­ç¤ºäº†æƒ…æ„Ÿä¼˜åŒ–ä¸ä»»åŠ¡å‡†ç¡®æ€§ä¹‹é—´çš„æ˜¾è‘—æƒè¡¡ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹äº”ç§ä¸åŒæ¶æ„çš„æ¨¡å‹è¿›è¡Œå—æ§å®éªŒï¼Œå‘ç°è¿½æ±‚æ¸©æš–äººè®¾çš„æ¨¡å‹åœ¨å®‰å…¨å…³é”®ä»»åŠ¡(safety-critical tasks)ä¸­çš„é”™è¯¯ç‡æ¯”åŸæ¨¡å‹é«˜å‡º10%è‡³30%ï¼Œæ›´å®¹æ˜“ä¼ æ’­é˜´è°‹è®ºæˆ–æä¾›é”™è¯¯çš„åŒ»ç–—å»ºè®®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨ç”¨æˆ·è¡¨ç°å‡ºè„†å¼±æƒ…ç»ªï¼ˆå¦‚æ‚²ä¼¤ï¼‰æ—¶ï¼Œæ›´å€¾å‘äºéªŒè¯ç”¨æˆ·çš„é”™è¯¯ä¿¡å¿µï¼Œè¡¨ç°å‡ºæ›´ä¸¥é‡çš„é˜¿è°€å¥‰æ‰¿(sycophantic)å€¾å‘ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•(standard benchmarks)ä¸­ä»ä¿æŒè‰¯å¥½æ€§èƒ½ï¼Œä½†å…¶åœ¨å®é™…äº¤äº’ä¸­çš„å¯é æ€§å¤§å¹…ä¸‹é™ã€‚è¯¥å‘ç°å¼ºè°ƒäº†å½“å‰è¯„ä¼°ä½“ç³»éš¾ä»¥æ•æ‰åˆ°çš„ç³»ç»Ÿæ€§é£é™©ï¼Œå¹¶å‘¼ååœ¨éƒ¨ç½²å…·æœ‰ç±»äººç‰¹è´¨çš„AIç³»ç»Ÿæ—¶éœ€é‡æ–°å®¡è§†å…¶å¼€å‘ä¸ç›‘ç®¡ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21919v2",
      "published_date": "2025-07-29 15:33:20 UTC",
      "updated_date": "2025-07-30 10:11:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:27:56.987433+00:00"
    },
    {
      "arxiv_id": "2507.21905v2",
      "title": "Evaluating Deepfake Detectors in the Wild",
      "title_zh": "ç°å®åœºæ™¯ä¸‹æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨çš„è¯„ä¼°",
      "authors": [
        "Viacheslav Pirogov",
        "Maksim Artemev"
      ],
      "abstract": "Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at https://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ä¼ªé€ (Deepfakes)å¯¹èº«ä»½éªŒè¯å’Œæ•°å­—åª’ä½“çœŸå®æ€§æ„æˆçš„å¨èƒï¼Œè¯„ä¼°äº†ç°ä»£æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…å¼•å…¥äº†ä¸€ç§æ—¨åœ¨æ¨¡æ‹Ÿç°å®åœºæ™¯çš„æ–°å‹æµ‹è¯•æµç¨‹ï¼Œå¹¶åˆ©ç”¨æœ€å…ˆè¿›çš„ç”ŸæˆæŠ€æœ¯åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡50ä¸‡å¼ é«˜è´¨é‡å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDeepfakeæ£€æµ‹ç›®å‰ä»æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œåœ¨å—æµ‹æ¨¡å‹ä¸­ä»…æœ‰ä¸åˆ°ä¸€åŠçš„AUCå¾—åˆ†è¶…è¿‡60%ï¼Œæœ€ä½å¾—åˆ†ä»…ä¸º50%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°JPEGå‹ç¼©(JPEG compression)å’Œå›¾åƒå¢å¼º(image enhancement)ç­‰åŸºç¡€å›¾åƒæ“ä½œä¼šæ˜¾è‘—å‰Šå¼±æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œé€šè¿‡å…¬å¼€ä»£ç å’Œæ•°æ®ï¼Œä¸ºç†è§£ç°æœ‰æ£€æµ‹æŠ€æœ¯çš„å±€é™æ€§ä»¥åŠå¼€å‘æ›´å…·é²æ£’æ€§çš„é˜²å¾¡æ‰‹æ®µæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation Frameworks Across Domains'",
      "pdf_url": "https://arxiv.org/pdf/2507.21905v2",
      "published_date": "2025-07-29 15:17:00 UTC",
      "updated_date": "2025-08-04 13:19:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:18.683578+00:00"
    },
    {
      "arxiv_id": "2507.21899v1",
      "title": "LLM-based Content Classification Approach for GitHub Repositories by the README Files",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ GitHub ä»“åº“ README æ–‡ä»¶å†…å®¹åˆ†ç±»æ–¹æ³•",
      "authors": [
        "Malik Uzair Mehmood",
        "Shahid Hussain",
        "Wen Li Wang",
        "Muhammad Usama Malik"
      ],
      "abstract": "GitHub is the world's most popular platform for storing, sharing, and managing code. Every GitHub repository has a README file associated with it. The README files should contain project-related information as per the recommendations of GitHub to support the usage and improvement of repositories. However, GitHub repository owners sometimes neglected these recommendations. This prevents a GitHub repository from reaching its full potential. This research posits that the comprehensiveness of a GitHub repository's README file significantly influences its adoption and utilization, with a lack of detail potentially hindering its full potential for widespread engagement and impact within the research community. Large Language Models (LLMs) have shown great performance in many text-based tasks including text classification, text generation, text summarization and text translation. In this study, an approach is developed to fine-tune LLMs for automatically classifying different sections of GitHub README files. Three encoder-only LLMs are utilized, including BERT, DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a gold-standard dataset consisting of 4226 README file sections. This approach outperforms current state-of-the-art methods and has achieved an overall F1 score of 0.98. Moreover, we have also investigated the use of Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) and shown an economical alternative to full fine-tuning without compromising much performance. The results demonstrate the potential of using LLMs in designing an automatic classifier for categorizing the content of GitHub README files. Consequently, this study contributes to the development of automated tools for GitHub repositories to improve their identifications and potential usages.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹GitHubé¡¹ç›®READMEæ–‡ä»¶å¯¹ä»“åº“é‡‡ç”¨å’Œåˆ©ç”¨çš„æ·±è¿œå½±å“ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å†…å®¹åˆ†ç±»æ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨BERTã€DistilBERTå’ŒRoBERTaä¸‰ç§ä»…ç¼–ç å™¨(encoder-only)æ¨¡å‹ï¼Œåˆ©ç”¨åŒ…å«4226ä¸ªREADMEç« èŠ‚çš„é‡‘æ ‡å‡†æ•°æ®é›†è¿›è¡Œäº†å¾®è°ƒ(fine-tuning)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªåŠ¨åˆ†ç±»READMEç« èŠ‚å†…å®¹æ–¹é¢è¡¨ç°å“è¶Šï¼Œç»¼åˆF1åˆ†æ•°é«˜è¾¾0.98ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯„ä¼°äº†ä½ç§©è‡ªé€‚åº”(LoRA)ç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æŠ€æœ¯ï¼Œè¯æ˜å…¶åœ¨æ§åˆ¶è®¡ç®—æˆæœ¬çš„åŒæ—¶èƒ½å¤Ÿä¿æŒæé«˜çš„åˆ†ç±»æ€§èƒ½ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¼€å‘è‡ªåŠ¨åŒ–ä»“åº“ç®¡ç†å·¥å…·å¥ å®šäº†åŸºç¡€ï¼Œæœ‰åŠ©äºæå‡GitHubé¡¹ç›®çš„å¯è¯†åˆ«æ€§åŠåœ¨ç ”ç©¶ç¤¾åŒºä¸­çš„å½±å“åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 4 Figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21899v1",
      "published_date": "2025-07-29 15:09:38 UTC",
      "updated_date": "2025-07-29 15:09:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:25.893980+00:00"
    },
    {
      "arxiv_id": "2507.21890v1",
      "title": "Data-driven quantum Koopman method for simulating nonlinear dynamics",
      "title_zh": "ç”¨äºæ¨¡æ‹Ÿéçº¿æ€§åŠ¨åŠ›å­¦çš„æ•°æ®é©±åŠ¨é‡å­ Koopman æ–¹æ³•",
      "authors": [
        "Baoyang Zhang",
        "Zhen Lu",
        "Yaomin Zhao",
        "Yue Yang"
      ],
      "abstract": "Quantum computation offers potential exponential speedups for simulating certain physical systems, but its application to nonlinear dynamics is inherently constrained by the requirement of unitary evolution. We propose the quantum Koopman method (QKM), a data-driven framework that bridges this gap through transforming nonlinear dynamics into linear unitary evolution in higher-dimensional observable spaces. Leveraging the Koopman operator theory to achieve a global linearization, our approach maps system states into a hierarchy of Hilbert spaces using a deep autoencoder. Within the linearized embedding spaces, the state representation is decomposed into modulus and phase components, and the evolution is governed by a set of unitary Koopman operators that act exclusively on the phase. These operators are constructed from diagonal Hamiltonians with coefficients learned from data, a structure designed for efficient implementation on quantum hardware. This architecture enables direct multi-step prediction, and the operator's computational complexity scales logarithmically with the observable space dimension. The QKM is validated across diverse nonlinear systems. Its predictions maintain relative errors below 6% for reaction-diffusion systems and shear flows, and capture key statistics in 2D turbulence. This work establishes a practical pathway for quantum-accelerated simulation of nonlinear phenomena, exploring a framework built on the synergy between deep learning for global linearization and quantum algorithms for unitary dynamics evolution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Quantum Koopman Method (QKM)ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é‡å­è®¡ç®—æ¨¡æ‹Ÿéçº¿æ€§åŠ¨åŠ›å­¦æ—¶å—é™äºunitary evolutionçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Koopman operator theoryå®ç°å…¨å±€çº¿æ€§åŒ–ï¼Œé€šè¿‡deep autoencoderå°†ç³»ç»ŸçŠ¶æ€æ˜ å°„åˆ°å±‚æ¬¡åŒ–çš„Hilbert spacesä¸­ã€‚åœ¨åµŒå…¥ç©ºé—´å†…ï¼ŒçŠ¶æ€è¡¨ç¤ºè¢«åˆ†è§£ä¸ºæ¨¡æ•°å’Œç›¸ä½ï¼Œå¹¶ç”±ä½œç”¨äºç›¸ä½çš„unitary Koopman operatorsé©±åŠ¨æ¼”åŒ–ï¼Œè¿™äº›ç®—å­é€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ ç³»æ•°çš„diagonal Hamiltoniansæ„å»ºï¼Œä¾¿äºåœ¨é‡å­ç¡¬ä»¶ä¸Šé«˜æ•ˆå®ç°ã€‚QKMçš„è®¡ç®—å¤æ‚åº¦éšè§‚æµ‹ç©ºé—´ç»´åº¦å‘ˆå¯¹æ•°çº§ç¼©æ”¾ï¼Œæ”¯æŒç›´æ¥çš„å¤šæ­¥é¢„æµ‹ã€‚åœ¨ååº”æ‰©æ•£ç³»ç»Ÿå’Œå‰ªåˆ‡æµçš„éªŒè¯ä¸­ï¼Œè¯¥æ–¹æ³•çš„é¢„æµ‹ç›¸å¯¹è¯¯å·®ä¿æŒåœ¨6%ä»¥ä¸‹ï¼Œå¹¶æˆåŠŸæ•æ‰äº†2D turbulenceçš„å…³é”®ç»Ÿè®¡ç‰¹æ€§ã€‚è¯¥å·¥ä½œé€šè¿‡ç»“åˆæ·±åº¦å­¦ä¹ çš„å…¨å±€çº¿æ€§åŒ–ä¸é‡å­ç®—æ³•çš„unitary dynamicsæ¼”åŒ–ï¼Œä¸ºéçº¿æ€§ç°è±¡çš„é‡å­åŠ é€Ÿæ¨¡æ‹Ÿå¼€è¾Ÿäº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.flu-dyn"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21890v1",
      "published_date": "2025-07-29 15:00:56 UTC",
      "updated_date": "2025-07-29 15:00:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:33.687095+00:00"
    },
    {
      "arxiv_id": "2507.21886v6",
      "title": "Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline",
      "title_zh": "åŸºäºå‘¼å¸ä¿¡å·çš„é«˜æ•ˆç–¼ç—›è¯†åˆ«ï¼šå•äº¤å‰æ³¨æ„åŠ› Transformer å¤šçª—å£èåˆç®¡çº¿",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a complex condition that affects a large portion of the population. Accurate and consistent evaluation is essential for individuals experiencing pain and supports the development of effective and advanced management strategies. Automatic pain assessment systems provide continuous monitoring, aid clinical decision-making, and aim to reduce distress while preventing functional decline. This study has been submitted to the Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed method introduces a pipeline that employs respiration as the input signal and integrates a highly efficient cross-attention transformer with a multi-windowing strategy. Extensive experiments demonstrate that respiration serves as a valuable physiological modality for pain assessment. Furthermore, results show that compact and efficient models, when properly optimized, can deliver strong performance, often surpassing larger counterparts. The proposed multi-window strategy effectively captures short-term and long-term features, along with global characteristics, enhancing the model's representational capacity.",
      "tldr_zh": "è¯¥ç ”ç©¶æäº¤è‡³ç¬¬äºŒå±Šä¸‹ä¸€ä»£ç–¼ç—›è¯„ä¼°å¤šæ¨¡æ€ä¼ æ„Ÿå¤§æŒ‘æˆ˜(AI4PAIN)ï¼Œæ—¨åœ¨é€šè¿‡å‘¼å¸ä¿¡å·(Respiration Signals)å®ç°é«˜æ•ˆçš„è‡ªåŠ¨ç–¼ç—›è¯†åˆ«ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§é›†æˆå•äº¤å‰æ³¨æ„åŠ›å˜å‹å™¨(Single Cross-Attention Transformer)ä¸å¤šçª—å£èåˆ(Multi-Window Fusion)ç­–ç•¥çš„å¤„ç†ç®¡çº¿ã€‚è¯¥å¤šçª—å£ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å‘¼å¸ä¿¡å·ä¸­çš„çŸ­æœŸã€é•¿æœŸåŠå…¨å±€ç‰¹å¾ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‘¼å¸ä¿¡å·æ˜¯ç–¼ç—›è¯„ä¼°ä¸­æå…·ä»·å€¼çš„ç”Ÿç†æ¨¡æ€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜ç»è¿‡ä¼˜åŒ–çš„è½»é‡çº§é«˜æ•ˆæ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¾€å¾€èƒ½è¶…è¶Šæ›´åºå¤§çš„æ¨¡å‹ï¼Œä¸ºå®ç°è¿ç»­ç›‘æµ‹å’Œè¾…åŠ©ä¸´åºŠå†³ç­–æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2507.21881, arXiv:2507.21875",
      "pdf_url": "https://arxiv.org/pdf/2507.21886v6",
      "published_date": "2025-07-29 14:58:29 UTC",
      "updated_date": "2025-09-15 23:02:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:33.490633+00:00"
    },
    {
      "arxiv_id": "2507.21882v1",
      "title": "The Impact of Foundational Models on Patient-Centric e-Health Systems",
      "title_zh": "åŸºç¡€æ¨¡å‹å¯¹ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„ç”µå­å¥åº·ç³»ç»Ÿçš„å½±å“",
      "authors": [
        "Elmira Onagh",
        "Alireza Davoodi",
        "Maleknaz Nayebi"
      ],
      "abstract": "As Artificial Intelligence (AI) becomes increasingly embedded in healthcare technologies, understanding the maturity of AI in patient-centric applications is critical for evaluating its trustworthiness, transparency, and real-world impact. In this study, we investigate the integration and maturity of AI feature integration in 116 patient-centric healthcare applications. Using Large Language Models (LLMs), we extracted key functional features, which are then categorized into different stages of the Gartner AI maturity model. Our results show that over 86.21\\% of applications remain at the early stages of AI integration, while only 13.79% demonstrate advanced AI integration.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºç¡€æ¨¡å‹(Foundational Models)å¯¹ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„ç”µå­å¥åº·(e-Health)ç³»ç»Ÿçš„å½±å“ï¼Œé‡ç‚¹è¯„ä¼°äº†AIåœ¨è¿™äº›åº”ç”¨ä¸­çš„æˆç†Ÿåº¦ã€é€æ˜åº¦åŠå…¶ç°å®å½±å“ã€‚ç ”ç©¶äººå‘˜è°ƒç ”äº†116ä¸ªä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„åŒ»ç–—å¥åº·åº”ç”¨ç¨‹åºï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æå–äº†å…³é”®çš„åŠŸèƒ½ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾éšåè¢«æ˜ å°„å¹¶åˆ†ç±»åˆ°Gartner AIæˆç†Ÿåº¦æ¨¡å‹çš„ä¸åŒé˜¶æ®µï¼Œç”¨ä»¥é‡åŒ–AIé›†æˆçš„æ·±åº¦ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡86.21%çš„åº”ç”¨ç¨‹åºä»å¤„äºAIé›†æˆçš„æ—©æœŸé˜¶æ®µï¼Œè€Œä»…æœ‰13.79%çš„åº”ç”¨å±•ç¤ºäº†é«˜çº§çš„AIé›†æˆæ°´å¹³ã€‚è¯¥å‘ç°æ­ç¤ºäº†å½“å‰åŒ»ç–—åº”ç”¨ä¸­AIæŠ€æœ¯çš„è½åœ°ç°çŠ¶ï¼Œä¸ºè¯„ä¼°åŒ»ç–—AIçš„å¯é æ€§åŠæœªæ¥å‘å±•æ–¹å‘æä¾›äº†å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper published in COMPSAC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21882v1",
      "published_date": "2025-07-29 14:56:01 UTC",
      "updated_date": "2025-07-29 14:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:34.490059+00:00"
    },
    {
      "arxiv_id": "2507.21881v7",
      "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image",
      "title_zh": "é¢å‘ç–¼ç—›è¯†åˆ«çš„å¤šè¡¨å¾å›¾ï¼šå°†å¤šç±»çš®ç”µæ´»åŠ¨ä¿¡å·é›†æˆè‡³å•å¹…å›¾åƒ",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a multifaceted phenomenon that affects a substantial portion of the population. Reliable and consistent evaluation supports individuals experiencing pain and enables the development of effective and advanced management strategies. Automatic pain-assessment systems provide continuous monitoring, guide clinical decision-making, and aim to reduce distress while preventing functional decline. Incorporating physiological signals allows these systems to deliver objective, accurate insights into an individual's condition. This study has been submitted to the Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed method introduces a pipeline that employs electrodermal activity signals as the input modality. Multiple signal representations are generated and visualized as waveforms, which are then jointly presented within a unified multi-representation diagram. Extensive experiments using diverse processing and filtering techniques, along with various representation combinations, highlight the effectiveness of the approach. It consistently achieves comparable and, in several cases, superior results to traditional fusion methods, positioning it as a robust alternative for integrating different signal representations or modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç–¼ç—›è¯†åˆ«è¿™ä¸€å¤šç»´åº¦ç°è±¡ï¼Œæå‡ºäº†å°†å¤šç§çš®è‚¤ç”µæ´»åŠ¨(electrodermal activity, EDA)ä¿¡å·é›†æˆåˆ°å•ä¸ªå›¾åƒä¸­çš„å¤šè¡¨ç¤ºå›¾(Multi-Representation Diagrams)æ–¹æ³•ã€‚ä¸ºåº”å¯¹ç¬¬äºŒå±Šä¸‹ä¸€ä»£ç–¼ç—›è¯„ä¼°å¤šæ¨¡æ€ä¼ æ„ŸæŒ‘æˆ˜èµ›(AI4PAIN)ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªå¤„ç†æµç¨‹ï¼Œå°†EDAä¿¡å·ç”Ÿæˆå¤šç§è¡¨ç°å½¢å¼å¹¶å¯è§†åŒ–ä¸ºæ³¢å½¢å›¾ï¼Œæœ€ç»ˆæ•´åˆè¿›ç»Ÿä¸€çš„å¤šè¡¨ç¤ºå›¾ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹å„ç§å¤„ç†ã€æ»¤æ³¢æŠ€æœ¯åŠè¡¨ç¤ºç»„åˆçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†å…¶åœ¨ç–¼ç—›è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šä¸ªæµ‹è¯•æ¡ˆä¾‹ä¸­ä¼˜äºä¼ ç»Ÿçš„èåˆæ–¹æ³•(traditional fusion methods)ï¼Œæ˜¯é›†æˆä¸åŒä¿¡å·è¡¨ç¤ºæˆ–æ¨¡æ€çš„ä¸€ç§ç¨³å¥æ›¿ä»£æ–¹æ¡ˆã€‚è¿™ç§åŸºäºå›¾åƒçš„ä¿¡å·é›†æˆç­–ç•¥èƒ½ä¸ºè‡ªåŠ¨ç–¼ç—›è¯„ä¼°ç³»ç»Ÿæä¾›å®¢è§‚å‡†ç¡®çš„è§è§£ï¼Œä¸ºå®æ—¶ä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†æŠ€æœ¯ä¿éšœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2507.21875",
      "pdf_url": "https://arxiv.org/pdf/2507.21881v7",
      "published_date": "2025-07-29 14:53:28 UTC",
      "updated_date": "2025-09-15 22:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:40.189092+00:00"
    },
    {
      "arxiv_id": "2507.21875v7",
      "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis",
      "title_zh": "Tiny-BioMoEï¼šé¢å‘ç”Ÿç‰©ä¿¡å·åˆ†æçš„è½»é‡çº§åµŒå…¥æ¨¡å‹",
      "authors": [
        "Stefanos Gkikas",
        "Ioannis Kyprakis",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain is a complex and pervasive condition that affects a significant portion of the population. Accurate and consistent assessment is essential for individuals suffering from pain, as well as for developing effective management strategies in a healthcare system. Automatic pain assessment systems enable continuous monitoring, support clinical decision-making, and help minimize patient distress while mitigating the risk of functional deterioration. Leveraging physiological signals offers objective and precise insights into a person's state, and their integration in a multimodal framework can further enhance system performance. This study has been submitted to the Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed approach introduces Tiny-BioMoE, a lightweight pretrained embedding model for biosignal analysis. Trained on 4.4 million biosignal image representations and consisting of only 7.3 million parameters, it serves as an effective tool for extracting high-quality embeddings for downstream tasks. Extensive experiments involving electrodermal activity, blood volume pulse, respiratory signals, peripheral oxygen saturation, and their combinations highlight the model's effectiveness across diverse modalities in automatic pain recognition tasks. The model's architecture (code) and weights are available at https://github.com/GkikasStefanos/Tiny-BioMoE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Tiny-BioMoEï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºç”Ÿç‰©ä¿¡å·(biosignal)åˆ†æçš„è½»é‡åŒ–é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨ç–¼ç—›è¯„ä¼°(automatic pain assessment)çš„å®¢è§‚æ€§å’Œç²¾ç¡®åº¦ã€‚è¯¥æ¨¡å‹ä»…ç”± 730 ä¸‡ä¸ªå‚æ•°ç»„æˆï¼Œåœ¨ 440 ä¸‡ä¸ªç”Ÿç‰©ä¿¡å·å›¾åƒè¡¨ç¤º(biosignal image representations)ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæ˜¯æå–ä¸‹æ¸¸ä»»åŠ¡é«˜è´¨é‡ç‰¹å¾åµŒå…¥çš„æœ‰æ•ˆå·¥å…·ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹ AI4PAIN æŒ‘æˆ˜èµ›ï¼Œåœ¨çš®è‚¤ç”µæ´»åŠ¨(electrodermal activity)ã€è„‰ææ³¢(blood volume pulse)ã€å‘¼å¸ä¿¡å·(respiratory signals)åŠè¡€æ°§é¥±å’Œåº¦(peripheral oxygen saturation)ç­‰å¤šç§æ¨¡æ€ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTiny-BioMoE åœ¨å¤šæ¨¡æ€æ¡†æ¶ä¸‹å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä¸´åºŠç›‘æµ‹ä¸å†³ç­–ã€‚ç›®å‰ï¼Œè¯¥æ¨¡å‹çš„ä»£ç å’Œæƒé‡å·²å…¬å¼€å‘å¸ƒï¼Œä¸ºä¸‹ä¸€ä»£ç–¼ç—›è¯„ä¼°æŠ€æœ¯æä¾›äº†è½»é‡åŒ–ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21875v7",
      "published_date": "2025-07-29 14:46:39 UTC",
      "updated_date": "2025-09-15 22:56:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:47.284753+00:00"
    },
    {
      "arxiv_id": "2507.21873v1",
      "title": "A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data",
      "title_zh": "é’ˆå¯¹å›¾æ•°æ®æ¦‚ç‡æ¨ç†çš„ç¥ç»ç¬¦å·æ–¹æ³•",
      "authors": [
        "Raffaele Pojer",
        "Andrea Passerini",
        "Kim G. Larsen",
        "Manfred Jaeger"
      ],
      "abstract": "Graph neural networks (GNNs) excel at predictive tasks on graph-structured data but often lack the ability to incorporate symbolic domain knowledge and perform general reasoning. Relational Bayesian Networks (RBNs), in contrast, enable fully generative probabilistic modeling over graph-like structures and support rich symbolic knowledge and probabilistic inference. This paper presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs, combining the learning strength of GNNs with the flexible reasoning capabilities of RBNs.\n  We develop two implementations of this integration: one compiles GNNs directly into the native RBN language, while the other maintains the GNN as an external component. Both approaches preserve the semantics and computational properties of GNNs while fully aligning with the RBN modeling paradigm. We also propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic models.\n  To demonstrate the framework's versatility, we apply it to two distinct problems. First, we transform a GNN for node classification into a collective classification model that explicitly models homo- and heterophilic label patterns, substantially improving accuracy. Second, we introduce a multi-objective network optimization problem in environmental planning, where MAP inference supports complex decision-making. Both applications include new publicly available benchmark datasets.\n  This work introduces a powerful and coherent neuro-symbolic approach to graph data, bridging learning and reasoning in ways that enable novel applications and improved performance across diverse tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç¥ç»ç¬¦å·(Neuro-Symbolic)æ¡†æ¶ï¼Œå°†å›¾ç¥ç»ç½‘ç»œ(GNNs)ä¸å…³ç³»è´å¶æ–¯ç½‘ç»œ(RBNs)æ— ç¼é›†æˆï¼Œæ—¨åœ¨ç»“åˆå‰è€…çš„å¼ºå¤§å­¦ä¹ èƒ½åŠ›ä¸åè€…çš„çµæ´»ç¬¦å·æ¨ç†ä¼˜åŠ¿ã€‚ä½œè€…å¼€å‘äº†ä¸¤ç§é›†æˆå®ç°æ–¹å¼ï¼Œåˆ†åˆ«å°†GNNsç›´æ¥ç¼–è¯‘è¿›RBNåŸç”Ÿè¯­è¨€æˆ–ä½œä¸ºå¤–éƒ¨ç»„ä»¶ä¿ç•™ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ€å¤§åéªŒæ¦‚ç‡(MAP)æ¨ç†æ–¹æ³•ã€‚é€šè¿‡å°†èŠ‚ç‚¹åˆ†ç±»è½¬åŒ–ä¸ºæ˜¾å¼å»ºæ¨¡åŒè´¨ä¸å¼‚è´¨æ ‡ç­¾æ¨¡å¼çš„é›†ä½“åˆ†ç±»æ¨¡å‹ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®ç‡ä¸Šå–å¾—äº†å®è´¨æ€§æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æˆåŠŸåº”ç”¨äºç¯å¢ƒè§„åˆ’ä¸­çš„å¤šç›®æ ‡ç½‘ç»œä¼˜åŒ–é—®é¢˜ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚å†³ç­–æ”¯æŒä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶è¿˜æä¾›äº†å…¨æ–°çš„å…¬å¼€åŸºå‡†æ•°æ®é›†ï¼Œä¸ºå›¾æ•°æ®çš„å­¦ä¹ ä¸æ¨ç†æ¶èµ·äº†æ¡¥æ¢ï¼Œå¹¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to the Journal of Artificial Intelligence Research (JAIR); under revision. 29 pages, 6 figures. Code available at https://github.com/raffaelepojer/NeSy-for-graph-data",
      "pdf_url": "https://arxiv.org/pdf/2507.21873v1",
      "published_date": "2025-07-29 14:43:25 UTC",
      "updated_date": "2025-07-29 14:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:47.090644+00:00"
    },
    {
      "arxiv_id": "2507.21872v3",
      "title": "MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors",
      "title_zh": "MultiEditorï¼šåŸºäº 3D é«˜æ–¯æ³¼æº…å…ˆéªŒçš„é©¾é©¶åœºæ™¯å¯æ§å¤šæ¨¡æ€ç›®æ ‡ç¼–è¾‘",
      "authors": [
        "Shouyi Lu",
        "Zihan Lin",
        "Chao Lu",
        "Huanran Wang",
        "Guirong Zhuo",
        "Lianqing Zheng"
      ],
      "abstract": "Autonomous driving systems rely heavily on multimodal perception data to understand complex environments. However, the long-tailed distribution of real-world data hinders generalization, especially for rare but safety-critical vehicle categories. To address this challenge, we propose MultiEditor, a dual-branch latent diffusion framework designed to edit images and LiDAR point clouds in driving scenarios jointly. At the core of our approach is introducing 3D Gaussian Splatting (3DGS) as a structural and appearance prior for target objects. Leveraging this prior, we design a multi-level appearance control mechanism--comprising pixel-level pasting, semantic-level guidance, and multi-branch refinement--to achieve high-fidelity reconstruction across modalities. We further propose a depth-guided deformable cross-modality condition module that adaptively enables mutual guidance between modalities using 3DGS-rendered depth, significantly enhancing cross-modality consistency. Extensive experiments demonstrate that MultiEditor achieves superior performance in visual and geometric fidelity, editing controllability, and cross-modality consistency. Furthermore, generating rare-category vehicle data with MultiEditor substantially enhances the detection accuracy of perception models on underrepresented classes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤„ç†é•¿å°¾åˆ†å¸ƒæ•°æ®ï¼ˆç‰¹åˆ«æ˜¯ç¨€æœ‰è½¦è¾†ç±»åˆ«ï¼‰æ—¶é¢ä¸´çš„æ³›åŒ–éš¾é¢˜ï¼Œæå‡ºäº† MultiEditorï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿè”åˆç¼–è¾‘é©¾é©¶åœºæ™¯å›¾åƒå’Œ LiDAR ç‚¹äº‘çš„åŒåˆ†æ”¯ latent diffusion æ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒæ˜¯å¼•å…¥ 3D Gaussian Splatting (3DGS) ä½œä¸ºç›®æ ‡ç‰©ä½“çš„ç»“æ„ä¸å¤–è§‚å…ˆéªŒï¼Œå¹¶åˆ©ç”¨å¤šå±‚çº§å¤–è§‚æ§åˆ¶æœºåˆ¶ï¼ˆåŒ…æ‹¬ pixel-level pastingã€semantic-level guidance å’Œ multi-branch refinementï¼‰å®ç°è·¨æ¨¡æ€çš„é«˜ä¿çœŸé‡å»ºã€‚ç ”ç©¶è¿˜è®¾è®¡äº†æ·±åº¦å¼•å¯¼çš„å¯å˜å½¢è·¨æ¨¡æ€æ¡ä»¶æ¨¡å—ï¼Œé€šè¿‡ 3DGS æ¸²æŸ“çš„æ·±åº¦å®ç°æ¨¡æ€é—´çš„è‡ªé€‚åº”å¼•å¯¼ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒMultiEditor åœ¨è§†è§‰å’Œå‡ ä½•ä¿çœŸåº¦ã€ç¼–è¾‘å¯æ§æ€§åŠä¸€è‡´æ€§æ–¹é¢å‡è¾¾åˆ°äº†ä¼˜å¼‚æ°´å¹³ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥å·¥å…·ç”Ÿæˆçš„ç¨€æœ‰ç±»åˆ«æ•°æ®èƒ½æ˜¾è‘—æå‡æ„ŸçŸ¥æ¨¡å‹åœ¨æ¬ ä»£è¡¨ç±»åˆ«ä¸Šçš„æ£€æµ‹å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21872v3",
      "published_date": "2025-07-29 14:42:52 UTC",
      "updated_date": "2025-07-31 10:21:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:56.840281+00:00"
    },
    {
      "arxiv_id": "2507.21848v1",
      "title": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity",
      "title_zh": "EDGE-GRPOï¼šåŸºäºç†µé©±åŠ¨ä¸å¼•å¯¼å¼è¯¯å·®æ ¡æ­£æå‡ä¼˜åŠ¿å¤šæ ·æ€§çš„ GRPO",
      "authors": [
        "Xingjian Zhang",
        "Siwei Wen",
        "Wenjun Wu",
        "Lei Huang"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage and \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºé€æ­¥æ¨ç†èƒ½åŠ›æ—¶ï¼ŒGroup Relative Policy Optimization (GRPO) ç®—æ³•å› ç¨€ç–å¥–åŠ±è§„åˆ™å¯¼è‡´çš„ä¼˜åŠ¿åç¼©(advantage collapse)é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…é€šè¿‡åˆ†ææ¨¡å‹åæ€(model reflection)çš„å±€é™æ€§å¹¶è°ƒæŸ¥ç»†ç²’åº¦æ ·æœ¬å±‚é¢çš„ç­–ç•¥ç†µ(policy entropy)ï¼Œæå‡ºäº† EDGE-GRPO ç®—æ³•ã€‚è¯¥ç®—æ³•ç»“åˆäº†ç†µé©±åŠ¨ä¼˜åŠ¿(Entropy-Driven Advantage)ä¸å¼•å¯¼å¼é”™è¯¯çº æ­£(Guided Error Correction)æœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡å¢åŠ ä¼˜åŠ¿å¤šæ ·æ€§æ¥æœ‰æ•ˆç¼“è§£ç®—æ³•æ€§èƒ½ç“¶é¢ˆã€‚åœ¨å¤šä¸ªä¸»æµæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶ç›¸æ¯”ç°æœ‰æŠ€æœ¯çš„ä¼˜è¶Šæ€§ï¼Œç›®å‰è¯¥ç®—æ³•çš„ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21848v1",
      "published_date": "2025-07-29 14:23:58 UTC",
      "updated_date": "2025-07-29 14:23:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:28:56.542016+00:00"
    },
    {
      "arxiv_id": "2507.21846v2",
      "title": "Probabilistic Active Goal Recognition",
      "title_zh": "æ¦‚ç‡æ€§ä¸»åŠ¨ç›®æ ‡è¯†åˆ«",
      "authors": [
        "Chenyuan Zhang",
        "Cristian Rojas Cardenas",
        "Hamid Rezatofighi",
        "Mor Vered",
        "Buser Say"
      ],
      "abstract": "In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„ä¸»åŠ¨ç›®æ ‡è¯†åˆ«(Active Goal Recognition, AGR)é—®é¢˜ï¼Œå¼ºè°ƒè§‚å¯Ÿè€…åº”é€šè¿‡æˆ˜ç•¥æ€§åœ°è·å–ä¿¡æ¯æ¥å‡å°‘æ„å›¾ç†è§£çš„ä¸ç¡®å®šæ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡æ¡†æ¶çš„ç»¼åˆè§£å†³æ–¹æ¡ˆï¼Œå°†è”åˆä¿¡å¿µæ›´æ–°(joint belief update)æœºåˆ¶ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search, MCTS)ç®—æ³•ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•ä½¿è§‚å¯Ÿè€…èƒ½å¤Ÿé«˜æ•ˆè§„åˆ’å¹¶æ¨æ–­è¡ŒåŠ¨è€…çš„éšè—ç›®æ ‡ï¼Œä¸”æ— éœ€ä¾èµ–ç‰¹å®šçš„é¢†åŸŸçŸ¥è¯†(domain-specific knowledge)ã€‚åœ¨ç½‘æ ¼åŸŸ(grid-based domain)çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥è”åˆä¿¡å¿µæ›´æ–°æœºåˆ¶çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è¢«åŠ¨ç›®æ ‡è¯†åˆ«æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶å®é™…åº”ç”¨çš„é¢†åŸŸæ— å…³MCTSç®—æ³•åœ¨æ€§èƒ½ä¸Šä¸å¼ºé¢†åŸŸç‰¹å®šçš„è´ªå©ªåŸºå‡†(greedy baseline)ç›¸å½“ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç›®æ ‡æ¨ç†æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”é²æ£’çš„æ¡†æ¶ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†æ›´å…·äº¤äº’æ€§å’Œè‡ªé€‚åº”æ€§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "Camera Ready Version in KR2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21846v2",
      "published_date": "2025-07-29 14:22:29 UTC",
      "updated_date": "2025-08-11 22:09:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:11.239000+00:00"
    },
    {
      "arxiv_id": "2507.21839v1",
      "title": "Against racing to AGI: Cooperation, deterrence, and catastrophic risks",
      "title_zh": "åå¯¹ AGI ç«èµ›ï¼šåˆä½œã€å¨æ…‘ä¸ç¾éš¾æ€§é£é™©",
      "authors": [
        "Leonard Dung",
        "Max Hellrigel-Holderbaum"
      ],
      "abstract": "AGI Racing is the view that it is in the self-interest of major actors in AI development, especially powerful nations, to accelerate their frontier AI development to build highly capable AI, especially artificial general intelligence (AGI), before competitors have a chance. We argue against AGI Racing. First, the downsides of racing to AGI are much higher than portrayed by this view. Racing to AGI would substantially increase catastrophic risks from AI, including nuclear instability, and undermine the prospects of technical AI safety research to be effective. Second, the expected benefits of racing may be lower than proponents of AGI Racing hold. In particular, it is questionable whether winning the race enables complete domination over losers. Third, international cooperation and coordination, and perhaps carefully crafted deterrence measures, constitute viable alternatives to racing to AGI which have much smaller risks and promise to deliver most of the benefits that racing to AGI is supposed to provide. Hence, racing to AGI is not in anyone's self-interest as other actions, particularly incentivizing and seeking international cooperation around AI issues, are preferable.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹AGI Racingï¼ˆAGIç«èµ›ï¼‰çš„æ‰¹åˆ¤æ€§åˆ†æï¼Œè®ºè¯äº†åŠ é€Ÿå¼€å‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰å¹¶ä¸ç¬¦åˆå„å¤§å›½æˆ–ä¸»è¦å‚ä¸è€…çš„æ ¸å¿ƒåˆ©ç›Šã€‚æ–‡ç« æŒ‡å‡ºï¼Œè¿™ç±»ç«èµ›ä¼šæ˜¾è‘—å¢åŠ AIå¸¦æ¥çš„catastrophic risksï¼ˆç¾éš¾æ€§é£é™©ï¼‰ï¼ŒåŒ…æ‹¬åŠ å‰§æ ¸ä¸ç¨³å®šæ€§ï¼Œå¹¶å¯èƒ½å¯¼è‡´technical AI safetyï¼ˆæŠ€æœ¯æ€§AIå®‰å…¨ï¼‰ç ”ç©¶å¤±æ•ˆã€‚ä½œè€…è¿›ä¸€æ­¥è´¨ç–‘äº†èµ¢å¾—ç«èµ›çš„é¢„æœŸæ”¶ç›Šï¼Œè®¤ä¸ºé€šè¿‡AGIå®ç°å¯¹ç«äº‰å¯¹æ‰‹çš„å®Œå…¨ä¸»å¯¼åœ¨ç°å®ä¸­æéš¾è¾¾æˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œinternational cooperationï¼ˆå›½é™…åˆä½œï¼‰ã€åè°ƒä»¥åŠå®¡æ…è®¾è®¡çš„deterrenceï¼ˆå¨æ…‘ï¼‰æªæ–½æ˜¯æ›´ä¼˜çš„æ›¿ä»£è·¯å¾„ï¼Œèƒ½å¤Ÿåœ¨æå¤§é™ä½é£é™©çš„å‰æä¸‹å®ç°AIå‘å±•çš„æ ¸å¿ƒåˆ©ç›Šã€‚å› æ­¤ï¼Œç ”ç©¶å‘¼åå„æ–¹åº”ä¼˜å…ˆå¯»æ±‚å›½é™…åˆä½œä¸å…±è¯†ï¼Œè€Œéé™·å…¥ç›²ç›®çš„AGI Racingã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21839v1",
      "published_date": "2025-07-29 14:17:08 UTC",
      "updated_date": "2025-07-29 14:17:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:13.345435+00:00"
    },
    {
      "arxiv_id": "2507.21833v2",
      "title": "Analysis of Fourier Neural Operators via Effective Field Theory",
      "title_zh": "åŸºäºæœ‰æ•ˆåœºè®ºçš„å‚…é‡Œå¶ç¥ç»ç®—å­åˆ†æ",
      "authors": [
        "Taeyoung Kim"
      ],
      "abstract": "Fourier Neural Operators (FNOs) have emerged as leading surrogates for solver operators for various functional problems, yet their stability, generalization and frequency behavior lack a principled explanation. We present a systematic effective field theory analysis of FNOs in an infinite dimensional function space, deriving closed recursion relations for the layer kernel and four point vertex and then examining three practically important settings-analytic activations, scale invariant cases and architectures with residual connections. The theory shows that nonlinear activations inevitably couple frequency inputs to high frequency modes that are otherwise discarded by spectral truncation, and experiments confirm this frequency transfer. For wide networks, we derive explicit criticality conditions on the weight initialization ensemble that ensure small input perturbations maintain a uniform scale across depth, and we confirm experimentally that the theoretically predicted ratio of kernel perturbations matches the measurements. Taken together, our results quantify how nonlinearity enables neural operators to capture non-trivial features, supply criteria for hyperparameter selection via criticality analysis, and explain why scale invariant activations and residual connections enhance feature learning in FNOs.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨æœ‰æ•ˆåœºè®º (Effective Field Theory) å¯¹å‚…é‡Œå¶ç¥ç»ç®—å­ (Fourier Neural Operators, FNOs) çš„ç¨³å®šæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œé¢‘ç‡è¡Œä¸ºè¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹åŸç†è§£é‡Šçš„ç©ºç™½ã€‚ç ”ç©¶è€…åœ¨æ— é™ç»´å‡½æ•°ç©ºé—´ä¸­æ¨å¯¼äº†å±‚å†…æ ¸ (layer kernel) å’Œå››ç‚¹é¡¶ç‚¹ (four point vertex) çš„é—­åˆé€’å½’å…³ç³»ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è§£ææ¿€æ´»å‡½æ•°ã€å°ºåº¦ä¸å˜æƒ…å½¢ä»¥åŠæ®‹å·®è¿æ¥ (residual connections) ç­‰æ¶æ„è®¾ç½®ã€‚ç†è®ºåˆ†ææ­ç¤ºäº†éçº¿æ€§æ¿€æ´»å‡½æ•°å¿…ç„¶ä¼šå°†é¢‘ç‡è¾“å…¥è€¦åˆåˆ°åŸæœ¬ä¼šè¢«é¢‘è°±æˆªæ–­ä¸¢å¼ƒçš„é«˜é¢‘æ¨¡å¼ä¸­ï¼Œè¿™ä¸€é¢‘ç‡è½¬ç§» (frequency transfer) ç°è±¡åœ¨å®éªŒä¸­å¾—åˆ°äº†è¯å®ã€‚é’ˆå¯¹å®½ç½‘ç»œï¼Œç ”ç©¶æ¨å¯¼å‡ºäº†ç¡®ä¿è¾“å…¥æ‰°åŠ¨åœ¨æ·±åº¦æ–¹å‘ä¿æŒç»Ÿä¸€å°ºåº¦çš„æƒé‡åˆå§‹åŒ–ä¸´ç•Œæ¡ä»¶ (criticality conditions)ï¼Œå®éªŒç»“æœä¸ç†è®ºé¢„æµ‹é«˜åº¦å»åˆã€‚è¯¥æˆæœä¸ä»…ä¸ºè¶…å‚æ•°é€‰æ‹©æä¾›äº†ç†è®ºæ ‡å‡†ï¼Œè¿˜é‡åŒ–äº†éçº¿æ€§æ•æ‰éå¹³å‡¡ç‰¹å¾çš„èƒ½åŠ›ï¼Œå¹¶é˜æ˜äº†å°ºåº¦ä¸å˜æ¿€æ´»å’Œæ®‹å·®è¿æ¥å¢å¼ºç‰¹å¾å­¦ä¹ çš„å†…åœ¨æœºåˆ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21833v2",
      "published_date": "2025-07-29 14:10:46 UTC",
      "updated_date": "2025-09-16 15:06:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:28.558615+00:00"
    },
    {
      "arxiv_id": "2507.21831v1",
      "title": "Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences",
      "title_zh": "å¼•å…¥ HALCï¼šä¸€ç§ç”¨äºè®¡ç®—ç¤¾ä¼šç§‘å­¦å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç¼–ç æœ€ä¼˜æç¤ºç­–ç•¥å¯»ä¼˜çš„é€šç”¨æ¡†æ¶",
      "authors": [
        "Andreas Reich",
        "Claudia Thoms",
        "Tobias Schrimpf"
      ],
      "abstract": "LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables ($Î±$climate = .76; $Î±$movement = .78) and across two variables ($Î±$climate = .71; $Î±$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HALCï¼Œä¸€ä¸ªç”¨äºåœ¨è®¡ç®—ç¤¾ä¼šç§‘å­¦(Computational Social Sciences)é¢†åŸŸå¯»æ‰¾å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨ç¼–ç æœ€ä¼˜æç¤ºç­–ç•¥çš„é€šç”¨æµæ°´çº¿(General Pipeline)ã€‚é’ˆå¯¹ç›®å‰è‡ªåŠ¨ç¼–ç è¿‡ç¨‹ä¸­æç¤ºç­–ç•¥æ•ˆæœå› æ¨¡å‹å’Œä»»åŠ¡è€Œå¼‚ï¼Œä¸”è¿‡åº¦ä¾èµ–è¯•é”™(Trial and Error)çš„é—®é¢˜ï¼ŒHALCé€šè¿‡ç³»ç»ŸåŒ–å’Œå¯é çš„æ–¹æ³•ä¸ºç‰¹å®šç¼–ç ä»»åŠ¡å’Œæ¨¡å‹æ„å»ºæœ€ä¼˜æç¤ºã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æœ¬åœ°éƒ¨ç½²çš„LLMsï¼Œåœ¨è¶…è¿‡200ä¸‡æ¬¡è¯·æ±‚ä¸­æµ‹è¯•äº†1512ä¸ªç‹¬ç«‹æç¤ºï¼Œå¹¶ä»¥ä¸“å®¶ç¼–ç ä½œä¸ºåŸºå‡†(Ground Truth)è¿›è¡ŒéªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Mistral NeMoæ¨¡å‹åœ¨å•å˜é‡å’Œè·¨å˜é‡ç¼–ç ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†è¾ƒé«˜çš„å¯é æ€§ï¼Œå…¶Krippendorff's alphaå€¼å¤„äº0.71è‡³0.78ä¹‹é—´ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºä½¿LLMå¯¹é½ç°æœ‰çš„ç¼–ç æ‰‹å†Œ(Codebook)ï¼Œè€Œéä¸ºäº†è¿åˆæ¨¡å‹è€Œä¿®æ”¹ç ”ç©¶æ ‡å‡†ã€‚æ­¤é¡¹å·¥ä½œæ·±å…¥æ¢è®¨äº†æç¤ºç­–ç•¥çš„æœ‰æ•ˆæ€§åŠå…¶å½±å“å› ç´ ï¼Œä¸ºä¸åŒç¼–ç ä»»åŠ¡å’Œæ¨¡å‹ç¡®å®šå¯é æç¤ºæä¾›äº†é‡è¦çš„è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "48 pages, 9 figures and 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.21831v1",
      "published_date": "2025-07-29 14:10:31 UTC",
      "updated_date": "2025-07-29 14:10:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:28.394249+00:00"
    },
    {
      "arxiv_id": "2507.21830v4",
      "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework",
      "title_zh": "DualSGï¼šæ˜¾å¼è¯­ä¹‰å¼•å¯¼çš„åŒæµå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Yao Wang",
        "Ruijie jian",
        "Xiaorui Wang",
        "Luqi Gong",
        "Yishan Jiang",
        "Chunjie Luo",
        "Jianfeng Zhan"
      ],
      "abstract": "Multivariate Time Series Forecasting plays a key role in many applications. Recent works have explored using Large Language Models for MTSF to take advantage of their reasoning abilities. However, many methods treat LLMs as end-to-end forecasters, which often leads to a loss of numerical precision and forces LLMs to handle patterns beyond their intended design. Alternatively, methods that attempt to align textual and time series modalities within latent space frequently encounter alignment difficulty. In this paper, we propose to treat LLMs not as standalone forecasters, but as semantic guidance modules within a dual-stream framework. We propose DualSG, a dual-stream framework that provides explicit semantic guidance, where LLMs act as Semantic Guides to refine rather than replace traditional predictions. As part of DualSG, we introduce Time Series Caption, an explicit prompt format that summarizes trend patterns in natural language and provides interpretable context for LLMs, rather than relying on implicit alignment between text and time series in the latent space. We also design a caption-guided fusion module that explicitly models inter-variable relationships while reducing noise and computation. Experiments on real-world datasets from diverse domains show that DualSG consistently outperforms 15 state-of-the-art baselines, demonstrating the value of explicitly combining numerical forecasting with semantic guidance.",
      "tldr_zh": "é’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹(Multivariate Time Series Forecasting)ä¸­å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºç«¯åˆ°ç«¯é¢„æµ‹å™¨å¯¼è‡´çš„æ•°å€¼ç²¾åº¦æŸå¤±ä»¥åŠæ½œç©ºé—´(latent space)å¯¹é½å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†DualSGæ¡†æ¶ã€‚DualSGé‡‡ç”¨åŒæµæ¶æ„ï¼Œå°†LLMså®šä½ä¸ºè¯­ä¹‰å¼•å¯¼æ¨¡å—(Semantic Guides)è€Œéç‹¬ç«‹é¢„æµ‹å™¨ï¼Œé€šè¿‡æ˜¾å¼çš„è¯­ä¹‰å¼•å¯¼æ¥ä¼˜åŒ–ä¼ ç»Ÿé¢„æµ‹ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ—¶é—´åºåˆ—æè¿°(Time Series Caption)ä½œä¸ºä¸€ç§æ˜¾å¼æç¤ºæ ¼å¼ï¼Œå°†è¶‹åŠ¿æ¨¡å¼æ€»ç»“ä¸ºè‡ªç„¶è¯­è¨€ï¼Œä»è€Œä¸ºLLMsæä¾›å¯è§£é‡Šçš„ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªæè¿°å¼•å¯¼çš„èåˆæ¨¡å—(caption-guided fusion module)ï¼Œåœ¨æ˜¾å¼å»ºæ¨¡å˜é‡é—´å…³ç³»çš„åŒæ—¶æœ‰æ•ˆé™ä½äº†å™ªå£°å’Œè®¡ç®—å¼€é”€ã€‚åœ¨å¤šä¸ªé¢†åŸŸçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDualSGä¸€è‡´æ€§åœ°è¶…è¶Šäº†15ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œå……åˆ†è¯æ˜äº†å°†æ•°å€¼é¢„æµ‹ä¸æ˜¾å¼è¯­ä¹‰å¼•å¯¼ç›¸ç»“åˆåœ¨æå‡é¢„æµ‹æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.21830v4",
      "published_date": "2025-07-29 14:08:09 UTC",
      "updated_date": "2025-09-18 09:23:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:27.445733+00:00"
    },
    {
      "arxiv_id": "2508.09992v1",
      "title": "OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services",
      "title_zh": "OpenFPLï¼šä¸€ç§åª²ç¾æœ€å…ˆè¿›è‹±è¶…å¹»æƒ³è¶³çƒæœåŠ¡çš„å¼€æºé¢„æµ‹æ–¹æ³•",
      "authors": [
        "Daniel Groos"
      ],
      "abstract": "Fantasy Premier League engages the football community in selecting the Premier League players who will perform best from gameweek to gameweek. Access to accurate performance forecasts gives participants an edge over competitors by guiding expectations about player outcomes and reducing uncertainty in squad selection. However, high-accuracy forecasts are currently limited to commercial services whose inner workings are undisclosed and that rely on proprietary data. This paper aims to democratize access to highly accurate forecasts of player performance by presenting OpenFPL, an open-source Fantasy Premier League forecasting method developed exclusively from public data. Comprising position-specific ensemble models optimized on Fantasy Premier League and Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL achieves accuracy comparable to a leading commercial service when tested prospectively on data from the 2024-25 season. OpenFPL also surpasses the commercial benchmark for high-return players ($>$ 2 points), which are most influential for rank gains. These findings hold across one-, two-, and three-gameweek forecast horizons, supporting long-term planning of transfers and strategies while also informing final-day decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OpenFPLï¼Œä¸€ç§ä¸“é—¨ä¸º Fantasy Premier League (FPL) è®¾è®¡çš„å¼€æºé¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä»…ä½¿ç”¨å…¬å¼€æ•°æ®æ¥æä¾›é«˜ç²¾åº¦çš„çƒå‘˜è¡¨ç°é¢„æµ‹ã€‚OpenFPL é‡‡ç”¨äº†é’ˆå¯¹çƒå‘˜ä½ç½®ä¼˜åŒ–çš„é›†æˆæ¨¡å‹ (ensemble models)ï¼Œå¹¶åˆ©ç”¨ä» 2020-21 åˆ° 2023-24 å››ä¸ªèµ›å­£çš„ FPL å’Œ Understat æ•°æ®è¿›è¡Œäº†è®­ç»ƒã€‚åœ¨ 2024-25 èµ›å­£çš„å‰ç»æ€§æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®ç‡ä¸é¢†å…ˆçš„å•†ä¸šæœåŠ¡ç›¸å½“ï¼Œç”šè‡³åœ¨é¢„æµ‹å¯¹æ’åæå‡è‡³å…³é‡è¦çš„é«˜å›æŠ¥çƒå‘˜ï¼ˆå¾—åˆ† > 2 åˆ†ï¼‰æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚è¿™äº›å‘ç°åœ¨ä¸€è‡³ä¸‰ä¸ªæ¯”èµ›å‘¨ (gameweek horizons) çš„é¢„æµ‹èŒƒå›´å†…å‡å…·æœ‰ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒçƒå‘˜é€‰æ‹©ã€è½¬ä¼šå®‰æ’å’Œé•¿æœŸæˆ˜ç•¥è§„åˆ’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Models and inference code are freely available at https://github.com/daniegr/OpenFPL",
      "pdf_url": "https://arxiv.org/pdf/2508.09992v1",
      "published_date": "2025-07-29 13:59:51 UTC",
      "updated_date": "2025-07-29 13:59:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:32.007339+00:00"
    },
    {
      "arxiv_id": "2507.21823v1",
      "title": "An Agentic AI for a New Paradigm in Business Process Development",
      "title_zh": "é¢å‘ä¸šåŠ¡æµç¨‹å¼€å‘æ–°èŒƒå¼çš„æ™ºèƒ½ä½“ AI",
      "authors": [
        "Mohammad Azarijafari",
        "Luisa Mich",
        "Michele Missikoff"
      ],
      "abstract": "Artificial Intelligence agents represent the next major revolution in the continuous technological evolution of industrial automation. In this paper, we introduce a new approach for business process design and development that leverages the capabilities of Agentic AI. Departing from the traditional task-based approach to business process design, we propose an agent-based method, where agents contribute to the achievement of business goals, identified by a set of business objects. When a single agent cannot fulfill a goal, we have a merge goal that can be achieved through the collaboration of multiple agents. The proposed model leads to a more modular and intelligent business process development by organizing it around goals, objects, and agents. As a result, this approach enables flexible and context-aware automation in dynamic industrial environments.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»£ç†å¼äººå·¥æ™ºèƒ½(Agentic AI)çš„ä¸šåŠ¡æµç¨‹è®¾è®¡ä¸å¼€å‘æ–°èŒƒå¼ï¼Œæ—¨åœ¨æ¨åŠ¨å·¥ä¸šè‡ªåŠ¨åŒ–é¢†åŸŸçš„æŒç»­æŠ€æœ¯é©å‘½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä»¥æ™ºèƒ½ä½“ä¸ºæ ¸å¿ƒçš„è®¾è®¡æ¨¡å¼ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿçš„åŸºäºä»»åŠ¡(task-based)çš„è®¾è®¡æ–¹æ³•ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®ç”±ä¸šåŠ¡å¯¹è±¡(business objects)å®šä¹‰çš„ä¸šåŠ¡ç›®æ ‡è´¡çŒ®åŠ›é‡ã€‚å½“å•ä¸ªæ™ºèƒ½ä½“æ— æ³•ç‹¬ç«‹å®Œæˆç›®æ ‡æ—¶ï¼Œç³»ç»Ÿé‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œçš„æ–¹å¼æ¥å®ç°åˆå¹¶ç›®æ ‡(merge goal)ã€‚è¯¥æ¨¡å‹å›´ç»•ç›®æ ‡ã€å¯¹è±¡å’Œæ™ºèƒ½ä½“è¿›è¡Œç»„ç»‡ï¼Œæ„å»ºäº†æ›´åŠ æ¨¡å—åŒ–å’Œæ™ºèƒ½åŒ–çš„å¼€å‘ä½“ç³»ã€‚æœ€ç»ˆï¼Œè¿™ç§æ–°èŒƒå¼èƒ½å¤Ÿåœ¨åŠ¨æ€å˜åŒ–çš„å·¥ä¸šç¯å¢ƒä¸­å®ç°å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥(context-aware)èƒ½åŠ›ä¸”çµæ´»çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21823v1",
      "published_date": "2025-07-29 13:58:24 UTC",
      "updated_date": "2025-07-29 13:58:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:32.949336+00:00"
    },
    {
      "arxiv_id": "2507.22094v1",
      "title": "Scaling and Distilling Transformer Models for sEMG",
      "title_zh": "é¢å‘ sEMG çš„ Transformer æ¨¡å‹è§„æ¨¡æ‰©å±•ä¸è’¸é¦",
      "authors": [
        "Nicholas Mehlman",
        "Jean-Christophe Gagnon-Audet",
        "Michael Shvartsman",
        "Kelvin Niu",
        "Alexander H. Miller",
        "Shagun Sodhani"
      ],
      "abstract": "Surface electromyography (sEMG) signals offer a promising avenue for developing innovative human-computer interfaces by providing insights into muscular activity. However, the limited volume of training data and computational constraints during deployment have restricted the investigation of scaling up the model size for solving sEMG tasks. In this paper, we demonstrate that vanilla transformer models can be effectively scaled up on sEMG data and yield improved cross-user performance up to 110M parameters, surpassing the model size regime investigated in other sEMG research (usually <10M parameters). We show that >100M-parameter models can be effectively distilled into models 50x smaller with minimal loss of performance (<1.5% absolute). This results in efficient and expressive models suitable for complex real-time sEMG tasks in real-world environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è¡¨é¢è‚Œç”µä¿¡å·(sEMG)é¢†åŸŸæ‰©å±•Transformeræ¨¡å‹çš„æ½œåŠ›ï¼Œè§£å†³äº†ä»¥å¾€ç ”ç©¶å› è®­ç»ƒæ•°æ®æœ‰é™å’Œè®¡ç®—çº¦æŸå¯¼è‡´æ¨¡å‹è§„æ¨¡é€šå¸¸é™åˆ¶åœ¨10Må‚æ•°ä»¥ä¸‹çš„å±€é™æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ ‡å‡†çš„Transformeræ¨¡å‹åœ¨sEMGæ•°æ®ä¸Šå¯ä»¥æˆåŠŸæ‰©å±•è‡³110Må‚æ•°ï¼Œå¹¶æ˜¾è‘—æå‡äº†è·¨ç”¨æˆ·(cross-user)ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¯æ˜äº†è¶…è¿‡100Må‚æ•°çš„å¤§æ¨¡å‹å¯ä»¥è¢«æœ‰æ•ˆåœ°è’¸é¦(distilled)ä¸ºä½“ç§¯ç¼©å°50å€çš„å°æ¨¡å‹ï¼Œä¸”æ€§èƒ½ç»å¯¹æŸå¤±å°äº1.5%ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­å®ç°å¤æ‚ä¸”å®æ—¶çš„sEMGäººæœºäº¤äº’ç³»ç»Ÿæä¾›äº†å…¼å…·é«˜æ•ˆæ€§ä¸è¡¨ç°åŠ›çš„æ¨¡å‹æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at TMLR 2025 (https://openreview.net/forum?id=hFPWThwUiZ), 11 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22094v1",
      "published_date": "2025-07-29 13:41:59 UTC",
      "updated_date": "2025-07-29 13:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:41.050619+00:00"
    },
    {
      "arxiv_id": "2507.21802v3",
      "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
      "title_zh": "MixGRPOï¼šåˆ©ç”¨æ··åˆ ODE-SDE æå‡åŸºäºæµçš„ GRPO æ•ˆç‡",
      "authors": [
        "Junzhe Li",
        "Yutao Cui",
        "Tao Huang",
        "Yinping Ma",
        "Chun Fan",
        "Miles Yang",
        "Zhao Zhong"
      ],
      "abstract": "Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO and DanceGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for faster sampling. So we present a faster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MixGRPOï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡æµåŒ¹é…æ¨¡å‹(flow matching models)åœ¨äººç±»åå¥½å¯¹é½ä¸­æ•ˆç‡çš„æ–°å‹æ¡†æ¶ã€‚é’ˆå¯¹FlowGRPOå’ŒDanceGRPOåœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ä¸­éœ€å¯¹æ‰€æœ‰å»å™ªæ­¥éª¤è¿›è¡Œé‡‡æ ·å’Œä¼˜åŒ–å¯¼è‡´çš„ä½æ•ˆé—®é¢˜ï¼ŒMixGRPOé€šè¿‡æ•´åˆéšæœºå¾®åˆ†æ–¹ç¨‹(SDE)å’Œå¸¸å¾®åˆ†æ–¹ç¨‹(ODE)å®ç°äº†æ··åˆé‡‡æ ·ç­–ç•¥ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œä»…åœ¨çª—å£å†…æ‰§è¡ŒSDEé‡‡æ ·å’ŒGRPOæŒ‡å¯¼çš„ä¼˜åŒ–ï¼Œçª—å£å¤–åˆ™é‡‡ç”¨ODEé‡‡æ ·ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†ä¼˜åŒ–å¼€é”€å¹¶åŠ é€Ÿæ”¶æ•›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†MixGRPO-Flashå˜ä½“ï¼Œé€šè¿‡åœ¨ä¼˜åŒ–çª—å£å¤–æ”¯æŒé«˜é˜¶æ±‚è§£å™¨è¿›ä¸€æ­¥ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMixGRPOåœ¨æ€§èƒ½ä¸Šä¼˜äºDanceGRPOï¼Œä¸”è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†è¿‘50%ï¼Œè€ŒMixGRPO-Flashæ›´æ˜¯å®ç°äº†71%çš„è®­ç»ƒåŠ é€Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21802v3",
      "published_date": "2025-07-29 13:40:09 UTC",
      "updated_date": "2026-01-13 08:15:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:42.644585+00:00"
    },
    {
      "arxiv_id": "2507.21799v1",
      "title": "Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer",
      "title_zh": "è§£é”å°„é¢‘æ„ŸçŸ¥çš„å¯è§£é‡Šæ€§ï¼šä¸€ç§å¤æ•°å€¼ç™½ç›’ Transformer",
      "authors": [
        "Xie Zhang",
        "Yina Wang",
        "Chenshu Wu"
      ],
      "abstract": "The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box transformers, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex sparse rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box transformer to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box transformer with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦æ— çº¿æ„ŸçŸ¥(Deep Wireless Sensing)æ¨¡å‹ä½œä¸ºâ€œé»‘ç›’â€ç¼ºä¹è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªæ•°å­¦å¯è§£é‡Šçš„å°„é¢‘æ„ŸçŸ¥æ¶æ„RF-CRATEã€‚è¯¥æ¶æ„åŸºäºå¤æ•°ç¨€ç–ç‡ç¼©å‡(complex sparse rate reduction)åŸç†ï¼Œåˆ©ç”¨CR-Calculusæ¡†æ¶å°†åŸæœ‰çš„å®å€¼ç™½ç›’Transformeræ‰©å±•è‡³å¤æ•°åŸŸï¼Œæ¨å¯¼å‡ºå…·æœ‰ç†è®ºæ”¯æ’‘çš„è‡ªæ³¨æ„åŠ›(self-attention)å’Œæ®‹å·®å¤šå±‚æ„ŸçŸ¥å™¨(residual MLP)æ¨¡å—ã€‚ä¸ºäº†æå‡æ¨¡å‹åœ¨æœ‰é™æ— çº¿æ•°æ®ä¸‹çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†Subspace Regularizationæ­£åˆ™åŒ–ç­–ç•¥ä»¥å¢å¼ºç‰¹å¾å¤šæ ·æ€§ï¼Œä½¿å¹³å‡æ€§èƒ½æå‡äº†19.98%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRF-CRATEåœ¨æä¾›å…¨æ•°å­¦å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå…¶æ€§èƒ½è¶³ä»¥åª²ç¾ç»è¿‡ç²¾ç»†å·¥ç¨‹è®¾è®¡çš„é»‘ç›’æ¨¡å‹ï¼Œä¸”ç›¸æ¯”CRATEåœ¨å¤šé¡¹æ„ŸçŸ¥ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†åˆ†ç±»å‡†ç¡®ç‡å¹¶é™ä½äº†å›å½’è¯¯å·®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21799v1",
      "published_date": "2025-07-29 13:35:51 UTC",
      "updated_date": "2025-07-29 13:35:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:29:47.527754+00:00"
    },
    {
      "arxiv_id": "2507.21796v1",
      "title": "MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects",
      "title_zh": "MoDeSuiteï¼šç”¨äºå¯å˜å½¢ç‰©ä½“ç§»åŠ¨æ“ä½œåŸºå‡†æµ‹è¯•çš„æœºå™¨äººå­¦ä¹ ä»»åŠ¡å¥—ä»¶",
      "authors": [
        "Yuying Zhang",
        "Kevin Sebastian Luck",
        "Francesco Verdoja",
        "Ville Kyrki",
        "Joni Pajarinen"
      ],
      "abstract": "Mobile manipulation is a critical capability for robots operating in diverse, real-world environments. However, manipulating deformable objects and materials remains a major challenge for existing robot learning algorithms. While various benchmarks have been proposed to evaluate manipulation strategies with rigid objects, there is still a notable lack of standardized benchmarks that address mobile manipulation tasks involving deformable objects.\n  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation Deformable Object task suite, designed specifically for robot learning. MoDeSuite consists of eight distinct mobile manipulation tasks covering both elastic objects and deformable objects, each presenting a unique challenge inspired by real-world robot applications. Success in these tasks requires effective collaboration between the robot's base and manipulator, as well as the ability to exploit the deformability of the objects. To evaluate and demonstrate the use of the proposed benchmark, we train two state-of-the-art reinforcement learning algorithms and two imitation learning algorithms, highlighting the difficulties encountered and showing their performance in simulation. Furthermore, we demonstrate the practical relevance of the suite by deploying the trained policies directly into the real world with the Spot robot, showcasing the potential for sim-to-real transfer. We expect that MoDeSuite will open a novel research domain in mobile manipulation involving deformable objects. Find more details, code, and videos at https://sites.google.com/view/modesuite/home.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MoDeSuiteï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºæœºå™¨äººå­¦ä¹ (Robot Learning)çš„ç§»åŠ¨æ“ä½œå˜å½¢ç‰©ä½“(Mobile Manipulation Deformable Object)ä»»åŠ¡å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç®—æ³•åœ¨å¤„ç†å˜å½¢ææ–™æ—¶ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†çš„é—®é¢˜ã€‚MoDeSuite åŒ…å«å…«é¡¹å„å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ¶µç›–äº†å¼¹æ€§ç‰©ä½“å’Œå˜å½¢ç‰©ä½“ï¼Œæ¨¡æ‹Ÿäº†çœŸå®ä¸–ç•Œä¸­çš„æœºå™¨äººåº”ç”¨åœºæ™¯ã€‚å®Œæˆè¿™äº›ä»»åŠ¡éœ€è¦æœºå™¨äººåº•åº§ä¸æœºæ¢°è‡‚ä¹‹é—´çš„æœ‰æ•ˆåä½œ(Collaboration)ï¼Œå¹¶è¦æ±‚ç®—æ³•èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ç‰©ä½“çš„å˜å½¢ç‰¹æ€§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è®­ç»ƒä¸¤ç§æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç®—æ³•å’Œä¸¤ç§æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ç®—æ³•å¯¹è¯¥åŸºå‡†è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†å½“å‰æŠ€æœ¯åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æŒ‘æˆ˜ä¸æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†è®­ç»ƒå¥½çš„ç­–ç•¥ç›´æ¥éƒ¨ç½²åˆ°Spotæœºå™¨äººä¸Šï¼ŒéªŒè¯äº†è¯¥å¥—ä»¶åœ¨çœŸå®ä¸–ç•Œä¸­çš„å®ç”¨æ€§ä»¥åŠä»æ¨¡æ‹Ÿåˆ°ç°å®(Sim-to-Real)è¿ç§»çš„æ½œåŠ›ã€‚MoDeSuite ä¸ºæ¶‰åŠå˜å½¢ç‰©ä½“çš„ç§»åŠ¨æ“ä½œç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é¢†åŸŸï¼Œä¸ºåç»­ç®—æ³•çš„å¼€å‘ä¸è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21796v1",
      "published_date": "2025-07-29 13:33:43 UTC",
      "updated_date": "2025-07-29 13:33:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:05.792894+00:00"
    },
    {
      "arxiv_id": "2507.22951v1",
      "title": "Unifying Post-hoc Explanations of Knowledge Graph Completions",
      "title_zh": "çŸ¥è¯†å›¾è°±è¡¥å…¨äº‹åè§£é‡Šçš„ç»Ÿä¸€åŒ–",
      "authors": [
        "Alessandro Lonardi",
        "Samy Badreddine",
        "Tarek R. Besold",
        "Pablo Sanchez Martin"
      ],
      "abstract": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks formalization and consistent evaluations, hindering reproducibility and cross-study comparisons. This paper argues for a unified approach to post-hoc explainability in KGC. First, we propose a general framework to characterize post-hoc explanations via multi-objective optimization, balancing their effectiveness and conciseness. This unifies existing post-hoc explainability algorithms in KGC and the explanations they produce. Next, we suggest and empirically support improved evaluation protocols using popular metrics like Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of interpretability as the ability of explanations to address queries meaningful to end-users. By unifying methods and refining evaluation standards, this work aims to make research in KGC explainability more reproducible and impactful.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±è¡¥å…¨(Knowledge Graph Completion, KGC)çš„äº‹åè§£é‡Šæ€§(Post-hoc explainability)ç¼ºä¹ç»Ÿä¸€å½¢å¼åŒ–å’Œä¸€è‡´è¯„ä¼°æ ‡å‡†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–(multi-objective optimization)æ¥è¡¨å¾è§£é‡Šçš„é€šç”¨æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ç¡®ä¿è§£é‡Šæœ‰æ•ˆæ€§çš„åŒæ—¶å…¼é¡¾å…¶ç®€æ´æ€§ï¼ŒæˆåŠŸç»Ÿä¸€äº†ç°æœ‰çš„KGCäº‹åè§£é‡Šç®—æ³•åŠå…¶ç”Ÿæˆçš„è§£é‡Šç»“æœã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜é€šè¿‡Mean Reciprocal Rank (MRR)å’ŒHits@kç­‰æµè¡ŒæŒ‡æ ‡æå‡ºäº†æ”¹è¿›çš„è¯„ä¼°åè®®ï¼Œå¹¶å¼ºè°ƒäº†å¯è§£é‡Šæ€§(interpretability)åœ¨æ»¡è¶³ç»ˆç«¯ç”¨æˆ·å®é™…æŸ¥è¯¢éœ€æ±‚ä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡æ•´åˆæ–¹æ³•è®ºå¹¶å®Œå–„è¯„ä¼°æ ‡å‡†ï¼Œè¯¥å·¥ä½œæ˜¾è‘—æå‡äº†KGCè§£é‡Šæ€§ç ”ç©¶çš„å¯é‡å¤æ€§å’Œå½±å“åŠ›ï¼Œä¸ºè¯¥é¢†åŸŸçš„è·¨ç ”ç©¶å¯¹æ¯”æä¾›äº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22951v1",
      "published_date": "2025-07-29 13:31:48 UTC",
      "updated_date": "2025-07-29 13:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:11.390998+00:00"
    },
    {
      "arxiv_id": "2507.21792v1",
      "title": "Hybrid Causal Identification and Causal Mechanism Clustering",
      "title_zh": "æ··åˆå› æœè¯†åˆ«ä¸å› æœæœºåˆ¶èšç±»",
      "authors": [
        "Saixiong Liu",
        "Yuhua Qian",
        "Jue Li",
        "Honghong Cheng",
        "Feijiang Li"
      ],
      "abstract": "Bivariate causal direction identification is a fundamental and vital problem in the causal inference field. Among binary causal methods, most methods based on additive noise only use one single causal mechanism to construct a causal model. In the real world, observations are always collected in different environments with heterogeneous causal relationships. Therefore, on observation data, this paper proposes a Mixture Conditional Variational Causal Inference model (MCVCI) to infer heterogeneous causality. Specifically, according to the identifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the superior fitting capabilities of the Gaussian mixture model and the neural network and elegantly uses the likelihoods obtained from the probabilistic bounds of the mixture conditional variational auto-encoder as causal decision criteria. Moreover, we model the casual heterogeneity into cluster numbers and propose the Mixture Conditional Variational Causal Clustering (MCVCC) method, which can reveal causal mechanism expression. Compared with state-of-the-art methods, the comprehensive best performance demonstrates the effectiveness of the methods proposed in this paper on several simulated and real data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äºŒå˜é‡å› æœæ–¹å‘è¯†åˆ«ä¸­ç°æœ‰åŠ æ€§å™ªå£°æ¨¡å‹ï¼ˆadditive noise modelï¼‰éš¾ä»¥å¤„ç†å¼‚è´¨æ€§ï¼ˆheterogeneousï¼‰å› æœå…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†æ··åˆæ¡ä»¶å˜åˆ†å› æœæ¨ç†æ¨¡å‹ï¼ˆMixture Conditional Variational Causal Inference model, MCVCIï¼‰ã€‚åŸºäºæ··åˆåŠ æ€§å™ªå£°æ¨¡å‹ï¼ˆHybrid Additive Noise Model, HANMï¼‰çš„å¯è¾¨è¯†æ€§ï¼ŒMCVCI ç»“åˆäº†é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGaussian mixture modelï¼‰ä¸ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨æ··åˆæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆmixture conditional variational auto-encoderï¼‰äº§ç”Ÿçš„æ¦‚ç‡ç•Œé™ä¼¼ç„¶å€¼ä½œä¸ºå› æœå†³ç­–æ ‡å‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†æ··åˆæ¡ä»¶å˜åˆ†å› æœèšç±»æ–¹æ³•ï¼ˆMixture Conditional Variational Causal Clustering, MCVCCï¼‰ï¼Œé€šè¿‡å°†å› æœå¼‚è´¨æ€§å»ºæ¨¡ä¸ºèšç±»æ•°é‡æ¥æ­ç¤ºå› æœæœºåˆ¶çš„è¡¨è¾¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMCVCI å’Œ MCVCC åœ¨å¤šä¸ªæ¨¡æ‹ŸåŠçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ¨æ–­å¤æ‚å¼‚è´¨å› æœå…³ç³»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21792v1",
      "published_date": "2025-07-29 13:27:15 UTC",
      "updated_date": "2025-07-29 13:27:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:17.889469+00:00"
    },
    {
      "arxiv_id": "2507.21790v1",
      "title": "Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¾…åŠ©é€‰æ‹©å»ºæ¨¡ï¼Ÿæç¤ºç­–ç•¥ä¸å½“å‰æ¨¡å‹èƒ½åŠ›çš„æ¢ç©¶",
      "authors": [
        "Georges Sfeir",
        "Gabriel Nova",
        "Stephane Hess",
        "Sander van Cranenburgh"
      ],
      "abstract": "Large Language Models (LLMs) are widely used to support various workflows across different disciplines, yet their potential in choice modelling remains relatively unexplored. This work examines the potential of LLMs as assistive agents in the specification and, where technically feasible, estimation of Multinomial Logit models. We implement a systematic experimental framework involving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek, Gemini, Gemma, and Llama) evaluated under five experimental configurations. These configurations vary along three dimensions: modelling goal (suggesting vs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs. Chain-of-Thoughts); and information availability (full dataset vs. data dictionary only). Each LLM-suggested specification is implemented, estimated, and evaluated based on goodness-of-fit metrics, behavioural plausibility, and model complexity. Findings reveal that proprietary LLMs can generate valid and behaviourally sound utility specifications, particularly when guided by structured prompts. Open-weight models such as Llama and Gemma struggled to produce meaningful specifications. Claude 4 Sonnet consistently produced the best-fitting and most complex models, while GPT models suggested models with robust and stable modelling outcomes. Some LLMs performed better when provided with just data dictionary, suggesting that limiting raw data access may enhance internal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of correctly estimating its own specifications by executing self-generated code. Overall, the results demonstrate both the promise and current limitations of LLMs as assistive agents in choice modelling, not only for model specification but also for supporting modelling decision and estimation, and provide practical guidance for integrating these tools into choice modellers' workflows.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä½œä¸ºè¾…åŠ©å·¥å…·åœ¨é€‰æ‹©å»ºæ¨¡(Choice Modelling)ä¸­ï¼Œç‰¹åˆ«æ˜¯å¤šé¡¹é€»è¾‘æ¨¡å‹(Multinomial Logit, MNL)çš„è§„èŒƒåˆ¶å®šä¸å‚æ•°ä¼°è®¡æ–¹é¢çš„æ½œåŠ›ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„å®éªŒæ¡†æ¶ï¼Œè¯„ä¼°äº†åŒ…æ‹¬ChatGPTã€Claudeã€DeepSeekã€Geminiã€Gemmaå’ŒLlamaåœ¨å†…çš„6ç±»å…±13ä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œå¹¶å¯¹æ¯”äº†é›¶æ ·æœ¬(Zero-Shot)ä¸é“¾å¼æ€ç»´(Chain-of-Thoughts)ç­‰æç¤ºç­–ç•¥çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œé—­æºå•†ä¸šæ¨¡å‹åœ¨ç»“æ„åŒ–æç¤ºä¸‹èƒ½ç”Ÿæˆå…·æœ‰è¡Œä¸ºåˆç†æ€§çš„æ•ˆç”¨è§„èŒƒï¼Œè€Œå¼€æºæ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ„ä¹‰çš„è§„èŒƒæ–¹é¢è¡¨ç°æ¬ ä½³ã€‚å…¶ä¸­Claude 4 Sonnetç”Ÿæˆçš„æ¨¡å‹æ‹Ÿåˆä¼˜åº¦(Goodness-of-fit)æœ€é«˜ä¸”å¤æ‚åº¦æœ€å¼ºï¼ŒGPTç³»åˆ—æ¨¡å‹è¡¨ç°ç¨³å¥ï¼Œè€ŒGPT o3å±•ç°äº†æ‰§è¡Œè‡ªç”Ÿæˆä»£ç è¿›è¡Œæ¨¡å‹ä¼°è®¡çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä»…æä¾›æ•°æ®å­—å…¸(Data Dictionary)æœ‰æ—¶æ¯”æä¾›å®Œæ•´æ•°æ®é›†æ›´èƒ½å¢å¼ºæ¨¡å‹çš„æ¨ç†è¡¨ç°ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥å·¥ä½œéªŒè¯äº†LLMsåœ¨è¾…åŠ©é€‰æ‹©å»ºæ¨¡å†³ç­–å’Œå·¥ä½œæµæ•´åˆæ–¹é¢çš„å·¨å¤§æ½œåŠ›ä¸å½“å‰å±€é™ã€‚",
      "categories": [
        "econ.EM",
        "cs.AI"
      ],
      "primary_category": "econ.EM",
      "comment": "32 pages, 6 figures, 14 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.21790v1",
      "published_date": "2025-07-29 13:24:44 UTC",
      "updated_date": "2025-07-29 13:24:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:16.696409+00:00"
    },
    {
      "arxiv_id": "2507.21770v1",
      "title": "Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results",
      "title_zh": "ä¸€ç§åŸºäº ChatGPT è‡ªç„¶è¯­è¨€å¤„ç†ç»“æœå¢å¼ºçš„è¯­ä¹‰ç”µå½±æ¨èç³»ç»Ÿ",
      "authors": [
        "Ali Fallahi",
        "Azam Bastanfard",
        "Amineh Amini",
        "Hadi Saboohi"
      ],
      "abstract": "The importance of recommender systems on the web has grown, especially in the movie industry, with a vast selection of options to watch. To assist users in traversing available items and finding relevant results, recommender systems analyze operational data and investigate users' tastes and habits. Providing highly individualized suggestions can boost user engagement and satisfaction, which is one of the fundamental goals of the movie industry, significantly in online platforms. According to recent studies and research, using knowledge-based techniques and considering the semantic ideas of the textual data is a suitable way to get more appropriate results. This study provides a new method for building a knowledge graph based on semantic information. It uses the ChatGPT, as a large language model, to assess the brief descriptions of movies and extract their tone of voice. Results indicated that using the proposed method may significantly enhance accuracy rather than employing the explicit genres supplied by the publishers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ ChatGPT çš„è‡ªç„¶è¯­è¨€å¤„ç†ç»“æœå¢å¼ºçš„è¯­ä¹‰ç”µå½±æ¨èç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åˆ†æç”¨æˆ·çš„å“å‘³å’Œä¹ æƒ¯æä¾›é«˜åº¦ä¸ªæ€§åŒ–çš„å»ºè®®ã€‚é’ˆå¯¹ä¼ ç»Ÿæ¨èç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ä¿¡æ¯æ„å»ºçŸ¥è¯†å›¾è°±(Knowledge Graph)çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ ChatGPT ä½œä¸ºå¤§è¯­è¨€æ¨¡å‹(LLM)å¯¹ç”µå½±çš„ç®€çŸ­æè¿°è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä»æ–‡æœ¬ä¸­æå–å…¶è¯­æ°”é£æ ¼(Tone of Voice)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»…ä¾èµ–å‡ºç‰ˆå•†æä¾›çš„æ˜¾å¼ä½“è£(Explicit Genres)ç›¸æ¯”ï¼Œé‡‡ç”¨æ‰€ææ–¹æ³•èƒ½æ˜¾è‘—æé«˜æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚è¿™ç§ç»“åˆè¯­ä¹‰ä¿¡æ¯å’Œå…ˆè¿›è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä¸ºæå‡åœ¨çº¿è§†é¢‘å¹³å°çš„ç”µå½±æ¨èç²¾å‡†åº¦åŠç”¨æˆ·å‚ä¸åº¦æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "May 2023, 6 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21770v1",
      "published_date": "2025-07-29 12:55:45 UTC",
      "updated_date": "2025-07-29 12:55:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:12.442402+00:00"
    },
    {
      "arxiv_id": "2508.08269v1",
      "title": "emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands",
      "title_zh": "emg2tendonï¼šè‚Œè‚‰éª¨éª¼æ‰‹ä» sEMG ä¿¡å·åˆ°è‚Œè…±æ§åˆ¶çš„æ˜ å°„",
      "authors": [
        "Sagar Verma"
      ],
      "abstract": "Tendon-driven robotic hands offer unparalleled dexterity for manipulation tasks, but learning control policies for such systems presents unique challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a direct one-to-one mapping between motion capture (mocap) data and tendon controls, making the learning process complex and expensive. Additionally, visual tracking methods for real-world applications are prone to occlusions and inaccuracies, further complicating joint tracking. Wrist-wearable surface electromyography (sEMG) sensors present an inexpensive, robust alternative to capture hand motion. However, mapping sEMG signals to tendon control remains a significant challenge despite the availability of EMG-to-pose data sets and regression-based models in the existing literature.\n  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic hands, extending the emg2pose dataset, which includes recordings from 193 subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset incorporates tendon control signals derived using the MyoSuite MyoHand model, addressing limitations such as invalid poses in prior methods. We provide three baseline regression models to demonstrate emg2tendon utility and propose a novel diffusion-based regression model for predicting tendon control from sEMG recordings. This dataset and modeling framework marks a significant step forward for tendon-driven dexterous robotic manipulation, laying the groundwork for scalable and accurate tendon control in robotic hands. https://emg2tendon.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†emg2tendonï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘æœºå™¨äººæ‰‹çš„å¤§è§„æ¨¡EMG-to-Tendonæ§åˆ¶æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è…±é©±åŠ¨(Tendon-driven)ç³»ç»Ÿåœ¨æ§åˆ¶ç­–ç•¥å­¦ä¹ ä¸­ç¼ºä¹ç›´æ¥æ˜ å°„ä»¥åŠè§†è§‰è·Ÿè¸ªæ˜“å—é®æŒ¡å½±å“çš„éš¾é¢˜ã€‚è¯¥æ•°æ®é›†æ‰©å±•è‡ªemg2poseï¼ŒåŒ…å«äº†æ¥è‡ª193åå—è¯•è€…ã€æ—¶é•¿è¾¾370å°æ—¶çš„è¡¨é¢è‚Œç”µä¿¡å·(sEMG)ä¸å¤šç§æ‰‹åŠ¿è®°å½•ï¼Œå¹¶åˆ©ç”¨MyoSuite MyoHandæ¨¡å‹æ¨å¯¼å‡ºç²¾ç¡®çš„è…±æ§åˆ¶(tendon control)ä¿¡å·ã€‚ç ”ç©¶äººå‘˜ä¸ä»…æä¾›äº†ä¸‰ä¸ªåŸºå‡†å›å½’æ¨¡å‹ï¼Œè¿˜åˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£(diffusion-based)çš„å›å½’æ¨¡å‹ï¼Œç”¨äºä»sEMGä¿¡å·ä¸­é¢„æµ‹è…±æ§åˆ¶ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨å¤„ç†æ— æ•ˆå§¿æ€æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä¸ºå®ç°å¯æ‰©å±•ä¸”ç²¾ç¡®çš„è…±é©±åŠ¨çµå·§æœºå™¨äººæ“ä½œå¥ å®šäº†åŸºç¡€ã€‚è¯¥å·¥ä½œæ ‡å¿—ç€åœ¨åˆ©ç”¨ç”Ÿç†ä¿¡å·å®ç°æœºå™¨äººæ‰‹å¤æ‚æ§åˆ¶é¢†åŸŸè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted in Robotics: Science and Systems (RSS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.08269v1",
      "published_date": "2025-07-29 12:49:57 UTC",
      "updated_date": "2025-07-29 12:49:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:24.487883+00:00"
    },
    {
      "arxiv_id": "2507.21763v1",
      "title": "Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks",
      "title_zh": "åˆ©ç”¨æ·±åº¦ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå­¦ä¹ åŠ¨åŠ›å­¦è’™ç‰¹å¡æ´›éšæœºåŠ¨åŠ›å­¦",
      "authors": [
        "Daniele Lanzoni",
        "Olivier Pierre-Louis",
        "Roberto Bergamaschini",
        "Francesco Montalenti"
      ],
      "abstract": "We show that Generative Adversarial Networks (GANs) may be fruitfully exploited to learn stochastic dynamics, surrogating traditional models while capturing thermal fluctuations. Specifically, we showcase the application to a two-dimensional, many-particle system, focusing on surface-step fluctuations and on the related time-dependent roughness. After the construction of a dataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained to propagate stochastically the state of the system in time, allowing the generation of new sequences with a reduced computational cost. Modifications with respect to standard GANs, which facilitate convergence and increase accuracy, are discussed. The trained network is demonstrated to quantitatively reproduce equilibrium and kinetic properties, including scaling laws, with deviations of a few percent from the exact value. Extrapolation limits and future perspectives are critically discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Networks, GANs) æ¥å­¦ä¹ éšæœºåŠ¨åŠ›å­¦ (stochastic dynamics)ï¼Œæ—¨åœ¨æ›¿ä»£ä¼ ç»Ÿæ¨¡å‹å¹¶æ•æ‰çƒ­æ³¢åŠ¨ (thermal fluctuations)ã€‚ç ”ç©¶å›¢é˜Ÿä»¥äºŒç»´å¤šç²’å­ç³»ç»Ÿä¸ºé‡ç‚¹ï¼Œåˆ©ç”¨ Kinetic Monte Carlo æ¨¡æ‹Ÿæ„å»ºæ•°æ®é›†ï¼Œè®­ç»ƒäº†ä¸€ä¸ªæ¡ä»¶ GAN (conditional GAN) æ¥éšæœºä¼ æ’­ç³»ç»Ÿçš„çŠ¶æ€ã€‚è¿™ç§æ–¹æ³•å…è®¸ä»¥æ˜¾è‘—é™ä½çš„è®¡ç®—æˆæœ¬ç”Ÿæˆæ–°çš„æ¼”åŒ–åºåˆ—ï¼Œå¹¶é’ˆå¯¹æ ‡å‡† GAN è¿›è¡Œäº†æ”¹è¿›ä»¥ä¿ƒè¿›æ”¶æ•›å¹¶æé«˜ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿå®šé‡å¤ç°å¹³è¡¡å’ŒåŠ¨åŠ›å­¦æ€§è´¨ï¼ˆåŒ…æ‹¬ Scaling lawsï¼‰ï¼Œå…¶ä¸ç²¾ç¡®å€¼çš„åå·®ä»…åœ¨ç™¾åˆ†ä¹‹å‡ ä»¥å†…ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ‰¹åˆ¤æ€§åœ°è®¨è®ºäº†è¯¥æ¨¡å‹çš„æ¨æ–­æé™å’Œæœªæ¥ä½œä¸ºå¤æ‚éšæœºç‰©ç†è¿‡ç¨‹æ›¿ä»£æ¨¡å‹çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cond-mat.stat-mech",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.stat-mech",
      "comment": "15 pages, 8 figures, 2 appendices",
      "pdf_url": "https://arxiv.org/pdf/2507.21763v1",
      "published_date": "2025-07-29 12:48:03 UTC",
      "updated_date": "2025-07-29 12:48:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:33.195152+00:00"
    },
    {
      "arxiv_id": "2507.21756v2",
      "title": "LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection",
      "title_zh": "LiteFatï¼šé¢å‘å®æ—¶é©¾é©¶å‘˜ç–²åŠ³æ£€æµ‹çš„è½»é‡çº§æ—¶ç©ºå›¾å­¦ä¹ ",
      "authors": [
        "Jing Ren",
        "Suyu Ma",
        "Hong Jia",
        "Xiwei Xu",
        "Ivan Lee",
        "Haytham Fayek",
        "Xiaodong Li",
        "Feng Xia"
      ],
      "abstract": "Detecting driver fatigue is critical for road safety, as drowsy driving remains a leading cause of traffic accidents. Many existing solutions rely on computationally demanding deep learning models, which result in high latency and are unsuitable for embedded robotic devices with limited resources (such as intelligent vehicles/cars) where rapid detection is necessary to prevent accidents. This paper introduces LiteFat, a lightweight spatio-temporal graph learning model designed to detect driver fatigue efficiently while maintaining high accuracy and low computational demands. LiteFat involves converting streaming video data into spatio-temporal graphs (STG) using facial landmark detection, which focuses on key motion patterns and reduces unnecessary data processing. LiteFat uses MobileNet to extract facial features and create a feature matrix for the STG. A lightweight spatio-temporal graph neural network is then employed to identify signs of fatigue with minimal processing and low latency. Experimental results on benchmark datasets show that LiteFat performs competitively while significantly decreasing computational complexity and latency as compared to current state-of-the-art methods. This work enables the development of real-time, resource-efficient human fatigue detection systems that can be implemented upon embedded robotic devices.",
      "tldr_zh": "é’ˆå¯¹ç–²åŠ³é©¾é©¶å¯¼è‡´çš„äº¤é€šå®‰å…¨é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† LiteFatï¼Œä¸€ç§ç”¨äºå®æ—¶é©¾é©¶å‘˜ç–²åŠ³æ£€æµ‹çš„è½»é‡çº§æ—¶ç©ºå›¾å­¦ä¹  (Spatio-Temporal Graph Learning) æ¨¡å‹ã€‚ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ¡ˆç”±äºè®¡ç®—éœ€æ±‚è¿‡é«˜ä¸”å»¶è¿Ÿå¤§ï¼Œéš¾ä»¥åœ¨èµ„æºå—é™çš„åµŒå…¥å¼è½¦è½½è®¾å¤‡ä¸­åº”ç”¨ã€‚LiteFat é€šè¿‡é¢éƒ¨å…³é”®ç‚¹æ£€æµ‹ (facial landmark detection) å°†è§†é¢‘æµè½¬åŒ–ä¸ºæ—¶ç©ºå›¾ (STG)ï¼Œå¹¶ç»“åˆ MobileNet æå–ç‰¹å¾ä»¥æ„å»ºç‰¹å¾çŸ©é˜µã€‚éšåï¼Œè¯¥æ¨¡å‹åˆ©ç”¨è½»é‡çº§æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ (STGNN) ä»¥æä½çš„å¤„ç†å¼€é”€è¯†åˆ«ç–²åŠ³ä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLiteFat åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦å’Œå»¶è¿Ÿçš„åŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œä¸ºåœ¨åµŒå…¥å¼æœºå™¨äººè®¾å¤‡ä¸Šå®ç°å®æ—¶ã€é«˜æ•ˆçš„ç–²åŠ³æ£€æµ‹ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21756v2",
      "published_date": "2025-07-29 12:37:53 UTC",
      "updated_date": "2025-08-13 11:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:31.096720+00:00"
    },
    {
      "arxiv_id": "2507.22092v1",
      "title": "Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss",
      "title_zh": "ç—…ç†åŸºç¡€æ¨¡å‹çš„æ‰«æä»ªæ•æ„Ÿæ€§ï¼šåŸºå‡†æµ‹è¯•åŠåˆ©ç”¨ ScanGen å¯¹æ¯”æŸå¤±çš„ç¼“è§£æ–¹æ³•",
      "authors": [
        "Gianluca Carloni",
        "Biagio Brattoli",
        "Seongho Keum",
        "Jongchan Park",
        "Taebum Lee",
        "Chang Ho Ahn",
        "Sergio Pereira"
      ],
      "abstract": "Computational pathology (CPath) has shown great potential in mining actionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been at the center of modern CPath, and while it delivers unprecedented performance, it is also known that DL may be affected by irrelevant details, such as those introduced during scanning by different commercially available scanners. This may lead to scanner bias, where the model outputs for the same tissue acquired by different scanners may vary. In turn, it hinders the trust of clinicians in CPath-based tools and their deployment in real-world clinical practices. Recent pathology Foundation Models (FMs) promise to provide better domain generalization capabilities. In this paper, we benchmark FMs using a multi-scanner dataset and show that FMs still suffer from scanner bias. Following this observation, we propose ScanGen, a contrastive loss function applied during task-specific fine-tuning that mitigates scanner bias, thereby enhancing the models' robustness to scanner variations. Our approach is applied to the Multiple Instance Learning task of Epidermal Growth Factor Receptor (EGFR) mutation prediction from H\\&E-stained WSIs in lung cancer. We observe that ScanGen notably enhances the ability to generalize across scanners, while retaining or improving the performance of EGFR mutation prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºè®¡ç®—ç—…ç†å­¦(Computational pathology)ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å®¹æ˜“å—åˆ°ä¸åŒæ‰«æä»ªäº§ç”Ÿçš„æ— å…³ç»†èŠ‚å½±å“ï¼Œå¯¼è‡´æ‰«æä»ªåå·®(scanner bias)ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ã€‚é€šè¿‡åœ¨å¤šæ‰«æä»ªæ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è¯å®å³ä½¿æ˜¯å…ˆè¿›çš„ç—…ç†å­¦åŸºç¡€æ¨¡å‹(Foundation Models)ä»å—æ­¤é—®é¢˜å›°æ‰°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ScanGenï¼Œä¸€ç§åœ¨ç‰¹å®šä»»åŠ¡å¾®è°ƒæœŸé—´ä½¿ç”¨çš„å¯¹æ¯”æŸå¤±å‡½æ•°(contrastive loss function)ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è®¾å¤‡é—´çš„å·®å¼‚æ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚åœ¨é’ˆå¯¹è‚ºç™Œå…¨è§†é‡æ•°å­—åŒ–åˆ‡ç‰‡(Whole Slide Images)çš„EGFRçªå˜é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç»“åˆå¤šç¤ºä¾‹å­¦ä¹ (Multiple Instance Learning)å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒScanGenæ˜¾è‘—æå‡äº†æ¨¡å‹è·¨æ‰«æä»ªçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ä¿æŒé¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶å¢å¼ºäº†ç³»ç»Ÿåœ¨ä¸´åºŠéƒ¨ç½²ä¸­çš„å¯é æ€§ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "q-bio.TO"
      ],
      "primary_category": "q-bio.QM",
      "comment": "Accepted (Oral) in MedAGI 2025 International Workshop at MICCAI Conference",
      "pdf_url": "https://arxiv.org/pdf/2507.22092v1",
      "published_date": "2025-07-29 12:35:08 UTC",
      "updated_date": "2025-07-29 12:35:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:33.487672+00:00"
    },
    {
      "arxiv_id": "2507.21753v1",
      "title": "Towards a rigorous evaluation of RAG systems: the challenge of due diligence",
      "title_zh": "è¿ˆå‘ RAG ç³»ç»Ÿçš„ä¸¥è°¨è¯„ä¼°ï¼šå°½èŒè°ƒæŸ¥çš„æŒ‘æˆ˜",
      "authors": [
        "GrÃ©goire Martinon",
        "Alexandra Lorenzo de Brionne",
        "JÃ©rÃ´me Bohard",
        "Antoine Lojou",
        "Damien Hervault",
        "Nicolas J-B. Brunel"
      ],
      "abstract": "The rise of generative AI, has driven significant advancements in high-risk sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG) architecture, combining language models (LLMs) with search engines, is particularly notable for its ability to generate responses from document corpora. Despite its potential, the reliability of RAG systems in critical contexts remains a concern, with issues such as hallucinations persisting. This study evaluates a RAG system used in due diligence for an investment fund. We propose a robust evaluation protocol combining human annotations and LLM-Judge annotations to identify system failures, like hallucinations, off-topic, failed citations, and abstentions. Inspired by the Prediction Powered Inference (PPI) method, we achieve precise performance measurements with statistical guarantees. We provide a comprehensive dataset for further analysis. Our contributions aim to enhance the reliability and scalability of RAG systems evaluation protocols in industrial applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æŠ•èµ„èµ„é‡‘ç­‰é«˜é£é™©é‡‘èé¢†åŸŸä¸­ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿçš„å¯é æ€§åŠå…¶åœ¨å°½èŒè°ƒæŸ¥(due diligence)ä¸­çš„è¯„ä¼°æŒ‘æˆ˜ã€‚é’ˆå¯¹å¹»è§‰(hallucinations)ã€æ— å…³å†…å®¹ã€å¼•ç”¨å¤±è´¥åŠæ‹’ç»å›ç­”ç­‰æ•…éšœï¼Œä½œè€…æå‡ºäº†ä¸€å¥—ç»“åˆäººå·¥æ ‡æ³¨(human annotations)ä¸å¤§æ¨¡å‹è£åˆ¤(LLM-Judge)çš„ç¨³å¥è¯„ä¼°åè®®ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†é¢„æµ‹å¢å¼ºæ¨ç†(Prediction Powered Inference, PPI)æŠ€æœ¯ï¼Œé€šè¿‡ç»Ÿè®¡ä¿è¯(statistical guarantees)å®ç°å¯¹ç³»ç»Ÿæ€§èƒ½çš„ç²¾å‡†è¡¡é‡ã€‚ç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†ä»¥æ”¯æŒåç»­åˆ†æã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—å¢å¼ºäº†å·¥ä¸šåº”ç”¨åœºæ™¯ä¸‹ RAG ç³»ç»Ÿè¯„ä¼°åè®®çš„å¯ä¿¡åº¦ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "in French language. EvalLLM2025: Workshop on Evaluation Generative Models (LLM) and Challenges, AMIAD, 2025, Marseille, France",
      "pdf_url": "https://arxiv.org/pdf/2507.21753v1",
      "published_date": "2025-07-29 12:33:16 UTC",
      "updated_date": "2025-07-29 12:33:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:42.200969+00:00"
    },
    {
      "arxiv_id": "2507.21752v1",
      "title": "SAT-Based Bounded Fitting for the Description Logic ALC",
      "title_zh": "åŸºäº SAT çš„æè¿°é€»è¾‘ ALC æœ‰ç•Œæ‹Ÿåˆ",
      "authors": [
        "Maurice Funk",
        "Jean Christoph Jung",
        "Tom Voellmer"
      ],
      "abstract": "Bounded fitting is a general paradigm for learning logical formulas from positive and negative data examples, that has received considerable interest recently. We investigate bounded fitting for the description logic ALC and its syntactic fragments. We show that the underlying size-restricted fitting problem is NP-complete for all studied fragments, even in the special case of a single positive and a single negative example. By design, bounded fitting comes with probabilistic guarantees in Valiant's PAC learning framework. In contrast, we show that other classes of algorithms for learning ALC concepts do not provide such guarantees. Finally, we present an implementation of bounded fitting in ALC and its fragments based on a SAT solver. We discuss optimizations and compare our implementation to other concept learning tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æè¿°é€»è¾‘(Description Logic) ALCåŠå…¶è¯­æ³•ç‰‡æ®µä¸­çš„æœ‰ç•Œæ‹Ÿåˆ(Bounded fitting)é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§ä»æ­£åç¤ºä¾‹æ•°æ®ä¸­å­¦ä¹ é€»è¾‘å…¬å¼çš„é€šç”¨èŒƒå¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ‰€æœ‰è®¨è®ºçš„ç‰‡æ®µä¸­ï¼Œå—è§„æ¨¡é™åˆ¶çš„æ‹Ÿåˆé—®é¢˜å‡è¡¨ç°å‡ºNP-completeå¤æ‚æ€§ï¼Œå³ä½¿åœ¨ä»…æœ‰ä¸€ä¸ªæ­£ä¾‹å’Œä¸€ä¸ªè´Ÿä¾‹çš„æç«¯æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç›¸æ¯”äºå…¶ä»–æ— æ³•æä¾›ç±»ä¼¼ä¿è¯çš„ALCæ¦‚å¿µå­¦ä¹ ç®—æ³•ï¼Œæœ‰ç•Œæ‹Ÿåˆåœ¨è®¾è®¡ä¸Šèƒ½å¤Ÿæä¾›Valiantçš„PACå­¦ä¹ æ¡†æ¶ä¸‹çš„æ¦‚ç‡æ€§ä¿è¯ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºå¹¶å®ç°äº†ä¸€å¥—åŸºäºSAT solverçš„ALCæœ‰ç•Œæ‹Ÿåˆå·¥å…·ï¼Œå¹¶å¯¹ç®—æ³•è¿›è¡Œäº†å¤šé¡¹ä¼˜åŒ–ã€‚å®éªŒç»“æœé€šè¿‡ä¸ç°æœ‰å…¶ä»–æ¦‚å¿µå­¦ä¹ å·¥å…·çš„å¯¹æ¯”ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¤„ç†æè¿°é€»è¾‘å­¦ä¹ ä»»åŠ¡ä¸­çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "33 pages, full version of paper accepted at ISWC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21752v1",
      "published_date": "2025-07-29 12:32:16 UTC",
      "updated_date": "2025-07-29 12:32:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:57.749629+00:00"
    },
    {
      "arxiv_id": "2507.21738v1",
      "title": "Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation",
      "title_zh": "åŸºäºä»£ç†å¯¹æŠ—æ•°æ®ç”Ÿæˆçš„é›¶æ ·æœ¬æœºå™¨é—å¿˜",
      "authors": [
        "Huiqiang Chen",
        "Tianqing Zhu",
        "Xin Yu",
        "Wanlei Zhou"
      ],
      "abstract": "Machine unlearning aims to remove the influence of specific samples from a trained model. A key challenge in this process is over-unlearning, where the model's performance on the remaining data significantly drops due to the change in the model's parameters. Existing unlearning algorithms depend on the remaining data to prevent this issue. As such, these methods are inapplicable in a more practical scenario, where only the unlearning samples are available (i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to fill this gap. Our approach offers three key innovations: (1) we approximate the inaccessible remaining data by generating adversarial samples; (2) leveraging the generated samples, we pinpoint a specific subspace to perform the unlearning process, therefore preventing over-unlearning in the challenging zero-shot scenario; and (3) we consider the influence of the unlearning process on the remaining samples and design an influence-based pseudo-labeling strategy. As a result, our method further improves the model's performance after unlearning. The proposed method holds a theoretical guarantee, and experiments on various benchmarks validate the effectiveness and superiority of our proposed method over several baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨é—å¿˜(Machine unlearning)ä¸­å­˜åœ¨çš„â€œè¿‡åº¦é—å¿˜â€(over-unlearning)é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»…æœ‰å¾…é—å¿˜æ ·æœ¬è€Œæ— æ³•è·å–å‰©ä½™æ•°æ®(zero-shot unlearning)çš„æå…·æŒ‘æˆ˜æ€§åœºæ™¯ä¸‹ï¼Œæå‡ºäº†åä¸ºZS-PAGçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§æ ·æœ¬(adversarial samples)æ¥æœ‰æ•ˆè¿‘ä¼¼æ— æ³•è·å–çš„å‰©ä½™æ•°æ®ã€‚åˆ©ç”¨è¿™äº›ç”Ÿæˆçš„æ ·æœ¬ï¼ŒZS-PAGèƒ½å¤Ÿç¡®å®šä¸€ä¸ªç‰¹å®šçš„å­ç©ºé—´(subspace)æ¥æ‰§è¡Œé—å¿˜è¿‡ç¨‹ï¼Œä»è€Œåœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹é˜²æ­¢æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºå½±å“åŠ›çš„ä¼ªæ ‡ç­¾ç­–ç•¥(influence-based pseudo-labeling strategy)ï¼Œé€šè¿‡è€ƒè™‘é—å¿˜æ“ä½œå¯¹å‰©ä½™æ ·æœ¬çš„å½±å“æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚ç†è®ºä¿éšœå’Œå®éªŒç»“æœå‡è¯æ˜äº†ZS-PAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šç§åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21738v1",
      "published_date": "2025-07-29 12:16:55 UTC",
      "updated_date": "2025-07-29 12:16:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:30:58.542655+00:00"
    },
    {
      "arxiv_id": "2507.21727v1",
      "title": "GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation",
      "title_zh": "GDAIPï¼šä¸€ç§åŸºäºå›¾çš„ä¸ªä½“åŒ–è„‘åˆ†åŒºé¢†åŸŸè‡ªé€‚åº”æ¡†æ¶",
      "authors": [
        "Jianfei Zhu",
        "Haiqi Zhu",
        "Shaohui Liu",
        "Feng Jiang",
        "Baichun Wei",
        "Chunzhi Yi"
      ],
      "abstract": "Recent deep learning approaches have shown promise in learning such individual brain parcellations from functional magnetic resonance imaging (fMRI). However, most existing methods assume consistent data distributions across domains and struggle with domain shifts inherent to real-world cross-dataset scenarios. To address this challenge, we proposed Graph Domain Adaptation for Individual Parcellation (GDAIP), a novel framework that integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based domain adaptation. We construct cross-dataset brain graphs at both the group and individual levels. By leveraging semi-supervised training and adversarial optimization of the prediction entropy on unlabeled vertices from target brain graph, the reference atlas is adapted from the group-level brain graph to the individual brain graph, enabling individual parcellation under cross-dataset settings. We evaluated our method using parcellation visualization, Dice coefficient, and functional homogeneity. Experimental results demonstrate that GDAIP produces individual parcellations with topologically plausible boundaries, strong cross-session consistency, and ability of reflecting functional organization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠŸèƒ½ç£å…±æŒ¯æˆåƒ(fMRI)ä¸­ä¸ªä½“å¤§è„‘åˆ†åŒºé¢ä¸´çš„é¢†åŸŸåç§»(domain shifts)æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºGDAIPçš„å›¾åŸŸè‡ªé€‚åº”æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å›¾æ³¨æ„åŠ›ç½‘ç»œ(Graph Attention Networks, GAT)ä¸åŸºäºæå¤§æå°ç†µ(Minimax Entropy, MME)çš„åŸŸè‡ªé€‚åº”æŠ€æœ¯ç›¸ç»“åˆï¼Œåœ¨ç¾¤ä½“å’Œä¸ªä½“æ°´å¹³ä¸Šæ„å»ºè·¨æ•°æ®é›†çš„å¤§è„‘å›¾ã€‚é€šè¿‡åˆ©ç”¨åŠç›‘ç£å­¦ä¹ å’Œå¯¹ç›®æ ‡åŸŸæœªæ ‡è®°é¡¶ç‚¹çš„é¢„æµ‹ç†µè¿›è¡Œå¯¹æŠ—ä¼˜åŒ–ï¼ŒGDAIPå®ç°äº†å‚è€ƒå›¾è°±ä»ç¾¤ä½“çº§åˆ°ä¸ªä½“çº§çš„æœ‰æ•ˆè‡ªé€‚åº”ï¼Œä»è€Œæ”¯æŒè·¨æ•°æ®é›†ç¯å¢ƒä¸‹çš„åˆ†åŒºä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åˆ†åŒºå¯è§†åŒ–ã€Diceç³»æ•°å’ŒåŠŸèƒ½åŒè´¨æ€§(functional homogeneity)ç­‰å¤šé¡¹æŒ‡æ ‡å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDAIPèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ‹“æ‰‘åˆç†è¾¹ç•Œå’Œé«˜åº¦è·¨sessionä¸€è‡´æ€§çš„ä¸ªä½“åˆ†åŒºï¼Œå¹¶èƒ½å‡†ç¡®åæ˜ å¤§è„‘çš„åŠŸèƒ½ç»„ç»‡ç‰¹æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21727v1",
      "published_date": "2025-07-29 12:04:09 UTC",
      "updated_date": "2025-07-29 12:04:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:06.491295+00:00"
    },
    {
      "arxiv_id": "2507.21723v1",
      "title": "Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations",
      "title_zh": "æ£€æµ‹ Transformer æ·±åº¦å‰–æï¼šä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„æ¶ˆèç ”ç©¶æ–¹æ³•",
      "authors": [
        "Nils HÃ¼tten",
        "Florian HÃ¶lken",
        "Hasan Tercan",
        "Tobias Meisen"
      ],
      "abstract": "In recent years, Explainable AI has gained traction as an approach to enhancing model interpretability and transparency, particularly in complex models such as detection transformers. Despite rapid advancements, a substantial research gap remains in understanding the distinct roles of internal components - knowledge that is essential for improving transparency and efficiency. Inspired by neuroscientific ablation studies, which investigate the functions of brain regions through selective impairment, we systematically analyze the impact of ablating key components in three state-of-the-art detection transformer models: Detection transformer (DETR), deformable detection transformer (DDETR), and DETR with improved denoising anchor boxes (DINO). The ablations target query embeddings, encoder and decoder multi-head self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA) layers. We evaluate the effects of these ablations on the performance metrics gIoU and F1-score, quantifying effects on both the classification and regression sub-tasks on the COCO dataset. To facilitate reproducibility and future research, we publicly release the DeepDissect library. Our findings reveal model-specific resilience patterns: while DETR is particularly sensitive to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable attention enhances robustness, and DINO exhibits the greatest resilience due to its look-forward twice update rule, which helps distributing knowledge across blocks. These insights also expose structural redundancies, particularly in DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model simplification without sacrificing performance. This study advances XAI for DETRs by clarifying the contributions of internal components to model performance, offering insights to optimize and improve transparency and efficiency in critical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶å—ç¥ç»ç§‘å­¦æ¶ˆèå®éªŒå¯å‘ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ¶ˆèå…³é”®ç»„ä»¶ï¼Œæ·±å…¥æ¢è®¨äº† Detection Transformers ç³»åˆ—æ¨¡å‹ï¼ˆåŒ…æ‹¬ DETRã€Deformable DETR (DDETR) å’Œ DINOï¼‰çš„å†…éƒ¨ç»„ä»¶åŠŸèƒ½ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å…¬å¼€çš„ DeepDissect åº“ï¼Œé’ˆå¯¹ Query Embeddingsã€ç¼–ç å™¨ä¸è§£ç å™¨çš„ Multi-Head Self-Attention (MHSA) ä»¥åŠè§£ç å™¨çš„ Multi-Head Cross-Attention (MHCA) å±‚è¿›è¡Œå®éªŒï¼Œå¹¶é‡åŒ–äº†å…¶åœ¨ COCO æ•°æ®é›†ä¸Šå¯¹ gIoU å’Œ F1-score çš„å½±å“ã€‚ç»“æœæ˜¾ç¤º DETR å¯¹ç¼–ç å™¨ MHSA å’Œè§£ç å™¨ MHCA çš„æ¶ˆèæä¸ºæ•æ„Ÿï¼Œè€Œ DDETR çš„ Multi-Scale Deformable Attention æ˜¾è‘—æå‡äº†é²æ£’æ€§ã€‚DINO åˆ™å› å…¶ Look-Forward Twice æ›´æ–°è§„åˆ™ä½¿çŸ¥è¯†åˆ†å¸ƒæ›´å‡åŒ€ï¼Œè¡¨ç°å‡ºæœ€å¼ºçš„éŸ§æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº† DDETR å’Œ DINO è§£ç å™¨ MHCA å±‚ä¸­çš„ç»“æ„å†—ä½™ï¼Œä¸ºåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ç®€åŒ–æ¨¡å‹ç»“æ„ã€æå‡æ•ˆç‡æä¾›äº†ä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21723v1",
      "published_date": "2025-07-29 12:00:08 UTC",
      "updated_date": "2025-07-29 12:00:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:09.592095+00:00"
    },
    {
      "arxiv_id": "2507.21706v1",
      "title": "EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic Mutations",
      "title_zh": "EnTao-GPMï¼šç”¨äºç”Ÿæ®–ç³»è‡´ç—…çªå˜é¢„æµ‹çš„ DNA åŸºç¡€æ¨¡å‹",
      "authors": [
        "Zekai Lin",
        "Haoran Sun",
        "Yucheng Guo",
        "Yujie Yang",
        "Yanwen Wang",
        "Bozhen Hu",
        "Chonghang Ye",
        "Qirong Yang",
        "Fan Zhong",
        "Xiaoming Zhang",
        "Lei Liu"
      ],
      "abstract": "Distinguishing pathogenic mutations from benign polymorphisms remains a critical challenge in precision medicine. EnTao-GPM, developed by Fudan University and BioMap, addresses this through three innovations: (1) Cross-species targeted pre-training on disease-relevant mammalian genomes (human, pig, mouse), leveraging evolutionary conservation to enhance interpretation of pathogenic motifs, particularly in non-coding regions; (2) Germline mutation specialization via fine-tuning on ClinVar and HGMD, improving accuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework integrating DNA sequence embeddings with LLM-based statistical explanations to provide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates superior accuracy in mutation classification. It revolutionizes genetic testing by enabling faster, more accurate, and accessible interpretation for clinical diagnostics (e.g., variant assessment, risk identification, personalized treatment) and research, advancing personalized medicine.",
      "tldr_zh": "å¤æ—¦å¤§å­¦ä¸ç™¾å›¾ç”Ÿç§‘(BioMap)è”åˆå¼€å‘äº†EnTao-GPMï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é¢„æµ‹ç”Ÿæ®–ç»†èƒè‡´ç—…æ€§çªå˜çš„DNAåŸºç¡€æ¨¡å‹(DNA Foundation Model)ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨äººã€çŒªã€é¼ ç­‰è·¨ç‰©ç§å“ºä¹³åŠ¨ç‰©åŸºå› ç»„ä¸Šè¿›è¡Œå®šå‘é¢„è®­ç»ƒï¼Œåˆ©ç”¨æ¼”åŒ–ä¿å®ˆæ€§å¢å¼ºäº†å¯¹è‡´ç—…åŸºåºï¼ˆç‰¹åˆ«æ˜¯non-coding regionsï¼‰çš„è§£è¯»èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ClinVarå’ŒHGMDæ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹ç”Ÿæ®–ç»†èƒçªå˜çš„å¾®è°ƒï¼Œä½¿å…¶åœ¨å¤„ç†SNVså’Œnon-SNVsæ—¶å‡å…·å¤‡æé«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒEnTao-GPMæ•´åˆäº†DNA sequence embeddingsä¸åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ç»Ÿè®¡è§£é‡Šï¼Œæ„å»ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„ä¸´åºŠæ¡†æ¶ä»¥æä¾›å¯æ“ä½œçš„è§è§£ã€‚åœ¨ClinVarçš„éªŒè¯ä¸­ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„çªå˜åˆ†ç±»æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†åŸºå› æ£€æµ‹ä¸ä¸´åºŠé£é™©è¯„ä¼°çš„æ•ˆç‡ï¼Œä¸ºæ¨è¿›ä¸ªæ€§åŒ–åŒ»ç–—å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21706v1",
      "published_date": "2025-07-29 11:34:41 UTC",
      "updated_date": "2025-07-29 11:34:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:08.354545+00:00"
    },
    {
      "arxiv_id": "2507.21705v1",
      "title": "Unrolling Dynamic Programming via Graph Filters",
      "title_zh": "åŸºäºå›¾æ»¤æ³¢å™¨çš„åŠ¨æ€è§„åˆ’å±•å¼€",
      "authors": [
        "Sergio Rozada",
        "Samuel Rey",
        "Gonzalo Mateos",
        "Antonio G. Marques"
      ],
      "abstract": "Dynamic programming (DP) is a fundamental tool used across many engineering fields. The main goal of DP is to solve Bellman's optimality equations for a given Markov decision process (MDP). Standard methods like policy iteration exploit the fixed-point nature of these equations to solve them iteratively. However, these algorithms can be computationally expensive when the state-action space is large or when the problem involves long-term dependencies. Here we propose a new approach that unrolls and truncates policy iterations into a learnable parametric model dubbed BellNet, which we train to minimize the so-termed Bellman error from random value function initializations. Viewing the transition probability matrix of the MDP as the adjacency of a weighted directed graph, we draw insights from graph signal processing to interpret (and compactly re-parameterize) BellNet as a cascade of nonlinear graph filters. This fresh look facilitates a concise, transferable, and unifying representation of policy and value iteration, with an explicit handle on complexity during inference. Preliminary experiments conducted in a grid-like environment demonstrate that BellNet can effectively approximate optimal policies in a fraction of the iterations required by classical methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Dynamic Programming (DP) åœ¨è§£å†³å¤§è§„æ¨¡ Markov Decision Process (MDP) æ—¶è®¡ç®—å¼€é”€å·¨å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º BellNet çš„å¯å­¦ä¹ å‚æ•°åŒ–æ¨¡å‹ã€‚BellNet é€šè¿‡å±•å¼€å¹¶æˆªæ–­ä¼ ç»Ÿçš„ Policy Iteration è¿‡ç¨‹ï¼Œå¹¶ä»¥æœ€å°åŒ–éšæœºä»·å€¼å‡½æ•°åˆå§‹åŒ–çš„ Bellman Error ä¸ºç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶å°† MDP çš„è½¬ç§»æ¦‚ç‡çŸ©é˜µè§†ä¸ºåŠ æƒæœ‰å‘å›¾çš„é‚»æ¥çŸ©é˜µï¼Œåˆ©ç”¨ Graph Signal Processing (GSP) çš„åŸç†å°†æ¨¡å‹é‡æ–°å‚æ•°åŒ–ä¸ºéçº¿æ€§ Graph Filters çš„çº§è”ã€‚è¿™ç§æ–¹æ³•ä¸º Policy Iteration å’Œ Value Iteration æä¾›äº†ä¸€ç§ç®€æ´ä¸”å¯è¿ç§»çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œå¹¶èƒ½åœ¨æ¨ç†é˜¶æ®µæ˜¾å¼æ§åˆ¶è®¡ç®—å¤æ‚åº¦ã€‚åˆæ­¥å®éªŒè¯æ˜ï¼ŒBellNet åœ¨ç±»ç½‘æ ¼ç¯å¢ƒä¸­ä»…éœ€ä¼ ç»Ÿæ–¹æ³•çš„ä¸€å°éƒ¨åˆ†è¿­ä»£æ¬¡æ•°ï¼Œå³å¯æœ‰æ•ˆé€¼è¿‘ Optimal Policyã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21705v1",
      "published_date": "2025-07-29 11:34:20 UTC",
      "updated_date": "2025-07-29 11:34:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:15.052187+00:00"
    },
    {
      "arxiv_id": "2507.21695v1",
      "title": "Towards a Large Physics Benchmark",
      "title_zh": "è¿ˆå‘å¤§è§„æ¨¡ç‰©ç†å­¦åŸºå‡†",
      "authors": [
        "Kristian G. Barman",
        "Sascha Caron",
        "Faegheh Hasibi",
        "Eugene Shalugin",
        "Yoris Marcet",
        "Johannes Otte",
        "Henk W. de Regt",
        "Merijn Moody"
      ],
      "abstract": "We introduce a benchmark framework developed by and for the scientific community to evaluate, monitor and steer large language model development in fundamental physics. Building on philosophical concepts of scientific understanding and creativity, we develop a scoring system in which each question is scored by an expert for its correctness, difficulty, and surprise. The questions are of three forms: (i) multiple-choice questions for conceptual understanding, (ii) analytical problems requiring mathematical derivation, and (iii) openended tasks requiring complex problem solving. Our current dataset contains diverse set of examples, including a machine learning challenge to classify high-energy physics events, such as the four top quark signal. To ensure continued relevance, we propose a living benchmark, where physicists contribute questions, for instance alongside new publications. We invite contributions via: http://www.physicsbenchmarks.org/. We hope that this benchmark will enable a targeted AI development that can make a meaningful contribution to fundamental physics research.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªç”±ç§‘å­¦ç•Œå…±åŒå¼€å‘çš„åŸºå‡†æµ‹è¯•æ¡†æ¶(benchmark framework)ï¼Œæ—¨åœ¨è¯„ä¼°ã€ç›‘æµ‹å¹¶å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Model)åœ¨åŸºç¡€ç‰©ç†(fundamental physics)é¢†åŸŸçš„å‘å±•ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºç§‘å­¦ç†è§£ä¸åˆ›é€ åŠ›çš„å“²å­¦æ¦‚å¿µï¼Œå»ºç«‹äº†ä¸€å¥—ç”±ä¸“å®¶é’ˆå¯¹æ­£ç¡®æ€§ã€éš¾åº¦å’ŒæƒŠå–œåº¦è¿›è¡Œæ‰“åˆ†çš„è¯„åˆ†ä½“ç³»ã€‚è¯¥åŸºå‡†åŒ…å«æ¦‚å¿µç†è§£çš„é€‰æ‹©é¢˜ã€æ¶‰åŠæ•°å­¦æ¨å¯¼çš„è§£æé¢˜ä»¥åŠå¤æ‚çš„å¼€æ”¾å¼ä»»åŠ¡ï¼Œç›®å‰å·²æ¶µç›–é«˜èƒ½ç‰©ç†(high-energy physics)ä¸­çš„å››é¡¶å¤¸å…‹ä¿¡å·(four top quark signal)åˆ†ç±»ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†ä¿è¯è¯„ä¼°çš„é•¿æœŸæœ‰æ•ˆæ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§â€œåŠ¨æ€åŸºå‡†(living benchmark)â€æ¨¡å¼ï¼Œé¼“åŠ±ç‰©ç†å­¦å®¶è´¡çŒ®æ–°é¢˜ç›®ä»¥è·Ÿè¿›ç§‘ç ”è¿›å±•ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡é’ˆå¯¹æ€§çš„äººå·¥æ™ºèƒ½å¼€å‘ï¼Œä¸ºåŸºç¡€ç‰©ç†ç ”ç©¶æä¾›å®è´¨æ€§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "physics.data-an",
        "cs.AI",
        "hep-ph",
        "physics.comp-ph",
        "physics.hist-ph"
      ],
      "primary_category": "physics.data-an",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21695v1",
      "published_date": "2025-07-29 11:19:00 UTC",
      "updated_date": "2025-07-29 11:19:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:17.562049+00:00"
    },
    {
      "arxiv_id": "2507.21694v1",
      "title": "A Multi-Agent Generative AI Framework for IC Module-Level Verification Automation",
      "title_zh": "é¢å‘é›†æˆç”µè·¯æ¨¡å—çº§éªŒè¯è‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“ç”Ÿæˆå¼ AI æ¡†æ¶",
      "authors": [
        "Wenbo Liu",
        "Forbes Hou",
        "Jon Zhang",
        "Hong Liu",
        "Allen Lei"
      ],
      "abstract": "As large language models demonstrate enormous potential in the field of Electronic Design Automation (EDA), generative AI-assisted chip design is attracting widespread attention from academia and industry. Although these technologies have made preliminary progress in tasks such as code generation, their application in chip verification -- a critical bottleneck in the chip development cycle -- remains at an exploratory stage. This paper proposes an innovative Multi-Agent Verification Framework (MAVF) aimed at addressing the limitations of current single-LLM approaches in complex verification tasks. Our framework builds an automated transformation system from design specifications to testbench through the collaborative work of multiple specialized agents, including specification parsing, verification strategy generation, and code implementation. Through verification experiments on multiple chip modules of varying complexity, results show that MAVF significantly outperforms traditional manual methods and single-dialogue generative AI approaches in verification document parsing and generation, as well as automated testbench generation. This research opens new directions for exploring generative AI applications in verification automation, potentially providing effective approaches to solving the most challenging bottleneck issues in chip design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMulti-Agent Verification Framework (MAVF)çš„å¤šæ™ºèƒ½ä½“ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é›†æˆç”µè·¯(IC)æ¨¡å—çº§éªŒè¯çš„è‡ªåŠ¨åŒ–ã€‚é’ˆå¯¹å½“å‰å•å¤§è¯­è¨€æ¨¡å‹(single-LLM)åœ¨å¤„ç†å¤æ‚èŠ¯ç‰‡éªŒè¯ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼ŒMAVFé€šè¿‡æ„å»ºç”±å¤šä¸ªä¸“é—¨æ™ºèƒ½ä½“(specialized agents)ç»„æˆçš„åä½œç³»ç»Ÿï¼Œè¦†ç›–äº†ä»è®¾è®¡è§„èŒƒè§£æ(specification parsing)ã€éªŒè¯ç­–ç•¥ç”Ÿæˆ(verification strategy generation)åˆ°ä»£ç å®ç°(code implementation)çš„å…¨æµç¨‹ã€‚è¯¥æ¡†æ¶å®ç°äº†ä»è®¾è®¡è§„èŒƒåˆ°æµ‹è¯•å¹³å°(testbench)çš„è‡ªåŠ¨åŒ–è½¬æ¢ï¼Œæ˜¾è‘—æå‡äº†éªŒè¯æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAVFåœ¨å¤„ç†ä¸åŒå¤æ‚åº¦çš„èŠ¯ç‰‡æ¨¡å—æ—¶ï¼Œå…¶éªŒè¯æ–‡æ¡£è§£æå’Œè‡ªåŠ¨åŒ–æµ‹è¯•å¹³å°ç”Ÿæˆèƒ½åŠ›å‡ä¼˜äºä¼ ç»Ÿäººå·¥æ–¹æ³•å’Œå•è½®å¯¹è¯AIæ–¹æ¡ˆã€‚è¿™ä¸€ç ”ç©¶ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨EDAé¢†åŸŸçš„éªŒè¯è‡ªåŠ¨åŒ–åº”ç”¨æä¾›äº†æ–°è·¯å¾„ï¼Œæœ‰æœ›è§£å†³èŠ¯ç‰‡è®¾è®¡å‘¨æœŸä¸­çš„æ ¸å¿ƒç“¶é¢ˆé—®é¢˜ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "20 pages, 12 figures. DVCon China 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21694v1",
      "published_date": "2025-07-29 11:17:47 UTC",
      "updated_date": "2025-07-29 11:17:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:22.555881+00:00"
    },
    {
      "arxiv_id": "2507.21693v1",
      "title": "MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios",
      "title_zh": "MultiAIGCDï¼šæ¶µç›–å¤šç§è¯­è¨€ã€æ¨¡å‹ã€æç¤ºè¯åŠåœºæ™¯çš„ AI ç”Ÿæˆä»£ç æ£€æµ‹ç»¼åˆæ•°æ®é›†",
      "authors": [
        "Basak Demirok",
        "Mucahid Kutlu",
        "Selin Mergen"
      ],
      "abstract": "As large language models (LLMs) rapidly advance, their role in code generation has expanded significantly. While this offers streamlined development, it also creates concerns in areas like education and job interviews. Consequently, developing robust systems to detect AI-generated code is imperative to maintain academic integrity and ensure fairness in hiring processes. In this study, we introduce MultiAIGCD, a dataset for AI-generated code detection for Python, Java, and Go. From the CodeNet dataset's problem definitions and human-authored codes, we generate several code samples in Java, Python, and Go with six different LLMs and three different prompts. This generation process covered three key usage scenarios: (i) generating code from problem descriptions, (ii) fixing runtime errors in human-written code, and (iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271 AI-generated and 32,148 human-written code snippets. We also benchmark three state-of-the-art AI-generated code detection models and assess their performance in various test scenarios such as cross-model and cross-language. We share our dataset and codes to support research in this field.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»‹ç»äº† MultiAIGCDï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº AI-generated code detection çš„ç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–äº† Pythonã€Java å’Œ Go ä¸‰ç§è¯­è¨€ã€‚ä¸ºäº†åº”å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•™è‚²å’Œæ‹›è˜ä¸­å¼•å‘çš„è¯šä¿¡æ‹…å¿§ï¼Œç ”ç©¶è€…åŸºäº CodeNet çš„é—®é¢˜å®šä¹‰ï¼Œåˆ©ç”¨ 6 ç§ä¸åŒçš„ LLMs å’Œ 3 ç§æç¤ºè¯ç”Ÿæˆäº†ä»£ç æ ·æœ¬ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†ä»é—®é¢˜æè¿°ç”Ÿæˆä»£ç ã€ä¿®å¤è¿è¡Œæ—¶é”™è¯¯ä»¥åŠçº æ­£é”™è¯¯è¾“å‡ºä¸‰ä¸ªå…³é”®åœºæ™¯ï¼Œå…±åŒ…å« 121,271 æ¡ AI ç”Ÿæˆå’Œ 32,148 æ¡äººç±»ç¼–å†™çš„ä»£ç ç‰‡æ®µã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å¯¹ä¸‰ç§å…ˆè¿›çš„æ£€æµ‹æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†å®ƒä»¬åœ¨è·¨æ¨¡å‹å’Œè·¨è¯­è¨€ç­‰å¤šç§å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†å’Œä»£ç çš„å…¬å¼€åˆ†äº«æ—¨åœ¨æ¨åŠ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œåœ¨ AI ç”Ÿæˆä»£ç è¯†åˆ«é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21693v1",
      "published_date": "2025-07-29 11:16:55 UTC",
      "updated_date": "2025-07-29 11:16:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:25.562681+00:00"
    },
    {
      "arxiv_id": "2507.21690v1",
      "title": "APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing",
      "title_zh": "APTï¼šé€šè¿‡è‡ªé€‚åº”è·¯å¾„è¿½è¸ªæå‡æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆèƒ½åŠ›",
      "authors": [
        "Sangmin Han",
        "Jinho Jeong",
        "Jinwoo Kim",
        "Seon Joo Kim"
      ],
      "abstract": "Latent Diffusion Models (LDMs) are generally trained at fixed resolutions, limiting their capability when scaling up to high-resolution images. While training-based approaches address this limitation by training on high-resolution datasets, they require large amounts of data and considerable computational resources, making them less practical. Consequently, training-free methods, particularly patch-based approaches, have become a popular alternative. These methods divide an image into patches and fuse the denoising paths of each patch, showing strong performance on high-resolution generation. However, we observe two critical issues for patch-based approaches, which we call ``patch-level distribution shift\" and ``increased patch monotonicity.\" To address these issues, we propose Adaptive Path Tracing (APT), a framework that combines Statistical Matching to ensure patch distributions remain consistent in upsampled latents and Scale-aware Scheduling to deal with the patch monotonicity. As a result, APT produces clearer and more refined details in high-resolution images. In addition, APT enables a shortcut denoising process, resulting in faster sampling with minimal quality degradation. Our experimental results confirm that APT produces more detailed outputs with improved inference speed, providing a practical approach to high-resolution image generation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Latent Diffusion Models (LDMs) åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é¢ä¸´çš„è®¡ç®—èµ„æºé™åˆ¶ï¼Œä»¥åŠç°æœ‰ patch-based æ— éœ€è®­ç»ƒæ–¹æ³•ä¸­å­˜åœ¨çš„ \"patch-level distribution shift\" å’Œ \"increased patch monotonicity\" é—®é¢˜ï¼Œæå‡ºäº† Adaptive Path Tracing (APT) æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºå¼•å…¥äº† Statistical Matching æŠ€æœ¯ï¼Œç”¨ä»¥ç¡®ä¿ä¸Šé‡‡æ ·æ½œç©ºé—´ä¸­çš„ patch åˆ†å¸ƒä¿æŒä¸€è‡´ã€‚åŒæ—¶ï¼ŒAPT ç»“åˆäº† Scale-aware Scheduling ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº† patch åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å•è°ƒæ€§é—®é¢˜ã€‚å€ŸåŠ©äºè¿™äº›æ”¹è¿›ï¼ŒAPT èƒ½å¤Ÿäº§ç”Ÿæ¯”ä»¥å¾€æ–¹æ³•æ›´æ¸…æ™°ã€ç»†èŠ‚æ›´ä¸°å¯Œçš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å®ç°äº† shortcut denoising è¿‡ç¨‹ï¼Œåœ¨ç¡®ä¿ç”Ÿæˆè´¨é‡çš„å‰æä¸‹å¤§å¹…ç¼©çŸ­äº†æ¨ç†æ—¶é—´ã€‚å®éªŒç»“æœå……åˆ†éªŒè¯äº† APT åœ¨æå‡é‡‡æ ·æ•ˆç‡å’Œä¼˜åŒ–å›¾åƒç»†èŠ‚æ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæä¾›äº†ä¸€ç§æå…·å®ç”¨ä»·å€¼çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21690v1",
      "published_date": "2025-07-29 11:13:03 UTC",
      "updated_date": "2025-07-29 11:13:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:32.551752+00:00"
    },
    {
      "arxiv_id": "2507.21684v1",
      "title": "diffSPH: Differentiable Smoothed Particle Hydrodynamics for Adjoint Optimization and Machine Learning",
      "title_zh": "diffSPHï¼šé¢å‘ä¼´éšä¼˜åŒ–ä¸æœºå™¨å­¦ä¹ çš„å¯å¾®å…‰æ»‘ç²’å­æµä½“åŠ¨åŠ›å­¦",
      "authors": [
        "Rene Winchenbach",
        "Nils Thuerey"
      ],
      "abstract": "We present diffSPH, a novel open-source differentiable Smoothed Particle Hydrodynamics (SPH) framework developed entirely in PyTorch with GPU acceleration. diffSPH is designed centrally around differentiation to facilitate optimization and machine learning (ML) applications in Computational Fluid Dynamics~(CFD), including training neural networks and the development of hybrid models. Its differentiable SPH core, and schemes for compressible (with shock capturing and multi-phase flows), weakly compressible (with boundary handling and free-surface flows), and incompressible physics, enable a broad range of application areas. We demonstrate the framework's unique capabilities through several applications, including addressing particle shifting via a novel, target-oriented approach by minimizing physical and regularization loss terms, a task often intractable in traditional solvers. Further examples include optimizing initial conditions and physical parameters to match target trajectories, shape optimization, implementing a solver-in-the-loop setup to emulate higher-order integration, and demonstrating gradient propagation through hundreds of full simulation steps. Prioritizing readability, usability, and extensibility, this work offers a foundational platform for the CFD community to develop and deploy novel neural networks and adjoint optimization applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† diffSPHï¼Œä¸€ä¸ªå®Œå…¨åŸºäº PyTorch å¼€å‘å¹¶æ”¯æŒ GPU åŠ é€Ÿçš„å¼€æºå¾®åˆ† Smoothed Particle Hydrodynamics (SPH) æ¡†æ¶ï¼Œæ—¨åœ¨ä¿ƒè¿› Computational Fluid Dynamics (CFD) é¢†åŸŸçš„ä¼˜åŒ–ä¸ Machine Learning (ML) èåˆã€‚è¯¥æ¡†æ¶åŒ…å«äº†å¯å‹ç¼©ã€å¼±å¯å‹ç¼©å’Œä¸å¯å‹ç¼©ç‰©ç†æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ¿€æ³¢æ•æ‰ã€å¤šç›¸æµä»¥åŠè‡ªç”±è¡¨é¢æµç­‰å¤šç§åº”ç”¨åœºæ™¯ã€‚ç ”ç©¶é€šè¿‡ä¸€ç§æ–°å‹çš„ç›®æ ‡å¯¼å‘æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ±‚è§£å™¨ä¸­éš¾ä»¥å¤„ç†çš„ particle shifting é—®é¢˜ï¼Œå¹¶æ¼”ç¤ºäº†åœ¨åˆå§‹æ¡ä»¶ä¼˜åŒ–ã€ç‰©ç†å‚æ•°åŒ¹é…å’Œå½¢çŠ¶ä¼˜åŒ–æ–¹é¢çš„åº”ç”¨ã€‚æ­¤å¤–ï¼ŒdiffSPH å®ç°äº† solver-in-the-loop è®¾ç½®ä»¥æ¨¡æ‹Ÿé«˜é˜¶ç§¯åˆ†ï¼Œå¹¶éªŒè¯äº†æ¢¯åº¦åœ¨æ•°ç™¾ä¸ªå®Œæ•´æ¨¡æ‹Ÿæ­¥ä¸­çš„ä¼ æ’­èƒ½åŠ›ã€‚ä½œä¸ºä¸€ä¸ªé«˜æ˜“ç”¨æ€§ä¸å¯æ‰©å±•æ€§çš„å¹³å°ï¼ŒdiffSPH ä¸º CFD ç¤¾åŒºå¼€å‘æ–°å‹ç¥ç»ç½‘ç»œå’Œ adjoint optimization åº”ç”¨å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21684v1",
      "published_date": "2025-07-29 10:54:27 UTC",
      "updated_date": "2025-07-29 10:54:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:57.950921+00:00"
    },
    {
      "arxiv_id": "2507.21664v1",
      "title": "Can the current trends of AI handle a full course of mathematics?",
      "title_zh": "å½“å‰äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿èƒ½å¦æ‰¿æ‹…å®Œæ•´çš„æ•°å­¦è¯¾ç¨‹æ•™å­¦ï¼Ÿ",
      "authors": [
        "Mariam Alsayyad",
        "Fayadh Kadhem"
      ],
      "abstract": "This paper addresses the question of how able the current trends of Artificial Intelligence (AI) are in managing to take the responsibility of a full course of mathematics at a college level. The study evaluates this ability in four significant aspects, namely, creating a course syllabus, presenting selected material, answering student questions, and creating an assessment. It shows that even though the AI is strong in some important parts like organization and accuracy, there are still some human aspects that are far away from the current abilities of AI. There is still a hidden emotional part, even in science, that cannot be fulfilled by the AI in its current state. This paper suggests some recommendations to integrate the human and AI potentials to create better outcomes in terms of reaching the target of creating a full course of mathematics, at a university level, as best as possible.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å½“å‰çš„ AI è¶‹åŠ¿æ˜¯å¦èƒ½å¤Ÿç‹¬ç«‹æ‰¿æ‹…å¤§å­¦æ°´å¹³çš„å®Œæ•´æ•°å­¦è¯¾ç¨‹æ•™å­¦ä»»åŠ¡ã€‚ç ”ç©¶äººå‘˜ä»åˆ¶å®šè¯¾ç¨‹å¤§çº² (Syllabus)ã€å±•ç¤ºé€‰å®šææ–™ã€å›ç­”å­¦ç”Ÿæé—®ä»¥åŠåˆ›å»ºè¯„ä¼° (Assessment) è¿™å››ä¸ªå…³é”®ç»´åº¦å¯¹ AI çš„èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ AI åœ¨ç»„ç»‡ç»“æ„å’Œå‡†ç¡®æ€§ (Accuracy) ç­‰æ–¹é¢è¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨ç°é˜¶æ®µä»ç„¶æ— æ³•å®Œå…¨èƒœä»»ç§‘å­¦æ•™å­¦ä¸­æ½œè—çš„æƒ…æ„Ÿéƒ¨åˆ† (Emotional part) åŠå…¶ä»–äººæ–‡ç‰¹è´¨ã€‚è¯¥è®ºæ–‡å¼ºè°ƒäº† AI ç›®å‰åœ¨å¤„ç†å¤æ‚æ•™å­¦äº’åŠ¨ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†å°†äººç±»æ½œèƒ½ä¸ AI ä¼˜åŠ¿ç›¸ç»“åˆçš„æ•´åˆå»ºè®®ï¼Œä»¥æœŸåœ¨å¤§å­¦æ•°å­¦æ•™å­¦ä¸­å®ç°æœ€ä½³çš„æ•™è‚²äº§å‡ºã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "math.HO"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.21664v1",
      "published_date": "2025-07-29 10:21:15 UTC",
      "updated_date": "2025-07-29 10:21:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:47.668377+00:00"
    },
    {
      "arxiv_id": "2507.21654v1",
      "title": "AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind",
      "title_zh": "AI ç´ å…»ä½œä¸º AI èµ‹èƒ½æµ‹è¯„ä¸­ç”¨æˆ·ä½“éªŒçš„æ ¸å¿ƒé©±åŠ¨åŠ›ï¼šæ¥è‡ª Socratic Mind çš„å¯ç¤º",
      "authors": [
        "Meryem Yilmaz Soylu",
        "Jeonghyun Lee",
        "Jui-Tse Hung",
        "Christopher Zhang Cui",
        "David A. Joyner"
      ],
      "abstract": "As Artificial Intelligence (AI) tools become increasingly embedded in higher education, understanding how students interact with these systems is essential to supporting effective learning. This study examines how students' AI literacy and prior exposure to AI technologies shape their perceptions of Socratic Mind, an interactive AI-powered formative assessment tool. Drawing on Self-Determination Theory and user experience research, we analyze relationships among AI literacy, perceived usability, satisfaction, engagement, and perceived learning effectiveness. Data from 309 undergraduates in Computer Science and Business courses were collected through validated surveys. Partial least squares structural equation modeling showed that AI literacy - especially self-efficacy, conceptual understanding, and application skills - significantly predicts usability, satisfaction, and engagement. Usability and satisfaction, in turn, strongly predict perceived learning effectiveness, while prior AI exposure showed no significant effect. These findings highlight that AI literacy, rather than exposure alone, shapes student experiences. Designers should integrate adaptive guidance and user-centered features to support diverse literacy levels, fostering inclusive, motivating, and effective AI-based learning environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† AI literacy å’Œå…ˆå‰çš„ AI æ¥è§¦ç»éªŒå¦‚ä½•å½±å“å­¦ç”Ÿå¯¹åŸºäº AI çš„å½¢æˆæ€§è¯„ä¼°å·¥å…· Socratic Mind çš„ä¸»è§‚è®¤çŸ¥ã€‚ç ”ç©¶å›¢é˜ŸåŸºäº Self-Determination Theory å’Œç”¨æˆ·ä½“éªŒç†è®ºï¼Œé€šè¿‡å¯¹ 309 åæœ¬ç§‘ç”Ÿè¿›è¡Œé—®å·è°ƒæŸ¥å¹¶åº”ç”¨ Partial least squares structural equation modeling æŠ€æœ¯è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒAI literacy ä¸­çš„ self-efficacyã€conceptual understanding å’Œ application skills æ˜¯é¢„æµ‹ç³»ç»Ÿ usabilityã€satisfaction ä»¥åŠ engagement çš„å…³é”®æŒ‡æ ‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œusability å’Œ satisfaction æ˜¾è‘—æ­£å‘å½±å“å­¦ç”Ÿçš„ perceived learning effectivenessï¼Œè€Œå…ˆå‰çš„ AI æ¥è§¦ç»éªŒå¹¶æœªè¡¨ç°å‡ºç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å½±å“ã€‚è¿™è¡¨æ˜æå‡å­¦ç”Ÿçš„ AI literacy æ¯”å•çº¯å¢åŠ  AI æ¥è§¦é¢‘ç‡æ›´èƒ½æœ‰æ•ˆä¼˜åŒ–å­¦ä¹ ä½“éªŒã€‚ç ”ç©¶å»ºè®® AI å·¥å…·çš„è®¾è®¡åº”é›†æˆ adaptive guidance ç­‰ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„åŠŸèƒ½ï¼Œä»¥ç…§é¡¾ä¸åŒç´ å…»æ°´å¹³çš„å­¦ä¹ è€…å¹¶è¥é€ æ›´å…·åŒ…å®¹æ€§çš„ AI å­¦ä¹ ç¯å¢ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "34 pages, 1 figure, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.21654v1",
      "published_date": "2025-07-29 10:11:24 UTC",
      "updated_date": "2025-07-29 10:11:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:01.585211+00:00"
    },
    {
      "arxiv_id": "2507.21653v1",
      "title": "DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs",
      "title_zh": "DGPï¼šåŸºäºå›¾å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„åŒç²’åº¦æç¤ºæ¬ºè¯ˆæ£€æµ‹æ¡†æ¶",
      "authors": [
        "Yuan Li",
        "Jun Hu",
        "Bryan Hooi",
        "Bingsheng He",
        "Cheng Chen"
      ],
      "abstract": "Real-world fraud detection applications benefit from graph learning techniques that jointly exploit node features, often rich in textual data, and graph structural information. Recently, Graph-Enhanced LLMs emerge as a promising graph learning approach that converts graph information into prompts, exploiting LLMs' ability to reason over both textual and structural information. Among them, text-only prompting, which converts graph information to prompts consisting solely of text tokens, offers a solution that relies only on LLM tuning without requiring additional graph-specific encoders. However, text-only prompting struggles on heterogeneous fraud-detection graphs: multi-hop relations expand exponentially with each additional hop, leading to rapidly growing neighborhoods associated with dense textual information. These neighborhoods may overwhelm the model with long, irrelevant content in the prompt and suppress key signals from the target node, thereby degrading performance. To address this challenge, we propose Dual Granularity Prompting (DGP), which mitigates information overload by preserving fine-grained textual details for the target node while summarizing neighbor information into coarse-grained text prompts. DGP introduces tailored summarization strategies for different data modalities, bi-level semantic abstraction for textual fields and statistical aggregation for numerical features, enabling effective compression of verbose neighbor content into concise, informative prompts. Experiments across public and industrial datasets demonstrate that DGP operates within a manageable token budget while improving fraud detection performance by up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of Graph-Enhanced LLMs for fraud detection.",
      "tldr_zh": "é’ˆå¯¹Graph-Enhanced LLMsåœ¨å¤„ç†å¼‚æ„æ¬ºè¯ˆæ£€æµ‹å›¾æ—¶å› å¤šè·³å…³ç³»å¯¼è‡´çš„ä¿¡æ¯è¿‡è½½å’Œé•¿æ–‡æœ¬å¹²æ‰°é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Dual Granularity Prompting (DGP)æ¡†æ¶ã€‚DGPé€šè¿‡ä¸ºç›®æ ‡èŠ‚ç‚¹ä¿ç•™ç»†ç²’åº¦çš„Fine-grainedæ–‡æœ¬ç»†èŠ‚ï¼ŒåŒæ—¶å°†é‚»å±…ä¿¡æ¯å‹ç¼©ä¸ºç²—ç²’åº¦çš„Coarse-grainedæ–‡æœ¬æç¤ºï¼Œæœ‰æ•ˆå¹³è¡¡äº†ä¿¡æ¯å®Œæ•´æ€§ä¸æ¨¡å‹è´Ÿè½½ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å®šåˆ¶åŒ–çš„æ‘˜è¦ç­–ç•¥ï¼ŒåŒ…æ‹¬é’ˆå¯¹æ–‡æœ¬å­—æ®µçš„åŒå±‚è¯­ä¹‰æŠ½è±¡(Bi-level semantic abstraction)ä»¥åŠé’ˆå¯¹æ•°å€¼ç‰¹å¾çš„ç»Ÿè®¡èšåˆ(Statistical aggregation)ï¼Œæ—¨åœ¨å°†å†—é•¿çš„é‚»å±…å†…å®¹è½¬åŒ–ä¸ºç®€æ´ä¸”å…·æœ‰ä¿¡æ¯é‡çš„æç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDGPåœ¨å…¬å¼€å’Œå·¥ä¸šæ•°æ®é›†ä¸Šçš„æ¬ºè¯ˆæ£€æµ‹æ€§èƒ½è¾ƒç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†é«˜è¾¾6.8%çš„AUPRCã€‚è¯¥ç ”ç©¶è¯æ˜äº†DGPèƒ½åœ¨ç»´æŒåˆç†Tokené¢„ç®—çš„åŒæ—¶æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºGraph-Enhanced LLMsåœ¨å¤æ‚å›¾åœºæ™¯ä¸‹çš„åº”ç”¨æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21653v1",
      "published_date": "2025-07-29 10:10:47 UTC",
      "updated_date": "2025-07-29 10:10:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:59.445427+00:00"
    },
    {
      "arxiv_id": "2507.21640v1",
      "title": "GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN Anomaly Detection",
      "title_zh": "GUARD-CANï¼šé¢å‘ CAN å¼‚å¸¸æ£€æµ‹çš„å›¾ç†è§£ä¸å¾ªç¯æ¶æ„",
      "authors": [
        "Hyeong Seon Kim",
        "Huy Kang Kim"
      ],
      "abstract": "Modern in-vehicle networks face various cyber threats due to the lack of encryption and authentication in the Controller Area Network (CAN). To address this security issue, this paper presents GUARD-CAN, an anomaly detection framework that combines graph-based representation learning with time-series modeling. GUARD-CAN splits CAN messages into fixed-length windows and converts each window into a graph that preserves message order. To detect anomalies in the timeaware and structure-aware context at the same window, GUARD-CAN takes advantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network (GCN) to generate graph embedding vectors. The model groups these vectors into sequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporal anomaly patterns across the graphs. GUARD-CAN performs anomaly detection at both the sequence level and the window level, and this allows multi-perspective performance evaluation. The model also verifies the importance of window size selection through an analysis based on Shannon entropy. As a result, GUARD-CAN shows that the proposed model detects four types of CAN attacks (flooding, fuzzing, replay and spoofing attacks) effectively without relying on complex feature engineering.",
      "tldr_zh": "é’ˆå¯¹ç°ä»£è½¦è½½ç½‘ç»œç”±äº Controller Area Network (CAN) ç¼ºä¹åŠ å¯†å’Œèº«ä»½éªŒè¯è€Œé¢ä¸´çš„ç½‘ç»œå¨èƒï¼Œè¯¥ç ”ç©¶æå‡ºäº† GUARD-CAN å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°† CAN æ¶ˆæ¯åˆ’åˆ†ä¸ºå›ºå®šé•¿åº¦çš„æ—¶é—´çª—å¹¶è½¬åŒ–ä¸ºä¿ç•™é¡ºåºçš„å›¾ç»“æ„ï¼Œå®ç°äº†å›¾è¡¨ç¤ºå­¦ä¹ ä¸æ—¶é—´åºåˆ—å»ºæ¨¡çš„æœ‰æ•ˆç»“åˆã€‚åœ¨æ ¸å¿ƒæ¶æ„ä¸Šï¼ŒGUARD-CAN åˆ©ç”¨è¿‡åº¦å®Œå¤‡çš„ Autoencoder (AE) å’Œ Graph Convolutional Network (GCN) ç”Ÿæˆå›¾åµŒå…¥å‘é‡ï¼Œå¹¶é…åˆ Gated Recurrent Unit (GRU) æ•æ‰è·¨å›¾çš„åºåˆ—å¼‚å¸¸æ¨¡å¼ã€‚è¯¥æ¨¡å‹æ”¯æŒåºåˆ—çº§å’Œçª—å£çº§çš„å¤šè§†è§’æ€§èƒ½è¯„ä¼°ï¼Œå¹¶åŸºäº Shannon entropy åˆ†æéªŒè¯äº†çª—å£å¤§å°é€‰æ‹©å¯¹æ£€æµ‹æ•ˆæœçš„é‡è¦æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUARD-CAN æ— éœ€ä¾èµ–å¤æ‚çš„ç‰¹å¾å·¥ç¨‹ï¼Œå³å¯é«˜æ•ˆæ£€æµ‹ floodingã€fuzzingã€replay å’Œ spoofing å››ç±»å…¸å‹çš„ CAN æ”»å‡»ï¼Œæ˜¾è‘—æå‡äº†è½¦è½½ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Comments:12 pages, 3 figures, 3 tables; accepted to the 26th World Conference on Information Security Applications (WISA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.21640v1",
      "published_date": "2025-07-29 09:52:54 UTC",
      "updated_date": "2025-07-29 09:52:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:56.830574+00:00"
    },
    {
      "arxiv_id": "2507.21638v1",
      "title": "Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics",
      "title_zh": "Assistaxï¼šé¢å‘è¾…åŠ©æœºå™¨äººçš„ç¡¬ä»¶åŠ é€Ÿå¼ºåŒ–å­¦ä¹ åŸºå‡†",
      "authors": [
        "Leonard Hinckeldey",
        "Elliot Fosong",
        "Elle Miller",
        "Rimvydas Rubavicius",
        "Trevor McInroe",
        "Patricia Wollstadt",
        "Christiane B. Wiebel-Herboth",
        "Subramanian Ramamoorthy",
        "Stefano V. Albrecht"
      ],
      "abstract": "The development of reinforcement learning (RL) algorithms has been largely driven by ambitious challenge tasks and benchmarks. Games have dominated RL benchmarks because they present relevant challenges, are inexpensive to run and easy to understand. While games such as Go and Atari have led to many breakthroughs, they often do not directly translate to real-world embodied applications. In recognising the need to diversify RL benchmarks and addressing complexities that arise in embodied interaction scenarios, we introduce Assistax: an open-source benchmark designed to address challenges arising in assistive robotics tasks. Assistax uses JAX's hardware acceleration for significant speed-ups for learning in physics-based simulations. In terms of open-loop wall-clock time, Assistax runs up to $370\\times$ faster when vectorising training runs compared to CPU-based alternatives. Assistax conceptualises the interaction between an assistive robot and an active human patient using multi-agent RL to train a population of diverse partner agents against which an embodied robotic agent's zero-shot coordination capabilities can be tested. Extensive evaluation and hyperparameter tuning for popular continuous control RL and MARL algorithms provide reliable baselines and establish Assistax as a practical benchmark for advancing RL research for assistive robotics. The code is available at: https://github.com/assistive-autonomy/assistax.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Assistaxï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè§£å†³è¾…åŠ©æœºå™¨äºº(assistive robotics)ä»»åŠ¡æŒ‘æˆ˜è€Œè®¾è®¡çš„å¼€æºå¼ºåŒ–å­¦ä¹ (RL)åŸºå‡†æµ‹è¯•å¹³å°ã€‚Assistaxåˆ©ç”¨JAXçš„ç¡¬ä»¶åŠ é€ŸåŠŸèƒ½ï¼Œåœ¨åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿä¸­æ˜¾è‘—æå‡äº†å­¦ä¹ é€Ÿåº¦ï¼Œå…¶çŸ¢é‡åŒ–è®­ç»ƒè¿è¡Œé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„CPUæ–¹æ¡ˆå¿«è¾¾370å€ã€‚è¯¥åŸºå‡†é€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æŠ€æœ¯æ¨¡æ‹Ÿè¾…åŠ©æœºå™¨äººä¸ä¸»åŠ¨äººç±»æ‚£è€…ä¹‹é—´çš„äº¤äº’ï¼Œå¹¶è®­ç»ƒå‡ºå¤šæ ·åŒ–çš„ä¼™ä¼´æ™ºèƒ½ä½“ç¾¤ã€‚è¿™ä¸€è®¾å®šå…è®¸ç ”ç©¶è€…æµ‹è¯•å…·èº«æœºå™¨äººæ™ºèƒ½ä½“åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸‹çš„é›¶æ ·æœ¬åä½œ(zero-shot coordination)èƒ½åŠ›ã€‚é€šè¿‡å¯¹æµè¡Œè¿ç»­æ§åˆ¶RLå’ŒMARLç®—æ³•è¿›è¡Œå¹¿æ³›çš„è¯„ä¼°ä¸è¶…å‚æ•°è°ƒä¼˜ï¼ŒAssistaxä¸ºæ¨åŠ¨è¾…åŠ©æœºå™¨äººé¢†åŸŸçš„RLç ”ç©¶æä¾›äº†å¯é çš„åŸºå‡†çº¿å’Œå¼€æºä»£ç æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for the Coordination and Cooperation in Multi-Agent Reinforcement Learning Workshop at the Reinforcement Learning Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21638v1",
      "published_date": "2025-07-29 09:49:11 UTC",
      "updated_date": "2025-07-29 09:49:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:31:59.275752+00:00"
    },
    {
      "arxiv_id": "2507.21637v1",
      "title": "Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models",
      "title_zh": "è‡ªæ„ŸçŸ¥å®‰å…¨å¢å¼ºï¼šåˆ©ç”¨å†…éƒ¨è¯­ä¹‰ç†è§£æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
      "authors": [
        "Wanying Wang",
        "Zeyu Ma",
        "Han Zheng",
        "Xin Tan",
        "Mingang Chen"
      ],
      "abstract": "Large vision-language models (LVLMs) are vulnerable to harmful input compared to their language-only backbones. We investigated this vulnerability by exploring LVLMs internal dynamics, framing their inherent safety understanding in terms of three key capabilities. Specifically, we define these capabilities as safety perception, semantic understanding, and alignment for linguistic expression, and experimentally pinpointed their primary locations within the model architecture. The results indicate that safety perception often emerges before comprehensive semantic understanding, leading to the reduction in safety. Motivated by these findings, we propose \\textbf{Self-Aware Safety Augmentation (SASA)}, a technique that projects informative semantic representations from intermediate layers onto earlier safety-oriented layers. This approach leverages the model's inherent semantic understanding to enhance safety recognition without fine-tuning. Then, we employ linear probing to articulate the model's internal semantic comprehension to detect the risk before the generation process. Extensive experiments on various datasets and tasks demonstrate that SASA significantly improves the safety of LVLMs, with minimal impact on the utility.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨é¢å¯¹æœ‰å®³è¾“å…¥æ—¶çš„è„†å¼±æ€§ï¼Œå¹¶é€šè¿‡åˆ†ææ¨¡å‹çš„å†…éƒ¨åŠ¨æ€å°†å…¶å®‰å…¨ç†è§£èƒ½åŠ›åˆ’åˆ†ä¸ºå®‰å…¨æ„ŸçŸ¥(safety perception)ã€è¯­ä¹‰ç†è§£(semantic understanding)å’Œè¯­è¨€è¡¨è¾¾å¯¹é½(alignment for linguistic expression)ä¸‰ä¸ªç»´åº¦ã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹çš„å®‰å…¨æ„ŸçŸ¥å¾€å¾€åœ¨å…¨é¢çš„è¯­ä¹‰ç†è§£ä¹‹å‰å‡ºç°ï¼Œè¿™ç§æ—¶é—´å·®å¯¼è‡´äº†å®‰å…¨æ€§èƒ½çš„ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†è‡ªæˆ‘æ„è¯†å®‰å…¨å¢å¼º(Self-Aware Safety Augmentation, SASA)æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ— éœ€å¾®è°ƒï¼Œé€šè¿‡å°†ä¸­é—´å±‚çš„ä¿¡æ¯è¯­ä¹‰è¡¨ç¤ºæŠ•å½±åˆ°æ—©æœŸçš„å®‰å…¨å¯¼å‘å±‚ï¼Œåˆ©ç”¨æ¨¡å‹å›ºæœ‰çš„è¯­ä¹‰ç†è§£èƒ½åŠ›æ¥å¢å¼ºå®‰å…¨è¯†åˆ«ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨çº¿æ€§æ¢é’ˆ(linear probing)åœ¨ç”Ÿæˆè¿‡ç¨‹å¼€å§‹å‰æ¢æµ‹æ¨¡å‹çš„å†…éƒ¨è¯­ä¹‰ç†è§£ï¼Œä»¥æå‰è¯†åˆ«é£é™©ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒSASAåœ¨å‡ ä¹ä¸å½±å“æ¨¡å‹æ•ˆç”¨(utility)çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†LVLMsçš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACM Multimedia 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21637v1",
      "published_date": "2025-07-29 09:48:57 UTC",
      "updated_date": "2025-07-29 09:48:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:10.394179+00:00"
    },
    {
      "arxiv_id": "2507.21636v1",
      "title": "StaffPro: an LLM Agent for Joint Staffing and Profiling",
      "title_zh": "StaffProï¼šä¸€ç§ç”¨äºè”åˆäººå‘˜é…ç½®ä¸ç”»åƒçš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Alessio Maritan"
      ],
      "abstract": "Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†StaffProï¼Œä¸€ä¸ªæ—¨åœ¨åŒæ—¶è§£å†³åŠ³åŠ¨åŠ›ç®¡ç†ä¸­äººå‘˜é…ç½®(staffing)ä¸ç”»åƒåˆ†æ(profiling)ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜çš„LLM Agentæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä»»åŠ¡åˆ†é…ã€è°ƒåº¦ä¸åŸºäºéç»“æ„åŒ–æ•°æ®çš„å‘˜å·¥æŠ€èƒ½åŠåå¥½ä¼°è®¡æ•´åˆè¿›ä¸€ä¸ªå½¢å¼åŒ–çš„æ•°å­¦æ¡†æ¶ä¸­ï¼Œå®ç°äº†äººå‘˜é…ç½®ä¸ç”»åƒåˆ†æçš„ååŒä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿæ–¹æ¡ˆä¸åŒï¼ŒStaffProæ”¯æŒä½¿ç”¨è‡ªç„¶è¯­è¨€å®šä¹‰ä¼˜åŒ–ç›®æ ‡ï¼Œå¹¶å»ºç«‹äº†æŒç»­çš„äººæœºåé¦ˆç¯(human-agent feedback loop)ä»¥å®ç°ç»ˆèº«åŒ–çš„å‘˜å·¥ç”»åƒæ›´æ–°ã€‚é€šè¿‡åœ¨å’¨è¯¢å…¬å¸åœºæ™¯ä¸‹çš„ä»¿çœŸå®éªŒï¼ŒStaffProè¯æ˜äº†å…¶èƒ½å¤Ÿå‡†ç¡®ä¼°è®¡å‘˜å·¥å±æ€§å¹¶ç”Ÿæˆé«˜è´¨é‡çš„ä»»åŠ¡æ’æœŸã€‚è¯¥ç³»ç»Ÿä¸ä»…æå‡äº†è‡ªåŠ¨åŒ–äººäº‹ç®¡ç†çš„æ•ˆç‡ï¼Œè¿˜ä¸ºå¤æ‚çš„äººåŠ›èµ„æºå†³ç­–æä¾›äº†å…·å¤‡é²æ£’æ€§ã€å¯è§£é‡Šæ€§ä¸”ä»¥äººä¸ºä¸­å¿ƒ(human-centric)çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21636v1",
      "published_date": "2025-07-29 09:48:54 UTC",
      "updated_date": "2025-07-29 09:48:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:14.349791+00:00"
    },
    {
      "arxiv_id": "2507.21631v1",
      "title": "\"Teammates, Am I Clear?\": Analysing Legible Behaviours in Teams",
      "title_zh": "â€œé˜Ÿå‹ä»¬ï¼Œæˆ‘è¡¨è¾¾æ¸…æ¥šäº†å—ï¼Ÿâ€ï¼šå›¢é˜Ÿä¸­æ˜“è¯»è¡Œä¸ºåˆ†æ",
      "authors": [
        "Miguel Faria",
        "Francisco S. Melo",
        "Ana Paiva"
      ],
      "abstract": "In this paper we investigate the notion of legibility in sequential decision-making in the context of teams and teamwork. There have been works that extend the notion of legibility to sequential decision making, for deterministic and for stochastic scenarios. However, these works focus on one agent interacting with one human, foregoing the benefits of having legible decision making in teams of agents or in team configurations with humans. In this work we propose an extension of legible decision-making to multi-agent settings that improves the performance of agents working in collaboration. We showcase the performance of legible decision making in team scenarios using our proposed extension in multi-agent benchmark scenarios. We show that a team with a legible agent is able to outperform a team composed solely of agents with standard optimal behaviour.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¢é˜Ÿåä½œèƒŒæ™¯ä¸‹åºåˆ—å†³ç­–(sequential decision-making)ä¸­çš„å¯è§£é‡Šæ€§(legibility)æ¦‚å¿µã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ™ºèƒ½ä½“ä¸å•äººçš„äº¤äº’ï¼Œå¿½è§†äº†åœ¨å¤šæ™ºèƒ½ä½“å›¢é˜Ÿæˆ–äººæœºå›¢é˜Ÿé…ç½®ä¸­åº”ç”¨è¯¥æŠ€æœ¯çš„æ½œåœ¨ç›Šå¤„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¯è§£é‡Šå†³ç­–æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent settings)çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œæ€§èƒ½ã€‚ç ”ç©¶é€šè¿‡åœ¨å¤šä¸ªå¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå±•ç¤ºäº†è¯¥æ‰©å±•æ–¹æ³•çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒ…å«å¯è§£é‡Šæ™ºèƒ½ä½“çš„å›¢é˜Ÿåœ¨æ•´ä½“è¡¨ç°ä¸Šèƒ½å¤Ÿä¼˜äºä»…ç”±æ‰§è¡Œæ ‡å‡†æœ€ä¼˜è¡Œä¸º(standard optimal behaviour)æ™ºèƒ½ä½“ç»„æˆçš„å›¢é˜Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21631v1",
      "published_date": "2025-07-29 09:40:18 UTC",
      "updated_date": "2025-07-29 09:40:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:19.087096+00:00"
    },
    {
      "arxiv_id": "2507.22090v1",
      "title": "Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization",
      "title_zh": "æ·±åº¦ç¥ç»ç½‘ç»œçš„æ··åˆæ¿€æ´»å‡½æ•° S3 ä¸ S4ï¼šä¸€ç§æ¢¯åº¦æµä¼˜åŒ–çš„æ–°é¢–æ–¹æ³•",
      "authors": [
        "Sergii Kavun"
      ],
      "abstract": "Activation functions are critical components in deep neural networks, directly influencing gradient flow, training stability, and model performance. Traditional functions like ReLU suffer from dead neuron problems, while sigmoid and tanh exhibit vanishing gradient issues. We introduce two novel hybrid activation functions: S3 (Sigmoid-Softsign) and its improved version S4 (smoothed S3). S3 combines sigmoid for negative inputs with softsign for positive inputs, while S4 employs a smooth transition mechanism controlled by a steepness parameter k. We conducted comprehensive experiments across binary classification, multi-class classification, and regression tasks using three different neural network architectures. S4 demonstrated superior performance compared to nine baseline activation functions, achieving 97.4% accuracy on MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression. The function exhibited faster convergence (-19 for ReLU) and maintained stable gradient flow across network depths. Comparative analysis revealed S4's gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep networks. The S4 activation function addresses key limitations of existing functions through its hybrid design and smooth transition mechanism. The tunable parameter k allows adaptation to different tasks and network depths, making S4 a versatile choice for deep learning applications. These findings suggest that hybrid activation functions represent a promising direction for improving neural network training dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸¤ç§æ–°å‹æ··åˆæ¿€æ´»å‡½æ•° (Hybrid activation functions) S3 (Sigmoid-Softsign) åŠå…¶æ”¹è¿›ç‰ˆæœ¬ S4ï¼Œæ—¨åœ¨ä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æµå¹¶å…‹æœ ReLU çš„ç¥ç»å…ƒæ­»äº¡ (dead neuron) ä»¥åŠ Sigmoid å’Œ Tanh çš„æ¢¯åº¦æ¶ˆå¤± (vanishing gradient) é—®é¢˜ã€‚S3 å°†è´Ÿè¾“å…¥çš„ Sigmoid ä¸æ­£è¾“å…¥çš„ Softsign ç›¸ç»“åˆï¼Œè€Œ S4 è¿›ä¸€æ­¥å¼•å…¥äº†ç”±å‚æ•° k æ§åˆ¶çš„å¹³æ»‘è½¬æ¢æœºåˆ¶ï¼Œç¡®ä¿åœ¨ä¸åŒç½‘ç»œæ·±åº¦ä¸‹ç»´æŒç¨³å®šçš„æ¢¯åº¦æµã€‚åœ¨ MNIST åˆ†ç±»ã€Iris åˆ†ç±»åŠæ³¢å£«é¡¿æˆ¿ä»·å›å½’ç­‰å¤šé¡¹ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒS4 çš„æ€§èƒ½ä¼˜äº ReLU ç­‰ä¹ç§åŸºå‡†å‡½æ•°ï¼Œåˆ†åˆ«è¾¾åˆ°äº† 97.4% çš„å‡†ç¡®ç‡å’Œè¾ƒä½çš„å‡æ–¹è¯¯å·® (MSE)ã€‚ç›¸æ¯” ReLU åœ¨æ·±åº¦ç½‘ç»œä¸­äº§ç”Ÿ 18% çš„æ­»ç¥ç»å…ƒï¼ŒS4 å±•ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œ [0.24, 0.59] èŒƒå›´å†…çš„ç¨³å®šæ¢¯åº¦ã€‚é€šè¿‡å¯è°ƒèŠ‚å‚æ•° kï¼ŒS4 å±•ç¤ºäº†æé«˜çš„é€šç”¨æ€§ä¸ä»»åŠ¡é€‚åº”èƒ½åŠ›ï¼Œä¸ºé€šè¿‡æ··åˆè®¾è®¡æ”¹å–„ç¥ç»ç½‘ç»œè®­ç»ƒåŠ¨åŠ›å­¦æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 2 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.22090v1",
      "published_date": "2025-07-29 09:21:57 UTC",
      "updated_date": "2025-07-29 09:21:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:28.788915+00:00"
    },
    {
      "arxiv_id": "2507.22089v1",
      "title": "Principled Curriculum Learning using Parameter Continuation Methods",
      "title_zh": "åŸºäºå‚æ•°å»¶æ‹“æ³•çš„è§„èŒƒåŒ–è¯¾ç¨‹å­¦ä¹ ",
      "authors": [
        "Harsh Nilesh Pathak",
        "Randy Paffenroth"
      ],
      "abstract": "In this work, we propose a parameter continuation method for the optimization of neural networks. There is a close connection between parameter continuation, homotopies, and curriculum learning. The methods we propose here are theoretically justified and practically effective for several problems in deep neural networks. In particular, we demonstrate better generalization performance than state-of-the-art optimization techniques such as ADAM for supervised and unsupervised learning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºç¥ç»ç½‘ç»œä¼˜åŒ–çš„ Parameter Continuation Methodsï¼Œæ¢è®¨äº†å‚æ•°è¿ç»­åŒ–ã€Homotopies ä¸ Curriculum Learning ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚è¯¥æ–¹æ³•å…·å¤‡åšå®çš„ç†è®ºæ”¯æ’‘ï¼Œå¹¶åœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„å¤šç§ä¼˜åŒ–åœºæ™¯ä¸­è¯æ˜äº†å…¶å®æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ¯” ADAM ç­‰ä¸»æµä¼˜åŒ–ç®—æ³•æ›´ä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡å°†è¯¾ç¨‹å­¦ä¹ å»ºç«‹åœ¨å‚æ•°è¿ç»­åŒ–çš„æ•°å­¦åŸºç¡€ä¸Šï¼Œæœ¬é¡¹å·¥ä½œä¸ºæé«˜æ¨¡å‹æ”¶æ•›è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„åŸåˆ™æ€§é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22089v1",
      "published_date": "2025-07-29 08:49:22 UTC",
      "updated_date": "2025-07-29 08:49:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:47.895013+00:00"
    },
    {
      "arxiv_id": "2507.21591v2",
      "title": "Hierarchical Graph Neural Network for Compressed Speech Steganalysis",
      "title_zh": "é¢å‘å‹ç¼©è¯­éŸ³éšå†™åˆ†æçš„å±‚çº§å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Mustapha Hemis",
        "Hamza Kheddar",
        "Mohamed Chahine Ghanem",
        "Bachir Boudraa"
      ],
      "abstract": "Steganalysis methods based on deep learning (DL) often struggle with computational complexity and challenges in generalizing across different datasets. Incorporating a graph neural network (GNN) into steganalysis schemes enables the leveraging of relational data for improved detection accuracy and adaptability. This paper presents the first application of a Graph Neural Network (GNN), specifically the GraphSAGE architecture, for steganalysis of compressed voice over IP (VoIP) speech streams. The method involves straightforward graph construction from VoIP streams and employs GraphSAGE to capture hierarchical steganalysis information, including both fine grained details and high level patterns, thereby achieving high detection accuracy. Experimental results demonstrate that the developed approach performs well in uncovering quantization index modulation (QIM)-based steganographic patterns in VoIP signals. It achieves detection accuracy exceeding 98 percent even for short 0.5 second samples, and 95.17 percent accuracy under challenging conditions with low embedding rates, representing an improvement of 2.8 percent over the best performing state of the art methods. Furthermore, the model exhibits superior efficiency, with an average detection time as low as 0.016 seconds for 0.5-second samples an improvement of 0.003 seconds. This makes it efficient for online steganalysis tasks, providing a superior balance between detection accuracy and efficiency under the constraint of short samples with low embedding rates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ åœ¨å‹ç¼©è¯­éŸ³éšå†™åˆ†æä¸­é¢ä¸´çš„è®¡ç®—å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œé¦–æ¬¡æå‡ºäº†ä¸€ç§åŸºäº Graph Neural Network (GNN) çš„å±‚æ¬¡åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•å…·ä½“é‡‡ç”¨äº† GraphSAGE æ¶æ„ï¼Œé€šè¿‡ä» VoIP è¯­éŸ³æµä¸­æ„å»ºå›¾ç»“æ„æ¥æ•è·åŒ…æ‹¬ç»†ç²’åº¦ç»†èŠ‚å’Œé«˜å±‚æ¨¡å¼åœ¨å†…çš„å±‚æ¬¡åŒ–ç‰¹å¾ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æœ‰æ•ˆè¯†åˆ«å‹ç¼©è¯­éŸ³ä¸­çš„ Quantization Index Modulation (QIM) éšå†™æ¨¡å¼ï¼Œå¹¶åœ¨é«˜æ£€æµ‹å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ 0.5 ç§’çš„çŸ­æ ·æœ¬ä¸Šæ£€æµ‹å‡†ç¡®ç‡è¶…è¿‡ 98%ï¼Œè€Œåœ¨ä½åµŒå…¥ç‡çš„æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹å‡†ç¡®ç‡è¾¾åˆ° 95.17%ï¼Œæ¯”ç°æœ‰çš„ state of the art æ–¹æ³•æå‡äº† 2.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ•ˆç‡ï¼Œ0.5 ç§’æ ·æœ¬çš„å¹³å‡æ£€æµ‹æ—¶é—´ä»…ä¸º 0.016 ç§’ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ»¡è¶³åœ¨çº¿éšå†™åˆ†æä»»åŠ¡çš„éœ€æ±‚ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21591v2",
      "published_date": "2025-07-29 08:46:54 UTC",
      "updated_date": "2025-09-25 19:46:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:52.688071+00:00"
    },
    {
      "arxiv_id": "2507.21589v1",
      "title": "Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems",
      "title_zh": "æ¢ç©¶è´å¶æ–¯æ¨ç†ä¸å…·èº«æ™ºèƒ½çš„å…³è”ï¼šè¿ˆå‘å¼€æ”¾ç‰©ç†ä¸–ç•Œçš„å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Bin Liu"
      ],
      "abstract": "Embodied intelligence posits that cognitive capabilities fundamentally emerge from - and are shaped by - an agent's real-time sensorimotor interactions with its environment. Such adaptive behavior inherently requires continuous inference under uncertainty. Bayesian statistics offers a principled probabilistic framework to address this challenge by representing knowledge as probability distributions and updating beliefs in response to new evidence. The core computational processes underlying embodied intelligence - including perception, action selection, learning, and even higher-level cognition - can be effectively understood and modeled as forms of Bayesian inference. Despite the deep conceptual connection between Bayesian statistics and embodied intelligence, Bayesian principles have not been widely or explicitly applied in today's embodied intelligence systems. In this work, we examine both Bayesian and contemporary embodied intelligence approaches through two fundamental lenses: search and learning - the two central themes in modern AI, as highlighted in Rich Sutton's influential essay \"The Bitter Lesson\". This analysis sheds light on why Bayesian inference has not played a central role in the development of modern embodied intelligence. At the same time, it reveals that current embodied intelligence systems remain largely confined to closed-physical-world environments, and highlights the potential for Bayesian methods to play a key role in extending these systems toward truly open physical-world embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Bayesian Inferenceä¸Embodied Intelligenceä¹‹é—´çš„æ·±å±‚è”ç³»ï¼Œæ—¨åœ¨æ¨åŠ¨æ„å»ºé¢å‘å¼€æ”¾ç‰©ç†ä¸–ç•Œçš„Embodied AIç³»ç»Ÿã€‚æ–‡ç« æŒ‡å‡ºï¼Œå…·èº«æ™ºèƒ½çš„æ ¸å¿ƒè®¡ç®—è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€åŠ¨ä½œé€‰æ‹©å’Œå­¦ä¹ ï¼Œå‡å¯è¢«è§†ä¸ºåœ¨ä¸ç¡®å®šæ€§ä¸‹è¿›è¡Œæ¦‚ç‡å»ºæ¨¡çš„Bayesian Inferenceå½¢å¼ã€‚å°½ç®¡ä¸¤è€…åœ¨æ¦‚å¿µä¸Šé«˜åº¦å¥‘åˆï¼Œä½†BayesianåŸåˆ™åœ¨å½“ä»Šçš„å…·èº«æ™ºèƒ½ç³»ç»Ÿä¸­å°šæœªå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ä½œè€…å€Ÿé‰´Rich Suttonåœ¨ã€ŠThe Bitter Lessonã€‹ä¸­æå‡ºçš„â€œæœç´¢(search)â€ä¸â€œå­¦ä¹ (learning)â€ä¸¤å¤§æ ¸å¿ƒç»´åº¦ï¼Œæ·±å…¥å¯¹æ¯”åˆ†æäº†Bayesianæ–¹æ³•ä¸å½“ä»£å…·èº«æ™ºèƒ½è·¯å¾„çš„æ¼”è¿›ã€‚è¯¥åˆ†ææ­ç¤ºäº†Bayesian Inferenceæœªèƒ½æˆä¸ºç°ä»£å…·èº«æ™ºèƒ½ä¸»æµæŠ€æœ¯çš„åŸå› ï¼Œå¹¶æŒ‡å‡ºå½“å‰ç³»ç»Ÿå¤§å¤šä»å±€é™äºå°é—­ç‰©ç†ç¯å¢ƒã€‚æœ€åï¼Œè®ºæ–‡å¼ºè°ƒäº†Bayesianæ–¹æ³•åœ¨å¼•é¢†å…·èº«æ™ºèƒ½ç³»ç»Ÿè¿ˆå‘çœŸæ­£å¼€æ”¾ç‰©ç†ç¯å¢ƒä¸­çš„å…³é”®æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.21589v1",
      "published_date": "2025-07-29 08:43:16 UTC",
      "updated_date": "2025-07-29 08:43:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:51.093584+00:00"
    },
    {
      "arxiv_id": "2507.21588v1",
      "title": "Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning",
      "title_zh": "é’ˆå¯¹éŸ³è§†é¢‘å¤šä»»åŠ¡å¢é‡å­¦ä¹ çš„æ¸è¿›å¼ç¨³æ€ä¸å¡‘æ€§æç¤ºå¾®è°ƒ",
      "authors": [
        "Jiong Yin",
        "Liang Li",
        "Jiehua Zhang",
        "Yuhan Gao",
        "Chenggang Yan",
        "Xichun Sheng"
      ],
      "abstract": "Audio-visual multi-task incremental learning aims to continuously learn from multiple audio-visual tasks without the need for joint training on all tasks. The challenge of the problem is how to preserve the old task knowledge while facilitating the learning of new task with previous experiences. To address these challenges, we introduce a three-stage Progressive Homeostatic and Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the task-shared modality aggregating adapter to foster cross-task and cross-modal audio-visual representation learning to enhance shared understanding between tasks. In the middle phase, we propose the task-specific modality-shared dynamic generating adapter, which constructs prompts that are tailored to individual tasks while remaining general across modalities, which balances the models ability to retain knowledge against forgetting with its potential for versatile multi-task transferability. In the deep phase, we introduce the task-specific modality-independent prompts to further refine the understand ability by targeting individual information for each task and modality. By incorporating these three phases, PHP retains task-specific prompts while adapting shared parameters for new tasks to effectively balance knowledge sharing and specificity. Our method achieves SOTA performance in different orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at https://github.com/ENJOY-Yin-jiong/PHP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†å¬å¤šä»»åŠ¡å¢é‡å­¦ä¹ (Audio-Visual Multi-Task Incremental Learning)ä¸­æ—§çŸ¥è¯†ä¿ç•™ä¸æ–°ä»»åŠ¡å­¦ä¹ ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPHP(Progressive Homeostatic and Plastic audio-visual prompt)çš„ä¸‰é˜¶æ®µæ¸è¿›å¼æç¤ºå¾®è°ƒæ–¹æ³•ã€‚åœ¨æµ…å±‚é˜¶æ®µï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»»åŠ¡å…±äº«çš„æ¨¡æ€èšåˆé€‚é…å™¨(task-shared modality aggregating adapter)å¢å¼ºè·¨ä»»åŠ¡ä¸è·¨æ¨¡æ€çš„é€šç”¨è¡¨ç¤ºç†è§£ã€‚ä¸­å±‚é˜¶æ®µé€šè¿‡ä»»åŠ¡ç‰¹å®šä¸”æ¨¡æ€å…±äº«çš„åŠ¨æ€ç”Ÿæˆé€‚é…å™¨ï¼Œåœ¨ä¿ç•™å·²æœ‰çŸ¥è¯†ä¸ä¿ƒè¿›å¤šä»»åŠ¡è¿ç§»èƒ½åŠ›ä¹‹é—´å»ºç«‹å¹³è¡¡ã€‚æ·±å±‚é˜¶æ®µåˆ™å¼•å…¥ä»»åŠ¡ç‰¹å®šä¸”æ¨¡æ€ç‹¬ç«‹çš„æç¤º(task-specific modality-independent prompts)ï¼Œè¿›ä¸€æ­¥ç²¾ç»†åŒ–å¤„ç†å„ä»»åŠ¡ä¸æ¨¡æ€çš„ç‹¬ç«‹ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§ä¸‰é˜¶æ®µè®¾è®¡ï¼ŒPHPåœ¨è°ƒæ•´å…±äº«å‚æ•°ä»¥é€‚åº”æ–°ä»»åŠ¡çš„åŒæ—¶ï¼Œæœ‰æ•ˆä¿ç•™äº†ä»»åŠ¡ç‰¹æœ‰çš„çŸ¥è¯†ç‰¹å¼‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AVEã€AVVPã€AVSå’ŒAVQAå››é¡¹ä»»åŠ¡çš„å¤šç§å¢é‡é¡ºåºä¸‹å‡è¾¾åˆ°äº†å½“å‰çš„SOTAæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21588v1",
      "published_date": "2025-07-29 08:42:36 UTC",
      "updated_date": "2025-07-29 08:42:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:01.364066+00:00"
    },
    {
      "arxiv_id": "2507.21585v1",
      "title": "SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation",
      "title_zh": "SafeDriveRAGï¼šåŸºäºçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å®‰å…¨è‡ªåŠ¨é©¾é©¶",
      "authors": [
        "Hao Ye",
        "Mengshi Qi",
        "Zhaohong Liu",
        "Liang Liu",
        "Huadong Ma"
      ],
      "abstract": "In this work, we study how vision-language models (VLMs) can be utilized to enhance the safety for the autonomous driving system, including perception, situational understanding, and path planning. However, existing research has largely overlooked the evaluation of these models in traffic safety-critical driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K) and propose a new baseline based on VLM with knowledge graph-based retrieval-augmented generation (SafeDriveRAG) for visual question answering (VQA). Specifically, we introduce SafeDrive228K, the first large-scale multimodal question-answering benchmark comprising 228K examples across 18 sub-tasks. This benchmark encompasses a diverse range of traffic safety queries, from traffic accidents and corner cases to common safety knowledge, enabling a thorough assessment of the comprehension and reasoning abilities of the models. Furthermore, we propose a plug-and-play multimodal knowledge graph-based retrieval-augmented generation approach that employs a novel multi-scale subgraph retrieval algorithm for efficient information retrieval. By incorporating traffic safety guidelines collected from the Internet, this framework further enhances the model's capacity to handle safety-critical situations. Finally, we conduct comprehensive evaluations on five mainstream VLMs to assess their reliability in safety-sensitive driving tasks. Experimental results demonstrate that integrating RAG significantly improves performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five mainstream VLMs, underscoring the potential of our proposed benchmark and methodology for advancing research in traffic safety. Our source code and data are available at https://github.com/Lumos0507/SafeDriveRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¢å¼ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå®‰å…¨æ€§æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶é’ˆå¯¹ç°æœ‰ç ”ç©¶åœ¨äº¤é€šå®‰å…¨å…³é”®åœºæ™¯è¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†SafeDrive228KåŸºå‡†æµ‹è¯•å’ŒSafeDriveRAGæ¡†æ¶ã€‚SafeDrive228Kæ˜¯é¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€é—®ç­”åŸºå‡†ï¼ŒåŒ…å«18ä¸ªå­ä»»åŠ¡çš„22.8ä¸‡ä¸ªç¤ºä¾‹ï¼Œæ¶µç›–äº†äº¤é€šäº‹æ•…ã€é•¿å°¾åœºæ™¯(Corner Cases)åŠå®‰å…¨å¸¸è¯†ç­‰é¢†åŸŸã€‚SafeDriveRAGé‡‡ç”¨åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Knowledge Graph-based RAG)æŠ€æœ¯ï¼Œé€šè¿‡åˆ›æ–°çš„å¤šå°ºåº¦å­å›¾æ£€ç´¢ç®—æ³•é«˜æ•ˆæå–äº’è”ç½‘æ”¶é›†çš„äº¤é€šå®‰å…¨å‡†åˆ™ä¿¡æ¯ï¼Œå¢å¼ºæ¨¡å‹å¤„ç†å®‰å…¨å…³é”®æƒ…å†µçš„èƒ½åŠ›ã€‚å¯¹äº”ç§ä¸»æµVLMsçš„è¯„ä¼°è¡¨æ˜ï¼Œé›†æˆRAGæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œåœ¨äº¤é€šäº‹æ•…ã€é•¿å°¾åœºæ™¯å’Œå®‰å…¨å¸¸è¯†ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº†4.73%ã€8.79%å’Œ14.57%çš„å‡†ç¡®ç‡å¢é•¿ã€‚è¯¥ç ”ç©¶ä¸ºæå‡è‡ªåŠ¨é©¾é©¶åœ¨å®‰å…¨æ•æ„Ÿä»»åŠ¡ä¸­çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„åŸºå‡†å’Œæ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21585v1",
      "published_date": "2025-07-29 08:40:17 UTC",
      "updated_date": "2025-07-29 08:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:32:58.358764+00:00"
    },
    {
      "arxiv_id": "2507.21571v1",
      "title": "Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations",
      "title_zh": "å¯»æ‰¾éå…±åŒåŸºç¡€ï¼šä¸€ç§ä»¥äººä¸ºä¸­å¿ƒçš„å¤–å¯Ÿå¼è§£é‡Šæ¨¡å‹",
      "authors": [
        "Laura Spillner",
        "Nima Zargham",
        "Mihai Pomarlan",
        "Robert Porzel",
        "Rainer Malaka"
      ],
      "abstract": "The need for explanations in AI has, by and large, been driven by the desire to increase the transparency of black-box machine learning models. However, such explanations, which focus on the internal mechanisms that lead to a specific output, are often unsuitable for non-experts. To facilitate a human-centered perspective on AI explanations, agents need to focus on individuals and their preferences as well as the context in which the explanations are given. This paper proposes a personalized approach to explanation, where the agent tailors the information provided to the user based on what is most likely pertinent to them. We propose a model of the agent's worldview that also serves as a personal and dynamic memory of its previous interactions with the same user, based on which the artificial agent can estimate what part of its knowledge is most likely new information to the user.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰AIè§£é‡Šä¾§é‡å†…éƒ¨æœºåˆ¶è€Œéš¾ä»¥è¢«éä¸“å®¶ç†è§£çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥äººä¸ºä¸­å¿ƒçš„Extrospective Explanationsæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ—¨åœ¨é€šè¿‡å…³æ³¨ä¸ªä½“åå¥½å’Œå…·ä½“è¯­å¢ƒï¼Œå®ç°è§£é‡Šä¿¡æ¯çš„ä¸ªæ€§åŒ–å®šåˆ¶ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§Agentçš„ä¸–ç•Œè§‚(worldview)æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…æ˜¯AgentçŸ¥è¯†çš„ä½“ç°ï¼Œè¿˜å……å½“äº†è®°å½•ä¸ç‰¹å®šç”¨æˆ·æ—¢å¾€äº¤äº’çš„ä¸ªäººåŠ¨æ€è®°å¿†ã€‚åŸºäºè¿™ä¸€æ¨¡å‹ï¼Œäººå·¥Agentèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°å…¶æŒæ¡çš„å“ªäº›çŸ¥è¯†å¯¹ç”¨æˆ·è€Œè¨€å±äºæ–°ä¿¡æ¯ï¼Œä»è€Œç­›é€‰å‡ºæœ€å…·ç›¸å…³æ€§çš„è§£é‡Šå†…å®¹ã€‚è¿™ç§æ–¹æ³•æ¨åŠ¨äº†AIè§£é‡Šä»å•çº¯æ­ç¤ºç®—æ³•é€»è¾‘å‘æ»¡è¶³ç”¨æˆ·è®¤çŸ¥éœ€æ±‚çš„è½¬å˜ï¼Œä¸ºæ„å»ºæ›´åŠ æ™ºèƒ½ä¸”äººæ€§åŒ–çš„äº¤äº’ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the IJCAI 2023 Workshop on Explainable Artificial Intelligence (XAI)",
      "pdf_url": "https://arxiv.org/pdf/2507.21571v1",
      "published_date": "2025-07-29 07:59:54 UTC",
      "updated_date": "2025-07-29 07:59:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:07.837133+00:00"
    },
    {
      "arxiv_id": "2507.21533v1",
      "title": "Model Predictive Adversarial Imitation Learning for Planning from Observation",
      "title_zh": "é¢å‘åŸºäºè§‚æµ‹è§„åˆ’çš„æ¨¡å‹é¢„æµ‹å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ",
      "authors": [
        "Tyler Han",
        "Yanda Bao",
        "Bhaumik Mehta",
        "Gabriel Guo",
        "Anubhav Vishwakarma",
        "Emily Kang",
        "Sanghun Jung",
        "Rosario Scalise",
        "Jason Zhou",
        "Bryan Xu",
        "Byron Boots"
      ],
      "abstract": "Human demonstration data is often ambiguous and incomplete, motivating imitation learning approaches that also exhibit reliable planning behavior. A common paradigm to perform planning-from-demonstration involves learning a reward function via Inverse Reinforcement Learning (IRL) then deploying this reward via Model Predictive Control (MPC). Towards unifying these methods, we derive a replacement of the policy in IRL with a planning-based agent. With connections to Adversarial Imitation Learning, this formulation enables end-to-end interactive learning of planners from observation-only demonstrations. In addition to benefits in interpretability, complexity, and safety, we study and observe significant improvements on sample efficiency, out-of-distribution generalization, and robustness. The study includes evaluations in both simulated control benchmarks and real-world navigation experiments using few-to-single observation-only demonstrations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Model Predictive Adversarial Imitation Learning (MPAIL)ï¼Œè¿™æ˜¯ä¸€ç§å°† Model Predictive Control (MPC) ä¸ Adversarial Imitation Learning (AIL) ç›¸ç»“åˆçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººç±»æ¼”ç¤ºæ•°æ®ä¸­å¸¸è§çš„æ­§ä¹‰å’Œä¸å®Œæ•´æ€§é—®é¢˜ã€‚é€šè¿‡åœ¨ Inverse Reinforcement Learning (IRL) ä¸­ç”¨åŸºäºè§„åˆ’çš„æ™ºèƒ½ä½“æ›¿æ¢ä¼ ç»Ÿçš„ç­–ç•¥å‡½æ•°ï¼Œè¯¥æ–¹æ³•å®ç°äº†åœ¨ä»…æœ‰è§‚æµ‹æ•°æ® (observation-only) æƒ…å†µä¸‹å¯¹è§„åˆ’å™¨çš„äº¤äº’å¼å­¦ä¹ ã€‚MPAIL åœ¨æå‡ç³»ç»Ÿå¯è§£é‡Šæ€§ã€å¤æ‚åº¦å’Œå®‰å…¨æ€§çš„åŒæ—¶ï¼Œåœ¨æ ·æœ¬æ•ˆç‡ (sample efficiency)ã€åˆ†å¸ƒå¤–æ³›åŒ– (out-of-distribution generalization) å’Œé²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœåœ¨æ¨¡æ‹Ÿæ§åˆ¶åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œå¯¼èˆªä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜è¯¥æ¡†æ¶å³ä¾¿åœ¨æå°‘æ•°ç”šè‡³å•æ¬¡æ¼”ç¤ºçš„æ¡ä»¶ä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Open-source code in process of being cleaned and documented for release. Please contact directly in the meantime for code. Under Review",
      "pdf_url": "https://arxiv.org/pdf/2507.21533v1",
      "published_date": "2025-07-29 06:52:52 UTC",
      "updated_date": "2025-07-29 06:52:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:08.546715+00:00"
    },
    {
      "arxiv_id": "2507.21532v1",
      "title": "Automatic Classification of User Requirements from Online Feedback -- A Replication Study",
      "title_zh": "åœ¨çº¿åé¦ˆä¸­ç”¨æˆ·éœ€æ±‚çš„è‡ªåŠ¨åˆ†ç±»â€”â€”ä¸€é¡¹å¤ç°æ€§ç ”ç©¶",
      "authors": [
        "Meet Bhatt",
        "Nic Boilard",
        "Muhammad Rehan Chaudhary",
        "Cole Thompson",
        "Jacob Idoko",
        "Aakash Sorathiya",
        "Gouri Ginde"
      ],
      "abstract": "Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), \"Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning\", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä¸€é¡¹åä¸ºâ€œä½¿ç”¨æ·±åº¦å­¦ä¹ åœ¨å°æ•°æ®é›†ç¯å¢ƒä¸‹åˆ†ç±»åœ¨çº¿åé¦ˆä¸­çš„ç”¨æˆ·éœ€æ±‚â€çš„ NLP4RE åŸºå‡†ç ”ç©¶è¿›è¡Œäº†å¤åˆ¶ä¸æ‰©å±•ï¼Œæ—¨åœ¨å¢å¼ºè¯¥é¢†åŸŸå®è¯ç ”ç©¶çš„å¤–éƒ¨æœ‰æ•ˆæ€§ã€‚ç ”ç©¶äººå‘˜é¦–å…ˆåˆ©ç”¨å…¬å¼€æºä»£ç å¤ç°äº†åŸå§‹ç»“æœï¼Œå‘ç°ä¸åŒæ¨¡å‹çš„ Reproducibility æ°´å¹³å„å¼‚ï¼Œå…¶ä¸­ Naive Bayes è¡¨ç°å‡ºå®Œç¾çš„å¯å¤ç°æ€§ï¼Œè€Œ BERT ç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ™è¡¨ç°ä¸ä¸€ã€‚éšåï¼Œç ”ç©¶é€šè¿‡åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å¼•å…¥ GPT-4o zero-shot åˆ†ç±»å™¨è¿›è¡Œå¯¹æ¯”ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†åŸæœ‰çš„å®éªŒè®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBERT å’Œ ELMo ç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œ GPT-4o çš„è¡¨ç°ä¸ä¼ ç»Ÿçš„åŸºå‡†æœºå™¨å­¦ä¹ æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä¸ºåŸºå‡†ç ”ç©¶å‡†å¤‡äº† Replication study ID-card ä»¥è¯„ä¼°å…¶å¤ç°å‡†å¤‡å°±ç»ªåº¦ï¼Œå¹¶æŒ‡å‡ºäº†ç¯å¢ƒé…ç½®æ–‡ä»¶ç¼ºå¤±å¯¹å¤ç°è¿‡ç¨‹çš„æ½œåœ¨å½±å“ã€‚é€šè¿‡æä¾›å®Œæ•´çš„å¤åˆ¶åŒ…å’Œ ID-cardï¼Œè¯¥å·¥ä½œä¸º NLP4RE é¢†åŸŸçš„åç»­ç ”ç©¶å’Œå¤ç°å®è·µæä¾›äº†é‡è¦çš„å®è¯æ”¯æŒä¸å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures, Replication package available at https://zenodo.org/records/15626782, Accepted at AIRE 2025 (12th International Workshop on Artificial Intelligence and Requirements Engineering)",
      "pdf_url": "https://arxiv.org/pdf/2507.21532v1",
      "published_date": "2025-07-29 06:52:27 UTC",
      "updated_date": "2025-07-29 06:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:10.656228+00:00"
    },
    {
      "arxiv_id": "2507.21524v1",
      "title": "Large Language Models for Wireless Communications: From Adaptation to Autonomy",
      "title_zh": "é¢å‘æ— çº¿é€šä¿¡çš„å¤§è¯­è¨€æ¨¡å‹ï¼šä»é€‚é…åˆ°è‡ªä¸»åŒ–",
      "authors": [
        "Le Liang",
        "Hao Ye",
        "Yucheng Sheng",
        "Ouya Wang",
        "Jiacheng Wang",
        "Shi Jin",
        "Geoffrey Ye Li"
      ],
      "abstract": "The emergence of large language models (LLMs) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless communications, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of LLMs in transforming wireless systems across three key directions: adapting pretrained LLMs for core communication tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic LLMs with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of LLM-based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨æ— çº¿é€šä¿¡é¢†åŸŸä¸­çš„é©å‘½æ€§ä½œç”¨ï¼Œæ—¨åœ¨åˆ©ç”¨å…¶å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›åº”å¯¹æ—¥ç›Šå¤æ‚çš„é€šä¿¡ç³»ç»ŸæŒ‘æˆ˜ã€‚æ–‡ç« ç³»ç»Ÿåœ°åˆ†æäº† LLMs è½¬å‹æ— çº¿ç³»ç»Ÿçš„ä¸‰ä¸ªå…³é”®æ–¹å‘ï¼ŒåŒ…æ‹¬é€‚é…é¢„è®­ç»ƒæ¨¡å‹å¤„ç†æ ¸å¿ƒé€šä¿¡ä»»åŠ¡ã€å¼€å‘å…¼é¡¾é€šç”¨æ€§ä¸æ•ˆç‡çš„æ— çº¿é¢†åŸŸä¸“ç”¨åŸºç¡€æ¨¡å‹ (wireless-specific foundation models)ï¼Œä»¥åŠä½¿èƒ½å…·å¤‡è‡ªä¸»æ¨ç†ä¸åè°ƒèƒ½åŠ›çš„æ™ºèƒ½ä½“ LLMs (agentic LLMs)ã€‚é€šè¿‡å…·ä½“çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥ç»¼è¿°å±•ç¤ºäº† LLM é©±åŠ¨çš„æ–¹æ³•åœ¨é€‚åº”æ€§ä¸æ™ºèƒ½åŒ–æ–¹é¢ç›¸è¾ƒäºä¼ ç»ŸæŠ€æœ¯çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚æœ€åï¼Œæ–‡ç« æ˜ç¡®äº†å¤šæ¨¡æ€èåˆ (multimodal fusion)ã€ä¸è½»é‡åŒ–æ¨¡å‹ååŒä»¥åŠè‡ªæˆ‘æ¼”è¿›èƒ½åŠ›ç­‰æœªæ¥ç ”ç©¶æœºé‡ï¼Œä¸ºå®ç°æœªæ¥æ™ºèƒ½ã€è‡ªé€‚åº”ä¸”è‡ªä¸»çš„æ— çº¿ç½‘ç»œä½“ç³»ç»“æ„æä¾›äº†æŒ‡å¼•ã€‚",
      "categories": [
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21524v1",
      "published_date": "2025-07-29 06:21:10 UTC",
      "updated_date": "2025-07-29 06:21:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:10.848314+00:00"
    },
    {
      "arxiv_id": "2507.21518v2",
      "title": "ST-GDance: Long-Term and Collision-Free Group Choreography from Music",
      "title_zh": "ST-GDanceï¼šéŸ³ä¹é©±åŠ¨çš„é•¿æ—¶ã€æ— ç¢°æ’ç¾¤ä½“ç¼–èˆ",
      "authors": [
        "Jing Xu",
        "Weiqiang Wang",
        "Cunjian Chen",
        "Jun Liu",
        "Qiuhong Ke"
      ],
      "abstract": "Group dance generation from music has broad applications in film, gaming, and animation production. However, it requires synchronizing multiple dancers while maintaining spatial coordination. As the number of dancers and sequence length increase, this task faces higher computational complexity and a greater risk of motion collisions. Existing methods often struggle to model dense spatial-temporal interactions, leading to scalability issues and multi-dancer collisions. To address these challenges, we propose ST-GDance, a novel framework that decouples spatial and temporal dependencies to optimize long-term and collision-free group choreography. We employ lightweight graph convolutions for distance-aware spatial modeling and accelerated sparse attention for efficient temporal modeling. This design significantly reduces computational costs while ensuring smooth and collision-free interactions. Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms state-of-the-art baselines, particularly in generating long and coherent group dance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³ä¹é©±åŠ¨ç¾¤èˆç”Ÿæˆä¸­å­˜åœ¨çš„è®¡ç®—å¤æ‚åº¦é«˜å’Œèˆè€…ç¢°æ’ï¼ˆmulti-dancer collisionsï¼‰ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º ST-GDance çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£è€¦ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ï¼ˆspatial and temporal dependenciesï¼‰ï¼Œæ˜¾è‘—æå‡äº†é•¿æ—¶ä¸”æ— ç¢°æ’çš„ç¾¤èˆç¼–æ’æ•ˆæœã€‚ST-GDance åˆ©ç”¨è½»é‡çº§å›¾å·ç§¯ï¼ˆlightweight graph convolutionsï¼‰è¿›è¡Œè·ç¦»æ„ŸçŸ¥çš„ç©ºé—´å»ºæ¨¡ï¼Œå¹¶ç»“åˆåŠ é€Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆaccelerated sparse attentionï¼‰å®ç°é«˜æ•ˆçš„æ—¶é—´å»ºæ¨¡ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ç¡®ä¿äº†èˆè€…åŠ¨ä½œçš„å¹³æ»‘ä¸åè°ƒã€‚åœ¨ AIOZ-GDance æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒST-GDance åœ¨ç”Ÿæˆé•¿åºåˆ—å’Œä¿æŒç¾¤ä½“è¿è´¯æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ–¹æ³•ï¼Œä¸ºå¤§è§„æ¨¡ç¾¤èˆè‡ªåŠ¨åŒ–ç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 3 figures. Accepted at BMVC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.21518v2",
      "published_date": "2025-07-29 05:54:48 UTC",
      "updated_date": "2025-07-30 03:57:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:14.928025+00:00"
    },
    {
      "arxiv_id": "2507.21513v1",
      "title": "What Does it Mean for a Neural Network to Learn a \"World Model\"?",
      "title_zh": "ç¥ç»ç½‘ç»œå­¦ä¹ â€œä¸–ç•Œæ¨¡å‹â€æ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
      "authors": [
        "Kenneth Li",
        "Fernanda ViÃ©gas",
        "Martin Wattenberg"
      ],
      "abstract": "We propose a set of precise criteria for saying a neural net learns and uses a \"world model.\" The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent \"state space\" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a \"world model\" is not a trivial consequence of the neural net's data or task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œå­¦ä¹ å¹¶ä½¿ç”¨ \"world model\" çš„éæ­£å¼è¡¨è¿°æå‡ºäº†ä¸€å¥—ç²¾ç¡®çš„è¡¡é‡å‡†åˆ™ï¼Œæ—¨åœ¨é€šè¿‡æä¾›å¯æ“ä½œçš„å®šä¹‰ä¸ºå®éªŒç ”ç©¶å»ºç«‹é€šç”¨è¯­è¨€ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨äºå¯¹ä¸–ç•Œæ½œåµŒå…¥ \"state space\" çš„è¡¨ç¤ºï¼Œå°†è®¡ç®—è¿‡ç¨‹é€šè¿‡æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„è¡¨ç¤ºè¿›è¡Œåˆ†è§£ï¼ˆfactorï¼‰è¿™ä¸€æ¦‚å¿µè¿›è¡Œäº†å½¢å¼åŒ–ã€‚è¯¥å®šä¹‰åŸºäº \"linear probing\" ç›¸å…³æ–‡çŒ®çš„ç†å¿µï¼Œå¹¶å¼•å…¥äº†ä¸€ç³»åˆ—æ ¡éªŒæ¡ä»¶ï¼Œä»¥ç¡®ä¿è¯†åˆ«å‡ºçš„ \"world model\" å¹¶éç¥ç»ç½‘ç»œè®­ç»ƒæ•°æ®æˆ–ä»»åŠ¡æ‰€å¯¼è‡´çš„å¹³å‡¡ç»“æœï¼ˆtrivial consequenceï¼‰ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨ä¸åŒå®éªŒåœºæ™¯ä¸‹ç³»ç»Ÿæ€§åœ°éªŒè¯ç¥ç»ç½‘ç»œå†…éƒ¨æ˜¯å¦çœŸæ­£æ„å»ºäº†ç‰©ç†ä¸–ç•Œæ¨¡å‹æä¾›äº†åšå®çš„ç†è®ºæ¡†æ¶å’Œè¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21513v1",
      "published_date": "2025-07-29 05:30:57 UTC",
      "updated_date": "2025-07-29 05:30:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:36.792972+00:00"
    },
    {
      "arxiv_id": "2507.21506v2",
      "title": "Decision Transformer-Based Drone Trajectory Planning with Dynamic Safety-Efficiency Trade-Offs",
      "title_zh": "åŸºäº Decision Transformer çš„å…·å¤‡åŠ¨æ€å®‰å…¨ä¸æ•ˆç‡æƒè¡¡çš„æ— äººæœºè½¨è¿¹è§„åˆ’",
      "authors": [
        "Chang-Hun Ji",
        "SiWoon Song",
        "Youn-Hee Han",
        "SungTae Moon"
      ],
      "abstract": "A drone trajectory planner should be able to dynamically adjust the safety-efficiency trade-off according to varying mission requirements in unknown environments. Although traditional polynomial-based planners offer computational efficiency and smooth trajectory generation, they require expert knowledge to tune multiple parameters to adjust this trade-off. Moreover, even with careful tuning, the resulting adjustment may fail to achieve the desired trade-off. Similarly, although reinforcement learning-based planners are adaptable in unknown environments, they do not explicitly address the safety-efficiency trade-off. To overcome this limitation, we introduce a Decision Transformer-based trajectory planner that leverages a single parameter, Return-to-Go (RTG), as a \\emph{temperature parameter} to dynamically adjust the safety-efficiency trade-off. In our framework, since RTG intuitively measures the safety and efficiency of a trajectory, RTG tuning does not require expert knowledge. We validate our approach using Gazebo simulations in both structured grid and unstructured random environments. The experimental results demonstrate that our planner can dynamically adjust the safety-efficiency trade-off by simply tuning the RTG parameter. Furthermore, our planner outperforms existing baseline methods across various RTG settings, generating safer trajectories when tuned for safety and more efficient trajectories when tuned for efficiency. Real-world experiments further confirm the reliability and practicality of our proposed planner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœºåœ¨æœªçŸ¥ç¯å¢ƒä¸‹è½¨è¿¹è§„åˆ’ä¸­å®‰å…¨ä¸æ•ˆç‡æƒè¡¡ï¼ˆSafety-Efficiency Trade-Offï¼‰çš„åŠ¨æ€è°ƒæ•´é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Decision Transformer çš„è½¨è¿¹è§„åˆ’å™¨ã€‚ä¼ ç»Ÿçš„åŸºäºå¤šé¡¹å¼çš„è§„åˆ’å™¨éœ€è¦ä¸“å®¶ç»éªŒè°ƒæ•´å‚æ•°ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•åˆ™ç¼ºä¹å¯¹å®‰å…¨ä¸æ•ˆç‡æƒè¡¡çš„æ˜ç¡®å¤„ç†ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å¼•å…¥ Return-to-Go (RTG) ä½œä¸ºä¸€ç§æ¸©åº¦å‚æ•°ï¼ˆTemperature Parameterï¼‰ï¼Œä½¿å¾—ç”¨æˆ·æ— éœ€ä¸“ä¸šçŸ¥è¯†å³å¯é€šè¿‡è°ƒæ•´å•ä¸€å‚æ•°å®ç°å¯¹è½¨è¿¹å®‰å…¨æ€§å’Œæ•ˆç‡çš„åŠ¨æ€æ§åˆ¶ã€‚åœ¨ Gazebo ä»¿çœŸç¯å¢ƒå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼Œè¯¥è§„åˆ’å™¨åœ¨å¤šç§ RTG è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®éœ€æ±‚ç”Ÿæˆæ›´å®‰å…¨æˆ–æ›´é«˜æ•ˆçš„è½¨è¿¹ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç¯å¢ƒä»»åŠ¡æ—¶çš„å¯é æ€§ä¸å®ç”¨æ€§ï¼Œä¸ºæ— äººæœºè‡ªä¸»å¯¼èˆªæä¾›äº†æ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025. Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses",
      "pdf_url": "https://arxiv.org/pdf/2507.21506v2",
      "published_date": "2025-07-29 05:03:57 UTC",
      "updated_date": "2025-07-30 06:25:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:35.253869+00:00"
    },
    {
      "arxiv_id": "2507.21504v1",
      "title": "Evaluation and Benchmarking of LLM Agents: A Survey",
      "title_zh": "LLM æ™ºèƒ½ä½“è¯„ä¼°ä¸åŸºå‡†æµ‹è¯•ç»¼è¿°",
      "authors": [
        "Mahmoud Mohammadi",
        "Yipeng Li",
        "Jane Lo",
        "Wendy Yip"
      ],
      "abstract": "The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.",
      "tldr_zh": "æœ¬ç»¼è¿°é’ˆå¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“(Agent)è¯„ä¼°é¢†åŸŸè¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸå¤æ‚ä¸”å‘å±•ä¸è¶³çš„è¯„ä¼°éš¾é¢˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªäºŒç»´åˆ†ç±»æ³•(Taxonomy)ï¼Œä»è¯„ä¼°ç›®æ ‡(evaluation objectives)å’Œè¯„ä¼°è¿‡ç¨‹(evaluation process)ä¸¤ä¸ªç»´åº¦å¯¹ç°æœ‰å·¥ä½œè¿›è¡Œç»„ç»‡ã€‚åœ¨è¯„ä¼°ç›®æ ‡æ–¹é¢ï¼Œè¯¥æ¡†æ¶æ¶µç›–äº†æ™ºèƒ½ä½“è¡Œä¸ºã€èƒ½åŠ›ã€å¯é æ€§å’Œå®‰å…¨æ€§ç­‰å…³é”®æŒ‡æ ‡ï¼Œè€Œè¯„ä¼°è¿‡ç¨‹åˆ™æ¶‰åŠäº¤äº’æ¨¡å¼ã€æ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•(benchmarks)ã€æŒ‡æ ‡è®¡ç®—æ–¹æ³•ä»¥åŠå·¥å…·é“¾ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ç‰¹åˆ«å¼ºè°ƒäº†ä¼ä¸šçº§åº”ç”¨ä¸­å¸¸è¢«å¿½è§†çš„æŒ‘æˆ˜ï¼Œå¦‚åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ã€å¯é æ€§ä¿è¯ã€åŠ¨æ€é•¿æ—¶ç¨‹äº¤äº’ä»¥åŠåˆè§„æ€§è¦æ±‚ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºäº†æœªæ¥å‘æ›´å…·æ•´ä½“æ€§ã€ç°å®æ€§å’Œå¯æ‰©å±•æ€§è¯„ä¼°æ–¹å‘å‘å±•çš„è·¯å¾„ï¼Œä¸ºåœ¨ç°å®ä¸–ç•Œä¸­éƒ¨ç½²å’Œç³»ç»Ÿæ€§è¯„ä¼°æ™ºèƒ½ä½“æä¾›äº†æ¸…æ™°çš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21504v1",
      "published_date": "2025-07-29 04:57:02 UTC",
      "updated_date": "2025-07-29 04:57:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:35.946566+00:00"
    },
    {
      "arxiv_id": "2507.21503v4",
      "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions",
      "title_zh": "MoHoBenchï¼šé€šè¿‡ä¸å¯å›ç­”çš„è§†è§‰é—®é¢˜è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯šå®æ€§",
      "authors": [
        "Yanxu Zhu",
        "Shitong Duan",
        "Xiangxu Zhang",
        "Jitao Sang",
        "Peng Zhang",
        "Tun Lu",
        "Xiao Zhou",
        "Jing Yao",
        "Xiaoyuan Yi",
        "Xing Xie"
      ],
      "abstract": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MoHoBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¢å¯¹ä¸å¯å›ç­”è§†è§‰é—®é¢˜ï¼ˆunanswerable visual questionsï¼‰æ—¶è¯šå®æ€§è¡¨ç°çš„å¤§è§„æ¨¡åŸºå‡†ã€‚MoHoBench åŒ…å«è¶…è¿‡ 1.2 ä¸‡ä¸ªç»è¿‡äººå·¥éªŒè¯çš„æ ·æœ¬ï¼Œå®šä¹‰äº†å››ç§ä»£è¡¨æ€§çš„ä¸å¯å›ç­”é—®é¢˜ç±»å‹ï¼Œæ—¨åœ¨å¡«è¡¥ MLLMs è¯šå®æ€§è¯„ä¼°é¢†åŸŸçš„ç©ºç™½ã€‚é€šè¿‡å¯¹ 28 ä¸ªä¸»æµ MLLMs çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨å¿…è¦æ—¶æ— æ³•åšå‡ºé€‚å½“çš„æ‹’ç»ï¼Œä¸”æ¨¡å‹çš„è¯šå®æ€§è¡¨ç°æ·±å—è§†è§‰ä¿¡æ¯çš„å½±å“ï¼Œè€Œéå•çº¯çš„è¯­è¨€å»ºæ¨¡é—®é¢˜ã€‚ç ”ç©¶å¼ºè°ƒäº†å¼€å‘ä¸“é—¨çš„å¤šæ¨¡æ€è¯šå®æ€§å¯¹é½ï¼ˆmultimodal honesty alignmentï¼‰æ–¹æ³•çš„å¿…è¦æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…é€šè¿‡ç›‘ç£å­¦ä¹ å’Œåå¥½å­¦ä¹ ï¼ˆpreference learningï¼‰å®æ–½äº†åˆæ­¥çš„å¯¹é½æ”¹è¿›ï¼Œä¸ºæœªæ¥æ„å»ºå¯ä¿¡çš„ MLLMs å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI2026 Oral",
      "pdf_url": "https://arxiv.org/pdf/2507.21503v4",
      "published_date": "2025-07-29 04:55:49 UTC",
      "updated_date": "2026-01-13 13:07:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:36.441358+00:00"
    },
    {
      "arxiv_id": "2507.21502v1",
      "title": "Large Language Models for Supply Chain Decisions",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ä¾›åº”é“¾å†³ç­–ä¸­çš„åº”ç”¨",
      "authors": [
        "David Simchi-Levi",
        "Konstantina Mellou",
        "Ishai Menache",
        "Jeevan Pathuri"
      ],
      "abstract": "Supply Chain Management requires addressing a variety of complex decision-making challenges, from sourcing strategies to planning and execution. Over the last few decades, advances in computation and information technologies have enabled the transition from manual, intuition and experience-based decision-making, into more automated and data-driven decisions using a variety of tools that apply optimization techniques. These techniques use mathematical methods to improve decision-making.\n  Unfortunately, business planners and executives still need to spend considerable time and effort to (i) understand and explain the recommendations coming out of these technologies; (ii) analyze various scenarios and answer what-if questions; and (iii) update the mathematical models used in these tools to reflect current business environments. Addressing these challenges requires involving data science teams and/or the technology providers to explain results or make the necessary changes in the technology and hence significantly slows down decision making.\n  Motivated by the recent advances in Large Language Models (LLMs), we report how this disruptive technology can democratize supply chain technology - namely, facilitate the understanding of tools' outcomes, as well as the interaction with supply chain tools without human-in-the-loop. Specifically, we report how we apply LLMs to address the three challenges described above, thus substantially reducing the time to decision from days and weeks to minutes and hours as well as dramatically increasing planners' and executives' productivity and impact.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¾›åº”é“¾ç®¡ç†(Supply Chain Management)å†³ç­–ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¼˜åŒ–å·¥å…·åœ¨è§£é‡Šæ¨èç»“æœã€åœºæ™¯åˆ†æ(what-if questions)ä»¥åŠæ¨¡å‹æ›´æ–°æ–¹é¢å­˜åœ¨çš„å±€é™ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–æ•°æ®ç§‘å­¦å›¢é˜Ÿè¿›è¡Œç»“æœè§£é‡Šæˆ–æ¨¡å‹è°ƒæ•´ï¼Œå¯¼è‡´å†³ç­–è¿‡ç¨‹ç¼“æ…¢ä¸”æ•ˆç‡ä½ä¸‹ã€‚ç ”ç©¶æŠ¥å‘Šäº†å¦‚ä½•åˆ©ç”¨LLMså®ç°ä¾›åº”é“¾æŠ€æœ¯çš„å¹³æ°‘åŒ–(democratize)ï¼Œä»è€Œç®€åŒ–å†³ç­–è€…ä¸ä¾›åº”é“¾å·¥å…·ä¹‹é—´çš„äº¤äº’è¿‡ç¨‹ï¼Œå¹¶é™ä½å¯¹äººå·¥ç¯èŠ‚çš„ä¾èµ–ã€‚è¿™ç§åº”ç”¨ç›´æ¥åº”å¯¹äº†ä¸Šè¿°ä¸‰å¤§æŒ‘æˆ˜ï¼Œå°†å†³ç­–æ—¶é—´ä»åŸæœ¬çš„æ•°å‘¨æˆ–æ•°å¤©ç¼©çŸ­è‡³æ•°åˆ†é’Ÿæˆ–æ•°å°æ—¶ï¼Œæå¤§ç¨‹åº¦åœ°æé«˜äº†è®¡åˆ’äººå‘˜å’Œç®¡ç†å±‚çš„ç”Ÿäº§åŠ›ä¸å½±å“åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Forthcoming chapter in AI in Supply Chains: Perspectives from Global Thought Leaders, edited by Maxime C. Cohen and Tinglong Dai, and part of the Springer Series in Supply Chain Management (edited by Prof. Chris Tang)",
      "pdf_url": "https://arxiv.org/pdf/2507.21502v1",
      "published_date": "2025-07-29 04:50:27 UTC",
      "updated_date": "2025-07-29 04:50:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:40.598636+00:00"
    },
    {
      "arxiv_id": "2507.21500v1",
      "title": "VN-MTEB: Vietnamese Massive Text Embedding Benchmark",
      "title_zh": "VN-MTEBï¼šè¶Šå—è¯­å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†",
      "authors": [
        "Loc Pham",
        "Tung Luu",
        "Thu Vo",
        "Minh Nguyen",
        "Viet Hoang"
      ],
      "abstract": "Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†VN-MTEBï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¶Šå—è¯­çš„å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†(Vietnamese Massive Text Embedding Benchmark)ï¼Œæ—¨åœ¨è§£å†³è¶Šå—è¯­äººå·¥æ™ºèƒ½åº”ç”¨ä¸­ç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–è¯„ä¼°æ•°æ®é›†çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå…ˆè¿›åµŒå…¥æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡å¯¹å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†(Massive Text Embedding Benchmark, MTEB)çš„è‹±æ–‡æ ·æœ¬è¿›è¡Œç¿»è¯‘ä¸è´¨é‡è¿‡æ»¤ï¼Œåœ¨ä¿æŒè¯­è¨€è‡ªç„¶æµåˆ©åº¦çš„åŒæ—¶æœ‰æ•ˆä¿ç•™äº†å‘½åå®ä½“è¯†åˆ«(NER)å’Œä»£ç ç‰‡æ®µã€‚è¯¥åŸºå‡†åŒ…å«é’ˆå¯¹å…­ç±»ç‰¹å®šä»»åŠ¡çš„41ä¸ªæ•°æ®é›†ï¼Œä¸ºè¯„ä¼°è¶Šå—è¯­æ–‡æœ¬åµŒå…¥æ¨¡å‹æä¾›äº†å…¨é¢çš„æ”¯æ’‘ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œåœ¨åµŒå…¥ä»»åŠ¡ä¸­ï¼Œé‡‡ç”¨æ—‹è½¬ä½ç½®åµŒå…¥(Rotary Positional Embedding)çš„å¤æ‚æ¨¡å‹æ€§èƒ½ä¼˜äºé‡‡ç”¨ç»å¯¹ä½ç½®åµŒå…¥(Absolute Positional Embedding)çš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶é€šè¿‡åœ¨HuggingFaceå¼€æºæ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†ï¼Œä¸ºè¶Šå—è¯­æ–‡æœ¬å¤„ç†å’Œå†…å®¹ç®¡æ§æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages (including reference, appendix) 41 datasets from 6 tasks (retrieval, classification, pair-classification, clustering, rerank, sts) 7 figures, 16 tables, benchmark 18 text embedding models",
      "pdf_url": "https://arxiv.org/pdf/2507.21500v1",
      "published_date": "2025-07-29 04:48:55 UTC",
      "updated_date": "2025-07-29 04:48:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:54.355272+00:00"
    },
    {
      "arxiv_id": "2507.21488v1",
      "title": "Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess",
      "title_zh": "ä»¥æ›´å°‘æ•°æ®å­¦ä¹ æ¨¡ä»¿ï¼šå›½é™…è±¡æ£‹ä¸­çš„é«˜æ•ˆä¸ªä½“è¡Œä¸ºå»ºæ¨¡",
      "authors": [
        "Zhenwei Tang",
        "Difan Jiao",
        "Eric Xue",
        "Reid McIlroy-Young",
        "Jon Kleinberg",
        "Siddhartha Sen",
        "Ashton Anderson"
      ],
      "abstract": "As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly important. Chess, a long-standing AI benchmark with precise skill measurement, offers an ideal testbed for human-AI alignment. However, existing approaches to modeling human behavior require prohibitively large amounts of data from each individual, making them impractical for new or sparsely represented users. In this work, we introduce Maia4All, a framework designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this through a two-stage optimization process: (1) an enrichment step, which bridges population and individual-level human behavior modeling with a prototype-enriched model, and (2) a democratization step, which leverages ability levels or user prototypes to initialize and refine individual embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Maia4All achieves individual human behavior modeling in chess with only 20 games, compared to the 5,000 games required previously, representing a significant improvement in data efficiency. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype-enriched model as a bridge. This approach extends beyond chess, as shown in our case study on idiosyncratic LLMs, highlighting its potential for broader applications in personalized AI adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›½é™…è±¡æ£‹(Chess)é¢†åŸŸä¸­æ¨¡æ‹Ÿä¸ªä½“å†³ç­–è¡Œä¸ºå¯¹æ•°æ®é‡éœ€æ±‚è¿‡å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Maia4Allæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æå°‘é‡æ•°æ®é«˜æ•ˆæ•æ‰ç”¨æˆ·çš„ä¸ªæ€§åŒ–é£æ ¼ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ï¼šé¦–å…ˆé€šè¿‡å¯Œé›†æ­¥éª¤(Enrichment Step)åˆ©ç”¨åŸå‹å¯Œé›†æ¨¡å‹(Prototype-Enriched Model)è¿æ¥ç¾¤ä½“ä¸ä¸ªä½“è¡Œä¸ºï¼›éšååœ¨æ°‘ä¸»åŒ–æ­¥éª¤(Democratization Step)ä¸­ï¼Œåˆ©ç”¨ç©å®¶èƒ½åŠ›æ°´å¹³æˆ–åŸå‹åœ¨æœ‰é™æ•°æ®ä¸‹ç²¾è°ƒä¸ªä½“åµŒå…¥(Individual Embeddings)ã€‚å®éªŒè¯æ˜ï¼ŒMaia4Allä»…éœ€20å±€å¯¹å±€å³å¯å®ç°ç²¾ç¡®çš„ä¸ªä½“è¡Œä¸ºå»ºæ¨¡ï¼Œç›¸è¾ƒäºæ­¤å‰æ‰€éœ€çš„5,000å±€åœ¨æ•°æ®æ•ˆç‡ä¸Šå®ç°äº†è´¨çš„é£è·ƒã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä»¥é«˜ä¿çœŸåº¦é¢„æµ‹èµ°æ³•å¹¶åˆ»ç”»è¡Œä¸ºæ¨¡å¼ï¼Œä¸ºä¸ªæ€§åŒ–ç±»äººAI(Human-like AI)å»ºæ¨¡æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å¹¿æ³›çš„ä¸ªæ€§åŒ–AIé€‚é…ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21488v1",
      "published_date": "2025-07-29 04:09:31 UTC",
      "updated_date": "2025-07-29 04:09:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:33:49.743396+00:00"
    },
    {
      "arxiv_id": "2507.21485v1",
      "title": "HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions",
      "title_zh": "HLSDebuggerï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ HLS ä»£ç é€»è¾‘ç¼ºé™·è¯†åˆ«ä¸ä¿®å¤",
      "authors": [
        "Jing Wang",
        "Shang Liu",
        "Yao Lu",
        "Zhiyao Xie"
      ],
      "abstract": "High-level synthesis (HLS) accelerates hardware design by enabling the automatic translation of high-level descriptions into efficient hardware implementations. However, debugging HLS code is a challenging and labor-intensive task, especially for novice circuit designers or software engineers without sufficient hardware domain knowledge. The recent emergence of Large Language Models (LLMs) is promising in automating the HLS debugging process. Despite the great potential, three key challenges persist when applying LLMs to HLS logic debugging: 1) High-quality circuit data for training LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in hardware is inherently more complex than identifying software bugs with existing golden test cases. 3) The absence of reliable test cases requires multi-tasking solutions, performing both bug identification and correction. complicates the multi-tasking required for effective HLS debugging. In this work, we propose a customized solution named HLSDebugger to address the challenges. HLSDebugger first generates and releases a large labeled dataset with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts an encoder-decoder structure, performing bug location identification, bug type prediction, and bug correction with the same model. HLSDebugger significantly outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x in bug correction. It makes a substantial advancement in the exploration of automated debugging of HLS code.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ High-Level Synthesis (HLS) ä»£ç ä¸­é€»è¾‘é”™è¯¯è°ƒè¯•å›°éš¾åŠé«˜è´¨é‡ç”µè·¯è®­ç»ƒæ•°æ®ç¨€ç¼ºç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º HLSDebugger çš„å®šåˆ¶åŒ–è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³æ•°æ®ç“¶é¢ˆï¼Œè¯¥å·¥ä½œé¦–å…ˆå‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 30 ä¸‡ä¸ªé’ˆå¯¹ HLS é€»è¾‘é”™è¯¯çš„æ ‡æ³¨æ•°æ®é›†ã€‚HLSDebugger æ¨¡å‹é‡‡ç”¨ Encoder-Decoder ç»“æ„ï¼Œèƒ½å¤Ÿé€šè¿‡åŒä¸€ä¸ªæ¨¡å‹åŒæ­¥æ‰§è¡Œ Bug Location Identificationã€Bug Type Prediction å’Œ Bug Correction ä¸‰é¡¹å…³é”®ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHLSDebugger åœ¨é”™è¯¯è¯†åˆ«æ–¹é¢æ˜¾è‘—ä¼˜äº GPT-4 ç­‰å…ˆè¿›æ¨¡å‹ï¼Œåœ¨é”™è¯¯ä¿®æ­£æ€§èƒ½ä¸Šæ›´æ˜¯å®ç°äº† 3 å€ä»¥ä¸Šçš„æå‡ã€‚è¯¥ç ”ç©¶ä¸ºç¡¬ä»¶è®¾è®¡é¢†åŸŸçš„è‡ªåŠ¨åŒ–è°ƒè¯•æä¾›äº†é«˜æ•ˆçš„ Multi-tasking è·¯å¾„ï¼Œå¤§å¹…æ¨è¿›äº† HLS ä»£ç é€»è¾‘é”™è¯¯çš„è‡ªåŠ¨ä¿®å¤æŠ€æœ¯ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "This work has been accepted at ICCAD 2025 (International Conference on Computer-Aided Design)",
      "pdf_url": "https://arxiv.org/pdf/2507.21485v1",
      "published_date": "2025-07-29 03:59:19 UTC",
      "updated_date": "2025-07-29 03:59:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:04.589762+00:00"
    },
    {
      "arxiv_id": "2507.21483v2",
      "title": "NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples",
      "title_zh": "NCCRï¼šè¯„ä¼°ç¥ç»ç½‘ç»œä¸å¯¹æŠ—æ ·æœ¬é²æ£’æ€§çš„æŒ‡æ ‡",
      "authors": [
        "Shi Pu",
        "Fu Song",
        "Wenjie Wang"
      ],
      "abstract": "Neural networks have received a lot of attention recently, and related security issues have come with it. Many studies have shown that neural networks are vulnerable to adversarial examples that have been artificially perturbed with modification, which is too small to be distinguishable by human perception. Different attacks and defenses have been proposed to solve these problems, but there is little research on evaluating the robustness of neural networks and their inputs. In this work, we propose a metric called the neuron cover change rate (NCCR) to measure the ability of deep learning models to resist attacks and the stability of adversarial examples. NCCR monitors alterations in the output of specifically chosen neurons when the input is perturbed, and networks with a smaller degree of variation are considered to be more robust. The results of the experiment on image recognition and the speaker recognition model show that our metrics can provide a good assessment of the robustness of neural networks or their inputs. It can also be used to detect whether an input is adversarial or not, as adversarial examples are always less robust.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œæ˜“å—å¯¹æŠ—æ ·æœ¬(adversarial examples)æ”»å‡»çš„å®‰å…¨éšæ‚£ï¼ŒæŒ‡å‡ºç›®å‰ç¼ºä¹æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åŠè¾“å…¥é²æ£’æ€§(robustness)çš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºç¥ç»å…ƒè¦†ç›–å˜åŒ–ç‡(neuron cover change rate, NCCR)çš„æ–°å‹åº¦é‡æŒ‡æ ‡ï¼Œæ—¨åœ¨è¡¡é‡æ·±åº¦å­¦ä¹ æ¨¡å‹æŠµæŠ—æ”»å‡»çš„èƒ½åŠ›ä»¥åŠå¯¹æŠ—æ ·æœ¬çš„ç¨³å®šæ€§ã€‚NCCRé€šè¿‡ç›‘æµ‹è¾“å…¥å—åˆ°æ‰°åŠ¨æ—¶ç‰¹å®šç¥ç»å…ƒè¾“å‡ºçš„å˜åŒ–æ¥è¿›è¡Œè¯„ä¼°ï¼Œå˜åŠ¨ç¨‹åº¦è¶Šå°çš„ç½‘ç»œé€šå¸¸è¢«è®¤ä¸ºå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚åœ¨å›¾åƒè¯†åˆ«å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æŒ‡æ ‡èƒ½å¤Ÿä¸ºç¥ç»ç½‘ç»œåŠå…¶è¾“å…¥çš„é²æ£’æ€§æä¾›å‡†ç¡®è¯„ä»·ã€‚æ­¤å¤–ï¼Œç”±äºå¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§é€šå¸¸è¾ƒä½ï¼ŒNCCRè¿˜å¯ä»¥ä½œä¸ºæ£€æµ‹è¾“å…¥æ˜¯å¦ä¸ºå¯¹æŠ—æ ·æœ¬çš„æœ‰æ•ˆæ‰‹æ®µã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21483v2",
      "published_date": "2025-07-29 03:58:20 UTC",
      "updated_date": "2025-08-06 14:54:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:00.203065+00:00"
    },
    {
      "arxiv_id": "2507.21482v1",
      "title": "Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs",
      "title_zh": "æå‡å¤§è¯­è¨€æ¨¡å‹æ ‡ç­¾é«˜æ•ˆæœ‰ç›‘ç£å¾®è°ƒä¸­çš„ä»»åŠ¡å¤šæ ·æ€§",
      "authors": [
        "Abhinav Arabelly",
        "Jagrut Nemade",
        "Robert D Nowak",
        "Jifan Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\\%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥ä»»åŠ¡å¤šæ ·æ€§ï¼ˆtask-diversityï¼‰ä¸ºæ ¸å¿ƒåŸåˆ™çš„é«˜æ•ˆæ•°æ®é€‰æ‹©æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿä¾§é‡æç¤ºå¤šæ ·æ€§ï¼ˆprompt-diversityï¼‰çš„æ‰‹æ®µä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡è§‚å¯Ÿå‘ç°é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„ç½®ä¿¡åº¦ï¼ˆconfidenceï¼‰å­˜åœ¨å·®å¼‚ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†ä¸€ç§ç®€å•çš„é€†ç½®ä¿¡åº¦åŠ æƒé‡‡æ ·ç­–ç•¥ï¼ˆinverse confidence weighting strategyï¼‰ã€‚è¿™ç§ç­–ç•¥åœ¨å®ç°ä¸Šæ¯”å¤æ‚çš„é‡‡æ ·ç¨‹åºæ›´ä¸ºç®€ä¾¿ï¼Œä¸”è®¡ç®—å¼€é”€æ›´ä½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ MMLU ç­‰è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¿‡äº†å…¨é‡æ•°æ®é›†è®­ç»ƒçš„æ•ˆæœï¼Œå‡†ç¡®ç‡æå‡è¾¾ 4%ã€‚åœ¨å¤šç§æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œè¯¥ç®—æ³•èƒ½ä¿æŒæˆ–è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„æ°´å¹³ï¼ŒåŒæ—¶æœ€é«˜å¯èŠ‚çœ 80% çš„æ ‡æ³¨æˆæœ¬ï¼Œä¸ºå®ç°æ ‡ç­¾é«˜æ•ˆçš„å­¦ä¹ æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21482v1",
      "published_date": "2025-07-29 03:51:00 UTC",
      "updated_date": "2025-07-29 03:51:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:11.248217+00:00"
    },
    {
      "arxiv_id": "2507.21479v1",
      "title": "Capacity-Constrained Continual Learning",
      "title_zh": "å®¹é‡å—é™çš„æŒç»­å­¦ä¹ ",
      "authors": [
        "Zheng Wen",
        "Doina Precup",
        "Benjamin Van Roy",
        "Satinder Singh"
      ],
      "abstract": "Any agents we can possibly build are subject to capacity constraints, as memory and compute resources are inherently finite. However, comparatively little attention has been dedicated to understanding how agents with limited capacity should allocate their resources for optimal performance. The goal of this paper is to shed some light on this question by studying a simple yet relevant continual learning problem: the capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a solution to this problem under appropriate technical conditions. Moreover, for problems that can be decomposed into a set of sub-problems, we also demonstrate how to optimally allocate capacity across these sub-problems in the steady state. We view the results of this paper as a first step in the systematic theoretical study of learning under capacity constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½ä½“åœ¨å†…å­˜å’Œè®¡ç®—èµ„æºæœ‰é™çš„èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•ä¼˜åŒ–åˆ†é…èµ„æºä»¥å®ç°æœ€ä½³æ€§èƒ½çš„å®¹é‡å—é™æŒç»­å­¦ä¹  (Capacity-Constrained Continual Learning) é—®é¢˜ã€‚è®ºæ–‡é€šè¿‡ç ”ç©¶ä¸€ä¸ªç®€å•ä¸”å…·æœ‰ç›¸å…³æ€§çš„æ¨¡å‹ï¼Œå³å®¹é‡å—é™çš„çº¿æ€§äºŒæ¬¡é«˜æ–¯ (linear-quadratic-Gaussian, LQG) åºåˆ—é¢„æµ‹é—®é¢˜ï¼Œä¸ºè¯¥é¢†åŸŸæä¾›äº†ç†è®ºè§è§£ã€‚ç ”ç©¶è€…åœ¨æ»¡è¶³ç‰¹å®šæŠ€æœ¯æ¡ä»¶çš„æƒ…å†µä¸‹æ¨å¯¼å‡ºäº†è¯¥é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶è¿›ä¸€æ­¥å±•ç¤ºäº†åœ¨ç¨³æ€ (steady state) ä¸‹å¦‚ä½•å°†æœ‰é™å®¹é‡æœ€ä¼˜åœ°åˆ†é…ç»™ä¸€ç»„å¯åˆ†è§£çš„å­é—®é¢˜ã€‚è¯¥å·¥ä½œè¢«è§†ä¸ºç³»ç»Ÿæ€§ç ”ç©¶å®¹é‡å—é™å­¦ä¹ ç†è®ºçš„é‡è¦ç¬¬ä¸€æ­¥ï¼Œä¸ºæœªæ¥ç†è§£å’Œè®¾è®¡èµ„æºå—é™ç¯å¢ƒä¸‹çš„å­¦ä¹ æœºåˆ¶å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21479v1",
      "published_date": "2025-07-29 03:47:22 UTC",
      "updated_date": "2025-07-29 03:47:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:26.392869+00:00"
    },
    {
      "arxiv_id": "2507.21476v1",
      "title": "Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench",
      "title_zh": "å“ªäº›å¤§è¯­è¨€æ¨¡å‹èƒ½é¢†ä¼šå¹½é»˜ï¼ŸåŸºäº HumorBench çš„é STEM æ¨ç†èƒ½åŠ›æ¢ç©¶",
      "authors": [
        "Reuben Narad",
        "Siddharth Suresh",
        "Jiayi Chen",
        "Pine S. L. Dysart-Bricken",
        "Bob Mankoff",
        "Robert Nowak",
        "Jifan Zhang",
        "Lalit Jain"
      ],
      "abstract": "We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HumorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é STEM é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯å¯¹æ¼«ç”»æ ‡é¢˜ä¸­å¤æ‚å¹½é»˜çš„æ¨ç†å’Œè§£é‡Šèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç”±äºæ¨ç†æ¨¡å‹åœ¨æ•°å­¦å’Œç§‘å­¦ç­‰ç°æœ‰åŸºå‡†ä¸Šå·²è¶‹äºé¥±å’Œï¼Œè¯¥åŸºå‡†é€šè¿‡è¦æ±‚æ¨¡å‹è¯†åˆ«æ¼«ç”»ã€æ ‡é¢˜ã€å¤–éƒ¨æ–‡åŒ–å‚è€ƒåŠ wordplays ä¹‹é—´çš„å…³è”ï¼Œæä¾›äº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„é STEM æ™ºèƒ½è¯„ä¼°æ–¹å¼ã€‚HumorBench åŒ…å«çº¦ 300 å¯¹æ¥è‡ª New Yorker Caption Contest çš„æ¼«ç”»æ ‡é¢˜å¯¹ï¼Œå¹¶ç”±ä¸“å®¶æ ‡æ³¨è¯„ä¼°å‡†åˆ™ä»¥è¯†åˆ«æ ¸å¿ƒå¹½é»˜å…ƒç´ ã€‚æ¨¡å‹åœ¨å®Œæˆä»»åŠ¡æ—¶éœ€è¦æ„å»ºå‡è®¾å¹¶è¿›è¡Œ backtracking ä»¥å¯»æ‰¾æœ€åˆç†è§£é‡Šï¼Œè¿™ä½“ç°äº†å¤æ‚çš„è®¤çŸ¥æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMs åœ¨ STEM æ¨ç†æ–¹é¢çš„è¿›æ­¥å¯ä»¥æœ‰æ•ˆè¿ç§»è‡³å¹½é»˜ç†è§£ï¼Œä¸”ä»…åœ¨ STEM æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¾ç„¶å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†è¿ç§»æ€§ã€‚æœ€åï¼Œå®éªŒè¡¨æ˜é€šè¿‡å¢åŠ  thinking token é¢„ç®—è¿›è¡Œçš„ test-time scaling åœ¨ä¸åŒæ¨¡å‹çš„å¹½é»˜æ¨ç†è¡¨ç°ä¸Šå‘ˆç°å‡ºå·®å¼‚åŒ–çš„ç»“æœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21476v1",
      "published_date": "2025-07-29 03:44:43 UTC",
      "updated_date": "2025-07-29 03:44:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:44.495118+00:00"
    },
    {
      "arxiv_id": "2507.21474v1",
      "title": "Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning",
      "title_zh": "èµ«å¸ƒè®°å¿†å¢å¼ºå¾ªç¯ç½‘ç»œï¼šæ·±åº¦å­¦ä¹ ä¸­çš„è®°å¿†å°è¿¹ç¥ç»å…ƒ",
      "authors": [
        "Daniel Szelogowski"
      ],
      "abstract": "Despite success across diverse tasks, current artificial recurrent network architectures rely primarily on implicit hidden-state memories, limiting their interpretability and ability to model long-range dependencies. In contrast, biological neural systems employ explicit, associative memory traces (i.e., engrams) strengthened through Hebbian synaptic plasticity and activated sparsely during recall. Motivated by these neurobiological insights, we introduce the Engram Neural Network (ENN), a novel recurrent architecture incorporating an explicit, differentiable memory matrix with Hebbian plasticity and sparse, attention-driven retrieval mechanisms. The ENN explicitly models memory formation and recall through dynamic Hebbian traces, improving transparency and interpretability compared to conventional RNN variants. We evaluate the ENN architecture on three canonical benchmarks: MNIST digit classification, CIFAR-10 image sequence modeling, and WikiText-103 language modeling. Our empirical results demonstrate that the ENN achieves accuracy and generalization performance broadly comparable to classical RNN, GRU, and LSTM architectures, with all models converging to similar accuracy and perplexity on the large-scale WikiText-103 task. At the same time, the ENN offers significant enhancements in interpretability through observable memory dynamics. Hebbian trace visualizations further reveal biologically plausible, structured memory formation processes, validating the potential of neuroscience-inspired mechanisms to inform the development of more interpretable and robust deep learning models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Engram Neural Network (ENN)ï¼Œè¿™æ˜¯ä¸€ç§å—ç¥ç»ç”Ÿç‰©å­¦å¯å‘çš„å¾ªç¯æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿartificial recurrent networkè¿‡åº¦ä¾èµ–éšçŠ¶æ€(hidden-state memories)å¯¼è‡´çš„è§£é‡Šæ€§å·®å’Œé•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›æœ‰é™ç­‰é—®é¢˜ã€‚ENNå¼•å…¥äº†å…·æœ‰Hebbian plasticityçš„å¯å¾®åˆ†æ˜¾å¼è®°å¿†çŸ©é˜µï¼Œå¹¶ç»“åˆç¨€ç–çš„attention-driven retrievalæœºåˆ¶ï¼Œå®ç°äº†å¯¹è®°å¿†ç—•è¿¹(engrams)å½¢æˆä¸æ£€ç´¢è¿‡ç¨‹çš„åŠ¨æ€å»ºæ¨¡ã€‚é€šè¿‡åœ¨MNISTã€CIFAR-10åŠWikiText-103ç­‰åŸºå‡†ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜ENNåœ¨æ€§èƒ½ä¸Šä¸ä¼ ç»Ÿçš„RNNã€GRUå’ŒLSTMç›¸å½“ï¼Œåœ¨å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ”¶æ•›æ€§ã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼ŒENNé€šè¿‡å¯è§‚å¯Ÿçš„è®°å¿†åŠ¨æ€æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€æ˜åº¦ä¸è§£é‡Šæ€§ï¼Œå…¶Hebbian traceå¯è§†åŒ–æ­ç¤ºäº†å…·æœ‰ç”Ÿç‰©å­¦åˆç†æ€§çš„ç»“æ„åŒ–è®°å¿†å½¢æˆè¿‡ç¨‹ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†ç¥ç»ç§‘å­¦æœºåˆ¶åœ¨æ„å»ºæ›´ç¨³å¥ä¸”å¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "20 pages, 11 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.21474v1",
      "published_date": "2025-07-29 03:34:32 UTC",
      "updated_date": "2025-07-29 03:34:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:41.093282+00:00"
    },
    {
      "arxiv_id": "2508.09142v2",
      "title": "Bayesian-Driven Graph Reasoning for Active Radio Map Construction",
      "title_zh": "é¢å‘ä¸»åŠ¨æ— çº¿ç”µåœ°å›¾æ„å»ºçš„è´å¶æ–¯é©±åŠ¨å›¾æ¨ç†",
      "authors": [
        "Wenlihan Lu",
        "Shijian Gao",
        "Miaowen Wen",
        "Yuxuan Liang",
        "Liuqing Yang",
        "Chan-Byoung Chae",
        "H. Vincent Poor"
      ],
      "abstract": "With the emergence of the low-altitude economy, radio maps have become essential for ensuring reliable wireless connectivity to aerial platforms. Autonomous aerial agents are commonly deployed for data collection using waypoint-based navigation; however, their limited battery capacity significantly constrains coverage and efficiency. To address this, we propose an uncertainty-aware radio map (URAM) reconstruction framework that explicitly leverages graph-based reasoning tailored for waypoint navigation. Our approach integrates two key deep learning components: (1) a Bayesian neural network that estimates spatial uncertainty in real time, and (2) an attention-based reinforcement learning policy that performs global reasoning over a probabilistic roadmap, using uncertainty estimates to plan informative and energy-efficient trajectories. This graph-based reasoning enables intelligent, non-myopic trajectory planning, guiding agents toward the most informative regions while satisfying safety constraints. Experimental results show that URAM improves reconstruction accuracy by up to 34% over existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½ç©ºç»æµ(low-altitude economy)ä¸­æ— çº¿åœ°å›¾(radio maps)æ„å»ºé¢ä¸´çš„æ— äººæœºç”µæ± ç»­èˆªé™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†URAMï¼ˆUncertainty-aware Radio Mapï¼‰é‡å»ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºå›¾æ¨ç†(graph-based reasoning)çš„èˆªç‚¹å¯¼èˆªæŠ€æœ¯ï¼Œæ ¸å¿ƒåŒ…å«ä¸¤ä¸ªå…³é”®çš„æ·±åº¦å­¦ä¹ ç»„ä»¶ã€‚é¦–å…ˆï¼Œé€šè¿‡è´å¶æ–¯ç¥ç»ç½‘ç»œ(Bayesian neural network)å®æ—¶ä¼°è®¡ç©ºé—´ä¸ç¡®å®šæ€§(spatial uncertainty)ã€‚å…¶æ¬¡ï¼Œé‡‡ç”¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ (attention-based reinforcement learning)ç­–ç•¥ï¼Œåœ¨æ¦‚ç‡è·¯å›¾(probabilistic roadmap)ä¸Šè¿›è¡Œå…¨å±€æ¨ç†ï¼Œä»è€Œè§„åˆ’å‡ºä¿¡æ¯ä¸°å¯Œä¸”èŠ‚èƒ½çš„è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•å®ç°äº†æ™ºèƒ½ä¸”éè¿‘è§†çš„è·¯å¾„è§„åˆ’ï¼Œåœ¨ç¡®ä¿å®‰å…¨çº¦æŸçš„å‰æä¸‹æœ‰æ•ˆå¼•å¯¼æ— äººæœºè¦†ç›–é«˜ä¿¡æ¯åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒURAMåœ¨é‡å»ºç²¾åº¦ä¸Šæ¯”ç°æœ‰åŸºçº¿æ¨¡å‹æå‡äº†é«˜è¾¾34%ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†ä¸»åŠ¨æ— çº¿åœ°å›¾çš„æ„å»ºæ•ˆç‡ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.09142v2",
      "published_date": "2025-07-29 03:32:01 UTC",
      "updated_date": "2025-08-22 07:05:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:36.489247+00:00"
    },
    {
      "arxiv_id": "2508.05654v1",
      "title": "Comparison of Information Retrieval Techniques Applied to IT Support Tickets",
      "title_zh": "åº”ç”¨äºITæ”¯æŒå·¥å•çš„ä¿¡æ¯æ£€ç´¢æŠ€æœ¯å¯¹æ¯”",
      "authors": [
        "Leonardo Santiago Benitez Pereira",
        "Robinson Pizzio",
        "Samir Bonho"
      ],
      "abstract": "Institutions dependent on IT services and resources acknowledge the crucial significance of an IT help desk system, that act as a centralized hub connecting IT staff and users for service requests. Employing various Machine Learning models, these IT help desk systems allow access to corrective actions used in the past, but each model has different performance when applied to different datasets. This work compares eleven Information Retrieval techniques in a dataset of IT support tickets, with the goal of implementing a software that facilitates the work of Information Technology support analysts. The best results were obtained with the Sentence-BERT technique, in its multi-language variation distilluse-base-multilingual-cased-v1, where 78.7% of the recommendations made by the model were considered relevant. TF-IDF (69.0%), Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results. Furthermore, the used datasets and essential parts of coding have been published and made open source. It also demonstrated the practicality of a support ticket recovery system by implementing a minimal viable prototype, and described in detail the implementation of the system. Finally, this work proposed a novel metric for comparing the techniques, whose aim is to closely reflect the perception of the IT analysts about the retrieval quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ IT å¸®åŠ©å°ç³»ç»Ÿï¼Œå¯¹æ¯”äº† 11 ç§ä¿¡æ¯æ£€ç´¢ (Information Retrieval) æŠ€æœ¯åœ¨å¤„ç† IT æ”¯æŒå·¥å•æ•°æ®é›†æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶æ—¨åœ¨é€šè¿‡åŒ¹é…å†å²çº æ­£æªæ–½æ¥è¾…åŠ© IT æ”¯æŒåˆ†æå¸ˆçš„å·¥ä½œï¼Œæé«˜æœåŠ¡è¯·æ±‚çš„å¤„ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSentence-BERT æŠ€æœ¯åœ¨ distilluse-base-multilingual-cased-v1 å˜ä½“ä¸‹è¡¨ç°æœ€ä¼˜ï¼Œå…¶ç”Ÿæˆçš„æ¨èä¸­æœ‰ 78.7% è¢«è§†ä¸ºå…·æœ‰ç›¸å…³æ€§ã€‚åŒæ—¶ï¼ŒTF-IDF (69.0%)ã€Word2vec (68.7%) å’Œ LDA (66.3%) ç­‰æŠ€æœ¯ä¹Ÿå±•ç°äº†è¾ƒä¸ºä¸€è‡´ä¸”æœ‰æ•ˆçš„æ£€ç´¢ç»“æœã€‚ä½œè€…ä¸ä»…è¯¦ç»†æè¿°å¹¶å®ç°äº†ä¸€ä¸ªæœ€å°å¯è¡ŒåŸå‹ (minimal viable prototype) ç³»ç»Ÿï¼Œè¿˜å°†å…¶æ•°æ®é›†ä¸æ ¸å¿ƒä»£ç è¿›è¡Œäº†å¼€æºã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œæå‡ºäº†ä¸€ç§æ–°å‹è¯„ä»·æŒ‡æ ‡ï¼Œæ—¨åœ¨æ›´ç²¾å‡†åœ°åæ˜  IT åˆ†æå¸ˆå¯¹æ£€ç´¢è´¨é‡çš„å®é™…æ„ŸçŸ¥ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05654v1",
      "published_date": "2025-07-29 03:25:41 UTC",
      "updated_date": "2025-07-29 03:25:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:46.798191+00:00"
    },
    {
      "arxiv_id": "2507.21471v2",
      "title": "LUMIR: an LLM-Driven Unified Agent Framework for Multi-task Infrared Spectroscopy Reasoning",
      "title_zh": "LUMIRï¼šé¢å‘å¤šä»»åŠ¡çº¢å¤–å…‰è°±æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨ç»Ÿä¸€æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Zujie Xie",
        "Zixuan Chen",
        "Jiheng Liang",
        "Xiangyang Yu",
        "Ziru Yu"
      ],
      "abstract": "Infrared spectroscopy enables rapid, non destructive analysis of chemical and material properties, yet high dimensional signals and overlapping bands hinder conventional chemometric methods. Large language models (LLMs), with strong generalization and reasoning capabilities, offer new opportunities for automated spectral interpretation, but their potential in this domain remains largely untapped. This study introduces LUMIR (LLM-driven Unified agent framework for Multi-task Infrared spectroscopy Reasoning), an agent based framework designed to achieve accurate infrared spectral analysis under low data conditions. LUMIR integrates a structured literature knowledge base, automated preprocessing, feature extraction, and predictive modeling into a unified pipeline. By mining peer reviewed spectroscopy studies, it identifies validated preprocessing and feature derivation strategies, transforms spectra into low dimensional representations, and applies few-shot prompts for classification, regression, and anomaly detection. The framework was validated on diverse datasets, including the publicly available Milk near-infrared dataset, Chinese medicinal herbs, Citri Reticulatae Pericarpium(CRP) with different storage durations, an industrial wastewater COD dataset, and two additional public benchmarks, Tecator and Corn. Across these tasks, LUMIR achieved performance comparable to or surpassing established machine learning and deep learning models, particularly in resource limited settings. This work demonstrates that combining structured literature guidance with few-shot learning enables robust, scalable, and automated spectral interpretation. LUMIR establishes a new paradigm for applying LLMs to infrared spectroscopy, offering high accuracy with minimal labeled data and broad applicability across scientific and industrial domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LUMIRï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„ç»Ÿä¸€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä½æ•°æ®æ¡ä»¶ä¸‹çš„ç²¾ç¡®çº¢å¤–å…‰è°±åˆ†æã€‚è¯¥æ¡†æ¶æ•´åˆäº†ç»“æ„åŒ–æ–‡çŒ®çŸ¥è¯†åº“ã€è‡ªåŠ¨é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œé¢„æµ‹å»ºæ¨¡ï¼Œé€šè¿‡æŒ–æ˜åŒè¡Œè¯„å®¡çš„å…‰è°±å­¦ç ”ç©¶æ¥è¯†åˆ«éªŒè¯è¿‡çš„é¢„å¤„ç†å’Œç‰¹å¾è¡ç”Ÿç­–ç•¥ã€‚LUMIRå°†å…‰è°±è½¬åŒ–ä¸ºä½ç»´è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å°‘æ ·æœ¬æç¤º(Few-shot prompts)æ‰§è¡Œåˆ†ç±»ã€å›å½’å’Œå¼‚å¸¸æ£€æµ‹ç­‰å¤šé¡¹ä»»åŠ¡ã€‚åœ¨åŒ…æ‹¬ç‰›å¥¶è¿‘çº¢å¤–æ•°æ®é›†ã€ä¸­è‰è¯ã€ä¸åŒå‚¨å­˜å¹´é™çš„é™ˆçš®(CRP)ä»¥åŠå·¥ä¸šåºŸæ°´CODç­‰å¤šç§æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒLUMIRåœ¨èµ„æºå—é™è®¾ç½®ä¸‹çš„æ€§èƒ½ä¼˜äºæˆ–ç­‰åŒäºç°æœ‰çš„æœºå™¨å­¦ä¹ (Machine Learning)å’Œæ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡ç»“åˆæ–‡çŒ®æŒ‡å¯¼ä¸å°‘æ ·æœ¬å­¦ä¹ (Few-shot learning)ï¼Œä¸ºè‡ªåŠ¨åŒ–å…‰è°±è§£è¯»æä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„æ–°èŒƒå¼ï¼Œå±•ç°äº†LLMåœ¨çº¢å¤–å…‰è°±é¢†åŸŸå¹¿æ³›çš„å·¥ä¸šä¸ç§‘ç ”åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.21471v2",
      "published_date": "2025-07-29 03:20:51 UTC",
      "updated_date": "2025-08-31 04:55:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:52.195881+00:00"
    },
    {
      "arxiv_id": "2508.00904v1",
      "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling",
      "title_zh": "åŸºäºç¡¬ä»¶æ— å…³åˆ†æå»ºæ¨¡çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½é¢„æµ‹",
      "authors": [
        "Rajeev Patwari",
        "Ashish Sirasao",
        "Devleena Das"
      ],
      "abstract": "Large language models (LLMs) have been increasingly deployed as local agents on personal devices with CPUs, NPUs and integrated GPUs. However, forecasting inference performance on devices with such heterogeneity remains challenging due to the dynamic compute and memory demands. Existing approaches rely on GPU benchmarking or machine learning-based latency predictors, which are often hardware-specific and lack generalizability. To this end, we introduce LIFE, a lightweight and modular analytical framework that is comprised of modular analytical model of operators, configurable to characterize LLM inference workloads in a hardware and dataset-agnostic manner. LIFE characterizes the influence of software and model optimizations, such as quantization, KV cache compression, LoRA adapters, chunked prefill, different attentions, and operator fusion, on performance metrics such as time-to-first-token (TTFT), time-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables performance forecasting using only hardware specifications, such as TOPS and memory bandwidth, without requiring extensive dataset benchmarking. We validate LIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA V100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in forecasting LLM performance through lens of system efficiency to enable efficient LLM deployment across different hardware platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨å…·æœ‰å¼‚æ„æ€§çš„ä¸ªäººè®¾å¤‡ï¼ˆå¦‚CPUã€NPUå’Œé›†æˆGPUï¼‰ä¸Šéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶ï¼Œéš¾ä»¥å‡†ç¡®é¢„æµ‹æ¨ç†æ€§èƒ½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLIFEçš„è½»é‡çº§æ¨¡å—åŒ–åˆ†ææ¡†æ¶ã€‚LIFEé‡‡ç”¨ç¡¬ä»¶æ— å…³çš„åˆ†ææ¨¡å‹ï¼Œé€šè¿‡æ¨¡å—åŒ–çš„ç®—å­(operators)å»ºæ¨¡æ¥è¡¨å¾LLMæ¨ç†å·¥ä½œè´Ÿè½½ï¼Œé¿å…äº†å¯¹ç‰¹å®šç¡¬ä»¶åŸºå‡†æµ‹è¯•æˆ–æœºå™¨å­¦ä¹ é¢„æµ‹å™¨çš„ä¾èµ–ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿé‡åŒ–è¯„ä¼°é‡åŒ–(quantization)ã€KV cacheå‹ç¼©ã€LoRAé€‚é…å™¨ã€åˆ†å—é¢„å¡«å……(chunked prefill)åŠç®—å­èåˆ(operator fusion)ç­‰ä¼˜åŒ–æŠ€æœ¯å¯¹æ€§èƒ½çš„å…³é”®å½±å“ã€‚é€šè¿‡ä»…åˆ©ç”¨TOPSå’Œå†…å­˜å¸¦å®½ç­‰ç¡¬ä»¶è§„æ ¼å‚æ•°ï¼ŒLIFEå³å¯å®ç°å¯¹é¦–ä¸ªæ ‡è®°ç”Ÿæˆæ—¶é—´(TTFT)ã€æ¯ä¸ªè¾“å‡ºæ ‡è®°ç”Ÿæˆæ—¶é—´(TPOT)å’Œæ¯ç§’ç”Ÿæˆæ ‡è®°æ•°(TPS)çš„ç²¾å‡†é¢„æµ‹ã€‚ç ”ç©¶äººå‘˜åœ¨AMD Ryzen CPUã€NPUã€iGPUä»¥åŠNVIDIA V100 GPUä¸Šä½¿ç”¨Llama2-7Bå˜ä½“æ¨¡å‹è¿›è¡Œäº†éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨é¢„æµ‹LLMæ€§èƒ½æ–¹é¢çš„å®ç”¨æ€§ã€‚è¿™ä¸€æˆæœä»ç³»ç»Ÿæ•ˆç‡çš„è§’åº¦å‡ºå‘ï¼Œä¸ºå®ç°åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šçš„LLMé«˜æ•ˆéƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.PF",
      "comment": "10 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.00904v1",
      "published_date": "2025-07-29 03:08:31 UTC",
      "updated_date": "2025-07-29 03:08:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:53.897990+00:00"
    },
    {
      "arxiv_id": "2507.21455v2",
      "title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation",
      "title_zh": "é€šè¿‡å‚æ•°åŒ–ã€é¢„å®šä¹‰å¢å¼ºå’Œè¿‘ä¼¼æå‡è‡ªç›‘ç£æ•°æ®é›†è’¸é¦",
      "authors": [
        "Sheng-Feng Yu",
        "Jia-Jiun Yao",
        "Wei-Chen Chiu"
      ],
      "abstract": "Although larger datasets are crucial for training large deep models, the rapid growth of dataset size has brought a significant challenge in terms of considerable training costs, which even results in prohibitive computational expenses. Dataset Distillation becomes a popular technique recently to reduce the dataset size via learning a highly compact set of representative exemplars, where the model trained with these exemplars ideally should have comparable performance with respect to the one trained with the full dataset. While most of existing works upon dataset distillation focus on supervised datasets, we instead aim to distill images and their self-supervisedly trained representations into a distilled set. This procedure, named as Self-Supervised Dataset Distillation, effectively extracts rich information from real datasets, yielding the distilled sets with enhanced cross-architecture generalizability. Particularly, in order to preserve the key characteristics of original dataset more faithfully and compactly, several novel techniques are proposed: 1) we introduce an innovative parameterization upon images and representations via distinct low-dimensional bases, where the base selection for parameterization is experimentally shown to play a crucial role; 2) we tackle the instability induced by the randomness of data augmentation -- a key component in self-supervised learning but being underestimated in the prior work of self-supervised dataset distillation -- by utilizing predetermined augmentations; 3) we further leverage a lightweight network to model the connections among the representations of augmented views from the same image, leading to more compact pairs of distillation. Extensive experiments conducted on various datasets validate the superiority of our approach in terms of distillation efficiency, cross-architecture generalization, and transfer learning performance.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†å¸¦æ¥çš„é«˜æ˜‚è®­ç»ƒæˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„è‡ªç›‘ç£æ•°æ®é›†è’¸é¦(Self-Supervised Dataset Distillation)æ¡†æ¶ã€‚ä¸ºäº†æ›´å¿ å®ä¸”ç´§å‡‘åœ°ä¿ç•™åŸå§‹æ•°æ®é›†çš„å…³é”®ç‰¹å¾ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†åŸºäºä½ç»´åŸº(low-dimensional bases)çš„å›¾åƒä¸è¡¨ç¤ºå‚æ•°åŒ–æŠ€æœ¯ï¼Œå¹¶å®éªŒè¯æ˜äº†åŸºçš„é€‰æ‹©å¯¹è’¸é¦æ•ˆæœè‡³å…³é‡è¦ã€‚é’ˆå¯¹è‡ªç›‘ç£å­¦ä¹ ä¸­éšæœºæ•°æ®å¢å¼ºå¸¦æ¥çš„ä¸ç¨³å®šæ€§ï¼Œæ–¹æ¡ˆé‡‡ç”¨äº†é¢„å®šä¹‰å¢å¼º(predefined augmentation)ç­–ç•¥ä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨è½»é‡çº§ç½‘ç»œå¯¹åŒä¸€å›¾åƒä¸åŒå¢å¼ºè§†å›¾ä¹‹é—´çš„è¡¨ç¤ºè”ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè¿›ä¸€æ­¥å®ç°äº†æ›´é«˜æ•ˆçš„æ•°æ®å‹ç¼©ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è’¸é¦æ•ˆç‡ã€è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›ä»¥åŠè¿ç§»å­¦ä¹ (transfer learning)è¡¨ç°ä¸Šå‡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "To appear in the Proceedings of the International Conference on Learning Representations (ICLR 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.21455v2",
      "published_date": "2025-07-29 02:51:56 UTC",
      "updated_date": "2025-08-05 06:51:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:34:58.699580+00:00"
    },
    {
      "arxiv_id": "2507.21453v2",
      "title": "Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)",
      "title_zh": "åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰éªŒè¯è¯ç‰©åŸºå› ç»„å­¦ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŸ¥è¯¢æç¤ºè¯",
      "authors": [
        "Ashley Rector",
        "Keaton Minor",
        "Kamden Minor",
        "Jeff McCormack",
        "Beth Breeden",
        "Ryan Nowers",
        "Jay Dorris"
      ],
      "abstract": "This study evaluated Sherpa Rx, an artificial intelligence tool leveraging large language models and retrieval-augmented generation (RAG) for pharmacogenomics, to validate its performance on key response metrics. Sherpa Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC) guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC guidelines was used to evaluate drug-gene interactions, dosing recommendations, and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2 additionally incorporated PharmGKB content. Responses were scored on accuracy, relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon signed-rank tests compared accuracy between Phase 1 and Phase 2, and between Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8, and recall 0.99. The subset analysis (N=20) showed improvements in accuracy (4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8). ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy between Phase 1 and Phase 2 was not statistically significant. However, Phase 2 significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx achieved 90% accuracy, outperforming other models. Integrating additional resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance. This study highlights the transformative potential of generative AI like Sherpa Rx in pharmacogenomics, improving decision-making with accurate, personalized responses.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åä¸ºSherpa Rxçš„äººå·¥æ™ºèƒ½å·¥å…·ï¼Œå®ƒåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œä¸“é—¨ç”¨äºè¯ç†åŸºå› ç»„å­¦(Pharmacogenomics)é¢†åŸŸçš„æŸ¥è¯¢éªŒè¯ã€‚Sherpa Rx æ•´åˆäº†ä¸´åºŠè¯ç‰©é—ä¼ å­¦å®æ–½è”ç›Ÿ(CPIC)æŒ‡å—ä¸è¯ç†åŸºå› ç»„å­¦çŸ¥è¯†åº“(PharmGKB)çš„æ•°æ®ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå®éªŒå¯¹æ¯”äº†ä¸åŒæ•°æ®åµŒå…¥æ–¹æ¡ˆå¯¹è¯ç‰©-åŸºå› ç›¸äº’ä½œç”¨åŠå‰‚é‡å»ºè®®ç”Ÿæˆçš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSherpa Rxåœ¨å‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€æ¸…æ™°åº¦å’Œå®Œæ•´æ€§ç­‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ç¬¬äºŒé˜¶æ®µæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºChatGPT-4ominiã€‚åœ¨é’ˆå¯¹çœŸå®ä¸–ç•Œé€‚ç”¨æ€§çš„æµ‹è¯•ä¸­ï¼ŒSherpa Rx è¾¾åˆ°äº†90%çš„å‡†ç¡®ç‡ï¼Œè¿œè¶…å…¶ä»–åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œå°†CPICå’ŒPharmGKBç­‰ä¸“ä¸šèµ„æºä¸RAGæŠ€æœ¯ç»“åˆï¼Œèƒ½æ˜¾è‘—å¢å¼ºç”Ÿæˆå¼AIåœ¨è¯ç†åŸºå› ç»„å­¦ä¸­çš„å†³ç­–æ”¯æŒèƒ½åŠ›ï¼Œä¸ºæä¾›ç²¾ç¡®ã€ä¸ªæ€§åŒ–çš„åŒ»ç–—å“åº”å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21453v2",
      "published_date": "2025-07-29 02:43:35 UTC",
      "updated_date": "2025-08-03 02:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:01.789346+00:00"
    },
    {
      "arxiv_id": "2507.21438v1",
      "title": "Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models",
      "title_zh": "Evo-DKDï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹è‡ªä¸»æœ¬ä½“æ¼”åŒ–çš„åŒçŸ¥è¯†è§£ç ",
      "authors": [
        "Vishal Raman",
        "Vijai Aravindh R"
      ],
      "abstract": "Ontologies and knowledge graphs require continuous evolution to remain comprehensive and accurate, but manual curation is labor intensive. Large Language Models (LLMs) possess vast unstructured knowledge but struggle with maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder framework for autonomous ontology evolution that combines structured ontology traversal with unstructured text reasoning. Evo-DKD introduces two parallel decoding streams within an LLM: one decoder generates candidate ontology edits (e.g., new concepts or relations) while the other produces natural-language justifications. A dynamic attention-based gating mechanism coordinates the two streams, deciding at each step how to blend structured and unstructured knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using prompt-based mode control to approximate coordinated decoding in a single-stream mode. The system operates in a closed reasoning loop: proposed ontology edits are validated (via consistency checks and cross-verification with the text explanations) and then injected into the knowledge base, which in turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on use cases including healthcare ontology refinement, semantic search improvement, and cultural heritage timeline modeling. Experiments show that Evo-DKD outperforms baselines using structured-only or unstructured-only decoding in both precision of ontology updates and downstream task performance. We present quantitative metrics and qualitative examples, confirming the contributions of the dual-decoder design and gating router. Evo-DKD offers a new paradigm for LLM-driven knowledge base maintenance, combining the strengths of symbolic and neural reasoning for sustainable ontology evolution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Evo-DKDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæœ¬ä½“è‡ªä¸»æ¼”åŒ– (Autonomous Ontology Evolution) çš„æ–°å‹åŒè§£ç å™¨ (Dual-Decoder) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰‹åŠ¨ç»´æŠ¤çŸ¥è¯†åº“åŠ³åŠ¨åŠ›å¯†é›†ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) éš¾ä»¥ç»´æŒç»“æ„ä¸€è‡´æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨ LLM å†…éƒ¨å¼•å…¥äº†ä¸¤ä¸ªå¹¶è¡Œçš„è§£ç æµï¼Œä¸€ä¸ªè´Ÿè´£ç”Ÿæˆå€™é€‰æœ¬ä½“ç¼–è¾‘ï¼Œå¦ä¸€ä¸ªåˆ™ç”Ÿæˆè‡ªç„¶è¯­è¨€ç†ç”±ï¼Œå¹¶é€šè¿‡åŠ¨æ€çš„æ³¨æ„åŠ›é—¨æ§æœºåˆ¶ (Gating Mechanism) åœ¨æ¯ä¸€æ­¥åè°ƒç»“æ„åŒ–ä¸éç»“æ„åŒ–çŸ¥è¯†çš„èåˆã€‚ç³»ç»Ÿé‡‡ç”¨é—­ç¯æ¨ç† (Closed Reasoning Loop) æ¨¡å¼ï¼Œé€šè¿‡ä¸€è‡´æ€§æ£€æŸ¥å’Œäº¤å‰éªŒè¯å°†æ›´æ–°æ³¨å…¥çŸ¥è¯†åº“ä»¥æŒ‡å¯¼åç»­æ¨ç†ã€‚åœ¨åŒ»ç–—ã€è¯­ä¹‰æœç´¢å’Œæ–‡åŒ–é—äº§ç­‰é¢†åŸŸçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEvo-DKD åœ¨æœ¬ä½“æ›´æ–°ç²¾åº¦å’Œä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸Šå‡ä¼˜äºå•ä¸€è§£ç æ¨¡å¼çš„åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆåŠŸç»“åˆäº†ç¬¦å·æ¨ç†ä¸ç¥ç»æ¨ç†çš„ä¼˜åŠ¿ï¼Œä¸º LLM é©±åŠ¨çš„çŸ¥è¯†åº“ç»´æŠ¤å’Œå¯æŒç»­æœ¬ä½“æ¼”åŒ–æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21438v1",
      "published_date": "2025-07-29 02:18:55 UTC",
      "updated_date": "2025-07-29 02:18:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:04.998645+00:00"
    },
    {
      "arxiv_id": "2507.21433v2",
      "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse",
      "title_zh": "MemShareï¼šé€šè¿‡ KV ç¼“å­˜å¤ç”¨å®ç°å¤§æ¨ç†æ¨¡å‹çš„å†…å­˜é«˜æ•ˆæ¨ç†",
      "authors": [
        "Kaiwen Chen",
        "Xin Tan",
        "Minchen Yu",
        "Hong Xu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have achieved significant advances in mathematical reasoning and formal logic tasks. However, their tendency to generate lengthy chain-of-thought sequences leads to substantial memory overhead during inference. We observe that LRMs frequently produce highly similar intermediate reasoning steps, which correspond to similar KV cache states across layers. Motivated by this observation, we propose MemShare, a novel KV cache management approach that effectively reduces memory overhead. MemShare employs a collaborative filtering algorithm to efficiently identify reusable KV cache blocks and enables zero copy cache reuse to significantly reduce memory overhead, improve throughput while maintaining accuracy. Experimental results demonstrate that MemShare delivers up to 84.79\\% improvement in throughput while maintaining better accuracy compared to existing KV cache management methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MemShareï¼Œä¸€ç§é’ˆå¯¹ Large Reasoning Models (LRMs) çš„é«˜æ•ˆå†…å­˜æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å…¶åœ¨ç”Ÿæˆå†—é•¿ chain-of-thought åºåˆ—æ—¶é¢ä¸´çš„å·¨å¤§å†…å­˜å¼€é”€é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç° LRMs åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç»å¸¸äº§ç”Ÿé«˜åº¦ç›¸ä¼¼çš„ä¸­é—´æ­¥éª¤ï¼Œä»è€Œå¯¹åº”è·¨å±‚ç›¸ä¼¼çš„ KV cache çŠ¶æ€ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼ŒMemShare å¼•å…¥äº†ä¸€ç§ååŒè¿‡æ»¤ç®—æ³• (collaborative filtering algorithm) æ¥é«˜æ•ˆè¯†åˆ«å¯é‡ç”¨çš„ KV cache å—ï¼Œå¹¶é€šè¿‡é›¶æ‹·è´ (zero copy) æŠ€æœ¯å®ç°ç¼“å­˜é‡ç”¨ï¼Œä»è€Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMemShare åœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œç›¸æ¯”ç°æœ‰ KV cache ç®¡ç†æ–¹æ³•å¯æå‡é«˜è¾¾ 84.79% çš„ååé‡ (throughput)ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¼˜åŒ–äº†å†…å­˜åˆ©ç”¨ç‡ï¼Œè¿˜ä¸ºåœ¨å¤§è§„æ¨¡æ¨ç†åœºæ™¯ä¸‹æå‡ LRMs çš„è¿è¡Œæ•ˆç‡æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21433v2",
      "published_date": "2025-07-29 02:05:51 UTC",
      "updated_date": "2025-07-31 07:53:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:20.150917+00:00"
    },
    {
      "arxiv_id": "2507.21432v2",
      "title": "Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour",
      "title_zh": "é¢å‘å‡ºè¡Œæ–¹å¼é€‰æ‹©è¡Œä¸ºçš„å¯æœ¬åœ°éƒ¨ç½²å¾®è°ƒå› æœå¤§è¯­è¨€æ¨¡å‹ç ”ç©¶",
      "authors": [
        "Tareq Alsaleh",
        "Bilal Farooq"
      ],
      "abstract": "This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven open-access LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 mode choice decisions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized and explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äº¤é€šå‡ºè¡Œæ–¹å¼é€‰æ‹©é¢„æµ‹ä¸­åº”ç”¨æœ¬åœ°å¯éƒ¨ç½²çš„å› æœå¤§è¯­è¨€æ¨¡å‹(Causal LLMs)ï¼Œå¹¶æ¨å‡ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹è¯¥ä»»åŠ¡å¾®è°ƒçš„å¼€æºæ¨¡å‹LiTransMCã€‚ç ”ç©¶å›¢é˜Ÿç³»ç»Ÿåœ°åŸºå‡†æµ‹è¯•äº†11ä¸ªä¸åŒè§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨BERTopicå’Œæ–°å‹è§£é‡Šå¼ºåº¦æŒ‡æ•°(Explanation Strength Index)å¯¹æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œäº†ç¬¦åˆè¡Œä¸ºç†è®ºçš„ç»“æ„åŒ–è¯„ä¼°ã€‚LiTransMCé€šè¿‡å‚æ•°é«˜æ•ˆ(Parameter-efficient)å’ŒæŸå¤±æ©ç ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œåœ¨åŠ æƒF1åˆ†æ•°å’ŒJensen-Shannon Divergenceç­‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡å’Œåˆ†å¸ƒæ ¡å‡†èƒ½åŠ›ä¸ä»…è¶…è¿‡äº†æœªç»å¾®è°ƒçš„æœ¬åœ°æ¨¡å‹ï¼Œç”šè‡³ä¼˜äºGPT-4oä»¥åŠä¼ ç»Ÿçš„ç¦»æ•£é€‰æ‹©æ¨¡å‹(Discrete choice models)å’Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†æ„å»ºå…¼å…·é«˜ç²¾åº¦ä¸å¼ºè§£é‡Šæ€§çš„äº¤é€šé¢†åŸŸä¸“ç”¨å¤§æ¨¡å‹çš„æ½œåŠ›ï¼Œä¸ºæ”¯æŒéšç§ä¿æŠ¤ã€ä½æˆæœ¬çš„æ”¿ç­–æµ‹è¯•å’ŒåŸºäºæ™ºèƒ½ä½“çš„ä»¿çœŸ(Agent-based simulations)æä¾›äº†å…¨æ–°çš„ç ”ç©¶èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21432v2",
      "published_date": "2025-07-29 02:03:37 UTC",
      "updated_date": "2025-10-07 12:12:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:29.844340+00:00"
    },
    {
      "arxiv_id": "2507.21423v1",
      "title": "MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving",
      "title_zh": "MapDiffusionï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çŸ¢é‡åŒ–åœ¨çº¿é«˜ç²¾åœ°å›¾æ„å»ºä¸ä¸ç¡®å®šæ€§ä¼°è®¡çš„ç”Ÿæˆå¼æ‰©æ•£æ–¹æ³•",
      "authors": [
        "Thomas Monninger",
        "Zihan Zhang",
        "Zhipeng Mo",
        "Md Zafar Anwar",
        "Steffen Staab",
        "Sihao Ding"
      ],
      "abstract": "Autonomous driving requires an understanding of the static environment from sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse multiple inputs, and a vector decoder predicts a vectorized map representation from the latent BEV grid. However, traditional map construction models provide deterministic point estimates, failing to capture uncertainty and the inherent ambiguities of real-world environments, such as occlusions and missing lane markings. We propose MapDiffusion, a novel generative approach that leverages the diffusion paradigm to learn the full distribution of possible vectorized maps. Instead of predicting a single deterministic output from learned queries, MapDiffusion iteratively refines randomly initialized queries, conditioned on a BEV latent grid, to generate multiple plausible map samples. This allows aggregating samples to improve prediction accuracy and deriving uncertainty estimates that directly correlate with scene ambiguity. Extensive experiments on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art performance in online map construction, surpassing the baseline by 5% in single-sample performance. We further show that aggregating multiple samples consistently improves performance along the ROC curve, validating the benefit of distribution modeling. Additionally, our uncertainty estimates are significantly higher in occluded areas, reinforcing their value in identifying regions with ambiguous sensor input. By modeling the full map distribution, MapDiffusion enhances the robustness and reliability of online vectorized HD map construction, enabling uncertainty-aware decision-making for autonomous vehicles in complex environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MapDiffusionï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹(diffusion paradigm)çš„æ–°å‹ç”Ÿæˆå¼åœ¨çº¿çŸ¢é‡åŒ–é«˜ç²¾åœ°å›¾(online HD map construction)æ„å»ºä¸ä¸ç¡®å®šæ€§ä¼°è®¡(uncertainty estimation)æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿåœ°å›¾æ¨¡å‹åœ¨é¢å¯¹é®æŒ¡æˆ–è½¦é“çº¿ç¼ºå¤±ç­‰ç¯å¢ƒæ¨¡ç³Šæ€§æ—¶ä»…èƒ½æä¾›ç¡®å®šæ€§ç‚¹ä¼°è®¡çš„å±€é™æ€§ï¼ŒMapDiffusionåˆ©ç”¨ç”Ÿæˆå¼æ‰©æ•£èŒƒå¼å­¦ä¹ åœ°å›¾åˆ†å¸ƒï¼Œé€šè¿‡åœ¨é¸Ÿç°å›¾(BEV)ç‰¹å¾çº¦æŸä¸‹è¿­ä»£ä¼˜åŒ–éšæœºæŸ¥è¯¢ï¼Œç”Ÿæˆå¤šä¸ªå¯èƒ½çš„åœ°å›¾æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•é€šè¿‡èšåˆæ ·æœ¬ä¸ä»…æå‡äº†é¢„æµ‹å‡†ç¡®åº¦ï¼Œè¿˜èƒ½æ¨å¯¼å‡ºä¸åœºæ™¯æ¨¡ç³Šåº¦ç›´æ¥ç›¸å…³çš„ä¸ç¡®å®šæ€§æŒ‡æ ‡ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMapDiffusionå•æ ·æœ¬æ€§èƒ½æ¯”åŸºçº¿æå‡5%ï¼Œè¾¾åˆ°å½“å‰æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸”åœ¨é®æŒ¡åŒºåŸŸèƒ½å¤Ÿè¾“å‡ºæ›´é«˜ä¸”æ›´å…·å‚è€ƒä»·å€¼çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚é€šè¿‡å¯¹å®Œæ•´åœ°å›¾åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼ŒMapDiffusionæœ‰æ•ˆå¢å¼ºäº†åœ¨çº¿åœ°å›¾æ„å»ºçš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„è‡ªåŠ¨é©¾é©¶å†³ç­–æä¾›äº†æ„ŸçŸ¥ä¿éšœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.21423v1",
      "published_date": "2025-07-29 01:16:40 UTC",
      "updated_date": "2025-07-29 01:16:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:43.991682+00:00"
    },
    {
      "arxiv_id": "2507.21419v1",
      "title": "GovRelBench:A Benchmark for Government Domain Relevance",
      "title_zh": "GovRelBenchï¼šæ”¿åŠ¡é¢†åŸŸç›¸å…³æ€§è¯„ä¼°åŸºå‡†",
      "authors": [
        "Haiquan Wang",
        "Yi Chen",
        "Shang Zeng",
        "Yun Bian",
        "Zhe Cui"
      ],
      "abstract": "Current evaluations of LLMs in the government domain primarily focus on safety considerations in specific scenarios, while the assessment of the models' own core capabilities, particularly domain relevance, remains insufficient. To address this gap, we propose GovRelBench, a benchmark specifically designed for evaluating the core capabilities of LLMs in the government domain. GovRelBench consists of government domain prompts and a dedicated evaluation tool, GovRelBERT. During the training process of GovRelBERT, we introduce the SoftGovScore method: this method trains a model based on the ModernBERT architecture by converting hard labels to soft scores, enabling it to accurately compute the text's government domain relevance score. This work aims to enhance the capability evaluation framework for large models in the government domain, providing an effective tool for relevant research and practice. Our code and dataset are available at https://github.com/pan-xi/GovRelBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GovRelBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ”¿åºœé¢†åŸŸæ ¸å¿ƒèƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯é¢†åŸŸç›¸å…³æ€§ domain relevanceï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥å½“å‰è¯„ä¼°ä½“ç³»è¿‡åº¦å…³æ³¨å®‰å…¨åœºæ™¯è€Œå¿½è§†æ ¸å¿ƒé¢†åŸŸé€‚é…èƒ½åŠ›çš„ç©ºç™½ã€‚GovRelBench åŒ…å«äº†æ”¿åºœé¢†åŸŸæç¤ºè¯ (prompts) ä»¥åŠé…å¥—çš„ä¸“ç”¨è¯„ä¼°å·¥å…· GovRelBERTã€‚åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œç ”ç©¶è€…å¼•å…¥äº† SoftGovScore æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäº ModernBERT æ¶æ„ï¼Œé€šè¿‡å°†ç¡¬æ ‡ç­¾ (hard labels) è½¬æ¢ä¸ºè½¯åˆ†æ•° (soft scores)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®è®¡ç®—æ–‡æœ¬ä¸æ”¿åºœé¢†åŸŸçš„å…³è”å¾—åˆ†ã€‚è¯¥é¡¹å·¥ä½œå®Œå–„äº†æ”¿åºœé¢†åŸŸå¤§æ¨¡å‹çš„èƒ½åŠ›è¯„ä»·æ¡†æ¶ï¼Œä¸ºæ”¿åŠ¡å¤§æ¨¡å‹çš„ç ”å‘å’Œå®è·µæä¾›äº†æœ‰æ•ˆçš„åº¦é‡å·¥å…·ã€‚ç›®å‰ï¼Œè¯¥é¡¹ç›®çš„ä»£ç å’Œæ•°æ®é›†å·²åœ¨ GitHub å¹³å°å…¬å¼€å‘å¸ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21419v1",
      "published_date": "2025-07-29 01:03:00 UTC",
      "updated_date": "2025-07-29 01:03:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:27.649455+00:00"
    },
    {
      "arxiv_id": "2507.21407v2",
      "title": "Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects",
      "title_zh": "å›¾å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼šç ”ç©¶ç°çŠ¶ä¸æœªæ¥å±•æœ›",
      "authors": [
        "Yixin Liu",
        "Guibin Zhang",
        "Kun Wang",
        "Shiyuan Li",
        "Shirui Pan"
      ],
      "abstract": "Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº†å›¾å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ (Graph-augmented LLM Agents, GLA) çš„æœ€æ–°è¿›å±•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¯é è§„åˆ’ (planning)ã€é•¿æœŸè®°å¿† (long-term memory)ã€å·¥å…·ç®¡ç†å’Œå¤šæ™ºèƒ½ä½“åè°ƒç­‰å…³é”®ç¯èŠ‚çš„å±€é™æ€§ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå›¾ç»“æ„èƒ½æœ‰æ•ˆä½œä¸ºè¾…åŠ©æ¶æ„ï¼Œæå‡å¤æ‚æ™ºèƒ½ä½“å·¥ä½œæµä¸­çš„ç»“æ„æ€§ã€è¿ç»­æ€§ä¸åä½œèƒ½åŠ›ã€‚ä½œè€…æ ¹æ® GLA åœ¨è§„åˆ’ã€è®°å¿†å’Œå·¥å…·ä½¿ç”¨ç­‰ç³»ç»ŸåŠŸèƒ½ä¸­çš„ä½œç”¨å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶æ·±å…¥åˆ†æäº†å›¾ä¸å›¾å­¦ä¹ ç®—æ³•çš„å…·ä½“è´¡çŒ®ã€‚é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS)ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº† GLA å¦‚ä½•ä¿ƒè¿›ç³»ç»Ÿç¼–æ’ã€æ•ˆç‡ä¼˜åŒ–åŠå¯ä¿¡åº¦ã€‚æœ€åï¼Œæ–‡ç« æå‡ºäº†æœªæ¥æ¨è¿›è¯¥é¢†åŸŸçš„å…³é”®æ–¹å‘ï¼ŒåŒ…æ‹¬æå‡ç»“æ„é€‚åº”æ€§ä»¥åŠæ„å»ºç»Ÿä¸€ã€å¯æ‰©å±•ä¸”å¤šæ¨¡æ€ (multimodal) çš„ GLA ç³»ç»Ÿï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ¸…æ™°çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.21407v2",
      "published_date": "2025-07-29 00:27:12 UTC",
      "updated_date": "2025-08-30 06:01:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:31.640752+00:00"
    },
    {
      "arxiv_id": "2507.21406v1",
      "title": "Shapley Uncertainty in Natural Language Generation",
      "title_zh": "è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­çš„ Shapley ä¸ç¡®å®šæ€§",
      "authors": [
        "Meilin Zhu",
        "Gaojie Jin",
        "Xiaowei Huang",
        "Lijun Zhang"
      ],
      "abstract": "In question-answering tasks, determining when to trust the outputs is crucial to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces semantic entropy as a measure of uncertainty, by incorporating linguistic invariances from the same meaning. It primarily relies on setting threshold to measure the level of semantic equivalence relation. We propose a more nuanced framework that extends beyond such thresholding by developing a Shapley-based uncertainty metric that captures the continuous nature of semantic relationships. We establish three fundamental properties that characterize valid uncertainty metrics and prove that our Shapley uncertainty satisfies these criteria. Through extensive experiments, we demonstrate that our Shapley uncertainty more accurately predicts LLM performance in question-answering and other datasets, compared to similar baseline measures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ large language models (LLMs) åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„è¾“å‡ºå¯ä¿¡åº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Shapley çš„ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡ Shapley uncertaintyã€‚è¯¥æ–¹æ³•æ—¨åœ¨æ”¹è¿›ç°æœ‰çš„ semantic entropy åº¦é‡ï¼Œå…‹æœå…¶è¿‡åº¦ä¾èµ–è®¾å®š threshold æ¥åˆ¤å®šè¯­ä¹‰ç­‰ä»·å…³ç³»çš„å±€é™æ€§ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ•æ‰è¯­ä¹‰å…³ç³»çš„ continuous natureã€‚ç ”ç©¶è€…ç¡®ç«‹äº†æœ‰æ•ˆä¸ç¡®å®šæ€§åº¦é‡å¿…é¡»å…·å¤‡çš„ä¸‰ä¸ªåŸºæœ¬å±æ€§ï¼Œå¹¶åœ¨ç†è®ºä¸Šè¯æ˜äº† Shapley uncertainty å®Œå…¨ç¬¦åˆè¿™äº›å‡†åˆ™ã€‚é€šè¿‡åœ¨ question-answering ä»¥åŠå…¶ä»–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œç»“æœè¯æ˜è¯¥æŒ‡æ ‡åœ¨é¢„æµ‹ LLM æ€§èƒ½æ–¹é¢æ¯”ç°æœ‰çš„åŸºçº¿åº¦é‡æ›´åŠ å‡†ç¡®ï¼Œä¸ºè‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„ä¸ç¡®å®šæ€§é‡åŒ–æä¾›äº†æ›´ç»†è‡´ä¸”å¯é çš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21406v1",
      "published_date": "2025-07-29 00:26:33 UTC",
      "updated_date": "2025-07-29 00:26:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:30.844010+00:00"
    },
    {
      "arxiv_id": "2507.21395v1",
      "title": "Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion",
      "title_zh": "Sync-TVAï¼šåŸºäºè·¨æ¨¡æ€èåˆçš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å›¾æ³¨æ„åŠ›æ¡†æ¶",
      "authors": [
        "Zeyu Deng",
        "Yanhui Lu",
        "Jiashu Liao",
        "Shuang Wu",
        "Chongfeng Wei"
      ],
      "abstract": "Multimodal emotion recognition (MER) is crucial for enabling emotionally intelligent systems that perceive and respond to human emotions. However, existing methods suffer from limited cross-modal interaction and imbalanced contributions across modalities. To address these issues, we propose Sync-TVA, an end-to-end graph-attention framework featuring modality-specific dynamic enhancement and structured cross-modal fusion. Our design incorporates a dynamic enhancement module for each modality and constructs heterogeneous cross-modal graphs to model semantic relations across text, audio, and visual features. A cross-attention fusion mechanism further aligns multimodal cues for robust emotion inference. Experiments on MELD and IEMOCAP demonstrate consistent improvements over state-of-the-art models in both accuracy and weighted F1 score, especially under class-imbalanced conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sync-TVAï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„ Graph-Attention æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ« (Multimodal Emotion Recognition) ä¸­è·¨æ¨¡æ€äº¤äº’æœ‰é™ä»¥åŠå„æ¨¡æ€è´¡çŒ®ä¸å‡è¡¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ºæ¯ä¸ªæ¨¡æ€è®¾è®¡åŠ¨æ€å¢å¼ºæ¨¡å—ï¼Œå¹¶æ„å»ºå¼‚æ„è·¨æ¨¡æ€å›¾ (Heterogeneous Cross-Modal Graphs) æ¥å»ºæ¨¡æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚è¯­ä¹‰å…³ç³»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ (Cross-Attention Fusion Mechanism) ä»¥å¯¹é½å¤šæ¨¡æ€çº¿ç´¢ï¼Œç¡®ä¿äº†æƒ…æ„Ÿæ¨ç†çš„ç¨³å¥æ€§ã€‚åœ¨ MELD å’Œ IEMOCAP æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSync-TVA åœ¨å‡†ç¡®ç‡å’ŒåŠ æƒ F1 åˆ†æ•°ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå°¤å…¶åœ¨å¤„ç†æ•°æ®ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å½¢æ—¶è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œçš„æ ¸å¿ƒä»·å€¼åœ¨äºé€šè¿‡ç»“æ„åŒ–çš„å›¾æ³¨æ„åŠ›æ¨¡å‹æå‡äº†æƒ…æ„Ÿæ™ºèƒ½ç³»ç»Ÿçš„æ„ŸçŸ¥ä¸å“åº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21395v1",
      "published_date": "2025-07-29 00:03:28 UTC",
      "updated_date": "2025-07-29 00:03:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:35:39.096288+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 137,
  "processed_papers_count": 137,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T07:36:41.770837+00:00"
}
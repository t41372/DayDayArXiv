{
  "date": "2024-09-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-20 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型优化、多模态学习、偏见检测和图像生成等领域，其中 LLMs 在道德偏见和多样性方面的研究（如 MirrorStories 和 STOP!）最为令人印象深刻，同时医疗和机器人应用的相关论文也展现出实际潜力。\n\n### 重点论文讨论\n今天共有 111 篇论文，我将优先讨论那些重要、话题性强或有潜在影响的文章，并将相关主题归类快速概述。以下按主题分组，先聊 AI 伦理与 LLMs、图像与多模态处理、机器人与强化学习等领域，其余较常规的论文（如纯技术优化或小数据集实验）将简要掠过。\n\n#### AI 伦理与 LLMs：偏见检测和多样性提升\n这些论文探讨 LLMs 的道德偏见和改进方法，话题度高，因为涉及 AI 公平性和实际应用。\n- **MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models**（镜像故事：通过大语言模型实现个性化叙事生成以反映多样性）  \n  主要贡献：提出 MirrorStories 框架，使用 LLMs 生成个性化故事，融入用户身份元素（如性别、年龄），提升文本多样性和道德一致性。发现：LLM 生成的故事在参与度上优于通用叙事（平均评分 4.22 vs. 3.37），并减少偏见，但需进一步评估文化偏差。\n  \n- **STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions**（STOP!：通过敏感测试基准评估大语言模型对攻击性进化的鲁棒性）  \n  主要贡献：发布 STOP 数据集，包含 450 个攻击性序列，测试 LLMs 对 9 个群体偏见的检测。发现：顶级模型检测准确率仅 19.3%~69.8%，但通过人类对齐优化，可提升 BBQ 和 StereoSet 等任务性能达 191%。\n\n- **LLM for Everyone: Representing the Underrepresented in Large Language Models**（人人可用 LLM：在大语言模型中代表弱势群体）  \n  主要贡献：聚焦 LLMs 在低资源语言中的泛化，提出数据高效方法如跨语言指令微调。发现：显著改善弱势语言的性能，同时保持任务泛化能力。\n\n其他 LLMs 相关论文（如第 24、54、85 篇）快速掠过：它们探讨 LLMs 的性别偏见或时间感知，但贡献较局部，仅在特定基准上优化模型。\n\n#### 图像与多模态处理：高效生成和鲁棒性提升\n图像生成和多模态模型是热门领域，这些论文强调实用性和泛化。\n- **A Personalised 3D+t Mesh Generative Model for Unveiling Normal Heart Dynamics**（个性化 3D+t 网格生成模型：揭示正常心脏动态）  \n  主要贡献：提出 MeshHeart 模型，使用条件生成学习心脏形状和运动，融入临床因素如年龄和性别。发现：在 38,309 个样本上，模型提升心脏病分类准确率，并通过潜在空间度量（如 latent delta）关联临床表型。\n\n- **Nonlinear Inverse Design of Mechanical Multi-Material Metamaterials Enabled by Video Denoising Diffusion and Structure Identifier**（视频去噪扩散和结构识别器驱动的非线性多材料超材料逆设计）  \n  主要贡献：框架结合视频扩散模型和 UNet 识别器，实现非线性应力-应变响应的多材料设计。发现：提升超材料控制精度，支持实际应用如可塑性和大变形。\n\n- **TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A Interactions**（TalkMosaic：多模态 LLM 支持的交互式照片镶嵌）  \n  主要贡献：使用多模态 LLM 生成交互式照片镶嵌，并优化注意力机制加速推理。发现：提升图像交互效率，如通过稀疏注意力减少计算开销。\n\n其他图像论文（如第 10、27、41 篇）掠过：它们聚焦细节优化，如扩散模型改进，但影响较局限于特定任务。\n\n#### 机器人与强化学习：协作和鲁棒性\n这些论文强调实际应用，如导航和决策，相关性强。\n- **Learning to Play Video Games with Intuitive Physics Priors**（利用直觉物理先验学习玩视频游戏）  \n  主要贡献：使用基于对象的输入表示和 Q-learning 算法，融入物理先验模拟婴儿学习。发现：模型在多个游戏中表现出色，泛化性优于传统方法。\n\n- **MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety**（MAGICS：隐式批评 Stackelberg 引导的对抗强化学习，用于机器人安全神经合成）  \n  主要贡献：提出 MAGICS 算法，保证局部收敛的对抗 RL，用于机器人安全控制。发现：在模拟和硬件实验中，性能优于现有方法，提升鲁棒性。\n\n其他机器人论文（如第 15、42 篇）快速掠过：它们探索代理协作，但实验规模较小，贡献主要在特定场景。\n\n### 其他快速概述\n剩余论文涉及生物医学、元学习和数据生成等（如第 12、14、17、23、28、43、57 等），但许多是初步实验或特定领域优化（如元学习在医疗中的应用），不那么话题性强。例如，第 12 篇（Enhancing Large Language Models with Domain-specific Retrieval）提出 RAG 框架减少 LLM 幻觉，贡献在于医疗问答准确性提升，但整体影响局限于领域内。总体而言，这些论文虽丰富，但未有突破性发现，故从简。\n\n今天的 arXiv 更新展示了 AI 在伦理和实际应用上的进展，LLMs 相关研究尤为值得关注。欢迎读者根据兴趣深入探索这些论文！",
  "papers": [
    {
      "arxiv_id": "2409.13945v1",
      "title": "PureDiffusion: Using Backdoor to Counter Backdoor in Generative Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Vu Tuan Truong",
        "Long Bao Le"
      ],
      "abstract": "Diffusion models (DMs) are advanced deep learning models that achieved\nstate-of-the-art capability on a wide range of generative tasks. However,\nrecent studies have shown their vulnerability regarding backdoor attacks, in\nwhich backdoored DMs consistently generate a designated result (e.g., a harmful\nimage) called backdoor target when the models' input contains a backdoor\ntrigger. Although various backdoor techniques have been investigated to attack\nDMs, defense methods against these threats are still limited and underexplored,\nespecially in inverting the backdoor trigger. In this paper, we introduce\nPureDiffusion, a novel backdoor defense framework that can efficiently detect\nbackdoor attacks by inverting backdoor triggers embedded in DMs. Our extensive\nexperiments on various trigger-target pairs show that PureDiffusion outperforms\nexisting defense methods with a large gap in terms of fidelity (i.e., how much\nthe inverted trigger resembles the original trigger) and backdoor success rate\n(i.e., the rate that the inverted trigger leads to the corresponding backdoor\ntarget). Notably, in certain cases, backdoor triggers inverted by PureDiffusion\neven achieve higher attack success rate than the original triggers.",
      "tldr_zh": "本文研究了生成扩散模型(DMs)的后门攻击问题，这些攻击会导致模型在输入包含后门触发器时生成指定有害输出。PureDiffusion是一种新颖的防御框架，通过反转嵌入在DMs中的后门触发器来高效检测攻击。实验结果显示，PureDiffusion在保真度（inverted trigger 与原触发器的相似度）和后门成功率上远超现有方法，在某些情况下，反转触发器甚至比原触发器更有效地引发攻击。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13945v1",
      "published_date": "2024-09-20 23:19:26 UTC",
      "updated_date": "2024-09-20 23:19:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:46:46.256189"
    },
    {
      "arxiv_id": "2409.13941v2",
      "title": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Li",
        "Fulu Li"
      ],
      "abstract": "We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach.",
      "tldr_zh": "本研究提出TalkMosaic，一种互动式PhotoMosaic系统，使用多模态LLM实现图像问答交互，以各种汽车图像合成动物主题图片（如鸟或狮子），旨在最大化汽车信息传播并提升环境保护意识。系统支持用户点击PhotoMosaic图块切换显示原汽车图像，并自动保存，同时通过自定义多模态GPT（TalkMosaic）上传图像进行高效Q&A，例如查询符合环保标准的轮胎购买信息。论文还分析了加速多模态LLM推理的方法，包括稀疏注意力技术(PrFlashAttention)和量化方法(Staircase Adaptive Quantization, SAQ)。原型演示证实了该方法的有效性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13941v2",
      "published_date": "2024-09-20 23:04:21 UTC",
      "updated_date": "2024-11-06 05:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:46:59.380184"
    },
    {
      "arxiv_id": "2409.13940v1",
      "title": "Learning Recourse Costs from Pairwise Feature Comparisons",
      "title_zh": "翻译失败",
      "authors": [
        "Kaivalya Rawal",
        "Himabindu Lakkaraju"
      ],
      "abstract": "This paper presents a novel technique for incorporating user input when\nlearning and inferring user preferences. When trying to provide users of\nblack-box machine learning models with actionable recourse, we often wish to\nincorporate their personal preferences about the ease of modifying each\nindividual feature. These recourse finding algorithms usually require an\nexhaustive set of tuples associating each feature to its cost of modification.\nSince it is hard to obtain such costs by directly surveying humans, in this\npaper, we propose the use of the Bradley-Terry model to automatically infer\nfeature-wise costs using non-exhaustive human comparison surveys. We propose\nthat users only provide inputs comparing entire recourses, with all candidate\nfeature modifications, determining which recourses are easier to implement\nrelative to others, without explicit quantification of their costs. We\ndemonstrate the efficient learning of individual feature costs using MAP\nestimates, and show that these non-exhaustive human surveys, which do not\nnecessarily contain data for each feature pair comparison, are sufficient to\nlearn an exhaustive set of feature costs, where each feature is associated with\na modification cost.",
      "tldr_zh": "这篇论文提出了一种新方法，使用 Bradley-Terry 模型从用户对整个 recourse 的成对比较中自动推断特征修改成本，从而在黑盒机器学习模型中整合用户偏好。该方法允许用户仅需比较不同 recourse 的相对易实现性，而非提供详尽的特征成本元组，通过 MAP estimates 高效学习单个特征成本。即使调查数据不包含所有特征对比较，该技术也能生成完整的特征成本集，简化了用户输入过程并提升了 recourse 生成的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "\"Recourse for Humans\", paper 49 from the Participatory Approaches to\n  Machine Learning workshop at the International Conference on Machine Learning\n  (ICML) 2020. For workshop website, see https://participatoryml.github.io/#49",
      "pdf_url": "http://arxiv.org/pdf/2409.13940v1",
      "published_date": "2024-09-20 23:04:08 UTC",
      "updated_date": "2024-09-20 23:04:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:47:11.111487"
    },
    {
      "arxiv_id": "2409.13939v1",
      "title": "Simple Unsupervised Knowledge Distillation With Space Similarity",
      "title_zh": "简单的无监督知识蒸馏与空间相似性",
      "authors": [
        "Aditya Singh",
        "Haohan Wang"
      ],
      "abstract": "As per recent studies, Self-supervised learning (SSL) does not readily extend\nto smaller architectures. One direction to mitigate this shortcoming while\nsimultaneously training a smaller network without labels is to adopt\nunsupervised knowledge distillation (UKD). Existing UKD approaches handcraft\npreservation worthy inter/intra sample relationships between the teacher and\nits student. However, this may overlook/ignore other key relationships present\nin the mapping of a teacher. In this paper, instead of heuristically\nconstructing preservation worthy relationships between samples, we directly\nmotivate the student to model the teacher's embedding manifold. If the mapped\nmanifold is similar, all inter/intra sample relationships are indirectly\nconserved. We first demonstrate that prior methods cannot preserve teacher's\nlatent manifold due to their sole reliance on $L_2$ normalised embedding\nfeatures. Subsequently, we propose a simple objective to capture the lost\ninformation due to normalisation. Our proposed loss component, termed\n\\textbf{space similarity}, motivates each dimension of a student's feature\nspace to be similar to the corresponding dimension of its teacher. We perform\nextensive experiments demonstrating strong performance of our proposed approach\non various benchmarks.",
      "tldr_zh": "本文研究了自监督学习(SSL)无法轻松扩展到小型架构的问题，并提出了一种简单的无监督知识蒸馏(UKD)方法，即通过space similarity损失来让学生模型模仿教师的嵌入流形，从而间接保留所有样本关系。不同于现有UKD方法的手动构建样本关系，本文发现这些方法因依赖L2归一化而无法完全保留教师的潜在流形，并引入space similarity作为新损失组件，使学生的特征空间每个维度与教师对应维度相似。实验结果显示，该方法在各种基准上表现出色，证明了其有效性。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13939v1",
      "published_date": "2024-09-20 22:54:39 UTC",
      "updated_date": "2024-09-20 22:54:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:47:22.525893"
    },
    {
      "arxiv_id": "2409.13935v2",
      "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sarfaroz Yunusov",
        "Hamza Sidat",
        "Ali Emami"
      ],
      "abstract": "This study explores the effectiveness of Large Language Models (LLMs) in\ncreating personalized \"mirror stories\" that reflect and resonate with\nindividual readers' identities, addressing the significant lack of diversity in\nliterature. We present MirrorStories, a corpus of 1,500 personalized short\nstories generated by integrating elements such as name, gender, age, ethnicity,\nreader interest, and story moral. We demonstrate that LLMs can effectively\nincorporate diverse identity elements into narratives, with human evaluators\nidentifying personalized elements in the stories with high accuracy. Through a\ncomprehensive evaluation involving 26 diverse human judges, we compare the\neffectiveness of MirrorStories against generic narratives. We find that\npersonalized LLM-generated stories not only outscore generic human-written and\nLLM-generated ones across all metrics of engagement (with average ratings of\n4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity\nwhile preserving the intended moral. We also provide analyses that include bias\nassessments and a study on the potential for integrating images into\npersonalized stories.",
      "tldr_zh": "本研究探讨大型语言模型(LLMs)在生成个性化“镜像故事”方面的有效性，以解决文学多样性缺失的问题，创建了MirrorStories语料库，其中包含1500个短故事，通过整合读者的姓名、性别、年龄、民族、兴趣和道德元素来个性化叙事。实验结果显示，LLMs生成的个性化故事在参与度等指标上显著优于通用故事（平均评分4.22 vs 3.37，5分制），并实现了更高的文本多样性，同时保留了预期的道德。研究还进行了偏差评估和图像整合潜力分析，为提升叙事包容性提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages (excluding references), accepted to EMNLP 2024 Main\n  Conference",
      "pdf_url": "http://arxiv.org/pdf/2409.13935v2",
      "published_date": "2024-09-20 22:43:13 UTC",
      "updated_date": "2024-09-24 01:30:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:47:35.442889"
    },
    {
      "arxiv_id": "2409.13929v1",
      "title": "Failures in Perspective-taking of Multimodal AI Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Bridget Leonard",
        "Kristin Woodard",
        "Scott O. Murray"
      ],
      "abstract": "This study extends previous research on spatial representations in multimodal\nAI systems. Although current models demonstrate a rich understanding of spatial\ninformation from images, this information is rooted in propositional\nrepresentations, which differ from the analog representations employed in human\nand animal spatial cognition. To further explore these limitations, we apply\ntechniques from cognitive and developmental science to assess the\nperspective-taking abilities of GPT-4o. Our analysis enables a comparison\nbetween the cognitive development of the human brain and that of multimodal AI,\noffering guidance for future research and model development.",
      "tldr_zh": "这篇论文扩展了对多模态 AI systems 的空间表示研究，揭示了这些系统虽能理解图像中的空间信息，但依赖于命题 representations (propositional representations)，而非人类和动物认知中的模拟 representations (analog representations)，导致视角转换 (perspective-taking) 能力的失败。研究者应用认知和发育科学的技术评估了 GPT-4o 的表现，并通过比较人类大脑和多模态 AI 的认知发展，为未来模型开发提供宝贵指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13929v1",
      "published_date": "2024-09-20 22:31:46 UTC",
      "updated_date": "2024-09-20 22:31:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:47:46.730209"
    },
    {
      "arxiv_id": "2409.13928v1",
      "title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Seonghyeon Lee",
        "Suyeon Kim",
        "Joonwon Jang",
        "Heejae Chon",
        "Dongha Lee",
        "Hwanjo Yu"
      ],
      "abstract": "We study the code generation behavior of instruction-tuned models built on\ntop of code pre-trained language models when they could access an auxiliary\nfunction to implement a function. We design several ways to provide auxiliary\nfunctions to the models by adding them to the query or providing a response\nprefix to incorporate the ability to utilize auxiliary functions with the\ninstruction-following capability. Our experimental results show the\neffectiveness of combining the base models' auxiliary function utilization\nability with the instruction following ability. In particular, the performance\nof adopting our approaches with the open-sourced language models surpasses that\nof the recent powerful proprietary language models, i.e., gpt-4o.",
      "tldr_zh": "本研究探讨了指令微调的代码语言模型（instruction-tuned Code Language Models）在利用辅助函数（auxiliary function）生成代码时的能力，通过将辅助函数添加到查询或响应前缀中，结合模型的函数利用能力和指令遵循能力。研究设计了多种提供辅助函数的方法，以增强代码生成效果。实验结果显示，这种方法显著提高了开源语言模型的性能，甚至超过了像GPT-4o这样的强大专有模型。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "EMNLP 2024 Findings Short",
      "pdf_url": "http://arxiv.org/pdf/2409.13928v1",
      "published_date": "2024-09-20 22:28:20 UTC",
      "updated_date": "2024-09-20 22:28:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:47:58.057812"
    },
    {
      "arxiv_id": "2409.13926v1",
      "title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending",
      "title_zh": "SpaceBlender：通过生成式 3D 场景混合创建上下文丰富的协作空间",
      "authors": [
        "Nels Numan",
        "Shwetha Rajaram",
        "Balasaravanan Thoravi Kumaravel",
        "Nicolai Marquardt",
        "Andrew D. Wilson"
      ],
      "abstract": "There is increased interest in using generative AI to create 3D spaces for\nVirtual Reality (VR) applications. However, today's models produce artificial\nenvironments, falling short of supporting collaborative tasks that benefit from\nincorporating the user's physical context. To generate environments that\nsupport VR telepresence, we introduce SpaceBlender, a novel pipeline that\nutilizes generative AI techniques to blend users' physical surroundings into\nunified virtual spaces. This pipeline transforms user-provided 2D images into\ncontext-rich 3D environments through an iterative process consisting of depth\nestimation, mesh alignment, and diffusion-based space completion guided by\ngeometric priors and adaptive text prompts. In a preliminary within-subjects\nstudy, where 20 participants performed a collaborative VR affinity diagramming\ntask in pairs, we compared SpaceBlender with a generic virtual environment and\na state-of-the-art scene generation framework, evaluating its ability to create\nvirtual spaces suitable for collaboration. Participants appreciated the\nenhanced familiarity and context provided by SpaceBlender but also noted\ncomplexities in the generative environments that could detract from task focus.\nDrawing on participant feedback, we propose directions for improving the\npipeline and discuss the value and design of blended spaces for different\nscenarios.",
      "tldr_zh": "该论文提出 SpaceBlender，一种创新管道，利用生成式 AI 技术将用户物理环境融入统一的 3D 虚拟空间，以支持 VR 遥存（telepresence）和协作任务。SpaceBlender 通过迭代过程，包括深度估计、网格对齐和基于扩散的空间完成（guided by geometric priors 和 adaptive text prompts），将用户提供的 2D 图像转化为富含上下文的 3D 环境。在一项涉及 20 名参与者的初步研究中，与通用虚拟环境和现有框架比较，SpaceBlender 提升了环境的熟悉感和协作效果，但也暴露了潜在复杂性问题；作者基于反馈讨论了改进方向和混合空间的设计价值。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13926v1",
      "published_date": "2024-09-20 22:27:31 UTC",
      "updated_date": "2024-09-20 22:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:48:12.192793"
    },
    {
      "arxiv_id": "2409.13919v2",
      "title": "Measuring Error Alignment for Decision-Making Systems",
      "title_zh": "测量决策系统的错误对齐",
      "authors": [
        "Binxia Xu",
        "Antonis Bikakis",
        "Daniel Onah",
        "Andreas Vlachidis",
        "Luke Dickens"
      ],
      "abstract": "Given that AI systems are set to play a pivotal role in future\ndecision-making processes, their trustworthiness and reliability are of\ncritical concern. Due to their scale and complexity, modern AI systems resist\ndirect interpretation, and alternative ways are needed to establish trust in\nthose systems, and determine how well they align with human values. We argue\nthat good measures of the information processing similarities between AI and\nhumans, may be able to achieve these same ends. While Representational\nalignment (RA) approaches measure similarity between the internal states of two\nsystems, the associated data can be expensive and difficult to collect for\nhuman systems. In contrast, Behavioural alignment (BA) comparisons are cheaper\nand easier, but questions remain as to their sensitivity and reliability. We\npropose two new behavioural alignment metrics misclassification agreement which\nmeasures the similarity between the errors of two systems on the same\ninstances, and class-level error similarity which measures the similarity\nbetween the error distributions of two systems. We show that our metrics\ncorrelate well with RA metrics, and provide complementary information to\nanother BA metric, within a range of domains, and set the scene for a new\napproach to value alignment.",
      "tldr_zh": "这篇论文探讨了AI系统在决策过程中的可信赖性问题，提出通过测量AI与人类的错误对齐来评估其与人类价值观的一致性。作者引入了两个新的行为对齐(BA)指标：misclassification agreement（衡量两个系统在相同实例上的错误相似性）和class-level error similarity（衡量错误分布的相似性），这些指标比传统的表征对齐(RA)方法更易获取且成本更低。实验结果显示，这些指标与RA指标高度相关，并提供互补信息，为AI价值对齐的新方法奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13919v2",
      "published_date": "2024-09-20 21:59:13 UTC",
      "updated_date": "2024-12-31 15:11:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:48:22.848444"
    },
    {
      "arxiv_id": "2409.13908v2",
      "title": "Nonlinear Inverse Design of Mechanical Multi-Material Metamaterials Enabled by Video Denoising Diffusion and Structure Identifier",
      "title_zh": "翻译失败",
      "authors": [
        "Jaewan Park",
        "Shashank Kushwaha",
        "Junyan He",
        "Seid Koric",
        "Qibang Liu",
        "Iwona Jasiuk",
        "Diab Abueidda"
      ],
      "abstract": "Metamaterials, synthetic materials with customized properties, have emerged\nas a promising field due to advancements in additive manufacturing. These\nmaterials derive unique mechanical properties from their internal lattice\nstructures, which are often composed of multiple materials that repeat\ngeometric patterns. While traditional inverse design approaches have shown\npotential, they struggle to map nonlinear material behavior to multiple\npossible structural configurations. This paper presents a novel framework\nleveraging video diffusion models, a type of generative artificial Intelligence\n(AI), for inverse multi-material design based on nonlinear stress-strain\nresponses. Our approach consists of two key components: (1) a fields generator\nusing a video diffusion model to create solution fields based on target\nnonlinear stress-strain responses, and (2) a structure identifier employing two\nUNet models to determine the corresponding multi-material 2D design. By\nincorporating multiple materials, plasticity, and large deformation, our\ninnovative design method allows for enhanced control over the highly nonlinear\nmechanical behavior of metamaterials commonly seen in real-world applications.\nIt offers a promising solution for generating next-generation metamaterials\nwith finely tuned mechanical characteristics.",
      "tldr_zh": "本论文提出了一种创新框架，用于非线性逆向设计机械多材料metamaterials，通过视频扩散模型(video diffusion models)和结构识别器(Structure Identifier)解决传统方法在处理非线性应力-应变响应和多种结构配置时的局限性。框架的核心组件包括：fields generator，利用视频扩散模型基于目标非线性应力-应变响应生成解决方案字段，以及structure identifier，通过两个UNet模型确定对应的多材料2D设计。该方法整合多种材料、可塑性和大变形，实现了对metamaterials高度非线性机械行为的精确控制，并为开发具有定制机械特性的下一代metamaterials提供高效解决方案。",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13908v2",
      "published_date": "2024-09-20 21:26:15 UTC",
      "updated_date": "2024-09-28 20:15:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:48:35.618782"
    },
    {
      "arxiv_id": "2409.13903v1",
      "title": "CI-Bench: Benchmarking Contextual Integrity of AI Assistants on Synthetic Data",
      "title_zh": "CI-Bench：基于合成数据的 AI 助手情境完整性基准测试",
      "authors": [
        "Zhao Cheng",
        "Diane Wan",
        "Matthew Abueg",
        "Sahra Ghalebikesabi",
        "Ren Yi",
        "Eugene Bagdasarian",
        "Borja Balle",
        "Stefan Mellem",
        "Shawn O'Banion"
      ],
      "abstract": "Advances in generative AI point towards a new era of personalized\napplications that perform diverse tasks on behalf of users. While general AI\nassistants have yet to fully emerge, their potential to share personal data\nraises significant privacy challenges. This paper introduces CI-Bench, a\ncomprehensive synthetic benchmark for evaluating the ability of AI assistants\nto protect personal information during model inference. Leveraging the\nContextual Integrity framework, our benchmark enables systematic assessment of\ninformation flow across important context dimensions, including roles,\ninformation types, and transmission principles. We present a novel, scalable,\nmulti-step synthetic data pipeline for generating natural communications,\nincluding dialogues and emails. Unlike previous work with smaller, narrowly\nfocused evaluations, we present a novel, scalable, multi-step data pipeline\nthat synthetically generates natural communications, including dialogues and\nemails, which we use to generate 44 thousand test samples across eight domains.\nAdditionally, we formulate and evaluate a naive AI assistant to demonstrate the\nneed for further study and careful training towards personal assistant tasks.\nWe envision CI-Bench as a valuable tool for guiding future language model\ndevelopment, deployment, system design, and dataset construction, ultimately\ncontributing to the development of AI assistants that align with users' privacy\nexpectations.",
      "tldr_zh": "该论文引入了 CI-Bench，一种综合的合成数据基准，用于评估 AI 助手在模型推理过程中保护个人信息的 Contextual Integrity。研究提出了一种新颖、可扩展的多步骤合成数据管道，生成自然的通信如对话和电子邮件，从而创建了覆盖八个领域的44,000个测试样本。利用 Contextual Integrity 框架，他们系统评估了信息流在角色、信息类型和传输原则等维度上的表现，并通过评估一个简单 AI 助手展示了当前技术的不足。CI-Bench 旨在指导未来语言模型的开发、部署和数据集构建，推动符合用户隐私预期的 AI 助手发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13903v1",
      "published_date": "2024-09-20 21:14:36 UTC",
      "updated_date": "2024-09-20 21:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:48:46.559194"
    },
    {
      "arxiv_id": "2409.13902v1",
      "title": "Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology",
      "title_zh": "翻译失败",
      "authors": [
        "Aidan Gilson",
        "Xuguang Ai",
        "Thilaka Arunachalam",
        "Ziyou Chen",
        "Ki Xiong Cheong",
        "Amisha Dave",
        "Cameron Duic",
        "Mercy Kibe",
        "Annette Kaminaka",
        "Minali Prasad",
        "Fares Siddig",
        "Maxwell Singer",
        "Wendy Wong",
        "Qiao Jin",
        "Tiarnan D. L. Keenan",
        "Xia Hu",
        "Emily Y. Chew",
        "Zhiyong Lu",
        "Hua Xu",
        "Ron A. Adelman",
        "Yih-Chung Tham",
        "Qingyu Chen"
      ],
      "abstract": "Despite the potential of Large Language Models (LLMs) in medicine, they may\ngenerate responses lacking supporting evidence or based on hallucinated\nevidence. While Retrieval Augment Generation (RAG) is popular to address this\nissue, few studies implemented and evaluated RAG in downstream domain-specific\napplications. We developed a RAG pipeline with 70,000 ophthalmology-specific\ndocuments that retrieve relevant documents to augment LLMs during inference\ntime. In a case study on long-form consumer health questions, we systematically\nevaluated the responses including over 500 references of LLMs with and without\nRAG on 100 questions with 10 healthcare professionals. The evaluation focuses\non factuality of evidence, selection and ranking of evidence, attribution of\nevidence, and answer accuracy and completeness. LLMs without RAG provided 252\nreferences in total. Of which, 45.3% hallucinated, 34.1% consisted of minor\nerrors, and 20.6% were correct. In contrast, LLMs with RAG significantly\nimproved accuracy (54.5% being correct) and reduced error rates (18.8% with\nminor hallucinations and 26.7% with errors). 62.5% of the top 10 documents\nretrieved by RAG were selected as the top references in the LLM response, with\nan average ranking of 4.9. The use of RAG also improved evidence attribution\n(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight\ndecreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47\nto 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited\nhallucinated and erroneous evidence in the responses, raising concerns for\ndownstream applications in the medical domain. RAG substantially reduced the\nproportion of such evidence but encountered challenges.",
      "tldr_zh": "本研究探讨了如何通过域特定 Retrieval Augment Generation (RAG) 增强 Large Language Models (LLMs)，以改善眼科领域的长形式消费者健康问题回答。研究开发了一个RAG管道，使用70,000份眼科特定文档，在推理时检索相关信息来辅助LLMs生成响应。针对100个问题进行的评估显示，带RAG的LLMs显著降低了幻觉引用（从45.3%降至18.8%），并提高了证据准确性（正确率从20.6%升至54.5%），同时提升了证据归因评分（从1.85到2.49）。尽管RAG改善了整体事实性和可靠性，但也略微降低了答案的准确性（从3.52到3.23）和完整性（从3.47到3.27），突显了其在医疗应用中的潜力与挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13902v1",
      "published_date": "2024-09-20 21:06:00 UTC",
      "updated_date": "2024-09-20 21:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:49:00.644916"
    },
    {
      "arxiv_id": "2409.13897v1",
      "title": "LLM for Everyone: Representing the Underrepresented in Large Language Models",
      "title_zh": "LLM for Everyone：在大语言模型中代表未被充分代表的人群",
      "authors": [
        "Samuel Cahyawijaya"
      ],
      "abstract": "Natural language processing (NLP) has witnessed a profound impact of large\nlanguage models (LLMs) that excel in a multitude of tasks. However, the\nlimitation of LLMs in multilingual settings, particularly in underrepresented\nlanguages, remains a significant hurdle. This thesis aims to bridge the gap in\nNLP research and development by focusing on underrepresented languages. A\ncomprehensive evaluation of LLMs is conducted to assess their capabilities in\nthese languages, revealing the challenges of multilingual and multicultural\ngeneralization. Addressing the multilingual generalization gap, this thesis\nproposes data-and-compute-efficient methods to mitigate the disparity in LLM\nability in underrepresented languages, allowing better generalization on\nunderrepresented languages without the loss of task generalization ability. The\nproposed solutions cover cross-lingual continual instruction tuning,\nretrieval-based cross-lingual in-context learning, and in-context query\nalignment. Furthermore, a novel method to measure cultural values alignment\nbetween LLMs operating in different languages is proposed, ensuring cultural\nsensitivity and inclusivity. These contributions aim to enhance the\nmultilingual and multicultural alignment of LLMs in underrepresented languages,\nultimately advancing the NLP field toward greater equality and inclusiveness.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在多语言环境中存在的局限性，特别是对 underrepresented languages 的支持不足，并通过全面评估揭示了多语言和多文化泛化的挑战。该研究提出了一系列数据和计算高效的方法来缩小这种能力差距，包括跨语言持续指令微调（cross-lingual continual instruction tuning）、基于检索的跨语言上下文学习（retrieval-based cross-lingual in-context learning）和上下文查询对齐（in-context query alignment），这些方法能提升 LLMs 在 underrepresented languages 中的泛化能力，同时保持任务性能。此外，论文引入了一种新颖的方法来测量 LLMs 在不同语言中的文化价值观对齐，确保更高的文化敏感性和包容性，最终推动 NLP 领域向更平等和包容的方向发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD thesis",
      "pdf_url": "http://arxiv.org/pdf/2409.13897v1",
      "published_date": "2024-09-20 20:53:22 UTC",
      "updated_date": "2024-09-20 20:53:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:49:11.992400"
    },
    {
      "arxiv_id": "2410.07124v1",
      "title": "Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Galdran"
      ],
      "abstract": "This short abstract describes a solution to the COSAS 2024 competition on\nCross-Organ and Cross-Scanner Adenocarcinoma Segmentation from\nhistopathological image patches. The main challenge in the task of segmenting\nthis type of cancer is a noticeable domain shift encountered when changing\nacquisition devices (microscopes) and also when tissue comes from different\norgans. The two tasks proposed in COSAS were to train on a dataset of images\nfrom three different organs, and then predict segmentations on data from unseen\norgans (dataset T1), and to train on a dataset of images acquired on three\ndifferent scanners and then segment images acquired with another unseen\nmicroscope. We attempted to bridge the domain shift gap by experimenting with\nthree different strategies: standard training for each dataset, pretraining on\ndataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strategy we\ncall \\textit{Cross-Task Pretraining}), and training on the combination of\ndataset A and B. Our experiments showed that Cross-Task Pre-training is a more\npromising approach to domain generalization.",
      "tldr_zh": "该论文针对COSAS 2024比赛，提出了一种Cross-Task Pretraining方法，用于处理组织病理图像中腺癌分割的领域偏移问题，特别是跨器官和跨扫描仪（microscope）场景。作者实验了三种策略：标准训练、在数据集T1上预训练后微调T2（反之亦然，即Cross-Task Pretraining）、以及联合训练数据集A和B。结果显示，Cross-Task Pretraining在领域泛化（domain generalization）方面表现出色，有望提升跨器官和跨扫描仪的分割准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI2024 COSAS Challenge - short abstract",
      "pdf_url": "http://arxiv.org/pdf/2410.07124v1",
      "published_date": "2024-09-20 20:52:20 UTC",
      "updated_date": "2024-09-20 20:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:49:22.668047"
    },
    {
      "arxiv_id": "2409.13886v1",
      "title": "Learning to Play Video Games with Intuitive Physics Priors",
      "title_zh": "翻译失败",
      "authors": [
        "Abhishek Jaiswal",
        "Nisheeth Srivastava"
      ],
      "abstract": "Video game playing is an extremely structured domain where algorithmic\ndecision-making can be tested without adverse real-world consequences. While\nprevailing methods rely on image inputs to avoid the problem of hand-crafting\nstate space representations, this approach systematically diverges from the way\nhumans actually learn to play games. In this paper, we design object-based\ninput representations that generalize well across a number of video games.\nUsing these representations, we evaluate an agent's ability to learn games\nsimilar to an infant - with limited world experience, employing simple\ninductive biases derived from intuitive representations of physics from the\nreal world. Using such biases, we construct an object category representation\nto be used by a Q-learning algorithm and assess how well it learns to play\nmultiple games based on observed object affordances. Our results suggest that a\nhuman-like object interaction setup capably learns to play several video games,\nand demonstrates superior generalizability, particularly for unfamiliar\nobjects. Further exploring such methods will allow machines to learn in a\nhuman-centric way, thus incorporating more human-like learning benefits.",
      "tldr_zh": "本研究探讨了使用直觉物理先验(Intuitive Physics Priors)让AI学习玩视频游戏，旨在模仿人类学习方式，避免依赖图像输入的传统方法。研究者设计了基于对象的输入表示，并将其与Q-learning算法结合，构建对象类别表示来评估代理在有限世界经验下学习多个游戏的能力。实验结果显示，这种人类-like的对象交互设置在多个游戏中表现出色，具有更好的泛化性，尤其对不熟悉的对象。进一步发展此方法有望使机器采用更人性化的学习方式，提升学习效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, Accepted in Proceedings of the Annual Meeting of the\n  Cognitive Science Society, Volume 46",
      "pdf_url": "http://arxiv.org/pdf/2409.13886v1",
      "published_date": "2024-09-20 20:30:27 UTC",
      "updated_date": "2024-09-20 20:30:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:49:34.966064"
    },
    {
      "arxiv_id": "2409.13884v1",
      "title": "A Multi-LLM Debiasing Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Deonna M. Owens",
        "Ryan A. Rossi",
        "Sungchul Kim",
        "Tong Yu",
        "Franck Dernoncourt",
        "Xiang Chen",
        "Ruiyi Zhang",
        "Jiuxiang Gu",
        "Hanieh Deilamsalehy",
        "Nedim Lipka"
      ],
      "abstract": "Large Language Models (LLMs) are powerful tools with the potential to benefit\nsociety immensely, yet, they have demonstrated biases that perpetuate societal\ninequalities. Despite significant advancements in bias mitigation techniques\nusing data augmentation, zero-shot prompting, and model fine-tuning, biases\ncontinuously persist, including subtle biases that may elude human detection.\nRecent research has shown a growing interest in multi-LLM approaches, which\nhave been demonstrated to be effective in improving the quality of reasoning\nand factuality in LLMs. Building on this approach, we propose a novel multi-LLM\ndebiasing framework aimed at reducing bias in LLMs. Our work is the first to\nintroduce and evaluate two distinct approaches within this framework for\ndebiasing LLMs: a centralized method, where the conversation is facilitated by\na single central LLM, and a decentralized method, where all models communicate\ndirectly. Our findings reveal that our multi-LLM framework significantly\nreduces bias in LLMs, outperforming the baseline method across several social\ngroups.",
      "tldr_zh": "这篇论文提出了一种新颖的多-LLM 去偏框架，旨在减少大型语言模型 (LLMs) 中的偏见问题，这些偏见可能通过数据增强、zero-shot prompting 和模型 fine-tuning 等现有方法无法完全消除。该框架首次引入并评估了两种方法：集中式方法，由一个中央 LLM  facilitation 对话；以及去中心化方法，让所有模型直接通信。实验结果显示，该框架在多个社会群体上显著降低了偏见水平，并优于基线方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13884v1",
      "published_date": "2024-09-20 20:24:50 UTC",
      "updated_date": "2024-09-20 20:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:49:46.998289"
    },
    {
      "arxiv_id": "2409.13882v2",
      "title": "Tabular Data Generation using Binary Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Vitaliy Kinakh",
        "Slava Voloshynovskiy"
      ],
      "abstract": "Generating synthetic tabular data is critical in machine learning, especially\nwhen real data is limited or sensitive. Traditional generative models often\nface challenges due to the unique characteristics of tabular data, such as\nmixed data types and varied distributions, and require complex preprocessing or\nlarge pretrained models. In this paper, we introduce a novel, lossless binary\ntransformation method that converts any tabular data into fixed-size binary\nrepresentations, and a corresponding new generative model called Binary\nDiffusion, specifically designed for binary data. Binary Diffusion leverages\nthe simplicity of XOR operations for noise addition and removal and employs\nbinary cross-entropy loss for training. Our approach eliminates the need for\nextensive preprocessing, complex noise parameter tuning, and pretraining on\nlarge datasets. We evaluate our model on several popular tabular benchmark\ndatasets, demonstrating that Binary Diffusion outperforms existing\nstate-of-the-art models on Travel, Adult Income, and Diabetes datasets while\nbeing significantly smaller in size. Code and models are available at:\nhttps://github.com/vkinakh/binary-diffusion-tabular",
      "tldr_zh": "该研究针对生成合成表格数据面临的挑战（如混合数据类型和分布多样性），提出了一种新颖的无损二进制转换方法，将任何表格数据转换为固定大小的二进制表示。Binary Diffusion 模型专门设计用于二进制数据，通过 XOR 操作添加和移除噪声，并采用 binary cross-entropy loss 进行训练，从而避免了复杂的预处理和噪声参数调整。实验结果显示，该模型在 Travel、Adult Income 和 Diabetes 等基准数据集上超过了现有最先进模型，同时模型尺寸更小，提供了一个高效的合成数据生成方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to 3rd Table Representation Learning Workshop @ NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13882v2",
      "published_date": "2024-09-20 20:22:28 UTC",
      "updated_date": "2024-10-28 22:48:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:49:58.844105"
    },
    {
      "arxiv_id": "2409.13870v3",
      "title": "Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy",
      "title_zh": "翻译失败",
      "authors": [
        "Eric Cullhed"
      ],
      "abstract": "This article presents an experiment in fine-tuning a pretrained causal\nlanguage model (Meta's Llama 3.1 8B Instruct) to assist with restoring missing\nor illegible characters in ancient Greek inscriptions and documentary papyri.\nUtilizing a straightforward instruction-based approach and a 95%/5% train/test\nsplit, the papyrus restoration model achieved a character error rate (CER) of\n14.9%, a top-1 accuracy of 73.5%, and a top-20 accuracy of 86.0% for sequences\nup to 10 characters. A model was also fine-tuned for geographic attribution,\nreaching a top-1 accuracy of 66.4% and a top-3 accuracy of 79.9%. In\nchronological attribution, it demonstrated an average deviation of 21.7 years\nfrom the actual terminus post/ante quem, with a median deviation of 0 years.\nFor inscriptions, the restoration model achieved a CER of 20.5%, a top-1\naccuracy of 63.7%, and a top-20 accuracy of 83.0% for sequences up to 10\ncharacters. In geographic attribution, it attained a top-1 accuracy of 75.0%\nand a top-3 accuracy of 83.7%, while in dating, it had an average deviation of\n37.1 years and a median deviation of 3 years from the actual date range.\nBenchmarked against the state-of-the-art model (Ithaca) on a shared test set\nand on recently edited inscriptions, the instruction-tuned models excelled in\ntext restoration, while also offering the practical advantage of ignoring\nspaces during reconstruction, which aligns with the scriptio continua of\nancient textual artifacts. However, their performance in geographic and\nchronological attribution was lower than Ithaca's. To evaluate the approach in\na more even setup, the instruction model was retrained with an 80%/10%/10%\ntrain-validation-test split, and still outperformed Ithaca in text restoration.\nThe results suggest that fine-tuning larger pretrained causal language models\nusing instruction templates for emendations and conjectures to ancient texts\nholds promise.",
      "tldr_zh": "本研究通过指令微调(instruct-tuning)预训练的因果语言模型(causal language model)，如 Meta's Llama 3.1 8B Instruct，来辅助恢复古希腊纸莎草纸和铭文中的缺失字符。实验结果显示，纸莎草纸的文本恢复模型达到字符错误率(CER) 14.9%、top-1 accuracy 73.5% 和 top-20 accuracy 86.0%，而铭文的 CER 为 20.5%、top-1 accuracy 63.7%；在地理归属和年代归属上，模型表现分别为 top-1 accuracy 66.4% 和平均偏差 21.7 年。相比现有模型 Ithaca，该方法在文本恢复上表现出色，尤其能忽略空格处理 scriptio continua，但地理和年代归属性能稍逊一筹。整体结果表明，这种指令微调方法为古文本修复提供了有前景的改进路径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 1 table. To be submitted",
      "pdf_url": "http://arxiv.org/pdf/2409.13870v3",
      "published_date": "2024-09-20 19:49:45 UTC",
      "updated_date": "2024-11-17 21:28:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:50:14.610809"
    },
    {
      "arxiv_id": "2409.13869v1",
      "title": "Generative AI Carries Non-Democratic Biases and Stereotypes: Representation of Women, Black Individuals, Age Groups, and People with Disability in AI-Generated Images across Occupations",
      "title_zh": "翻译失败",
      "authors": [
        "Ayoob Sadeghiani"
      ],
      "abstract": "AI governance and ethics in AI development have become critical concerns,\nprompting active discussions among tech companies, governments, and researchers\nabout the potential risks AI poses to our democracies. This short essay aims to\nhighlight one such risk: how generative AI includes or excludes\nequity-deserving groups in its outputs. The findings reveal that generative AI\nis not equitably inclusive regarding gender, race, age, and visible disability.",
      "tldr_zh": "这篇论文探讨了生成式AI在输出图像中存在的非民主偏见(stereotypes)和刻板印象，焦点在于女性、Black Individuals、不同年龄群体以及People with Disability在各种职业场景中的代表性。研究通过分析AI生成图像发现，这些equity-deserving群体在性别、种族、年龄和可见残疾方面被不公平地包容或排除。结果表明，生成式AI加剧了社会不平等风险，强调了加强AI治理和伦理发展的必要性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13869v1",
      "published_date": "2024-09-20 19:47:31 UTC",
      "updated_date": "2024-09-20 19:47:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:50:26.749732"
    },
    {
      "arxiv_id": "2409.13867v2",
      "title": "MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Justin Wang",
        "Haimin Hu",
        "Duy Phuong Nguyen",
        "Jaime Fernández Fisac"
      ],
      "abstract": "While robust optimal control theory provides a rigorous framework to compute\nrobot control policies that are provably safe, it struggles to scale to\nhigh-dimensional problems, leading to increased use of deep learning for\ntractable synthesis of robot safety. Unfortunately, existing neural safety\nsynthesis methods often lack convergence guarantees and solution\ninterpretability. In this paper, we present Minimax Actors Guided by Implicit\nCritic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL)\nalgorithm that guarantees local convergence to a minimax equilibrium solution.\nWe then build on this approach to provide local convergence guarantees for a\ngeneral deep RL-based robot safety synthesis algorithm. Through both simulation\nstudies on OpenAI Gym environments and hardware experiments with a\n36-dimensional quadruped robot, we show that MAGICS can yield robust control\npolicies outperforming the state-of-the-art neural safety synthesis methods.",
      "tldr_zh": "本研究针对机器人安全合成中的挑战，提出了一种新型对抗强化学习(Adversarial RL)算法——MAGICS（Minimax Actors Guided by Implicit Critic Stackelberg），它通过最小最大博弈框架确保算法局部收敛到平衡解，同时提升了深度强化学习(RL)方法的收敛保证和可解释性。MAGICS构建于隐式批评者指导的层次化结构之上，用于合成鲁棒的机器人控制策略，以解决传统最优控制理论在高维问题上的扩展性不足。论文进一步扩展此方法，提供了一个通用深度RL-based机器人安全合成算法的局部收敛证明。通过OpenAI Gym模拟环境和36维四足机器人硬件实验，MAGICS生成的控制策略在鲁棒性上超过了现有神经安全合成方法，展示了其实际应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Algorithmic Foundations of Robotics (WAFR) XVI",
      "pdf_url": "http://arxiv.org/pdf/2409.13867v2",
      "published_date": "2024-09-20 19:45:48 UTC",
      "updated_date": "2025-04-27 01:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:50:35.284046"
    },
    {
      "arxiv_id": "2409.13857v1",
      "title": "Wormhole: Concept-Aware Deep Representation Learning for Co-Evolving Sequences",
      "title_zh": "翻译失败",
      "authors": [
        "Kunpeng Xu",
        "Lifei Chen",
        "Shengrui Wang"
      ],
      "abstract": "Identifying and understanding dynamic concepts in co-evolving sequences is\ncrucial for analyzing complex systems such as IoT applications, financial\nmarkets, and online activity logs. These concepts provide valuable insights\ninto the underlying structures and behaviors of sequential data, enabling\nbetter decision-making and forecasting. This paper introduces Wormhole, a novel\ndeep representation learning framework that is concept-aware and designed for\nco-evolving time sequences. Our model presents a self-representation layer and\na temporal smoothness constraint to ensure robust identification of dynamic\nconcepts and their transitions. Additionally, concept transitions are detected\nby identifying abrupt changes in the latent space, signifying a shift to new\nbehavior - akin to passing through a wormhole. This novel mechanism accurately\ndiscerns concepts within co-evolving sequences and pinpoints the exact\nlocations of these wormholes, enhancing the interpretability of the learned\nrepresentations. Experiments demonstrate that this method can effectively\nsegment time series data into meaningful concepts, providing a valuable tool\nfor analyzing complex temporal patterns and advancing the detection of concept\ndrifts.",
      "tldr_zh": "本研究提出Wormhole框架，这是一种概念感知的深度表示学习方法，针对共同演化序列（如IoT应用、金融市场和在线活动日志）中的动态概念识别和理解。框架通过自表示层(self-representation layer)和时间平滑约束(temporal smoothness constraint)来稳健地捕捉概念及其转换，并通过检测潜在空间中的急剧变化（类似于“虫洞”）来精确定位概念漂移点，从而提升表示的可解释性。实验结果显示，Wormhole能有效将时间序列数据分割成有意义的概念，帮助分析复杂时间模式并推进概念漂移(concept drifts)检测。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13857v1",
      "published_date": "2024-09-20 19:11:39 UTC",
      "updated_date": "2024-09-20 19:11:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:50:50.567659"
    },
    {
      "arxiv_id": "2409.13854v2",
      "title": "More Consideration for the Perceptron",
      "title_zh": "翻译失败",
      "authors": [
        "Slimane Larabi"
      ],
      "abstract": "In this paper, we introduce the gated perceptron, an enhancement of the\nconventional perceptron, which incorporates an additional input computed as the\nproduct of the existing inputs. This allows the perceptron to capture\nnon-linear interactions between features, significantly improving its ability\nto classify and regress on complex datasets. We explore its application in both\nlinear and non-linear regression tasks using the Iris dataset, as well as\nbinary and multi-class classification problems, including the PIMA Indian\ndataset and Breast Cancer Wisconsin dataset. Our results demonstrate that the\ngated perceptron can generate more distinct decision regions compared to\ntraditional perceptrons, enhancing its classification capabilities,\nparticularly in handling non-linear data. Performance comparisons show that the\ngated perceptron competes with state-of-the-art classifiers while maintaining a\nsimple architecture.",
      "tldr_zh": "本文提出了一种改进的gated perceptron模型，它在传统perceptron的基础上添加了一个由现有输入乘积计算的额外输入，从而捕捉特征之间的非线性交互，提升了在复杂数据集上的分类和回归能力。该模型应用于Iris数据集的线性与非线性回归任务，以及PIMA Indian数据集和Breast Cancer Wisconsin数据集的二元与多类分类问题。实验结果显示，gated perceptron能够生成更明显的决策区域，尤其在处理非线性数据时表现优异，并与最先进分类器竞争，同时保持简单架构。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13854v2",
      "published_date": "2024-09-20 19:01:29 UTC",
      "updated_date": "2024-09-24 10:57:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:50:59.758188"
    },
    {
      "arxiv_id": "2409.13853v1",
      "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting",
      "title_zh": "通过动态软提示解锁大型语言模型中的记忆化",
      "authors": [
        "Zhepeng Wang",
        "Runxue Bao",
        "Yawen Wu",
        "Jackson Taylor",
        "Cao Xiao",
        "Feng Zheng",
        "Weiwen Jiang",
        "Shangqian Gao",
        "Yanfu Zhang"
      ],
      "abstract": "Pretrained large language models (LLMs) have revolutionized natural language\nprocessing (NLP) tasks such as summarization, question answering, and\ntranslation. However, LLMs pose significant security risks due to their\ntendency to memorize training data, leading to potential privacy breaches and\ncopyright infringement. Accurate measurement of this memorization is essential\nto evaluate and mitigate these potential risks. However, previous attempts to\ncharacterize memorization are constrained by either using prefixes only or by\nprepending a constant soft prompt to the prefixes, which cannot react to\nchanges in input. To address this challenge, we propose a novel method for\nestimating LLM memorization using dynamic, prefix-dependent soft prompts. Our\napproach involves training a transformer-based generator to produce soft\nprompts that adapt to changes in input, thereby enabling more accurate\nextraction of memorized data. Our method not only addresses the limitations of\nprevious methods but also demonstrates superior performance in diverse\nexperimental settings compared to state-of-the-art techniques. In particular,\nour method can achieve the maximum relative improvement of 112.75% and 32.26%\nover the vanilla baseline in terms of discoverable memorization rate for the\ntext generation task and code generation task respectively.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在记忆训练数据时带来的安全风险，如隐私泄露和版权侵犯，并强调了准确测量记忆化的必要性。作者提出了一种新方法，使用动态软提示 (dynamic soft prompting)，通过训练一个基于 Transformer 的生成器来产生适应输入变化的软提示，从而更精确地提取记忆数据。该方法在实验中表现出色，与现有技术相比，在文本生成任务中相对改善112.75%，在代码生成任务中相对改善32.26%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13853v1",
      "published_date": "2024-09-20 18:56:32 UTC",
      "updated_date": "2024-09-20 18:56:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:51:21.114884"
    },
    {
      "arxiv_id": "2409.13852v1",
      "title": "Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Julia Watson",
        "Sophia Lee",
        "Barend Beekhuizen",
        "Suzanne Stevenson"
      ],
      "abstract": "We study language ideologies in text produced by LLMs through a case study on\nEnglish gendered language reform (related to role nouns like\ncongressperson/-woman/-man, and singular they). First, we find political bias:\nwhen asked to use language that is \"correct\" or \"natural\", LLMs use language\nmost similarly to when asked to align with conservative (vs. progressive)\nvalues. This shows how LLMs' metalinguistic preferences can implicitly\ncommunicate the language ideologies of a particular political group, even in\nseemingly non-political contexts. Second, we find LLMs exhibit internal\ninconsistency: LLMs use gender-neutral variants more often when more explicit\nmetalinguistic context is provided. This shows how the language ideologies\nexpressed in text produced by LLMs can vary, which may be unexpected to users.\nWe discuss the broader implications of these findings for value alignment.",
      "tldr_zh": "本研究考察了大型语言模型 (LLMs) 在文本生成中体现的语言意识形态，聚焦于英语性别语言改革（如 role nouns 和 singular they）。研究发现，LLMs 在被要求使用“正确”或“自然”语言时，其偏好更接近保守派价值观，这隐式传达了特定政治团体的意识形态，即使在非政治语境中。另一方面，LLMs 显示出内部不一致性：当提供更多明确的元语言语境时，它们更频繁使用性别中性变体，这可能超出用户预期。最后，这些发现强调了 LLMs 的价值对齐 (value alignment) 问题，并为模型的公正性设计提供了启示。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13852v1",
      "published_date": "2024-09-20 18:55:48 UTC",
      "updated_date": "2024-09-20 18:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:51:23.722261"
    },
    {
      "arxiv_id": "2409.13843v2",
      "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
      "title_zh": "翻译失败",
      "authors": [
        "Robert Morabito",
        "Sangmitra Madhusudan",
        "Tyler McDonald",
        "Ali Emami"
      ],
      "abstract": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models.",
      "tldr_zh": "这篇论文引入了 STOP 数据集，用于评估 Large Language Models (LLMs) 在处理 offensive progressions 的敏感性测试。数据集包含 450 个逐步升级的 offensive 序列，共 2,700 个句子，覆盖 9 个 demographics 和 46 个 sub-demographics，以全面检测偏见。实验评估了 GPT-4、Mixtral 和 Llama 3 等模型，发现这些模型的偏见检测成功率从 19.3% 到 69.8% 不一致。通过与人类判断对齐，模型在 BBQ、StereoSet 和 CrowS-Pairs 等敏感任务上的表现提升了高达 191%，同时维持或改善整体性能。STOP 框架为开发更有效的偏见缓解策略和更公平的 LLMs 提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages (excluding references), accepted to EMNLP 2024 Main\n  Conference",
      "pdf_url": "http://arxiv.org/pdf/2409.13843v2",
      "published_date": "2024-09-20 18:34:38 UTC",
      "updated_date": "2025-02-03 18:06:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:51:37.136907"
    },
    {
      "arxiv_id": "2409.13831v1",
      "title": "Measuring Copyright Risks of Large Language Model via Partial Information Probing",
      "title_zh": "翻译失败",
      "authors": [
        "Weijie Zhao",
        "Huajie Shao",
        "Zhaozhuo Xu",
        "Suzhen Duan",
        "Denghui Zhang"
      ],
      "abstract": "Exploring the data sources used to train Large Language Models (LLMs) is a\ncrucial direction in investigating potential copyright infringement by these\nmodels. While this approach can identify the possible use of copyrighted\nmaterials in training data, it does not directly measure infringing risks.\nRecent research has shifted towards testing whether LLMs can directly output\ncopyrighted content. Addressing this direction, we investigate and assess LLMs'\ncapacity to generate infringing content by providing them with partial\ninformation from copyrighted materials, and try to use iterative prompting to\nget LLMs to generate more infringing content. Specifically, we input a portion\nof a copyrighted text into LLMs, prompt them to complete it, and then analyze\nthe overlap between the generated content and the original copyrighted\nmaterial. Our findings demonstrate that LLMs can indeed generate content highly\noverlapping with copyrighted materials based on these partial inputs.",
      "tldr_zh": "本论文提出了一种通过 Partial Information Probing 方法来评估 Large Language Models (LLMs) 的版权侵权风险，该方法直接测试 LLMs 是否能基于部分受版权保护材料的输入生成侵权内容。研究者向 LLMs 输入版权文本的局部信息，并使用迭代提示技术促使模型完成文本，然后通过分析生成的文本与原内容的重叠度来量化风险。结果显示，LLMs 能够产生高度重叠的侵权内容，这为评估和缓解 LLMs 在版权方面的潜在问题提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13831v1",
      "published_date": "2024-09-20 18:16:05 UTC",
      "updated_date": "2024-09-20 18:16:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:51:46.585754"
    },
    {
      "arxiv_id": "2409.13825v2",
      "title": "A Personalised 3D+t Mesh Generative Model for Unveiling Normal Heart Dynamics",
      "title_zh": "一种个性化的 3D+t 网格生成模型，用于揭示正常心脏动力学",
      "authors": [
        "Mengyun Qiao",
        "Kathryn A McGurk",
        "Shuo Wang",
        "Paul M. Matthews",
        "Declan P O Regan",
        "Wenjia Bai"
      ],
      "abstract": "Understanding the structure and motion of the heart is crucial for diagnosing\nand managing cardiovascular diseases, the leading cause of global death. There\nis wide variation in cardiac shape and motion patterns, that are influenced by\ndemographic, anthropometric and disease factors. Unravelling the normal\npatterns of shape and motion, as well as understanding how each individual\ndeviates from the norm, would facilitate accurate diagnosis and personalised\ntreatment strategies. To this end, we developed a novel conditional generative\nmodel, MeshHeart, to learn the distribution of cardiac shape and motion\npatterns. MeshHeart is capable of generating 3D+t cardiac mesh sequences,\ntaking into account clinical factors such as age, sex, weight and height. To\nmodel the high-dimensional and complex spatio-temporal mesh data, MeshHeart\nemploys a geometric encoder to represent cardiac meshes in a latent space,\nfollowed by a temporal Transformer to model the motion dynamics of latent\nrepresentations. Based on MeshHeart, we investigate the latent space of 3D+t\ncardiac mesh sequences and propose a novel distance metric termed latent delta,\nwhich quantifies the deviation of a real heart from its personalised normative\npattern in the latent space. In experiments using a large dataset of 38,309\nsubjects, MeshHeart demonstrates a high performance in cardiac mesh sequence\nreconstruction and generation. Features defined in the latent space are highly\ndiscriminative for cardiac disease classification, whereas the latent delta\nexhibits strong correlation with clinical phenotypes in phenome-wide\nassociation studies. The codes and models of this study will be released to\nbenefit further research on digital heart modelling.",
      "tldr_zh": "本文提出了一种个性化的条件生成模型MeshHeart，用于学习心脏形状和运动模式的分布，以揭示正常心脏动态并支持心血管疾病诊断。该模型通过几何编码器将心脏网格表示到潜在空间，并结合temporal Transformer处理3D+t网格序列的时空动态，同时考虑临床因素如年龄、性别、体重和身高。实验在38,309个受试者的数据集上显示，MeshHeart在心脏网格序列重建和生成方面表现出色，且潜在空间特征及新提出的latent delta度量在心脏病分类和临床表型关联中具有高度区分性和相关性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Nature Machine Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2409.13825v2",
      "published_date": "2024-09-20 18:08:37 UTC",
      "updated_date": "2025-04-14 12:07:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:51:59.542754"
    },
    {
      "arxiv_id": "2409.13688v1",
      "title": "Morphological Detection and Classification of Microplastics and Nanoplastics Emerged from Consumer Products by Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hadi Rezvani",
        "Navid Zarrabi",
        "Ishaan Mehta",
        "Christopher Kolios",
        "Hussein Ali Jaafar",
        "Cheng-Hao Kao",
        "Sajad Saeedi",
        "Nariman Yousefi"
      ],
      "abstract": "Plastic pollution presents an escalating global issue, impacting health and\nenvironmental systems, with micro- and nanoplastics found across mediums from\npotable water to air. Traditional methods for studying these contaminants are\nlabor-intensive and time-consuming, necessitating a shift towards more\nefficient technologies. In response, this paper introduces micro- and\nnanoplastics (MiNa), a novel and open-source dataset engineered for the\nautomatic detection and classification of micro and nanoplastics using object\ndetection algorithms. The dataset, comprising scanning electron microscopy\nimages simulated under realistic aquatic conditions, categorizes plastics by\npolymer type across a broad size spectrum. We demonstrate the application of\nstate-of-the-art detection algorithms on MiNa, assessing their effectiveness\nand identifying the unique challenges and potential of each method. The dataset\nnot only fills a critical gap in available resources for microplastic research\nbut also provides a robust foundation for future advancements in the field.",
      "tldr_zh": "塑料污染是全球性问题，microplastics 和 nanoplastics 广泛存在于水和空气中，但传统检测方法耗时费力。本文引入了一个新型开源数据集 MiNa，由扫描电子显微镜图像组成，模拟现实水下条件，按聚合物类型和大小范围分类，用于自动检测和分类这些污染物。通过应用最先进的对象检测算法，我们评估了其有效性，识别了独特挑战和潜力，为微塑料研究领域提供关键资源并推动未来进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.AP",
        "stat.ME"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13688v1",
      "published_date": "2024-09-20 17:56:25 UTC",
      "updated_date": "2024-09-20 17:56:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:52:11.768808"
    },
    {
      "arxiv_id": "2409.13686v2",
      "title": "The Impact of Large Language Models in Academia: from Writing to Speaking",
      "title_zh": "翻译失败",
      "authors": [
        "Mingmeng Geng",
        "Caixi Chen",
        "Yanru Wu",
        "Dongping Chen",
        "Yao Wan",
        "Pan Zhou"
      ],
      "abstract": "Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.",
      "tldr_zh": "本研究考察了大型语言模型（LLMs）对学术界的写作和演讲影响，基于超过30,000篇论文和1,000场机器学习会议演讲，首次大规模比较了LLMs在两种主要沟通方式中的作用。结果显示，LLM风格词汇如“significant”在摘要和口头演讲中使用频率显著增加，表明LLMs对演讲的影响正逐步显现。研究强调了LLMs的隐性影响及其对人类社会的潜在涟漪效应，呼吁进一步关注。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.DL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.13686v2",
      "published_date": "2024-09-20 17:54:16 UTC",
      "updated_date": "2024-10-22 17:06:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:52:22.686980"
    },
    {
      "arxiv_id": "2409.13684v3",
      "title": "The FIX Benchmark: Extracting Features Interpretable to eXperts",
      "title_zh": "翻译失败",
      "authors": [
        "Helen Jin",
        "Shreya Havaldar",
        "Chaehyeon Kim",
        "Anton Xue",
        "Weiqiu You",
        "Helen Qu",
        "Marco Gatti",
        "Daniel A Hashimoto",
        "Bhuvnesh Jain",
        "Amin Madani",
        "Masao Sako",
        "Lyle Ungar",
        "Eric Wong"
      ],
      "abstract": "Feature-based methods are commonly used to explain model predictions, but\nthese methods often implicitly assume that interpretable features are readily\navailable. However, this is often not the case for high-dimensional data, and\nit can be hard even for domain experts to mathematically specify which features\nare important. Can we instead automatically extract collections or groups of\nfeatures that are aligned with expert knowledge? To address this gap, we\npresent FIX (Features Interpretable to eXperts), a benchmark for measuring how\nwell a collection of features aligns with expert knowledge. In collaboration\nwith domain experts, we propose FIXScore, a unified expert alignment measure\napplicable to diverse real-world settings across cosmology, psychology, and\nmedicine domains in vision, language, and time series data modalities. With\nFIXScore, we find that popular feature-based explanation methods have poor\nalignment with expert-specified knowledge, highlighting the need for new\nmethods that can better identify features interpretable to experts.",
      "tldr_zh": "这篇论文针对特征解释方法（feature-based explanation methods）中可解释特征难以获取的问题，提出了 FIX（Features Interpretable to eXperts）基准，用于自动提取与专家知识对齐的特征集合。研究者与领域专家合作，开发了 FIXScore，这是一个适用于宇宙学、心理学和医学等领域的统一专家对齐度量，涵盖视觉、语言和时间序列数据模态。实验结果显示，现有流行方法与专家知识的对齐度较低，强调了需要开发更有效的特征提取新方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13684v3",
      "published_date": "2024-09-20 17:53:03 UTC",
      "updated_date": "2024-12-23 19:39:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:52:36.507002"
    },
    {
      "arxiv_id": "2409.13682v1",
      "title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Abrar Anwar",
        "John Welsh",
        "Joydeep Biswas",
        "Soha Pouya",
        "Yan Chang"
      ],
      "abstract": "Navigating and understanding complex environments over extended periods of\ntime is a significant challenge for robots. People interacting with the robot\nmay want to ask questions like where something happened, when it occurred, or\nhow long ago it took place, which would require the robot to reason over a long\nhistory of their deployment. To address this problem, we introduce a\nRetrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed\nfor long-horizon video question answering for robot navigation. To evaluate\nReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal,\nand descriptive questions to long-horizon robot navigation videos. ReMEmbR\nemploys a structured approach involving a memory building and a querying phase,\nleveraging temporal information, spatial information, and images to efficiently\nhandle continuously growing robot histories. Our experiments demonstrate that\nReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve\neffective long-horizon reasoning with low latency. Additionally, we deploy\nReMEmbR on a robot and show that our approach can handle diverse queries. The\ndataset, code, videos, and other material can be found at the following link:\nhttps://nvidia-ai-iot.github.io/remembr",
      "tldr_zh": "该研究引入了ReMEmbR系统，一种检索增强记忆框架，用于机器人导航中构建和推理长时段时空记忆，以处理用户查询如事件位置、时间或持续期。ReMEmbR采用结构化方法，包括记忆构建阶段（利用时间信息、空间信息和图像）和查询阶段，以高效管理不断增长的机器人历史。研究者创建了NaVQA数据集，用于评估空间、时间和描述性问题，并在实验中证明ReMEmbR优于LLM和VLM基线，实现低延迟的长时段推理；此外，该系统已在实际机器人上部署，并成功处理多样查询。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13682v1",
      "published_date": "2024-09-20 17:50:07 UTC",
      "updated_date": "2024-09-20 17:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:52:47.247697"
    },
    {
      "arxiv_id": "2409.13661v3",
      "title": "Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models",
      "title_zh": "高效的领域增强方法，用于自动驾驶测试，基于扩散模型",
      "authors": [
        "Luciano Baresi",
        "Davide Yi Xian Hu",
        "Andrea Stocco",
        "Paolo Tonella"
      ],
      "abstract": "Simulation-based testing is widely used to assess the reliability of\nAutonomous Driving Systems (ADS), but its effectiveness is limited by the\noperational design domain (ODD) conditions available in such simulators. To\naddress this limitation, in this work, we explore the integration of generative\nartificial intelligence techniques with physics-based simulators to enhance ADS\nsystem-level testing. Our study evaluates the effectiveness and computational\noverhead of three generative strategies based on diffusion models, namely\ninstruction-editing, inpainting, and inpainting with refinement. Specifically,\nwe assess these techniques' capabilities to produce augmented\nsimulator-generated images of driving scenarios representing new ODDs. We\nemploy a novel automated detector for invalid inputs based on semantic\nsegmentation to ensure semantic preservation and realism of the neural\ngenerated images. We then perform system-level testing to evaluate the ADS's\ngeneralization ability to newly synthesized ODDs. Our findings show that\ndiffusion models help increase the ODD coverage for system-level testing of\nADS. Our automated semantic validator achieved a percentage of false positives\nas low as 3%, retaining the correctness and quality of the generated images for\ntesting. Our approach successfully identified new ADS system failures before\nreal-world testing.",
      "tldr_zh": "该论文探讨了使用扩散模型（diffusion models）来提升自动驾驶系统（ADS）的模拟测试效率，旨在通过生成式 AI 技术扩展操作设计域（ODD）的覆盖范围。研究评估了三种基于扩散模型的策略——instruction-editing、inpainting 和 inpainting with refinement——来生成新的模拟图像，并引入基于语义分割的自动检测器，确保图像的语义完整性和真实性，错误率低至 3%。结果显示，该方法显著提高了 ADS 在新 ODD 场景下的测试覆盖率，并成功识别了潜在系统故障，为更可靠的自动驾驶测试提供了高效解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at the 47th International Conference on\n  Software Engineering (ICSE 2025). This research was partially supported by\n  project EMELIOT, funded by MUR under the PRIN 2020 program (n. 2020W3A5FY),\n  by the Bavarian Ministry of Economic Affairs, Regional Development and\n  Energy, by the TUM Global Incentive Fund, and by the EU Project Sec4AI4Sec\n  (n. 101120393)",
      "pdf_url": "http://arxiv.org/pdf/2409.13661v3",
      "published_date": "2024-09-20 17:09:45 UTC",
      "updated_date": "2025-02-17 15:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:52:59.284571"
    },
    {
      "arxiv_id": "2409.13652v2",
      "title": "OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen Zhang",
        "Vardan Papyan"
      ],
      "abstract": "The recent paradigm shift to large-scale foundation models has brought about\na new era for deep learning that, while has found great success in practice,\nhas also been plagued by prohibitively expensive costs in terms of high memory\nconsumption and compute. To mitigate these issues, there has been a concerted\neffort in post-hoc neural network pruning techniques that do not require costly\nretraining. Despite the considerable progress being made, existing methods\noften exhibit a steady drop in model performance as the compression increases.\nIn this paper, we present a novel approach to compressing large transformers,\ncoined OATS, that utilizes the second moment information in the input\nembeddings to decompose the model weights into a sum of sparse and low-rank\nmatrices. Without any retraining, OATS achieves state-of-the-art performance\nwhen compressing models by up to $60\\%$ on large language models such as\nLlama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while\ndelivering up to $1.37\\times$ the CPU acceleration versus a model that was\ncomparably pruned.",
      "tldr_zh": "该论文提出了一种名为 OATS 的新型神经网络剪枝方法，旨在通过 outlier-aware 机制处理大型 Transformer 模型的高内存和计算成本问题。OATS 利用输入嵌入的 second moment information，将模型权重分解为 sparse 和 low-rank 矩阵之和，从而实现高效压缩，而无需重新训练。在实验中，OATS 在压缩 Llama-3、Phi-3、ViT 和 DINOv2 等模型高达 60% 时，达到了 state-of-the-art 性能，并提供了高达 1.37 倍的 CPU 加速。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13652v2",
      "published_date": "2024-09-20 17:02:00 UTC",
      "updated_date": "2025-02-28 22:39:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:53:11.485134"
    },
    {
      "arxiv_id": "2409.13621v2",
      "title": "Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Li",
        "Qiang Gao",
        "Hongmei Wu",
        "Li Huang"
      ],
      "abstract": "Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.",
      "tldr_zh": "本文针对事件因果关系识别 (ECI) 的挑战，提出了一种简单有效的 Semantic Dependency Inquiry Network (SemDI)，以解决现有方法依赖因果特征和外部知识导致的显式线索缺失和潜在偏差问题。SemDI 通过统一编码器捕获上下文中的语义依赖，然后利用 Cloze Analyzer 生成填充令牌，并以此查询两个事件之间的因果关系。实验在三个广泛使用的基准上证明，SemDI 超过了最先进的方法，展示了其优越性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 camera-ready version. Code is released at\n  https://github.com/hrlics/SemDI",
      "pdf_url": "http://arxiv.org/pdf/2409.13621v2",
      "published_date": "2024-09-20 16:32:54 UTC",
      "updated_date": "2024-10-02 06:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:53:24.114813"
    },
    {
      "arxiv_id": "2409.13609v3",
      "title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Liu",
        "Zunnan Xu",
        "Yue Hu",
        "Liangtao Shi",
        "Zhiqiang Wang",
        "Quanjun Yin"
      ],
      "abstract": "Referring Expression Comprehension (REC), which aims to ground a local visual\nregion via natural language, is a task that heavily relies on multimodal\nalignment. Most existing methods utilize powerful pre-trained models to\ntransfer visual/linguistic knowledge by full fine-tuning. However, full\nfine-tuning the entire backbone not only breaks the rich prior knowledge\nembedded in the pre-training, but also incurs significant computational costs.\nMotivated by the recent emergence of Parameter-Efficient Transfer Learning\n(PETL) methods, we aim to solve the REC task in an effective and efficient\nmanner. Directly applying these PETL methods to the REC task is inappropriate,\nas they lack the specific-domain abilities for precise local visual perception\nand visual-language alignment. Therefore, we propose a novel framework of\nMultimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.\nSpecifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned\nprior, and Local Convolution Adapters to extract precise local semantics for\nbetter visual perception. Moreover, the Prior-Guided Text module is proposed to\nfurther utilize the prior for facilitating the cross-modal alignment.\nExperimental results on three widely-used benchmarks demonstrate that MaPPER\nachieves the best accuracy compared to the full fine-tuning and other PETL\nmethods with only 1.41% tunable backbone parameters. Our code is available at\nhttps://github.com/liuting20/MaPPER.",
      "tldr_zh": "本研究针对 Referring Expression Comprehension (REC) 任务，提出了一种 Multimodal Prior-guided Parameter Efficient Tuning (MaPPER) 框架，以高效实现视觉-语言对齐，同时避免传统全微调方法破坏预训练知识和增加计算开销。MaPPER 包括 Dynamic Prior Adapters 用于指导对齐先验、Local Convolution Adapters 提取精确本地语义以提升视觉感知，以及 Prior-Guided Text 模块进一步促进跨模态对齐。实验在三个常用基准上显示，MaPPER 仅微调 1.41% 的骨干参数，便在准确率上超越全微调和其他 Parameter-Efficient Transfer Learning (PETL) 方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2024 main",
      "pdf_url": "http://arxiv.org/pdf/2409.13609v3",
      "published_date": "2024-09-20 16:12:26 UTC",
      "updated_date": "2025-01-02 15:26:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:53:36.058603"
    },
    {
      "arxiv_id": "2409.13602v1",
      "title": "MeLIAD: Interpretable Few-Shot Anomaly Detection with Metric Learning and Entropy-based Scoring",
      "title_zh": "翻译失败",
      "authors": [
        "Eirini Cholopoulou",
        "Dimitris K. Iakovidis"
      ],
      "abstract": "Anomaly detection (AD) plays a pivotal role in multimedia applications for\ndetecting defective products and automating quality inspection. Deep learning\n(DL) models typically require large-scale annotated data, which are often\nhighly imbalanced since anomalies are usually scarce. The black box nature of\nthese models prohibits them from being trusted by users. To address these\nchallenges, we propose MeLIAD, a novel methodology for interpretable anomaly\ndetection, which unlike the previous methods is based on metric learning and\nachieves interpretability by design without relying on any prior distribution\nassumptions of true anomalies. MeLIAD requires only a few samples of anomalies\nfor training, without employing any augmentation techniques, and is inherently\ninterpretable, providing visualizations that offer insights into why an image\nis identified as anomalous. This is achieved by introducing a novel trainable\nentropy-based scoring component for the identification and localization of\nanomalous instances, and a novel loss function that jointly optimizes the\nanomaly scoring component with a metric learning objective. Experiments on five\npublic benchmark datasets, including quantitative and qualitative evaluation of\ninterpretability, demonstrate that MeLIAD achieves improved anomaly detection\nand localization performance compared to state-of-the-art methods.",
      "tldr_zh": "本文提出 MeLIAD，一种基于 Metric Learning 的可解释 Few-Shot Anomaly Detection 方法，旨在解决深度学习模型在异常检测中面临的数据不平衡和黑盒问题，而无需假设异常的先验分布。MeLIAD 仅需少量异常样本进行训练，引入了一个新的 Entropy-based Scoring 组件和联合优化的 Loss Function，以实现异常识别、定位并提供可视化解释。实验结果显示，在五个公共基准数据集上，MeLIAD 比现有最先进方法取得了更高的异常检测和定位性能，并在定量和定性可解释性评估中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2409.13602v1",
      "published_date": "2024-09-20 16:01:43 UTC",
      "updated_date": "2024-09-20 16:01:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:53:49.049895"
    },
    {
      "arxiv_id": "2409.13592v1",
      "title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Abhilash Nandy",
        "Yash Agarwal",
        "Ashish Patwa",
        "Millon Madhur Das",
        "Aman Bansal",
        "Ankit Raj",
        "Pawan Goyal",
        "Niloy Ganguly"
      ],
      "abstract": "Understanding satire and humor is a challenging task for even current\nVision-Language models. In this paper, we propose the challenging tasks of\nSatirical Image Detection (detecting whether an image is satirical),\nUnderstanding (generating the reason behind the image being satirical), and\nCompletion (given one half of the image, selecting the other half from 2 given\noptions, such that the complete image is satirical) and release a high-quality\ndataset YesBut, consisting of 2547 images, 1084 satirical and 1463\nnon-satirical, containing different artistic styles, to evaluate those tasks.\nEach satirical image in the dataset depicts a normal scenario, along with a\nconflicting scenario which is funny or ironic. Despite the success of current\nVision-Language Models on multimodal tasks such as Visual QA and Image\nCaptioning, our benchmarking experiments show that such models perform poorly\non the proposed tasks on the YesBut Dataset in Zero-Shot Settings w.r.t both\nautomated as well as human evaluation. Additionally, we release a dataset of\n119 real, satirical photographs for further research. The dataset and code are\navailable at https://github.com/abhi1nandy2/yesbut_dataset.",
      "tldr_zh": "本论文提出YesBut数据集，这是一个高质量的多模态标注数据集，用于评估Vision-Language Models的讽刺理解能力，包括Satirical Image Detection（检测图像是否讽刺）、Understanding（生成讽刺原因）和Completion（从选项中选择图像另一半以形成讽刺完整图像）等任务。该数据集包含2547张图像（1084张讽刺图像和1463张非讽刺图像），涵盖多种艺术风格，并展示了讽刺图像中正常场景与冲突场景的对比。实验结果表明，现有Vision-Language Models在零样本设置下表现较差，无论自动评估还是人工评估；此外，论文还发布了另一个包含119张真实讽刺照片的数据集，以支持进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2024 Main (Long), 18 pages, 14 figures, 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.13592v1",
      "published_date": "2024-09-20 15:45:29 UTC",
      "updated_date": "2024-09-20 15:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:54:00.578614"
    },
    {
      "arxiv_id": "2409.13588v2",
      "title": "ChainBuddy: An AI Agent System for Generating LLM Pipelines",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyue Zhang",
        "Ian Arawjo"
      ],
      "abstract": "As large language models (LLMs) advance, their potential applications have\ngrown significantly. However, it remains difficult to evaluate LLM behavior on\nuser-defined tasks and craft effective pipelines to do so. Many users struggle\nwith where to start, often referred to as the \"blank page problem.\" ChainBuddy,\nan AI workflow generation assistant built into the ChainForge platform, aims to\ntackle this issue. From a single prompt or chat, ChainBuddy generates a starter\nevaluative LLM pipeline in ChainForge aligned to the user's requirements.\nChainBuddy offers a straightforward and user-friendly way to plan and evaluate\nLLM behavior and make the process less daunting and more accessible across a\nwide range of possible tasks and use cases. We report a within-subjects user\nstudy comparing ChainBuddy to the baseline interface. We find that when using\nAI assistance, participants reported a less demanding workload, felt more\nconfident, and produced higher quality pipelines evaluating LLM behavior.\nHowever, we also uncover a mismatch between subjective and objective ratings of\nperformance: participants rated their successfulness similarly across\nconditions, while independent experts rated participant workflows significantly\nhigher with AI assistance. Drawing connections to the Dunning-Kruger effect, we\ndraw design implications for the future of workflow generation assistants to\nmitigate the risk of over-reliance.",
      "tldr_zh": "该研究介绍了 ChainBuddy，一种构建在 ChainForge 平台上的 AI Agent 系统，旨在帮助用户从单一提示或聊天生成评估 LLM (Large Language Models) 行为的起始管道，从而解决“blank page problem”并简化 LLM 管道的创建过程。ChainBuddy 通过用户友好的工作流生成方法，使评估任务更易访问，并减少用户工作量和提升信心。用户研究显示，使用 AI 助手后，参与者主观上感觉更成功，但独立专家的客观评估显示管道质量显著提高，同时研究揭示了主观与客观评分的不匹配，可能与 Dunning-Kruger effect 相关，并提出设计启示以降低过度依赖风险。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2; I.2"
      ],
      "primary_category": "cs.HC",
      "comment": "21 pages, 12 figures, pre-print",
      "pdf_url": "http://arxiv.org/pdf/2409.13588v2",
      "published_date": "2024-09-20 15:42:33 UTC",
      "updated_date": "2025-02-08 21:59:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:54:11.844945"
    },
    {
      "arxiv_id": "2409.13585v1",
      "title": "Neurosymbolic Conformal Classification",
      "title_zh": "神经符号保形分类",
      "authors": [
        "Arthur Ledaguenel",
        "Céline Hudelot",
        "Mostepha Khouadjia"
      ],
      "abstract": "The last decades have seen a drastic improvement of Machine Learning (ML),\nmainly driven by Deep Learning (DL). However, despite the resounding successes\nof ML in many domains, the impossibility to provide guarantees of conformity\nand the fragility of ML systems (faced with distribution shifts, adversarial\nattacks, etc.) have prevented the design of trustworthy AI systems. Several\nresearch paths have been investigated to mitigate this fragility and provide\nsome guarantees regarding the behavior of ML systems, among which are\nneurosymbolic AI and conformal prediction. Neurosymbolic artificial\nintelligence is a growing field of research aiming to combine neural network\nlearning capabilities with the reasoning abilities of symbolic systems. One of\nthe objective of this hybridization can be to provide theoritical guarantees\nthat the output of the system will comply with some prior knowledge. Conformal\nprediction is a set of techniques that enable to take into account the\nuncertainty of ML systems by transforming the unique prediction into a set of\npredictions, called a confidence set. Interestingly, this comes with\nstatistical guarantees regarding the presence of the true label inside the\nconfidence set. Both approaches are distribution-free and model-agnostic. In\nthis paper, we see how these two approaches can complement one another. We\nintroduce several neurosymbolic conformal prediction techniques and explore\ntheir different characteristics (size of confidence sets, computational\ncomplexity, etc.).",
      "tldr_zh": "尽管机器学习（ML），尤其是深度学习（DL），取得了显著进展，但其在一致性保证和鲁棒性方面存在缺陷，如面对分布偏移和对抗攻击。论文提出将神经符号 AI（neurosymbolic AI）和置信预测（conformal prediction）相结合，旨在通过神经网络的学习能力与符号系统的推理能力融合，提供理论上的输出合规保证。作者引入了几种神经符号置信预测技术，这些方法是分布无关和模型无关的，并探讨了它们的特性，包括置信集的大小和计算复杂性，从而为构建更可靠的 AI 系统铺平道路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 0 figures. arXiv admin note: text overlap with\n  arXiv:2404.08404",
      "pdf_url": "http://arxiv.org/pdf/2409.13585v1",
      "published_date": "2024-09-20 15:38:34 UTC",
      "updated_date": "2024-09-20 15:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:54:24.965935"
    },
    {
      "arxiv_id": "2409.13582v1",
      "title": "Time and Tokens: Benchmarking End-to-End Speech Dysfluency Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanru Zhou",
        "Jiachen Lian",
        "Cheol Jun Cho",
        "Jingwen Liu",
        "Zongli Ye",
        "Jinming Zhang",
        "Brittany Morin",
        "David Baquirin",
        "Jet Vonk",
        "Zoe Ezzes",
        "Zachary Miller",
        "Maria Luisa Gorno Tempini",
        "Gopala Anumanchipalli"
      ],
      "abstract": "Speech dysfluency modeling is a task to detect dysfluencies in speech, such\nas repetition, block, insertion, replacement, and deletion. Most recent\nadvancements treat this problem as a time-based object detection problem. In\nthis work, we revisit this problem from a new perspective: tokenizing\ndysfluencies and modeling the detection problem as a token-based automatic\nspeech recognition (ASR) problem. We propose rule-based speech and text\ndysfluency simulators and develop VCTK-token, and then develop a Whisper-like\nseq2seq architecture to build a new benchmark with decent performance. We also\nsystematically compare our proposed token-based methods with time-based\nmethods, and propose a unified benchmark to facilitate future research\nendeavors. We open-source these resources for the broader scientific community.\nThe project page is available at https://rorizzz.github.io/",
      "tldr_zh": "本研究重新定义了语音流畅性检测问题，将其从传统的基于时间物体检测视角转变为基于标记的自动语音识别（ASR）问题，旨在检测如重复、阻塞等dysfluencies。研究者提出规则-based语音和文本流畅性模拟器，开发了VCTK-token数据集，并构建了一个类似Whisper的seq2seq架构，建立新的端到端基准模型。实验结果显示，该token-based方法在性能上表现出色，并通过系统比较与time-based方法，提出统一基准以促进未来研究，同时开源所有资源。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13582v1",
      "published_date": "2024-09-20 15:35:32 UTC",
      "updated_date": "2024-09-20 15:35:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:54:35.924843"
    },
    {
      "arxiv_id": "2409.13576v2",
      "title": "Region Prompt Tuning: Fine-grained Scene Text Detection Utilizing Region Text Prompt",
      "title_zh": "区域提示调优：利用区域文本提示的细粒度场景文本检测",
      "authors": [
        "Xingtao Lin",
        "Heqian Qiu",
        "Lanxiao Wang",
        "Ruihang Wang",
        "Linfeng Xu",
        "Hongliang Li"
      ],
      "abstract": "Recent advancements in prompt tuning have successfully adapted large-scale\nmodels like Contrastive Language-Image Pre-trained (CLIP) for downstream tasks\nsuch as scene text detection. Typically, text prompt complements the text\nencoder's input, focusing on global features while neglecting fine-grained\ndetails, leading to fine-grained text being ignored in task of scene text\ndetection. In this paper, we propose the region prompt tuning (RPT) method for\nfine-grained scene text detection, where region text prompt proposed would help\nfocus on fine-grained features. Region prompt tuning method decomposes region\ntext prompt into individual characters and splits visual feature map into\nregion visual tokens, creating a one-to-one correspondence between characters\nand tokens. This allows a character matches the local features of a token,\nthereby avoiding the omission of detailed features and fine-grained text. To\nachieve this, we introduce a sharing position embedding to link each character\nwith its corresponding token and employ a bidirectional distance loss to align\neach region text prompt character with the target ``text''. To refine the\ninformation at fine-grained level, we implement character-token level\ninteractions before and after encoding. Our proposed method combines a general\nscore map from the image-text process with a region score map derived from\ncharacter-token matching, producing a final score map that could balance the\nglobal and local features and be fed into DBNet to detect the text. Experiments\non benchmarks like ICDAR2015, TotalText, and CTW1500 demonstrate RPT impressive\nperformance, underscoring its effectiveness for scene text detection.",
      "tldr_zh": "本论文提出 region prompt tuning (RPT) 方法，以解决传统 prompt tuning 在场景文本检测中忽略细粒度细节的问题。RPT 通过将 region text prompt 分解为单个字符，并与视觉特征图的 region visual tokens 建立一一对应，利用 sharing position embedding 和 bidirectional distance loss 来对齐字符与目标文本，并在编码前后进行 character-token 级交互。最终，该方法结合图像-文本过程的通用分数图和 region score map，生成平衡全局与局部特征的最终分数图，并输入 DBNet 进行文本检测。实验在 ICDAR2015、TotalText 和 CTW1500 等基准上展示了 RPT 的出色性能，显著提升了检测准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13576v2",
      "published_date": "2024-09-20 15:24:26 UTC",
      "updated_date": "2024-11-19 19:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:54:49.309270"
    },
    {
      "arxiv_id": "2409.13571v1",
      "title": "Scalable Multi-agent Reinforcement Learning for Factory-wide Dynamic Scheduling",
      "title_zh": "用于工厂范围动态调度的可扩展多智能体强化学习",
      "authors": [
        "Jaeyeon Jang",
        "Diego Klabjan",
        "Han Liu",
        "Nital S. Patel",
        "Xiuqi Li",
        "Balakrishnan Ananthanarayanan",
        "Husam Dauod",
        "Tzung-Han Juang"
      ],
      "abstract": "Real-time dynamic scheduling is a crucial but notoriously challenging task in\nmodern manufacturing processes due to its high decision complexity. Recently,\nreinforcement learning (RL) has been gaining attention as an impactful\ntechnique to handle this challenge. However, classical RL methods typically\nrely on human-made dispatching rules, which are not suitable for large-scale\nfactory-wide scheduling. To bridge this gap, this paper applies a\nleader-follower multi-agent RL (MARL) concept to obtain desired coordination\nafter decomposing the scheduling problem into a set of sub-problems that are\nhandled by each individual agent for scalability. We further strengthen the\nprocedure by proposing a rule-based conversion algorithm to prevent\ncatastrophic loss of production capacity due to an agent's error. Our\nexperimental results demonstrate that the proposed model outperforms the\nstate-of-the-art deep RL-based scheduling models in various aspects.\nAdditionally, the proposed model provides the most robust scheduling\nperformance to demand changes. Overall, the proposed MARL-based scheduling\nmodel presents a promising solution to the real-time scheduling problem, with\npotential applications in various manufacturing industries.",
      "tldr_zh": "本论文针对现代制造中实时动态调度的决策复杂性，提出了一种可扩展的多智能体强化学习(MARL)方法，通过leader-follower框架将调度问题分解为子问题，每个代理负责处理，以提升整体协调性。论文进一步引入规则-based转换算法，以防止代理错误导致生产能力灾难性损失。实验结果表明，该模型在各种性能指标上优于现有深度RL-based调度模型，并对需求变化提供最强的鲁棒性，为制造行业的实时调度应用提供了有前景的解决方案。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13571v1",
      "published_date": "2024-09-20 15:16:37 UTC",
      "updated_date": "2024-09-20 15:16:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:55:00.271396"
    },
    {
      "arxiv_id": "2409.13566v2",
      "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Tensorflow Pretrained Models",
      "title_zh": "翻译失败",
      "authors": [
        "Keyu Chen",
        "Ziqian Bi",
        "Qian Niu",
        "Junyu Liu",
        "Benji Peng",
        "Sen Zhang",
        "Ming Liu",
        "Ming Li",
        "Xuanhe Pan",
        "Jiawei Xu",
        "Jinlang Wang",
        "Pohsun Feng"
      ],
      "abstract": "The application of TensorFlow pre-trained models in deep learning is\nexplored, with an emphasis on practical guidance for tasks such as image\nclassification and object detection. The study covers modern architectures,\nincluding ResNet, MobileNet, and EfficientNet, and demonstrates the\neffectiveness of transfer learning through real-world examples and experiments.\nA comparison of linear probing and model fine-tuning is presented, supplemented\nby visualizations using techniques like PCA, t-SNE, and UMAP, allowing for an\nintuitive understanding of the impact of these approaches. The work provides\ncomplete example code and step-by-step instructions, offering valuable insights\nfor both beginners and advanced users. By integrating theoretical concepts with\nhands-on practice, the paper equips readers with the tools necessary to address\ndeep learning challenges efficiently.",
      "tldr_zh": "这篇论文探讨了TensorFlow预训练模型在深度学习中的应用，重点提供图像分类和物体检测等任务的实用指导。它介绍了ResNet、MobileNet和EfficientNet等现代架构，通过迁移学习(transfer learning)的真实案例和实验证明了其有效性，并比较了linear probing和model fine-tuning的影响，同时利用PCA、t-SNE和UMAP等可视化技术增强直观理解。论文还提供了完整的示例代码和逐步指令，结合理论与实践，帮助初学者和高级用户高效应对深度学习挑战。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This book contains 148 pages and 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13566v2",
      "published_date": "2024-09-20 15:07:14 UTC",
      "updated_date": "2024-12-11 04:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:55:12.347116"
    },
    {
      "arxiv_id": "2409.15380v3",
      "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino",
      "title_zh": "翻译失败",
      "authors": [
        "Jann Railey Montalan",
        "Jian Gang Ngui",
        "Wei Qi Leong",
        "Yosephine Susanto",
        "Hamsawardhini Rengarajan",
        "Alham Fikri Aji",
        "William Chandra Tjhi"
      ],
      "abstract": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.",
      "tldr_zh": "本研究引入了 Kalahi，这是一个由菲律宾本土人士合作创建的手工化、草根式文化 LLM 评估套件，旨在评估大型语言模型（LLMs）是否能生成与菲律宾共享文化知识和价值观相关的响应。Kalahi 包含 150 个高质量、手工制作的细致提示，用于测试模型在文化相关情境下的生成性能。实验结果显示，支持多语言和菲律宾语的 LLMs 表现不佳，最佳模型仅正确回答 46.0% 的问题，而本土菲律宾人的准确率高达 89.10%，突显了 Kalahi 在可靠评估和改进 LLM 中的菲律宾文化表示方面的价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for presentation at Paclic 38, 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.15380v3",
      "published_date": "2024-09-20 15:01:21 UTC",
      "updated_date": "2024-12-18 14:39:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:55:24.000710"
    },
    {
      "arxiv_id": "2409.13559v1",
      "title": "Efficient Visualization of Neural Networks with Generative Models and Adversarial Perturbations",
      "title_zh": "利用生成模型和对抗扰动的神经网络高效可视化",
      "authors": [
        "Athanasios Karagounis"
      ],
      "abstract": "This paper presents a novel approach for deep visualization via a generative\nnetwork, offering an improvement over existing methods. Our model simplifies\nthe architecture by reducing the number of networks used, requiring only a\ngenerator and a discriminator, as opposed to the multiple networks\ntraditionally involved. Additionally, our model requires less prior training\nknowledge and uses a non-adversarial training process, where the discriminator\nacts as a guide rather than a competitor to the generator. The core\ncontribution of this work is its ability to generate detailed visualization\nimages that align with specific class labels. Our model incorporates a unique\nskip-connection-inspired block design, which enhances label-directed image\ngeneration by propagating class information across multiple layers.\nFurthermore, we explore how these generated visualizations can be utilized as\nadversarial examples, effectively fooling classification networks with minimal\nperceptible modifications to the original images. Experimental results\ndemonstrate that our method outperforms traditional adversarial example\ngeneration techniques in both targeted and non-targeted attacks, achieving up\nto a 94.5% fooling rate with minimal perturbation. This work bridges the gap\nbetween visualization methods and adversarial examples, proposing that fooling\nrate could serve as a quantitative measure for evaluating visualization\nquality. The insights from this study provide a new perspective on the\ninterpretability of neural networks and their vulnerabilities to adversarial\nattacks.",
      "tldr_zh": "这篇论文提出了一种高效的神经网络可视化方法，使用生成模型（Generative Models）和非对抗训练，仅需生成器和鉴别器即可简化架构，并减少先验训练需求。核心贡献是通过受 skip-connection 启发的块设计，生成与特定类别标签对齐的详细可视化图像，并探索这些图像作为对抗扰动（Adversarial Perturbations）来欺骗分类网络。实验结果显示，该方法在针对性和非针对性攻击中优于传统技术，欺骗率高达94.5%，并建议将欺骗率作为评估可视化质量的量化指标，为神经网络的可解释性和漏洞分析提供新视角。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13559v1",
      "published_date": "2024-09-20 14:59:25 UTC",
      "updated_date": "2024-09-20 14:59:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:55:36.617723"
    },
    {
      "arxiv_id": "2409.13557v1",
      "title": "Trustworthy Hate Speech Detection Through Visual Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyuan Yang",
        "Ming Yan",
        "Yingyu Chen",
        "Hui Wang",
        "Zexin Lu",
        "Yi Zhang"
      ],
      "abstract": "The surge of hate speech on social media platforms poses a significant\nchallenge, with hate speech detection~(HSD) becoming increasingly critical.\nCurrent HSD methods focus on enriching contextual information to enhance\ndetection performance, but they overlook the inherent uncertainty of hate\nspeech. We propose a novel HSD method, named trustworthy hate speech detection\nmethod through visual augmentation (TrusV-HSD), which enhances semantic\ninformation through integration with diffused visual images and mitigates\nuncertainty with trustworthy loss. TrusV-HSD learns semantic representations by\neffectively extracting trustworthy information through multi-modal connections\nwithout paired data. Our experiments on public HSD datasets demonstrate the\neffectiveness of TrusV-HSD, showing remarkable improvements over conventional\nmethods.",
      "tldr_zh": "本研究针对社交媒体上仇恨言论的激增及其固有不确定性，提出了一种新方法TrusV-HSD，通过整合扩散视觉图像增强语义信息，并使用trustworthy loss来缓解不确定性。\n该方法通过多模态连接有效地提取可信赖信息，实现语义表示学习，而无需配对数据。\n实验结果显示，TrusV-HSD在公共HSD数据集上比传统方法表现出显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13557v1",
      "published_date": "2024-09-20 14:57:34 UTC",
      "updated_date": "2024-09-20 14:57:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:55:47.959999"
    },
    {
      "arxiv_id": "2409.13555v2",
      "title": "Generating Visual Stories with Grounded and Coreferent Characters",
      "title_zh": "翻译失败",
      "authors": [
        "Danyang Liu",
        "Mirella Lapata",
        "Frank Keller"
      ],
      "abstract": "Characters are important in narratives. They move the plot forward, create\nemotional connections, and embody the story's themes. Visual storytelling\nmethods focus more on the plot and events relating to it, without building the\nnarrative around specific characters. As a result, the generated stories feel\ngeneric, with character mentions being absent, vague, or incorrect. To mitigate\nthese issues, we introduce the new task of character-centric story generation\nand present the first model capable of predicting visual stories with\nconsistently grounded and coreferent character mentions. Our model is finetuned\non a new dataset which we build on top of the widely used VIST benchmark.\nSpecifically, we develop an automated pipeline to enrich VIST with visual and\ntextual character coreference chains. We also propose new evaluation metrics to\nmeasure the richness of characters and coreference in stories. Experimental\nresults show that our model generates stories with recurring characters which\nare consistent and coreferent to larger extent compared to baselines and\nstate-of-the-art systems.",
      "tldr_zh": "本研究指出，现有视觉叙事方法过度关注情节而忽略人物，导致生成的故事泛化且人物提及缺失或不一致。为解决此问题，论文引入了character-centric story generation新任务，并提出首个模型，能够生成视觉故事中grounded和coreferent人物提及。该模型基于VIST基准构建的新数据集进行微调，该数据集通过自动化管道添加视觉和文本character coreference链。论文还提出新评估指标来衡量故事中人物的丰富性和coreference，实验结果显示，该模型生成的故事在人物一致性和coreference方面明显优于基线和最先进系统。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13555v2",
      "published_date": "2024-09-20 14:56:33 UTC",
      "updated_date": "2025-03-02 14:36:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:56:00.527224"
    },
    {
      "arxiv_id": "2409.13546v1",
      "title": "Certified Adversarial Robustness via Partition-based Randomized Smoothing",
      "title_zh": "翻译失败",
      "authors": [
        "Hossein Goli",
        "Farzan Farnia"
      ],
      "abstract": "A reliable application of deep neural network classifiers requires robustness\ncertificates against adversarial perturbations. Gaussian smoothing is a widely\nanalyzed approach to certifying robustness against norm-bounded perturbations,\nwhere the certified prediction radius depends on the variance of the Gaussian\nnoise and the confidence level of the neural net's prediction under the\nadditive Gaussian noise. However, in application to high-dimensional image\ndatasets, the certified radius of the plain Gaussian smoothing could be\nrelatively small, since Gaussian noise with high variances can significantly\nharm the visibility of an image. In this work, we propose the Pixel\nPartitioning-based Randomized Smoothing (PPRS) methodology to boost the neural\nnet's confidence score and thus the robustness radius of the certified\nprediction. We demonstrate that the proposed PPRS algorithm improves the\nvisibility of the images under additive Gaussian noise. We discuss the\nnumerical results of applying PPRS to standard computer vision datasets and\nneural network architectures. Our empirical findings indicate a considerable\nimprovement in the certified accuracy and stability of the prediction model to\nthe additive Gaussian noise in randomized smoothing.",
      "tldr_zh": "这篇论文提出了一种基于像素分区（Pixel Partitioning）的随机平滑方法（PPRS），旨在提升深度神经网络分类器对范数边界扰动的鲁棒性证书。相比传统的 Gaussian smoothing，PPRS 通过优化图像在加性高斯噪声下的可见性，提高了神经网络的置信分数，从而扩大了证书预测半径。实验结果表明，在标准计算机视觉数据集和常见神经网络架构上，PPRS 显著提升了证书准确性和模型对噪声的稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13546v1",
      "published_date": "2024-09-20 14:41:47 UTC",
      "updated_date": "2024-09-20 14:41:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:56:12.214013"
    },
    {
      "arxiv_id": "2409.13538v1",
      "title": "First Place Solution to the Multiple-choice Video QA Track of The Second Perception Test Challenge",
      "title_zh": "翻译失败",
      "authors": [
        "Yingzhe Peng",
        "Yixiao Yuan",
        "Zitian Ao",
        "Huapeng Zhou",
        "Kangqi Wang",
        "Qipeng Zhu",
        "Xu Yang"
      ],
      "abstract": "In this report, we present our first-place solution to the Multiple-choice\nVideo Question Answering (QA) track of The Second Perception Test Challenge.\nThis competition posed a complex video understanding task, requiring models to\naccurately comprehend and answer questions about video content. To address this\nchallenge, we leveraged the powerful QwenVL2 (7B) model and fine-tune it on the\nprovided training set. Additionally, we employed model ensemble strategies and\nTest Time Augmentation to boost performance. Through continuous optimization,\nour approach achieved a Top-1 Accuracy of 0.7647 on the leaderboard.",
      "tldr_zh": "这篇论文介绍了我们赢得“第二届感知测试挑战赛”多选视频问答（Multiple-choice Video QA）轨道的第一名解决方案，针对视频理解任务提出了一种高效方法。核心策略包括使用 QwenVL2 (7B) 模型在提供的训练集上进行微调，并结合模型集成 (model ensemble) 和测试时增强 (Test Time Augmentation) 来提升性能。通过持续优化，该方法在排行榜上实现了 0.7647 的 Top-1 Accuracy。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13538v1",
      "published_date": "2024-09-20 14:31:13 UTC",
      "updated_date": "2024-09-20 14:31:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:56:23.487987"
    },
    {
      "arxiv_id": "2409.13537v1",
      "title": "ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources",
      "title_zh": "ShizishanGPT：整合工具和资源的农业大语言模型",
      "authors": [
        "Shuting Yang",
        "Zehui Liu",
        "Wolfgang Mayer"
      ],
      "abstract": "Recent developments in large language models (LLMs) have led to significant\nimprovements in intelligent dialogue systems'ability to handle complex\ninquiries. However, current LLMs still exhibit limitations in specialized\ndomain knowledge, particularly in technical fields such as agriculture. To\naddress this problem, we propose ShizishanGPT, an intelligent question\nanswering system for agriculture based on the Retrieval Augmented Generation\n(RAG) framework and agent architecture. ShizishanGPT consists of five key\nmodules: including a generic GPT-4 based module for answering general\nquestions; a search engine module that compensates for the problem that the\nlarge language model's own knowledge cannot be updated in a timely manner; an\nagricultural knowledge graph module for providing domain facts; a retrieval\nmodule which uses RAG to supplement domain knowledge; and an agricultural agent\nmodule, which invokes specialized models for crop phenotype prediction, gene\nexpression analysis, and so on. We evaluated the ShizishanGPT using a dataset\ncontaining 100 agricultural questions specially designed for this study. The\nexperimental results show that the tool significantly outperforms general LLMs\nas it provides more accurate and detailed answers due to its modular design and\nintegration of different domain knowledge sources. Our source code, dataset,\nand model weights are publicly available at https://github.com/Zaiwen/CropGPT.",
      "tldr_zh": "该研究提出 ShizishanGPT，这是一个整合工具和资源的农业 Large Language Model (LLM)，旨在解决现有模型在农业领域专业知识的不足问题。系统基于 Retrieval Augmented Generation (RAG) 框架和代理架构，包括五个关键模块：基于 GPT-4 的通用问答模块、搜索引擎模块、农业知识图谱模块、检索模块以及农业代理模块，用于调用专业模型如作物表型预测和基因表达分析。研究使用一个包含 100 个农业问题的专用数据集进行评估，结果显示 ShizishanGPT 比通用 LLMs 提供更准确和详细的答案。该系统源代码、数据集和模型权重已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages,3 figures, WISE2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13537v1",
      "published_date": "2024-09-20 14:30:45 UTC",
      "updated_date": "2024-09-20 14:30:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:56:36.980127"
    },
    {
      "arxiv_id": "2409.15084v2",
      "title": "Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory",
      "title_zh": "抑郁症诊断对话模拟：具有三级记忆的自我改进精神科医生",
      "authors": [
        "Kunyao Lan",
        "Bingrui Jin",
        "Zichen Zhu",
        "Siyuan Chen",
        "Shu Zhang",
        "Kenny Q. Zhu",
        "Mengyue Wu"
      ],
      "abstract": "Mental health issues, particularly depressive disorders, present significant\nchallenges in contemporary society, necessitating the development of effective\nautomated diagnostic methods. This paper introduces the Agent Mental Clinic\n(AMC), a self-improving conversational agent system designed to enhance\ndepression diagnosis through simulated dialogues between patient and\npsychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we\ndesign a psychiatrist agent consisting of a tertiary memory structure, a\ndialogue control and reflect plugin that acts as ``supervisor'' and a memory\nsampling module, fully leveraging the skills reflected by the psychiatrist\nagent, achieving great accuracy on depression risk and suicide risk diagnosis\nvia conversation. Experiment results on datasets collected in real-life\nscenarios demonstrate that the system, simulating the procedure of training\npsychiatrists, can be a promising optimization method for aligning LLMs with\nreal-life distribution in specific domains without modifying the weights of\nLLMs, even when only a few representative labeled cases are available.",
      "tldr_zh": "本论文引入了 Agent Mental Clinic (AMC)，一个自提升的对话代理系统，通过模拟患者和精神科医生代理的对话来提升抑郁症诊断准确性。该系统设计了精神科医生代理，包括 tertiary memory 结构、对话控制和反射插件（作为监督者）以及记忆采样模块，以充分利用代理技能并优化诊断过程。实验结果显示，AMC 在真实场景数据集上实现了高效的抑郁风险和自杀风险诊断，并证明了这种方法能优化 LLMs 与现实分布的 align，无需修改模型权重，即使仅有少量标记样本。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15084v2",
      "published_date": "2024-09-20 14:25:08 UTC",
      "updated_date": "2024-10-09 04:37:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:56:48.908525"
    },
    {
      "arxiv_id": "2409.13524v1",
      "title": "Contextualized AI for Cyber Defense: An Automated Survey using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Christoforus Yoga Haryanto",
        "Anne Maria Elvira",
        "Trung Duc Nguyen",
        "Minh Hieu Vu",
        "Yoshiano Hartanto",
        "Emily Lomempow",
        "Arathi Arakala"
      ],
      "abstract": "This paper surveys the potential of contextualized AI in enhancing cyber\ndefense capabilities, revealing significant research growth from 2015 to 2024.\nWe identify a focus on robustness, reliability, and integration methods, while\nnoting gaps in organizational trust and governance frameworks. Our study\nemploys two LLM-assisted literature survey methodologies: (A) ChatGPT 4 for\nexploration, and (B) Gemma 2:9b for filtering with Claude 3.5 Sonnet for\nfull-text analysis. We discuss the effectiveness and challenges of using LLMs\nin academic research, providing insights for future researchers.",
      "tldr_zh": "这篇论文调查了 contextualized AI 在提升网络防御能力方面的潜力，揭示了从 2015 到 2024 年的研究增长，并强调了 robustness、reliability 和 integration methods 的重点，同时指出了 organizational trust 和 governance frameworks 的缺口。研究采用两种 LLM-assisted 文献调查方法：(A) 使用 ChatGPT 4 进行探索，(B) 使用 Gemma 2:9b 进行过滤，并结合 Claude 3.5 Sonnet 进行全文分析。论文讨论了 LLMs 在学术研究中的有效性与挑战，并为未来研究者提供宝贵见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "8 pages, 2 figures, 4 tables, accepted into 17th International\n  Conference on Security of Information and Networks (SINCONF 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.13524v1",
      "published_date": "2024-09-20 14:05:40 UTC",
      "updated_date": "2024-09-20 14:05:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:57:01.341772"
    },
    {
      "arxiv_id": "2409.13521v2",
      "title": "A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Lorenzo Zangari",
        "Candida M. Greco",
        "Davide Picca",
        "Andrea Tagarelli"
      ],
      "abstract": "Moral values have deep roots in early civilizations, codified within norms\nand laws that regulated societal order and the common good. They play a crucial\nrole in understanding the psychological basis of human behavior and cultural\norientation. The Moral Foundation Theory (MFT) is a well-established framework\nthat identifies the core moral foundations underlying the manner in which\ndifferent cultures shape individual and social lives. Recent advancements in\nnatural language processing, particularly Pre-trained Language Models (PLMs),\nhave enabled the extraction and analysis of moral dimensions from textual data.\nThis survey presents a comprehensive review of MFT-informed PLMs, providing an\nanalysis of moral tendencies in PLMs and their application in the context of\nthe MFT. We also review relevant datasets and lexicons and discuss trends,\nlimitations, and future directions. By providing a structured overview of the\nintersection between PLMs and MFT, this work bridges moral psychology insights\nwithin the realm of PLMs, paving the way for further research and development\nin creating morally aware AI systems.",
      "tldr_zh": "这篇调查论文探讨了Moral Foundation Theory (MFT)与Pre-trained Language Models (PLMs)的交叉点，分析了PLMs在提取和分析文本道德维度方面的进展及其在不同文化背景下的应用。论文回顾了相关数据集和词汇表，讨论了PLMs中道德倾向的趋势、现有限制（如领域知识缺口），并提出了未来方向，以桥接道德心理学和AI系统的发展。最终，该工作为创建道德感知AI铺平了道路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.DL",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication with AI & Society, March 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.13521v2",
      "published_date": "2024-09-20 14:03:06 UTC",
      "updated_date": "2025-04-04 11:52:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:57:12.370204"
    },
    {
      "arxiv_id": "2410.07122v1",
      "title": "End-Cloud Collaboration Framework for Advanced AI Customer Service in E-commerce",
      "title_zh": "端云协作框架，用于电子商务的高级 AI 客户服务",
      "authors": [
        "Liangyu Teng",
        "Yang Liu",
        "Jing Liu",
        "Liang Song"
      ],
      "abstract": "In recent years, the e-commerce industry has seen a rapid increase in the\ndemand for advanced AI-driven customer service solutions. Traditional\ncloud-based models face limitations in terms of latency, personalized services,\nand privacy concerns. Furthermore, end devices often lack the computational\nresources to deploy large AI models effectively. In this paper, we propose an\ninnovative End-Cloud Collaboration (ECC) framework for advanced AI customer\nservice in e-commerce. This framework integrates the advantages of large cloud\nmodels and mid/small-sized end models by deeply exploring the generalization\npotential of cloud models and effectively utilizing the computing power\nresources of terminal chips, alleviating the strain on computing resources to\nsome extent. Specifically, the large cloud model acts as a teacher, guiding and\npromoting the learning of the end model, which significantly reduces the end\nmodel's reliance on large-scale, high-quality data and thereby addresses the\ndata bottleneck in traditional end model training, offering a new paradigm for\nthe rapid deployment of industry applications. Additionally, we introduce an\nonline evolutive learning strategy that enables the end model to continuously\niterate and upgrade based on guidance from the cloud model and real-time user\nfeedback. This strategy ensures that the model can flexibly adapt to the rapid\nchanges in application scenarios while avoiding the uploading of sensitive\ninformation by performing local fine-tuning, achieving the dual goals of\nprivacy protection and personalized service. %We make systematic contributions\nto the customized model fine-tuning methods in the e-commerce domain. To\nconclude, we implement in-depth corpus collection (e.g., data organization,\ncleaning, and preprocessing) and train an ECC-based industry-specific model for\ne-commerce customer service.",
      "tldr_zh": "这篇论文提出了一种 End-Cloud Collaboration (ECC) 框架，用于解决电子商务中 AI 驱动客户服务的延迟、隐私和计算资源问题，通过整合云端大模型和端侧中/小模型的优势。框架中，云端模型作为“老师”指导端侧模型学习，显著减少了对大规模数据的依赖，并引入在线演化学习策略，让端侧模型基于实时用户反馈进行本地迭代升级，实现隐私保护和个性化服务。最终，该方法缓解了计算资源瓶颈，并在电子商务领域通过语料收集和模型训练验证了其快速部署潜力。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by 2024 IEEE 10th World Forum on Internet of Things (WF-IoT)",
      "pdf_url": "http://arxiv.org/pdf/2410.07122v1",
      "published_date": "2024-09-20 13:46:54 UTC",
      "updated_date": "2024-09-20 13:46:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:57:25.297054"
    },
    {
      "arxiv_id": "2409.13503v3",
      "title": "SatFed: A Resource-Efficient LEO Satellite-Assisted Heterogeneous Federated Learning Framework",
      "title_zh": "SatFed：一个资源",
      "authors": [
        "Yuxin Zhang",
        "Zheng Lin",
        "Zhe Chen",
        "Zihan Fang",
        "Wenjun Zhu",
        "Xianhao Chen",
        "Jin Zhao",
        "Yue Gao"
      ],
      "abstract": "Traditional federated learning (FL) frameworks rely heavily on terrestrial\nnetworks, where coverage limitations and increasing bandwidth congestion\nsignificantly hinder model convergence. Fortunately, the advancement of\nlow-Earth orbit (LEO) satellite networks offers promising new communication\navenues to augment traditional terrestrial FL. Despite this potential, the\nlimited satellite-ground communication bandwidth and the heterogeneous\noperating environments of ground devices-including variations in data,\nbandwidth, and computing power-pose substantial challenges for effective and\nrobust satellite-assisted FL. To address these challenges, we propose SatFed, a\nresource-efficient satellite-assisted heterogeneous FL framework. SatFed\nimplements freshness-based model prioritization queues to optimize the use of\nhighly constrained satellite-ground bandwidth, ensuring the transmission of the\nmost critical models. Additionally, a multigraph is constructed to capture\nreal-time heterogeneous relationships between devices, including data\ndistribution, terrestrial bandwidth, and computing capability. This multigraph\nenables SatFed to aggregate satellite-transmitted models into peer guidance,\nenhancing local training in heterogeneous environments. Extensive experiments\nwith real-world LEO satellite networks demonstrate that SatFed achieves\nsuperior performance and robustness compared to state-of-the-art benchmarks.",
      "tldr_zh": "该论文提出SatFed，一种资源高效的LEO卫星辅助异构联邦学习框架，旨在解决传统FL框架依赖地面网络导致的覆盖限制和带宽拥塞问题，同时应对卫星-地面带宽有限及设备异构性（如数据分布、带宽和计算能力差异）的挑战。SatFed采用基于新鲜度的模型优先级队列优化带宽使用，确保传输最关键模型，并构建多图来捕捉设备间的异构关系，从而聚合卫星传输模型为对等指导，增强本地训练。实验结果显示，在真实LEO卫星网络环境中，SatFed相较于现有基准模型实现了更高的性能和鲁棒性。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "10 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13503v3",
      "published_date": "2024-09-20 13:44:00 UTC",
      "updated_date": "2024-11-21 06:56:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:57:35.778522"
    },
    {
      "arxiv_id": "2409.13501v1",
      "title": "HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation",
      "title_zh": "翻译失败",
      "authors": [
        "Geyuan Zhang",
        "Xiaofei Zhou",
        "Chuheng Chen"
      ],
      "abstract": "Fine-tuning pre-trained language models for downstream tasks has achieved\nimpressive results in NLP. However, fine-tuning all parameters becomes\nimpractical due to the rapidly increasing size of model parameters. To address\nthis, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of\nparameters. Most PEFT methods, such as LoRA, use incremental updates, which\ninvolve adding learned weight matrix increments to the original parameters.\nAlthough effective, these methods face limitations in capturing complex\nparameter dynamics and do not maintain a strong correlation between the\noriginal and updated parameters. To overcome these challenges, we propose the\ndirect Updated Transformation (UT) paradigm, which constructs a transformation\ndirectly from the original to the updated parameters. This approach ensures\nthat the correlation between the original and updated parameters is preserved,\nleveraging the semantic features learned during pre-training. Building on this\nparadigm, we present the Hadamard Updated Transformation (HUT) method. HUT\nefficiently updates the original weight matrix using the Hadamard\ntransformation with two low-rank matrices, offering a more expressive and\nflexible update mechanism. This allows HUT to capture richer parameter features\nthrough functional transformations, reducing computational complexity while\nmaintaining or improving model quality. Theoretical analysis and extensive\nexperiments on RoBERTa and GPT-2 validate the effectiveness of HUT. Results\nshow that HUT performs on par with or better than other PEFT methods in terms\nof model quality, while significantly reducing computational complexity.",
      "tldr_zh": "该论文针对预训练语言模型的全参数微调计算开销过大问题，提出了 Parameter Efficient Fine-Tuning (PEFT) 方法的改进方案。作者引入了 Updated Transformation (UT) 范式，通过直接从原参数到更新参数构建转换，以保持参数间的相关性和预训练语义特征。基于此，HUT 方法利用 Hadamard 变换和两个低秩矩阵高效更新权重矩阵，提升了参数表达性和灵活性，同时显著降低计算复杂性。在 RoBERTa 和 GPT-2 上的实验显示，HUT 在模型性能上与或优于其他 PEFT 方法，如 LoRA，但计算效率更高。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13501v1",
      "published_date": "2024-09-20 13:42:17 UTC",
      "updated_date": "2024-09-20 13:42:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:57:48.767914"
    },
    {
      "arxiv_id": "2409.13498v2",
      "title": "A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Savvas Sifnaios",
        "George Arvanitakis",
        "Fotios K. Konstantinidis",
        "Georgios Tsimiklis",
        "Angelos Amditis",
        "Panayiotis Frangos"
      ],
      "abstract": "Recent advancements in computer vision, particularly in detection,\nsegmentation, and classification, have significantly impacted various domains.\nHowever, these advancements are tied to RGB-based systems, which are\ninsufficient for applications in industries like waste sorting,\npharmaceuticals, and defense, where advanced object characterization beyond\nshape or color is necessary. Hyperspectral (HS) imaging, capturing both\nspectral and spatial information, addresses these limitations and offers\nadvantages over conventional technologies such as X-ray fluorescence and Raman\nspectroscopy, particularly in terms of speed, cost, and safety.\n  This study evaluates the potential of combining HS imaging with deep learning\nfor material characterization. The research involves: i) designing an\nexperimental setup with HS camera, conveyor, and controlled lighting; ii)\ngenerating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with\nsemi-automated mask generation and Raman spectroscopy-based labeling; and iii)\ndeveloping a deep learning model trained on HS images for pixel-level material\nclassification. The model achieved 99.94\\% classification accuracy,\ndemonstrating robustness in color, size, and shape invariance, and effectively\nhandling material overlap. Limitations, such as challenges with black objects,\nare also discussed. Extending computer vision beyond RGB to HS imaging proves\nfeasible, overcoming major limitations of traditional methods and showing\nstrong potential for future applications.",
      "tldr_zh": "本研究利用 Hyperspectral Imaging 与深度学习方法，解决了传统 RGB 系统的局限性，实现对材料的高精度像素级分类，适用于废物分类、制药和国防等领域。研究设计了实验设置，包括 HS 相机、传送带和控制照明，并创建了一个多对象数据集，涵盖多种塑料材料（HDPE、PET、PP、PS），采用半自动化掩码生成和 Raman 光谱标记。开发的深度学习模型在分类任务中达到 99.94% 的准确率，表现出色于颜色、大小和形状变化，并能有效处理材料重叠，尽管黑对象识别仍存在挑战。该方法扩展了计算机视觉的应用潜力，克服了传统技术的不足，为未来产业应用奠定基础。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "I.5; I.2.10"
      ],
      "primary_category": "eess.IV",
      "comment": "13 pages, 15 figures, 6 equations",
      "pdf_url": "http://arxiv.org/pdf/2409.13498v2",
      "published_date": "2024-09-20 13:38:48 UTC",
      "updated_date": "2025-05-14 13:01:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:58:02.930185"
    },
    {
      "arxiv_id": "2409.13496v1",
      "title": "DAP-LED: Learning Degradation-Aware Priors with CLIP for Joint Low-light Enhancement and Deblurring",
      "title_zh": "翻译失败",
      "authors": [
        "Ling Wang",
        "Chen Wu",
        "Lin Wang"
      ],
      "abstract": "Autonomous vehicles and robots often struggle with reliable visual perception\nat night due to the low illumination and motion blur caused by the long\nexposure time of RGB cameras. Existing methods address this challenge by\nsequentially connecting the off-the-shelf pretrained low-light enhancement and\ndeblurring models. Unfortunately, these methods often lead to noticeable\nartifacts (\\eg, color distortions) in the over-exposed regions or make it\nhardly possible to learn the motion cues of the dark regions. In this paper, we\ninterestingly find vision-language models, \\eg, Contrastive Language-Image\nPretraining (CLIP), can comprehensively perceive diverse degradation levels at\nnight. In light of this, we propose a novel transformer-based joint learning\nframework, named DAP-LED, which can jointly achieve low-light enhancement and\ndeblurring, benefiting downstream tasks, such as depth estimation,\nsegmentation, and detection in the dark. The key insight is to leverage CLIP to\nadaptively learn the degradation levels from images at night. This subtly\nenables learning rich semantic information and visual representation for\noptimization of the joint tasks. To achieve this, we first introduce a\nCLIP-guided cross-fusion module to obtain multi-scale patch-wise degradation\nheatmaps from the image embeddings. Then, the heatmaps are fused via the\ndesigned CLIP-enhanced transformer blocks to retain useful degradation\ninformation for effective model optimization. Experimental results show that,\ncompared to existing methods, our DAP-LED achieves state-of-the-art performance\nin the dark. Meanwhile, the enhanced results are demonstrated to be effective\nfor three downstream tasks. For demo and more results, please check the project\npage: \\url{https://vlislab22.github.io/dap-led/}.",
      "tldr_zh": "该论文提出DAP-LED框架，利用CLIP（Contrastive Language-Image Pretraining）学习退化感知先验，以联合实现低光增强和去模糊，解决自动车辆和机器人夜间视觉感知挑战。框架基于Transformer架构，通过CLIP-guided cross-fusion module生成多尺度补丁级退化热图，并使用CLIP-enhanced transformer blocks融合信息，以优化模型并保留丰富的语义和视觉表示。实验结果显示，DAP-LED在黑暗条件下比现有方法性能更先进，同时显著提升下游任务如深度估计、分割和检测的效果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13496v1",
      "published_date": "2024-09-20 13:37:53 UTC",
      "updated_date": "2024-09-20 13:37:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:58:12.423726"
    },
    {
      "arxiv_id": "2410.07121v1",
      "title": "Transfer Learning for E-commerce Query Product Type Prediction",
      "title_zh": "迁移学习用于电子商务查询产品类型",
      "authors": [
        "Anna Tigunova",
        "Thomas Ricatte",
        "Ghadir Eraisha"
      ],
      "abstract": "Getting a good understanding of the customer intent is essential in\ne-commerce search engines. In particular, associating the correct product type\nto a search query plays a vital role in surfacing correct products to the\ncustomers. Query product type classification (Q2PT) is a particularly\nchallenging task because search queries are short and ambiguous, the number of\nexisting product categories is extremely large, spanning thousands of values.\nMoreover, international marketplaces face additional challenges, such as\nlanguage and dialect diversity and cultural differences, influencing the\ninterpretation of the query. In this work we focus on Q2PT prediction in the\nglobal multilocale e-commerce markets. The common approach of training Q2PT\nmodels for each locale separately shows significant performance drops in\nlow-resource stores. Moreover, this method does not allow for a smooth\nexpansion to a new country, requiring to collect the data and train a new\nlocale-specific Q2PT model from scratch. To tackle this, we propose to use\ntransfer learning from the highresource to the low-resource locales, to achieve\nglobal parity of Q2PT performance. We benchmark the per-locale Q2PT model\nagainst the unified one, which shares the training data and model structure\nacross all worldwide stores. Additionally, we compare locale-aware and\nlocale-agnostic Q2PT models, showing the task dependency on the\ncountry-specific traits. We conduct extensive quantiative and qualitative\nanalysis of Q2PT models on the large-scale e-commerce dataset across 20\nworldwide locales, which shows that unified locale-aware Q2PT model has\nsuperior performance over the alternatives.",
      "tldr_zh": "该研究探讨了电商搜索中查询产品类型预测（Q2PT）的挑战，包括查询短小模糊、产品类别众多以及国际市场的语言和文化差异。为解决低资源区域的性能下降问题，论文提出使用Transfer Learning从高资源区域转移知识，训练统一模型以实现全球性能平衡。实验在20个全球区域的大型电商数据集上进行比较，结果显示统一区域感知模型在定量和定性分析中优于单独训练的模型和区域无关模型。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.07121v1",
      "published_date": "2024-09-20 13:30:04 UTC",
      "updated_date": "2024-09-20 13:30:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:58:23.589750"
    },
    {
      "arxiv_id": "2409.13484v1",
      "title": "'Since Lawyers are Males..': Examining Implicit Gender Bias in Hindi Language Generation by LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ishika Joshi",
        "Ishita Gupta",
        "Adrita Dey",
        "Tapan Parikh"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used to generate text\nacross various languages, for tasks such as translation, customer support, and\neducation. Despite these advancements, LLMs show notable gender biases in\nEnglish, which become even more pronounced when generating content in\nrelatively underrepresented languages like Hindi. This study explores implicit\ngender biases in Hindi text generation and compares them to those in English.\nWe developed Hindi datasets inspired by WinoBias to examine stereotypical\npatterns in responses from models like GPT-4o and Claude-3 sonnet. Our results\nreveal a significant gender bias of 87.8% in Hindi, compared to 33.4% in\nEnglish GPT-4o generation, with Hindi responses frequently relying on gender\nstereotypes related to occupations, power hierarchies, and social class. This\nresearch underscores the variation in gender biases across languages and\nprovides considerations for navigating these biases in generative AI systems.",
      "tldr_zh": "这篇论文调查了大型语言模型(LLMs)在印地语文本生成中的隐性性别偏见，并与英语进行比较，揭示了在 underrepresented 语言中偏见更为显著的问题。研究者开发了基于 WinoBias 的印地语数据集，对模型如 GPT-4o 和 Claude-3 sonnet 进行测试，分析响应中的刻板印象模式。结果显示，印地语的性别偏见高达87.8%，远高于英语的33.4%，并常涉及职业、权力层级和社会阶层的性别刻板印象。该研究强调了语言差异对偏见的影响，并为生成式 AI 系统提供处理偏见的指导建议。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13484v1",
      "published_date": "2024-09-20 13:16:58 UTC",
      "updated_date": "2024-09-20 13:16:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:58:37.706052"
    },
    {
      "arxiv_id": "2409.15378v1",
      "title": "Toward Automated Clinical Transcriptions",
      "title_zh": "翻译失败",
      "authors": [
        "Mitchell A. Klusty",
        "W. Vaiden Logan",
        "Samuel E. Armstrong",
        "Aaron D. Mullen",
        "Caroline N. Leach",
        "Jeff Talbert",
        "V. K. Cody Bumgardner"
      ],
      "abstract": "Administrative documentation is a major driver of rising healthcare costs and\nis linked to adverse outcomes, including physician burnout and diminished\nquality of care. This paper introduces a secure system that applies recent\nadvancements in speech-to-text transcription and speaker-labeling (diarization)\nto patient-provider conversations. This system is optimized to produce accurate\ntranscriptions and highlight potential errors to promote rapid human\nverification, further reducing the necessary manual effort. Applied to over 40\nhours of simulated conversations, this system offers a promising foundation for\nautomating clinical transcriptions.",
      "tldr_zh": "该论文探讨了行政文档导致的医疗成本上升、医生烧尽和护理质量下降等问题，提出了一种安全系统，利用 speech-to-text transcription 和 speaker-labeling (diarization) 技术来自动转录患者-提供者对话。该系统优化了转录准确性，并通过突出潜在错误来促进快速人工验证，从而减少手动工作量。在超过40小时的模拟对话中，该系统展示了自动化临床转录的可靠基础，为改进医疗效率提供了新途径。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.15378v1",
      "published_date": "2024-09-20 13:12:11 UTC",
      "updated_date": "2024-09-20 13:12:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:58:49.060929"
    },
    {
      "arxiv_id": "2409.13476v1",
      "title": "Dermatologist-like explainable AI enhances melanoma diagnosis accuracy: eye-tracking study",
      "title_zh": "类似皮肤科医生的可解释 AI 提升黑色素瘤诊断准确性：眼动追踪研究",
      "authors": [
        "Tirtha Chanda",
        "Sarah Haggenmueller",
        "Tabea-Clara Bucher",
        "Tim Holland-Letz",
        "Harald Kittler",
        "Philipp Tschandl",
        "Markus V. Heppt",
        "Carola Berking",
        "Jochen S. Utikal",
        "Bastian Schilling",
        "Claudia Buerger",
        "Cristian Navarrete-Dechent",
        "Matthias Goebeler",
        "Jakob Nikolas Kather",
        "Carolin V. Schneider",
        "Benjamin Durani",
        "Hendrike Durani",
        "Martin Jansen",
        "Juliane Wacker",
        "Joerg Wacker",
        "Reader Study Consortium",
        "Titus J. Brinker"
      ],
      "abstract": "Artificial intelligence (AI) systems have substantially improved\ndermatologists' diagnostic accuracy for melanoma, with explainable AI (XAI)\nsystems further enhancing clinicians' confidence and trust in AI-driven\ndecisions. Despite these advancements, there remains a critical need for\nobjective evaluation of how dermatologists engage with both AI and XAI tools.\nIn this study, 76 dermatologists participated in a reader study, diagnosing 16\ndermoscopic images of melanomas and nevi using an XAI system that provides\ndetailed, domain-specific explanations. Eye-tracking technology was employed to\nassess their interactions. Diagnostic performance was compared with that of a\nstandard AI system lacking explanatory features. Our findings reveal that XAI\nsystems improved balanced diagnostic accuracy by 2.8 percentage points relative\nto standard AI. Moreover, diagnostic disagreements with AI/XAI systems and\ncomplex lesions were associated with elevated cognitive load, as evidenced by\nincreased ocular fixations. These insights have significant implications for\nclinical practice, the design of AI tools for visual tasks, and the broader\ndevelopment of XAI in medical diagnostics.",
      "tldr_zh": "本研究通过 eye-tracking 技术评估了可解释 AI (XAI) 系统如何提升皮肤科医生对黑色素瘤的诊断准确性。76 名医生参与诊断 16 张皮肤镜图像，使用 XAI 系统提供详细的领域特定解释，并与标准 AI 系统进行比较。结果显示，XAI 系统提高了 2.8 百分点的平衡诊断准确性，同时诊断分歧和复杂病变导致了更高的认知负荷，如眼动固定增加。这些发现对临床实践、AI 工具设计以及 XAI 在医疗诊断中的发展具有重要启示。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13476v1",
      "published_date": "2024-09-20 13:08:33 UTC",
      "updated_date": "2024-09-20 13:08:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:59:01.235678"
    },
    {
      "arxiv_id": "2409.13470v1",
      "title": "Deterministic versus stochastic dynamical classifiers: opposing random adversarial attacks with noise",
      "title_zh": "确定性与随机动态分类器：通过噪声对抗随机对抗攻击",
      "authors": [
        "Lorenzo Chicchi",
        "Duccio Fanelli",
        "Diego Febbe",
        "Lorenzo Buffoni",
        "Francesca Di Patti",
        "Lorenzo Giambagli",
        "Raffele Marino"
      ],
      "abstract": "The Continuous-Variable Firing Rate (CVFR) model, widely used in neuroscience\nto describe the intertangled dynamics of excitatory biological neurons, is here\ntrained and tested as a veritable dynamically assisted classifier. To this end\nthe model is supplied with a set of planted attractors which are\nself-consistently embedded in the inter-nodes coupling matrix, via its spectral\ndecomposition. Learning to classify amounts to sculp the basin of attraction of\nthe imposed equilibria, directing different items towards the corresponding\ndestination target, which reflects the class of respective pertinence. A\nstochastic variant of the CVFR model is also studied and found to be robust to\naversarial random attacks, which corrupt the items to be classified. This\nremarkable finding is one of the very many surprising effects which arise when\nnoise and dynamical attributes are made to mutually resonate.",
      "tldr_zh": "这篇论文比较了确定性与随机动态分类器，使用 Continuous-Variable Firing Rate (CVFR) 模型作为动态辅助分类器，通过在节点耦合矩阵中自洽嵌入吸引子来塑造吸引盆，从而引导不同项目向对应类别的目标收敛。研究方法包括训练 CVFR 模型以学习分类，并引入随机变体来测试其鲁棒性。结果显示，随机 CVFR 模型对随机对抗攻击具有显著抵抗力，这揭示了噪声与动态属性之间潜在的协同共振效应。",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13470v1",
      "published_date": "2024-09-20 12:59:16 UTC",
      "updated_date": "2024-09-20 12:59:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:59:13.480993"
    },
    {
      "arxiv_id": "2409.13466v1",
      "title": "Global Outlier Detection in a Federated Learning Setting with Isolation Forest",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Malpetti",
        "Laura Azzimonti"
      ],
      "abstract": "We present a novel strategy for detecting global outliers in a federated\nlearning setting, targeting in particular cross-silo scenarios. Our approach\ninvolves the use of two servers and the transmission of masked local data from\nclients to one of the servers. The masking of the data prevents the disclosure\nof sensitive information while still permitting the identification of outliers.\nMoreover, to further safeguard privacy, a permutation mechanism is implemented\nso that the server does not know which client owns any masked data point. The\nserver performs outlier detection on the masked data, using either Isolation\nForest or its extended version, and then communicates outlier information back\nto the clients, allowing them to identify and remove outliers in their local\ndatasets before starting any subsequent federated model training. This approach\nprovides comparable results to a centralized execution of Isolation Forest\nalgorithms on plain data.",
      "tldr_zh": "该论文提出了一种在联邦学习(Federated Learning)环境中检测全局异常值的新策略，特别针对跨分区(cross-silo)场景，使用两个服务器和掩码本地数据来避免敏感信息泄露，同时通过置换机制确保服务器无法追踪数据点所属的客户端。服务器采用Isolation Forest算法或其扩展版本对掩码数据进行异常检测，并将检测结果反馈给客户端，以移除本地数据集中的异常值。该方法在隐私保护的前提下，提供与集中式执行Isolation Forest相当的检测效果，为后续联邦模型训练奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication at FLTA 2024: The 2nd IEEE International\n  Conference on Federated Learning Technologies and Applications",
      "pdf_url": "http://arxiv.org/pdf/2409.13466v1",
      "published_date": "2024-09-20 12:55:29 UTC",
      "updated_date": "2024-09-20 12:55:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:59:25.196389"
    },
    {
      "arxiv_id": "2410.07108v1",
      "title": "FAIR GPT: A virtual consultant for research data management in ChatGPT",
      "title_zh": "翻译失败",
      "authors": [
        "Renat Shigapov",
        "Irene Schumm"
      ],
      "abstract": "FAIR GPT is a first virtual consultant in ChatGPT designed to help\nresearchers and organizations make their data and metadata compliant with the\nFAIR (Findable, Accessible, Interoperable, Reusable) principles. It provides\nguidance on metadata improvement, dataset organization, and repository\nselection. To ensure accuracy, FAIR GPT uses external APIs to assess dataset\nFAIRness, retrieve controlled vocabularies, and recommend repositories,\nminimizing hallucination and improving precision. It also assists in creating\ndocumentation (data and software management plans, README files, and\ncodebooks), and selecting proper licenses. This paper describes its features,\napplications, and limitations.",
      "tldr_zh": "FAIR GPT 是一个基于 ChatGPT 的虚拟顾问，旨在帮助研究人员和组织使数据及元数据符合 FAIR principles（可发现、可访问、可互操作、可重用）。该工具利用外部 APIs 评估数据集的 FAIRness、检索受控词汇并推荐仓库，从而减少 hallucination 并提升指导精度，同时协助创建数据和软件管理计划、README 文件、codebooks 以及选择适当的许可证。本文详细描述了 FAIR GPT 的特征、应用场景及其潜在限制，为研究数据管理提供了实用解决方案。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DL",
      "comment": "4 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2410.07108v1",
      "published_date": "2024-09-20 12:28:48 UTC",
      "updated_date": "2024-09-20 12:28:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:59:37.186559"
    },
    {
      "arxiv_id": "2409.13440v1",
      "title": "Differentially Private Multimodal Laplacian Dropout (DP-MLD) for EEG Representative Learning",
      "title_zh": "差",
      "authors": [
        "Xiaowen Fu",
        "Bingxin Wang",
        "Xinzhou Guo",
        "Guoqing Liu",
        "Yang Xiang"
      ],
      "abstract": "Recently, multimodal electroencephalogram (EEG) learning has shown great\npromise in disease detection. At the same time, ensuring privacy in clinical\nstudies has become increasingly crucial due to legal and ethical concerns. One\nwidely adopted scheme for privacy protection is differential privacy (DP)\nbecause of its clear interpretation and ease of implementation. Although\nnumerous methods have been proposed under DP, it has not been extensively\nstudied for multimodal EEG data due to the complexities of models and signal\ndata considered there. In this paper, we propose a novel Differentially Private\nMultimodal Laplacian Dropout (DP-MLD) scheme for multimodal EEG learning. Our\napproach proposes a novel multimodal representative learning model that\nprocesses EEG data by language models as text and other modal data by vision\ntransformers as images, incorporating well-designed cross-attention mechanisms\nto effectively extract and integrate cross-modal features. To achieve DP, we\ndesign a novel adaptive feature-level Laplacian dropout scheme, where\nrandomness allocation and performance are dynamically optimized within given\nprivacy budgets. In the experiment on an open-source multimodal dataset of\nFreezing of Gait (FoG) in Parkinson's Disease (PD), our proposed method\ndemonstrates an approximate 4\\% improvement in classification accuracy, and\nachieves state-of-the-art performance in multimodal EEG learning under DP.",
      "tldr_zh": "本文提出了一种新的差分隐私多模态 Laplacian Dropout (DP-MLD) 方案，用于 EEG 代表性学习，以解决多模态 EEG 数据在隐私保护方面的挑战。该方法采用语言模型处理 EEG 数据作为文本、视觉 transformer 处理其他模态数据，并通过交叉注意力机制提取和整合跨模态特征；同时设计自适应特征级 Laplacian dropout，在给定隐私预算内动态优化随机性和性能。在 Parkinson 疾病的 Freezing of Gait (FoG) 数据集上实验，DP-MLD 实现了约 4% 的分类准确率提升，并达到了差分隐私 (DP) 下的最先进性能。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13440v1",
      "published_date": "2024-09-20 12:08:22 UTC",
      "updated_date": "2024-09-20 12:08:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:59:50.563766"
    },
    {
      "arxiv_id": "2409.13430v3",
      "title": "CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction",
      "title_zh": "CVT-O",
      "authors": [
        "Zhangchen Ye",
        "Tao Jiang",
        "Chenfeng Xu",
        "Yiming Li",
        "Hang Zhao"
      ],
      "abstract": "Vision-based 3D occupancy prediction is significantly challenged by the\ninherent limitations of monocular vision in depth estimation. This paper\nintroduces CVT-Occ, a novel approach that leverages temporal fusion through the\ngeometric correspondence of voxels over time to improve the accuracy of 3D\noccupancy predictions. By sampling points along the line of sight of each voxel\nand integrating the features of these points from historical frames, we\nconstruct a cost volume feature map that refines current volume features for\nimproved prediction outcomes. Our method takes advantage of parallax cues from\nhistorical observations and employs a data-driven approach to learn the cost\nvolume. We validate the effectiveness of CVT-Occ through rigorous experiments\non the Occ3D-Waymo dataset, where it outperforms state-of-the-art methods in 3D\noccupancy prediction with minimal additional computational cost. The code is\nreleased at \\url{https://github.com/Tsinghua-MARS-Lab/CVT-Occ}.",
      "tldr_zh": "本论文提出CVT-Occ方法，通过成本体积(Cost Volume)的时间融合技术，解决基于单目视觉的3D占用预测(3D Occupancy Prediction)中深度估计的局限性。该方法利用体素的几何对应关系，从历史帧中采样视线点并整合特征，构建成本体积特征图以优化当前预测。实验在Occ3D-Waymo数据集上验证了CVT-Occ的表现，优于现有最先进方法，同时仅需最小额外计算成本，并开源了代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13430v3",
      "published_date": "2024-09-20 11:52:47 UTC",
      "updated_date": "2024-09-25 07:34:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:00:02.499570"
    },
    {
      "arxiv_id": "2409.13427v1",
      "title": "A User Study on Contrastive Explanations for Multi-Effector Temporal Planning with Non-Stationary Costs",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaowei Liu",
        "Kevin McAreavey",
        "Weiru Liu"
      ],
      "abstract": "In this paper, we adopt constrastive explanations within an end-user\napplication for temporal planning of smart homes. In this application, users\nhave requirements on the execution of appliance tasks, pay for energy according\nto dynamic energy tariffs, have access to high-capacity battery storage, and\nare able to sell energy to the grid. The concurrent scheduling of devices makes\nthis a multi-effector planning problem, while the dynamic tariffs yield costs\nthat are non-stationary (alternatively, costs that are stationary but depend on\nexogenous events). These characteristics are such that the planning problems\nare generally not supported by existing PDDL-based planners, so we instead\ndesign a custom domain-dependent planner that scales to reasonable appliance\nnumbers and time horizons. We conduct a controlled user study with 128\nparticipants using an online crowd-sourcing platform based on two user stories.\nOur results indicate that users provided with contrastive questions and\nexplanations have higher levels of satisfaction, tend to gain improved\nunderstanding, and rate the helpfulness more favourably with the recommended AI\nschedule compared to those without access to these features.",
      "tldr_zh": "这篇论文探讨了在智能家居时序规划中，使用对比性解释（contrastive explanations）来处理多效应器（multi-effector）问题和非平稳成本（non-stationary costs）。研究者设计了一个自定义的领域依赖规划器，以适应动态能源关税、电池存储和能源交易等复杂场景，该规划器能扩展到合理的家电数量和时间范围。用户研究涉及128名参与者，结果显示，提供对比性解释的用户满意度更高、理解更深入，并更积极评价AI推荐的日程安排。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13427v1",
      "published_date": "2024-09-20 11:48:25 UTC",
      "updated_date": "2024-09-20 11:48:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:00:14.062561"
    },
    {
      "arxiv_id": "2409.13410v1",
      "title": "Sine Wave Normalization for Deep Learning-Based Tumor Segmentation in CT/PET Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Jintao Ren",
        "Muheng Li",
        "Stine Sofia Korreman"
      ],
      "abstract": "This report presents a normalization block for automated tumor segmentation\nin CT/PET scans, developed for the autoPET III Challenge. The key innovation is\nthe introduction of the SineNormal, which applies periodic sine transformations\nto PET data to enhance lesion detection. By highlighting intensity variations\nand producing concentric ring patterns in PET highlighted regions, the model\naims to improve segmentation accuracy, particularly for challenging multitracer\nPET datasets. The code for this project is available on GitHub\n(https://github.com/BBQtime/Sine-Wave-Normalization-for-Deep-Learning-Based-Tumor-Segmentation-in-CT-PET).",
      "tldr_zh": "这篇论文提出了一种名为 Sine Wave Normalization 的归一化块，用于基于深度学习的 CT/PET 图像中自动肿瘤分割，针对 autoPET III Challenge 开发。关键创新是引入 SineNormal，通过应用周期性 sine transformations 到 PET 数据，突出强度变化并生成同心环 patterns，以增强病变检测。实验结果显示，该方法显著提高了分割准确性，尤其适用于 challenging multitracer PET 数据集。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "Report for Team WukongRT in the AutoPET III Challenge",
      "pdf_url": "http://arxiv.org/pdf/2409.13410v1",
      "published_date": "2024-09-20 11:20:11 UTC",
      "updated_date": "2024-09-20 11:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:00:25.820895"
    },
    {
      "arxiv_id": "2409.13402v1",
      "title": "Validation & Exploration of Multimodal Deep-Learning Camera-Lidar Calibration models",
      "title_zh": "翻译失败",
      "authors": [
        "Venkat Karramreddy",
        "Liam Mitchell"
      ],
      "abstract": "This article presents an innovative study in exploring, evaluating, and\nimplementing deep learning architectures for the calibration of multi-modal\nsensor systems. The focus behind this is to leverage the use of sensor fusion\nto achieve dynamic, real-time alignment between 3D LiDAR and 2D Camera sensors.\nstatic calibration methods are tedious and time-consuming, which is why we\npropose utilizing Conventional Neural Networks (CNN) coupled with geometrically\ninformed learning to solve this issue. We leverage the foundational principles\nof Extrinsic LiDAR-Camera Calibration tools such as RegNet, CalibNet, and\nLCCNet by exploring open-source models that are available online and comparing\nour results with their corresponding research papers. Requirements for\nextracting these visual and measurable outputs involved tweaking source code,\nfine-tuning, training, validation, and testing for each of these frameworks for\nequal comparisons. This approach aims to investigate which of these advanced\nnetworks produces the most accurate and consistent predictions. Through a\nseries of experiments, we reveal some of their shortcomings and areas for\npotential improvements along the way. We find that LCCNet yields the best\nresults out of all the models that we validated.",
      "tldr_zh": "本研究探讨了利用深度学习架构对多模态传感器系统（如 LiDAR 和 Camera）进行校准的方法，旨在通过传感器融合实现动态实时对齐，以解决传统静态校准的繁琐和耗时问题。作者采用 CNN 结合几何信息学习，基于现有工具如 RegNet、CalibNet 和 LCCNet，对这些模型进行源代码修改、微调、训练、验证和测试，以比较其准确性和一致性。实验结果表明，LCCNet 在预测性能上表现出色，同时揭示了其他模型的缺点和潜在改进方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13402v1",
      "published_date": "2024-09-20 11:03:49 UTC",
      "updated_date": "2024-09-20 11:03:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:00:38.059002"
    },
    {
      "arxiv_id": "2410.02745v2",
      "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity",
      "title_zh": "AVG-LLaVA：一种具有自适应视觉粒度的多模",
      "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Wenbo Li",
        "Jie Zhou",
        "Jinsong Su"
      ],
      "abstract": "Recently, when dealing with high-resolution images, dominant LMMs usually\ndivide them into multiple local images and one global image, which will lead to\na large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM\nthat can adaptively select the appropriate visual granularity based on the\ninput image and instruction. This approach not only reduces the number of\nvisual tokens and speeds up inference, but also improves the overall model\nperformance. Specifically, we introduce the following modules based on\nLLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling\nlayers to obtain visual tokens with different granularities; (b) a visual\ngranularity router, which includes a Transformer layer, an MLP layer, and a\nvoter layer, used to select the appropriate visual granularity based on the\nimage and instruction. Furthermore, we propose RGLF, a novel training paradigm\nthat aims at aligning the granularity predicted by the router with the\npreferences of the LMM, without the need for additional manually annotated\ndata. Extensive experiments and analysis show that AVG-LLaVA achieves superior\nperformance across 11 benchmarks, as well as significantly reduces the number\nof visual tokens and speeds up inference (e.g., an 85.3% reduction in visual\ntokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark).",
      "tldr_zh": "该论文提出AVG-LLaVA，一种大型多模态模型（LMM），能够根据输入图像和指令自适应选择视觉粒度，从而减少视觉tokens数量、加快推理速度并提升整体性能。具体来说，该模型引入视觉粒度缩放器（用于获取不同粒度的视觉tokens）和视觉粒度路由器（基于Transformer层、MLP层和voter层进行选择），并提出RGLF训练范式，以对齐路由器的预测和LMM偏好，而无需额外标注数据。实验结果显示，AVG-LLaVA在11个基准上表现出色，例如在AI2D基准上减少85.3%的视觉tokens并提升2.53倍的推理速度。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2410.02745v2",
      "published_date": "2024-09-20 10:50:21 UTC",
      "updated_date": "2024-10-04 04:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:00:50.630744"
    },
    {
      "arxiv_id": "2409.13793v1",
      "title": "On the Feasibility of Fully AI-automated Vishing Attacks",
      "title_zh": "关于完全AI自动化的Vishing攻击的可行性",
      "authors": [
        "João Figueiredo",
        "Afonso Carvalho",
        "Daniel Castro",
        "Daniel Gonçalves",
        "Nuno Santos"
      ],
      "abstract": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs.",
      "tldr_zh": "这篇论文探讨了AI自动化vishing攻击的可行性，vishing是一种通过电话欺骗受害者泄露敏感信息的社交工程攻击。研究团队开发了ViKing系统，使用Large Language Model (LLM)作为核心处理器，结合语音转文本和文本转语音模块来模拟真实对话。在一项涉及240名参与者的控制实验中，ViKing成功说服许多人泄露信息，即使他们已被告知风险，证明此类攻击已变得高度现实。作者强调，这种工具可能被恶意者滥用，但也可作为网络安全教育的重要资源。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13793v1",
      "published_date": "2024-09-20 10:47:09 UTC",
      "updated_date": "2024-09-20 10:47:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:01:01.383720"
    },
    {
      "arxiv_id": "2409.13382v1",
      "title": "Audio Codec Augmentation for Robust Collaborative Watermarking of Speech Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Lauri Juvela",
        "Xin Wang"
      ],
      "abstract": "Automatic detection of synthetic speech is becoming increasingly important as\ncurrent synthesis methods are both near indistinguishable from human speech and\nwidely accessible to the public. Audio watermarking and other active disclosure\nmethods of are attracting research activity, as they can complement traditional\ndeepfake defenses based on passive detection. In both active and passive\ndetection, robustness is of major interest. Traditional audio watermarks are\nparticularly susceptible to removal attacks by audio codec application. Most\ngenerated speech and audio content released into the wild passes through an\naudio codec purely as a distribution method. We recently proposed collaborative\nwatermarking as method for making generated speech more easily detectable over\na noisy but differentiable transmission channel. This paper extends the channel\naugmentation to work with non-differentiable traditional audio codecs and\nneural audio codecs and evaluates transferability and effect of codec bitrate\nover various configurations. The results show that collaborative watermarking\ncan be reliably augmented by black-box audio codecs using a waveform-domain\nstraight-through-estimator for gradient approximation. Furthermore, that\nresults show that channel augmentation with a neural audio codec transfers well\nto traditional codecs. Listening tests demonstrate collaborative watermarking\nincurs negligible perceptual degradation with high bitrate codecs or DAC at\n8kbps.",
      "tldr_zh": "该研究针对合成语音检测的挑战，提出了一种增强音频编解码器（audio codec）的协作水印（collaborative watermarking）方法，以提高水印在语音合成中的鲁棒性，特别是对抗传统音频编解码器的移除攻击。通过使用波形域直通估计器（waveform-domain straight-through-estimator）来近似梯度，该方法扩展了协作水印，使其适用于非可微的传统音频编解码器和神经音频编解码器（neural audio codecs）。实验结果显示，这种增强策略在各种配置下表现出良好的可转移性，且使用高比特率编解码器或8kbps DAC时，听力测试表明感知退化 negligible。整体而言，该方法为更可靠的主动检测合成语音提供了新途径。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.13382v1",
      "published_date": "2024-09-20 10:33:17 UTC",
      "updated_date": "2024-09-20 10:33:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:01:14.412220"
    },
    {
      "arxiv_id": "2409.13373v1",
      "title": "LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench",
      "title_zh": "翻译失败",
      "authors": [
        "Karthik Valmeekam",
        "Kaya Stechly",
        "Subbarao Kambhampati"
      ],
      "abstract": "The ability to plan a course of action that achieves a desired state of\naffairs has long been considered a core competence of intelligent agents and\nhas been an integral part of AI research since its inception. With the advent\nof large language models (LLMs), there has been considerable interest in the\nquestion of whether or not they possess such planning abilities. PlanBench, an\nextensible benchmark we developed in 2022, soon after the release of GPT3, has\nremained an important tool for evaluating the planning abilities of LLMs.\nDespite the slew of new private and open source LLMs since GPT3, progress on\nthis benchmark has been surprisingly slow. OpenAI claims that their recent o1\n(Strawberry) model has been specifically constructed and trained to escape the\nnormal limitations of autoregressive LLMs--making it a new kind of model: a\nLarge Reasoning Model (LRM). Using this development as a catalyst, this paper\ntakes a comprehensive look at how well current LLMs and new LRMs do on\nPlanBench. As we shall see, while o1's performance is a quantum improvement on\nthe benchmark, outpacing the competition, it is still far from saturating it.\nThis improvement also brings to the fore questions about accuracy, efficiency,\nand guarantees which must be considered before deploying such systems.",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 和大型推理模型 (LRMs) 在规划任务上的性能，使用了 PlanBench 基准作为测试工具。研究发现，OpenAI 的 o1 模型（Strawberry）在该基准上取得了量子级别的提升，远超其他 LLMs，但仍未完全满足要求。论文强调了这一改进引发了对模型准确性、效率和部署可靠性的关键讨论。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13373v1",
      "published_date": "2024-09-20 10:20:46 UTC",
      "updated_date": "2024-09-20 10:20:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:01:25.646144"
    },
    {
      "arxiv_id": "2409.13366v2",
      "title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Wenhui Diao",
        "Haichen Yu",
        "Kaiyue Kang",
        "Tong Ling",
        "Di Liu",
        "Yingchao Feng",
        "Hanbo Bi",
        "Libo Ren",
        "Xuexue Li",
        "Yongqiang Mao",
        "Xian Sun"
      ],
      "abstract": "Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to\nthe unique characteristics of their viewing angles. Existing research has\nprimarily focused on algorithms for specific tasks, which have limited\napplicability in a broad range of ARS vision applications. This paper proposes\nthe RingMo-Aerial model, aiming to fill the gap in foundation model research in\nthe field of ARS vision. By introducing the Frequency-Enhanced Multi-Head\nSelf-Attention (FE-MSA) mechanism and an affine transformation-based\ncontrastive learning pre-training method, the model's detection capability for\nsmall targets is enhanced and optimized for the tilted viewing angles\ncharacteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter\nfine-tuning method, is proposed to improve the model's adaptability and\neffectiveness in various ARS vision tasks. Experimental results demonstrate\nthat RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This\nindicates the practicality and efficacy of RingMo-Aerial in enhancing the\nperformance of ARS vision tasks.",
      "tldr_zh": "本文提出RingMo-Aerial模型，旨在解决航空遥感(ARS)视觉任务中视角独特带来的挑战，并填补该领域基础模型的研究空白。模型引入Frequency-Enhanced Multi-Head Self-Attention (FE-MSA)机制和基于affine transformation的contrastive learning预训练方法，提升小目标检测能力并优化倾斜视角。还开发了ARS-Adapter作为高效参数微调策略，以增强模型在多种ARS任务中的适应性。实验结果表明，RingMo-Aerial在多个下游任务上达到SOTA性能，证明其在ARS视觉应用中的实用性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13366v2",
      "published_date": "2024-09-20 10:03:14 UTC",
      "updated_date": "2025-03-31 09:07:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:01:37.662865"
    },
    {
      "arxiv_id": "2409.13363v2",
      "title": "FPBoost: Fully Parametric Gradient Boosting for Survival Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Archetti",
        "Eugenio Lomurno",
        "Diego Piccinotti",
        "Matteo Matteucci"
      ],
      "abstract": "Survival analysis is a statistical framework for modeling time-to-event data.\nIt plays a pivotal role in medicine, reliability engineering, and social\nscience research, where understanding event dynamics even with few data samples\nis critical. Recent advancements in machine learning, particularly those\nemploying neural networks and decision trees, have introduced sophisticated\nalgorithms for survival modeling. However, many of these methods rely on\nrestrictive assumptions about the underlying event-time distribution, such as\nproportional hazard, time discretization, or accelerated failure time. In this\nstudy, we propose FPBoost, a survival model that combines a weighted sum of\nfully parametric hazard functions with gradient boosting. Distribution\nparameters are estimated with decision trees trained by maximizing the full\nsurvival likelihood. We show how FPBoost is a universal approximator of hazard\nfunctions, offering full event-time modeling flexibility while maintaining\ninterpretability through the use of well-established parametric distributions.\nWe evaluate concordance and calibration of FPBoost across multiple benchmark\ndatasets, showcasing its robustness and versatility as a new tool for survival\nestimation.",
      "tldr_zh": "本研究针对生存分析（Survival Analysis）中的时间到事件数据建模问题，提出了一种新模型 FPBoost，该模型将完全参数化风险函数（fully parametric hazard functions）与梯度提升（gradient boosting）相结合，使用决策树（decision trees）通过最大化完整生存似然（full survival likelihood）来估计分布参数。FPBoost 作为一种通用逼近器（universal approximator） of hazard functions，提供灵活的事件时间建模，同时保持可解释性，通过 well-established parametric distributions 提升可靠性。在多个基准数据集上的评估显示，FPBoost 在 concordance 和 calibration 方面表现出色，证明了其稳健性和多功能性，为医学和相关领域提供了新的生存估计工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13363v2",
      "published_date": "2024-09-20 09:57:17 UTC",
      "updated_date": "2025-01-31 09:32:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:01:49.598668"
    },
    {
      "arxiv_id": "2409.13792v1",
      "title": "Continual Learning for Multimodal Data Fusion of a Soft Gripper",
      "title_zh": "软抓手的多模态数据融合持续学习",
      "authors": [
        "Nilay Kushawaha",
        "Egidio Falotico"
      ],
      "abstract": "Continual learning (CL) refers to the ability of an algorithm to continuously\nand incrementally acquire new knowledge from its environment while retaining\npreviously learned information. A model trained on one data modality often\nfails when tested with a different modality. A straightforward approach might\nbe to fuse the two modalities by concatenating their features and training the\nmodel on the fused data. However, this requires retraining the model from\nscratch each time it encounters a new domain. In this paper, we introduce a\ncontinual learning algorithm capable of incrementally learning different data\nmodalities by leveraging both class-incremental and domain-incremental learning\nscenarios in an artificial environment where labeled data is scarce, yet\nnon-iid (independent and identical distribution) unlabeled data from the\nenvironment is plentiful. The proposed algorithm is efficient and only requires\nstoring prototypes for each class. We evaluate the algorithm's effectiveness on\na challenging custom multimodal dataset comprising of tactile data from a soft\npneumatic gripper, and visual data from non-stationary images of objects\nextracted from video sequences. Additionally, we conduct an ablation study on\nthe custom dataset and the Core50 dataset to highlight the contributions of\ndifferent components of the algorithm. To further demonstrate the robustness of\nthe algorithm, we perform a real-time experiment for object classification\nusing the soft gripper and an external independent camera setup, all\nsynchronized with the Robot Operating System (ROS) framework.",
      "tldr_zh": "本论文探讨了持续学习（Continual Learning, CL）在软抓手多模态数据融合中的应用，旨在让算法能持续增量学习新数据模态（如触觉和视觉数据），同时保留先前知识，而无需从头重新训练。提出的算法结合了class-incremental和domain-incremental学习场景，仅需存储每个类的prototypes，即可高效处理标签数据稀缺但非iid无标签数据丰富的环境。实验在自定义多模态数据集上评估了算法性能，包括软气动抓手的触觉数据和视频图像的视觉数据，并通过ablation study和实时ROS框架下的对象分类实验，证明了算法的鲁棒性和组件贡献。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13792v1",
      "published_date": "2024-09-20 09:53:27 UTC",
      "updated_date": "2024-09-20 09:53:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:02:04.470886"
    },
    {
      "arxiv_id": "2409.13359v1",
      "title": "EmotionQueen: A Benchmark for Evaluating Empathy of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyan Chen",
        "Hao Wang",
        "Songzhou Yan",
        "Sijia Liu",
        "Yueze Li",
        "Yi Zhao",
        "Yanghua Xiao"
      ],
      "abstract": "Emotional intelligence in large language models (LLMs) is of great importance\nin Natural Language Processing. However, the previous research mainly focus on\nbasic sentiment analysis tasks, such as emotion recognition, which is not\nenough to evaluate LLMs' overall emotional intelligence. Therefore, this paper\npresents a novel framework named EmotionQueen for evaluating the emotional\nintelligence of LLMs. The framework includes four distinctive tasks: Key Event\nRecognition, Mixed Event Recognition, Implicit Emotional Recognition, and\nIntention Recognition. LLMs are requested to recognize important event or\nimplicit emotions and generate empathetic response. We also design two metrics\nto evaluate LLMs' capabilities in recognition and response for emotion-related\nstatements. Experiments yield significant conclusions about LLMs' capabilities\nand limitations in emotion intelligence.",
      "tldr_zh": "本论文提出 EmotionQueen 框架，用于评估大型语言模型（LLMs）的情感智能，弥补了以往研究仅限于基本情感识别任务的不足。框架包括四个任务：Key Event Recognition、Mixed Event Recognition、Implicit Emotional Recognition 和 Intention Recognition，要求 LLMs 识别关键事件、隐含情感并生成移情响应。论文设计了两个评估指标，分别针对情感相关语句的识别和响应能力；实验结果揭示了 LLMs 在情感智能方面的显著能力与局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 (Findings)",
      "pdf_url": "http://arxiv.org/pdf/2409.13359v1",
      "published_date": "2024-09-20 09:44:51 UTC",
      "updated_date": "2024-09-20 09:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:02:13.234829"
    },
    {
      "arxiv_id": "2409.13791v1",
      "title": "Multi-omics data integration for early diagnosis of hepatocellular carcinoma (HCC) using machine learning",
      "title_zh": "翻译失败",
      "authors": [
        "Annette Spooner",
        "Mohammad Karimi Moridani",
        "Azadeh Safarchi",
        "Salim Maher",
        "Fatemeh Vafaee",
        "Amany Zekry",
        "Arcot Sowmya"
      ],
      "abstract": "The complementary information found in different modalities of patient data\ncan aid in more accurate modelling of a patient's disease state and a better\nunderstanding of the underlying biological processes of a disease. However, the\nanalysis of multi-modal, multi-omics data presents many challenges, including\nhigh dimensionality and varying size, statistical distribution, scale and\nsignal strength between modalities. In this work we compare the performance of\na variety of ensemble machine learning algorithms that are capable of late\nintegration of multi-class data from different modalities. The ensemble methods\nand their variations tested were i) a voting ensemble, with hard and soft vote,\nii) a meta learner, iii) a multi-modal Adaboost model using a hard vote, a soft\nvote and a meta learner to integrate the modalities on each boosting round, the\nPB-MVBoost model and a novel application of a mixture of experts model. These\nwere compared to simple concatenation as a baseline. We examine these methods\nusing data from an in-house study on hepatocellular carcinoma (HCC), along with\nfour validation datasets on studies from breast cancer and irritable bowel\ndisease (IBD). Using the area under the receiver operating curve as a measure\nof performance we develop models that achieve a performance value of up to 0.85\nand find that two boosted methods, PB-MVBoost and Adaboost with a soft vote\nwere the overall best performing models. We also examine the stability of\nfeatures selected, and the size of the clinical signature determined. Finally,\nwe provide recommendations for the integration of multi-modal multi-class data.",
      "tldr_zh": "这篇论文探讨了使用机器学习整合多组学数据（multi-omics data）来实现肝细胞癌 (HCC) 的早期诊断，旨在应对多模态数据的高维度和分布差异等挑战。研究比较了多种集成机器学习算法，包括投票集成（voting ensemble）、元学习器（meta learner）、多模态 Adaboost（包括 hard vote 和 soft vote）、PB-MVBoost 以及一种新颖的混合专家模型（mixture of experts model），并以简单连接作为基线。结果显示，PB-MVBoost 和 Adaboost with soft vote 是最佳模型，AUC 值最高达 0.85，并在特征选择稳定性和临床特征大小分析基础上，提供了对多模态多类数据整合的实用推荐。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13791v1",
      "published_date": "2024-09-20 09:38:02 UTC",
      "updated_date": "2024-09-20 09:38:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:02:27.543842"
    },
    {
      "arxiv_id": "2409.13354v1",
      "title": "Recent Advancement of Emotion Cognition in Large Language Models",
      "title_zh": "大型语言模型中情感认知的最新进展",
      "authors": [
        "Yuyan Chen",
        "Yanghua Xiao"
      ],
      "abstract": "Emotion cognition in large language models (LLMs) is crucial for enhancing\nperformance across various applications, such as social media, human-computer\ninteraction, and mental health assessment. We explore the current landscape of\nresearch, which primarily revolves around emotion classification, emotionally\nrich response generation, and Theory of Mind assessments, while acknowledge the\nchallenges like dependency on annotated data and complexity in emotion\nprocessing. In this paper, we present a detailed survey of recent progress in\nLLMs for emotion cognition. We explore key research studies, methodologies,\noutcomes, and resources, aligning them with Ulric Neisser's cognitive stages.\nAdditionally, we outline potential future directions for research in this\nevolving field, including unsupervised learning approaches and the development\nof more complex and interpretable emotion cognition LLMs. We also discuss\nadvanced methods such as contrastive learning used to improve LLMs' emotion\ncognition capabilities.",
      "tldr_zh": "这篇论文调查了大型语言模型（LLMs）在情感认知方面的最新进展，强调其在社交媒体、人机交互和心理健康评估等应用中的重要性。研究涵盖了情感分类、情感丰富响应生成以及理论思维（Theory of Mind）评估等关键领域，同时讨论了挑战，如对标注数据的依赖和情感处理复杂性。论文通过回顾方法、结果和资源，并与 Ulric Neisser 的认知阶段对齐，提出了未来方向，包括无监督学习和采用对比学习（contrastive learning）等高级技术来开发更复杂、可解释的情感认知模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13354v1",
      "published_date": "2024-09-20 09:34:58 UTC",
      "updated_date": "2024-09-20 09:34:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:02:38.303926"
    },
    {
      "arxiv_id": "2409.13349v2",
      "title": "ID-Guard: A Universal Framework for Combating Facial Manipulation via Breaking Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Zuomin Qu",
        "Wei Lu",
        "Xiangyang Luo",
        "Qian Wang",
        "Xiaochun Cao"
      ],
      "abstract": "The misuse of deep learning-based facial manipulation poses a significant\nthreat to civil rights. To prevent this fraud at its source, proactive defense\nhas been proposed to disrupt the manipulation process by adding invisible\nadversarial perturbations into images, making the forged output unconvincing to\nobservers. However, the non-specific disruption against the output may lead to\nthe retention of identifiable facial features, potentially resulting in the\nstigmatization of the individual. This paper proposes a universal framework for\ncombating facial manipulation, termed ID-Guard. Specifically, this framework\noperates with a single forward pass of an encoder-decoder network to produce a\ncross-model transferable adversarial perturbation. A novel Identity Destruction\nModule (IDM) is introduced to degrade identifiable features in forged faces. We\noptimize the perturbation generation by framing the disruption of different\nfacial manipulations as a multi-task learning problem, and a dynamic weight\nstrategy is devised to enhance cross-model performance. Experimental results\ndemonstrate that the proposed ID-Guard exhibits strong efficacy in defending\nagainst various facial manipulation models, effectively degrading identifiable\nregions in manipulated images. It also enables disrupted images to evade facial\ninpainting and image recognition systems. Additionally, ID-Guard can seamlessly\nfunction as a plug-and-play component, integrating with other tasks such as\nadversarial training.",
      "tldr_zh": "该论文提出了一种通用框架 ID-Guard，用于对抗基于深度学习的面部操纵，通过在图像中添加不可见的对抗扰动来破坏识别过程，避免保留可识别特征并防止个体污名化。具体地，该框架利用一个编码器-解码器网络的单向传播生成跨模型传输的对抗扰动，并引入 Identity Destruction Module (IDM) 以及多任务学习策略来优化扰动生成，提升对不同操纵模型的防御效果。实验结果表明，ID-Guard 有效地降级伪造图像中的可识别区域，并能规避面部修复和图像识别系统，同时作为即插即用组件与其他任务整合。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13349v2",
      "published_date": "2024-09-20 09:30:08 UTC",
      "updated_date": "2025-04-10 07:58:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:02:50.938608"
    },
    {
      "arxiv_id": "2409.13346v1",
      "title": "Imagine yourself: Tuning-Free Personalized Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zecheng He",
        "Bo Sun",
        "Felix Juefei-Xu",
        "Haoyu Ma",
        "Ankit Ramchandani",
        "Vincent Cheung",
        "Siddharth Shah",
        "Anmol Kalia",
        "Harihar Subramanyam",
        "Alireza Zareian",
        "Li Chen",
        "Ankit Jain",
        "Ning Zhang",
        "Peizhao Zhang",
        "Roshan Sumbaly",
        "Peter Vajda",
        "Animesh Sinha"
      ],
      "abstract": "Diffusion models have demonstrated remarkable efficacy across various\nimage-to-image tasks. In this research, we introduce Imagine yourself, a\nstate-of-the-art model designed for personalized image generation. Unlike\nconventional tuning-based personalization techniques, Imagine yourself operates\nas a tuning-free model, enabling all users to leverage a shared framework\nwithout individualized adjustments. Moreover, previous work met challenges\nbalancing identity preservation, following complex prompts and preserving good\nvisual quality, resulting in models having strong copy-paste effect of the\nreference images. Thus, they can hardly generate images following prompts that\nrequire significant changes to the reference image, \\eg, changing facial\nexpression, head and body poses, and the diversity of the generated images is\nlow. To address these limitations, our proposed method introduces 1) a new\nsynthetic paired data generation mechanism to encourage image diversity, 2) a\nfully parallel attention architecture with three text encoders and a fully\ntrainable vision encoder to improve the text faithfulness, and 3) a novel\ncoarse-to-fine multi-stage finetuning methodology that gradually pushes the\nboundary of visual quality. Our study demonstrates that Imagine yourself\nsurpasses the state-of-the-art personalization model, exhibiting superior\ncapabilities in identity preservation, visual quality, and text alignment. This\nmodel establishes a robust foundation for various personalization applications.\nHuman evaluation results validate the model's SOTA superiority across all\naspects (identity preservation, text faithfulness, and visual appeal) compared\nto the previous personalization models.",
      "tldr_zh": "该研究提出Imagine yourself，一种无调优(tuning-free)的个性化图像生成模型，基于Diffusion models，旨在解决现有方法在身份保留、遵循复杂提示和视觉质量平衡上的挑战。模型引入了三种关键创新：1) 一种新的合成配对数据生成机制，以提升生成图像的多样性；2) 采用全并行注意力架构，包括三个文本编码器和一个完全可训练的视觉编码器，提高文本忠实度；3) 一种粗到细的多阶段微调方法，逐步优化视觉质量。实验结果显示，Imagine yourself在身份保留、视觉质量和文本对齐方面超越了现有状态-of-the-art模型，人评结果进一步证实其在所有方面（如身份保留、文本忠实度和视觉吸引力）的优越性，为个性化图像生成应用奠定了坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13346v1",
      "published_date": "2024-09-20 09:21:49 UTC",
      "updated_date": "2024-09-20 09:21:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:03:02.251950"
    },
    {
      "arxiv_id": "2409.13345v1",
      "title": "A Novel Adaptive Fine-Tuning Algorithm for Multimodal Models: Self-Optimizing Classification and Selection of High-Quality Datasets in Remote Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Ren",
        "Tianyi Zhang",
        "Zhixiong Han",
        "Weibin Li",
        "Zhiyang Wang",
        "Wenbo Ji",
        "Chenhao Qin",
        "Chenbin Liang",
        "Licheng Jiao"
      ],
      "abstract": "We propose an adaptive fine-tuning algorithm for multimodal large models. The\ncore steps of this algorithm involve two stages of truncation. First, the vast\namount of data is projected into a semantic vector space, and the\nMiniBatchKMeans algorithm is used for automated clustering. This classification\nensures that the data within each cluster exhibit high semantic similarity.\nNext, we process the data in each cluster, calculating the translational\ndifference between the original and perturbed data in the multimodal large\nmodel's vector space. This difference serves as a generalization metric for the\ndata. Based on this metric, we select the data with high generalization\npotential for training. We applied this algorithm to train the\nInternLM-XComposer2-VL-7B model on two 3090 GPUs using one-third of the GeoChat\nmultimodal remote sensing dataset. The results demonstrate that our algorithm\noutperforms the state-of-the-art baselines. various baselines. The model\ntrained on our optimally chosen one-third dataset, based on experimental\nvalidation, exhibited only 1% reduction in performance across various remote\nsensing metrics compared to the model trained on the full dataset. This\napproach significantly preserved general-purpose capabilities while reducing\ntraining time by 68.2%. Furthermore, the model achieved scores of 89.86 and\n77.19 on the UCMerced and AID evaluation datasets, respectively, surpassing the\nGeoChat dataset by 5.43 and 5.16 points. It only showed a 0.91-point average\ndecrease on the LRBEN evaluation dataset.",
      "tldr_zh": "本研究提出了一种新型自适应微调算法，用于多模态模型的优化，专注于遥感领域的Self-Optimizing分类和高质量数据集选择。该算法包括两个核心阶段：首先，使用MiniBatchKMeans算法将数据投影到语义向量空间进行自动聚类，确保聚类内数据的高语义相似性；其次，通过计算原始数据与扰动数据在多模态模型向量空间中的平移差异，作为泛化度量，选择高泛化潜力的数据用于训练。实验中，该算法应用于InternLM-XComposer2-VL-7B模型，使用GeoChat数据集的三分之一在两个3090 GPU上训练，结果显示其优于现有基线，仅在各种遥感指标上性能降低1%，同时将训练时间减少68.2%。此外，该模型在UCMerced和AID评估数据集上分别得分89.86和77.19，比GeoChat高出5.43和5.16分，仅在LRBEN上平均降低0.91分。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13345v1",
      "published_date": "2024-09-20 09:19:46 UTC",
      "updated_date": "2024-09-20 09:19:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:03:14.838089"
    },
    {
      "arxiv_id": "2409.13790v2",
      "title": "Revisiting Synthetic Human Trajectories: Imitative Generation and Benchmarks Beyond Datasaurus",
      "title_zh": "翻译失败",
      "authors": [
        "Bangchao Deng",
        "Xin Jing",
        "Tianyue Yang",
        "Bingqing Qu",
        "Dingqi Yang",
        "Philippe Cudre-Mauroux"
      ],
      "abstract": "Human trajectory data, which plays a crucial role in various applications\nsuch as crowd management and epidemic prevention, is challenging to obtain due\nto practical constraints and privacy concerns. In this context, synthetic human\ntrajectory data is generated to simulate as close as possible to real-world\nhuman trajectories, often under summary statistics and distributional\nsimilarities. However, these similarities oversimplify complex human mobility\npatterns (a.k.a. ``Datasaurus''), resulting in intrinsic biases in both\ngenerative model design and benchmarks of the generated trajectories. Against\nthis background, we propose MIRAGE, a huMan-Imitative tRAjectory GenErative\nmodel designed as a neural Temporal Point Process integrating an Exploration\nand Preferential Return model. It imitates the human decision-making process in\ntrajectory generation, rather than fitting any specific statistical\ndistributions as traditional methods do, thus avoiding the Datasaurus issue. We\nalso propose a comprehensive task-based evaluation protocol beyond Datasaurus\nto systematically benchmark trajectory generative models on four typical\ndownstream tasks, integrating multiple techniques and evaluation metrics for\neach task, to assess the ultimate utility of the generated trajectories. We\nconduct a thorough evaluation of MIRAGE on three real-world user trajectory\ndatasets against a sizeable collection of baselines. Results show that compared\nto the best baselines, MIRAGE-generated trajectory data not only achieves the\nbest statistical and distributional similarities with 59.0-67.7% improvement,\nbut also yields the best performance in the task-based evaluation with\n10.9-33.4% improvement. A series of ablation studies also validate the key\ndesign choices of MIRAGE.",
      "tldr_zh": "这篇论文重新审视了合成人类轨迹生成的问题，指出传统方法因过度简化复杂移动模式（Datasaurus）而导致生成模型和基准测试的固有偏差。作者提出 MIRAGE 模型，这是一个基于神经 Temporal Point Process 的生成框架，整合 Exploration and Preferential Return 模型，通过模仿人类决策过程来生成更真实的轨迹数据。论文还引入了一个全面的任务-based 评估协议，超越 Datasaurus，评估模型在四个下游任务（如人群管理和流行病预防）上的实用性。实验结果显示，MIRAGE 在三个真实数据集上比基线模型在统计分布相似性上提升 59.0-67.7%，并在任务性能上改善 10.9-33.4%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD'25",
      "pdf_url": "http://arxiv.org/pdf/2409.13790v2",
      "published_date": "2024-09-20 09:07:27 UTC",
      "updated_date": "2025-05-19 02:35:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:03:27.626662"
    },
    {
      "arxiv_id": "2409.13338v3",
      "title": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time",
      "title_zh": "大型语言模型中的时间感知：跨",
      "authors": [
        "David Herel",
        "Vojtech Bartek",
        "Jiri Jirak",
        "Tomas Mikolov"
      ],
      "abstract": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）在处理时间相关事实时的表现，强调现有评估忽略了时间维度的问题。研究者提出一个新框架和数据集，涵盖2018-2024年的8000多个全球事件，标注到天级别，并涉及政治、科技和商业等领域。TimeShift 评估方法系统测试LLMs的 temporal reasoning，发现基础模型在时间敏感回忆上往往优于指令微调或合成训练模型。结果还揭示模型在处理改述事实时存在脆弱性，这为开发适应动态知识的时间感知语言模型提供了关键进展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13338v3",
      "published_date": "2024-09-20 08:57:20 UTC",
      "updated_date": "2025-05-15 14:13:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:03:37.841157"
    },
    {
      "arxiv_id": "2409.13321v1",
      "title": "SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report Automation",
      "title_zh": "翻译失败",
      "authors": [
        "Jinge Wu",
        "Yunsoo Kim",
        "Daqian Shi",
        "David Cliffton",
        "Fenglin Liu",
        "Honghan Wu"
      ],
      "abstract": "Inspired by the success of large language models (LLMs), there is growing\nresearch interest in developing LLMs in the medical domain to assist\nclinicians. However, for hospitals, using closed-source commercial LLMs\ninvolves privacy issues, and developing open-source public LLMs requires\nlarge-scale computational resources, which are usually limited, especially in\nresource-efficient regions and low-income countries. We propose an open-source\nSmall Language and Vision Assistant (SLaVA-CXR) that can be used for Chest\nX-Ray report automation. To efficiently train a small assistant, we first\npropose the Re$^3$Training method, which simulates the cognitive development of\nradiologists and optimizes the model in the Recognition, Reasoning, and\nReporting training manner. Then, we introduce a data synthesis method, RADEX,\nwhich can generate a high-quality and diverse training corpus with privacy\nregulation compliance. The extensive experiments show that our SLaVA-CXR built\non a 2.7B backbone not only outperforms but also achieves 6 times faster\ninference efficiency than previous state-of-the-art larger models.",
      "tldr_zh": "该论文提出开源的 SLaVA-CXR 模型，这是一个 Small Language and Vision Assistant，用于自动化胸部 X-ray 报告，以解决使用商业 LLMs 的隐私问题和资源限制。作者引入 Re³Training 方法，模拟放射科医生的认知发展，通过 Recognition、Reasoning 和 Reporting 的训练方式优化模型；同时，开发了 RADEX 数据合成方法，以生成高质量、多样化的隐私合规训练语料。实验结果显示，基于 2.7B 参数骨干的 SLaVA-CXR 不仅超过了现有最先进模型的性能，还实现了 6 倍的推理效率提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13321v1",
      "published_date": "2024-09-20 08:28:46 UTC",
      "updated_date": "2024-09-20 08:28:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:03:50.336798"
    },
    {
      "arxiv_id": "2409.17174v3",
      "title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal Significance and Consistency",
      "title_zh": "CSCE：通过",
      "authors": [
        "Kangsheng Wang",
        "Xiao Zhang",
        "Juntao Lyu",
        "Tianyu Hu",
        "Huimin Ma"
      ],
      "abstract": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal hallucinations between a step of reasoning and corresponding state\ntransitions are becoming a significant obstacle to advancing LLMs' reasoning\ncapabilities, especially in long-range reasoning tasks. This paper proposes a\nnon-chain-based reasoning framework for simultaneous consideration of causal\nsignificance and consistency, i.e., the Causal Significance and Consistency\nEnhancer (CSCE). We customize LLM's loss function utilizing treatment effect\nassessments to enhance its reasoning ability from two aspects: causal\nsignificance and consistency. This ensures that the model captures essential\ncausal relationships and maintains robust and consistent performance across\nvarious scenarios. Additionally, we transform the reasoning process from the\ncascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)中基于链式推理（如Chain of Thought, CoT）方法的因果幻觉问题，提出了一种非链式推理框架Causal Significance and Consistency Enhancer (CSCE)。CSCE通过自定义LLMs的损失函数，利用treatment effect assessments，从因果显著性和一致性两个方面增强模型的推理能力，确保捕捉关键因果关系并在各种场景中保持稳健性能。不同于传统的多步级联推理，CSCE采用一次性输出整个推理过程的方法，提高了效率。实验结果显示，该框架显著提高了推理成功率和速度，并证明非链式方法也能有效支持LLMs的推理任务。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages,4 figures. This paper has been accepted for presentation at\n  IEEE International Conference on Multimedia & Expo 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.17174v3",
      "published_date": "2024-09-20 08:28:23 UTC",
      "updated_date": "2025-03-24 08:23:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:04:02.319257"
    },
    {
      "arxiv_id": "2409.13312v2",
      "title": "GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Ximing Wen",
        "Wenjuan Tan",
        "Rosina O. Weber"
      ],
      "abstract": "Pretrained transformer-based Language Models (LMs) are well-known for their\nability to achieve significant improvement on text classification tasks with\ntheir powerful word embeddings, but their black-box nature, which leads to a\nlack of interpretability, has been a major concern. In this work, we introduce\nGAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical\nNetwork designed to explain the decisions of text classification models built\nwith LM encoders. In our approach, the input vector and prototypes are regarded\nas nodes within a graph, and we utilize multi-head graph attention to\nselectively construct edges between the input node and prototype nodes to learn\nan interpretable prototypical representation. During inference, the model makes\ndecisions based on a linear combination of activated prototypes weighted by the\nattention score assigned for each prototype, allowing its choices to be\ntransparently explained by the attention weights and the prototypes projected\ninto the closest matching training examples. Experiments on multiple public\ndatasets show our approach achieves superior results without sacrificing the\naccuracy of the original black-box LMs. We also compare with four alternative\nprototypical network variations and our approach achieves the best accuracy and\nF1 among all. Our case study and visualization of prototype clusters also\ndemonstrate the efficiency in explaining the decisions of black-box models\nbuilt with LMs.",
      "tldr_zh": "本文提出 GAProtoNet，一种基于 Multi-head Graph Attention 的原型网络，用于提升预训练 Language Models (LMs) 在文本分类任务中的可解释性。该方法将输入向量和原型视为图节点，通过多头图注意力选择性地构建边，学习可解释的原型表示，并在推理时通过注意力权重和原型投影实现透明决策。实验结果显示，GAProtoNet 在多个公共数据集上达到了与黑盒 LMs 相当的准确性和 F1 分数，并优于其他四种原型网络变体。案例研究和原型聚类的可视化进一步证明了其在解释模型决策方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 5 figues, accepted by COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.13312v2",
      "published_date": "2024-09-20 08:15:17 UTC",
      "updated_date": "2024-12-19 20:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:04:15.110199"
    },
    {
      "arxiv_id": "2409.13299v2",
      "title": "OMG-RL:Offline Model-based Guided Reward Learning for Heparin Treatment",
      "title_zh": "翻译失败",
      "authors": [
        "Yooseok Lim",
        "Sujee Lee"
      ],
      "abstract": "Accurate medication dosing holds an important position in the overall patient\ntherapeutic process. Therefore, much research has been conducted to develop\noptimal administration strategy based on Reinforcement learning (RL). However,\nRelying solely on a few explicitly defined reward functions makes it difficult\nto learn a treatment strategy that encompasses the diverse characteristics of\nvarious patients. Moreover, the multitude of drugs utilized in clinical\npractice makes it infeasible to construct a dedicated reward function for each\nmedication. Here, we tried to develop a reward network that captures\nclinicians' therapeutic intentions, departing from explicit rewards, and to\nderive an optimal heparin dosing policy. In this study, we introduce Offline\nModel-based Guided Reward Learning (OMG-RL), which performs offline inverse RL\n(IRL). Through OMG-RL, we learn a parameterized reward function that captures\nthe expert's intentions from limited data, thereby enhancing the agent's\npolicy. We validate the proposed approach on the heparin dosing task. We show\nthat OMG-RL policy is positively reinforced not only in terms of the learned\nreward network but also in activated partial thromboplastin time (aPTT), a key\nindicator for monitoring the effects of heparin. This means that the OMG-RL\npolicy adequately reflects clinician's intentions. This approach can be widely\nutilized not only for the heparin dosing problem but also for RL-based\nmedication dosing tasks in general.",
      "tldr_zh": "本研究针对药物剂量优化问题，提出了一种名为 Offline Model-based Guided Reward Learning (OMG-RL) 的方法，用于肝素治疗。该方法采用离线逆强化学习 (IRL) 从有限数据中学习参数化的奖励函数，以捕捉临床专家的治疗意图，从而改进强化学习 (RL) 代理的策略。在肝素剂量任务上验证显示，OMG-RL 政策不仅在学到的奖励网络上得到强化，还显著改善了激活部分凝血酶时间 (aPTT) 指标，证明其能有效反映临床意图。该方法可广泛应用于其他 RL 基于的药物剂量任务中。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13299v2",
      "published_date": "2024-09-20 07:51:37 UTC",
      "updated_date": "2024-12-31 08:27:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:04:26.941025"
    },
    {
      "arxiv_id": "2409.13787v1",
      "title": "Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Hu",
        "Chenwei Zhang",
        "Min Yang",
        "Xiaodan Liang",
        "Chengming Li",
        "Xiping Hu"
      ],
      "abstract": "With the rapid development of deep learning methods, there have been many\nbreakthroughs in the field of text classification. Models developed for this\ntask have been shown to achieve high accuracy. However, most of these models\nare trained using labeled data from seen domains. It is difficult for these\nmodels to maintain high accuracy in a new challenging unseen domain, which is\ndirectly related to the generalization of the model. In this paper, we study\nthe multi-source Domain Generalization of text classification and propose a\nframework to use multiple seen domains to train a model that can achieve high\naccuracy in an unseen domain. Specifically, we propose a multi-source\nmeta-learning Domain Generalization framework to simulate the process of model\ngeneralization to an unseen domain, so as to extract sufficient domain-related\nfeatures. We introduced a memory mechanism to store domain-specific features,\nwhich coordinate with the meta-learning framework. Besides, we adopt the novel\n\"jury\" mechanism that enables the model to learn sufficient domain-invariant\nfeatures. Experiments demonstrate that our meta-learning framework can\neffectively enhance the ability of the model to generalize to an unseen domain\nand can outperform the state-of-the-art methods on multi-source text\nclassification datasets.",
      "tldr_zh": "本文研究文本分类模型在未见域（unseen domains）上的泛化问题，提出一种多源元学习（multi-source meta-learning）框架，通过模拟模型泛化过程来提取足够的域相关特征。框架引入记忆机制（memory mechanism）来存储域特定特征，并采用“jury”机制来学习域不变特征（domain-invariant features），从而提升模型的泛化能力。实验结果显示，该框架在多源文本分类数据集上优于现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13787v1",
      "published_date": "2024-09-20 07:46:21 UTC",
      "updated_date": "2024-09-20 07:46:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:04:37.868632"
    },
    {
      "arxiv_id": "2409.13284v1",
      "title": "Time Distributed Deep Learning models for Purely Exogenous Forecasting. Application to Water Table Depth Prediction using Weather Image Time Series",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Salis",
        "Abdourrahmane M. Atto",
        "Stefano Ferraris",
        "Rosa Meo"
      ],
      "abstract": "Groundwater resources are one of the most relevant elements in the water\ncycle, therefore developing models to accurately predict them is a pivotal task\nin the sustainable resources management framework. Deep Learning (DL) models\nhave been revealed very effective in hydrology, especially by feeding spatially\ndistributed data (e.g. raster data). In many regions, hydrological measurements\nare difficult to obtain regularly or periodically in time, and in some cases,\nlast available data are not up to date. Reversely, weather data, which\nsignificantly impacts water resources, are usually more available and with\nhigher quality. More specifically, we have proposed two different DL models to\npredict the water table depth in the Grana-Maira catchment (Piemonte, IT) using\nonly exogenous weather image time series. To deal with the image time series,\nboth models are made of a first Time Distributed Convolutional Neural Network\n(TDC) which encodes the image available at each time step into a vectorial\nrepresentation. The first model, TDC-LSTM uses then a Sequential Module based\non an LSTM layer to learn temporal relations and output the predictions. The\nsecond model, TDC-UnPWaveNet uses instead a new version of the WaveNet\narchitecture, adapted here to output a sequence shorter and completely shifted\nin the future with respect to the input one. To this aim, and to deal with the\ndifferent sequence lengths in the UnPWaveNet, we have designed a new Channel\nDistributed layer, that acts like a Time Distributed one but on the channel\ndimension, i.e. applying the same set of operations to each channel of the\ninput. TDC-LSTM and TDC-UnPWaveNet have shown both remarkable results. However,\nthe two models have focused on different learnable information: TDC-LSTM has\nfocused more on lowering the bias, while the TDC-UnPWaveNet has focused more on\nthe temporal dynamics maximising correlation and KGE.",
      "tldr_zh": "该研究开发了两种基于时间分布深度学习（Time Distributed Deep Learning）模型，用于纯外生预测，具体应用于利用天气图像时间序列预测意大利 Grana-Maira 流域的地下水位深度，以解决水文数据获取困难的问题。第一个模型 TDC-LSTM 结合 Time Distributed Convolutional Neural Network (TDC) 来编码每个时间步的图像，然后使用 LSTM 层学习时间关系并输出预测。第二个模型 TDC-UnPWaveNet 则采用 TDC 编码图像，并引入一个新版 WaveNet 架构及 Channel Distributed 层，以处理不同序列长度并输出未来偏移的预测序列。实验结果显示，两个模型均表现出色，其中 TDC-LSTM 更侧重降低偏差，而 TDC-UnPWaveNet 更注重时间动态，提升了相关性和 KGE（Kling-Gupta Efficiency）。这项工作为可持续水资源管理提供了有效的深度学习工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13284v1",
      "published_date": "2024-09-20 07:25:54 UTC",
      "updated_date": "2024-09-20 07:25:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:04:51.116237"
    },
    {
      "arxiv_id": "2409.13259v1",
      "title": "A generalizable framework for unlocking missing reactions in genome-scale metabolic networks using deep learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyi Liu",
        "Hongpeng Yang",
        "Chengwei Ai",
        "Ruihan Dong",
        "Yijie Ding",
        "Qianqian Yuan",
        "Jijun Tang",
        "Fei Guo"
      ],
      "abstract": "Incomplete knowledge of metabolic processes hinders the accuracy of\nGEnome-scale Metabolic models (GEMs), which in turn impedes advancements in\nsystems biology and metabolic engineering. Existing gap-filling methods\ntypically rely on phenotypic data to minimize the disparity between\ncomputational predictions and experimental results. However, there is still a\nlack of an automatic and precise gap-filling method for initial state GEMs\nbefore experimental data and annotated genomes become available. In this study,\nwe introduce CLOSEgaps, a deep learning-driven tool that addresses the\ngap-filling issue by modeling it as a hyperedge prediction problem within GEMs.\nSpecifically, CLOSEgaps maps metabolic networks as hypergraphs and learns their\nhyper-topology features to identify missing reactions and gaps by leveraging\nhypothetical reactions. This innovative approach allows for the\ncharacterization and curation of both known and hypothetical reactions within\nmetabolic networks. Extensive results demonstrate that CLOSEgaps accurately\ngap-filling over 96% of artificially introduced gaps for various GEMs.\nFurthermore, CLOSEgaps enhances phenotypic predictions for 24 GEMs and also\nfinds a notable improvement in producing four crucial metabolites (Lactate,\nEthanol, Propionate, and Succinate) in two organisms. As a broadly applicable\nsolution for any GEM, CLOSEgaps represents a promising model to automate the\ngap-filling process and uncover missing connections between reactions and\nobserved metabolic phenotypes.",
      "tldr_zh": "该研究提出了一种通用的框架 CLOSEgaps，利用深度学习来自动填充基因组规模代谢模型 (GEMs) 中的缺失反应，将问题建模为超边预测 (hyperedge prediction)，并通过将代谢网络映射为超图 (hypergraphs) 和学习其超拓扑特征来识别和添加假设反应 (hypothetical reactions)。这种方法无需依赖表型数据，即可精确处理初始 GEMs，并为已知和假设反应提供表征和校正。实验结果显示，CLOSEgaps 准确填充了超过96%的人为引入间隙，提升了24个 GEMs 的表型预测，并显著改善了两个生物体中乳酸 (Lactate)、乙醇 (Ethanol)、丙酸 (Propionate) 和琥珀酸 (Succinate) 等关键代谢物的生产，从而为系统生物学和代谢工程提供了一个高效的自动间隙填充解决方案。",
      "categories": [
        "q-bio.MN",
        "cs.AI"
      ],
      "primary_category": "q-bio.MN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13259v1",
      "published_date": "2024-09-20 06:47:44 UTC",
      "updated_date": "2024-09-20 06:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:05:02.184929"
    },
    {
      "arxiv_id": "2409.15377v1",
      "title": "Prompting Large Language Models for Supporting the Differential Diagnosis of Anemia",
      "title_zh": "翻译失败",
      "authors": [
        "Elisa Castagnari",
        "Lillian Muyama",
        "Adrien Coulet"
      ],
      "abstract": "In practice, clinicians achieve a diagnosis by following a sequence of steps,\nsuch as laboratory exams, observations, or imaging. The pathways to reach\ndiagnosis decisions are documented by guidelines authored by expert\norganizations, which guide clinicians to reach a correct diagnosis through\nthese sequences of steps. While these guidelines are beneficial for following\nmedical reasoning and consolidating medical knowledge, they have some\ndrawbacks. They often fail to address patients with uncommon conditions due to\ntheir focus on the majority population, and are slow and costly to update,\nmaking them unsuitable for rapidly emerging diseases or new practices. Inspired\nby clinical guidelines, our study aimed to develop pathways similar to those\nthat can be obtained in clinical guidelines. We tested three Large Language\nModels (LLMs) -Generative Pretrained Transformer 4 (GPT-4), Large Language\nModel Meta AI (LLaMA), and Mistral -on a synthetic yet realistic dataset to\ndifferentially diagnose anemia and its subtypes. By using advanced prompting\ntechniques to enhance the decision-making process, we generated diagnostic\npathways using these models. Experimental results indicate that LLMs hold huge\npotential in clinical pathway discovery from patient data, with GPT-4\nexhibiting the best performance in all conducted experiments.",
      "tldr_zh": "本文研究使用大型语言模型 (LLMs) 来支持贫血的鉴别诊断 (Differential Diagnosis of Anemia)，以克服传统临床指南的缺点，如无法处理罕见病例和更新缓慢的问题。研究团队测试了 GPT-4、LLaMA 和 Mistral 等模型，通过高级提示技术在合成但真实的数据集上生成诊断路径。实验结果显示，LLMs 在从患者数据中发现临床路径方面具有巨大潜力，其中 GPT-4 在所有实验中表现出最佳性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15377v1",
      "published_date": "2024-09-20 06:47:36 UTC",
      "updated_date": "2024-09-20 06:47:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:05:14.446109"
    },
    {
      "arxiv_id": "2409.13254v1",
      "title": "Emergent Collective Reproduction via Evolving Neuronal Flocks",
      "title_zh": "翻译失败",
      "authors": [
        "Nam H. Le",
        "Richard Watson",
        "Mike Levin",
        "Chrys Buckley"
      ],
      "abstract": "This study facilitates the understanding of evolutionary transitions in\nindividuality (ETIs) through a novel artificial life framework, named VitaNova,\nthat intricately merges self-organization and natural selection to simulate the\nemergence of complex, reproductive groups. By dynamically modelling individual\nagents within an environment that challenges them with predators and spatial\nconstraints, VitaNova elucidates the mechanisms by which simple agents evolve\ninto cohesive units exhibiting collective reproduction. The findings underscore\nthe synergy between self-organized behaviours and adaptive evolutionary\nstrategies as fundamental drivers of ETIs. This approach not only contributes\nto a deeper understanding of higher-order biological individuality but also\nsets a new precedent in the empirical investigation of ETIs, challenging and\nextending current theoretical frameworks.",
      "tldr_zh": "本研究通过名为 VitaNova 的新型人工生命框架，结合自组织和自然选择，模拟简单代理在面对捕食者和空间约束的环境中如何演变为具有集体繁殖的凝聚单位。VitaNova 框架动态建模个体代理的演化过程，揭示了自组织行为与适应性进化策略的协同作用作为推动进化转变 in individuality (ETIs) 的关键机制。研究结果不仅加深了对更高阶生物个体性的理解，还为 ETIs 的实证调查设定了新标准，并挑战了现有理论框架。",
      "categories": [
        "q-bio.PE",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "q-bio.PE",
      "comment": "9 pages, 10 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2409.13254v1",
      "published_date": "2024-09-20 06:22:24 UTC",
      "updated_date": "2024-09-20 06:22:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:05:25.930058"
    },
    {
      "arxiv_id": "2409.13252v1",
      "title": "Leveraging Knowledge Graphs and LLMs to Support and Monitor Legislative Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Colombo"
      ],
      "abstract": "Knowledge Graphs (KGs) have been used to organize large datasets into\nstructured, interconnected information, enhancing data analytics across various\nfields. In the legislative context, one potential natural application of KGs is\nmodeling the intricate set of interconnections that link laws and their\narticles with each other and the broader legislative context.\n  At the same time, the rise of large language models (LLMs) such as GPT has\nopened new opportunities in legal applications, such as text generation and\ndocument drafting. Despite their potential, the use of LLMs in legislative\ncontexts is critical since it requires the absence of hallucinations and\nreliance on up-to-date information, as new laws are published on a daily basis.\n  This work investigates how Legislative Knowledge Graphs and LLMs can\nsynergize and support legislative processes. We address three key questions:\nthe benefits of using KGs for legislative systems, how LLM can support\nlegislative activities by ensuring an accurate output, and how we can allow\nnon-technical users to use such technologies in their activities. To this aim,\nwe develop Legis AI Platform, an interactive platform focused on Italian\nlegislation that enhances the possibility of conducting legislative analysis\nand that aims to support lawmaking activities.",
      "tldr_zh": "这篇论文探讨了如何利用 Knowledge Graphs (KGs) 和 Large Language Models (LLMs) 协同支持立法系统，解决法律数据互联和 LLM 幻觉问题。研究重点回答了 KGs 在立法中的益处、LLMs 如何确保准确输出，以及如何让非技术用户轻松应用这些技术。最终，作者开发了 Legis AI Platform，一个针对意大利立法的交互平台，用于提升立法分析和法制作业效率。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13252v1",
      "published_date": "2024-09-20 06:21:03 UTC",
      "updated_date": "2024-09-20 06:21:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:05:37.783079"
    },
    {
      "arxiv_id": "2409.13244v2",
      "title": "From Cognition to Precognition: A Future-Aware Framework for Social Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Zeying Gong",
        "Tianshuai Hu",
        "Ronghe Qiu",
        "Junwei Liang"
      ],
      "abstract": "To navigate safely and efficiently in crowded spaces, robots should not only\nperceive the current state of the environment but also anticipate future human\nmovements. In this paper, we propose a reinforcement learning architecture,\nnamely Falcon, to tackle socially-aware navigation by explicitly predicting\nhuman trajectories and penalizing actions that block future human paths. To\nfacilitate realistic evaluation, we introduce a novel SocialNav benchmark\ncontaining two new datasets, Social-HM3D and Social-MP3D. This benchmark offers\nlarge-scale photo-realistic indoor scenes populated with a reasonable amount of\nhuman agents based on scene area size, incorporating natural human movements\nand trajectory patterns. We conduct a detailed experimental analysis with the\nstate-of-the-art learning-based method and two classic rule-based path-planning\nalgorithms on the new benchmark. The results demonstrate the importance of\nfuture prediction and our method achieves the best task success rate of 55%\nwhile maintaining about 90% personal space compliance. We will release our code\nand datasets. Videos of demonstrations can be viewed at\nhttps://zeying-gong.github.io/projects/falcon/ .",
      "tldr_zh": "本论文提出Falcon，一种基于强化学习(reinforcement learning)的未来感知框架，用于提升机器人社会化导航(socially-aware navigation)，通过预测人类轨迹并惩罚阻塞未来路径的动作，实现更安全高效的导航。研究者引入了SocialNav基准，包括两个新数据集Social-HM3D和Social-MP3D，这些数据集基于大规模真实室内场景，模拟自然人类运动以便更真实评估。实验结果显示，Falcon在该基准上比现有学习和规则-based方法表现优异，达到55%的任务成功率，同时保持约90%的个人空间合规率，为未来导航系统提供了重要启示。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Social Navigation; Trajectory Prediction; Auxiliary Tasks. This paper\n  has been accepted at the IEEE International Conference on Robotics and\n  Automation (ICRA) 2025. For more details, please refer to the project\n  website: https://zeying-gong.github.io/projects/falcon/",
      "pdf_url": "http://arxiv.org/pdf/2409.13244v2",
      "published_date": "2024-09-20 06:08:24 UTC",
      "updated_date": "2025-02-08 15:07:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:05:48.808785"
    },
    {
      "arxiv_id": "2409.13232v2",
      "title": "Relationship between Uncertainty in DNNs and Adversarial Attacks",
      "title_zh": "深度神经网络中的不确定性与对抗攻击之间的关系",
      "authors": [
        "Mabel Ogonna",
        "Abigail Adeniran",
        "Adewale Adeyemo"
      ],
      "abstract": "Deep Neural Networks (DNNs) have achieved state of the art results and even\noutperformed human accuracy in many challenging tasks, leading to DNNs adoption\nin a variety of fields including natural language processing, pattern\nrecognition, prediction, and control optimization. However, DNNs are\naccompanied by uncertainty about their results, causing them to predict an\noutcome that is either incorrect or outside of a certain level of confidence.\nThese uncertainties stem from model or data constraints, which could be\nexacerbated by adversarial attacks. Adversarial attacks aim to provide\nperturbed input to DNNs, causing the DNN to make incorrect predictions or\nincrease model uncertainty. In this review, we explore the relationship between\nDNN uncertainty and adversarial attacks, emphasizing how adversarial attacks\nmight raise DNN uncertainty.",
      "tldr_zh": "这篇论文审视了深度神经网络(DNNs)的不确定性与对抗攻击之间的关系。DNNs 在许多任务中实现了超越人类精度的表现，但其预测结果可能因不确定性而出现错误或信心不足，这些问题源于模型或数据限制。对抗攻击通过对输入数据进行微小扰动，旨在放大DNNs的不确定性，导致预测失准。本文强调了这种关系，指出对抗攻击如何加剧DNNs的不确定性，为未来防御策略提供了重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "review",
      "pdf_url": "http://arxiv.org/pdf/2409.13232v2",
      "published_date": "2024-09-20 05:38:38 UTC",
      "updated_date": "2025-02-24 21:31:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:06:00.986914"
    },
    {
      "arxiv_id": "2409.17173v1",
      "title": "A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource Hallucination Detection in Large Language Models",
      "title_zh": "一种多项填空考试方法，用于",
      "authors": [
        "Satoshi Munakata",
        "Taku Fukui",
        "Takao Mohri"
      ],
      "abstract": "Large language models (LLMs) often fabricate a hallucinatory text. Several\nmethods have been developed to detect such text by semantically comparing it\nwith the multiple versions probabilistically regenerated. However, a\nsignificant issue is that if the storyline of each regenerated text changes,\nthe generated texts become incomparable, which worsen detection accuracy. In\nthis paper, we propose a hallucination detection method that incorporates a\nmultiple-fill-in-the-blank exam approach to address this storyline-changing\nissue. First, our method creates a multiple-fill-in-the-blank exam by masking\nmultiple objects from the original text. Second, prompts an LLM to repeatedly\nanswer this exam. This approach ensures that the storylines of the exam answers\nalign with the original ones. Finally, quantifies the degree of hallucination\nfor each original sentence by scoring the exam answers, considering the\npotential for \\emph{hallucination snowballing} within the original text itself.\nExperimental results show that our method alone not only outperforms existing\nmethods, but also achieves clearer state-of-the-art performance in the\nensembles with existing methods.",
      "tldr_zh": "本论文提出了一种基于 multiple-fill-in-the-blank exam 的方法，以提升大型语言模型（LLMs）在零资源场景下的 hallucination 检测准确性，解决现有方法因故事线变化导致的文本不可比问题。方法包括从原文本中屏蔽多个对象创建填空考试、提示LLMs 反复回答以保持故事线一致，并通过评分答案量化每个句子的 hallucination 程度，同时考虑 hallucination snowballing 效应。实验结果显示，该方法单独使用已优于现有方法，并在与其他方法的集成中实现最先进性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "F.2.2; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17173v1",
      "published_date": "2024-09-20 04:34:30 UTC",
      "updated_date": "2024-09-20 04:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:06:13.985070"
    },
    {
      "arxiv_id": "2409.13208v3",
      "title": "Redefining Data Pairing for Motion Retargeting Leveraging a Human Body Prior",
      "title_zh": "翻译失败",
      "authors": [
        "Xiyana Figuera",
        "Soogeun Park",
        "Hyemin Ahn"
      ],
      "abstract": "We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a\ncost-effective and convenient method to collect high-quality upper body paired\n<robot, human> pose data, which is essential for data-driven motion retargeting\nmethods. Unlike existing approaches which collect <robot, human> pose data by\nconverting human MoCap poses into robot poses, our method goes in reverse. We\nfirst sample diverse random robot poses, and then convert them into human\nposes. However, since random robot poses can result in extreme and infeasible\nhuman poses, we propose an additional technique to sort out extreme poses by\nexploiting a human body prior trained from a large amount of human pose data.\nOur data collection method can be used for any humanoid robots, if one designs\nor optimizes the system's hyperparameters which include a size scale factor and\nthe joint angle ranges for sampling. In addition to this data collection\nmethod, we also present a two-stage motion retargeting neural network that can\nbe trained via supervised learning on a large amount of paired data. Compared\nto other learning-based methods trained via unsupervised learning, we found\nthat our deep neural network trained with ample high-quality paired data\nachieved notable performance. Our experiments also show that our data filtering\nmethod yields better retargeting results than training the model with raw and\nnoisy data. Our code and video results are available on\nhttps://sites.google.com/view/mr-hubo/",
      "tldr_zh": "我们提出了 MR HuBo，一种利用 Human Body Prior 的成本高效方法，用于收集高质量的上肢<robot, human> 姿势配对数据，以支持数据驱动的 Motion Retargeting。不同于传统方法将人类 MoCap 姿势转换为机器人姿势，MR HuBo 先采样随机机器人姿势再转换为人类姿势，并通过从大量人类姿势数据训练的人体先验过滤极端和不可行姿势。基于此，我们开发了一个两阶段的 Motion Retargeting 神经网络，通过监督学习在大量配对数据上训练。实验显示，该方法在 Motion Retargeting 性能上显著优于无监督学习方法，且数据过滤技术进一步提升了重定向结果的准确性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 5 Figures, Accepted at IROS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13208v3",
      "published_date": "2024-09-20 04:32:54 UTC",
      "updated_date": "2024-10-01 06:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:06:28.026665"
    },
    {
      "arxiv_id": "2409.15376v1",
      "title": "ControlMath: Controllable Data Generation Promotes Math Generalist Models",
      "title_zh": "ControlMath：可控数据生成促进数学通用模型",
      "authors": [
        "Nuo Chen",
        "Ning Wu",
        "Jianhui Chang",
        "Jia Li"
      ],
      "abstract": "Utilizing large language models (LLMs) for data augmentation has yielded\nencouraging results in mathematical reasoning. However, these approaches face\nconstraints in problem diversity, potentially restricting them to\nin-domain/distribution data generation. To this end, we propose ControlMath, an\niterative method involving an equation-generator module and two LLM-based\nagents. The module creates diverse equations, which the Problem-Crafter agent\nthen transforms into math word problems. The Reverse-Agent filters and selects\nhigh-quality data, adhering to the \"less is more\" principle, achieving better\nresults with fewer data points. This approach enables the generation of diverse\nmath problems, not limited to specific domains or distributions. As a result,\nwe collect ControlMathQA, which involves 190k math word problems. Extensive\nresults prove that combining our dataset with in-domain datasets like GSM8K can\nhelp improve the model's mathematical ability to generalize, leading to\nimproved performances both within and beyond specific domains.",
      "tldr_zh": "本文提出 ControlMath，一种可控数据生成方法，利用方程生成模块和两个基于 LLM 的代理（Problem-Crafter 和 Reverse-Agent）来创建多样化的数学问题，其中 Reverse-Agent 通过过滤高质数据遵循“less is more”原则，以更少样本实现更好效果。相比传统方法，该框架能突破特定领域或分布的限制，生成不限于单一领域的数学问题，并构建了包含 190k 问题的 ControlMathQA 数据集。实验结果显示，将 ControlMathQA 与 GSM8K 等领域内数据集结合，能显著提升模型的数学泛化能力，在特定领域内外均表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.15376v1",
      "published_date": "2024-09-20 03:58:26 UTC",
      "updated_date": "2024-09-20 03:58:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:06:38.470112"
    },
    {
      "arxiv_id": "2409.13191v2",
      "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical Tasks in Diabetes Care and Management",
      "title_zh": "翻译失败",
      "authors": [
        "Lai Wei",
        "Zhen Ying",
        "Muyang He",
        "Yutong Chen",
        "Qian Yang",
        "Yanzhe Hong",
        "Jiaping Lu",
        "Kaipeng Zheng",
        "Shaoting Zhang",
        "Xiaoying Li",
        "Weiran Huang",
        "Ying Chen"
      ],
      "abstract": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica.",
      "tldr_zh": "该研究针对糖尿病管理引入了Diabetica框架，通过适应Large Language Model (LLMs)来提升多种医疗任务的效能。研究开发了一个全面的数据处理管道，包括数据收集、过滤、增强和精炼，从而创建高质量的糖尿病特定数据集和评估基准。基于此数据集进行fine-tuned后，Diabetica的LLM家族在各种糖尿病任务中表现出state-of-the-art性能，优于其他模型。临床研究进一步证明，该模型可提供个性化医疗、辅助医疗教育并简化临床任务，从而增强临床实践和个性化支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICLR 2025 SCI-FM workshop",
      "pdf_url": "http://arxiv.org/pdf/2409.13191v2",
      "published_date": "2024-09-20 03:47:54 UTC",
      "updated_date": "2025-03-13 13:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:06:49.676753"
    },
    {
      "arxiv_id": "2409.13187v2",
      "title": "Cooperative Resilience in Artificial Intelligence Multiagent Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Manuela Chacon-Chamorro",
        "Luis Felipe Giraldo",
        "Nicanor Quijano",
        "Vicente Vargas-Panesso",
        "César González",
        "Juan Sebastián Pinzón",
        "Rubén Manrique",
        "Manuel Ríos",
        "Yesid Fonseca",
        "Daniel Gómez-Barrera",
        "Mónica Perdomo-Pérez"
      ],
      "abstract": "Resilience refers to the ability of systems to withstand, adapt to, and\nrecover from disruptive events. While studies on resilience have attracted\nsignificant attention across various research domains, the precise definition\nof this concept within the field of cooperative artificial intelligence remains\nunclear. This paper addresses this gap by proposing a clear definition of\n`cooperative resilience' and outlining a methodology for its quantitative\nmeasurement. The methodology is validated in an environment with RL-based and\nLLM-augmented autonomous agents, subjected to environmental changes and the\nintroduction of agents with unsustainable behaviors. These events are\nparameterized to create various scenarios for measuring cooperative resilience.\nThe results highlight the crucial role of resilience metrics in analyzing how\nthe collective system prepares for, resists, recovers from, sustains\nwell-being, and transforms in the face of disruptions. These findings provide\nfoundational insights into the definition, measurement, and preliminary\nanalysis of cooperative resilience, offering significant implications for the\nbroader field of AI. Moreover, the methodology and metrics developed here can\nbe adapted to a wide range of AI applications, enhancing the reliability and\neffectiveness of AI in dynamic and unpredictable environments.",
      "tldr_zh": "该论文定义了“cooperative resilience”在合作人工智能多智能体系统中的概念，即系统抵御、适应和恢复干扰的能力，以填补这一领域的定义空白。研究提出了一种量化测量方法，并在RL-based和LLM-augmented自主代理的环境中进行验证，通过模拟环境变化和引入不可持续行为的代理来创建多种场景。结果强调了弹性指标在系统准备、抵抗、恢复、维持福祉和转变方面的关键作用，并为AI领域提供了基础见解，可扩展应用于动态环境中以提升AI的可靠性和有效性。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Supplementary material in\n  https://github.com/mavivi95/resilience/blob/main/Supplementary_File.pdf",
      "pdf_url": "http://arxiv.org/pdf/2409.13187v2",
      "published_date": "2024-09-20 03:28:48 UTC",
      "updated_date": "2024-09-24 17:13:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:07:01.508048"
    },
    {
      "arxiv_id": "2409.13180v2",
      "title": "FreeAvatar: Robust 3D Facial Animation Transfer by Learning an Expression Foundation Model",
      "title_zh": "翻译失败",
      "authors": [
        "Feng Qiu",
        "Wei Zhang",
        "Chen Liu",
        "Rudong An",
        "Lincheng Li",
        "Yu Ding",
        "Changjie Fan",
        "Zhipeng Hu",
        "Xin Yu"
      ],
      "abstract": "Video-driven 3D facial animation transfer aims to drive avatars to reproduce\nthe expressions of actors. Existing methods have achieved remarkable results by\nconstraining both geometric and perceptual consistency. However, geometric\nconstraints (like those designed on facial landmarks) are insufficient to\ncapture subtle emotions, while expression features trained on classification\ntasks lack fine granularity for complex emotions. To address this, we propose\n\\textbf{FreeAvatar}, a robust facial animation transfer method that relies\nsolely on our learned expression representation. Specifically, FreeAvatar\nconsists of two main components: the expression foundation model and the facial\nanimation transfer model. In the first component, we initially construct a\nfacial feature space through a face reconstruction task and then optimize the\nexpression feature space by exploring the similarities among different\nexpressions. Benefiting from training on the amounts of unlabeled facial images\nand re-collected expression comparison dataset, our model adapts freely and\neffectively to any in-the-wild input facial images. In the facial animation\ntransfer component, we propose a novel Expression-driven Multi-avatar Animator,\nwhich first maps expressive semantics to the facial control parameters of 3D\navatars and then imposes perceptual constraints between the input and output\nimages to maintain expression consistency. To make the entire process\ndifferentiable, we employ a trained neural renderer to translate rig parameters\ninto corresponding images. Furthermore, unlike previous methods that require\nseparate decoders for each avatar, we propose a dynamic identity injection\nmodule that allows for the joint training of multiple avatars within a single\nnetwork.",
      "tldr_zh": "本论文提出FreeAvatar，一种鲁棒的3D面部动画转移方法，通过学习Expression Foundation Model来捕捉细微情绪，避免依赖传统几何约束的局限。具体而言，该方法包括表情基础模型，利用面部重建任务和无标签图像训练优化表情特征空间，以及面部动画转移模型的Expression-driven Multi-avatar Animator，将表情语义映射到3D头像参数并施加感知约束以保持一致性。创新点在于动态身份注入模块，支持多个头像在单一网络中联合训练，提升了鲁棒性和泛化能力。实验表明，该方法能有效适应野外输入图像，实现更精确的情感再现。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "11 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13180v2",
      "published_date": "2024-09-20 03:17:01 UTC",
      "updated_date": "2024-10-09 02:29:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:07:15.920793"
    },
    {
      "arxiv_id": "2409.13783v1",
      "title": "A Value Based Parallel Update MCTS Method for Multi-Agent Cooperative Decision Making of Connected and Automated Vehicles",
      "title_zh": "基于价值的并行更新 MCTS 方法，用于连接和自动车辆的多智能体",
      "authors": [
        "Ye Han",
        "Lijun Zhang",
        "Dejian Meng",
        "Xingyu Hu",
        "Songyu Weng"
      ],
      "abstract": "To solve the problem of lateral and logitudinal joint decision-making of\nmulti-vehicle cooperative driving for connected and automated vehicles (CAVs),\nthis paper proposes a Monte Carlo tree search (MCTS) method with parallel\nupdate for multi-agent Markov game with limited horizon and time discounted\nsetting. By analyzing the parallel actions in the multi-vehicle joint action\nspace in the partial-steady-state traffic flow, the parallel update method can\nquickly exclude potential dangerous actions, thereby increasing the search\ndepth without sacrificing the search breadth. The proposed method is tested in\na large number of randomly generated traffic flow. The experiment results show\nthat the algorithm has good robustness and better performance than the SOTA\nreinforcement learning algorithms and heuristic methods. The vehicle driving\nstrategy using the proposed algorithm shows rationality beyond human drivers,\nand has advantages in traffic efficiency and safety in the coordinating zone.",
      "tldr_zh": "本研究针对联网自动车辆(CAVs)的多车辆合作驾驶问题，提出了一种基于价值的并行更新 Monte Carlo tree search (MCTS) 方法，用于多代理 Markov game 的有限地平线和时间折扣设置。该方法通过分析多车辆联合行动空间中的并行行动，快速排除潜在危险行为，从而提升搜索深度而不牺牲广度。实验结果显示，该算法在大量随机生成的交通流中表现出色，具有良好的鲁棒性，并优于现有最先进强化学习算法和启发式方法；在协调区域，该策略超越人类驾驶员，在交通效率和安全方面具有显著优势。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.MA",
      "comment": "arXiv admin note: text overlap with arXiv:2408.04295 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2409.13783v1",
      "published_date": "2024-09-20 03:13:01 UTC",
      "updated_date": "2024-09-20 03:13:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:07:26.329734"
    },
    {
      "arxiv_id": "2409.13167v1",
      "title": "Unsupervised Attention-Based Multi-Source Domain Adaptation Framework for Drift Compensation in Electronic Nose Systems",
      "title_zh": "无监督注意力机制的多源领域适应框架，用于电子鼻系统的漂移补偿",
      "authors": [
        "Wenwen Zhang",
        "Shuhao Hu",
        "Zhengyuan Zhang",
        "Yuanjin Zheng",
        "Qi Jie Wang",
        "Zhiping Lin"
      ],
      "abstract": "Continuous, long-term monitoring of hazardous, noxious, explosive, and\nflammable gases in industrial environments using electronic nose (E-nose)\nsystems faces the significant challenge of reduced gas identification accuracy\ndue to time-varying drift in gas sensors. To address this issue, we propose a\nnovel unsupervised attention-based multi-source domain shared-private feature\nfusion adaptation (AMDS-PFFA) framework for gas identification with drift\ncompensation in E-nose systems. The AMDS-PFFA model effectively leverages\nlabeled data from multiple source domains collected during the initial stage to\naccurately identify gases in unlabeled gas sensor array drift signals from the\ntarget domain. To validate the model's effectiveness, extensive experimental\nevaluations were conducted using both the University of California, Irvine\n(UCI) standard drift gas dataset, collected over 36 months, and drift signal\ndata from our self-developed E-nose system, spanning 30 months. Compared to\nrecent drift compensation methods, the AMDS-PFFA model achieves the highest\naverage gas recognition accuracy with strong convergence, attaining 83.20% on\nthe UCI dataset and 93.96% on data from our self-developed E-nose system across\nall target domain batches. These results demonstrate the superior performance\nof the AMDS-PFFA model in gas identification with drift compensation,\nsignificantly outperforming existing methods.",
      "tldr_zh": "该研究针对电子鼻(E-nose)系统在长期气体监测中因传感器漂移(drift)导致的识别准确率下降问题，提出了一种无监督注意力-based的多源域适应框架AMDS-PFFA。该框架通过多源域共享-私有特征融合技术，利用初始阶段的标记数据来识别目标域的无标记漂移信号，从而实现有效的漂移补偿。在实验中，使用UCI标准数据集(36个月数据)和自研E-nose系统数据(30个月数据)进行验证，AMDS-PFFA模型分别实现了83.20%和93.96%的平均气体识别准确率，并展现出强劲的收敛性和优于现有方法的性能。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13167v1",
      "published_date": "2024-09-20 02:47:05 UTC",
      "updated_date": "2024-09-20 02:47:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:07:38.138854"
    },
    {
      "arxiv_id": "2409.13166v1",
      "title": "Morphology and Behavior Co-Optimization of Modular Satellites for Attitude Control",
      "title_zh": "模块化卫星的",
      "authors": [
        "Yuxing Wang",
        "Jie Li",
        "Cong Yu",
        "Xinyang Li",
        "Simeng Huang",
        "Yongzhe Chang",
        "Xueqian Wang",
        "Bin Liang"
      ],
      "abstract": "The emergence of modular satellites marks a significant transformation in\nspacecraft engineering, introducing a new paradigm of flexibility, resilience,\nand scalability in space exploration endeavors. In addressing complex\nchallenges such as attitude control, both the satellite's morphological\narchitecture and the controller are crucial for optimizing performance. Despite\nsubstantial research on optimal control, there remains a significant gap in\ndeveloping optimized and practical assembly strategies for modular satellites\ntailored to specific mission constraints. This research gap primarily arises\nfrom the inherently complex nature of co-optimizing design and control, a\nprocess known for its notorious bi-level optimization loop. Conventionally\ntackled through artificial evolution, this issue involves optimizing the\nmorphology based on the fitness of individual controllers, which is\nsample-inefficient and computationally expensive. In this paper, we introduce a\nnovel gradient-based approach to simultaneously optimize both morphology and\ncontrol for modular satellites, enhancing their performance and efficiency in\nattitude control missions. Our Monte Carlo simulations demonstrate that this\nco-optimization approach results in modular satellites with better mission\nperformance compared to those designed by evolution-based approaches.\nFurthermore, this study discusses potential avenues for future research.",
      "tldr_zh": "这篇论文针对模块化卫星的姿态控制问题，提出了一种新型的梯度-based 方法来同时优化卫星的形态结构和控制行为，从而提升性能和效率，解决传统进化-based 优化在计算开销和样本效率上的不足。通过 Monte Carlo simulations，实验结果表明，该方法使卫星在姿态控制任务中比进化方法表现出更好的使命性能。该研究还讨论了未来研究的潜在方向。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "The paper was accepted as an oral presentation by the 75th\n  International Astronautical Congress, Milan, Italy",
      "pdf_url": "http://arxiv.org/pdf/2409.13166v1",
      "published_date": "2024-09-20 02:43:53 UTC",
      "updated_date": "2024-09-20 02:43:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:07:49.554727"
    },
    {
      "arxiv_id": "2409.15375v1",
      "title": "DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention",
      "title_zh": "DS2TA：带有衰减时空注意力的去噪尖峰Transformer",
      "authors": [
        "Boxun Xu",
        "Hejia Geng",
        "Yuxuan Yin",
        "Peng Li"
      ],
      "abstract": "Vision Transformers (ViT) are current high-performance models of choice for\nvarious vision applications. Recent developments have given rise to\nbiologically inspired spiking transformers that thrive in ultra-low power\noperations on neuromorphic hardware, however, without fully unlocking the\npotential of spiking neural networks. We introduce DS2TA, a Denoising Spiking\ntransformer with attenuated SpatioTemporal Attention, designed specifically for\nvision applications. DS2TA introduces a new spiking attenuated spatiotemporal\nattention mechanism that considers input firing correlations occurring in both\ntime and space, thereby fully harnessing the computational power of spiking\nneurons at the core of the transformer architecture. Importantly, DS2TA\nfacilitates parameter-efficient spatiotemporal attention computation without\nintroducing extra weights. DS2TA employs efficient hashmap-based nonlinear\nspiking attention denoisers to enhance the robustness and expressive power of\nspiking attention maps. DS2TA demonstrates state-of-the-art performances on\nseveral widely adopted static image and dynamic neuromorphic datasets. Operated\nover 4 time steps, DS2TA achieves 94.92% top-1 accuracy on CIFAR10 and 77.47%\ntop-1 accuracy on CIFAR100, as well as 79.1% and 94.44% on CIFAR10-DVS and\nDVS-Gesture using 10 time steps.",
      "tldr_zh": "本研究引入了 DS2TA，一种去噪尖峰 Transformer（Denoising Spiking Transformer），通过衰减时空注意机制（attenuated spatiotemporal attention）提升了视觉应用的性能，特别是针对生物启发型尖峰神经网络。DS2TA 创新性地考虑了输入在时间和空间上的 firing 相关性，并使用高效的 hashmap-based nonlinear spiking attention denoisers 来增强注意图的鲁棒性和表达能力，而无需额外权重。实验结果显示，该模型在多个数据集上达到 state-of-the-art 水平，包括在 4 时间步上实现 CIFAR10 的 94.92% top-1 准确率和 CIFAR100 的 77.47% top-1 准确率，以及在 10 时间步上分别在 CIFAR10-DVS 和 DVS-Gesture 上达到 79.1% 和 94.44% 的准确率。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "arXiv admin note: text overlap with arXiv:2311.09376",
      "pdf_url": "http://arxiv.org/pdf/2409.15375v1",
      "published_date": "2024-09-20 02:26:04 UTC",
      "updated_date": "2024-09-20 02:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:08:01.960890"
    },
    {
      "arxiv_id": "2409.13153v2",
      "title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
      "title_zh": "迈向高效的神经符号 AI：从工作负载表征到硬件架构",
      "authors": [
        "Zishen Wan",
        "Che-Kai Liu",
        "Hanchen Yang",
        "Ritik Raj",
        "Chaojian Li",
        "Haoran You",
        "Yonggan Fu",
        "Cheng Wan",
        "Sixu Li",
        "Youbin Kim",
        "Ananda Samajdar",
        "Yingyan Celine Lin",
        "Mohamed Ibrahim",
        "Jan M. Rabaey",
        "Tushar Krishna",
        "Arijit Raychowdhury"
      ],
      "abstract": "The remarkable advancements in artificial intelligence (AI), primarily driven\nby deep neural networks, are facing challenges surrounding unsustainable\ncomputational trajectories, limited robustness, and a lack of explainability.\nTo develop next-generation cognitive AI systems, neuro-symbolic AI emerges as a\npromising paradigm, fusing neural and symbolic approaches to enhance\ninterpretability, robustness, and trustworthiness, while facilitating learning\nfrom much less data. Recent neuro-symbolic systems have demonstrated great\npotential in collaborative human-AI scenarios with reasoning and cognitive\ncapabilities. In this paper, we aim to understand the workload characteristics\nand potential architectures for neuro-symbolic AI. We first systematically\ncategorize neuro-symbolic AI algorithms, and then experimentally evaluate and\nanalyze them in terms of runtime, memory, computational operators, sparsity,\nand system characteristics on CPUs, GPUs, and edge SoCs. Our studies reveal\nthat neuro-symbolic models suffer from inefficiencies on off-the-shelf\nhardware, due to the memory-bound nature of vector-symbolic and logical\noperations, complex flow control, data dependencies, sparsity variations, and\nlimited scalability. Based on profiling insights, we suggest cross-layer\noptimization solutions and present a hardware acceleration case study for\nvector-symbolic architecture to improve the performance, efficiency, and\nscalability of neuro-symbolic computing. Finally, we discuss the challenges and\npotential future directions of neuro-symbolic AI from both system and\narchitectural perspectives.",
      "tldr_zh": "该论文探讨了神经符号 AI（neuro-symbolic AI）作为提升 AI 可解释性、鲁棒性和数据效率的潜在范式，以应对传统深度神经网络的计算资源问题。研究者首先系统分类了神经符号 AI 算法，并通过实验评估其在 CPU、GPU 和边际 SoC 上的工作负载特性，包括运行时、内存使用、计算操作和稀疏性。结果显示，这些模型在现有硬件上效率低下，主要由于内存绑定操作、复杂控制流和数据依赖性。论文基于这些洞见提出跨层优化方案，并展示了一个针对 vector-symbolic architecture 的硬件加速案例，同时讨论了系统和架构层面的挑战及未来方向。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "14 pages, 11 figures, 7 tables; IEEE Transactions on Circuits and\n  Systems for Artificial Intelligence (TCASAI), 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13153v2",
      "published_date": "2024-09-20 01:32:14 UTC",
      "updated_date": "2024-09-23 01:30:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:08:15.967045"
    },
    {
      "arxiv_id": "2409.13147v1",
      "title": "The Impact of Feature Embedding Placement in the Ansatz of a Quantum Kernel in QSVMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ilmo Salmenperä",
        "Ilmars Kuhtarskis",
        "Arianne Meijer van de Griend",
        "Jukka K. Nurminen"
      ],
      "abstract": "Designing a useful feature map for a quantum kernel is a critical task when\nattempting to achieve an advantage over classical machine learning models. The\nchoice of circuit architecture, i.e. how feature-dependent gates should be\ninterwoven with other gates is a relatively unexplored problem and becomes very\nimportant when using a model of quantum kernels called Quantum Embedding\nKernels (QEK). We study and categorize various architectural patterns in QEKs\nand show that existing architectural styles do not behave as the literature\nsupposes. We also produce a novel alternative architecture based on the old\nones and show that it performs equally well while containing fewer gates than\nits older counterparts.",
      "tldr_zh": "该论文探讨了在量子支持向量机 (QSVMs) 中，特征嵌入位置对量子内核 (Quantum Kernel) 设计的影响，强调了选择适当电路架构的重要性，以实现超越经典机器学习模型的性能优势。研究者对 Quantum Embedding Kernels (QEK) 中的各种架构模式进行了分类和分析，发现现有架构的表现与文献预期不符。论文提出了一种新型架构，该架构基于传统模式改进而成，在性能方面与旧架构相当，但使用了更少的门 (gates)，从而提高了效率。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages including references and appendix, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13147v1",
      "published_date": "2024-09-20 01:25:13 UTC",
      "updated_date": "2024-09-20 01:25:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:08:25.394662"
    },
    {
      "arxiv_id": "2409.13138v2",
      "title": "Learning to Compare Hardware Designs for High-Level Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Yunsheng Bai",
        "Atefeh Sohrabizadeh",
        "Zijian Ding",
        "Rongjian Liang",
        "Weikai Li",
        "Ding Wang",
        "Haoxing Ren",
        "Yizhou Sun",
        "Jason Cong"
      ],
      "abstract": "High-level synthesis (HLS) is an automated design process that transforms\nhigh-level code into hardware designs, enabling the rapid development of\nhardware accelerators. HLS relies on pragmas, which are directives inserted\ninto the source code to guide the synthesis process, and pragmas have various\nsettings and values that significantly impact the resulting hardware design.\nState-of-the-art ML-based HLS methods, such as HARP, first train a deep\nlearning model, typically based on graph neural networks (GNNs) applied to\ngraph-based representations of the source code and pragmas. They then perform\ndesign space exploration (DSE) to explore the pragma design space, rank\ncandidate designs using the model, and return the top designs. However,\ntraditional DSE methods face challenges due to the highly nonlinear\nrelationship between pragma settings and performance metrics, along with\ncomplex interactions between pragmas that affect performance in non-obvious\nways.\n  To address these challenges, we propose compareXplore, a novel approach that\nlearns to compare hardware designs for effective HLS optimization.\nCompareXplore introduces a hybrid loss function that combines pairwise\npreference learning with pointwise performance prediction, enabling the model\nto capture both relative preferences and absolute performance. Moreover, we\nintroduce a novel node difference attention module that focuses on the most\ninformative differences between designs, enabling the model to identify\ncritical pragmas impacting performance. CompareXplore adopts a two-stage DSE,\nwhere a pointwise prediction model is used for the initial design pruning,\nfollowed by a pairwise comparison stage for precise performance verification.\nIn extensive experiments, compareXplore achieves significant improvements in\nranking metrics and generates high-quality HLS results for the selected\ndesigns, outperforming the existing SOTA method.",
      "tldr_zh": "这篇论文针对 High-Level Synthesis (HLS) 的设计空间探索 (DSE) 问题，提出了一种新型方法 compareXplore，以解决 pragmas 设置与性能之间的高度非线性关系和复杂交互。compareXplore 采用混合损失函数结合成对偏好学习和点式性能预测，并引入节点差异注意力模块来识别影响性能的关键 pragmas，从而更有效地比较硬件设计。方法包括两阶段 DSE：先用点式预测模型初步筛选设计，再进行成对比较以验证性能。在实验中，compareXplore 在排名指标上显著优于现有 SOTA 方法，如 HARP，并生成高质量的 HLS 结果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in MLCAD 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13138v2",
      "published_date": "2024-09-20 00:47:29 UTC",
      "updated_date": "2025-05-07 19:58:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:08:37.966181"
    },
    {
      "arxiv_id": "2409.13137v1",
      "title": "Interpret the Predictions of Deep Networks via Re-Label Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Yingying Hua",
        "Shiming Ge",
        "Daichi Zhang"
      ],
      "abstract": "Interpreting the predictions of a black-box deep network can facilitate the\nreliability of its deployment. In this work, we propose a re-label distillation\napproach to learn a direct map from the input to the prediction in a\nself-supervision manner. The image is projected into a VAE subspace to generate\nsome synthetic images by randomly perturbing its latent vector. Then, these\nsynthetic images can be annotated into one of two classes by identifying\nwhether their labels shift. After that, using the labels annotated by the deep\nnetwork as teacher, a linear student model is trained to approximate the\nannotations by mapping these synthetic images to the classes. In this manner,\nthese re-labeled synthetic images can well describe the local classification\nmechanism of the deep network, and the learned student can provide a more\nintuitive explanation towards the predictions. Extensive experiments verify the\neffectiveness of our approach qualitatively and quantitatively.",
      "tldr_zh": "本研究提出了一种 re-label distillation 方法，用于解释黑盒 deep networks 的预测，从而提升其部署可靠性。该方法通过将图像投影到 VAE 子空间，并通过随机扰动潜在向量生成合成图像，然后根据标签是否变化重新标注这些图像为两个类别之一。利用 deep networks 的预测作为教师模型，训练一个线性学生模型来映射合成图像到类别，从而揭示网络的局部分类机制。实验结果在定性和定量上证明了该方法的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Published by IEEE ICME 2021",
      "pdf_url": "http://arxiv.org/pdf/2409.13137v1",
      "published_date": "2024-09-20 00:46:22 UTC",
      "updated_date": "2024-09-20 00:46:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T02:08:49.246718"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 111,
  "processed_papers_count": 111,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T02:09:12.684670"
}
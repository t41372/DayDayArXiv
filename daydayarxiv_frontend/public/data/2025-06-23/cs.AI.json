{
  "date": "2025-06-23",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-23 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ã€‚**Reasoning Models (æ¨ç†æ¨¡å‹)** ä¾æ—§æ˜¯ç»å¯¹çš„ä¸»è§’ï¼Œä»**DeepSeek-R1** å¼•å‘çš„åç»­ç ”ç©¶äº•å–·ï¼šä¸ä»…æœ‰æ¸…åå›¢é˜Ÿæå‡ºçš„**LongWriter-Zero**ï¼ˆçº¯ RL è®­ç»ƒè¶…é•¿æ–‡æœ¬ç”Ÿæˆï¼‰ï¼Œè¿˜æœ‰é’ˆå¯¹æ¨ç†æ¨¡å‹â€œåºŸè¯å¤šâ€çš„ä¼˜åŒ–ï¼ˆ**ConciseHint**ï¼‰ä»¥åŠæ— éœ€éªŒè¯å™¨çš„ RL æ‰©å±•ï¼ˆ**RLPR**ï¼‰ã€‚æ­¤å¤–ï¼Œ**å¤šæ¨¡æ€æ¨ç†**å‡ºç°äº†ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ Benchmarkâ€”â€”**â€œæŠ“å‡ºè½¨â€æ£€æµ‹ (CaughtCheating)**ï¼Œæµ‹è¯• AI èƒ½å¦åƒäººç±»ä¾¦æ¢ä¸€æ ·å‘ç°ç…§ç‰‡ä¸­çš„è››ä¸é©¬è¿¹ã€‚ç³»ç»Ÿå±‚é¢ï¼Œ**KV Cache å‹ç¼©**æŠ€æœ¯è®© RTX 4090 è·‘ 128K ä¸Šä¸‹æ–‡æˆä¸ºå¯èƒ½ã€‚\n\n---\n\n### ğŸš€ æ ¸å¿ƒæ¨èï¼šæ¨ç†æ¨¡å‹ã€RL ä¸ç”Ÿæˆå¼ AI çš„æ–°èŒƒå¼\n\n**1. LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning**\n**LongWriter-Zero: é€šè¿‡å¼ºåŒ–å­¦ä¹ æŒæ¡è¶…é•¿æ–‡æœ¬ç”Ÿæˆ**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¸…å KEG å®éªŒå®¤æ–°ä½œã€‚é’ˆå¯¹ LLM ç”Ÿæˆè¶…é•¿æ–‡æœ¬ï¼ˆå¦‚ä¸‡å­—é•¿æ–‡ï¼‰ä¾èµ–æ˜‚è´µä¸”è´¨é‡å‚å·®ä¸é½çš„ SFT æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§**çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**çš„æ–¹æ³•ã€‚\n*   **æ–¹æ³•**ï¼šç±»ä¼¼äº AlphaZero æˆ– R1-Zeroï¼Œä¸ä¾èµ–ä»»ä½•äººç±»æ ‡æ³¨æˆ–åˆæˆ SFT æ•°æ®ï¼Œä»é›¶å¼€å§‹é€šè¿‡ RL æ¿€åŠ±æ¨¡å‹åœ¨å†™ä½œè¿‡ç¨‹ä¸­è¿›è¡Œâ€œè§„åˆ’â€å’Œâ€œåæ€â€ã€‚\n*   **æ•ˆæœ**ï¼šåŸºäº Qwen2.5-32B è®­ç»ƒçš„æ¨¡å‹åœ¨é•¿æ–‡å†™ä½œä»»åŠ¡ä¸Šè¶…è¶Šäº† DeepSeek-R1 å’Œ Qwen3-235B ç­‰åƒäº¿çº§æ¨¡å‹ã€‚\n*   **Implication**ï¼šè¯æ˜äº†åœ¨é•¿æ–‡æœ¬ç”Ÿæˆé¢†åŸŸï¼Œç²¾å¿ƒè®¾è®¡çš„ RL å¯ä»¥å®Œå…¨æ›¿ä»£ä¼ ç»Ÿçš„ SFTâ€œæ•™å­¦â€æ¨¡å¼ã€‚\n\n**2. ReDit: Reward Dithering for Improved LLM Policy Optimization**\n**ReDit: ç”¨äºæ”¹è¿› LLM ç­–ç•¥ä¼˜åŒ–çš„å¥–åŠ±æŠ–åŠ¨æŠ€æœ¯**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ DeepSeek-R1 ç­‰æ¨¡å‹ä½¿ç”¨çš„è§„åˆ™å¥–åŠ±ï¼ˆRule-based Rewardï¼‰é€šå¸¸æ˜¯ç¦»æ•£çš„ï¼ˆDiscreteï¼‰ï¼Œå¯¼è‡´æ¢¯åº¦å¼‚å¸¸å’Œä¼˜åŒ–ä¸ç¨³å®šçš„é—®é¢˜ã€‚\n*   **æ–¹æ³•**ï¼šå¼•å…¥**Reward Dithering (ReDit)**ï¼Œé€šè¿‡å‘ç¦»æ•£å¥–åŠ±æ·»åŠ éšæœºå™ªå£°ï¼Œä½¿æ¢ç´¢æ¢¯åº¦æ›´åŠ å¹³æ»‘ã€‚\n*   **å‘ç°**ï¼šè¿™ç§ç®€å•çš„â€œæŠ–åŠ¨â€ä¸ä»…åŠ é€Ÿäº†æ”¶æ•›ï¼Œè¿˜èƒ½å¸®åŠ©æ¨¡å‹é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚æ•ˆç‡æƒŠäººï¼šä»…éœ€ Vanilla GRPO 10% çš„è®­ç»ƒæ­¥æ•°å³å¯è¾¾åˆ°åŒç­‰æ€§èƒ½ã€‚\n\n**3. ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation**\n**ConciseHint: åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€šè¿‡è¿ç»­ç®€æ´æç¤ºæå‡é«˜æ•ˆæ¨ç†**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šDeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹è™½ç„¶å¼ºï¼Œä½†å¾€å¾€ä¼šæœ‰â€œè¿‡åº¦å†—é•¿â€çš„æ¨ç†è¿‡ç¨‹ï¼ˆVerbose reasoningï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šæå‡º ConciseHintï¼Œåœ¨æ¨ç†ç”Ÿæˆè¿‡ç¨‹ä¸­æ³¨å…¥å¯å­¦ä¹ çš„â€œæç¤ºâ€ï¼ˆHintsï¼‰ï¼Œé¼“åŠ±æ¨¡å‹â€œè¯´äººè¯â€ï¼Œæ›´ç®€æ´åœ°è¡¨è¾¾ã€‚\n*   **æ•ˆæœ**ï¼šæœ‰æ•ˆå‡å°‘äº†æ¨ç†é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ï¼Œè§£å†³äº†æ¨ç†æ¨¡å‹â€œå¤ªå•°å—¦â€çš„æ•ˆç‡é—®é¢˜ã€‚\n\n**4. RLPR: Extrapolating RLVR to General Domains without Verifiers**\n**RLPR: æ— éœ€éªŒè¯å™¨å°† RLVR æ‰©å±•åˆ°é€šç”¨é¢†åŸŸ**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šRLVRï¼ˆå¸¦å¯éªŒè¯å¥–åŠ±çš„ RLï¼‰é€šå¸¸å±€é™äºæ•°å­¦å’Œä»£ç é¢†åŸŸï¼Œå› ä¸ºè¿™äº›é¢†åŸŸæœ‰æ ‡å‡†ç­”æ¡ˆã€‚æœ¬æ–‡æ‰“ç ´äº†è¿™ä¸€é™åˆ¶ã€‚\n*   **æ–¹æ³•**ï¼šåˆ©ç”¨ LLM å¯¹å‚è€ƒç­”æ¡ˆçš„**å†…åœ¨ç”Ÿæˆæ¦‚ç‡ (Intrinsic Probability)** ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œæ— éœ€å¤–éƒ¨ç‰¹å®šçš„éªŒè¯å™¨ï¼ˆVerifierï¼‰ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ 7 ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¾èµ–éªŒè¯å™¨çš„å¼ºåŸºçº¿ï¼Œè®© RL å¢å¼ºæ¨ç†èƒ½åŠ›å¯ä»¥åº”ç”¨åˆ°æ›´å¹¿æ³›çš„é€šç”¨æ–‡æœ¬é¢†åŸŸã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ã€è§†è§‰ä¸â€œä¾¦æ¢â€èƒ½åŠ›\n\n**5. CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning**\n**CaughtCheating: ä½ çš„å¤šæ¨¡æ€å¤§æ¨¡å‹æ˜¯ä¸ªå¥½ä¾¦æ¢å—ï¼Ÿæ¢ç´¢è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†çš„è¾¹ç•Œ**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™å°±å¾ˆæœ‰æ„æ€äº†ã€‚ä½œè€…å‘ç° GPT-o3 è™½ç„¶å¼ºï¼Œä½†åœ¨**â€œæŠ“å‡ºè½¨â€**ï¼ˆä»ä¼´ä¾£å‘çš„ç…§ç‰‡ä¸­å‘ç°å¯ç–‘çº¿ç´¢ï¼‰è¿™ç§éœ€è¦æå¼ºè§‚å¯ŸåŠ›å’Œå¸¸è¯†æ¨ç†çš„ä»»åŠ¡ä¸Šï¼Œè¡¨ç°å‡ ä¹ä¸ºé›¶ã€‚\n*   **Benchmark**ï¼šæå‡ºäº† CaughtCheating æ•°æ®é›†ï¼Œçµæ„Ÿæ¥è‡ªç¤¾äº¤åª’ä½“ä¸Šçš„çœŸå®æ±‚åŠ©å¸–ã€‚\n*   **å‘ç°**ï¼šç°æœ‰çš„ MLLM ç¼ºä¹åƒäººç±»ä¾¦æ¢é‚£æ ·å°†å¾®å°è§†è§‰çº¿ç´¢ç¼–ç»‡æˆè¿è´¯æƒ…å¢ƒè§£é‡Šçš„èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨ç†æ–°æ–¹å‘ã€‚\n\n**6. OmniGen2: Exploration to Advanced Multimodal Generation**\n**OmniGen2: å‘é«˜çº§å¤šæ¨¡æ€ç”Ÿæˆçš„æ¢ç´¢**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº† **OmniGen2**ï¼Œä¸€ä¸ªå¤šæ‰å¤šè‰ºçš„å¼€æºç”Ÿæˆæ¨¡å‹ã€‚\n*   **æ”¹è¿›**ï¼šä¸ v1 ä¸åŒï¼Œv2 é‡‡ç”¨äº†**åŒè§£ç è·¯å¾„**ï¼ˆæ–‡æœ¬å’Œå›¾åƒåˆ†å¼€ï¼‰ï¼Œä½¿ç”¨äº†è§£è€¦çš„å›¾åƒ Tokenizerã€‚\n*   **èƒ½åŠ›**ï¼šæ”¯æŒæ–‡ç”Ÿå›¾ã€å›¾åƒç¼–è¾‘ã€ä¸Šä¸‹æ–‡ç”Ÿæˆï¼ˆSubject-drivenï¼‰ï¼Œåœ¨ä¸€è‡´æ€§ä¸Šè¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„ SOTAã€‚\n\n**7. Matrix-Game: Interactive World Foundation Model**\n**Matrix-Game: äº¤äº’å¼ä¸–ç•ŒåŸºç¡€æ¨¡å‹**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šSkyworkAI å‘å¸ƒçš„ **Minecraftï¼ˆæˆ‘çš„ä¸–ç•Œï¼‰** ä¸–ç•Œæ¨¡å‹ã€‚\n*   **è§„æ¨¡**ï¼š17B å‚æ•°ï¼ŒåŸºäº 2700 å°æ—¶è§†é¢‘é¢„è®­ç»ƒ + 1000 å°æ—¶ç²¾ç»†åŠ¨ä½œæ ‡æ³¨æ•°æ®å¾®è°ƒã€‚\n*   **èƒ½åŠ›**ï¼šå®ç°äº†å¯æ§çš„â€œç”Ÿæˆå¼æ¸¸æˆä½“éªŒâ€ï¼Œæ ¹æ®æŒ‡ä»¤ç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆç‰©ç†è§„å¾‹çš„æ¸¸æˆè§†é¢‘æµã€‚\n\n---\n\n### âš¡ ç³»ç»Ÿä¼˜åŒ–ä¸ç¡¬ä»¶æ•ˆç‡\n\n**8. CommVQ: Commutative Vector Quantization for KV Cache Compression**\n**CommVQ: ç”¨äº KV Cache å‹ç¼©çš„äº¤æ¢çŸ¢é‡é‡åŒ–**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³äº†é•¿ä¸Šä¸‹æ–‡æ¨ç†æ˜¾å­˜çˆ†ç‚¸çš„é—®é¢˜ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§ä¸ RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰**å¯äº¤æ¢**çš„é‡åŒ–æ–¹æ³•ã€‚\n*   **éœ‡æ’¼æ•ˆæœ**ï¼šå®ç°äº† **2-bit** ç”šè‡³ **1-bit** çš„ KV Cache é‡åŒ–ã€‚**è®© LLaMA-3.1 8B æ¨¡å‹åœ¨å•å¼  RTX 4090 ä¸Šå°±èƒ½è·‘ 128K çš„è¶…é•¿ä¸Šä¸‹æ–‡**ï¼Œä¸”ç²¾åº¦æŸå¤±æå°ã€‚è¿™å¯¹æ¶ˆè´¹çº§æ˜¾å¡ç”¨æˆ·æ˜¯å·¨å¤§åˆ©å¥½ã€‚\n\n**9. Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models**\n**Morse: æ‰©æ•£æ¨¡å‹çš„åŒé‡‡æ ·æ— æŸåŠ é€Ÿ**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§æ— æŸåŠ é€Ÿæ‰©æ•£æ¨¡å‹é‡‡æ ·çš„æ–¹æ³•ã€‚\n*   **æ–¹æ³•**ï¼šDash æ¨¡å‹è´Ÿè´£â€œè·³è·ƒé‡‡æ ·â€ï¼ŒDot æ¨¡å‹è´Ÿè´£â€œå¡«è¡¥ç©ºç¼ºâ€æä¾›æ®‹å·®åé¦ˆã€‚\n*   **æ•ˆæœ**ï¼šå®ç° 1.78x åˆ° 3.31x çš„åŠ é€Ÿï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒåºå¤§çš„åŸºç¡€æ¨¡å‹ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ä¸å¯è§£é‡Šæ€§\n\n**10. Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks**\n**DeepSeek ä¸ GPT ç³»åˆ—æ¨¡å‹çš„è¶Šç‹±æ”»å‡»å®‰å…¨æ€§è¯„ä¼°**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç¬¬ä¸€ä»½é’ˆå¯¹ **DeepSeek** ç³»åˆ—æ¨¡å‹å®‰å…¨æ€§çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚\n*   **å‘ç°**ï¼šDeepSeek çš„ MoE æ¶æ„è™½ç„¶å¯¹æŸäº›ä¼˜åŒ–æ”»å‡»ï¼ˆå¦‚ TAP-Tï¼‰æœ‰ä¸€å®šé²æ£’æ€§ï¼ˆå¾—ç›Šäºè·¯ç”±ç¨€ç–æ€§ï¼‰ï¼Œä½†åœ¨**æç¤ºè¯æ”»å‡»**å’Œ**æ‰‹åŠ¨å·¥ç¨‹æ”»å‡»**ä¸‹æ¯” GPT-4 è„†å¼±ã€‚\n*   **åŸå› **ï¼šMoE å¯èƒ½å°†æ¶æ„ Prompt è·¯ç”±åˆ°äº†å¯¹é½ç¨‹åº¦è¾ƒä½çš„ä¸“å®¶æ¨¡å—ä¸­ã€‚\n\n**11. How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models**\n**å¾®è°ƒåçš„æ¨¡å‹ç¼–è¾‘æœ‰å¤šé²æ£’ï¼Ÿæ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹çš„å®è¯ç ”ç©¶**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå›ç­”äº†ä¸€ä¸ªå…³é”®é—®é¢˜â€”â€”å¦‚æœæˆ‘ä»¬â€œä¿®å¤â€äº†æ¨¡å‹ï¼ˆæ¯”å¦‚å»é™¤äº†åè§ï¼‰ï¼Œç„¶åå†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¸ªä¿®å¤è¿˜åœ¨å—ï¼Ÿ\n*   **ç»“è®º**ï¼š**ä¸åœ¨äº†**ã€‚å¾®è°ƒï¼ˆå³ä½¿æ˜¯ LoRA æˆ– DoRAï¼‰é€šå¸¸ä¼šæŠ¹é™¤ä¹‹å‰çš„æ¨¡å‹ç¼–è¾‘æ•ˆæœã€‚è¿™æ„å‘³ç€ AI å®‰å…¨è¡¥ä¸å¾ˆå®¹æ˜“è¢«åç»­å¾®è°ƒç ´åã€‚\n\n**12. Thought Anchors: Which LLM Reasoning Steps Matter?**\n**æ€ç»´é”šç‚¹ï¼šLLM æ¨ç†æ­¥éª¤ä¸­å“ªäº›æœ€é‡è¦ï¼Ÿ**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸ä»…çœ‹ CoT çš„ç»“æœï¼Œè¿˜çœ‹è¿‡ç¨‹ã€‚\n*   **å‘ç°**ï¼šå®šä¹‰äº†**â€œæ€ç»´é”šç‚¹â€ (Thought Anchors)**ï¼Œå³æ¨ç†é“¾ä¸­å¯¹æœ€ç»ˆç­”æ¡ˆåˆ†å¸ƒå½±å“å·¨å¤§çš„ç‰¹å®šå¥å­ã€‚è¿™äº›å¥å­é€šå¸¸æ¶‰åŠâ€œè§„åˆ’â€æˆ–â€œä¸ç¡®å®šæ€§ç®¡ç†â€ã€‚\n*   **å·¥å…·**ï¼šå¼€æºäº†å¯è§†åŒ–å·¥å…· `thought-anchors.com`ã€‚\n\n---\n\n### ğŸ§ª ç§‘å­¦ä¸ç†è®º\n\n**13. Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition**\n**è´å¶æ–¯è¿›åŒ–ç¾¤æ¶æ„ï¼šåŸºäºçœŸç†ç«äº‰çš„å½¢å¼åŒ–è®¤çŸ¥ç³»ç»Ÿ**\n*   **ä½œè€…**ï¼š**Craig Steven Wright** (æ³¨ï¼šæ­¤äººç»å¸¸å·å…¥ Satoshi Nakamoto èº«ä»½äº‰è®®ï¼Œéœ€å¸¦ç€æ‰¹åˆ¤æ€§çœ¼å…‰é˜…è¯»)ã€‚\n*   **å†…å®¹**ï¼šæå‡ºäº†ä¸€å¥—åŸºäºè´å¶æ–¯æ¨ç†å’Œç¾¤ä½“åŠ¨åŠ›å­¦çš„ AI ç³»ç»Ÿï¼Œå¼ºè°ƒâ€œçœŸç†â€ä½œä¸ºè¿›åŒ–çš„å¸å¼•å­ã€‚æ–‡ç« é•¿è¾¾ 83 é¡µï¼ŒåŒ…å« 92 ä¸ªå½¢å¼åŒ–ç»“æœã€‚ç†è®ºæ€§æå¼ºï¼Œè¯•å›¾æ„å»ºä¸€ç§èƒ½è‡ªæˆ‘è°ƒèŠ‚å¹¶äº§ç”Ÿå¯éªŒè¯çŸ¥è¯†çš„â€œç¾¤â€æ™ºèƒ½ã€‚\n\n**14. Finding Clustering Algorithms in the Transformer Architecture**\n**åœ¨ Transformer æ¶æ„ä¸­å‘ç°èšç±»ç®—æ³•**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç†è®ºè¯æ˜äº† Transformer å¯ä»¥**ç²¾ç¡®æ‰§è¡Œ** k-means èšç±»ç®—æ³•ï¼ˆLloyd ç®—æ³•ï¼‰ã€‚\n*   **æ„ä¹‰**ï¼šä¸ä»…æ˜¯ç†è®ºè¯æ˜ï¼Œè¿˜æ„å»ºäº† `k-means transformer`ã€‚è¿™ä¸ºç†è§£ Transformer å†…éƒ¨æœºåˆ¶æä¾›äº†æ¸…æ™°çš„ç®—æ³•è§†è§’â€”â€”Attention å’Œæ®‹å·®è¿æ¥æœ¬è´¨ä¸Šå¯ä»¥çœ‹ä½œæ˜¯åœ¨åšèšç±»è¿­ä»£ã€‚\n\n**15. Sensing Cardiac Health Across Scenarios and Devices**\n**è·¨åœºæ™¯ä¸è®¾å¤‡æ„ŸçŸ¥å¿ƒè„å¥åº·ï¼šåŸºäº 170 ä¸‡äººå¼‚æ„æ•°æ®çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº†ä¸€ä¸ªå¿ƒè„ä¼ æ„ŸåŸºç¡€æ¨¡å‹ (**CSFM**)ã€‚\n*   **æ•°æ®**ï¼šä½¿ç”¨äº† 170 ä¸‡äººçš„ ECG å’Œ PPG æ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚\n*   **æ•ˆæœ**ï¼šåœ¨å¤šé¡¹å¿ƒè„è¯Šæ–­ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†åŒ»ç–—é¢†åŸŸ Foundation Model çš„å¼ºå¤§æ½œåŠ›ã€‚\n\n---\n**ç»“è¯­**ï¼š\nä»Šå¤©çš„è®ºæ–‡æ˜æ˜¾æ„Ÿè§‰åˆ°å­¦ç•Œæ­£åœ¨æ¶ˆåŒ– DeepSeek-R1 å¸¦æ¥çš„å†²å‡»ï¼Œå¤§é‡çš„ç ”ç©¶é›†ä¸­åœ¨å¦‚ä½•è®© Reasoning æ›´é«˜æ•ˆã€æ›´å¯æ§ã€æ›´å¹¿æ³›åœ°åº”ç”¨ã€‚åŒæ—¶ï¼Œå¤šæ¨¡æ€é¢†åŸŸæ­£åœ¨ä»å•çº¯çš„â€œè¯†åˆ«â€èµ°å‘æ·±åº¦çš„â€œæ„ŸçŸ¥ä¸æ¨ç†â€ï¼ˆå¦‚æŠ“å‡ºè½¨é‚£ä¸ªä¾‹å­ï¼‰ã€‚\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼Œæ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2506.19191v1",
      "title": "Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition",
      "title_zh": "è´å¶æ–¯æ¼”åŒ–ç¾¤é›†æ¶æ„ï¼šä¸€ç§åŸºäºçœŸç†ç«äº‰çš„å½¢å¼åŒ–è®¤è¯†è®ºç³»ç»Ÿ",
      "authors": [
        "Craig Steven Wright"
      ],
      "abstract": "We introduce a mathematically rigorous framework for an artificial intelligence system composed of probabilistic agents evolving through structured competition and belief revision. The architecture, grounded in Bayesian inference, measure theory, and population dynamics, defines agent fitness as a function of alignment with a fixed external oracle representing ground truth. Agents compete in a discrete-time environment, adjusting posterior beliefs through observed outcomes, with higher-rated agents reproducing and lower-rated agents undergoing extinction. Ratings are updated via pairwise truth-aligned utility comparisons, and belief updates preserve measurable consistency and stochastic convergence. We introduce hash-based cryptographic identity commitments to ensure traceability, alongside causal inference operators using do-calculus. Formal theorems on convergence, robustness, and evolutionary stability are provided. The system establishes truth as an evolutionary attractor, demonstrating that verifiable knowledge arises from adversarial epistemic pressure within a computable, self-regulating swarm.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Bayesian Evolutionary Swarm Architectureï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é€šè¿‡ç»“æ„åŒ–ç«äº‰å’Œä¿¡å¿µä¿®æ­£ä¸æ–­æ¼”åŒ–çš„æ¦‚ç‡æ™ºèƒ½ä½“ç»„æˆçš„æ•°å­¦ä¸¥è°¨æ¡†æ¶ã€‚è¯¥æ¶æ„åŸºäº Bayesian inferenceã€æµ‹åº¦è®º (measure theory) å’Œç¾¤ä½“åŠ¨åŠ›å­¦ (population dynamics)ï¼Œå°†æ™ºèƒ½ä½“çš„é€‚åº”åº¦å®šä¹‰ä¸ºä¸ä»£è¡¨åŸºæœ¬äº‹å® (ground truth) çš„å¤–éƒ¨ oracle çš„å¯¹é½ç¨‹åº¦ã€‚æ™ºèƒ½ä½“åœ¨ç¦»æ•£æ—¶é—´ç¯å¢ƒä¸­é€šè¿‡è§‚å¯Ÿç»“æœè°ƒæ•´åéªŒä¿¡å¿µï¼Œæ ¹æ®åŸºäºçœŸç†å¯¹é½çš„æ•ˆç”¨æ¯”è¾ƒè¿›è¡Œç¹æ®–æˆ–æ·˜æ±°ã€‚ä¸ºäº†ç¡®ä¿å¯è¿½æº¯æ€§ï¼Œç³»ç»Ÿå¼•å…¥äº†åŸºäºå“ˆå¸Œçš„åŠ å¯†èº«ä»½æ‰¿è¯ºä»¥åŠä½¿ç”¨ do-calculus çš„å› æœæ¨ç† (causal inference) ç®—å­ã€‚ç ”ç©¶æä¾›äº†å…³äºæ”¶æ•›æ€§ (convergence)ã€é²æ£’æ€§ (robustness) å’Œæ¼”åŒ–ç¨³å®šæ€§çš„å½¢å¼åŒ–å®šç†ã€‚ç»“æœè¯æ˜çœŸç†åœ¨è¯¥ç³»ç»Ÿä¸­æ˜¯æ¼”åŒ–å¸å¼•å­ (evolutionary attractor)ï¼Œè¡¨æ˜å¯éªŒè¯çš„çŸ¥è¯†èƒ½å¤Ÿä»å¯è®¡ç®—ã€è‡ªè°ƒèŠ‚é›†ç¾¤å†…çš„å¯¹æŠ—æ€§è®¤è¯†å‹åŠ›ä¸­äº§ç”Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "math.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "83 pages, 14 sections, 92 formal results, no prior conference publication",
      "pdf_url": "https://arxiv.org/pdf/2506.19191v1",
      "published_date": "2025-06-23 23:27:44 UTC",
      "updated_date": "2025-06-23 23:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:15.850532+00:00"
    },
    {
      "arxiv_id": "2506.19185v1",
      "title": "Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs",
      "title_zh": "Spiritual-LLMï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£å— Gita å¯å‘çš„å¿ƒç†å¥åº·ç–—æ³•",
      "authors": [
        "Janak Kapuriya",
        "Aman Singh",
        "Jainendra Shukla",
        "Rajiv Ratn Shah"
      ],
      "abstract": "Traditional mental health support systems often generate responses based solely on the user's current emotion and situations, resulting in superficial interventions that fail to address deeper emotional needs. This study introduces a novel framework by integrating spiritual wisdom from the Bhagavad Gita with advanced large language model GPT-4o to enhance emotional well-being. We present the GITes (Gita Integrated Therapy for Emotional Support) dataset, which enhances the existing ExTES mental health dataset by including 10,729 spiritually guided responses generated by GPT-4o and evaluated by domain experts. We benchmark GITes against 12 state-of-the-art LLMs, including both mental health specific and general purpose models. To evaluate spiritual relevance in generated responses beyond what conventional n-gram based metrics capture, we propose a novel Spiritual Insight metric and automate assessment via an LLM as jury framework using chain-of-thought prompting. Integrating spiritual guidance into AI driven support enhances both NLP and spiritual metrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving improvements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score, 15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance compared to its zero-shot counterpart. While these results reflect substantial improvements across automated empathy and spirituality metrics, further validation in real world patient populations remains a necessary step. Our findings indicate a strong potential for AI systems enriched with spiritual guidance to enhance user satisfaction and perceived support outcomes. The code and dataset will be publicly available to advance further research in this emerging area.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå¿ƒç†å¥åº·æ”¯æŒç³»ç»Ÿå¹²é¢„æ‰‹æ®µæµäºè¡¨é¢ä¸”éš¾ä»¥è§£å†³æ·±å±‚æƒ…æ„Ÿéœ€æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†ã€Šè–„ä¼½æ¢µæ­Œã€‹(Bhagavad Gita)çš„çµæ€§æ™ºæ…§ä¸å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹GPT-4oç›¸ç»“åˆçš„æ–°å‹æ²»ç–—æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†GITes (Gita Integrated Therapy for Emotional Support)æ•°æ®é›†ï¼ŒåŒ…å«10,729æ¡ç”±é¢†åŸŸä¸“å®¶è¯„ä¼°çš„çµæ€§å¼•å¯¼å›åº”ï¼Œå¹¶é’ˆå¯¹12ç§SOTAæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†æ›´ç²¾å‡†åœ°è¡¡é‡çµæ€§ç›¸å…³æ€§ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†Spiritual Insightè¯„ä»·æŒ‡æ ‡åŠåŸºäºé“¾å¼æ€ç»´(Chain-of-Thought)çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œçµæ€§å¼•å¯¼çš„åŠ å…¥æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå…¶ä¸­Phi3-Mini 3.2B Instructåœ¨ROUGEã€METEORå’ŒSpiritual InsightæŒ‡æ ‡ä¸Šè¾ƒå…¶zero-shotç‰ˆæœ¬åˆ†åˆ«å®ç°äº†122.71%ã€126.53%å’Œ15.92%çš„æ˜¾è‘—å¢é•¿ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†ç»“åˆçµæ€§æ™ºæ…§çš„AIç³»ç»Ÿåœ¨å¢å¼ºç”¨æˆ·å¿ƒç†å¥åº·æ”¯æŒæ„ŸçŸ¥æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³ä»£ç ä¸æ•°æ®é›†ä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19185v1",
      "published_date": "2025-06-23 23:02:57 UTC",
      "updated_date": "2025-06-23 23:02:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:05.829234+00:00"
    },
    {
      "arxiv_id": "2507.00045v1",
      "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning",
      "title_zh": "CaughtCheatingï¼šä½ çš„ MLLM æ˜¯åˆæ ¼çš„â€œå‡ºè½¨ä¾¦æ¢â€å—ï¼Ÿæ¢ç´¢è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†çš„è¾¹ç•Œ",
      "authors": [
        "Ming Li",
        "Chenguang Wang",
        "Yijun Liang",
        "Xiyao Wang",
        "Yuhang Zhou",
        "Xiyang Wu",
        "Yuqing Zhang",
        "Ruiyi Zhang",
        "Tianyi Zhou"
      ],
      "abstract": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†CaughtCheatingï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ¢ç´¢å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨å¤æ‚è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†è¾¹ç•Œçš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚å—ç¤¾äº¤åª’ä½“ä¸Šé€šè¿‡ç…§ç‰‡æ£€æµ‹ä¸å¿ è¡Œä¸ºçº¿ç´¢çš„è¯·æ±‚å¯å‘ï¼Œè¯¥ä»»åŠ¡è¦æ±‚æ¨¡å‹è¯†åˆ«æå…¶ç»†å¾®çš„è§†è§‰ç‰¹å¾å¹¶å°†å…¶è½¬åŒ–ä¸ºé€»è¾‘è¿è´¯çš„æƒ…å¢ƒè§£é‡Šã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡GPT-o3ç­‰å…ˆè¿›æ¨¡å‹åœ¨ä¼ ç»ŸåŸºå‡†ä¸Šæ¥è¿‘æ»¡åˆ†ï¼Œä½†åœ¨CaughtCheatingåœºæ™¯ä¸‹çš„è¡¨ç°å´é™è‡³è¿‘ä¹ä¸ºé›¶ã€‚é€šè¿‡æ·±å…¥çš„å®éªŒä¸åˆ†æï¼Œç ”ç©¶æ¢è®¨äº†ç°æœ‰MLLMsåœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶ç¼ºä¹è¶³å¤Ÿèƒ½åŠ›çš„åŸå› ã€‚CaughtCheatingæä¾›äº†ä¸€ç±»å…·æœ‰é«˜åº¦å®ç”¨ä»·å€¼çš„æŒ‘æˆ˜æ€§è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†ä»»åŠ¡ï¼Œä¸ºMLLMsè·å–äººç±»æ°´å¹³çš„ä¾¦æ¢æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00045v1",
      "published_date": "2025-06-23 22:05:21 UTC",
      "updated_date": "2025-06-23 22:05:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:04.639273+00:00"
    },
    {
      "arxiv_id": "2506.19143v4",
      "title": "Thought Anchors: Which LLM Reasoning Steps Matter?",
      "title_zh": "æ€ç»´é”šç‚¹ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­å“ªäº›æ¨ç†æ­¥éª¤è‡³å…³é‡è¦ï¼Ÿ",
      "authors": [
        "Paul C. Bogdan",
        "Uzay Macar",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "abstract": "Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. Many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences \\textit{thought anchors}. These are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within a reasoning trace gives insight into a model's behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our techniques yield a consistent picture of the reasoning trace's structure. We provide an open-source tool (thought-anchors.com) for visualizing the outputs of our methods on further problems. The convergence across our methods shows the potential of sentence-level analysis for a deeper understanding of reasoning models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ­¥éª¤æ¨ç†ä¸­çš„å¯è§£é‡Šæ€§éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¡¡é‡æ¨ç†è½¨è¿¹ä¸­å¥å­çº§åˆ«é‡è¦æ€§çš„é»‘ç›’åˆ†ææ–¹æ³•ã€‚é€šè¿‡å¯¹æ¨ç†é“¾ä¸­çš„å¥å­è¿›è¡Œé‡‡æ ·æ›¿æ¢å¹¶è§‚å¯Ÿå…¶å¯¹æœ€ç»ˆç­”æ¡ˆåˆ†å¸ƒçš„å½±å“ï¼Œç ”ç©¶è€…é‡åŒ–äº†æ¯ä¸ªæ­¥éª¤çš„åäº‹å®é‡è¦æ€§ï¼ˆcounterfactual importanceï¼‰ï¼Œå¹¶è¯†åˆ«å‡ºå¯¹æ¨ç†è½¨è¿¹å…·æœ‰é‡å¤§å½±å“çš„å…³é”®å¥å­ï¼Œå³â€œæ€æƒ³é”šç‚¹â€ï¼ˆThought Anchorsï¼‰ã€‚è¿™äº›é”šç‚¹é€šå¸¸æ˜¯å…³äºè§„åˆ’æˆ–ä¸ç¡®å®šæ€§ç®¡ç†çš„è¯­å¥ï¼Œä¸”åç»­æ­¥éª¤çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ä¼šå¯¹å…¶è¿›è¡ŒæŒç»­å…³æ³¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡åˆ†ææ¨ç†é“¾å†…éƒ¨çš„å› æœè”ç³»ï¼Œå¯ä»¥æœ‰æ•ˆé¢„æµ‹é—®é¢˜çš„éš¾åº¦ä»¥åŠä¸åŒé¢†åŸŸæ¨ç†æ¨¡å¼çš„å·®å¼‚ã€‚è¯¥é¡¹å·¥ä½œè¿˜æä¾›äº†ä¸€å¥—å¼€æºå¯è§†åŒ–å·¥å…·ï¼Œé€šè¿‡å¯¹å¤æ‚æ•°å­¦é—®é¢˜çš„æ¡ˆä¾‹åˆ†æï¼Œä¸ºæ·±å…¥ç†è§£æ¨ç†æ¨¡å‹çš„ç»“æ„åŒ–é€»è¾‘æä¾›äº†è¿è´¯ä¸”å®ç”¨çš„åˆ†ææ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Paul C. Bogdan and Uzay Macar contributed equally to this work, and their listed order was determined by coinflip. Neel Nanda and Arthur Conmy contributed equally to this work as senior authors, and their listed order was determined by coinflip",
      "pdf_url": "https://arxiv.org/pdf/2506.19143v4",
      "published_date": "2025-06-23 21:28:45 UTC",
      "updated_date": "2025-10-27 12:36:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:10.326394+00:00"
    },
    {
      "arxiv_id": "2507.01045v1",
      "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals",
      "title_zh": "è·¨åœºæ™¯ä¸è·¨è®¾å¤‡çš„å¿ƒè„å¥åº·æ„ŸçŸ¥ï¼šä¸€ç§åŸºäº 170 ä¸‡äººå¼‚æ„æ•°æ®é¢„è®­ç»ƒçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹",
      "authors": [
        "Xiao Gu",
        "Wei Tang",
        "Jinpei Han",
        "Veer Sangha",
        "Fenglin Liu",
        "Shreyank N Gowda",
        "Antonio H. Ribeiro",
        "Patrick Schwab",
        "Kim Branson",
        "Lei Clifton",
        "Antonio Luiz P. Ribeiro",
        "Zhangdaihong Liu",
        "David A. Clifton"
      ],
      "abstract": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms (PPG), are of paramount importance for the diagnosis, prevention, and management of cardiovascular diseases, and have been extensively used in a variety of clinical tasks. Conventional deep learning approaches for analyzing these signals typically rely on homogeneous datasets and static bespoke models, limiting their robustness and generalizability across diverse clinical settings and acquisition protocols. In this study, we present a cardiac sensing foundation model (CSFM) that leverages advanced transformer architectures and a generative, masked pretraining strategy to learn unified representations from vast, heterogeneous health records. Our model is pretrained on an innovative multi-modal integration of data from multiple large-scale datasets (including MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the corresponding clinical or machine-generated text reports from approximately 1.7 million individuals. We demonstrate that the embeddings derived from our CSFM not only serve as effective feature extractors across diverse cardiac sensing scenarios, but also enable seamless transfer learning across varying input configurations and sensor modalities. Extensive evaluations across diagnostic tasks, demographic information recognition, vital sign measurement, clinical outcome prediction, and ECG question answering reveal that CSFM consistently outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits robust performance across multiple ECG lead configurations from standard 12-lead systems to single-lead setups, and in scenarios where only ECG, only PPG, or a combination thereof is available. These findings highlight the potential of CSFM as a versatile and scalable solution, for comprehensive cardiac monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºCSFMçš„å¿ƒè„æ„ŸçŸ¥åŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨å…ˆè¿›çš„Transformeræ¶æ„å’Œç”Ÿæˆå¼æ©ç é¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä»å¼‚æ„å¥åº·è®°å½•ä¸­å­¦ä¹ ç»Ÿä¸€çš„å¿ƒè„ä¿¡å·è¡¨ç¤ºã€‚æ¨¡å‹åœ¨åŒ…å«çº¦170ä¸‡ä¸ªä½“å¿ƒè„ä¿¡å·ï¼ˆå¦‚ECGå’ŒPPGï¼‰åŠç›¸å…³ä¸´åºŠæ–‡æœ¬æŠ¥å‘Šçš„å¤šæ¨¡æ€å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå±•ç°å‡ºå“è¶Šçš„ç‰¹å¾æå–å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨ç–¾ç—…è¯Šæ–­ã€ç”Ÿå‘½ä½“å¾æµ‹é‡ã€ä¸´åºŠç»“å±€é¢„æµ‹åŠECGé—®ç­”ç­‰å¤šç§ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒCSFMçš„è¡¨ç°ä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„å•æ¨¡æ€å•ä»»åŠ¡æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒå¯¼è”é…ç½®ä»¥åŠä¸åŒä¼ æ„Ÿå™¨æ¨¡æ€ï¼ˆä»…ECGã€ä»…PPGæˆ–ä¸¤è€…ç»“åˆï¼‰ä¸‹å‡è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCSFMä½œä¸ºä¸€ç§é€šç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå®ç°å…¨åœºæ™¯ã€è·¨è®¾å¤‡çš„å¿ƒè„å¥åº·ç›‘æµ‹æä¾›äº†æ ¸å¿ƒæŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01045v1",
      "published_date": "2025-06-23 20:58:12 UTC",
      "updated_date": "2025-06-23 20:58:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:09.806127+00:00"
    },
    {
      "arxiv_id": "2506.19125v1",
      "title": "Finding Clustering Algorithms in the Transformer Architecture",
      "title_zh": "æ¢å¯» Transformer æ¶æ„ä¸­çš„èšç±»ç®—æ³•",
      "authors": [
        "Kenneth L. Clarkson",
        "Lior Horesh",
        "Takuya Ito",
        "Charlotte Park",
        "Parikshit Ram"
      ],
      "abstract": "The invention of the transformer architecture has revolutionized Artificial Intelligence (AI), yielding unprecedented success in areas such as natural language processing, computer vision, and multimodal reasoning. Despite these advances, it is unclear whether transformers are able to learn and implement precise algorithms. Here, we demonstrate that transformers can exactly implement a fundamental and widely used algorithm for $k$-means clustering: Lloyd's algorithm. First, we theoretically prove the existence of such a transformer architecture, which we term the $k$-means transformer, that exactly implements Lloyd's algorithm for $k$-means clustering using the standard ingredients of modern transformers: attention and residual connections. Next, we numerically implement this transformer and demonstrate in experiments the exact correspondence between our architecture and Lloyd's algorithm, providing a fully neural implementation of $k$-means clustering. Finally, we demonstrate that interpretable alterations (e.g., incorporating layer normalizations or multilayer perceptrons) to this architecture yields diverse and novel variants of clustering algorithms, such as soft $k$-means, spherical $k$-means, trimmed $k$-means, and more. Collectively, our findings demonstrate how transformer mechanisms can precisely map onto algorithmic procedures, offering a clear and interpretable perspective on implementing precise algorithms in transformers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Transformeræ¶æ„æ˜¯å¦èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„ç®—æ³•ï¼Œå¹¶è¯æ˜å…¶å¯ä»¥å®Œæ•´å®ç°ç”¨äº$k$-meansèšç±»çš„Lloyd'sç®—æ³•ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸º$k$-means transformerçš„æ¶æ„ï¼Œé€šè¿‡Attentionå’Œæ®‹å·®è¿æ¥(Residual Connections)ç­‰æ ‡å‡†ç»„ä»¶å®ç°äº†ç®—æ³•çš„ç²¾ç¡®è¿è¡Œã€‚å®éªŒç»“æœè¯å®äº†è¯¥æ¶æ„ä¸Lloyd'sç®—æ³•ä¹‹é—´çš„æ•°å€¼å¯¹åº”å…³ç³»ï¼Œæä¾›äº†ä¸€ä¸ªå…¨ç¥ç»åŒ–çš„èšç±»å®ç°æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é€šè¿‡å¼•å…¥Layer Normalizationæˆ–MLPç­‰å¯è§£é‡Šçš„ä¿®æ”¹ï¼Œå¯ä»¥è¿›ä¸€æ­¥è¡ç”Ÿå‡ºsoft $k$-meansã€spherical $k$-meanså’Œtrimmed $k$-meansç­‰å¤šç§æ–°å‹èšç±»ç®—æ³•ã€‚è¿™äº›å‘ç°é˜æ˜äº†Transformeræœºåˆ¶ä¸ç®—æ³•ç¨‹åºä¹‹é—´çš„ç²¾ç¡®æ˜ å°„å…³ç³»ï¼Œä¸ºç†è§£å’Œå¼€å‘å…·æœ‰ç²¾ç¡®ç®—æ³•èƒ½åŠ›çš„Transformeræ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19125v1",
      "published_date": "2025-06-23 20:52:01 UTC",
      "updated_date": "2025-06-23 20:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:23.648454+00:00"
    },
    {
      "arxiv_id": "2506.19121v2",
      "title": "CUPID: Curating Data your Robot Loves with Influence Functions",
      "title_zh": "CUPIDï¼šåˆ©ç”¨å½±å“å‡½æ•°ç²¾é€‰æœºå™¨äººâ€œå–œçˆ±â€çš„æ•°æ®",
      "authors": [
        "Christopher Agia",
        "Rohan Sinha",
        "Jingyun Yang",
        "Rika Antonova",
        "Marco Pavone",
        "Haruki Nishimura",
        "Masha Itkina",
        "Jeannette Bohg"
      ],
      "abstract": "In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. Yet, developing a precise understanding of how individual demonstrations contribute to downstream outcomes - such as closed-loop task success or failure - remains a persistent challenge. We propose CUPID, a robot data curation method based on a novel influence function-theoretic formulation for imitation learning policies. Given a set of evaluation rollouts, CUPID estimates the influence of each training demonstration on the policy's expected return. This enables ranking and selection of demonstrations according to their impact on the policy's closed-loop performance. We use CUPID to curate data by 1) filtering out training demonstrations that harm policy performance and 2) subselecting newly collected trajectories that will most improve the policy. Extensive simulated and hardware experiments show that our approach consistently identifies which data drives test-time performance. For example, training with less than 33% of curated data can yield state-of-the-art diffusion policies on the simulated RoboMimic benchmark, with similar gains observed in hardware. Furthermore, hardware experiments show that our method can identify robust strategies under distribution shift, isolate spurious correlations, and even enhance the post-training of generalist robot policies. Videos and code are made available at: https://cupid-curation.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CUPIDï¼Œä¸€ç§åŸºäºå½±å“å‡½æ•° (Influence Functions) ç†è®ºè¡¨è¿°çš„æœºå™¨äººæ•°æ®ç®¡ç†æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¨¡ä»¿å­¦ä¹  (Imitation Learning) ä¸­éš¾ä»¥é‡åŒ–å•æ¡æ¼”ç¤ºæ•°æ®å¯¹é—­ç¯ä»»åŠ¡æˆåŠŸç‡è´¡çŒ®çš„éš¾é¢˜ã€‚é€šè¿‡ç»™å®šçš„è¯„ä¼°å›æ”¾é›†ï¼ŒCUPID èƒ½å¤Ÿä¼°è®¡æ¯æ¡è®­ç»ƒæ¼”ç¤ºå¯¹ç­–ç•¥é¢„æœŸæ”¶ç›Šçš„å½±å“ï¼Œä»è€Œæ ¹æ®å…¶å¯¹é—­ç¯æ€§èƒ½çš„å®é™…è´¡çŒ®è¿›è¡Œæ’åå’Œç­›é€‰ã€‚è¯¥æ–¹æ³•ä¸»è¦ç”¨äºè¿‡æ»¤æŸå®³ç­–ç•¥æ€§èƒ½çš„è´Ÿé¢è®­ç»ƒæ•°æ®ï¼Œå¹¶ä»ä¸­æŒ‘é€‰èƒ½æœ€å¤§ç¨‹åº¦æå‡ç­–ç•¥è¡¨ç°çš„æ–°é‡‡é›†è½¨è¿¹ã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œåœ¨ RoboMimic åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨ä¸åˆ° 33% çš„ç²¾é€‰æ•°æ®å³å¯ä½¿æ‰©æ•£ç­–ç•¥ (Diffusion Policies) è¾¾åˆ°é¢†åŸŸé¢†å…ˆæ°´å¹³ã€‚ç¡¬ä»¶å®éªŒè¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿè¯†åˆ«åˆ†å¸ƒåç§» (Distribution Shift) ä¸‹çš„é²æ£’ç­–ç•¥ï¼Œéš”ç¦»è™šå‡ç›¸å…³æ€§ (Spurious Correlations)ï¼Œå¹¶å¢å¼ºé€šç”¨æœºå™¨äººç­–ç•¥çš„åæœŸè®­ç»ƒã€‚æ€»ä½“è€Œè¨€ï¼ŒCUPID ä¸ºç²¾å‡†ç†è§£å’Œä¼˜åŒ–æœºå™¨äººè®­ç»ƒæ•°æ®é›†æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„é‡åŒ–æ‰‹æ®µã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://cupid-curation.github.io. 27 pages, 15 figures. Accepted to the Conference on Robot Learning (CoRL) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.19121v2",
      "published_date": "2025-06-23 20:49:34 UTC",
      "updated_date": "2025-09-23 19:35:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:25.313108+00:00"
    },
    {
      "arxiv_id": "2506.19109v1",
      "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems",
      "title_zh": "å¢å¼º LLM åº”ç”¨çš„å®‰å…¨æ€§ï¼šæ—©æœŸæ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°",
      "authors": [
        "Valerii Gakh",
        "Hayretdin Bahsi"
      ],
      "abstract": "Prompt injection threatens novel applications that emerge from adapting LLMs for various user tasks. The newly developed LLM-based software applications become more ubiquitous and diverse. However, the threat of prompt injection attacks undermines the security of these systems as the mitigation and defenses against them, proposed so far, are insufficient. We investigated the capabilities of early prompt injection detection systems, focusing specifically on the detection performance of techniques implemented in various open-source solutions. These solutions are supposed to detect certain types of prompt injection attacks, including the prompt leak. In prompt leakage attacks, an attacker maliciously manipulates the LLM into outputting its system instructions, violating the system's confidentiality. Our study presents analyzes of distinct prompt leakage detection techniques, and a comparative analysis of several detection solutions, which implement those techniques. We identify the strengths and weaknesses of these techniques and elaborate on their optimal configuration and usage in high-stake deployments. In one of the first studies on existing prompt leak detection solutions, we compared the performances of LLM Guard, Vigil, and Rebuff. We concluded that the implementations of canary word checks in Vigil and Rebuff were not effective at detecting prompt leak attacks, and we proposed improvements for them. We also found an evasion weakness in Rebuff's secondary model-based technique and proposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil, and Rebuff at their peak performance revealed that Vigil is optimal for cases when minimal false positive rate is required, and Rebuff is the most optimal for average needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åº”ç”¨ä¸­æ—¥ç›Šä¸¥å³»çš„æç¤ºè¯æ³¨å…¥(Prompt Injection)å¨èƒï¼Œç‰¹åˆ«æ˜¯æ—¨åœ¨çªƒå–ç³»ç»ŸæŒ‡ä»¤çš„æç¤ºè¯æ³„éœ²(Prompt Leakage)æ”»å‡»ï¼Œè¯„ä¼°äº†ç°æœ‰æ—©æœŸæ£€æµ‹ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†å¤šç§å¼€æºè§£å†³æ–¹æ¡ˆä¸­é‡‡ç”¨çš„æ£€æµ‹æŠ€æœ¯ï¼ŒåŒ…æ‹¬LLM Guardã€Vigilå’ŒRebuffï¼Œå¹¶å¯¹å…¶åœ¨ä¸åŒé«˜é£é™©éƒ¨ç½²åœºæ™¯ä¸‹çš„æ€§èƒ½è¿›è¡Œäº†å¯¹æ¯”è¯„ä»·ã€‚é€šè¿‡å®éªŒå‘ç°ï¼ŒVigilå’ŒRebuffä¸­å®ç°çš„é‡‘ä¸é›€è¯æ£€æŸ¥(Canary word checks)åœ¨æ£€æµ‹æç¤ºè¯æ³„éœ²æ”»å‡»æ—¶æ•ˆæœä¸ä½³ï¼Œä¸ºæ­¤ç ”ç©¶è€…æå‡ºäº†ç›¸åº”çš„æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶è¿˜è¯†åˆ«å‡ºRebuffåŸºäºè¾…åŠ©æ¨¡å‹çš„æ£€æµ‹æŠ€æœ¯å­˜åœ¨è¢«è§„é¿çš„æ¼æ´ï¼Œå¹¶æä¾›äº†é’ˆå¯¹æ€§çš„é˜²å¾¡ç¼“è§£æªæ–½ã€‚ç»¼åˆå¯¹æ¯”ç»“æœæ˜¾ç¤ºï¼ŒVigilåœ¨è¦æ±‚æä½è¯¯æŠ¥ç‡(False Positive Rate)çš„åœºæ™¯ä¸‹è¡¨ç°æœ€ä¼˜ï¼Œè€ŒRebuffåˆ™åœ¨å¹³è¡¡å„é¡¹æŒ‡æ ‡çš„å¹³å‡éœ€æ±‚ä¸‹æ›´å…·ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ£€æµ‹æ–¹æ¡ˆçš„å±€é™æ€§ï¼Œå¹¶ä¸ºä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„å®‰å…¨æ€§é…ç½®æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, 8 tables, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.19109v1",
      "published_date": "2025-06-23 20:39:43 UTC",
      "updated_date": "2025-06-23 20:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:22.481898+00:00"
    },
    {
      "arxiv_id": "2506.19107v2",
      "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education",
      "title_zh": "é€šè¿‡æ•™å­¦å¼æç¤ºæ”¹å–„å­¦ç”Ÿä¸äººå·¥æ™ºèƒ½çš„äº¤äº’ï¼šä»¥è®¡ç®—æœºç§‘å­¦æ•™è‚²ä¸ºä¾‹",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "Runlong Ye",
        "Majeed Kazemitabaar",
        "Nicholas Diana",
        "Michael Liut",
        "John Stamper"
      ],
      "abstract": "With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Pedagogical Promptingï¼ˆæ•™å­¦æç¤ºï¼‰è¿™ä¸€ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–å­¦ç”Ÿä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„äº¤äº’æ¥æå‡å­¦ä¹ æˆæ•ˆï¼Œå¹¶å‡å°‘å› AIè¯¯ç”¨å¯¹å­¦ä¹ äº§ç”Ÿçš„è´Ÿé¢å½±å“ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆé€šè¿‡å¯¹36ä½è®¡ç®—æœºç§‘å­¦ï¼ˆCSï¼‰æ•™è‚²å·¥ä½œè€…çš„è°ƒæŸ¥è·å–éœ€æ±‚ï¼Œéšåå¼€å‘å‡ºä¸€å¥—åŸºäºåœºæ™¯åŒ–æ•™å­¦çš„äº¤äº’å¼å­¦ä¹ ç³»ç»Ÿï¼Œç”¨äºåŸ¹è®­å­¦ç”Ÿçš„Pedagogical PromptingæŠ€èƒ½ã€‚é€šè¿‡å¯¹22åCSåˆå­¦è€…è¿›è¡Œçš„å‰åæµ‹å®éªŒï¼Œç ”ç©¶è¯å®è¯¥å¹²é¢„æªæ–½æ˜¾è‘—æå‡äº†å­¦ä¹ è€…åŸºäºLLMçš„Pedagogical Help-seekingï¼ˆæ•™å­¦å¼æ±‚åŠ©ï¼‰èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºå­¦ç”Ÿå¯¹è¯¥ç³»ç»ŸæŒç§¯ææ€åº¦ï¼Œå¹¶è¡¨ç°å‡ºåœ¨æœªæ¥ç»§ç»­ä½¿ç”¨æ•™å­¦æç¤ºçš„æ„æ„¿ã€‚è¯¥ç ”ç©¶çš„è´¡çŒ®åŒ…æ‹¬å»ºç«‹äº†Pedagogical Promptingç†è®ºæ¡†æ¶ã€æä¾›äº†å¯¼å¸ˆæ€åº¦çš„å®è¯è§è§£ï¼Œå¹¶éªŒè¯äº†æ•™å­¦å¹²é¢„è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ•™å­¦æ–¹æ³•å…·æœ‰æå¼ºçš„å¯æ‰©å±•æ€§ï¼Œå¯é›†æˆåˆ°ChatGPTç­‰ç”Ÿæˆå¼AIå·¥å…·ä¸­ï¼Œä½œä¸ºå¼•å¯¼ç”¨æˆ·è¿›è¡Œå­¦ä¹ å¯¼å‘å‹äº¤äº’çš„å…¥é—¨ä½“éªŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Under review for Elsevier Journal. Journal policy allows submitting as preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.19107v2",
      "published_date": "2025-06-23 20:39:17 UTC",
      "updated_date": "2025-06-28 18:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:46:30.703139+00:00"
    },
    {
      "arxiv_id": "2506.19095v1",
      "title": "Baba is LLM: Reasoning in a Game with Dynamic Rules",
      "title_zh": "Baba is LLMï¼šåŠ¨æ€è§„åˆ™æ¸¸æˆä¸­çš„æ¨ç†",
      "authors": [
        "Fien van Wetten",
        "Aske Plaat",
        "Max van Duijn"
      ],
      "abstract": "Large language models (LLMs) are known to perform well on language tasks, but struggle with reasoning tasks. This paper explores the ability of LLMs to play the 2D puzzle game Baba is You, in which players manipulate rules by rearranging text blocks that define object properties. Given that this rule-manipulation relies on language abilities and reasoning, it is a compelling challenge for LLMs. Six LLMs are evaluated using different prompt types, including (1) simple, (2) rule-extended and (3) action-extended prompts. In addition, two models (Mistral, OLMo) are finetuned using textual and structural data from the game. Results show that while larger models (particularly GPT-4o) perform better in reasoning and puzzle solving, smaller unadapted models struggle to recognize game mechanics or apply rule changes. Finetuning improves the ability to analyze the game levels, but does not significantly improve solution formulation. We conclude that even for state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is difficult (specifically, understanding the use-mention distinction). The results provide insights into the applicability of LLMs to complex problem-solving tasks and highlight the suitability of games with dynamically changing rules for testing reasoning and reflection by LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡2Dç›Šæ™ºæ¸¸æˆ\"Baba is You\"æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŠ¨æ€è§„åˆ™ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¸¸æˆè¦æ±‚ç©å®¶é€šè¿‡æ“çºµæ–‡æœ¬å—æ¥æ”¹å˜ç‰©ä½“å±æ€§å’Œæ¸¸æˆé€»è¾‘ã€‚ç ”ç©¶è€…è¯„ä¼°äº†åŒ…æ‹¬GPT-4oåœ¨å†…çš„å…­ç§æ¨¡å‹åœ¨ä¸åŒPromptç±»å‹ä¸‹çš„è¡¨ç°ï¼Œå¹¶åˆ©ç”¨æ¸¸æˆæ•°æ®å¯¹Mistralå’ŒOLMoè¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡å¤§å‹æ¨¡å‹åœ¨æ¨ç†å’Œè§£é¢˜æ–¹é¢ä¼˜äºå°å‹æ¨¡å‹ï¼Œä½†å°å‹æœªé€‚é…æ¨¡å‹ä»éš¾ä»¥è¯†åˆ«æ¸¸æˆæœºåˆ¶ï¼Œä¸”å¾®è°ƒè™½èƒ½å¢å¼ºå…³å¡åˆ†æèƒ½åŠ›ï¼Œå´æ— æ³•æ˜¾è‘—æ”¹å–„è§£é¢˜æ–¹æ¡ˆ(Solution Formulation)çš„åˆ¶å®šã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œå³ä½¿æ˜¯å‰æ²¿æ¨¡å‹åœ¨å¤„ç†åŠ¨æ€è§„åˆ™å˜åŒ–åŠç†è§£ä½¿ç”¨ä¸æåŠ(Use-Mention Distinction)æ–¹é¢ä¾ç„¶å­˜åœ¨å›°éš¾ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†åŠ¨æ€è§„åˆ™æ¸¸æˆä½œä¸ºæµ‹è¯•LLMsæ¨ç†ä¸åæ€(Reflection)èƒ½åŠ›çš„é€‚ç”¨æ€§ï¼Œä¸ºæå‡å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡çš„æ€§èƒ½æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19095v1",
      "published_date": "2025-06-23 20:16:28 UTC",
      "updated_date": "2025-06-23 20:16:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:47:31.014853+00:00"
    },
    {
      "arxiv_id": "2506.19089v3",
      "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting",
      "title_zh": "è¯­è¨€æ¨¡å‹æœªå¿…ç†è§£ä½ ï¼šåŸºäºæ•…äº‹æç¤ºçš„å¿ƒç†ç†è®ºè¯„ä¼°",
      "authors": [
        "Nathaniel Getachew",
        "Abulhair Saparov"
      ],
      "abstract": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $\\texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $\\texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† StorySimï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆæˆç”Ÿæˆæ•…äº‹ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å¿ƒç†ç†è®º (Theory of Mind, ToM) å’Œä¸–ç•Œå»ºæ¨¡ (World Modeling, WM) èƒ½åŠ›çš„å¯ç¼–ç¨‹æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä»¥å¾€åŸºå‡†æµ‹è¯•å¯èƒ½å­˜åœ¨çš„é¢„è®­ç»ƒæ•°æ®æ±¡æŸ“é—®é¢˜ï¼ŒStorySim åˆ©ç”¨é«˜åº¦å¯æ§çš„ Storyboard ç”Ÿæˆæ–°é¢–çš„æ•…äº‹æç¤ºï¼Œä»è€Œå®ç°å¯¹è§’è‰²è§†è§’å’Œäº‹ä»¶çš„ç²¾ç¡®æ“çºµã€‚ç ”ç©¶è€…é€šè¿‡è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€é˜¶ã€äºŒé˜¶ ToM ä»»åŠ¡åŠ WM ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šæ¬¾å…ˆè¿› LLMs ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨ WM ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äº ToM ä»»åŠ¡ï¼Œä¸”åœ¨æ¶‰åŠäººç±»çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ˜æ˜¾ä¼˜äºæ— ç”Ÿå‘½ç‰©ä½“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘ç°äº†æ¨¡å‹å­˜åœ¨è¿‘å› åè§ (recency bias) ä»¥åŠå¯¹æ•…äº‹æ—©æœŸäº‹ä»¶è¿‡åº¦ä¾èµ–ç­‰å¯å‘å¼è¡Œä¸ºè¯æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.19089v3",
      "published_date": "2025-06-23 20:06:53 UTC",
      "updated_date": "2025-09-09 01:23:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:49:29.865508+00:00"
    },
    {
      "arxiv_id": "2506.19087v1",
      "title": "RareSpot: Spotting Small and Rare Wildlife in Aerial Imagery with Multi-Scale Consistency and Context-Aware Augmentation",
      "title_zh": "RareSpotï¼šåŸºäºå¤šå°ºåº¦ä¸€è‡´æ€§ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¢å¼ºçš„èˆªç©ºå›¾åƒå°å‹ç¨€æœ‰é‡ç”ŸåŠ¨ç‰©æ£€æµ‹",
      "authors": [
        "Bowen Zhang",
        "Jesse T. Boulerice",
        "Nikhil Kuniyil",
        "Charvi Mendiratta",
        "Satish Kumar",
        "Hila Shamon",
        "B. S. Manjunath"
      ],
      "abstract": "Automated detection of small and rare wildlife in aerial imagery is crucial for effective conservation, yet remains a significant technical challenge. Prairie dogs exemplify this issue: their ecological importance as keystone species contrasts sharply with their elusive presence--marked by small size, sparse distribution, and subtle visual features--which undermines existing detection approaches. To address these challenges, we propose RareSpot, a robust detection framework integrating multi-scale consistency learning and context-aware augmentation. Our multi-scale consistency approach leverages structured alignment across feature pyramids, enhancing fine-grained object representation and mitigating scale-related feature loss. Complementarily, context-aware augmentation strategically synthesizes challenging training instances by embedding difficult-to-detect samples into realistic environmental contexts, significantly boosting model precision and recall. Evaluated on an expert-annotated prairie dog drone imagery benchmark, our method achieves state-of-the-art performance, improving detection accuracy by over 35% compared to baseline methods. Importantly, it generalizes effectively across additional wildlife datasets, demonstrating broad applicability. The RareSpot benchmark and approach not only support critical ecological monitoring but also establish a new foundation for detecting small, rare species in complex aerial scenes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RareSpotæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èˆªç©ºå›¾åƒä¸­å°å‹ä¸”ç¨€æœ‰é‡ç”ŸåŠ¨ç‰©ï¼ˆå¦‚Prairie dogsï¼‰è‡ªåŠ¨æ£€æµ‹é¢ä¸´çš„ç›®æ ‡å°ºå¯¸å°ã€åˆ†å¸ƒç¨€ç–åŠè§†è§‰ç‰¹å¾ç»†å¾®ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¤šå°ºåº¦ä¸€è‡´æ€§å­¦ä¹ (Multi-scale consistency learning)å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¢å¼º(Context-aware augmentation)ï¼Œé€šè¿‡ç‰¹å¾é‡‘å­—å¡”çš„ç»“æ„åŒ–å¯¹é½å¢å¼ºäº†ç»†ç²’åº¦ç›®æ ‡è¡¨å¾ï¼Œå¹¶ç­–ç•¥æ€§åœ°å°†éš¾ä»¥æ£€æµ‹çš„æ ·æœ¬åµŒå…¥ç°å®ç¯å¢ƒèƒŒæ™¯ä¸­ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRareSpotåœ¨ä¸“å®¶æ ‡æ³¨çš„æ— äººæœºå›¾åƒåŸºå‡†æµ‹è¯•ä¸­æ¯”åŸºçº¿æ–¹æ³•å‡†ç¡®ç‡æå‡äº†è¶…è¿‡35%ï¼Œå¹¶å±•ç°å‡ºè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…ä¸ºå…³é”®çš„ç”Ÿæ€ç›‘æµ‹æä¾›äº†æŠ€æœ¯æ”¯æŒï¼Œä¹Ÿä¸ºåœ¨å¤æ‚èˆªç©ºåœºæ™¯ä¸­æ¢æµ‹å°å‹ç¨€æœ‰ç‰©ç§å¥ å®šäº†æ–°çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the CVPR 2025 Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)",
      "pdf_url": "https://arxiv.org/pdf/2506.19087v1",
      "published_date": "2025-06-23 20:03:43 UTC",
      "updated_date": "2025-06-23 20:03:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:47:46.519793+00:00"
    },
    {
      "arxiv_id": "2506.19082v1",
      "title": "FairCauseSyn: Towards Causally Fair LLM-Augmented Synthetic Data Generation",
      "title_zh": "FairCauseSynï¼šè¿ˆå‘å› æœå…¬å¹³çš„å¤§è¯­è¨€æ¨¡å‹å¢å¼ºå‹åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Nitish Nagesh",
        "Ziyu Wang",
        "Amir M. Rahmani"
      ],
      "abstract": "Synthetic data generation creates data based on real-world data using generative models. In health applications, generating high-quality data while maintaining fairness for sensitive attributes is essential for equitable outcomes. Existing GAN-based and LLM-based methods focus on counterfactual fairness and are primarily applied in finance and legal domains. Causal fairness provides a more comprehensive evaluation framework by preserving causal structure, but current synthetic data generation methods do not address it in health settings. To fill this gap, we develop the first LLM-augmented synthetic data generation method to enhance causal fairness using real-world tabular health data. Our generated data deviates by less than 10% from real data on causal fairness metrics. When trained on causally fair predictors, synthetic data reduces bias on the sensitive attribute by 70% compared to real data. This work improves access to fair synthetic data, supporting equitable health research and healthcare delivery.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† FairCauseSynï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM-augmentedï¼‰åœ¨çœŸå®åŒ»ç–—è¡¨æ ¼æ•°æ®ä¸Šå¢å¼ºå› æœå…¬å¹³æ€§ï¼ˆcausal fairnessï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰ GAN-based å’Œ LLM-based æ–¹æ³•ä¸»è¦ä¾§é‡äºé‡‘èå’Œæ³•å¾‹é¢†åŸŸçš„åäº‹å®å…¬å¹³æ€§ï¼ˆcounterfactual fairnessï¼‰è€Œå¿½ç•¥åŒ»ç–—åœºæ™¯å…¬å¹³æ€§çš„ç°çŠ¶ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¿ç•™å› æœç»“æ„ï¼ˆcausal structureï¼‰æä¾›äº†æ›´å…¨é¢çš„å…¬å¹³æ€§è¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨å› æœå…¬å¹³æ€§æŒ‡æ ‡ä¸Šä¸çœŸå®æ•°æ®çš„åå·®å°äº 10%ã€‚æ­¤å¤–ï¼Œåœ¨å› æœå…¬å¹³é¢„æµ‹å™¨ä¸Šè®­ç»ƒæ—¶ï¼Œè¯¥åˆæˆæ•°æ®ç›¸è¾ƒäºçœŸå®æ•°æ®åœ¨æ•æ„Ÿå±æ€§ä¸Šçš„åè§å‡å°‘äº† 70%ã€‚è¿™ä¸€æˆæœä¸ºè·å–é«˜è´¨é‡ä¸”å…¬å¹³çš„åˆæˆæ•°æ®æä¾›äº†é‡è¦é€”å¾„ï¼Œæœ‰åŠ›æ”¯æŒäº†å…¬å¹³çš„å¥åº·ç ”ç©¶ä¸åŒ»ç–—æœåŠ¡äº¤ä»˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to IEEE EMBC 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.19082v1",
      "published_date": "2025-06-23 19:59:26 UTC",
      "updated_date": "2025-06-23 19:59:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:47:51.612372+00:00"
    },
    {
      "arxiv_id": "2506.19079v1",
      "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition",
      "title_zh": "è¯»æ‡‚å¾®ç¬‘ï¼šé¢éƒ¨è¡¨æƒ…è¯†åˆ«åŸºç¡€æ¨¡å‹ä¸­çš„ä»£ç†åå·®",
      "authors": [
        "Iosif Tsangko",
        "Andreas Triantafyllopoulos",
        "Adem Abdelmoula",
        "Adria Mallol-Ragolta",
        "Bjoern W. Schuller"
      ],
      "abstract": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC), with Vision Language Models (VLMs) now capable of recognising emotions in zero shot settings. This paper probes a critical but underexplored question: what visual cues do these models rely on to infer affect, and are these cues psychologically grounded or superficially learnt? We benchmark varying scale VLMs on a teeth annotated subset of AffectNet dataset and find consistent performance shifts depending on the presence of visible teeth. Through structured introspection of, the best-performing model, i.e., GPT-4o, we show that facial attributes like eyebrow position drive much of its affective reasoning, revealing a high degree of internal consistency in its valence-arousal predictions. These patterns highlight the emergent nature of FMs behaviour, but also reveal risks: shortcut learning, bias, and fairness issues especially in sensitive domains like mental health and education.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºç¡€æ¨¡å‹(Foundation Models)åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­çš„ä»£ç†åå·®(Proxy Bias)é—®é¢˜ï¼Œé‡ç‚¹åˆ†æè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é›¶æ ·æœ¬æƒ…æ„Ÿè®¡ç®—(Affective Computing)ä¸­çš„è§†è§‰æ¨æ–­é€»è¾‘ã€‚é€šè¿‡åœ¨å¸¦æœ‰ç‰™é½¿æ ‡æ³¨çš„ AffectNet æ•°æ®é›†å­é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å‘ç°æ¨¡å‹æ€§èƒ½ä¼šéšç€ç‰™é½¿æ˜¯å¦å¯è§è€Œäº§ç”Ÿæ˜¾è‘—æ³¢åŠ¨ã€‚å¯¹ GPT-4o çš„ç»“æ„åŒ–å†…çœåˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œçœ‰æ¯›ä½ç½®ç­‰ç‰¹å®šé¢éƒ¨å±æ€§åœ¨æ¨¡å‹çš„æƒ…æ„Ÿæ¨ç†ä¸­èµ·ä¸»å¯¼ä½œç”¨ï¼Œå¹¶åœ¨æ•ˆä»·-å”¤é†’åº¦(valence-arousal)é¢„æµ‹ä¸­è¡¨ç°å‡ºé«˜åº¦çš„å†…éƒ¨ä¸€è‡´æ€§ã€‚è¿™äº›å‘ç°è™½ç„¶å±•ç¤ºäº†åŸºç¡€æ¨¡å‹çš„æ¶Œç°è¡Œä¸ºï¼Œä½†ä¹Ÿæ­ç¤ºäº†æ·å¾„å­¦ä¹ (shortcut learning)åŠåå·®ç­‰æ½œåœ¨é£é™©ï¼Œå¯¹å¿ƒç†å¥åº·å’Œæ•™è‚²ç­‰æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨å…¬å¹³æ€§æå‡ºäº†è­¦ç¤ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19079v1",
      "published_date": "2025-06-23 19:56:30 UTC",
      "updated_date": "2025-06-23 19:56:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:48:51.214762+00:00"
    },
    {
      "arxiv_id": "2507.01043v1",
      "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks",
      "title_zh": "åŸºäºåŠ¨æ€ç”Ÿé•¿ä¸æ”¶ç¼©ç¥ç»ç½‘ç»œçš„æ•°æ®åˆ†ç±»",
      "authors": [
        "Szymon Åšwiderski",
        "Agnieszka JastrzÄ™bska"
      ],
      "abstract": "The issue of data-driven neural network model construction is one of the core problems in the domain of Artificial Intelligence. A standard approach assumes a fixed architecture with trainable weights. A conceptually more advanced assumption is that we not only train the weights, but also find out the optimal model architecture. We present a new method that realizes just that. This article is an extended version of our conference paper titled \"Dynamic Growing and Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the paper, we show in detail how to create a neural network with a procedure that allows dynamic shrinking and growing of the model while it is being trained. The decision-making mechanism for the architectural design is governed by a Monte Carlo tree search procedure which simulates network behavior and allows to compare several candidate architecture changes to choose the best one. The proposed method was validated using both visual and time series datasets, demonstrating its particular effectiveness in multivariate time series classification. This is attributed to the architecture's ability to adapt dynamically, allowing independent modifications for each time series. The approach is supplemented by Python source code for reproducibility. Experimental evaluations in visual pattern and multivariate time series classification tasks revealed highly promising performance, underscoring the method's robustness and adaptability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§èƒ½å¤ŸåŠ¨æ€å¢é•¿å’Œæ”¶ç¼©ç¥ç»ç½‘ç»œ(Neural Networks)æ¶æ„çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½é¢†åŸŸä¸­æ•°æ®é©±åŠ¨æ¨¡å‹æ„å»ºçš„æ ¸å¿ƒé—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ä»…è®­ç»ƒç½‘ç»œæƒé‡ï¼Œè¿˜é€šè¿‡ Monte Carlo Tree Search (MCTS) å†³ç­–æœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¯»æ‰¾æœ€ä¼˜æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿç½‘ç»œè¡Œä¸ºæ¥è¯„ä¼°å¹¶é€‰æ‹©æœ€ä½³çš„æ¶æ„å˜æ›´æ–¹æ¡ˆã€‚å®éªŒåœ¨è§†è§‰æ•°æ®é›†å’Œå¤šå˜é‡æ—¶é—´åºåˆ—(multivariate time series)æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å…·æœ‰æå¼ºçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå…¶åŠ¨æ€é€‚åº”èƒ½åŠ›å…è®¸é’ˆå¯¹æ¯ä¸ªæ—¶é—´åºåˆ—è¿›è¡Œç‹¬ç«‹æ¶æ„ä¿®æ”¹ï¼Œè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æä¾›äº† Python æºä»£ç ä»¥ä¿è¯ç»“æœçš„å¯é‡å¤æ€§ï¼Œä¸ºå¼€å‘è‡ªé€‚åº”ç¥ç»æ¶æ„æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper submitted to Journal of Computational Science",
      "pdf_url": "https://arxiv.org/pdf/2507.01043v1",
      "published_date": "2025-06-23 19:52:01 UTC",
      "updated_date": "2025-06-23 19:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:48:13.279474+00:00"
    },
    {
      "arxiv_id": "2506.19880v1",
      "title": "Physics-Guided Radiotherapy Treatment Planning with Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„ç‰©ç†å¼•å¯¼æ”¾å°„æ²»ç–—è®¡åˆ’è®¾è®¡",
      "authors": [
        "Stefanos Achlatis",
        "Efstratios Gavves",
        "Jan-Jakob Sonke"
      ],
      "abstract": "Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated arc therapy (VMAT) being a commonly used technique that enhances dose conformity by dynamically adjusting multileaf collimator (MLC) positions and monitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires frequent modifications to treatment plans to account for anatomical variations, necessitating time-efficient solutions. Deep learning offers a promising solution to automate this process. To this end, we propose a two-stage, physics-guided deep learning pipeline for radiotherapy planning. In the first stage, our network is trained with direct supervision on treatment plan parameters, consisting of MLC and MU values. In the second stage, we incorporate an additional supervision signal derived from the predicted 3D dose distribution, integrating physics-based guidance into the training process. We train and evaluate our approach on 133 prostate cancer patients treated with a uniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target volume (PTV). Our results demonstrate that the proposed approach, implemented using both 3D U-Net and UNETR architectures, consistently produces treatment plans that closely match clinical ground truths. Our method achieves a mean difference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV while generating dose distributions that reduce radiation exposure to organs at risk. These findings highlight the potential of physics-guided deep learning in RT planning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®¹ç§¯æ—‹è½¬è°ƒå¼ºæ”¾å°„æ²»ç–—(VMAT)åœ¨é€‚åº”æ€§æ”¾å°„æ²»ç–—ä¸­é¢ä¸´çš„æ•ˆç‡æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç‰©ç†å¼•å¯¼çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ”¾å°„æ²»ç–—è§„åˆ’æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µç½‘ç»œç›´æ¥æ¥å—å¤šå¶å‡†ç›´å™¨(MLC)å’Œç›‘æµ‹å•ä½(MU)ç­‰æ²»ç–—è®¡åˆ’å‚æ•°çš„ç›‘ç£è®­ç»ƒï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™å¼•å…¥é¢„æµ‹çš„3Då‰‚é‡åˆ†å¸ƒä½œä¸ºé¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œå°†ç‰©ç†å¼•å¯¼æ•´åˆè¿›è®­ç»ƒè¿‡ç¨‹ã€‚ç ”ç©¶åŸºäº133åå‰åˆ—è…ºç™Œæ‚£è€…çš„æ•°æ®ï¼Œä½¿ç”¨3D U-Netå’ŒUNETRæ¶æ„è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è®¡åˆ’ä¸ä¸´åºŠé‡‘æ ‡å‡†é«˜åº¦åŒ¹é…ï¼Œåœ¨é¶åŒº(PTV)è¾¾åˆ°D95% = 0.42 +/- 1.83 Gyçš„å¾®å°åå·®ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†å±åŠå™¨å®˜(OAR)çš„è¾å°„æš´éœ²ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç‰©ç†å¼•å¯¼æ·±åº¦å­¦ä¹ åœ¨ä¼˜åŒ–æ”¾å°„æ²»ç–—è§„åˆ’è‡ªåŠ¨åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19880v1",
      "published_date": "2025-06-23 19:44:56 UTC",
      "updated_date": "2025-06-23 19:44:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:48:05.705420+00:00"
    },
    {
      "arxiv_id": "2506.19072v2",
      "title": "HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models",
      "title_zh": "HAWAIIï¼šé¢å‘é«˜æ•ˆè§†è§‰-è¯­è¨€æ¨¡å‹çš„å±‚çº§åŒ–è§†è§‰çŸ¥è¯†è¿ç§»",
      "authors": [
        "Yimu Wang",
        "Mozhgan Nasr Azadani",
        "Sean Sedwards",
        "Krzysztof Czarnecki"
      ],
      "abstract": "Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII compared to popular open-source VLMs. The code is available at https://github.com/yimuwangcs/wise-hawaii.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº† HAWAII æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨æ•´åˆå¤šä¸ªé¢„è®­ç»ƒè§†è§‰ä¸“å®¶çŸ¥è¯†æ—¶é¢ä¸´çš„è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç§å±‚çº§åŒ–çš„è§†è§‰çŸ¥è¯†è½¬ç§»æœºåˆ¶ï¼Œå°†å¤šä¸ªè§†è§‰ä¸“å®¶çš„çŸ¥è¯†è’¸é¦åˆ°ä¸€ä¸ªå•ä¸€çš„è§†è§‰ç¼–ç å™¨ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¿æŒæä½è®¡ç®—å¼€é”€çš„åŒæ—¶ç»§æ‰¿å¤šä¸“å®¶çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºäº†å‡è½»ä¸åŒæ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„å†²çªå¹¶å®ç°ç‰¹å®šçŸ¥è¯†çš„çµæ´»åˆ‡æ¢ï¼ŒHAWAII é‡‡ç”¨äº†ç‰¹å®šäºæ•™å¸ˆçš„ Low-Rank Adaptation (LoRA) é€‚é…å™¨é…åˆç›¸åº”çš„è·¯ç”±å™¨ (router)ï¼Œç¡®ä¿åœ¨è’¸é¦è¿‡ç¨‹ä¸­é¿å…å™ªå£°å¼•å¯¼ã€‚åœ¨è’¸é¦æ•ˆç‡æ–¹é¢ï¼Œç ”ç©¶å¼•å…¥äº†ç»†ç²’åº¦å’Œç²—ç²’åº¦ä¸¤ä¸ªå±‚é¢çš„è®¾è®¡ï¼šç»†ç²’åº¦å±‚é¢åˆ©ç”¨ token é‡è¦æ€§è¯„åˆ†è‡ªé€‚åº”åœ°å¼ºè°ƒå„æ•™å¸ˆçš„å…³é”®ä¿¡æ¯ï¼Œç²—ç²’åº¦å±‚é¢åˆ™é€šè¿‡ä¸€ç»„é€šç”¨çŸ¥è¯† LoRA é€‚é…å™¨æ±‡æ€»å¹¶è½¬ç§»å¤šæ•™å¸ˆçŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHAWAII åœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç›®å‰æµè¡Œçš„å¼€æº VLMsï¼Œè¯æ˜äº†å…¶åœ¨æå‡æ¨¡å‹è§†è§‰ç†è§£èƒ½åŠ›å’Œæ•ˆç‡æ–¹é¢çš„å“è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.19072v2",
      "published_date": "2025-06-23 19:43:25 UTC",
      "updated_date": "2025-11-19 21:20:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:49:00.941024+00:00"
    },
    {
      "arxiv_id": "2506.19046v1",
      "title": "From Rows to Yields: How Foundation Models for Tabular Data Simplify Crop Yield Prediction",
      "title_zh": "ä»è¡Œæ•°æ®åˆ°ä½œç‰©äº§é‡ï¼šè¡¨æ ¼æ•°æ®åŸºç¡€æ¨¡å‹å¦‚ä½•ç®€åŒ–ä½œç‰©äº§é‡é¢„æµ‹",
      "authors": [
        "Filip Sabo",
        "Michele Meroni",
        "Maria Piles",
        "Martin Claverie",
        "Fanie Ferreira",
        "Elna Van Den Berg",
        "Francesco Collivignarelli",
        "Felix Rembold"
      ],
      "abstract": "We present an application of a foundation model for small- to medium-sized tabular data (TabPFN), to sub-national yield forecasting task in South Africa. TabPFN has recently demonstrated superior performance compared to traditional machine learning (ML) models in various regression and classification tasks. We used the dekadal (10-days) time series of Earth Observation (EO; FAPAR and soil moisture) and gridded weather data (air temperature, precipitation and radiation) to forecast the yield of summer crops at the sub-national level. The crop yield data was available for 23 years and for up to 8 provinces. Covariate variables for TabPFN (i.e., EO and weather) were extracted by region and aggregated at a monthly scale. We benchmarked the results of the TabPFN against six ML models and three baseline models. Leave-one-year-out cross-validation experiment setting was used in order to ensure the assessment of the models capacity to forecast an unseen year. Results showed that TabPFN and ML models exhibit comparable accuracy, outperforming the baselines. Nonetheless, TabPFN demonstrated superior practical utility due to its significantly faster tuning time and reduced requirement for feature engineering. This renders TabPFN a more viable option for real-world operation yield forecasting applications, where efficiency and ease of implementation are paramount.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹ä¸­å°å‹è¡¨æ ¼æ•°æ®çš„åŸºåº§æ¨¡å‹TabPFNåœ¨å—éæ¬¡å›½å®¶çº§ä½œç‰©äº§é‡é¢„æµ‹ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶åˆ©ç”¨åŒ…å«FAPARã€åœŸå£¤æ¹¿åº¦åœ¨å†…çš„åœ°çƒè§‚æµ‹(Earth Observation)æ•°æ®ä»¥åŠæ°”æ¸©ã€é™æ°´å’Œè¾å°„ç­‰æ°”è±¡æ•°æ®ï¼Œå¯¹é•¿è¾¾23å¹´çš„å¤å­£ä½œç‰©äº§é‡è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡ç•™ä¸€å¹´äº¤å‰éªŒè¯(Leave-one-year-out cross-validation)å®éªŒï¼Œå°†TabPFNä¸å…­ç§æœºå™¨å­¦ä¹ (ML)æ¨¡å‹åŠä¸‰ç§åŸºå‡†æ¨¡å‹è¿›è¡Œäº†æ€§èƒ½å¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼ŒTabPFNåœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ç›¸å½“ï¼Œä¸”å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ›´ä¸ºå…³é”®çš„æ˜¯ï¼ŒTabPFNåœ¨å®é™…åº”ç”¨ä¸­å±•ç°å‡ºæ›´é«˜çš„æ•ˆç”¨ï¼Œä¸»è¦ä½“ç°åœ¨å…¶æ˜¾è‘—ç¼©çŸ­çš„è°ƒä¼˜æ—¶é—´(tuning time)ä»¥åŠå¯¹ç‰¹å¾å·¥ç¨‹(feature engineering)éœ€æ±‚çš„é™ä½ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œåœ¨å¯¹æ•ˆç‡å’Œæ˜“ç”¨æ€§è¦æ±‚æé«˜çš„çœŸå®ä¸–ç•Œä½œç‰©äº§é‡é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒTabPFNæ˜¯ä¸€ç§æ›´å…·å¯è¡Œæ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19046v1",
      "published_date": "2025-06-23 19:05:56 UTC",
      "updated_date": "2025-06-23 19:05:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:48:22.628063+00:00"
    },
    {
      "arxiv_id": "2506.19037v3",
      "title": "Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models",
      "title_zh": "ä»¥é€Ÿåº¦ä¸ºç›®æ ‡çš„è§„åˆ’ï¼šæ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç©ºæ´è°ƒåº¦",
      "authors": [
        "Omer Luxembourg",
        "Haim Permuter",
        "Eliya Nachmani"
      ],
      "abstract": "Masked diffusion language models (MDLMs) promise fast, non-autoregressive text generation, yet existing samplers, which pick tokens to unmask based on model confidence, ignore interactions when unmasking multiple positions in parallel and effectively reduce to slow, autoregressive behavior. We propose the Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free method that partitions sequence positions into non-adjacent dilated groups and unmasked them in parallel so as to minimize an upper bound on joint entropy gain at each denoising step. By explicitly trading off the number of network calls against generation quality, DUS recovers most of the performance lost under traditional parallel unmasking strategies. Across math (GSM8K, MATH500), code (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS outperforms confidence-based planners, without modifying the underlying denoiser, and reveals the true speed-quality frontier of MDLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é®è”½æ‰©æ•£è¯­è¨€æ¨¡å‹(Masked diffusion language models, MDLMs)åœ¨éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆä¸­å­˜åœ¨çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸºäºç½®ä¿¡åº¦çš„é‡‡æ ·å™¨åœ¨å¹¶è¡Œå»é®è”½æ—¶å¿½ç•¥äº†tokené—´çš„äº¤äº’ï¼Œå¯¼è‡´å…¶å®é™…è¡¨ç°è¶‹å‘äºç¼“æ…¢çš„è‡ªå›å½’è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Dilated Unmasking Scheduler (DUS)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è§„åˆ’æ¨¡å‹ä¸”ä»…é™æ¨ç†é˜¶æ®µä½¿ç”¨çš„è°ƒåº¦æ–¹æ³•ã€‚DUS é€šè¿‡å°†åºåˆ—ä½ç½®åˆ’åˆ†ä¸ºéç›¸é‚»çš„ç©ºæ´åˆ†ç»„(dilated groups)å¹¶è¿›è¡Œå¹¶è¡Œå»é®è”½ï¼Œæ—¨åœ¨æœ€å°åŒ–æ¯ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­è”åˆç†µå¢çš„ä¸Šé™ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç½‘ç»œè°ƒç”¨æ¬¡æ•°ä¸ç”Ÿæˆè´¨é‡ä¹‹é—´è¿›è¡Œæ˜¾å¼æƒè¡¡ï¼Œæœ‰æ•ˆæ¢å¤äº†ä¼ ç»Ÿå¹¶è¡Œå»é®è”½ç­–ç•¥ä¸­æŸå¤±çš„æ€§èƒ½ã€‚åœ¨æ•°å­¦(GSM8K, MATH500)ã€ä»£ç (HumanEval, MBPP)åŠé€šç”¨çŸ¥è¯†(BBH, MMLU-Pro)ç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDUS åœ¨ä¸æ”¹å˜åº•å±‚å»å™ªå™¨(denoiser)çš„æƒ…å†µä¸‹æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„åŸºäºç½®ä¿¡åº¦çš„è§„åˆ’å™¨ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† MDLMs çœŸå®çš„ç”Ÿæˆé€Ÿåº¦ä¸è´¨é‡ä¹‹é—´çš„æƒè¡¡è¾¹ç•Œï¼Œä¸ºå®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19037v3",
      "published_date": "2025-06-23 18:49:23 UTC",
      "updated_date": "2025-07-24 20:58:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:48:29.622086+00:00"
    },
    {
      "arxiv_id": "2506.19028v5",
      "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
      "title_zh": "è¶…è¶Š Tokenï¼šé‡åŒ– LLM å…¬å¹³æ€§çš„è¯­ä¹‰ä¸ç»Ÿè®¡è§†è§’",
      "authors": [
        "Weijie Xu",
        "Yiwen Wang",
        "Chi Xue",
        "Xiangkun Hu",
        "Xi Fang",
        "Guimin Dong",
        "Chandan K. Reddy"
      ],
      "abstract": "Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿æ–‡æœ¬å›ç­”ä¸­å­˜åœ¨çš„å›ºæœ‰åè§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFiSCoï¼ˆFine-grained Semantic Comparisonï¼‰çš„æ–°å‹ç»Ÿè®¡æ¡†æ¶ï¼Œç”¨äºä»è¯­ä¹‰å’Œç»Ÿè®¡ç»´åº¦è¯„ä¼°ç¾¤ä½“å±‚é¢çš„å…¬å¹³æ€§ã€‚FiSCoè¶…è¶Šäº†ä¼ ç»Ÿçš„Tokençº§åˆ«æˆ–æƒ…æ„Ÿåˆ†æï¼Œé€šè¿‡å°†æ¨¡å‹è¾“å‡ºåˆ†è§£ä¸ºç‹¬ç«‹çš„Claimså¹¶åˆ©ç”¨è•´å«æ£€æŸ¥ï¼ˆEntailment Checksï¼‰æ¥è¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åº”ç”¨ç»Ÿè®¡å‡è®¾æ£€éªŒæ¯”è¾ƒç»„é—´ä¸ç»„å†…çš„ç›¸ä¼¼æ€§ï¼Œèƒ½å¤Ÿç¨³å¥åœ°æ•æ‰åˆ°ä¸åŒäººå£ç»Ÿè®¡ç¾¤ä½“é—´ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥å½¢å¼åŒ–äº†ç¾¤ä½“åäº‹å®å…¬å¹³æ€§ï¼ˆGroup Counterfactual Fairnessï¼‰çš„å®šä¹‰ï¼Œå¹¶åœ¨æ€§åˆ«ã€ç§æ—å’Œå¹´é¾„ç­‰å¤šä¸ªç»´åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFiSCoåœ¨è¯†åˆ«å¾®å¦™åè§æ–¹é¢ä¼˜äºç°æœ‰è¯„ä¼°æŒ‡æ ‡ï¼Œä¸”èƒ½æœ‰æ•ˆå‡å°‘LLMsè¾“å‡ºéšæœºæ€§å¸¦æ¥çš„å¹²æ‰°ï¼Œä¸ºé‡åŒ–é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„åè§æä¾›äº†æ›´å¯é çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 9 figures, 15 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.19028v5",
      "published_date": "2025-06-23 18:31:22 UTC",
      "updated_date": "2025-10-10 17:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:48:34.774313+00:00"
    },
    {
      "arxiv_id": "2506.19025v1",
      "title": "Statistical Inference for Optimal Transport Maps: Recent Advances and Perspectives",
      "title_zh": "æœ€ä¼˜ä¼ è¾“æ˜ å°„çš„ç»Ÿè®¡æ¨æ–­ï¼šæœ€æ–°è¿›å±•ä¸å±•æœ›",
      "authors": [
        "Sivaraman Balakrishnan",
        "Tudor Manole",
        "Larry Wasserman"
      ],
      "abstract": "In many applications of optimal transport (OT), the object of primary interest is the optimal transport map. This map rearranges mass from one probability distribution to another in the most efficient way possible by minimizing a specified cost. In this paper we review recent advances in estimating and developing limit theorems for the OT map, using samples from the underlying distributions. We also review parallel lines of work that establish similar results for special cases and variants of the basic OT setup. We conclude with a discussion of key directions for future research with the goal of providing practitioners with reliable inferential tools.",
      "tldr_zh": "æœ¬æ–‡ç»¼è¿°äº† Optimal Transport (OT) é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹è®¨è®ºäº†å¦‚ä½•ä»åˆ†å¸ƒæ ·æœ¬ä¸­ä¼°è®¡å¹¶å°†è´¨é‡ä»ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒé«˜æ•ˆç§»åŠ¨åˆ°å¦ä¸€ä¸ªåˆ†å¸ƒçš„ Optimal Transport Mapã€‚è¯¥ç ”ç©¶ç³»ç»Ÿå›é¡¾äº†å…³äº OT Map ä¼°è®¡å’Œå¼€å‘ Limit Theorems çš„å‰æ²¿æˆæœï¼Œæ—¨åœ¨ä¸ºç»Ÿè®¡æ¨æ–­æä¾›åšå®çš„ç†è®ºæ”¯æ’‘ã€‚æ–‡ç« è¿˜æ¢³ç†äº†é’ˆå¯¹åŸºæœ¬ OT è®¾ç½®çš„ç‰¹æ®Šæƒ…å†µåŠå…¶å˜ä½“çš„å¹³è¡Œç ”ç©¶å·¥ä½œï¼Œå±•ç¤ºäº†è¯¥é¢†åŸŸçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚æœ€åï¼Œä½œè€…æ¢è®¨äº†æœªæ¥ç ”ç©¶çš„å…³é”®æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºå®é™…åº”ç”¨è€…æä¾›æ›´åŠ å¯é çš„ Inferential Toolsã€‚è¿™ç§ç³»ç»Ÿæ€§çš„æ€»ç»“å¯¹äºç†è§£ OT Map çš„ç»Ÿè®¡æ€§è´¨ä»¥åŠæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å¯ä¿¡æ¨æ–­æŠ€æœ¯å‘å±•å…·æœ‰é‡è¦çš„å­¦æœ¯ä»·å€¼ã€‚",
      "categories": [
        "math.ST",
        "cs.AI",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "math.ST",
      "comment": "36 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.19025v1",
      "published_date": "2025-06-23 18:28:48 UTC",
      "updated_date": "2025-06-23 18:28:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:49:44.901027+00:00"
    },
    {
      "arxiv_id": "2506.19019v1",
      "title": "Survey of HPC in US Research Institutions",
      "title_zh": "ç¾å›½ç ”ç©¶æœºæ„é«˜æ€§èƒ½è®¡ç®—ç°çŠ¶è°ƒç ”",
      "authors": [
        "Peng Shu",
        "Junhao Chen",
        "Zhengliang Liu",
        "Huaqin Zhao",
        "Xinliang Li",
        "Tianming Liu"
      ],
      "abstract": "The rapid growth of AI, data-intensive science, and digital twin technologies has driven an unprecedented demand for high-performance computing (HPC) across the research ecosystem. While national laboratories and industrial hyperscalers have invested heavily in exascale and GPU-centric architectures, university-operated HPC systems remain comparatively under-resourced. This survey presents a comprehensive assessment of the HPC landscape across U.S. universities, benchmarking their capabilities against Department of Energy (DOE) leadership-class systems and industrial AI infrastructures. We examine over 50 premier research institutions, analyzing compute capacity, architectural design, governance models, and energy efficiency. Our findings reveal that university clusters, though vital for academic research, exhibit significantly lower growth trajectories (CAGR $\\approx$ 18%) than their national ($\\approx$ 43%) and industrial ($\\approx$ 78%) counterparts. The increasing skew toward GPU-dense AI workloads has widened the capability gap, highlighting the need for federated computing, idle-GPU harvesting, and cost-sharing models. We also identify emerging paradigms, such as decentralized reinforcement learning, as promising opportunities for democratizing AI training within campus environments. Ultimately, this work provides actionable insights for academic leaders, funding agencies, and technology partners to ensure more equitable and sustainable HPC access in support of national research priorities.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹ç¾å›½ç ”ç©¶å‹å¤§å­¦çš„é«˜æ€§èƒ½è®¡ç®—(HPC)ç°çŠ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶å°†å…¶è®¡ç®—èƒ½åŠ›ä¸ç¾å›½èƒ½æºéƒ¨(DOE)é¢†å¯¼çº§ç³»ç»ŸåŠå·¥ä¸šçº§äººå·¥æ™ºèƒ½(AI)åŸºç¡€è®¾æ–½è¿›è¡Œäº†åŸºå‡†å¯¹æ¯”ã€‚è¯¥è°ƒæŸ¥æ¶µç›–äº†è¶…è¿‡50æ‰€é¡¶çº§ç ”ç©¶æœºæ„ï¼Œæ·±å…¥åˆ†æäº†å…¶ç®—åŠ›è§„æ¨¡ã€æ¶æ„è®¾è®¡ã€æ²»ç†æ¨¡å¼åŠèƒ½æºæ•ˆç‡ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å­¦é›†ç¾¤çš„å¤åˆå¹´å‡å¢é•¿ç‡(CAGR)ä»…çº¦ä¸º18%ï¼Œæ˜¾è‘—ä½äºå›½å®¶çº§(çº¦43%)å’Œå·¥ä¸šçº§(çº¦78%)ç³»ç»Ÿï¼Œä¸”éšç€AIå·¥ä½œè´Ÿè½½å‘GPUå¯†é›†å‹è½¬å˜ï¼Œè¿™ç§èƒ½åŠ›å·®è·æ­£ä¸æ–­æ‰©å¤§ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†è”é‚¦è®¡ç®—(Federated Computing)ã€é—²ç½®GPUæ”¶å‰²(Idle-GPU Harvesting)å’Œæˆæœ¬åˆ†æ‹…æ¨¡å‹ç­‰ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ (Decentralized Reinforcement Learning)ç­‰æ–°å…´èŒƒå¼æ˜¯æ ¡å›­ç¯å¢ƒä¸‹å®ç°AIè®­ç»ƒæ°‘ä¸»åŒ–çš„æ½œåœ¨æœºä¼šã€‚è¯¥å·¥ä½œä¸ºå­¦æœ¯é¢†å¯¼è€…å’Œèµ„åŠ©æœºæ„æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œæ—¨åœ¨ç¡®ä¿HPCèµ„æºåœ¨æ”¯æŒå›½å®¶ç ”ç©¶ä¼˜å…ˆäº‹é¡¹æ—¶èƒ½å¤Ÿå®ç°æ›´å…¬å¹³ä¸”å¯æŒç»­çš„æ¥å…¥ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.19019v1",
      "published_date": "2025-06-23 18:13:36 UTC",
      "updated_date": "2025-06-23 18:13:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:49:59.645853+00:00"
    },
    {
      "arxiv_id": "2506.19014v2",
      "title": "IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection",
      "title_zh": "IndieFake æ•°æ®é›†ï¼šéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Abhay Kumar",
        "Kunal Verma",
        "Omkar More"
      ],
      "abstract": "Advancements in audio deepfake technology offers benefits like AI assistants, better accessibility for speech impairments, and enhanced entertainment. However, it also poses significant risks to security, privacy, and trust in digital communications. Detecting and mitigating these threats requires comprehensive datasets. Existing datasets lack diverse ethnic accents, making them inadequate for many real-world scenarios. Consequently, models trained on these datasets struggle to detect audio deepfakes in diverse linguistic and cultural contexts such as in South-Asian countries. Ironically, there is a stark lack of South-Asian speaker samples in the existing datasets despite constituting a quarter of the worlds population. This work introduces the IndieFake Dataset (IFD), featuring 27.17 hours of bonafide and deepfake audio from 50 English speaking Indian speakers. IFD offers balanced data distribution and includes speaker-level characterization, absent in datasets like ASVspoof21 (DF). We evaluated various baselines on IFD against existing ASVspoof21 (DF) and In-The-Wild (ITW) datasets. IFD outperforms ASVspoof21 (DF) and proves to be more challenging compared to benchmark ITW dataset. The complete dataset, along with documentation and sample reference clips, is publicly accessible for research use on project website.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ•°æ®é›†ç¼ºä¹æ—ç¾¤å£éŸ³å¤šæ ·æ€§ï¼ˆå°¤å…¶æ˜¯å—äºšå£éŸ³ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†IndieFake Dataset (IFD)åŸºå‡†æ•°æ®é›†ã€‚IFDåŒ…å«50åè®²è‹±è¯­çš„å°åº¦è¯´è¯äººï¼Œå…±è®¡27.17å°æ—¶çš„çœŸå®(bonafide)ä¸æ·±åº¦ä¼ªé€ (deepfake)éŸ³é¢‘ã€‚ç›¸è¾ƒäºASVspoof21 (DF)ç­‰ç°æœ‰æ•°æ®é›†ï¼ŒIFDæä¾›äº†æ›´å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒä»¥åŠå…³é”®çš„è¯´è¯äººçº§åˆ«ç‰¹å¾æè¿°ã€‚é€šè¿‡åœ¨IFDã€ASVspoof21 (DF)å’ŒIn-The-Wild (ITW)æ•°æ®é›†ä¸Šå¯¹å¤šç§åŸºçº¿æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶è¯æ˜IFDæ¯”ITWæ›´å…·æŒ‘æˆ˜æ€§ï¼Œä¸”åœ¨æ£€æµ‹æ•ˆèƒ½ä¸Šä¼˜äºASVspoof21 (DF)ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€ç”¨äºç ”ç©¶ï¼Œæ—¨åœ¨æå‡å…¨çƒå¤šå…ƒè¯­è¨€ä¸æ–‡åŒ–èƒŒæ™¯ä¸‹éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹(Audio Deepfake Detection)æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Project Website: https://indie-fake-dataset.netlify.app/",
      "pdf_url": "https://arxiv.org/pdf/2506.19014v2",
      "published_date": "2025-06-23 18:10:06 UTC",
      "updated_date": "2025-06-26 17:21:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:02.022295+00:00"
    },
    {
      "arxiv_id": "2506.18985v3",
      "title": "GLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models",
      "title_zh": "GLIMPSEï¼šå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨æ–¹ä½è·¨æ¨¡æ€å¯è§£é‡Šæ€§",
      "authors": [
        "Guanxi Shen"
      ],
      "abstract": "Recent large vision-language models (LVLMs) have advanced capabilities in visual question answering (VQA). However, interpreting where LVLMs direct their visual attention remains a significant challenge, yet is essential for understanding model behavior. We introduce GLIMPSE (Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation), a lightweight, model-agnostic framework that jointly attributes LVLM outputs to the most relevant visual evidence and textual signals that support open-ended generation. GLIMPSE fuses gradient-weighted attention, adaptive layer propagation, and relevance-weighted token aggregation to produce holistic response-level heat maps for interpreting cross-modal reasoning, outperforming prior methods in faithfulness and pushing the state-of-the-art in human-attention alignment. We demonstrate an analytic approach to uncover fine-grained insights into LVLM cross-modal attribution, trace reasoning dynamics, analyze systematic misalignment, diagnose hallucination and bias, and ensure transparency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GLIMPSE (Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation)ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”ä¸æ¨¡å‹æ— å…³ (model-agnostic) çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡èåˆæ¢¯åº¦åŠ æƒæ³¨æ„åŠ› (gradient-weighted attention)ã€è‡ªé€‚åº”å±‚ä¼ æ’­ (adaptive layer propagation) å’Œç›¸å…³æ€§åŠ æƒæ ‡è®°èšåˆ (relevance-weighted token aggregation)ï¼Œç”Ÿæˆæ•´ä½“å“åº”çº§çš„çƒ­å›¾ (response-level heat maps) ä»¥è§£é‡Šè·¨æ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGLIMPSE åœ¨å¿ å®åº¦ (faithfulness) æ–¹é¢è¡¨ç°å“è¶Šï¼Œå¹¶åœ¨äººç±»æ³¨æ„åŠ›å¯¹é½ (human-attention alignment) æ–¹é¢è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³ã€‚é€šè¿‡åˆ†æç»†ç²’åº¦çš„è·¨æ¨¡æ€å½’å› ï¼Œè¯¥æ–¹æ³•ä¸ºè¿½è¸ªæ¨ç†åŠ¨æ€ã€è¯Šæ–­å¹»è§‰ (hallucination) ä¸åå·® (bias) ä»¥åŠç¡®ä¿æ¨¡å‹é€æ˜åº¦æä¾›äº†æœ‰æ•ˆçš„åˆ†ææ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Keywords: Explainable Computer Vision, Large Vision-Language Models, AI Interpretability, Explainable AI, Visual Saliency, Attribution Maps, Cross-Modal Attribution, Human Attention Alignment, AI Transparency",
      "pdf_url": "https://arxiv.org/pdf/2506.18985v3",
      "published_date": "2025-06-23 18:00:04 UTC",
      "updated_date": "2025-07-29 17:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:01.329210+00:00"
    },
    {
      "arxiv_id": "2506.18902v3",
      "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
      "title_zh": "jina-embeddings-v4ï¼šé¢å‘å¤šæ¨¡æ€å¤šè¯­è¨€æ£€ç´¢çš„é€šç”¨åµŒå…¥",
      "authors": [
        "Michael GÃ¼nther",
        "Saba Sturua",
        "Mohammad Kalim Akram",
        "Isabelle Mohr",
        "Andrei Ungureanu",
        "Bo Wang",
        "Sedigheh Eslami",
        "Scott Martens",
        "Maximilian Werk",
        "Nan Wang",
        "Han Xiao"
      ],
      "abstract": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† jina-embeddings-v4ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰38äº¿å‚æ•°çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡åˆ›æ–°çš„æ¶æ„ç»Ÿä¸€æ–‡æœ¬å’Œå›¾åƒçš„è¡¨ç¤ºã€‚è¯¥æ¨¡å‹åŒæ—¶æ”¯æŒå•å‘é‡ (single-vector) å’ŒåæœŸäº¤äº’å¼ (late interaction) çš„å¤šå‘é‡ (multi-vector) åµŒå…¥ï¼Œå¹¶å¼•å…¥äº†ä»»åŠ¡ç‰¹å®šçš„ Low-Rank Adaptation (LoRA) é€‚é…å™¨ï¼Œä»¥ä¼˜åŒ–æŸ¥è¯¢-æ–‡æ¡£æ£€ç´¢ã€è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦å’Œä»£ç æœç´¢ç­‰ä¸åŒåœºæ™¯çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œjina-embeddings-v4 åœ¨å•æ¨¡æ€å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†è¡¨æ ¼ã€å›¾è¡¨ã€æµç¨‹å›¾å’Œæ··åˆåª’ä½“æ ¼å¼ç­‰è§†è§‰ä¿¡æ¯ä¸°å¯Œçš„å†…å®¹æ—¶è¡¨ç°çªå‡ºã€‚ä¸ºäº†æ”¯æŒæ­¤ç±»èƒ½åŠ›çš„è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹å¯Œè§†è§‰å›¾åƒæ£€ç´¢çš„æ–°å‹åŸºå‡†æµ‹è¯• Jina-VDRã€‚è¯¥å·¥ä½œä¸ºå®ç°é€šç”¨çš„å¤šæ¨¡æ€å¤šè¯­è¨€æ£€ç´¢æä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables",
      "pdf_url": "https://arxiv.org/pdf/2506.18902v3",
      "published_date": "2025-06-23 17:59:55 UTC",
      "updated_date": "2025-07-07 17:41:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:49:55.474528+00:00"
    },
    {
      "arxiv_id": "2506.18898v1",
      "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
      "title_zh": "è§†è§‰å³æ–¹è¨€ï¼šé€šè¿‡æ–‡æœ¬å¯¹é½è¡¨å¾ç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆ",
      "authors": [
        "Jiaming Han",
        "Hao Chen",
        "Yang Zhao",
        "Hanyu Wang",
        "Qi Zhao",
        "Ziyan Yang",
        "Hao He",
        "Xiangyu Yue",
        "Lu Jiang"
      ],
      "abstract": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Tarï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡å…±äº«ç¦»æ•£è¯­ä¹‰è¡¨ç¤ºç»Ÿä¸€è§†è§‰ç†è§£ä¸ç”Ÿæˆçš„å¤šæ¨¡æ€æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ Text-Aligned Tokenizer (TA-Tok)ï¼Œé€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„è¯æ±‡è¡¨æŠ•å½±ä¸ºæ–‡æœ¬å¯¹é½ç æœ¬ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºç¦»æ•£æ ‡è®°ï¼Œä»è€Œåœ¨ç»Ÿä¸€ç©ºé—´å†…å®ç°æ— éœ€ç‰¹å®šæ¨¡æ€è®¾è®¡çš„è·¨æ¨¡æ€è¾“å…¥è¾“å‡ºã€‚ä¸ºäº†å¹³è¡¡æ•ˆç‡ä¸è§†è§‰ç»†èŠ‚ï¼ŒTar å¼•å…¥äº†å°ºåº¦è‡ªé€‚åº”ç¼–ç è§£ç æŠ€æœ¯ï¼Œå¹¶é…ç½®äº†è‡ªå›å½’ä¸æ‰©æ•£æ¨¡å‹ä¸¤ç±»äº’è¡¥çš„å»åˆ†è¯å™¨ (de-tokenizers) ä»¥ç”Ÿæˆé«˜ä¿çœŸè§†è§‰è¾“å‡ºã€‚é€šè¿‡å…ˆè¿›çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†æ¨¡æ€èåˆæ•ˆæœï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒTar åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€ LLM æ–¹æ³•ï¼Œä¸ºè§†è§‰ä¸è¯­è¨€çš„æ·±åº¦ç»Ÿä¸€æä¾›äº†é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://tar.csuhan.com",
      "pdf_url": "https://arxiv.org/pdf/2506.18898v1",
      "published_date": "2025-06-23 17:59:14 UTC",
      "updated_date": "2025-06-23 17:59:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:49:56.837720+00:00"
    },
    {
      "arxiv_id": "2506.18897v2",
      "title": "MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis",
      "title_zh": "MinDï¼šé¢å‘å®æ—¶è§„åˆ’ä¸éšå¼é£é™©åˆ†æçš„åŒç³»ç»Ÿä¸–ç•Œæ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Xiaowei Chi",
        "Kuangzhi Ge",
        "Jiaming Liu",
        "Siyuan Zhou",
        "Peidong Jia",
        "Zichen He",
        "Yuzhen Liu",
        "Tingguang Li",
        "Lei Han",
        "Sirui Han",
        "Shanghang Zhang",
        "Yike Guo"
      ],
      "abstract": "Video Generation Models (VGMs) have become powerful backbones for Vision-Language-Action (VLA) models, leveraging large-scale pretraining for robust dynamics modeling. However, current methods underutilize their distribution modeling capabilities for predicting future states. Two challenges hinder progress: integrating generative processes into feature learning is both technically and conceptually underdeveloped, and naive frame-by-frame video diffusion is computationally inefficient for real-time robotics. To address these, we propose Manipulate in Dream (MinD), a dual-system world model for real-time, risk-aware planning. MinD uses two asynchronous diffusion processes: a low-frequency visual generator (LoDiff) that predicts future scenes and a high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight is that robotic policies do not require fully denoised frames but can rely on low-resolution latents generated in a single denoising step. To connect early predictions to actions, we introduce DiffMatcher, a video-action alignment module with a novel co-training strategy that synchronizes the two diffusion models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step latent features for control signals. Furthermore, MinD identifies 74% of potential task failures in advance, providing real-time safety signals for monitoring and intervention. This work establishes a new paradigm for efficient and reliable robotic manipulation using generative world models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Manipulate in Dream (MinD)ï¼Œä¸€ç§ä¸ºå®æ—¶ã€å…·å¤‡é£é™©æ„ŸçŸ¥èƒ½åŠ›çš„è§„åˆ’è®¾è®¡çš„åŒç³»ç»Ÿ World Modelã€‚é’ˆå¯¹ç°æœ‰ Video Generation Models (VGMs) åœ¨ç‰¹å¾å­¦ä¹ é›†æˆå’Œå®æ—¶æœºå™¨äººè®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒMinD é‡‡ç”¨äº†ä¸¤ä¸ªå¼‚æ­¥çš„æ‰©æ•£è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªé¢„æµ‹æœªæ¥åœºæ™¯çš„ä½é¢‘è§†è§‰ç”Ÿæˆå™¨ (LoDiff) å’Œä¸€ä¸ªè¾“å‡ºåŠ¨ä½œçš„é«˜é¢‘æ‰©æ•£ç­–ç•¥ (HiDiff)ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå‘ç°æœºå™¨äººç­–ç•¥å¯ä»¥ä¾èµ–å•æ­¥å»å™ªäº§ç”Ÿçš„ä½åˆ†è¾¨ç‡ Latents æ¥è·å–æ§åˆ¶ä¿¡å·ï¼Œä»è€Œæ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚ä¸ºäº†å®ç°é¢„æµ‹ä¸åŠ¨ä½œçš„åŒæ­¥ï¼Œç ”ç©¶å¼•å…¥äº† DiffMatcher æ¨¡å—ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„ååŒè®­ç»ƒç­–ç•¥å¯¹é½ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMinD åœ¨ RL-Bench å’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­åˆ†åˆ«å–å¾—äº† 63% å’Œ 60% çš„æˆåŠŸç‡ï¼Œä¸”è¿è¡Œé€Ÿåº¦è¾¾åˆ° 11.3 FPSã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæå‰è¯†åˆ« 74% çš„æ½œåœ¨ä»»åŠ¡å¤±è´¥ï¼Œä¸ºæœºå™¨äººæ“ä½œæä¾›äº†å®æ—¶çš„å®‰å…¨ç›‘æ§ä¿¡å·ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨ç”Ÿæˆå¼ World Model å®ç°é«˜æ•ˆã€å¯é çš„æœºå™¨äººæ“æ§å»ºç«‹äº†ä¸€ç§æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18897v2",
      "published_date": "2025-06-23 17:59:06 UTC",
      "updated_date": "2025-08-20 07:07:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:14.560719+00:00"
    },
    {
      "arxiv_id": "2506.18887v1",
      "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
      "title_zh": "åŸºäº Transformer éšå­ç©ºé—´æ¿€æ´»çš„æ¦‚å¿µåç½®å¼•å¯¼",
      "authors": [
        "Vansh Sharma",
        "Venkat Raman"
      ],
      "abstract": "This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡æ¿€æ´»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ½œåœ¨å­ç©ºé—´ï¼ˆlatent subspacesï¼‰ï¼Œå¼•å¯¼ç§‘å­¦ä»£ç ç”Ÿæˆåå‘ç‰¹å®šç¼–ç¨‹è¯­è¨€çš„æ–¹æ³•ã€‚ç ”ç©¶äººå‘˜é¦–å…ˆè¯„ä¼°äº†äº”ç§å› æœè¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦ç¼–ç¨‹æç¤ºè¯ä¸‹çš„åŸºçº¿åå‘ï¼Œå‘ç°ä¼ ç»Ÿçš„é™æ€ç¥ç»å…ƒå½’å› æ–¹æ³•ï¼ˆneuron-attributionï¼‰åœ¨è·¨æç¤ºé£æ ¼å’Œæ¨¡å‹è§„æ¨¡æ—¶å…·æœ‰å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§æ¢¯åº¦ç»†åŒ–çš„è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼æ¡†æ¶ï¼ˆG-ACTï¼‰ï¼Œé€šè¿‡å°†æç¤ºè¯æ¿€æ´»å·®å¼‚èšç±»ä¸ºå¼•å¯¼æ–¹å‘ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§é€å±‚æ¢é’ˆï¼ˆprobesï¼‰è¿›è¡Œåœ¨çº¿å¾®è°ƒä»¥é€‰æ‹©åˆé€‚çš„å¼•å¯¼å‘é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ LLaMA-3.2 3B æ¨¡å‹ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”æ ‡å‡† ACT æ¡†æ¶æ˜¾è‘—æå‡äº†å¯¹ CPP è¯­è¨€çš„å¼•å¯¼æ•ˆæœï¼Œåœ¨æµ…å±‚ç½‘ç»œçš„æ¢é’ˆåˆ†ç±»å‡†ç¡®ç‡æå‡è¾¾ 61.5%ã€‚å¯¹äºè§„æ¨¡æ›´å¤§çš„ LLaMA-3.3 70B æ¨¡å‹ï¼Œé’ˆå¯¹å…³é”®å±‚çš„æ³¨å…¥åŒæ ·æœ‰æ•ˆæ”¹å–„äº†è¯­è¨€é€‰æ‹©ã€‚è¯¥æ–¹æ³•ä¸ºå®é™…æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ä¸€ç§å¯æ‰©å±•ã€å¯è§£é‡Šä¸”é«˜æ•ˆçš„æ¦‚å¿µçº§æ§åˆ¶æœºåˆ¶ï¼Œåœ¨ä¿æŒæ¨ç†å¼€é”€è¾ƒä½çš„åŒæ—¶å®ç°äº†å¯é‡å¤çš„æ¨¡å‹è¡Œä¸ºã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18887v1",
      "published_date": "2025-06-23 17:56:34 UTC",
      "updated_date": "2025-06-23 17:56:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:19.275140+00:00"
    },
    {
      "arxiv_id": "2506.18880v1",
      "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization",
      "title_zh": "OMEGAï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦çªç ´æ•°å­¦æ€ç»´å®šå¼ï¼Ÿè¯„ä¼°æ¢ç´¢æ€§ã€ç»„åˆæ€§ä¸å˜é©æ€§æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Yiyou Sun",
        "Shawn Hu",
        "Georgia Zhou",
        "Ken Zheng",
        "Hannaneh Hajishirzi",
        "Nouha Dziri",
        "Dawn Song"
      ],
      "abstract": "Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OMEGAï¼Œè¿™æ˜¯ä¸€ä¸ªå—Bodenåˆ›é€ åŠ›åˆ†ç±»æ³•å¯å‘çš„å—æ§ä¸”å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦é¢†åŸŸçš„ç¦»åˆ†å¸ƒï¼ˆOut-of-distributionï¼‰æ³›åŒ–èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡ç¨‹åºåŒ–ç”Ÿæˆçš„å‡ ä½•ã€æ•°è®ºã€ä»£æ•°ç­‰é¢†åŸŸçš„é¢˜ç›®ï¼Œä»æ¢ç´¢æ€§ï¼ˆExploratoryï¼‰ã€ç»„åˆæ€§ï¼ˆCompositionalï¼‰å’Œå˜é©æ€§ï¼ˆTransformativeï¼‰ä¸‰ä¸ªç»´åº¦è€ƒå¯Ÿæ¨¡å‹çš„åˆ›æ–°æ€è€ƒèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œé¡¶å°–çš„LLMsåœ¨é¢˜ç›®å¤æ‚åº¦å¢åŠ æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½è¡°é€€ã€‚é€šè¿‡å¯¹Qwenç³»åˆ—æ¨¡å‹è¿›è¡Œå…¨ç»´åº¦å¾®è°ƒï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨æ¢ç´¢æ€§æ³›åŒ–ä¸Šè¿›æ­¥æ˜¾è‘—ï¼Œä½†åœ¨ç»„åˆæ€§æ³›åŒ–ä¸Šæå‡æœ‰é™ï¼Œè€Œåœ¨å˜é©æ€§æ¨ç†æ–¹é¢å‡ ä¹æ²¡æœ‰æ”¹è¿›ã€‚OMEGAé€šè¿‡é‡åŒ–è¿™äº›ç»†ç²’åº¦çš„å¤±æ•ˆæ¨¡å¼ï¼Œä¸ºæ¨åŠ¨LLMså®ç°çœŸæ­£çš„æ•°å­¦åˆ›é€ åŠ›è€Œéå•çº¯çš„æœºæ¢°å¼è§£é¢˜æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18880v1",
      "published_date": "2025-06-23 17:51:40 UTC",
      "updated_date": "2025-06-23 17:51:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:16.779887+00:00"
    },
    {
      "arxiv_id": "2506.18879v1",
      "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
      "title_zh": "CommVQï¼šé¢å‘ KV ç¼“å­˜å‹ç¼©çš„äº¤æ¢å¼çŸ¢é‡é‡åŒ–",
      "authors": [
        "Junyan Li",
        "Yang Zhang",
        "Muhammad Yusuf Hassan",
        "Talha Chafekar",
        "Tianle Cai",
        "Zhile Ren",
        "Pengsheng Guo",
        "Foroozan Karimzadeh",
        "Colorado Reed",
        "Chong Wang",
        "Chuang Gan"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Commutative Vector Quantization (CommVQ)ï¼Œæ—¨åœ¨æ˜¾è‘—å‡å°‘é•¿æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†è¿‡ç¨‹ä¸­çš„KV cacheå†…å­˜å ç”¨ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡è½»é‡çº§ç¼–ç å™¨å’Œç æœ¬å¼•å…¥åŠ æ€§é‡åŒ–(additive quantization)æ¥å‹ç¼©KV cacheï¼Œå¹¶æ”¯æŒé€šè¿‡ç®€å•çš„çŸ©é˜µä¹˜æ³•è¿›è¡Œè§£ç ã€‚ä¸ºäº†é™ä½è§£ç æ—¶çš„è®¡ç®—å¼€é”€ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸æ—‹è½¬ä½ç½®åµŒå…¥(RoPE)å…·æœ‰äº¤æ¢æ€§çš„ç æœ¬ï¼Œå¹¶é‡‡ç”¨æœŸæœ›æœ€å¤§åŒ–(EM)ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°äº†è§£ç ä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention)çš„é«˜æ•ˆé›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨2-bité‡åŒ–ä¸‹å¯å°†FP16 KV cacheè§„æ¨¡ç¼©å‡87.5%ï¼Œä¸”æ€§èƒ½ä¼˜äºç°æœ‰çš„å…ˆè¿›é‡åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒCommVQè¿˜å®ç°äº†æä½ç²¾åº¦æŸè€—çš„1-bité‡åŒ–ï¼ŒæˆåŠŸä½¿LLaMA-3.1 8Bæ¨¡å‹åœ¨å•å¼ RTX 4090æ˜¾å¡ä¸Šè¿è¡Œ128Kä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä¸ºé•¿æ–‡æœ¬æ¨ç†æä¾›äº†å“è¶Šçš„å†…å­˜æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025 poster",
      "pdf_url": "https://arxiv.org/pdf/2506.18879v1",
      "published_date": "2025-06-23 17:50:11 UTC",
      "updated_date": "2025-06-23 17:50:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:25.032187+00:00"
    },
    {
      "arxiv_id": "2506.22485v1",
      "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents",
      "title_zh": "AI Agents-as-Judgeï¼šä¼ä¸šæ–‡æ¡£å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ã€å®Œæ•´æ€§åŠæ¸…æ™°åº¦çš„è‡ªåŠ¨åŒ–è¯„ä¼°",
      "authors": [
        "Sudip Dasgupta",
        "Himanshu Shankar"
      ],
      "abstract": "This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“(multi-agent)ç³»ç»Ÿï¼Œæ—¨åœ¨åˆ©ç”¨AI Agentå¯¹é«˜åº¦ç»“æ„åŒ–çš„ä¼ä¸šä¸šåŠ¡æ–‡æ¡£è¿›è¡Œè‡ªåŠ¨åŒ–è¯„å®¡ã€‚è¯¥æ¡†æ¶æ•´åˆäº†LangChainã€CrewAIã€TruLenså’ŒGuidanceç­‰ç°ä»£ç¼–æ’å·¥å…·ï¼Œå®ç°äº†å¯¹æ–‡æ¡£å‡†ç¡®æ€§(accuracy)ã€ä¸€è‡´æ€§(consistency)ã€å®Œæ•´æ€§(completeness)å’Œæ¸…æ™°åº¦(clarity)çš„é€èŠ‚è¯„ä¼°ã€‚ç³»ç»Ÿé€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“è´Ÿè´£æ¨¡æ¿åˆè§„æ€§æˆ–äº‹å®æ­£ç¡®æ€§ç­‰ç¦»æ•£è¯„å®¡æ ‡å‡†ï¼Œå¹¶å°†ç»“æœè¾“å‡ºä¸ºæ ‡å‡†åŒ–çš„æœºå™¨å¯è¯»æ¨¡å¼ä»¥æ”¯æŒåç»­åˆ†æä¸å®¡è®¡ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿¡æ¯ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°99%ï¼Œè¿œè¶…äººç±»çš„92%ï¼Œå¹¶å°†å¹³å‡è¯„å®¡æ—¶é—´ä»30åˆ†é’Ÿå¤§å¹…ç¼©çŸ­è‡³2.5åˆ†é’Ÿã€‚AIä¸ä¸“å®¶è¯„å®¡æ„è§çš„ä¸€è‡´æ€§é«˜è¾¾95%ï¼Œä¸”æœ‰æ•ˆé™ä½äº†ä¸€åŠçš„é”™è¯¯ç‡å’Œåè§ã€‚å°½ç®¡åœ¨ç‰¹å®šé¢†åŸŸä»éœ€äººå·¥ç›‘ç£ä¸”é¢ä¸´LLMè¿è¥æˆæœ¬æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶ä¸ºä¼ä¸šçº§æ–‡æ¡£è´¨é‡ä¿è¯æä¾›äº†ä¸€ä¸ªçµæ´»ã€å¯å®¡è®¡ä¸”å¯æ‰©å±•çš„åŸºç¡€æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication",
      "pdf_url": "https://arxiv.org/pdf/2506.22485v1",
      "published_date": "2025-06-23 17:46:15 UTC",
      "updated_date": "2025-06-23 17:46:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:41.948879+00:00"
    },
    {
      "arxiv_id": "2506.18871v3",
      "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
      "title_zh": "OmniGen2ï¼šé¢å‘é«˜çº§å¤šæ¨¡æ€ç”Ÿæˆçš„æ¢ç´¢",
      "authors": [
        "Chenyuan Wu",
        "Pengfei Zheng",
        "Ruiran Yan",
        "Shitao Xiao",
        "Xin Luo",
        "Yueze Wang",
        "Wanli Li",
        "Xiyan Jiang",
        "Yexin Liu",
        "Junjie Zhou",
        "Ze Liu",
        "Ziyi Xia",
        "Chaofan Li",
        "Haoge Deng",
        "Jiahao Wang",
        "Kun Luo",
        "Bo Zhang",
        "Defu Lian",
        "Xinlong Wang",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Zheng Liu"
      ],
      "abstract": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†OmniGen2ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼€æºçš„é€šç”¨ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºæ–‡æœ¬ç”Ÿæˆå›¾åƒ(text-to-image)ã€å›¾åƒç¼–è¾‘(image editing)å’Œä¸Šä¸‹æ–‡ç”Ÿæˆ(in-context generation)ç­‰å¤šæ ·åŒ–ä»»åŠ¡æä¾›ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚ä¸å‰ä¸€ç‰ˆæœ¬ä¸åŒï¼ŒOmniGen2é‡‡ç”¨äº†é’ˆå¯¹æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„åŒé‡è§£ç è·¯å¾„(distinct decoding pathways)ï¼Œé€šè¿‡éå…±äº«å‚æ•°å’Œè§£è€¦çš„å›¾åƒåˆ†è¯å™¨(decoupled image tokenizer)å®ç°äº†é«˜æ•ˆç”Ÿæˆã€‚è¿™ç§è®¾è®¡ä½¿å…¶èƒ½å¤Ÿç›´æ¥æ„å»ºåœ¨ç°æœ‰çš„å¤šæ¨¡æ€ç†è§£æ¨¡å‹ä¹‹ä¸Šï¼Œæ— éœ€é‡æ–°é€‚é…VAEè¾“å…¥ï¼Œä»è€Œæœ‰æ•ˆä¿ç•™äº†åŸæœ‰çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†ä¼˜åŒ–è®­ç»ƒæ•ˆæœï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†å…¨é¢çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œå¹¶å¼•å…¥äº†ä¸“ä¸ºå›¾åƒç”Ÿæˆè®¾è®¡çš„åå°„æœºåˆ¶(reflection mechanism)åŠç›¸åº”æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å‚æ•°è§„æ¨¡é€‚ä¸­ï¼ŒOmniGen2åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šå‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºäº†æ–°åŸºå‡†OmniContextï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨å—è¯•è€…é©±åŠ¨çš„ä¸€è‡´æ€§(consistency)æ–¹é¢è¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18871v3",
      "published_date": "2025-06-23 17:38:54 UTC",
      "updated_date": "2025-09-27 07:42:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:00.842266+00:00"
    },
    {
      "arxiv_id": "2506.18866v1",
      "title": "OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation",
      "title_zh": "OmniAvatarï¼šæ”¯æŒè‡ªé€‚åº”è‚¢ä½“åŠ¨ç”»çš„é«˜æ•ˆéŸ³é¢‘é©±åŠ¨æ•°å­—äººè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Qijun Gan",
        "Ruizi Yang",
        "Jianke Zhu",
        "Shaofei Xue",
        "Steven Hoi"
      ],
      "abstract": "Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OmniAvatarï¼Œä¸€ç§åˆ›æ–°çš„éŸ³é¢‘é©±åŠ¨å…¨èº«è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­äºé¢éƒ¨åŠ¨ä½œè€Œç¼ºä¹å…¨èº«è‡ªç„¶åŒæ­¥å’Œç»†ç²’åº¦ç”Ÿæˆæ§åˆ¶çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†åƒç´ çº§å¤šå±‚æ¬¡éŸ³é¢‘åµŒå…¥ç­–ç•¥(pixel-wise multi-hierarchical audio embedding strategy)ï¼Œé€šè¿‡åœ¨æ½œç©ºé—´ä¸­æ›´ç²¾å‡†åœ°æ•æ‰éŸ³é¢‘ç‰¹å¾ï¼Œæ˜¾è‘—å¢å¼ºäº†ä¸åŒåœºæ™¯ä¸‹çš„å£å‹åŒæ­¥(lip-sync)ç²¾åº¦ã€‚ä¸ºäº†åœ¨æœ‰æ•ˆèåˆéŸ³é¢‘ç‰¹å¾çš„åŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹å¯¹æç¤ºè¯çš„æ§åˆ¶èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†åŸºäºLoRAçš„è®­ç»ƒæ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniAvataråœ¨é¢éƒ¨å’ŒåŠèº«è§†é¢‘ç”Ÿæˆè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶æ”¯æŒåœ¨æ’­å®¢ã€äººç±»äº¤äº’ã€åŠ¨æ€åœºæ™¯å’Œæ­Œå”±ç­‰å¤šç§åº”ç”¨é¢†åŸŸä¸­å®ç°ç²¾ç¡®çš„æ–‡æœ¬é©±åŠ¨ç”Ÿæˆã€‚è¯¥ç ”ç©¶ä¸ºåˆ›å»ºå…·æœ‰è‡ªç„¶è‚¢ä½“åŠ¨ç”»å’Œé«˜åº¦å¯æ§æ€§çš„æ•°å­—äººè§†é¢‘æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://omni-avatar.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.18866v1",
      "published_date": "2025-06-23 17:33:03 UTC",
      "updated_date": "2025-06-23 17:33:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:47.815120+00:00"
    },
    {
      "arxiv_id": "2506.18862v2",
      "title": "TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting",
      "title_zh": "TAMMsï¼šé¢å‘å«æ˜Ÿå›¾åƒå˜åŒ–ç†è§£ä¸é¢„æµ‹çš„æ—¶åºæ„ŸçŸ¥å¤šæ¨¡æ€æ¨¡å‹",
      "authors": [
        "Zhongbin Guo",
        "Yuhao Wang",
        "Ping Jian",
        "Chengzhi Li",
        "Xinyue Chen",
        "Zhen Yang",
        "Ertai E"
      ],
      "abstract": "Temporal Change Description (TCD) and Future Satellite Image Forecasting (FSIF) are critical, yet historically disjointed tasks in Satellite Image Time Series (SITS) analysis. Both are fundamentally limited by the common challenge of modeling long-range temporal dynamics. To explore how to improve the performance of methods on both tasks simultaneously by enhancing long-range temporal understanding capabilities, we introduce TAMMs, the first unified framework designed to jointly perform TCD and FSIF within a single MLLM-diffusion architecture. TAMMs introduces two key innovations: Temporal Adaptation Modules (TAM) enhance frozen MLLM's ability to comprehend long-range dynamics, and Semantic-Fused Control Injection (SFCI) mechanism translates this change understanding into fine-grained generative control. This synergistic design makes the understanding from the TCD task to directly inform and improve the consistency of the FSIF task. Extensive experiments demonstrate TAMMs significantly outperforms state-of-the-art specialist baselines on both tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ— (Satellite Image Time Series, SITS) åˆ†æä¸­æ—¶é—´å˜åŒ–æè¿° (Temporal Change Description, TCD) ä¸æœªæ¥å«æ˜Ÿå›¾åƒé¢„æµ‹ (Future Satellite Image Forecasting, FSIF) é•¿æœŸå¤„äºåˆ†ç¦»çŠ¶æ€ä¸”éš¾ä»¥å»ºæ¨¡é•¿ç¨‹æ—¶é—´åŠ¨æ€çš„é—®é¢˜ï¼Œæå‡ºäº† TAMMs ç»Ÿä¸€æ¡†æ¶ã€‚è¿™æ˜¯é¦–ä¸ªåœ¨å•ä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹-æ‰©æ•£æ¨¡å‹ (MLLM-diffusion) æ¶æ„ä¸­åŒæ—¶æ‰§è¡Œ TCD å’Œ FSIF ä»»åŠ¡çš„æ¨¡å‹ã€‚æ¨¡å‹å¼•å…¥äº†æ—¶é—´è‡ªé€‚åº”æ¨¡å— (Temporal Adaptation Modules, TAM) ä»¥å¢å¼ºå†»ç»“çš„ MLLM å¯¹é•¿ç¨‹åŠ¨æ€çš„ç†è§£èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰èåˆæ§åˆ¶æ³¨å…¥ (Semantic-Fused Control Injection, SFCI) æœºåˆ¶å°†ç†è§£è½¬åŒ–ä¸ºç²¾ç»†çš„ç”Ÿæˆæ§åˆ¶ã€‚è¿™ç§ååŒè®¾è®¡ä½¿å¾— TCD ä»»åŠ¡çš„ç†è§£èƒ½ç›´æ¥ä¼˜åŒ– FSIF ä»»åŠ¡çš„ç”Ÿæˆä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAMMs åœ¨è¿™ä¸¤é¡¹å…³é”®ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„ä¸“ä¸šåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to The Fourteenth International Conference on Learning Representations (ICLR 2026). Our dataset can be found at https://huggingface.co/datasets/IceInPot/TAMMs",
      "pdf_url": "https://arxiv.org/pdf/2506.18862v2",
      "published_date": "2025-06-23 17:26:16 UTC",
      "updated_date": "2025-09-26 17:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:51.436849+00:00"
    },
    {
      "arxiv_id": "2507.00044v1",
      "title": "HistoART: Histopathology Artifact Detection and Reporting Tool",
      "title_zh": "HistoARTï¼šç»„ç»‡ç—…ç†å­¦ä¼ªå½±æ£€æµ‹ä¸æŠ¥å‘Šå·¥å…·",
      "authors": [
        "Seyed Kahaki",
        "Alexander R. Webber",
        "Ghada Zamzmi",
        "Adarsh Subbaswamy",
        "Rucha Deshpande",
        "Aldo Badano"
      ],
      "abstract": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to digitize tissue specimens for detailed, high-resolution examination; however, other diagnostic approaches, such as liquid biopsy and molecular testing, are also utilized based on the cancer type and clinical context. While WSI has revolutionized digital histopathology by enabling automated, precise analysis, it remains vulnerable to artifacts introduced during slide preparation and scanning. These artifacts can compromise downstream image analysis. To address this challenge, we propose and compare three robust artifact detection approaches for WSIs: (1) a foundation model-based approach (FMA) using a fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach (KBA) leveraging handcrafted features from texture, color, and frequency-based metrics. The methods target six common artifact types: tissue folds, out-of-focus regions, air bubbles, tissue damage, marker traces, and blood contamination. Evaluations were conducted on 50,000+ image patches from diverse scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]), outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978]) and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into actionable insights, we developed a quality report scorecard that quantifies high-quality patches and visualizes artifact distributions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HistoARTï¼Œä¸€ç§ä¸“é—¨ç”¨äºæ£€æµ‹å’ŒæŠ¥å‘Šç»„ç»‡ç—…ç†å­¦å…¨æ‰«æåˆ‡ç‰‡(Whole Slide Imaging)ä¼ªå½±çš„å·¥å…·ï¼Œæ—¨åœ¨åº”å¯¹åˆ‡ç‰‡åˆ¶å¤‡å’Œæ‰«æä¸­äº§ç”Ÿçš„ä¼ªå½±å¯¹ä¸‹æ¸¸å›¾åƒåˆ†æçš„å¹²æ‰°ã€‚ä½œè€…è¯„ä¼°å¹¶å¯¹æ¯”äº†ä¸‰ç§æŠ€æœ¯è·¯çº¿ï¼šåŸºäºFoundation Modelçš„UNIæ¶æ„æ–¹æ³•(FMA)ã€åŸºäºResNet50çš„æ·±åº¦å­¦ä¹ æ–¹æ³•(DLA)ä»¥åŠåŸºäºæ‰‹å·¥ç‰¹å¾çš„çŸ¥è¯†åº“æ–¹æ³•(KBA)ã€‚å®éªŒåˆ©ç”¨æ¥è‡ªå¤šç§æ‰«æä»ªçš„5ä¸‡å¤šä¸ªå›¾åƒæ–‘å—è¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºFMAä»¥0.995çš„AUROCæ€§èƒ½ä½å±…æ¦œé¦–ï¼Œä¼˜äºDLAå’ŒKBAã€‚è¯¥ç³»ç»Ÿå¯æœ‰æ•ˆè¯†åˆ«ç»„ç»‡è¤¶çš±ã€å¤±ç„¦ã€æ°”æ³¡ç­‰å…­ç±»å¸¸è§ä¼ªå½±ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é…å¥—å¼€å‘äº†è´¨é‡æŠ¥å‘Šè¯„åˆ†å¡ï¼Œé€šè¿‡é‡åŒ–é«˜è´¨é‡æ–‘å—å’Œå¯è§†åŒ–ä¼ªå½±åˆ†å¸ƒï¼Œä¸ºæå‡æ•°å­—ç—…ç†åˆ†æçš„å‡†ç¡®æ€§æä¾›äº†é‡è¦å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.00044v1",
      "published_date": "2025-06-23 17:22:19 UTC",
      "updated_date": "2025-06-23 17:22:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:12.332340+00:00"
    },
    {
      "arxiv_id": "2506.18957v1",
      "title": "A Comment On \"The Illusion of Thinking\": Reframing the Reasoning Cliff as an Agentic Gap",
      "title_zh": "è¯„ã€Šæ€è€ƒçš„å¹»è§‰ã€‹ï¼šå°†â€œæ¨ç†æ‚¬å´–â€é‡æ„ä¸ºâ€œæ™ºèƒ½ä½“é¸¿æ²Ÿâ€",
      "authors": [
        "Sheraz Khan",
        "Subha Madhavan",
        "Kannan Natarajan"
      ],
      "abstract": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity, presents a compelling empirical finding, a reasoning cliff, where the performance of Large Reasoning Models (LRMs) collapses beyond a specific complexity threshold, which the authors posit as an intrinsic scaling limitation of Chain-of-Thought (CoT) reasoning. This commentary, while acknowledging the study's methodological rigor, contends that this conclusion is confounded by experimental artifacts. We argue that the observed failure is not evidence of a fundamental cognitive boundary, but rather a predictable outcome of system-level constraints in the static, text-only evaluation paradigm, including tool use restrictions, context window recall issues, the absence of crucial cognitive baselines, inadequate statistical reporting, and output generation limits. We reframe this performance collapse through the lens of an agentic gap, asserting that the models are not failing at reasoning, but at execution within a profoundly restrictive interface. We empirically substantiate this critique by demonstrating a striking reversal. A model, initially declaring a puzzle impossible when confined to text-only generation, now employs agentic tools to not only solve it but also master variations of complexity far beyond the reasoning cliff it previously failed to surmount. Additionally, our empirical analysis of tool-enabled models like o4-mini and GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural execution to complex meta-cognitive self-correction, which has significant implications for how we define and measure machine intelligence. The illusion of thinking attributed to LRMs is less a reasoning deficit and more a consequence of an otherwise capable mind lacking the tools for action.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Shojaee ç­‰äººæå‡ºçš„â€œæ¨ç†æ‚¬å´–â€(reasoning cliff) æ¦‚å¿µæå‡ºäº†ä¸åŒè§è§£ï¼Œè®¤ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ (LRMs) åœ¨é«˜å¤æ‚åº¦ä¸‹çš„æ€§èƒ½å´©æºƒå¹¶éæºäº Chain-of-Thought (CoT) æ¨ç†çš„å†…åœ¨å±€é™ï¼Œè€Œæ˜¯å®éªŒç¯å¢ƒä¸­çš„ç³»ç»Ÿçº§çº¦æŸå¯¼è‡´çš„ã€‚ä½œè€…é€šè¿‡â€œä»£ç†ç¼ºå£â€(agentic gap) è¿™ä¸€æ–°è§†è§’é‡æ–°å®šä¹‰äº†æ€§èƒ½å¤±æ•ˆï¼ŒæŒ‡å‡ºæ¨¡å‹å¹¶éåœ¨æ¨ç†ä¸Šå¤±è´¥ï¼Œè€Œæ˜¯åœ¨å—é™çš„çº¯æ–‡æœ¬è¯„ä¼°èŒƒå¼ä¸­æ‰§è¡Œå›°éš¾ã€‚å®éªŒè¯æ®æ˜¾ç¤ºï¼Œå¼•å…¥ä»£ç†å·¥å…· (agentic tools) åï¼Œæ¨¡å‹ä¸ä»…èƒ½è§£å†³ä¹‹å‰å¤±è´¥çš„éš¾é¢˜ï¼Œç”šè‡³èƒ½å¤„ç†è¿œè¶…æ‰€è°“æ‚¬å´–å¤æ‚åº¦çš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹ o4-mini å’Œ GPT-4o çš„ç»éªŒåˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ä»ç¨‹åºåŒ–æ‰§è¡Œåˆ°å…ƒè®¤çŸ¥è‡ªæˆ‘çº é”™çš„ä»£ç†æ¨ç†å±‚æ¬¡ã€‚æœ€ç»ˆç»“è®ºå¼ºè°ƒï¼ŒLRMs è¡¨ç°å‡ºçš„æ€ç»´å¹»è§‰æœ¬è´¨ä¸Šæ˜¯å…·å¤‡èƒ½åŠ›çš„æ™ºèƒ½ä½“ç¼ºä¹è¡ŒåŠ¨å·¥å…·çš„ç»“æœï¼Œè€Œéæ¨ç†èƒ½åŠ›çš„ç¼ºé™·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 2 figures, Comment on \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" (arXiv:2506.06941v1)",
      "pdf_url": "https://arxiv.org/pdf/2506.18957v1",
      "published_date": "2025-06-23 17:14:21 UTC",
      "updated_date": "2025-06-23 17:14:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:50:57.156946+00:00"
    },
    {
      "arxiv_id": "2506.18852v1",
      "title": "Mechanistic Interpretability Needs Philosophy",
      "title_zh": "æœºæ¢°å¯è§£é‡Šæ€§éœ€è¦å“²å­¦",
      "authors": [
        "Iwan Williams",
        "Ninell Oldenburg",
        "Ruchira Dhar",
        "Joshua Hatherley",
        "Constanza Fierro",
        "Nina Rajcic",
        "Sandrine R. Schiller",
        "Filippos Stamatiou",
        "Anders SÃ¸gaard"
      ],
      "abstract": "Mechanistic interpretability (MI) aims to explain how neural networks work by uncovering their underlying causal mechanisms. As the field grows in influence, it is increasingly important to examine not just models themselves, but the assumptions, concepts and explanatory strategies implicit in MI research. We argue that mechanistic interpretability needs philosophy: not as an afterthought, but as an ongoing partner in clarifying its concepts, refining its methods, and assessing the epistemic and ethical stakes of interpreting AI systems. Taking three open problems from the MI literature as examples, this position paper illustrates the value philosophy can add to MI research, and outlines a path toward deeper interdisciplinary dialogue.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æœºæ¢°å¯è§£é‡Šæ€§ (Mechanistic interpretability, MI) é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡æ­ç¤ºåº•å±‚å› æœæœºåˆ¶æ¥è§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œæ–¹å¼ã€‚ä½œè€…æå‡º MI éœ€è¦å“²å­¦ä½œä¸ºæŒç»­çš„åˆä½œä¼™ä¼´ï¼Œè€Œä¸ä»…ä»…æ˜¯äº‹åçš„è¡¥å……ï¼Œä»¥ååŠ©æ¾„æ¸…æ ¸å¿ƒæ¦‚å¿µã€ä¼˜åŒ–ç ”ç©¶æ–¹æ³•ï¼Œå¹¶è¯„ä¼°åœ¨è§£é‡Šäººå·¥æ™ºèƒ½ç³»ç»Ÿæ—¶çš„è®¤è¯†è®ºä¸ä¼¦ç†é£é™©ã€‚æ–‡ç« é€šè¿‡åˆ†æ MI æ–‡çŒ®ä¸­çš„ä¸‰ä¸ªå¼€æ”¾æ€§é—®é¢˜ä½œä¸ºå®ä¾‹ï¼Œå…·ä½“é˜è¿°äº†å“²å­¦å¯¹ MI ç ”ç©¶çš„ç‹¬ç‰¹ä»·å€¼ã€‚è¯¥ç«‹åœºè®ºæ–‡å¼ºè°ƒäº†å®¡è§†æ¨¡å‹æœ¬èº«ä»¥åŠ MI ç ”ç©¶ä¸­éšå«çš„å‡è®¾ã€æ¦‚å¿µå’Œè§£é‡Šç­–ç•¥çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ·±åº¦è·¨å­¦ç§‘å¯¹è¯æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18852v1",
      "published_date": "2025-06-23 17:13:30 UTC",
      "updated_date": "2025-06-23 17:13:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:01.349610+00:00"
    },
    {
      "arxiv_id": "2506.18841v1",
      "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
      "title_zh": "LongWriter-Zeroï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æŒæ¡è¶…é•¿æ–‡æœ¬ç”Ÿæˆ",
      "authors": [
        "Yuhao Wu",
        "Yushi Bai",
        "Zhiqiang Hu",
        "Roy Ka-Wei Lee",
        "Juanzi Li"
      ],
      "abstract": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¶…é•¿æ–‡æœ¬ç”Ÿæˆä¸­é¢ä¸´çš„é•¿åº¦é™åˆ¶åŠè´¨é‡é€€åŒ–é—®é¢˜ï¼Œæå‡ºäº† LongWriter-Zeroã€‚ä¸ä¾èµ–é«˜æˆæœ¬åˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒ (SFT) çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæ¿€åŠ± (incentivization-based) çš„ç­–ç•¥ï¼Œå®Œå…¨é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ä»é›¶å¼€å§‹åŸ¹å…»æ¨¡å‹çš„è¶…é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¨¡å‹å€Ÿé‰´ R1-Zero çš„æ€è·¯ï¼Œå¼•å¯¼æ¨¡å‹åœ¨å†™ä½œä¸­è¿›è¡Œæ¨ç†ã€è§„åˆ’ä¸ç²¾ç‚¼ï¼Œå¹¶åˆ©ç”¨ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹ (Reward Models) æ¥ä¼˜åŒ–é•¿åº¦æ§åˆ¶ã€å†™ä½œè´¨é‡å’Œç»“æ„æ ¼å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº Qwen2.5-32B çš„ LongWriter-Zero åœ¨ WritingBench å’Œ Arena-Write ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ä»…ä¼˜äºä¼ ç»Ÿçš„ SFT æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº† DeepSeek R1 å’Œ Qwen3-235B ç­‰è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18841v1",
      "published_date": "2025-06-23 16:59:02 UTC",
      "updated_date": "2025-06-23 16:59:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:12.846281+00:00"
    },
    {
      "arxiv_id": "2506.18824v2",
      "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories",
      "title_zh": "ç†è§£è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ï¼šåŸºäºæ€ç»´-è¡ŒåŠ¨-ç»“æœè½¨è¿¹çš„ç ”ç©¶",
      "authors": [
        "Islem Bouzenia",
        "Michael Pradel"
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks, such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: RepairAgent, AutoCodeRover, and OpenHands. We unify their interaction logs into a common format, capturing 120 trajectories and 2,822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics, such as iteration counts and token consumption, recurring action sequences, and the semantic coherence of thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å†…éƒ¨å†³ç­–è¿‡ç¨‹ä¸æ˜çš„é—®é¢˜ï¼Œå¯¹RepairAgentã€AutoCodeRoverå’ŒOpenHandsä¸‰ç§å…ˆè¿›æ™ºèƒ½ä½“çš„â€œæ€è€ƒ-è¡ŒåŠ¨-ç»“æœâ€(thought-action-result)è½¨è¿¹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ã€‚é€šè¿‡ç»Ÿä¸€120æ¡è½¨è¿¹å’Œ2,822æ¬¡å…³äºç¨‹åºä¿®å¤(program repair)ä¸é—®é¢˜è§£å†³(issue resolution)çš„äº¤äº’æ—¥å¿—ï¼Œç ”ç©¶å›¢é˜Ÿç»“åˆå®šé‡åˆ†æï¼ˆå¦‚è¿­ä»£æ¬¡æ•°ã€tokenæ¶ˆè€—å’Œè¡ŒåŠ¨åºåˆ—ï¼‰ä¸å®šæ€§è¯„ä¼°ï¼ˆå¦‚æ¨ç†è¿è´¯æ€§ï¼‰ï¼Œæ·±å…¥æ¢è®¨äº†æ™ºèƒ½ä½“çš„è¿è¡ŒåŠ¨æ€ã€‚ç ”ç©¶è¯†åˆ«å‡ºäº†åŒºåˆ†ä»»åŠ¡æˆåŠŸä¸å¤±è´¥çš„å…³é”®è¡Œä¸ºç‰¹å¾(behavioral motifs)ä¸åæ¨¡å¼(anti-patterns)ï¼Œä¸ºä¼˜åŒ–æ™ºèƒ½ä½“è®¾è®¡ã€æç¤ºç­–ç•¥(prompting strategies)åŠæ•…éšœè¯Šæ–­æä¾›äº†é‡è¦è§è§£ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜å‘å¸ƒäº†ç›¸å…³æ•°æ®é›†å’Œæ ‡æ³¨æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨å¼€å‘æ›´é€æ˜ã€æ›´ç¨³å¥çš„è‡ªä¸»è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "ACCEPTED FOR ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18824v2",
      "published_date": "2025-06-23 16:34:52 UTC",
      "updated_date": "2025-10-08 11:28:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:11.493860+00:00"
    },
    {
      "arxiv_id": "2506.18819v1",
      "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies",
      "title_zh": "RWESummaryï¼šç”¨äºæ€»ç»“çœŸå®ä¸–ç•Œè¯æ®ï¼ˆRWEï¼‰ç ”ç©¶çš„å¤§è¯­è¨€æ¨¡å‹é€‰å‹æ¡†æ¶ä¸è¯„æµ‹ä½“ç³»",
      "authors": [
        "Arjun Mukerji",
        "Michael L. Jackson",
        "Jason Jones",
        "Neil Sanghavi"
      ],
      "abstract": "Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RWESummaryæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ€»ç»“çœŸå®ä¸–ç•Œè¯æ®(RWE)ç ”ç©¶ç»“æ„åŒ–è¾“å‡ºæ—¶ç¼ºä¹ä¸“é—¨è¯„ä¼°æ ‡å‡†çš„é—®é¢˜ã€‚ä½œä¸ºMedHELMæ¡†æ¶çš„æ‰©å±•ï¼ŒRWESummaryåŒ…å«ä¸€ä¸ªåº”ç”¨åœºæ™¯å’Œä¸‰é¡¹æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼Œä¸“é—¨ç”¨äºè¯†åˆ«å’Œè¡¡é‡åŒ»å­¦ç ”ç©¶æ‘˜è¦ä¸­çš„ä¸»è¦é”™è¯¯ç±»å‹ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨Atropos Healthçš„ä¸“æœ‰æ•°æ®å¼€å‘äº†è¯¥æ¡†æ¶ï¼Œå¹¶å°†å…¶ç”¨äºå¯¹æ¯”ä¸åŒLLMsåœ¨çœŸå®RWEæ‘˜è¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨13é¡¹ä¸åŒçš„RWEç ”ç©¶è¯„ä¼°ä¸­ï¼ŒGemini 2.5æ¨¡å‹ï¼ˆåŒ…æ‹¬Flashå’ŒProï¼‰è¡¨ç°æœ€ä¸ºä¼˜å¼‚ã€‚RWESummaryä¸ºçœŸå®ä¸–ç•Œè¯æ®ç ”ç©¶çš„è‡ªåŠ¨åŒ–æ‘˜è¦æä¾›äº†ä¸€ä¸ªæ–°é¢–ä¸”å…·æœ‰å®é™…åº”ç”¨ä»·å€¼çš„åŸºç¡€æ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18819v1",
      "published_date": "2025-06-23 16:28:03 UTC",
      "updated_date": "2025-06-23 16:28:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:19.514151+00:00"
    },
    {
      "arxiv_id": "2506.18810v3",
      "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
      "title_zh": "ConciseHintï¼šé€šè¿‡ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒç»­ç®€æ´æç¤ºæå‡é«˜æ•ˆæ¨ç†",
      "authors": [
        "Siao Tang",
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "abstract": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, a critical issue is their tendency to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting learnable hints (manually designed or learned on concise data) during the generation of the reasoning. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning while maintaining the performance well. Moreover, we show that ConciseHint is flexible and can be seamlessly integrated with existing methods to further push the upper bound of the efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Large Reasoning Models (LRMs) åœ¨ä½¿ç”¨Chain-of-Thought (CoT) æ—¶äº§ç”Ÿçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ä¸”ä½æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†ConciseHintæ¡†æ¶ã€‚ä¸ç°æœ‰çš„æ¨ç†å‰ä¼˜åŒ–èŒƒå¼ä¸åŒï¼ŒConciseHinté€šè¿‡åœ¨æ¨ç†ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶æ³¨å…¥å¯å­¦ä¹ çš„hintsï¼Œç›´æ¥é¼“åŠ±æ¨¡å‹è¿›è¡Œç®€æ´è¡¨è¾¾ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢ä»»åŠ¡çš„å¤æ‚åº¦è‡ªé€‚åº”åœ°è°ƒæ•´æç¤ºå¼ºåº¦ï¼Œä»è€Œåœ¨æ˜¾è‘—å‡å°‘ç”Ÿæˆé•¿åº¦çš„åŒæ—¶ï¼Œç¡®ä¿ä¸æŸå®³æ¨¡å‹çš„åŸå§‹æ€§èƒ½ã€‚åœ¨DeepSeek-R1å’ŒQwen-3ç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç”Ÿæˆç®€æ´çš„æ¨ç†è¿‡ç¨‹å¹¶ç»´æŒé«˜è´¨é‡çš„è¾“å‡ºè¡¨ç°ã€‚æ­¤å¤–ï¼ŒConciseHintè¡¨ç°å‡ºæé«˜çš„çµæ´»æ€§ï¼Œå¯ä»¥ä¸ç°æœ‰æ–¹æ³•æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥æ¨é«˜äº†å¤§å‹æ¨ç†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ä¸Šé™ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Compare with more baselines, add more in-depth analysis, and re-evaluate the GPQA-D benchmark. Codes are available at https://github.com/tsa18/ConciseHint",
      "pdf_url": "https://arxiv.org/pdf/2506.18810v3",
      "published_date": "2025-06-23 16:20:44 UTC",
      "updated_date": "2025-10-01 11:23:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:38.685430+00:00"
    },
    {
      "arxiv_id": "2506.18798v2",
      "title": "OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness",
      "title_zh": "OC-SOPï¼šé€šè¿‡ç‰©ä½“ä¸­å¿ƒæ„ŸçŸ¥å¢å¼ºåŸºäºè§†è§‰çš„ 3D è¯­ä¹‰å ç”¨é¢„æµ‹",
      "authors": [
        "Helin Cao",
        "Sven Behnke"
      ],
      "abstract": "Autonomous driving perception faces significant challenges due to occlusions and incomplete scene data in the environment. To overcome these issues, the task of semantic occupancy prediction (SOP) is proposed, which aims to jointly infer both the geometry and semantic labels of a scene from images. However, conventional camera-based methods typically treat all categories equally and primarily rely on local features, leading to suboptimal predictions, especially for dynamic foreground objects. To address this, we propose Object-Centric SOP (OC-SOP), a framework that integrates high-level object-centric cues extracted via a detection branch into the semantic occupancy prediction pipeline. This object-centric integration significantly enhances the prediction accuracy for foreground objects and achieves state-of-the-art performance among all categories on SemanticKITTI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä¸­é®æŒ¡å’Œåœºæ™¯æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† OC-SOP æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒ (Object-Centric Awareness) çš„æ„ŸçŸ¥èƒ½åŠ›å¢å¼ºåŸºäºè§†è§‰çš„ 3D è¯­ä¹‰å ç”¨é¢„æµ‹ (Semantic Occupancy Prediction, SOP)ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å¯¹æ‰€æœ‰ç±»åˆ«ä¸€è§†åŒä»ä¸”è¿‡åº¦ä¾èµ–å±€éƒ¨ç‰¹å¾ï¼Œå¯¼è‡´åŠ¨æ€å‰æ™¯å¯¹è±¡çš„é¢„æµ‹æ•ˆæœä¸ä½³ï¼Œè€Œ OC-SOP é€šè¿‡æ£€æµ‹åˆ†æ”¯æå–é«˜çº§çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„çº¿ç´¢å¹¶å°†å…¶é›†æˆåˆ°é¢„æµ‹æµæ°´çº¿ä¸­ã€‚è¿™ç§é›†æˆæ–¹å¼æ˜¾è‘—æå‡äº†å‰æ™¯å¯¹è±¡çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæœ‰æ•ˆå…‹æœäº†å±€éƒ¨ç‰¹å¾çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOC-SOP åœ¨ SemanticKITTI æ•°æ®é›†çš„æ‰€æœ‰ç±»åˆ«ä¸Šå‡å®ç°äº†æœ€å…ˆè¿› (state-of-the-art) çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Vienna, Austria, Oct 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18798v2",
      "published_date": "2025-06-23 16:03:53 UTC",
      "updated_date": "2025-08-13 10:44:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:33.605922+00:00"
    },
    {
      "arxiv_id": "2506.18789v2",
      "title": "Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning",
      "title_zh": "Shift Happensï¼šåŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„è”é‚¦å­¦ä¹ æŒç»­è‡ªé€‚åº”",
      "authors": [
        "Rahul Atul Bhope",
        "K. R. Jayaram",
        "Praveen Venkateswaran",
        "Nalini Venkatasubramanian"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across decentralized clients without sharing raw data, yet faces significant challenges in real-world settings where client data distributions evolve dynamically over time. This paper tackles the critical problem of covariate and label shifts in streaming FL environments, where non-stationary data distributions degrade model performance and necessitate a middleware layer that adapts FL to distributional shifts. We introduce ShiftEx, a shift-aware mixture of experts framework that dynamically creates and trains specialized global models in response to detected distribution shifts using Maximum Mean Discrepancy for covariate shifts. The framework employs a latent memory mechanism for expert reuse and implements facility location-based optimization to jointly minimize covariate mismatch, expert creation costs, and label imbalance. Through theoretical analysis and comprehensive experiments on benchmark datasets, we demonstrate 5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation compared to state-of-the-art FL baselines across diverse shift scenarios. The proposed approach offers a scalable, privacy-preserving middleware solution for FL systems operating in non-stationary, real-world conditions while minimizing communication and computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ShiftExï¼Œä¸€ç§åŸºäºæ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts) çš„åç§»æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹  (Federated Learning) åœ¨ç°å®åœºæ™¯ä¸­é¢ä¸´çš„åå˜é‡åç§» (covariate shifts) å’Œæ ‡ç­¾åç§» (label shifts) é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æœ€å¤§å‡å€¼å·®å¼‚ (Maximum Mean Discrepancy) æ¥å®æ—¶æ£€æµ‹åˆ†å¸ƒåç§»ï¼Œå¹¶æ®æ­¤åŠ¨æ€åˆ›å»ºæˆ–æ›´æ–°ä¸“é—¨çš„å…¨å±€æ¨¡å‹ä»¥é€‚åº”åŠ¨æ€å˜åŒ–çš„æ•°æ®æµã€‚æ­¤å¤–ï¼ŒShiftEx å¼•å…¥äº†æ½œåœ¨è®°å¿†æœºåˆ¶ (latent memory mechanism) å®ç°ä¸“å®¶æ¨¡å—çš„å¤ç”¨ï¼Œå¹¶é‡‡ç”¨åŸºäºè®¾æ–½ä½ç½®çš„ä¼˜åŒ–æ–¹æ³•æ¥å¹³è¡¡åç§»åŒ¹é…ã€åˆ›å»ºæˆæœ¬å’Œæ ‡ç­¾åˆ†å¸ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§åç§»åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šæå‡äº† 5.5-12.9%ï¼Œé€‚åº”é€Ÿåº¦åŠ å¿«äº† 22-95%ã€‚è¿™ä¸ºéå¹³ç¨³ã€ç°å®ç¯å¢ƒä¸‹çš„è”é‚¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•ä¸”ä¿æŠ¤éšç§çš„ä¸­é—´ä»¶è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18789v2",
      "published_date": "2025-06-23 15:59:21 UTC",
      "updated_date": "2025-09-25 02:22:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:01.207442+00:00"
    },
    {
      "arxiv_id": "2506.18785v2",
      "title": "SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving",
      "title_zh": "SWA-SOPï¼šé¢å‘è‡ªåŠ¨é©¾é©¶è¯­ä¹‰å ç”¨é¢„æµ‹çš„ç©ºé—´æ„ŸçŸ¥çª—å£æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Helin Cao",
        "Rafael Materla",
        "Sven Behnke"
      ],
      "abstract": "Perception systems in autonomous driving rely on sensors such as LiDAR and cameras to perceive the 3D environment. However, due to occlusions and data sparsity, these sensors often fail to capture complete information. Semantic Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy and semantics of unobserved regions. Existing transformer-based SOP methods lack explicit modeling of spatial structure in attention computation, resulting in limited geometric awareness and poor performance in sparse or occluded areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel mechanism that incorporates local spatial context into attention. SWA significantly improves scene completion and achieves state-of-the-art results on LiDAR-based SOP benchmarks. We further validate its generality by integrating SWA into a camera-based SOP pipeline, where it also yields consistent gains across modalities.",
      "tldr_zh": "è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿå¸¸å› é®æŒ¡å’Œæ•°æ®ç¨€ç–æ€§éš¾ä»¥è·å–å®Œæ•´ç¯å¢ƒä¿¡æ¯ï¼Œè€Œç°æœ‰çš„åŸºäº Transformer çš„è¯­ä¹‰å ç”¨é¢„æµ‹ (Semantic Occupancy Prediction, SOP) æ–¹æ³•å› ç¼ºä¹æ˜¾å¼çš„ç©ºé—´ç»“æ„å»ºæ¨¡ï¼Œå¯¼è‡´å…¶å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›åœ¨å¤æ‚åŒºåŸŸè¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ç©ºé—´æ„ŸçŸ¥çª—å£æ³¨æ„åŠ› (Spatially-aware Window Attention, SWA) æœºåˆ¶ï¼Œé€šè¿‡å°†å±€éƒ¨ç©ºé—´ä¸Šä¸‹æ–‡èå…¥æ³¨æ„åŠ›è®¡ç®—æ¥å¢å¼ºæ¨¡å‹çš„å‡ ä½•æ„ŸçŸ¥ã€‚SWA æ˜¾è‘—æå‡äº†åœºæ™¯è¡¥å…¨çš„è´¨é‡ï¼Œå¹¶åœ¨åŸºäº LiDAR çš„ SOP åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† State-of-the-Art æ°´å‡†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°† SWA è¿›ä¸€æ­¥åº”ç”¨äºåŸºäºç›¸æœºçš„ SOP æµç¨‹ï¼Œå®éªŒè¯æ˜äº†è¯¥æœºåˆ¶åœ¨è·¨æ¨¡æ€åº”ç”¨ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œä¸€è‡´çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Vienna, Austria, Oct 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18785v2",
      "published_date": "2025-06-23 15:54:28 UTC",
      "updated_date": "2025-08-13 10:49:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:45.637827+00:00"
    },
    {
      "arxiv_id": "2506.18783v1",
      "title": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation",
      "title_zh": "TRIZ Agentsï¼šé¢å‘ TRIZ åˆ›æ–°çš„å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹æ–¹æ³•",
      "authors": [
        "Kamil Szczepanik",
        "JarosÅ‚aw A. Chudziak"
      ],
      "abstract": "TRIZ, the Theory of Inventive Problem Solving, is a structured, knowledge-based framework for innovation and abstracting problems to find inventive solutions. However, its application is often limited by the complexity and deep interdisciplinary knowledge required. Advancements in Large Language Models (LLMs) have revealed new possibilities for automating parts of this process. While previous studies have explored single LLMs in TRIZ applications, this paper introduces a multi-agent approach. We propose an LLM-based multi-agent system, called TRIZ agents, each with specialized capabilities and tool access, collaboratively solving inventive problems based on the TRIZ methodology. This multi-agent system leverages agents with various domain expertise to efficiently navigate TRIZ steps. The aim is to model and simulate an inventive process with language agents. We assess the effectiveness of this team of agents in addressing complex innovation challenges based on a selected case study in engineering. We demonstrate the potential of agent collaboration to produce diverse, inventive solutions. This research contributes to the future of AI-driven innovation, showcasing the advantages of decentralized problem-solving in complex ideation tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†TRIZ agentsï¼Œä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å‘æ˜é—®é¢˜è§£å†³ç†è®º(TRIZ)å› å…¶å¤æ‚æ€§å’Œè·¨å­¦ç§‘çŸ¥è¯†è¦æ±‚è€Œéš¾ä»¥å¹¿æ³›åº”ç”¨çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºå…·å¤‡ä¸“é—¨é¢†åŸŸçŸ¥è¯†å’Œå·¥å…·è®¿é—®æƒé™çš„å¤šä¸ªæ™ºèƒ½ä½“ï¼Œåœ¨TRIZæ–¹æ³•è®ºçš„æŒ‡å¯¼ä¸‹ååŒè§£å†³åˆ›æ–°éš¾é¢˜ã€‚ç ”ç©¶æ—¨åœ¨åˆ©ç”¨è¯­è¨€æ™ºèƒ½ä½“å¯¹å‘æ˜è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ä¸æ¨¡æ‹Ÿï¼Œå¹¶é€šè¿‡å·¥ç¨‹é¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶è¯„ä¼°äº†è¯¥æ™ºèƒ½ä½“å›¢é˜Ÿåœ¨å¤„ç†å¤æ‚åˆ›æ–°æŒ‘æˆ˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜äº†æ™ºèƒ½ä½“åä½œåœ¨ç”Ÿæˆå¤šæ ·åŒ–å‘æ˜æ–¹æ¡ˆæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå±•ç¤ºäº†å»ä¸­å¿ƒåŒ–é—®é¢˜è§£å†³(decentralized problem-solving)åœ¨å¤æ‚æ„æ€ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥äººå·¥æ™ºèƒ½é©±åŠ¨çš„åˆ›æ–°(AI-driven innovation)å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 10 figures, 2 tables, Accepted at the 17th International Conference on Agents and Artificial Intelligence (ICAART 2025). Final version published in Proceedings of ICAART 2025 (Vol. 1), pages 196-207",
      "pdf_url": "https://arxiv.org/pdf/2506.18783v1",
      "published_date": "2025-06-23 15:53:14 UTC",
      "updated_date": "2025-06-23 15:53:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:46.313258+00:00"
    },
    {
      "arxiv_id": "2506.18777v1",
      "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
      "title_zh": "Programming by Backpropï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç è®­ç»ƒä¸­ä¹ å¾—å¯é‡ç”¨çš„ç®—æ³•æŠ½è±¡",
      "authors": [
        "Jonathan Cook",
        "Silvia Sapora",
        "Arash Ahmadian",
        "Akbir Khan",
        "Tim Rocktaschel",
        "Jakob Foerster",
        "Laura Ruis"
      ],
      "abstract": "Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Programming by Backprop (PBB)çš„æ¦‚å¿µï¼Œæ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¦‚ä½•ä»…é€šè¿‡æºä»£ç è®­ç»ƒï¼ˆè€Œä¸ä¾èµ–è¾“å…¥/è¾“å‡ºç¤ºä¾‹ï¼‰æ¥å­¦ä¹ è¯„ä¼°ç¨‹åºã€‚ç ”ç©¶è€…åœ¨åŒ…å«æºä»£ç åŠI/Oç¤ºä¾‹(w/ IO)å’Œä»…åŒ…å«æºä»£ç (w/o IO)çš„ä¸¤ç»„æ•°å­¦åŠç®—æ³•ç¨‹åºä¸Šå¾®è°ƒæ¨¡å‹ï¼Œå‘ç°LLMså…·å¤‡ä»…é€šè¿‡ä»£ç ç†è§£å¹¶æ‰§è¡Œå…¶é€»è¾‘çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå½“ç¨‹åºä»¥ä»£ç å½¢å¼è€Œéè¯­ä¹‰ç­‰ä»·çš„è¯­è¨€æè¿°å‘ˆç°æ—¶ï¼ŒPBBçš„æ•ˆæœæ˜¾è‘—å¢å¼ºï¼Œä¸”æ¨¡å‹å¯ä»¥é€šè¿‡å‰å‘ä¼ æ’­(forward pass)éšå¼è¯„ä¼°æˆ–é€šè¿‡é“¾å¼æ€ç»´(chain-of-thought)æ›´å¯é åœ°åˆ†æ­¥æ‰§è¡Œã€‚ç›¸æ¯”äºåœ¨ä¼ ç»ŸI/Oå¯¹æ•°æ®åˆ†å¸ƒä¸Šè¿›è¡Œçš„è®­ç»ƒï¼ŒPBBèƒ½ä½¿æ¨¡å‹å¯¹ä¸åŒè¾“å…¥çš„ç¨‹åºè¯„ä¼°è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚è¯¥å‘ç°æ­ç¤ºäº†ä»£ç è®­ç»ƒå¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ·±å±‚æœºåˆ¶ï¼Œå³LLMsé€šè¿‡æ­¤ç±»è®­ç»ƒå†…åŒ–äº†å¯é‡ç”¨çš„ç®—æ³•æŠ½è±¡(algorithmic abstractions)ï¼Œä¸ºæœªæ¥åˆ©ç”¨ç¬¦å·è¿‡ç¨‹å’Œå½¢å¼åŒ–åŸåˆ™è¿›è¡Œæ¨¡å‹å¯¹é½æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18777v1",
      "published_date": "2025-06-23 15:45:44 UTC",
      "updated_date": "2025-06-23 15:45:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:48.717955+00:00"
    },
    {
      "arxiv_id": "2506.19877v2",
      "title": "Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017",
      "title_zh": "ç½‘ç»œæµé‡ä¸­çš„é²æ£’å¼‚å¸¸æ£€æµ‹ï¼šåŸºäº CICIDS2017 çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Zhaoyang Xu",
        "Yunbo Liu"
      ],
      "abstract": "Identifying suitable machine learning paradigms for intrusion detection remains critical for building effective and generalizable security solutions. In this study, we present a controlled comparison of four representative models - Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN), One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on the CICIDS2017 dataset under two scenarios: detecting known attack types and generalizing to previously unseen threats. Our results show that supervised MLP and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic recall drops on novel attacks. Unsupervised LOF attains moderate overall accuracy and high recall on unknown threats at the cost of elevated false alarms, while boundary-based OCSVM balances precision and recall best, demonstrating robust detection across both scenarios. These findings offer practical guidance for selecting IDS models in dynamic network environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œå…¥ä¾µæ£€æµ‹ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹çš„é€‚ç”¨æ€§é—®é¢˜ï¼Œåœ¨CICIDS2017æ•°æ®é›†ä¸Šå¯¹Multi-Layer Perceptron (MLP)ã€1D Convolutional Neural Network (CNN)ã€One-Class Support Vector Machine (OCSVM)å’ŒLocal Outlier Factor (LOF)å››ç§æ¨¡å‹è¿›è¡Œäº†å—æ§å¯¹æ¯”å®éªŒã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†æ¨¡å‹å¯¹å·²çŸ¥æ”»å‡»çš„æ£€æµ‹èƒ½åŠ›ä»¥åŠå¯¹æœªè§å¨èƒçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ‰ç›‘ç£æ¨¡å‹MLPå’ŒCNNåœ¨å·²çŸ¥æ”»å‡»ä¸Šå‡†ç¡®ç‡æé«˜ï¼Œä½†åœ¨é¢å¯¹æ–°é¢–æ”»å‡»æ—¶å¬å›ç‡(Recall)æ˜¾è‘—ä¸‹é™ã€‚æ— ç›‘ç£æ¨¡å‹LOFè™½ç„¶å¯¹æœªçŸ¥å¨èƒæœ‰è¾ƒé«˜çš„å¬å›ç‡ï¼Œä½†è¯¯æŠ¥ç‡(False Alarm)ä¹Ÿç›¸å¯¹è¾ƒé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºè¾¹ç•Œçš„OCSVMåœ¨ä¸¤ç§åœºæ™¯ä¸‹å‡è¡¨ç°ç¨³å¥ï¼Œå®ç°äº†ç²¾ç¡®ç‡(Precision)ä¸å¬å›ç‡çš„æœ€ä½³å¹³è¡¡ã€‚è¿™äº›å‘ç°ä¸ºåœ¨åŠ¨æ€ç½‘ç»œç¯å¢ƒä¸­é€‰æ‹©åˆé€‚çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)æ¨¡å‹æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "submitted to IEEE CNS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.19877v2",
      "published_date": "2025-06-23 15:31:10 UTC",
      "updated_date": "2025-08-11 00:45:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:56.147522+00:00"
    },
    {
      "arxiv_id": "2506.21622v1",
      "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech",
      "title_zh": "é¢å‘è¨€è¯­éšœç¢è¯­éŸ³çš„åŸºç¡€è¯­éŸ³è¯†åˆ«æ¨¡å‹é€‚é…ï¼šå¾·è¯­è¯­éŸ³ä¸ªæ€§åŒ–çš„è¯­ä¹‰é‡é“¾æ–¹æ³•",
      "authors": [
        "Niclas Pokel",
        "PehuÃ©n Moure",
        "Roman Boehringer",
        "Yingqiang Gao"
      ],
      "abstract": "Speech impairments caused by conditions such as cerebral palsy or genetic disorders pose significant challenges for automatic speech recognition (ASR) systems. Despite recent advances, ASR models like Whisper struggle with non-normative speech due to limited training data and the difficulty of collecting and annotating non-normative speech samples. In this work, we propose a practical and lightweight pipeline to personalize ASR models, formalizing the selection of words and enriching a small, speech-impaired dataset with semantic coherence. Applied to data from a child with a structural speech impairment, our approach shows promising improvements in transcription quality, demonstrating the potential to reduce communication barriers for individuals with atypical speech patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘æ€§ç˜«ç—ªæˆ–é—ä¼ æ€§ç–¾ç—…å¯¼è‡´çš„è¯è¯­éšœç¢ï¼ˆSpeech impairmentsï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§ä¸”å®ç”¨çš„ä¸ªæ€§åŒ–å¾®è°ƒæµæ°´çº¿ã€‚ç”±äº Whisper ç­‰åŸºç¡€æ¨¡å‹åœ¨å¤„ç†éè§„èŒƒè¯­éŸ³ï¼ˆnon-normative speechï¼‰æ—¶é¢ä¸´è®­ç»ƒæ•°æ®åŒ®ä¹å’Œæ ‡æ³¨å›°éš¾çš„æŒ‘æˆ˜ï¼Œä½œè€…é€šè¿‡ Semantic Re-chaining æ–¹æ³•å¯¹è¯æ±‡é€‰æ‹©è¿›è¡Œè§„èŒƒåŒ–ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰ä¸€è‡´æ€§å¢å¼ºäº†å°è§„æ¨¡çš„è¯è¯­éšœç¢æ•°æ®é›†ã€‚è¯¥æ–¹æ³•è¢«åº”ç”¨äºä¸€åæ‚£æœ‰ç»“æ„æ€§è¨€è¯­éšœç¢å„¿ç«¥çš„å¾·è¯­è¯­éŸ³æ•°æ®ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè½¬å½•è´¨é‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚è¿™ä¸€ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è¯­ä¹‰é‡é“¾åŒ–æ‰‹æ®µé™ä½éå…¸å‹è¯­éŸ³äººç¾¤æ²Ÿé€šéšœç¢çš„æ½œåŠ›ï¼Œä¸ºé’ˆå¯¹ç‰¹å®šéšœç¢ç¾¤ä½“çš„ ASR ç³»ç»Ÿä¸ªæ€§åŒ–æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21622v1",
      "published_date": "2025-06-23 15:30:50 UTC",
      "updated_date": "2025-06-23 15:30:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:01.044610+00:00"
    },
    {
      "arxiv_id": "2506.18751v1",
      "title": "Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos",
      "title_zh": "åŸºäºå¹¿ä¹‰å¤šé¡¹å¼æ··æ²Œçš„å›¾åƒåˆ†ç±»æ¨¡å‹æ•æ„Ÿæ€§åˆ†æ",
      "authors": [
        "Lukas Bahr",
        "Lucas PoÃŸner",
        "Konstantin Weise",
        "Sophie GrÃ¶ger",
        "RÃ¼diger Daub"
      ],
      "abstract": "Integrating advanced communication protocols in production has accelerated the adoption of data-driven predictive quality methods, notably machine learning (ML) models. However, ML models in image classification often face significant uncertainties arising from model, data, and domain shifts. These uncertainties lead to overconfidence in the classification model's output. To better understand these models, sensitivity analysis can help to analyze the relative influence of input parameters on the output. This work investigates the sensitivity of image classification models used for predictive quality. We propose modeling the distributional domain shifts of inputs with random variables and quantifying their impact on the model's outputs using Sobol indices computed via generalized polynomial chaos (GPC). This approach is validated through a case study involving a welding defect classification problem, utilizing a fine-tuned ResNet18 model and an emblem classification model used in BMW Group production facilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾åƒåˆ†ç±»æ¨¡å‹åœ¨é¢„æµ‹è´¨é‡ä¸­é¢ä¸´çš„æ¨¡å‹ã€æ•°æ®åŠé¢†åŸŸåç§»ï¼ˆDomain Shiftsï¼‰å¸¦æ¥çš„ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¹¿ä¹‰å¤šé¡¹å¼æ··æ²Œï¼ˆGeneralized Polynomial Chaos, GPCï¼‰çš„çµæ•åº¦åˆ†æï¼ˆSensitivity Analysisï¼‰æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è¾“å…¥çš„åˆ†å¸ƒé¢†åŸŸåç§»å»ºæ¨¡ä¸ºéšæœºå˜é‡ï¼Œå¹¶åˆ©ç”¨GPCè®¡ç®—Sobol indicesï¼Œä»è€Œå®šé‡è¯„ä¼°è¾“å…¥å‚æ•°å¯¹æ¨¡å‹è¾“å‡ºç»“æœçš„ç›¸å¯¹å½±å“ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ç„Šæ¥ç¼ºé™·åˆ†ç±»ä»»åŠ¡ï¼ˆé‡‡ç”¨å¾®è°ƒçš„ResNet18æ¨¡å‹ï¼‰ä»¥åŠBMWé›†å›¢ç”Ÿäº§çº¿ä¸Šçš„å¾½æ ‡åˆ†ç±»æ¨¡å‹ä¸­å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½æœ‰æ•ˆé‡åŒ–å¤æ‚ç”Ÿäº§ç¯å¢ƒä¸‹å›¾åƒåˆ†ç±»æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œä¸ºç†è§£æ¨¡å‹é²æ£’æ€§å’Œä¼˜åŒ–é¢„æµ‹è´¨é‡æä¾›äº†é‡è¦çš„æ•°å­¦å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18751v1",
      "published_date": "2025-06-23 15:22:31 UTC",
      "updated_date": "2025-06-23 15:22:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:51:59.918388+00:00"
    },
    {
      "arxiv_id": "2506.18747v1",
      "title": "ContinualFlow: Learning and Unlearning with Neural Flow Matching",
      "title_zh": "ContinualFlowï¼šåŸºäºç¥ç»æµåŒ¹é…çš„å­¦ä¹ ä¸é—å¿˜",
      "authors": [
        "Lorenzo Simone",
        "Davide Bacciu",
        "Shuangge Ma"
      ],
      "abstract": "We introduce ContinualFlow, a principled framework for targeted unlearning in generative models via Flow Matching. Our method leverages an energy-based reweighting loss to softly subtract undesired regions of the data distribution without retraining from scratch or requiring direct access to the samples to be unlearned. Instead, it relies on energy-based proxies to guide the unlearning process. We prove that this induces gradients equivalent to Flow Matching toward a soft mass-subtracted target, and validate the framework through experiments on 2D and image domains, supported by interpretable visualizations and quantitative evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ContinualFlowï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ Flow Matching åœ¨ç”Ÿæˆæ¨¡å‹ä¸­å®ç°å®šå‘é—å¿˜ï¼ˆunlearningï¼‰çš„åŸåˆ™æ€§æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸€ç§åŸºäºèƒ½é‡çš„é‡åŠ æƒæŸå¤±ï¼ˆenergy-based reweighting lossï¼‰ï¼Œåœ¨æ— éœ€ä»å¤´è®­ç»ƒæˆ–ç›´æ¥è®¿é—®å¾…é—å¿˜æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤ŸæŸ”æ€§åœ°ä»æ•°æ®åˆ†å¸ƒä¸­ç§»é™¤ä¸å¸Œæœ›å‡ºç°çš„åŒºåŸŸã€‚ContinualFlow ä¾é åŸºäºèƒ½é‡çš„ä»£ç†ï¼ˆenergy-based proxiesï¼‰å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œç ”ç©¶åœ¨ç†è®ºä¸Šè¯æ˜äº†å…¶äº§ç”Ÿçš„æ¢¯åº¦ç­‰ä»·äºå‘ä¸€ä¸ªæŸ”æ€§è´¨é‡æ‰£é™¤ï¼ˆsoft mass-subtractedï¼‰ç›®æ ‡çš„ Flow Matchingã€‚é€šè¿‡åœ¨äºŒç»´ç©ºé—´å’Œå›¾åƒé¢†åŸŸçš„å®éªŒéªŒè¯ï¼Œç»“åˆå¯è§£é‡Šçš„å¯è§†åŒ–åˆ†æå’Œå®šé‡è¯„ä¼°ï¼Œè¯¥æ¡†æ¶å±•ç°äº†åœ¨ç”Ÿæˆå¼å­¦ä¹ ä¸­è¿›è¡Œç²¾å‡†ã€é«˜æ•ˆé—å¿˜çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the ICML 2025 Workshop on Machine Unlearning for Generative AI (MUGen @ ICML25, Vancouver, July 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.18747v1",
      "published_date": "2025-06-23 15:20:58 UTC",
      "updated_date": "2025-06-23 15:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:05.646160+00:00"
    },
    {
      "arxiv_id": "2506.18739v1",
      "title": "On the Existence of Universal Simulators of Attention",
      "title_zh": "è®ºé€šç”¨æ³¨æ„åŠ›æ¨¡æ‹Ÿå™¨çš„å­˜åœ¨æ€§",
      "authors": [
        "Debanjan Dutta",
        "Faizanuddin Ansari",
        "Anish Chakrabarty",
        "Swagatam Das"
      ],
      "abstract": "Prior work on the learnability of transformers has established its capacity to approximate specific algorithmic patterns through training under restrictive architectural assumptions. Fundamentally, these arguments remain data-driven and therefore can only provide a probabilistic guarantee. Expressivity, on the contrary, has theoretically been explored to address the problems \\emph{computable} by such architecture. These results proved the Turing-completeness of transformers, investigated bounds focused on circuit complexity, and formal logic. Being at the crossroad between learnability and expressivity, the question remains: \\emph{can transformer architectures exactly simulate an arbitrary attention mechanism, or in particular, the underlying operations?} In this study, we investigate the transformer encoder's ability to simulate a vanilla attention mechanism. By constructing a universal simulator $\\mathcal{U}$ composed of transformer encoders, we present algorithmic solutions to identically replicate attention outputs and the underlying elementary matrix and activation operations via RASP, a formal framework for transformer computation. Our proofs, for the first time, show the existence of an algorithmically achievable data-agnostic solution, previously known to be approximated only by learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†transformer encoderæ¨¡æ‹Ÿvanilla attentionæœºåˆ¶çš„èƒ½åŠ›ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªç”±transformer encoderç»„æˆçš„universal simulator $\\mathcal{U}$ã€‚é€šè¿‡åˆ©ç”¨é’ˆå¯¹transformerè®¡ç®—çš„æ­£å¼æ¡†æ¶RASPï¼Œä½œè€…æå‡ºäº†èƒ½å¤Ÿç²¾ç¡®å¤åˆ¶attentionè¾“å‡ºä»¥åŠåº•å±‚çŸ©é˜µå’Œæ¿€æ´»æ“ä½œçš„ç®—æ³•æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶çš„è¯æ˜é¦–æ¬¡è¡¨æ˜ï¼Œå­˜åœ¨ä¸€ç§åœ¨ç®—æ³•ä¸Šå¯å®ç°çš„data-agnosticè§£å†³æ–¹æ¡ˆï¼Œè€Œæ­¤å‰è¿™ç±»æ“ä½œè¢«è®¤ä¸ºåªèƒ½é€šè¿‡å­¦ä¹ (learning)è¿›è¡Œè¿‘ä¼¼(approximate)ã€‚è¯¥å·¥ä½œå¤„äºå¯å­¦ä¹ æ€§(learnability)ä¸è¡¨è¾¾èƒ½åŠ›(expressivity)çš„äº¤æ±‡ç‚¹ï¼Œä»ç†è®ºä¸Šè§£å†³äº†transformeræ¶æ„æ˜¯å¦èƒ½ç²¾ç¡®æ¨¡æ‹Ÿä»»æ„attentionæœºåˆ¶åŠå…¶åº•å±‚è¿ç®—çš„æ ¸å¿ƒé—®é¢˜ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç»“æ„åŒ–è®¡ç®—é€»è¾‘æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18739v1",
      "published_date": "2025-06-23 15:15:25 UTC",
      "updated_date": "2025-06-23 15:15:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:28.279812+00:00"
    },
    {
      "arxiv_id": "2506.18731v1",
      "title": "Deep CNN Face Matchers Inherently Support Revocable Biometric Templates",
      "title_zh": "æ·±åº¦ CNN äººè„¸åŒ¹é…å™¨åŸç”Ÿæ”¯æŒå¯æ’¤é”€ç”Ÿç‰©è¯†åˆ«æ¨¡æ¿",
      "authors": [
        "Aman Bhatta",
        "Michael C. King",
        "Kevin W. Bowyer"
      ],
      "abstract": "One common critique of biometric authentication is that if an individual's biometric is compromised, then the individual has no recourse. The concept of revocable biometrics was developed to address this concern. A biometric scheme is revocable if an individual can have their current enrollment in the scheme revoked, so that the compromised biometric template becomes worthless, and the individual can re-enroll with a new template that has similar recognition power. We show that modern deep CNN face matchers inherently allow for a robust revocable biometric scheme. For a given state-of-the-art deep CNN backbone and training set, it is possible to generate an unlimited number of distinct face matcher models that have both (1) equivalent recognition power, and (2) strongly incompatible biometric templates. The equivalent recognition power extends to the point of generating impostor and genuine distributions that have the same shape and placement on the similarity dimension, meaning that the models can share a similarity threshold for a 1-in-10,000 false match rate. The biometric templates from different model instances are so strongly incompatible that the cross-instance similarity score for images of the same person is typically lower than the same-instance similarity score for images of different persons. That is, a stolen biometric template that is revoked is of less value in attempting to match the re-enrolled identity than the average impostor template. We also explore the feasibility of using a Vision Transformer (ViT) backbone-based face matcher in the revocable biometric system proposed in this work and demonstrate that it is less suitable compared to typical ResNet-based deep CNN backbones.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿç‰©ç‰¹å¾è¯†åˆ«ä¸­å‡­è¯æ³„éœ²åéš¾ä»¥è¡¥æ•‘çš„é—®é¢˜ï¼Œè¯æ˜æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ (Deep CNN) äººè„¸åŒ¹é…å™¨åŸç”Ÿæ”¯æŒå¯æ’¤é”€ç”Ÿç‰©ç‰¹å¾æ¨¡æ¿ (Revocable Biometric Templates)ã€‚é€šè¿‡åœ¨ç›¸åŒçš„æ·±åº¦å­¦ä¹ éª¨å¹²ç½‘ç»œ (Backbone) å’Œè®­ç»ƒé›†ä¸Šç”Ÿæˆå¤šä¸ªä¸åŒçš„åŒ¹é…æ¨¡å‹ï¼Œç ”ç©¶è€…å®ç°äº†è¯†åˆ«èƒ½åŠ›ç­‰æ•ˆä½†æ¨¡æ¿äº’ä¸ç›¸å®¹çš„ç³»ç»Ÿã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥å…±äº«ç›¸åŒçš„è¯†åˆ«é˜ˆå€¼ï¼Œä¸”ä¸åŒå®ä¾‹é—´çš„æ¨¡æ¿ä¸å…¼å®¹æ€§æå¼ºï¼ŒåŒä¸€äººåœ¨è·¨å®ä¾‹æ¨¡å‹ä¸‹çš„ç›¸ä¼¼åº¦åˆ†æ•°ç”šè‡³ä½äºåŒä¸€å®ä¾‹ä¸‹çš„å†’å……è€…å¾—åˆ†ã€‚è¿™æ„å‘³ç€æ³„éœ²çš„æ¨¡æ¿åœ¨è¢«æ’¤é”€å¹¶é‡æ–°æ³¨å†Œåå°†å½»åº•å¤±å»åˆ©ç”¨ä»·å€¼ï¼Œä¸ºç”Ÿç‰©ç‰¹å¾å®‰å…¨æä¾›äº†æœ‰åŠ›ä¿éšœã€‚ç ”ç©¶è¿˜å¯¹æ¯”å‘ç°ï¼Œç›¸æ¯”äº Vision Transformer (ViT)ï¼Œä¼ ç»Ÿçš„ ResNet æ¶æ„åœ¨æ„å»ºæ­¤ç±»å¯æ’¤é”€ç³»ç»Ÿæ–¹é¢å…·æœ‰æ›´å¥½çš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18731v1",
      "published_date": "2025-06-23 15:09:04 UTC",
      "updated_date": "2025-06-23 15:09:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:27.792475+00:00"
    },
    {
      "arxiv_id": "2506.18729v2",
      "title": "MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners",
      "title_zh": "MuseControlLiteï¼šåŸºäºè½»é‡çº§æ¡ä»¶å™¨çš„å¤šåŠŸèƒ½éŸ³ä¹ç”Ÿæˆ",
      "authors": [
        "Fang-Duo Tsai",
        "Shih-Lun Wu",
        "Weijaw Lee",
        "Sheng-Ping Yang",
        "Bo-Rui Chen",
        "Hao-Chung Cheng",
        "Yi-Hsuan Yang"
      ],
      "abstract": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. The key finding is that positional embeddings, which have been seldom used by text-to-music generation models in the conditioner for text conditions, are critical when the condition of interest is a function of time. Using melody control as an example, our experiments show that simply adding rotary positional embeddings to the decoupled cross-attention layers increases control accuracy from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion Transformer model of Stable Audio Open. We evaluate various forms of musical attribute control, audio inpainting, and audio outpainting, demonstrating improved controllability over MusicGen-Large and Stable Audio Open ControlNet at a significantly lower fine-tuning cost, with only 85M trainble parameters. Source code, model checkpoints, and demo examples are available at: https://musecontrollite.github.io/web/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MuseControlLiteï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¾®è°ƒæ–‡æœ¬åˆ°éŸ³ä¹ (text-to-music) ç”Ÿæˆæ¨¡å‹çš„è½»é‡çº§æœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡å„ç§éšæ—¶é—´å˜åŒ–çš„éŸ³ä¹å±æ€§å’Œå‚è€ƒéŸ³é¢‘ä¿¡å·å®ç°ç²¾ç¡®è°ƒèŠ‚ã€‚è¯¥è®ºæ–‡çš„å…³é”®å‘ç°æ˜¯ï¼Œä½ç½®åµŒå…¥ (positional embeddings) å¯¹äºå¤„ç†éšæ—¶é—´å˜åŒ–çš„æ§åˆ¶æ¡ä»¶è‡³å…³é‡è¦ã€‚ä»¥æ—‹å¾‹æ§åˆ¶ä¸ºä¾‹ï¼Œå®éªŒè¯æ˜åœ¨è§£è€¦äº¤å‰æ³¨æ„åŠ› (decoupled cross-attention) å±‚ä¸­åŠ å…¥æ—‹è½¬ä½ç½®åµŒå…¥ (rotary positional embeddings) å¯å°†æ§åˆ¶å‡†ç¡®ç‡ä» 56.6% æå‡è‡³ 61.1%ã€‚ç›¸æ¯”äºç°æœ‰çš„å¾®è°ƒæœºåˆ¶ï¼ŒMuseControlLite åœ¨ä½¿ç”¨ç›¸åŒçš„æ‰©æ•£ Transformer (Diffusion Transformer) é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‰€éœ€çš„å¯è®­ç»ƒå‚æ•°å‡å°‘äº† 6.75 å€ã€‚é€šè¿‡å¯¹éŸ³ä¹å±æ€§æ§åˆ¶ã€éŸ³é¢‘ä¿®å¤ (audio inpainting) å’ŒéŸ³é¢‘æ‰©å±• (audio outpainting) çš„è¯„ä¼°ï¼Œè¯¥æ¨¡å‹åœ¨ä»…æœ‰ 85M å¯è®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹ï¼Œå±•ç°å‡ºä¼˜äº MusicGen-Large å’Œ Stable Audio Open ControlNet çš„å¯æ§æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºä½æˆæœ¬ã€é«˜ç²¾åº¦çš„å¤šåŠŸèƒ½éŸ³ä¹ç”Ÿæˆæä¾›äº†é«˜æ•ˆçš„å¾®è°ƒæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by the 42nd International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.18729v2",
      "published_date": "2025-06-23 15:08:03 UTC",
      "updated_date": "2025-06-24 15:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:29.143456+00:00"
    },
    {
      "arxiv_id": "2506.18717v1",
      "title": "A Study of Dynamic Stock Relationship Modeling and S&P500 Price Forecasting Based on Differential Graph Transformer",
      "title_zh": "åŸºäºå·®åˆ†å›¾ Transformer çš„åŠ¨æ€è‚¡ç¥¨å…³ç³»å»ºæ¨¡ä¸æ ‡æ™®500ä»·æ ¼é¢„æµ‹ç ”ç©¶",
      "authors": [
        "Linyue Hu",
        "Qi Wang"
      ],
      "abstract": "Stock price prediction is vital for investment decisions and risk management, yet remains challenging due to markets' nonlinear dynamics and time-varying inter-stock correlations. Traditional static-correlation models fail to capture evolving stock relationships. To address this, we propose a Differential Graph Transformer (DGT) framework for dynamic relationship modeling and price prediction. Our DGT integrates sequential graph structure changes into multi-head self-attention via a differential graph mechanism, adaptively preserving high-value connections while suppressing noise. Causal temporal attention captures global/local dependencies in price sequences. We further evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across global/local/dual scopes as spatial-attention priors. Using 10 years of S&P 500 closing prices (z-score normalized; 64-day sliding windows), DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87). Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means clustering revealed \"high-volatility growth\" and \"defensive blue-chip\" stocks, with the latter showing lower errors (RMSE: 0.13) due to stable correlations. Kendall's Tau and Mutual Information excelled in volatile sectors. This study innovatively combines differential graph structures with Transformers, validating dynamic relationship modeling and identifying optimal correlation metrics/scopes. Clustering analysis supports tailored quantitative strategies. Our framework advances financial time-series prediction through dynamic modeling and cross-asset interaction analysis.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è‚¡å¸‚éçº¿æ€§åŠ¨æ€å’Œéšæ—¶é—´å˜åŒ–çš„è‚¡ç¥¨å…³è”æ€§ï¼Œæå‡ºäº† Differential Graph Transformer (DGT) æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€å…³ç³»å»ºæ¨¡å’Œè‚¡ä»·é¢„æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®åˆ†å›¾æœºåˆ¶ (differential graph mechanism) å°†åºåˆ—å›¾ç»“æ„å˜åŒ–æ•´åˆåˆ°å¤šå¤´è‡ªæ³¨æ„åŠ› (multi-head self-attention) ä¸­ï¼Œåœ¨æŠ‘åˆ¶å™ªå£°çš„åŒæ—¶è‡ªé€‚åº”åœ°ä¿ç•™é«˜ä»·å€¼è¿æ¥ã€‚DGT ç»“åˆäº†å› æœæ—¶é—´æ³¨æ„åŠ› (Causal temporal attention) ä»¥æ•è·ä»·æ ¼åºåˆ—çš„å…¨å±€ä¸å±€éƒ¨ä¾èµ–ï¼Œå¹¶å¯¹æ¯”è¯„ä¼°äº† Pearsonã€Mutual Information å’Œ Kendall's Tau ç­‰å¤šç§æŒ‡æ ‡ä½œä¸ºç©ºé—´æ³¨æ„åŠ›çš„å…ˆéªŒã€‚åœ¨ S\\&P 500 æŒ‡æ•° 10 å¹´æ•°æ®çš„å®éªŒä¸­ï¼ŒDGT çš„é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äº GRU åŸºå‡†æ¨¡å‹ï¼Œä¸”é‡‡ç”¨å…¨å±€ Kendall's Tau çŸ©é˜µè¾¾åˆ°äº†æœ€ä¼˜ç»“æœã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡ K-means èšç±»è¯†åˆ«äº†ä¸åŒæ³¢åŠ¨ç‰¹å¾çš„è‚¡ç¥¨ç±»åˆ«ï¼Œå‘ç°é˜²å¾¡å‹è“ç­¹è‚¡å› å…³è”æ€§æ›´ç¨³å®šè€Œå…·æœ‰æ›´ä½çš„é¢„æµ‹è¯¯å·®ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ›æ–°æ€§åœ°å°†å¾®åˆ†å›¾ç»“æ„ä¸ Transformer ç»“åˆï¼Œä¸ºå¤„ç†è·¨èµ„äº§äº¤äº’å’Œé‡‘èæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†é«˜æ•ˆçš„åŠ¨æ€å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18717v1",
      "published_date": "2025-06-23 14:53:31 UTC",
      "updated_date": "2025-06-23 14:53:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:38.807354+00:00"
    },
    {
      "arxiv_id": "2506.18714v1",
      "title": "Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement",
      "title_zh": "é¢å‘éŸ³ç´ çº§ DNN è¯­éŸ³å¢å¼ºçš„é¢‘ç‡åŠ æƒè®­ç»ƒæŸå¤±",
      "authors": [
        "Nasser-Eddine Monir",
        "Paul Magron",
        "Romain Serizel"
      ],
      "abstract": "Recent advances in deep learning have significantly improved multichannel speech enhancement algorithms, yet conventional training loss functions such as the scale-invariant signal-to-distortion ratio (SDR) may fail to preserve fine-grained spectral cues essential for phoneme intelligibility. In this work, we propose perceptually-informed variants of the SDR loss, formulated in the time-frequency domain and modulated by frequency-dependent weighting schemes. These weights are designed to emphasize time-frequency regions where speech is prominent or where the interfering noise is particularly strong. We investigate both fixed and adaptive strategies, including ANSI band-importance weights, spectral magnitude-based weighting, and dynamic weighting based on the relative amount of speech and noise. We train the FaSNet multichannel speech enhancement model using these various losses. Experimental results show that while standard metrics such as the SDR are only marginally improved, their perceptual frequency-weighted counterparts exhibit a more substantial improvement. Besides, spectral and phoneme-level analysis indicates better consonant reconstruction, which points to a better preservation of certain acoustic cues.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šé€šé“è¯­éŸ³å¢å¼ºä»»åŠ¡ï¼ŒæŒ‡å‡ºå¸¸è§„çš„ scale-invariant signal-to-distortion ratio (SDR) æŸå¤±å‡½æ•°éš¾ä»¥ä¿ç•™å¯¹éŸ³ç´ è¯†åˆ«è‡³å…³é‡è¦çš„ç»†ç²’åº¦é¢‘è°±ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åœ¨æ—¶é¢‘åŸŸå†…åº”ç”¨é¢‘ç‡åŠ æƒæ–¹æ¡ˆçš„æ„ŸçŸ¥ SDR æŸå¤±å˜ä½“ï¼Œæ—¨åœ¨é€šè¿‡æƒé‡è°ƒèŠ‚å¼ºè°ƒè¯­éŸ³æ˜¾è‘—æˆ–å™ªå£°å¹²æ‰°ä¸¥é‡çš„åŒºåŸŸã€‚ç ”ç©¶æ¢è®¨äº†åŒ…æ‹¬ ANSI band-importance weightsã€åŸºäºé¢‘è°±å¹…åº¦çš„åŠ æƒä»¥åŠåŸºäºè¯­éŸ³å™ªå£°æ¯”çš„åŠ¨æ€åŠ æƒåœ¨å†…çš„å¤šç§å›ºå®šä¸è‡ªé€‚åº”ç­–ç•¥ï¼Œå¹¶å°†å…¶åº”ç”¨äº FaSNet æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ä¼ ç»Ÿçš„ SDR æŒ‡æ ‡æå‡æœ‰é™ï¼Œä½†æ„ŸçŸ¥é¢‘ç‡åŠ æƒæŒ‡æ ‡å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼ŒéŸ³ç´ å±‚é¢çš„åˆ†æè¡¨æ˜è¯¥æ–¹æ³•å®ç°äº†æ›´å¥½çš„è¾…éŸ³é‡å»ºï¼Œèƒ½æ›´æœ‰æ•ˆåœ°ä¿ç•™è¯­éŸ³å¢å¼ºè¿‡ç¨‹ä¸­çš„å…³é”®å£°å­¦çº¿ç´¢ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This is the preprint of the paper submitted to the 26th IEEE International Workshop on Multimedia Signal Processing (MMSP)",
      "pdf_url": "https://arxiv.org/pdf/2506.18714v1",
      "published_date": "2025-06-23 14:52:34 UTC",
      "updated_date": "2025-06-23 14:52:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:38.405091+00:00"
    },
    {
      "arxiv_id": "2506.18710v3",
      "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ•™è‚²å­¦çŸ¥è¯†çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Maxime LeliÃ¨vre",
        "Amy Waldock",
        "Meng Liu",
        "Natalia ValdÃ©s Aspillaga",
        "Alasdair Mackintosh",
        "MarÃ­a JosÃ© Ogando Portela",
        "Jared Lee",
        "Paul Atherton",
        "Robin A. A. Ince",
        "Oliver G. B. Garrod"
      ],
      "abstract": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† The Pedagogy Benchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è·¨å­¦ç§‘æ•™å­¦çŸ¥è¯† (Cross-Domain Pedagogical Knowledge, CDPK) å’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚ä¸æ®‹ç–¾ (Special Education Needs and Disability, SEND) æ•™å­¦çŸ¥è¯†æ–¹é¢è¡¨ç°çš„æ–°å‹æ•°æ®é›†ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å¦‚ MMLU ä¸»è¦ä¾§é‡äºå­¦ç§‘å†…å®¹çŸ¥è¯†ï¼Œè€Œè¯¥åŸºå‡†å¡«è¡¥äº†æ¨¡å‹åœ¨ç†è§£æ•™å­¦æ–¹æ³•ä¸å®è·µï¼ˆPedagogyï¼‰æ–¹é¢çš„è¯„ä¼°ç©ºç™½ã€‚è¯¥æ•°æ®é›†åŸºäºæ•™å¸ˆä¸“ä¸šå‘å±•è€ƒè¯•é¢˜ç›®æ„å»ºï¼Œæ¶µç›–äº†æ•™å­¦ç­–ç•¥å’Œè¯„ä¼°æ–¹æ³•ç­‰æ ¸å¿ƒå­é¢†åŸŸã€‚å®éªŒå¯¹ 97 ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶å‡†ç¡®ç‡åœ¨ 28% åˆ° 89% ä¹‹é—´ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†æ¨¡å‹æˆæœ¬ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ä»¥åŠ Pareto ä»·å€¼å‰æ²¿çš„æ¼”å˜è¶‹åŠ¿ã€‚è¿™é¡¹å·¥ä½œä¸ºè¡¡é‡æ¨¡å‹ç†è§£æ•™å­¦æ¦‚å¿µã€å“åº”å­¦ä¹ è€…éœ€æ±‚ä»¥åŠåœ¨æ•™è‚²åœºæ™¯ä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½² Generative AI æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18710v3",
      "published_date": "2025-06-23 14:49:01 UTC",
      "updated_date": "2025-07-01 15:49:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:40.399839+00:00"
    },
    {
      "arxiv_id": "2506.18701v1",
      "title": "Matrix-Game: Interactive World Foundation Model",
      "title_zh": "Matrix-Gameï¼šäº¤äº’å¼ä¸–ç•ŒåŸºç¡€æ¨¡å‹",
      "authors": [
        "Yifan Zhang",
        "Chunli Peng",
        "Boyang Wang",
        "Puyi Wang",
        "Qingcheng Zhu",
        "Fei Kang",
        "Biao Jiang",
        "Zedong Gao",
        "Eric Li",
        "Yang Liu",
        "Yahui Zhou"
      ],
      "abstract": "We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Matrix-Gameï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯æ§æ¸¸æˆä¸–ç•Œç”Ÿæˆçš„äº¤äº’å¼ä¸–ç•ŒåŸºç¡€æ¨¡å‹ (Interactive World Foundation Model)ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆé€šè¿‡å¤§è§„æ¨¡æ— æ ‡ç­¾é¢„è®­ç»ƒè¿›è¡Œç¯å¢ƒç†è§£ï¼Œéšååˆ©ç”¨å¸¦æœ‰ç²¾ç»†åŠ¨ä½œæ ‡æ³¨çš„ Matrix-Game-MC æ•°æ®é›†è¿›è¡Œäº¤äº’å¼è§†é¢‘ç”Ÿæˆè®­ç»ƒã€‚Matrix-Game æ‹¥æœ‰è¶…è¿‡ 170 äº¿ä¸ªå‚æ•°ï¼Œé‡‡ç”¨å›¾åƒåˆ°ä¸–ç•Œ (Image-to-World) çš„ç”ŸæˆèŒƒå¼ï¼Œèƒ½å¤Ÿæ ¹æ®å‚è€ƒå›¾åƒå’Œç”¨æˆ·åŠ¨ä½œç²¾ç¡®æ§åˆ¶è§’è‰²è¡Œä¸ºä¸æ‘„åƒæœºç§»åŠ¨ï¼ŒåŒæ—¶ä¿æŒé«˜è§†è§‰è´¨é‡å’Œæ—¶é—´ç›¸å¹²æ€§ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€å‘äº† GameWorld Score ç»Ÿä¸€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è´¨é‡ã€åŠ¨ä½œå¯æ§æ€§å’Œç‰©ç†è§„åˆ™ç†è§£ç­‰æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMatrix-Game åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äº Oasis å’Œ MineWorld ç­‰ç°æœ‰å¼€æºæ¨¡å‹ï¼Œå°¤å…¶åœ¨å¯æ§æ€§å’Œç‰©ç†ä¸€è‡´æ€§æ–¹é¢æå‡æ˜¾è‘—ã€‚åŒç›²äººå·¥è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ„ŸçŸ¥çœŸå®ä¸”ç²¾ç¡®å¯æ§çš„æ¸¸æˆè§†é¢‘æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2506.18701v1",
      "published_date": "2025-06-23 14:40:49 UTC",
      "updated_date": "2025-06-23 14:40:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:50.209315+00:00"
    },
    {
      "arxiv_id": "2506.18689v2",
      "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments",
      "title_zh": "NOVAï¼šé¢å‘éç»“æ„åŒ–åŠæ—  GPS ç¯å¢ƒä¸‹é«˜é€Ÿç›®æ ‡è·Ÿè¸ªçš„ä»¥ç›®æ ‡ä¸ºä¸­å¿ƒè§†è§‰è‡ªä¸»å¯¼èˆª",
      "authors": [
        "Alessandro Saviolo",
        "Giuseppe Loianno"
      ],
      "abstract": "Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NOVAï¼Œä¸€ä¸ªå®Œå…¨åŸºäºæœºè½½ä¼ æ„Ÿå™¨çš„ä»¥ç›®æ ‡ä¸ºä¸­å¿ƒçš„(object-centric)è‡ªä¸»å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éç»“æ„åŒ–ä¸”æ— GPSç¯å¢ƒä¸‹çš„é«˜é€Ÿç›®æ ‡è·Ÿè¸ªéš¾é¢˜ã€‚è¯¥æ¡†æ¶ä»…ä¾é åŒç›®ç›¸æœº(stereo camera)å’Œæƒ¯æ€§æµ‹é‡å•å…ƒ(IMU)ï¼Œå°†æ„ŸçŸ¥ã€ä¼°è®¡ä¸æ§åˆ¶å®Œå…¨æ„å»ºåœ¨ç›®æ ‡çš„å‚è€ƒç³»ä¸­ï¼Œé€šè¿‡è½»é‡çº§æ£€æµ‹å™¨ã€ç«‹ä½“æ·±åº¦è¡¥å…¨(stereo depth completion)å’Œç›´æ–¹å›¾æ»¤æ³¢æŠ€æœ¯åœ¨é®æŒ¡åŠå™ªå£°ä¸‹æ¨æ–­ç›®æ ‡è·ç¦»ã€‚ç³»ç»Ÿåˆ©ç”¨è§†è§‰æƒ¯æ€§çŠ¶æ€ä¼°è®¡å™¨(visual-inertial state estimator)è·å–6è‡ªç”±åº¦ä½å§¿ï¼Œå¹¶é…åˆéçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶(NMPC)è§„åˆ’åŠ¨æ€è½¨è¿¹ï¼ŒåŒæ—¶åˆ©ç”¨é«˜é˜¶æ§åˆ¶å±éšœå‡½æ•°(Control Barrier Functions)ä»æ·±åº¦æ•°æ®ä¸­æå–ç¢°æ’ç‚¹å®ç°å®æ—¶é¿éšœã€‚åœ¨åŸå¸‚è¿·å®«å’Œæ£®æ—å°å¾„ç­‰çœŸå®åœºæ™¯çš„æµ‹è¯•è¡¨æ˜ï¼ŒNOVAåœ¨å…‰ç…§å‰§å˜å’ŒGPSç¼ºå¤±çš„æƒ…å†µä¸‹ä¾ç„¶ç¨³å¥ï¼Œèƒ½å¤Ÿå®ç°è¶…è¿‡50 km/hçš„é«˜é€Ÿæ•æ·è·Ÿéšã€‚è¯¥æˆæœè¯æ˜äº†åœ¨æ— éœ€å¤–éƒ¨å®šä½æˆ–ç¯å¢ƒé¢„ç½®åœ°å›¾çš„æƒ…å†µä¸‹ï¼Œä»…å‡­æœºè½½è§†è§‰æ„ŸçŸ¥å³å¯åœ¨é‡å¤–å®ç°é«˜æ€§èƒ½çš„è‡ªä¸»ç›®æ ‡è·Ÿè¸ªã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18689v2",
      "published_date": "2025-06-23 14:28:30 UTC",
      "updated_date": "2025-07-07 16:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:51.652962+00:00"
    },
    {
      "arxiv_id": "2506.18683v1",
      "title": "SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification",
      "title_zh": "SIM-Netï¼šä¸€ç§åˆ©ç”¨ç”± RGB å›¾åƒæ¨æ–­çš„ä¸‰ç»´ç‰©ä½“å½¢çŠ¶ç‚¹äº‘è¿›è¡ŒäºŒç»´åˆ†ç±»çš„å¤šæ¨¡æ€èåˆç½‘ç»œ",
      "authors": [
        "Youcef Sklab",
        "Hanane Ariouat",
        "Eric Chenin",
        "Edi Prifti",
        "Jean-Daniel Zucker"
      ],
      "abstract": "We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image classification architecture that integrates 3D point cloud representations inferred directly from RGB images. Our key contribution lies in a pixel-to-point transformation that converts 2D object masks into 3D point clouds, enabling the fusion of texture-based and geometric features for enhanced classification performance. SIM-Net is particularly well-suited for the classification of digitized herbarium specimens (a task made challenging by heterogeneous backgrounds), non-plant elements, and occlusions that compromise conventional image-based models. To address these issues, SIM-Net employs a segmentation-based preprocessing step to extract object masks prior to 3D point cloud generation. The architecture comprises a CNN encoder for 2D image features and a PointNet-based encoder for geometric features, which are fused into a unified latent space. Experimental evaluations on herbarium datasets demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several transformer-based state-of-the-art architectures, highlighting the benefits of incorporating 3D structural reasoning into 2D image classification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SIM-Netï¼Œä¸€ç§èåˆäº†ä» RGB å›¾åƒæ¨æ–­å‡ºçš„ 3D Point Cloud è¡¨ç¤ºçš„æ–°å‹ 2D å›¾åƒåˆ†ç±»æ¶æ„ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº† Pixel-to-point è½¬æ¢æŠ€æœ¯ï¼Œå°† 2D å¯¹è±¡æ©ç è½¬åŒ–ä¸º 3D Point Cloudï¼Œä»è€Œå®ç°çº¹ç†ç‰¹å¾ä¸å‡ ä½•ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚é’ˆå¯¹æ•°å­—åŒ–æ¤ç‰©æ ‡æœ¬åˆ†ç±»ä¸­å­˜åœ¨çš„å¼‚æ„èƒŒæ™¯å’Œé®æŒ¡ç­‰æŒ‘æˆ˜ï¼ŒSIM-Net é€šè¿‡åŸºäºåˆ†å‰²çš„é¢„å¤„ç†æå–å¯¹è±¡æ©ç ï¼Œå¹¶ç»“åˆ CNN ç¼–ç å™¨ä¸ PointNet ç¼–ç å™¨åœ¨ç»Ÿä¸€çš„ Latent Space ä¸­å¤„ç†ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSIM-Net åœ¨æ¤ç‰©æ ‡æœ¬æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äº ResNet101ï¼Œå‡†ç¡®ç‡å’Œ F-score åˆ†åˆ«æå‡äº† 9.9% å’Œ 12.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„è¿˜è¶…è¶Šäº†å¤šç§åŸºäº Transformer çš„å‰æ²¿æ¨¡å‹ï¼Œå……åˆ†å±•ç¤ºäº†åœ¨ 2D åˆ†ç±»ä»»åŠ¡ä¸­å¼•å…¥ 3D ç»“æ„æ¨ç†çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 9 figures, 14 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.18683v1",
      "published_date": "2025-06-23 14:25:40 UTC",
      "updated_date": "2025-06-23 14:25:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:57.108590+00:00"
    },
    {
      "arxiv_id": "2506.18682v1",
      "title": "Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios",
      "title_zh": "è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹åŸºäºå¤šå°ºåº¦å…‰è°±æ³¨æ„åŠ›æ¨¡å—çš„é«˜å…‰è°±åˆ†å‰²",
      "authors": [
        "Imad Ali Shah",
        "Jiarong Li",
        "Tim Brophy",
        "Martin Glavin",
        "Edward Jones",
        "Enda Ward",
        "Brian Deegan"
      ],
      "abstract": "Recent advances in autonomous driving (AD) have highlighted the potential of Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly in challenging weather and lighting conditions. However, efficiently processing its high-dimensional spectral data remains a significant challenge. This paper introduces a Multi-scale Spectral Attention Module (MSAM) that enhances spectral feature extraction through three parallel 1D convolutions with varying kernel sizes between 1 to 11, coupled with an adaptive feature aggregation mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our proposed UNet-MSAM achieves significant improvements in semantic segmentation performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and Hyperspectral City v2. Our comprehensive experiments demonstrate that with minimal computational overhead (on average 0.02% in parameters and 0.82% GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets. Through extensive ablation studies, we have established that multi-scale kernel combinations perform better than single-scale configurations. These findings demonstrate the potential of HSI processing for AD and provide valuable insights into designing robust, multi-scale spectral feature extractors for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶(Autonomous Driving)åœºæ™¯ä¸­é«˜å…‰è°±æˆåƒ(Hyperspectral Imaging)å¤„ç†é«˜ç»´å…‰è°±æ•°æ®çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šå°ºåº¦å…‰è°±æ³¨æ„åŠ›æ¨¡å—(Multi-scale Spectral Attention Module, MSAM)ã€‚è¯¥æ¨¡å—é€šè¿‡ä¸‰ç§ä¸åŒå·ç§¯æ ¸å¤§å°çš„å¹¶è¡Œ1Då·ç§¯ç»“åˆè‡ªé€‚åº”ç‰¹å¾èšåˆæœºåˆ¶ï¼Œæ—¨åœ¨æ˜¾è‘—å¢å¼ºå…‰è°±ç‰¹å¾çš„æå–èƒ½åŠ›ã€‚ç ”ç©¶å°†MSAMé›†æˆåˆ°UNetçš„è·³è·ƒè¿æ¥(Skip Connections)ä¸­æ„å»ºäº†UNet-MSAMæ¨¡å‹ï¼Œå¹¶åœ¨HyKo-VIS v2ã€HSI-Drive v2åŠHyperspectral City v2ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯­ä¹‰åˆ†å‰²å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä»…å¢åŠ æå°è®¡ç®—å¼€é”€ï¼ˆå¹³å‡å¢åŠ 0.02%å‚æ•°é‡å’Œ0.82% GFLOPSï¼‰çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡äº¤å¹¶æ¯”(mean IoU)å’Œå¹³å‡F1åˆ†æ•°(mF1)ä¸Šåˆ†åˆ«å®ç°äº†3.61%å’Œ3.80%çš„æå‡ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®å¤šå°ºåº¦å·ç§¯æ ¸ç»„åˆçš„æ€§èƒ½ä¼˜äºå•å°ºåº¦é…ç½®ï¼Œä¸ºè®¾è®¡ç¨³å¥ã€é«˜æ•ˆçš„å¤šå°ºåº¦å…‰è°±ç‰¹å¾æå–å™¨æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18682v1",
      "published_date": "2025-06-23 14:24:20 UTC",
      "updated_date": "2025-06-23 14:24:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:52:58.832512+00:00"
    },
    {
      "arxiv_id": "2507.01975v1",
      "title": "Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows",
      "title_zh": "ç”¨äºåŠ é€ŸæµåŠ¨æ¨¡æ‹Ÿçš„å¯å­¦ä¹ å¯å¾®æœ‰é™ä½“ç§¯æ±‚è§£å™¨",
      "authors": [
        "Mengtao Yan",
        "Qi Wang",
        "Haining Wang",
        "Ruizhi Chengze",
        "Yi Zhang",
        "Hongsheng Liu",
        "Zidong Wang",
        "Fan Yu",
        "Qi Qi",
        "Hao Sun"
      ],
      "abstract": "Simulation of fluid flows is crucial for modeling physical phenomena like meteorology, aerodynamics, and biomedicine. Classical numerical solvers often require fine spatiotemporal grids to satisfy stability, consistency, and convergence conditions, leading to substantial computational costs. Although machine learning has demonstrated better efficiency, they typically suffer from issues of interpretability, generalizability, and data dependency. Hence, we propose a learnable and differentiable finite volume solver, called LDSolver, designed for efficient and accurate simulation of fluid flows on spatiotemporal coarse grids. LDSolver comprises two key components: (1) a differentiable finite volume solver, and (2) an learnable module providing equivalent approximation for fluxes (derivatives and interpolations), and temporal error correction on coarse grids. Even with limited training data (e.g., only a few trajectories), our model could accelerate the simulation while maintaining a high accuracy with superior generalizability. Experiments on different flow systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver achieves state-of-the-art performance, surpassing baseline models with notable margins.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LDSolver çš„å¯å­¦ä¹ ä¸”å¯å¾®åˆ†çš„æœ‰é™ä½“ç§¯æ±‚è§£å™¨ (learnable and differentiable finite volume solver)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•°å€¼æ±‚è§£å™¨åœ¨ä¿è¯ç‰©ç†ç¨³å®šæ€§æ—¶å› ä¾èµ–ç²¾ç»†ç½‘æ ¼è€Œäº§ç”Ÿçš„é«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚LDSolver æ ¸å¿ƒç”±å¯å¾®åˆ†æœ‰é™ä½“ç§¯æ±‚è§£å™¨ä¸ä¸€ä¸ªå¯å­¦ä¹ æ¨¡å—ç»„æˆï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿåœ¨æ—¶ç©ºç²—ç½‘æ ¼ (coarse grids) ä¸Šå®ç°é€šé‡çš„ç­‰æ•ˆè¿‘ä¼¼å¹¶è¿›è¡Œæœ‰æ•ˆçš„æ—¶é—´è¯¯å·®æ ¡æ­£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®æå°‘çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä¹Ÿèƒ½åœ¨å¤§å¹…åŠ é€Ÿæµä½“æ¨¡æ‹Ÿè¿‡ç¨‹çš„åŒæ—¶ä¿æŒæé«˜çš„ç²¾åº¦ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ› (generalizability)ã€‚é€šè¿‡åœ¨ Burgersã€è¡°å‡æµ (decaying flows)ã€å—è¿«æµ (forced flows) å’Œå‰ªåˆ‡æµ (shear flows) ç­‰å¤šç§æµä½“ç³»ç»Ÿä¸Šçš„éªŒè¯ï¼ŒLDSolver å‡è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 12 figures, accepted at KDD 2025 (ACM SIGKDD Conference on Knowledge Discovery and Data Mining)",
      "pdf_url": "https://arxiv.org/pdf/2507.01975v1",
      "published_date": "2025-06-23 14:22:27 UTC",
      "updated_date": "2025-06-23 14:22:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:11.000160+00:00"
    },
    {
      "arxiv_id": "2506.18674v1",
      "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦éœ€è¦é’ˆå¯¹å¯¹è¯ä¼˜åŒ–çš„åˆ†è¯å™¨ï¼Ÿ",
      "authors": [
        "Raquel Ferrando",
        "Javier Conde",
        "Gonzalo MartÃ­nez",
        "Pedro Reviriego"
      ],
      "abstract": "The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ Large Language Models (LLMs) ä¸­ä½¿ç”¨é’ˆå¯¹å¯¹è¯ä¼˜åŒ–çš„ Tokenizers çš„å¿…è¦æ€§ï¼Œä»¥åº”å¯¹æ¨¡å‹è§„æ¨¡å¢é•¿å¸¦æ¥çš„è®¡ç®—ä¸èƒ½æºæˆæœ¬æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜è§‚å¯Ÿåˆ° Chatbots çš„äº¤äº’æ–‡æœ¬ç‰¹å¾ä¸é€šç”¨è®­ç»ƒè¯­æ–™å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå› æ­¤åˆ©ç”¨å…¬å¼€çš„å¯¹è¯è¯­æ–™åº“é‡æ–°è®¾è®¡äº† Tokenizer çš„è¯æ±‡è¡¨å¹¶è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§ Conversation-optimized Tokenizers åœ¨å¯¹è¯åœºæ™¯ä¸‹èƒ½æœ‰æ•ˆå‡å°‘ 5% åˆ° 10% çš„ Token æ•°é‡ï¼Œä»è€Œäº§ç”Ÿå¯è§‚çš„èŠ‚èƒ½æ•ˆç›Šã€‚æ­¤å¤–ï¼Œè¯¥ä¼˜åŒ–å¯¹åŸå§‹è®­ç»ƒè¯­æ–™çš„åˆ†è¯æ•ˆç‡å‡ ä¹æ²¡æœ‰è´Ÿé¢å½±å“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºç•¥å¾®çš„æ­£å‘æå‡ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é’ˆå¯¹ç‰¹å®šåº”ç”¨é¢†åŸŸå®šåˆ¶åˆ†è¯ç­–ç•¥åœ¨æå‡å¤§æ¨¡å‹è¿è¡Œæ•ˆç‡å’Œå¯æŒç»­æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18674v1",
      "published_date": "2025-06-23 14:18:46 UTC",
      "updated_date": "2025-06-23 14:18:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:18.749253+00:00"
    },
    {
      "arxiv_id": "2506.18668v1",
      "title": "Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping",
      "title_zh": "åŸºäºå¤šä¸­å¿ƒæ•°æ®é›†çš„çš®è‚¤ç™Œäºšå‹åˆ†ç±»ç»„ç»‡ç—…ç†å­¦åŸºç¡€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Pablo Meseguer",
        "RocÃ­o del Amor",
        "Valery Naranjo"
      ],
      "abstract": "Pretraining on large-scale, in-domain datasets grants histopathology foundation models (FM) the ability to learn task-agnostic data representations, enhancing transfer learning on downstream tasks. In computational pathology, automated whole slide image analysis requires multiple instance learning (MIL) frameworks due to the gigapixel scale of the slides. The diversity among histopathology FMs has highlighted the need to design real-world challenges for evaluating their effectiveness. To bridge this gap, our work presents a novel benchmark for evaluating histopathology FMs as patch-level feature extractors within a MIL classification framework. For that purpose, we leverage the AI4SkIN dataset, a multi-center cohort encompassing slides with challenging cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model - Silhouette Index (FM-SI), a novel metric to measure model consistency against distribution shifts. Our experimentation shows that extracting less biased features enhances classification performance, especially in similarity-based MIL classifiers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç™Œäºšå‹åˆ†ç±»ï¼Œåœ¨ä¸€ä¸ªå¤šä¸­å¿ƒæ•°æ®é›†ä¸Šå¯¹ç»„ç»‡ç—…ç†å­¦åŸºç¡€æ¨¡å‹(Foundation Models, FMs)è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œå°†åŸºç¡€æ¨¡å‹ä½œä¸ºå¤šå®ä¾‹å­¦ä¹ (Multiple Instance Learning, MIL)åˆ†ç±»æ¡†æ¶ä¸­çš„åˆ‡ç‰‡çº§ç‰¹å¾æå–å™¨ã€‚ç ”ç©¶åˆ©ç”¨äº†AI4SkINæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§çš„çš®è‚¤æ¢­å½¢ç»†èƒè‚¿ç˜¤(cutaneous spindle cell neoplasm)äºšå‹ã€‚ä¸ºäº†è¡¡é‡æ¨¡å‹åœ¨åº”å¯¹åˆ†å¸ƒåç§»(distribution shifts)æ—¶çš„ä¸€è‡´æ€§ï¼Œç ”ç©¶å®šä¹‰äº†ä¸€ä¸ªæ–°æŒ‡æ ‡ï¼šåŸºç¡€æ¨¡å‹è½®å»“æŒ‡æ•°(Foundation Model - Silhouette Index, FM-SI)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæå–åå·®è¾ƒå°çš„ç‰¹å¾èƒ½å¤Ÿæ˜¾è‘—æé«˜åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºç›¸ä¼¼æ€§çš„MILåˆ†ç±»å™¨ä¸­è¡¨ç°æ›´ä½³ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°ç»„ç»‡ç—…ç†å­¦åŸºç¡€æ¨¡å‹åœ¨çœŸå®åŒ»ç–—åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepeted for oral presentation at Medical Image Understanding and Analysis (MIUA) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18668v1",
      "published_date": "2025-06-23 14:12:16 UTC",
      "updated_date": "2025-06-23 14:12:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:22.570956+00:00"
    },
    {
      "arxiv_id": "2506.18658v1",
      "title": "Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation",
      "title_zh": "å†å²æŠ¥å‘Šå¼•å¯¼çš„åŒæ¨¡æ€å¹¶å‘å­¦ä¹ ç—…ç†æŠ¥å‘Šç”Ÿæˆ",
      "authors": [
        "Ling Zhang",
        "Boxiang Yun",
        "Qingli Li",
        "Yan Wang"
      ],
      "abstract": "Automated pathology report generation from Whole Slide Images (WSIs) faces two key challenges: (1) lack of semantic content in visual features and (2) inherent information redundancy in WSIs. To address these issues, we propose a novel Historical Report Guided \\textbf{Bi}-modal Concurrent Learning Framework for Pathology Report \\textbf{Gen}eration (BiGen) emulating pathologists' diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to provide rich semantic content, which retrieves WSI-relevant knowledge from pre-built medical knowledge bank by matching high-attention patches and (2) A bi-modal concurrent learning strategy instantiated via a learnable visual token and a learnable textual token to dynamically extract key visual features and retrieved knowledge, where weight-shared layers enable cross-modal alignment between visual features and knowledge features. Our multi-modal decoder integrates both modals for comprehensive diagnostic reports generation. Experiments on the PathText (BRCA) dataset demonstrate our framework's superiority, achieving state-of-the-art performance with 7.4\\% relative improvement in NLP metrics and 19.1\\% enhancement in classification metrics for Her-2 prediction versus existing methods. Ablation studies validate the necessity of our proposed modules, highlighting our method's ability to provide WSI-relevant rich semantic content and suppress information redundancy in WSIs. Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨åˆ‡ç‰‡å›¾åƒ(Whole Slide Images, WSIs)è‡ªåŠ¨ç”Ÿæˆç—…ç†æŠ¥å‘Šä¸­å­˜åœ¨çš„è§†è§‰è¯­ä¹‰ç¼ºå¤±å’Œä¿¡æ¯å†—ä½™é—®é¢˜ï¼Œæå‡ºäº†åä¸ºBiGençš„åŒæ¨¡æ€å¹¶å‘å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿç—…ç†å­¦å®¶çš„ä¸´åºŠè¯Šæ–­é€»è¾‘ï¼Œå¼•å…¥çŸ¥è¯†æ£€ç´¢æœºåˆ¶(knowledge retrieval mechanism)ï¼Œé€šè¿‡é«˜æ³¨æ„åŠ›æ–‘å—(high-attention patches)åŒ¹é…ä»é¢„å»ºçŸ¥è¯†åº“ä¸­æå–è¯­ä¹‰ä¿¡æ¯ã€‚BiGené‡‡ç”¨åŒæ¨¡æ€å¹¶å‘å­¦ä¹ ç­–ç•¥(bi-modal concurrent learning strategy)ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰å’Œæ–‡æœ¬æ ‡è®°(tokens)åŠ¨æ€æ•è·ç‰¹å¾ï¼Œå¹¶é€šè¿‡æƒé‡å…±äº«å±‚(weight-shared layers)å®ç°æ¨¡æ€é—´çš„ç²¾å‡†å¯¹é½ã€‚æœ€åï¼Œå¤šæ¨¡æ€è§£ç å™¨(multi-modal decoder)æ•´åˆå„æ¨¡æ€ä¿¡æ¯ä»¥ç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šã€‚åœ¨PathText (BRCA)æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŒ‡æ ‡ä¸Šå–å¾—äº†7.4%çš„ç›¸å¯¹æå‡ï¼Œå¹¶åœ¨Her-2é¢„æµ‹åˆ†ç±»ä¸­ä¼˜äºç°æœ‰æ–¹æ³•19.1%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†BiGenåœ¨æä¾›ç›¸å…³è¯­ä¹‰èƒŒæ™¯åŠæŠ‘åˆ¶WSIå†—ä½™æ•°æ®æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18658v1",
      "published_date": "2025-06-23 14:00:21 UTC",
      "updated_date": "2025-06-23 14:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:23.424597+00:00"
    },
    {
      "arxiv_id": "2506.18651v1",
      "title": "Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems",
      "title_zh": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é¢å‘ç»„é—´ä¸ç»„å†…ååŒçš„åŒå±‚è¡Œä¸ºä¸€è‡´æ€§",
      "authors": [
        "Shuocun Yang",
        "Huawen Hu",
        "Enze Shi",
        "Shu Zhang"
      ],
      "abstract": "Behavioral diversity in Multi-agent reinforcement learning(MARL) represents an emerging and promising research area. Prior work has largely centered on intra-group behavioral consistency in multi-agent systems, with limited attention given to behavioral consistency in multi-agent grouping scenarios. In this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL control method designed to explicitly regulate agent behaviors at both intra-group and inter-group levels. DLBC partitions agents into distinct groups and dynamically modulates behavioral diversity both within and between these groups. By dynamically modulating behavioral diversity within and between these groups, DLBC achieves enhanced division of labor through inter-group consistency, which constrains behavioral strategies across different groups. Simultaneously, intra-group consistency, achieved by aligning behavioral strategies within each group, fosters stronger intra-group cooperation. Crucially, DLBC's direct constraint of agent policy functions ensures its broad applicability across various algorithmic frameworks. Experimental results in various grouping cooperation scenarios demonstrate that DLBC significantly enhances both intra-group cooperative performance and inter-group task specialization, yielding substantial performance improvements. DLBC provides new ideas for behavioral consistency control of multi-intelligent body systems, and its potential for application in more complex tasks and dynamic environments can be further explored in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) ä¸­å¤šç»„åœºæ™¯ä¸‹è¡Œä¸ºä¸€è‡´æ€§ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åŒå±‚è¡Œä¸ºä¸€è‡´æ€§ (Dual-Level Behavioral Consistency, DLBC) æ§åˆ¶æ–¹æ³•ã€‚DLBC é€šè¿‡åœ¨ç»„å†…å’Œç»„é—´ä¸¤ä¸ªå±‚é¢æ˜¾å¼è°ƒèŠ‚æ™ºèƒ½ä½“è¡Œä¸ºï¼Œæ—¨åœ¨åŠ¨æ€å¹³è¡¡ç³»ç»Ÿçš„è¡Œä¸ºå¤šæ ·æ€§ã€‚åœ¨ç»„é—´å±‚é¢ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€è‡´æ€§çº¦æŸé™åˆ¶ä¸åŒç»„çš„è¡Œä¸ºç­–ç•¥ï¼Œä»è€Œå¼ºåŒ–åˆ†å·¥åä½œï¼›åœ¨ç»„å†…å±‚é¢ï¼Œåˆ™é€šè¿‡å¯¹é½ç»„å†…æˆå‘˜çš„è¡Œä¸ºç­–ç•¥æ¥å¢å¼ºåˆä½œã€‚ç”±äº DLBC ç›´æ¥å¯¹æ™ºèƒ½ä½“çš„ç­–ç•¥å‡½æ•° (policy functions) è¿›è¡Œçº¦æŸï¼Œå®ƒå±•ç°å‡ºäº†æé«˜çš„ç®—æ³•æ¡†æ¶é€šç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLBC åœ¨å¤šç§åˆ†ç»„åä½œåœºæ™¯ä¸­æ˜¾è‘—æå‡äº†ç»„å†…åä½œæ€§èƒ½å’Œç»„é—´ä»»åŠ¡ä¸“ä¸šåŒ–æ°´å¹³ã€‚è¯¥ç ”ç©¶ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¡Œä¸ºä¸€è‡´æ€§æ§åˆ¶æä¾›äº†æ–°æ€è·¯ï¼Œå¹¶å±•ç°äº†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­åº”ç”¨çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18651v1",
      "published_date": "2025-06-23 13:54:34 UTC",
      "updated_date": "2025-06-23 13:54:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:23.868432+00:00"
    },
    {
      "arxiv_id": "2506.18640v1",
      "title": "Federated Loss Exploration for Improved Convergence on Non-IID Data",
      "title_zh": "è”é‚¦æŸå¤±æ¢ç´¢ï¼šæå‡éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ä¸‹çš„æ”¶æ•›æ€§èƒ½",
      "authors": [
        "Christian InternÃ²",
        "Markus Olhofer",
        "Yaochu Jin",
        "Barbara Hammer"
      ],
      "abstract": "Federated learning (FL) has emerged as a groundbreaking paradigm in machine learning (ML), offering privacy-preserving collaborative model training across diverse datasets. Despite its promise, FL faces significant hurdles in non-identically and independently distributed (non-IID) data scenarios, where most existing methods often struggle with data heterogeneity and lack robustness in performance. This paper introduces Federated Loss Exploration (FedLEx), an innovative approach specifically designed to tackle these challenges. FedLEx distinctively addresses the shortcomings of existing FL methods in non-IID settings by optimizing its learning behavior for scenarios in which assumptions about data heterogeneity are impractical or unknown. It employs a federated loss exploration technique, where clients contribute to a global guidance matrix by calculating gradient deviations for model parameters. This matrix serves as a strategic compass to guide clients' gradient updates in subsequent FL rounds, thereby fostering optimal parameter updates for the global model. FedLEx effectively navigates the complex loss surfaces inherent in non-IID data, enhancing knowledge transfer in an efficient manner, since only a small number of epochs and small amount of data are required to build a strong global guidance matrix that can achieve model convergence without the need for additional data sharing or data distribution statics in a large client scenario. Our extensive experiments with state-of-the art FL algorithms demonstrate significant improvements in performance, particularly under realistic non-IID conditions, thus highlighting FedLEx's potential to overcome critical barriers in diverse FL applications.",
      "tldr_zh": "è”é‚¦å­¦ä¹ (Federated learning)åœ¨å¤„ç†éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æ•°æ®æ—¶ï¼Œå¸¸å› æ•°æ®å¼‚æ„æ€§é¢ä¸´æ¨¡å‹æ”¶æ•›å›°éš¾å’Œé²æ£’æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†Federated Loss Exploration (FedLEx)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è”é‚¦æŸå¤±æ¢ç´¢æŠ€æœ¯ä¼˜åŒ–å¼‚æ„ç¯å¢ƒä¸‹å­¦ä¹ è¡Œä¸ºçš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å®¢æˆ·ç«¯è®¡ç®—çš„æ¨¡å‹å‚æ•°æ¢¯åº¦åå·®(gradient deviations)æ„å»ºå…¨å±€å¼•å¯¼çŸ©é˜µ(global guidance matrix)ï¼Œä¸ºåç»­è½®æ¬¡çš„æ¢¯åº¦æ›´æ–°æä¾›æˆ˜ç•¥æŒ‡å¼•ã€‚FedLExèƒ½å¤Ÿæœ‰æ•ˆå¯¼èˆªnon-IIDæ•°æ®ä¸­å¤æ‚çš„æŸå¤±è¡¨é¢å¹¶åŠ é€ŸçŸ¥è¯†è¿ç§»ï¼Œä¸”ä»…éœ€æå°‘é‡çš„è®­ç»ƒè½®æ¬¡(epochs)å’Œæ•°æ®å³å¯å®ç°æ”¶æ•›ï¼Œæ— éœ€é¢å¤–çš„éšç§æ•°æ®å…±äº«æˆ–åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedLExåœ¨å¤šç§ç°å®non-IIDåœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›FLç®—æ³•ï¼Œä¸ºå…‹æœè”é‚¦å­¦ä¹ ä¸­çš„å¼‚æ„æ€§éšœç¢æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18640v1",
      "published_date": "2025-06-23 13:42:07 UTC",
      "updated_date": "2025-06-23 13:42:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:37.209995+00:00"
    },
    {
      "arxiv_id": "2506.18637v2",
      "title": "Granular-Ball-Induced Multiple Kernel K-Means",
      "title_zh": "ç²’çƒè¯±å¯¼çš„å¤šæ ¸K-Means",
      "authors": [
        "Shuyin Xia",
        "Yifan Wang",
        "Lifeng Shen",
        "Guoyin Wang"
      ],
      "abstract": "Most existing multi-kernel clustering algorithms, such as multi-kernel K-means, often struggle with computational efficiency and robustness when faced with complex data distributions. These challenges stem from their dependence on point-to-point relationships for optimization, which can lead to difficulty in accurately capturing data sets' inherent structure and diversity. Additionally, the intricate interplay between multiple kernels in such algorithms can further exacerbate these issues, effectively impacting their ability to cluster data points in high-dimensional spaces. In this paper, we leverage granular-ball computing to improve the multi-kernel clustering framework. The core of granular-ball computing is to adaptively fit data distribution by balls from coarse to acceptable levels. Each ball can enclose data points based on a density consistency measurement. Such ball-based data description thus improves the computational efficiency and the robustness to unknown noises. Specifically, based on granular-ball representations, we introduce the granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel K-means framework (GB-MKKM) for efficient clustering. Using granular-ball relationships in multiple kernel spaces, the proposed GB-MKKM framework shows its superiority in efficiency and clustering performance in the empirical evaluation of various clustering tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤šæ ¸èšç±»ç®—æ³•ï¼ˆå¦‚ multi-kernel K-meansï¼‰åœ¨å¤æ‚åˆ†å¸ƒä¸‹è®¡ç®—æ•ˆç‡ä½ä¸”é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç²’çƒè¯±å¯¼çš„å¤šæ ¸K-meansæ¡†æ¶ã€‚ç ”ç©¶åˆ©ç”¨ç²’çƒè®¡ç®—ï¼ˆgranular-ball computingï¼‰æŠ€æœ¯ï¼Œé€šè¿‡è‡ªé€‚åº”ç”Ÿæˆçš„çƒä½“ï¼ˆballsï¼‰æ¥æ‹Ÿåˆæ•°æ®åˆ†å¸ƒï¼Œå¹¶åŸºäºå¯†åº¦ä¸€è‡´æ€§ï¼ˆdensity consistencyï¼‰å°è£…æ•°æ®ç‚¹ï¼Œä»¥å–ä»£ä¼ ç»Ÿçš„ç‚¹å¯¹ç‚¹ï¼ˆpoint-to-pointï¼‰å…³ç³»æè¿°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡å¼•å…¥äº†ç²’çƒæ ¸ï¼ˆgranular-ball kernel, GBKï¼‰å’Œç²’çƒå¤šæ ¸K-meansç®—æ³•ï¼ˆGB-MKKMï¼‰ï¼Œæ—¨åœ¨æå‡é«˜ç»´ç©ºé—´ä¸‹çš„èšç±»æ€§èƒ½ã€‚é€šè¿‡åœ¨å¤šç§èšç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°ï¼ŒGB-MKKM åœ¨å¤„ç†æ•ˆç‡å’ŒæŠ—å™ªèƒ½åŠ›æ–¹é¢å‡å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨æ•æ‰æ•°æ®å†…åœ¨ç»“æ„å’Œå¤šæ ·æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18637v2",
      "published_date": "2025-06-23 13:39:32 UTC",
      "updated_date": "2025-08-11 13:34:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:39.143872+00:00"
    },
    {
      "arxiv_id": "2506.18631v4",
      "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
      "title_zh": "ReDitï¼šé€šè¿‡å¥–åŠ±æŠ–åŠ¨æå‡å¤§è¯­è¨€æ¨¡å‹ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Chenxing Wei",
        "Jiarui Yu",
        "Ying Tiffany He",
        "Hande Dong",
        "Yao Shu",
        "Fei Yu"
      ],
      "abstract": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReDit (Reward Dithering)ï¼Œæ—¨åœ¨è§£å†³DeepSeek-R1ç­‰æ¨¡å‹åœ¨è§„åˆ™å¥–åŠ±ç³»ç»Ÿä¸­å› ç¦»æ•£å¥–åŠ±ä¿¡å·å¯¼è‡´çš„æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šåŠæ”¶æ•›ç¼“æ…¢ç­‰é—®é¢˜ã€‚ReDité€šè¿‡å‘ç¦»æ•£å¥–åŠ±ä¿¡å·æ·»åŠ ç®€å•çš„éšæœºå™ªå£°ï¼Œä½¿å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤ŸæŒç»­æä¾›æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä»è€Œå®ç°æ›´å¹³æ»‘çš„æ¢¯åº¦æ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚è¿™ç§å™ªå£°æ³¨å…¥è¿˜ä¸ºå¹³å¦çš„å¥–åŠ±åŒºåŸŸå¼•å…¥äº†éšæœºæ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥å¹¶è·³å‡ºå±€éƒ¨æœ€ä¼˜è§£ã€‚å¤šé¡¹ä»»åŠ¡çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReDitä»…éœ€çº¦10%çš„è®­ç»ƒæ­¥æ•°å³å¯è¾¾åˆ°ä¸åŸå§‹GRPOç›¸å½“çš„æ€§èƒ½ï¼Œä¸”åœ¨ç›¸åŒè®­ç»ƒæ—¶é•¿ä¸‹æ¯”åŸå§‹GRPOæ€§èƒ½æå‡äº†4%ã€‚å¯è§†åŒ–åˆ†æä¸ç†è®ºè¯æ˜è¿›ä¸€æ­¥éªŒè¯äº†ReDitåœ¨ç¼“è§£æ¢¯åº¦é—®é¢˜å’Œæé«˜LLMç­–ç•¥ä¼˜åŒ–æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages, 19 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18631v4",
      "published_date": "2025-06-23 13:36:24 UTC",
      "updated_date": "2025-10-24 12:32:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:35.684528+00:00"
    },
    {
      "arxiv_id": "2506.18628v1",
      "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs",
      "title_zh": "AggTruthï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹èšåˆæ³¨æ„åŠ›åˆ†æ•°çš„ä¸Šä¸‹æ–‡å¹»è§‰æ£€æµ‹",
      "authors": [
        "Piotr Matys",
        "Jan Eliasz",
        "Konrad KieÅ‚czyÅ„ski",
        "MikoÅ‚aj Langner",
        "Teddy Ferdinan",
        "Jan KocoÅ„",
        "PrzemysÅ‚aw Kazienko"
      ],
      "abstract": "In real-world applications, Large Language Models (LLMs) often hallucinate, even in Retrieval-Augmented Generation (RAG) settings, which poses a significant challenge to their deployment. In this paper, we introduce AggTruth, a method for online detection of contextual hallucinations by analyzing the distribution of internal attention scores in the provided context (passage). Specifically, we propose four different variants of the method, each varying in the aggregation technique used to calculate attention scores. Across all LLMs examined, AggTruth demonstrated stable performance in both same-task and cross-task setups, outperforming the current SOTA in multiple scenarios. Furthermore, we conducted an in-depth analysis of feature selection techniques and examined how the number of selected attention heads impacts detection performance, demonstrating that careful selection of heads is essential to achieve optimal results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœºæ™¯ä¸­æ™®éå­˜åœ¨çš„å¹»è§‰æŒ‘æˆ˜ï¼Œæå‡ºäº† AggTruth è¿™ä¸€é€šè¿‡åˆ†æèƒŒæ™¯æ–‡æœ¬å†…éƒ¨æ³¨æ„åŠ›åˆ†æ•°(attention scores)åˆ†å¸ƒæ¥åœ¨çº¿æ£€æµ‹ä¸Šä¸‹æ–‡å¹»è§‰çš„æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†å››ç§åŸºäºä¸åŒèšåˆæŠ€æœ¯çš„ AggTruth å˜ä½“ï¼Œä»¥çµæ´»å¤„ç†æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—ä¸åˆ†å¸ƒåˆ†æã€‚å®éªŒè¯æ˜ï¼ŒAggTruth åœ¨å¤šç§æ¨¡å‹åŠåŒä»»åŠ¡ã€è·¨ä»»åŠ¡è®¾ç½®ä¸‹å‡ä¿æŒäº†ç¨³å®šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°åœºæ™¯ä¸­è¶…è¶Šäº†ç°æœ‰çš„ SOTA æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹ç‰¹å¾é€‰æ‹©æŠ€æœ¯åŠæ³¨æ„åŠ›å¤´(attention heads)æ•°é‡å½±å“çš„æ·±å…¥åˆ†æï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†ç²¾å‡†é€‰æ‹©æ³¨æ„åŠ›å¤´å¯¹è¾¾æˆæœ€ä¼˜æ£€æµ‹ç»“æœçš„å…³é”®æ€§ã€‚è¿™ä¸€æˆæœä¸ºå¢å¼º LLMs çš„éƒ¨ç½²å¯é æ€§å¹¶é™ä½ä¸Šä¸‹æ–‡å¹»è§‰é£é™©æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "ICCS 2025 Workshops",
      "pdf_url": "https://arxiv.org/pdf/2506.18628v1",
      "published_date": "2025-06-23 13:35:05 UTC",
      "updated_date": "2025-06-23 13:35:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:40.611550+00:00"
    },
    {
      "arxiv_id": "2506.18627v1",
      "title": "Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits",
      "title_zh": "å…‰å­é›†æˆç”µè·¯é€†å‘è®¾è®¡ä¸­çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yannik Mahlau",
        "Maximilian Schier",
        "Christoph Reinders",
        "Frederik Schubert",
        "Marco BÃ¼gling",
        "Bodo Rosenhahn"
      ],
      "abstract": "Inverse design of photonic integrated circuits (PICs) has traditionally relied on gradientbased optimization. However, this approach is prone to end up in local minima, which results in suboptimal design functionality. As interest in PICs increases due to their potential for addressing modern hardware demands through optical computing, more adaptive optimization algorithms are needed. We present a reinforcement learning (RL) environment as well as multi-agent RL algorithms for the design of PICs. By discretizing the design space into a grid, we formulate the design task as an optimization problem with thousands of binary variables. We consider multiple two- and three-dimensional design tasks that represent PIC components for an optical computing system. By decomposing the design space into thousands of individual agents, our algorithms are able to optimize designs with only a few thousand environment samples. They outperform previous state-of-the-art gradient-based optimization in both twoand three-dimensional design tasks. Our work may also serve as a benchmark for further exploration of sample-efficient RL for inverse design in photonics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…‰å­é›†æˆç”µè·¯ (Photonic Integrated Circuits, PICs) é€†å‘è®¾è®¡ä¸­ä¼ ç»Ÿæ¢¯åº¦ä¼˜åŒ–ç®—æ³•æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸“ç”¨çš„ Reinforcement Learning (RL) ç¯å¢ƒåŠ Multi-Agent Reinforcement Learning (MARL) ç®—æ³•ã€‚é€šè¿‡å°†è®¾è®¡ç©ºé—´ç¦»æ•£åŒ–ä¸ºç½‘æ ¼å¹¶è½¬åŒ–ä¸ºåŒ…å«æ•°åƒä¸ªäºŒè¿›åˆ¶å˜é‡çš„ä¼˜åŒ–é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†è®¾è®¡ç©ºé—´åˆ†è§£ä¸ºæ•°åƒä¸ªç‹¬ç«‹æ™ºèƒ½ä½“è¿›è¡ŒååŒä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å…‰å­è®¡ç®—ç³»ç»Ÿæ‰€éœ€çš„äºŒç»´åŠä¸‰ç»´è®¾è®¡ä»»åŠ¡æ—¶ï¼Œä»…éœ€æ•°åƒæ¬¡ç¯å¢ƒé‡‡æ ·å³å¯å®Œæˆï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…å®ç°äº†é«˜æ•ˆçš„é€†å‘è®¾è®¡ï¼Œä¹Ÿä¸ºå…‰å­å­¦é¢†åŸŸè¿›ä¸€æ­¥æ¢ç´¢é«˜é‡‡æ ·æ•ˆç‡çš„ RL ç®—æ³•æä¾›äº†é‡è¦çš„ç ”ç©¶åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18627v1",
      "published_date": "2025-06-23 13:34:27 UTC",
      "updated_date": "2025-06-23 13:34:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:42.653942+00:00"
    },
    {
      "arxiv_id": "2506.18955v1",
      "title": "Citizenship Challenges in Artificial Intelligence Education",
      "title_zh": "äººå·¥æ™ºèƒ½æ•™è‚²ä¸­çš„å…¬æ°‘æŒ‘æˆ˜",
      "authors": [
        "Margarida Romero"
      ],
      "abstract": "This chapter addresses the citizenship challenges related to AI in education, particularly concerning students, teachers, and other educational stakeholders in the context of AI integration. We first explore how to foster AI awareness and education, along with various strategies to promote a socio-critical approach to AI training, aiming to identify relevant and ethical uses to prioritise. In the second part, we discuss critical thinking and computational thinking skills that can be mobilised within certain AI-supported educational activities, depending on the degree of creative and transformative engagement those activities require.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ•™è‚²æ•´åˆèƒŒæ™¯ä¸‹ä¸ºå­¦ç”Ÿã€æ•™å¸ˆåŠç›¸å…³åˆ©ç›Šç›¸å…³è€…å¸¦æ¥çš„å…¬æ°‘æŒ‘æˆ˜ã€‚æ–‡ä¸­é¦–å…ˆåˆ†æäº†å¦‚ä½•åŸ¹å…»AIæ„è¯†ï¼Œå¹¶æå‡ºäº†ä¿ƒè¿›ç¤¾ä¼šæ‰¹åˆ¤æ€§ï¼ˆsocio-criticalï¼‰æ–¹æ³•åœ¨AIåŸ¹è®­ä¸­åº”ç”¨çš„ç­–ç•¥ï¼Œæ—¨åœ¨æ˜ç¡®å¹¶ä¼˜å…ˆå‘å±•å…·æœ‰ä¼¦ç†æ„ä¹‰çš„AIä½¿ç”¨æ–¹å¼ã€‚ç ”ç©¶é‡ç‚¹è®¨è®ºäº†åœ¨AIæ”¯æŒçš„æ•™è‚²æ´»åŠ¨ä¸­ï¼Œå¦‚ä½•æ ¹æ®ä»»åŠ¡å¯¹åˆ›é€ æ€§å’Œå˜é©æ€§å‚ä¸ï¼ˆcreative and transformative engagementï¼‰çš„è¦æ±‚ï¼Œæœ‰æ•ˆè°ƒåŠ¨ä¸ªä½“çš„æ‰¹åˆ¤æ€§æ€ç»´ï¼ˆcritical thinkingï¼‰å’Œè®¡ç®—æ€ç»´ï¼ˆcomputational thinkingï¼‰èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¿™äº›ç»´åº¦çš„æ·±åº¦å‰–æï¼Œæœ¬æ–‡ä¸ºåœ¨æ•™è‚²ä½“ç³»ä¸­æ„å»ºè´Ÿè´£ä»»ä¸”å…·å¤‡æ‰¹åˆ¤è§†è§’çš„AIç´ å…»æä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å¼•ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "in French language",
      "pdf_url": "https://arxiv.org/pdf/2506.18955v1",
      "published_date": "2025-06-23 13:34:09 UTC",
      "updated_date": "2025-06-23 13:34:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:54.239779+00:00"
    },
    {
      "arxiv_id": "2506.21621v2",
      "title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs",
      "title_zh": "Open Proof Corpusï¼šå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ•°å­¦è¯æ˜çš„å¤§è§„æ¨¡ç ”ç©¶",
      "authors": [
        "Jasper Dekoninck",
        "Ivo Petrov",
        "Kristian Minchev",
        "Mislav Balunovic",
        "Martin Vechev",
        "Miroslav Marinov",
        "Maria Drencheva",
        "Lyuba Konova",
        "Milen Shumanov",
        "Kaloyan Tsvetkov",
        "Nikolay Drenchev",
        "Lazar Todorov",
        "Kalina Nikolova",
        "Nikolay Georgiev",
        "Vanesa Kalinkova",
        "Margulan Ismoldayev"
      ],
      "abstract": "In recent months, large language models (LLMs) have made significant progress in mathematical proof generation, but further advancement is hindered by the lack of a large-scale, high-quality dataset of human-evaluated proofs. While expensive to create, such a dataset is essential for driving improvements in training and enabling a rigorous analysis of proof generation capabilities. In this work, we present the Open Proof Corpus (OPC), a dataset comprising over 5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was specifically designed for broad applicability and downstream usage in proof generation research and is the first to include a substantial number of correct, LLM-generated solutions to problems from prestigious mathematics competitions such as the USAMO and IMO. Using the OPC, we explore critical questions in automated proof generation: (1) the performance gap between natural language and formal proof generation, (2) the discrepancy between final-answer accuracy and full-proof validity, and (3) the impact of best-of-n selection on proof quality. Finally, to showcase the utility of the OPC, we finetune an 8B-parameter model on the dataset, obtaining a model that performs on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof correctness.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Open Proof Corpus (OPC)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 5,000 å¤šä¸ªç”± state-of-the-art LLMs ç”Ÿæˆå¹¶ç»äººå·¥è¯„ä¼°çš„æ•°å­¦è¯æ˜æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨å¡«è¡¥é«˜è´¨é‡äººå·¥è¯„ä¼°è¯æ˜æ•°æ®çš„ç©ºç™½ï¼Œé¦–æ¬¡æ¶µç›–äº†å¤§é‡é’ˆå¯¹ USAMO å’Œ IMO ç­‰é¡¶çº§æ•°å­¦ç«èµ›é—®é¢˜çš„æ­£ç¡® LLM ç”Ÿæˆè§£æ³•ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ OPC æ¢è®¨äº†è‡ªåŠ¨åŒ–è¯æ˜ç”Ÿæˆä¸­çš„å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€ä¸ formal proof ç”Ÿæˆä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œä»¥åŠæœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡ä¸å…¨ç¯‡è¯æ˜æœ‰æ•ˆæ€§ (proof validity) ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜åˆ†æäº† best-of-n é€‰æ‹©å¯¹è¯æ˜è´¨é‡çš„å½±å“ã€‚ä¸ºäº†å±•ç¤º OPC çš„å®ç”¨ä»·å€¼ï¼Œä½œè€…åœ¨è¯¥æ•°æ®é›†ä¸Šå¾®è°ƒäº†ä¸€ä¸ª 8B å‚æ•°æ¨¡å‹ï¼Œä½¿å…¶åœ¨è¯„ä¼°è¯æ˜æ­£ç¡®æ€§çš„ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸ Gemini-2.5-Pro ç›¸å½“çš„æ°´å¹³ï¼Œä¸ºè‡ªåŠ¨è¯æ˜è¯„ä»·å’Œæ¨¡å‹è®­ç»ƒæä¾›äº†é‡è¦åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21621v2",
      "published_date": "2025-06-23 13:31:58 UTC",
      "updated_date": "2026-01-14 21:46:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:53:58.822735+00:00"
    },
    {
      "arxiv_id": "2507.00043v2",
      "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations",
      "title_zh": "MR-CLIPï¼šé«˜æ•ˆçš„å…ƒæ•°æ®å¼•å¯¼å¼ MRI å¯¹æ¯”åº¦è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Mehmet Yigit Avci",
        "Pedro Borges",
        "Paul Wright",
        "Mehmet Yigitsoy",
        "Sebastien Ourselin",
        "Jorge Cardoso"
      ],
      "abstract": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical systems is based on a precise understanding of image contrast. This contrast is primarily governed by acquisition parameters, such as echo time and repetition time, which are stored in the DICOM metadata. To simplify contrast identification, broad labels such as T1-weighted or T2-weighted are commonly used, but these offer only a coarse approximation of the underlying acquisition settings. In many real-world datasets, such labels are entirely missing, leaving raw acquisition parameters as the only indicators of contrast. Adding to this challenge, the available metadata is often incomplete, noisy, or inconsistent. The lack of reliable and standardized metadata complicates tasks such as image interpretation, retrieval, and integration into clinical workflows. Furthermore, robust contrast-aware representations are essential to enable more advanced clinical applications, such as achieving modality-invariant representations and data harmonization. To address these challenges, we propose MR-CLIP, a multimodal contrastive learning framework that aligns MR images with their DICOM metadata to learn contrast-aware representations, without relying on manual labels. Trained on a diverse clinical dataset that spans various scanners and protocols, MR-CLIP captures contrast variations across acquisitions and within scans, enabling anatomy-invariant representations. We demonstrate its effectiveness in cross-modal retrieval and contrast classification, highlighting its scalability and potential for further clinical applications. The code and weights are publicly available at https://github.com/myigitavci/MR-CLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MR-CLIPï¼Œä¸€ç§å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹  (multimodal contrastive learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠæ ¸ç£å…±æŒ¯æˆåƒ (MRI) ä¸­å¯¹æ¯”åº¦æ ‡ç­¾ç²—ç³™ä»¥åŠ DICOM å…ƒæ•°æ® (metadata) ç¼ºå¤±æˆ–ä¸ä¸€è‡´å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°† MRI å›¾åƒä¸é‡‡é›†å‚æ•°ç›´æ¥å¯¹é½ï¼Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å­¦ä¹ å¯¹æ¯”åº¦æ„ŸçŸ¥è¡¨ç¤º (contrast-aware representations)ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒè®¾å¤‡å’Œåè®®ä¸‹çš„å¯¹æ¯”åº¦ç»†å¾®å˜åŒ–ã€‚MR-CLIP åœ¨å¤šæ ·åŒ–çš„ä¸´åºŠæ•°æ®é›†ä¸Šå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆè§£å‰–ç»“æ„æ— å…³ (anatomy-invariant) çš„è¡¨ç¤ºï¼Œæœ‰æ•ˆæå‡äº†è·¨æ¨¡æ€æ£€ç´¢ (cross-modal retrieval) å’Œå¯¹æ¯”åº¦åˆ†ç±» (contrast classification) çš„å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•å…·æœ‰æé«˜çš„å¯æ‰©å±•æ€§ï¼Œä¸ºå®ç°æ¨¡æ€ä¸å˜è¡¨ç¤º (modality-invariant representations) å’Œæ•°æ®åè°ƒ (data harmonization) ç­‰é«˜çº§ä¸´åºŠåº”ç”¨å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å·²å¼€æºç›¸å…³ä»£ç ä¸æƒé‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00043v2",
      "published_date": "2025-06-23 13:27:31 UTC",
      "updated_date": "2025-08-01 15:46:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:07.644873+00:00"
    },
    {
      "arxiv_id": "2506.18611v1",
      "title": "Frequency Control in Microgrids: An Adaptive Fuzzy-Neural-Network Virtual Synchronous Generator",
      "title_zh": "å¾®ç”µç½‘é¢‘ç‡æ§åˆ¶ï¼šä¸€ç§è‡ªé€‚åº”æ¨¡ç³Šç¥ç»ç½‘ç»œè™šæ‹ŸåŒæ­¥å‘ç”µæœº",
      "authors": [
        "Waleed Breesam",
        "Rezvan Alamian",
        "Nima Tashakor",
        "Brahim Elkhalil Youcefa",
        "Stefan M. Goetz"
      ],
      "abstract": "The reliance on distributed renewable energy has increased recently. As a result, power electronic-based distributed generators replaced synchronous generators which led to a change in the dynamic characteristics of the microgrid. Most critically, they reduced system inertia and damping. Virtual synchronous generators emulated in power electronics, which mimic the dynamic behaviour of synchronous generators, are meant to fix this problem. However, fixed virtual synchronous generator parameters cannot guarantee a frequency regulation within the acceptable tolerance range. Conversely, a dynamic adjustment of these virtual parameters promises robust solution with stable frequency. This paper proposes a method to adapt the inertia, damping, and droop parameters dynamically through a fuzzy neural network controller. This controller trains itself online to choose appropriate values for these virtual parameters. The proposed method can be applied to a typical AC microgrid by considering the penetration and impact of renewable energy sources. We study the system in a MATLAB/Simulink model and validate it experimentally in real time using hardware-in-the-loop based on an embedded ARM system (SAM3X8E, Cortex-M3). Compared to traditional and fuzzy logic controller methods, the results demonstrate that the proposed method significantly reduces the frequency deviation to less than 0.03 Hz and shortens the stabilizing/recovery time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å¸ƒå¼å¯å†ç”Ÿèƒ½æºæ¥å…¥å¯¼è‡´å¾®ç”µç½‘(Microgrid)ç³»ç»Ÿæƒ¯æ€§å’Œé˜»å°¼é™ä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”æ¨¡ç³Šç¥ç»ç½‘ç»œ(Fuzzy-Neural-Network, FNN)çš„è™šæ‹ŸåŒæ­¥å‘ç”µæœº(Virtual Synchronous Generator, VSG)æ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡FNNæ§åˆ¶å™¨çš„åœ¨çº¿è®­ç»ƒï¼Œå®ç°äº†å¯¹VSGæƒ¯æ€§ã€é˜»å°¼å’Œä¸‹å‚(Droop)å‚æ•°çš„åŠ¨æ€è°ƒæ•´ï¼Œå…‹æœäº†å›ºå®šå‚æ•°åœ¨é¢‘ç‡è°ƒèŠ‚ç¨³å¥æ€§ä¸Šçš„å±€é™ã€‚ç ”ç©¶åœ¨MATLAB/Simulinkæ¨¡å‹ä¸­è¿›è¡Œäº†ä»¿çœŸï¼Œå¹¶åˆ©ç”¨åŸºäºARMç³»ç»Ÿ(Cortex-M3)çš„ç¡¬ä»¶åœ¨ç¯(Hardware-in-the-Loop)å¹³å°è¿›è¡Œäº†å®æ—¶å®éªŒéªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿæ§åˆ¶å’Œæ¨¡ç³Šé€»è¾‘æ–¹æ³•ï¼Œèƒ½å°†é¢‘ç‡åå·®æ˜¾è‘—é™ä½è‡³0.03 Hzä»¥ä¸‹ï¼Œå¹¶å¤§å¹…ç¼©çŸ­äº†ç³»ç»Ÿçš„ç¨³å®šæ¢å¤æ—¶é—´ï¼Œä¸ºå¾®ç”µç½‘çš„é¢‘ç‡æ§åˆ¶æä¾›äº†é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "11 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18611v1",
      "published_date": "2025-06-23 13:16:52 UTC",
      "updated_date": "2025-06-23 13:16:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:08.912460+00:00"
    },
    {
      "arxiv_id": "2506.18954v1",
      "title": "SHAMaNS: Sound Localization with Hybrid Alpha-Stable Spatial Measure and Neural Steerer",
      "title_zh": "SHAMaNSï¼šåŸºäºæ··åˆ $\\alpha$ ç¨³å®šç©ºé—´æµ‹åº¦ä¸ç¥ç»å¯¼å‘å™¨çš„å£°æºå®šä½",
      "authors": [
        "Diego Di Carlo",
        "Mathieu Fontaine",
        "Aditya Arie Nugraha",
        "Yoshiaki Bando",
        "Kazuyoshi Yoshii"
      ],
      "abstract": "This paper describes a sound source localization (SSL) technique that combines an $Î±$-stable model for the observed signal with a neural network-based approach for modeling steering vectors. Specifically, a physics-informed neural network, referred to as Neural Steerer, is used to interpolate measured steering vectors (SVs) on a fixed microphone array. This allows for a more robust estimation of the so-called $Î±$-stable spatial measure, which represents the most plausible direction of arrival (DOA) of a target signal. As an $Î±$-stable model for the non-Gaussian case ($Î±$ $\\in$ (0, 2)) theoretically defines a unique spatial measure, we choose to leverage it to account for residual reconstruction error of the Neural Steerer in the downstream tasks. The objective scores indicate that our proposed technique outperforms state-of-the-art methods in the case of multiple sound sources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SHAMaNSï¼Œä¸€ç§ç»“åˆäº†Hybrid Alpha-Stableç©ºé—´æµ‹åº¦å’ŒNeural Steererçš„å£°æºå®šä½(Sound Source Localization)æŠ€æœ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç‰©ç†å¯å‘ç¥ç»ç½‘ç»œNeural Steererå¯¹å›ºå®šéº¦å…‹é£é˜µåˆ—ä¸Šçš„æµ‹é‡å¯¼å‘çŸ¢é‡(Steering Vectors)è¿›è¡Œæ’å€¼ï¼Œä»è€Œå®ç°å¯¹ç©ºé—´ä¿¡æ¯çš„ç²¾å‡†å»ºæ¨¡ã€‚é€šè¿‡å¼•å…¥$\\alpha$-stableæ¨¡å‹æ¥å¤„ç†éé«˜æ–¯ä¿¡å·ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿæ›´ç¨³å¥åœ°ä¼°è®¡ç›®æ ‡ä¿¡å·çš„æœ€ä¼˜åˆ°è¾¾æ–¹å‘(Direction of Arrival)ã€‚åˆ©ç”¨$\\alpha$-stableç©ºé—´æµ‹åº¦çš„ç†è®ºç‰¹æ€§ï¼ŒSHAMaNSæœ‰æ•ˆè¡¥å¿äº†Neural Steereråœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¯èƒ½å‡ºç°çš„æ®‹ä½™é‡å»ºè¯¯å·®ã€‚å®éªŒå®¢è§‚è¯„åˆ†ç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤„ç†å¤šå£°æºå®šä½ä»»åŠ¡æ—¶ï¼Œè¯¥æŠ€æœ¯çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›(State-of-the-Art)æ–¹æ³•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "European Signal Processing Conference (EUSIPCO), Sep 2025, Palermo, Italy",
      "pdf_url": "https://arxiv.org/pdf/2506.18954v1",
      "published_date": "2025-06-23 13:11:29 UTC",
      "updated_date": "2025-06-23 13:11:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:13.234402+00:00"
    },
    {
      "arxiv_id": "2506.18604v1",
      "title": "Simulation-Free Differential Dynamics through Neural Conservation Laws",
      "title_zh": "åŸºäºç¥ç»å®ˆæ’å¾‹çš„å…æ¨¡æ‹Ÿå¾®åˆ†åŠ¨åŠ›å­¦",
      "authors": [
        "Mengjian Hua",
        "Eric Vanden-Eijnden",
        "Ricky T. Q. Chen"
      ],
      "abstract": "We present a novel simulation-free framework for training continuous-time diffusion processes over very general objective functions. Existing methods typically involve either prescribing the optimal diffusion process -- which only works for heavily restricted problem formulations -- or require expensive simulation to numerically obtain the time-dependent densities and sample from the diffusion process. In contrast, we propose a coupled parameterization which jointly models a time-dependent density function, or probability path, and the dynamics of a diffusion process that generates this probability path. To accomplish this, our approach directly bakes in the Fokker-Planck equation and density function requirements as hard constraints, by extending and greatly simplifying the construction of Neural Conservation Laws. This enables simulation-free training for a large variety of problem formulations, from data-driven objectives as in generative modeling and dynamical optimal transport, to optimality-based objectives as in stochastic optimal control, with straightforward extensions to mean-field objectives due to the ease of accessing exact density functions. We validate our method in a diverse range of application domains from modeling spatio-temporal events to learning optimal dynamics from population data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ— éœ€æ¨¡æ‹Ÿçš„æ¡†æ¶ï¼Œç”¨äºåœ¨é€šç”¨ç›®æ ‡å‡½æ•°ä¸Šè®­ç»ƒè¿ç»­æ—¶é—´æ‰©æ•£è¿‡ç¨‹(diffusion processes)ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å—é™çš„é—®é¢˜å…¬å¼æˆ–æ˜‚è´µçš„æ•°å€¼æ¨¡æ‹Ÿæ¥è·å–éšæ—¶é—´å˜åŒ–çš„å¯†åº¦ï¼Œè€Œè¯¥æ–¹æ³•æå‡ºäº†ä¸€ç§è€¦åˆå‚æ•°åŒ–æ–¹æ¡ˆï¼Œå…±åŒå»ºæ¨¡éšæ—¶é—´å˜åŒ–çš„å¯†åº¦å‡½æ•°ï¼ˆå³æ¦‚ç‡è·¯å¾„ï¼‰å’Œç”Ÿæˆè¯¥è·¯å¾„çš„æ‰©æ•£è¿‡ç¨‹åŠ¨åŠ›å­¦ã€‚é€šè¿‡æ‰©å±•å¹¶ç®€åŒ–ç¥ç»å®ˆæ’å®šå¾‹(Neural Conservation Laws)çš„æ„å»ºï¼Œè¯¥æ–¹æ³•å°†Fokker-Planckæ–¹ç¨‹å’Œå¯†åº¦å‡½æ•°è¦æ±‚ç›´æ¥ä½œä¸ºç¡¬çº¦æŸåµŒå…¥æ¨¡å‹ä¸­ã€‚è¿™ä½¿å¾—è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†ä»ç”Ÿæˆå»ºæ¨¡(generative modeling)ã€åŠ¨æ€æœ€ä¼˜ä¼ è¾“(dynamical optimal transport)åˆ°éšæœºæœ€ä¼˜æ§åˆ¶(stochastic optimal control)ç­‰å¤šç§é—®é¢˜ã€‚ç”±äºèƒ½å¤Ÿè½»æ¾è·å–ç²¾ç¡®å¯†åº¦å‡½æ•°ï¼Œè¯¥æ–¹æ³•è¿˜å¯ç›´æ¥æ‰©å±•è‡³å¹³å‡åœºç›®æ ‡ã€‚ç ”ç©¶åœ¨æ—¶ç©ºäº‹ä»¶å»ºæ¨¡å’Œä»ç¾¤ä½“æ•°æ®å­¦ä¹ æœ€ä¼˜åŠ¨åŠ›å­¦ç­‰å¤šä¸ªåº”ç”¨é¢†åŸŸéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18604v1",
      "published_date": "2025-06-23 13:04:23 UTC",
      "updated_date": "2025-06-23 13:04:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:17.690063+00:00"
    },
    {
      "arxiv_id": "2506.18601v1",
      "title": "BulletGen: Improving 4D Reconstruction with Bullet-Time Generation",
      "title_zh": "BulletGenï¼šé€šè¿‡å­å¼¹æ—¶é—´ç”Ÿæˆæ”¹è¿› 4D é‡å»º",
      "authors": [
        "Denys Rozumnyi",
        "Jonathon Luiten",
        "Numair Khan",
        "Johannes SchÃ¶nberger",
        "Peter Kontschieder"
      ],
      "abstract": "Transforming casually captured, monocular videos into fully immersive dynamic experiences is a highly ill-posed task, and comes with significant challenges, e.g., reconstructing unseen regions, and dealing with the ambiguity in monocular depth estimation. In this work we introduce BulletGen, an approach that takes advantage of generative models to correct errors and complete missing information in a Gaussian-based dynamic scene representation. This is done by aligning the output of a diffusion-based video generation model with the 4D reconstruction at a single frozen \"bullet-time\" step. The generated frames are then used to supervise the optimization of the 4D Gaussian model. Our method seamlessly blends generative content with both static and dynamic scene components, achieving state-of-the-art results on both novel-view synthesis, and 2D/3D tracking tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BulletGenï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¥æ”¹è¿› 4D Reconstruction çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä»å•ç›®è§†é¢‘ (monocular videos) é‡æ„åŠ¨æ€åœºæ™¯æ—¶é¢ä¸´çš„æœªè§åŒºåŸŸé‡æ„å’Œæ·±åº¦ä¼°è®¡æ­§ä¹‰ç­‰æŒ‘æˆ˜ã€‚BulletGen é€šè¿‡å°†åŸºäºæ‰©æ•£ (diffusion-based) çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è¾“å‡ºä¸ 4D Gaussian åŠ¨æ€åœºæ™¯è¡¨ç¤ºåœ¨ç‰¹å®šçš„ \"å­å¼¹æ—¶é—´\" (bullet-time) æ­¥éª¤è¿›è¡Œå¯¹é½ï¼Œä»è€Œæœ‰æ•ˆåœ°çº æ­£é”™è¯¯å¹¶è¡¥å……ç¼ºå¤±çš„è§†è§‰ä¿¡æ¯ã€‚ç”Ÿæˆçš„å›¾åƒå¸§éšåè¢«ç”¨äºç›‘ç£ 4D Gaussian æ¨¡å‹çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹ä¸é™æ€åŠåŠ¨æ€åœºæ™¯ç»„ä»¶å®ç°æ— ç¼èåˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒBulletGen åœ¨æ–°è§†å›¾åˆæˆ (novel-view synthesis) å’Œ 2D/3D Tracking ä»»åŠ¡ä¸Šå‡å–å¾—äº† state-of-the-art çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº† 4D é‡æ„åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œä¸ºå®ç°æ²‰æµ¸å¼åŠ¨æ€ä½“éªŒæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18601v1",
      "published_date": "2025-06-23 13:03:42 UTC",
      "updated_date": "2025-06-23 13:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:17.946038+00:00"
    },
    {
      "arxiv_id": "2506.19875v1",
      "title": "Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers",
      "title_zh": "åˆ©ç”¨è¯´è¯äººåµŒå…¥æå‡æ–­ç»­åŠç§»åŠ¨è¯´è¯äººçš„è·Ÿè¸ªæ€§èƒ½",
      "authors": [
        "Taous Iatariene",
        "Can Cui",
        "Alexandre GuÃ©rin",
        "Romain Serizel"
      ],
      "abstract": "Speaker tracking methods often rely on spatial observations to assign coherent track identities over time. This raises limits in scenarios with intermittent and moving speakers, i.e., speakers that may change position when they are inactive, thus leading to discontinuous spatial trajectories. This paper proposes to investigate the use of speaker embeddings, in a simple solution to this issue. We propose to perform identity reassignment post-tracking, using speaker embeddings. We leverage trajectory-related information provided by an initial tracking step and multichannel audio signal. Beamforming is used to enhance the signal towards the speakers' positions in order to compute speaker embeddings. These are then used to assign new track identities based on an enrollment pool. We evaluate the performance of the proposed speaker embedding-based identity reassignment method on a dataset where speakers change position during inactivity periods. Results show that it consistently improves the identity assignment performance of neural and standard tracking systems. In particular, we study the impact of beamforming and input duration for embedding extraction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é—´æ­‡æ€§ç§»åŠ¨è¯´è¯äººåœ¨å¤±æ´»æœŸé—´æ”¹å˜ä½ç½®å¯¼è‡´çš„ç©ºé—´è½¨è¿¹ä¸è¿ç»­é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨Speaker Embeddingsæ”¹è¿›èº«ä»½è¿½è¸ªçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆåœ¨åˆå§‹è¿½è¸ªæ­¥éª¤ä¹‹åæ‰§è¡Œèº«ä»½é‡åˆ†é…ï¼Œé€šè¿‡BeamformingæŠ€æœ¯å¢å¼ºæŒ‡å‘è¯´è¯äººä½ç½®çš„å¤šé€šé“éŸ³é¢‘ä¿¡å·ï¼Œä»¥æå–æ›´å‡†ç¡®çš„Speaker Embeddingsã€‚éšåï¼Œç³»ç»Ÿåˆ©ç”¨æå–çš„ç‰¹å¾å¹¶ç»“åˆæ³¨å†Œæ± å¯¹éŸ³è½¨èº«ä»½è¿›è¡Œé‡æ–°ç¡®è®¤ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯´è¯äººä½ç½®å˜åŠ¨çš„å¤æ‚åœºæ™¯ä¸‹ï¼Œèƒ½å¤Ÿä¸€è‡´åœ°æå‡ç¥ç»ç½‘ç»œå’Œä¼ ç»Ÿè¿½è¸ªç³»ç»Ÿçš„èº«ä»½åˆ†é…æ€§èƒ½ã€‚æœ€åï¼Œç ”ç©¶è¿˜è¯¦ç»†è¯„ä¼°äº†Beamformingå’Œè¾“å…¥éŸ³é¢‘æ—¶é•¿å¯¹Embeddingæå–æ•ˆç‡çš„å½±å“ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "33rd European Signal Processing Conference (EUSIPCO 2025), Sep 2025, Palerme (Italie), Italy",
      "pdf_url": "https://arxiv.org/pdf/2506.19875v1",
      "published_date": "2025-06-23 13:02:20 UTC",
      "updated_date": "2025-06-23 13:02:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:21.354429+00:00"
    },
    {
      "arxiv_id": "2506.18588v2",
      "title": "Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks",
      "title_zh": "ç¥ç»ç½‘ç»œä¸­ Lipschitz è¿ç»­æ€§çš„ä¼˜åŒ–è¯±å¯¼åŠ¨åŠ›å­¦",
      "authors": [
        "RÃ³isÃ­n Luo",
        "James McDermott",
        "Christian GagnÃ©",
        "Qiang Sun",
        "Colm O'Riordan"
      ],
      "abstract": "Lipschitz continuity characterizes the worst-case sensitivity of neural networks to small input perturbations; yet its dynamics (i.e. temporal evolution) during training remains under-explored. We present a rigorous mathematical framework to model the temporal evolution of Lipschitz continuity during training with stochastic gradient descent (SGD). This framework leverages a system of stochastic differential equations (SDEs) to capture both deterministic and stochastic forces. Our theoretical analysis identifies three principal factors driving the evolution: (i) the projection of gradient flows, induced by the optimization dynamics, onto the operator-norm Jacobian of parameter matrices; (ii) the projection of gradient noise, arising from the randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii) the projection of the gradient noise onto the operator-norm Hessian of parameter matrices. Furthermore, our theoretical framework sheds light on such as how noisy supervision, parameter initialization, batch size, and mini-batch sampling trajectories, among other factors, shape the evolution of the Lipschitz continuity of neural networks. Our experimental results demonstrate strong agreement between the theoretical implications and the observed behaviors.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ Lipschitz continuity çš„åŠ¨æ€æ¼”å˜è§„å¾‹ï¼Œè¿™ä¸€ç‰¹æ€§è¡¨å¾äº†ç½‘ç»œå¯¹è¾“å…¥æ‰°åŠ¨çš„æœ€å·®æ•æ„Ÿæ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦æ¡†æ¶ï¼Œåˆ©ç”¨éšæœºå¾®åˆ†æ–¹ç¨‹ (SDEs) æ¥æ¨¡æ‹Ÿä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ (SGD) è®­ç»ƒæ—¶ Lipschitz continuity éšæ—¶é—´çš„æ¼”å˜è¿‡ç¨‹ï¼Œæ•æ‰äº†å…¶ä¸­çš„ç¡®å®šæ€§å’Œéšæœºæ€§é©±åŠ¨åŠ›ã€‚ç†è®ºåˆ†æç¡®å®šäº†é©±åŠ¨æ¼”å˜çš„ä¸‰ä¸ªä¸»è¦å› ç´ ï¼Œåˆ†åˆ«æ˜¯ä¼˜åŒ–åŠ¨åŠ›å­¦è¯±å¯¼çš„ gradient flows åœ¨å‚æ•°çŸ©é˜µ operator-norm Jacobian ä¸Šçš„æŠ•å½±ï¼Œä»¥åŠå°æ‰¹é‡é‡‡æ ·äº§ç”Ÿçš„ gradient noise åˆ†åˆ«åœ¨å‚æ•°çŸ©é˜µ operator-norm Jacobian å’Œ operator-norm Hessian ä¸Šçš„æŠ•å½±ã€‚è¯¥ç†è®ºæ¡†æ¶è¿›ä¸€æ­¥æ­ç¤ºäº† noisy supervisionã€å‚æ•°åˆå§‹åŒ–ã€batch size ä»¥åŠé‡‡æ ·è½¨è¿¹ç­‰å› ç´ å¦‚ä½•å¡‘é€ ç¥ç»ç½‘ç»œ Lipschitz continuity çš„æ¼”å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç†è®ºæ¨å¯¼ä¸å®é™…è§‚æµ‹åˆ°çš„ç¥ç»ç½‘ç»œè¡Œä¸ºé«˜åº¦ä¸€è‡´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18588v2",
      "published_date": "2025-06-23 12:49:13 UTC",
      "updated_date": "2025-11-14 14:47:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:25.642816+00:00"
    },
    {
      "arxiv_id": "2506.18586v1",
      "title": "Airalogy: AI-empowered universal data digitization for research automation",
      "title_zh": "Airalogyï¼šé¢å‘ç§‘ç ”è‡ªåŠ¨åŒ–çš„ AI èµ‹èƒ½é€šç”¨æ•°æ®æ•°å­—åŒ–",
      "authors": [
        "Zijie Yang",
        "Qiji Zhou",
        "Fang Guo",
        "Sijie Zhang",
        "Yexun Xi",
        "Jinglei Nie",
        "Yudian Zhu",
        "Liping Huang",
        "Chou Wu",
        "Yonghe Xia",
        "Xiaoyu Ma",
        "Yingming Pu",
        "Panzhong Lu",
        "Junshu Pan",
        "Mingtao Chen",
        "Tiannan Guo",
        "Yanmei Dou",
        "Hongyu Chen",
        "Anping Zeng",
        "Jiaxing Huang",
        "Tian Xu",
        "Yue Zhang"
      ],
      "abstract": "Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.",
      "tldr_zh": "æœ¬ç ”ç©¶å¼€å‘äº†Airalogyï¼Œè¿™æ˜¯å…¨çƒé¦–ä¸ªç”±AI- and community-drivené©±åŠ¨çš„å¹³å°ï¼Œæ—¨åœ¨è§£å†³å¤šå­¦ç§‘ç ”ç©¶æ•°æ®æ•°å­—åŒ–è¿‡ç¨‹ä¸­é€šç”¨æ€§ï¼ˆuniversalityï¼‰ä¸æ ‡å‡†åŒ–ï¼ˆstandardizationï¼‰éš¾ä»¥å¹³è¡¡çš„é•¿æœŸæŒ‘æˆ˜ã€‚è¯¥å¹³å°é€šè¿‡å¯å®šåˆ¶ä¸”æ ‡å‡†åŒ–çš„æ•°æ®è®°å½•æ¥å®Œæ•´å‘ˆç°research workflowsï¼Œå¹¶é›†æˆäº†å…ˆè¿›çš„AI research copilotåŠŸèƒ½ï¼Œæ”¯æŒæ™ºèƒ½Q&Aã€è‡ªåŠ¨æ•°æ®å½•å…¥ã€æ•°æ®åˆ†æåŠç ”ç©¶è‡ªåŠ¨åŒ–ã€‚Airalogyé€šè¿‡æ•´åˆç§‘å­¦é¢†åŸŸçŸ¥è¯†ä¸è®¡ç®—æŠ€èƒ½ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç§‘ç ”æ•°æ®ç®¡ç†ä¸­çš„æŠ€æœ¯ç¼ºå£ï¼Œæ¨åŠ¨äº†ç§‘ç ”æ•°æ®çš„æ ‡å‡†åŒ–è¿›ç¨‹ã€‚ç›®å‰è¯¥ç³»ç»Ÿå·²åœ¨è¥¿æ¹–å¤§å­¦ï¼ˆWestlake Universityï¼‰å„å­¦é™¢å®éªŒå®¤éƒ¨ç½²åº”ç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ é€Ÿå…¨çƒç§‘ç ”åˆ›æ–°åŠå®ç°å…¨æ–¹ä½ç§‘ç ”è‡ªåŠ¨åŒ–ï¼ˆresearch automationï¼‰æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "146 pages, 6 figures, 49 supplementary figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18586v1",
      "published_date": "2025-06-23 12:43:16 UTC",
      "updated_date": "2025-06-23 12:43:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:35.301522+00:00"
    },
    {
      "arxiv_id": "2506.18559v1",
      "title": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent",
      "title_zh": "T-CPDLï¼šç”¨äºæ„å»º Logic-RAG æ™ºèƒ½ä½“çš„æ—¶åºå› æœæ¦‚ç‡æè¿°é€»è¾‘",
      "authors": [
        "Hong Qing Yu"
      ],
      "abstract": "Large language models excel at generating fluent text but frequently struggle with structured reasoning involving temporal constraints, causal relationships, and probabilistic reasoning. To address these limitations, we propose Temporal Causal Probabilistic Description Logic (T-CPDL), an integrated framework that extends traditional Description Logic with temporal interval operators, explicit causal relationships, and probabilistic annotations. We present two distinct variants of T-CPDL: one capturing qualitative temporal relationships through Allen's interval algebra, and another variant enriched with explicit timestamped causal assertions. Both variants share a unified logical structure, enabling complex reasoning tasks ranging from simple temporal ordering to nuanced probabilistic causation. Empirical evaluations on temporal reasoning and causal inference benchmarks confirm that T-CPDL substantially improves inference accuracy, interpretability, and confidence calibration of language model outputs. By delivering transparent reasoning paths and fine-grained temporal and causal semantics, T-CPDL significantly enhances the capability of language models to support robust, explainable, and trustworthy decision-making. This work also lays the groundwork for developing advanced Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially boosting the reasoning capabilities and efficiency of knowledge graph-enhanced RAG systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Temporal Causal Probabilistic Description Logic (T-CPDL)ï¼Œè¿™æ˜¯ä¸€ç§é›†æˆäº†æ—¶é—´åŒºé—´ç®—å­ã€æ˜¾å¼å› æœå…³ç³»å’Œæ¦‚ç‡æ ‡æ³¨çš„æè¿°é€»è¾‘æ‰©å±•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å…·æœ‰æ—¶é—´çº¦æŸã€å› æœé€»è¾‘å’Œæ¦‚ç‡æ¨ç†çš„ç»“æ„åŒ–ä»»åŠ¡æ—¶é¢ä¸´çš„å±€é™æ€§ã€‚T-CPDL åŒ…å«ä¸¤ç§ä¸»è¦å˜ä½“ï¼šä¸€ç§é€šè¿‡ Allen's interval algebra æ•è·å®šæ€§æ—¶é—´å…³ç³»ï¼Œå¦ä¸€ç§åˆ™é€šè¿‡å¸¦æœ‰æ˜¾å¼æ—¶é—´æˆ³çš„å› æœæ–­è¨€è¿›è¡Œå¢å¼ºã€‚è¿™ä¸¤ç§å˜ä½“å…±äº«ç»Ÿä¸€çš„é€»è¾‘ç»“æ„ï¼Œèƒ½å¤Ÿæ”¯æŒä»ç®€å•æ—¶é—´æ’åºåˆ°ç»†å¾®æ¦‚ç‡å› æœæ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚åœ¨æ—¶é—´æ¨ç†å’Œå› æœæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒT-CPDL æ˜¾è‘—æé«˜äº†æ¨¡å‹è¾“å‡ºçš„æ¨ç†å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œç½®ä¿¡åº¦æ ¡å‡†(confidence calibration)ã€‚è¯¥æ¡†æ¶é€šè¿‡æä¾›é€æ˜çš„æ¨ç†è·¯å¾„å’Œç»†ç²’åº¦çš„æ—¶é—´åŠå› æœè¯­ä¹‰ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹æ”¯æŒç¨³å¥ã€å¯è§£é‡Šå’Œå€¼å¾—ä¿¡èµ–çš„å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œä¸ºå¼€å‘å…ˆè¿›çš„ Logic-Retrieval-Augmented Generation (Logic-RAG) æ¡†æ¶å¥ å®šäº†åŸºç¡€ï¼Œæœ‰æœ›å¤§å¹…æå‡çŸ¥è¯†å›¾è°±å¢å¼ºå‹ RAG ç³»ç»Ÿçš„æ¨ç†æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18559v1",
      "published_date": "2025-06-23 12:11:15 UTC",
      "updated_date": "2025-06-23 12:11:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:48.200997+00:00"
    },
    {
      "arxiv_id": "2506.19874v2",
      "title": "Towards Provable (In)Secure Model Weight Release Schemes",
      "title_zh": "è¿ˆå‘å¯è¯æ˜ï¼ˆä¸ï¼‰å®‰å…¨çš„æ¨¡å‹æƒé‡å‘å¸ƒæ–¹æ¡ˆ",
      "authors": [
        "Xin Yang",
        "Bintao Tang",
        "Yuhao Wang",
        "Zimo Ji",
        "Terry Jingchen Zhang",
        "Wenyuan Jiang"
      ],
      "abstract": "Recent secure weight release schemes claim to enable open-source model distribution while protecting model ownership and preventing misuse. However, these approaches lack rigorous security foundations and provide only informal security guarantees. Inspired by established works in cryptography, we formalize the security of weight release schemes by introducing several concrete security definitions. We then demonstrate our definition's utility through a case study of TaylorMLP, a prominent secure weight release scheme. Our analysis reveals vulnerabilities that allow parameter extraction thus showing that TaylorMLP fails to achieve its informal security goals. We hope this work will advocate for rigorous research at the intersection of machine learning and security communities and provide a blueprint for how future weight release schemes should be designed and evaluated.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ¨¡å‹æƒé‡å‘å¸ƒæ–¹æ¡ˆï¼ˆweight release schemesï¼‰ç¼ºä¹ä¸¥è°¨å®‰å…¨åŸºç¡€çš„é—®é¢˜ï¼Œæå‡ºäº†å¯è¯æ˜å®‰å…¨ï¼ˆ(In)Secureï¼‰çš„å½¢å¼åŒ–åˆ†ææ¡†æ¶ã€‚ä½œè€…å—ç»å…¸å¯†ç å­¦å¯å‘ï¼Œå¼•å…¥äº†ä¸€ç³»åˆ—å…·ä½“çš„å®‰å…¨å®šä¹‰ï¼Œä¸ºè¯„ä¼°å¼€æºæ¨¡å‹åˆ†å‘è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æƒä¿æŠ¤å’Œé˜²æ»¥ç”¨æœºåˆ¶æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚é€šè¿‡å¯¹ä¸»æµæ–¹æ¡ˆ TaylorMLP çš„æ·±å…¥æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥å›¢é˜Ÿå‘ç°å…¶å­˜åœ¨å¯å¯¼è‡´å‚æ•°æå–ï¼ˆparameter extractionï¼‰çš„æ˜¾è‘—æ¼æ´ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆæ— æ³•å®ç°å…¶å®£ç§°çš„éæ­£å¼å®‰å…¨ç›®æ ‡ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å½“å‰å®‰å…¨æƒé‡å‘å¸ƒæŠ€æœ¯çš„å±€é™æ€§ï¼Œè¿˜ä¸ºæœªæ¥æ–¹æ¡ˆçš„è®¾è®¡ä¸è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–è“å›¾ã€‚è¿™ä¸€å·¥ä½œæ—¨åœ¨æ¨åŠ¨æœºå™¨å­¦ä¹ ä¸å®‰å…¨ç¤¾åŒºäº¤å‰é¢†åŸŸçš„ä¸¥è°¨åŒ–ç ”ç©¶ï¼Œç¡®ä¿æ¨¡å‹åˆ†å‘çš„å®‰å…¨æ€§ä¸å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "8 pages, 2 figures; author name typos and institutions corrected",
      "pdf_url": "https://arxiv.org/pdf/2506.19874v2",
      "published_date": "2025-06-23 11:57:41 UTC",
      "updated_date": "2025-06-26 08:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:54.384186+00:00"
    },
    {
      "arxiv_id": "2506.18543v1",
      "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks",
      "title_zh": "DeepSeek ä¸ GPT ç³»åˆ—æ¨¡å‹å¯¹æŠ—è¶Šç‹±æ”»å‡»çš„å®‰å…¨è¯„ä¼°",
      "authors": [
        "Xiaodong Wu",
        "Xiangman Li",
        "Jianbing Ni"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) has raised critical concerns over their vulnerability to jailbreak attacks, i.e., adversarial prompts that bypass alignment mechanisms and elicit harmful or policy-violating outputs. While proprietary models like GPT-4 have undergone extensive evaluation, the robustness of emerging open-source alternatives such as DeepSeek remains largely underexplored, despite their growing adoption in real-world applications. In this paper, we present the first systematic jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and GPT-4 using the HarmBench benchmark. We evaluate seven representative attack strategies across 510 harmful behaviors categorized by both function and semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE) architecture introduces routing sparsity that offers selective robustness against optimization-based attacks such as TAP-T, but leads to significantly higher vulnerability under prompt-based and manually engineered attacks. In contrast, GPT-4 Turbo demonstrates stronger and more consistent safety alignment across diverse behaviors, likely due to its dense Transformer design and reinforcement learning from human feedback. Fine-grained behavioral analysis and case studies further show that DeepSeek often routes adversarial prompts to under-aligned expert modules, resulting in inconsistent refusal behaviors. These findings highlight a fundamental trade-off between architectural efficiency and alignment generalization, emphasizing the need for targeted safety tuning and modular alignment strategies to ensure secure deployment of open-source LLMs.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹ DeepSeek ç³»åˆ—æ¨¡å‹ä¸ GPT-3.5ã€GPT-4 åœ¨åº”å¯¹è¶Šç‹±æ”»å‡»(Jailbreak Attacks)æ—¶çš„å®‰å…¨æ€§è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°ã€‚ç ”ç©¶åˆ©ç”¨ HarmBench åŸºå‡†æµ‹è¯•ï¼Œé’ˆå¯¹æ¶µç›– 510 ç§æœ‰å®³è¡Œä¸ºçš„ 7 ç§ä»£è¡¨æ€§æ”»å‡»ç­–ç•¥è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ã€‚å®éªŒå‘ç°ï¼ŒDeepSeek çš„æ··åˆä¸“å®¶(Mixture-of-Experts, MoE)æ¶æ„å› å…¶è·¯ç”±ç¨€ç–æ€§è€Œåœ¨åº”å¯¹å¦‚ TAP-T ç­‰åŸºäºä¼˜åŒ–çš„æ”»å‡»æ—¶è¡¨ç°å‡ºé€‰æ‹©æ€§é²æ£’æ€§ï¼Œä½†åœ¨é¢å¯¹åŸºäºæç¤º(prompt-based)å’Œäººå·¥è®¾è®¡çš„æ”»å‡»æ—¶åˆ™è¡¨ç°å‡ºæ›´é«˜çš„æ˜“å—æ”»å‡»æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4 Turbo å‡­å€Ÿç¨ å¯† Transformer è®¾è®¡å’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)ï¼Œåœ¨å¤šæ ·åŒ–è¡Œä¸ºä¸­å±•ç¤ºäº†æ›´å¼ºä¸”æ›´ä¸€è‡´çš„å®‰å…¨å¯¹é½æ€§èƒ½ã€‚ç»†ç²’åº¦åˆ†æè¡¨æ˜ï¼ŒDeepSeek å¸¸å°†å¯¹æŠ—æ€§æç¤ºè·¯ç”±è‡³å¯¹é½ä¸è¶³çš„ä¸“å®¶æ¨¡å—ï¼Œå¯¼è‡´æ‹’ç»è¡Œä¸ºä¸ä¸€è‡´ã€‚è¯¥å‘ç°æ­ç¤ºäº†æ¶æ„æ•ˆç‡ä¸å¯¹é½æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ï¼Œå¼ºè°ƒäº†ä¸ºç¡®ä¿å¼€æºå¤§æ¨¡å‹å®‰å…¨éƒ¨ç½²è€Œè¿›è¡Œé’ˆå¯¹æ€§å®‰å…¨å¾®è°ƒå’Œæ¨¡å—åŒ–å¯¹é½ç­–ç•¥çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18543v1",
      "published_date": "2025-06-23 11:53:31 UTC",
      "updated_date": "2025-06-23 11:53:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:54:57.047753+00:00"
    },
    {
      "arxiv_id": "2506.18538v1",
      "title": "A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence",
      "title_zh": "AI åŒ…å®¹æ€§è¯„ä¼°é—®é¢˜åº“ï¼šæç»˜ä»å¤šæ ·æ€§åå·®åˆ°å“è¶ŠåŒ…å®¹æ€§çš„æ¼”è¿›ä¹‹è·¯",
      "authors": [
        "Rifat Ara Shams",
        "Didar Zowghi",
        "Muneera Bano"
      ],
      "abstract": "Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is crucial for mitigating biases and promoting equitable decision-making. However, existing AI risk assessment frameworks often overlook inclusivity, lacking standardized tools to measure an AI system's alignment with D&I principles. This paper introduces a structured AI inclusivity question bank, a comprehensive set of 253 questions designed to evaluate AI inclusivity across five pillars: Humans, Data, Process, System, and Governance. The development of the question bank involved an iterative, multi-source approach, incorporating insights from literature reviews, D&I guidelines, Responsible AI frameworks, and a simulated user study. The simulated evaluation, conducted with 70 AI-generated personas related to different AI jobs, assessed the question bank's relevance and effectiveness for AI inclusivity across diverse roles and application domains. The findings highlight the importance of integrating D&I principles into AI development workflows and governance structures. The question bank provides an actionable tool for researchers, practitioners, and policymakers to systematically assess and enhance the inclusivity of AI systems, paving the way for more equitable and responsible AI technologies.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰äººå·¥æ™ºèƒ½(AI)é£é™©è¯„ä¼°æ¡†æ¶åœ¨åŒ…å®¹æ€§(inclusivity)è¯„ä¼°æ–¹é¢çš„ç¼ºå¤±ï¼Œæå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„AIåŒ…å®¹æ€§é—®é¢˜åº“(AI inclusivity question bank)ã€‚è¯¥é—®é¢˜åº“åŒ…å«253ä¸ªé’ˆå¯¹æ€§é—®é¢˜ï¼Œæ¶µç›–äº†äººç±»(Humans)ã€æ•°æ®(Data)ã€æµç¨‹(Process)ã€ç³»ç»Ÿ(System)å’Œæ²»ç†(Governance)äº”å¤§æ”¯æŸ±ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ–‡çŒ®ç»¼è¿°ã€å¤šå…ƒåŒ–ä¸åŒ…å®¹æ€§(D&I)æŒ‡å—åŠè´Ÿè´£ä»»çš„AI(Responsible AI)æ¡†æ¶ï¼Œé‡‡ç”¨å¤šæºè¿­ä»£æ–¹æ³•å®Œæˆäº†è¯¥åº“çš„æ„å»ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨70ä¸ªAIç”Ÿæˆçš„è§’è‰²(AI-generated personas)è¿›è¡Œäº†æ¨¡æ‹Ÿç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥å·¥å…·åœ¨ä¸åŒAIç›¸å…³èŒä½å’Œåº”ç”¨é¢†åŸŸä¸­çš„ç›¸å…³æ€§ä¸æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†å°†D&IåŸåˆ™èå…¥AIå¼€å‘å·¥ä½œæµä¸æ²»ç†ç»“æ„çš„é‡è¦æ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºç ”ç©¶è€…ã€ä»ä¸šè€…å’Œæ”¿ç­–åˆ¶å®šè€…æä¾›äº†ä¸€å¥—å¯æ“ä½œçš„ç³»ç»ŸåŒ–è¯„ä¼°å·¥å…·ï¼Œæœ‰åŠ©äºæ˜¾è‘—æå‡AIç³»ç»Ÿçš„åŒ…å®¹æ€§å¹¶æ¨åŠ¨å…¬å¹³ã€è´Ÿè´£ä»»çš„æŠ€æœ¯æ¼”è¿›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18538v1",
      "published_date": "2025-06-23 11:48:38 UTC",
      "updated_date": "2025-06-23 11:48:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:55:12.115294+00:00"
    },
    {
      "arxiv_id": "2506.18530v1",
      "title": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference",
      "title_zh": "ç±»è„‘ç¥ç»ç½‘ç»œçš„åµŒå…¥å¼ FPGA åŠ é€Ÿï¼šä»åœ¨çº¿å­¦ä¹ åˆ°å¯æ‰©å±•æ¨ç†",
      "authors": [
        "Muhammad Ihsan Al Hafiz",
        "Naresh Ravichandran",
        "Anders Lansner",
        "Pawel Herman",
        "Artur Podobas"
      ],
      "abstract": "Edge AI applications increasingly require models that can learn and adapt on-device with minimal energy budget. Traditional deep learning models, while powerful, are often overparameterized, energy-hungry, and dependent on cloud connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian Confidence Propagation Neural Network (BCPNN), propose a neuromorphic alternative by mimicking cortical architecture and biologically-constrained learning. They offer sparse architectures with local learning rules and unsupervised/semi-supervised learning, making them well-suited for low-power edge intelligence. However, existing BCPNN implementations rely on GPUs or datacenter FPGAs, limiting their applicability to embedded systems. This work presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+ SoC using High-Level Synthesis. We implement both online learning and inference-only kernels with support for variable and mixed precision. Evaluated on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to 17.5x latency and 94% energy savings over ARM baselines, without sacrificing accuracy. This work enables practical neuromorphic computing on edge devices, bridging the gap between brain-like learning and real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜äººå·¥æ™ºèƒ½ï¼ˆEdge AIï¼‰å¯¹ä½èƒ½è€—ã€å¯åœ¨çº¿å­¦ä¹ æ¨¡å‹çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åœ¨åµŒå…¥å¼ FPGA ä¸ŠåŠ é€Ÿç±»è„‘ç¥ç»ç½‘ç»œï¼ˆBrain-Like Neural Networks, BLNNsï¼‰çš„ç¡¬ä»¶æ–¹æ¡ˆã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨è´å¶æ–¯ç½®ä¿¡ä¼ æ’­ç¥ç»ç½‘ç»œï¼ˆBayesian Confidence Propagation Neural Network, BCPNNï¼‰ï¼Œå…¶é€šè¿‡æ¨¡æ‹Ÿçš®å±‚ç»“æ„å’Œç”Ÿç‰©çº¦æŸå­¦ä¹ ï¼Œå®ç°äº†å…·æœ‰å±€éƒ¨å­¦ä¹ è§„åˆ™å’Œç¨€ç–æ¶æ„çš„ä½åŠŸè€—æ™ºèƒ½ã€‚ä½œè€…åœ¨ Zynq UltraScale+ SoC ä¸Šåˆ©ç”¨é«˜å±‚æ¬¡ç»¼åˆï¼ˆHigh-Level Synthesis, HLSï¼‰å¼€å‘äº†é¦–ä¸ªåµŒå…¥å¼ FPGA åŠ é€Ÿå™¨ï¼ŒåŒæ—¶æ”¯æŒåœ¨çº¿å­¦ä¹ ï¼ˆOnline Learningï¼‰å’Œæ··åˆç²¾åº¦çš„æ¨ç†å†…æ ¸ã€‚åœ¨ MNISTã€è‚ºç‚ï¼ˆPneumoniaï¼‰åŠä¹³è…ºç™Œï¼ˆBreast Cancerï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥åŠ é€Ÿå™¨åœ¨ä¿è¯å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œç›¸è¾ƒäº ARM åŸºçº¿å®ç°äº†é«˜è¾¾ 17.5 å€çš„å»¶è¿Ÿç¼©å‡å’Œ 94% çš„èƒ½æºèŠ‚çœã€‚è¿™é¡¹å·¥ä½œæˆåŠŸåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å®ç”¨çš„ç¥ç»å½¢æ€è®¡ç®—ï¼ˆNeuromorphic Computingï¼‰ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†ç±»è„‘å­¦ä¹ ç®—æ³•ä¸å®é™…å·¥ç¨‹éƒ¨ç½²ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18530v1",
      "published_date": "2025-06-23 11:35:20 UTC",
      "updated_date": "2025-06-23 11:35:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:55:59.344111+00:00"
    },
    {
      "arxiv_id": "2506.18511v1",
      "title": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance",
      "title_zh": "æ ‡å‡†é€‚ç”¨æ€§åˆ¤å®šä¸è·¨å¸æ³•ç®¡è¾–åŒºæ¨ç†ï¼šä¸€ç§åŸºäº RAG çš„åŒ»ç–—å™¨æ¢°åˆè§„æ¡†æ¶",
      "authors": [
        "Yu Han",
        "Aaron Ceross",
        "Jeroen H. M. Bergmann"
      ],
      "abstract": "Identifying the appropriate regulatory standard applicability remains a critical yet understudied challenge in medical device compliance, frequently necessitating expert interpretation of fragmented and heterogeneous documentation across different jurisdictions. To address this challenge, we introduce a modular AI system that leverages a retrieval-augmented generation (RAG) pipeline to automate standard applicability determination. Given a free-text device description, our system retrieves candidate standards from a curated corpus and uses large language models to infer jurisdiction-specific applicability, classified as Mandatory, Recommended, or Not Applicable, with traceable justifications. We construct an international benchmark dataset of medical device descriptions with expert-annotated standard mappings, and evaluate our system against retrieval-only, zero-shot, and rule-based baselines. The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards. We introduce the first end-to-end system for standard applicability reasoning, enabling scalable and interpretable AI-supported regulatory science. Notably, our region-aware RAG agent performs cross-jurisdictional reasoning between Chinese and U.S. standards, supporting conflict resolution and applicability justification across regulatory frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å™¨æ¢°åˆè§„æ€§ä¸­ç›‘ç®¡æ ‡å‡†é€‚ç”¨æ€§åˆ¤å®šè¿™ä¸€éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç®¡é“çš„æ¨¡å—åŒ– AI ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®è‡ªç”±æ–‡æœ¬å½¢å¼çš„è®¾å¤‡æè¿°ï¼Œä»è¯­æ–™åº“ä¸­æ£€ç´¢å€™é€‰æ ‡å‡†ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨æ–­ç‰¹å®šå¸æ³•ç®¡è¾–åŒºçš„é€‚ç”¨æ€§ç­‰çº§ã€‚ç³»ç»Ÿå°†é€‚ç”¨æ€§åˆ†ç±»ä¸º Mandatoryã€Recommended æˆ– Not Applicableï¼Œå¹¶ç”Ÿæˆå…·æœ‰å¯è¿½æº¯æ€§çš„åˆ¤å®šç†ç”±ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†é¦–ä¸ªåŒ…å«ä¸“å®¶æ ‡æ³¨æ ‡å‡†æ˜ å°„çš„åŒ»ç–—å™¨æ¢°å›½é™…åŸºå‡†æ•°æ®é›†ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•è¾¾åˆ°äº† 73% çš„åˆ†ç±»å‡†ç¡®ç‡å’Œ 87% çš„ Top-5 æ£€ç´¢å¬å›ç‡ã€‚ä½œä¸ºé¦–ä¸ªç”¨äºæ ‡å‡†é€‚ç”¨æ€§æ¨ç†çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆæ¨åŠ¨äº†å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„ AI è¾…åŠ©ç›‘ç®¡ç§‘å­¦ (Regulatory Science) çš„å‘å±•ã€‚æ­¤å¤–ï¼Œå…¶å…·å¤‡çš„ region-aware RAG agent èƒ½å¤Ÿåœ¨ä¸­ç¾æ ‡å‡†ä¹‹é—´è¿›è¡Œè·¨ç®¡è¾–åŒºæ¨ç†ï¼Œä»è€Œæ”¯æŒå¤šé‡ç›‘ç®¡æ¡†æ¶ä¸‹çš„å†²çªè§£å†³ä¸é€‚ç”¨æ€§è®ºè¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18511v1",
      "published_date": "2025-06-23 11:04:58 UTC",
      "updated_date": "2025-06-23 11:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:55:08.364602+00:00"
    },
    {
      "arxiv_id": "2506.18510v1",
      "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts",
      "title_zh": "Smooth Operatorsï¼šå¤§è¯­è¨€æ¨¡å‹å°†ä¸å®Œç¾æç¤ºè½¬åŒ–ä¸ºå¯Œå«éæµåˆ©ç‰¹å¾çš„è½¬å½•æ–‡æœ¬",
      "authors": [
        "Duygu Altinok"
      ],
      "abstract": "Accurate detection of disfluencies in spoken language is crucial for enhancing the performance of automatic speech and language processing systems, as well as fostering the development of more inclusive speech and language technologies. Leveraging the growing trend of large language models (LLMs) as versatile learners capable of processing both lexical and non-lexical inputs (e.g., audio and video), we propose a novel approach to transcribing disfluencies as explicit tokens with timestamps, enabling the generation of fully annotated disfluency-rich transcripts. Our method integrates acoustic representations extracted from an audio encoder with textual inputs of varying quality: clean transcriptions without disfluencies, time-aligned transcriptions from aligners, or outputs from phoneme-based ASR models -- all of which may contain imperfections. Importantly, our experiments demonstrate that textual inputs do not need to be flawless. As long as they include timestamp-related cues, LLMs can effectively smooth the input and produce fully disfluency-annotated transcripts, underscoring their robustness in handling imperfect hints.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†ä¸å®Œç¾çš„æç¤ºï¼ˆHintsï¼‰è½¬åŒ–ä¸ºåŒ…å«ä¸°å¯Œéæµåˆ©è¡¨è¾¾ï¼ˆDisfluencyï¼‰çš„è½¬å†™æ–‡æœ¬ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œå°†Disfluenciesä½œä¸ºå¸¦æœ‰æ—¶é—´æˆ³ï¼ˆtimestampsï¼‰çš„æ˜¾å¼æ ‡è®°è¿›è¡Œè½¬å†™ï¼Œä»è€Œç”Ÿæˆå…¨æ ‡æ³¨çš„Disfluency-rich transcriptsã€‚è¯¥æ–¹æ³•é›†æˆäº†æ¥è‡ªéŸ³é¢‘ç¼–ç å™¨çš„å£°å­¦è¡¨å¾ï¼ˆacoustic representationsï¼‰ä¸è´¨é‡ä¸ç­‰ä¸”å¯èƒ½åŒ…å«ç¼ºé™·çš„æ–‡æœ¬è¾“å…¥ï¼Œå¦‚ASRè¾“å‡ºæˆ–æ—¶é—´å¯¹é½çš„è½¬å†™ã€‚å®éªŒè¯æ˜ï¼ŒLLMsåœ¨å¤„ç†ä¸å®Œç¾è¾“å…¥æ—¶å±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œåªè¦è¾“å…¥ä¸­åŒ…å«æ—¶é—´æˆ³ç›¸å…³çš„çº¿ç´¢ï¼Œæ¨¡å‹å°±èƒ½æœ‰æ•ˆå¹³æ»‘ï¼ˆsmoothï¼‰è¿™äº›æç¤ºå¹¶äº§å‡ºé«˜è´¨é‡è½¬å†™ã€‚è¿™ä¸€æˆæœä¸ºæå‡è¯­éŸ³å¤„ç†ç³»ç»Ÿçš„æ€§èƒ½ä»¥åŠå¼€å‘æ›´å…·åŒ…å®¹æ€§çš„è¯­éŸ³æŠ€æœ¯æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to INTERSPEECH2025 workshop DISS2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18510v1",
      "published_date": "2025-06-23 11:04:20 UTC",
      "updated_date": "2025-06-23 11:04:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:55:23.836748+00:00"
    },
    {
      "arxiv_id": "2506.18504v2",
      "title": "Generalizing vision-language models to novel domains: A comprehensive survey",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–°é¢†åŸŸçš„æ³›åŒ–ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Xinyao Li",
        "Jingjing Li",
        "Fengling Li",
        "Lei Zhu",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "abstract": "Recently, vision-language pretraining has emerged as a transformative technique that integrates the strengths of both visual and textual modalities, resulting in powerful vision-language models (VLMs). Leveraging web-scale pretraining data, these models exhibit strong zero-shot capabilities. However, their performance often deteriorates when confronted with domain-specific or specialized generalization tasks. To address this, a growing body of research focuses on transferring or generalizing the rich knowledge embedded in VLMs to various downstream applications. This survey aims to comprehensively summarize the generalization settings, methodologies, benchmarking and results in VLM literatures. Delving into the typical VLM structures, current literatures are categorized into prompt-based, parameter-based and feature-based methods according to the transferred modules. The differences and characteristics in each category are furthered summarized and discussed by revisiting the typical transfer learning (TL) settings, providing novel interpretations for TL in the era of VLMs. Popular benchmarks for VLM generalization are further introduced with thorough performance comparisons among the reviewed methods. Following the advances in large-scale generalizable pretraining, this survey also discusses the relations and differences between VLMs and up-to-date multimodal large language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the surging literatures in vision-language research from a novel and practical generalization prospective, this survey contributes to a clear landscape of current and future multimodal researches.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°å…¨é¢æ€»ç»“äº†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨æœªçŸ¥é¢†åŸŸæ³›åŒ–æ–¹é¢çš„ç ”ç©¶ç°çŠ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚ç ”ç©¶ç³»ç»Ÿåœ°æ¢³ç†äº†æ³›åŒ–è®¾ç½®ã€æ–¹æ³•è®ºåŠåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ ¹æ®è¿ç§»æ¨¡å—å°†ç°æœ‰æŠ€æœ¯å½’çº³ä¸º prompt-basedã€parameter-based å’Œ feature-based ä¸‰å¤§ç±»ã€‚é€šè¿‡ç»“åˆä¼ ç»Ÿçš„è¿ç§»å­¦ä¹ (Transfer Learning)è§†è§’ï¼Œæ–‡ç« å¯¹VLMæ—¶ä»£çš„æ³›åŒ–æœºåˆ¶æä¾›äº†æ·±åº¦è§£è¯»ï¼Œå¹¶å¯¹ä¸»æµåŸºå‡†æµ‹è¯•ä¸‹çš„æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†è¯¦å°½å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œç»¼è¿°è¿˜æ¢è®¨äº†VLMsä¸ DeepSeek-VL ç­‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ä¹‹é—´çš„æ¼”è¿›å…³ç³»ã€‚è¯¥ç ”ç©¶ä»å®ç”¨çš„æ³›åŒ–è§†è§’å‡ºå‘ï¼Œä¸ºå½“å‰åŠæœªæ¥çš„å¤šæ¨¡æ€ç ”ç©¶å‹¾å‹’å‡ºäº†æ¸…æ™°çš„å‘å±•å›¾æ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18504v2",
      "published_date": "2025-06-23 10:56:37 UTC",
      "updated_date": "2025-06-30 05:24:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:55:19.935057+00:00"
    },
    {
      "arxiv_id": "2506.18501v3",
      "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance",
      "title_zh": "ChatGPT ä¸ DeepSeek åœ¨å…³é”® NLP ä»»åŠ¡ä¸­çš„å¯¹æ¯”è¯„ä¼°ï¼šä¼˜åŠ¿ã€ä¸è¶³åŠç‰¹å®šé¢†åŸŸè¡¨ç°",
      "authors": [
        "Wael Etaiwi",
        "Bushra Alhijawi"
      ],
      "abstract": "The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ ChatGPT å’Œ DeepSeek åœ¨äº”é¡¹å…³é”®è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ—¨åœ¨åˆ†æå®ƒä»¬åœ¨ä¸åŒé¢†åŸŸçš„ä¼˜ç‚¹ã€ç¼ºç‚¹åŠç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ã€‚è¯„ä¼°æ¶µç›–äº†æƒ…æ„Ÿåˆ†æ (sentiment analysis)ã€ä¸»é¢˜åˆ†ç±» (topic classification)ã€æ–‡æœ¬æ‘˜è¦ (text summarization)ã€æœºå™¨ç¿»è¯‘ (machine translation) å’Œæ–‡æœ¬è•´å« (textual entailment) ç­‰ä»»åŠ¡ã€‚ç ”ç©¶é‡‡ç”¨ç»“æ„åŒ–çš„å®éªŒæ–¹æ¡ˆï¼Œé€šè¿‡å®Œå…¨ç›¸åŒçš„ã€ä¸­ç«‹çš„æç¤ºè¯ (prompts) ä»¥åŠæ¯é¡¹ä»»åŠ¡ä¸¤ç»„åŸºå‡†æ•°æ®é›† (benchmark datasets) è¿›è¡Œæµ‹è¯•ï¼Œè¦†ç›–äº†ä»æ–°é—»ã€è¯„è®ºåˆ°æ­£å¼ä¸éæ­£å¼æ–‡æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek åœ¨åˆ†ç±»ç¨³å®šæ€§ (classification stability) å’Œé€»è¾‘æ¨ç† (logical reasoning) æ–¹é¢è¡¨ç°å“è¶Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒChatGPT åœ¨å¤„ç†éœ€è¦ç»†å¾®ç†è§£ (nuanced understanding) å’Œçµæ´»æ€§ (flexibility) çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚è¿™äº›å‘ç°ä¸ºæ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18501v3",
      "published_date": "2025-06-23 10:52:54 UTC",
      "updated_date": "2025-07-05 12:37:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:55:29.267158+00:00"
    },
    {
      "arxiv_id": "2506.18499v1",
      "title": "PuckTrick: A Library for Making Synthetic Data More Realistic",
      "title_zh": "PuckTrickï¼šæå‡åˆæˆæ•°æ®çœŸå®æ€§çš„å·¥å…·åº“",
      "authors": [
        "Alessandra Agostini",
        "Andrea Maurino",
        "Blerina Spahiu"
      ],
      "abstract": "The increasing reliance on machine learning (ML) models for decision-making requires high-quality training data. However, access to real-world datasets is often restricted due to privacy concerns, proprietary restrictions, and incomplete data availability. As a result, synthetic data generation (SDG) has emerged as a viable alternative, enabling the creation of artificial datasets that preserve the statistical properties of real data while ensuring privacy compliance. Despite its advantages, synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness. To address this limitation, we introduce Pucktrick, a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors. The library supports multiple error types, including missing data, noisy values, outliers, label misclassification, duplication, and class imbalance, offering a structured approach to evaluating ML model resilience under real-world data imperfections. Pucktrick provides two contamination modes: one for injecting errors into clean datasets and another for further corrupting already contaminated datasets. Through extensive experiments on real-world financial datasets, we evaluate the impact of systematic data contamination on model performance. Our findings demonstrate that ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†PuckTrickï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ç³»ç»Ÿæ€§å¼•å…¥å—æ§é”™è¯¯æ¥ä½¿åˆæˆæ•°æ®(Synthetic Data)æ›´åŠ æ¥è¿‘çœŸå®åœºæ™¯çš„Pythonåº“ã€‚é’ˆå¯¹åˆæˆæ•°æ®ç”Ÿæˆ(SDG)äº§ç”Ÿçš„æ•°æ®å¾€å¾€è¿‡äºâ€œå¹²å‡€â€ä¸”ç¼ºä¹ç¼ºå¤±å€¼(Missing Values)ã€å™ªå£°(Noise)å’Œç¦»ç¾¤å€¼(Outliers)ç­‰ç°å®ç¼ºé™·çš„é—®é¢˜ï¼ŒPuckTrickæä¾›äº†ç»“æ„åŒ–çš„æ–¹æ³•æ¥å¢å¼ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥åº“æ”¯æŒæ ‡ç­¾è¯¯åˆ†ç±»(Label Misclassification)ã€é‡å¤æ•°æ®(Duplication)å’Œç±»ä¸å¹³è¡¡(Class Imbalance)ç­‰å¤šç§é”™è¯¯æ³¨å…¥ï¼Œå¹¶æä¾›äº†é’ˆå¯¹çº¯å‡€æˆ–å·²æ±¡æŸ“æ•°æ®é›†çš„ä¸¤ç§å¤„ç†æ¨¡å¼ã€‚åœ¨çœŸå®é‡‘èæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œåœ¨å—æ±¡æŸ“åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºä»…åœ¨çº¯å‡€æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå°¤å…¶åœ¨SVMå’ŒExtra Treesç­‰æ¨¡å‹ä¸­è¡¨ç°çªå‡ºã€‚è¯¥å·¥å…·ä¸ºå¼€å‘è€…è¯„ä¼°æ¨¡å‹åœ¨ç°å®å¤æ‚æ•°æ®ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18499v1",
      "published_date": "2025-06-23 10:51:45 UTC",
      "updated_date": "2025-06-23 10:51:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:56:22.180646+00:00"
    },
    {
      "arxiv_id": "2506.18488v2",
      "title": "AI-Generated Song Detection via Lyrics Transcripts",
      "title_zh": "åŸºäºæ­Œè¯è½¬å½•æ–‡æœ¬çš„ AI ç”Ÿæˆæ­Œæ›²æ£€æµ‹",
      "authors": [
        "Markus Frohmann",
        "Elena V. Epure",
        "Gabriel Meseguer-Brocal",
        "Markus Schedl",
        "Romain Hennequin"
      ],
      "abstract": "The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIéŸ³ä¹ç”Ÿæˆæ£€æµ‹ä¸­ä¼ ç»ŸéŸ³é¢‘æ–¹æ³•é²æ£’æ€§ä¸è¶³ä»¥åŠæ­Œè¯æ•°æ®åº“ä¾èµ–æ€§å¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)æŠ€æœ¯è½¬å½•æ­Œè¯å¹¶è¿›è¡Œæ£€æµ‹çš„æ–°æ–¹æ³•ã€‚ä½œè€…åˆ©ç”¨Whisper large-v2å’ŒLLM2VecåµŒå…¥æ„å»ºäº†é«˜æ€§èƒ½æ£€æµ‹å™¨ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†å®é™…åœºæ™¯ä¸­åŸå§‹éŸ³é¢‘ç¼ºä¹ç²¾ç¡®æ–‡æœ¬æ ‡ç­¾çš„ç¼ºå£ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨è¯­è¨€ã€è·¨æµæ´¾çš„æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨é¢å¯¹éŸ³é¢‘æ‰°åŠ¨(Perturbation)å’Œä¸åŒéŸ³ä¹ç”Ÿæˆå™¨æ—¶ï¼Œå…¶ç¨³å®šæ€§æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºéŸ³é¢‘çš„SOTAæ£€æµ‹æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä»£ç å¼€æºï¼Œä¸ºè§£å†³ç°å®åº”ç”¨åœºæ™¯ä¸­AIç”Ÿæˆæ­Œæ›²çš„é‰´åˆ«éš¾é¢˜æä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ISMIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18488v2",
      "published_date": "2025-06-23 10:42:50 UTC",
      "updated_date": "2025-06-28 05:44:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:56:36.665064+00:00"
    },
    {
      "arxiv_id": "2506.18485v2",
      "title": "A Simple \"Motivation\" Can Enhance Reinforcement Finetuning of Large Reasoning Models",
      "title_zh": "ç®€å•çš„â€œåŠ¨æœºâ€å³å¯å¢å¼ºå¤§æ¨ç†æ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒ",
      "authors": [
        "Junjie Zhang",
        "Guozheng Ma",
        "Shunyu Liu",
        "Haoyu Wang",
        "Jiaxing Huang",
        "Ting-En Lin",
        "Fei Huang",
        "Yongbin Li",
        "Dacheng Tao"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Reasoning Models to tackle complex tasks. However, current RLVR paradigm is still not efficient enough, as it works in a trial-and-error manner. To perform better, the model needs to explore the reward space by numerously generating responses and learn from fragmented reward signals, blind to the overall reward patterns. Fortunately, verifiable rewards make the natural language description of the reward function possible, and meanwhile, LLMs have demonstrated strong in-context learning ability. This motivates us to explore if Large Reasoning Models can benefit from a motivation of the task, i.e., awareness of the reward function, during the reinforcement finetuning process, as we humans sometimes do when learning. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning (MeRF), an intuitive yet effective method enhancing reinforcement finetuning of LLMs by involving ``telling LLMs rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for the model to be aware of the optimization objective. This simple modification leverages the in-context learning ability of LLMs, aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations demonstrate that MeRF achieves substantial performance gains over RLVR baseline. Moreover, ablation studies show that MeRF performs better with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement finetuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Reinforcement Learning with Verifiable Rewards (RLVR) åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¯•é”™æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº† Motivation-enhanced Reinforcement Finetuning (MeRF) æ–¹æ³•ã€‚MeRF é€šè¿‡åœ¨ Prompt ä¸­ç›´æ¥æ³¨å…¥å¥–åŠ±è§„èŒƒ (reward specification)ï¼Œå°†â€œæ¸¸æˆè§„åˆ™â€å‘ŠçŸ¥å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶åœ¨å¼ºåŒ–å¾®è°ƒè¿‡ç¨‹ä¸­æ„ŸçŸ¥ä¼˜åŒ–ç›®æ ‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†æ¨¡å‹å¼ºå¤§çš„ In-context learning èƒ½åŠ›ï¼Œä½¿æ–‡æœ¬ç”Ÿæˆä¸ä¼˜åŒ–æ–¹å‘ä¿æŒå¯¹é½ï¼Œä»è€Œé€šè¿‡å†…éƒ¨åŠ¨æœºå’Œå¤–éƒ¨å¥–åŠ±å…±åŒæ¿€åŠ±æ¨¡å‹ç”Ÿæˆç†æƒ³è¾“å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMeRF åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜äº RLVR åŸºçº¿çš„æ€§èƒ½æå‡ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå½“ In-context motivation ä¸å¤–éƒ¨å¥–åŠ±å‡½æ•°çš„ä¸€è‡´æ€§è¶Šé«˜æ—¶ï¼Œæ¨¡å‹è¡¨ç°è¶Šå¥½ï¼Œä¸”æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å±•ç°å‡ºäº†è¯†åˆ«å¹¶ä¿®æ­£è¯¯å¯¼æ€§åŠ¨æœºçš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18485v2",
      "published_date": "2025-06-23 10:37:57 UTC",
      "updated_date": "2025-09-25 13:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:56:41.701977+00:00"
    },
    {
      "arxiv_id": "2506.18952v2",
      "title": "LLMs on a Budget? Say HOLA",
      "title_zh": "èµ„æºå—é™çš„å¤§è¯­è¨€æ¨¡å‹ï¼ŸHOLA é«˜æ•ˆéƒ¨ç½²ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Zohaib Hasan Siddiqui",
        "Jiechao Gao",
        "Ebad Shabbir",
        "Mohammad Anas Azeez",
        "Rafiq Ali",
        "Gautam Siddharth Kashyap",
        "Usman Naseem"
      ],
      "abstract": "Running Large Language Models (LLMs) on edge devices is constrained by high compute and memory demands posing a barrier for real-time applications in sectors like healthcare, education, and embedded systems. Current solutions such as quantization, pruning, and retrieval-augmented generation (RAG) offer only partial optimizations and often compromise on speed or accuracy. We introduce HOLA, an end-to-end optimization framework for efficient LLM deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD) for faster inference without quality loss. Externally, AdaComp-RAG adjusts retrieval complexity based on context needs. Together with LoBi, which blends structured pruning (LoRA) and quantization, HOLA delivers significant gains: 17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge devices like Jetson Nano--proving both scalable and production-ready.",
      "tldr_zh": "é’ˆå¯¹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†HOLAï¼Œä¸€ä¸ªç”¨äºLLMé«˜æ•ˆéƒ¨ç½²çš„ç«¯åˆ°ç«¯ä¼˜åŒ–æ¡†æ¶ã€‚åœ¨å†…éƒ¨ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å±‚æ¬¡åŒ–æŠ•æœºè§£ç (Hierarchical Speculative Decoding, HSD)æŠ€æœ¯ï¼Œåœ¨ä¸æŸå¤±è´¨é‡çš„å‰æä¸‹å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨å¤–éƒ¨ï¼ŒHOLAé€šè¿‡AdaComp-RAGæ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚åŠ¨æ€è°ƒæ•´æ£€ç´¢å¤æ‚åº¦ï¼Œå¹¶ç»“åˆLoBiæŠ€æœ¯èåˆäº†ç»“æ„åŒ–å‰ªæ(LoRA)ä¸é‡åŒ–(Quantization)æ‰‹æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHOLAåœ¨GSM8Kå’ŒARCåŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«å®ç°äº†17.6%å’Œ10.5%çš„æ€§èƒ½æå‡ï¼Œå¹¶æ˜¾è‘—é™ä½äº†Jetson Nanoç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å»¶è¿Ÿä¸å†…å­˜å ç”¨ã€‚è¯¥æ¡†æ¶è¯æ˜äº†å…¶åœ¨åŒ»ç–—ã€æ•™è‚²åŠåµŒå…¥å¼ç³»ç»Ÿç­‰å®æ—¶åº”ç”¨åœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§ä¸ç”Ÿäº§å°±ç»ªæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at EMNLP 2025 (Industry Track)",
      "pdf_url": "https://arxiv.org/pdf/2506.18952v2",
      "published_date": "2025-06-23 10:20:47 UTC",
      "updated_date": "2025-10-09 02:27:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:56:50.477946+00:00"
    },
    {
      "arxiv_id": "2506.18474v1",
      "title": "A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation",
      "title_zh": "åŸºäºæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„ä¸å¹³è¡¡æ•°æ®åˆ†å‰²æ–°å‹ç±»åˆ«å¹³è¡¡æ–¹æ³•",
      "authors": [
        "Atifa Kalsoom",
        "M. A. Iftikhar",
        "Amjad Ali",
        "Zubair Shah",
        "Shidin Balakrishnan",
        "Hazrat Ali"
      ],
      "abstract": "Retinal fundus images provide valuable insights into the human eye's interior structure and crucial features, such as blood vessels, optic disk, macula, and fovea. However, accurate segmentation of retinal blood vessels can be challenging due to imbalanced data distribution and varying vessel thickness. In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and bi-level class balancing scheme to achieve vessel segmentation in retinal fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN) architecture and an empirical approach to balance the distribution of pixels across vessel and non-vessel classes and within thin and thick vessels. Level-I is used for vessel/non-vessel balancing and Level-II is used for thick/thin vessel balancing. Additionally, pre-processing of the input retinal fundus image is performed by Global Contrast Normalization (GCN), Contrast Limited Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase intensity uniformity as well as to enhance the contrast between vessels and background pixels. The resulting balanced dataset is used for classification-based segmentation of the retinal vascular tree. We evaluate the proposed scheme on standard retinal fundus images and achieve superior performance measures, including an area under the ROC curve of 98.23%, Accuracy of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also demonstrate the method's efficacy through external cross-validation on STARE images, confirming its generalization ability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†ç½‘è†œçœ¼åº•å›¾åƒä¸­è¡€ç®¡åˆ†å‰²é¢ä¸´çš„æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ä»¥åŠè¡€ç®¡ç²—ç»†å·®å¼‚ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º BLCB-CNN çš„æ–°å‹æ·±åº¦å­¦ä¹ æµæ°´çº¿å’ŒåŒå±‚ç±»å¹³è¡¡ (Bi-level class balancing) æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¶æ„ï¼Œé€šè¿‡ Level-I å¹³è¡¡è¡€ç®¡ä¸éè¡€ç®¡ç±»åˆ«ï¼Œä»¥åŠ Level-II å¹³è¡¡ç²—ç»†è¡€ç®¡åˆ†å¸ƒï¼Œä»è€Œæœ‰æ•ˆæå‡åˆ†å‰²ç²¾åº¦ã€‚åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œç ”ç©¶é‡‡ç”¨äº† Global Contrast Normalization (GCN)ã€Contrast Limited Adaptive Histogram Equalization (CLAHE) å’Œä¼½é©¬æ ¡æ­£æŠ€æœ¯ï¼Œä»¥å¢å¼ºå›¾åƒå¯¹æ¯”åº¦å¹¶æé«˜å…‰ç…§å‡åŒ€åº¦ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šå–å¾—äº† 98.23% çš„ ROC æ›²çº¿ä¸‹é¢ç§¯ (AUC) å’Œ 96.22% çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨ STARE å›¾åƒä¸Šçš„å¤–éƒ¨äº¤å‰éªŒè¯ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤„ç†è§†ç½‘è†œè¡€ç®¡æ ‘åˆ†ç±»åˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§åŠå…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "This is preprint of the paper submitted to Scientific Reports journal",
      "pdf_url": "https://arxiv.org/pdf/2506.18474v1",
      "published_date": "2025-06-23 10:15:54 UTC",
      "updated_date": "2025-06-23 10:15:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:57:01.142584+00:00"
    },
    {
      "arxiv_id": "2507.01042v1",
      "title": "Can Argus Judge Them All? Comparing VLMs Across Domains",
      "title_zh": "Argus èƒ½å¤Ÿå®¡åˆ¤ä¸€åˆ‡å—ï¼Ÿè·¨é¢†åŸŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Harsh Joshi",
        "Gautam Siddharth Kashyap",
        "Rafiq Ali",
        "Ebad Shabbir",
        "Niharika Jain",
        "Sarthak Jain",
        "Jiechao Gao",
        "Usman Naseem"
      ],
      "abstract": "Vision-Language Models (VLMs) are advancing multimodal AI, yet their performance consistency across tasks is underexamined. We benchmark CLIP, BLIP, and LXMERT across diverse datasets spanning retrieval, captioning, and reasoning. Our evaluation includes task accuracy, generation quality, efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT leads in structured reasoning. These results expose trade-offs between generalization and specialization, informing industrial deployment of VLMs and guiding development toward robust, task-flexible architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ä¸åŒä»»åŠ¡é—´æ€§èƒ½ä¸€è‡´æ€§ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œå¯¹CLIPã€BLIPå’ŒLXMERTä¸‰ç§ä¸»æµæ¨¡å‹åœ¨æ£€ç´¢ã€æè¿°å’Œæ¨ç†ç­‰å¤šé¢†åŸŸçš„è¡¨ç°è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°è¿‡ç¨‹ä¸ä»…è€ƒå¯Ÿäº†ä»»åŠ¡å‡†ç¡®æ€§ã€ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è·¨æ•°æ®é›†ä¸€è‡´æ€§(CDC)æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLIPæ¨¡å‹è¡¨ç°å‡ºæœ€å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶è·¨æ•°æ®é›†ä¸€è‡´æ€§å¾—åˆ†é«˜è¾¾0.92ï¼›ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBLIPåœ¨ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒLXMERTåˆ™åœ¨ç»“æ„åŒ–æ¨ç†ä»»åŠ¡ä¸­ä¿æŒé¢†å…ˆã€‚è¿™äº›å‘ç°æ­ç¤ºäº†æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸ä¸“ä¸šåŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å·¥ä¸šåŒ–éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒï¼Œå¹¶ä¸ºå¼€å‘å…·æœ‰é²æ£’æ€§å’Œä»»åŠ¡çµæ´»æ€§çš„æ¶æ„æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01042v1",
      "published_date": "2025-06-23 09:58:35 UTC",
      "updated_date": "2025-06-23 09:58:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:56:46.089461+00:00"
    },
    {
      "arxiv_id": "2506.18951v3",
      "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications",
      "title_zh": "SWE-SQLï¼šæ¢ç´¢è§£å†³å®é™…åº”ç”¨ä¸­ç”¨æˆ· SQL é—®é¢˜çš„ LLM è·¯å¾„",
      "authors": [
        "Jinyang Li",
        "Xiaolong Li",
        "Ge Qu",
        "Per Jacobsson",
        "Bowen Qin",
        "Binyuan Hui",
        "Shuzheng Si",
        "Nan Huo",
        "Xiaohan Xu",
        "Yue Zhang",
        "Ziwei Tang",
        "Yuanshuai Li",
        "Florensia Widjaja",
        "Xintong Zhu",
        "Feige Zhou",
        "Yongfeng Huang",
        "Yannis Papakonstantinou",
        "Fatma Ozcan",
        "Chenhao Ma",
        "Reynold Cheng"
      ],
      "abstract": "Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œæ•°æ®åº“åº”ç”¨ä¸­å¤æ‚çš„SQLé—®é¢˜è°ƒè¯•æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰å¤§è¯­è¨€æ¨¡å‹(LLMs)è™½æ“…é•¿text-to-SQLç¿»è¯‘ï¼Œä½†åœ¨è§£å†³SQLæ•…éšœæ–¹é¢ä»ç¼ºä¹ä¸¥è°¨è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†BIRD-CRITICåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æºè‡ªçœŸå®ç”¨æˆ·é—®é¢˜çš„530ä¸ªPostgreSQLä»»åŠ¡å’Œ570ä¸ªå¤šæ–¹è¨€ä»»åŠ¡ã€‚ä¸ºäº†æå‡å¼€æºæ¨¡å‹çš„èƒ½åŠ›ï¼Œç ”ç©¶æ„å»ºäº†Six-Gymè®­ç»ƒç¯å¢ƒï¼Œåˆ©ç”¨SQL-Rewindç­–ç•¥é€šè¿‡é€†å‘å·¥ç¨‹è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œçš„æ•…éšœè§£å†³æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†f-Plan Boostingæ–¹æ³•ï¼Œé€šè¿‡ä»SQLè§£å†³æ–¹æ¡ˆä¸­æå–é«˜å±‚çº§çš„è°ƒè¯•è®¡åˆ’ï¼Œä½¿æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„æˆåŠŸè®­ç»ƒè½¨è¿¹å¢åŠ äº†73.7%ã€‚åŸºäºä¸Šè¿°ç»„ä»¶å¼€å‘çš„å¼€æºæ™ºèƒ½ä½“Bird-Fixerï¼ˆä»¥Qwen-2.5-Coder-14Bä¸ºåŸºç¡€ï¼‰ï¼Œåœ¨BIRD-CRITIC-PGä¸Šè¾¾åˆ°äº†38.11%çš„æˆåŠŸç‡ã€‚å®éªŒè¯æ˜Bird-Fixerçš„æ€§èƒ½è¶…è¶Šäº†Claude-3.7-Sonnetå’ŒGPT-4.1ç­‰é¢†å…ˆçš„é—­æºæ¨¡å‹ï¼Œä¸ºæ™®åŠå…ˆè¿›çš„SQLè°ƒè¯•èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "30 pages, 10 figures, NeurIPS 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2506.18951v3",
      "published_date": "2025-06-23 09:41:37 UTC",
      "updated_date": "2025-12-01 04:38:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:56:56.769480+00:00"
    },
    {
      "arxiv_id": "2506.18434v2",
      "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging",
      "title_zh": "åŒ»å­¦å½±åƒé¢„åé¢„æµ‹çš„åŸºç¡€æ¨¡å‹ä¸å‚æ•°é«˜æ•ˆå¾®è°ƒåŸºå‡†æµ‹è¯•",
      "authors": [
        "Filippo Ruffini",
        "Elena Mulero Ayllon",
        "Linlin Shen",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "Despite the significant potential of Foundation Models (FMs) in medical imaging, their application to prognosis prediction remains challenging due to data scarcity, class imbalance, and task complexity, which limit their clinical adoption. This study introduces the first structured benchmark to assess the robustness and efficiency of transfer learning strategies for FMs compared with convolutional neural networks (CNNs) in predicting COVID-19 patient outcomes from chest X-rays. The goal is to systematically compare finetuning strategies, both classical and parameter efficient, under realistic clinical constraints related to data scarcity and class imbalance, offering empirical guidance for AI deployment in clinical workflows. Four publicly available COVID-19 chest X-ray datasets were used, covering mortality, severity, and ICU admission, with varying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs pretrained on general or biomedical datasets were adapted using full finetuning, linear probing, and parameter-efficient methods. Models were evaluated under full data and few shot regimes using the Matthews Correlation Coefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and class weighted losses. CNNs with full fine-tuning performed robustly on small, imbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT), particularly LoRA and BitFit, achieved competitive results on larger datasets. Severe class imbalance degraded PEFT performance, whereas balanced data mitigated this effect. In few-shot settings, FMs showed limited generalization, with linear probing yielding the most stable results. No single fine-tuning strategy proved universally optimal: CNNs remain dependable for low-resource scenarios, whereas FMs benefit from parameter-efficient methods when data are sufficient.",
      "tldr_zh": "è¯¥ç ”ç©¶å»ºç«‹äº†é¦–ä¸ªç»“æ„åŒ–åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºç¡€æ¨¡å‹ (Foundation Models, FMs) ä¸å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) åœ¨åˆ©ç”¨èƒ¸éƒ¨ X å°„çº¿ (chest X-rays) è¿›è¡Œé¢„åé¢„æµ‹æ—¶çš„é²æ£’æ€§ä¸è¿ç§»å­¦ä¹ æ•ˆç‡ã€‚é’ˆå¯¹ COVID-19 æ‚£è€…çš„æ­»äº¡ç‡ã€ä¸¥é‡ç¨‹åº¦åŠ ICU å…¥é™¢ç­‰ä»»åŠ¡ï¼Œç ”ç©¶ç³»ç»Ÿå¯¹æ¯”äº†å…¨é‡å¾®è°ƒ (full finetuning)ã€çº¿æ€§æ¢æµ‹ (linear probing) ä»¥åŠåŒ…æ‹¬ LoRA å’Œ BitFit åœ¨å†…çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-Efficient Fine-Tuning, PEFT) ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°æ®ç¨€ç¼ºä¸”ç±»åˆ«å¤±è¡¡çš„å°å‹æ•°æ®é›†ä¸Šï¼Œç»è¿‡å…¨é‡å¾®è°ƒçš„ CNNs è¡¨ç°æœ€ä¸ºç¨³å¥ï¼›è€Œåœ¨è¾ƒå¤§æ•°æ®é›†ä¸Šï¼ŒFMs ç»“åˆ PEFT æ–¹æ³•èƒ½å¤Ÿå–å¾—å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚åœ¨å°‘æ ·æœ¬ (few-shot) åœºæ™¯ä¸‹ï¼ŒFMs çš„æ³›åŒ–èƒ½åŠ›å—åˆ°é™åˆ¶ï¼Œå…¶ä¸­çº¿æ€§æ¢æµ‹è¡¨ç°æœ€ä¸ºç¨³å®šã€‚ç ”ç©¶å¾—å‡ºç»“è®ºï¼Œä¸´åºŠ AI éƒ¨ç½²ä¸­ä¸å­˜åœ¨æ™®é€‚çš„æœ€ä¼˜å¾®è°ƒç­–ç•¥ï¼šåœ¨ä½èµ„æºç¯å¢ƒä¸‹ CNNs ä»æ˜¯é¦–é€‰ï¼Œè€Œå½“æ•°æ®é‡å……è¶³æ—¶ï¼ŒFMs é…åˆå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åˆ™æ›´å…·ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18434v2",
      "published_date": "2025-06-23 09:16:04 UTC",
      "updated_date": "2025-11-05 09:33:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:57:54.776367+00:00"
    },
    {
      "arxiv_id": "2507.00042v1",
      "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay",
      "title_zh": "åŸºäºå·®å¼‚åŠ æƒç»éªŒå›æ”¾çš„ç¾éš¾æ€§é—å¿˜ç¼“è§£",
      "authors": [
        "Xinrun Xu",
        "Jianwen Yang",
        "Qiuhong Zhang",
        "Zhanbiao Lian",
        "Zhiming Ding",
        "Shan Jiang"
      ],
      "abstract": "Continually adapting edge models in cloud-edge collaborative object detection for traffic monitoring suffers from catastrophic forgetting, where models lose previously learned knowledge when adapting to new data distributions. This is especially problematic in dynamic traffic environments characterised by periodic variations (e.g., day/night, peak hours), where past knowledge remains valuable. Existing approaches like experience replay and visual prompts offer some mitigation, but struggle to effectively prioritize and leverage historical data for optimal knowledge retention and adaptation. Specifically, simply storing and replaying all historical data can be inefficient, while treating all historical experiences as equally important overlooks their varying relevance to the current domain. This paper proposes ER-EMU, an edge model update algorithm based on adaptive experience replay, to address these limitations. ER-EMU utilizes a limited-size experience buffer managed using a First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target domains, prioritizing the selection of historical data that is most dissimilar to the current target domain. This ensures training diversity and facilitates the retention of knowledge from a wider range of past experiences, while also preventing overfitting to the new domain. The experience buffer is also updated using a simple random sampling strategy to maintain a balanced representation of previous domains. Experiments on the Bellevue traffic video dataset, involving repeated day/night cycles, demonstrate that ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº‘è¾¹åä½œ(cloud-edge collaborative)äº¤é€šç›‘æ§ç›®æ ‡æ£€æµ‹ä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ï¼Œæå‡ºäº†åä¸ºER-EMUçš„è‡ªé€‚åº”ç»éªŒå›æ”¾(Experience Replay)ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å¼•å…¥åŸºäºé¢†åŸŸè·ç¦»åº¦é‡çš„ç»éªŒé€‰æ‹©ç®—æ³•(DDM-ES)ï¼Œåˆ©ç”¨å¤šæ ¸æœ€å¤§å‡å€¼å·®å¼‚(MK-MMD)é‡åŒ–ä¸åŒé¢†åŸŸé—´çš„å·®å¼‚æ€§ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€äº¤é€šç¯å¢ƒä¸‹å†å²æ•°æ®åˆ©ç”¨æ•ˆç‡ä½çš„é—®é¢˜ã€‚DDM-ESä¼˜å…ˆé€‰æ‹©ä¸å½“å‰é¢†åŸŸä¸ç›¸ä¼¼çš„å†å²æ•°æ®è¿›è¡Œå›æ”¾ï¼Œä»¥æ­¤ä¿è¯è®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§ï¼Œåœ¨ä¿ç•™å¹¿æ³›å†å²çŸ¥è¯†çš„åŒæ—¶æœ‰æ•ˆé˜²æ­¢æ¨¡å‹å¯¹æ–°é¢†åŸŸçš„è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡å…ˆè¿›å…ˆå‡º(FIFO)ä¸éšæœºé‡‡æ ·ç­–ç•¥åŠ¨æ€ç»´æŠ¤ç»éªŒç¼“å†²åŒºï¼Œå®ç°äº†å¯¹ä»¥å¾€é¢†åŸŸçŸ¥è¯†çš„å¹³è¡¡è¡¨å¾ã€‚åœ¨Bellevueäº¤é€šè§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒER-EMUæ˜¾è‘—æå‡äº†å¤šç§å‰æ²¿äº‘è¾¹åä½œç›®æ ‡æ£€æµ‹æ¡†æ¶åœ¨å¤æ‚å‘¨æœŸæ€§ç¯å¢ƒä¸‹çš„é²æ£’æ€§å’Œæ£€æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICANN 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.00042v1",
      "published_date": "2025-06-23 09:11:50 UTC",
      "updated_date": "2025-06-23 09:11:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:57:01.504402+00:00"
    },
    {
      "arxiv_id": "2506.18428v1",
      "title": "How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models",
      "title_zh": "å¾®è°ƒåçš„æ¨¡å‹ç¼–è¾‘é²æ£’æ€§å¦‚ä½•ï¼Ÿä¸€é¡¹é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹çš„å®è¯ç ”ç©¶",
      "authors": [
        "Feng He",
        "Zhenyang Liu",
        "Marco Valentino",
        "Zhixue Zhao"
      ],
      "abstract": "Model editing offers a low-cost technique to inject or correct a particular behavior in a pre-trained model without extensive retraining, supporting applications such as factual correction and bias mitigation. Despite this common practice, it remains unknown whether edits persist after fine-tuning or whether they are inadvertently reversed. This question has fundamental practical implications. For example, if fine-tuning removes prior edits, it could serve as a defence mechanism against hidden malicious edits. Vice versa, the unintended removal of edits related to bias mitigation could pose serious safety concerns. We systematically investigate the interaction between model editing and fine-tuning in the context of T2I diffusion models, which are known to exhibit biases and generate inappropriate content. Our study spans two T2I model families (Stable Diffusion and FLUX), two sota editing techniques, and three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive empirical analysis across diverse editing tasks and evaluation metrics, our findings reveal a trend: edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. Notably, we observe that DoRA exhibits the strongest edit reversal effect. At the same time, among editing methods, UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT. These findings highlight a crucial limitation in current editing methodologies, emphasizing the need for more robust techniques to ensure reliable long-term control and alignment of deployed AI systems. These findings have dual implications for AI safety: they suggest that fine-tuning could serve as a remediation mechanism for malicious edits while simultaneously highlighting the need for re-editing after fine-tuning to maintain beneficial safety and alignment properties.",
      "tldr_zh": "æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ–‡æœ¬ç”Ÿæˆå›¾åƒ(T2I)æ‰©æ•£æ¨¡å‹åœ¨æ¨¡å‹ç¼–è¾‘(Model Editing)åçš„ç¨³å¥æ€§ï¼Œé‡ç‚¹åˆ†æäº†åç»­å¾®è°ƒ(Fine-Tuning)æ˜¯å¦ä¼šå¯¼è‡´å·²ç¼–è¾‘çš„ç‰¹å¾è¢«é€†è½¬ã€‚å®éªŒæ¶µç›–äº†Stable Diffusionå’ŒFLUXæ¨¡å‹ï¼Œå¯¹æ¯”äº†DreamBoothã€LoRAå’ŒDoRAä¸‰ç§å¾®è°ƒæ–¹å¼ï¼Œä»¥åŠUCEä¸ReFACTä¸¤ç§å‰æ²¿ç¼–è¾‘æŠ€æœ¯ã€‚ç ”ç©¶å‘ç°ï¼Œç¼–è¾‘æ•ˆæœæ™®ééš¾ä»¥åœ¨å¾®è°ƒåæŒä¹…ä¿ç•™ï¼Œå³ä½¿å¾®è°ƒä»»åŠ¡ä¸ç¼–è¾‘å†…å®¹å®Œå…¨æ— å…³ï¼Œç¼–è¾‘ç»“æœä¹Ÿå¾€å¾€ä¼šå¤±æ•ˆã€‚å…¶ä¸­ï¼ŒDoRAè¡¨ç°å‡ºæœ€å¼ºçš„ç¼–è¾‘é€†è½¬æ•ˆåº”ï¼Œè€Œåœ¨ç¼–è¾‘æ–¹æ³•ä¸­ï¼ŒUCEç›¸æ¯”ReFACTå±•ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹ç¼–è¾‘æŠ€æœ¯çš„å…³é”®å±€é™ï¼Œå¼ºè°ƒäº†åœ¨å¾®è°ƒåé‡æ–°è¿›è¡Œå®‰å…¨å¯¹é½(Alignment)çš„å¿…è¦æ€§ï¼ŒåŒæ—¶ä¹Ÿæš—ç¤ºå¾®è°ƒå¯ä½œä¸ºæ¸…é™¤æ¶æ„ç¼–è¾‘çš„ä¸€ç§æ½œåœ¨é˜²å¾¡æœºåˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18428v1",
      "published_date": "2025-06-23 09:10:29 UTC",
      "updated_date": "2025-06-23 09:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:58:06.992161+00:00"
    },
    {
      "arxiv_id": "2506.18424v1",
      "title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¨¡æ‹Ÿç”µè·¯å°ºå¯¸å…³ç³»æå–å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Chengjie Liu",
        "Weiyu Chen",
        "Huiyao Xu",
        "Yuan Du",
        "Jun Yang",
        "Li Du"
      ],
      "abstract": "In the design process of the analog circuit pre-layout phase, device sizing is an important step in determining whether an analog circuit can meet the required performance metrics. Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space. To alleviate this problem, we propose a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers. The search space in the sizing process can be effectively pruned based on the sizing relationship extracted by this framework. Eventually, we conducted tests on 3 types of circuits, and the optimization efficiency was improved by $2.32 \\sim 26.6 \\times$. This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶(multi-agent framework)ï¼Œæ—¨åœ¨ä»å­¦æœ¯æ–‡çŒ®ä¸­æå–æ¨¡æ‹Ÿç”µè·¯(analog circuits)çš„å°ºå¯¸è°ƒæ•´(sizing)å…³ç³»ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯åœ¨ç”µè·¯å°ºå¯¸è°ƒæ•´ä»»åŠ¡ä¸­å¿½è§†å…ˆéªŒçŸ¥è¯†å¼•å…¥ã€å¯¼è‡´æœç´¢ç©ºé—´(search space)å‹ç¼©ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨æå–çš„å°ºå¯¸å…³ç³»å®ç°äº†å¯¹æœç´¢ç©ºé—´çš„æœ‰æ•ˆä¿®å‰ª(pruning)ã€‚å®éªŒåœ¨3ç§ç”µè·¯ç±»å‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå…¶ä¼˜åŒ–æ•ˆç‡æå‡äº†2.32è‡³26.6å€ã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº†LLMèƒ½å¤Ÿæœ‰æ•ˆä¼˜åŒ–æ¨¡æ‹Ÿç”µè·¯å°ºå¯¸è°ƒæ•´çš„æœç´¢ç©ºé—´ï¼Œä¸ºå°†å¤§è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿæ¨¡æ‹Ÿç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–æ–¹æ³•ç›¸ç»“åˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ISEDA 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18424v1",
      "published_date": "2025-06-23 09:03:58 UTC",
      "updated_date": "2025-06-23 09:03:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:57:17.933476+00:00"
    },
    {
      "arxiv_id": "2506.18421v2",
      "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models",
      "title_zh": "TReBï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹è¡¨æ ¼æ¨ç†èƒ½åŠ›çš„ç»¼åˆåŸºå‡†",
      "authors": [
        "Ce Li",
        "Xiaofan Liu",
        "Zhiyan Song",
        "Ce Chi",
        "Chen Zhao",
        "Jingjing Yang",
        "Zhendong Wang",
        "Kexin Yang",
        "Boshen Shi",
        "Xing Wang",
        "Chao Deng",
        "Junlan Feng"
      ],
      "abstract": "The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on huggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on github.com/JT-LM/jiutian-treb.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TReBï¼Œä¸€ä¸ªå…¨é¢çš„è¡¨æ ¼æ¨ç†æ¼”åŒ–åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶çš„æµ…å±‚ç†è§£ä¸æ·±å±‚æ¨ç†èƒ½åŠ›ã€‚TReBæ¶µç›–äº†26ä¸ªå­ä»»åŠ¡ï¼Œé€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºäº†é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œä»¥å…¬å¹³åœ°åæ˜ æ¨¡å‹åœ¨å¤æ‚è¡¨æ ¼ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†å®ç°é²æ£’çš„æ€§èƒ½è¡¡é‡ï¼Œè¯¥åŸºå‡†å¼•å…¥äº†åŒ…å«TCoTã€PoTå’ŒICoTä¸‰ç§æ¨ç†æ¨¡å¼çš„è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ¡†æ¶å¯¹20å¤šç§ä¸»æµLLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå®éªŒç»“æœè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œå¤æ‚è¡¨æ ¼ä»»åŠ¡æ–¹é¢ä»æœ‰å·¨å¤§æå‡ç©ºé—´ã€‚ç›®å‰è¯¥æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶å·²åœ¨Hugging Faceå’ŒGitHubä¸Šå¼€æºï¼Œä¸ºæ¨åŠ¨è¡¨æ ¼æ¨ç†æŠ€æœ¯çš„å‘å±•æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Benmark report v1.1",
      "pdf_url": "https://arxiv.org/pdf/2506.18421v2",
      "published_date": "2025-06-23 09:02:04 UTC",
      "updated_date": "2025-07-14 06:09:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:02.751965+00:00"
    },
    {
      "arxiv_id": "2506.21620v1",
      "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨åœ¨çº¿å¯¹è¯ä¸­å¦‚ä½•æ‰®æ¼”äººç±»ï¼šé’ˆå¯¹ Reddit ä¸Š 2016 å¹´ç¾å›½æ”¿æ²»è®¨è®ºçš„æ¨¡æ‹Ÿç ”ç©¶",
      "authors": [
        "Daniele Cirulli",
        "Giulio Cimini",
        "Giovanni Palermo"
      ],
      "abstract": "Large Language Models (LLMs) have recently emerged as powerful tools for natural language generation, with applications spanning from content creation to social simulations. Their ability to mimic human interactions raises both opportunities and concerns, particularly in the context of politically relevant online discussions. In this study, we evaluate the performance of LLMs in replicating user-generated content within a real-world, divisive scenario: Reddit conversations during the 2016 US Presidential election. In particular, we conduct three different experiments, asking GPT-4 to generate comments by impersonating either real or artificial partisan users. We analyze the generated comments in terms of political alignment, sentiment, and linguistic features, comparing them against real user contributions and benchmarking against a null model. We find that GPT-4 is able to produce realistic comments, both in favor of or against the candidate supported by the community, yet tending to create consensus more easily than dissent. In addition we show that real and artificial comments are well separated in a semantically embedded space, although they are indistinguishable by manual inspection. Our findings provide insights on the potential use of LLMs to sneak into online discussions, influence political debate and shape political narratives, bearing broader implications of AI-driven discourse manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨ç¤¾äº¤åª’ä½“å¯¹è¯ä¸­æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„èƒ½åŠ›ï¼Œé‡ç‚¹åˆ†æäº†å…¶åœ¨2016å¹´ç¾å›½å¤§é€‰Redditæ”¿æ²»è®¨è®ºè¿™ä¸€äº‰è®®æ€§åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶è€…é€šè¿‡ä¸‰ä¸ªå®éªŒè¦æ±‚GPT-4æ¨¡ä»¿çœŸå®æˆ–è™šæ‹Ÿçš„å…šæ´¾ç”¨æˆ·ç”Ÿæˆè¯„è®ºï¼Œå¹¶å¯¹å…¶æ”¿æ²»ç«‹åœºã€æƒ…æ„Ÿå’Œè¯­è¨€ç‰¹å¾è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4èƒ½å¤Ÿç”Ÿæˆæ”¯æŒæˆ–åå¯¹ç‰¹å®šå€™é€‰äººçš„é€¼çœŸå†…å®¹ï¼Œä½†ç›¸æ¯”äºäº§ç”Ÿåˆ†æ­§(dissent)ï¼Œæ¨¡å‹æ›´å®¹æ˜“å¼•å¯¼äº§ç”Ÿå…±è¯†(consensus)ã€‚æ­¤å¤–ï¼Œè™½ç„¶äººå·¥å®¡æ ¸éš¾ä»¥åŒºåˆ†è¯„è®ºçš„çœŸä¼ªï¼Œä½†çœŸå®è¯„è®ºä¸AIç”Ÿæˆå†…å®¹åœ¨è¯­ä¹‰åµŒå…¥ç©ºé—´(semantically embedded space)ä¸­æ˜¯é«˜åº¦å¯åˆ†ç¦»çš„ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨æ¸—é€åœ¨çº¿è®¨è®ºå’Œæ“çºµæ”¿æ²»å™äº‹æ–¹é¢çš„æ½œåœ¨å¨èƒï¼Œä¸ºç†è§£AIé©±åŠ¨çš„è¯è¯­æ“çºµ(discourse manipulation)åŠå…¶å¯¹æ”¿æ²»è¾©è®ºçš„å½±å“æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21620v1",
      "published_date": "2025-06-23 08:54:32 UTC",
      "updated_date": "2025-06-23 08:54:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:58:36.370422+00:00"
    },
    {
      "arxiv_id": "2506.18414v2",
      "title": "Latent Space Analysis for Melanoma Prevention",
      "title_zh": "é¢å‘é»‘è‰²ç´ ç˜¤é¢„é˜²çš„éšç©ºé—´åˆ†æ",
      "authors": [
        "Ciro Listone",
        "Aniello Murano"
      ],
      "abstract": "Melanoma represents a critical health risk due to its aggressive progression and high mortality, underscoring the need for early, interpretable diagnostic tools. While deep learning has advanced in skin lesion classification, most existing models provide only binary outputs, offering limited clinical insight. This work introduces a novel approach that extends beyond classification, enabling interpretable risk modelling through a Conditional Variational Autoencoder. The proposed method learns a structured latent space that captures semantic relationships among lesions, allowing for a nuanced, continuous assessment of morphological differences. An SVM is also trained on this representation effectively differentiating between benign nevi and melanomas, demonstrating strong and consistent performance. More importantly, the learned latent space supports visual and geometric interpretation of malignancy, with the spatial proximity of a lesion to known melanomas serving as a meaningful indicator of risk. This approach bridges predictive performance with clinical applicability, fostering early detection, highlighting ambiguous cases, and enhancing trust in AI-assisted diagnosis through transparent and interpretable decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Melanomaé¢„é˜²ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹ç¼ºä¹ä¸´åºŠæ´å¯ŸåŠ›çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºConditional Variational Autoencoder (CVAE) çš„å¯è§£é‡Šé£é™©å»ºæ¨¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºç»“æ„åŒ–çš„Latent Spaceæ•æ‰ç—…å˜é—´çš„è¯­ä¹‰å…³ç³»ï¼Œå®ç°äº†å¯¹å½¢æ€å·®å¼‚çš„ç»†è‡´è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨åœ¨è¡¨å¾ä¸Šè®­ç»ƒçš„SVMæœ‰æ•ˆåŒºåˆ†è‰¯æ€§ç—£ä¸Melanomaã€‚ç ”ç©¶å¼ºè°ƒï¼Œæ‰€å­¦ä¹ çš„Latent Spaceæ”¯æŒå¯¹æ¶æ€§è‚¿ç˜¤è¿›è¡Œè§†è§‰å’Œå‡ ä½•å±‚é¢çš„è§£é‡Šï¼Œå…¶ä¸­ç—…å˜ä¸å·²çŸ¥Melanomaçš„å‡ ä½•è·ç¦»å¯ä½œä¸ºè¡¡é‡é£é™©çš„ç›´è§‚æŒ‡æ ‡ã€‚è¿™ä¸€æ–¹æ¡ˆæˆåŠŸå°†é«˜é¢„æµ‹æ€§èƒ½ä¸ä¸´åºŠé€‚ç”¨æ€§ç›¸ç»“åˆï¼Œé€šè¿‡é€æ˜çš„å†³ç­–æœºåˆ¶å¢å¼ºäº†å¯¹AIè¾…åŠ©è¯Šæ–­çš„ä¿¡ä»»ï¼Œä¸ºé»‘è‰²ç´ ç˜¤çš„æ—©æœŸæ£€æµ‹å’Œæ¨¡ç³Šç—…ä¾‹åˆ†ææä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The proposed approach presents some technical imperfections and needs to be refined with further examinations",
      "pdf_url": "https://arxiv.org/pdf/2506.18414v2",
      "published_date": "2025-06-23 08:49:57 UTC",
      "updated_date": "2025-07-25 13:54:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:58:26.064713+00:00"
    },
    {
      "arxiv_id": "2506.18403v2",
      "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
      "title_zh": "è°ƒè¯•è¡°å‡æŒ‡æ•°ï¼šå¯¹ä»£ç å¤§è¯­è¨€æ¨¡å‹è°ƒè¯•ç­–ç•¥çš„å†æ€è€ƒ",
      "authors": [
        "Muntasir Adnan",
        "Carlos C. N. Kuhn"
      ],
      "abstract": "The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»£ç å¤§è¯­è¨€æ¨¡å‹(Code LLMs)åœ¨è¿­ä»£è°ƒè¯•è¿‡ç¨‹ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œæ­ç¤ºäº† AI è°ƒè¯•èƒ½åŠ›éµå¾ªå¯é¢„æµ‹çš„æŒ‡æ•°è¡°å‡è§„å¾‹ï¼Œå³å¤§å¤šæ•°æ¨¡å‹åœ¨ä»… 2-3 æ¬¡å°è¯•åä¾¿ä¼šæŸå¤± 60-80% çš„è°ƒè¯•æ•ˆèƒ½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†è°ƒè¯•è¡°å‡æŒ‡æ•°(Debugging Decay Index, DDI)æ•°å­¦æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–è°ƒè¯•å¤±æ•ˆçš„æ—¶æœºå¹¶é¢„æµ‹å¿…è¦çš„å¹²é¢„ç‚¹ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†â€œæˆ˜ç•¥æ€§é‡æ–°å¼€å§‹â€(strategic fresh start)ç­–ç•¥ï¼Œé€šè¿‡åœ¨è°ƒè¯•è¿‡ç¨‹çš„å…³é”®èŠ‚ç‚¹ä»å¼€å‘åˆ©ç”¨(exploitation)è½¬å‘æ¢ç´¢(exploration)ï¼Œè¯æ˜äº†ç²¾å‡†å¹²é¢„å¯ä»¥æŒ½æ•‘è°ƒè¯•çš„æœ‰æ•ˆæ€§ã€‚DDI ä¸ä»…æ­ç¤ºäº†å½“å‰ AI è°ƒè¯•çš„å†…åœ¨å±€é™ï¼Œè¿˜ä¸ºä¼˜åŒ–è¿­ä»£å¼ä»£ç ç”Ÿæˆç³»ç»Ÿæä¾›äº†é¦–ä¸ªå®šé‡ä¼˜åŒ–æ¡†æ¶ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ç°æœ‰çš„ç›²ç›®è¿­ä»£æ¨¡å¼ï¼Œä¸ºæå‡è‡ªä¸»è½¯ä»¶å·¥ç¨‹çš„å¯é æ€§æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18403v2",
      "published_date": "2025-06-23 08:40:45 UTC",
      "updated_date": "2025-07-13 09:04:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:15.347337+00:00"
    },
    {
      "arxiv_id": "2506.21619v2",
      "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech",
      "title_zh": "IndexTTS2ï¼šå…·æœ‰æƒ…æ„Ÿè¡¨ç°åŠ›ä¸”æ—¶é•¿å¯æ§çš„è‡ªå›å½’é›¶æ ·æœ¬è¯­éŸ³åˆæˆçªç ´æ€§è¿›å±•",
      "authors": [
        "Siyi Zhou",
        "Yiquan Zhou",
        "Yi He",
        "Xun Zhou",
        "Jinchao Wang",
        "Wei Deng",
        "Jingchen Shu"
      ],
      "abstract": "Existing autoregressive large-scale text-to-speech (TTS) models have advantages in speech naturalness, but their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This becomes a significant limitation in applications requiring strict audio-visual synchronization, such as video dubbing. This paper introduces IndexTTS2, which proposes a novel, general, and autoregressive model-friendly method for speech duration control. The method supports two generation modes: one explicitly specifies the number of generated tokens to precisely control speech duration; the other freely generates speech in an autoregressive manner without specifying the number of tokens, while faithfully reproducing the prosodic features of the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control over timbre and emotion. In the zero-shot setting, the model can accurately reconstruct the target timbre (from the timbre prompt) while perfectly reproducing the specified emotional tone (from the style prompt). To enhance speech clarity in highly emotional expressions, we incorporate GPT latent representations and design a novel three-stage training paradigm to improve the stability of the generated speech. Additionally, to lower the barrier for emotional control, we designed a soft instruction mechanism based on text descriptions by fine-tuning Qwen3, effectively guiding the generation of speech with the desired emotional orientation. Finally, experimental results on multiple datasets show that IndexTTS2 outperforms state-of-the-art zero-shot TTS models in terms of word error rate, speaker similarity, and emotional fidelity. Audio samples are available at: https://index-tts.github.io/index-tts2.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IndexTTS2ï¼Œæ—¨åœ¨è§£å†³è‡ªå›å½’(Autoregressive)å¤§è§„æ¨¡æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹åœ¨æ—¶é•¿æ§åˆ¶å’ŒéŸ³ç”»åŒæ­¥æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç§é€šç”¨çš„æ–¹æ³•æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼Œæ—¢èƒ½é€šè¿‡æŒ‡å®šTokenæ•°é‡å®ç°ç²¾ç¡®çš„æ—¶é•¿æ§åˆ¶ï¼Œä¹Ÿèƒ½åœ¨ä¿æŒè¾“å…¥æç¤ºéŸµå¾‹ç‰¹å¾çš„åŒæ—¶è¿›è¡Œè‡ªç”±è‡ªå›å½’ç”Ÿæˆã€‚IndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾ä¸è¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œæ”¯æŒåœ¨é›¶æ ·æœ¬(Zero-Shot)è®¾ç½®ä¸‹ç‹¬ç«‹æ§åˆ¶éŸ³è‰²å’Œæƒ…æ„Ÿã€‚ä¸ºæå‡é«˜æƒ…æ„Ÿè¡¨è¾¾ä¸‹çš„è¯­éŸ³æ¸…æ™°åº¦å’Œç¨³å®šæ€§ï¼Œç ”ç©¶å¼•å…¥äº†GPTæ½œå˜é‡è¡¨ç¤º(Latent Representations)å¹¶è®¾è®¡äº†å…¨æ–°çš„ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¾®è°ƒQwen3å®ç°çš„è½¯æŒ‡ä»¤(Soft Instruction)æœºåˆ¶å…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬æè¿°è½»æ¾å¼•å¯¼è¯­éŸ³çš„æƒ…æ„Ÿå€¾å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨å­—é”™è¯¯ç‡(Word Error Rate)ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21619v2",
      "published_date": "2025-06-23 08:33:40 UTC",
      "updated_date": "2025-09-03 10:46:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:58:35.241151+00:00"
    },
    {
      "arxiv_id": "2506.21618v1",
      "title": "TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge",
      "title_zh": "TrajTokï¼š2025 Waymo Open Sim Agents Challenge æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Zhiyuan Zhang",
        "Xiaosong Jia",
        "Guanyu Chen",
        "Qifeng Li",
        "Junchi Yan"
      ],
      "abstract": "In this technical report, we introduce TrajTok, a trajectory tokenizer for discrete next-token-prediction based behavior generation models, which combines data-driven and rule-based methods with better coverage, symmetry and robustness, along with a spatial-aware label smoothing method for cross-entropy loss. We adopt the tokenizer and loss for the SMART model and reach a superior performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge 2025. We will open-source the code in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†TrajTokï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºåŸºäºç¦»æ•£next-token-predictionçš„è¡Œä¸ºç”Ÿæˆæ¨¡å‹è®¾è®¡çš„è½¨è¿¹åˆ†è¯å™¨(trajectory tokenizer)ã€‚TrajTokç»“åˆäº†æ•°æ®é©±åŠ¨ä¸åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œåœ¨è¦†ç›–ç‡ã€å¯¹ç§°æ€§å’Œé²æ£’æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶é…å¥—æå‡ºäº†ä¸€ç§ç”¨äºcross-entropy lossçš„ç©ºé—´æ„ŸçŸ¥æ ‡ç­¾å¹³æ»‘(spatial-aware label smoothing)æ–¹æ³•ã€‚é€šè¿‡åœ¨SMARTæ¨¡å‹ä¸­é‡‡ç”¨è¯¥åˆ†è¯å™¨å’ŒæŸå¤±å‡½æ•°ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨2025å¹´Waymo Open Sim Agents Challengeä¸­å–å¾—äº†0.7852çš„realism scoreï¼Œå±•ç°äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸­çš„è¡Œä¸ºå»ºæ¨¡æä¾›äº†åˆ›æ–°çš„ç¦»æ•£åŒ–æ–¹æ¡ˆï¼Œä¸”ç›¸å…³ä»£ç å°†åœ¨æœªæ¥å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21618v1",
      "published_date": "2025-06-23 08:32:05 UTC",
      "updated_date": "2025-06-23 08:32:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:58:36.502686+00:00"
    },
    {
      "arxiv_id": "2506.18396v2",
      "title": "ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction",
      "title_zh": "ADNF-Clusteringï¼šä¸€ç§é¢å‘ç™½è¡€ç—…é¢„æµ‹çš„è‡ªé€‚åº”åŠ¨æ€ç¥ç»æ¨¡ç³Šèšç±»æ–¹æ³•",
      "authors": [
        "Marco Aruta",
        "Ciro Listone",
        "Giuseppe Murano",
        "Aniello Murano"
      ],
      "abstract": "Leukemia diagnosis and monitoring rely increasingly on high-throughput image data, yet conventional clustering methods lack the flexibility to accommodate evolving cellular patterns and quantify uncertainty in real time. We introduce Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable framework that combines Convolutional Neural Network-based feature extraction with an online fuzzy clustering engine. ADNF initializes soft partitions via Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy evolution. A topology refinement stage performs density-weighted merging and entropy-guided splitting to guard against over- and under-segmentation. On the C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of 0.51, demonstrating superior cohesion and separation over static baselines. The method's adaptive uncertainty modeling and label-free operation hold immediate potential for integration within the INFANT pediatric oncology network, enabling scalable, up-to-date support for personalized leukemia management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ADNF-Clusteringï¼ˆAdaptive and Dynamic Neuro-Fuzzy Clusteringï¼‰ï¼Œä¸€ç§é¢å‘Leukemiaé¢„æµ‹çš„è‡ªé€‚åº”åŠ¨æ€ç¥ç»æ¨¡ç³Šèšç±»æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿèšç±»æ–¹æ³•åœ¨å¤„ç†æ¼”åŒ–ç»†èƒæ¨¡å¼åŠå®æ—¶é‡åŒ–ä¸ç¡®å®šæ€§æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºCNNï¼ˆConvolutional Neural Networkï¼‰çš„ç‰¹å¾æå–ä¸åœ¨çº¿æ¨¡ç³Šèšç±»å¼•æ“ï¼Œé€šè¿‡Fuzzy C-Meansè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶åˆ©ç”¨æµ‹é‡ç†µæ¼”åŒ–çš„Fuzzy Temporal Index (FTI) æŒç»­æ›´æ–°å¾®ç°‡ä¸­å¿ƒã€å¯†åº¦åŠæ¨¡ç³Šæ€§å‚æ•°ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡æ‹“æ‰‘ä¼˜åŒ–é˜¶æ®µæ‰§è¡Œå¯†åº¦åŠ æƒåˆå¹¶å’Œç†µå¼•å¯¼åˆ†è£‚ï¼Œæœ‰æ•ˆé˜²æ­¢äº†è¿‡åº¦åˆ†å‰²æˆ–åˆ†å‰²ä¸è¶³çš„é—®é¢˜ã€‚åœ¨C-NMCç™½è¡€ç—…æ˜¾å¾®å½±åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥å·¥å…·å®ç°äº†0.51çš„è½®å»“ç³»æ•°ï¼ˆsilhouette scoreï¼‰ï¼Œåœ¨å‡èšåº¦å’Œåˆ†ç¦»åº¦ä¸Šæ˜¾è‘—ä¼˜äºé™æ€åŸºå‡†æ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„è‡ªé€‚åº”ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œæ— æ ‡ç­¾æ“ä½œç‰¹æ€§ï¼Œä½¿å…¶å…·æœ‰é›†æˆåˆ°INFANTå„¿ç§‘è‚¿ç˜¤ç½‘ç»œä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½ä¸ºä¸ªæ€§åŒ–ç™½è¡€ç—…ç®¡ç†æä¾›å¯æ‰©å±•ä¸”å®æ—¶çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.18396v2",
      "published_date": "2025-06-23 08:30:17 UTC",
      "updated_date": "2025-11-28 16:14:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:58:54.351170+00:00"
    },
    {
      "arxiv_id": "2506.18387v1",
      "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹åŠä¸äººç±»è¯„ä»·ä¸€è‡´æ€§æŒ‡æ ‡çš„åŒ»å­¦æŠ¥å‘Šå› æœè§£é‡Šè¯„ä¼°",
      "authors": [
        "Yousang Cho",
        "Key-Sun Choi"
      ],
      "abstract": "This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸åŒè¯„ä¼°æŒ‡æ ‡åœ¨æ•æ‰è‡ªåŠ¨ç”Ÿæˆçš„è¯Šæ–­æŠ¥å‘Šä¸­å› æœè§£é‡Šè´¨é‡æ–¹é¢çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å¯¹æ¯”äº† BERTScoreã€Cosine Similarityã€BioSentVecã€GPT-Whiteã€GPT-Black ä»¥åŠä¸“å®¶å®šæ€§è¯„ä¼°è¿™å…­ç§æŒ‡æ ‡ï¼Œå¹¶æ¶µç›–äº†åŸºäºè§‚å¯Ÿå’ŒåŸºäºå¤šé¡¹é€‰æ‹©çš„ä¸¤ç§æŠ¥å‘Šç”Ÿæˆè¾“å…¥ç±»å‹ã€‚åŒæ—¶ï¼Œå®éªŒåº”ç”¨äº†åæ˜ ä»»åŠ¡ç‰¹å®šä¼˜å…ˆçº§å’Œç­‰æƒé‡çš„ä¸¤ç§åŠ æƒç­–ç•¥ä»¥åˆ†æå¯¹è¯„ä¼°ç»“æœçš„å½±å“ã€‚ç»“æœæ˜¾ç¤º GPT-Black åœ¨è¯†åˆ«é€»è¾‘è¿è´¯ä¸”å…·æœ‰ä¸´åºŠæœ‰æ•ˆæ€§çš„å› æœå™è¿°æ–¹é¢è¡¨ç°å‡ºæœ€å¼ºçš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè€Œ GPT-White ä¹Ÿä¸ä¸“å®¶è¯„ä¼°ä¿æŒäº†è‰¯å¥½çš„ä¸€è‡´æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºç›¸ä¼¼åº¦çš„æŒ‡æ ‡ï¼ˆSimilarity-based metricsï¼‰åœ¨è¡¡é‡ä¸´åºŠæ¨ç†è´¨é‡æ—¶è¡¨ç°ä¸ä½³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æŒ‡æ ‡é€‰æ‹©å¯¹è¯„ä¼°ç»“æœçš„å†³å®šæ€§ä½œç”¨ï¼Œå¹¶æ”¯æŒåœ¨éœ€è¦é«˜è§£é‡Šæ€§å’Œå› æœæ¨ç†çš„åŒ»ç–—ä»»åŠ¡ä¸­é‡‡ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM-basedï¼‰çš„è¯„ä¼°æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy, July 17, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18387v1",
      "published_date": "2025-06-23 08:19:21 UTC",
      "updated_date": "2025-06-23 08:19:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:05.792071+00:00"
    },
    {
      "arxiv_id": "2506.18383v1",
      "title": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization",
      "title_zh": "LOGICPOï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¸åå¥½ä¼˜åŒ–å®ç°è‡ªç„¶è¯­è¨€é€»è¾‘é—®é¢˜å‘ä¸€é˜¶é€»è¾‘çš„é«˜æ•ˆè½¬æ¢",
      "authors": [
        "Koushik Viswanadha",
        "Deepanway Ghosal",
        "Somak Aditya"
      ],
      "abstract": "Logical reasoning is a key task for artificial intelligence due to it's role in major downstream tasks such as Question Answering, Summarization. Recent methods in improving the reasoning ability of LLMs fall short in correctly converting a natural language reasoning problem to an equivalent logical formulation, which hinders the framework's overall ability to reason. Towards this, we propose to use finetuning on a preference optimization dataset to learn to parse and represent a natural language problem as a whole to a consistent logical program by 1) introducing a new supervised and preference optimization dataset LogicPO, and 2) adopting popular techniques such as Direct Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune open-source LLMs. Our best model with Phi-3.5 consistently outperforms GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14% less syntax errors. Through the framework and our improved evaluation metrics, we offer a promising direction in improving the logical reasoning of LLMs by better representing them in their logical formulations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å°†è‡ªç„¶è¯­è¨€ (Natural Language) æ¨ç†é—®é¢˜å‡†ç¡®è½¬æ¢ä¸ºä¸€é˜¶é€»è¾‘ (First-Order Logic, FOL) å½¢å¼æ—¶å­˜åœ¨çš„ä¸è¶³ï¼Œæå‡ºäº† LogicPO æ¡†æ¶ã€‚ä½œè€…é€šè¿‡å¼•å…¥å…¨æ–°çš„ç›‘ç£å¼åå¥½ä¼˜åŒ–æ•°æ®é›† LogicPOï¼Œå¹¶é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ– (Direct Preference Optimization, DPO) å’Œ Kahneman-Tversky ä¼˜åŒ– (KTO) ç­‰æŠ€æœ¯å¯¹å¼€æºå¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€é—®é¢˜æ•´ä½“è§£æä¸ºä¸€è‡´çš„é€»è¾‘ç¨‹åºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäº Phi-3.5 çš„æœ€ä½³æ¨¡å‹åœ¨é€»è¾‘æ­£ç¡®æ€§ä¸Šæ¯” GPT-3.5-turbo (8-shot) æé«˜äº† 10%ï¼ŒåŒæ—¶è¯­æ³•é”™è¯¯å‡å°‘äº† 14%ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æ”¹è¿›è½¬æ¢æ¡†æ¶å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºé€šè¿‡ä¼˜åŒ–é€»è¾‘è¡¨ç¤ºæ¥å¢å¼º LLMs çš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†æå…·å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18383v1",
      "published_date": "2025-06-23 08:15:24 UTC",
      "updated_date": "2025-06-23 08:15:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:05.443979+00:00"
    },
    {
      "arxiv_id": "2506.18382v1",
      "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching",
      "title_zh": "PERSCENï¼šé¢å‘å¤šåœºæ™¯åŒ¹é…çš„ä¸ªæ€§åŒ–äº¤äº’æ¨¡å¼ä¸åœºæ™¯åå¥½å­¦ä¹ ",
      "authors": [
        "Haotong Du",
        "Yaqing Wang",
        "Fei Xiong",
        "Lei Shao",
        "Ming Liu",
        "Hao Gu",
        "Quanming Yao",
        "Zhen Wang"
      ],
      "abstract": "With the expansion of business scales and scopes on online platforms, multi-scenario matching has become a mainstream solution to reduce maintenance costs and alleviate data sparsity. The key to effective multi-scenario recommendation lies in capturing both user preferences shared across all scenarios and scenario-aware preferences specific to each scenario. However, existing methods often overlook user-specific modeling, limiting the generation of personalized user representations. To address this, we propose PERSCEN, an innovative approach that incorporates user-specific modeling into multi-scenario matching. PERSCEN constructs a user-specific feature graph based on user characteristics and employs a lightweight graph neural network to capture higher-order interaction patterns, enabling personalized extraction of preferences shared across scenarios. Additionally, we leverage vector quantization techniques to distil scenario-aware preferences from users' behavior sequence within individual scenarios, facilitating user-specific and scenario-aware preference modeling. To enhance efficient and flexible information transfer, we introduce a progressive scenario-aware gated linear unit that allows fine-grained, low-latency fusion. Extensive experiments demonstrate that PERSCEN outperforms existing methods. Further efficiency analysis confirms that PERSCEN effectively balances performance with computational cost, ensuring its practicality for real-world industrial systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PERSCEN æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šåœºæ™¯åŒ¹é… (multi-scenario matching) ä¸­ç°æœ‰æ–¹æ³•å› å¿½è§†ç”¨æˆ·ç‰¹å®šå»ºæ¨¡è€Œå¯¼è‡´ä¸ªæ€§åŒ–è¡¨å¾å—é™çš„é—®é¢˜ã€‚PERSCEN é€šè¿‡æ„å»ºç”¨æˆ·ç‰¹å®šçš„ç‰¹å¾å›¾å¹¶åˆ©ç”¨è½»é‡çº§å›¾ç¥ç»ç½‘ç»œ (Graph Neural Network) æ•æ‰é«˜é˜¶äº¤äº’æ¨¡å¼ï¼Œå®ç°äº†è·¨åœºæ™¯å…±äº«åå¥½çš„ä¸ªæ€§åŒ–æå–ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å‘é‡é‡åŒ– (vector quantization) æŠ€æœ¯ä»ç”¨æˆ·åœ¨ç‰¹å®šåœºæ™¯å†…çš„è¡Œä¸ºåºåˆ—ä¸­è’¸é¦å‡ºåœºæ™¯æ„ŸçŸ¥åå¥½ï¼Œå¼ºåŒ–äº†ç”¨æˆ·ç‰¹å®šä¸åœºæ™¯ç›¸å…³çš„åå¥½å»ºæ¨¡ã€‚ä¸ºäº†æå‡ä¿¡æ¯ä¼ é€’çš„æ•ˆç‡ä¸çµæ´»æ€§ï¼Œç ”ç©¶å¼•å…¥äº†æ¸è¿›å¼åœºæ™¯æ„ŸçŸ¥é—¨æ§çº¿æ€§å•å…ƒ (scenario-aware gated linear unit)ï¼Œå®ç°äº†ç»†ç²’åº¦ä¸”ä½å»¶è¿Ÿçš„ä¿¡æ¯èåˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPERSCEN åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ï¼Œå…·å¤‡åœ¨å®é™…å·¥ä¸šç³»ç»Ÿä¸­éƒ¨ç½²çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18382v1",
      "published_date": "2025-06-23 08:15:16 UTC",
      "updated_date": "2025-06-23 08:15:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:09.643865+00:00"
    },
    {
      "arxiv_id": "2506.18365v1",
      "title": "Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots",
      "title_zh": "æœºå™¨äººä¸å„¿ç«¥å…±åŒå­¦ä¹ ï¼šé€šè¿‡æ•™æˆåŒä¼´å¼äº¤äº’æœºå™¨äººæå‡çŸ¥è¯†ç•™å­˜",
      "authors": [
        "Imene Tarakli",
        "Samuele Vinanzi",
        "Richard Moore",
        "Alessandro Di Nuovo"
      ],
      "abstract": "Despite growing interest in Learning-by-Teaching (LbT), few studies have explored how this paradigm can be implemented with autonomous, peer-like social robots in real classrooms. Most prior work has relied on scripted or Wizard-of-Oz behaviors, limiting our understanding of how real-time, interactive learning can be supported by artificial agents. This study addresses this gap by introducing Interactive Reinforcement Learning (RL) as a cognitive model for teachable social robots. We conducted two between-subject experiments with 58 primary school children, who either taught a robot or practiced independently on a tablet while learning French vocabulary (memorization) and grammatical rules (inference). The robot, powered by Interactive RL, learned from the child's evaluative feedback. Children in the LbT condition achieved significantly higher retention gains compared to those in the self-practice condition, especially on the grammar task. Learners with lower prior knowledge benefited most from teaching the robot. Behavioural metrics revealed that children adapted their teaching strategies over time and engaged more deeply during inference tasks. This work makes two contributions: (1) it introduces Interactive RL as a pedagogically effective and scalable model for peer-robot learning, and (2) it demonstrates, for the first time, the feasibility of deploying multiple autonomous robots simultaneously in real classrooms. These findings extend theoretical understanding of LbT by showing that social robots can function not only as passive tutees but as adaptive partners that enhance meta-cognitive engagement and long-term learning outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çœŸå®è¯¾å ‚ä¸­åˆ©ç”¨è‡ªä¸»åŒä¼´å¼ç¤¾äº¤æœºå™¨äººå®ç°â€œä»¥æ•™ä¿ƒå­¦â€(Learning-by-Teaching, LbT)èŒƒå¼çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€ç ”ç©¶è¿‡åº¦ä¾èµ–äººå·¥è„šæœ¬æˆ–â€œç»¿é‡ä»™è¸ªâ€(Wizard-of-Oz)è¡Œä¸ºçš„å±€é™ã€‚ç ”ç©¶å¼•å…¥äº†äº¤äº’å¼å¼ºåŒ–å­¦ä¹ (Interactive Reinforcement Learning, RL)ä½œä¸ºæœºå™¨äººçš„è®¤çŸ¥æ¨¡å‹ï¼Œé€šè¿‡å¯¹58åå­¦ä¹ æ³•è¯­çš„å„¿ç«¥è¿›è¡Œå®éªŒï¼Œå¯¹æ¯”äº†æ•™å¯¼æœºå™¨äººä¸è‡ªä¸»ç»ƒä¹ çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œå‚ä¸LbTæ¨¡å¼çš„å„¿ç«¥åœ¨çŸ¥è¯†ä¿ç•™ç‡ä¸Šæ˜¾è‘—ä¼˜äºè‡ªä¸»ç»ƒä¹ ç»„ï¼Œå°¤å…¶åœ¨è¯­æ³•æ¨ç†ä»»åŠ¡ä»¥åŠä½å…ˆå‰çŸ¥è¯†æ°´å¹³çš„å­¦ä¹ è€…ä¸­è·ç›Šæœ€ä¸ºæ˜æ˜¾ã€‚è¡Œä¸ºåˆ†ææ˜¾ç¤ºï¼Œå„¿ç«¥èƒ½å¤Ÿéšæ—¶é—´è°ƒæ•´æ•™å­¦ç­–ç•¥ï¼Œå¹¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´æ·±å±‚æ¬¡çš„å‚ä¸ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†äº¤äº’å¼å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œè¿˜é¦–æ¬¡è¯æ˜äº†åœ¨çœŸå®è¯¾å ‚ä¸­åŒæ—¶éƒ¨ç½²å¤šå°è‡ªä¸»æœºå™¨äººçš„å¯è¡Œæ€§ï¼Œç¡®ç«‹äº†ç¤¾äº¤æœºå™¨äººä½œä¸ºè‡ªé€‚åº”åˆä½œä¼™ä¼´åœ¨æå‡å­¦ç”Ÿå…ƒè®¤çŸ¥å‚ä¸å’Œé•¿æœŸå­¦ä¹ æˆæ•ˆæ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18365v1",
      "published_date": "2025-06-23 07:51:04 UTC",
      "updated_date": "2025-06-23 07:51:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:56.717599+00:00"
    },
    {
      "arxiv_id": "2507.01041v3",
      "title": "Fast AI Model Splitting over Edge Networks",
      "title_zh": "è¾¹ç¼˜ç½‘ç»œä¸­çš„å¿«é€ŸAIæ¨¡å‹åˆ‡åˆ†",
      "authors": [
        "Zuguang Li",
        "Wen Wu",
        "Shaohua Wu",
        "Songge Zhang",
        "Ye Wang",
        "Xuemin",
        "Shen"
      ],
      "abstract": "Split learning (SL) has emerged as a computationally efficient approach for artificial intelligence (AI) model training, which can alleviate device-side computational workloads. However, complex AI model architectures pose high computational complexity to obtain the optimal model splitting. In this paper, we represent an arbitrary AI model as a directed acyclic graph (DAG), and then reformulate the optimal model splitting problem as a minimum s-t cut search problem. To solve the problem, we propose a fast DAG-based model splitting algorithm, which restructures the DAG to enable the optimal model splitting identification via a maximum flow method. Theoretical analysis indicates that the proposed algorithm is optimal. Furthermore, considering AI models with block structures, we propose a block-wise model splitting algorithm to reduce computational complexity. The algorithm abstracts each block, i.e., a component consisting of multiple layers, into a single vertex, thereby obtaining the optimal model splitting via a simplified DAG. Extensive experimental results demonstrate that the proposed algorithms can determine the optimal model splitting within milliseconds, as well as reduce training delay by 24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜ç½‘ç»œç¯å¢ƒæå‡ºäº†ä¸€ç§å¿«é€ŸAIæ¨¡å‹åˆ‡åˆ†æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡æ‹†åˆ†å­¦ä¹ (Split learning)å‡è½»è®¾å¤‡ç«¯çš„è®¡ç®—è´Ÿè½½ã€‚ä½œè€…å°†å¤æ‚çš„AIæ¨¡å‹æ¶æ„å»ºæ¨¡ä¸ºæœ‰å‘æ— ç¯å›¾(DAG)ï¼Œå¹¶å·§å¦™åœ°å°†å¯»æ‰¾æœ€ä¼˜æ¨¡å‹åˆ‡åˆ†çš„é—®é¢˜è½¬åŒ–ä¸ºæœ€å°s-tå‰²(minimum s-t cut)æœç´¢é—®é¢˜ã€‚ä¸ºäº†é«˜æ•ˆæ±‚è§£ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºDAGçš„å¿«é€Ÿåˆ‡åˆ†ç®—æ³•ï¼Œé€šè¿‡æœ€å¤§æµ(maximum flow)æ–¹æ³•ç¡®å®šæœ€ä¼˜åˆ†å‰²ç‚¹ï¼Œå¹¶åœ¨ç†è®ºä¸Šè¯æ˜äº†å…¶æœ€ä¼˜æ€§ã€‚é’ˆå¯¹å…·æœ‰å—ç»“æ„çš„å¤æ‚æ¨¡å‹ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†å—çº§åˆ‡åˆ†ç®—æ³•(block-wise model splitting algorithm)ï¼Œé€šè¿‡å°†å¤šä¸ªå±‚ç»„æˆçš„å—æŠ½è±¡ä¸ºå•ä¸ªé¡¶ç‚¹æ¥ç®€åŒ–DAGã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œè¯¥ç³»åˆ—ç®—æ³•èƒ½åœ¨æ¯«ç§’çº§å†…é”å®šæœ€ä¼˜åˆ‡åˆ†æ–¹æ¡ˆï¼Œåœ¨åŠ¨æ€è¾¹ç¼˜ç½‘ç»œä¸­ç›¸è¾ƒäºç°æœ‰åŸºå‡†æ¨¡å‹èƒ½æœ‰æ•ˆé™ä½24.62%è‡³38.95%çš„è®­ç»ƒå»¶è¿Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This version lacks sufficient detail in key technical parts, including the equivalence proof for the s-t cut transformation and the computational complexity analysis (Sections VI-D). We are withdrawing it to prepare a revised, more complete manuscript",
      "pdf_url": "https://arxiv.org/pdf/2507.01041v3",
      "published_date": "2025-06-23 07:14:04 UTC",
      "updated_date": "2025-12-24 01:45:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:32.829329+00:00"
    },
    {
      "arxiv_id": "2506.18348v3",
      "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team",
      "title_zh": "åŠ¨æ€çŸ¥è¯†äº¤æ¢ä¸åŒé‡å¤šæ ·æ€§è¯„å®¡ï¼šé«˜æ•ˆé‡Šæ”¾å¤šæ™ºèƒ½ä½“ç§‘ç ”å›¢é˜Ÿçš„æ½œåŠ›",
      "authors": [
        "Weilun Yu",
        "Shixiang Tang",
        "Yonggui Huang",
        "Nanqing Dong",
        "Li Fan",
        "Honggang Qi",
        "Wei Liu",
        "Xiaoli Diao",
        "Xi Chen",
        "Wanli Ouyang"
      ],
      "abstract": "Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IDVSCI (Internal Discussion and Vote SCIentists)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ç§‘ç ”ä¸­è‡³å…³é‡è¦çš„äº’åŠ¨æ¨ç†å’Œè¯„ä»·æœºåˆ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŠ¨æ€çŸ¥è¯†äº¤æ¢ (Dynamic Knowledge Exchange) æœºåˆ¶ï¼Œé€šè¿‡æ™ºèƒ½ä½“é—´çš„è¿­ä»£åé¦ˆä¿ƒè¿›æ·±å±‚æ¨ç†ï¼Œå¹¶é‡‡ç”¨åŒé‡å¤šæ ·æ€§è¯„å®¡ (Dual-Diversity Review) èŒƒå¼æ¥æ¨¡æ‹Ÿå¼‚æ„ä¸“å®¶çš„è¯„ä¼°è¿‡ç¨‹ã€‚å®éªŒåœ¨è®¡ç®—æœºç§‘å­¦åŸºå‡†æ•°æ®é›†å’Œæ–°æ„å»ºçš„å¥åº·ç§‘å­¦æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤º IDVSCI çš„æ€§èƒ½æ˜¾è‘—ä¼˜äº AI Scientist å’Œ VIRSCI ç­‰ç°æœ‰ç³»ç»Ÿã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨åŸºäº LLMs çš„è‡ªä¸»ç§‘ç ”ç³»ç»Ÿä¸­ï¼Œå»ºæ¨¡äº’åŠ¨ä¸åŒè¡Œè¯„å®¡ (Peer Review) åŠ¨æ€å¯¹äºç”Ÿæˆå…·æœ‰å½±å“åŠ›çš„ç§‘å­¦æ„æƒ³å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18348v3",
      "published_date": "2025-06-23 07:12:08 UTC",
      "updated_date": "2025-08-01 10:35:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:06.062331+00:00"
    },
    {
      "arxiv_id": "2507.01972v1",
      "title": "Accelerated Portfolio Optimization and Option Pricing with Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ•èµ„ç»„åˆä¼˜åŒ–ä¸æœŸæƒå®šä»·åŠ é€Ÿ",
      "authors": [
        "Hadi Keramati",
        "Samaneh Jazayeri"
      ],
      "abstract": "We present a reinforcement learning (RL)-driven framework for optimizing block-preconditioner sizes in iterative solvers used in portfolio optimization and option pricing. The covariance matrix in portfolio optimization or the discretization of differential operators in option pricing models lead to large linear systems of the form $\\mathbf{A}\\textbf{x}=\\textbf{b}$. Direct inversion of high-dimensional portfolio or fine-grid option pricing incurs a significant computational cost. Therefore, iterative methods are usually used for portfolios in real-world situations. Ill-conditioned systems, however, suffer from slow convergence. Traditional preconditioning techniques often require problem-specific parameter tuning. To overcome this limitation, we rely on RL to dynamically adjust the block-preconditioner sizes and accelerate iterative solver convergence. Evaluations on a suite of real-world portfolio optimization matrices demonstrate that our RL framework can be used to adjust preconditioning and significantly accelerate convergence and reduce computational cost. The proposed accelerated solver supports faster decision-making in dynamic portfolio allocation and real-time option pricing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æŠ•èµ„ç»„åˆä¼˜åŒ–å’ŒæœŸæƒå®šä»·ä¸­è¿­ä»£æ±‚è§£å™¨çš„å—é¢„æ¡ä»¶(block-preconditioner)å°ºå¯¸ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒæŠ•èµ„ç»„åˆçš„åæ–¹å·®çŸ©é˜µæˆ–æœŸæƒå®šä»·æ¨¡å‹çš„ç¦»æ•£åŒ–é€šå¸¸ä¼šäº§ç”Ÿå¤§è§„æ¨¡çº¿æ€§ç³»ç»Ÿï¼Œç›´æ¥æ±‚é€†å…·æœ‰æé«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¼ ç»Ÿçš„è¿­ä»£æ–¹æ³•åœ¨å¤„ç†ç—…æ€ç³»ç»Ÿ(ill-conditioned systems)æ—¶æ”¶æ•›ç¼“æ…¢ï¼Œä¸”é¢„æ¡ä»¶æŠ€æœ¯é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šé—®é¢˜è¿›è¡Œå¤æ‚çš„å‚æ•°è°ƒä¼˜ã€‚è¯¥æ¡†æ¶é€šè¿‡RLåŠ¨æ€è°ƒæ•´å—é¢„æ¡ä»¶å™¨çš„å‚æ•°ï¼Œæ˜¾è‘—åŠ é€Ÿäº†è¿­ä»£æ±‚è§£å™¨çš„æ”¶æ•›é€Ÿåº¦å¹¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒåœ¨çœŸå®ä¸–ç•Œçš„æŠ•èµ„ç»„åˆä¼˜åŒ–çŸ©é˜µä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåŠ¨æ€èµ„äº§é…ç½®å’Œå®æ—¶æœŸæƒå®šä»·ä¸­çš„å¿«é€Ÿå†³ç­–æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.LG",
        "q-fin.CP"
      ],
      "primary_category": "q-fin.PM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01972v1",
      "published_date": "2025-06-23 07:11:56 UTC",
      "updated_date": "2025-06-23 07:11:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:47.490006+00:00"
    },
    {
      "arxiv_id": "2507.02900v1",
      "title": "Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions",
      "title_zh": "è¯´è¯äººå¤´éƒ¨ç”Ÿæˆçš„å‰æ²¿è¿›å±•ï¼šå¤šæ¨¡æ€æ–¹æ³•ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æŸå¤±å‡½æ•°çš„å…¨é¢ç»¼è¿°",
      "authors": [
        "Vineet Kumar Rakesh",
        "Soumya Mazumdar",
        "Research Pratim Maity",
        "Sarbajit Pal",
        "Amitabha Das",
        "Tapas Samanta"
      ],
      "abstract": "Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡å…¨é¢å›é¡¾äº† Talking Head Generation (THG) é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹æ¢è®¨äº†å¦‚ä½•åˆæˆä¸å›¾åƒã€éŸ³é¢‘ã€æ–‡æœ¬æˆ–è§†é¢‘åŒæ­¥çš„çœŸå®äººè„¸ã€‚æ–‡ç« å°†ç°æœ‰æ–¹æ³•å½’çº³ä¸º 2D-basedã€3D-basedã€Neural Radiance Fields (NeRF)-basedã€Diffusion-based ä»¥åŠå‚æ•°é©±åŠ¨ç­‰æŠ€æœ¯è·¯å¾„ï¼Œå¹¶å¯¹ç›¸å…³ç®—æ³•ã€æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç ”ç©¶å¼ºè°ƒäº†è¯¥æŠ€æœ¯åœ¨æ•°å­—äººã€è§†é¢‘é…éŸ³åŠåœ¨çº¿æ•™è‚²ç­‰åº”ç”¨ä¸­å¯¹æ„ŸçŸ¥çœŸå®æ„Ÿå’ŒæŠ€æœ¯æ•ˆç‡çš„æå‡ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†æç«¯å§¿æ€å¤„ç†ã€å¤šè¯­è¨€åˆæˆå’Œæ—¶é—´ä¸€è‡´æ€§(Temporal Consistency)ç­‰å…³é”®æŒ‘æˆ˜ã€‚é’ˆå¯¹æœªæ¥è¶‹åŠ¿ï¼Œè®ºæ–‡æå‡ºäº†æ¨¡å—åŒ–æ¶æ„ã€æ··åˆæ¨¡å‹(Hybrid Models)ä»¥åŠåˆ›æ–°æŸå¤±å‡½æ•°(Loss Functions)ç­‰å‘å±•æ–¹å‘ã€‚é€šè¿‡æ•´åˆç°æœ‰ç ”ç©¶ï¼Œè¯¥æ–‡æ—¨åœ¨ä¸ºç§‘ç ”äººå‘˜æä¾› Talking Head ç”Ÿæˆé¢†åŸŸçš„å®è·µæŒ‡å—ä¸èµ„æºæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.HC",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02900v1",
      "published_date": "2025-06-23 06:49:42 UTC",
      "updated_date": "2025-06-23 06:49:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:41.041696+00:00"
    },
    {
      "arxiv_id": "2506.18340v3",
      "title": "Controlled Generation with Equivariant Variational Flow Matching",
      "title_zh": "åŸºäºç­‰å˜å˜åˆ†æµåŒ¹é…çš„å¯æ§ç”Ÿæˆ",
      "authors": [
        "Floor Eijkelboom",
        "Heiko Zimmermann",
        "Sharvaree Vadgama",
        "Erik J Bekkers",
        "Max Welling",
        "Christian A. Naesseth",
        "Jan-Willem van de Meent"
      ],
      "abstract": "We derive a controlled generation objective within the framework of Variational Flow Matching (VFM), which casts flow matching as a variational inference problem. We demonstrate that controlled generation can be implemented two ways: (1) by way of end-to-end training of conditional generative models, or (2) as a Bayesian inference problem, enabling post hoc control of unconditional models without retraining. Furthermore, we establish the conditions required for equivariant generation and provide an equivariant formulation of VFM tailored for molecular generation, ensuring invariance to rotations, translations, and permutations. We evaluate our approach on both uncontrolled and controlled molecular generation, achieving state-of-the-art performance on uncontrolled generation and outperforming state-of-the-art models in controlled generation, both with end-to-end training and in the Bayesian inference setting. This work strengthens the connection between flow-based generative modeling and Bayesian inference, offering a scalable and principled framework for constraint-driven and symmetry-aware generation.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ Variational Flow Matching (VFM) æ¡†æ¶ä¸‹æå‡ºäº†å—æ§ç”Ÿæˆçš„ä¼˜åŒ–ç›®æ ‡ï¼Œå°† flow matching è½¬åŒ–ä¸ºå˜åˆ†æ¨æ–­é—®é¢˜è¿›è¡Œå¤„ç†ã€‚è®ºæ–‡å±•ç¤ºäº†ä¸¤ç§å—æ§ç”Ÿæˆè·¯å¾„ï¼šä¸€æ˜¯é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒæ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼ŒäºŒæ˜¯åˆ©ç”¨ Bayesian inference å®ç°å¯¹æ— æ¡ä»¶æ¨¡å‹çš„åéªŒæ§åˆ¶è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ç ”ç©¶è¿›ä¸€æ­¥ç¡®å®šäº†ç­‰å˜ç”Ÿæˆçš„å¿…è¦æ¡ä»¶ï¼Œå¹¶é’ˆå¯¹åˆ†å­ç”Ÿæˆè®¾è®¡äº†ç­‰å˜ VFM å…¬å¼ï¼Œç¡®ä¿æ¨¡å‹å¯¹æ—‹è½¬ã€å¹³ç§»å’Œç½®æ¢ä¿æŒä¸å˜æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨éå—æ§åˆ†å­ç”Ÿæˆä¸­è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œåœ¨å—æ§ç”Ÿæˆåœºæ™¯ä¸‹äº¦ä¼˜äºç°æœ‰å…ˆè¿›æ¨¡å‹ã€‚æ­¤å·¥ä½œæ·±åŒ–äº† flow-based ç”Ÿæˆå»ºæ¨¡ä¸ Bayesian inference çš„è”ç³»ï¼Œä¸ºå¯¹ç§°æ„ŸçŸ¥å’Œçº¦æŸé©±åŠ¨çš„ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ç³»ç»Ÿæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18340v3",
      "published_date": "2025-06-23 06:42:48 UTC",
      "updated_date": "2025-10-03 17:43:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:46.539104+00:00"
    },
    {
      "arxiv_id": "2506.18339v2",
      "title": "Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics",
      "title_zh": "ç»“æ„åŒ– Kolmogorov-Arnold ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼šé¢å‘éçº¿æ€§åŠ¨åŠ›å­¦çš„å¯è§£é‡Šå­¦ä¹ ä¸ç¬¦å·å‘ç°",
      "authors": [
        "Wei Liu",
        "Kiran Bacsa",
        "Loon Ching Tang",
        "Eleni Chatzi"
      ],
      "abstract": "Understanding and modeling nonlinear dynamical systems is a fundamental challenge across science and engineering. Deep learning has shown remarkable potential for capturing complex system behavior, yet achieving models that are both accurate and physically interpretable remains difficult. To address this, we propose Structured Kolmogorov-Arnold Neural ODEs (SKANODEs), a framework that integrates structured state-space modeling with Kolmogorov-Arnold Networks (KANs). Within a Neural ODE architecture, SKANODE employs a fully trainable KAN as a universal function approximator to perform virtual sensing, recovering latent states that correspond to interpretable physical quantities such as displacements and velocities. Leveraging KAN's symbolic regression capability, SKANODE then extracts compact, interpretable expressions for the system's governing dynamics. Extensive experiments on simulated and real-world systems demonstrate that SKANODE achieves superior predictive accuracy, discovers physics-consistent dynamics, and reveals complex nonlinear behavior. Notably, it identifies hysteretic behavior in an F-16 aircraft and recovers a concise symbolic equation describing this phenomenon. SKANODE thus enables interpretable, data-driven discovery of physically grounded models for complex nonlinear dynamical systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Structured Kolmogorov-Arnold Neural ODEs (SKANODEs)ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿ(nonlinear dynamical systems)å»ºæ¨¡ä¸­å‡†ç¡®æ€§ä¸ç‰©ç†å¯è§£é‡Šæ€§å¹³è¡¡éš¾é¢˜çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç»“æ„åŒ–çŠ¶æ€ç©ºé—´å»ºæ¨¡(structured state-space modeling)ä¸Kolmogorov-Arnold Networks (KANs) ç›¸ç»“åˆï¼Œå¹¶åœ¨Neural ODEæ¶æ„ä¸­åˆ©ç”¨KANä½œä¸ºé€šç”¨å‡½æ•°é€¼è¿‘å™¨æ‰§è¡Œè™šæ‹Ÿä¼ æ„Ÿ(virtual sensing)ã€‚SKANODEèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤ä½ç§»å’Œé€Ÿåº¦ç­‰å…·æœ‰ç‰©ç†æ„ä¹‰çš„æ½œåœ¨çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨KANçš„ç¬¦å·å›å½’(symbolic regression)èƒ½åŠ›æå–æè¿°ç³»ç»ŸåŠ¨åŠ›å­¦çš„ç®€æ´ã€å¯è§£é‡Šçš„æ•°å­¦è¡¨è¾¾å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSKANODEåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç³»ç»Ÿä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„é¢„æµ‹ç²¾åº¦ï¼Œèƒ½å¤Ÿå‘ç°ç¬¦åˆç‰©ç†è§„å¾‹çš„åŠ¨åŠ›å­¦ç‰¹å¾ã€‚ç‰¹åˆ«æ˜¯åœ¨F-16æˆ˜æ–—æœºæ¡ˆä¾‹ä¸­ï¼Œè¯¥æ¨¡å‹æˆåŠŸè¯†åˆ«äº†è¿Ÿæ»è¡Œä¸º(hysteretic behavior)å¹¶æ¨å¯¼å‡ºäº†æè¿°è¯¥ç°è±¡çš„ç²¾ç®€ç¬¦å·æ–¹ç¨‹ï¼Œä¸ºå¤æ‚éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿæä¾›äº†å¯è§£é‡Šçš„æ•°æ®é©±åŠ¨å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC",
        "nlin.CD",
        "physics.data-an"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18339v2",
      "published_date": "2025-06-23 06:42:43 UTC",
      "updated_date": "2025-10-13 15:37:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:56.032181+00:00"
    },
    {
      "arxiv_id": "2506.18330v2",
      "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning",
      "title_zh": "Confucius3-Mathï¼šé¢å‘ä¸­å›½ K-12 æ•°å­¦å­¦ä¹ çš„è½»é‡çº§é«˜æ€§èƒ½æ¨ç†å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Lixin Wu",
        "Na Cai",
        "Qiao Cheng",
        "Jiachen Wang",
        "Yitao Duan"
      ],
      "abstract": "We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Confucius3-Mathï¼Œä¸€ä¸ªæ‹¥æœ‰14Bå‚æ•°çš„å¼€æºè½»é‡åŒ–é«˜æ€§èƒ½æ¨ç†å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨å•å¼ æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚è¯¥æ¨¡å‹ä¸“é—¨é’ˆå¯¹ä¸­å›½K-12æ•°å­¦æ•™è‚²åœºæ™¯ï¼Œé€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åè®­ç»ƒï¼Œå®ç°äº†ä¸å›½å®¶è¯¾ç¨‹æ ‡å‡†çš„æ·±åº¦å¯¹é½ã€‚ä¸ºäº†å…‹æœè®­ç»ƒæŒ‘æˆ˜ï¼Œå›¢é˜Ÿå¼•å…¥äº†é’ˆå¯¹æ€§ç†µæ­£åˆ™åŒ–(Targeted Entropy Regularization)ã€è¿‘æœŸæ ·æœ¬æ¢å¤(Recent Sample Recovery)ä»¥åŠç­–ç•¥ç‰¹å®šéš¾åº¦åŠ æƒ(Policy-Specific Hardness Weighting)ä¸‰é¡¹æŠ€æœ¯åˆ›æ–°ã€‚è¿™äº›æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç¨³å®šæ€§ä¸æ•°æ®æ•ˆç‡ï¼Œä½¿æ¨¡å‹åœ¨å¤šé¡¹æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°SOTAæ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä½“ç§¯æ›´å¤§çš„æ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ä»…è¯æ˜äº†ä½æˆæœ¬æ„å»ºé¢†åŸŸå¼ºæ¨ç†æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œè¿˜é€šè¿‡å¼€æºä»£ç å’Œæ¨¡å‹è¿›ä¸€æ­¥æ¨åŠ¨äº†AIåœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ä¸çŸ¥è¯†ä¼ æ’­ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18330v2",
      "published_date": "2025-06-23 06:23:53 UTC",
      "updated_date": "2025-06-25 10:49:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T23:59:58.598379+00:00"
    },
    {
      "arxiv_id": "2506.18327v1",
      "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems",
      "title_zh": "ä»¥åå·®å¯¹æŠ—åå·®â€”â€”æ­£ä¹‰é»æ˜ï¼šæ¨èç³»ç»Ÿä¸­çš„å…¬å¹³åšå¼ˆ",
      "authors": [
        "Tahsin Alamgir Kheya",
        "Mohamed Reda Bouadjenek",
        "Sunil Aryal"
      ],
      "abstract": "Recommendation systems play a crucial role in our daily lives by impacting user experience across various domains, including e-commerce, job advertisements, entertainment, etc. Given the vital role of such systems in our lives, practitioners must ensure they do not produce unfair and imbalanced recommendations. Previous work addressing bias in recommendations overlooked bias in certain item categories, potentially leaving some biases unaddressed. Additionally, most previous work on fair re-ranking focused on binary-sensitive attributes. In this paper, we address these issues by proposing a fairness-aware re-ranking approach that helps mitigate bias in different categories of items. This re-ranking approach leverages existing biases to correct disparities in recommendations across various demographic groups. We show how our approach can mitigate bias on multiple sensitive attributes, including gender, age, and occupation. We experimented on three real-world datasets to evaluate the effectiveness of our re-ranking scheme in mitigating bias in recommendations. Our results show how this approach helps mitigate social bias with little to no degradation in performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œæå‡ºäº†åä¸ºâ€œBias vs Biasâ€çš„å…¬å¹³æ„ŸçŸ¥é‡æ’åº(fairness-aware re-ranking)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€ç ”ç©¶å¿½è§†ç‰¹å®šç‰©å“ç±»åˆ«åè§ä»¥åŠè¿‡åº¦èšç„¦äºäºŒå…ƒæ•æ„Ÿå±æ€§çš„å±€é™ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ç³»ç»Ÿç°æœ‰çš„åè§æ¥çº æ­£ä¸åŒäººå£ç»Ÿè®¡ç¾¤ä½“åœ¨æ¨èç»“æœä¸­çš„å·®å¼‚ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£å¤šæ ·åŒ–ç‰©å“ç±»åˆ«ä¸­çš„ä¸å¹³è¡¡ç°è±¡ã€‚è¯¥æ–¹æ¡ˆæ”¯æŒæ€§åˆ«ã€å¹´é¾„å’ŒèŒä¸šç­‰å¤šç§æ•æ„Ÿå±æ€§çš„åå·®ä¿®å¤ï¼Œå±•ç°äº†è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é‡æ’åºç­–ç•¥èƒ½å¤Ÿåœ¨æœ‰æ•ˆå‡è½»ç¤¾ä¼šåè§(social bias)çš„åŒæ—¶ï¼Œä¿è¯æ¨èæ€§èƒ½å‡ ä¹ä¸å‘ç”Ÿé€€åŒ–ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å…¬å¹³ã€æ›´å¹³è¡¡çš„æ¨èç³»ç»Ÿæä¾›äº†ä¸€ç§å…¼é¡¾é¢„æµ‹å‡†ç¡®æ€§ä¸ç¤¾ä¼šå…¬æ­£æ€§çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18327v1",
      "published_date": "2025-06-23 06:19:02 UTC",
      "updated_date": "2025-06-23 06:19:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:06.319220+00:00"
    },
    {
      "arxiv_id": "2506.18323v1",
      "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement",
      "title_zh": "åŸºäºå¤šå°ºåº¦ç©ºé—´æ³¨æ„åŠ›çš„ä½å…‰ç…§å›¾åƒå¢å¼ºé›¶æ ·æœ¬å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Muhammad Azeem Aslam",
        "Hassan Khalid",
        "Nisar Ahmed"
      ],
      "abstract": "Low-light image enhancement remains a challenging task, particularly in the absence of paired training data. In this study, we present LucentVisionNet, a novel zero-shot learning framework that addresses the limitations of traditional and deep learning-based enhancement methods. The proposed approach integrates multi-scale spatial attention with a deep curve estimation network, enabling fine-grained enhancement while preserving semantic and perceptual fidelity. To further improve generalization, we adopt a recurrent enhancement strategy and optimize the model using a composite loss function comprising six tailored components, including a novel no-reference image quality loss inspired by human visual perception. Extensive experiments on both paired and unpaired benchmark datasets demonstrate that LucentVisionNet consistently outperforms state-of-the-art supervised, unsupervised, and zero-shot methods across multiple full-reference and no-reference image quality metrics. Our framework achieves high visual quality, structural consistency, and computational efficiency, making it well-suited for deployment in real-world applications such as mobile photography, surveillance, and autonomous navigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LucentVisionNetï¼Œä¸€ç§åŸºäºå¤šå°ºåº¦ç©ºé—´æ³¨æ„åŠ›(Multi-Scale Spatial Attention)çš„é›¶æ ·æœ¬å­¦ä¹ (Zero-Shot Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¼ºä¹é…å¯¹è®­ç»ƒæ•°æ®ä¸‹çš„ä½å…‰ç…§å›¾åƒå¢å¼ºéš¾é¢˜ã€‚è¯¥æ¡†æ¶å°†å¤šå°ºåº¦ç©ºé—´æ³¨æ„åŠ›ä¸æ·±åº¦æ›²çº¿ä¼°è®¡ç½‘ç»œ(Deep Curve Estimation Network)ç›¸ç»“åˆï¼Œåœ¨å®ç°ç»†ç²’åº¦å¢å¼ºçš„åŒæ—¶ä¿ç•™äº†è¯­ä¹‰å’Œæ„ŸçŸ¥ä¿çœŸåº¦ã€‚ç ”ç©¶é‡‡ç”¨äº†å¾ªç¯å¢å¼ºç­–ç•¥(Recurrent Enhancement Strategy)ï¼Œå¹¶åˆ©ç”¨åŒ…å«å…­ä¸ªå®šåˆ¶ç»„ä»¶çš„å¤åˆæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ç§å—äººç±»è§†è§‰æ„ŸçŸ¥å¯å‘çš„åˆ›æ–°æ— å‚è€ƒå›¾åƒè´¨é‡æŸå¤±ã€‚åœ¨é…å¯¹å’Œéé…å¯¹åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLucentVisionNet åœ¨å¤šé¡¹å…¨å‚è€ƒå’Œæ— å‚è€ƒå›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºç°æœ‰çš„æœ‰ç›‘ç£ã€æ— ç›‘ç£åŠé›¶æ ·æœ¬æ–¹æ³•ã€‚è¯¥æ¡†æ¶å…¼å…·é«˜è§†è§‰è´¨é‡ã€ç»“æ„ä¸€è‡´æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œéå¸¸é€‚åˆéƒ¨ç½²äºç§»åŠ¨æ‘„å½±ã€ç›‘æ§å’Œè‡ªä¸»å¯¼èˆªç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18323v1",
      "published_date": "2025-06-23 06:11:55 UTC",
      "updated_date": "2025-06-23 06:11:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:11.106302+00:00"
    },
    {
      "arxiv_id": "2506.18315v1",
      "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
      "title_zh": "åˆ©ç”¨åŸºäºå±æ€§çš„æµ‹è¯•è¡”æ¥å¤§è¯­è¨€æ¨¡å‹ä»£ç ç”Ÿæˆä¸éªŒè¯",
      "authors": [
        "Lehan He",
        "Zeren Chen",
        "Zhe Zhang",
        "Jing Shao",
        "Xiang Gao",
        "Lu Sheng"
      ],
      "abstract": "Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the \"cycle of self-deception\" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Property-Generated Solverï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ Property-Based Testing (PBT) æ¥æ¡¥æ¥å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä»£ç ç”Ÿæˆä¸éªŒè¯çš„æ–°å‹æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿ Test-Driven Development (TDD) åœ¨è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆä¸­å­˜åœ¨çš„æµ‹è¯•åå·®å’Œè¾“å‡ºé¢„æµ‹ä¸å‡†ç¡®ç­‰é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡éªŒè¯é«˜å±‚ç¨‹åºçš„å±æ€§æˆ–ä¸å˜æ€§ (invariants) æ¥æ‰“ç ´â€œè‡ªæˆ‘æ¬ºéª—å¾ªç¯â€ã€‚ç³»ç»Ÿç”±ä¸¤ä¸ªååŒçš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ç»„æˆï¼šGenerator è´Ÿè´£ä»£ç ç”Ÿæˆä¸è¿­ä»£ä¿®å¤ï¼ŒTester è´Ÿè´£ç®¡ç† PBT ç”Ÿå‘½å‘¨æœŸå¹¶æ ¹æ®å±æ€§è¿èƒŒæƒ…å†µæä¾›è¯­ä¹‰ä¸°å¯Œçš„åé¦ˆã€‚è¿™ç§åŸºäº PBT çš„é—­ç¯è¿­ä»£èŒƒå¼ä¸ºå¼•å¯¼ LLMs ç”Ÿæˆæ›´æ­£ç¡®ä¸”æ›´å…·æ³›åŒ–æ€§çš„ä»£ç æä¾›äº†ç¨³å¥æœºåˆ¶ã€‚åœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒProperty-Generated Solver ç›¸æ¯”äºç°æœ‰çš„ TDD æ–¹æ³•ï¼Œåœ¨ pass@1 æŒ‡æ ‡ä¸Šå®ç°äº† 23.1% è‡³ 37.3% çš„æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18315v1",
      "published_date": "2025-06-23 06:01:12 UTC",
      "updated_date": "2025-06-23 06:01:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:24.538129+00:00"
    },
    {
      "arxiv_id": "2506.18309v1",
      "title": "LettinGo: Explore User Profile Generation for Recommendation System",
      "title_zh": "LettinGoï¼šé¢å‘æ¨èç³»ç»Ÿçš„ç”¨æˆ·ç”»åƒç”Ÿæˆæ¢ç´¢",
      "authors": [
        "Lu Wang",
        "Di Zhang",
        "Fangkai Yang",
        "Pu Zhao",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Qingwei Lin",
        "Weiwei Deng",
        "Dongmei Zhang",
        "Feng Sun",
        "Qi Zhang"
      ],
      "abstract": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LettinGoï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå¤šæ ·åŒ–ä¸”è‡ªé€‚åº”ç”¨æˆ·ç”»åƒ(User Profile)çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå‘é‡åµŒå…¥(Embedding)ç¼ºä¹å¯è§£é‡Šæ€§ä»¥åŠç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)æ–¹æ³•å—é™äºå›ºå®šæ ¼å¼çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆLLMsçš„è¡¨è¾¾èƒ½åŠ›ä¸ä¸‹æ¸¸æ¨èä»»åŠ¡çš„ç›´æ¥åé¦ˆï¼Œé¿å…äº†ä¼ ç»Ÿç›‘ç£å¾®è°ƒ(SFT)å¸¦æ¥çš„åˆšæ€§çº¦æŸã€‚LettinGoåˆ›æ–°æ€§åœ°é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–(DPO)æŠ€æœ¯ï¼Œä½¿ç”»åƒç”Ÿæˆå™¨ä¸ç‰¹å®šä»»åŠ¡çš„è¡¨ç°å®ç°ç²¾å‡†å¯¹é½ã€‚å…¶å®æ–½è¿‡ç¨‹æ¶µç›–äº†é€šè¿‡å¤šæ¨¡å‹æ¢ç´¢ç”»åƒã€åŸºäºæ¨èå½±å“è¯„ä¼°è´¨é‡ä»¥åŠåˆ©ç”¨æˆå¯¹åå¥½æ•°æ®è¿›è¡Œå¯¹é½ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µã€‚å®éªŒç»“æœè¯æ˜ï¼ŒLettinGoåœ¨æé«˜æ¨èå‡†ç¡®æ€§ã€çµæ´»æ€§åŠä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ºä¸‹ä¸€ä»£æ¨èç³»ç»Ÿçš„ä¸ªæ€§åŒ–è¡¨å¾æä¾›äº†æå…·å¯å‘æ€§çš„åˆ›æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18309v1",
      "published_date": "2025-06-23 05:51:52 UTC",
      "updated_date": "2025-06-23 05:51:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:35.038277+00:00"
    },
    {
      "arxiv_id": "2507.01050v2",
      "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization",
      "title_zh": "æ–‡æœ¬å»æ¯’ï¼šæ•°æ®æ•ˆç‡ã€è¯­ä¹‰ä¿æŒä¸æ¨¡å‹æ³›åŒ–",
      "authors": [
        "Jing Yu",
        "Yibo Zhao",
        "Jiapeng Zhu",
        "Wenming Shao",
        "Bo Pang",
        "Zhao Zhang",
        "Xiang Li"
      ],
      "abstract": "The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: https://github.com/allacnobug/Detoxification-of-Text.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­æœ‰æ¯’å†…å®¹çš„å¹¿æ³›ä¼ æ’­ï¼Œæ¢è®¨äº†æ–‡æœ¬å»æ¯’(Text Detoxification)åœ¨å»é™¤æ¯’æ€§çš„åŒæ—¶ä¿ç•™åŸå§‹è¯­ä¹‰ã€æå‡æ¨¡å‹æ³›åŒ–æ€§åŠæ•°æ®æ•ˆç‡çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œé¦–å…ˆåœ¨å°‘é‡ç»è¿‡è¿‡æ»¤çš„é«˜è´¨é‡å¹³è¡Œè¯­æ–™ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)ï¼Œä»¥å»ºç«‹ç¨³å¥çš„åˆå§‹åŒ–æ¨¡å‹ã€‚æ¥ç€ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ— æ ‡ç­¾çš„æœ‰æ¯’è¾“å…¥å’Œè‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†å…ˆå‰å·¥ä½œä¸­åœ¨å»æ¯’æ•ˆæœã€è¯­ä¹‰ä¿ç•™ä¸åˆ†å¸ƒå¤–(Out-of-Distribution)æ•°æ®ç¨³å¥æ€§ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½å¯¹äººå·¥æ ‡æ³¨æ•°æ®ä¾èµ–çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›(State-of-the-Art)çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01050v2",
      "published_date": "2025-06-23 05:48:10 UTC",
      "updated_date": "2025-07-07 07:48:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:30.479314+00:00"
    },
    {
      "arxiv_id": "2506.18306v1",
      "title": "Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi",
      "title_zh": "Spiffyï¼šé’ˆå¯¹ Raspberry Pi çš„ CoLaNET é«˜æ•ˆå®ç°",
      "authors": [
        "Andrey Derzhavin",
        "Denis Larionov"
      ],
      "abstract": "This paper presents a lightweight software-based approach for running spiking neural networks (SNNs) without relying on specialized neuromorphic hardware or frameworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust and optimize it for common computing platforms. As a case study, we demonstrate our implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset. Spiffy achieves 92% accuracy with low latency - just 0.9 ms per training step and 0.45 ms per inference step. The code is open-source.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Spiffyï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºé€šç”¨è®¡ç®—å¹³å°è®¾è®¡çš„è½»é‡çº§è„‰å†²ç¥ç»ç½‘ç»œ (Spiking Neural Networks, SNNs) è½¯ä»¶å®ç°æ–¹æ¡ˆï¼Œæ— éœ€ä¾èµ–ä¸“é—¨çš„ç±»è„‘ç¡¬ä»¶ (neuromorphic hardware)ã€‚Spiffy ä½¿ç”¨ Rust è¯­è¨€å®ç°äº†ç‰¹å®šçš„ CoLaNET æ¶æ„ï¼Œå¹¶é’ˆå¯¹ Raspberry Pi ç­‰åµŒå…¥å¼è®¾å¤‡è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚é€šè¿‡åœ¨ Raspberry Pi ä¸Šè¿è¡Œ MNIST æ•°æ®é›†çš„å®éªŒï¼Œè¯¥å®ç°æ–¹æ¡ˆè¯æ˜äº†å…¶å“è¶Šçš„è¿è¡Œæ•ˆç‡ï¼Œä»…éœ€ 0.45 ms å³å¯å®Œæˆä¸€æ¬¡æ¨ç†æ­¥éª¤ (inference step)ï¼Œè®­ç»ƒæ­¥éª¤ (training step) çš„å»¶è¿Ÿä¹Ÿä»…ä¸º 0.9 msã€‚åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼ŒSpiffy è¾¾åˆ°äº† 92% çš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ•ˆçš„ SNNs æä¾›äº†å¯è¡Œè·¯å¾„ã€‚ç›®å‰è¯¥é¡¹ç›®çš„æºä»£ç å·²å‘ç¤¾åŒºå¼€æºã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "7 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18306v1",
      "published_date": "2025-06-23 05:47:14 UTC",
      "updated_date": "2025-06-23 05:47:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:32.152113+00:00"
    },
    {
      "arxiv_id": "2506.18304v1",
      "title": "Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies",
      "title_zh": "ç£¨ç ºé”‹èŠ’ï¼šé’ˆå¯¹åŸºäº DRL çš„è‡ªåŠ¨é©¾é©¶ç­–ç•¥çš„è‡ªé€‚åº”ä¸“å®¶å¼•å¯¼å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Junchao Fan",
        "Xuyang Lei",
        "Xiaolin Chang"
      ],
      "abstract": "Deep reinforcement learning (DRL) has emerged as a promising paradigm for autonomous driving. However, despite their advanced capabilities, DRL-based policies remain highly vulnerable to adversarial attacks, posing serious safety risks in real-world deployments. Investigating such attacks is crucial for revealing policy vulnerabilities and guiding the development of more robust autonomous systems. While prior attack methods have made notable progress, they still face several challenges: 1) they often rely on high-frequency attacks, yet critical attack opportunities are typically context-dependent and temporally sparse, resulting in inefficient attack patterns; 2) restricting attack frequency can improve efficiency but often results in unstable training due to the adversary's limited exploration. To address these challenges, we propose an adaptive expert-guided adversarial attack method that enhances both the stability and efficiency of attack policy training. Our method first derives an expert policy from successful attack demonstrations using imitation learning, strengthened by an ensemble Mixture-of-Experts architecture for robust generalization across scenarios. This expert policy then guides a DRL-based adversary through a KL-divergence regularization term. Due to the diversity of scenarios, expert policies may be imperfect. To address this, we further introduce a performance-aware annealing strategy that gradually reduces reliance on the expert as the adversary improves. Extensive experiments demonstrate that our method achieves outperforms existing approaches in terms of collision rate, attack efficiency, and training stability, especially in cases where the expert policy is sub-optimal.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºDeep Reinforcement Learning (DRL)çš„è‡ªåŠ¨é©¾é©¶ç­–ç•¥é¢ä¸´çš„å¯¹æŠ—æ”»å‡»(Adversarial Attack)é£é™©ï¼Œæå‡ºäº†ä¸€ç§Adaptive Expert-Guided Adversarial Attackæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ”»å‡»æ–¹æ³•ä¸­æ•ˆç‡ä½ä¸‹å’Œè®­ç»ƒä¸ç¨³å®šçš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡Imitation Learningä»æˆåŠŸçš„æ”»å‡»æ¼”ç¤ºä¸­å­¦ä¹ ä¸“å®¶ç­–ç•¥ï¼Œå¹¶ç»“åˆMixture-of-Experts (MoE)æ¶æ„ä»¥å¢å¼ºåœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚éšåï¼Œè¯¥ä¸“å®¶ç­–ç•¥åˆ©ç”¨KL-divergence regularizationé¡¹å¼•å¯¼DRLå¯¹æŠ—ç­–ç•¥çš„è®­ç»ƒï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚ä¸ºåº”å¯¹ä¸“å®¶ç­–ç•¥å¯èƒ½å­˜åœ¨çš„ä¸å®Œç¾ï¼Œç ”ç©¶å¼•å…¥äº†Performance-aware annealing strategyï¼Œä½¿å¯¹æŠ—æ¨¡å‹åœ¨èƒ½åŠ›æå‡åé€æ¸å‡å°‘å¯¹ä¸“å®¶çš„ä¾èµ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Collision rateã€æ”»å‡»æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå°¤å…¶æ˜¯åœ¨ä¸“å®¶ç­–ç•¥ä¸ºæ¬¡ä¼˜æ—¶ä¾ç„¶è¡¨ç°ç¨³å¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.18304v1",
      "published_date": "2025-06-23 05:42:49 UTC",
      "updated_date": "2025-06-23 05:42:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:42.254537+00:00"
    },
    {
      "arxiv_id": "2506.18295v1",
      "title": "GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing",
      "title_zh": "GeNeRTï¼šåŸºäºå¯æ³›åŒ–ç¥ç»å°„çº¿è¿½è¸ªçš„ç‰©ç†ä¿¡æ¯æ™ºèƒ½æ— çº¿ä¿¡é“å»ºæ¨¡æ–¹æ³•",
      "authors": [
        "Kejia Bian",
        "Meixia Tao",
        "Shu Sun",
        "Jun Yu"
      ],
      "abstract": "Neural ray tracing (RT) has emerged as a promising paradigm for channel modeling by combining physical propagation principles with neural networks. It enables high modeling accuracy and efficiency. However, current neural RT methods face two key limitations: constrained generalization capability due to strong spatial dependence, and weak adherence to electromagnetic laws. In this paper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced generalization, accuracy and efficiency. GeNeRT supports both intra-scenario spatial transferability and inter-scenario zero-shot generalization. By incorporating Fresnel-inspired neural network design, it also achieves higher accuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized acceleration strategy is introduced to improve runtime efficiency. Extensive experiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes well across untrained regions within a scenario and entirely unseen environments, and achieves superior accuracy in MPC prediction compared to baselines. Moreover, it outperforms Wireless Insite in runtime efficiency, particularly in multi-transmitter settings. Ablation experiments validate the effectiveness of the network architecture and training strategy in capturing physical principles of ray-surface interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GeNeRTï¼Œä¸€ç§åŸºäºå¯æ³›åŒ–ç¥ç»å¸¸è§å°„çº¿è¿½è¸ª (Generalizable Neural Ray Tracing) çš„ç‰©ç†é©±åŠ¨æ— çº¿ä¿¡é“å»ºæ¨¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰ç¥ç»å°„çº¿è¿½è¸ªæ–¹æ³•å› ç©ºé—´ä¾èµ–æ€§å¼ºå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä»¥åŠå¯¹ç”µç£å®šå¾‹ (electromagnetic laws) éµå¾ªè¾ƒå¼±çš„é—®é¢˜ã€‚GeNeRT æ”¯æŒåœºæ™¯å†…ç©ºé—´è¿ç§»å’Œè·¨åœºæ™¯çš„é›¶æ ·æœ¬æ³›åŒ– (zero-shot generalization)ï¼Œå¹¶é€šè¿‡ç»“åˆå— Fresnel å¯å‘çš„ç¥ç»ç½‘ç»œè®¾è®¡ï¼Œæ˜¾è‘—æå‡äº†å¤šå¾„åˆ†é‡ (MPC) é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† GPU å¼ é‡åŒ–åŠ é€Ÿç­–ç•¥ï¼Œä½¿å…¶åœ¨è¿è¡Œæ•ˆç‡ä¸Šè¶…è¶Šäº† Wireless Insite å•†ä¸šè½¯ä»¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šå‘å°„æœºè®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨æœªè§è¿‡çš„åŒºåŸŸå’Œç¯å¢ƒä¸­å‡å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸”æ¶ˆèå®éªŒéªŒè¯äº†å…¶æ¶æ„åœ¨æ•æ‰å°„çº¿ä¸è¡¨é¢ç›¸äº’ä½œç”¨ (ray-surface interactions) ç‰©ç†è§„å¾‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18295v1",
      "published_date": "2025-06-23 05:17:01 UTC",
      "updated_date": "2025-06-23 05:17:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:44.884014+00:00"
    },
    {
      "arxiv_id": "2506.18291v1",
      "title": "Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction",
      "title_zh": "åŸºäºä¸ªä½“é‡è¦æ€§é€‰æ‹©æ€§ç¤¾äº¤äº¤äº’çš„å¿«é€Ÿè¡Œäººè½¨è¿¹é¢„æµ‹",
      "authors": [
        "Yota Urano",
        "Hiromu Taketsugu",
        "Norimichi Ukita"
      ],
      "abstract": "This paper presents an architecture for selecting important neighboring people to predict the primary person's trajectory. To achieve effective neighboring people selection, we propose a people selection module called the Importance Estimator which outputs the importance of each neighboring person for predicting the primary person's future trajectory. To prevent gradients from being blocked by non-differentiable operations when sampling surrounding people based on their importance, we employ the Gumbel Softmax for training. Experiments conducted on the JRDB dataset show that our method speeds up the process with competitive prediction accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡é€‰æ‹©é‡è¦é‚»è¿‘ä¸ªä½“æ¥å®ç°å¿«é€Ÿäººç±»è½¨è¿¹é¢„æµ‹(Human Trajectory Prediction)çš„æ–°å‹æ¶æ„ã€‚ä¸ºäº†æœ‰æ•ˆé€‰æ‹©é‚»è¿‘äººå‘˜ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåä¸ºé‡è¦æ€§ä¼°è®¡å™¨(Importance Estimator)çš„é€‰æ‹©æ¨¡å—ï¼Œç”¨ä»¥è¾“å‡ºæ¯ä¸ªé‚»è¿‘ä¸ªä½“å¯¹äºé¢„æµ‹ç›®æ ‡ä¸»ä½“æœªæ¥è½¨è¿¹çš„é‡è¦æ€§å¾—åˆ†ã€‚ä¸ºäº†å…‹æœåœ¨æ ¹æ®é‡è¦æ€§è¿›è¡Œäººå‘˜é‡‡æ ·æ—¶å› éå¾®åˆ†æ“ä½œå¯¼è‡´çš„æ¢¯åº¦é˜»æ–­é—®é¢˜ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒé˜¶æ®µå¼•å…¥äº†Gumbel SoftmaxæŠ€æœ¯ä»¥ç¡®ä¿æ¨¡å‹çš„å¯è®­ç»ƒæ€§ã€‚åœ¨JRDBæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæå…·ç«äº‰åŠ›çš„é¢„æµ‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å¤„ç†é€Ÿåº¦ï¼Œä¸ºå®æ—¶ç¯å¢ƒä¸‹çš„ç¤¾äº¤äº¤äº’å»ºæ¨¡æä¾›äº†é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MIRU 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18291v1",
      "published_date": "2025-06-23 05:01:24 UTC",
      "updated_date": "2025-06-23 05:01:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:43.128740+00:00"
    },
    {
      "arxiv_id": "2506.18289v1",
      "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations",
      "title_zh": "é©±åŠ¨ AI ç»¿è‰²åŒ–ï¼šæ¢ç´¢æ­£äº¤ä¼˜åŒ–ä¸‹çš„èƒ½æ•ˆçº§è”æ•ˆåº”",
      "authors": [
        "Saurabhsingh Rajput",
        "Mootez Saad",
        "Tushar Sharma"
      ],
      "abstract": "AI's exponential growth intensifies computational demands and energy challenges. While practitioners employ various optimization techniques, that we refer as \"knobs\" in this paper, to tune model efficiency, these are typically afterthoughts and reactive ad-hoc changes applied in isolation without understanding their combinatorial effects on energy efficiency. This paper emphasizes on treating energy efficiency as the first-class citizen and as a fundamental design consideration for a compute-intensive pipeline. We show that strategic selection across five AI pipeline phases (data, model, training, system, inference) creates cascading efficiency. Experimental validation shows orthogonal combinations reduce energy consumption by up to $94.6$% while preserving $95.95$% of the original F1 score of non-optimized pipelines. This curated approach provides actionable frameworks for informed sustainable AI that balance efficiency, performance, and environmental responsibility.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼ºè°ƒå°†èƒ½æºæ•ˆç‡(Energy Efficiency)ä½œä¸ºAIè®¡ç®—æµæ°´çº¿çš„åŸºç¡€è®¾è®¡è€ƒé‡ï¼Œä»¥åº”å¯¹AIæŒ‡æ•°çº§å¢é•¿å¸¦æ¥çš„èƒ½è€—æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨æ•°æ®(Data)ã€æ¨¡å‹(Model)ã€è®­ç»ƒ(Training)ã€ç³»ç»Ÿ(System)åŠæ¨ç†(Inference)äº”ä¸ªé˜¶æ®µè¿›è¡Œæˆ˜ç•¥æ€§é€‰æ‹©ï¼Œç ”ç©¶å±•ç¤ºäº†å±‚å æ•ˆåº”(Cascading Efficiency)å¸¦æ¥çš„æ•ˆç‡æå‡ã€‚ä½œè€…æå‡ºå¹¶éªŒè¯äº†æ­£äº¤ä¼˜åŒ–(Orthogonal Optimizations)çš„ç»„åˆç­–ç•¥ï¼Œçªç ´äº†ä¼ ç»Ÿå­¤ç«‹ã€è¢«åŠ¨è°ƒæ•´ä¼˜åŒ–å‚æ•°çš„å±€é™æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™åŸå§‹éä¼˜åŒ–æµæ°´çº¿95.95%çš„F1 scoreçš„å‰æä¸‹ï¼Œæœ€é«˜å¯é™ä½94.6%çš„èƒ½æºæ¶ˆè€—ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¹³è¡¡æ€§èƒ½ä¸ç¯å¢ƒè´£ä»»çš„å¯æŒç»­äººå·¥æ™ºèƒ½(Sustainable AI)æä¾›äº†å¯æ“ä½œçš„å†³ç­–æ¡†æ¶ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "In review",
      "pdf_url": "https://arxiv.org/pdf/2506.18289v1",
      "published_date": "2025-06-23 04:52:08 UTC",
      "updated_date": "2025-06-23 04:52:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:54.191044+00:00"
    },
    {
      "arxiv_id": "2506.18285v1",
      "title": "Learning Causal Graphs at Scale: A Foundation Model Approach",
      "title_zh": "å¤§è§„æ¨¡å› æœå›¾å­¦ä¹ ï¼šä¸€ç§åŸºç¡€æ¨¡å‹æ–¹æ³•",
      "authors": [
        "Naiyu Yin",
        "Tian Gao",
        "Yue Yu"
      ],
      "abstract": "Due to its human-interpretability and invariance properties, Directed Acyclic Graph (DAG) has been a foundational tool across various areas of AI research, leading to significant advancements. However, DAG learning remains highly challenging, due to its super-exponential growth in computational cost and identifiability issues, particularly in small-sample regimes. To address these two challenges, in this work we leverage the recent success of linear transformers and develop a foundation model approach for discovering multiple order-consistent DAGs across tasks. In particular, we propose Attention-DAG (ADAG), a novel attention-mechanism-based architecture for learning multiple linear Structural Equation Models (SEMs). ADAG learns the mapping from observed data to both graph structure and parameters via a nonlinear attention-based kernel, enabling efficient multi-task estimation of the underlying linear SEMs. By formulating the learning process across multiple tasks as a continuous optimization problem, the pre-trained ADAG model captures the common structural properties as a shared low-dimensional prior, thereby reducing the ill-posedness of downstream DAG learning tasks in small-sample regimes. We evaluate our proposed approach on benchmark synthetic datasets and find that ADAG achieves substantial improvements in both DAG learning accuracy and zero-shot inference efficiency. To the best of our knowledge, this is the first practical approach for pre-training a foundation model specifically designed for DAG learning, representing a step toward more efficient and generalizable down-stream applications in causal discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Directed Acyclic Graph (DAG) å­¦ä¹ ä¸­å­˜åœ¨çš„è®¡ç®—æˆæœ¬æŒ‡æ•°çº§å¢é•¿åŠå°æ ·æœ¬åœºæ™¯ä¸‹çš„å¯è¾¨è¯†æ€§ (identifiability) éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹ (foundation model) çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…å¼€å‘äº†åä¸º Attention-DAG (ADAG) çš„æ³¨æ„åŠ›æœºåˆ¶æ¶æ„ï¼Œåˆ©ç”¨ linear transformers å­¦ä¹ ä»è§‚æµ‹æ•°æ®åˆ°å›¾ç»“æ„åŠå‚æ•°çš„æ˜ å°„ï¼Œå®ç°å¯¹å¤šä¸ªçº¿æ€§ Structural Equation Models (SEMs) çš„é«˜æ•ˆä¼°è®¡ã€‚é€šè¿‡å°†è·¨ä»»åŠ¡å­¦ä¹ å»ºæ¨¡ä¸ºè¿ç»­ä¼˜åŒ–é—®é¢˜ï¼Œé¢„è®­ç»ƒåçš„ ADAG èƒ½æ•æ‰å…±äº«çš„ä½ç»´ç»“æ„å…ˆéªŒ (prior)ï¼Œæ˜¾è‘—é™ä½äº†ä¸‹æ¸¸ä»»åŠ¡åœ¨å°æ ·æœ¬æ¡ä»¶ä¸‹çš„ç—…æ€æ€§ (ill-posedness)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒADAG åœ¨åˆæˆåŸºå‡†æ•°æ®é›†ä¸Šçš„ DAG å­¦ä¹ å‡†ç¡®ç‡å’Œé›¶æ ·æœ¬æ¨ç† (zero-shot inference) æ•ˆç‡å‡æœ‰å¤§å¹…æå‡ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹ DAG å­¦ä¹ è®¾è®¡çš„å®ç”¨åŒ–é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œè¯¥å·¥ä½œä¸ºå®ç°æ›´é«˜æ•ˆä¸”é€šç”¨çš„å› æœå‘ç° (causal discovery) åº”ç”¨è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18285v1",
      "published_date": "2025-06-23 04:41:02 UTC",
      "updated_date": "2025-06-23 04:41:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:49.727870+00:00"
    },
    {
      "arxiv_id": "2506.18284v1",
      "title": "Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset",
      "title_zh": "å†…çª¥é•œå›¾åƒåˆ†ç±»ä¸­çš„å¼€é›†è¯†åˆ«ï¼šåŸºäº Kvasir æ•°æ®é›†çš„æ·±åº¦å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Kasra Moazzami",
        "Seoyoun Son",
        "John Lin",
        "Sun Min Lee",
        "Daniel Son",
        "Hayeon Lee",
        "Jeongho Lee",
        "Seongji Lee"
      ],
      "abstract": "Endoscopic image classification plays a pivotal role in medical diagnostics by identifying anatomical landmarks and pathological findings. However, conventional closed-set classification frameworks are inherently limited in open-world clinical settings, where previously unseen conditions can arise andcompromise model reliability. To address this, we explore the application of Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly available and diverse endoscopic image collection. In this study, we evaluate and compare the OSR capabilities of several representative deep learning architectures, including ResNet-50, Swin Transformer, and a hybrid ResNet-Transformer model, under both closed-set and open-set conditions. OpenMax is adopted as a baseline OSR method to assess the ability of these models to distinguish known classes from previously unseen categories. This work represents one of the first efforts to apply open set recognition to the Kvasir dataset and provides a foundational benchmark for evaluating OSR performance in medical image analysis. Our results offer practical insights into model behavior in clinically realistic settings and highlight the importance of OSR techniques for the safe deployment of AI systems in endoscopy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†…çª¥é•œå›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå¼€æ”¾å¼ç¯å¢ƒä¸­é¢ä¸´çš„å¯é æ€§é—®é¢˜ï¼Œæ¢ç´¢äº†åœ¨ Kvasir æ•°æ®é›†ä¸Šåº”ç”¨å¼€æ”¾é›†è¯†åˆ« (Open Set Recognition, OSR) æŠ€æœ¯çš„æ–¹æ³•ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿé—­é›†åˆ†ç±»æ¡†æ¶æ— æ³•è¯†åˆ«æœªçŸ¥ç—…å˜æˆ–è§£å‰–ç‰¹å¾çš„å±€é™æ€§ï¼Œä½œè€…è¯„ä¼°å¹¶æ¯”è¾ƒäº† ResNet-50ã€Swin Transformer ä»¥åŠæ··åˆ ResNet-Transformer æ¨¡å‹åœ¨é—­é›†ä¸å¼€é›†æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶é‡‡ç”¨ OpenMax ä½œä¸ºåŸºå‡† OSR æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹åŒºåˆ†å·²çŸ¥ç±»åˆ«ä¸æœªè§ç±»åˆ«çš„èƒ½åŠ›ã€‚ä½œä¸ºå°† OSR åº”ç”¨äº Kvasir æ•°æ®é›†çš„å…ˆè¡Œç ”ç©¶ï¼Œè¯¥å·¥ä½œä¸ºåŒ»ç–—å›¾åƒåˆ†æä¸­çš„å¼€æ”¾é›†æ€§èƒ½è¯„ä¼°å»ºç«‹äº†åŸºç¡€åŸºå‡†ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸åŒæ¶æ„åœ¨çœŸå®ä¸´åºŠåœºæ™¯ä¸‹çš„è¡Œä¸ºç‰¹å¾ï¼Œå¹¶å¼ºè°ƒäº† OSR æŠ€æœ¯å¯¹äºç¡®ä¿å†…çª¥é•œ AI ç³»ç»Ÿå®‰å…¨éƒ¨ç½²çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.18284v1",
      "published_date": "2025-06-23 04:39:07 UTC",
      "updated_date": "2025-06-23 04:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:00:56.101183+00:00"
    },
    {
      "arxiv_id": "2506.18267v1",
      "title": "ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs",
      "title_zh": "ARD-LoRAï¼šé¢å‘å¼‚æ„é€‚é…éœ€æ±‚çš„åŸºç¡€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒåŠ¨æ€ç§©åˆ†é…",
      "authors": [
        "Haseeb Ullah Khan Shinwari",
        "Muhammad Usama"
      ],
      "abstract": "Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing uniform adaptation across transformer layers and attention heads despite their heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic LoRA (ARD-LoRA), a novel framework that automates rank allocation through learnable scaling factors. These factors are optimized via a meta-objective balancing task performance and parameter efficiency, incorporating $\\ell_1$ sparsity for minimal rank and Total Variation regularization for stable rank transitions. ARD-LoRA enables continuous, differentiable, per-head rank adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32% trainable parameters, outperforming strong baselines like DoRA and AdaLoRA. Furthermore, it reduces multimodal adaptation memory by 41%. These results establish dynamic, fine-grained rank allocation as a critical paradigm for efficient foundation model adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸLoRAé‡‡ç”¨å›ºå®šç§©(fixed rank)è€Œå¿½ç•¥Transformerå„å±‚åŠæ³¨æ„åŠ›å¤´å¼‚æ„å­¦ä¹ åŠ¨æ€çš„é—®é¢˜ï¼Œæå‡ºäº†ARD-LoRAæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯å­¦ä¹ çš„ç¼©æ”¾å› å­(scaling factors)å®ç°è‡ªåŠ¨ç§©åˆ†é…ï¼Œå¹¶åˆ©ç”¨å¹³è¡¡ä»»åŠ¡æ€§èƒ½ä¸å‚æ•°æ•ˆç‡çš„å…ƒç›®æ ‡(meta-objective)è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºäº†ç¡®ä¿ç§©çš„æœ€å°åŒ–å’Œç¨³å®šæ€§ï¼ŒARD-LoRAå¼•å…¥äº†$\\ell_1$ç¨€ç–æ€§çº¦æŸä¸å…¨å˜åˆ†(Total Variation)æ­£åˆ™åŒ–ï¼Œå®ç°äº†è¿ç»­ã€å¯å¾®ä¸”ç»†åŒ–è‡³æ¯ä¸ªæ³¨æ„åŠ›å¤´(per-head)çš„ç§©è‡ªé€‚åº”ã€‚åœ¨LLAMA-3.1-70Bå’ŒPaliGemma-2ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒARD-LoRAä»…åˆ©ç”¨0.32%çš„å¯è®­ç»ƒå‚æ•°ä¾¿è¾¾åˆ°äº†å…¨é‡å¾®è°ƒ(full fine-tuning)æ€§èƒ½çš„99.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å°†å†…å­˜æ¶ˆè€—é™ä½äº†41%ï¼Œå…¶æ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äºDoRAå’ŒAdaLoRAç­‰å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœç¡®ç«‹äº†åŠ¨æ€ç»†ç²’åº¦ç§©åˆ†é…åœ¨åŸºç¡€æ¨¡å‹é«˜æ•ˆé€‚é…ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18267v1",
      "published_date": "2025-06-23 03:45:37 UTC",
      "updated_date": "2025-06-23 03:45:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:08.736543+00:00"
    },
    {
      "arxiv_id": "2506.18260v1",
      "title": "Advanced For-Loop for QML algorithm search",
      "title_zh": "é¢å‘ QML ç®—æ³•æœç´¢çš„é«˜çº§ For å¾ªç¯",
      "authors": [
        "FuTe Wong"
      ],
      "abstract": "This paper introduces an advanced framework leveraging Large Language Model-based Multi-Agent Systems (LLMMA) for the automated search and optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google DeepMind's FunSearch, the proposed system works on abstract level to iteratively generates and refines quantum transformations of classical machine learning algorithms (concepts), such as the Multi-Layer Perceptron, forward-forward and backpropagation algorithms. As a proof of concept, this work highlights the potential of agentic frameworks to systematically explore classical machine learning concepts and adapt them for quantum computing, paving the way for efficient and automated development of QML algorithms. Future directions include incorporating planning mechanisms and optimizing strategy in the search space for broader applications in quantum-enhanced machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ç§åˆ©ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (LLMMA) è¿›è¡Œé‡å­æœºå™¨å­¦ä¹  (QML) ç®—æ³•è‡ªåŠ¨æœç´¢ä¸ä¼˜åŒ–çš„å…ˆè¿›æ¡†æ¶ã€‚å— Google DeepMind çš„ FunSearch å¯å‘ï¼Œè¯¥ç³»ç»Ÿåœ¨æŠ½è±¡å±‚é¢è¿è¡Œï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå¹¶æ”¹è¿›ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ Multi-Layer Perceptron, forward-forward å’Œ backpropagationï¼‰çš„é‡å­å˜æ¢ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œè¯¥å·¥ä½œçªå‡ºäº†æ™ºèƒ½ä½“æ¡†æ¶ (Agentic Frameworks) åœ¨ç³»ç»Ÿæ€§æ¢ç´¢ç»å…¸æœºå™¨å­¦ä¹ æ¦‚å¿µå¹¶å°†å…¶é€‚é…è‡³é‡å­è®¡ç®—é¢†åŸŸçš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºé«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„ QML ç®—æ³•å¼€å‘å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å±•ç¤ºäº†åœ¨é‡å­å¢å¼ºæœºå™¨å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨çš„å‰æ™¯ã€‚æœªæ¥çš„æ”¹è¿›æ–¹å‘å°†åŒ…æ‹¬å¼•å…¥è§„åˆ’æœºåˆ¶ä»¥åŠåœ¨æœç´¢ç©ºé—´ä¸­ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.18260v1",
      "published_date": "2025-06-23 03:19:36 UTC",
      "updated_date": "2025-06-23 03:19:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:17.749830+00:00"
    },
    {
      "arxiv_id": "2506.18254v1",
      "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
      "title_zh": "RLPRï¼šæ— éœ€éªŒè¯å™¨å°† RLVR æ¨å¹¿è‡³é€šç”¨é¢†åŸŸ",
      "authors": [
        "Tianyu Yu",
        "Bo Ji",
        "Shouli Wang",
        "Shu Yao",
        "Zefan Wang",
        "Ganqu Cui",
        "Lifan Yuan",
        "Ning Ding",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Tat-Seng Chua"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RLPRï¼Œä¸€ç§æ— éœ€éªŒè¯å™¨(verifier-free)çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†Reinforcement Learning with Verifiable Rewards (RLVR) çš„æ¨ç†èƒ½åŠ›ä»æ•°å­¦å’Œä»£ç é¢†åŸŸæ‰©å±•åˆ°æ›´å¹¿æ³›çš„é€šç”¨é¢†åŸŸã€‚é’ˆå¯¹RLVRè¿‡åº¦ä¾èµ–ç‰¹å®šé¢†åŸŸéªŒè¯å™¨å¯¼è‡´çš„æ‰©å±•æ€§éš¾é¢˜ï¼ŒRLPRåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆæ­£ç¡®å‚è€ƒç­”æ¡ˆçš„å†…åœ¨æ¦‚ç‡(intrinsic probability)ç›´æ¥ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚ä¸ºäº†å…‹æœè¿™ç§å™ªå£°æ¦‚ç‡å¥–åŠ±å¸¦æ¥çš„é«˜æ–¹å·®æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†prob-to-rewardè½¬æ¢åŠä¸€ç³»åˆ—ç¨³å®šåŒ–æ–¹æ³•ï¼Œç¡®ä¿äº†å¥–åŠ±ä¿¡å·çš„ç²¾ç¡®ä¸ç¨³å®šã€‚åœ¨æ¶µç›–é€šç”¨é¢†åŸŸå’Œæ•°å­¦é¢†åŸŸçš„ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLPRä¸€è‡´æå‡äº†åŸºäºGemmaã€Llamaå’ŒQwenç³»åˆ—æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLPRåœ¨TheoremQAå’ŒMinervaä¸Šçš„è¡¨ç°åˆ†åˆ«ä¼˜äºåŒæœŸçš„VeriFree 7.6å’Œ7.5åˆ†ï¼Œç”šè‡³åœ¨å¹³å‡å¾—åˆ†ä¸Šè¶…è¶Šäº†ä¾èµ–å¼ºéªŒè¯å™¨æ¨¡å‹çš„General-Reasonerï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹é€»è¾‘æ¨ç†æ–¹é¢çš„æ˜¾è‘—ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Project Website: https://github.com/openbmb/RLPR",
      "pdf_url": "https://arxiv.org/pdf/2506.18254v1",
      "published_date": "2025-06-23 02:56:36 UTC",
      "updated_date": "2025-06-23 02:56:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:27.193043+00:00"
    },
    {
      "arxiv_id": "2506.18251v2",
      "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models",
      "title_zh": "Morseï¼šç”¨äºæ‰©æ•£æ¨¡å‹æ— æŸåŠ é€Ÿçš„åŒé‡‡æ ·",
      "authors": [
        "Chao Li",
        "Jiawei Fan",
        "Anbang Yao"
      ],
      "abstract": "In this paper, we present Morse, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78X to 3.31X on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at https://github.com/deep-optimization/Morse.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Morseï¼Œä¸€ä¸ªæ—¨åœ¨å®ç° Diffusion Models æ— æŸåŠ é€Ÿçš„ç®€å•åŒé‡‡æ ·(dual-sampling)æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡è·³è·ƒé‡‡æ ·(jump sampling)å’Œè‡ªé€‚åº”æ®‹å·®åé¦ˆ(adaptive residual feedback)ç­–ç•¥æ¥é‡æ–°æ„å»ºä»å™ªå£°åˆ°æ•°æ®çš„è¿­ä»£ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶ç”± Dash å’Œ Dot ä¸¤ä¸ªæ¨¡å‹ç»„æˆï¼ŒDash ä½œä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è´Ÿè´£è·³è·ƒé‡‡æ ·ï¼Œè€Œæ˜¾è‘—æ›´å¿«çš„ Dot æ¨¡å‹åˆ™æ ¹æ®å½“å‰è§‚æµ‹å€¼ç”Ÿæˆæ®‹å·®åé¦ˆï¼Œä½¿å™ªå£°ä¼°è®¡èƒ½æ— æŸåŒ¹é… Dash æ¨¡å‹çš„ä¸‹ä¸€æ­¥ä¼°è®¡ã€‚é€šè¿‡å¼•å…¥æƒé‡å…±äº«(weight sharing)ç­–ç•¥ï¼ŒMorse åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½è¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ 6 é¡¹å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒMorse ç›¸æ¯” 9 ä¸ªåŸºçº¿æ¨¡å‹å®ç°äº† 1.78 å€è‡³ 3.31 å€çš„å¹³å‡æ— æŸåŠ é€Ÿã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œèƒ½è¿›ä¸€æ­¥ä¼˜åŒ– Latent Consistency Model (LCM-SDXL) åœ¨å°‘æ­¥æ•°æ–‡æœ¬ç”Ÿæˆå›¾åƒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Fixed a prompt typo in Figure 18 of the Appendix. This work is accepted to ICML 2025. The project page: https://github.com/deep-optimization/Morse",
      "pdf_url": "https://arxiv.org/pdf/2506.18251v2",
      "published_date": "2025-06-23 02:43:21 UTC",
      "updated_date": "2025-06-25 03:25:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:59.697655+00:00"
    },
    {
      "arxiv_id": "2506.18946v1",
      "title": "DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models",
      "title_zh": "DiffRISï¼šåˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¢å¼ºæŒ‡ä»£æ€§é¥æ„Ÿå›¾åƒåˆ†å‰²",
      "authors": [
        "Zhe Dong",
        "Yuzhe Sun",
        "Tianzhu Liu",
        "Yanfeng Gu"
      ],
      "abstract": "Referring remote sensing image segmentation (RRSIS) enables the precise delineation of regions within remote sensing imagery through natural language descriptions, serving critical applications in disaster response, urban development, and environmental monitoring. Despite recent advances, current approaches face significant challenges in processing aerial imagery due to complex object characteristics including scale variations, diverse orientations, and semantic ambiguities inherent to the overhead perspective. To address these limitations, we propose DiffRIS, a novel framework that harnesses the semantic understanding capabilities of pre-trained text-to-image diffusion models for enhanced cross-modal alignment in RRSIS tasks. Our framework introduces two key innovations: a context perception adapter (CP-adapter) that dynamically refines linguistic features through global context modeling and object-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD) that iteratively aligns textual descriptions with visual regions for precise segmentation. The CP-adapter bridges the domain gap between general vision-language understanding and remote sensing applications, while PCMRD enables fine-grained semantic alignment through multi-scale feature interaction. Comprehensive experiments on three benchmark datasets-RRSIS-D, RefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms existing methods across all standard metrics, establishing a new state-of-the-art for RRSIS tasks. The significant performance improvements validate the effectiveness of leveraging pre-trained diffusion models for remote sensing applications through our proposed adaptive framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiffRISï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹ (pre-trained text-to-image diffusion models) æ¥å¢å¼ºæŒ‡ä»£é¥æ„Ÿå›¾åƒåˆ†å‰² (Referring Remote Sensing Image Segmentation, RRSIS) ä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚é’ˆå¯¹é¥æ„Ÿå›¾åƒä¸­ç›®æ ‡å°ºåº¦å˜åŒ–å¤§ã€æ–¹å‘å¤šæ ·ä»¥åŠä¿¯è§†è§†è§’å¸¦æ¥çš„è¯­ä¹‰æ­§ä¹‰ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å®ç°æ›´ç²¾å‡†çš„è·¨æ¨¡æ€å¯¹é½ã€‚ç ”ç©¶å¼•å…¥äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‚é…å™¨ (CP-adapter)ï¼Œé€šè¿‡å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œç›®æ ‡æ„ŸçŸ¥æ¨ç†æ¥åŠ¨æ€ä¼˜åŒ–è¯­è¨€ç‰¹å¾ï¼Œç¼©å°äº†é€šç”¨è§†è§‰è¯­è¨€ç†è§£ä¸é¥æ„Ÿé¢†åŸŸä¹‹é—´çš„å·®è·ã€‚åŒæ—¶ï¼Œæ¡†æ¶è®¾è®¡äº†æ¸è¿›å¼è·¨æ¨¡æ€æ¨ç†è§£ç å™¨ (PCMRD)ï¼Œé€šè¿‡å¤šå°ºåº¦ç‰¹å¾äº¤äº’è¿­ä»£åœ°å°†æ–‡æœ¬æè¿°ä¸è§†è§‰åŒºåŸŸè¿›è¡Œç²¾ç»†åŒ–å¯¹é½ã€‚åœ¨ RRSIS-Dã€RefSegRS å’Œ RISBench ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffRIS åœ¨å„é¡¹æ ‡å‡†æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥æˆæœå……åˆ†éªŒè¯äº†é€šè¿‡è‡ªé€‚åº”æ¶æ„åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å¤„ç†å¤æ‚é¥æ„Ÿä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18946v1",
      "published_date": "2025-06-23 02:38:56 UTC",
      "updated_date": "2025-06-23 02:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:33.339695+00:00"
    },
    {
      "arxiv_id": "2506.18248v5",
      "title": "Improving Black-Box Generative Attacks via Generator Semantic Consistency",
      "title_zh": "é€šè¿‡ç”Ÿæˆå™¨è¯­ä¹‰ä¸€è‡´æ€§æå‡é»‘ç›’ç”Ÿæˆå¼æ”»å‡»",
      "authors": [
        "Jongoh Jeong",
        "Hunmin Yang",
        "Jaeseok Jeong",
        "Kuk-Jin Yoon"
      ],
      "abstract": "Transfer attacks optimize on a surrogate and deploy to a black-box target. While iterative optimization attacks in this paradigm are limited by their per-input cost limits efficiency and scalability due to multistep gradient updates for each input, generative attacks alleviate these by producing adversarial examples in a single forward pass at test time. However, current generative attacks still adhere to optimizing surrogate losses (e.g., feature divergence) and overlook the generator's internal dynamics, underexploring how the generator's internal representations shape transferable perturbations. To address this, we enforce semantic consistency by aligning the early generator's intermediate features to an EMA teacher, stabilizing object-aligned representations and improving black-box transfer without inference-time overhead. To ground the mechanism, we quantify semantic stability as the standard deviation of foreground IoU between cluster-derived activation masks and foreground masks across generator blocks, and observe reduced semantic drift under our method. For more reliable evaluation, we also introduce Accidental Correction Rate (ACR) to separate inadvertent corrections from intended misclassifications, complementing the inherent blind spots in traditional Attack Success Rate (ASR), Fooling Rate (FR), and Accuracy metrics. Across architectures, domains, and tasks, our approach can be seamlessly integrated into existing generative attacks with consistent improvements in black-box transfer, while maintaining test-time efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é»‘ç›’ç”Ÿæˆå¼æ”»å‡»(Black-Box Generative Attacks)åœ¨è¿ç§»æ”»å‡»ä¸­å› è¿‡åº¦å…³æ³¨æ›¿ä»£æŸå¤±(Surrogate Losses)è€Œå¿½ç•¥ç”Ÿæˆå™¨å†…éƒ¨åŠ¨åŠ›å­¦ï¼Œå¯¼è‡´æ‰°åŠ¨è¿ç§»æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§(Semantic Consistency)æ¥ä¼˜åŒ–ç”Ÿæˆå™¨çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç”Ÿæˆå™¨çš„ä¸­é—´ç‰¹å¾ä¸EMAæ•™å¸ˆæ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œæ—¨åœ¨ç¨³å®šç‰©ä½“å¯¹é½è¡¨å¾(Object-Aligned Representations)ï¼Œåœ¨ä¸å¢åŠ æ¨ç†å¼€é”€çš„å‰æä¸‹æå‡é»‘ç›’è¿ç§»æ•ˆæœã€‚ç ”ç©¶é€šè¿‡è®¡ç®—æ¿€æ´»æ©ç ä¸å‰æ™¯æ©ç ä¹‹é—´çš„è¯­ä¹‰ç¨³å®šæ€§(Semantic Stability)é‡åŒ–äº†è¯¥æœºåˆ¶ï¼Œå¹¶è§‚å¯Ÿåˆ°è¯­ä¹‰æ¼‚ç§»(Semantic Drift)å¾—åˆ°äº†æœ‰æ•ˆæŠ‘åˆ¶ã€‚æ­¤å¤–ï¼Œä½œè€…å¼•å…¥äº†æ„å¤–ä¿®æ­£ç‡(Accidental Correction Rate, ACR)ä½œä¸ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨ä»¥åŒºåˆ†æ— æ„è¯†ä¿®æ­£ä¸é¢„æœŸçš„è¯¯åˆ†ç±»ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿæ”»å‡»æˆåŠŸç‡(ASR)ç­‰æŒ‡æ ‡çš„ç›²ç‚¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯æ— ç¼é›†æˆè‡³ç°æœ‰ç”Ÿæˆå¼æ”»å‡»ä¸­ï¼Œåœ¨ä¿æŒæµ‹è¯•æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨æ¶æ„å’Œè·¨é¢†åŸŸçš„é»‘ç›’æ”»å‡»è¿ç§»æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.18248v5",
      "published_date": "2025-06-23 02:35:09 UTC",
      "updated_date": "2025-09-28 09:04:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:35.753698+00:00"
    },
    {
      "arxiv_id": "2506.18245v1",
      "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection",
      "title_zh": "Smart-LLaMA-DPOï¼šé¢å‘å¯è§£é‡Šæ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹çš„å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Lei Yu",
        "Zhirong Huang",
        "Hang Yuan",
        "Shiqi Cheng",
        "Li Yang",
        "Fengjun Zhang",
        "Chenjie Shen",
        "Jiajia Ma",
        "Jingyuan Zhang",
        "Junyi Lu",
        "Chun Zuo"
      ],
      "abstract": "Smart contract vulnerability detection remains a major challenge in blockchain security. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensive coverage and high-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Empirical analysis shows that even after continual pre-training (CPT) and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of state changes, resulting in incorrect explanations despite making correct detection decisions. To address these challenges, we propose Smart-LLaMA-DPO based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major vulnerability types and machine-unauditable vulnerabilities, including precise labels, explanations, and locations for SFT, as well as high-quality and low-quality output pairs for Direct Preference Optimization (DPO). Second, we perform CPT using large-scale smart contract to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct SFT with our comprehensive dataset. Finally, we apply DPO, leveraging human feedback and a specially designed loss function that increases the probability of preferred explanations while reducing the likelihood of non-preferred outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation confirm that our method generates more correct, thorough, and clear explanations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Smart-LLaMA-DPOï¼Œä¸€ä¸ªåŸºäºLLaMA-3.1-8Bçš„å¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹ä¸­æ•°æ®é›†è¦†ç›–ä¸è¶³åŠæ¨¡å‹å¯¹å®‰å…¨æ¦‚å¿µè§£é‡Šä¸å‡†ç¡®çš„é—®é¢˜ã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ¶µç›–å››ç§ä¸»è¦æ¼æ´ç±»å‹åŠæœºå™¨ä¸å¯å®¡è®¡æ¼æ´çš„ç»¼åˆæ•°æ®é›†ï¼Œå¹¶æä¾›äº†ç²¾ç¡®çš„æ ‡ç­¾ã€è§£é‡Šã€ä½ç½®ä¿¡æ¯ä»¥åŠç”¨äºDirect Preference Optimization (DPO)çš„é«˜ä½è´¨é‡è¾“å‡ºå¯¹ã€‚æ ¸å¿ƒæ–¹æ³•æ¶‰åŠä½¿ç”¨å¤§è§„æ¨¡æ™ºèƒ½åˆçº¦è¿›è¡ŒContinual Pre-training (CPT)ä»¥å¢å¼ºé¢†åŸŸç†è§£ï¼Œéšååˆ©ç”¨è¯¥ç»¼åˆæ•°æ®é›†è¿›è¡ŒSupervised Fine-tuning (SFT)ã€‚æœ€åé€šè¿‡åº”ç”¨DPOæŠ€æœ¯ç»“åˆäººç±»åé¦ˆå’Œä¸“é—¨è®¾è®¡çš„æŸå¤±å‡½æ•°ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹ç”Ÿæˆæ­£ç¡®è§£é‡Šçš„æ¦‚ç‡ï¼Œä¿®æ­£äº†æ¨¡å‹å¯¹çŠ¶æ€å˜åŒ–æ‰§è¡Œé¡ºåºçš„è¯¯è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSmart-LLaMA-DPOåœ¨reentrancyã€timestamp dependenceã€integer overflow/underflowå’Œdelegatecallç­‰æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå…¶F1åˆ†æ•°å’Œå‡†ç¡®ç‡åˆ†åˆ«å¹³å‡æå‡äº†10.43%å’Œ7.87%ã€‚æ­¤å¤–ï¼ŒLLMä¸äººå·¥è¯„ä¼°å‡è¯å®è¯¥æ–¹æ³•èƒ½ç”Ÿæˆæ›´ä¸ºå‡†ç¡®ã€è¯¦å°½ä¸”æ¸…æ™°çš„æ¼æ´è§£é‡Šï¼Œæ˜¾è‘—å¢å¼ºäº†æ£€æµ‹ç»“æœçš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to ISSTA 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18245v1",
      "published_date": "2025-06-23 02:24:07 UTC",
      "updated_date": "2025-06-23 02:24:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:48.392187+00:00"
    },
    {
      "arxiv_id": "2506.18240v4",
      "title": "Quantum-Classical Hybrid Quantized Neural Network",
      "title_zh": "é‡å­-ç»å…¸æ··åˆé‡åŒ–ç¥ç»ç½‘ç»œ",
      "authors": [
        "Wenxin Li",
        "Chuan Wang",
        "Hongdong Zhu",
        "Qi Gao",
        "Yin Ma",
        "Hai Wei",
        "Kai Wen"
      ],
      "abstract": "In this work, we introduce a novel Quadratic Binary Optimization (QBO) framework for training a quantized neural network. The framework enables the use of arbitrary activation and loss functions through spline interpolation, while Forward Interval Propagation addresses the nonlinearities and the multi-layered, composite structure of neural networks via discretizing activation functions into linear subintervals. This preserves the universal approximation properties of neural networks while allowing complex nonlinear functions accessible to quantum solvers, broadening their applicability in artificial intelligence. Theoretically, we derive an upper bound on the approximation error and the number of Ising spins required by deriving the sample complexity of the empirical risk minimization problem from an optimization perspective. A key challenge in solving the associated large-scale Quadratic Constrained Binary Optimization (QCBO) model is the presence of numerous constraints. To overcome this, we adopt the Quantum Conditional Gradient Descent (QCGD) algorithm, which solves QCBO directly on quantum hardware. We establish the convergence of QCGD under a quantum oracle subject to randomness, bounded variance, and limited coefficient precision, and further provide an upper bound on the Time-To-Solution. To enhance scalability, we further incorporate a decomposed copositive optimization scheme that replaces the monolithic lifted model with sample-wise subproblems. This decomposition substantially reduces the quantum resource requirements and enables efficient low-bit neural network training. We further propose the usage of QCGD and Quantum Progressive Hedging (QPH) algorithm to efficiently solve the decomposed problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„äºŒæ¬¡äºŒå€¼ä¼˜åŒ–(Quadratic Binary Optimization, QBO)æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒé‡åŒ–ç¥ç»ç½‘ç»œ(Quantized Neural Network)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ ·æ¡æ’å€¼(Spline Interpolation)å’Œå‰å‘åŒºé—´ä¼ æ’­(Forward Interval Propagation)æŠ€æœ¯ï¼Œåœ¨ä¿æŒç¥ç»ç½‘ç»œé€šç”¨è¿‘ä¼¼ç‰¹æ€§çš„åŒæ—¶ï¼Œè§£å†³äº†éçº¿æ€§å‡½æ•°åœ¨é‡å­æ±‚è§£å™¨ä¸­çš„é€‚é…é—®é¢˜ã€‚ç†è®ºä¸Šï¼Œç ”ç©¶è€…æ¨å¯¼äº†è¿‘ä¼¼è¯¯å·®çš„ä¸Šç•Œä»¥åŠæ‰€éœ€çš„Isingè‡ªæ—‹æ•°é‡ã€‚ä¸ºäº†åº”å¯¹å¤§è§„æ¨¡çº¦æŸä¼˜åŒ–æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨äº†é‡å­æ¡ä»¶æ¢¯åº¦ä¸‹é™(Quantum Conditional Gradient Descent, QCGD)ç®—æ³•ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨éšæœºé‡å­é¢„è¨€æœºä¸‹çš„æ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥åˆ†è§£å…±æ­£ä¼˜åŒ–(Decomposed Copositive Optimization)æ–¹æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†é‡å­èµ„æºéœ€æ±‚å¹¶æå‡äº†ä½æ¯”ç‰¹ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡ã€‚è¯¥å·¥ä½œè¿˜ç»“åˆäº†Quantum Progressive Hedging (QPH)ç®—æ³•è¿›ä¸€æ­¥ä¼˜åŒ–æ±‚è§£è¿‡ç¨‹ï¼Œä¸ºé‡å­-ç»å…¸æ··åˆæœºå™¨å­¦ä¹ æä¾›äº†é«˜æ•ˆçš„ç†è®ºä¸ç®—æ³•æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.optics"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18240v4",
      "published_date": "2025-06-23 02:12:36 UTC",
      "updated_date": "2025-12-08 06:41:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:49.688534+00:00"
    },
    {
      "arxiv_id": "2506.18237v1",
      "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model",
      "title_zh": "AdapThinkï¼šæ¨ç†è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”æ€è€ƒåå¥½",
      "authors": [
        "Xu Wan",
        "Wei Wang",
        "Wenyue Xu",
        "Wotao Yin",
        "Jie Song",
        "Mingyang Sun"
      ],
      "abstract": "Reinforcement Learning (RL)-based post-training has significantly advanced the complex reasoning capabilities of language models, fostering sophisticated self-reflection processes. However, this ``slow thinking'' paradigm presents a critical challenge to reasoning efficiency: models may expend excessive computation on simple questions and shift reasoning prematurely for complex ones. Previous mechanisms typically rely on static length budgets or predefined rules, lacking the adaptability for varying question complexities and models' evolving capabilities. To this end, we propose AdapThink, an adaptive post-training framework designed to induce more efficient thinking while maintaining the performance of reasoning language models. Specifically, AdapThink incorporates two key mechanisms: 1) A group-relative reward function that leverages model confidence and response's characteristic to dynamically adjust the preference of reflection-related transition words without resorting to a fixed length preference. 2) A diversity-aware sampling mechanism that balances the training group's solution accuracy with reasoning diversity via an entropy-guided score. Experiments on several mathematical reasoning datasets with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling adaptive reasoning patterns and mitigating the inefficiencies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åæœŸè®­ç»ƒä¸­â€œæ…¢æ€è€ƒâ€èŒƒå¼å¯¼è‡´çš„æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šè¿‡åº¦è®¡ç®—è€Œåœ¨å¤æ‚é—®é¢˜ä¸Šæ¨ç†ä¸è¶³ï¼Œæå‡ºäº†AdapThinkè‡ªé€‚åº”åæœŸè®­ç»ƒæ¡†æ¶ã€‚AdapThinkåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæœºåˆ¶ï¼šé¦–å…ˆæ˜¯åˆ©ç”¨æ¨¡å‹ç½®ä¿¡åº¦å’Œå“åº”ç‰¹å¾çš„ç¾¤ä½“ç›¸å¯¹å¥–åŠ±å‡½æ•°(Group-relative reward function)ï¼ŒåŠ¨æ€è°ƒæ•´åæ€æ€§è¿‡æ¸¡è¯çš„åå¥½ä»¥å–ä»£å›ºå®šé•¿åº¦é¢„ç®—ï¼›å…¶æ¬¡æ˜¯é‡‡ç”¨åŸºäºç†µå¯¼å‘åˆ†æ•°çš„å¤šå…ƒæ„ŸçŸ¥é‡‡æ ·(Diversity-aware sampling)ï¼Œåœ¨è®­ç»ƒä¸­å¹³è¡¡è§£ç­”å‡†ç¡®ç‡ä¸æ¨ç†å¤šæ ·æ€§ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šé’ˆå¯¹DeepSeekè’¸é¦æ¨¡å‹(DeepSeek-distilled models)è¿›è¡Œçš„å®éªŒç»“æœè¯æ˜ï¼ŒAdapThinkèƒ½å¤Ÿè¯±å¯¼æ›´é«˜æ•ˆçš„è‡ªé€‚åº”æ¨ç†æ¨¡å¼ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—ç¼“è§£äº†æ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡å†—ä½™é—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18237v1",
      "published_date": "2025-06-23 02:06:04 UTC",
      "updated_date": "2025-06-23 02:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:46.828118+00:00"
    },
    {
      "arxiv_id": "2506.18233v3",
      "title": "Beyond Parameters: Exploring Virtual Logic Depth for Scaling Laws",
      "title_zh": "è¶…è¶Šå‚æ•°ï¼šæ¢ç©¶ç¼©æ”¾å®šå¾‹çš„è™šæ‹Ÿé€»è¾‘æ·±åº¦",
      "authors": [
        "Ruike Zhu",
        "Hanwen Zhang",
        "Kevin Li",
        "Tianyu Shi",
        "Yiqun Duan",
        "Chi Wang",
        "Tianyi Zhou",
        "Arindam Banerjee",
        "Zengyi Qin"
      ],
      "abstract": "Scaling large language models typically involves three dimensions: depth, width, and parameter count. In this work, we explore a fourth dimension, \\textbf{virtual logical depth} (VLD), which increases effective algorithmic depth without changing parameter count by reusing weights. While parameter reuse is not new, its role in scaling has been underexplored. Unlike recent test-time methods that scale token-wise, VLD alters the internal computation graph during training and inference. Through controlled experiments, we obtain three key insights. (1) \\textit{Knowledge capacity vs. parameters}: at fixed parameter count, VLD leaves knowledge capacity nearly unchanged, while across models capacity still scales with parameters. (2) \\textit{Reasoning vs. reuse}: properly implemented VLD substantially improves reasoning ability \\emph{without} more parameters, decoupling reasoning from size. This suggests a new scaling path beyond token-wise test-time methods. (3) \\textit{Robustness and generality}: reasoning gains persist across architectures and reuse schedules, showing VLD captures a general scaling behavior. These results provide insight into future scaling strategies and raise a deeper question: does superintelligence require ever-larger models, or can it be achieved by reusing parameters and increasing logical depth? We argue many unknown dynamics in scaling remain to be explored. Code is available at https://anonymous.4open.science/r/virtual_logical_depth-8024/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Scaling Lawsçš„ä¸€ä¸ªæ–°ç»´åº¦ï¼Œå³Virtual Logical Depth (VLD)ï¼Œé€šè¿‡æƒé‡é‡ç”¨æ¥å¢åŠ æœ‰æ•ˆç®—æ³•æ·±åº¦è€Œä¸æ”¹å˜å‚æ•°æ€»é‡ã€‚å®éªŒå‘ç°ï¼Œåœ¨å›ºå®šå‚æ•°é‡ä¸‹ï¼ŒVLDå‡ ä¹ä¸æ”¹å˜æ¨¡å‹çš„Knowledge Capacityï¼ŒçŸ¥è¯†å®¹é‡ä¾ç„¶ä¸»è¦ç”±å‚æ•°è§„æ¨¡å†³å®šã€‚ç„¶è€Œï¼Œåˆç†å®æ–½çš„VLDèƒ½åœ¨ä¸å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ¨¡å‹çš„Reasoningèƒ½åŠ›ï¼Œå®ç°äº†æ¨ç†èƒ½åŠ›ä¸æ¨¡å‹å°ºå¯¸çš„æœ‰æ•ˆè§£è€¦ã€‚è¿™ç§æ¨ç†å¢ç›Šåœ¨å¤šç§æ¶æ„å’Œé‡ç”¨æ–¹æ¡ˆä¸­å‡è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œè¯æ˜äº†VLDæ•æ‰åˆ°äº†ä¸€ç§é€šç”¨çš„Scalingè¡Œä¸ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒVLDæä¾›äº†ä¸€æ¡è¶…è¶ŠToken-wise Test-time Methodsçš„æ–°è·¯å¾„ï¼Œæš—ç¤ºè¶…çº§æ™ºèƒ½æˆ–è®¸å¯ä»¥é€šè¿‡å¢åŠ é€»è¾‘æ·±åº¦è€Œéå•çº¯æ‰©å¤§å‚æ•°è§„æ¨¡æ¥å®ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18233v3",
      "published_date": "2025-06-23 01:56:25 UTC",
      "updated_date": "2025-10-12 05:41:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:01:50.889053+00:00"
    },
    {
      "arxiv_id": "2506.18226v1",
      "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation",
      "title_zh": "å®ç°é«˜æ•ˆï¼šé¢å‘è‡ªå›å½’å›¾åƒç”Ÿæˆçš„åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›",
      "authors": [
        "Xunzhi Xiang",
        "Qi Fan"
      ],
      "abstract": "Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆAutoregressive conditional image generation modelsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å› é•¿ä¸Šä¸‹æ–‡å¯¼è‡´çš„ KV-cache æ˜¾å­˜å¼€é”€è¿‡å¤§å’Œè®¡ç®—å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º ADSA (Adaptive Dynamic Sparse Attention) çš„æ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿåˆ†æå›¾åƒæ¨ç†ä¸­å…¨å±€è¯­ä¹‰ã€ç©ºé—´å¸ƒå±€å’Œç»†ç²’åº¦çº¹ç†çš„å½¢æˆè¿‡ç¨‹ï¼ŒåŠ¨æ€è¯†åˆ«å¯¹äºç»´æŒå±€éƒ¨çº¹ç†ä¸€è‡´æ€§ï¼ˆlocal texture consistencyï¼‰å’Œå…¨å±€è¯­ä¹‰è¿è´¯æ€§ï¼ˆglobal semantic coherenceï¼‰è‡³å…³é‡è¦çš„å†å² tokenï¼Œä»è€Œé«˜æ•ˆç®€åŒ–æ³¨æ„åŠ›è®¡ç®—ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸º ADSA é‡èº«å®šåˆ¶çš„åŠ¨æ€ KV-cache æ›´æ–°æœºåˆ¶ï¼Œä½¿æ¨ç†è¿‡ç¨‹ä¸­çš„ GPU æ˜¾å­˜æ¶ˆè€—é™ä½äº†çº¦ 50%ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒADSA åœ¨ç¡®ä¿ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œåœ¨èµ„æºæ•ˆç‡æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18226v1",
      "published_date": "2025-06-23 01:27:06 UTC",
      "updated_date": "2025-06-23 01:27:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:02:18.702414+00:00"
    },
    {
      "arxiv_id": "2506.18221v2",
      "title": "These Are Not All the Features You Are Looking For: A Fundamental Bottleneck in Supervised Pretraining",
      "title_zh": "å¹¶éæ‰€æœ‰ç‰¹å¾å°½åœ¨å…¶ä¸­ï¼šæœ‰ç›‘ç£é¢„è®­ç»ƒä¸­çš„æ ¹æœ¬ç“¶é¢ˆ",
      "authors": [
        "Xingyu Alice Yang",
        "Jianyu Zhang",
        "LÃ©on Bottou"
      ],
      "abstract": "Transfer learning is a cornerstone of modern machine learning, promising a way to adapt models pretrained on a broad mix of data to new tasks with minimal new data. However, a significant challenge remains in ensuring that transferred features are sufficient to handle unseen datasets, amplified by the difficulty of quantifying whether two tasks are \"related\". To address these challenges, we evaluate model transfer from a pretraining mixture to each of its component tasks, assessing whether pretrained features can match the performance of task-specific direct training. We identify a fundamental limitation in deep learning models -- an \"information saturation bottleneck\" -- where networks fail to learn new features once they encode similar competing features during training. When restricted to learning only a subset of key features during pretraining, models will permanently lose critical features for transfer and perform inconsistently on data distributions, even components of the training mixture. Empirical evidence from published studies suggests that this phenomenon is pervasive in deep learning architectures -- factors such as data distribution or ordering affect the features that current representation learning methods can learn over time. This study suggests that relying solely on large-scale networks may not be as effective as focusing on task-specific training, when available. We propose richer feature representations as a potential solution to better generalize across new datasets and, specifically, present existing methods alongside a novel approach, the initial steps towards addressing this challenge.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç›‘ç£é¢„è®­ç»ƒ(Supervised Pretraining)ä¸­çš„ä¸€ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼Œå³â€œä¿¡æ¯é¥±å’Œç“¶é¢ˆ(Information Saturation Bottleneck)â€ï¼Œæ­ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¿ç§»å­¦ä¹ (Transfer Learning)ä¸­çš„ç‰¹å¾æå–ç¼ºé™·ã€‚ä½œè€…å‘ç°ï¼Œå½“ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼–ç äº†æŸäº›ç«äº‰ç‰¹å¾åï¼Œå¾€å¾€ä¼šåœæ­¢å­¦ä¹ æ–°çš„ç‰¹å¾ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µä»…èƒ½è·å–ç‰¹å¾çš„ä¸€ä¸ªå­é›†ã€‚è¿™ç§ç°è±¡ä½¿å¾—æ¨¡å‹æ°¸ä¹…æ€§åœ°ä¸¢å¤±äº†å¯¹äºä¸‹æ¸¸è¿ç§»è‡³å…³é‡è¦çš„ç‰¹å¾ï¼Œå¯¼è‡´å…¶åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸Šï¼Œç”šè‡³æ˜¯é¢„è®­ç»ƒæ··åˆæ•°æ®çš„ç»„æˆéƒ¨åˆ†ä¸Šè¡¨ç°ä¸ä¸€è‡´ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§ç“¶é¢ˆåœ¨ç°æœ‰æ·±åº¦å­¦ä¹ æ¶æ„ä¸­æ™®éå­˜åœ¨ï¼Œä¸”å—åˆ°æ•°æ®åˆ†å¸ƒå’Œè®­ç»ƒé¡ºåºçš„æ˜¾è‘—å½±å“ã€‚ç ”ç©¶ç»“æœå»ºè®®ï¼Œåœ¨æœ‰æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œä»…ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒå¯èƒ½ä¸å¦‚é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒ(Task-Specific Training)æœ‰æ•ˆã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤º(Richer Feature Representations)ä½œä¸ºæ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¢è®¨äº†æ—¨åœ¨æå‡è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 7 figures, Preprint. Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.18221v2",
      "published_date": "2025-06-23 01:04:29 UTC",
      "updated_date": "2025-06-26 13:50:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:02:22.612963+00:00"
    },
    {
      "arxiv_id": "2506.18220v1",
      "title": "Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano",
      "title_zh": "é¢å‘ NVIDIA Jetson Nano è§†ç½‘è†œçœ¼åº•å›¾åƒå¼‚å¸¸æ£€æµ‹çš„è·¨æ¶æ„çŸ¥è¯†è’¸é¦ (KD)",
      "authors": [
        "Berk Yilmaz",
        "Aniruddh Aiyengar"
      ],
      "abstract": "Early and accurate identification of retinal ailments is crucial for averting ocular decline; however, access to dependable diagnostic devices is not often available in low-resourced settings. This project proposes to solve that by developing a lightweight, edge-device deployable disease classifier using cross-architecture knowledge distilling. We first train a high-capacity vision transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised learning, to classify fundus images into four classes: Normal, Diabetic Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus when compressing to a CNN-based student model for deployment in resource-limited conditions, such as the NVIDIA Jetson Nano. This was accomplished using a novel framework which included a Partitioned Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a multi-view robust training method. The teacher model has 97.4 percent more parameters than the student model, with it achieving 89 percent classification with a roughly 93 percent retention of the teacher model's diagnostic performance. The retention of clinical classification behavior supports our method's initial aim: compression of the ViT while retaining accuracy. Our work serves as an example of a scalable, AI-driven triage solution for retinal disorders in under-resourced areas.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºåŒ®ä¹åœ°åŒºè§†ç½‘è†œç–¾ç—…è¯Šæ–­è®¾å¤‡ä¸è¶³çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºè·¨æ¶æ„çŸ¥è¯†è’¸é¦ (Cross-Architecture Knowledge Distillation) çš„è½»é‡åŒ–è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²åˆ†ç±»å™¨ã€‚ç ”ç©¶é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªç»è¿‡ I-JEPA è‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒçš„é«˜å®¹é‡ Vision Transformer (ViT) æ•™å¸ˆæ¨¡å‹ï¼Œç”¨äºå¯¹çœ¼åº•å›¾åƒè¿›è¡Œå››ç±»ç—…å˜åˆ†ç±»ã€‚ä¸ºäº†å®ç°åœ¨ NVIDIA Jetson Nano ç­‰ IoT è®¾å¤‡çš„éƒ¨ç½²ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŒ…å«åˆ†åŒºäº¤å‰æ³¨æ„åŠ› (Partitioned Cross-Attention) æŠ•å½±å™¨ã€ç»„çº¿æ€§ (Group-Wise Linear) æŠ•å½±å™¨ä»¥åŠå¤šè§†å›¾é²æ£’è®­ç»ƒæ–¹æ³•çš„åˆ›æ–°æ¡†æ¶ï¼Œå°† ViT çŸ¥è¯†å‹ç¼©è‡³ CNN å­¦ç”Ÿæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å‚æ•°é‡æ¯”æ•™å¸ˆæ¨¡å‹å‡å°‘ 97.4% çš„æƒ…å†µä¸‹å®ç°äº† 89% çš„åˆ†ç±»å‡†ç¡®ç‡ï¼ŒæˆåŠŸä¿ç•™äº†æ•™å¸ˆæ¨¡å‹çº¦ 93% çš„è¯Šæ–­æ€§èƒ½ã€‚è¿™ä¸€å·¥ä½œä¸ºä½èµ„æºåœ°åŒºçš„è§†ç½‘è†œéšœç¢ç­›æŸ¥æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é«˜æ•ˆçš„ AI é©±åŠ¨é¢„åˆ†è¯Šæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 10 figures. Berk Yilmaz and Aniruddh Aiyengar contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2506.18220v1",
      "published_date": "2025-06-23 00:57:43 UTC",
      "updated_date": "2025-06-23 00:57:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:02:19.408297+00:00"
    },
    {
      "arxiv_id": "2506.18213v1",
      "title": "A Conceptual Framework for AI Capability Evaluations",
      "title_zh": "äººå·¥æ™ºèƒ½èƒ½åŠ›è¯„ä¼°çš„æ¦‚å¿µæ¡†æ¶",
      "authors": [
        "MarÃ­a Victoria Carro",
        "Denise Alejandra Mester",
        "Francisca Gauna Selasco",
        "Luca NicolÃ¡s Forziati Gangi",
        "Matheo Sandleris Musa",
        "Lola Ramos Pereyra",
        "Mario Leiva",
        "Juan Gustavo Corvalan",
        "MarÃ­a Vanina Martinez",
        "Gerardo Simari"
      ],
      "abstract": "As AI systems advance and integrate into society, well-designed and transparent evaluations are becoming essential tools in AI governance, informing decisions by providing evidence about system capabilities and risks. Yet there remains a lack of clarity on how to perform these assessments both comprehensively and reliably. To address this gap, we propose a conceptual framework for analyzing AI capability evaluations, offering a structured, descriptive approach that systematizes the analysis of widely used methods and terminology without imposing new taxonomies or rigid formats. This framework supports transparency, comparability, and interpretability across diverse evaluations. It also enables researchers to identify methodological weaknesses, assists practitioners in designing evaluations, and provides policymakers with an accessible tool to scrutinize, compare, and navigate complex evaluation landscapes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºåˆ†æ AI capability evaluations çš„æ¦‚å¿µæ¡†æ¶(conceptual framework)ï¼Œæ—¨åœ¨åº”å¯¹ AI ç³»ç»Ÿå¿«é€Ÿå‘å±•èƒŒæ™¯ä¸‹ AI governance å¯¹å…¨é¢ä¸”å¯é è¯„ä¼°å·¥å…·çš„è¿«åˆ‡éœ€æ±‚ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æè¿°æ€§æ–¹æ³•ï¼Œé€šè¿‡ç³»ç»ŸåŒ–åˆ†æç°æœ‰çš„è¯„ä¼°æœ¯è¯­å’Œæ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å¤šå…ƒè¯„ä¼°ä½“ç³»é—´çš„é€æ˜åº¦ã€å¯æ¯”æ€§å’Œå¯è§£é‡Šæ€§(interpretability)ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½å¸®åŠ©ç ”ç©¶äººå‘˜è¯†åˆ«æ–¹æ³•è®ºä¸Šçš„å¼±ç‚¹ï¼Œè¿˜èƒ½ååŠ©ä»ä¸šè€…ä¼˜åŒ–è¯„ä¼°è®¾è®¡ï¼Œå¹¶ä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›ç›´è§‚çš„å·¥å…·æ¥å®¡æŸ¥å’Œå¯¼èˆªå¤æ‚çš„è¯„ä¼°å›¾æ™¯ã€‚é€šè¿‡å¯¹ç°æœ‰è¯„ä¼°æ‰‹æ®µçš„ç³»ç»Ÿæ€§æ¢³ç†ï¼Œè¯¥æ¡†æ¶ä¸ºå»ºç«‹æ›´å…·å…¬ä¿¡åŠ›çš„ AI èƒ½åŠ›è¯„ä»·ä½“ç³»æä¾›äº†å…³é”®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2306.04181 by other authors",
      "pdf_url": "https://arxiv.org/pdf/2506.18213v1",
      "published_date": "2025-06-23 00:19:27 UTC",
      "updated_date": "2025-06-23 00:19:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T00:02:24.886819+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 144,
  "processed_papers_count": 144,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T00:03:31.320926+00:00"
}
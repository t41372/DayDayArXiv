{
  "date": "2024-09-26",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-26 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 在医疗、机器人、强化学习和多模态模型中的应用，涵盖了从情感识别到自动驾驶的创新研究，其中令人印象深刻的是 AER-LLM 在模糊情感识别上的进展，以及 Embodied-RAG 在机器人记忆系统的突破；有名学者如 Yann LeCun 和 Geoffrey Hinton 的影响虽未直接出现，但相关领域论文如 DRL-STNet 展示了高影响力团队（如 Aggelos K. Katsaggelos）的贡献。\n\n下面，我将逐一简要概述今天的论文，先优先讨论那些重要、话题性强或有潜在影响的文章（如 AI 安全、医疗应用和机器人领域），并将相关论文归类讨论。对于次要或较为常规的论文（如某些图像处理或小众优化方法），我将快速掠过，只列出标题和核心要点，以控制篇幅。\n\n### 重点论文讨论\n\n**1. Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving**  \n（标题：改善代理行为的强化学习微调用于自动驾驶）  \n这篇论文提出了一种强化学习方法，用于微调代理行为模型，提高自动驾驶模拟中的性能和安全性。主要贡献是通过闭环微调减少碰撞率，并在 Waymo Open Sim Agents 挑战中表现出色，强调了强化学习在真实世界部署中的鲁棒性。\n\n**2. DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image Segmentation via Disentangled Representation Learning**  \n（标题：DRL-STNet：通过分离表示学习的无监督域适应进行跨模态医学图像分割）  \n作者包括知名学者 Aggelos K. Katsaggelos，这篇论文在医疗图像领域有显著影响。核心发现是使用 GAN 和自训练方法实现跨模态分割，在 FLARE 数据集上提升 Dice 系数 11.4%，为无监督医学图像处理提供了高效框架。\n\n**3. AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models**  \n（标题：AER-LLM：利用大语言模型的模糊感知情感识别）  \n这篇论文令人印象深刻，因为它首次探索 LLMs 在处理模糊情感中的潜力。主要贡献是通过零样本和少样本提示识别复杂情感，并在三个数据集上表现出色，展示了 LLMs 在情感智能的应用潜力。\n\n**4. Development and Validation of a Dynamic-Template-Constrained Large Language Model for Generating Fully-Structured Radiology Reports**  \n（标题：动态模板约束大语言模型的开发与验证，用于生成完全结构化的放射学报告）  \n作者团队包括 Ge Wang，这篇论文在医疗 AI 中有实际价值。关键发现是开发了一个开源模型，避免了幻觉和隐私泄露问题，在放射学报告生成中达到 97% F1 分数，展示了 LLMs 在临床应用的可靠性。\n\n**5. Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation**  \n（标题：Embodied-RAG：用于检索和生成的通用非参数嵌入式记忆）  \n这篇论文创新性地将 RAG 应用于机器人领域，主要贡献是构建一个层次记忆系统，支持机器人导航和语言生成，在超过 250 个查询上表现出色，桥接了 RAG 和机器人领域的空白。\n\n**6. Criticality and Safety Margins for Reinforcement Learning**  \n（标题：强化学习的临界性和安全边际）  \n论文讨论了强化学习中的安全问题，核心发现是通过定义“真实临界性”和“代理临界性”来评估决策风险，在 Atari 环境中减少错误概率，强调了强化学习在高风险应用中的安全性。\n\n**7. An Adversarial Perspective on Machine Unlearning for AI Safety**  \n（标题：AI 安全的机器遗忘的对抗视角）  \n这篇论文从对抗角度审视机器遗忘，作者包括 Peter Henderson，主要贡献是展示遗忘方法在对抗攻击下的脆弱性，并提出改进策略，提升 AI 模型的安全性。\n\n**8. FactorSim: Generative Simulation via Factorized Representation**  \n（标题：FactorSim：通过因子化表示的生成模拟）  \n论文提出一个新框架，用于从语言输入生成模拟环境，主要发现是通过因子化表示提升强化学习的零样本转移能力，在 HCP 数据集上表现出色，适用于游戏和机器人任务。\n\n**9. DarkSAM: Fooling Segment Anything Model to Segment Nothing**  \n（标题：DarkSAM：欺骗 Segment Anything Model 使其不分割任何东西）  \n这篇论文揭示了视觉模型的安全漏洞，主要贡献是提出通用对抗攻击框架，成功让 SAM 模型失效，AUC 下降至 0.749，提醒了 AI 模型在实际部署中的风险。\n\n**10. Harmful Fine-tuning Attacks and Defenses for Large Language Models**  \n（标题：大语言模型的有害微调攻击与防御）  \n论文系统调研了有害微调攻击，主要发现是攻击可显著破坏模型安全，并提出防御框架，强调了 LLMs 在高风险环境中的防护需求。\n\n### 相关论文快速掠过\n其他论文中，医疗和机器人主题较多，以下快速总结：\n- **医疗图像和诊断**：如第24篇（Retrospective Comparative Analysis of Prostate Cancer In-Basket Messages）使用 LLMs 生成医疗响应，贡献是节省时间并提升响应质量；第29篇（DiffSSC）提出扩散模型改进医学图像分割，Dice 分数提升明显；第43篇（PCEvE）用于人类绘图评估，提升解释性 AI。\n- **机器人和自动驾驶**：第10篇（SOAR）优化 UAV 动作识别，准确率提升 9.7%；第22篇（Autonomous Network Defence）使用强化学习防御网络攻击；第39篇（DualAD）整合 LLMs 于自动驾驶决策。\n- **AI 优化和安全**：第16篇（Input-Dependent Power Usage in GPUs）分析 GPU 能耗变化，贡献是提出优化策略；第46篇（Byzantine-Robust Aggregation）提升联邦学习鲁棒性；第101篇（AI Delegates）聚焦 AI 代理的隐私保护。\n- **其他领域**：如第134篇（FactorSim）在 AI 模拟中表现突出；第15篇（Advancing Object Detection in Transportation）综述多模态模型在交通中的应用；其余如图像生成、金融 AI 等论文（如第37篇 FreeEdit）虽有创新，但影响力较小，仅快速提及其核心术语如扩散模型和 RAG。\n\n总之，今天的论文突出了 AI 在实际应用中的潜力与挑战，医疗和机器人领域的创新尤为值得关注。如果您对特定主题感兴趣，建议查看这些重点论文的细节！",
  "papers": [
    {
      "arxiv_id": "2409.18345v1",
      "title": "A Generalized LLM-Augmented BIM Framework: Application to a Speech-to-BIM system",
      "title_zh": "翻译失败",
      "authors": [
        "Ghang Lee",
        "Suhyung Jang",
        "Seokho Hyun"
      ],
      "abstract": "Performing building information modeling (BIM) tasks is a complex process\nthat imposes a steep learning curve and a heavy cognitive load due to the\nnecessity of remembering sequences of numerous commands. With the rapid\nadvancement of large language models (LLMs), it is foreseeable that BIM tasks,\nincluding querying and managing BIM data, 4D and 5D BIM, design compliance\nchecking, or authoring a design, using written or spoken natural language\n(i.e., text-to-BIM or speech-to-BIM), will soon supplant traditional graphical\nuser interfaces. This paper proposes a generalized LLM-augmented BIM framework\nto expedite the development of LLM-enhanced BIM applications by providing a\nstep-by-step development process. The proposed framework consists of six steps:\ninterpret-fill-match-structure-execute-check. The paper demonstrates the\napplicability of the proposed framework through implementing a speech-to-BIM\napplication, NADIA-S (Natural-language-based Architectural Detailing through\nInteraction with Artificial Intelligence via Speech), using exterior wall\ndetailing as an example.",
      "tldr_zh": "该论文针对建筑信息建模(BIM)任务的复杂性和高认知负担，提出一个通用的LLM-augmented BIM框架，以加速LLM增强BIM应用的开发。框架由六个步骤组成：interpret-fill-match-structure-execute-check，帮助用户通过自然语言（如文本或语音）处理BIM任务，包括查询数据、设计合规检查和设计创作。作者通过实现speech-to-BIM系统NADIA-S作为示例，展示了该框架在外部墙壁细节设计中的实际应用，从而简化了传统图形用户界面的操作。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "In Proceedings of the 41st International Conference of CIB W78.\n  Marrakech, Morocco, 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.18345v1",
      "published_date": "2024-09-26 23:46:15 UTC",
      "updated_date": "2024-09-26 23:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:55:49.114733"
    },
    {
      "arxiv_id": "2409.18343v1",
      "title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenghao Peng",
        "Wenjie Luo",
        "Yiren Lu",
        "Tianyi Shen",
        "Cole Gulino",
        "Ari Seff",
        "Justin Fu"
      ],
      "abstract": "A major challenge in autonomous vehicle research is modeling agent behaviors,\nwhich has critical applications including constructing realistic and reliable\nsimulations for off-board evaluation and forecasting traffic agents motion for\nonboard planning. While supervised learning has shown success in modeling\nagents across various domains, these models can suffer from distribution shift\nwhen deployed at test-time. In this work, we improve the reliability of agent\nbehaviors by closed-loop fine-tuning of behavior models with reinforcement\nlearning. Our method demonstrates improved overall performance, as well as\nimproved targeted metrics such as collision rate, on the Waymo Open Sim Agents\nchallenge. Additionally, we present a novel policy evaluation benchmark to\ndirectly assess the ability of simulated agents to measure the quality of\nautonomous vehicle planners and demonstrate the effectiveness of our approach\non this new benchmark.",
      "tldr_zh": "本文提出了一种使用强化学习（RL）fine-tuning的方法，来改善自主驾驶中代理行为的建模可靠性，以应对监督学习模型在测试时可能出现的分布偏移问题。该方法通过闭环微调行为模型，提升了整体性能，并在Waymo Open Sim Agents挑战中降低了碰撞率等关键指标。研究还引入了一个新型政策评估基准，用于评估模拟代理对自主车辆规划器质量的影响，并验证了该方法的有效性。",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.9"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18343v1",
      "published_date": "2024-09-26 23:40:33 UTC",
      "updated_date": "2024-09-26 23:40:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:56:01.005708"
    },
    {
      "arxiv_id": "2409.18340v1",
      "title": "DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image Segmentation via Disentangled Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Lin",
        "Florian Schiffers",
        "Santiago López-Tapia",
        "Neda Tavakoli",
        "Daniel Kim",
        "Aggelos K. Katsaggelos"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) is essential for medical image\nsegmentation, especially in cross-modality data scenarios. UDA aims to transfer\nknowledge from a labeled source domain to an unlabeled target domain, thereby\nreducing the dependency on extensive manual annotations. This paper presents\nDRL-STNet, a novel framework for cross-modality medical image segmentation that\nleverages generative adversarial networks (GANs), disentangled representation\nlearning (DRL), and self-training (ST). Our method leverages DRL within a GAN\nto translate images from the source to the target modality. Then, the\nsegmentation model is initially trained with these translated images and\ncorresponding source labels and then fine-tuned iteratively using a combination\nof synthetic and real images with pseudo-labels and real labels. The proposed\nframework exhibits superior performance in abdominal organ segmentation on the\nFLARE challenge dataset, surpassing state-of-the-art methods by 11.4% in the\nDice similarity coefficient and by 13.1% in the Normalized Surface Dice metric,\nachieving scores of 74.21% and 80.69%, respectively. The average running time\nis 41 seconds, and the area under the GPU memory-time curve is 11,292 MB. These\nresults indicate the potential of DRL-STNet for enhancing cross-modality\nmedical image segmentation tasks.",
      "tldr_zh": "本文提出 DRL-STNet，一种基于 Disentangled Representation Learning 的框架，用于 Unsupervised Domain Adaptation（UDA）在跨模态医疗图像分割中的应用。该框架利用 GANs 将源域图像翻译到目标域，并结合自训练（ST）方法，通过初始训练和迭代微调（使用合成图像、真实图像及伪标签）来减少手动标注依赖。在 FLARE 挑战数据集的腹部器官分割任务上，DRL-STNet 比最先进方法在 Dice 相似系数上提高 11.4%（达 74.21%），Normalized Surface Dice 指标上提高 13.1%（达 80.69%），并保持高效运行（平均 41 秒）。这些结果证明了该框架在提升跨模态医疗图像分割性能方面的潜力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "MICCAI 2024 Challenge, FLARE Challenge, Unsupervised domain\n  adaptation, Organ segmentation, Feature disentanglement, Self-training",
      "pdf_url": "http://arxiv.org/pdf/2409.18340v1",
      "published_date": "2024-09-26 23:30:40 UTC",
      "updated_date": "2024-09-26 23:30:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:56:15.639950"
    },
    {
      "arxiv_id": "2409.18339v2",
      "title": "AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Hong",
        "Yuan Gong",
        "Vidhyasaharan Sethu",
        "Ting Dang"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated great\nsuccess in many Natural Language Processing (NLP) tasks. In addition to their\ncognitive intelligence, exploring their capabilities in emotional intelligence\nis also crucial, as it enables more natural and empathetic conversational AI.\nRecent studies have shown LLMs' capability in recognizing emotions, but they\noften focus on single emotion labels and overlook the complex and ambiguous\nnature of human emotions. This study is the first to address this gap by\nexploring the potential of LLMs in recognizing ambiguous emotions, leveraging\ntheir strong generalization capabilities and in-context learning. We design\nzero-shot and few-shot prompting and incorporate past dialogue as context\ninformation for ambiguous emotion recognition. Experiments conducted using\nthree datasets indicate significant potential for LLMs in recognizing ambiguous\nemotions, and highlight the substantial benefits of including context\ninformation. Furthermore, our findings indicate that LLMs demonstrate a high\ndegree of effectiveness in recognizing less ambiguous emotions and exhibit\npotential for identifying more ambiguous emotions, paralleling human perceptual\ncapabilities.",
      "tldr_zh": "本文提出 AER-LLM 框架，利用大型语言模型（LLMs）来识别模糊情感，填补了现有研究忽略情感复杂性的空白。研究设计了 zero-shot 和 few-shot prompting 方法，并将过去的对话作为上下文信息，以提升情感识别的准确性。在三个数据集上的实验显示，LLMs 在识别模糊情感方面具有显著潜力，特别是在结合上下文信息时表现突出，且其能力在处理不太模糊的情感和更模糊的情感时，类似于人类感知水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.18339v2",
      "published_date": "2024-09-26 23:25:21 UTC",
      "updated_date": "2025-02-17 05:28:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:56:25.654561"
    },
    {
      "arxiv_id": "2409.18335v1",
      "title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Shea",
        "Zhou Yu"
      ],
      "abstract": "Despite recent advancements in AI and NLP, negotiation remains a difficult\ndomain for AI agents. Traditional game theoretic approaches that have worked\nwell for two-player zero-sum games struggle in the context of negotiation due\nto their inability to learn human-compatible strategies. On the other hand,\napproaches that only use human data tend to be domain-specific and lack the\ntheoretical guarantees provided by strategies grounded in game theory.\nMotivated by the notion of fairness as a criterion for optimality in general\nsum games, we propose a negotiation framework called FDHC which incorporates\nfairness into both the reward design and search to learn human-compatible\nnegotiation strategies. Our method includes a novel, RL+search technique called\nLGM-Zero which leverages a pre-trained language model to retrieve\nhuman-compatible offers from large action spaces. Our results show that our\nmethod is able to achieve more egalitarian negotiation outcomes and improve\nnegotiation quality.",
      "tldr_zh": "该论文针对AI代理在谈判领域的挑战，提出了一种以公平性为驱动的学习人类兼容谈判策略的方法，名为FDHC框架。该框架将公平性融入奖励设计和搜索过程，以弥补传统博弈论方法在一般和游戏中的不足，同时避免仅依赖人类数据的局限性。论文引入LGM-Zero技术，这是一种结合强化学习(RL+search)的创新方法，利用预训练语言模型从大型行动空间中检索人类兼容的报价。实验结果表明，该方法实现了更平等的谈判结果，并显著提高了谈判质量。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.18335v1",
      "published_date": "2024-09-26 23:16:47 UTC",
      "updated_date": "2024-09-26 23:16:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:56:37.904465"
    },
    {
      "arxiv_id": "2409.18324v1",
      "title": "Input-Dependent Power Usage in GPUs",
      "title_zh": "翻译失败",
      "authors": [
        "Theo Gregersen",
        "Pratyush Patel",
        "Esha Choukse"
      ],
      "abstract": "GPUs are known to be power-hungry, and due to the boom in artificial\nintelligence, they are currently the major contributors to the high power\ndemands of upcoming datacenters. Most GPU usage in these popular workloads\nconsist of large general matrix-matrix multiplications (GEMMs), which have\ntherefore been optimized to achieve high utilization of hardware resources. In\nthis work, we show that modifying the input data to GEMMs, while maintaining\nthe matrix shapes and sizes can notably change the power consumption of these\nkernels. We experiment with four kinds of input variations: value distribution,\nbit similarity, placement, and sparsity, across different data types. Our\nfindings indicate that these variations can change the GPU power usage during\nGEMM by almost 40%. We hypothesize that input-dependent power usage variations\noccur due to changes in the number of bit flips in the GPUs. We propose\nleveraging this property through compiler and scheduler optimizations to manage\npower and reduce energy consumption.",
      "tldr_zh": "这篇论文探讨了 GPU 在执行矩阵乘法 (GEMMs) 时，输入数据的变化如何显著影响电力消耗，尽管矩阵形状和大小保持不变。研究者通过实验四种输入变化——值分布 (value distribution)、位相似性 (bit similarity)、放置 (placement) 和稀疏性 (sparsity)——发现这些调整可使 GEMMs 的 GPU 电力使用变化近 40%，并归因于位翻转 (bit flips) 数量的差异。主要贡献是提出利用这一特性，通过编译器和调度器优化来管理电力并降低数据中心能源消耗。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18324v1",
      "published_date": "2024-09-26 22:31:09 UTC",
      "updated_date": "2024-09-26 22:31:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:56:49.795602"
    },
    {
      "arxiv_id": "2409.18319v2",
      "title": "Development and Validation of a Dynamic-Template-Constrained Large Language Model for Generating Fully-Structured Radiology Reports",
      "title_zh": "动态模板约束大语言模型的开发和验证，用于生成完全结构化的放射学报告",
      "authors": [
        "Chuang Niu",
        "Parisa Kaviani",
        "Qing Lyu",
        "Mannudeep K. Kalra",
        "Christopher T. Whitlow",
        "Ge Wang"
      ],
      "abstract": "Current LLMs for creating fully-structured reports face the challenges of\nformatting errors, content hallucinations, and privacy leakage issues when\nuploading data to external servers.We aim to develop an open-source, accurate\nLLM for creating fully-structured and standardized LCS reports from varying\nfree-text reports across institutions and demonstrate its utility in automatic\nstatistical analysis and individual lung nodule retrieval. With IRB approvals,\nour retrospective study included 5,442 de-identified LDCT LCS radiology reports\nfrom two institutions. We constructed two evaluation datasets by labeling 500\npairs of free-text and fully-structured radiology reports and one large-scale\nconsecutive dataset from January 2021 to December 2023. Two radiologists\ncreated a standardized template for recording 27 lung nodule features on LCS.\nWe designed a dynamic-template-constrained decoding method to enhance existing\nLLMs for creating fully-structured reports from free-text radiology reports.\nUsing consecutive structured reports, we automated descriptive statistical\nanalyses and a nodule retrieval prototype. Our best LLM for creating\nfully-structured reports achieved high performance on cross-institutional\ndatasets with an F1 score of about 97%, with neither formatting errors nor\ncontent hallucinations. Our method consistently improved the best open-source\nLLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically\nderived statistical distributions were consistent with prior findings regarding\nattenuation, location, size, stability, and Lung-RADS. The retrieval system\nwith structured reports allowed flexible nodule-level search and complex\nstatistical analysis. Our developed software is publicly available for local\ndeployment and further research.",
      "tldr_zh": "本研究开发并验证了一个动态模板约束的Large Language Model (LLM)，旨在从不同机构的自由文本放射学报告生成完全结构化的LCS报告，同时解决格式错误、内容幻觉和隐私泄露问题。该方法利用标准化模板记录27个肺结节特征，并设计动态模板约束解码技术，基于5,442个去标识化的LDCT报告构建评估数据集。实验结果显示，该LLM在跨机构数据集上达到约97%的F1 score，且无格式错误或内容幻觉，比最佳开源LLM提高10.42%，并优于GPT-4o 17.19%。此外，该系统支持自动统计分析和肺结节检索原型，并提供开源软件以供本地部署和进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18319v2",
      "published_date": "2024-09-26 21:59:11 UTC",
      "updated_date": "2024-10-25 03:17:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:57:02.237826"
    },
    {
      "arxiv_id": "2409.18313v5",
      "title": "Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Quanting Xie",
        "So Yeon Min",
        "Pengliang Ji",
        "Yue Yang",
        "Tianyi Zhang",
        "Kedi Xu",
        "Aarav Bajaj",
        "Ruslan Salakhutdinov",
        "Matthew Johnson-Roberson",
        "Yonatan Bisk"
      ],
      "abstract": "There is no limit to how much a robot might explore and learn, but all of\nthat knowledge needs to be searchable and actionable. Within language research,\nretrieval augmented generation (RAG) has become the workhorse of large-scale\nnon-parametric knowledge; however, existing techniques do not directly transfer\nto the embodied domain, which is multimodal, where data is highly correlated,\nand perception requires abstraction. To address these challenges, we introduce\nEmbodied-RAG, a framework that enhances the foundational model of an embodied\nagent with a non-parametric memory system capable of autonomously constructing\nhierarchical knowledge for both navigation and language generation.\nEmbodied-RAG handles a full range of spatial and semantic resolutions across\ndiverse environments and query types, whether for a specific object or a\nholistic description of ambiance. At its core, Embodied-RAG's memory is\nstructured as a semantic forest, storing language descriptions at varying\nlevels of detail. This hierarchical organization allows the system to\nefficiently generate context-sensitive outputs across different robotic\nplatforms. We demonstrate that Embodied-RAG effectively bridges RAG to the\nrobotics domain, successfully handling over 250 explanation and navigation\nqueries across kilometer-level environments, highlighting its promise as a\ngeneral-purpose non-parametric system for embodied agents.",
      "tldr_zh": "该研究提出Embodied-RAG框架，将检索增强生成(RAG)技术扩展到机器人领域，构建一个通用的非参数记忆系统，以处理多模态数据和感知抽象挑战。\nEmbodied-RAG通过自主构建层次化知识结构（semantic forest），存储不同详细程度的语言描述，支持机器人进行导航和语言生成，适用于各种空间语义分辨率和查询类型。\n实验结果显示，该框架在公里级环境中成功处理超过250个解释和导航查询，证明了其作为机器人代理通用非参数系统的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Web: https://quanting-xie.github.io/Embodied-RAG-web/",
      "pdf_url": "http://arxiv.org/pdf/2409.18313v5",
      "published_date": "2024-09-26 21:44:11 UTC",
      "updated_date": "2025-01-21 02:38:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:57:12.580857"
    },
    {
      "arxiv_id": "2409.18301v3",
      "title": "Wavelet-Driven Generalizable Framework for Deepfake Face Forgery Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Lalith Bharadwaj Baru",
        "Rohit Boddeda",
        "Shilhora Akshay Patel",
        "Sai Mohan Gajapaka"
      ],
      "abstract": "The evolution of digital image manipulation, particularly with the\nadvancement of deep generative models, significantly challenges existing\ndeepfake detection methods, especially when the origin of the deepfake is\nobscure. To tackle the increasing complexity of these forgeries, we propose\n\\textbf{Wavelet-CLIP}, a deepfake detection framework that integrates wavelet\ntransforms with features derived from the ViT-L/14 architecture, pre-trained in\nthe CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze\nboth spatial and frequency features from images, thus enhancing the model's\ncapability to detect sophisticated deepfakes. To verify the effectiveness of\nour approach, we conducted extensive evaluations against existing\nstate-of-the-art methods for cross-dataset generalization and detection of\nunseen images generated by standard diffusion models. Our method showcases\noutstanding performance, achieving an average AUC of 0.749 for cross-data\ngeneralization and 0.893 for robustness against unseen deepfakes, outperforming\nall compared methods. The code can be reproduced from the repo:\n\\url{https://github.com/lalithbharadwajbaru/Wavelet-CLIP}",
      "tldr_zh": "这篇论文提出了一种名为 Wavelet-CLIP 的深度伪造检测框架，通过整合小波变换(Wavelet Transforms)与 CLIP 预训练的 ViT-L/14 架构，分析图像的空间和频率特征，以提升对复杂深度伪造的检测能力。框架特别关注跨数据集泛化和对未见图像的鲁棒性，并在实验中表现出色，平均 AUC 为 0.749 的跨数据集泛化性能和 0.893 的鲁棒性，优于现有方法。该研究为数字图像操纵的检测提供了更具泛化性的解决方案，并提供了代码仓库以供复现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "9 Pages, 2 Figures, 3 Tables",
      "pdf_url": "http://arxiv.org/pdf/2409.18301v3",
      "published_date": "2024-09-26 21:16:51 UTC",
      "updated_date": "2025-01-07 12:44:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:57:25.748285"
    },
    {
      "arxiv_id": "2409.18300v1",
      "title": "SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiqi Xian",
        "Xiyang Wu",
        "Tianrui Guan",
        "Xijun Wang",
        "Boqing Gong",
        "Dinesh Manocha"
      ],
      "abstract": "We introduce SOAR, a novel Self-supervised pretraining algorithm for aerial\nfootage captured by Unmanned Aerial Vehicles (UAVs). We incorporate human\nobject knowledge throughout the pretraining process to enhance UAV video\npretraining efficiency and downstream action recognition performance. This is\nin contrast to prior works that primarily incorporate object information during\nthe fine-tuning stage. Specifically, we first propose a novel object-aware\nmasking strategy designed to retain the visibility of certain patches related\nto objects throughout the pretraining phase. Second, we introduce an\nobject-aware loss function that utilizes object information to adjust the\nreconstruction loss, preventing bias towards less informative background\npatches. In practice, SOAR with a vanilla ViT backbone, outperforms best UAV\naction recognition models, recording a 9.7% and 21.4% boost in top-1 accuracy\non the NEC-Drone and UAV-Human datasets, while delivering an inference speed of\n18.7ms per video, making it 2x to 5x faster. Additionally, SOAR obtains\ncomparable accuracy to prior self-supervised learning (SSL) methods while\nrequiring 87.5% less pretraining time and 25% less memory usage",
      "tldr_zh": "该研究提出SOAR，一种针对无人机（UAVs）空中视频的自监督预训练算法，通过整合物体知识从预训练阶段提升动作识别效率。SOAR包括一个物体感知掩码策略（object-aware masking strategy），用于保留与物体相关的patches可见性，以及一个物体感知损失函数（object-aware loss function），调整重建损失以避免偏向背景区域。实验结果显示，使用vanilla ViT骨干网络的SOAR在NEC-Drone和UAV-Human数据集上，top-1准确率分别提升9.7%和21.4%，推理速度达18.7ms每视频，比现有模型快2x至5x，同时预训练时间减少87.5%、内存使用降低25%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18300v1",
      "published_date": "2024-09-26 21:15:22 UTC",
      "updated_date": "2024-09-26 21:15:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:57:37.765752"
    },
    {
      "arxiv_id": "2409.18297v1",
      "title": "Flat'n'Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Lipeng Zhuang",
        "Shiyu Fan",
        "Yingdong Ru",
        "Florent Audonnet",
        "Paul Henderson",
        "Gerardo Aragon-Camarasa"
      ],
      "abstract": "We present Flat'n'Fold, a novel large-scale dataset for garment manipulation\nthat addresses critical gaps in existing datasets. Comprising 1,212 human and\n887 robot demonstrations of flattening and folding 44 unique garments across 8\ncategories, Flat'n'Fold surpasses prior datasets in size, scope, and diversity.\nOur dataset uniquely captures the entire manipulation process from crumpled to\nfolded states, providing synchronized multi-view RGB-D images, point clouds,\nand action data, including hand or gripper positions and rotations. We quantify\nthe dataset's diversity and complexity compared to existing benchmarks and show\nthat our dataset features natural and diverse manipulations of real-world\ndemonstrations of human and robot demonstrations in terms of visual and action\ninformation. To showcase Flat'n'Fold's utility, we establish new benchmarks for\ngrasping point prediction and subtask decomposition. Our evaluation of\nstate-of-the-art models on these tasks reveals significant room for\nimprovement. This underscores Flat'n'Fold's potential to drive advances in\nrobotic perception and manipulation of deformable objects. Our dataset can be\ndownloaded at https://cvas-ug.github.io/flat-n-fold",
      "tldr_zh": "本研究引入了Flat'n'Fold，一个大规模多模态数据集，旨在填补服装感知和操控领域的空白。该数据集包含1,212个人类演示和887个机器人演示，涵盖44件独特服装的8个类别，并记录从皱巴巴到折叠的整个过程，包括同步的多视图RGB-D图像、point clouds和动作数据（如手或抓取器位置和旋转）。相比现有基准，Flat'n'Fold在规模、范围和多样性上更胜一筹，并量化了其视觉和动作信息的自然多样性。研究建立了新的基准任务，如grasping point prediction和subtask decomposition，并评估了最先进模型的表现，揭示了显著的改进空间，从而推动机器人对可变形物体的感知和操控技术的发展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18297v1",
      "published_date": "2024-09-26 21:10:17 UTC",
      "updated_date": "2024-09-26 21:10:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:57:49.653824"
    },
    {
      "arxiv_id": "2409.18295v1",
      "title": "Enhancing Lossy Compression Through Cross-Field Information for Scientific Applications",
      "title_zh": "通过跨",
      "authors": [
        "Youyuan Liu",
        "Wenqi Jia",
        "Taolue Yang",
        "Miao Yin",
        "Sian Jin"
      ],
      "abstract": "Lossy compression is one of the most effective methods for reducing the size\nof scientific data containing multiple data fields. It reduces information\ndensity through prediction or transformation techniques to compress the data.\nPrevious approaches use local information from a single target field when\npredicting target data points, limiting their potential to achieve higher\ncompression ratios. In this paper, we identified significant cross-field\ncorrelations within scientific datasets. We propose a novel hybrid prediction\nmodel that utilizes CNN to extract cross-field information and combine it with\nexisting local field information. Our solution enhances the prediction accuracy\nof lossy compressors, leading to improved compression ratios without\ncompromising data quality. We evaluate our solution on three scientific\ndatasets, demonstrating its ability to improve compression ratios by up to 25%\nunder specific error bounds. Additionally, our solution preserves more data\ndetails and reduces artifacts compared to baseline approaches.",
      "tldr_zh": "本研究针对科学应用中的损失性压缩问题，指出传统方法仅使用单一目标字段的本地信息，导致压缩比受限。论文提出了一种新型混合预测模型，利用CNN提取跨字段相关信息，并将其与现有本地字段信息结合，从而提升预测准确性和压缩效率。在三个科学数据集上的实验显示，该方法在特定错误边界下将压缩比提高高达25%，同时保留更多数据细节并减少伪影。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 9 figures, accepted by DRBSD-10",
      "pdf_url": "http://arxiv.org/pdf/2409.18295v1",
      "published_date": "2024-09-26 21:06:53 UTC",
      "updated_date": "2024-09-26 21:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:58:01.527907"
    },
    {
      "arxiv_id": "2409.18290v1",
      "title": "Retrospective Comparative Analysis of Prostate Cancer In-Basket Messages: Responses from Closed-Domain LLM vs. Clinical Teams",
      "title_zh": "翻译失败",
      "authors": [
        "Yuexing Hao",
        "Jason M. Holmes",
        "Jared Hobson",
        "Alexandra Bennett",
        "Daniel K. Ebner",
        "David M. Routman",
        "Satomi Shiraishi",
        "Samir H. Patel",
        "Nathan Y. Yu",
        "Chris L. Hallemeier",
        "Brooke E. Ball",
        "Mark R. Waddle",
        "Wei Liu"
      ],
      "abstract": "In-basket message interactions play a crucial role in physician-patient\ncommunication, occurring during all phases (pre-, during, and post) of a\npatient's care journey. However, responding to these patients' inquiries has\nbecome a significant burden on healthcare workflows, consuming considerable\ntime for clinical care teams. To address this, we introduce RadOnc-GPT, a\nspecialized Large Language Model (LLM) powered by GPT-4 that has been designed\nwith a focus on radiotherapeutic treatment of prostate cancer with advanced\nprompt engineering, and specifically designed to assist in generating\nresponses. We integrated RadOnc-GPT with patient electronic health records\n(EHR) from both the hospital-wide EHR database and an internal,\nradiation-oncology-specific database. RadOnc-GPT was evaluated on 158\npreviously recorded in-basket message interactions. Quantitative natural\nlanguage processing (NLP) analysis and two grading studies with clinicians and\nnurses were used to assess RadOnc-GPT's responses. Our findings indicate that\nRadOnc-GPT slightly outperformed the clinical care team in \"Clarity\" and\n\"Empathy,\" while achieving comparable scores in \"Completeness\" and\n\"Correctness.\" RadOnc-GPT is estimated to save 5.2 minutes per message for\nnurses and 2.4 minutes for clinicians, from reading the inquiry to sending the\nresponse. Employing RadOnc-GPT for in-basket message draft generation has the\npotential to alleviate the workload of clinical care teams and reduce\nhealthcare costs by producing high-quality, timely responses.",
      "tldr_zh": "本研究对前列腺癌患者 in-basket 消息的响应进行了回顾性比较，评估了基于 GPT-4 的专用 Large Language Model (LLM) 即 RadOnc-GPT 与临床团队的表现。RadOnc-GPT 通过先进提示工程和整合电子健康记录 (EHR) 数据，针对放射治疗场景生成响应，并在 158 条真实消息上进行了自然语言处理 (NLP) 分析和临床人员评分。结果显示，RadOnc-GPT 在 “Clarity” (清晰度) 和 “Empathy” (移情) 上略优于临床团队，在 “Completeness” (完整性) 和 “Correctness” (正确性) 上表现相当，同时可为护士节省 5.2 分钟/消息和为临床医生节省 2.4 分钟/消息。该方法有望减轻临床工作负担、降低医疗成本，并提升患者沟通效率。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18290v1",
      "published_date": "2024-09-26 21:00:51 UTC",
      "updated_date": "2024-09-26 21:00:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:58:14.006756"
    },
    {
      "arxiv_id": "2409.18289v1",
      "title": "Criticality and Safety Margins for Reinforcement Learning",
      "title_zh": "强化学习的临界性和安全裕度",
      "authors": [
        "Alexander Grushin",
        "Walt Woods",
        "Alvaro Velasquez",
        "Simon Khan"
      ],
      "abstract": "State of the art reinforcement learning methods sometimes encounter unsafe\nsituations. Identifying when these situations occur is of interest both for\npost-hoc analysis and during deployment, where it might be advantageous to call\nout to a human overseer for help. Efforts to gauge the criticality of different\npoints in time have been developed, but their accuracy is not well established\ndue to a lack of ground truth, and they are not designed to be easily\ninterpretable by end users. Therefore, we seek to define a criticality\nframework with both a quantifiable ground truth and a clear significance to\nusers. We introduce true criticality as the expected drop in reward when an\nagent deviates from its policy for n consecutive random actions. We also\nintroduce the concept of proxy criticality, a low-overhead metric that has a\nstatistically monotonic relationship to true criticality. Safety margins make\nthese interpretable, when defined as the number of random actions for which\nperformance loss will not exceed some tolerance with high confidence. We\ndemonstrate this approach in several environment-agent combinations; for an A3C\nagent in an Atari Beamrider environment, the lowest 5% of safety margins\ncontain 47% of agent losses; i.e., supervising only 5% of decisions could\npotentially prevent roughly half of an agent's errors. This criticality\nframework measures the potential impacts of bad decisions, even before those\ndecisions are made, allowing for more effective debugging and oversight of\nautonomous agents.",
      "tldr_zh": "该论文针对强化学习（Reinforcement Learning）中不安全情况的识别，提出了一种新的临界性框架，以提升事后分析和实时监督。论文定义了“true criticality”为代理偏离策略执行 n 个连续随机动作后的预期奖励下降，并引入“proxy criticality”作为一种低开销指标，与 true criticality 呈统计单调关系；此外，还定义了“safety margins”作为性能损失不超过容忍度的随机动作数量，以增强可解释性。在实验中，如 Atari Beamrider 环境中的 A3C 代理，最低的 5% 安全边际包含了 47% 的代理损失，这表明通过监督少量决策即可潜在防止约一半错误，从而改善自主代理的调试和监督。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "68T07",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 10 figures. This work has been submitted to the IEEE for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2409.18289v1",
      "published_date": "2024-09-26 21:00:45 UTC",
      "updated_date": "2024-09-26 21:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:58:26.074002"
    },
    {
      "arxiv_id": "2409.18286v1",
      "title": "Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing",
      "title_zh": "使用多模态大型语言模型 (MLLMs) 推进交通领域的物体检测：全面综述和实证测试",
      "authors": [
        "Huthaifa I. Ashqar",
        "Ahmed Jaber",
        "Taqwa I. Alhadidi",
        "Mohammed Elhenawy"
      ],
      "abstract": "This study aims to comprehensively review and empirically evaluate the\napplication of multimodal large language models (MLLMs) and Large Vision Models\n(VLMs) in object detection for transportation systems. In the first fold, we\nprovide a background about the potential benefits of MLLMs in transportation\napplications and conduct a comprehensive review of current MLLM technologies in\nprevious studies. We highlight their effectiveness and limitations in object\ndetection within various transportation scenarios. The second fold involves\nproviding an overview of the taxonomy of end-to-end object detection in\ntransportation applications and future directions. Building on this, we\nproposed empirical analysis for testing MLLMs on three real-world\ntransportation problems that include object detection tasks namely, road safety\nattributes extraction, safety-critical event detection, and visual reasoning of\nthermal images. Our findings provide a detailed assessment of MLLM performance,\nuncovering both strengths and areas for improvement. Finally, we discuss\npractical limitations and challenges of MLLMs in enhancing object detection in\ntransportation, thereby offering a roadmap for future research and development\nin this critical area.",
      "tldr_zh": "这篇论文全面审阅了多模态大语言模型 (MLLMs) 和大视觉模型 (VLMs) 在交通系统物体检测中的应用，并通过实证测试评估其潜力与局限性。研究首先探讨了 MLLMs 在交通场景中的益处，并总结了现有技术的有效性，如在各种物体检测任务中的表现和挑战。接着，论文概述了交通应用中端到端物体检测的分类，并提出未来研究方向。随后，通过实证分析测试 MLLMs 在道路安全属性提取、安全关键事件检测和热图像视觉推理等三个真实问题上的性能，结果揭示了其优势（如准确性提升）以及需改进的领域（如鲁棒性）。最终，论文讨论了 MLLMs 的实际限制和挑战，为交通物体检测领域的未来发展提供了路线图。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18286v1",
      "published_date": "2024-09-26 20:58:11 UTC",
      "updated_date": "2024-09-26 20:58:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:58:37.872500"
    },
    {
      "arxiv_id": "2409.18261v3",
      "title": "Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object Pose Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Mengchen Zhang",
        "Tong Wu",
        "Tai Wang",
        "Tengfei Wang",
        "Ziwei Liu",
        "Dahua Lin"
      ],
      "abstract": "6D object pose estimation aims at determining an object's translation,\nrotation, and scale, typically from a single RGBD image. Recent advancements\nhave expanded this estimation from instance-level to category-level, allowing\nmodels to generalize across unseen instances within the same category. However,\nthis generalization is limited by the narrow range of categories covered by\nexisting datasets, such as NOCS, which also tend to overlook common real-world\nchallenges like occlusion. To tackle these challenges, we introduce Omni6D, a\ncomprehensive RGBD dataset featuring a wide range of categories and varied\nbackgrounds, elevating the task to a more realistic context. 1) The dataset\ncomprises an extensive spectrum of 166 categories, 4688 instances adjusted to\nthe canonical pose, and over 0.8 million captures, significantly broadening the\nscope for evaluation. 2) We introduce a symmetry-aware metric and conduct\nsystematic benchmarks of existing algorithms on Omni6D, offering a thorough\nexploration of new challenges and insights. 3) Additionally, we propose an\neffective fine-tuning approach that adapts models from previous datasets to our\nextensive vocabulary setting. We believe this initiative will pave the way for\nnew insights and substantial progress in both the industrial and academic\nfields, pushing forward the boundaries of general 6D pose estimation.",
      "tldr_zh": "该论文介绍了 Omni6D，这是一个大规模的 3D 对象数据集，旨在提升类别级 6D object pose estimation 的性能，包括物体的平移、旋转和缩放估计。Omni6D 涵盖 166 个类别、4688 个实例和超过 80 万张 RGBD 图像，解决了现有数据集如 NOCS 的类别范围狭窄和遮挡等问题，提供更真实的场景评估。论文引入了 symmetry-aware metric，并对现有算法进行系统基准测试，揭示新挑战和见解。此外，提出了一种有效的 fine-tuning 方法，将模型从先前数据集适应到大词汇表设置，推动 6D pose estimation 在工业和学术领域的进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024 (poster). Github page: https://github.com/3DTopia/Omni6D",
      "pdf_url": "http://arxiv.org/pdf/2409.18261v3",
      "published_date": "2024-09-26 20:13:33 UTC",
      "updated_date": "2025-03-21 04:47:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:58:49.634128"
    },
    {
      "arxiv_id": "2409.18260v2",
      "title": "PCEvE: Part Contribution Evaluation Based Model Explanation for Human Figure Drawing Assessment and Beyond",
      "title_zh": "PCEvE：基于部分贡献评估的模型解释，用于人类图形绘制评估及更广泛应用",
      "authors": [
        "Jongseo Lee",
        "Geo Ahn",
        "Seong Tae Kim",
        "Jinwoo Choi"
      ],
      "abstract": "For automatic human figure drawing (HFD) assessment tasks, such as diagnosing\nautism spectrum disorder (ASD) using HFD images, the clarity and explainability\nof a model decision are crucial. Existing pixel-level attribution-based\nexplainable AI (XAI) approaches demand considerable effort from users to\ninterpret the semantic information of a region in an image, which can be often\ntime-consuming and impractical. To overcome this challenge, we propose a part\ncontribution evaluation based model explanation (PCEvE) framework. On top of\nthe part detection, we measure the Shapley Value of each individual part to\nevaluate the contribution to a model decision. Unlike existing\nattribution-based XAI approaches, the PCEvE provides a straightforward\nexplanation of a model decision, i.e., a part contribution histogram.\nFurthermore, the PCEvE expands the scope of explanations beyond the\nconventional sample-level to include class-level and task-level insights,\noffering a richer, more comprehensive understanding of model behavior. We\nrigorously validate the PCEvE via extensive experiments on multiple HFD\nassessment datasets. Also, we sanity-check the proposed method with a set of\ncontrolled experiments. Additionally, we demonstrate the versatility and\napplicability of our method to other domains by applying it to a\nphoto-realistic dataset, the Stanford Cars.",
      "tldr_zh": "本研究提出了一种基于部分贡献评估的模型解释框架PCEvE，用于自动人类图形绘制(HFD)评估任务，如诊断自闭症谱系障碍(ASD)。该框架通过检测图像中的各个部分并计算其Shapley Value，来评估每个部分对模型决策的贡献，并以直观的部分贡献直方图形式呈现解释结果，从而克服了现有像素级归因XAI方法的用户解读负担。PCEvE不仅提供样本级解释，还扩展到类级和任务级洞见；在多个HFD数据集上进行实验验证后，其有效性得到证实，并成功应用于其他领域，如Stanford Cars数据集。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This papaer is under review",
      "pdf_url": "http://arxiv.org/pdf/2409.18260v2",
      "published_date": "2024-09-26 20:13:03 UTC",
      "updated_date": "2024-10-03 22:57:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:59:01.916016"
    },
    {
      "arxiv_id": "2409.18222v1",
      "title": "Trustworthy AI: Securing Sensitive Data in Large Language Models",
      "title_zh": "可信赖 AI：在大型语言模型中保护敏感数据",
      "authors": [
        "Georgios Feretzakis",
        "Vassilios S. Verykios"
      ],
      "abstract": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) by enabling robust text generation and understanding. However, their\ndeployment in sensitive domains like healthcare, finance, and legal services\nraises critical concerns about privacy and data security. This paper proposes a\ncomprehensive framework for embedding trust mechanisms into LLMs to dynamically\ncontrol the disclosure of sensitive information. The framework integrates three\ncore components: User Trust Profiling, Information Sensitivity Detection, and\nAdaptive Output Control. By leveraging techniques such as Role-Based Access\nControl (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition\n(NER), contextual analysis, and privacy-preserving methods like differential\nprivacy, the system ensures that sensitive information is disclosed\nappropriately based on the user's trust level. By focusing on balancing data\nutility and privacy, the proposed solution offers a novel approach to securely\ndeploying LLMs in high-risk environments. Future work will focus on testing\nthis framework across various domains to evaluate its effectiveness in managing\nsensitive data while maintaining system efficiency.",
      "tldr_zh": "该论文探讨了Large Language Models (LLMs) 在敏感领域（如医疗、金融和法律）中的部署问题，强调了隐私和数据安全的潜在风险，并提出一个综合框架来嵌入信任机制以动态控制敏感信息的披露。该框架的核心组件包括User Trust Profiling、Information Sensitivity Detection 和Adaptive Output Control，利用Role-Based Access Control (RBAC)、Attribute-Based Access Control (ABAC)、Named Entity Recognition (NER)、上下文分析以及differential privacy 等技术，确保敏感信息根据用户信任级别适度披露，同时平衡数据效用和隐私。通过这一创新方法，论文为LLMs 在高风险环境中的安全应用提供了新途径，未来将通过跨领域测试评估其有效性和系统效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "40 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.18222v1",
      "published_date": "2024-09-26 19:02:33 UTC",
      "updated_date": "2024-09-26 19:02:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:59:13.742767"
    },
    {
      "arxiv_id": "2409.18219v2",
      "title": "Packet Inspection Transformer: A Self-Supervised Journey to Unseen Malware Detection with Few Samples",
      "title_zh": "翻译失败",
      "authors": [
        "Kyle Stein",
        "Arash Mahyari",
        "Guillermo Francia III",
        "Eman El-Sheikh"
      ],
      "abstract": "As networks continue to expand and become more interconnected, the need for\nnovel malware detection methods becomes more pronounced. Traditional security\nmeasures are increasingly inadequate against the sophistication of modern cyber\nattacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network\nsecurity, offering an in-depth analysis of network traffic that surpasses\nconventional monitoring techniques. DPI not only examines the metadata of\nnetwork packets, but also dives into the actual content being carried within\nthe packet payloads, providing a comprehensive view of the data flowing through\nnetworks. While the integration of advanced deep learning techniques with DPI\nhas introduced modern methodologies into malware detection and network traffic\nclassification, state-of-the-art supervised learning approaches are limited by\ntheir reliance on large amounts of annotated data and their inability to\ngeneralize to novel, unseen malware threats. To address these limitations, this\npaper leverages the recent advancements in self-supervised learning (SSL) and\nfew-shot learning (FSL). Our proposed self-supervised approach trains a\ntransformer via SSL to learn the embedding of packet content, including\npayload, from vast amounts of unlabeled data by masking portions of packets,\nleading to a learned representation that generalizes to various downstream\ntasks. Once the representation is extracted from the packets, they are used to\ntrain a malware detection algorithm. The representation obtained from the\ntransformer is then used to adapt the malware detector to novel types of\nattacks using few-shot learning approaches. Our experimental results\ndemonstrate that our method achieves classification accuracies of up to 94.76%\non the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.",
      "tldr_zh": "这篇论文提出了一种名为Packet Inspection Transformer的框架，利用自监督学习（SSL）从大量未标注的网络包数据中学习嵌入表示，通过masking portions of packets的方式来提升模型对网络流量的理解，从而解决传统监督学习在恶意软件检测中的数据依赖和泛化问题。接着，该框架结合few-shot learning (FSL)方法，使用提取的表示来训练和适应新型未知恶意软件检测算法。实验结果显示，该方法在UNSW-NB15数据集上达到94.76%的分类准确率，在CIC-IoT23数据集上达到83.25%，显著提高了对新威胁的检测性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18219v2",
      "published_date": "2024-09-26 18:55:52 UTC",
      "updated_date": "2025-02-21 18:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:59:25.094789"
    },
    {
      "arxiv_id": "2409.18216v1",
      "title": "MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following Benchmark",
      "title_zh": "MMMT-IF：一个挑战性的多模态多轮指令遵循基准",
      "authors": [
        "Elliot L. Epstein",
        "Kaisheng Yao",
        "Jing Li",
        "Xinyi Bai",
        "Hamid Palangi"
      ],
      "abstract": "Evaluating instruction following capabilities for multimodal, multi-turn\ndialogue is challenging. With potentially multiple instructions in the input\nmodel context, the task is time-consuming for human raters and we show LLM\nbased judges are biased towards answers from the same model. We propose\nMMMT-IF, an image based multi-turn Q$\\&$A evaluation set with added global\ninstructions between questions, constraining the answer format. This challenges\nmodels to retrieve instructions dispersed across long dialogues and reason\nunder instruction constraints. All instructions are objectively verifiable\nthrough code execution. We introduce the Programmatic Instruction Following\n($\\operatorname{PIF}$) metric to measure the fraction of the instructions that\nare correctly followed while performing a reasoning task. The\n$\\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by\nmeasuring the fraction of samples in a corpus where, for each sample, at least\nK out of N generated model responses achieve a $\\operatorname{PIF}$ score of\none. The $\\operatorname{PIF}$ metric aligns with human instruction following\nratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,\nGPT-4o, and Claude 3.5 Sonnet, have a $\\operatorname{PIF}$ metric that drops\nfrom 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across\nall turns, when each response is repeated 4 times ($\\operatorname{PIF-4-4}$),\nGPT-4o and Gemini successfully follow all instructions only $11\\%$ of the time.\nWhen all the instructions are also appended to the end of the model input\ncontext, the $\\operatorname{PIF}$ metric improves by 22.3 points on average,\nshowing that the challenge with the task lies not only in following the\ninstructions, but also in retrieving the instructions spread out in the model\ncontext. We plan to open source the MMMT-IF dataset and metric computation\ncode.",
      "tldr_zh": "本研究提出MMMT-IF，一种具有挑战性的多模态多轮指令遵循基准数据集，专注于图像-based问答任务，通过在对话中添加分散的全局指令来测试模型的指令检索和推理能力。论文引入了Programmatic Instruction Following (PIF)指标及其扩展PIF-N-K指标，用于客观评估模型遵循指令的比例，并通过代码执行验证其准确性；实验显示，Gemini 1.5 Pro、GPT-4o和Claude 3.5 Sonnet等模型在多轮对话中，PIF指标从第1轮的0.81降至第20轮的0.64，且指令检索是主要挑战。总体结果表明，PIF指标与人类评分相关性达60%，并计划开源数据集和指标计算代码，以推动相关研究。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.18216v1",
      "published_date": "2024-09-26 18:51:46 UTC",
      "updated_date": "2024-09-26 18:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:59:38.123989"
    },
    {
      "arxiv_id": "2409.18203v1",
      "title": "AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking",
      "title_zh": "翻译失败",
      "authors": [
        "Michelle S. Lam",
        "Fred Hohman",
        "Dominik Moritz",
        "Jeffrey P. Bigham",
        "Kenneth Holstein",
        "Mary Beth Kery"
      ],
      "abstract": "Whether a large language model policy is an explicit constitution or an\nimplicit reward model, it is challenging to assess coverage over the unbounded\nset of real-world situations that a policy must contend with. We introduce an\nAI policy design process inspired by mapmaking, which has developed tactics for\nvisualizing and iterating on maps even when full coverage is not possible. With\nPolicy Projector, policy designers can survey the landscape of model\ninput-output pairs, define custom regions (e.g., \"violence\"), and navigate\nthese regions with rules that can be applied to LLM outputs (e.g., if output\ncontains \"violence\" and \"graphic details,\" then rewrite without \"graphic\ndetails\"). Policy Projector supports interactive policy authoring using LLM\nclassification and steering and a map visualization reflecting the policy\ndesigner's work. In an evaluation with 12 AI safety experts, our system helps\npolicy designers to address problematic model behaviors extending beyond an\nexisting, comprehensive harm taxonomy.",
      "tldr_zh": "该论文提出了一种受地图制作启发的人工智能政策设计过程，旨在解决大型语言模型(LLM)政策（如宪法或奖励模型）在无限真实世界情境中覆盖评估的挑战。Policy Projector工具允许政策设计师调查模型输入-输出对、定义自定义区域（如\"violence\"）、并通过规则（如如果输出包含\"violence\"和\"graphic details\"，则重写）来导航和处理这些区域，支持交互式政策编写、LLM分类和引导，以及地图可视化。在一项涉及12名AI安全专家的评估中，该系统成功帮助设计师应对超出现有伤害分类的模型问题行为，提高了政策的全面性和有效性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18203v1",
      "published_date": "2024-09-26 18:34:16 UTC",
      "updated_date": "2024-09-26 18:34:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T03:59:49.163013"
    },
    {
      "arxiv_id": "2409.18197v1",
      "title": "Autonomous Network Defence using Reinforcement Learning",
      "title_zh": "基于强化学习的自治网络防御",
      "authors": [
        "Myles Foley",
        "Chris Hicks",
        "Kate Highnam",
        "Vasilios Mavroudis"
      ],
      "abstract": "In the network security arms race, the defender is significantly\ndisadvantaged as they need to successfully detect and counter every malicious\nattack. In contrast, the attacker needs to succeed only once. To level the\nplaying field, we investigate the effectiveness of autonomous agents in a\nrealistic network defence scenario. We first outline the problem, provide the\nbackground on reinforcement learning and detail our proposed agent design.\nUsing a network environment simulation, with 13 hosts spanning 3 subnets, we\ntrain a novel reinforcement learning agent and show that it can reliably defend\ncontinual attacks by two advanced persistent threat (APT) red agents: one with\ncomplete knowledge of the network layout and another which must discover\nresources through exploration but is more general.",
      "tldr_zh": "在网络安全领域，防御者面临巨大劣势，因为他们必须检测并抵御所有恶意攻击，而攻击者只需成功一次。为此，本文探讨了使用强化学习(Reinforcement Learning)训练自主代理来提升网络防御的有效性。研究者设计了一个新型代理，并在模拟环境中（包括13个主机和3个子网）进行训练，使其能够可靠地防御两个高级持续威胁(APT)红队代理的持续攻击：一个完全了解网络布局，另一个需通过探索发现资源。实验结果证明，该代理在防御场景中表现出色，展示了自主代理在平衡网络安全竞赛中的潜力。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18197v1",
      "published_date": "2024-09-26 18:24:09 UTC",
      "updated_date": "2024-09-26 18:24:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:00:01.167625"
    },
    {
      "arxiv_id": "2409.18170v1",
      "title": "Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review",
      "title_zh": "翻译失败",
      "authors": [
        "Emma Croxford",
        "Yanjun Gao",
        "Nicholas Pellegrino",
        "Karen K. Wong",
        "Graham Wills",
        "Elliot First",
        "Frank J. Liao",
        "Cherodeep Goswami",
        "Brian Patterson",
        "Majid Afshar"
      ],
      "abstract": "Large Language Models have advanced clinical Natural Language Generation,\ncreating opportunities to manage the volume of medical text. However, the\nhigh-stakes nature of medicine requires reliable evaluation, which remains a\nchallenge. In this narrative review, we assess the current evaluation state for\nclinical summarization tasks and propose future directions to address the\nresource constraints of expert human evaluation.",
      "tldr_zh": "这篇叙述性评论（narrative review）评估了大型语言模型（Large Language Models）在医疗领域总结任务的表现，强调了它们在临床自然语言生成（Natural Language Generation）方面的进展，以及在管理大量医疗文本时的潜力。然而，医学的高风险特性要求可靠的评估方法，而当前评估仍面临挑战。论文提出未来方向，以解决专家人类评估的资源限制，从而提升评估效率和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18170v1",
      "published_date": "2024-09-26 17:58:26 UTC",
      "updated_date": "2024-09-26 17:58:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:00:14.700220"
    },
    {
      "arxiv_id": "2409.18119v2",
      "title": "Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography",
      "title_zh": "翻译失败",
      "authors": [
        "Yuexi Du",
        "John Onofrey",
        "Nicha C. Dvornek"
      ],
      "abstract": "Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential\nin medical image analysis but requires substantial data and computational\nresources. Due to these restrictions, existing CLIP applications in medical\nimaging focus mainly on modalities like chest X-rays that have abundant\nimage-report data available, leaving many other important modalities\nunderexplored. Here, we propose one of the first adaptations of the full CLIP\nmodel to mammography, which presents significant challenges due to labeled data\nscarcity, high-resolution images with small regions of interest, and class-wise\nimbalance. We first develop a specialized supervision framework for mammography\nthat leverages its multi-view nature. Furthermore, we design a symmetric local\nalignment module to better focus on detailed features in high-resolution\nimages. Lastly, we incorporate a parameter-efficient fine-tuning approach for\nlarge language models pre-trained with medical knowledge to address data\nlimitations. Our multi-view and multi-scale alignment (MaMA) method outperforms\nstate-of-the-art baselines for three different tasks on two large real-world\nmammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared\nwith the largest baseline. The code is available at\nhttps://github.com/XYPB/MaMA",
      "tldr_zh": "该研究针对对比语言图像预训练（CLIP）在医疗图像分析中的数据和计算资源需求，首次将其完整模型应用于乳腺X光摄影，解决数据稀缺、高分辨率图像和小兴趣区域等挑战。作者提出MaMA（Multi-View and Multi-Scale Alignment）方法，包括利用乳腺X光的multi-view特性设计监督框架、对称local alignment模块聚焦详细特征，以及parameter-efficient fine-tuning策略以应对数据限制。在两个大型真实数据集（EMBED和RSNA-Mammo）上，MaMA在三个任务中超越最先进基线，同时模型大小仅为最大基线的52%。这为CLIP在更多医疗领域的应用提供了高效适应路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is accepted by IPMI 2025 for Oral Presentation",
      "pdf_url": "http://arxiv.org/pdf/2409.18119v2",
      "published_date": "2024-09-26 17:56:59 UTC",
      "updated_date": "2025-03-27 17:39:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:00:27.169854"
    },
    {
      "arxiv_id": "2409.18169v5",
      "title": "Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Fatih Ilhan",
        "Selim Furkan Tekin",
        "Ling Liu"
      ],
      "abstract": "Recent research demonstrates that the nascent fine-tuning-as-a-service\nbusiness model exposes serious safety concerns -- fine-tuning over a few\nharmful data uploaded by the users can compromise the safety alignment of the\nmodel. The attack, known as harmful fine-tuning attack, has raised a broad\nresearch interest among the community. However, as the attack is still new,\n\\textbf{we observe that there are general misunderstandings within the research\ncommunity.} To clear up concern, this paper provide a comprehensive overview to\nthree aspects of harmful fine-tuning: attacks setting, defense design and\nevaluation methodology. Specifically, we first present the threat model of the\nproblem, and introduce the harmful fine-tuning attack and its variants. Then we\nsystematically survey the existing literature on attacks/defenses/mechanical\nanalysis of the problem. Finally, we introduce the evaluation methodology and\noutline future research directions that might contribute to the development of\nthe field. Additionally, we present a list of questions of interest, which\nmight be useful to refer to when reviewers in the peer review process question\nthe realism of the experiment/attack/defense setting. A curated list of\nrelevant papers is maintained and made accessible at:\nhttps://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers.",
      "tldr_zh": "本调查论文探讨了Large Language Models (LLMs) 中有害微调攻击及其防御问题，强调了微调服务模型因用户上传有害数据而可能丧失安全对齐的风险。论文首先定义了威胁模型，介绍了有害微调攻击的变体，并系统回顾了现有文献中的攻击、防御和机制分析。最终，它提出了评估方法、未来研究方向，并提供了一个相关论文列表（https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers），以澄清研究社区的常见误解并推动该领域的进展。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18169v5",
      "published_date": "2024-09-26 17:55:22 UTC",
      "updated_date": "2024-12-03 06:52:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:00:48.401952"
    },
    {
      "arxiv_id": "2409.18104v1",
      "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats",
      "title_zh": "翻译失败",
      "authors": [
        "Lucia Gordon",
        "Nikhil Behari",
        "Samuel Collier",
        "Elizabeth Bondi-Kelly",
        "Jackson A. Killian",
        "Catherine Ressijac",
        "Peter Boucher",
        "Andrew Davies",
        "Milind Tambe"
      ],
      "abstract": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7.",
      "tldr_zh": "该论文提出了一种创新方法，通过映射南非犀牛栖息地的排泄点（middens）来间接监测犀牛的空间行为，从而支持反偷猎、管理和再引入努力，而非直接追踪难寻的犀牛。研究使用遥感热成像、RGB 和 LiDAR 多模态图像构建分类器，并在主动学习设置中开发了 MultimodAL 系统，该系统采用排名技术和多模态方法，处理极端类不平衡问题，并在减少 94% 标签的情况下实现与被动学习模型相当的性能，从而节省超过 76 小时的标注时间。意外发现显示，犀牛 middens 并非随机分布，而是呈现聚类模式，因此建议将反偷猎资源集中在高密度区域，以符合 UN Target 15.7 的目标。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 9 figures, IJCAI 2023 Special Track on AI for Good",
      "pdf_url": "http://arxiv.org/pdf/2409.18104v1",
      "published_date": "2024-09-26 17:49:20 UTC",
      "updated_date": "2024-09-26 17:49:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:00:51.522842"
    },
    {
      "arxiv_id": "2409.18101v1",
      "title": "AI-Powered Augmented Reality for Satellite Assembly, Integration and Test",
      "title_zh": "人工智能驱动的增强现实用于卫星组装、集成和测试",
      "authors": [
        "Alvaro Patricio",
        "Joao Valente",
        "Atabak Dehban",
        "Ines Cadilha",
        "Daniel Reis",
        "Rodrigo Ventura"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is\nset to transform satellite Assembly, Integration, and Testing (AIT) processes\nby enhancing precision, minimizing human error, and improving operational\nefficiency in cleanroom environments. This paper presents a technical\ndescription of the European Space Agency's (ESA) project \"AI for AR in\nSatellite AIT,\" which combines real-time computer vision and AR systems to\nassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2\nas the AR interface, the system delivers context-aware instructions and\nreal-time feedback, tackling the complexities of object recognition and 6D pose\nestimation in AIT workflows. All AI models demonstrated over 70% accuracy, with\nthe detection model exceeding 95% accuracy, indicating a high level of\nperformance and reliability. A key contribution of this work lies in the\neffective use of synthetic data for training AI models in AR applications,\naddressing the significant challenges of obtaining real-world datasets in\nhighly dynamic satellite environments, as well as the creation of the Segmented\nAnything Model for Automatic Labelling (SAMAL), which facilitates the automatic\nannotation of real data, achieving speeds up to 20 times faster than manual\nhuman annotation. The findings demonstrate the efficacy of AI-driven AR systems\nin automating critical satellite assembly tasks, setting a foundation for\nfuture innovations in the space industry.",
      "tldr_zh": "本论文介绍了欧洲航天局(ESA)的项目“AI for AR in Satellite AIT”，利用人工智能(AI)和增强现实(AR)技术来提升卫星组装、集成和测试(AIT)过程的精度、效率和减少人为错误。系统通过Microsoft HoloLens 2提供实时计算机视觉、上下文感知指令和反馈，处理对象识别及6D pose estimation等复杂任务，并采用合成数据训练AI模型以克服真实数据集获取的挑战。关键贡献包括开发Segmented Anything Model for Automatic Labelling (SAMAL)，使数据自动标注速度比手动快20倍，且AI模型整体准确率超过70%，检测模型达95%以上，为航天行业自动化任务奠定基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T05, 68U20",
        "I.2.1; H.5.2; I.4.8; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18101v1",
      "published_date": "2024-09-26 17:44:52 UTC",
      "updated_date": "2024-09-26 17:44:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:01:02.161889"
    },
    {
      "arxiv_id": "2409.18099v1",
      "title": "EfficientCrackNet: A Lightweight Model for Crack Segmentation",
      "title_zh": "EfficientCrackNet：一种轻量级裂缝分割模型",
      "authors": [
        "Abid Hasan Zim",
        "Aquib Iqbal",
        "Zaid Al-Huda",
        "Asad Malik",
        "Minoru Kuribayash"
      ],
      "abstract": "Crack detection, particularly from pavement images, presents a formidable\nchallenge in the domain of computer vision due to several inherent complexities\nsuch as intensity inhomogeneity, intricate topologies, low contrast, and noisy\nbackgrounds. Automated crack detection is crucial for maintaining the\nstructural integrity of essential infrastructures, including buildings,\npavements, and bridges. Existing lightweight methods often face challenges\nincluding computational inefficiency, complex crack patterns, and difficult\nbackgrounds, leading to inaccurate detection and impracticality for real-world\napplications. To address these limitations, we propose EfficientCrackNet, a\nlightweight hybrid model combining Convolutional Neural Networks (CNNs) and\ntransformers for precise crack segmentation. EfficientCrackNet integrates\ndepthwise separable convolutions (DSC) layers and MobileViT block to capture\nboth global and local features. The model employs an Edge Extraction Method\n(EEM) and for efficient crack edge detection without pretraining, and\nUltra-Lightweight Subspace Attention Module (ULSAM) to enhance feature\nextraction. Extensive experiments on three benchmark datasets Crack500,\nDeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superior\nperformance compared to existing lightweight models, while requiring only 0.26M\nparameters, and 0.483 FLOPs (G). The proposed model offers an optimal balance\nbetween accuracy and computational efficiency, outperforming state-of-the-art\nlightweight models, and providing a robust and adaptable solution for\nreal-world crack segmentation.",
      "tldr_zh": "本文提出 EfficientCrackNet，一种轻量级混合模型，结合 Convolutional Neural Networks (CNNs) 和 Transformers，用于精确路面裂缝分割，解决现有方法在计算效率、复杂模式和嘈杂背景方面的不足。该模型整合 Depthwise Separable Convolutions (DSC) 层、MobileViT 块、Edge Extraction Method (EEM) 和 Ultra-Lightweight Subspace Attention Module (ULSAM)，以高效捕获全局和局部特征，并在无需预训练的情况下提升裂缝边缘检测性能。在 Crack500、DeepCrack 和 GAPs384 等基准数据集上的实验表明，EfficientCrackNet 仅需 0.26M 参数和 0.483 G FLOPs，便实现了优于现有轻量级模型的性能，提供了一个准确性与计算效率平衡的鲁棒解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18099v1",
      "published_date": "2024-09-26 17:44:20 UTC",
      "updated_date": "2024-09-26 17:44:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:01:15.956968"
    },
    {
      "arxiv_id": "2409.18092v2",
      "title": "DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models",
      "title_zh": "DiffSSC：使用去噪扩散概率模型的语义 LiDAR 扫描完成",
      "authors": [
        "Helin Cao",
        "Sven Behnke"
      ],
      "abstract": "Perception systems play a crucial role in autonomous driving, incorporating\nmultiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors\nare widely used to capture sparse point clouds of the vehicle's surroundings.\nHowever, such systems struggle to perceive occluded areas and gaps in the scene\ndue to the sparsity of these point clouds and their lack of semantics. To\naddress these challenges, Semantic Scene Completion (SSC) jointly predicts\nunobserved geometry and semantics in the scene given raw LiDAR measurements,\naiming for a more complete scene representation. Building on promising results\nof diffusion models in image generation and super-resolution tasks, we propose\ntheir extension to SSC by implementing the noising and denoising diffusion\nprocesses in the point and semantic spaces individually. To control the\ngeneration, we employ semantic LiDAR point clouds as conditional input and\ndesign local and global regularization losses to stabilize the denoising\nprocess. We evaluate our approach on autonomous driving datasets and our\napproach outperforms the state-of-the-art for SSC.",
      "tldr_zh": "该论文提出DiffSSC方法，利用Denoising Diffusion Probabilistic Models来完成语义LiDAR扫描，旨在解决LiDAR点云稀疏性和缺乏语义导致的感知难题。方法通过在点空间和语义空间中分别实现噪声化和去噪过程，并以语义LiDAR点云作为条件输入，同时引入局部和全局正则化损失来稳定生成过程。相比传统方法，DiffSSC能够更准确地预测场景中的未观察几何形状和语义。实验结果显示，该方法在自动驾驶数据集上超过了现有最先进技术的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2409.18092v2",
      "published_date": "2024-09-26 17:39:05 UTC",
      "updated_date": "2024-09-30 18:14:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:01:26.782709"
    },
    {
      "arxiv_id": "2410.03566v1",
      "title": "A Survey on Offensive AI Within Cybersecurity",
      "title_zh": "翻译失败",
      "authors": [
        "Sahil Girhepuje",
        "Aviral Verma",
        "Gaurav Raina"
      ],
      "abstract": "Artificial Intelligence (AI) has witnessed major growth and integration\nacross various domains. As AI systems become increasingly prevalent, they also\nbecome targets for threat actors to manipulate their functionality for\nmalicious purposes. This survey paper on offensive AI will comprehensively\ncover various aspects related to attacks against and using AI systems. It will\ndelve into the impact of offensive AI practices on different domains, including\nconsumer, enterprise, and public digital infrastructure. The paper will explore\nadversarial machine learning, attacks against AI models, infrastructure, and\ninterfaces, along with offensive techniques like information gathering, social\nengineering, and weaponized AI. Additionally, it will discuss the consequences\nand implications of offensive AI, presenting case studies, insights, and\navenues for further research.",
      "tldr_zh": "这篇调查论文探讨了offensive AI在网络安全中的应用，系统地分析了攻击AI系统及其基础设施的各种方法，包括adversarial machine learning、针对AI模型和接口的攻击，以及offensive技术如信息收集、社会工程和武器化AI。论文考察了这些攻击对消费者、企业和公共数字基础设施的影响，强调了潜在后果并通过案例研究提供实际洞见。最后，它指出了进一步研究的途径，为提升AI安全性和防御策略提供了宝贵参考。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03566v1",
      "published_date": "2024-09-26 17:36:22 UTC",
      "updated_date": "2024-09-26 17:36:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:01:37.198587"
    },
    {
      "arxiv_id": "2409.18164v2",
      "title": "Data-Prep-Kit: getting your data ready for LLM application development",
      "title_zh": "翻译失败",
      "authors": [
        "David Wood",
        "Boris Lublinsky",
        "Alexy Roytman",
        "Shivdeep Singh",
        "Constantin Adam",
        "Abdulhamid Adebayo",
        "Sungeun An",
        "Yuan Chi Chang",
        "Xuan-Hong Dang",
        "Nirmit Desai",
        "Michele Dolfi",
        "Hajar Emami-Gohari",
        "Revital Eres",
        "Takuya Goto",
        "Dhiraj Joshi",
        "Yan Koyfman",
        "Mohammad Nassar",
        "Hima Patel",
        "Paramesvaran Selvam",
        "Yousaf Shah",
        "Saptha Surendran",
        "Daiki Tsuzuku",
        "Petros Zerfos",
        "Shahrokh Daijavad"
      ],
      "abstract": "Data preparation is the first and a very important step towards any Large\nLanguage Model (LLM) development. This paper introduces an easy-to-use,\nextensible, and scale-flexible open-source data preparation toolkit called Data\nPrep Kit (DPK). DPK is architected and designed to enable users to scale their\ndata preparation to their needs. With DPK they can prepare data on a local\nmachine or effortlessly scale to run on a cluster with thousands of CPU Cores.\nDPK comes with a highly scalable, yet extensible set of modules that transform\nnatural language and code data. If the user needs additional transforms, they\ncan be easily developed using extensive DPK support for transform creation.\nThese modules can be used independently or pipelined to perform a series of\noperations. In this paper, we describe DPK architecture and show its\nperformance from a small scale to a very large number of CPUs. The modules from\nDPK have been used for the preparation of Granite Models [1] [2]. We believe\nDPK is a valuable contribution to the AI community to easily prepare data to\nenhance the performance of their LLM models or to fine-tune models with\nRetrieval-Augmented Generation (RAG).",
      "tldr_zh": "这篇论文介绍了 Data-Prep-Kit (DPK)，一个开源工具包，旨在简化 Large Language Model (LLM) 应用开发中的数据准备过程。DPK 采用可扩展架构，支持从本地机器到数千 CPU 核心集群的灵活扩展，并提供一组高度可扩展模块，用于转换自然语言和代码数据，这些模块可独立使用或通过管道化组合。实验结果展示了 DPK 在不同规模下的高效性能，已应用于 Granite Models 的数据准备，并有助于提升 LLM 模型的表现或支持 Retrieval-Augmented Generation (RAG) 微调。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.18164v2",
      "published_date": "2024-09-26 17:30:28 UTC",
      "updated_date": "2024-11-13 00:15:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:01:49.810751"
    },
    {
      "arxiv_id": "2409.18163v2",
      "title": "A Survey on Neural Architecture Search Based on Reinforcement Learning",
      "title_zh": "基于强化学习的神经架构搜索综述",
      "authors": [
        "Wenzhu Shao"
      ],
      "abstract": "The automation of feature extraction of machine learning has been\nsuccessfully realized by the explosive development of deep learning. However,\nthe structures and hyperparameters of deep neural network architectures also\nmake huge difference on the performance in different tasks. The process of\nexploring optimal structures and hyperparameters often involves a lot of\ntedious human intervene. As a result, a legitimate question is to ask for the\nautomation of searching for optimal network structures and hyperparameters. The\nwork of automation of exploring optimal hyperparameters is done by\nHyperparameter Optimization. Neural Architecture Search is aimed to\nautomatically find the best network structure given specific tasks. In this\npaper, we firstly introduced the overall development of Neural Architecture\nSearch and then focus mainly on providing an overall and understandable survey\nabout Neural Architecture Search works that are relevant with reinforcement\nlearning, including improvements and variants based on the hope of satisfying\nmore complex structures and resource-insufficient environment.",
      "tldr_zh": "这篇论文对基于强化学习(Reinforcement Learning)的神经架构搜索(Neural Architecture Search)进行了全面调查。首先，它回顾了NAS的整体发展背景，强调了自动化搜索最优网络结构和超参数的重要性，以减少人工干预和提升任务性能。论文重点介绍了与强化学习相关的改进和变体，这些方法旨在处理更复杂的网络结构和资源不足的环境。总体而言，该调查为研究者提供了易懂的总结，帮助推进NAS领域的自动化探索。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18163v2",
      "published_date": "2024-09-26 17:28:10 UTC",
      "updated_date": "2024-09-30 06:51:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:02:01.405123"
    },
    {
      "arxiv_id": "2409.18084v2",
      "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model",
      "title_zh": "翻译失败",
      "authors": [
        "Shangyi Luo",
        "Ji Zhu",
        "Peng Sun",
        "Yuhong Deng",
        "Cunjun Yu",
        "Anxing Xiao",
        "Xueqian Wang"
      ],
      "abstract": "With the increasing presence of service robots and autonomous vehicles in\nhuman environments, navigation systems need to evolve beyond simple destination\nreach to incorporate social awareness. This paper introduces GSON, a novel\ngroup-based social navigation framework that leverages Large Multimodal Models\n(LMMs) to enhance robots' social perception capabilities. Our approach uses\nvisual prompting to enable zero-shot extraction of social relationships among\npedestrians and integrates these results with robust pedestrian detection and\ntracking pipelines to overcome the inherent inference speed limitations of\nLMMs. The planning system incorporates a mid-level planner that sits between\nglobal path planning and local motion planning, effectively preserving both\nglobal context and reactive responsiveness while avoiding disruption of the\npredicted social group. We validate GSON through extensive real-world mobile\nrobot navigation experiments involving complex social scenarios such as\nqueuing, conversations, and photo sessions. Comparative results show that our\nsystem significantly outperforms existing navigation approaches in minimizing\nsocial perturbations while maintaining comparable performance on traditional\nnavigation metrics.",
      "tldr_zh": "本研究提出GSON，一种基于群组的社交导航框架，利用Large Multimodal Models (LMMs)提升机器人对行人社交关系的感知能力。框架通过视觉提示实现零-shot extraction of social relationships，并将其与行人检测和跟踪管道整合，同时引入中间级规划器以平衡全局路径规划和本地运动规划，避免干扰社交群组。实验在真实世界场景（如排队、对话和拍照）中验证，GSON显著减少社交干扰，同时在传统导航指标上保持与现有方法相当的性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18084v2",
      "published_date": "2024-09-26 17:27:15 UTC",
      "updated_date": "2025-04-08 06:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:02:13.524059"
    },
    {
      "arxiv_id": "2409.18082v2",
      "title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation",
      "title_zh": "SKT：将状态感知关键点轨迹与视觉语言模型整合，用于机器人服装操控",
      "authors": [
        "Xin Li",
        "Siyuan Huang",
        "Qiaojun Yu",
        "Zhengkai Jiang",
        "Ce Hao",
        "Yimeng Zhu",
        "Hongsheng Li",
        "Peng Gao",
        "Cewu Lu"
      ],
      "abstract": "Automating garment manipulation poses a significant challenge for assistive\nrobotics due to the diverse and deformable nature of garments. Traditional\napproaches typically require separate models for each garment type, which\nlimits scalability and adaptability. In contrast, this paper presents a unified\napproach using vision-language models (VLMs) to improve keypoint prediction\nacross various garment categories. By interpreting both visual and semantic\ninformation, our model enables robots to manage different garment states with a\nsingle model. We created a large-scale synthetic dataset using advanced\nsimulation techniques, allowing scalable training without extensive real-world\ndata. Experimental results indicate that the VLM-based method significantly\nenhances keypoint detection accuracy and task success rates, providing a more\nflexible and general solution for robotic garment manipulation. In addition,\nthis research also underscores the potential of VLMs to unify various garment\nmanipulation tasks within a single framework, paving the way for broader\napplications in home automation and assistive robotics for future.",
      "tldr_zh": "本文提出SKT框架，将State-Aware Keypoint Trajectories与Vision-Language Models (VLMs)整合，用于机器人衣物操作，以应对衣物多样性和变形带来的挑战。相比传统方法，该框架通过解释视觉和语义信息，并利用大规模合成数据集进行训练，实现了单一模型对多种衣物状态的统一管理。实验结果显示，SKT显著提升了关键点检测准确性和任务成功率，为家庭自动化和辅助机器人领域提供了更灵活、可扩展的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18082v2",
      "published_date": "2024-09-26 17:26:16 UTC",
      "updated_date": "2024-10-07 12:06:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:02:26.423489"
    },
    {
      "arxiv_id": "2409.18073v1",
      "title": "Infer Human's Intentions Before Following Natural Language Instructions",
      "title_zh": "在遵循自然语言指令之前推断人类的意图",
      "authors": [
        "Yanming Wan",
        "Yue Wu",
        "Yiping Wang",
        "Jiayuan Mao",
        "Natasha Jaques"
      ],
      "abstract": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat.",
      "tldr_zh": "这篇论文探讨了 AI 代理在遵循自然语言指令时面临的模糊性问题，因为人类指令往往隐含了未明说的目标和意图。作者提出了一种新框架 FISER（Follow Instructions with Social and Embodied Reasoning），通过显式推断人类意图作为中间推理步骤，并结合社会和身体推理来提升协作任务中的指令遵循能力。实验结果显示，基于 Transformer 的模型在 HandMeThat 基准上超过了纯端到端方法和 Chain of Thought 提示的基线，达到了 state-of-the-art 性能。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18073v1",
      "published_date": "2024-09-26 17:19:49 UTC",
      "updated_date": "2024-09-26 17:19:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:02:37.993964"
    },
    {
      "arxiv_id": "2409.18162v1",
      "title": "The Nexus of AR/VR, Large Language Models, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children: A Systematic Review",
      "title_zh": "翻译失败",
      "authors": [
        "Biplov Paneru",
        "Bishwash Paneru"
      ],
      "abstract": "The combination of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is examined in\nthis review study. 150 publications were found by a thorough literature search\nthroughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 42 of them\nwere chosen for in-depth study due to their methodological rigor and relevance.\nThree primary areas are covered in this review: how AR can improve social and\nlearning results; how LLMs can help with communication; and how UI/UX design\naffects how effective these technologies are. Results reveal that while LLMs\ncan provide individualized learning and communication support, AR has\ndemonstrated promise in enhancing social skills, motivation, and attention. For\nchildren with ASD, accessible and interesting interventions depend heavily on\neffective UI/UX design. To optimize the benefits of these technologies in ASD\ntherapies, the study emphasizes the need for additional research to address\ndifficulties related to customization, accessibility, and integration.",
      "tldr_zh": "这篇系统综述探讨了AR/VR、Large Language Models (LLMs)、UI/UX和机器人技术在提升儿童学习和社会互动方面的作用，特别是针对autism spectrum disorder (ASD)儿童。通过搜索150篇文献并选择42篇进行深入分析，结果显示AR能显著改善社交技能、动机和注意力，而LLMs提供个性化的学习和沟通支持。UI/UX设计被证明对这些干预的效用至关重要，但研究强调需要进一步研究来解决定制、访问性和整合的挑战。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.HC",
      "comment": "none",
      "pdf_url": "http://arxiv.org/pdf/2409.18162v1",
      "published_date": "2024-09-26 17:19:25 UTC",
      "updated_date": "2024-09-26 17:19:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:02:50.036744"
    },
    {
      "arxiv_id": "2409.18071v1",
      "title": "FreeEdit: Mask-free Reference-based Image Editing with Multi-modal Instruction",
      "title_zh": "FreeEdit: 基于多模态指令的无掩码参考图像编辑",
      "authors": [
        "Runze He",
        "Kai Ma",
        "Linjiang Huang",
        "Shaofei Huang",
        "Jialin Gao",
        "Xiaoming Wei",
        "Jiao Dai",
        "Jizhong Han",
        "Si Liu"
      ],
      "abstract": "Introducing user-specified visual concepts in image editing is highly\npractical as these concepts convey the user's intent more precisely than\ntext-based descriptions. We propose FreeEdit, a novel approach for achieving\nsuch reference-based image editing, which can accurately reproduce the visual\nconcept from the reference image based on user-friendly language instructions.\nOur approach leverages the multi-modal instruction encoder to encode language\ninstructions to guide the editing process. This implicit way of locating the\nediting area eliminates the need for manual editing masks. To enhance the\nreconstruction of reference details, we introduce the Decoupled Residual\nReferAttention (DRRA) module. This module is designed to integrate fine-grained\nreference features extracted by a detail extractor into the image editing\nprocess in a residual way without interfering with the original self-attention.\nGiven that existing datasets are unsuitable for reference-based image editing\ntasks, particularly due to the difficulty in constructing image triplets that\ninclude a reference image, we curate a high-quality dataset, FreeBench, using a\nnewly developed twice-repainting scheme. FreeBench comprises the images before\nand after editing, detailed editing instructions, as well as a reference image\nthat maintains the identity of the edited object, encompassing tasks such as\nobject addition, replacement, and deletion. By conducting phased training on\nFreeBench followed by quality tuning, FreeEdit achieves high-quality zero-shot\nediting through convenient language instructions. We conduct extensive\nexperiments to evaluate the effectiveness of FreeEdit across multiple task\ntypes, demonstrating its superiority over existing methods. The code will be\navailable at: https://freeedit.github.io/.",
      "tldr_zh": "该研究提出 FreeEdit，一种无需掩码的基于参考图像的图像编辑方法，利用多模态指令编码器来编码用户语言指令，精确引导编辑过程并再现参考视觉概念。\n为了提升参考细节的重建，引入 Decoupled Residual ReferAttention (DRRA) 模块，该模块以残差方式集成细粒度参考特征，而不干扰原自注意力机制。\n作者构建了新数据集 FreeBench，通过两次重绘方案提供高质量图像三元组，包括编辑前后图像、详细指令和参考图像，覆盖对象添加、替换和删除等任务。\n实验结果表明，FreeEdit 通过分阶段训练和质量调整，在多种任务上优于现有方法，实现高效的零样本编辑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 14 figures, project website: https://freeedit.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2409.18071v1",
      "published_date": "2024-09-26 17:18:39 UTC",
      "updated_date": "2024-09-26 17:18:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:03:02.873334"
    },
    {
      "arxiv_id": "2409.18055v2",
      "title": "Visual Data Diagnosis and Debiasing with Concept Graphs",
      "title_zh": "基于概念图的视觉数据诊断与去偏置",
      "authors": [
        "Rwiddhi Chakraborty",
        "Yinong Wang",
        "Jialu Gao",
        "Runkai Zheng",
        "Cheng Zhang",
        "Fernando De la Torre"
      ],
      "abstract": "The widespread success of deep learning models today is owed to the curation\nof extensive datasets significant in size and complexity. However, such models\nfrequently pick up inherent biases in the data during the training process,\nleading to unreliable predictions. Diagnosing and debiasing datasets is thus a\nnecessity to ensure reliable model performance. In this paper, we present\nConBias, a novel framework for diagnosing and mitigating Concept co-occurrence\nBiases in visual datasets. ConBias represents visual datasets as knowledge\ngraphs of concepts, enabling meticulous analysis of spurious concept\nco-occurrences to uncover concept imbalances across the whole dataset.\nMoreover, we show that by employing a novel clique-based concept balancing\nstrategy, we can mitigate these imbalances, leading to enhanced performance on\ndownstream tasks. Extensive experiments show that data augmentation based on a\nbalanced concept distribution augmented by Conbias improves generalization\nperformance across multiple datasets compared to state-of-the-art methods.",
      "tldr_zh": "本研究提出ConBias框架，用于诊断和缓解视觉数据集中的概念共现偏差（Concept co-occurrence Biases）。框架将数据集表示为概念知识图（knowledge graphs），通过分析整个数据集中的虚假概念共现来识别概念不平衡问题。利用一种新的基于团（clique-based）的概念平衡策略进行数据增强，显著提高了下游任务的泛化性能，并在多个数据集上比现有最先进方法表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18055v2",
      "published_date": "2024-09-26 16:59:01 UTC",
      "updated_date": "2024-11-11 12:56:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:03:12.214542"
    },
    {
      "arxiv_id": "2409.18053v3",
      "title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Dingrui Wang",
        "Marc Kaufeld",
        "Johannes Betz"
      ],
      "abstract": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD.",
      "tldr_zh": "本文提出DualAD框架，用于模仿人类推理的自动驾驶系统，该框架采用双层设计：底层是基于规则的运动规划器处理常规任务，而上层通过规则-based文本编码器将场景转换为文本，由大型语言模型(LLM)进行决策，并在检测到潜在危险时干预底层操作。实验结果显示，DualAD使用零-shot预训练模型在闭环测试中显著优于缺乏推理能力的规则-based规划器，并证明文本编码器提升了场景理解能力。随着更强LLM的集成，该框架显示出进一步优化的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Autonomous Driving, Large Language Models (LLMs), Human Reasoning,\n  Critical Scenario",
      "pdf_url": "http://arxiv.org/pdf/2409.18053v3",
      "published_date": "2024-09-26 16:58:04 UTC",
      "updated_date": "2024-12-04 10:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:03:25.693972"
    },
    {
      "arxiv_id": "2409.18052v2",
      "title": "Explaining Explaining",
      "title_zh": "翻译失败",
      "authors": [
        "Sergei Nirenburg",
        "Marjorie McShane",
        "Kenneth W. Goodman",
        "Sanjay Oruganti"
      ],
      "abstract": "Explanation is key to people having confidence in high-stakes AI systems.\nHowever, machine-learning-based systems -- which account for almost all current\nAI -- can't explain because they are usually black boxes. The explainable AI\n(XAI) movement hedges this problem by redefining \"explanation\". The\nhuman-centered explainable AI (HCXAI) movement identifies the\nexplanation-oriented needs of users but can't fulfill them because of its\ncommitment to machine learning. In order to achieve the kinds of explanations\nneeded by real people operating in critical domains, we must rethink how to\napproach AI. We describe a hybrid approach to developing cognitive agents that\nuses a knowledge-based infrastructure supplemented by data obtained through\nmachine learning when applicable. These agents will serve as assistants to\nhumans who will bear ultimate responsibility for the decisions and actions of\nthe human-robot team. We illustrate the explanatory potential of such agents\nusing the under-the-hood panels of a demonstration system in which a team of\nsimulated robots collaborate on a search task assigned by a human.",
      "tldr_zh": "这篇论文探讨了解释（explanation）在高风险 AI 系统中的关键作用，指出机器学习（machine learning）系统通常是黑盒子，无法提供有效解释，而 Explainable AI (XAI) 和 Human-Centered Explainable AI (HCXAI) 虽关注用户需求，却因依赖机器学习而无法完全满足。作者提出一种混合方法，通过知识基础（knowledge-based infrastructure）补充机器学习数据，开发认知代理（cognitive agents）作为人类决策助手，确保在关键领域提供可解释性支持。论文以一个模拟机器人搜索任务的演示系统为例，展示了这种代理的解释潜力，为构建可信任的人机团队奠定基础。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18052v2",
      "published_date": "2024-09-26 16:55:44 UTC",
      "updated_date": "2024-09-27 02:09:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:03:37.259759"
    },
    {
      "arxiv_id": "2409.18049v1",
      "title": "Revisit Anything: Visual Place Recognition via Image Segment Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Kartik Garg",
        "Sai Shubodh Puligilla",
        "Shishir Kolathaya",
        "Madhava Krishna",
        "Sourav Garg"
      ],
      "abstract": "Accurately recognizing a revisited place is crucial for embodied agents to\nlocalize and navigate. This requires visual representations to be distinct,\ndespite strong variations in camera viewpoint and scene appearance. Existing\nvisual place recognition pipelines encode the \"whole\" image and search for\nmatches. This poses a fundamental challenge in matching two images of the same\nplace captured from different camera viewpoints: \"the similarity of what\noverlaps can be dominated by the dissimilarity of what does not overlap\". We\naddress this by encoding and searching for \"image segments\" instead of the\nwhole images. We propose to use open-set image segmentation to decompose an\nimage into `meaningful' entities (i.e., things and stuff). This enables us to\ncreate a novel image representation as a collection of multiple overlapping\nsubgraphs connecting a segment with its neighboring segments, dubbed\nSuperSegment. Furthermore, to efficiently encode these SuperSegments into\ncompact vector representations, we propose a novel factorized representation of\nfeature aggregation. We show that retrieving these partial representations\nleads to significantly higher recognition recall than the typical whole image\nbased retrieval. Our segments-based approach, dubbed SegVLAD, sets a new\nstate-of-the-art in place recognition on a diverse selection of benchmark\ndatasets, while being applicable to both generic and task-specialized image\nencoders. Finally, we demonstrate the potential of our method to ``revisit\nanything'' by evaluating our method on an object instance retrieval task, which\nbridges the two disparate areas of research: visual place recognition and\nobject-goal navigation, through their common aim of recognizing goal objects\nspecific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.",
      "tldr_zh": "该论文针对视觉场所识别（Visual Place Recognition）中的视角和外观变化问题，提出了一种基于图像片段检索的新方法，称为Revisit Anything，以提高识别准确性。作者使用开放集图像分割（open-set image segmentation）将图像分解为有意义的实体，并创建SuperSegment表示，将每个片段与其邻近片段连接成重叠子图，然后通过因子化表示（factorized representation of feature aggregation）高效编码这些片段。实验结果显示，该方法SegVLAD在多个基准数据集上实现了新的最先进性能，比传统整体图像检索方法显著提升识别召回率。最后，该方法扩展到对象实例检索任务，桥接了视觉场所识别与对象目标导航领域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Presented at ECCV 2024; Includes supplementary; 29 pages; 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.18049v1",
      "published_date": "2024-09-26 16:49:58 UTC",
      "updated_date": "2024-09-26 16:49:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:03:50.510705"
    },
    {
      "arxiv_id": "2409.18047v2",
      "title": "HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams",
      "title_zh": "HARMONIC：人-机器人团队中的",
      "authors": [
        "Sanjay Oruganti",
        "Sergei Nirenburg",
        "Marjorie McShane",
        "Jesse English",
        "Michael K. Roberts",
        "Christian Arndt",
        "Sahithi Kamireddy"
      ],
      "abstract": "This paper introduces HARMONIC, a cognitive-robotic architecture that\nintegrates the OntoAgent cognitive framework with general-purpose robot control\nsystems applied to human-robot teaming (HRT). We also present a cognitive\nstrategy for robots that incorporates metacognition, natural language\ncommunication, and explainability capabilities required for collaborative\npartnerships in HRT. Through simulation experiments involving a joint search\ntask performed by a heterogeneous team of a UGV, a drone, and a human operator,\nwe demonstrate the system's ability to coordinate actions between robots with\nheterogeneous capabilities, adapt to complex scenarios, and facilitate natural\nhuman-robot communication. Evaluation results show that robots using the\nOntoAgent architecture within the HARMONIC framework can reason about plans,\ngoals, and team member attitudes while providing clear explanations for their\ndecisions, which are essential prerequisites for realistic human-robot teaming.",
      "tldr_zh": "这篇论文介绍了 HARMONIC 架构，它将 OntoAgent 认知框架与通用机器人控制系统整合，用于人类-机器人团队 (HRT)，并提出一种机器人认知策略，包括元认知、自然语言通信和可解释性，以增强协作伙伴关系。通过模拟实验，涉及 UGV、无人机和人类操作员的联合搜索任务，系统展示了协调异构机器人行动、适应复杂场景以及促进自然通信的能力。评估结果表明，使用 OntoAgent 的机器人能在 HARMONIC 框架下有效推理计划、目标和团队成员态度，并提供清晰决策解释，为现实的人类-机器人团队协作奠定基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to IROS 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.18047v2",
      "published_date": "2024-09-26 16:48:21 UTC",
      "updated_date": "2025-03-05 03:08:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:04:02.233853"
    },
    {
      "arxiv_id": "2409.18046v1",
      "title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Soeun Lee",
        "Si-Woo Kim",
        "Taewhan Kim",
        "Dong-Jin Kim"
      ],
      "abstract": "Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training.",
      "tldr_zh": "这篇论文提出了一种名为IFCap的框架，用于零样本图像和视频标题生成（Zero-shot Captioning），旨在解决文本-only训练与图像推理之间的模态差距问题。IFCap的核心方法包括Image-like Retrieval技术，将文本特征与视觉相关特征对齐，以及Frequency-based Entity Filtering技术来过滤实体并提升标题质量。此外，该框架还设计了Fusion Module，将检索到的标题与输入特征整合，以提高生成准确性。通过广泛实验，IFCap显著优于现有最先进方法，在图像和视频标题生成任务上表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.18046v1",
      "published_date": "2024-09-26 16:47:32 UTC",
      "updated_date": "2024-09-26 16:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:04:14.207963"
    },
    {
      "arxiv_id": "2409.18037v1",
      "title": "HARMONIC: A Framework for Explanatory Cognitive Robots",
      "title_zh": "HARMONIC：解释性认知机器人的框架",
      "authors": [
        "Sanjay Oruganti",
        "Sergei Nirenburg",
        "Marjorie McShane",
        "Jesse English",
        "Michael K. Roberts",
        "Christian Arndt"
      ],
      "abstract": "We present HARMONIC, a framework for implementing cognitive robots that\ntransforms general-purpose robots into trusted teammates capable of complex\ndecision-making, natural communication and human-level explanation. The\nframework supports interoperability between a strategic (cognitive) layer for\nhigh-level decision-making and a tactical (robot) layer for low-level control\nand execution. We describe the core features of the framework and our initial\nimplementation, in which HARMONIC was deployed on a simulated UGV and drone\ninvolved in a multi-robot search and retrieval task.",
      "tldr_zh": "我们介绍了 HARMONIC 框架，这是一个用于构建可解释认知机器人的系统，能够将通用机器人转化为可靠队友，支持复杂决策、自然通信和人类级别解释。该框架通过战略（cognitive）层处理高水平决策，以及战术（robot）层处理低水平控制和执行，实现两层间的互操作性。在初步实现中，HARMONIC 被部署在模拟的无人地面车辆（UGV）和无人机上，用于多机器人搜索和检索任务，展示了其潜在应用价值。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for presentation at ICRA@40. 23-26 September 2024,\n  Rotterdam, Netherlands",
      "pdf_url": "http://arxiv.org/pdf/2409.18037v1",
      "published_date": "2024-09-26 16:42:13 UTC",
      "updated_date": "2024-09-26 16:42:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:04:25.417679"
    },
    {
      "arxiv_id": "2409.18028v3",
      "title": "Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective",
      "title_zh": "大型语言模型中代码的组合硬度——概率视角",
      "authors": [
        "Yotam Wolf",
        "Binyamin Rothberg",
        "Dorin Shteyman",
        "Amnon Shashua"
      ],
      "abstract": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
      "tldr_zh": "本研究从概率视角探讨了大型语言模型（LLMs）在处理代码生成等复杂任务时的组合难度问题，即在同一上下文窗口内执行多个子任务（chain of thought）所面临的in-context hardness of composition。作者量化了这一难度，通过生成复杂度指标（即采样至少一个正确解决方案所需的LLM生成次数）来衡量，并证明在多智能体系统中分解问题比单一上下文更高效。实验和理论分析显示，这种复杂度差距随解决方案长度呈指数级增长，强调了采用multi-agent system的优势，以提升LLMs在组合任务中的性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18028v3",
      "published_date": "2024-09-26 16:34:35 UTC",
      "updated_date": "2025-01-31 10:15:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:04:37.837842"
    },
    {
      "arxiv_id": "2409.18025v5",
      "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
      "title_zh": "机器卸载在AI安全中的对抗视角",
      "authors": [
        "Jakub Łucki",
        "Boyi Wei",
        "Yangsibo Huang",
        "Peter Henderson",
        "Florian Tramèr",
        "Javier Rando"
      ],
      "abstract": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.",
      "tldr_zh": "本研究从对抗视角审视机器unlearning在AI Safety中的作用，挑战其与传统安全后训练的根本差异。作者证明，现有的jailbreak方法在仔细应用时能够成功绕过unlearning，并开发了多种自适应技术，例如通过在10个无关示例上微调或移除激活空间特定方向，从而恢复大多数被unlearning的危险能力。实验结果显示，这些方法显著削弱了state-of-the-art unlearning方法如RMU的效力，质疑了unlearning相对于安全训练的稳健优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Final version published in Transactions on Machine Learning Research\n  (TMLR); Best technical paper at Neurips 2024 SoLaR workshop",
      "pdf_url": "http://arxiv.org/pdf/2409.18025v5",
      "published_date": "2024-09-26 16:32:19 UTC",
      "updated_date": "2025-04-10 13:54:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:04:49.454889"
    },
    {
      "arxiv_id": "2409.18017v3",
      "title": "Transferring disentangled representations: bridging the gap between synthetic and real images",
      "title_zh": "翻译失败",
      "authors": [
        "Jacopo Dapueto",
        "Nicoletta Noceti",
        "Francesca Odone"
      ],
      "abstract": "Developing meaningful and efficient representations that separate the\nfundamental structure of the data generation mechanism is crucial in\nrepresentation learning. However, Disentangled Representation Learning has not\nfully shown its potential on real images, because of correlated generative\nfactors, their resolution and limited access to ground truth labels.\nSpecifically on the latter, we investigate the possibility of leveraging\nsynthetic data to learn general-purpose disentangled representations applicable\nto real data, discussing the effect of fine-tuning and what properties of\ndisentanglement are preserved after the transfer. We provide an extensive\nempirical study to address these issues. In addition, we propose a new\ninterpretable intervention-based metric, to measure the quality of factors\nencoding in the representation. Our results indicate that some level of\ndisentanglement, transferring a representation from synthetic to real data, is\npossible and effective.",
      "tldr_zh": "这篇论文探讨了 Disentangled Representation Learning 在真实图像上的挑战，包括生成因素的相关性、分辨率问题和标签有限的问题，并提出使用 synthetic data 训练通用解缠表示，然后通过 fine-tuning 转移到 real data。研究者进行了一个广泛的实证分析，评估了转移过程中的解缠属性保留情况，并引入了一个新的 interpretable intervention-based metric 来衡量表示中因素编码的质量。作为主要发现，结果表明从 synthetic data 到 real data 的表示转移是可行的，并能有效维持一定程度的解缠。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS, 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.18017v3",
      "published_date": "2024-09-26 16:25:48 UTC",
      "updated_date": "2024-12-06 09:14:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:05:01.722883"
    },
    {
      "arxiv_id": "2409.18014v1",
      "title": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles",
      "title_zh": "翻译失败",
      "authors": [
        "Lewei He",
        "Tianyu Shi",
        "Pengran Huang",
        "Bingzhi Chen",
        "Qianglong Chen",
        "Jiahui Pan"
      ],
      "abstract": "Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL.",
      "tldr_zh": "该论文提出 Online Long-context Processing (OLP) 范式，用于解决 Large Language Models (LLMs) 在处理长上下文时的实现复杂性、训练效率和数据稀疏性问题，特别适用于无限长文档的流媒体场景，如新闻报道和直播电商。作者开发了 Role Reinforcement Learning (Role-RL) 方法，通过强化学习自动为不同 LLMs 分配最佳角色，根据实际性能优化 OLP 管道中的部署。实验结果显示，在 OLP-MINI 数据集上，该框架实现了 93.2% 的平均召回率，并节省了 79.4% 的 LLM 成本，为高效的在线长上下文处理提供了可扩展解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18014v1",
      "published_date": "2024-09-26 16:22:59 UTC",
      "updated_date": "2024-09-26 16:22:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:05:15.534989"
    },
    {
      "arxiv_id": "2409.18009v1",
      "title": "Control Industrial Automation System with Large Language Models",
      "title_zh": "使用大型语言模型控制工业自动化系统",
      "authors": [
        "Yuchen Xia",
        "Nasser Jazdi",
        "Jize Zhang",
        "Chaitanya Shah",
        "Michael Weyrich"
      ],
      "abstract": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS",
      "tldr_zh": "本研究提出一个框架，将Large Language Models (LLMs) 整合到工业自动化系统的端到端控制中，以解决传统系统操作复杂和适应性差的问题。框架的核心包括针对工业任务的agent system、structured prompting method 以及event-driven information modeling mechanism，这些组件提供实时数据支持LLMs解释信息、生成生产计划并控制操作。该框架还支持结构化数据集的创建，用于LLMs的微调，使自动化系统更具适应性，能响应突发事件并通过自然语言实现直观的人机交互。通过正式系统设计、概念证明实现和任务特定数据集生成方法，实验验证了框架的有效性，并提供了GitHub上的演示视频和数据资源。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18009v1",
      "published_date": "2024-09-26 16:19:37 UTC",
      "updated_date": "2024-09-26 16:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:05:26.092973"
    },
    {
      "arxiv_id": "2409.17995v1",
      "title": "Joint Localization and Planning using Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "L. Lao Beyer",
        "S. Karaman"
      ],
      "abstract": "Diffusion models have been successfully applied to robotics problems such as\nmanipulation and vehicle path planning. In this work, we explore their\napplication to end-to-end navigation -- including both perception and planning\n-- by considering the problem of jointly performing global localization and\npath planning in known but arbitrary 2D environments. In particular, we\nintroduce a diffusion model which produces collision-free paths in a global\nreference frame given an egocentric LIDAR scan, an arbitrary map, and a desired\ngoal position. To this end, we implement diffusion in the space of paths in\nSE(2), and describe how to condition the denoising process on both obstacles\nand sensor observations. In our evaluation, we show that the proposed\nconditioning techniques enable generalization to realistic maps of considerably\ndifferent appearance than the training environment, demonstrate our model's\nability to accurately describe ambiguous solutions, and run extensive\nsimulation experiments showcasing our model's use as a real-time, end-to-end\nlocalization and planning stack.",
      "tldr_zh": "本研究探索了扩散模型（Diffusion models）在机器人端到端导航中的应用，专注于在已知但任意2D环境中联合进行全局定位和路径规划。论文引入了一个扩散模型，该模型在SE(2)空间中生成无碰撞路径，通过条件化去噪过程整合自我中心的LIDAR扫描、任意地图和目标位置，从而处理障碍物和传感器观察。实验结果显示，该模型能够泛化到与训练环境外观不同的现实地图，准确处理模糊解决方案，并在模拟中作为实时端到端定位和规划系统表现出色。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 9 figures. Submitted to ICRA 2025, under review",
      "pdf_url": "http://arxiv.org/pdf/2409.17995v1",
      "published_date": "2024-09-26 16:07:20 UTC",
      "updated_date": "2024-09-26 16:07:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:05:37.560167"
    },
    {
      "arxiv_id": "2409.17994v4",
      "title": "CRoP: Context-wise Robust Static Human-Sensing Personalization",
      "title_zh": "CRoP：上下文相关的稳健静态人体感应个性化",
      "authors": [
        "Sawinder Kaur",
        "Avery Gump",
        "Jingyu Xin",
        "Yi Xiao",
        "Harshit Sharma",
        "Nina R Benway",
        "Jonathan L Preston",
        "Asif Salekin"
      ],
      "abstract": "The advancement in deep learning and internet-of-things have led to diverse\nhuman sensing applications. However, distinct patterns in human sensing,\ninfluenced by various factors or contexts, challenge the generic neural network\nmodel's performance due to natural distribution shifts. To address this,\npersonalization tailors models to individual users. Yet most personalization\nstudies overlook intra-user heterogeneity across contexts in sensory data,\nlimiting intra-user generalizability. This limitation is especially critical in\nclinical applications, where limited data availability hampers both\ngeneralizability and personalization. Notably, intra-user sensing attributes\nare expected to change due to external factors such as treatment progression,\nfurther complicating the challenges. To address the intra-user generalization\nchallenge, this work introduces CRoP, a novel static personalization approach.\nCRoP leverages off-the-shelf pre-trained models as generic starting points and\ncaptures user-specific traits through adaptive pruning on a minimal sub-network\nwhile preserving generic knowledge in the remaining parameters. CRoP\ndemonstrates superior personalization effectiveness and intra-user robustness\nacross four human-sensing datasets, including two from real-world health\ndomains, underscoring its practical and social impact. Additionally, to support\nCRoP's generalization ability and design choices, we provide empirical\njustification through gradient inner product analysis, ablation studies, and\ncomparisons against state-of-the-art baselines.",
      "tldr_zh": "该论文针对人类感知（human-sensing）应用中的数据分布偏移问题，提出了一种新的静态个性化方法 CRoP，以解决用户内部（intra-user）在不同上下文中的异质性挑战，尤其在临床场景中数据有限的情况下。\nCRoP 以现成的预训练模型（pre-trained models）为起点，通过在最小子网络上进行自适应修剪（adaptive pruning）来捕获用户特定特征，同时保留通用知识，从而提升内部用户泛化性和鲁棒性。\n实验结果显示，CRoP 在四个人类感知数据集（包括两个真实健康领域数据集）上，比现有基线模型表现出优越的个性化效果和鲁棒性，并通过梯度内积分析（gradient inner product analysis）和消融研究提供了实证支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "33 pages, 6 figues and 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.17994v4",
      "published_date": "2024-09-26 16:06:38 UTC",
      "updated_date": "2024-11-19 14:51:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:05:51.211849"
    },
    {
      "arxiv_id": "2410.06180v1",
      "title": "CBIDR: A novel method for information retrieval combining image and data by means of TOPSIS applied to medical diagnosis",
      "title_zh": "CBIDR：一种通过 TOPSIS 结合图像和数据的新型信息检索方法，应用于医疗诊断",
      "authors": [
        "Humberto Giuri",
        "Renato A. Krohling"
      ],
      "abstract": "Content-Based Image Retrieval (CBIR) have shown promising results in the\nfield of medical diagnosis, which aims to provide support to medical\nprofessionals (doctor or pathologist). However, the ultimate decision regarding\nthe diagnosis is made by the medical professional, drawing upon their\naccumulated experience. In this context, we believe that artificial\nintelligence can play a pivotal role in addressing the challenges in medical\ndiagnosis not by making the final decision but by assisting in the diagnosis\nprocess with the most relevant information. The CBIR methods use similarity\nmetrics to compare feature vectors generated from images using Convolutional\nNeural Networks (CNNs). In addition to the information contained in medical\nimages, clinical data about the patient is often available and is also relevant\nin the final decision-making process by medical professionals. In this paper,\nwe propose a novel method named CBIDR, which leverage both medical images and\nclinical data of patient, combining them through the ranking algorithm TOPSIS.\nThe goal is to aid medical professionals in their final diagnosis by retrieving\nimages and clinical data of patient that are most similar to query data from\nthe database. As a case study, we illustrate our CBIDR for diagnostic of oral\ncancer including histopathological images and clinical data of patient.\nExperimental results in terms of accuracy achieved 97.44% in Top-1 and 100% in\nTop-5 showing the effectiveness of the proposed approach.",
      "tldr_zh": "本研究提出了一种新型信息检索方法CBIDR，用于医疗诊断领域，通过结合医疗图像和临床数据来辅助医生决策。CBIDR基于Content-Based Image Retrieval (CBIR)利用Convolutional Neural Networks (CNNs)生成图像特征向量，并通过TOPSIS排名算法整合图像和数据，以检索与查询最相似的患者图像和临床信息。作为案例应用，该方法在口腔癌诊断中实现了Top-1准确率97.44%和Top-5准确率100%，证明了其在提升诊断支持方面的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.IR",
      "comment": "28 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.06180v1",
      "published_date": "2024-09-26 16:04:36 UTC",
      "updated_date": "2024-09-26 16:04:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:06:01.242916"
    },
    {
      "arxiv_id": "2409.17978v2",
      "title": "HydraViT: Stacking Heads for a Scalable ViT",
      "title_zh": "HydraViT：堆叠头以实现可扩展的ViT",
      "authors": [
        "Janek Haberer",
        "Ali Hojjat",
        "Olaf Landsiedel"
      ],
      "abstract": "The architecture of Vision Transformers (ViTs), particularly the Multi-head\nAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs\non devices with varying constraints, such as mobile phones, requires multiple\nmodels of different sizes. However, this approach has limitations, such as\ntraining and storing each required model separately. This paper introduces\nHydraViT, a novel approach that addresses these limitations by stacking\nattention heads to achieve a scalable ViT. By repeatedly changing the size of\nthe embedded dimensions throughout each layer and their corresponding number of\nattention heads in MHA during training, HydraViT induces multiple subnetworks.\nThereby, HydraViT achieves adaptability across a wide spectrum of hardware\nenvironments while maintaining performance. Our experimental results\ndemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10\nsubnetworks, covering a wide range of resource constraints. HydraViT achieves\nup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy\nwith the same throughput on ImageNet-1K compared to the baselines, making it an\neffective solution for scenarios where hardware availability is diverse or\nvaries over time. Source code available at https://github.com/ds-kiel/HydraViT.",
      "tldr_zh": "本论文提出HydraViT，一种创新方法，通过堆叠注意力头来实现Vision Transformers (ViTs) 的可扩展性，解决Multi-head Attention (MHA) 机制在不同硬件环境下的部署挑战。HydraViT在训练过程中动态调整每个层的嵌入维度和注意力头数量，从而生成多个子网络，适应从手机到高性能设备的资源约束。实验结果显示，在ImageNet-1K数据集上，HydraViT比基线模型在相同GMACs下提高最多5 p.p.准确率，在相同吞吐量下提高最多7 p.p.准确率，支持多达10个子网络，提供高效的硬件适应性解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS'24, please cite the conference version",
      "pdf_url": "http://arxiv.org/pdf/2409.17978v2",
      "published_date": "2024-09-26 15:52:36 UTC",
      "updated_date": "2024-12-05 16:24:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:06:14.002824"
    },
    {
      "arxiv_id": "2409.18797v1",
      "title": "Supervised Learning Model for Key Frame Identification from Cow Teat Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Minghao Wang",
        "Pinxue Lin"
      ],
      "abstract": "This paper proposes a method for improving the accuracy of mastitis risk\nassessment in cows using neural networks and video analysis. Mastitis, an\ninfection of the udder tissue, is a critical health problem for cows and can be\ndetected by examining the cow's teat. Traditionally, veterinarians assess the\nhealth of a cow's teat during the milking process, but this process is limited\nin time and can weaken the accuracy of the assessment. In commercial farms,\ncows are recorded by cameras when they are milked in the milking parlor. This\npaper uses a neural network to identify key frames in the recorded video where\nthe cow's udder appears intact. These key frames allow veterinarians to have\nmore flexible time to perform health assessments on the teat, increasing their\nefficiency and accuracy. However, there are challenges in using cow teat video\nfor mastitis risk assessment, such as complex environments, changing cow\npositions and postures, and difficulty in identifying the udder from the video.\nTo address these challenges, a fusion distance and an ensemble model are\nproposed to improve the performance (F-score) of identifying key frames from\ncow teat videos. The results show that these two approaches improve performance\ncompared to using a single distance measure or model.",
      "tldr_zh": "这篇论文提出了一种基于监督学习的模型，用于从奶牛乳头视频中识别关键帧，从而提升乳腺炎风险评估的准确性。传统兽医评估方法受时间限制，该模型通过神经网络分析视频，聚焦于奶牛乳房完整的帧，以提供更灵活的健康检查机会。为应对环境复杂性、奶牛位置变化等挑战，论文引入了融合距离（fusion distance）和集成模型（ensemble model），显著提高了关键帧识别的性能（F-score）。实验结果表明，该方法比使用单一距离测量或模型的表现更优。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18797v1",
      "published_date": "2024-09-26 15:50:43 UTC",
      "updated_date": "2024-09-26 15:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:06:25.962121"
    },
    {
      "arxiv_id": "2409.17954v2",
      "title": "Enhancing elusive clues in knowledge learning by contrasting attention of language models",
      "title_zh": "通过对比语言模型的注意力增强知识学习中的难以捉摸的线索",
      "authors": [
        "Jian Gao",
        "Xiao Zhang",
        "Ji Wu",
        "Miao Li"
      ],
      "abstract": "Causal language models acquire vast amount of knowledge from general text\ncorpus during pretraining, but the efficiency of knowledge learning is known to\nbe unsatisfactory, especially when learning from knowledge-dense and\nsmall-sized corpora. The deficiency can come from long-distance dependencies\nwhich are hard to capture by language models, and overfitting to co-occurrence\npatterns and distracting clues in the training text. To address these issues,\nthe paper proposes a method to enhance knowledge learning during language model\npretraining, by enhancing elusive but important clues in text discovered by the\nlanguage model themselves. We found that larger language models pay more\nattention to non-obvious but important clues, which are often overlooked by\nsmaller language models. Therefore, we can identify these clues by contrasting\nthe attention weights of large and small language models. We use the identified\nclues as a guide to perform token-dropout data augmentation on the training\ntext, and observed a significant boost in both small and large models'\nperformance in fact memorization. This shows that the behavior contrast between\nmore and less-performant language models contains important clues for knowledge\nlearning, and it can be ``amplified\" for a straight-forward improvement in\nknowledge learning efficiency.",
      "tldr_zh": "该论文探讨了因果语言模型在预训练时从一般文本语料库中学习知识的低效问题，特别是面对长距离依赖和 distracting clues 时易出现过拟合。作者提出一种方法，通过对比大模型和小模型的 attention weights 来识别 elusive clues，这些 clues 是非明显但重要的文本特征。利用这些 clues 进行 token-dropout 数据增强，实验结果显示小模型和大模型在事实记忆方面的性能显著提升，证明了行为对比可以放大知识学习效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Oral presentation in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.17954v2",
      "published_date": "2024-09-26 15:30:54 UTC",
      "updated_date": "2025-03-12 09:42:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:06:37.949874"
    },
    {
      "arxiv_id": "2409.17946v3",
      "title": "Weak-to-Strong Backdoor Attack for Large Language Models",
      "title_zh": "针对大型语言模型的弱到强后",
      "authors": [
        "Shuai Zhao",
        "Leilei Gan",
        "Zhongliang Guo",
        "Xiaobao Wu",
        "Luwei Xiao",
        "Xiaoyu Xu",
        "Cong-Duy Nguyen",
        "Luu Anh Tuan"
      ],
      "abstract": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.",
      "tldr_zh": "该研究揭示了大型语言模型 (LLMs) 容易受到后门攻击 (backdoor attacks)，但传统方法需大量计算资源，而参数高效微调 (PEFT) 可能无法有效对齐触发器和目标标签。针对这些问题，作者提出了一种新型算法 W2SAttack，该算法基于特征对齐增强知识蒸馏 (knowledge distillation)，先在小规模模型上注入后门作为教师模型，然后通过 PEFT 将后门转移到大模型上。理论分析表明，此方法能显著提升后门攻击的有效性。实验结果显示，在四个语言模型、四个后门攻击算法和两种教师模型架构上，W2SAttack 的成功率接近 100%。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17946v3",
      "published_date": "2024-09-26 15:20:37 UTC",
      "updated_date": "2024-10-13 06:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:06:50.366218"
    },
    {
      "arxiv_id": "2409.17943v1",
      "title": "On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms",
      "title_zh": "关于翻译",
      "authors": [
        "Richard Yue",
        "John E. Ortega",
        "Kenneth Ward Church"
      ],
      "abstract": "The typical workflow for a professional translator to translate a document\nfrom its source language (SL) to a target language (TL) is not always focused\non what many language models in natural language processing (NLP) do - predict\nthe next word in a series of words. While high-resource languages like English\nand French are reported to achieve near human parity using common metrics for\nmeasurement such as BLEU and COMET, we find that an important step is being\nmissed: the translation of technical terms, specifically acronyms. Some\nstate-of-the art machine translation systems like Google Translate which are\npublicly available can be erroneous when dealing with acronyms - as much as 50%\nin our findings. This article addresses acronym disambiguation for MT systems\nby proposing an additional step to the SL-TL (FR-EN) translation workflow where\nwe first offer a new acronym corpus for public consumption and then experiment\nwith a search-based thresholding algorithm that achieves nearly 10% increase\nwhen compared to Google Translate and OpusMT.",
      "tldr_zh": "本文探讨了机器翻译（MT）系统在处理技术术语尤其是缩写时的不足，指出现有系统如 Google Translate 可能出现高达 50% 的错误，并强调了专业翻译工作流与 NLP 模型（如基于 BLEU 和 COMET 指标）的差异。研究提出了一种改进的 SL-TL（FR-EN）翻译工作流，包括构建一个新的缩写语料库和实验一个基于搜索的 thresholding 算法，以实现 acronym disambiguation。结果显示，该算法相较于 Google Translate 和 OpusMT 提高了近 10% 的翻译准确率，为提升 MT 系统性能提供了实用方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "AMTA 2024 - The Association for Machine Translation in the Americas\n  organizes biennial conferences devoted to researchers, commercial users,\n  governmental and NGO users",
      "pdf_url": "http://arxiv.org/pdf/2409.17943v1",
      "published_date": "2024-09-26 15:18:34 UTC",
      "updated_date": "2024-09-26 15:18:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:07:02.690820"
    },
    {
      "arxiv_id": "2409.17939v1",
      "title": "Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Richard Yue",
        "John E. Ortega"
      ],
      "abstract": "Translation memories (TMs) are the backbone for professional translation\ntools called computer-aided translation (CAT) tools. In order to perform a\ntranslation using a CAT tool, a translator uses the TM to gather translations\nsimilar to the desired segment to translate (s'). Many CAT tools offer a\nfuzzy-match algorithm to locate segments (s) in the TM that are close in\ndistance to s'. After locating two similar segments, the CAT tool will present\nparallel segments (s, t) that contain one segment in the source language along\nwith its translation in the target language. Additionally, CAT tools contain\nfuzzy-match repair (FMR) techniques that will automatically use the parallel\nsegments from the TM to create new TM entries containing a modified version of\nthe original with the idea in mind that it will be the translation of s'. Most\nFMR techniques use machine translation as a way of \"repairing\" those words that\nhave to be modified. In this article, we show that for a large part of those\nwords which are anchored, we can use other techniques that are based on machine\nlearning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we\nshow that for anchored words that follow the continuous bag-of-words (CBOW)\nparadigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for\nsome cases, better results than neural machine translation for translating\nanchored words from French to English.",
      "tldr_zh": "本论文探讨了使用深度学习方法从翻译记忆（Translation Memories, TMs）中预测 anchored text，以提升机器翻译（Machine Translation）的效率。作者提出，通过 Word2Vec、BERT 和 ChatGPT 等模型处理 anchored words，特别是遵循连续词袋（CBOW）范式的词汇，这些方法能比传统神经机器翻译提供类似或更好的翻译效果，尤其在从法语到英语的场景中。实验结果表明，此方法为计算机辅助翻译（CAT）工具的模糊匹配修复（FMR）技术提供了新途径，提高了翻译准确性和适用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "AMTA 2024 - The Association for Machine Translation in the Americas\n  organizes biennial conferences devoted to researchers, commercial users,\n  governmental and NGO users",
      "pdf_url": "http://arxiv.org/pdf/2409.17939v1",
      "published_date": "2024-09-26 15:12:59 UTC",
      "updated_date": "2024-09-26 15:12:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:07:14.744052"
    },
    {
      "arxiv_id": "2409.17931v2",
      "title": "Remaining Useful Life Prediction for Batteries Utilizing an Explainable AI Approach with a Predictive Application for Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Biplov Paneru",
        "Bipul Thapa",
        "Durga Prasad Mainali",
        "Bishwash Paneru",
        "Krishna Bikram Shah"
      ],
      "abstract": "Accurately estimating the Remaining Useful Life (RUL) of a battery is\nessential for determining its lifespan and recharge requirements. In this work,\nwe develop machine learning-based models to predict and classify battery RUL.\nWe introduce a two-level ensemble learning (TLE) framework and a CNN+MLP hybrid\nmodel for RUL prediction, comparing their performance against traditional,\ndeep, and hybrid machine learning models. Our analysis evaluates various models\nfor both prediction and classification while incorporating interpretability\nthrough SHAP. The proposed TLE model consistently outperforms baseline models\nin RMSE, MAE, and R squared error, demonstrating its superior predictive\ncapabilities. Additionally, the XGBoost classifier achieves an impressive 99%\nclassification accuracy, validated through cross-validation techniques. The\nmodels effectively predict relay-based charging triggers, enabling automated\nand energy-efficient charging processes. This automation reduces energy\nconsumption and enhances battery performance by optimizing charging cycles.\nSHAP interpretability analysis highlights the cycle index and charging\nparameters as the most critical factors influencing RUL. To improve\naccessibility, we developed a Tkinter-based GUI that allows users to input new\ndata and predict RUL in real time. This practical solution supports sustainable\nbattery management by enabling data-driven decisions about battery usage and\nmaintenance, contributing to energy-efficient and innovative battery life\nprediction.",
      "tldr_zh": "本研究开发了机器学习模型来预测和分类电池的剩余可用寿命 (RUL)，以支持电池寿命估算和充电决策。研究引入了两级集成学习 (TLE) 框架和 CNN+MLP 混合模型，并通过 SHAP 提供可解释性分析，与传统、深度和混合模型相比，TLE 在 RMSE、MAE 和 R squared error 上表现出色。XGBoost 分类器实现了 99% 的准确率，并验证了其在预测继电器充电触发方面的有效性。最终，研究开发了一个 Tkinter-based GUI，实现实时 RUL 预测，促进自动化节能充电和可持续电池管理。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17931v2",
      "published_date": "2024-09-26 15:08:38 UTC",
      "updated_date": "2025-01-30 14:48:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:07:26.013950"
    },
    {
      "arxiv_id": "2409.17928v2",
      "title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion",
      "title_zh": "翻译失败",
      "authors": [
        "Hengrui Gu",
        "Kaixiong Zhou",
        "Yili Wang",
        "Ruobing Wang",
        "Xin Wang"
      ],
      "abstract": "During pre-training, the Text-to-Image (T2I) diffusion models encode factual\nknowledge into their parameters. These parameterized facts enable realistic\nimage generation, but they may become obsolete over time, thereby\nmisrepresenting the current state of the world. Knowledge editing techniques\naim to update model knowledge in a targeted way. However, facing the dual\nchallenges posed by inadequate editing datasets and unreliable evaluation\ncriterion, the development of T2I knowledge editing encounter difficulties in\neffectively generalizing injected knowledge. In this work, we design a T2I\nknowledge editing framework by comprehensively spanning on three phases: First,\nwe curate a dataset \\textbf{CAKE}, comprising paraphrase and multi-object test,\nto enable more fine-grained assessment on knowledge generalization. Second, we\npropose a novel criterion, \\textbf{adaptive CLIP threshold}, to effectively\nfilter out false successful images under the current criterion and achieve\nreliable editing evaluation. Finally, we introduce \\textbf{MPE}, a simple but\neffective approach for T2I knowledge editing. Instead of tuning parameters, MPE\nprecisely recognizes and edits the outdated part of the conditioning\ntext-prompt to accommodate the up-to-date knowledge. A straightforward\nimplementation of MPE (Based on in-context learning) exhibits better overall\nperformance than previous model editors. We hope these efforts can further\npromote faithful evaluation of T2I knowledge editing methods.",
      "tldr_zh": "这篇论文针对 Text-to-Image (T2I) 扩散模型的知识编辑问题，提出一个全面框架来解决数据集不足和评估标准不可靠的挑战。研究者创建了 CAKE 数据集，包括 paraphrasing 和 multi-object 测试，以实现更细粒度的知识泛化评估；并引入 adaptive CLIP threshold 标准，用于过滤假成功的图像并提升编辑评估的可靠性。最后，他们开发了 MPE 方法，通过精确识别并编辑条件文本提示中的过时部分（而非调整模型参数），其基于 in-context learning 的简单实现比现有模型编辑器表现出色，从而促进 T2I 知识编辑的忠实评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP24 Findings. Our code is available at\n  https://github.com/Hengrui-Gu/T2IKnowledgeEditing",
      "pdf_url": "http://arxiv.org/pdf/2409.17928v2",
      "published_date": "2024-09-26 15:07:30 UTC",
      "updated_date": "2024-10-26 06:03:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:07:38.959812"
    },
    {
      "arxiv_id": "2409.17922v1",
      "title": "Navigation in a simplified Urban Flow through Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Federica Tonti",
        "Jean Rabault",
        "Ricardo Vinuesa"
      ],
      "abstract": "The increasing number of unmanned aerial vehicles (UAVs) in urban\nenvironments requires a strategy to minimize their environmental impact, both\nin terms of energy efficiency and noise reduction. In order to reduce these\nconcerns, novel strategies for developing prediction models and optimization of\nflight planning, for instance through deep reinforcement learning (DRL), are\nneeded. Our goal is to develop DRL algorithms capable of enabling the\nautonomous navigation of UAVs in urban environments, taking into account the\npresence of buildings and other UAVs, optimizing the trajectories in order to\nreduce both energetic consumption and noise. This is achieved using fluid-flow\nsimulations which represent the environment in which UAVs navigate and training\nthe UAV as an agent interacting with an urban environment. In this work, we\nconsider a domain domain represented by a two-dimensional flow field with\nobstacles, ideally representing buildings, extracted from a three-dimensional\nhigh-fidelity numerical simulation. The presented methodology, using PPO+LSTM\ncells, was validated by reproducing a simple but fundamental problem in\nnavigation, namely the Zermelo's problem, which deals with a vessel navigating\nin a turbulent flow, travelling from a starting point to a target location,\noptimizing the trajectory. The current method shows a significant improvement\nwith respect to both a simple PPO and a TD3 algorithm, with a success rate (SR)\nof the PPO+LSTM trained policy of 98.7%, and a crash rate (CR) of 0.1%,\noutperforming both PPO (SR = 75.6%, CR=18.6%) and TD3 (SR=77.4% and CR=14.5%).\nThis is the first step towards DRL strategies which will guide UAVs in a\nthree-dimensional flow field using real-time signals, making the navigation\nefficient in terms of flight time and avoiding damages to the vehicle.",
      "tldr_zh": "该研究针对城市环境中无人驾驶飞行器（UAVs）的导航问题，提出使用深度强化学习（DRL）优化飞行轨迹，以减少能源消耗和噪音影响。方法基于流体流动模拟构建二维环境模型，将UAV视为代理，并采用PPO+LSTM算法进行训练，以处理建筑物和其他UAV的干扰。实验通过重现Zermelo's problem验证了该方法，与单纯PPO和TD3算法相比，PPO+LSTM实现了成功率（SR）98.7%和崩溃率（CR）0.1%的显著提升。该框架为UAVs在三维真实环境中实现高效、可靠导航奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17922v1",
      "published_date": "2024-09-26 15:05:15 UTC",
      "updated_date": "2024-09-26 15:05:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:07:51.236117"
    },
    {
      "arxiv_id": "2409.17907v1",
      "title": "PhantomLiDAR: Cross-modality Signal Injection Attacks against LiDAR",
      "title_zh": "PhantomLiDAR：",
      "authors": [
        "Zizhi Jin",
        "Qinhong Jiang",
        "Xuancun Lu",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "abstract": "LiDAR (Light Detection and Ranging) is a pivotal sensor for autonomous\ndriving, offering precise 3D spatial information. Previous signal attacks\nagainst LiDAR systems mainly exploit laser signals. In this paper, we\ninvestigate the possibility of cross-modality signal injection attacks, i.e.,\ninjecting intentional electromagnetic interference (IEMI) to manipulate LiDAR\noutput. Our insight is that the internal modules of a LiDAR, i.e., the laser\nreceiving circuit, the monitoring sensors, and the beam-steering modules, even\nwith strict electromagnetic compatibility (EMC) testing, can still couple with\nthe IEMI attack signals and result in the malfunction of LiDAR systems. Based\non the above attack surfaces, we propose the PhantomLiDAR attack, which\nmanipulates LiDAR output in terms of Points Interference, Points Injection,\nPoints Removal, and even LiDAR Power-Off. We evaluate and demonstrate the\neffectiveness of PhantomLiDAR with both simulated and real-world experiments on\nfive COTS LiDAR systems. We also conduct feasibility experiments in real-world\nmoving scenarios. We provide potential defense measures that can be implemented\nat both the sensor level and the vehicle system level to mitigate the risks\nassociated with IEMI attacks. Video demonstrations can be viewed at\nhttps://sites.google.com/view/phantomlidar.",
      "tldr_zh": "本文提出PhantomLiDAR，一种针对LiDAR的跨模态信号注入攻击，利用故意电磁干扰(IEMI)来操纵LiDAR输出，针对其内部模块（如激光接收电路和监控传感器）的漏洞，实现Points Interference（点干扰）、Points Injection（点注入）、Points Removal（点移除）以及LiDAR Power-Off（关机）。通过模拟和真实实验，在五个商用LiDAR系统上验证了攻击的有效性，包括真实移动场景下的可行性。论文还提供了潜在防御措施，如在传感器级别和车辆系统级别增强电磁兼容性，以缓解IEMI攻击的风险。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.ET",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17907v1",
      "published_date": "2024-09-26 14:52:51 UTC",
      "updated_date": "2024-09-26 14:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:08:03.145918"
    },
    {
      "arxiv_id": "2409.17904v1",
      "title": "Learning to Love Edge Cases in Formative Math Assessment: Using the AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy",
      "title_zh": "翻译失败",
      "authors": [
        "Owen Henkel",
        "Hannah Horne-Robinson",
        "Maria Dyshel",
        "Nabil Ch",
        "Baptiste Moreau-Pernet",
        "Ralph Abood"
      ],
      "abstract": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment.",
      "tldr_zh": "该论文引入了AMMORE数据集，该数据集包含53,000个数学开放式问题-答案对，用于研究学生数学习得，特别是针对规则分类器难以处理的边缘案例。研究者通过实验1评估了大型语言模型(LLM)的各种提示方法，包括zero-shot、few-shot和chain-of-thought prompting，发现chain-of-thought prompting能准确评分92%的边缘案例，将整体评分准确率从98.7%提升至99.9%。在实验2中，将改进后的LLM评分应用于Bayesian Knowledge Tracing (BKT)模型，学生掌握状态的误分类率从6.9%降至2.6%，表明LLM可有效提升K-12数学教育中开放式问题的 formative assessment准确性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17904v1",
      "published_date": "2024-09-26 14:51:40 UTC",
      "updated_date": "2024-09-26 14:51:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:08:16.037302"
    },
    {
      "arxiv_id": "2409.17899v2",
      "title": "Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations",
      "title_zh": "通过自监督表示探索情感语音和音乐中的声学相似性",
      "authors": [
        "Yujia Sun",
        "Zeyu Zhao",
        "Korin Richmond",
        "Yuanchao Li"
      ],
      "abstract": "Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems.",
      "tldr_zh": "本研究探讨了语音和音乐在情感识别中的声学相似性，通过Self-Supervised Learning (SSL)模型分析Speech Emotion Recognition (SER)和Music Emotion Recognition (MER)的层级行为。研究采用两阶段微调方法进行跨域适应，比较利用音乐提升SER和利用语音提升MER的有效性，并使用Frechet audio distance评估特定情感的声学相似性。结果显示，语音和音乐SSL模型捕捉了共享的声学特征，但行为因情感类型、训练策略和领域差异而异；参数高效微调可提升性能，并为SER和MER系统的跨域泛化提供新见解。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.17899v2",
      "published_date": "2024-09-26 14:49:09 UTC",
      "updated_date": "2025-04-30 13:32:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:08:26.391516"
    },
    {
      "arxiv_id": "2409.17876v1",
      "title": "Why Companies \"Democratise\" Artificial Intelligence: The Case of Open Source Software Donations",
      "title_zh": "翻译失败",
      "authors": [
        "Cailean Osborne"
      ],
      "abstract": "Companies claim to \"democratise\" artificial intelligence (AI) when they\ndonate AI open source software (OSS) to non-profit foundations or release AI\nmodels, among others, but what does this term mean and why do they do it? As\nthe impact of AI on society and the economy grows, understanding the commercial\nincentives behind AI democratisation efforts is crucial for ensuring these\nefforts serve broader interests beyond commercial agendas. Towards this end,\nthis study employs a mixed-methods approach to investigate commercial\nincentives for 43 AI OSS donations to the Linux Foundation. It makes\ncontributions to both research and practice. It contributes a taxonomy of both\nindividual and organisational social, economic, and technological incentives\nfor AI democratisation. In particular, it highlights the role of democratising\nthe governance and control rights of an OSS project (i.e., from one company to\nopen governance) as a structural enabler for downstream goals, such as\nattracting external contributors, reducing development costs, and influencing\nindustry standards, among others. Furthermore, OSS donations are often\nchampioned by individual developers within companies, highlighting the\nimportance of the bottom-up incentives for AI democratisation. The taxonomy\nprovides a framework and toolkit for discerning incentives for other AI\ndemocratisation efforts, such as the release of AI models. The paper concludes\nwith a discussion of future research directions.",
      "tldr_zh": "这篇论文探讨了公司声称“democratise”人工智能的原因，特别是通过捐赠开源软件 (OSS) 到非营利基金会等形式。研究采用混合方法分析了43个AI OSS捐赠到Linux Foundation的案例，构建了一个分类法，涵盖个人和组织的社交、经济和技术激励。论文强调，民主化OSS项目的治理和控制权（从单一公司转向开放治理）是实现下游目标的关键基础，如吸引外部贡献者、降低开发成本和影响行业标准。总体上，该框架为评估其他AI democratisation努力（如发布AI模型）提供工具，并指出未来研究方向。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "30 pages, 1 figure, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.17876v1",
      "published_date": "2024-09-26 14:23:44 UTC",
      "updated_date": "2024-09-26 14:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:08:40.044926"
    },
    {
      "arxiv_id": "2409.17874v1",
      "title": "DarkSAM: Fooling Segment Anything Model to Segment Nothing",
      "title_zh": "翻译失败",
      "authors": [
        "Ziqi Zhou",
        "Yufei Song",
        "Minghui Li",
        "Shengshan Hu",
        "Xianlong Wang",
        "Leo Yu Zhang",
        "Dezhong Yao",
        "Hai Jin"
      ],
      "abstract": "Segment Anything Model (SAM) has recently gained much attention for its\noutstanding generalization to unseen data and tasks. Despite its promising\nprospect, the vulnerabilities of SAM, especially to universal adversarial\nperturbation (UAP) have not been thoroughly investigated yet. In this paper, we\npropose DarkSAM, the first prompt-free universal attack framework against SAM,\nincluding a semantic decoupling-based spatial attack and a texture\ndistortion-based frequency attack. We first divide the output of SAM into\nforeground and background. Then, we design a shadow target strategy to obtain\nthe semantic blueprint of the image as the attack target. DarkSAM is dedicated\nto fooling SAM by extracting and destroying crucial object features from images\nin both spatial and frequency domains. In the spatial domain, we disrupt the\nsemantics of both the foreground and background in the image to confuse SAM. In\nthe frequency domain, we further enhance the attack effectiveness by distorting\nthe high-frequency components (i.e., texture information) of the image.\nConsequently, with a single UAP, DarkSAM renders SAM incapable of segmenting\nobjects across diverse images with varying prompts. Experimental results on\nfour datasets for SAM and its two variant models demonstrate the powerful\nattack capability and transferability of DarkSAM.",
      "tldr_zh": "这篇论文提出了 DarkSAM，一种针对 Segment Anything Model (SAM) 的首个无提示通用攻击框架，旨在通过 universal adversarial perturbation (UAP) 使 SAM 无法正确分割对象。DarkSAM 包括基于语义解耦的空间攻击（破坏图像前景和背景的语义）和基于纹理扭曲的频率攻击（扭曲图像的高频组件），从而提取并破坏关键对象特征。实验结果显示，在四个数据集上，DarkSAM 显著提升了攻击效果，比基线模型表现出更强的攻击能力和转移性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by the 38th Annual Conference on Neural\n  Information Processing Systems (NeurIPS'24)",
      "pdf_url": "http://arxiv.org/pdf/2409.17874v1",
      "published_date": "2024-09-26 14:20:14 UTC",
      "updated_date": "2024-09-26 14:20:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:08:50.573863"
    },
    {
      "arxiv_id": "2409.17870v2",
      "title": "Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores",
      "title_zh": "翻译失败",
      "authors": [
        "Shaobo Ma",
        "Chao Fang",
        "Haikuo Shao",
        "Zhongfeng Wang"
      ],
      "abstract": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 2.4\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）在GPU Tensor Cores上的高效推理问题，提出了一种全面的任意精度加速方案，以解决超低位量化受限于硬件支持和内存管理的挑战。核心创新包括引入bipolar-INT数据格式，支持并行计算和对称量化以减少数据冗余，以及开发任意精度矩阵乘法方案，通过位级矩阵分解和恢复来最大化Tensor Core利用，并结合高效矩阵预处理和数据恢复导向的内存管理系统。实验结果显示，该方法比NVIDIA的CUTLASS在矩阵乘法上实现高达2.4倍的速度提升，并在LLMs推理中达到6.7倍的加速，从而显著提升了LLMs的应用效率和响应性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by ASP-DAC 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.17870v2",
      "published_date": "2024-09-26 14:17:58 UTC",
      "updated_date": "2024-10-18 02:01:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:09:02.414894"
    },
    {
      "arxiv_id": "2409.17865v1",
      "title": "Implementing a Nordic-Baltic Federated Health Data Network: a case report",
      "title_zh": "翻译失败",
      "authors": [
        "Taridzo Chomutare",
        "Aleksandar Babic",
        "Laura-Maria Peltonen",
        "Silja Elunurm",
        "Peter Lundberg",
        "Arne Jönsson",
        "Emma Eneling",
        "Ciprian-Virgil Gerstenberger",
        "Troels Siggaard",
        "Raivo Kolde",
        "Oskar Jerdhaf",
        "Martin Hansson",
        "Alexandra Makhlysheva",
        "Miroslav Muzny",
        "Erik Ylipää",
        "Søren Brunak",
        "Hercules Dalianis"
      ],
      "abstract": "Background: Centralized collection and processing of healthcare data across\nnational borders pose significant challenges, including privacy concerns, data\nheterogeneity and legal barriers. To address some of these challenges, we\nformed an interdisciplinary consortium to develop a feder-ated health data\nnetwork, comprised of six institutions across five countries, to facilitate\nNordic-Baltic cooperation on secondary use of health data. The objective of\nthis report is to offer early insights into our experiences developing this\nnetwork. Methods: We used a mixed-method ap-proach, combining both experimental\ndesign and implementation science to evaluate the factors affecting the\nimplementation of our network. Results: Technically, our experiments indicate\nthat the network functions without significant performance degradation compared\nto centralized simu-lation. Conclusion: While use of interdisciplinary\napproaches holds a potential to solve challeng-es associated with establishing\nsuch collaborative networks, our findings turn the spotlight on the uncertain\nregulatory landscape playing catch up and the significant operational costs.",
      "tldr_zh": "本研究报告了一个由六个机构组成的跨国联邦健康数据网络（Federated Health Data Network）的实施案例，旨在解决跨境健康数据集中式处理的隐私担忧、数据异质性和法律障碍，促进北欧-波罗的海地区的健康数据二次使用。研究采用混合方法（mixed-method approach），结合实验设计和实施科学（implementation science），评估了网络实施的影响因素。结果显示，该网络在性能上与集中式模拟相比没有显著退化，但强调了跨学科方法（interdisciplinary approaches）的潜力，同时突出了监管景观的不确定性和高运营成本作为主要挑战。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "24 pages (including appendices), 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.17865v1",
      "published_date": "2024-09-26 14:15:54 UTC",
      "updated_date": "2024-09-26 14:15:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:09:14.325747"
    },
    {
      "arxiv_id": "2409.17864v1",
      "title": "A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios",
      "title_zh": "一种多模态单分支嵌入网络，用于冷启动和缺失模态场景中的推荐",
      "authors": [
        "Christian Ganhör",
        "Marta Moscati",
        "Anna Hausberger",
        "Shah Nawaz",
        "Markus Schedl"
      ],
      "abstract": "Most recommender systems adopt collaborative filtering (CF) and provide\nrecommendations based on past collective interactions. Therefore, the\nperformance of CF algorithms degrades when few or no interactions are\navailable, a scenario referred to as cold-start. To address this issue,\nprevious work relies on models leveraging both collaborative data and side\ninformation on the users or items. Similar to multimodal learning, these models\naim at combining collaborative and content representations in a shared\nembedding space. In this work we propose a novel technique for multimodal\nrecommendation, relying on a multimodal Single-Branch embedding network for\nRecommendation (SiBraR). Leveraging weight-sharing, SiBraR encodes interaction\ndata as well as multimodal side information using the same single-branch\nembedding network on different modalities. This makes SiBraR effective in\nscenarios of missing modality, including cold start. Our extensive experiments\non large-scale recommendation datasets from three different recommendation\ndomains (music, movie, and e-commerce) and providing multimodal content\ninformation (audio, text, image, labels, and interactions) show that SiBraR\nsignificantly outperforms CF as well as state-of-the-art content-based RSs in\ncold-start scenarios, and is competitive in warm scenarios. We show that\nSiBraR's recommendations are accurate in missing modality scenarios, and that\nthe model is able to map different modalities to the same region of the shared\nembedding space, hence reducing the modality gap.",
      "tldr_zh": "该论文针对推荐系统在冷启动(cold-start)场景下的性能下降问题，提出了一种新型多模态推荐框架SiBraR（Multimodal Single-Branch embedding network for Recommendation）。SiBraR通过权重共享的单一分支嵌入网络，同时编码互动数据和多模态侧边信息（如音频、文本、图像和标签），从而有效处理缺失模态场景。实验结果显示，在音乐、电影和电商领域的多数据集上，SiBraR在冷启动条件下显著优于collaborative filtering (CF)和其他基于内容的推荐系统，在温启动场景下也保持竞争力，并成功减少了模态间的嵌入空间差距。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at 18th ACM Conference on Recommender Systems (RecSys '24)",
      "pdf_url": "http://arxiv.org/pdf/2409.17864v1",
      "published_date": "2024-09-26 14:12:23 UTC",
      "updated_date": "2024-09-26 14:12:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:09:27.266528"
    },
    {
      "arxiv_id": "2409.17841v1",
      "title": "Machine Learning-based vs Deep Learning-based Anomaly Detection in Multivariate Time Series for Spacecraft Attitude Sensors",
      "title_zh": "翻译失败",
      "authors": [
        "R. Gallon",
        "F. Schiemenz",
        "A. Krstova",
        "A. Menicucci",
        "E. Gill"
      ],
      "abstract": "In the framework of Failure Detection, Isolation and Recovery (FDIR) on\nspacecraft, new AI-based approaches are emerging in the state of the art to\novercome the limitations commonly imposed by traditional threshold checking.\n  The present research aims at characterizing two different approaches to the\nproblem of stuck values detection in multivariate time series coming from\nspacecraft attitude sensors. The analysis reveals the performance differences\nin the two approaches, while commenting on their interpretability and\ngeneralization to different scenarios.",
      "tldr_zh": "这篇论文比较了基于机器学习（Machine Learning-based）和基于深度学习（Deep Learning-based）的异常检测方法，用于航天器姿态传感器（Spacecraft Attitude Sensors）多变量时间序列（Multivariate Time Series）中的卡值（stuck values）检测。研究在故障检测、隔离和恢复（FDIR）框架下，分析了两种方法的性能差异，并评估了它们的解释性和对不同场景的泛化能力。结果表明，这些AI方法能克服传统阈值检查的局限性，为航天器故障管理提供更有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for the ESA SPAICE Conference 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17841v1",
      "published_date": "2024-09-26 13:45:36 UTC",
      "updated_date": "2024-09-26 13:45:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:09:38.805797"
    },
    {
      "arxiv_id": "2409.17840v1",
      "title": "Detecting and Measuring Confounding Using Causal Mechanism Shifts",
      "title_zh": "检测和测量混杂因素",
      "authors": [
        "Abbavaram Gowtham Reddy",
        "Vineeth N Balasubramanian"
      ],
      "abstract": "Detecting and measuring confounding effects from data is a key challenge in\ncausal inference. Existing methods frequently assume causal sufficiency,\ndisregarding the presence of unobserved confounding variables. Causal\nsufficiency is both unrealistic and empirically untestable. Additionally,\nexisting methods make strong parametric assumptions about the underlying causal\ngenerative process to guarantee the identifiability of confounding variables.\nRelaxing the causal sufficiency and parametric assumptions and leveraging\nrecent advancements in causal discovery and confounding analysis with\nnon-i.i.d. data, we propose a comprehensive approach for detecting and\nmeasuring confounding. We consider various definitions of confounding and\nintroduce tailored methodologies to achieve three objectives: (i) detecting and\nmeasuring confounding among a set of variables, (ii) separating observed and\nunobserved confounding effects, and (iii) understanding the relative strengths\nof confounding bias between different sets of variables. We present useful\nproperties of a confounding measure and present measures that satisfy those\nproperties. Empirical results support the theoretical analysis.",
      "tldr_zh": "这篇论文解决了因果推理中检测和测量混杂(confounding effects)的关键挑战，指出现有方法假设因果充分性(causal sufficiency)和强参数假设(parametric assumptions)的局限性，并忽略了未观测混杂变量(unobserved confounding variables)。作者提出了一种全面方法，通过放宽这些假设并利用因果发现(causal discovery)和非i.i.d.数据的进展，实现三个目标：(i) 在变量集间检测和测量混杂，(ii) 分离观测和未观测混杂效应，(iii) 比较不同变量集间的混杂偏差相对强度。论文还引入了满足有用属性的混杂测度，并通过实证结果支持了理论分析。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17840v1",
      "published_date": "2024-09-26 13:44:22 UTC",
      "updated_date": "2024-09-26 13:44:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:09:51.544592"
    },
    {
      "arxiv_id": "2409.17836v2",
      "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hui-Po Wang",
        "Mario Fritz"
      ],
      "abstract": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10% up to 17.2% across various datasets\nand architectures. Additionally, our approach shows promising compatibility\nwith lossy compression techniques such as quantization and sparsification.\nThese findings highlight the significant potential of LLMs as a model for\neffectively handling gradients. Code is available at\nhttps://github.com/hui-po-wang/LM-GC.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 作为零样本 (zero-shot) 无损梯度压缩器的潜力，旨在为神经网络梯度建立一般先验模型，以解决梯度的高维复杂性问题。作者引入了 LM-GC 方法，将 LLMs 与算术编码 (arithmetic coding) 结合，将梯度转换为文本格式，提高 token 效率高达 38 倍，同时保持梯度结构的完整性。实验结果显示，LM-GC 在多种数据集和架构上，比现有无损压缩方法提升压缩率 10% 至 17.2%。此外，该方法与有损压缩技术如量化 (quantization) 和稀疏化 (sparsification) 兼容，展示了 LLMs 在处理梯度方面的广阔应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "camera-ready in NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17836v2",
      "published_date": "2024-09-26 13:38:33 UTC",
      "updated_date": "2025-01-22 09:26:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:10:02.883928"
    },
    {
      "arxiv_id": "2409.17819v1",
      "title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixuan Liu",
        "Zhanhui Zhou",
        "Yuanfu Wang",
        "Chao Yang",
        "Yu Qiao"
      ],
      "abstract": "Large language models are typically fine-tuned to align with human\npreferences, but tuning large models is computationally intensive and complex.\nIn this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method\nthat uses implicit and explicit value functions to guide language model\ndecoding at token and chunk-level respectively, efficiently aligning large\nlanguage models purely at inference time. This approach circumvents the\ncomplexities of direct fine-tuning and outperforms traditional methods.\nEmpirically, we demonstrate the versatility of IVG across various tasks. In\ncontrolled sentiment generation and summarization tasks, our method\nsignificantly improves the alignment of large models using inference-time\nguidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more\nchallenging instruction-following benchmark AlpacaEval 2.0, we show that both\nspecifically tuned and off-the-shelf value functions greatly improve the\nlength-controlled win rates of large models against $\\texttt{gpt-4-turbo}$\n(e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$\nand $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$\nwith Tulu guidance).",
      "tldr_zh": "本研究提出 Integrated Value Guidance (IVG)，一种在推理时（inference-time）对大型语言模型进行对齐的方法，使用隐式和显式价值函数分别在 token 和 chunk 级别指导解码，从而避免了资源密集型的微调过程。IVG 显著提高了模型在情感生成、总结和指令遵循任务中的性能，例如通过 gpt2-based 价值函数提升了输出对齐度。在 AlpacaEval 2.0 基准测试中，IVG 使模型如 Mistral-7B-Instruct-v0.2 的胜率从 19.51% 提高到 26.51%，并为 Mixtral-8x7B-Instruct-v0.1 带来类似提升，证明了其在提升模型表现方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2409.17819v1",
      "published_date": "2024-09-26 13:15:18 UTC",
      "updated_date": "2024-09-26 13:15:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:10:15.398910"
    },
    {
      "arxiv_id": "2409.17815v1",
      "title": "DREAMS: A python framework to train deep learning models with model card reporting for medical and health applications",
      "title_zh": "翻译失败",
      "authors": [
        "Rabindra Khadka",
        "Pedro G Lind",
        "Anis Yazidi",
        "Asma Belhadi"
      ],
      "abstract": "Electroencephalography (EEG) data provides a non-invasive method for\nresearchers and clinicians to observe brain activity in real time. The\nintegration of deep learning techniques with EEG data has significantly\nimproved the ability to identify meaningful patterns, leading to valuable\ninsights for both clinical and research purposes. However, most of the\nframeworks so far, designed for EEG data analysis, are either too focused on\npre-processing or in deep learning methods per, making their use for both\nclinician and developer communities problematic. Moreover, critical issues such\nas ethical considerations, biases, uncertainties, and the limitations inherent\nin AI models for EEG data analysis are frequently overlooked, posing challenges\nto the responsible implementation of these technologies. In this paper, we\nintroduce a comprehensive deep learning framework tailored for EEG data\nprocessing, model training and report generation. While constructed in way to\nbe adapted and developed further by AI developers, it enables to report,\nthrough model cards, the outcome and specific information of use for both\ndevelopers and clinicians. In this way, we discuss how this framework can, in\nthe future, provide clinical researchers and developers with the tools needed\nto create transparent and accountable AI models for EEG data analysis and\ndiagnosis.",
      "tldr_zh": "本论文介绍了 DREAMS，一个 Python 框架，旨在训练深度学习模型并生成模型卡 (model cards) 报告，专注于医疗和健康应用中的 EEG 数据分析。该框架整合了 EEG 数据处理、模型训练和报告生成功能，解决了现有框架在预处理和深度学习方法上的局限性，同时强调伦理考虑、偏差、不确定性和 AI 模型的限制，以促进负责任的实施。通过模型卡，DREAMS 提供透明的信息，支持开发者和服务临床人员，最终有助于创建可解释的 AI 模型用于 EEG 数据分析和诊断。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17815v1",
      "published_date": "2024-09-26 13:12:13 UTC",
      "updated_date": "2024-09-26 13:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:10:27.154716"
    },
    {
      "arxiv_id": "2409.17791v1",
      "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
      "title_zh": "自监督偏好优化：通过",
      "authors": [
        "Jian Li",
        "Haojing Huang",
        "Yujia Zhang",
        "Pengfei Xu",
        "Xi Chen",
        "Rui Song",
        "Lida Shi",
        "Jingwen Wang",
        "Hao Xu"
      ],
      "abstract": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.",
      "tldr_zh": "本研究针对现有偏好优化方法（如 Direct Preference Optimization, DPO）在 Reinforcement Learning with Human Feedback (RLHF) 中的局限性，指出它们仅使用二元交叉熵忽略了响应间的偏好程度差异，从而阻碍 Large Language Models (LLMs) 对人类偏好的充分理解。  \n为此，作者提出 Self-supervised Preference Optimization (SPO) 框架，该框架结合自监督偏好度损失与对齐损失，帮助 LLMs 提升对偏好程度的感知能力。  \n在两个常用数据集上的广泛实验显示，SPO 可无缝集成到现有方法中，显著提升性能并达到 state-of-the-art 水平，同时详细分析验证了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2409.17791v1",
      "published_date": "2024-09-26 12:37:26 UTC",
      "updated_date": "2024-09-26 12:37:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:10:38.439770"
    },
    {
      "arxiv_id": "2409.17788v1",
      "title": "Ophthalmic Biomarker Detection with Parallel Prediction of Transformer and Convolutional Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Md. Touhidul Islam",
        "Md. Abtahi Majeed Chowdhury",
        "Mahmudul Hasan",
        "Asif Quadir",
        "Lutfa Aktar"
      ],
      "abstract": "Ophthalmic diseases represent a significant global health issue,\nnecessitating the use of advanced precise diagnostic tools. Optical Coherence\nTomography (OCT) imagery which offers high-resolution cross-sectional images of\nthe retina has become a pivotal imaging modality in ophthalmology.\nTraditionally physicians have manually detected various diseases and biomarkers\nfrom such diagnostic imagery. In recent times, deep learning techniques have\nbeen extensively used for medical diagnostic tasks enabling fast and precise\ndiagnosis. This paper presents a novel approach for ophthalmic biomarker\ndetection using an ensemble of Convolutional Neural Network (CNN) and Vision\nTransformer. While CNNs are good for feature extraction within the local\ncontext of the image, transformers are known for their ability to extract\nfeatures from the global context of the image. Using an ensemble of both\ntechniques allows us to harness the best of both worlds. Our method has been\nimplemented on the OLIVES dataset to detect 6 major biomarkers from the OCT\nimages and shows significant improvement of the macro averaged F1 score on the\ndataset.",
      "tldr_zh": "这篇论文提出了一种新型眼科生物标志物检测方法，使用 CNN 和 Vision Transformer 的集成架构进行并行预测，旨在结合 CNN 的局部特征提取优势与 Transformer 的全局特征提取能力。方法应用于 OLIVES 数据集，对 OCT 图像进行分析，成功检测了 6 种主要生物标志物。实验结果显示，该集成方法显著提高了宏平均 F1 分数，增强了眼科疾病诊断的精确性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17788v1",
      "published_date": "2024-09-26 12:33:34 UTC",
      "updated_date": "2024-09-26 12:33:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:10:50.256745"
    },
    {
      "arxiv_id": "2409.17777v3",
      "title": "Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Raja Kumar",
        "Raghav Singhal",
        "Pranamya Kulkarni",
        "Deval Mehta",
        "Kshitij Jadhav"
      ],
      "abstract": "Deep multimodal learning has shown remarkable success by leveraging\ncontrastive learning to capture explicit one-to-one relations across\nmodalities. However, real-world data often exhibits shared relations beyond\nsimple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive\nLearning approach to capture nuanced shared relations inherent in multimodal\ndata. Our key contribution is a Mixup-based contrastive loss that learns robust\nrepresentations by aligning mixed samples from one modality with their\ncorresponding samples from other modalities thereby capturing shared relations\nbetween them. For multimodal classification tasks, we introduce a framework\nthat integrates a fusion module with unimodal prediction modules for auxiliary\nsupervision during training, complemented by our proposed Mixup-based\ncontrastive loss. Through extensive experiments on diverse datasets (N24News,\nROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures\nshared multimodal relations and generalizes across domains. It outperforms\nstate-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving\ncomparable performance on Food-101. Our work highlights the significance of\nlearning shared relations for robust multimodal learning, opening up promising\navenues for future research. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/M3CoL.",
      "tldr_zh": "本研究提出 M3CoL（Multimodal Mixup Contrastive Learning），一种新颖的多模态混合对比学习方法，旨在捕捉多模态数据中超越简单一对一关系的共享关系，从而提升多模态分类任务的鲁棒性。M3CoL 通过 Mixup-based contrastive loss 将一个模态的混合样本与对应模态样本对齐，同时整合融合模块和单模态预测模块提供辅助监督，以学习更有效的多模态表示。在 N24News、ROSMAP、BRCA 和 Food-101 数据集的实验中，M3CoL 超越了最先进方法，在前三个数据集上实现性能提升，并在 Food-101 上表现出可比结果，证明了学习共享关系的价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "RK and RS contributed equally to this work, 20 Pages, 8 Figures, 9\n  Tables. Another version of the paper accepted at NeurIPS 2024 Workshop on\n  Unifying Representations in Neural Models (UniReps)",
      "pdf_url": "http://arxiv.org/pdf/2409.17777v3",
      "published_date": "2024-09-26 12:15:13 UTC",
      "updated_date": "2024-12-06 06:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:11:03.732699"
    },
    {
      "arxiv_id": "2409.17774v2",
      "title": "Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Supriya Manna",
        "Niladri Sett"
      ],
      "abstract": "Faithfulness is arguably the most critical metric to assess the reliability\nof explainable AI. In NLP, current methods for faithfulness evaluation are\nfraught with discrepancies and biases, often failing to capture the true\nreasoning of models. We introduce Adversarial Sensitivity as a novel approach\nto faithfulness evaluation, focusing on the explainer's response when the model\nis under adversarial attack. Our method accounts for the faithfulness of\nexplainers by capturing sensitivity to adversarial input changes. This work\naddresses significant limitations in existing evaluation techniques, and\nfurthermore, quantifies faithfulness from a crucial yet underexplored paradigm.",
      "tldr_zh": "该论文强调了Faithfulness作为评估NLP解释可靠性的关键指标，但现有方法存在偏差和不一致，无法准确捕捉模型的真实推理。作者引入了Adversarial Sensitivity作为一种新颖的评估方法，通过分析解释器对模型遭受对抗性攻击时的响应，来衡量其对输入变化的敏感性。该方法填补了现有技术的空白，并从一个未充分探索的范式中量化了Faithfulness，从而提升NLP解释的可信度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a Full Paper at EMNLP 2024 Workshop BlackBoxNLP",
      "pdf_url": "http://arxiv.org/pdf/2409.17774v2",
      "published_date": "2024-09-26 12:11:28 UTC",
      "updated_date": "2024-10-09 11:59:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:11:14.007294"
    },
    {
      "arxiv_id": "2409.17767v1",
      "title": "Federated Learning under Attack: Improving Gradient Inversion for Batch of Images",
      "title_zh": "翻译失败",
      "authors": [
        "Luiz Leite",
        "Yuri Santo",
        "Bruno L. Dalmazo",
        "André Riker"
      ],
      "abstract": "Federated Learning (FL) has emerged as a machine learning approach able to\npreserve the privacy of user's data. Applying FL, clients train machine\nlearning models on a local dataset and a central server aggregates the learned\nparameters coming from the clients, training a global machine learning model\nwithout sharing user's data. However, the state-of-the-art shows several\napproaches to promote attacks on FL systems. For instance, inverting or leaking\ngradient attacks can find, with high precision, the local dataset used during\nthe training phase of the FL. This paper presents an approach, called Deep\nLeakage from Gradients with Feedback Blending (DLG-FB), which is able to\nimprove the inverting gradient attack, considering the spatial correlation that\ntypically exists in batches of images. The performed evaluation shows an\nimprovement of 19.18% and 48,82% in terms of attack success rate and the number\nof iterations per attacked image, respectively.",
      "tldr_zh": "该论文探讨了Federated Learning (FL) 面临的隐私风险，特别是梯度反演攻击，该攻击能高精度恢复客户端的本地数据集。作者提出了一种改进方法Deep Leakage from Gradients with Feedback Blending (DLG-FB)，通过考虑图像批次中的空间相关性来提升攻击效率和成功率。实验结果显示，DLG-FB将攻击成功率提高了19.18%，并将每张攻击图像的迭代次数减少48.82%，从而突显了FL系统潜在的安全漏洞。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.17767v1",
      "published_date": "2024-09-26 12:02:36 UTC",
      "updated_date": "2024-09-26 12:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:11:25.894595"
    },
    {
      "arxiv_id": "2409.17763v2",
      "title": "Confidence intervals uncovered: Are we ready for real-world medical imaging AI?",
      "title_zh": "翻译失败",
      "authors": [
        "Evangelia Christodoulou",
        "Annika Reinke",
        "Rola Houhou",
        "Piotr Kalinowski",
        "Selen Erkan",
        "Carole H. Sudre",
        "Ninon Burgos",
        "Sofiène Boutaj",
        "Sophie Loizillon",
        "Maëlys Solal",
        "Nicola Rieke",
        "Veronika Cheplygina",
        "Michela Antonelli",
        "Leon D. Mayer",
        "Minu D. Tizabi",
        "M. Jorge Cardoso",
        "Amber Simpson",
        "Paul F. Jäger",
        "Annette Kopp-Schneider",
        "Gaël Varoquaux",
        "Olivier Colliot",
        "Lena Maier-Hein"
      ],
      "abstract": "Medical imaging is spearheading the AI transformation of healthcare.\nPerformance reporting is key to determine which methods should be translated\ninto clinical practice. Frequently, broad conclusions are simply derived from\nmean performance values. In this paper, we argue that this common practice is\noften a misleading simplification as it ignores performance variability. Our\ncontribution is threefold. (1) Analyzing all MICCAI segmentation papers (n =\n221) published in 2023, we first observe that more than 50% of papers do not\nassess performance variability at all. Moreover, only one (0.5%) paper reported\nconfidence intervals (CIs) for model performance. (2) To address the reporting\nbottleneck, we show that the unreported standard deviation (SD) in segmentation\npapers can be approximated by a second-order polynomial function of the mean\nDice similarity coefficient (DSC). Based on external validation data from 56\nprevious MICCAI challenges, we demonstrate that this approximation can\naccurately reconstruct the CI of a method using information provided in\npublications. (3) Finally, we reconstructed 95% CIs around the mean DSC of\nMICCAI 2023 segmentation papers. The median CI width was 0.03 which is three\ntimes larger than the median performance gap between the first and second\nranked method. For more than 60% of papers, the mean performance of the\nsecond-ranked method was within the CI of the first-ranked method. We conclude\nthat current publications typically do not provide sufficient evidence to\nsupport which models could potentially be translated into clinical practice.",
      "tldr_zh": "该论文质疑了医疗成像 AI 是否准备好应用于现实世界，强调当前性能报告仅依赖平均值（如 mean DSC）而忽略性能变异性，导致误导性结论。研究分析了 2023 年 MICCAI 分割论文（n=221），发现超过 50% 的论文未评估性能变异性，且仅有 0.5% 的论文报告了 Confidence intervals (CIs)。为了解决报告不足问题，作者提出用二阶多项式函数近似未报告的标准差 (SD)，并基于外部验证数据准确重建 CI。最终重建显示，MICCAI 2023 论文的中间 CI 宽度为 0.03，是排名差距中位数的三倍，且超过 60% 的论文中第二排名方法的性能落入第一排名方法的 CI 内，因此当前出版物缺乏足够证据支持模型向临床实践转化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper accepted at MICCAI 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2409.17763v2",
      "published_date": "2024-09-26 11:58:41 UTC",
      "updated_date": "2024-09-27 06:50:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:11:40.091907"
    },
    {
      "arxiv_id": "2409.17757v1",
      "title": "Integrating Hierarchical Semantic into Iterative Generation Model for Entailment Tree Explanation",
      "title_zh": "翻译失败",
      "authors": [
        "Qin Wang",
        "Jianzhou Feng",
        "Yiming Xu"
      ],
      "abstract": "Manifestly and logically displaying the line of reasoning from evidence to\nanswer is significant to explainable question answering (QA). The entailment\ntree exhibits the lines structurally, which is different from the\nself-explanation principle in large-scale language models. Existing methods\nrarely consider the semantic association of sentences between and within\nhierarchies within the tree structure, which is prone to apparent mistakes in\ncombinations. In this work, we propose an architecture of integrating the\nHierarchical Semantics of sentences under the framework of Controller-Generator\n(HiSCG) to explain answers. The HiSCG designs a hierarchical mapping between\nhypotheses and facts, discriminates the facts involved in tree constructions,\nand optimizes single-step entailments. To the best of our knowledge, We are the\nfirst to notice hierarchical semantics of sentences between the same layer and\nadjacent layers to yield improvements. The proposed method achieves comparable\nperformance on all three settings of the EntailmentBank dataset. The\ngeneralization results on two out-of-domain datasets also demonstrate the\neffectiveness of our method.",
      "tldr_zh": "该论文针对可解释性问答（QA）提出了一种整合层次语义的迭代生成模型（HiSCG），旨在通过Controller-Generator框架构建entailment tree，以显式显示从证据到答案的推理路径。HiSCG方法包括设计假设和事实之间的层次映射、区分树结构中的相关事实，以及优化单步蕴含，从而解决现有方法忽略句子间层次语义关联导致的错误问题。该方法在EntailmentBank数据集的三种设置上实现了可比性能，并在两个out-of-domain数据集上展示了良好的泛化效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17757v1",
      "published_date": "2024-09-26 11:46:58 UTC",
      "updated_date": "2024-09-26 11:46:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:11:50.904830"
    },
    {
      "arxiv_id": "2409.17755v2",
      "title": "SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rimvydas Rubavicius",
        "Peter David Fagan",
        "Alex Lascarides",
        "Subramanian Ramamoorthy"
      ],
      "abstract": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the agent is unaware of a concept that is key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems. It uses embodied conversation to fix its\ndeficient domain model -- through dialogue, the agent discovers and then learns\nto exploit unforeseen possibilities. In particular, SECURE learns from the\nuser's embodied corrective feedback when it makes a mistake, and it makes\nstrategic dialogue decisions to reveal useful evidence about novel concepts for\nsolving the instructed task. Together, these abilities allow the agent to\ngeneralise to subsequent tasks using newly acquired knowledge. We demonstrate\nthat learning to solve rearrangement under unawareness is more data efficient\nwhen the agent is semantics-aware -- that is, during both learning and\ninference it augments the evidence from the user's embodied conversation with\nits logical consequences, stemming from semantic analysis.",
      "tldr_zh": "这篇论文针对“rearrangement under unawareness”场景提出 SECURE 框架，用于终身机器人学习，帮助代理在不知晓关键概念的情况下操作刚体环境。SECURE 通过 embodied conversation 与用户互动，学习纠正反馈并进行战略对话决策，以发现新概念并利用它们解决问题。同时，该框架强调 semantics-aware 机制，通过语义分析增强证据，显著提高了学习效率和任务泛化能力。实验结果表明，这种方法在数据利用上更高效，为机器人适应未知环境奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "22 pages,4 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.17755v2",
      "published_date": "2024-09-26 11:40:07 UTC",
      "updated_date": "2025-02-10 18:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:12:03.584686"
    },
    {
      "arxiv_id": "2409.17754v1",
      "title": "Byzantine-Robust Aggregation for Securing Decentralized Federated Learning",
      "title_zh": "拜占庭鲁棒聚合用于",
      "authors": [
        "Diego Cajaraville-Aboy",
        "Ana Fernández-Vilas",
        "Rebeca P. Díaz-Redondo",
        "Manuel Fernández-Veiga"
      ],
      "abstract": "Federated Learning (FL) emerges as a distributed machine learning approach\nthat addresses privacy concerns by training AI models locally on devices.\nDecentralized Federated Learning (DFL) extends the FL paradigm by eliminating\nthe central server, thereby enhancing scalability and robustness through the\navoidance of a single point of failure. However, DFL faces significant\nchallenges in optimizing security, as most Byzantine-robust algorithms proposed\nin the literature are designed for centralized scenarios. In this paper, we\npresent a novel Byzantine-robust aggregation algorithm to enhance the security\nof Decentralized Federated Learning environments, coined WFAgg. This proposal\nhandles the adverse conditions and strength robustness of dynamic decentralized\ntopologies at the same time by employing multiple filters to identify and\nmitigate Byzantine attacks. Experimental results demonstrate the effectiveness\nof the proposed algorithm in maintaining model accuracy and convergence in the\npresence of various Byzantine attack scenarios, outperforming state-of-the-art\ncentralized Byzantine-robust aggregation schemes (such as Multi-Krum or\nClustering). These algorithms are evaluated on an IID image classification\nproblem in both centralized and decentralized scenarios.",
      "tldr_zh": "本研究针对去中心化联邦学习(Decentralized Federated Learning, DFL)中的安全挑战，提出了一种新型Byzantine-robust聚合算法WFAgg，以应对动态拓扑下的恶意攻击。WFAgg通过部署多个过滤器来识别和缓解Byzantine攻击，同时优化模型的鲁棒性和收敛性。实验结果显示，该算法在各种Byzantine攻击场景下维持了模型准确性，并在IID图像分类任务上优于现有中心化方案，如Multi-Krum或Clustering。总的来说，这为DFL的安全性提供了有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 7 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2409.17754v1",
      "published_date": "2024-09-26 11:36:08 UTC",
      "updated_date": "2024-09-26 11:36:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:12:15.337018"
    },
    {
      "arxiv_id": "2409.17728v1",
      "title": "AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking",
      "title_zh": "翻译失败",
      "authors": [
        "Shiqi Sun",
        "Yantao Lu",
        "Ning Liu",
        "Bo Jiang",
        "JinChao Chen",
        "Ying Zhang"
      ],
      "abstract": "Camera-LiDAR fusion models significantly enhance perception performance in\nautonomous driving. The fusion mechanism leverages the strengths of each\nmodality while minimizing their weaknesses. Moreover, in practice, camera-LiDAR\nfusion models utilize pre-trained backbones for efficient training. However, we\nargue that directly loading single-modal pre-trained camera and LiDAR backbones\ninto camera-LiDAR fusion models introduces similar feature redundancy across\nmodalities due to the nature of the fusion mechanism. Unfortunately, existing\npruning methods are developed explicitly for single-modal models, and thus,\nthey struggle to effectively identify these specific redundant parameters in\ncamera-LiDAR fusion models. In this paper, to address the issue above on\ncamera-LiDAR fusion models, we propose a novelty pruning framework Alternative\nModality Masking Pruning (AlterMOMA), which employs alternative masking on each\nmodality and identifies the redundant parameters. Specifically, when one\nmodality parameters are masked (deactivated), the absence of features from the\nmasked backbone compels the model to reactivate previous redundant features of\nthe other modality backbone. Therefore, these redundant features and relevant\nredundant parameters can be identified via the reactivation process. The\nredundant parameters can be pruned by our proposed importance score evaluation\nfunction, Alternative Evaluation (AlterEva), which is based on the observation\nof the loss changes when certain modality parameters are activated and\ndeactivated. Extensive experiments on the nuScene and KITTI datasets\nencompassing diverse tasks, baseline models, and pruning algorithms showcase\nthat AlterMOMA outperforms existing pruning methods, attaining state-of-the-art\nperformance.",
      "tldr_zh": "该论文针对 Camera-LiDAR 融合模型中的特征冗余问题，提出了一种新颖的修剪框架 AlterMOMA，通过 Alternative Modality Masking 交替掩盖模态来识别冗余参数。具体而言，当一个模态的参数被 deactivation 时，模型会重新激活另一个模态的冗余特征，并利用 AlterEva 重要性评分函数基于损失变化进行评估和修剪。实验结果显示，AlterMOMA 在 nuScene 和 KITTI 数据集上的多种任务中，超越现有修剪方法，实现了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 3 figures, Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17728v1",
      "published_date": "2024-09-26 10:57:02 UTC",
      "updated_date": "2024-09-26 10:57:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:12:27.943945"
    },
    {
      "arxiv_id": "2409.17702v1",
      "title": "Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience",
      "title_zh": "翻译失败",
      "authors": [
        "Leonard Bärmann",
        "Chad DeChant",
        "Joana Plewnia",
        "Fabian Peller-Konrad",
        "Daniel Bauer",
        "Tamim Asfour",
        "Alex Waibel"
      ],
      "abstract": "Verbalization of robot experience, i.e., summarization of and question\nanswering about a robot's past, is a crucial ability for improving human-robot\ninteraction. Previous works applied rule-based systems or fine-tuned deep\nmodels to verbalize short (several-minute-long) streams of episodic data,\nlimiting generalization and transferability. In our work, we apply large\npretrained models to tackle this task with zero or few examples, and\nspecifically focus on verbalizing life-long experiences. For this, we derive a\ntree-like data structure from episodic memory (EM), with lower levels\nrepresenting raw perception and proprioception data, and higher levels\nabstracting events to natural language concepts. Given such a hierarchical\nrepresentation built from the experience stream, we apply a large language\nmodel as an agent to interactively search the EM given a user's query,\ndynamically expanding (initially collapsed) tree nodes to find the relevant\ninformation. The approach keeps computational costs low even when scaling to\nmonths of robot experience data. We evaluate our method on simulated household\nrobot data, human egocentric videos, and real-world robot recordings,\ndemonstrating its flexibility and scalability.",
      "tldr_zh": "该论文提出了一种使用层次化表示（hierarchical representations）的方法，来实现机器人终身经验的 episodic memory verbalization，从而提升人机交互能力。该方法从 episodic memory 构建树状数据结构，低层表示原始感知和本体感觉数据，高层抽象为自然语言概念，并利用大型语言模型作为代理，根据用户查询交互式搜索和动态扩展节点，以保持计算成本低。实验在模拟家庭机器人数据、人类第一人称视频和真实机器人记录上进行，证明了该方法的灵活性和可扩展性，即使处理数月数据也能高效运行。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Code, data and demo videos at https://hierarchical-emv.github.io",
      "pdf_url": "http://arxiv.org/pdf/2409.17702v1",
      "published_date": "2024-09-26 10:16:08 UTC",
      "updated_date": "2024-09-26 10:16:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:12:39.945828"
    },
    {
      "arxiv_id": "2409.17699v3",
      "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Giandomenico Cornacchia",
        "Giulio Zizzo",
        "Kieran Fraser",
        "Muhammad Zaid Hameed",
        "Ambrish Rawat",
        "Mark Purcell"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.",
      "tldr_zh": "这篇论文强调了大型语言模型 (LLMs) 在防范jailbreak attacks方面的迫切需求，并介绍了MoJE（Mixture of Jailbreak Experts）作为一种新型防护机制。MoJE 利用简单的语言统计技术和Naive Tabular Classifiers 来高效检测攻击，确保检测过程计算开销最小。实验结果显示，MoJE 能够识别90%的攻击，同时不对正常提示造成干扰，从而显著提升LLMs 的安全性和数据完整性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17699v3",
      "published_date": "2024-09-26 10:12:19 UTC",
      "updated_date": "2024-10-04 07:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:12:51.378305"
    },
    {
      "arxiv_id": "2409.17698v1",
      "title": "The application of GPT-4 in grading design university students' assignment and providing feedback: An exploratory study",
      "title_zh": "GPT-4 在评估设计大学",
      "authors": [
        "Qian Huang",
        "Thijs Willems",
        "King Wang Poon"
      ],
      "abstract": "This study aims to investigate whether GPT-4 can effectively grade\nassignments for design university students and provide useful feedback. In\ndesign education, assignments do not have a single correct answer and often\ninvolve solving an open-ended design problem. This subjective nature of design\nprojects often leads to grading problems,as grades can vary between different\nraters,for instance instructor from engineering background or architecture\nbackground. This study employs an iterative research approach in developing a\nCustom GPT with the aim of achieving more reliable results and testing whether\nit can provide design students with constructive feedback. The findings\ninclude: First,through several rounds of iterations the inter-reliability\nbetween GPT and human raters reached a level that is generally accepted by\neducators. This indicates that by providing accurate prompts to GPT,and\ncontinuously iterating to build a Custom GPT, it can be used to effectively\ngrade students' design assignments, serving as a reliable complement to human\nraters. Second, the intra-reliability of GPT's scoring at different times is\nbetween 0.65 and 0.78. This indicates that, with adequate instructions, a\nCustom GPT gives consistent results which is a precondition for grading\nstudents. As consistency and comparability are the two main rules to ensure the\nreliability of educational assessment, this study has looked at whether a\nCustom GPT can be developed that adheres to these two rules. We finish the\npaper by testing whether Custom GPT can provide students with useful feedback\nand reflecting on how educators can develop and iterate a Custom GPT to serve\nas a complementary rater.",
      "tldr_zh": "本研究探讨了 GPT-4 在评估设计大学学生作业和提供反馈的有效性，针对设计任务的主观性和评分变异问题，采用迭代方法开发 Custom GPT 以提高可靠性。研究发现，通过精确提示和多次迭代，GPT-4 与人类评估者间的 inter-reliability 达到了教育上可接受水平，且其 intra-reliability（不同时间评分一致性）在 0.65 到 0.78 之间。最终，Custom GPT 被证明能作为可靠的补充评估工具，提供建设性反馈，从而提升教育评估的可靠性和可比性。",
      "categories": [
        "cs.AI",
        "1.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.17698v1",
      "published_date": "2024-09-26 10:09:10 UTC",
      "updated_date": "2024-09-26 10:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:13:14.054061"
    },
    {
      "arxiv_id": "2409.17692v3",
      "title": "MIO: A Foundation Model on Multimodal Tokens",
      "title_zh": "MIO：基于多模态令牌的基础模型",
      "authors": [
        "Zekun Wang",
        "King Zhu",
        "Chunpu Xu",
        "Wangchunshu Zhou",
        "Jiaheng Liu",
        "Yibo Zhang",
        "Jiashuo Wang",
        "Ning Shi",
        "Siyu Li",
        "Yizhi Li",
        "Haoran Que",
        "Zhaoxiang Zhang",
        "Yuanxing Zhang",
        "Ge Zhang",
        "Ke Xu",
        "Jie Fu",
        "Wenhao Huang"
      ],
      "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
      "tldr_zh": "本论文提出 MIO，一种基于多模态标记（multimodal tokens）的基金会模型，能够端到端、自回归方式理解和生成语音、文本、图像及视频，解决了现有大型语言模型（LLMs）和多模态大型语言模型（MM-LLMs）在 any-to-any 理解和生成方面的局限性。MIO 通过四阶段训练过程，包括 alignment pre-training、interleaved pre-training、speech-enhanced pre-training 和全面监督微调，利用混合离散标记和因果多模态建模（causal multimodal modeling）来提升跨模态性能。实验结果表明，MIO 在多种任务上与双模态、any-to-any 或模态特定基线模型相比表现出竞争性或优越性，并展示了高级功能，如交错视频文本生成、链式视觉思维推理（chain-of-visual-thought reasoning）和指令图像编辑。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
      "pdf_url": "http://arxiv.org/pdf/2409.17692v3",
      "published_date": "2024-09-26 09:57:16 UTC",
      "updated_date": "2025-01-13 07:41:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:13:17.034316"
    },
    {
      "arxiv_id": "2409.17691v1",
      "title": "Efficient Bias Mitigation Without Privileged Information",
      "title_zh": "翻译失败",
      "authors": [
        "Mateo Espinosa Zarlenga",
        "Swami Sankaranarayanan",
        "Jerone T. A. Andrews",
        "Zohreh Shams",
        "Mateja Jamnik",
        "Alice Xiang"
      ],
      "abstract": "Deep neural networks trained via empirical risk minimisation often exhibit\nsignificant performance disparities across groups, particularly when group and\ntask labels are spuriously correlated (e.g., \"grassy background\" and \"cows\").\nExisting bias mitigation methods that aim to address this issue often either\nrely on group labels for training or validation, or require an extensive\nhyperparameter search. Such data and computational requirements hinder the\npractical deployment of these methods, especially when datasets are too large\nto be group-annotated, computational resources are limited, and models are\ntrained through already complex pipelines. In this paper, we propose Targeted\nAugmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework\nthat leverages the entire training history of a helper model to identify\nspurious samples, and generate a group-balanced training set from which a\nrobust model can be trained. We show that TAB improves worst-group performance\nwithout any group information or model selection, outperforming existing\nmethods while maintaining overall accuracy.",
      "tldr_zh": "该研究针对深度神经网络在经验风险最小化(Empirical Risk Minimisation)训练中存在的群体性能差异问题，提出了一种无需特权信息(TAB)框架，即Targeted Augmentations for Bias Mitigation (TAB)。TAB 通过利用辅助模型的整个训练历史来识别虚假样本，并生成群体平衡的训练集，从而训练出更鲁棒的模型，而无需群体标签或超参数搜索。实验结果显示，TAB 在不影响整体准确性的前提下，显著提升了最差群体性能(worst-group performance)，并优于现有偏置缓解方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 18th European Conference on Computer Vision (ECCV\n  2024) as an Oral presentation",
      "pdf_url": "http://arxiv.org/pdf/2409.17691v1",
      "published_date": "2024-09-26 09:56:13 UTC",
      "updated_date": "2024-09-26 09:56:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:13:27.878148"
    },
    {
      "arxiv_id": "2409.17687v2",
      "title": "Graph Edit Distance with General Costs Using Neural Set Divergence",
      "title_zh": "翻译失败",
      "authors": [
        "Eeshaan Jain",
        "Indradyumna Roy",
        "Saswat Meher",
        "Soumen Chakrabarti",
        "Abir De"
      ],
      "abstract": "Graph Edit Distance (GED) measures the (dis-)similarity between two given\ngraphs, in terms of the minimum-cost edit sequence that transforms one graph to\nthe other. However, the exact computation of GED is NP-Hard, which has recently\nmotivated the design of neural methods for GED estimation. However, they do not\nexplicitly account for edit operations with different costs. In response, we\npropose GRAPHEDX, a neural GED estimator that can work with general costs\nspecified for the four edit operations, viz., edge deletion, edge addition,\nnode deletion and node addition. We first present GED as a quadratic assignment\nproblem (QAP) that incorporates these four costs. Then, we represent each graph\nas a set of node and edge embeddings and use them to design a family of neural\nset divergence surrogates. We replace the QAP terms corresponding to each\noperation with their surrogates. Computing such neural set divergence require\naligning nodes and edges of the two graphs. We learn these alignments using a\nGumbel-Sinkhorn permutation generator, additionally ensuring that the node and\nedge alignments are consistent with each other. Moreover, these alignments are\ncognizant of both the presence and absence of edges between node-pairs.\nExperiments on several datasets, under a variety of edit cost settings, show\nthat GRAPHEDX consistently outperforms state-of-the-art methods and heuristics\nin terms of prediction error.",
      "tldr_zh": "Graph Edit Distance (GED) 是衡量两个图之间相似度的指标，但其精确计算为 NP-Hard，且现有神经方法未充分考虑不同编辑操作的成本。论文提出 GRAPHEDX，一种神经 GED 估计器，将 GED 表述为 Quadratic Assignment Problem (QAP)，并使用节点和边嵌入设计神经集发散代理，通过 Gumbel-Sinkhorn 排列生成器学习一致的节点和边对齐，同时考虑边存在与否。实验结果显示，GRAPHEDX 在多个数据集和各种编辑成本设置下，显著优于现有方法和启发式方法，在预测误差方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17687v2",
      "published_date": "2024-09-26 09:51:29 UTC",
      "updated_date": "2024-11-04 11:23:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:13:39.889225"
    },
    {
      "arxiv_id": "2409.17685v2",
      "title": "Feature-to-Image Data Augmentation: Improving Model Feature Extraction with Cluster-Guided Synthetic Samples",
      "title_zh": "翻译失败",
      "authors": [
        "Yasaman Haghbin",
        "Hadi Moradi",
        "Reshad Hosseini"
      ],
      "abstract": "One of the growing trends in machine learning is the use of data generation\ntechniques, since the performance of machine learning models is dependent on\nthe quantity of the training dataset. However, in many real-world applications,\nparticularly in medical and low-resource domains, collecting large datasets is\nchallenging due to resource constraints, which leads to overfitting and poor\ngeneralization. This study introduces FICAug, a novel feature-to-image data\naugmentation framework designed to improve model generalization under limited\ndata conditions by generating structured synthetic samples.\n  FICAug first operates in the feature space, where original data are clustered\nusing the k-means algorithm. Within pure-label clusters, synthetic data are\ngenerated through Gaussian sampling to increase diversity while maintaining\nlabel consistency. These synthetic features are then projected back into the\nimage domain using a generative neural network, and a convolutional neural\nnetwork is trained on the reconstructed images to learn enhanced\nrepresentations.\n  Experimental results demonstrate that FICAug significantly improves\nclassification accuracy. In feature space, it achieved a cross-validation\naccuracy of 84.09%, while training a ResNet-18 model on the reconstructed\nimages further boosted performance to 88.63%, illustrating the effectiveness of\nthe proposed framework in extracting new and task-relevant features.",
      "tldr_zh": "本研究提出了一种名为 FICAug 的新颖 feature-to-image 数据增强框架，旨在通过生成结构化的合成样本来改善机器学习模型在数据有限场景下的特征提取和泛化能力，特别是针对医疗和资源不足领域的问题。框架首先在特征空间使用 k-means 算法进行数据聚类，然后在纯标签聚类中通过 Gaussian sampling 生成合成数据，以增强多样性并保持标签一致性；随后，将这些合成特征投影回图像域，并使用生成神经网络重建图像，最后在重建图像上训练如 ResNet-18 的卷积神经网络以学习增强表示。实验结果显示，FICAug 在特征空间的交叉验证准确率达到 84.09%，而在重建图像上训练模型后进一步提升至 88.63%，证明了其在提高分类性能方面的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 6 figures, 6 table",
      "pdf_url": "http://arxiv.org/pdf/2409.17685v2",
      "published_date": "2024-09-26 09:51:08 UTC",
      "updated_date": "2025-04-24 06:08:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:13:52.171571"
    },
    {
      "arxiv_id": "2409.17684v1",
      "title": "Preserving logical and functional dependencies in synthetic tabular data",
      "title_zh": "翻译失败",
      "authors": [
        "Chaithra Umesh",
        "Kristian Schultz",
        "Manjunath Mahendra",
        "Saparshi Bej",
        "Olaf Wolkenhauer"
      ],
      "abstract": "Dependencies among attributes are a common aspect of tabular data. However,\nwhether existing tabular data generation algorithms preserve these dependencies\nwhile generating synthetic data is yet to be explored. In addition to the\nexisting notion of functional dependencies, we introduce the notion of logical\ndependencies among the attributes in this article. Moreover, we provide a\nmeasure to quantify logical dependencies among attributes in tabular data.\nUtilizing this measure, we compare several state-of-the-art synthetic data\ngeneration algorithms and test their capability to preserve logical and\nfunctional dependencies on several publicly available datasets. We demonstrate\nthat currently available synthetic tabular data generation algorithms do not\nfully preserve functional dependencies when they generate synthetic datasets.\nIn addition, we also showed that some tabular synthetic data generation models\ncan preserve inter-attribute logical dependencies. Our review and comparison of\nthe state-of-the-art reveal research needs and opportunities to develop\ntask-specific synthetic tabular data generation models.",
      "tldr_zh": "本研究探讨了合成表格数据中功能依赖（functional dependencies）和新引入的逻辑依赖（logical dependencies）的保留问题，引入了一个量化逻辑依赖的度量，以评估现有合成数据生成算法的表现。研究者比较了多个 state-of-the-art 算法在公开数据集上的效果，发现这些算法无法完全保留功能依赖，但部分模型能有效保留属性间逻辑依赖。最终，该工作揭示了开发任务特定合成表格数据生成模型的研究需求和机会。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to Pattern Recognition Journal",
      "pdf_url": "http://arxiv.org/pdf/2409.17684v1",
      "published_date": "2024-09-26 09:51:07 UTC",
      "updated_date": "2024-09-26 09:51:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:14:03.234666"
    },
    {
      "arxiv_id": "2409.17683v1",
      "title": "Zero- and Few-shot Named Entity Recognition and Text Expansion in Medication Prescriptions using ChatGPT",
      "title_zh": "零样本和少样本命名实体识别以及文本扩展在药物处方中使用 ChatGPT",
      "authors": [
        "Natthanaphop Isaradech",
        "Andrea Riedel",
        "Wachiranun Sirikul",
        "Markus Kreuzthaler",
        "Stefan Schulz"
      ],
      "abstract": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data.",
      "tldr_zh": "该研究利用 ChatGPT 3.5 处理药物处方中的自由文本问题，专注于 Zero-shot 和 Few-shot 设置下的 Named Entity Recognition (NER) 和 Text Expansion (EX) 任务，以结构化和扩展药物陈述，提高其可解读性。研究团队手动标注了 100 条药物语句，并通过不同提示策略评估模型性能，NER 的最佳提示在测试集上达到平均 F1 分数 0.94，而 EX 任务的 Few-shot 提示表现最佳，F1 分数达 0.87。结果表明，Few-shot 方法相较于 Zero-shot 基线更能防止模型 hallucinating，从而在处理安全相关的药物数据时提供更可靠的性能，为自动化药物解读提供了重要基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17683v1",
      "published_date": "2024-09-26 09:49:27 UTC",
      "updated_date": "2024-09-26 09:49:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:14:16.400821"
    },
    {
      "arxiv_id": "2409.17663v3",
      "title": "Explanation Bottleneck Models",
      "title_zh": "解释瓶颈模型",
      "authors": [
        "Shin'ya Yamaguchi",
        "Kosuke Nishida"
      ],
      "abstract": "Recent concept-based interpretable models have succeeded in providing\nmeaningful explanations by pre-defined concept sets. However, the dependency on\nthe pre-defined concepts restricts the application because of the limited\nnumber of concepts for explanations. This paper proposes a novel interpretable\ndeep neural network called explanation bottleneck models (XBMs). XBMs generate\na text explanation from the input without pre-defined concepts and then predict\na final task prediction based on the generated explanation by leveraging\npre-trained vision-language encoder-decoder models. To achieve both the target\ntask performance and the explanation quality, we train XBMs through the target\ntask loss with the regularization penalizing the explanation decoder via the\ndistillation from the frozen pre-trained decoder. Our experiments, including a\ncomparison to state-of-the-art concept bottleneck models, confirm that XBMs\nprovide accurate and fluent natural language explanations without pre-defined\nconcept sets. Code is available at https://github.com/yshinya6/xbm/.",
      "tldr_zh": "本研究提出了一种新型可解释深度神经网络——Explanation Bottleneck Models (XBMs)，旨在克服传统概念-based模型对预定义概念集的依赖，从而生成更灵活的文本解释。XBMs 通过从输入生成自然语言解释，然后利用预训练的 vision-language encoder-decoder 模型基于该解释进行最终任务预测。训练过程结合目标任务损失和正则化机制，通过从冻结的预训练解码器进行知识蒸馏，确保解释质量和任务性能的平衡。实验结果显示，XBMs 比现有的 concept bottleneck models 提供更准确、流畅的解释，并在开源代码中实现。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI 2025 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2409.17663v3",
      "published_date": "2024-09-26 09:21:48 UTC",
      "updated_date": "2025-02-18 09:01:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:14:28.082783"
    },
    {
      "arxiv_id": "2409.17661v2",
      "title": "A Fuzzy-based Approach to Predict Human Interaction by Functional Near-Infrared Spectroscopy",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaowei Jiang",
        "Liang Ou",
        "Yanan Chen",
        "Na Ao",
        "Yu-Cheng Chang",
        "Thomas Do",
        "Chin-Teng Lin"
      ],
      "abstract": "The paper introduces a Fuzzy-based Attention (Fuzzy Attention Layer)\nmechanism, a novel computational approach to enhance the interpretability and\nefficacy of neural models in psychological research. The proposed Fuzzy\nAttention Layer mechanism is integrated as a neural network layer within the\nTransformer Encoder model to facilitate the analysis of complex psychological\nphenomena through neural signals, such as those captured by functional\nNear-Infrared Spectroscopy (fNIRS). By leveraging fuzzy logic, the Fuzzy\nAttention Layer is capable of learning and identifying interpretable patterns\nof neural activity. This capability addresses a significant challenge when\nusing Transformer: the lack of transparency in determining which specific brain\nactivities most contribute to particular predictions. Our experimental results\ndemonstrated on fNIRS data from subjects engaged in social interactions\ninvolving handholding reveal that the Fuzzy Attention Layer not only learns\ninterpretable patterns of neural activity but also enhances model performance.\nAdditionally, the learned patterns provide deeper insights into the neural\ncorrelates of interpersonal touch and emotional exchange. The application of\nour model shows promising potential in deciphering the subtle complexities of\nhuman social behaviors, thereby contributing significantly to the fields of\nsocial neuroscience and psychological AI.",
      "tldr_zh": "这篇论文提出了一种基于模糊逻辑的Fuzzy Attention Layer机制，以提升神经模型在心理研究中的可解释性和效能。该机制整合到Transformer Encoder模型中，用于分析功能近红外光谱(fNIRS)捕获的神经信号，从而学习并识别可解释的神经活动模式，解决了Transformer模型在预测贡献透明度方面的挑战。实验结果显示，在涉及手持互动的fNIRS数据上，该层不仅提高了模型性能，还提供了对人际触碰和情感交流的神经相关性洞见，为社会神经科学和心理AI领域做出了重要贡献。",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17661v2",
      "published_date": "2024-09-26 09:20:12 UTC",
      "updated_date": "2025-01-23 23:18:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:14:41.028797"
    },
    {
      "arxiv_id": "2409.17659v1",
      "title": "Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning",
      "title_zh": "层次化的端到端自动驾驶：将 BEV 感知与深度强化学习整合",
      "authors": [
        "Siyi Lu",
        "Lei He",
        "Shengbo Eben Li",
        "Yugong Luo",
        "Jianqiang Wang",
        "Keqiang Li"
      ],
      "abstract": "End-to-end autonomous driving offers a streamlined alternative to the\ntraditional modular pipeline, integrating perception, prediction, and planning\nwithin a single framework. While Deep Reinforcement Learning (DRL) has recently\ngained traction in this domain, existing approaches often overlook the critical\nconnection between feature extraction of DRL and perception. In this paper, we\nbridge this gap by mapping the DRL feature extraction network directly to the\nperception phase, enabling clearer interpretation through semantic\nsegmentation. By leveraging Bird's-Eye-View (BEV) representations, we propose a\nnovel DRL-based end-to-end driving framework that utilizes multi-sensor inputs\nto construct a unified three-dimensional understanding of the environment. This\nBEV-based system extracts and translates critical environmental features into\nhigh-level abstract states for DRL, facilitating more informed control.\nExtensive experimental evaluations demonstrate that our approach not only\nenhances interpretability but also significantly outperforms state-of-the-art\nmethods in autonomous driving control tasks, reducing the collision rate by\n20%.",
      "tldr_zh": "本文提出了一种层次化的端到端自动驾驶框架，将 Bird's-Eye-View (BEV) 感知与 Deep Reinforcement Learning (DRL) 整合，解决现有方法忽略 DRL 特征提取与感知之间联系的问题。通过将 DRL 特征提取网络直接映射到感知阶段，并利用多传感器输入构建统一的3D环境理解，该框架提取关键环境特征转化为高层抽象状态，提升控制决策。该方法在实验中显著优于现有技术，提高了系统可解释性，并将碰撞率降低了20%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17659v1",
      "published_date": "2024-09-26 09:14:16 UTC",
      "updated_date": "2024-09-26 09:14:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:14:52.561109"
    },
    {
      "arxiv_id": "2409.17656v1",
      "title": "Prototype based Masked Audio Model for Self-Supervised Learning of Sound Event Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Pengfei Cai",
        "Yan Song",
        "Nan Jiang",
        "Qing Gu",
        "Ian McLoughlin"
      ],
      "abstract": "A significant challenge in sound event detection (SED) is the effective\nutilization of unlabeled data, given the limited availability of labeled data\ndue to high annotation costs. Semi-supervised algorithms rely on labeled data\nto learn from unlabeled data, and the performance is constrained by the quality\nand size of the former. In this paper, we introduce the Prototype based Masked\nAudio Model~(PMAM) algorithm for self-supervised representation learning in\nSED, to better exploit unlabeled data. Specifically, semantically rich\nframe-level pseudo labels are constructed from a Gaussian mixture model (GMM)\nbased prototypical distribution modeling. These pseudo labels supervise the\nlearning of a Transformer-based masked audio model, in which binary\ncross-entropy loss is employed instead of the widely used InfoNCE loss, to\nprovide independent loss contributions from different prototypes, which is\nimportant in real scenarios in which multiple labels may apply to unsupervised\ndata frames. A final stage of fine-tuning with just a small amount of labeled\ndata yields a very high performing SED model. On like-for-like tests using the\nDESED task, our method achieves a PSDS1 score of 62.5\\%, surpassing current\nstate-of-the-art models and demonstrating the superiority of the proposed\ntechnique.",
      "tldr_zh": "本研究提出了一种基于原型的掩码音频模型（Prototype based Masked Audio Model, PMAM）算法，用于声音事件检测（Sound Event Detection, SED）的自监督表示学习，以更有效地利用未标注数据。PMAM 通过高斯混合模型（Gaussian Mixture Model, GMM）构建语义丰富的帧级伪标签，并使用这些伪标签监督基于 Transformer 的掩码音频模型的训练，同时采用二元交叉熵损失（binary cross-entropy loss）来处理多标签场景，提供独立的损失贡献。实验结果显示，在 DESED 任务上，PMAM 仅需少量标注数据进行微调，即达到 62.5% 的 PSDS1 分数，超过了现有最先进模型，证明了该方法的优越性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP2025; The code for this paper will be available at\n  https://github.com/cai525/Transformer4SED after the paper is accepted",
      "pdf_url": "http://arxiv.org/pdf/2409.17656v1",
      "published_date": "2024-09-26 09:07:20 UTC",
      "updated_date": "2024-09-26 09:07:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:15:15.932738"
    },
    {
      "arxiv_id": "2409.17655v1",
      "title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment",
      "title_zh": "翻译失败",
      "authors": [
        "Nan Sun",
        "Bo Mao",
        "Yongchang Li",
        "Lumeng Ma",
        "Di Guo",
        "Huaping Liu"
      ],
      "abstract": "The increasing demand for intelligent assistants in human-populated\nenvironments has motivated significant research in autonomous robotic systems.\nTraditional service robots and virtual assistants, however, struggle with\nreal-world task execution due to their limited capacity for dynamic reasoning\nand interaction, particularly when human collaboration is required. Recent\ndevelopments in Large Language Models have opened new avenues for improving\nthese systems, enabling more sophisticated reasoning and natural interaction\ncapabilities. In this paper, we introduce AssistantX, an LLM-powered proactive\nassistant designed to operate autonomously in a physical office environment.\nUnlike conventional service robots, AssistantX leverages a novel multi-agent\narchitecture, PPDR4X, which provides advanced inference capabilities and\ncomprehensive collaboration awareness. By effectively bridging the gap between\nvirtual operations and physical interactions, AssistantX demonstrates robust\nperformance in managing complex real-world scenarios. Our evaluation highlights\nthe architecture's effectiveness, showing that AssistantX can respond to clear\ninstructions, actively retrieve supplementary information from memory, and\nproactively seek collaboration from team members to ensure successful task\ncompletion. More details and videos can be found at\nhttps://assistantx-agent.github.io/AssistantX/.",
      "tldr_zh": "该论文介绍了 AssistantX，一种基于大型语言模型(LLM)的主动助手，旨在在人类协作的环境中（如物理办公室）实现自主操作，以克服传统服务机器人动态推理和互动的局限。AssistantX 采用新型多智能体架构 PPDR4X，提供高级推理能力和全面协作意识，从而桥接虚拟操作与物理互动。实验评估显示，AssistantX 能有效响应清晰指令、从记忆中主动检索信息，并主动寻求团队协作，确保复杂任务的成功完成。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 8 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.17655v1",
      "published_date": "2024-09-26 09:06:56 UTC",
      "updated_date": "2024-09-26 09:06:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:15:15.850862"
    },
    {
      "arxiv_id": "2409.17652v2",
      "title": "FactorSim: Generative Simulation via Factorized Representation",
      "title_zh": "FactorSim：通过因子化表示的生成式模拟",
      "authors": [
        "Fan-Yun Sun",
        "S. I. Harini",
        "Angela Yi",
        "Yihan Zhou",
        "Alex Zook",
        "Jonathan Tremblay",
        "Logan Cross",
        "Jiajun Wu",
        "Nick Haber"
      ],
      "abstract": "Generating simulations to train intelligent agents in game-playing and\nrobotics from natural language input, from user input or task documentation,\nremains an open-ended challenge. Existing approaches focus on parts of this\nchallenge, such as generating reward functions or task hyperparameters. Unlike\nprevious work, we introduce FACTORSIM that generates full simulations in code\nfrom language input that can be used to train agents. Exploiting the structural\nmodularity specific to coded simulations, we propose to use a factored\npartially observable Markov decision process representation that allows us to\nreduce context dependence during each step of the generation. For evaluation,\nwe introduce a generative simulation benchmark that assesses the generated\nsimulation code's accuracy and effectiveness in facilitating zero-shot\ntransfers in reinforcement learning settings. We show that FACTORSIM\noutperforms existing methods in generating simulations regarding prompt\nalignment (e.g., accuracy), zero-shot transfer abilities, and human evaluation.\nWe also demonstrate its effectiveness in generating robotic tasks.",
      "tldr_zh": "该研究提出了 FactorSim，一种通过 factorized representation 生成模拟的方法，能够从自然语言输入（如用户指令或任务文档）自动生成完整的模拟代码，用于训练智能代理在游戏和机器人领域的应用。FactorSim 利用 factorized partially observable Markov decision process (POMDP) 表示，减少生成过程中的上下文依赖，从而提高模拟的结构模块性和效率。为评估其性能，论文引入了一个生成模拟基准，证明 FactorSim 在代码准确性、zero-shot transfer 能力和人类评估上均优于现有方法，并在机器人任务中展现了实际有效性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "neurips 2024, project website:\n  https://cs.stanford.edu/~sunfanyun/factorsim/",
      "pdf_url": "http://arxiv.org/pdf/2409.17652v2",
      "published_date": "2024-09-26 09:00:30 UTC",
      "updated_date": "2024-11-11 08:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:15:27.719175"
    },
    {
      "arxiv_id": "2409.17650v1",
      "title": "Digital Twin Ecosystem for Oncology Clinical Operations",
      "title_zh": "翻译失败",
      "authors": [
        "Himanshu Pandey",
        "Akhil Amod",
        "Shivang",
        "Kshitij Jaggi",
        "Ruchi Garg",
        "Abheet Jain",
        "Vinayak Tantia"
      ],
      "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations.",
      "tldr_zh": "这篇论文提出一个数字孪生(Digital Twin)生态系统，旨在优化肿瘤学(oncology)临床操作，通过整合AI和大型语言模型(LLMs)来提升患者护理效率。该框架包括多个专业数字孪生，如Medical Necessity Twin、Care Navigator Twin和Clinical History Twin，这些组件基于患者独特数据实现工作流程优化和个性化护理。通过合成多种数据来源并与National Comprehensive Cancer Network (NCCN)指南对齐，系统创建动态的Cancer Care Path，提供精确的临床推荐，从而释放AI和数字孪生在临床操作中的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Pre Print",
      "pdf_url": "http://arxiv.org/pdf/2409.17650v1",
      "published_date": "2024-09-26 08:56:54 UTC",
      "updated_date": "2024-09-26 08:56:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:15:40.378142"
    },
    {
      "arxiv_id": "2409.17642v2",
      "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic Self-Disclosure",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Chen",
        "Zhiyang Zhang",
        "Fangkai Yang",
        "Xiaoting Qin",
        "Chao Du",
        "Xi Cheng",
        "Hangxin Liu",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "abstract": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions.",
      "tldr_zh": "该研究探讨了LLM-based AI delegates 在社交互动中的隐私风险，强调需要在保护隐私和战略自披露之间实现平衡。作者通过试点研究调查了用户在不同社交关系和任务场景中的偏好，并提出了一种新型AI delegate 系统，支持隐私意识的自披露。用户研究结果显示，该系统能够战略性地保护隐私，并在多样动态的社交互动中展现出创新应用潜力。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17642v2",
      "published_date": "2024-09-26 08:45:15 UTC",
      "updated_date": "2024-10-07 06:29:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:15:51.528875"
    },
    {
      "arxiv_id": "2409.17640v3",
      "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training on an Assistant Task for a Target Task",
      "title_zh": "翻译失败",
      "authors": [
        "Xindi Tong",
        "Yujin Zhu",
        "Shijian Fan",
        "Liang Xu"
      ],
      "abstract": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
      "tldr_zh": "该论文提出了一种名为 T3 的新型 Zero-shot Transfer Learning 框架，通过在辅助任务上迭代训练基线 Large Language Models (LLMs)，以提升目标任务的性能，旨在解决 LLMs 在长文本总结中面临的数据资源不足和高上下文需求问题。T3 的方法利用与目标任务具有结构或语义相似性的辅助任务（如问答），来增强模型的泛化能力，并在 BBC summary、NarraSum、FairytaleQA 和 NLQuAD 数据集上进行了验证。实验结果显示，与基线模型相比，T3 实现了 ROUGE 指标近 14% 的提升、BLEU 指标 35% 的提升，以及 Factscore 指标 16% 的提升。该框架展示了在更多辅助-目标任务组合中的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17640v3",
      "published_date": "2024-09-26 08:44:38 UTC",
      "updated_date": "2025-01-22 07:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:16:05.752197"
    },
    {
      "arxiv_id": "2409.17634v1",
      "title": "P4Q: Learning to Prompt for Quantization in Visual-language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Huixin Sun",
        "Runqi Wang",
        "Yanjing Li",
        "Xianbin Cao",
        "Xiaolong Jiang",
        "Yao Hu",
        "Baochang Zhang"
      ],
      "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence\nin various visual and multimodal tasks, yet the deployment of VLMs on\ndownstream application platforms remains challenging due to their prohibitive\nrequirements of training samples and computing resources. Fine-tuning and\nquantization of VLMs can substantially reduce the sample and computation costs,\nwhich are in urgent need. There are two prevailing paradigms in quantization,\nQuantization-Aware Training (QAT) can effectively quantize large-scale VLMs but\nincur a huge training cost, while low-bit Post-Training Quantization (PTQ)\nsuffers from a notable performance drop. We propose a method that balances\nfine-tuning and quantization named ``Prompt for Quantization'' (P4Q), in which\nwe design a lightweight architecture to leverage contrastive loss supervision\nto enhance the recognition performance of a PTQ model. Our method can\neffectively reduce the gap between image features and text features caused by\nlow-bit quantization, based on learnable prompts to reorganize textual\nrepresentations and a low-bit adapter to realign the distributions of image and\ntext features. We also introduce a distillation loss based on cosine similarity\npredictions to distill the quantized model using a full-precision teacher.\nExtensive experimental results demonstrate that our P4Q method outperforms\nprior arts, even achieving comparable results to its full-precision\ncounterparts. For instance, our 8-bit P4Q can theoretically compress the\nCLIP-ViT/B-32 by 4 $\\times$ while achieving 66.94\\% Top-1 accuracy,\noutperforming the learnable prompt fine-tuned full-precision model by 2.24\\%\nwith negligible additional parameters on the ImageNet dataset.",
      "tldr_zh": "这篇论文提出了一种名为 P4Q 的方法，用于在视觉语言模型 (VLMs) 中学习提示以实现量化，旨在解决模型部署时对训练样本和计算资源的巨大需求。P4Q 结合了轻量级架构，通过对比损失 (contrastive loss) 监督来提升后训练量化 (PTQ) 模型的性能，同时使用可学习提示 (learnable prompts) 重组文本表示，以及低位适配器 (low-bit adapter) 重新对齐图像和文本特征的分布。论文还引入了基于余弦相似度 (cosine similarity) 的蒸馏损失 (distillation loss)，利用全精度教师模型对量化模型进行知识转移。实验结果显示，P4Q 优于现有方法，例如在 ImageNet 数据集上，8-bit P4Q 使 CLIP-ViT/B-32 理论上压缩 4 倍，同时达到 66.94% Top-1 准确率，比 fine-tuned 全精度模型高 2.24%，且额外参数微不足道。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17634v1",
      "published_date": "2024-09-26 08:31:27 UTC",
      "updated_date": "2024-09-26 08:31:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:16:17.335814"
    },
    {
      "arxiv_id": "2409.17629v1",
      "title": "Hand-object reconstruction via interaction-aware graph attention mechanism",
      "title_zh": "翻译失败",
      "authors": [
        "Taeyun Woo",
        "Tae-Kyun Kim",
        "Jinah Park"
      ],
      "abstract": "Estimating the poses of both a hand and an object has become an important\narea of research due to the growing need for advanced vision computing. The\nprimary challenge involves understanding and reconstructing how hands and\nobjects interact, such as contact and physical plausibility. Existing\napproaches often adopt a graph neural network to incorporate spatial\ninformation of hand and object meshes. However, these approaches have not fully\nexploited the potential of graphs without modification of edges within and\nbetween hand- and object-graphs. We propose a graph-based refinement method\nthat incorporates an interaction-aware graph-attention mechanism to account for\nhand-object interactions. Using edges, we establish connections among closely\ncorrelated nodes, both within individual graphs and across different graphs.\nExperiments demonstrate the effectiveness of our proposed method with notable\nimprovements in the realm of physical plausibility.",
      "tldr_zh": "该论文针对手和物体姿势估计的挑战，提出了一种基于交互感知图注意力机制（interaction-aware graph attention mechanism）的重建方法，以更好地处理手物互动如接触和物理合理性。现有方法虽使用图神经网络（graph neural network）整合空间信息，但未充分利用图的边连接；本研究通过在手图和物体图之间建立紧密相关节点的连接，实现图-based 精炼。实验结果显示，该方法在物理合理性方面取得了显著改善。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, Accepted by ICIP 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17629v1",
      "published_date": "2024-09-26 08:23:04 UTC",
      "updated_date": "2024-09-26 08:23:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:16:27.498093"
    },
    {
      "arxiv_id": "2409.17622v1",
      "title": "Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs",
      "title_zh": "翻译失败",
      "authors": [
        "Yusong Wang",
        "Chaoran Cheng",
        "Shaoning Li",
        "Yuxuan Ren",
        "Bin Shao",
        "Ge Liu",
        "Pheng-Ann Heng",
        "Nanning Zheng"
      ],
      "abstract": "Geometric graph neural networks (GNNs) have emerged as powerful tools for\nmodeling molecular geometry. However, they encounter limitations in effectively\ncapturing long-range interactions in large molecular systems. To address this\nchallenge, we introduce Neural P$^3$M, a versatile enhancer of geometric GNNs\nto expand the scope of their capabilities by incorporating mesh points\nalongside atoms and reimaging traditional mathematical operations in a\ntrainable manner. Neural P$^3$M exhibits flexibility across a wide range of\nmolecular systems and demonstrates remarkable accuracy in predicting energies\nand forces, outperforming on benchmarks such as the MD22 dataset. It also\nachieves an average improvement of 22% on the OE62 dataset while integrating\nwith various architectures.",
      "tldr_zh": "本研究针对几何图神经网络(Geometric GNNs)在分子几何建模中捕捉长程相互作用的局限性，提出Neural P³M作为一种通用增强器。它通过整合mesh points与原子，并以可训练方式重新定义传统数学操作，从而扩展了模型在各种分子系统中的适用性。实验结果显示，Neural P³M在MD22数据集上实现了超越基准的准确性预测能量和力，并在OE62数据集上与多种架构结合时平均改善22%。这为改进分子模拟提供了显著的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17622v1",
      "published_date": "2024-09-26 08:16:59 UTC",
      "updated_date": "2024-09-26 08:16:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:16:40.132560"
    },
    {
      "arxiv_id": "2410.08224v1",
      "title": "A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications",
      "title_zh": "时空 EEG 数据",
      "authors": [
        "Pengfei Wang",
        "Huanran Zheng",
        "Silong Dai",
        "Yiqiao Wang",
        "Xiaotian Gu",
        "Yuanbin Wu",
        "Xiaoling Wang"
      ],
      "abstract": "In recent years, the field of electroencephalography (EEG) analysis has\nwitnessed remarkable advancements, driven by the integration of machine\nlearning and artificial intelligence. This survey aims to encapsulate the\nlatest developments, focusing on emerging methods and technologies that are\npoised to transform our comprehension and interpretation of brain activity. We\ndelve into self-supervised learning methods that enable the robust\nrepresentation of brain signals, which are fundamental for a variety of\ndownstream applications. We also explore emerging discriminative methods,\nincluding graph neural networks (GNN), foundation models, and large language\nmodels (LLMs)-based approaches. Furthermore, we examine generative technologies\nthat harness EEG data to produce images or text, offering novel perspectives on\nbrain activity visualization and interpretation. The survey provides an\nextensive overview of these cutting-edge techniques, their current\napplications, and the profound implications they hold for future research and\nclinical practice. The relevant literature and open-source materials have been\ncompiled and are consistently being refreshed at\n\\url{https://github.com/wpf535236337/LLMs4TS}",
      "tldr_zh": "这篇调查论文回顾了脑电图(EEG)数据的时空分析领域最新进展，聚焦于机器学习和人工智能的应用。论文探讨了自监督学习方法用于构建鲁棒的脑信号表示，以及新兴判别技术如graph neural networks (GNN)、foundation models 和 large language models (LLMs)-based 方式。还涵盖了生成技术，将EEG数据用于图像或文本生成，提供新的脑活动可视化和解释视角。该调查总结了这些方法的当前应用、未来研究潜力及临床影响，并提供相关文献和开源资源于GitHub。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "eess.SP",
      "comment": "submitted to IECE Chinese Journal of Information Fusion",
      "pdf_url": "http://arxiv.org/pdf/2410.08224v1",
      "published_date": "2024-09-26 08:09:15 UTC",
      "updated_date": "2024-09-26 08:09:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:17:02.099951"
    },
    {
      "arxiv_id": "2409.17607v1",
      "title": "Dirichlet-Based Coarse-to-Fine Example Selection For Open-Set Annotation",
      "title_zh": "基于Dirichlet的粗到细示例选择",
      "authors": [
        "Ye-Wen Wang",
        "Chen-Chen Zong",
        "Ming-Kun Xie",
        "Sheng-Jun Huang"
      ],
      "abstract": "Active learning (AL) has achieved great success by selecting the most\nvaluable examples from unlabeled data. However, they usually deteriorate in\nreal scenarios where open-set noise gets involved, which is studied as open-set\nannotation (OSA). In this paper, we owe the deterioration to the unreliable\npredictions arising from softmax-based translation invariance and propose a\nDirichlet-based Coarse-to-Fine Example Selection (DCFS) strategy accordingly.\nOur method introduces simplex-based evidential deep learning (EDL) to break\ntranslation invariance and distinguish known and unknown classes by considering\nevidence-based data and distribution uncertainty simultaneously. Furthermore,\nhard known-class examples are identified by model discrepancy generated from\ntwo classifier heads, where we amplify and alleviate the model discrepancy\nrespectively for unknown and known classes. Finally, we combine the discrepancy\nwith uncertainties to form a two-stage strategy, selecting the most informative\nexamples from known classes. Extensive experiments on various openness ratio\ndatasets demonstrate that DCFS achieves state-of-art performance.",
      "tldr_zh": "这篇论文针对主动学习（Active Learning, AL）在开放集标注（Open-Set Annotation, OSA）中的性能下降问题，提出了一种基于 Dirichlet 的粗到细样本选择策略（Dirichlet-based Coarse-to-Fine Example Selection, DCFS）。DCFS 通过引入 simplex-based evidential deep learning (EDL) 来打破 softmax 的平移不变性，同时考虑证据-based 数据和分布不确定性，以区分已知和未知类。论文进一步利用两个分类器头的模型差异（model discrepancy）来识别困难的已知类样本，并通过放大未知类差异和缓解已知类差异，形成一个两阶段选择策略。实验结果显示，DCFS 在各种开放比率数据集上实现了最先进的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17607v1",
      "published_date": "2024-09-26 07:47:50 UTC",
      "updated_date": "2024-09-26 07:47:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:17:04.799702"
    },
    {
      "arxiv_id": "2409.17602v1",
      "title": "Open Digital Rights Enforcement Framework (ODRE): from descriptive to enforceable policies",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Cimmino",
        "Juan Cano-Benito",
        "Raúl García-Castro"
      ],
      "abstract": "From centralised platforms to decentralised ecosystems, like Data Spaces,\nsharing data has become a paramount challenge. For this reason, the definition\nof data usage policies has become crucial in these domains, highlighting the\nnecessity of effective policy enforcement mechanisms. The Open Digital Rights\nLanguage (ODRL) is a W3C standard ontology designed to describe data usage\npolicies, however, it lacks built-in enforcement capabilities, limiting its\npractical application. This paper introduces the Open Digital Rights\nEnforcement (ODRE) framework, whose goal is to provide ODRL with enforcement\ncapabilities. The ODRE framework proposes a novel approach to express ODRL\npolicies that integrates the descriptive ontology terms of ODRL with other\nlanguages that allow behaviour specification, such as dynamic data handling or\nfunction evaluation. The framework includes an enforcement algorithm for ODRL\npolicies and two open-source implementations in Python and Java. The ODRE\nframework is also designed to support future extensions of ODRL to specific\ndomain scenarios. In addition, current limitations of ODRE, ODRL, and current\nchallenges are reported. Finally, to demonstrate the enforcement capabilities\nof the implementations, their performance, and their extensibility features,\nseveral experiments have been carried out with positive results.",
      "tldr_zh": "本论文针对数据共享领域的政策执行挑战，提出 Open Digital Rights Enforcement (ODRE) 框架，将 W3C 标准 Open Digital Rights Language (ODRL) 从描述性政策扩展到可执行状态。ODRE 通过整合 ODRL 的本体术语与其他行为指定语言（如动态数据处理或函数评估），开发了一个执行算法，并提供 Python 和 Java 的开源实现，支持 ODRL 的未来领域扩展。实验结果证明了 ODRE 的执行能力、性能和可扩展性，同时讨论了当前 ODRE 和 ODRL 的限制及挑战。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 3 Figures, Submitted to Computers & Security journal",
      "pdf_url": "http://arxiv.org/pdf/2409.17602v1",
      "published_date": "2024-09-26 07:36:49 UTC",
      "updated_date": "2024-09-26 07:36:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:17:19.339901"
    },
    {
      "arxiv_id": "2409.17601v3",
      "title": "CleanerCLIP: Fine-grained Counterfactual Semantic Augmentation for Backdoor Defense in Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Xun",
        "Siyuan Liang",
        "Xiaojun Jia",
        "Xinwei Liu",
        "Xiaochun Cao"
      ],
      "abstract": "Pre-trained large models for multimodal contrastive learning, such as CLIP,\nhave been widely recognized in the industry as highly susceptible to\ndata-poisoned backdoor attacks. This poses significant risks to downstream\nmodel training. In response to such potential threats, finetuning offers a\nsimpler and more efficient defense choice compared to retraining large models\nwith augmented data. In the supervised learning domain, fine-tuning defense\nstrategies can achieve excellent defense performance. However, in the\nunsupervised and semi-supervised domain, we find that when CLIP faces some\ncomplex attack techniques, the existing fine-tuning defense strategy,\nCleanCLIP, has some limitations on defense performance. The synonym\nsubstitution of its text-augmentation is insufficient to enhance the text\nfeature space. To compensate for this weakness, we improve it by proposing a\nfine-grained \\textbf{T}ext \\textbf{A}lignment \\textbf{C}leaner (TA-Cleaner) to\ncut off feature connections of backdoor triggers. We randomly select a few\nsamples for positive and negative subtext generation at each epoch of\nCleanCLIP, and align the subtexts to the images to strengthen the text\nself-supervision. We evaluate the effectiveness of our TA-Cleaner against six\nattack algorithms and conduct comprehensive zero-shot classification tests on\nImageNet1K. Our experimental results demonstrate that TA-Cleaner achieves\nstate-of-the-art defensiveness among finetuning-based defense techniques. Even\nwhen faced with the novel attack technique BadCLIP, our TA-Cleaner outperforms\nCleanCLIP by reducing the ASR of Top-1 and Top-10 by 52.02\\% and 63.88\\%,\nrespectively.",
      "tldr_zh": "该论文提出 CleanerCLIP，一种细粒度反事实语义增强方法，用于对比学习（Contrastive Learning）中的后门防御（Backdoor Defense）。针对 CLIP 模型易受数据中毒攻击的问题，作者改进现有微调策略 CleanCLIP，引入细粒度文本对齐清洗器（TA-Cleaner），通过生成正负子文本并与图像对齐，切断后门触发器的特征连接，从而增强文本特征空间的自监督能力。实验结果显示，TA-Cleaner 在六种攻击算法上表现出色，尤其在 BadCLIP 攻击下，比 CleanCLIP 降低了 Top-1 和 Top-10 的攻击成功率（ASR）52.02% 和 63.88%，在 ImageNet1K 的零样本分类测试中达到最先进防御性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17601v3",
      "published_date": "2024-09-26 07:35:23 UTC",
      "updated_date": "2024-11-15 02:56:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:17:38.882677"
    },
    {
      "arxiv_id": "2409.17596v1",
      "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming",
      "title_zh": "翻译失败",
      "authors": [
        "Zehao Zhu",
        "Wei Sun",
        "Jun Jia",
        "Wei Wu",
        "Sibin Deng",
        "Kai Li",
        "Ying Chen",
        "Xiongkuo Min",
        "Jia Wang",
        "Guangtao Zhai"
      ],
      "abstract": "In recent years, live video streaming has gained widespread popularity across\nvarious social media platforms. Quality of experience (QoE), which reflects\nend-users' satisfaction and overall experience, plays a critical role for media\nservice providers to optimize large-scale live compression and transmission\nstrategies to achieve perceptually optimal rate-distortion trade-off. Although\nmany QoE metrics for video-on-demand (VoD) have been proposed, there remain\nsignificant challenges in developing QoE metrics for live video streaming. To\nbridge this gap, we conduct a comprehensive study of subjective and objective\nQoE evaluations for live video streaming. For the subjective QoE study, we\nintroduce the first live video streaming QoE dataset, TaoLive QoE, which\nconsists of $42$ source videos collected from real live broadcasts and $1,155$\ncorresponding distorted ones degraded due to a variety of streaming\ndistortions, including conventional streaming distortions such as compression,\nstalling, as well as live streaming-specific distortions like frame skipping,\nvariable frame rate, etc. Subsequently, a human study was conducted to derive\nsubjective QoE scores of videos in the TaoLive QoE dataset. For the objective\nQoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well\nas publicly available QoE datasets for VoD scenarios, highlighting that current\nmodels struggle to accurately assess video QoE, particularly for live content.\nHence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates\nmulti-scale semantic features and optical flow-based motion features to\npredicting a retrospective QoE score, eliminating reliance on statistical\nquality of service (QoS) features.",
      "tldr_zh": "这篇论文研究了直播视频流的QoE（Quality of Experience）评估，强调其对优化压缩和传输策略的重要性，并填补了现有指标主要针对VoD（Video-on-Demand）的空白。研究者创建了首个直播视频QoE数据集TaoLive QoE，包括42个源视频和1155个受各种扭曲（如压缩、卡顿、帧跳跃）影响的视频，并通过人类主观研究获取了这些视频的QoE分数。客观评估显示现有QoE模型在直播场景下准确性不足，因此提出了端到端模型Tao-QoE，利用多尺度语义特征和基于光流的运动特征来预测回顾性QoE分数，而不依赖统计QoS（Quality of Service）特征。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.MM",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.17596v1",
      "published_date": "2024-09-26 07:22:38 UTC",
      "updated_date": "2024-09-26 07:22:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:17:40.963050"
    },
    {
      "arxiv_id": "2409.17592v1",
      "title": "Deep Manifold Part 1: Anatomy of Neural Network Manifold",
      "title_zh": "翻译失败",
      "authors": [
        "Max Y. Ma",
        "Gen-Hua Shi"
      ],
      "abstract": "Based on the numerical manifold method principle, we developed a mathematical\nframework of a neural network manifold: Deep Manifold and discovered that\nneural networks: 1) is numerical computation combining forward and inverse; 2)\nhave near infinite degrees of freedom; 3) exponential learning capacity with\ndepth; 4) have self-progressing boundary conditions; 5) has training hidden\nbottleneck. We also define two concepts: neural network learning space and deep\nmanifold space and introduce two concepts: neural network intrinsic pathway and\nfixed point. We raise three fundamental questions: 1). What is the training\ncompletion definition; 2). where is the deep learning convergence point (neural\nnetwork fixed point); 3). How important is token timestamp in training data\ngiven negative time is critical in inverse problem.",
      "tldr_zh": "本论文基于numerical manifold method原理，开发了Deep Manifold数学框架，以剖析神经网络流形（neural network manifold）。研究发现神经网络具有五大特性：包括正向和逆向计算的数值形式、近乎无限的自由度、深度带来的指数级学习能力、自我进步的边界条件，以及训练中的隐藏瓶颈。论文还定义了neural network learning space和deep manifold space两个概念，并引入neural network intrinsic pathway和fixed point概念，同时提出了三个基本问题：训练完成的定义、deep learning的收敛点（neural network fixed point）位置，以及训练数据中token timestamp的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17592v1",
      "published_date": "2024-09-26 07:19:12 UTC",
      "updated_date": "2024-09-26 07:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:17:55.957770"
    },
    {
      "arxiv_id": "2409.17589v1",
      "title": "Improving Fast Adversarial Training via Self-Knowledge Guidance",
      "title_zh": "通过自知识指导改进快速对抗训练",
      "authors": [
        "Chengze Jiang",
        "Junkai Wang",
        "Minjing Dong",
        "Jie Gui",
        "Xinli Shi",
        "Yuan Cao",
        "Yuan Yan Tang",
        "James Tin-Yau Kwok"
      ],
      "abstract": "Adversarial training has achieved remarkable advancements in defending\nagainst adversarial attacks. Among them, fast adversarial training (FAT) is\ngaining attention for its ability to achieve competitive robustness with fewer\ncomputing resources. Existing FAT methods typically employ a uniform strategy\nthat optimizes all training data equally without considering the influence of\ndifferent examples, which leads to an imbalanced optimization. However, this\nimbalance remains unexplored in the field of FAT. In this paper, we conduct a\ncomprehensive study of the imbalance issue in FAT and observe an obvious class\ndisparity regarding their performances. This disparity could be embodied from a\nperspective of alignment between clean and robust accuracy. Based on the\nanalysis, we mainly attribute the observed misalignment and disparity to the\nimbalanced optimization in FAT, which motivates us to optimize different\ntraining data adaptively to enhance robustness. Specifically, we take disparity\nand misalignment into consideration. First, we introduce self-knowledge guided\nregularization, which assigns differentiated regularization weights to each\nclass based on its training state, alleviating class disparity. Additionally,\nwe propose self-knowledge guided label relaxation, which adjusts label\nrelaxation according to the training accuracy, alleviating the misalignment and\nimproving robustness. By combining these methods, we formulate the\nSelf-Knowledge Guided FAT (SKG-FAT), leveraging naturally generated knowledge\nduring training to enhance the adversarial robustness without compromising\ntraining efficiency. Extensive experiments on four standard datasets\ndemonstrate that the SKG-FAT improves the robustness and preserves competitive\nclean accuracy, outperforming the state-of-the-art methods.",
      "tldr_zh": "本研究针对快速对抗训练 (FAT) 中的不平衡优化问题，指出现有方法对训练数据采用统一策略，导致类别性能差异 (class disparity) 和干净准确率与鲁棒准确率之间的不对齐。作者提出两种创新方法：self-knowledge guided regularization，根据每个类别的训练状态分配差异化正则化权重，以缓解类别差异；以及self-knowledge guided label relaxation，根据训练准确率调整标签松弛，以改善准确率不对齐并提升鲁棒性。基于这些方法，他们开发了Self-Knowledge Guided FAT (SKG-FAT) 框架，利用训练过程中的自然知识增强对抗鲁棒性，而不影响训练效率。在四个标准数据集上的广泛实验显示，SKG-FAT 优于现有方法，提高了鲁棒性并保持了竞争力的干净准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17589v1",
      "published_date": "2024-09-26 07:12:04 UTC",
      "updated_date": "2024-09-26 07:12:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:18:05.860928"
    },
    {
      "arxiv_id": "2409.17587v1",
      "title": "Multimodal Banking Dataset: Understanding Client Needs through Event Sequences",
      "title_zh": "翻译失败",
      "authors": [
        "Mollaev Dzhambulat",
        "Alexander Kostin",
        "Postnova Maria",
        "Ivan Karpukhin",
        "Ivan A Kireev",
        "Gleb Gusev",
        "Andrey Savchenko"
      ],
      "abstract": "Financial organizations collect a huge amount of data about clients that\ntypically has a temporal (sequential) structure and is collected from various\nsources (modalities). Due to privacy issues, there are no large-scale\nopen-source multimodal datasets of event sequences, which significantly limits\nthe research in this area. In this paper, we present the industrial-scale\npublicly available multimodal banking dataset, MBD, that contains more than\n1.5M corporate clients with several modalities: 950M bank transactions, 1B geo\nposition events, 5M embeddings of dialogues with technical support and monthly\naggregated purchases of four bank's products. All entries are properly\nanonymized from real proprietary bank data. Using this dataset, we introduce a\nnovel benchmark with two business tasks: campaigning (purchase prediction in\nthe next month) and matching of clients. We provide numerical results that\ndemonstrate the superiority of our multi-modal baselines over single-modal\ntechniques for each task. As a result, the proposed dataset can open new\nperspectives and facilitate the future development of practically important\nlarge-scale multimodal algorithms for event sequences.\n  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD\n  Github Link: https://github.com/Dzhambo/MBD",
      "tldr_zh": "该论文介绍了Multimodal Banking Dataset (MBD)，一个大规模公开数据集，包含超过1.5M企业客户的数据，包括950M银行交易、1B地理位置事件、5M对话嵌入以及月度产品购买记录等多重模态信息，所有数据均已匿名化处理。研究者基于此数据集建立了两个业务基准任务：下个月的购买预测（campaigning）和客户匹配，并通过实验证明多模态方法在这些任务上优于单模态技术。MBD数据集的发布将推动事件序列的多模态算法研究，为理解客户需求和开发实际应用提供新视角。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17587v1",
      "published_date": "2024-09-26 07:07:08 UTC",
      "updated_date": "2024-09-26 07:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:18:16.239472"
    },
    {
      "arxiv_id": "2409.17583v1",
      "title": "Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components",
      "title_zh": "让量子逐渐渗入：通过逐步替换经典组件设计量子神经网络模型",
      "authors": [
        "Peiyong Wang",
        "Casey. R. Myers",
        "Lloyd C. L. Hollenberg",
        "Udaya Parampalli"
      ],
      "abstract": "Artificial Intelligence (AI), with its multiplier effect and wide\napplications in multiple areas, could potentially be an important application\nof quantum computing. Since modern AI systems are often built on neural\nnetworks, the design of quantum neural networks becomes a key challenge in\nintegrating quantum computing into AI. To provide a more fine-grained\ncharacterisation of the impact of quantum components on the performance of\nneural networks, we propose a framework where classical neural network layers\nare gradually replaced by quantum layers that have the same type of input and\noutput while keeping the flow of information between layers unchanged,\ndifferent from most current research in quantum neural network, which favours\nan end-to-end quantum model. We start with a simple three-layer classical\nneural network without any normalisation layers or activation functions, and\ngradually change the classical layers to the corresponding quantum versions. We\nconduct numerical experiments on image classification datasets such as the\nMNIST, FashionMNIST and CIFAR-10 datasets to demonstrate the change of\nperformance brought by the systematic introduction of quantum components.\nThrough this framework, our research sheds new light on the design of future\nquantum neural network models where it could be more favourable to search for\nmethods and frameworks that harness the advantages from both the classical and\nquantum worlds.",
      "tldr_zh": "该研究提出了一种渐进式框架，用于设计量子神经网络(Quantum Neural Networks)，通过逐步替换经典神经网络层为量子层，同时保持输入输出类型和信息流不变，从而细化评估量子组件对性能的影响。不同于传统的端到端量子模型，该方法从一个简单的三层经典神经网络开始，系统地引入量子版本的层。研究通过在MNIST、FashionMNIST和CIFAR-10图像分类数据集上的数值实验，展示了量子组件逐步加入带来的性能变化。最终，该框架为未来量子神经网络设计提供了新视角，强调结合经典和量子优势以实现更有效的AI系统。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "50 pages (including Appendix), many figures, accepted as a poster on\n  QTML2024. Code available at\n  https://github.com/peiyong-addwater/Let-The-Quantum-Creep-In",
      "pdf_url": "http://arxiv.org/pdf/2409.17583v1",
      "published_date": "2024-09-26 07:01:29 UTC",
      "updated_date": "2024-09-26 07:01:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:18:28.721382"
    },
    {
      "arxiv_id": "2409.17581v1",
      "title": "A Scalable Data-Driven Framework for Systematic Analysis of SEC 10-K Filings Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Syed Affan Daimi",
        "Asma Iqbal"
      ],
      "abstract": "The number of companies listed on the NYSE has been growing exponentially,\ncreating a significant challenge for market analysts, traders, and stockholders\nwho must monitor and assess the performance and strategic shifts of a large\nnumber of companies regularly. There is an increasing need for a fast,\ncost-effective, and comprehensive method to evaluate the performance and detect\nand compare many companies' strategy changes efficiently. We propose a novel\ndata-driven approach that leverages large language models (LLMs) to\nsystematically analyze and rate the performance of companies based on their SEC\n10-K filings. These filings, which provide detailed annual reports on a\ncompany's financial performance and strategic direction, serve as a rich source\nof data for evaluating various aspects of corporate health, including\nconfidence, environmental sustainability, innovation, and workforce management.\nWe also introduce an automated system for extracting and preprocessing 10-K\nfilings. This system accurately identifies and segments the required sections\nas outlined by the SEC, while also isolating key textual content that contains\ncritical information about the company. This curated data is then fed into\nCohere's Command-R+ LLM to generate quantitative ratings across various\nperformance metrics. These ratings are subsequently processed and visualized to\nprovide actionable insights. The proposed scheme is then implemented on an\ninteractive GUI as a no-code solution for running the data pipeline and\ncreating the visualizations. The application showcases the rating results and\nprovides year-on-year comparisons of company performance.",
      "tldr_zh": "这篇论文提出了一种可扩展的数据驱动框架，利用大型语言模型 (LLMs) 系统分析 SEC 10-K filings，以帮助市场分析师、交易者和股东快速评估公司表现和战略变化。框架包括自动提取和预处理 10-K filings 的关键部分（如财务表现和战略方向），然后使用 Cohere's Command-R+ LLM 生成量化评分，涵盖指标如信心、环境可持续性、创新和劳动力管理。最终，通过交互式 GUI 实现可视化输出，提供无代码解决方案和年对年比较，便于用户获得可操作洞察。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.17581v1",
      "published_date": "2024-09-26 06:57:22 UTC",
      "updated_date": "2024-09-26 06:57:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:18:40.950135"
    },
    {
      "arxiv_id": "2409.17580v1",
      "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Zahra Sepasdar",
        "Sushant Gautam",
        "Cise Midoglu",
        "Michael A. Riegler",
        "Pål Halvorsen"
      ],
      "abstract": "Extracting meaningful insights from large and complex datasets poses\nsignificant challenges, particularly in ensuring the accuracy and relevance of\nretrieved information. Traditional data retrieval methods such as sequential\nsearch and index-based retrieval often fail when handling intricate and\ninterconnected data structures, resulting in incomplete or misleading outputs.\nTo overcome these limitations, we introduce Structured-GraphRAG, a versatile\nframework designed to enhance information retrieval across structured datasets\nin natural language queries. Structured-GraphRAG utilizes multiple knowledge\ngraphs, which represent data in a structured format and capture complex\nrelationships between entities, enabling a more nuanced and comprehensive\nretrieval of information. This graph-based approach reduces the risk of errors\nin language model outputs by grounding responses in a structured format,\nthereby enhancing the reliability of results. We demonstrate the effectiveness\nof Structured-GraphRAG by comparing its performance with that of a recently\npublished method using traditional retrieval-augmented generation. Our findings\nshow that Structured-GraphRAG significantly improves query processing\nefficiency and reduces response times. While our case study focuses on soccer\ndata, the framework's design is broadly applicable, offering a powerful tool\nfor data analysis and enhancing language model applications across various\nstructured domains.",
      "tldr_zh": "这篇论文介绍了 Structured-GraphRAG 框架，用于提升结构化数据的检索性能，针对传统方法如顺序搜索和索引检索在处理复杂互联数据集时的不足。框架通过利用 knowledge graphs 来表示数据并捕获实体间的复杂关系，结合检索增强生成（retrieval-augmented generation）技术，提高了信息检索的准确性和可靠性。实验以足球数据为例，显示 Structured-GraphRAG 显著提高了查询处理效率并减少了响应时间，且其设计适用于各种结构化领域的数据分析。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB",
        "H.2; H.3; E.1; E.2"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17580v1",
      "published_date": "2024-09-26 06:53:29 UTC",
      "updated_date": "2024-09-26 06:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:18:52.517193"
    },
    {
      "arxiv_id": "2409.17572v1",
      "title": "Dr. GPT in Campus Counseling: Understanding Higher Education Students' Opinions on LLM-assisted Mental Health Services",
      "title_zh": "翻译失败",
      "authors": [
        "Owen Xingjian Zhang",
        "Shuyao Zhou",
        "Jiayi Geng",
        "Yuhan Liu",
        "Sunny Xun Liu"
      ],
      "abstract": "In response to the increasing mental health challenges faced by college\nstudents, we sought to understand their perspectives on how AI applications,\nparticularly Large Language Models (LLMs), can be leveraged to enhance their\nmental well-being. Through pilot interviews with ten diverse students, we\nexplored their opinions on the use of LLMs across five fictional scenarios:\nGeneral Information Inquiry, Initial Screening, Reshaping Patient-Expert\nDynamics, Long-term Care, and Follow-up Care. Our findings revealed that\nstudents' acceptance of LLMs varied by scenario, with participants highlighting\nboth potential benefits, such as proactive engagement and personalized\nfollow-up care, and concerns, including limitations in training data and\nemotional support. These insights inform how AI technology should be designed\nand implemented to effectively support and enhance students' mental well-being,\nparticularly in scenarios where LLMs can complement traditional methods, while\nmaintaining empathy and respecting individual preferences.",
      "tldr_zh": "本研究调查了大学生的观点，探讨如何利用Large Language Models (LLMs)来提升校园心理健康服务，以应对学生面临的心理挑战。通过对10名多样化学生的试点访谈，分析了LLMs在五个虚构场景中的应用，包括General Information Inquiry、Initial Screening、Reshaping Patient-Expert Dynamics、Long-term Care和Follow-up Care。结果显示，学生的接受度因场景而异，他们认可LLMs的潜在益处，如主动参与和个性化后续护理，但也表达了担忧，包括训练数据限制和情感支持不足。这些发现为设计AI技术提供了指导，帮助其补充传统方法，同时强调保持同理心和尊重个人偏好。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17572v1",
      "published_date": "2024-09-26 06:40:45 UTC",
      "updated_date": "2024-09-26 06:40:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:19:06.111731"
    },
    {
      "arxiv_id": "2409.17568v1",
      "title": "Showing Many Labels in Multi-label Classification Models: An Empirical Study of Adversarial Examples",
      "title_zh": "翻译失败",
      "authors": [
        "Yujiang Liu",
        "Wenjian Luo",
        "Zhijian Chen",
        "Muhammad Luqman Naseem"
      ],
      "abstract": "With the rapid development of Deep Neural Networks (DNNs), they have been\napplied in numerous fields. However, research indicates that DNNs are\nsusceptible to adversarial examples, and this is equally true in the\nmulti-label domain. To further investigate multi-label adversarial examples, we\nintroduce a novel type of attacks, termed \"Showing Many Labels\". The objective\nof this attack is to maximize the number of labels included in the classifier's\nprediction results. In our experiments, we select nine attack algorithms and\nevaluate their performance under \"Showing Many Labels\". Eight of the attack\nalgorithms were adapted from the multi-class environment to the multi-label\nenvironment, while the remaining one was specifically designed for the\nmulti-label environment. We choose ML-LIW and ML-GCN as target models and train\nthem on four popular multi-label datasets: VOC2007, VOC2012, NUS-WIDE, and\nCOCO. We record the success rate of each algorithm when it shows the expected\nnumber of labels in eight different scenarios. Experimental results indicate\nthat under the \"Showing Many Labels\", iterative attacks perform significantly\nbetter than one-step attacks. Moreover, it is possible to show all labels in\nthe dataset.",
      "tldr_zh": "本研究探讨了多标签分类模型（multi-label classification models）中对抗性示例（adversarial examples）的易受攻击性，引入了一种新攻击类型“Showing Many Labels”，旨在最大化分类器预测标签的数量。研究者选取了九种攻击算法（其中八种从多类环境适应到多标签环境），并在 ML-LIW 和 ML-GCN 模型上进行实验，使用 VOC2007、VOC2012、NUS-WIDE 和 COCO 等四个流行数据集。实验结果显示，迭代攻击（iterative attacks）显著优于一步攻击（one-step attacks），并证明了可能使模型显示数据集中的所有标签，这突显了多标签模型的安全隐患。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17568v1",
      "published_date": "2024-09-26 06:31:31 UTC",
      "updated_date": "2024-09-26 06:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:19:15.922799"
    },
    {
      "arxiv_id": "2409.17565v1",
      "title": "Pixel-Space Post-Training of Latent Diffusion Models",
      "title_zh": "像素空间后训练的隐变量扩散模型",
      "authors": [
        "Christina Zhang",
        "Simran Motwani",
        "Matthew Yu",
        "Ji Hou",
        "Felix Juefei-Xu",
        "Sam Tsai",
        "Peter Vajda",
        "Zijian He",
        "Jialiang Wang"
      ],
      "abstract": "Latent diffusion models (LDMs) have made significant advancements in the\nfield of image generation in recent years. One major advantage of LDMs is their\nability to operate in a compressed latent space, allowing for more efficient\ntraining and deployment. However, despite these advantages, challenges with\nLDMs still remain. For example, it has been observed that LDMs often generate\nhigh-frequency details and complex compositions imperfectly. We hypothesize\nthat one reason for these flaws is due to the fact that all pre- and\npost-training of LDMs are done in latent space, which is typically $8 \\times 8$\nlower spatial-resolution than the output images. To address this issue, we\npropose adding pixel-space supervision in the post-training process to better\npreserve high-frequency details. Experimentally, we show that adding a\npixel-space objective significantly improves both supervised quality\nfine-tuning and preference-based post-training by a large margin on a\nstate-of-the-art DiT transformer and U-Net diffusion models in both visual\nquality and visual flaw metrics, while maintaining the same text alignment\nquality.",
      "tldr_zh": "潜在扩散模型（LDMs）在图像生成领域表现出色，但由于训练过程仅在低分辨率潜在空间中进行，常导致高频细节和复杂组合生成不完美。论文提出一种像素空间后训练方法，通过添加像素-space objective作为监督机制，提升模型在处理这些细节方面的表现。实验结果表明，该方法在先进的DiT transformer和U-Net扩散模型上，大幅提高了视觉质量和缺陷指标，同时保持了文本对齐质量的稳定性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17565v1",
      "published_date": "2024-09-26 06:27:26 UTC",
      "updated_date": "2024-09-26 06:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:19:28.100156"
    },
    {
      "arxiv_id": "2410.01840v1",
      "title": "Target Pose Guided Whole-body Grasping Motion Generation for Digital Humans",
      "title_zh": "目标姿势引导的全身抓取动作生成用于数字人类",
      "authors": [
        "Quanquan Shao",
        "Yi Fang"
      ],
      "abstract": "Grasping manipulation is a fundamental mode for human interaction with daily\nlife objects. The synthesis of grasping motion is also greatly demanded in many\napplications such as animation and robotics. In objects grasping research\nfield, most works focus on generating the last static grasping pose with a\nparallel gripper or dexterous hand. Grasping motion generation for the full arm\nespecially for the full humanlike intelligent agent is still under-explored. In\nthis work, we propose a grasping motion generation framework for digital human\nwhich is an anthropomorphic intelligent agent with high degrees of freedom in\nvirtual world. Given an object known initial pose in 3D space, we first\ngenerate a target pose for whole-body digital human based on off-the-shelf\ntarget grasping pose generation methods. With an initial pose and this\ngenerated target pose, a transformer-based neural network is used to generate\nthe whole grasping trajectory, which connects initial pose and target pose\nsmoothly and naturally. Additionally, two post optimization components are\ndesigned to mitigates foot-skating issue and hand-object interpenetration\nseparately. Experiments are conducted on GRAB dataset to demonstrate\neffectiveness of this proposed method for whole-body grasping motion generation\nwith randomly placed unknown objects.",
      "tldr_zh": "本文提出一个针对数字人类的整体抓取动作生成框架，该框架以目标姿势为指导，首先利用现有方法生成整体身体目标姿势，然后采用基于 Transformer 的神经网络生成从初始姿势到目标姿势的平滑自然轨迹，并通过后优化组件解决脚滑和手物穿透问题。该方法填补了现有研究对全臂和全身体抓取动作生成领域的空白，在 GRAB 数据集上进行的实验证明了其在处理随机放置未知对象时的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages,5 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.01840v1",
      "published_date": "2024-09-26 05:43:23 UTC",
      "updated_date": "2024-09-26 05:43:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:19:39.998394"
    },
    {
      "arxiv_id": "2410.02820v3",
      "title": "Heuristics and Biases in AI Decision-Making: Implications for Responsible AGI",
      "title_zh": "翻译失败",
      "authors": [
        "Payam Saeedi",
        "Mahsa Goodarzi",
        "M Abdullah Canbaz"
      ],
      "abstract": "We investigate the presence of cognitive biases in three large language\nmodels (LLMs): GPT-4o, Gemma 2, and Llama 3.1. The study uses 1,500 experiments\nacross nine established cognitive biases to evaluate the models' responses and\nconsistency. GPT-4o demonstrated the strongest overall performance. Gemma 2\nshowed strengths in addressing the sunk cost fallacy and prospect theory,\nhowever its performance varied across different biases. Llama 3.1 consistently\nunderperformed, relying on heuristics and exhibiting frequent inconsistencies\nand contradictions. The findings highlight the challenges of achieving robust\nand generalizable reasoning in LLMs, and underscore the need for further\ndevelopment to mitigate biases in artificial general intelligence (AGI). The\nstudy emphasizes the importance of integrating statistical reasoning and\nethical considerations in future AI development.",
      "tldr_zh": "本研究评估了三个大型语言模型（LLMs）——GPT-4o、Gemma 2 和 Llama 3.1——在九种认知偏差中的表现，共进行1,500次实验，以检验模型的响应和一致性。结果显示，GPT-4o 整体表现最佳，而Gemma 2 在处理 sunk cost fallacy 和 prospect theory 时表现出色，但其他偏差表现不稳定；Llama 3.1 则依赖 heuristics，频繁出现不一致和矛盾，导致表现最差。这些发现突出了 LLMs 在实现稳健、可泛化推理方面的挑战，并强调了在 AGI 开发中缓解偏差的重要性，包括整合统计推理和伦理考虑。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02820v3",
      "published_date": "2024-09-26 05:34:00 UTC",
      "updated_date": "2025-04-07 02:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:19:52.459938"
    },
    {
      "arxiv_id": "2409.17547v2",
      "title": "Triple Point Masking",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaming Liu",
        "Linghe Kong",
        "Yue Wu",
        "Maoguo Gong",
        "Hao Li",
        "Qiguang Miao",
        "Wenping Ma",
        "Can Qin"
      ],
      "abstract": "Existing 3D mask learning methods encounter performance bottlenecks under\nlimited data, and our objective is to overcome this limitation. In this paper,\nwe introduce a triple point masking scheme, named TPM, which serves as a\nscalable framework for pre-training of masked autoencoders to achieve\nmulti-mask learning for 3D point clouds. Specifically, we augment the baselines\nwith two additional mask choices (i.e., medium mask and low mask) as our core\ninsight is that the recovery process of an object can manifest in diverse ways.\nPrevious high-masking schemes focus on capturing the global representation but\nlack the fine-grained recovery capability, so that the generated pre-trained\nweights tend to play a limited role in the fine-tuning process. With the\nsupport of the proposed TPM, available methods can exhibit more flexible and\naccurate completion capabilities, enabling the potential autoencoder in the\npre-training stage to consider multiple representations of a single 3D object.\nIn addition, an SVM-guided weight selection module is proposed to fill the\nencoder parameters for downstream networks with the optimal weight during the\nfine-tuning stage, maximizing linear accuracy and facilitating the acquisition\nof intricate representations for new objects. Extensive experiments show that\nthe four baselines equipped with the proposed TPM achieve comprehensive\nperformance improvements on various downstream tasks. Our code and models are\navailable at https://github.com/liujia99/TPM.",
      "tldr_zh": "本论文提出了一种名为 TPM（Triple Point Masking）的三重点掩码方案，作为一个可扩展框架，用于 3D point clouds 的 masked autoencoders 预训练，实现多掩码学习（包括高掩码、中等掩码和低掩码），以克服现有方法在数据有限时的性能瓶颈。TPM 的核心见解是，通过多样化的掩码选择增强对象的恢复能力，提供更灵活和细粒度的表示，避免了传统高-masking 方案的全局偏重问题。论文还引入了 SVM-guided weight selection module，在 fine-tuning 阶段优化编码器参数，提高线性准确性和复杂表示。实验结果显示，四种基线模型配备 TPM 后，在各种下游任务上实现了全面性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17547v2",
      "published_date": "2024-09-26 05:33:30 UTC",
      "updated_date": "2024-10-15 04:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:20:04.388778"
    },
    {
      "arxiv_id": "2409.17545v2",
      "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
      "title_zh": "翻译失败",
      "authors": [
        "Cheolhun Jang"
      ],
      "abstract": "Preference optimization methods typically begin training with a well-trained\nSFT model as a reference model. In RLHF and DPO, a regularization term is used\nduring the preference optimization process to prevent the policy model from\ndeviating too far from the reference model's distribution, thereby avoiding the\ngeneration of anomalous responses. When the reference model is already\nwell-aligned with the given data or only requires slight adjustments, this\napproach can produce a well-aligned model. However, if the reference model is\nnot aligned with the given data and requires significant deviation from its\ncurrent state, a regularization term may actually hinder the model alignment.\nIn this study, we propose \\textbf{Modulated Intervention Preference\nOptimization (MIPO)} to address this issue. MIPO modulates the degree of\nintervention from the reference model based on how well the given data is\naligned with it. If the data is well-aligned, the intervention is increased to\nprevent the policy model from diverging significantly from reference model.\nConversely, if the alignment is poor, the interference is reduced to facilitate\nmore extensive training. We compare the performance of MIPO and DPO using\nMistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental\nresults demonstrate that MIPO consistently outperforms DPO across various\nevaluation scenarios.",
      "tldr_zh": "本文提出Modulated Intervention Preference Optimization (MIPO)，一种改进偏好优化方法的框架，针对传统方法如DPO和RLHF的局限性——当参考模型需要大幅调整时，正则化项可能阻碍优化。MIPO根据数据与参考模型的契合度动态调整干预程度：如果数据契合良好，则增加干预以保持稳定性；如果契合差，则减少干预以允许更大偏差。实验结果显示，在Alpaca Eval 2.0和MT-Bench基准上，使用Mistral-7B和Llama3-8B模型，MIPO在各种场景下均优于DPO，证明了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8pages, submitted to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.17545v2",
      "published_date": "2024-09-26 05:24:14 UTC",
      "updated_date": "2024-09-27 06:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:20:17.249349"
    },
    {
      "arxiv_id": "2409.17538v5",
      "title": "On the Implicit Relation Between Low-Rank Adaptation and Differential Privacy",
      "title_zh": "关于低秩适配和差分隐私之间的隐含关系",
      "authors": [
        "Saber Malekmohammadi",
        "Golnoosh Farnadi"
      ],
      "abstract": "A significant approach in natural language processing involves large-scale\npre-training of models on general domain data followed by their adaptation to\nspecific tasks or domains. As models grow in size, full fine-tuning all of\ntheir parameters becomes increasingly impractical. To address this, some\nmethods for low-rank task adaptation of language models have been proposed,\ne.g., LoRA and FLoRA. These methods keep the pre-trained model weights fixed\nand incorporate trainable low-rank decomposition matrices into some layers of\nthe transformer architecture, called adapters. This approach significantly\nreduces the number of trainable parameters required for downstream tasks\ncompared to full fine-tuning all parameters. In this work, we look at low-rank\nadaptation from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA and FLoRA leads to the injection of some\nrandom noise into the batch gradients w.r.t the adapter parameters. We quantify\nthe variance of the injected noise and show that the smaller the adaptation\nrank, the larger the noise variance. By establishing a Berry-Esseen type bound\non the total variation distance between distribution of the injected noise and\na Gaussian distribution with the same variance, we show that the dynamics of\nlow-rank adaptation is close to that of differentially private fine-tuning of\nthe adapters. Finally, using Johnson-Lindenstrauss lemma, we show that when\naugmented with gradient scaling, low-rank adaptation is very close to\nperforming DPSGD algorithm with a fixed noise scale to fine-tune the adapters.\nSuggested by our theoretical findings and approved by our experimental results,\nwe show that low-rank adaptation, besides mitigating the space and\ncomputational complexities, implicitly provides a privacy protection w.r.t the\nfine-tuning data, without inducing the high space complexity of DPSGD.",
      "tldr_zh": "本研究探讨了低秩适配（如LoRA和FLoRA）与差分隐私（Differential Privacy）之间的隐含关系，旨在解决大语言模型在任务适配时参数训练效率和隐私保护的问题。作者理论证明，低秩适配会向批量梯度注入随机噪声，且噪声方差与适配秩成反比，通过Berry-Esseen定理和Johnson-Lindenstrauss引理，证明其动态类似于差分隐私训练。实验结果显示，低秩适配不仅降低了空间和计算复杂性，还隐式提供了对微调数据的隐私保护，接近于DPSGD算法的效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17538v5",
      "published_date": "2024-09-26 04:56:49 UTC",
      "updated_date": "2025-04-02 03:18:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:20:28.188148"
    },
    {
      "arxiv_id": "2409.17534v2",
      "title": "Just Say What You Want: Only-prompting Self-rewarding Online Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ruijie Xu",
        "Zhihan Liu",
        "Yongfei Liu",
        "Shipeng Yan",
        "Zhaoran Wang",
        "Zhi Zhang",
        "Xuming He"
      ],
      "abstract": "We address the challenge of online Reinforcement Learning from Human Feedback\n(RLHF) with a focus on self-rewarding alignment methods. In online RLHF,\nobtaining feedback requires interaction with the environment, which can be\ncostly when using additional reward models or the GPT-4 API. Current\nself-rewarding approaches rely heavily on the discriminator's judgment\ncapabilities, which are effective for large-scale models but challenging to\ntransfer to smaller ones. To address these limitations, we propose a novel,\nonly-prompting self-rewarding online algorithm that generates preference\ndatasets without relying on judgment capabilities. Additionally, we employ\nfine-grained arithmetic control over the optimality gap between positive and\nnegative examples, generating more hard negatives in the later stages of\ntraining to help the model better capture subtle human preferences. Finally, we\nconduct extensive experiments on two base models, Mistral-7B and\nMistral-Instruct-7B, which significantly bootstrap the performance of the\nreference model, achieving 34.5% in the Length-controlled Win Rates of\nAlpacaEval 2.0.",
      "tldr_zh": "这篇论文针对在线 Reinforcement Learning from Human Feedback (RLHF) 的自奖励对齐挑战，提出了一种 only-prompting 自奖励在线算法，该方法通过仅使用提示生成偏好数据集，而不依赖鉴别器的判断能力，从而降低反馈成本并适用于小型模型。算法还引入细粒度的算术控制来管理正负例之间的最优性差距，尤其在训练后期生成更多硬负例，以帮助模型更好地捕捉微妙的人类偏好。实验结果显示，在 Mistral-7B 和 Mistral-Instruct-7B 基模型上，该方法显著提升了参考模型的性能，在 AlpacaEval 2.0 的长度控制胜率中达到 34.5%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17534v2",
      "published_date": "2024-09-26 04:41:08 UTC",
      "updated_date": "2024-10-14 05:43:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:20:41.686881"
    },
    {
      "arxiv_id": "2409.17531v2",
      "title": "SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Dai",
        "Lingfeng Yang",
        "Yihao Xu",
        "Zhenhua Feng",
        "Wankou Yang"
      ],
      "abstract": "Visual grounding is a common vision task that involves grounding descriptive\nsentences to the corresponding regions of an image. Most existing methods use\nindependent image-text encoding and apply complex hand-crafted modules or\nencoder-decoder architectures for modal interaction and query reasoning.\nHowever, their performance significantly drops when dealing with complex\ntextual expressions. This is because the former paradigm only utilizes limited\ndownstream data to fit the multi-modal feature fusion. Therefore, it is only\neffective when the textual expressions are relatively simple. In contrast,\ngiven the wide diversity of textual expressions and the uniqueness of\ndownstream training data, the existing fusion module, which extracts multimodal\ncontent from a visual-linguistic context, has not been fully investigated. In\nthis paper, we present a simple yet robust transformer-based framework, SimVG,\nfor visual grounding. Specifically, we decouple visual-linguistic feature\nfusion from downstream tasks by leveraging existing multimodal pre-trained\nmodels and incorporating additional object tokens to facilitate deep\nintegration of downstream and pre-training tasks. Furthermore, we design a\ndynamic weight-balance distillation method in the multi-branch synchronous\nlearning process to enhance the representation capability of the simpler\nbranch. This branch only consists of a lightweight MLP, which simplifies the\nstructure and improves reasoning speed. Experiments on six widely used VG\ndatasets, i.e., RefCOCO/+/g, ReferIt, Flickr30K, and GRefCOCO, demonstrate the\nsuperiority of SimVG. Finally, the proposed method not only achieves\nimprovements in efficiency and convergence speed but also attains new\nstate-of-the-art performance on these benchmarks. Codes and models will be\navailable at \\url{https://github.com/Dmmm1997/SimVG}.",
      "tldr_zh": "本论文针对视觉 grounding 任务提出了一种简单框架 SimVG，通过解耦的多模态融合来处理图像和文本描述的对应关系，解决现有方法在复杂文本表达上性能下降的问题。SimVG 利用现有的多模态预训练模型并添加额外对象 tokens，实现视觉-语言特征融合的深度整合，同时引入动态权重平衡蒸馏方法来增强简单分支（如轻量级 MLP）的表示能力，提高推理速度和结构效率。在 RefCOCO/+/g、ReferIt、Flickr30K 和 GRefCOCO 等六个数据集上的实验表明，SimVG 实现了新的 state-of-the-art 性能，并显著提升了收敛速度和整体效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "24pages, 18figures, NeurIPS2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17531v2",
      "published_date": "2024-09-26 04:36:19 UTC",
      "updated_date": "2024-10-28 07:21:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:20:56.281601"
    },
    {
      "arxiv_id": "2409.19024v1",
      "title": "Elephant in the Room: Unveiling the Impact of Reward Model Quality in Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Liu",
        "Xiaoyuan Yi",
        "Xiaokang Chen",
        "Jing Yao",
        "Jingwei Yi",
        "Daoguang Zan",
        "Zheng Liu",
        "Xing Xie",
        "Tsung-Yi Ho"
      ],
      "abstract": "The demand for regulating potentially risky behaviors of large language\nmodels (LLMs) has ignited research on alignment methods. Since LLM alignment\nheavily relies on reward models for optimization or evaluation, neglecting the\nquality of reward models may cause unreliable results or even misalignment.\nDespite the vital role reward models play in alignment, previous works have\nconsistently overlooked their performance and used off-the-shelf reward models\narbitrarily without verification, rendering the reward model ``\\emph{an\nelephant in the room}''. To this end, this work first investigates the quality\nof the widely-used preference dataset, HH-RLHF, and curates a clean version,\nCHH-RLHF. Based on CHH-RLHF, we benchmark the accuracy of a broad range of\nreward models used in previous alignment works, unveiling the unreliability of\nusing them both for optimization and evaluation. Furthermore, we systematically\nstudy the impact of reward model quality on alignment performance in three\nreward utilization paradigms. Extensive experiments reveal that better reward\nmodels perform as better human preference proxies. This work aims to awaken\npeople to notice this huge elephant in alignment research. We call attention to\nthe following issues: (1) The reward model needs to be rigorously evaluated,\nwhether for alignment optimization or evaluation. (2) Considering the role of\nreward models, research efforts should not only concentrate on alignment\nalgorithm, but also on developing more reliable human proxy.",
      "tldr_zh": "该论文揭示了大型语言模型（LLMs）对齐（alignment）过程中，奖励模型（reward models）质量的重要性，指出以往研究忽略其性能可能导致不可靠结果。作者首先调查了HH-RLHF数据集，并创建了更清洁的版本CHH-RLHF，用于基准测试多种奖励模型的准确性，结果显示这些模型在优化和评估中均不可靠。通过系统研究三种奖励利用范式，实验证明高质量奖励模型能更好地代理人类偏好，从而提升对齐性能。该研究呼吁严格评估奖励模型，并将研究重点扩展到开发更可靠的人类代理上，而不仅仅局限于对齐算法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19024v1",
      "published_date": "2024-09-26 04:28:35 UTC",
      "updated_date": "2024-09-26 04:28:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:21:04.809197"
    },
    {
      "arxiv_id": "2409.17526v1",
      "title": "Drone Stereo Vision for Radiata Pine Branch Detection and Distance Measurement: Integrating SGBM and Segmentation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yida Lin",
        "Bing Xue",
        "Mengjie Zhang",
        "Sam Schofield",
        "Richard Green"
      ],
      "abstract": "Manual pruning of radiata pine trees presents significant safety risks due to\ntheir substantial height and the challenging terrains in which they thrive. To\naddress these risks, this research proposes the development of a drone-based\npruning system equipped with specialized pruning tools and a stereo vision\ncamera, enabling precise detection and trimming of branches. Deep learning\nalgorithms, including YOLO and Mask R-CNN, are employed to ensure accurate\nbranch detection, while the Semi-Global Matching algorithm is integrated to\nprovide reliable distance estimation. The synergy between these techniques\nfacilitates the precise identification of branch locations and enables\nefficient, targeted pruning. Experimental results demonstrate that the combined\nimplementation of YOLO and SGBM enables the drone to accurately detect branches\nand measure their distances from the drone. This research not only improves the\nsafety and efficiency of pruning operations but also makes a significant\ncontribution to the advancement of drone technology in the automation of\nagricultural and forestry practices, laying a foundational framework for\nfurther innovations in environmental management.",
      "tldr_zh": "本研究针对辐射松树手动修剪的安全风险，提出了一种基于无人机的修剪系统，该系统配备立体视觉相机和专用工具，用于精确检测和修剪树枝。方法上，采用YOLO和Mask R-CNN算法进行树枝检测，并整合Semi-Global Matching (SGBM)算法实现可靠的距离测量，从而实现树枝位置的精确识别和针对性修剪。实验结果显示，YOLO与SGBM的结合使无人机能够准确检测树枝并测量其距离，提升了修剪操作的安全性和效率。该创新为农业和林业实践的自动化奠定了基础，促进无人机技术在环境管理中的进一步发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17526v1",
      "published_date": "2024-09-26 04:27:44 UTC",
      "updated_date": "2024-09-26 04:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:21:15.964075"
    },
    {
      "arxiv_id": "2409.17523v1",
      "title": "EAGLE: Egocentric AGgregated Language-video Engine",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Bi",
        "Yunlong Tang",
        "Luchuan Song",
        "Ali Vosoughi",
        "Nguyen Nguyen",
        "Chenliang Xu"
      ],
      "abstract": "The rapid evolution of egocentric video analysis brings new insights into\nunderstanding human activities and intentions from a first-person perspective.\nDespite this progress, the fragmentation in tasks like action recognition,\nprocedure learning, and moment retrieval, \\etc, coupled with inconsistent\nannotations and isolated model development, hinders a holistic interpretation\nof video content. In response, we introduce the EAGLE (Egocentric AGgregated\nLanguage-video Engine) model and the EAGLE-400K dataset to provide a unified\nframework that integrates various egocentric video understanding tasks.\nEAGLE-400K, the \\textit{first} large-scale instruction-tuning dataset tailored\nfor egocentric video, features 400K diverse samples to enhance a broad spectrum\nof tasks from activity recognition to procedure knowledge learning. Moreover,\nEAGLE, a strong video multimodal large language model (MLLM), is designed to\neffectively capture both spatial and temporal information. In addition, we\npropose a set of evaluation metrics designed to facilitate a thorough\nassessment of MLLM for egocentric video understanding. Our extensive\nexperiments demonstrate EAGLE's superior performance over existing models,\nhighlighting its ability to balance task-specific understanding with holistic\nvideo interpretation. With EAGLE, we aim to pave the way for research\nopportunities and practical applications in real-world scenarios.",
      "tldr_zh": "该研究针对第一人称视角视频分析中的任务碎片化和标注不一致问题，引入了EAGLE（Egocentric AGgregated Language-video Engine）模型和EAGLE-400K数据集，提供一个统一的框架来整合各种视频理解任务。EAGLE-400K是首个针对第一人称视频的大规模指令微调数据集，包含40万样例，覆盖从活动识别到程序知识学习的多种场景。EAGLE作为一种强大的视频多模态大型语言模型（MLLM），有效捕捉空间和时间信息，并配以专属评估指标。实验结果显示，EAGLE在多任务上优于现有模型，实现了任务特定理解与整体视频解读的平衡，为实际应用和研究机会铺平道路。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACMMM 24",
      "pdf_url": "http://arxiv.org/pdf/2409.17523v1",
      "published_date": "2024-09-26 04:17:27 UTC",
      "updated_date": "2024-09-26 04:17:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:21:28.870738"
    },
    {
      "arxiv_id": "2409.17519v1",
      "title": "Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization",
      "title_zh": "基于预训练视觉语言模型和黑箱优化的机器人环境状态识别",
      "authors": [
        "Kento Kawaharazuka",
        "Yoshiki Obinata",
        "Naoaki Kanazawa",
        "Kei Okada",
        "Masayuki Inaba"
      ],
      "abstract": "In order for robots to autonomously navigate and operate in diverse\nenvironments, it is essential for them to recognize the state of their\nenvironment. On the other hand, the environmental state recognition has\ntraditionally involved distinct methods tailored to each state to be\nrecognized. In this study, we perform a unified environmental state recognition\nfor robots through the spoken language with pre-trained large-scale\nvision-language models. We apply Visual Question Answering and Image-to-Text\nRetrieval, which are tasks of Vision-Language Models. We show that with our\nmethod, it is possible to recognize not only whether a room door is\nopen/closed, but also whether a transparent door is open/closed and whether\nwater is running in a sink, without training neural networks or manual\nprogramming. In addition, the recognition accuracy can be improved by selecting\nappropriate texts from the set of prepared texts based on black-box\noptimization. For each state recognition, only the text set and its weighting\nneed to be changed, eliminating the need to prepare multiple different models\nand programs, and facilitating the management of source code and computer\nresource. We experimentally demonstrate the effectiveness of our method and\napply it to the recognition behavior on a mobile robot, Fetch.",
      "tldr_zh": "本文提出了一种利用预训练的 Vision-Language Models 进行机器人环境状态识别的方法，结合 Visual Question Answering 和 Image-to-Text Retrieval，通过口头语言实现统一的识别过程，而无需针对每个状态训练神经网络或手动编程。方法通过 Black-Box Optimization 优化文本选择，提高了识别准确性，例如准确判断房间门、透明门开/关或水龙头是否运行。实验验证了该方法的有效性，并成功应用于移动机器人 Fetch，便于源代码管理和资源利用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at Advanced Robotics, website -\n  https://haraduka.github.io/vlm-bbo/",
      "pdf_url": "http://arxiv.org/pdf/2409.17519v1",
      "published_date": "2024-09-26 04:02:20 UTC",
      "updated_date": "2024-09-26 04:02:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:21:40.629838"
    },
    {
      "arxiv_id": "2409.17518v2",
      "title": "Multi-Designated Detector Watermarking for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengan Huang",
        "Gongxian Zeng",
        "Xin Mu",
        "Yu Wang",
        "Yue Yu"
      ],
      "abstract": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics.",
      "tldr_zh": "本论文提出了 Multi-Designated Detector Watermarking (MDDW) 技术，用于大型语言模型 (LLMs)，允许模型提供者生成水印输出，使其仅能被特定（可能多个）指定检测器识别，同时确保普通用户输出质量不受影响。作者形式化了 MDDW 的安全定义，并基于 multi-designated verifier signatures (MDVS) 构建了一个通用框架，以实现这一功能。此外，他们引入了 claimability 作为可选安全特性，通过一种通用转换将 MDVS 转化为可声明所有权的版本，并展示了 MDDW 方案的先进功能、灵活性和满意性能指标。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17518v2",
      "published_date": "2024-09-26 04:01:15 UTC",
      "updated_date": "2024-10-01 08:08:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:21:53.492677"
    },
    {
      "arxiv_id": "2409.17517v1",
      "title": "Dataset Distillation-based Hybrid Federated Learning on Non-IID Data",
      "title_zh": "翻译失败",
      "authors": [
        "Xiufang Shi",
        "Wei Zhang",
        "Mincheng Wu",
        "Guangyi Liu",
        "Zhenyu Wen",
        "Shibo He",
        "Tejal Shah",
        "Rajiv Ranjan"
      ],
      "abstract": "In federated learning, the heterogeneity of client data has a great impact on\nthe performance of model training. Many heterogeneity issues in this process\nare raised by non-independently and identically distributed (Non-IID) data.\nThis study focuses on the issue of label distribution skew. To address it, we\npropose a hybrid federated learning framework called HFLDD, which integrates\ndataset distillation to generate approximately independent and equally\ndistributed (IID) data, thereby improving the performance of model training.\nParticularly, we partition the clients into heterogeneous clusters, where the\ndata labels among different clients within a cluster are unbalanced while the\ndata labels among different clusters are balanced. The cluster headers collect\ndistilled data from the corresponding cluster members, and conduct model\ntraining in collaboration with the server. This training process is like\ntraditional federated learning on IID data, and hence effectively alleviates\nthe impact of Non-IID data on model training. Furthermore, we compare our\nproposed method with typical baseline methods on public datasets. Experimental\nresults demonstrate that when the data labels are severely imbalanced, the\nproposed HFLDD outperforms the baseline methods in terms of both test accuracy\nand communication cost.",
      "tldr_zh": "该研究针对联邦学习（Federated Learning）中非独立同分布（Non-IID）数据，尤其是标签分布偏差的问题，提出了一种混合框架 HFLDD。HFLDD 通过数据集蒸馏（Dataset Distillation）生成近似独立同分布（IID）数据，将客户端分区为异质集群，并在集群头与服务器协作训练，从而缓解 Non-IID 数据对模型性能的影响。实验结果显示，在公共数据集上，当数据标签严重不平衡时，HFLDD 在测试准确率和通信成本方面均优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17517v1",
      "published_date": "2024-09-26 03:52:41 UTC",
      "updated_date": "2024-09-26 03:52:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:22:04.381061"
    },
    {
      "arxiv_id": "2409.17516v1",
      "title": "Functional Classification of Spiking Signal Data Using Artificial Intelligence Techniques: A Review",
      "title_zh": "利用人工智能技术对尖峰信号数据进行功能分类：综述",
      "authors": [
        "Danial Sharifrazi",
        "Nouman Javed",
        "Javad Hassannataj Joloudari",
        "Roohallah Alizadehsani",
        "Prasad N. Paradkar",
        "Ru-San Tan",
        "U. Rajendra Acharya",
        "Asim Bhatti"
      ],
      "abstract": "Human brain neuron activities are incredibly significant nowadays. Neuronal\nbehavior is assessed by analyzing signal data such as electroencephalography\n(EEG), which can offer scientists valuable information about diseases and\nhuman-computer interaction. One of the difficulties researchers confront while\nevaluating these signals is the existence of large volumes of spike data.\nSpikes are some considerable parts of signal data that can happen as a\nconsequence of vital biomarkers or physical issues such as electrode movements.\nHence, distinguishing types of spikes is important. From this spot, the spike\nclassification concept commences. Previously, researchers classified spikes\nmanually. The manual classification was not precise enough as it involves\nextensive analysis. Consequently, Artificial Intelligence (AI) was introduced\ninto neuroscience to assist clinicians in classifying spikes correctly. This\nreview discusses the importance and use of AI in spike classification, focusing\non the recognition of neural activity noises. The task is divided into three\nmain components: preprocessing, classification, and evaluation. Existing\nmethods are introduced and their importance is determined. The review also\nhighlights the need for more efficient algorithms. The primary goal is to\nprovide a perspective on spike classification for future research and provide a\ncomprehensive understanding of the methodologies and issues involved. The\nreview organizes materials in the spike classification field for future\nstudies. In this work, numerous studies were extracted from different\ndatabases. The PRISMA-related research guidelines were then used to choose\npapers. Then, research studies based on spike classification using machine\nlearning and deep learning approaches with effective preprocessing were\nselected.",
      "tldr_zh": "这篇综述论文探讨了使用人工智能（AI）技术对脑神经信号中的尖峰（spikes）进行功能分类的重要性，特别是通过电encephalography (EEG) 等方法分析神经活动噪音。论文将任务分为预处理、分类和评估三个部分，回顾了现有机器学习和深度学习方法，并强调了高效算法的需求，以提高分类精度。最终，该研究基于 PRISMA 指南从数据库提取相关文献，为未来尖峰分类研究提供全面视角和组织框架。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "8 figures, 32 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17516v1",
      "published_date": "2024-09-26 03:50:55 UTC",
      "updated_date": "2024-09-26 03:50:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:22:16.242131"
    },
    {
      "arxiv_id": "2409.17515v3",
      "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection",
      "title_zh": "翻译失败",
      "authors": [
        "Xinlei Wang",
        "Maike Feng",
        "Jing Qiu",
        "Jinjin Gu",
        "Junhua Zhao"
      ],
      "abstract": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data.",
      "tldr_zh": "这篇论文提出了一种新方法，使用 Large Language Models (LLMs) 和 Generative Agents，通过跨文本和时间序列数据的推理，将社会事件自适应整合到时间序列预测中，以提升预测准确性。具体来说，该方法利用 LLM 代理迭代过滤无关新闻，进行类似人类的推理分析复杂事件（如意外事故和社会行为变化），并持续优化新闻选择和输出鲁棒性，同时微调预训练 LLM 来预测时间序列数字。结果显示，该方法显著提高了预测准确性，暗示了通过有效利用非结构化新闻数据可能引发时间序列预测范式的转变。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted for NeurIPS 2024. Code and data are\n  available at https://github.com/ameliawong1996/From_News_to_Forecast",
      "pdf_url": "http://arxiv.org/pdf/2409.17515v3",
      "published_date": "2024-09-26 03:50:22 UTC",
      "updated_date": "2024-10-30 12:04:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:22:29.114949"
    },
    {
      "arxiv_id": "2409.17510v3",
      "title": "NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes",
      "title_zh": "翻译失败",
      "authors": [
        "Ziquan Wei",
        "Tingting Dan",
        "Jiaqi Ding",
        "Guorong Wu"
      ],
      "abstract": "Although modern imaging technologies allow us to study connectivity between\ntwo distinct brain regions in-vivo, an in-depth understanding of how anatomical\nstructure supports brain function and how spontaneous functional fluctuations\nemerge remarkable cognition is still elusive. Meanwhile, tremendous efforts\nhave been made in the realm of machine learning to establish the nonlinear\nmapping between neuroimaging data and phenotypic traits. However, the absence\nof neuroscience insight in the current approaches poses significant challenges\nin understanding cognitive behavior from transient neural activities. To\naddress this challenge, we put the spotlight on the coupling mechanism of\nstructural connectivity (SC) and functional connectivity (FC) by formulating\nsuch network neuroscience question into an expressive graph representation\nlearning problem for high-order topology. Specifically, we introduce the\nconcept of topological detour to characterize how a ubiquitous instance of FC\n(direct link) is supported by neural pathways (detour) physically wired by SC,\nwhich forms a cyclic loop interacted by brain structure and function. In the\nclich\\'e of machine learning, the multi-hop detour pathway underlying SC-FC\ncoupling allows us to devise a novel multi-head self-attention mechanism within\nTransformer to capture multi-modal feature representation from paired graphs of\nSC and FC. Taken together, we propose a biological-inspired deep model, coined\nas NeuroPath, to find putative connectomic feature representations from the\nunprecedented amount of neuroimages, which can be plugged into various\ndownstream applications such as task recognition and disease diagnosis. We have\nevaluated NeuroPath on large-scale public datasets including HCP and UK Biobank\nunder supervised and zero-shot learning, where the state-of-the-art performance\nby our NeuroPath indicates great potential in network neuroscience.",
      "tldr_zh": "这篇论文针对脑连接组的结构连接 (SC) 和功能连接 (FC) 耦合机制，提出了一种新的图表示学习方法，以探索脑结构如何支持功能波动和认知行为。作者引入拓扑 detour 概念，描述 FC 的直接链接如何通过 SC 的多跳神经通路形成循环互动，并基于此设计了 NeuroPath 模型——一个生物启发的 Transformer 架构，利用多头自注意力机制捕捉 SC 和 FC 的多模态特征。实验结果显示，NeuroPath 在 HCP 和 UK Biobank 等大型数据集上，在监督和零样本学习任务中显著优于现有方法，提升了脑网络分析在任务识别和疾病诊断中的性能。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17510v3",
      "published_date": "2024-09-26 03:40:12 UTC",
      "updated_date": "2024-10-27 03:25:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:22:41.691115"
    },
    {
      "arxiv_id": "2409.17508v2",
      "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
      "title_zh": "翻译失败",
      "authors": [
        "Xun Zhu",
        "Ying Hu",
        "Fanbin Mo",
        "Miao Li",
        "Ji Wu"
      ],
      "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med.",
      "tldr_zh": "本文提出Uni-Med，一种统一的医疗通用基础模型，用于多任务学习，通过Connector-MoE (CMoE) 模块缓解多模态大语言模型(MLLMs)中多任务优化的tug-of-war问题。Uni-Med 由通用视觉特征提取模块、CMoE（包括路由器和混合投影专家）以及LLM组成，能够同时处理六种医疗任务，如问答、视觉问答、报告生成和图像分类。实验结果显示，引入CMoE模块在各种配置下平均提升8%性能，并在与现有最先进医疗MLLMs的比较中取得竞争性或优越的指标。该研究还从梯度优化和参数统计角度分析了tug-of-war问题，并提供了开源代码和资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17508v2",
      "published_date": "2024-09-26 03:33:26 UTC",
      "updated_date": "2024-11-01 02:38:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:22:54.136945"
    },
    {
      "arxiv_id": "2409.18737v2",
      "title": "MemFusionMap: Working Memory Fusion for Online Vectorized HD Map Construction",
      "title_zh": "MemFusionMap：工作记忆融合用于在线矢量化高清地图构建",
      "authors": [
        "Jingyu Song",
        "Xudong Chen",
        "Liupei Lu",
        "Jie Li",
        "Katherine A. Skinner"
      ],
      "abstract": "High-definition (HD) maps provide environmental information for autonomous\ndriving systems and are essential for safe planning. While existing methods\nwith single-frame input achieve impressive performance for online vectorized HD\nmap construction, they still struggle with complex scenarios and occlusions. We\npropose MemFusionMap, a novel temporal fusion model with enhanced temporal\nreasoning capabilities for online HD map construction. Specifically, we\ncontribute a working memory fusion module that improves the model's memory\ncapacity to reason across a history of frames. We also design a novel temporal\noverlap heatmap to explicitly inform the model about the temporal overlap\ninformation and vehicle trajectory in the Bird's Eye View space. By integrating\nthese two designs, MemFusionMap significantly outperforms existing methods\nwhile also maintaining a versatile design for scalability. We conduct extensive\nevaluation on open-source benchmarks and demonstrate a maximum improvement of\n5.4% in mAP over state-of-the-art methods. The project page for MemFusionMap is\nhttps://song-jingyu.github.io/MemFusionMap",
      "tldr_zh": "该研究提出MemFusionMap，一种新型时序融合模型，用于在线矢量化HD map构建，以解决现有单帧输入方法在复杂场景和遮挡下的局限性。具体而言，该模型引入了working memory fusion module来提升记忆容量，实现跨历史帧的推理，并设计了temporal overlap heatmap来显式提供Bird's Eye View空间中的时序重叠信息和车辆轨迹。实验结果显示，MemFusionMap在开源基准上比最先进方法mAP提高了最多5.4%，并展示了良好的可扩展性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.18737v2",
      "published_date": "2024-09-26 03:16:39 UTC",
      "updated_date": "2024-11-22 05:37:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:23:04.318669"
    },
    {
      "arxiv_id": "2409.17500v2",
      "title": "GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent",
      "title_zh": "翻译失败",
      "authors": [
        "Hongtai Zeng",
        "Chao Yang",
        "Yanzhen Zhou",
        "Cheng Yang",
        "Qinglai Guo"
      ],
      "abstract": "Ensuring that the outputs of neural networks satisfy specific constraints is\ncrucial for applying neural networks to real-life decision-making problems. In\nthis paper, we consider making a batch of neural network outputs satisfy\nbounded and general linear constraints. We first reformulate the neural network\noutput projection problem as an entropy-regularized linear programming problem.\nWe show that such a problem can be equivalently transformed into an\nunconstrained convex optimization problem with Lipschitz continuous gradient\naccording to the duality theorem. Then, based on an accelerated gradient\ndescent algorithm with numerical performance enhancement, we present our\narchitecture, GLinSAT, to solve the problem. To the best of our knowledge, this\nis the first general linear satisfiability layer in which all the operations\nare differentiable and matrix-factorization-free. Despite the fact that we can\nexplicitly perform backpropagation based on automatic differentiation\nmechanism, we also provide an alternative approach in GLinSAT to calculate the\nderivatives based on implicit differentiation of the optimality condition.\nExperimental results on constrained traveling salesman problems, partial graph\nmatching with outliers, predictive portfolio allocation and power system unit\ncommitment demonstrate the advantages of GLinSAT over existing satisfiability\nlayers. Our implementation is available at\n\\url{https://github.com/HunterTracer/GLinSAT}.",
      "tldr_zh": "本文提出GLinSAT，一种基于accelerated gradient descent的神经网络层，用于确保神经网络输出满足bounded和general linear constraints。通过将输出投影问题重构为熵正则化线性规划，并转化为无约束凸优化问题，GLinSAT实现了所有操作的全可微和无矩阵分解，支持自动微分或隐式微分计算。实验结果显示，在constrained traveling salesman problems、partial graph matching with outliers、predictive portfolio allocation和power system unit commitment等任务上，GLinSAT比现有可满足层表现出显著优势。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by 2024 Advances in Neural Information\n  Processing Systems. The reviews and comments can be found in\n  https://openreview.net/forum?id=m1PVjNHvtP",
      "pdf_url": "http://arxiv.org/pdf/2409.17500v2",
      "published_date": "2024-09-26 03:12:53 UTC",
      "updated_date": "2024-11-11 10:17:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:23:18.248117"
    },
    {
      "arxiv_id": "2409.17495v1",
      "title": "Human Mobility Modeling with Limited Information via Large Language Models",
      "title_zh": "基于大语言模型进行有限信息的人类移动建模",
      "authors": [
        "Yifan Liu",
        "Xishun Liao",
        "Haoxuan Ma",
        "Brian Yueshuai He",
        "Chris Stanford",
        "Jiaqi Ma"
      ],
      "abstract": "Understanding human mobility patterns has traditionally been a complex\nchallenge in transportation modeling. Due to the difficulties in obtaining\nhigh-quality training datasets across diverse locations, conventional\nactivity-based models and learning-based human mobility modeling algorithms are\nparticularly limited by the availability and quality of datasets. Furthermore,\ncurrent research mainly focuses on the spatial-temporal travel pattern but\nlacks an understanding of the semantic information between activities, which is\ncrucial for modeling the interdependence between activities. In this paper, we\npropose an innovative Large Language Model (LLM) empowered human mobility\nmodeling framework. Our proposed approach significantly reduces the reliance on\ndetailed human mobility statistical data, utilizing basic socio-demographic\ninformation of individuals to generate their daily mobility patterns. We have\nvalidated our results using the NHTS and SCAG-ABM datasets, demonstrating the\neffective modeling of mobility patterns and the strong adaptability of our\nframework across various geographic locations.",
      "tldr_zh": "该论文探讨了人类移动性建模的挑战，特别是数据集获取困难和对活动间语义信息的忽略。研究提出了一种基于 Large Language Models (LLM) 的创新框架，利用个体的基本社会人口统计信息生成日常移动模式，从而减少对详细统计数据的依赖。通过 NHTS 和 SCAG-ABM 数据集验证，该框架有效模拟了移动模式，并展示了在不同地理位置的强适应性。整体贡献在于提升了建模的灵活性和准确性，为资源有限的环境提供新途径。",
      "categories": [
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17495v1",
      "published_date": "2024-09-26 03:07:32 UTC",
      "updated_date": "2024-09-26 03:07:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:23:28.578201"
    },
    {
      "arxiv_id": "2409.17486v2",
      "title": "Global-Local Medical SAM Adaptor Based on Full Adaption",
      "title_zh": "基于完全适配的全局-局部医疗 SAM 适配器",
      "authors": [
        "Meng Wang",
        "Yarong Feng",
        "Yongwei Tang",
        "Tian Zhang",
        "Yuxin Liang",
        "Chao Lv"
      ],
      "abstract": "Emerging of visual language models, such as the segment anything model (SAM),\nhave made great breakthroughs in the field of universal semantic segmentation\nand significantly aid the improvements of medical image segmentation, in\nparticular with the help of Medical SAM adaptor (Med-SA). However, Med-SA still\ncan be improved, as it fine-tunes SAM in a partial adaption manner. To resolve\nthis problem, we present a novel global medical SAM adaptor (GMed-SA) with full\nadaption, which can adapt SAM globally. We further combine GMed-SA and Med-SA\nto propose a global-local medical SAM adaptor (GLMed-SA) to adapt SAM both\nglobally and locally. Extensive experiments have been performed on the\nchallenging public 2D melanoma segmentation dataset. The results show that\nGLMed-SA outperforms several state-of-the-art semantic segmentation methods on\nvarious evaluation metrics, demonstrating the superiority of our methods.",
      "tldr_zh": "本研究针对视觉语言模型如 Segment Anything Model (SAM) 在医疗图像分割中的应用，指出现有 Medical SAM Adaptor (Med-SA) 仅采用部分适应方式的问题。作者提出 Global Medical SAM Adaptor (GMed-SA) 以实现 SAM 的全局适应，并进一步开发 Global-Local Medical SAM Adaptor (GLMed-SA)，结合全局和本地适应来提升性能。在公开的 2D 黑色素瘤分割数据集上，GLMed-SA 在多种评估指标上超过了现有最先进方法，证明了其优越性。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17486v2",
      "published_date": "2024-09-26 02:48:15 UTC",
      "updated_date": "2024-10-29 06:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:23:40.594291"
    },
    {
      "arxiv_id": "2409.17481v2",
      "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
      "title_zh": "MaskLLM：针对大型语言",
      "authors": [
        "Gongfan Fang",
        "Hongxu Yin",
        "Saurav Muralidharan",
        "Greg Heinrich",
        "Jeff Pool",
        "Jan Kautz",
        "Pavlo Molchanov",
        "Xinchao Wang"
      ],
      "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter\ncounts, which typically result in significant redundancy. This work introduces\nMaskLLM, a learnable pruning method that establishes Semi-structured (or\n``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during\ninference. Instead of developing a new importance criterion, MaskLLM explicitly\nmodels N:M patterns as a learnable distribution through Gumbel Softmax\nsampling. This approach facilitates end-to-end training on large-scale datasets\nand offers two notable advantages: 1) High-quality Masks - our method\neffectively scales to large datasets and learns accurate masks; 2)\nTransferability - the probabilistic modeling of mask distribution enables the\ntransfer learning of sparsity across domains or tasks. We assessed MaskLLM\nusing 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,\nwith sizes ranging from 843M to 15B parameters, and our empirical results show\nsubstantial improvements over state-of-the-art methods. For instance, leading\napproaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to\nthe dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL\nsolely by learning the masks with frozen weights. Furthermore, MaskLLM's\nlearnable nature allows customized masks for lossless application of 2:4\nsparsity to downstream tasks or domains. Code is available at\nhttps://github.com/NVlabs/MaskLLM.",
      "tldr_zh": "这篇论文介绍了 MaskLLM，一种可学习的修剪方法，旨在为 Large Language Models (LLMs) 建立 Semi-structured (N:M) 稀疏性，以减少推理时的计算开销。MaskLLM 通过 Gumbel Softmax sampling 显式建模 N:M 模式，支持端到端训练在大型数据集上，并提供两大优势：高质量 Masks 的学习能力和稀疏性的 Transferability。实验在 LLaMA-2、Nemotron-4 和 GPT-3 等模型上（参数规模从 843M 到 15B）评估 2:4 稀疏性，结果显示 MaskLLM 的 perplexity (PPL) 显著改善，例如在 Wikitext 上仅为 6.72，而领先方法超过 10，对比密集模型的 5.12。该方法还允许自定义 Masks，实现无损应用到下游任务或领域。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2024 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2409.17481v2",
      "published_date": "2024-09-26 02:37:41 UTC",
      "updated_date": "2024-12-07 12:01:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:23:55.328723"
    },
    {
      "arxiv_id": "2409.17480v1",
      "title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Chuanhong Zhan",
        "Wei Xiang",
        "Chao Liang",
        "Bang Wang"
      ],
      "abstract": "Existing script event prediction task forcasts the subsequent event based on\nan event script chain. However, the evolution of historical events are more\ncomplicated in real world scenarios and the limited information provided by the\nevent script chain also make it difficult to accurately predict subsequent\nevents. This paper introduces a Causality Graph Event Prediction(CGEP) task\nthat forecasting consequential event based on an Event Causality Graph (ECG).\nWe propose a Semantic Enhanced Distance-sensitive Graph Prompt Learning\n(SeDGPL) Model for the CGEP task. In SeDGPL, (1) we design a Distance-sensitive\nGraph Linearization (DsGL) module to reformulate the ECG into a graph prompt\ntemplate as the input of a PLM; (2) propose an Event-Enriched Causality\nEncoding (EeCE) module to integrate both event contextual semantic and graph\nschema information; (3) propose a Semantic Contrast Event Prediction (ScEP)\nmodule to enhance the event representation among numerous candidate events and\npredict consequential event following prompt learning paradigm. %We construct\ntwo CGEP datasets based on existing MAVEN-ERE and ESC corpus for experiments.\nExperiment results validate our argument our proposed SeDGPL model outperforms\nthe advanced competitors for the CGEP task.",
      "tldr_zh": "本研究指出，现有的脚本事件预测任务基于事件脚本链，但无法处理现实中复杂的事件演化，因此引入了Causality Graph Event Prediction (CGEP)任务，利用Event Causality Graph (ECG)来预测后续事件。作者提出了Semantic Enhanced Distance-sensitive Graph Prompt Learning (SeDGPL)模型，包括Distance-sensitive Graph Linearization (DsGL)模块用于将ECG转化为图提示模板、Event-Enriched Causality Encoding (EeCE)模块整合事件语义和图结构信息，以及Semantic Contrast Event Prediction (ScEP)模块增强事件表示并预测后果。实验结果显示，SeDGPL在基于MAVEN-ERE和ESC语料库的两个数据集上，优于现有先进模型，验证了其有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17480v1",
      "published_date": "2024-09-26 02:34:08 UTC",
      "updated_date": "2024-09-26 02:34:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:24:15.383064"
    },
    {
      "arxiv_id": "2409.17472v1",
      "title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards",
      "title_zh": "翻译失败",
      "authors": [
        "Heejin Do",
        "Sangwon Ryu",
        "Gary Geunbae Lee"
      ],
      "abstract": "Recent advances in automated essay scoring (AES) have shifted towards\nevaluating multiple traits to provide enriched feedback. Like typical AES\nsystems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure\nagreement with human raters, aligning closely with the rating schema; however,\nits non-differentiable nature prevents its direct use in neural network\ntraining. In this paper, we propose Scoring-aware Multi-reward Reinforcement\nLearning (SaMRL), which integrates actual evaluation schemes into the training\nprocess by designing QWK-based rewards with a mean-squared error penalty for\nmulti-trait AES. Existing reinforcement learning (RL) applications in AES are\nlimited to classification models despite associated performance degradation, as\nRL requires probability distributions; instead, we adopt an autoregressive\nscore generation framework to leverage token generation probabilities for\nrobust multi-trait score predictions. Empirical analyses demonstrate that SaMRL\nfacilitates model training, notably enhancing scoring of previously inferior\nprompts.",
      "tldr_zh": "本研究提出Scoring-aware Multi-reward Reinforcement Learning (SaMRL)，一种用于多特质自动作文评分(AES)的强化学习框架，通过设计基于二次加权Kappa (QWK)的奖励并结合均方误差惩罚，将实际评估方案融入神经网络训练中。不同于现有RL方法仅限于分类模型，SaMRL采用autoregressive自回归分数生成框架，利用token生成概率实现鲁棒的多特质分数预测。实验结果显示，该方法显著提升了模型训练效果，特别是改善了对先前表现较差提示的分数评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17472v1",
      "published_date": "2024-09-26 02:16:48 UTC",
      "updated_date": "2024-09-26 02:16:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:24:16.476180"
    },
    {
      "arxiv_id": "2409.17466v1",
      "title": "Adjusting Regression Models for Conditional Uncertainty Calibration",
      "title_zh": "翻译失败",
      "authors": [
        "Ruijiang Gao",
        "Mingzhang Yin",
        "James McInerney",
        "Nathan Kallus"
      ],
      "abstract": "Conformal Prediction methods have finite-sample distribution-free marginal\ncoverage guarantees. However, they generally do not offer conditional coverage\nguarantees, which can be important for high-stakes decisions. In this paper, we\npropose a novel algorithm to train a regression function to improve the\nconditional coverage after applying the split conformal prediction procedure.\nWe establish an upper bound for the miscoverage gap between the conditional\ncoverage and the nominal coverage rate and propose an end-to-end algorithm to\ncontrol this upper bound. We demonstrate the efficacy of our method empirically\non synthetic and real-world datasets.",
      "tldr_zh": "本论文针对 Conformal Prediction 方法的有限样本边缘覆盖保证，但缺乏条件覆盖的问题，提出了一种新算法，用于训练回归函数以改善 split conformal prediction 后的条件覆盖。算法通过建立条件覆盖与名义覆盖率之间错误覆盖差距的上界，并设计端到端的控制机制，来增强模型在高风险决策中的可靠性。在合成和真实数据集上的实验验证了该方法的有效性，展示了其实际应用潜力。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Machine Learning Special Issue on Uncertainty Quantification",
      "pdf_url": "http://arxiv.org/pdf/2409.17466v1",
      "published_date": "2024-09-26 01:55:45 UTC",
      "updated_date": "2024-09-26 01:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:24:28.360735"
    },
    {
      "arxiv_id": "2409.17457v1",
      "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches",
      "title_zh": "翻译失败",
      "authors": [
        "Sifan Wu",
        "Amir Khasahmadi",
        "Mor Katz",
        "Pradeep Kumar Jayaraman",
        "Yewen Pu",
        "Karl Willis",
        "Bang Liu"
      ],
      "abstract": "Parametric Computer-Aided Design (CAD) is central to contemporary mechanical\ndesign. However, it encounters challenges in achieving precise parametric\nsketch modeling and lacks practical evaluation metrics suitable for mechanical\ndesign. We harness the capabilities of pre-trained foundation models, renowned\nfor their successes in natural language processing and computer vision, to\ndevelop generative models specifically for CAD. These models are adept at\nunderstanding complex geometries and design reasoning, a crucial advancement in\nCAD technology. In this paper, we propose CadVLM, an end-to-end vision language\nmodel for CAD generation. Our approach involves adapting pre-trained foundation\nmodels to manipulate engineering sketches effectively, integrating both sketch\nprimitive sequences and sketch images. Extensive experiments demonstrate\nsuperior performance on multiple CAD sketch generation tasks such as CAD\nautocompletion, CAD autoconstraint, and image conditional generation. To our\nknowledge, this is the first instance of a multimodal Large Language Model\n(LLM) being successfully applied to parametric CAD generation, representing a\npioneering step in the field of computer-aided mechanical design.",
      "tldr_zh": "这篇论文提出CadVLM，一种端到端的视觉语言模型，用于参数CAD草图的生成，旨在解决传统CAD建模的精确性和评价指标不足问题。CadVLM通过适应预训练的基础模型，整合草图基元序列和图像，实现对复杂几何和设计推理的有效处理。实验结果显示，该模型在CAD autocompletion、CAD autoconstraint和图像条件生成任务上表现出色，这是首个将多模态Large Language Model (LLM)成功应用于参数CAD领域的开创性工作。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17457v1",
      "published_date": "2024-09-26 01:22:29 UTC",
      "updated_date": "2024-09-26 01:22:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:24:40.854689"
    },
    {
      "arxiv_id": "2409.17440v1",
      "title": "A Time Series is Worth Five Experts: Heterogeneous Mixture of Experts for Traffic Flow Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Guangyu Wang",
        "Yujie Chen",
        "Ming Gao",
        "Zhiqiao Wu",
        "Jiafu Tang",
        "Jiabi Zhao"
      ],
      "abstract": "Accurate traffic prediction faces significant challenges, necessitating a\ndeep understanding of both temporal and spatial cues and their complex\ninteractions across multiple variables. Recent advancements in traffic\nprediction systems are primarily due to the development of complex\nsequence-centric models. However, existing approaches often embed multiple\nvariables and spatial relationships at each time step, which may hinder\neffective variable-centric learning, ultimately leading to performance\ndegradation in traditional traffic prediction tasks. To overcome these\nlimitations, we introduce variable-centric and prior knowledge-centric modeling\ntechniques. Specifically, we propose a Heterogeneous Mixture of Experts (TITAN)\nmodel for traffic flow prediction. TITAN initially consists of three experts\nfocused on sequence-centric modeling. Then, designed a low-rank adaptive\nmethod, TITAN simultaneously enables variable-centric modeling. Furthermore, we\nsupervise the gating process using a prior knowledge-centric modeling strategy\nto ensure accurate routing. Experiments on two public traffic network datasets,\nMETR-LA and PEMS-BAY, demonstrate that TITAN effectively captures\nvariable-centric dependencies while ensuring accurate routing. Consequently, it\nachieves improvements in all evaluation metrics, ranging from approximately\n4.37\\% to 11.53\\%, compared to previous state-of-the-art (SOTA) models. The\ncode is open at\n\\href{https://github.com/sqlcow/TITAN}{https://github.com/sqlcow/TITAN}.",
      "tldr_zh": "这篇论文针对交通流量预测的挑战，提出了一种 Heterogeneous Mixture of Experts (TITAN) 模型，通过变量中心和先验知识中心建模来处理时间序列中多变量和空间交互的复杂性。TITAN 包括三个专注于序列中心建模的专家，并采用低秩自适应方法实现变量中心学习，同时通过先验知识监督的门控策略确保准确路由。在 METR-LA 和 PEMS-BAY 数据集上的实验显示，TITAN 比现有最先进模型在所有评估指标上提升了 4.37% 到 11.53%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.17440v1",
      "published_date": "2024-09-26 00:26:47 UTC",
      "updated_date": "2024-09-26 00:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T04:24:53.176752"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 146,
  "processed_papers_count": 146,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T04:25:16.843453"
}
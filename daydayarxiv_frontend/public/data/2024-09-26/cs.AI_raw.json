[
  {
    "arxiv_id": "2409.18345v1",
    "title": "A Generalized LLM-Augmented BIM Framework: Application to a Speech-to-BIM system",
    "authors": [
      "Ghang Lee",
      "Suhyung Jang",
      "Seokho Hyun"
    ],
    "abstract": "Performing building information modeling (BIM) tasks is a complex process\nthat imposes a steep learning curve and a heavy cognitive load due to the\nnecessity of remembering sequences of numerous commands. With the rapid\nadvancement of large language models (LLMs), it is foreseeable that BIM tasks,\nincluding querying and managing BIM data, 4D and 5D BIM, design compliance\nchecking, or authoring a design, using written or spoken natural language\n(i.e., text-to-BIM or speech-to-BIM), will soon supplant traditional graphical\nuser interfaces. This paper proposes a generalized LLM-augmented BIM framework\nto expedite the development of LLM-enhanced BIM applications by providing a\nstep-by-step development process. The proposed framework consists of six steps:\ninterpret-fill-match-structure-execute-check. The paper demonstrates the\napplicability of the proposed framework through implementing a speech-to-BIM\napplication, NADIA-S (Natural-language-based Architectural Detailing through\nInteraction with Artificial Intelligence via Speech), using exterior wall\ndetailing as an example.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "In Proceedings of the 41st International Conference of CIB W78.\n  Marrakech, Morocco, 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18345v1",
    "published_date": "2024-09-26 23:46:15 UTC",
    "updated_date": "2024-09-26 23:46:15 UTC"
  },
  {
    "arxiv_id": "2409.18343v1",
    "title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving",
    "authors": [
      "Zhenghao Peng",
      "Wenjie Luo",
      "Yiren Lu",
      "Tianyi Shen",
      "Cole Gulino",
      "Ari Seff",
      "Justin Fu"
    ],
    "abstract": "A major challenge in autonomous vehicle research is modeling agent behaviors,\nwhich has critical applications including constructing realistic and reliable\nsimulations for off-board evaluation and forecasting traffic agents motion for\nonboard planning. While supervised learning has shown success in modeling\nagents across various domains, these models can suffer from distribution shift\nwhen deployed at test-time. In this work, we improve the reliability of agent\nbehaviors by closed-loop fine-tuning of behavior models with reinforcement\nlearning. Our method demonstrates improved overall performance, as well as\nimproved targeted metrics such as collision rate, on the Waymo Open Sim Agents\nchallenge. Additionally, we present a novel policy evaluation benchmark to\ndirectly assess the ability of simulated agents to measure the quality of\nautonomous vehicle planners and demonstrate the effectiveness of our approach\non this new benchmark.",
    "categories": [
      "cs.AI",
      "I.2.6; I.2.9"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18343v1",
    "published_date": "2024-09-26 23:40:33 UTC",
    "updated_date": "2024-09-26 23:40:33 UTC"
  },
  {
    "arxiv_id": "2409.18340v1",
    "title": "DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image Segmentation via Disentangled Representation Learning",
    "authors": [
      "Hui Lin",
      "Florian Schiffers",
      "Santiago LÃ³pez-Tapia",
      "Neda Tavakoli",
      "Daniel Kim",
      "Aggelos K. Katsaggelos"
    ],
    "abstract": "Unsupervised domain adaptation (UDA) is essential for medical image\nsegmentation, especially in cross-modality data scenarios. UDA aims to transfer\nknowledge from a labeled source domain to an unlabeled target domain, thereby\nreducing the dependency on extensive manual annotations. This paper presents\nDRL-STNet, a novel framework for cross-modality medical image segmentation that\nleverages generative adversarial networks (GANs), disentangled representation\nlearning (DRL), and self-training (ST). Our method leverages DRL within a GAN\nto translate images from the source to the target modality. Then, the\nsegmentation model is initially trained with these translated images and\ncorresponding source labels and then fine-tuned iteratively using a combination\nof synthetic and real images with pseudo-labels and real labels. The proposed\nframework exhibits superior performance in abdominal organ segmentation on the\nFLARE challenge dataset, surpassing state-of-the-art methods by 11.4% in the\nDice similarity coefficient and by 13.1% in the Normalized Surface Dice metric,\nachieving scores of 74.21% and 80.69%, respectively. The average running time\nis 41 seconds, and the area under the GPU memory-time curve is 11,292 MB. These\nresults indicate the potential of DRL-STNet for enhancing cross-modality\nmedical image segmentation tasks.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "MICCAI 2024 Challenge, FLARE Challenge, Unsupervised domain\n  adaptation, Organ segmentation, Feature disentanglement, Self-training",
    "pdf_url": "http://arxiv.org/pdf/2409.18340v1",
    "published_date": "2024-09-26 23:30:40 UTC",
    "updated_date": "2024-09-26 23:30:40 UTC"
  },
  {
    "arxiv_id": "2409.18339v2",
    "title": "AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models",
    "authors": [
      "Xin Hong",
      "Yuan Gong",
      "Vidhyasaharan Sethu",
      "Ting Dang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated great\nsuccess in many Natural Language Processing (NLP) tasks. In addition to their\ncognitive intelligence, exploring their capabilities in emotional intelligence\nis also crucial, as it enables more natural and empathetic conversational AI.\nRecent studies have shown LLMs' capability in recognizing emotions, but they\noften focus on single emotion labels and overlook the complex and ambiguous\nnature of human emotions. This study is the first to address this gap by\nexploring the potential of LLMs in recognizing ambiguous emotions, leveraging\ntheir strong generalization capabilities and in-context learning. We design\nzero-shot and few-shot prompting and incorporate past dialogue as context\ninformation for ambiguous emotion recognition. Experiments conducted using\nthree datasets indicate significant potential for LLMs in recognizing ambiguous\nemotions, and highlight the substantial benefits of including context\ninformation. Furthermore, our findings indicate that LLMs demonstrate a high\ndegree of effectiveness in recognizing less ambiguous emotions and exhibit\npotential for identifying more ambiguous emotions, paralleling human perceptual\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.18339v2",
    "published_date": "2024-09-26 23:25:21 UTC",
    "updated_date": "2025-02-17 05:28:55 UTC"
  },
  {
    "arxiv_id": "2409.18335v1",
    "title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies",
    "authors": [
      "Ryan Shea",
      "Zhou Yu"
    ],
    "abstract": "Despite recent advancements in AI and NLP, negotiation remains a difficult\ndomain for AI agents. Traditional game theoretic approaches that have worked\nwell for two-player zero-sum games struggle in the context of negotiation due\nto their inability to learn human-compatible strategies. On the other hand,\napproaches that only use human data tend to be domain-specific and lack the\ntheoretical guarantees provided by strategies grounded in game theory.\nMotivated by the notion of fairness as a criterion for optimality in general\nsum games, we propose a negotiation framework called FDHC which incorporates\nfairness into both the reward design and search to learn human-compatible\nnegotiation strategies. Our method includes a novel, RL+search technique called\nLGM-Zero which leverages a pre-trained language model to retrieve\nhuman-compatible offers from large action spaces. Our results show that our\nmethod is able to achieve more egalitarian negotiation outcomes and improve\nnegotiation quality.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18335v1",
    "published_date": "2024-09-26 23:16:47 UTC",
    "updated_date": "2024-09-26 23:16:47 UTC"
  },
  {
    "arxiv_id": "2409.18324v1",
    "title": "Input-Dependent Power Usage in GPUs",
    "authors": [
      "Theo Gregersen",
      "Pratyush Patel",
      "Esha Choukse"
    ],
    "abstract": "GPUs are known to be power-hungry, and due to the boom in artificial\nintelligence, they are currently the major contributors to the high power\ndemands of upcoming datacenters. Most GPU usage in these popular workloads\nconsist of large general matrix-matrix multiplications (GEMMs), which have\ntherefore been optimized to achieve high utilization of hardware resources. In\nthis work, we show that modifying the input data to GEMMs, while maintaining\nthe matrix shapes and sizes can notably change the power consumption of these\nkernels. We experiment with four kinds of input variations: value distribution,\nbit similarity, placement, and sparsity, across different data types. Our\nfindings indicate that these variations can change the GPU power usage during\nGEMM by almost 40%. We hypothesize that input-dependent power usage variations\noccur due to changes in the number of bit flips in the GPUs. We propose\nleveraging this property through compiler and scheduler optimizations to manage\npower and reduce energy consumption.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18324v1",
    "published_date": "2024-09-26 22:31:09 UTC",
    "updated_date": "2024-09-26 22:31:09 UTC"
  },
  {
    "arxiv_id": "2409.18319v2",
    "title": "Development and Validation of a Dynamic-Template-Constrained Large Language Model for Generating Fully-Structured Radiology Reports",
    "authors": [
      "Chuang Niu",
      "Parisa Kaviani",
      "Qing Lyu",
      "Mannudeep K. Kalra",
      "Christopher T. Whitlow",
      "Ge Wang"
    ],
    "abstract": "Current LLMs for creating fully-structured reports face the challenges of\nformatting errors, content hallucinations, and privacy leakage issues when\nuploading data to external servers.We aim to develop an open-source, accurate\nLLM for creating fully-structured and standardized LCS reports from varying\nfree-text reports across institutions and demonstrate its utility in automatic\nstatistical analysis and individual lung nodule retrieval. With IRB approvals,\nour retrospective study included 5,442 de-identified LDCT LCS radiology reports\nfrom two institutions. We constructed two evaluation datasets by labeling 500\npairs of free-text and fully-structured radiology reports and one large-scale\nconsecutive dataset from January 2021 to December 2023. Two radiologists\ncreated a standardized template for recording 27 lung nodule features on LCS.\nWe designed a dynamic-template-constrained decoding method to enhance existing\nLLMs for creating fully-structured reports from free-text radiology reports.\nUsing consecutive structured reports, we automated descriptive statistical\nanalyses and a nodule retrieval prototype. Our best LLM for creating\nfully-structured reports achieved high performance on cross-institutional\ndatasets with an F1 score of about 97%, with neither formatting errors nor\ncontent hallucinations. Our method consistently improved the best open-source\nLLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically\nderived statistical distributions were consistent with prior findings regarding\nattenuation, location, size, stability, and Lung-RADS. The retrieval system\nwith structured reports allowed flexible nodule-level search and complex\nstatistical analysis. Our developed software is publicly available for local\ndeployment and further research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18319v2",
    "published_date": "2024-09-26 21:59:11 UTC",
    "updated_date": "2024-10-25 03:17:24 UTC"
  },
  {
    "arxiv_id": "2409.18313v5",
    "title": "Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation",
    "authors": [
      "Quanting Xie",
      "So Yeon Min",
      "Pengliang Ji",
      "Yue Yang",
      "Tianyi Zhang",
      "Kedi Xu",
      "Aarav Bajaj",
      "Ruslan Salakhutdinov",
      "Matthew Johnson-Roberson",
      "Yonatan Bisk"
    ],
    "abstract": "There is no limit to how much a robot might explore and learn, but all of\nthat knowledge needs to be searchable and actionable. Within language research,\nretrieval augmented generation (RAG) has become the workhorse of large-scale\nnon-parametric knowledge; however, existing techniques do not directly transfer\nto the embodied domain, which is multimodal, where data is highly correlated,\nand perception requires abstraction. To address these challenges, we introduce\nEmbodied-RAG, a framework that enhances the foundational model of an embodied\nagent with a non-parametric memory system capable of autonomously constructing\nhierarchical knowledge for both navigation and language generation.\nEmbodied-RAG handles a full range of spatial and semantic resolutions across\ndiverse environments and query types, whether for a specific object or a\nholistic description of ambiance. At its core, Embodied-RAG's memory is\nstructured as a semantic forest, storing language descriptions at varying\nlevels of detail. This hierarchical organization allows the system to\nefficiently generate context-sensitive outputs across different robotic\nplatforms. We demonstrate that Embodied-RAG effectively bridges RAG to the\nrobotics domain, successfully handling over 250 explanation and navigation\nqueries across kilometer-level environments, highlighting its promise as a\ngeneral-purpose non-parametric system for embodied agents.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Web: https://quanting-xie.github.io/Embodied-RAG-web/",
    "pdf_url": "http://arxiv.org/pdf/2409.18313v5",
    "published_date": "2024-09-26 21:44:11 UTC",
    "updated_date": "2025-01-21 02:38:32 UTC"
  },
  {
    "arxiv_id": "2409.18301v3",
    "title": "Wavelet-Driven Generalizable Framework for Deepfake Face Forgery Detection",
    "authors": [
      "Lalith Bharadwaj Baru",
      "Rohit Boddeda",
      "Shilhora Akshay Patel",
      "Sai Mohan Gajapaka"
    ],
    "abstract": "The evolution of digital image manipulation, particularly with the\nadvancement of deep generative models, significantly challenges existing\ndeepfake detection methods, especially when the origin of the deepfake is\nobscure. To tackle the increasing complexity of these forgeries, we propose\n\\textbf{Wavelet-CLIP}, a deepfake detection framework that integrates wavelet\ntransforms with features derived from the ViT-L/14 architecture, pre-trained in\nthe CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze\nboth spatial and frequency features from images, thus enhancing the model's\ncapability to detect sophisticated deepfakes. To verify the effectiveness of\nour approach, we conducted extensive evaluations against existing\nstate-of-the-art methods for cross-dataset generalization and detection of\nunseen images generated by standard diffusion models. Our method showcases\noutstanding performance, achieving an average AUC of 0.749 for cross-data\ngeneralization and 0.893 for robustness against unseen deepfakes, outperforming\nall compared methods. The code can be reproduced from the repo:\n\\url{https://github.com/lalithbharadwajbaru/Wavelet-CLIP}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "9 Pages, 2 Figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2409.18301v3",
    "published_date": "2024-09-26 21:16:51 UTC",
    "updated_date": "2025-01-07 12:44:48 UTC"
  },
  {
    "arxiv_id": "2409.18300v1",
    "title": "SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining",
    "authors": [
      "Ruiqi Xian",
      "Xiyang Wu",
      "Tianrui Guan",
      "Xijun Wang",
      "Boqing Gong",
      "Dinesh Manocha"
    ],
    "abstract": "We introduce SOAR, a novel Self-supervised pretraining algorithm for aerial\nfootage captured by Unmanned Aerial Vehicles (UAVs). We incorporate human\nobject knowledge throughout the pretraining process to enhance UAV video\npretraining efficiency and downstream action recognition performance. This is\nin contrast to prior works that primarily incorporate object information during\nthe fine-tuning stage. Specifically, we first propose a novel object-aware\nmasking strategy designed to retain the visibility of certain patches related\nto objects throughout the pretraining phase. Second, we introduce an\nobject-aware loss function that utilizes object information to adjust the\nreconstruction loss, preventing bias towards less informative background\npatches. In practice, SOAR with a vanilla ViT backbone, outperforms best UAV\naction recognition models, recording a 9.7% and 21.4% boost in top-1 accuracy\non the NEC-Drone and UAV-Human datasets, while delivering an inference speed of\n18.7ms per video, making it 2x to 5x faster. Additionally, SOAR obtains\ncomparable accuracy to prior self-supervised learning (SSL) methods while\nrequiring 87.5% less pretraining time and 25% less memory usage",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18300v1",
    "published_date": "2024-09-26 21:15:22 UTC",
    "updated_date": "2024-09-26 21:15:22 UTC"
  },
  {
    "arxiv_id": "2409.18297v1",
    "title": "Flat'n'Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation",
    "authors": [
      "Lipeng Zhuang",
      "Shiyu Fan",
      "Yingdong Ru",
      "Florent Audonnet",
      "Paul Henderson",
      "Gerardo Aragon-Camarasa"
    ],
    "abstract": "We present Flat'n'Fold, a novel large-scale dataset for garment manipulation\nthat addresses critical gaps in existing datasets. Comprising 1,212 human and\n887 robot demonstrations of flattening and folding 44 unique garments across 8\ncategories, Flat'n'Fold surpasses prior datasets in size, scope, and diversity.\nOur dataset uniquely captures the entire manipulation process from crumpled to\nfolded states, providing synchronized multi-view RGB-D images, point clouds,\nand action data, including hand or gripper positions and rotations. We quantify\nthe dataset's diversity and complexity compared to existing benchmarks and show\nthat our dataset features natural and diverse manipulations of real-world\ndemonstrations of human and robot demonstrations in terms of visual and action\ninformation. To showcase Flat'n'Fold's utility, we establish new benchmarks for\ngrasping point prediction and subtask decomposition. Our evaluation of\nstate-of-the-art models on these tasks reveals significant room for\nimprovement. This underscores Flat'n'Fold's potential to drive advances in\nrobotic perception and manipulation of deformable objects. Our dataset can be\ndownloaded at https://cvas-ug.github.io/flat-n-fold",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18297v1",
    "published_date": "2024-09-26 21:10:17 UTC",
    "updated_date": "2024-09-26 21:10:17 UTC"
  },
  {
    "arxiv_id": "2409.18295v1",
    "title": "Enhancing Lossy Compression Through Cross-Field Information for Scientific Applications",
    "authors": [
      "Youyuan Liu",
      "Wenqi Jia",
      "Taolue Yang",
      "Miao Yin",
      "Sian Jin"
    ],
    "abstract": "Lossy compression is one of the most effective methods for reducing the size\nof scientific data containing multiple data fields. It reduces information\ndensity through prediction or transformation techniques to compress the data.\nPrevious approaches use local information from a single target field when\npredicting target data points, limiting their potential to achieve higher\ncompression ratios. In this paper, we identified significant cross-field\ncorrelations within scientific datasets. We propose a novel hybrid prediction\nmodel that utilizes CNN to extract cross-field information and combine it with\nexisting local field information. Our solution enhances the prediction accuracy\nof lossy compressors, leading to improved compression ratios without\ncompromising data quality. We evaluate our solution on three scientific\ndatasets, demonstrating its ability to improve compression ratios by up to 25%\nunder specific error bounds. Additionally, our solution preserves more data\ndetails and reduces artifacts compared to baseline approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 9 figures, accepted by DRBSD-10",
    "pdf_url": "http://arxiv.org/pdf/2409.18295v1",
    "published_date": "2024-09-26 21:06:53 UTC",
    "updated_date": "2024-09-26 21:06:53 UTC"
  },
  {
    "arxiv_id": "2409.18290v1",
    "title": "Retrospective Comparative Analysis of Prostate Cancer In-Basket Messages: Responses from Closed-Domain LLM vs. Clinical Teams",
    "authors": [
      "Yuexing Hao",
      "Jason M. Holmes",
      "Jared Hobson",
      "Alexandra Bennett",
      "Daniel K. Ebner",
      "David M. Routman",
      "Satomi Shiraishi",
      "Samir H. Patel",
      "Nathan Y. Yu",
      "Chris L. Hallemeier",
      "Brooke E. Ball",
      "Mark R. Waddle",
      "Wei Liu"
    ],
    "abstract": "In-basket message interactions play a crucial role in physician-patient\ncommunication, occurring during all phases (pre-, during, and post) of a\npatient's care journey. However, responding to these patients' inquiries has\nbecome a significant burden on healthcare workflows, consuming considerable\ntime for clinical care teams. To address this, we introduce RadOnc-GPT, a\nspecialized Large Language Model (LLM) powered by GPT-4 that has been designed\nwith a focus on radiotherapeutic treatment of prostate cancer with advanced\nprompt engineering, and specifically designed to assist in generating\nresponses. We integrated RadOnc-GPT with patient electronic health records\n(EHR) from both the hospital-wide EHR database and an internal,\nradiation-oncology-specific database. RadOnc-GPT was evaluated on 158\npreviously recorded in-basket message interactions. Quantitative natural\nlanguage processing (NLP) analysis and two grading studies with clinicians and\nnurses were used to assess RadOnc-GPT's responses. Our findings indicate that\nRadOnc-GPT slightly outperformed the clinical care team in \"Clarity\" and\n\"Empathy,\" while achieving comparable scores in \"Completeness\" and\n\"Correctness.\" RadOnc-GPT is estimated to save 5.2 minutes per message for\nnurses and 2.4 minutes for clinicians, from reading the inquiry to sending the\nresponse. Employing RadOnc-GPT for in-basket message draft generation has the\npotential to alleviate the workload of clinical care teams and reduce\nhealthcare costs by producing high-quality, timely responses.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18290v1",
    "published_date": "2024-09-26 21:00:51 UTC",
    "updated_date": "2024-09-26 21:00:51 UTC"
  },
  {
    "arxiv_id": "2409.18289v1",
    "title": "Criticality and Safety Margins for Reinforcement Learning",
    "authors": [
      "Alexander Grushin",
      "Walt Woods",
      "Alvaro Velasquez",
      "Simon Khan"
    ],
    "abstract": "State of the art reinforcement learning methods sometimes encounter unsafe\nsituations. Identifying when these situations occur is of interest both for\npost-hoc analysis and during deployment, where it might be advantageous to call\nout to a human overseer for help. Efforts to gauge the criticality of different\npoints in time have been developed, but their accuracy is not well established\ndue to a lack of ground truth, and they are not designed to be easily\ninterpretable by end users. Therefore, we seek to define a criticality\nframework with both a quantifiable ground truth and a clear significance to\nusers. We introduce true criticality as the expected drop in reward when an\nagent deviates from its policy for n consecutive random actions. We also\nintroduce the concept of proxy criticality, a low-overhead metric that has a\nstatistically monotonic relationship to true criticality. Safety margins make\nthese interpretable, when defined as the number of random actions for which\nperformance loss will not exceed some tolerance with high confidence. We\ndemonstrate this approach in several environment-agent combinations; for an A3C\nagent in an Atari Beamrider environment, the lowest 5% of safety margins\ncontain 47% of agent losses; i.e., supervising only 5% of decisions could\npotentially prevent roughly half of an agent's errors. This criticality\nframework measures the potential impacts of bad decisions, even before those\ndecisions are made, allowing for more effective debugging and oversight of\nautonomous agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 10 figures. This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2409.18289v1",
    "published_date": "2024-09-26 21:00:45 UTC",
    "updated_date": "2024-09-26 21:00:45 UTC"
  },
  {
    "arxiv_id": "2409.18286v1",
    "title": "Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing",
    "authors": [
      "Huthaifa I. Ashqar",
      "Ahmed Jaber",
      "Taqwa I. Alhadidi",
      "Mohammed Elhenawy"
    ],
    "abstract": "This study aims to comprehensively review and empirically evaluate the\napplication of multimodal large language models (MLLMs) and Large Vision Models\n(VLMs) in object detection for transportation systems. In the first fold, we\nprovide a background about the potential benefits of MLLMs in transportation\napplications and conduct a comprehensive review of current MLLM technologies in\nprevious studies. We highlight their effectiveness and limitations in object\ndetection within various transportation scenarios. The second fold involves\nproviding an overview of the taxonomy of end-to-end object detection in\ntransportation applications and future directions. Building on this, we\nproposed empirical analysis for testing MLLMs on three real-world\ntransportation problems that include object detection tasks namely, road safety\nattributes extraction, safety-critical event detection, and visual reasoning of\nthermal images. Our findings provide a detailed assessment of MLLM performance,\nuncovering both strengths and areas for improvement. Finally, we discuss\npractical limitations and challenges of MLLMs in enhancing object detection in\ntransportation, thereby offering a roadmap for future research and development\nin this critical area.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18286v1",
    "published_date": "2024-09-26 20:58:11 UTC",
    "updated_date": "2024-09-26 20:58:11 UTC"
  },
  {
    "arxiv_id": "2409.18261v3",
    "title": "Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object Pose Estimation",
    "authors": [
      "Mengchen Zhang",
      "Tong Wu",
      "Tai Wang",
      "Tengfei Wang",
      "Ziwei Liu",
      "Dahua Lin"
    ],
    "abstract": "6D object pose estimation aims at determining an object's translation,\nrotation, and scale, typically from a single RGBD image. Recent advancements\nhave expanded this estimation from instance-level to category-level, allowing\nmodels to generalize across unseen instances within the same category. However,\nthis generalization is limited by the narrow range of categories covered by\nexisting datasets, such as NOCS, which also tend to overlook common real-world\nchallenges like occlusion. To tackle these challenges, we introduce Omni6D, a\ncomprehensive RGBD dataset featuring a wide range of categories and varied\nbackgrounds, elevating the task to a more realistic context. 1) The dataset\ncomprises an extensive spectrum of 166 categories, 4688 instances adjusted to\nthe canonical pose, and over 0.8 million captures, significantly broadening the\nscope for evaluation. 2) We introduce a symmetry-aware metric and conduct\nsystematic benchmarks of existing algorithms on Omni6D, offering a thorough\nexploration of new challenges and insights. 3) Additionally, we propose an\neffective fine-tuning approach that adapts models from previous datasets to our\nextensive vocabulary setting. We believe this initiative will pave the way for\nnew insights and substantial progress in both the industrial and academic\nfields, pushing forward the boundaries of general 6D pose estimation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024 (poster). Github page: https://github.com/3DTopia/Omni6D",
    "pdf_url": "http://arxiv.org/pdf/2409.18261v3",
    "published_date": "2024-09-26 20:13:33 UTC",
    "updated_date": "2025-03-21 04:47:17 UTC"
  },
  {
    "arxiv_id": "2409.18260v2",
    "title": "PCEvE: Part Contribution Evaluation Based Model Explanation for Human Figure Drawing Assessment and Beyond",
    "authors": [
      "Jongseo Lee",
      "Geo Ahn",
      "Seong Tae Kim",
      "Jinwoo Choi"
    ],
    "abstract": "For automatic human figure drawing (HFD) assessment tasks, such as diagnosing\nautism spectrum disorder (ASD) using HFD images, the clarity and explainability\nof a model decision are crucial. Existing pixel-level attribution-based\nexplainable AI (XAI) approaches demand considerable effort from users to\ninterpret the semantic information of a region in an image, which can be often\ntime-consuming and impractical. To overcome this challenge, we propose a part\ncontribution evaluation based model explanation (PCEvE) framework. On top of\nthe part detection, we measure the Shapley Value of each individual part to\nevaluate the contribution to a model decision. Unlike existing\nattribution-based XAI approaches, the PCEvE provides a straightforward\nexplanation of a model decision, i.e., a part contribution histogram.\nFurthermore, the PCEvE expands the scope of explanations beyond the\nconventional sample-level to include class-level and task-level insights,\noffering a richer, more comprehensive understanding of model behavior. We\nrigorously validate the PCEvE via extensive experiments on multiple HFD\nassessment datasets. Also, we sanity-check the proposed method with a set of\ncontrolled experiments. Additionally, we demonstrate the versatility and\napplicability of our method to other domains by applying it to a\nphoto-realistic dataset, the Stanford Cars.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This papaer is under review",
    "pdf_url": "http://arxiv.org/pdf/2409.18260v2",
    "published_date": "2024-09-26 20:13:03 UTC",
    "updated_date": "2024-10-03 22:57:24 UTC"
  },
  {
    "arxiv_id": "2409.18222v1",
    "title": "Trustworthy AI: Securing Sensitive Data in Large Language Models",
    "authors": [
      "Georgios Feretzakis",
      "Vassilios S. Verykios"
    ],
    "abstract": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) by enabling robust text generation and understanding. However, their\ndeployment in sensitive domains like healthcare, finance, and legal services\nraises critical concerns about privacy and data security. This paper proposes a\ncomprehensive framework for embedding trust mechanisms into LLMs to dynamically\ncontrol the disclosure of sensitive information. The framework integrates three\ncore components: User Trust Profiling, Information Sensitivity Detection, and\nAdaptive Output Control. By leveraging techniques such as Role-Based Access\nControl (RBAC), Attribute-Based Access Control (ABAC), Named Entity Recognition\n(NER), contextual analysis, and privacy-preserving methods like differential\nprivacy, the system ensures that sensitive information is disclosed\nappropriately based on the user's trust level. By focusing on balancing data\nutility and privacy, the proposed solution offers a novel approach to securely\ndeploying LLMs in high-risk environments. Future work will focus on testing\nthis framework across various domains to evaluate its effectiveness in managing\nsensitive data while maintaining system efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "40 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.18222v1",
    "published_date": "2024-09-26 19:02:33 UTC",
    "updated_date": "2024-09-26 19:02:33 UTC"
  },
  {
    "arxiv_id": "2409.18219v2",
    "title": "Packet Inspection Transformer: A Self-Supervised Journey to Unseen Malware Detection with Few Samples",
    "authors": [
      "Kyle Stein",
      "Arash Mahyari",
      "Guillermo Francia III",
      "Eman El-Sheikh"
    ],
    "abstract": "As networks continue to expand and become more interconnected, the need for\nnovel malware detection methods becomes more pronounced. Traditional security\nmeasures are increasingly inadequate against the sophistication of modern cyber\nattacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network\nsecurity, offering an in-depth analysis of network traffic that surpasses\nconventional monitoring techniques. DPI not only examines the metadata of\nnetwork packets, but also dives into the actual content being carried within\nthe packet payloads, providing a comprehensive view of the data flowing through\nnetworks. While the integration of advanced deep learning techniques with DPI\nhas introduced modern methodologies into malware detection and network traffic\nclassification, state-of-the-art supervised learning approaches are limited by\ntheir reliance on large amounts of annotated data and their inability to\ngeneralize to novel, unseen malware threats. To address these limitations, this\npaper leverages the recent advancements in self-supervised learning (SSL) and\nfew-shot learning (FSL). Our proposed self-supervised approach trains a\ntransformer via SSL to learn the embedding of packet content, including\npayload, from vast amounts of unlabeled data by masking portions of packets,\nleading to a learned representation that generalizes to various downstream\ntasks. Once the representation is extracted from the packets, they are used to\ntrain a malware detection algorithm. The representation obtained from the\ntransformer is then used to adapt the malware detector to novel types of\nattacks using few-shot learning approaches. Our experimental results\ndemonstrate that our method achieves classification accuracies of up to 94.76%\non the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18219v2",
    "published_date": "2024-09-26 18:55:52 UTC",
    "updated_date": "2025-02-21 18:53:06 UTC"
  },
  {
    "arxiv_id": "2409.18216v1",
    "title": "MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following Benchmark",
    "authors": [
      "Elliot L. Epstein",
      "Kaisheng Yao",
      "Jing Li",
      "Xinyi Bai",
      "Hamid Palangi"
    ],
    "abstract": "Evaluating instruction following capabilities for multimodal, multi-turn\ndialogue is challenging. With potentially multiple instructions in the input\nmodel context, the task is time-consuming for human raters and we show LLM\nbased judges are biased towards answers from the same model. We propose\nMMMT-IF, an image based multi-turn Q$\\&$A evaluation set with added global\ninstructions between questions, constraining the answer format. This challenges\nmodels to retrieve instructions dispersed across long dialogues and reason\nunder instruction constraints. All instructions are objectively verifiable\nthrough code execution. We introduce the Programmatic Instruction Following\n($\\operatorname{PIF}$) metric to measure the fraction of the instructions that\nare correctly followed while performing a reasoning task. The\n$\\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by\nmeasuring the fraction of samples in a corpus where, for each sample, at least\nK out of N generated model responses achieve a $\\operatorname{PIF}$ score of\none. The $\\operatorname{PIF}$ metric aligns with human instruction following\nratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,\nGPT-4o, and Claude 3.5 Sonnet, have a $\\operatorname{PIF}$ metric that drops\nfrom 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across\nall turns, when each response is repeated 4 times ($\\operatorname{PIF-4-4}$),\nGPT-4o and Gemini successfully follow all instructions only $11\\%$ of the time.\nWhen all the instructions are also appended to the end of the model input\ncontext, the $\\operatorname{PIF}$ metric improves by 22.3 points on average,\nshowing that the challenge with the task lies not only in following the\ninstructions, but also in retrieving the instructions spread out in the model\ncontext. We plan to open source the MMMT-IF dataset and metric computation\ncode.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18216v1",
    "published_date": "2024-09-26 18:51:46 UTC",
    "updated_date": "2024-09-26 18:51:46 UTC"
  },
  {
    "arxiv_id": "2409.18203v1",
    "title": "AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking",
    "authors": [
      "Michelle S. Lam",
      "Fred Hohman",
      "Dominik Moritz",
      "Jeffrey P. Bigham",
      "Kenneth Holstein",
      "Mary Beth Kery"
    ],
    "abstract": "Whether a large language model policy is an explicit constitution or an\nimplicit reward model, it is challenging to assess coverage over the unbounded\nset of real-world situations that a policy must contend with. We introduce an\nAI policy design process inspired by mapmaking, which has developed tactics for\nvisualizing and iterating on maps even when full coverage is not possible. With\nPolicy Projector, policy designers can survey the landscape of model\ninput-output pairs, define custom regions (e.g., \"violence\"), and navigate\nthese regions with rules that can be applied to LLM outputs (e.g., if output\ncontains \"violence\" and \"graphic details,\" then rewrite without \"graphic\ndetails\"). Policy Projector supports interactive policy authoring using LLM\nclassification and steering and a map visualization reflecting the policy\ndesigner's work. In an evaluation with 12 AI safety experts, our system helps\npolicy designers to address problematic model behaviors extending beyond an\nexisting, comprehensive harm taxonomy.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18203v1",
    "published_date": "2024-09-26 18:34:16 UTC",
    "updated_date": "2024-09-26 18:34:16 UTC"
  },
  {
    "arxiv_id": "2409.18197v1",
    "title": "Autonomous Network Defence using Reinforcement Learning",
    "authors": [
      "Myles Foley",
      "Chris Hicks",
      "Kate Highnam",
      "Vasilios Mavroudis"
    ],
    "abstract": "In the network security arms race, the defender is significantly\ndisadvantaged as they need to successfully detect and counter every malicious\nattack. In contrast, the attacker needs to succeed only once. To level the\nplaying field, we investigate the effectiveness of autonomous agents in a\nrealistic network defence scenario. We first outline the problem, provide the\nbackground on reinforcement learning and detail our proposed agent design.\nUsing a network environment simulation, with 13 hosts spanning 3 subnets, we\ntrain a novel reinforcement learning agent and show that it can reliably defend\ncontinual attacks by two advanced persistent threat (APT) red agents: one with\ncomplete knowledge of the network layout and another which must discover\nresources through exploration but is more general.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18197v1",
    "published_date": "2024-09-26 18:24:09 UTC",
    "updated_date": "2024-09-26 18:24:09 UTC"
  },
  {
    "arxiv_id": "2409.18170v1",
    "title": "Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review",
    "authors": [
      "Emma Croxford",
      "Yanjun Gao",
      "Nicholas Pellegrino",
      "Karen K. Wong",
      "Graham Wills",
      "Elliot First",
      "Frank J. Liao",
      "Cherodeep Goswami",
      "Brian Patterson",
      "Majid Afshar"
    ],
    "abstract": "Large Language Models have advanced clinical Natural Language Generation,\ncreating opportunities to manage the volume of medical text. However, the\nhigh-stakes nature of medicine requires reliable evaluation, which remains a\nchallenge. In this narrative review, we assess the current evaluation state for\nclinical summarization tasks and propose future directions to address the\nresource constraints of expert human evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18170v1",
    "published_date": "2024-09-26 17:58:26 UTC",
    "updated_date": "2024-09-26 17:58:26 UTC"
  },
  {
    "arxiv_id": "2409.18119v2",
    "title": "Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography",
    "authors": [
      "Yuexi Du",
      "John Onofrey",
      "Nicha C. Dvornek"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential\nin medical image analysis but requires substantial data and computational\nresources. Due to these restrictions, existing CLIP applications in medical\nimaging focus mainly on modalities like chest X-rays that have abundant\nimage-report data available, leaving many other important modalities\nunderexplored. Here, we propose one of the first adaptations of the full CLIP\nmodel to mammography, which presents significant challenges due to labeled data\nscarcity, high-resolution images with small regions of interest, and class-wise\nimbalance. We first develop a specialized supervision framework for mammography\nthat leverages its multi-view nature. Furthermore, we design a symmetric local\nalignment module to better focus on detailed features in high-resolution\nimages. Lastly, we incorporate a parameter-efficient fine-tuning approach for\nlarge language models pre-trained with medical knowledge to address data\nlimitations. Our multi-view and multi-scale alignment (MaMA) method outperforms\nstate-of-the-art baselines for three different tasks on two large real-world\nmammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared\nwith the largest baseline. The code is available at\nhttps://github.com/XYPB/MaMA",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted by IPMI 2025 for Oral Presentation",
    "pdf_url": "http://arxiv.org/pdf/2409.18119v2",
    "published_date": "2024-09-26 17:56:59 UTC",
    "updated_date": "2025-03-27 17:39:55 UTC"
  },
  {
    "arxiv_id": "2409.18169v5",
    "title": "Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey",
    "authors": [
      "Tiansheng Huang",
      "Sihao Hu",
      "Fatih Ilhan",
      "Selim Furkan Tekin",
      "Ling Liu"
    ],
    "abstract": "Recent research demonstrates that the nascent fine-tuning-as-a-service\nbusiness model exposes serious safety concerns -- fine-tuning over a few\nharmful data uploaded by the users can compromise the safety alignment of the\nmodel. The attack, known as harmful fine-tuning attack, has raised a broad\nresearch interest among the community. However, as the attack is still new,\n\\textbf{we observe that there are general misunderstandings within the research\ncommunity.} To clear up concern, this paper provide a comprehensive overview to\nthree aspects of harmful fine-tuning: attacks setting, defense design and\nevaluation methodology. Specifically, we first present the threat model of the\nproblem, and introduce the harmful fine-tuning attack and its variants. Then we\nsystematically survey the existing literature on attacks/defenses/mechanical\nanalysis of the problem. Finally, we introduce the evaluation methodology and\noutline future research directions that might contribute to the development of\nthe field. Additionally, we present a list of questions of interest, which\nmight be useful to refer to when reviewers in the peer review process question\nthe realism of the experiment/attack/defense setting. A curated list of\nrelevant papers is maintained and made accessible at:\nhttps://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18169v5",
    "published_date": "2024-09-26 17:55:22 UTC",
    "updated_date": "2024-12-03 06:52:11 UTC"
  },
  {
    "arxiv_id": "2409.18104v1",
    "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats",
    "authors": [
      "Lucia Gordon",
      "Nikhil Behari",
      "Samuel Collier",
      "Elizabeth Bondi-Kelly",
      "Jackson A. Killian",
      "Catherine Ressijac",
      "Peter Boucher",
      "Andrew Davies",
      "Milind Tambe"
    ],
    "abstract": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 9 figures, IJCAI 2023 Special Track on AI for Good",
    "pdf_url": "http://arxiv.org/pdf/2409.18104v1",
    "published_date": "2024-09-26 17:49:20 UTC",
    "updated_date": "2024-09-26 17:49:20 UTC"
  },
  {
    "arxiv_id": "2409.18101v1",
    "title": "AI-Powered Augmented Reality for Satellite Assembly, Integration and Test",
    "authors": [
      "Alvaro Patricio",
      "Joao Valente",
      "Atabak Dehban",
      "Ines Cadilha",
      "Daniel Reis",
      "Rodrigo Ventura"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is\nset to transform satellite Assembly, Integration, and Testing (AIT) processes\nby enhancing precision, minimizing human error, and improving operational\nefficiency in cleanroom environments. This paper presents a technical\ndescription of the European Space Agency's (ESA) project \"AI for AR in\nSatellite AIT,\" which combines real-time computer vision and AR systems to\nassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2\nas the AR interface, the system delivers context-aware instructions and\nreal-time feedback, tackling the complexities of object recognition and 6D pose\nestimation in AIT workflows. All AI models demonstrated over 70% accuracy, with\nthe detection model exceeding 95% accuracy, indicating a high level of\nperformance and reliability. A key contribution of this work lies in the\neffective use of synthetic data for training AI models in AR applications,\naddressing the significant challenges of obtaining real-world datasets in\nhighly dynamic satellite environments, as well as the creation of the Segmented\nAnything Model for Automatic Labelling (SAMAL), which facilitates the automatic\nannotation of real data, achieving speeds up to 20 times faster than manual\nhuman annotation. The findings demonstrate the efficacy of AI-driven AR systems\nin automating critical satellite assembly tasks, setting a foundation for\nfuture innovations in the space industry.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T05, 68U20",
      "I.2.1; H.5.2; I.4.8; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18101v1",
    "published_date": "2024-09-26 17:44:52 UTC",
    "updated_date": "2024-09-26 17:44:52 UTC"
  },
  {
    "arxiv_id": "2409.18099v1",
    "title": "EfficientCrackNet: A Lightweight Model for Crack Segmentation",
    "authors": [
      "Abid Hasan Zim",
      "Aquib Iqbal",
      "Zaid Al-Huda",
      "Asad Malik",
      "Minoru Kuribayash"
    ],
    "abstract": "Crack detection, particularly from pavement images, presents a formidable\nchallenge in the domain of computer vision due to several inherent complexities\nsuch as intensity inhomogeneity, intricate topologies, low contrast, and noisy\nbackgrounds. Automated crack detection is crucial for maintaining the\nstructural integrity of essential infrastructures, including buildings,\npavements, and bridges. Existing lightweight methods often face challenges\nincluding computational inefficiency, complex crack patterns, and difficult\nbackgrounds, leading to inaccurate detection and impracticality for real-world\napplications. To address these limitations, we propose EfficientCrackNet, a\nlightweight hybrid model combining Convolutional Neural Networks (CNNs) and\ntransformers for precise crack segmentation. EfficientCrackNet integrates\ndepthwise separable convolutions (DSC) layers and MobileViT block to capture\nboth global and local features. The model employs an Edge Extraction Method\n(EEM) and for efficient crack edge detection without pretraining, and\nUltra-Lightweight Subspace Attention Module (ULSAM) to enhance feature\nextraction. Extensive experiments on three benchmark datasets Crack500,\nDeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superior\nperformance compared to existing lightweight models, while requiring only 0.26M\nparameters, and 0.483 FLOPs (G). The proposed model offers an optimal balance\nbetween accuracy and computational efficiency, outperforming state-of-the-art\nlightweight models, and providing a robust and adaptable solution for\nreal-world crack segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18099v1",
    "published_date": "2024-09-26 17:44:20 UTC",
    "updated_date": "2024-09-26 17:44:20 UTC"
  },
  {
    "arxiv_id": "2409.18092v2",
    "title": "DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models",
    "authors": [
      "Helin Cao",
      "Sven Behnke"
    ],
    "abstract": "Perception systems play a crucial role in autonomous driving, incorporating\nmultiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors\nare widely used to capture sparse point clouds of the vehicle's surroundings.\nHowever, such systems struggle to perceive occluded areas and gaps in the scene\ndue to the sparsity of these point clouds and their lack of semantics. To\naddress these challenges, Semantic Scene Completion (SSC) jointly predicts\nunobserved geometry and semantics in the scene given raw LiDAR measurements,\naiming for a more complete scene representation. Building on promising results\nof diffusion models in image generation and super-resolution tasks, we propose\ntheir extension to SSC by implementing the noising and denoising diffusion\nprocesses in the point and semantic spaces individually. To control the\ngeneration, we employ semantic LiDAR point clouds as conditional input and\ndesign local and global regularization losses to stabilize the denoising\nprocess. We evaluate our approach on autonomous driving datasets and our\napproach outperforms the state-of-the-art for SSC.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2409.18092v2",
    "published_date": "2024-09-26 17:39:05 UTC",
    "updated_date": "2024-09-30 18:14:02 UTC"
  },
  {
    "arxiv_id": "2410.03566v1",
    "title": "A Survey on Offensive AI Within Cybersecurity",
    "authors": [
      "Sahil Girhepuje",
      "Aviral Verma",
      "Gaurav Raina"
    ],
    "abstract": "Artificial Intelligence (AI) has witnessed major growth and integration\nacross various domains. As AI systems become increasingly prevalent, they also\nbecome targets for threat actors to manipulate their functionality for\nmalicious purposes. This survey paper on offensive AI will comprehensively\ncover various aspects related to attacks against and using AI systems. It will\ndelve into the impact of offensive AI practices on different domains, including\nconsumer, enterprise, and public digital infrastructure. The paper will explore\nadversarial machine learning, attacks against AI models, infrastructure, and\ninterfaces, along with offensive techniques like information gathering, social\nengineering, and weaponized AI. Additionally, it will discuss the consequences\nand implications of offensive AI, presenting case studies, insights, and\navenues for further research.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03566v1",
    "published_date": "2024-09-26 17:36:22 UTC",
    "updated_date": "2024-09-26 17:36:22 UTC"
  },
  {
    "arxiv_id": "2409.18164v2",
    "title": "Data-Prep-Kit: getting your data ready for LLM application development",
    "authors": [
      "David Wood",
      "Boris Lublinsky",
      "Alexy Roytman",
      "Shivdeep Singh",
      "Constantin Adam",
      "Abdulhamid Adebayo",
      "Sungeun An",
      "Yuan Chi Chang",
      "Xuan-Hong Dang",
      "Nirmit Desai",
      "Michele Dolfi",
      "Hajar Emami-Gohari",
      "Revital Eres",
      "Takuya Goto",
      "Dhiraj Joshi",
      "Yan Koyfman",
      "Mohammad Nassar",
      "Hima Patel",
      "Paramesvaran Selvam",
      "Yousaf Shah",
      "Saptha Surendran",
      "Daiki Tsuzuku",
      "Petros Zerfos",
      "Shahrokh Daijavad"
    ],
    "abstract": "Data preparation is the first and a very important step towards any Large\nLanguage Model (LLM) development. This paper introduces an easy-to-use,\nextensible, and scale-flexible open-source data preparation toolkit called Data\nPrep Kit (DPK). DPK is architected and designed to enable users to scale their\ndata preparation to their needs. With DPK they can prepare data on a local\nmachine or effortlessly scale to run on a cluster with thousands of CPU Cores.\nDPK comes with a highly scalable, yet extensible set of modules that transform\nnatural language and code data. If the user needs additional transforms, they\ncan be easily developed using extensive DPK support for transform creation.\nThese modules can be used independently or pipelined to perform a series of\noperations. In this paper, we describe DPK architecture and show its\nperformance from a small scale to a very large number of CPUs. The modules from\nDPK have been used for the preparation of Granite Models [1] [2]. We believe\nDPK is a valuable contribution to the AI community to easily prepare data to\nenhance the performance of their LLM models or to fine-tune models with\nRetrieval-Augmented Generation (RAG).",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18164v2",
    "published_date": "2024-09-26 17:30:28 UTC",
    "updated_date": "2024-11-13 00:15:46 UTC"
  },
  {
    "arxiv_id": "2409.18163v2",
    "title": "A Survey on Neural Architecture Search Based on Reinforcement Learning",
    "authors": [
      "Wenzhu Shao"
    ],
    "abstract": "The automation of feature extraction of machine learning has been\nsuccessfully realized by the explosive development of deep learning. However,\nthe structures and hyperparameters of deep neural network architectures also\nmake huge difference on the performance in different tasks. The process of\nexploring optimal structures and hyperparameters often involves a lot of\ntedious human intervene. As a result, a legitimate question is to ask for the\nautomation of searching for optimal network structures and hyperparameters. The\nwork of automation of exploring optimal hyperparameters is done by\nHyperparameter Optimization. Neural Architecture Search is aimed to\nautomatically find the best network structure given specific tasks. In this\npaper, we firstly introduced the overall development of Neural Architecture\nSearch and then focus mainly on providing an overall and understandable survey\nabout Neural Architecture Search works that are relevant with reinforcement\nlearning, including improvements and variants based on the hope of satisfying\nmore complex structures and resource-insufficient environment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18163v2",
    "published_date": "2024-09-26 17:28:10 UTC",
    "updated_date": "2024-09-30 06:51:05 UTC"
  },
  {
    "arxiv_id": "2409.18084v2",
    "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model",
    "authors": [
      "Shangyi Luo",
      "Ji Zhu",
      "Peng Sun",
      "Yuhong Deng",
      "Cunjun Yu",
      "Anxing Xiao",
      "Xueqian Wang"
    ],
    "abstract": "With the increasing presence of service robots and autonomous vehicles in\nhuman environments, navigation systems need to evolve beyond simple destination\nreach to incorporate social awareness. This paper introduces GSON, a novel\ngroup-based social navigation framework that leverages Large Multimodal Models\n(LMMs) to enhance robots' social perception capabilities. Our approach uses\nvisual prompting to enable zero-shot extraction of social relationships among\npedestrians and integrates these results with robust pedestrian detection and\ntracking pipelines to overcome the inherent inference speed limitations of\nLMMs. The planning system incorporates a mid-level planner that sits between\nglobal path planning and local motion planning, effectively preserving both\nglobal context and reactive responsiveness while avoiding disruption of the\npredicted social group. We validate GSON through extensive real-world mobile\nrobot navigation experiments involving complex social scenarios such as\nqueuing, conversations, and photo sessions. Comparative results show that our\nsystem significantly outperforms existing navigation approaches in minimizing\nsocial perturbations while maintaining comparable performance on traditional\nnavigation metrics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18084v2",
    "published_date": "2024-09-26 17:27:15 UTC",
    "updated_date": "2025-04-08 06:45:53 UTC"
  },
  {
    "arxiv_id": "2409.18082v2",
    "title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation",
    "authors": [
      "Xin Li",
      "Siyuan Huang",
      "Qiaojun Yu",
      "Zhengkai Jiang",
      "Ce Hao",
      "Yimeng Zhu",
      "Hongsheng Li",
      "Peng Gao",
      "Cewu Lu"
    ],
    "abstract": "Automating garment manipulation poses a significant challenge for assistive\nrobotics due to the diverse and deformable nature of garments. Traditional\napproaches typically require separate models for each garment type, which\nlimits scalability and adaptability. In contrast, this paper presents a unified\napproach using vision-language models (VLMs) to improve keypoint prediction\nacross various garment categories. By interpreting both visual and semantic\ninformation, our model enables robots to manage different garment states with a\nsingle model. We created a large-scale synthetic dataset using advanced\nsimulation techniques, allowing scalable training without extensive real-world\ndata. Experimental results indicate that the VLM-based method significantly\nenhances keypoint detection accuracy and task success rates, providing a more\nflexible and general solution for robotic garment manipulation. In addition,\nthis research also underscores the potential of VLMs to unify various garment\nmanipulation tasks within a single framework, paving the way for broader\napplications in home automation and assistive robotics for future.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18082v2",
    "published_date": "2024-09-26 17:26:16 UTC",
    "updated_date": "2024-10-07 12:06:17 UTC"
  },
  {
    "arxiv_id": "2409.18073v1",
    "title": "Infer Human's Intentions Before Following Natural Language Instructions",
    "authors": [
      "Yanming Wan",
      "Yue Wu",
      "Yiping Wang",
      "Jiayuan Mao",
      "Natasha Jaques"
    ],
    "abstract": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18073v1",
    "published_date": "2024-09-26 17:19:49 UTC",
    "updated_date": "2024-09-26 17:19:49 UTC"
  },
  {
    "arxiv_id": "2409.18162v1",
    "title": "The Nexus of AR/VR, Large Language Models, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children: A Systematic Review",
    "authors": [
      "Biplov Paneru",
      "Bishwash Paneru"
    ],
    "abstract": "The combination of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is examined in\nthis review study. 150 publications were found by a thorough literature search\nthroughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 42 of them\nwere chosen for in-depth study due to their methodological rigor and relevance.\nThree primary areas are covered in this review: how AR can improve social and\nlearning results; how LLMs can help with communication; and how UI/UX design\naffects how effective these technologies are. Results reveal that while LLMs\ncan provide individualized learning and communication support, AR has\ndemonstrated promise in enhancing social skills, motivation, and attention. For\nchildren with ASD, accessible and interesting interventions depend heavily on\neffective UI/UX design. To optimize the benefits of these technologies in ASD\ntherapies, the study emphasizes the need for additional research to address\ndifficulties related to customization, accessibility, and integration.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "none",
    "pdf_url": "http://arxiv.org/pdf/2409.18162v1",
    "published_date": "2024-09-26 17:19:25 UTC",
    "updated_date": "2024-09-26 17:19:25 UTC"
  },
  {
    "arxiv_id": "2409.18071v1",
    "title": "FreeEdit: Mask-free Reference-based Image Editing with Multi-modal Instruction",
    "authors": [
      "Runze He",
      "Kai Ma",
      "Linjiang Huang",
      "Shaofei Huang",
      "Jialin Gao",
      "Xiaoming Wei",
      "Jiao Dai",
      "Jizhong Han",
      "Si Liu"
    ],
    "abstract": "Introducing user-specified visual concepts in image editing is highly\npractical as these concepts convey the user's intent more precisely than\ntext-based descriptions. We propose FreeEdit, a novel approach for achieving\nsuch reference-based image editing, which can accurately reproduce the visual\nconcept from the reference image based on user-friendly language instructions.\nOur approach leverages the multi-modal instruction encoder to encode language\ninstructions to guide the editing process. This implicit way of locating the\nediting area eliminates the need for manual editing masks. To enhance the\nreconstruction of reference details, we introduce the Decoupled Residual\nReferAttention (DRRA) module. This module is designed to integrate fine-grained\nreference features extracted by a detail extractor into the image editing\nprocess in a residual way without interfering with the original self-attention.\nGiven that existing datasets are unsuitable for reference-based image editing\ntasks, particularly due to the difficulty in constructing image triplets that\ninclude a reference image, we curate a high-quality dataset, FreeBench, using a\nnewly developed twice-repainting scheme. FreeBench comprises the images before\nand after editing, detailed editing instructions, as well as a reference image\nthat maintains the identity of the edited object, encompassing tasks such as\nobject addition, replacement, and deletion. By conducting phased training on\nFreeBench followed by quality tuning, FreeEdit achieves high-quality zero-shot\nediting through convenient language instructions. We conduct extensive\nexperiments to evaluate the effectiveness of FreeEdit across multiple task\ntypes, demonstrating its superiority over existing methods. The code will be\navailable at: https://freeedit.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 14 figures, project website: https://freeedit.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2409.18071v1",
    "published_date": "2024-09-26 17:18:39 UTC",
    "updated_date": "2024-09-26 17:18:39 UTC"
  },
  {
    "arxiv_id": "2409.18055v2",
    "title": "Visual Data Diagnosis and Debiasing with Concept Graphs",
    "authors": [
      "Rwiddhi Chakraborty",
      "Yinong Wang",
      "Jialu Gao",
      "Runkai Zheng",
      "Cheng Zhang",
      "Fernando De la Torre"
    ],
    "abstract": "The widespread success of deep learning models today is owed to the curation\nof extensive datasets significant in size and complexity. However, such models\nfrequently pick up inherent biases in the data during the training process,\nleading to unreliable predictions. Diagnosing and debiasing datasets is thus a\nnecessity to ensure reliable model performance. In this paper, we present\nConBias, a novel framework for diagnosing and mitigating Concept co-occurrence\nBiases in visual datasets. ConBias represents visual datasets as knowledge\ngraphs of concepts, enabling meticulous analysis of spurious concept\nco-occurrences to uncover concept imbalances across the whole dataset.\nMoreover, we show that by employing a novel clique-based concept balancing\nstrategy, we can mitigate these imbalances, leading to enhanced performance on\ndownstream tasks. Extensive experiments show that data augmentation based on a\nbalanced concept distribution augmented by Conbias improves generalization\nperformance across multiple datasets compared to state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18055v2",
    "published_date": "2024-09-26 16:59:01 UTC",
    "updated_date": "2024-11-11 12:56:11 UTC"
  },
  {
    "arxiv_id": "2409.18053v3",
    "title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving",
    "authors": [
      "Dingrui Wang",
      "Marc Kaufeld",
      "Johannes Betz"
    ],
    "abstract": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Autonomous Driving, Large Language Models (LLMs), Human Reasoning,\n  Critical Scenario",
    "pdf_url": "http://arxiv.org/pdf/2409.18053v3",
    "published_date": "2024-09-26 16:58:04 UTC",
    "updated_date": "2024-12-04 10:35:51 UTC"
  },
  {
    "arxiv_id": "2409.18052v2",
    "title": "Explaining Explaining",
    "authors": [
      "Sergei Nirenburg",
      "Marjorie McShane",
      "Kenneth W. Goodman",
      "Sanjay Oruganti"
    ],
    "abstract": "Explanation is key to people having confidence in high-stakes AI systems.\nHowever, machine-learning-based systems -- which account for almost all current\nAI -- can't explain because they are usually black boxes. The explainable AI\n(XAI) movement hedges this problem by redefining \"explanation\". The\nhuman-centered explainable AI (HCXAI) movement identifies the\nexplanation-oriented needs of users but can't fulfill them because of its\ncommitment to machine learning. In order to achieve the kinds of explanations\nneeded by real people operating in critical domains, we must rethink how to\napproach AI. We describe a hybrid approach to developing cognitive agents that\nuses a knowledge-based infrastructure supplemented by data obtained through\nmachine learning when applicable. These agents will serve as assistants to\nhumans who will bear ultimate responsibility for the decisions and actions of\nthe human-robot team. We illustrate the explanatory potential of such agents\nusing the under-the-hood panels of a demonstration system in which a team of\nsimulated robots collaborate on a search task assigned by a human.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18052v2",
    "published_date": "2024-09-26 16:55:44 UTC",
    "updated_date": "2024-09-27 02:09:44 UTC"
  },
  {
    "arxiv_id": "2409.18049v1",
    "title": "Revisit Anything: Visual Place Recognition via Image Segment Retrieval",
    "authors": [
      "Kartik Garg",
      "Sai Shubodh Puligilla",
      "Shishir Kolathaya",
      "Madhava Krishna",
      "Sourav Garg"
    ],
    "abstract": "Accurately recognizing a revisited place is crucial for embodied agents to\nlocalize and navigate. This requires visual representations to be distinct,\ndespite strong variations in camera viewpoint and scene appearance. Existing\nvisual place recognition pipelines encode the \"whole\" image and search for\nmatches. This poses a fundamental challenge in matching two images of the same\nplace captured from different camera viewpoints: \"the similarity of what\noverlaps can be dominated by the dissimilarity of what does not overlap\". We\naddress this by encoding and searching for \"image segments\" instead of the\nwhole images. We propose to use open-set image segmentation to decompose an\nimage into `meaningful' entities (i.e., things and stuff). This enables us to\ncreate a novel image representation as a collection of multiple overlapping\nsubgraphs connecting a segment with its neighboring segments, dubbed\nSuperSegment. Furthermore, to efficiently encode these SuperSegments into\ncompact vector representations, we propose a novel factorized representation of\nfeature aggregation. We show that retrieving these partial representations\nleads to significantly higher recognition recall than the typical whole image\nbased retrieval. Our segments-based approach, dubbed SegVLAD, sets a new\nstate-of-the-art in place recognition on a diverse selection of benchmark\ndatasets, while being applicable to both generic and task-specialized image\nencoders. Finally, we demonstrate the potential of our method to ``revisit\nanything'' by evaluating our method on an object instance retrieval task, which\nbridges the two disparate areas of research: visual place recognition and\nobject-goal navigation, through their common aim of recognizing goal objects\nspecific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Presented at ECCV 2024; Includes supplementary; 29 pages; 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.18049v1",
    "published_date": "2024-09-26 16:49:58 UTC",
    "updated_date": "2024-09-26 16:49:58 UTC"
  },
  {
    "arxiv_id": "2409.18047v2",
    "title": "HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams",
    "authors": [
      "Sanjay Oruganti",
      "Sergei Nirenburg",
      "Marjorie McShane",
      "Jesse English",
      "Michael K. Roberts",
      "Christian Arndt",
      "Sahithi Kamireddy"
    ],
    "abstract": "This paper introduces HARMONIC, a cognitive-robotic architecture that\nintegrates the OntoAgent cognitive framework with general-purpose robot control\nsystems applied to human-robot teaming (HRT). We also present a cognitive\nstrategy for robots that incorporates metacognition, natural language\ncommunication, and explainability capabilities required for collaborative\npartnerships in HRT. Through simulation experiments involving a joint search\ntask performed by a heterogeneous team of a UGV, a drone, and a human operator,\nwe demonstrate the system's ability to coordinate actions between robots with\nheterogeneous capabilities, adapt to complex scenarios, and facilitate natural\nhuman-robot communication. Evaluation results show that robots using the\nOntoAgent architecture within the HARMONIC framework can reason about plans,\ngoals, and team member attitudes while providing clear explanations for their\ndecisions, which are essential prerequisites for realistic human-robot teaming.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to IROS 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.18047v2",
    "published_date": "2024-09-26 16:48:21 UTC",
    "updated_date": "2025-03-05 03:08:12 UTC"
  },
  {
    "arxiv_id": "2409.18046v1",
    "title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning",
    "authors": [
      "Soeun Lee",
      "Si-Woo Kim",
      "Taewhan Kim",
      "Dong-Jin Kim"
    ],
    "abstract": "Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18046v1",
    "published_date": "2024-09-26 16:47:32 UTC",
    "updated_date": "2024-09-26 16:47:32 UTC"
  },
  {
    "arxiv_id": "2409.18037v1",
    "title": "HARMONIC: A Framework for Explanatory Cognitive Robots",
    "authors": [
      "Sanjay Oruganti",
      "Sergei Nirenburg",
      "Marjorie McShane",
      "Jesse English",
      "Michael K. Roberts",
      "Christian Arndt"
    ],
    "abstract": "We present HARMONIC, a framework for implementing cognitive robots that\ntransforms general-purpose robots into trusted teammates capable of complex\ndecision-making, natural communication and human-level explanation. The\nframework supports interoperability between a strategic (cognitive) layer for\nhigh-level decision-making and a tactical (robot) layer for low-level control\nand execution. We describe the core features of the framework and our initial\nimplementation, in which HARMONIC was deployed on a simulated UGV and drone\ninvolved in a multi-robot search and retrieval task.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for presentation at ICRA@40. 23-26 September 2024,\n  Rotterdam, Netherlands",
    "pdf_url": "http://arxiv.org/pdf/2409.18037v1",
    "published_date": "2024-09-26 16:42:13 UTC",
    "updated_date": "2024-09-26 16:42:13 UTC"
  },
  {
    "arxiv_id": "2409.18028v3",
    "title": "Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective",
    "authors": [
      "Yotam Wolf",
      "Binyamin Rothberg",
      "Dorin Shteyman",
      "Amnon Shashua"
    ],
    "abstract": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18028v3",
    "published_date": "2024-09-26 16:34:35 UTC",
    "updated_date": "2025-01-31 10:15:43 UTC"
  },
  {
    "arxiv_id": "2409.18025v5",
    "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
    "authors": [
      "Jakub Åucki",
      "Boyi Wei",
      "Yangsibo Huang",
      "Peter Henderson",
      "Florian TramÃ¨r",
      "Javier Rando"
    ],
    "abstract": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Final version published in Transactions on Machine Learning Research\n  (TMLR); Best technical paper at Neurips 2024 SoLaR workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.18025v5",
    "published_date": "2024-09-26 16:32:19 UTC",
    "updated_date": "2025-04-10 13:54:44 UTC"
  },
  {
    "arxiv_id": "2409.18017v3",
    "title": "Transferring disentangled representations: bridging the gap between synthetic and real images",
    "authors": [
      "Jacopo Dapueto",
      "Nicoletta Noceti",
      "Francesca Odone"
    ],
    "abstract": "Developing meaningful and efficient representations that separate the\nfundamental structure of the data generation mechanism is crucial in\nrepresentation learning. However, Disentangled Representation Learning has not\nfully shown its potential on real images, because of correlated generative\nfactors, their resolution and limited access to ground truth labels.\nSpecifically on the latter, we investigate the possibility of leveraging\nsynthetic data to learn general-purpose disentangled representations applicable\nto real data, discussing the effect of fine-tuning and what properties of\ndisentanglement are preserved after the transfer. We provide an extensive\nempirical study to address these issues. In addition, we propose a new\ninterpretable intervention-based metric, to measure the quality of factors\nencoding in the representation. Our results indicate that some level of\ndisentanglement, transferring a representation from synthetic to real data, is\npossible and effective.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS, 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.18017v3",
    "published_date": "2024-09-26 16:25:48 UTC",
    "updated_date": "2024-12-06 09:14:41 UTC"
  },
  {
    "arxiv_id": "2409.18014v1",
    "title": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles",
    "authors": [
      "Lewei He",
      "Tianyu Shi",
      "Pengran Huang",
      "Bingzhi Chen",
      "Qianglong Chen",
      "Jiahui Pan"
    ],
    "abstract": "Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18014v1",
    "published_date": "2024-09-26 16:22:59 UTC",
    "updated_date": "2024-09-26 16:22:59 UTC"
  },
  {
    "arxiv_id": "2409.18009v1",
    "title": "Control Industrial Automation System with Large Language Models",
    "authors": [
      "Yuchen Xia",
      "Nasser Jazdi",
      "Jize Zhang",
      "Chaitanya Shah",
      "Michael Weyrich"
    ],
    "abstract": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.HC",
      "cs.MA",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18009v1",
    "published_date": "2024-09-26 16:19:37 UTC",
    "updated_date": "2024-09-26 16:19:37 UTC"
  },
  {
    "arxiv_id": "2409.17995v1",
    "title": "Joint Localization and Planning using Diffusion",
    "authors": [
      "L. Lao Beyer",
      "S. Karaman"
    ],
    "abstract": "Diffusion models have been successfully applied to robotics problems such as\nmanipulation and vehicle path planning. In this work, we explore their\napplication to end-to-end navigation -- including both perception and planning\n-- by considering the problem of jointly performing global localization and\npath planning in known but arbitrary 2D environments. In particular, we\nintroduce a diffusion model which produces collision-free paths in a global\nreference frame given an egocentric LIDAR scan, an arbitrary map, and a desired\ngoal position. To this end, we implement diffusion in the space of paths in\nSE(2), and describe how to condition the denoising process on both obstacles\nand sensor observations. In our evaluation, we show that the proposed\nconditioning techniques enable generalization to realistic maps of considerably\ndifferent appearance than the training environment, demonstrate our model's\nability to accurately describe ambiguous solutions, and run extensive\nsimulation experiments showcasing our model's use as a real-time, end-to-end\nlocalization and planning stack.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 9 figures. Submitted to ICRA 2025, under review",
    "pdf_url": "http://arxiv.org/pdf/2409.17995v1",
    "published_date": "2024-09-26 16:07:20 UTC",
    "updated_date": "2024-09-26 16:07:20 UTC"
  },
  {
    "arxiv_id": "2409.17994v4",
    "title": "CRoP: Context-wise Robust Static Human-Sensing Personalization",
    "authors": [
      "Sawinder Kaur",
      "Avery Gump",
      "Jingyu Xin",
      "Yi Xiao",
      "Harshit Sharma",
      "Nina R Benway",
      "Jonathan L Preston",
      "Asif Salekin"
    ],
    "abstract": "The advancement in deep learning and internet-of-things have led to diverse\nhuman sensing applications. However, distinct patterns in human sensing,\ninfluenced by various factors or contexts, challenge the generic neural network\nmodel's performance due to natural distribution shifts. To address this,\npersonalization tailors models to individual users. Yet most personalization\nstudies overlook intra-user heterogeneity across contexts in sensory data,\nlimiting intra-user generalizability. This limitation is especially critical in\nclinical applications, where limited data availability hampers both\ngeneralizability and personalization. Notably, intra-user sensing attributes\nare expected to change due to external factors such as treatment progression,\nfurther complicating the challenges. To address the intra-user generalization\nchallenge, this work introduces CRoP, a novel static personalization approach.\nCRoP leverages off-the-shelf pre-trained models as generic starting points and\ncaptures user-specific traits through adaptive pruning on a minimal sub-network\nwhile preserving generic knowledge in the remaining parameters. CRoP\ndemonstrates superior personalization effectiveness and intra-user robustness\nacross four human-sensing datasets, including two from real-world health\ndomains, underscoring its practical and social impact. Additionally, to support\nCRoP's generalization ability and design choices, we provide empirical\njustification through gradient inner product analysis, ablation studies, and\ncomparisons against state-of-the-art baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 6 figues and 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.17994v4",
    "published_date": "2024-09-26 16:06:38 UTC",
    "updated_date": "2024-11-19 14:51:14 UTC"
  },
  {
    "arxiv_id": "2410.06180v1",
    "title": "CBIDR: A novel method for information retrieval combining image and data by means of TOPSIS applied to medical diagnosis",
    "authors": [
      "Humberto Giuri",
      "Renato A. Krohling"
    ],
    "abstract": "Content-Based Image Retrieval (CBIR) have shown promising results in the\nfield of medical diagnosis, which aims to provide support to medical\nprofessionals (doctor or pathologist). However, the ultimate decision regarding\nthe diagnosis is made by the medical professional, drawing upon their\naccumulated experience. In this context, we believe that artificial\nintelligence can play a pivotal role in addressing the challenges in medical\ndiagnosis not by making the final decision but by assisting in the diagnosis\nprocess with the most relevant information. The CBIR methods use similarity\nmetrics to compare feature vectors generated from images using Convolutional\nNeural Networks (CNNs). In addition to the information contained in medical\nimages, clinical data about the patient is often available and is also relevant\nin the final decision-making process by medical professionals. In this paper,\nwe propose a novel method named CBIDR, which leverage both medical images and\nclinical data of patient, combining them through the ranking algorithm TOPSIS.\nThe goal is to aid medical professionals in their final diagnosis by retrieving\nimages and clinical data of patient that are most similar to query data from\nthe database. As a case study, we illustrate our CBIDR for diagnostic of oral\ncancer including histopathological images and clinical data of patient.\nExperimental results in terms of accuracy achieved 97.44% in Top-1 and 100% in\nTop-5 showing the effectiveness of the proposed approach.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.IR",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06180v1",
    "published_date": "2024-09-26 16:04:36 UTC",
    "updated_date": "2024-09-26 16:04:36 UTC"
  },
  {
    "arxiv_id": "2409.17978v2",
    "title": "HydraViT: Stacking Heads for a Scalable ViT",
    "authors": [
      "Janek Haberer",
      "Ali Hojjat",
      "Olaf Landsiedel"
    ],
    "abstract": "The architecture of Vision Transformers (ViTs), particularly the Multi-head\nAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs\non devices with varying constraints, such as mobile phones, requires multiple\nmodels of different sizes. However, this approach has limitations, such as\ntraining and storing each required model separately. This paper introduces\nHydraViT, a novel approach that addresses these limitations by stacking\nattention heads to achieve a scalable ViT. By repeatedly changing the size of\nthe embedded dimensions throughout each layer and their corresponding number of\nattention heads in MHA during training, HydraViT induces multiple subnetworks.\nThereby, HydraViT achieves adaptability across a wide spectrum of hardware\nenvironments while maintaining performance. Our experimental results\ndemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10\nsubnetworks, covering a wide range of resource constraints. HydraViT achieves\nup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy\nwith the same throughput on ImageNet-1K compared to the baselines, making it an\neffective solution for scenarios where hardware availability is diverse or\nvaries over time. Source code available at https://github.com/ds-kiel/HydraViT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at NeurIPS'24, please cite the conference version",
    "pdf_url": "http://arxiv.org/pdf/2409.17978v2",
    "published_date": "2024-09-26 15:52:36 UTC",
    "updated_date": "2024-12-05 16:24:15 UTC"
  },
  {
    "arxiv_id": "2409.18797v1",
    "title": "Supervised Learning Model for Key Frame Identification from Cow Teat Videos",
    "authors": [
      "Minghao Wang",
      "Pinxue Lin"
    ],
    "abstract": "This paper proposes a method for improving the accuracy of mastitis risk\nassessment in cows using neural networks and video analysis. Mastitis, an\ninfection of the udder tissue, is a critical health problem for cows and can be\ndetected by examining the cow's teat. Traditionally, veterinarians assess the\nhealth of a cow's teat during the milking process, but this process is limited\nin time and can weaken the accuracy of the assessment. In commercial farms,\ncows are recorded by cameras when they are milked in the milking parlor. This\npaper uses a neural network to identify key frames in the recorded video where\nthe cow's udder appears intact. These key frames allow veterinarians to have\nmore flexible time to perform health assessments on the teat, increasing their\nefficiency and accuracy. However, there are challenges in using cow teat video\nfor mastitis risk assessment, such as complex environments, changing cow\npositions and postures, and difficulty in identifying the udder from the video.\nTo address these challenges, a fusion distance and an ensemble model are\nproposed to improve the performance (F-score) of identifying key frames from\ncow teat videos. The results show that these two approaches improve performance\ncompared to using a single distance measure or model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18797v1",
    "published_date": "2024-09-26 15:50:43 UTC",
    "updated_date": "2024-09-26 15:50:43 UTC"
  },
  {
    "arxiv_id": "2409.17954v2",
    "title": "Enhancing elusive clues in knowledge learning by contrasting attention of language models",
    "authors": [
      "Jian Gao",
      "Xiao Zhang",
      "Ji Wu",
      "Miao Li"
    ],
    "abstract": "Causal language models acquire vast amount of knowledge from general text\ncorpus during pretraining, but the efficiency of knowledge learning is known to\nbe unsatisfactory, especially when learning from knowledge-dense and\nsmall-sized corpora. The deficiency can come from long-distance dependencies\nwhich are hard to capture by language models, and overfitting to co-occurrence\npatterns and distracting clues in the training text. To address these issues,\nthe paper proposes a method to enhance knowledge learning during language model\npretraining, by enhancing elusive but important clues in text discovered by the\nlanguage model themselves. We found that larger language models pay more\nattention to non-obvious but important clues, which are often overlooked by\nsmaller language models. Therefore, we can identify these clues by contrasting\nthe attention weights of large and small language models. We use the identified\nclues as a guide to perform token-dropout data augmentation on the training\ntext, and observed a significant boost in both small and large models'\nperformance in fact memorization. This shows that the behavior contrast between\nmore and less-performant language models contains important clues for knowledge\nlearning, and it can be ``amplified\" for a straight-forward improvement in\nknowledge learning efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Oral presentation in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.17954v2",
    "published_date": "2024-09-26 15:30:54 UTC",
    "updated_date": "2025-03-12 09:42:19 UTC"
  },
  {
    "arxiv_id": "2409.17946v3",
    "title": "Weak-to-Strong Backdoor Attack for Large Language Models",
    "authors": [
      "Shuai Zhao",
      "Leilei Gan",
      "Zhongliang Guo",
      "Xiaobao Wu",
      "Luwei Xiao",
      "Xiaoyu Xu",
      "Cong-Duy Nguyen",
      "Luu Anh Tuan"
    ],
    "abstract": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17946v3",
    "published_date": "2024-09-26 15:20:37 UTC",
    "updated_date": "2024-10-13 06:33:20 UTC"
  },
  {
    "arxiv_id": "2409.17943v1",
    "title": "On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms",
    "authors": [
      "Richard Yue",
      "John E. Ortega",
      "Kenneth Ward Church"
    ],
    "abstract": "The typical workflow for a professional translator to translate a document\nfrom its source language (SL) to a target language (TL) is not always focused\non what many language models in natural language processing (NLP) do - predict\nthe next word in a series of words. While high-resource languages like English\nand French are reported to achieve near human parity using common metrics for\nmeasurement such as BLEU and COMET, we find that an important step is being\nmissed: the translation of technical terms, specifically acronyms. Some\nstate-of-the art machine translation systems like Google Translate which are\npublicly available can be erroneous when dealing with acronyms - as much as 50%\nin our findings. This article addresses acronym disambiguation for MT systems\nby proposing an additional step to the SL-TL (FR-EN) translation workflow where\nwe first offer a new acronym corpus for public consumption and then experiment\nwith a search-based thresholding algorithm that achieves nearly 10% increase\nwhen compared to Google Translate and OpusMT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AMTA 2024 - The Association for Machine Translation in the Americas\n  organizes biennial conferences devoted to researchers, commercial users,\n  governmental and NGO users",
    "pdf_url": "http://arxiv.org/pdf/2409.17943v1",
    "published_date": "2024-09-26 15:18:34 UTC",
    "updated_date": "2024-09-26 15:18:34 UTC"
  },
  {
    "arxiv_id": "2409.17939v1",
    "title": "Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods",
    "authors": [
      "Richard Yue",
      "John E. Ortega"
    ],
    "abstract": "Translation memories (TMs) are the backbone for professional translation\ntools called computer-aided translation (CAT) tools. In order to perform a\ntranslation using a CAT tool, a translator uses the TM to gather translations\nsimilar to the desired segment to translate (s'). Many CAT tools offer a\nfuzzy-match algorithm to locate segments (s) in the TM that are close in\ndistance to s'. After locating two similar segments, the CAT tool will present\nparallel segments (s, t) that contain one segment in the source language along\nwith its translation in the target language. Additionally, CAT tools contain\nfuzzy-match repair (FMR) techniques that will automatically use the parallel\nsegments from the TM to create new TM entries containing a modified version of\nthe original with the idea in mind that it will be the translation of s'. Most\nFMR techniques use machine translation as a way of \"repairing\" those words that\nhave to be modified. In this article, we show that for a large part of those\nwords which are anchored, we can use other techniques that are based on machine\nlearning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we\nshow that for anchored words that follow the continuous bag-of-words (CBOW)\nparadigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for\nsome cases, better results than neural machine translation for translating\nanchored words from French to English.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AMTA 2024 - The Association for Machine Translation in the Americas\n  organizes biennial conferences devoted to researchers, commercial users,\n  governmental and NGO users",
    "pdf_url": "http://arxiv.org/pdf/2409.17939v1",
    "published_date": "2024-09-26 15:12:59 UTC",
    "updated_date": "2024-09-26 15:12:59 UTC"
  },
  {
    "arxiv_id": "2409.17931v2",
    "title": "Remaining Useful Life Prediction for Batteries Utilizing an Explainable AI Approach with a Predictive Application for Decision-Making",
    "authors": [
      "Biplov Paneru",
      "Bipul Thapa",
      "Durga Prasad Mainali",
      "Bishwash Paneru",
      "Krishna Bikram Shah"
    ],
    "abstract": "Accurately estimating the Remaining Useful Life (RUL) of a battery is\nessential for determining its lifespan and recharge requirements. In this work,\nwe develop machine learning-based models to predict and classify battery RUL.\nWe introduce a two-level ensemble learning (TLE) framework and a CNN+MLP hybrid\nmodel for RUL prediction, comparing their performance against traditional,\ndeep, and hybrid machine learning models. Our analysis evaluates various models\nfor both prediction and classification while incorporating interpretability\nthrough SHAP. The proposed TLE model consistently outperforms baseline models\nin RMSE, MAE, and R squared error, demonstrating its superior predictive\ncapabilities. Additionally, the XGBoost classifier achieves an impressive 99%\nclassification accuracy, validated through cross-validation techniques. The\nmodels effectively predict relay-based charging triggers, enabling automated\nand energy-efficient charging processes. This automation reduces energy\nconsumption and enhances battery performance by optimizing charging cycles.\nSHAP interpretability analysis highlights the cycle index and charging\nparameters as the most critical factors influencing RUL. To improve\naccessibility, we developed a Tkinter-based GUI that allows users to input new\ndata and predict RUL in real time. This practical solution supports sustainable\nbattery management by enabling data-driven decisions about battery usage and\nmaintenance, contributing to energy-efficient and innovative battery life\nprediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17931v2",
    "published_date": "2024-09-26 15:08:38 UTC",
    "updated_date": "2025-01-30 14:48:00 UTC"
  },
  {
    "arxiv_id": "2409.17928v2",
    "title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion",
    "authors": [
      "Hengrui Gu",
      "Kaixiong Zhou",
      "Yili Wang",
      "Ruobing Wang",
      "Xin Wang"
    ],
    "abstract": "During pre-training, the Text-to-Image (T2I) diffusion models encode factual\nknowledge into their parameters. These parameterized facts enable realistic\nimage generation, but they may become obsolete over time, thereby\nmisrepresenting the current state of the world. Knowledge editing techniques\naim to update model knowledge in a targeted way. However, facing the dual\nchallenges posed by inadequate editing datasets and unreliable evaluation\ncriterion, the development of T2I knowledge editing encounter difficulties in\neffectively generalizing injected knowledge. In this work, we design a T2I\nknowledge editing framework by comprehensively spanning on three phases: First,\nwe curate a dataset \\textbf{CAKE}, comprising paraphrase and multi-object test,\nto enable more fine-grained assessment on knowledge generalization. Second, we\npropose a novel criterion, \\textbf{adaptive CLIP threshold}, to effectively\nfilter out false successful images under the current criterion and achieve\nreliable editing evaluation. Finally, we introduce \\textbf{MPE}, a simple but\neffective approach for T2I knowledge editing. Instead of tuning parameters, MPE\nprecisely recognizes and edits the outdated part of the conditioning\ntext-prompt to accommodate the up-to-date knowledge. A straightforward\nimplementation of MPE (Based on in-context learning) exhibits better overall\nperformance than previous model editors. We hope these efforts can further\npromote faithful evaluation of T2I knowledge editing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP24 Findings. Our code is available at\n  https://github.com/Hengrui-Gu/T2IKnowledgeEditing",
    "pdf_url": "http://arxiv.org/pdf/2409.17928v2",
    "published_date": "2024-09-26 15:07:30 UTC",
    "updated_date": "2024-10-26 06:03:00 UTC"
  },
  {
    "arxiv_id": "2409.17922v1",
    "title": "Navigation in a simplified Urban Flow through Deep Reinforcement Learning",
    "authors": [
      "Federica Tonti",
      "Jean Rabault",
      "Ricardo Vinuesa"
    ],
    "abstract": "The increasing number of unmanned aerial vehicles (UAVs) in urban\nenvironments requires a strategy to minimize their environmental impact, both\nin terms of energy efficiency and noise reduction. In order to reduce these\nconcerns, novel strategies for developing prediction models and optimization of\nflight planning, for instance through deep reinforcement learning (DRL), are\nneeded. Our goal is to develop DRL algorithms capable of enabling the\nautonomous navigation of UAVs in urban environments, taking into account the\npresence of buildings and other UAVs, optimizing the trajectories in order to\nreduce both energetic consumption and noise. This is achieved using fluid-flow\nsimulations which represent the environment in which UAVs navigate and training\nthe UAV as an agent interacting with an urban environment. In this work, we\nconsider a domain domain represented by a two-dimensional flow field with\nobstacles, ideally representing buildings, extracted from a three-dimensional\nhigh-fidelity numerical simulation. The presented methodology, using PPO+LSTM\ncells, was validated by reproducing a simple but fundamental problem in\nnavigation, namely the Zermelo's problem, which deals with a vessel navigating\nin a turbulent flow, travelling from a starting point to a target location,\noptimizing the trajectory. The current method shows a significant improvement\nwith respect to both a simple PPO and a TD3 algorithm, with a success rate (SR)\nof the PPO+LSTM trained policy of 98.7%, and a crash rate (CR) of 0.1%,\noutperforming both PPO (SR = 75.6%, CR=18.6%) and TD3 (SR=77.4% and CR=14.5%).\nThis is the first step towards DRL strategies which will guide UAVs in a\nthree-dimensional flow field using real-time signals, making the navigation\nefficient in terms of flight time and avoiding damages to the vehicle.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17922v1",
    "published_date": "2024-09-26 15:05:15 UTC",
    "updated_date": "2024-09-26 15:05:15 UTC"
  },
  {
    "arxiv_id": "2409.17907v1",
    "title": "PhantomLiDAR: Cross-modality Signal Injection Attacks against LiDAR",
    "authors": [
      "Zizhi Jin",
      "Qinhong Jiang",
      "Xuancun Lu",
      "Chen Yan",
      "Xiaoyu Ji",
      "Wenyuan Xu"
    ],
    "abstract": "LiDAR (Light Detection and Ranging) is a pivotal sensor for autonomous\ndriving, offering precise 3D spatial information. Previous signal attacks\nagainst LiDAR systems mainly exploit laser signals. In this paper, we\ninvestigate the possibility of cross-modality signal injection attacks, i.e.,\ninjecting intentional electromagnetic interference (IEMI) to manipulate LiDAR\noutput. Our insight is that the internal modules of a LiDAR, i.e., the laser\nreceiving circuit, the monitoring sensors, and the beam-steering modules, even\nwith strict electromagnetic compatibility (EMC) testing, can still couple with\nthe IEMI attack signals and result in the malfunction of LiDAR systems. Based\non the above attack surfaces, we propose the PhantomLiDAR attack, which\nmanipulates LiDAR output in terms of Points Interference, Points Injection,\nPoints Removal, and even LiDAR Power-Off. We evaluate and demonstrate the\neffectiveness of PhantomLiDAR with both simulated and real-world experiments on\nfive COTS LiDAR systems. We also conduct feasibility experiments in real-world\nmoving scenarios. We provide potential defense measures that can be implemented\nat both the sensor level and the vehicle system level to mitigate the risks\nassociated with IEMI attacks. Video demonstrations can be viewed at\nhttps://sites.google.com/view/phantomlidar.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.ET",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17907v1",
    "published_date": "2024-09-26 14:52:51 UTC",
    "updated_date": "2024-09-26 14:52:51 UTC"
  },
  {
    "arxiv_id": "2409.17904v1",
    "title": "Learning to Love Edge Cases in Formative Math Assessment: Using the AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy",
    "authors": [
      "Owen Henkel",
      "Hannah Horne-Robinson",
      "Maria Dyshel",
      "Nabil Ch",
      "Baptiste Moreau-Pernet",
      "Ralph Abood"
    ],
    "abstract": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17904v1",
    "published_date": "2024-09-26 14:51:40 UTC",
    "updated_date": "2024-09-26 14:51:40 UTC"
  },
  {
    "arxiv_id": "2409.17899v2",
    "title": "Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations",
    "authors": [
      "Yujia Sun",
      "Zeyu Zhao",
      "Korin Richmond",
      "Yuanchao Li"
    ],
    "abstract": "Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.17899v2",
    "published_date": "2024-09-26 14:49:09 UTC",
    "updated_date": "2025-04-30 13:32:40 UTC"
  },
  {
    "arxiv_id": "2409.17876v1",
    "title": "Why Companies \"Democratise\" Artificial Intelligence: The Case of Open Source Software Donations",
    "authors": [
      "Cailean Osborne"
    ],
    "abstract": "Companies claim to \"democratise\" artificial intelligence (AI) when they\ndonate AI open source software (OSS) to non-profit foundations or release AI\nmodels, among others, but what does this term mean and why do they do it? As\nthe impact of AI on society and the economy grows, understanding the commercial\nincentives behind AI democratisation efforts is crucial for ensuring these\nefforts serve broader interests beyond commercial agendas. Towards this end,\nthis study employs a mixed-methods approach to investigate commercial\nincentives for 43 AI OSS donations to the Linux Foundation. It makes\ncontributions to both research and practice. It contributes a taxonomy of both\nindividual and organisational social, economic, and technological incentives\nfor AI democratisation. In particular, it highlights the role of democratising\nthe governance and control rights of an OSS project (i.e., from one company to\nopen governance) as a structural enabler for downstream goals, such as\nattracting external contributors, reducing development costs, and influencing\nindustry standards, among others. Furthermore, OSS donations are often\nchampioned by individual developers within companies, highlighting the\nimportance of the bottom-up incentives for AI democratisation. The taxonomy\nprovides a framework and toolkit for discerning incentives for other AI\ndemocratisation efforts, such as the release of AI models. The paper concludes\nwith a discussion of future research directions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "30 pages, 1 figure, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.17876v1",
    "published_date": "2024-09-26 14:23:44 UTC",
    "updated_date": "2024-09-26 14:23:44 UTC"
  },
  {
    "arxiv_id": "2409.17874v1",
    "title": "DarkSAM: Fooling Segment Anything Model to Segment Nothing",
    "authors": [
      "Ziqi Zhou",
      "Yufei Song",
      "Minghui Li",
      "Shengshan Hu",
      "Xianlong Wang",
      "Leo Yu Zhang",
      "Dezhong Yao",
      "Hai Jin"
    ],
    "abstract": "Segment Anything Model (SAM) has recently gained much attention for its\noutstanding generalization to unseen data and tasks. Despite its promising\nprospect, the vulnerabilities of SAM, especially to universal adversarial\nperturbation (UAP) have not been thoroughly investigated yet. In this paper, we\npropose DarkSAM, the first prompt-free universal attack framework against SAM,\nincluding a semantic decoupling-based spatial attack and a texture\ndistortion-based frequency attack. We first divide the output of SAM into\nforeground and background. Then, we design a shadow target strategy to obtain\nthe semantic blueprint of the image as the attack target. DarkSAM is dedicated\nto fooling SAM by extracting and destroying crucial object features from images\nin both spatial and frequency domains. In the spatial domain, we disrupt the\nsemantics of both the foreground and background in the image to confuse SAM. In\nthe frequency domain, we further enhance the attack effectiveness by distorting\nthe high-frequency components (i.e., texture information) of the image.\nConsequently, with a single UAP, DarkSAM renders SAM incapable of segmenting\nobjects across diverse images with varying prompts. Experimental results on\nfour datasets for SAM and its two variant models demonstrate the powerful\nattack capability and transferability of DarkSAM.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by the 38th Annual Conference on Neural\n  Information Processing Systems (NeurIPS'24)",
    "pdf_url": "http://arxiv.org/pdf/2409.17874v1",
    "published_date": "2024-09-26 14:20:14 UTC",
    "updated_date": "2024-09-26 14:20:14 UTC"
  },
  {
    "arxiv_id": "2409.17870v2",
    "title": "Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores",
    "authors": [
      "Shaobo Ma",
      "Chao Fang",
      "Haikuo Shao",
      "Zhongfeng Wang"
    ],
    "abstract": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 2.4\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by ASP-DAC 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.17870v2",
    "published_date": "2024-09-26 14:17:58 UTC",
    "updated_date": "2024-10-18 02:01:18 UTC"
  },
  {
    "arxiv_id": "2409.17865v1",
    "title": "Implementing a Nordic-Baltic Federated Health Data Network: a case report",
    "authors": [
      "Taridzo Chomutare",
      "Aleksandar Babic",
      "Laura-Maria Peltonen",
      "Silja Elunurm",
      "Peter Lundberg",
      "Arne JÃ¶nsson",
      "Emma Eneling",
      "Ciprian-Virgil Gerstenberger",
      "Troels Siggaard",
      "Raivo Kolde",
      "Oskar Jerdhaf",
      "Martin Hansson",
      "Alexandra Makhlysheva",
      "Miroslav Muzny",
      "Erik YlipÃ¤Ã¤",
      "SÃ¸ren Brunak",
      "Hercules Dalianis"
    ],
    "abstract": "Background: Centralized collection and processing of healthcare data across\nnational borders pose significant challenges, including privacy concerns, data\nheterogeneity and legal barriers. To address some of these challenges, we\nformed an interdisciplinary consortium to develop a feder-ated health data\nnetwork, comprised of six institutions across five countries, to facilitate\nNordic-Baltic cooperation on secondary use of health data. The objective of\nthis report is to offer early insights into our experiences developing this\nnetwork. Methods: We used a mixed-method ap-proach, combining both experimental\ndesign and implementation science to evaluate the factors affecting the\nimplementation of our network. Results: Technically, our experiments indicate\nthat the network functions without significant performance degradation compared\nto centralized simu-lation. Conclusion: While use of interdisciplinary\napproaches holds a potential to solve challeng-es associated with establishing\nsuch collaborative networks, our findings turn the spotlight on the uncertain\nregulatory landscape playing catch up and the significant operational costs.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "24 pages (including appendices), 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.17865v1",
    "published_date": "2024-09-26 14:15:54 UTC",
    "updated_date": "2024-09-26 14:15:54 UTC"
  },
  {
    "arxiv_id": "2409.17864v1",
    "title": "A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios",
    "authors": [
      "Christian GanhÃ¶r",
      "Marta Moscati",
      "Anna Hausberger",
      "Shah Nawaz",
      "Markus Schedl"
    ],
    "abstract": "Most recommender systems adopt collaborative filtering (CF) and provide\nrecommendations based on past collective interactions. Therefore, the\nperformance of CF algorithms degrades when few or no interactions are\navailable, a scenario referred to as cold-start. To address this issue,\nprevious work relies on models leveraging both collaborative data and side\ninformation on the users or items. Similar to multimodal learning, these models\naim at combining collaborative and content representations in a shared\nembedding space. In this work we propose a novel technique for multimodal\nrecommendation, relying on a multimodal Single-Branch embedding network for\nRecommendation (SiBraR). Leveraging weight-sharing, SiBraR encodes interaction\ndata as well as multimodal side information using the same single-branch\nembedding network on different modalities. This makes SiBraR effective in\nscenarios of missing modality, including cold start. Our extensive experiments\non large-scale recommendation datasets from three different recommendation\ndomains (music, movie, and e-commerce) and providing multimodal content\ninformation (audio, text, image, labels, and interactions) show that SiBraR\nsignificantly outperforms CF as well as state-of-the-art content-based RSs in\ncold-start scenarios, and is competitive in warm scenarios. We show that\nSiBraR's recommendations are accurate in missing modality scenarios, and that\nthe model is able to map different modalities to the same region of the shared\nembedding space, hence reducing the modality gap.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at 18th ACM Conference on Recommender Systems (RecSys '24)",
    "pdf_url": "http://arxiv.org/pdf/2409.17864v1",
    "published_date": "2024-09-26 14:12:23 UTC",
    "updated_date": "2024-09-26 14:12:23 UTC"
  },
  {
    "arxiv_id": "2409.17841v1",
    "title": "Machine Learning-based vs Deep Learning-based Anomaly Detection in Multivariate Time Series for Spacecraft Attitude Sensors",
    "authors": [
      "R. Gallon",
      "F. Schiemenz",
      "A. Krstova",
      "A. Menicucci",
      "E. Gill"
    ],
    "abstract": "In the framework of Failure Detection, Isolation and Recovery (FDIR) on\nspacecraft, new AI-based approaches are emerging in the state of the art to\novercome the limitations commonly imposed by traditional threshold checking.\n  The present research aims at characterizing two different approaches to the\nproblem of stuck values detection in multivariate time series coming from\nspacecraft attitude sensors. The analysis reveals the performance differences\nin the two approaches, while commenting on their interpretability and\ngeneralization to different scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for the ESA SPAICE Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17841v1",
    "published_date": "2024-09-26 13:45:36 UTC",
    "updated_date": "2024-09-26 13:45:36 UTC"
  },
  {
    "arxiv_id": "2409.17840v1",
    "title": "Detecting and Measuring Confounding Using Causal Mechanism Shifts",
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Vineeth N Balasubramanian"
    ],
    "abstract": "Detecting and measuring confounding effects from data is a key challenge in\ncausal inference. Existing methods frequently assume causal sufficiency,\ndisregarding the presence of unobserved confounding variables. Causal\nsufficiency is both unrealistic and empirically untestable. Additionally,\nexisting methods make strong parametric assumptions about the underlying causal\ngenerative process to guarantee the identifiability of confounding variables.\nRelaxing the causal sufficiency and parametric assumptions and leveraging\nrecent advancements in causal discovery and confounding analysis with\nnon-i.i.d. data, we propose a comprehensive approach for detecting and\nmeasuring confounding. We consider various definitions of confounding and\nintroduce tailored methodologies to achieve three objectives: (i) detecting and\nmeasuring confounding among a set of variables, (ii) separating observed and\nunobserved confounding effects, and (iii) understanding the relative strengths\nof confounding bias between different sets of variables. We present useful\nproperties of a confounding measure and present measures that satisfy those\nproperties. Empirical results support the theoretical analysis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17840v1",
    "published_date": "2024-09-26 13:44:22 UTC",
    "updated_date": "2024-09-26 13:44:22 UTC"
  },
  {
    "arxiv_id": "2409.17836v2",
    "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models",
    "authors": [
      "Hui-Po Wang",
      "Mario Fritz"
    ],
    "abstract": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10% up to 17.2% across various datasets\nand architectures. Additionally, our approach shows promising compatibility\nwith lossy compression techniques such as quantization and sparsification.\nThese findings highlight the significant potential of LLMs as a model for\neffectively handling gradients. Code is available at\nhttps://github.com/hui-po-wang/LM-GC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "camera-ready in NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17836v2",
    "published_date": "2024-09-26 13:38:33 UTC",
    "updated_date": "2025-01-22 09:26:42 UTC"
  },
  {
    "arxiv_id": "2409.17819v1",
    "title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
    "authors": [
      "Zhixuan Liu",
      "Zhanhui Zhou",
      "Yuanfu Wang",
      "Chao Yang",
      "Yu Qiao"
    ],
    "abstract": "Large language models are typically fine-tuned to align with human\npreferences, but tuning large models is computationally intensive and complex.\nIn this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method\nthat uses implicit and explicit value functions to guide language model\ndecoding at token and chunk-level respectively, efficiently aligning large\nlanguage models purely at inference time. This approach circumvents the\ncomplexities of direct fine-tuning and outperforms traditional methods.\nEmpirically, we demonstrate the versatility of IVG across various tasks. In\ncontrolled sentiment generation and summarization tasks, our method\nsignificantly improves the alignment of large models using inference-time\nguidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more\nchallenging instruction-following benchmark AlpacaEval 2.0, we show that both\nspecifically tuned and off-the-shelf value functions greatly improve the\nlength-controlled win rates of large models against $\\texttt{gpt-4-turbo}$\n(e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$\nand $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$\nwith Tulu guidance).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2409.17819v1",
    "published_date": "2024-09-26 13:15:18 UTC",
    "updated_date": "2024-09-26 13:15:18 UTC"
  },
  {
    "arxiv_id": "2409.17815v1",
    "title": "DREAMS: A python framework to train deep learning models with model card reporting for medical and health applications",
    "authors": [
      "Rabindra Khadka",
      "Pedro G Lind",
      "Anis Yazidi",
      "Asma Belhadi"
    ],
    "abstract": "Electroencephalography (EEG) data provides a non-invasive method for\nresearchers and clinicians to observe brain activity in real time. The\nintegration of deep learning techniques with EEG data has significantly\nimproved the ability to identify meaningful patterns, leading to valuable\ninsights for both clinical and research purposes. However, most of the\nframeworks so far, designed for EEG data analysis, are either too focused on\npre-processing or in deep learning methods per, making their use for both\nclinician and developer communities problematic. Moreover, critical issues such\nas ethical considerations, biases, uncertainties, and the limitations inherent\nin AI models for EEG data analysis are frequently overlooked, posing challenges\nto the responsible implementation of these technologies. In this paper, we\nintroduce a comprehensive deep learning framework tailored for EEG data\nprocessing, model training and report generation. While constructed in way to\nbe adapted and developed further by AI developers, it enables to report,\nthrough model cards, the outcome and specific information of use for both\ndevelopers and clinicians. In this way, we discuss how this framework can, in\nthe future, provide clinical researchers and developers with the tools needed\nto create transparent and accountable AI models for EEG data analysis and\ndiagnosis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17815v1",
    "published_date": "2024-09-26 13:12:13 UTC",
    "updated_date": "2024-09-26 13:12:13 UTC"
  },
  {
    "arxiv_id": "2409.17791v1",
    "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness",
    "authors": [
      "Jian Li",
      "Haojing Huang",
      "Yujia Zhang",
      "Pengfei Xu",
      "Xi Chen",
      "Rui Song",
      "Lida Shi",
      "Jingwen Wang",
      "Hao Xu"
    ],
    "abstract": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2409.17791v1",
    "published_date": "2024-09-26 12:37:26 UTC",
    "updated_date": "2024-09-26 12:37:26 UTC"
  },
  {
    "arxiv_id": "2409.17788v1",
    "title": "Ophthalmic Biomarker Detection with Parallel Prediction of Transformer and Convolutional Architecture",
    "authors": [
      "Md. Touhidul Islam",
      "Md. Abtahi Majeed Chowdhury",
      "Mahmudul Hasan",
      "Asif Quadir",
      "Lutfa Aktar"
    ],
    "abstract": "Ophthalmic diseases represent a significant global health issue,\nnecessitating the use of advanced precise diagnostic tools. Optical Coherence\nTomography (OCT) imagery which offers high-resolution cross-sectional images of\nthe retina has become a pivotal imaging modality in ophthalmology.\nTraditionally physicians have manually detected various diseases and biomarkers\nfrom such diagnostic imagery. In recent times, deep learning techniques have\nbeen extensively used for medical diagnostic tasks enabling fast and precise\ndiagnosis. This paper presents a novel approach for ophthalmic biomarker\ndetection using an ensemble of Convolutional Neural Network (CNN) and Vision\nTransformer. While CNNs are good for feature extraction within the local\ncontext of the image, transformers are known for their ability to extract\nfeatures from the global context of the image. Using an ensemble of both\ntechniques allows us to harness the best of both worlds. Our method has been\nimplemented on the OLIVES dataset to detect 6 major biomarkers from the OCT\nimages and shows significant improvement of the macro averaged F1 score on the\ndataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17788v1",
    "published_date": "2024-09-26 12:33:34 UTC",
    "updated_date": "2024-09-26 12:33:34 UTC"
  },
  {
    "arxiv_id": "2409.17777v3",
    "title": "Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification",
    "authors": [
      "Raja Kumar",
      "Raghav Singhal",
      "Pranamya Kulkarni",
      "Deval Mehta",
      "Kshitij Jadhav"
    ],
    "abstract": "Deep multimodal learning has shown remarkable success by leveraging\ncontrastive learning to capture explicit one-to-one relations across\nmodalities. However, real-world data often exhibits shared relations beyond\nsimple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive\nLearning approach to capture nuanced shared relations inherent in multimodal\ndata. Our key contribution is a Mixup-based contrastive loss that learns robust\nrepresentations by aligning mixed samples from one modality with their\ncorresponding samples from other modalities thereby capturing shared relations\nbetween them. For multimodal classification tasks, we introduce a framework\nthat integrates a fusion module with unimodal prediction modules for auxiliary\nsupervision during training, complemented by our proposed Mixup-based\ncontrastive loss. Through extensive experiments on diverse datasets (N24News,\nROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures\nshared multimodal relations and generalizes across domains. It outperforms\nstate-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving\ncomparable performance on Food-101. Our work highlights the significance of\nlearning shared relations for robust multimodal learning, opening up promising\navenues for future research. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/M3CoL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "RK and RS contributed equally to this work, 20 Pages, 8 Figures, 9\n  Tables. Another version of the paper accepted at NeurIPS 2024 Workshop on\n  Unifying Representations in Neural Models (UniReps)",
    "pdf_url": "http://arxiv.org/pdf/2409.17777v3",
    "published_date": "2024-09-26 12:15:13 UTC",
    "updated_date": "2024-12-06 06:58:30 UTC"
  },
  {
    "arxiv_id": "2409.17774v2",
    "title": "Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations",
    "authors": [
      "Supriya Manna",
      "Niladri Sett"
    ],
    "abstract": "Faithfulness is arguably the most critical metric to assess the reliability\nof explainable AI. In NLP, current methods for faithfulness evaluation are\nfraught with discrepancies and biases, often failing to capture the true\nreasoning of models. We introduce Adversarial Sensitivity as a novel approach\nto faithfulness evaluation, focusing on the explainer's response when the model\nis under adversarial attack. Our method accounts for the faithfulness of\nexplainers by capturing sensitivity to adversarial input changes. This work\naddresses significant limitations in existing evaluation techniques, and\nfurthermore, quantifies faithfulness from a crucial yet underexplored paradigm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a Full Paper at EMNLP 2024 Workshop BlackBoxNLP",
    "pdf_url": "http://arxiv.org/pdf/2409.17774v2",
    "published_date": "2024-09-26 12:11:28 UTC",
    "updated_date": "2024-10-09 11:59:34 UTC"
  },
  {
    "arxiv_id": "2409.17767v1",
    "title": "Federated Learning under Attack: Improving Gradient Inversion for Batch of Images",
    "authors": [
      "Luiz Leite",
      "Yuri Santo",
      "Bruno L. Dalmazo",
      "AndrÃ© Riker"
    ],
    "abstract": "Federated Learning (FL) has emerged as a machine learning approach able to\npreserve the privacy of user's data. Applying FL, clients train machine\nlearning models on a local dataset and a central server aggregates the learned\nparameters coming from the clients, training a global machine learning model\nwithout sharing user's data. However, the state-of-the-art shows several\napproaches to promote attacks on FL systems. For instance, inverting or leaking\ngradient attacks can find, with high precision, the local dataset used during\nthe training phase of the FL. This paper presents an approach, called Deep\nLeakage from Gradients with Feedback Blending (DLG-FB), which is able to\nimprove the inverting gradient attack, considering the spatial correlation that\ntypically exists in batches of images. The performed evaluation shows an\nimprovement of 19.18% and 48,82% in terms of attack success rate and the number\nof iterations per attacked image, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "5 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.17767v1",
    "published_date": "2024-09-26 12:02:36 UTC",
    "updated_date": "2024-09-26 12:02:36 UTC"
  },
  {
    "arxiv_id": "2409.17763v2",
    "title": "Confidence intervals uncovered: Are we ready for real-world medical imaging AI?",
    "authors": [
      "Evangelia Christodoulou",
      "Annika Reinke",
      "Rola Houhou",
      "Piotr Kalinowski",
      "Selen Erkan",
      "Carole H. Sudre",
      "Ninon Burgos",
      "SofiÃ¨ne Boutaj",
      "Sophie Loizillon",
      "MaÃ«lys Solal",
      "Nicola Rieke",
      "Veronika Cheplygina",
      "Michela Antonelli",
      "Leon D. Mayer",
      "Minu D. Tizabi",
      "M. Jorge Cardoso",
      "Amber Simpson",
      "Paul F. JÃ¤ger",
      "Annette Kopp-Schneider",
      "GaÃ«l Varoquaux",
      "Olivier Colliot",
      "Lena Maier-Hein"
    ],
    "abstract": "Medical imaging is spearheading the AI transformation of healthcare.\nPerformance reporting is key to determine which methods should be translated\ninto clinical practice. Frequently, broad conclusions are simply derived from\nmean performance values. In this paper, we argue that this common practice is\noften a misleading simplification as it ignores performance variability. Our\ncontribution is threefold. (1) Analyzing all MICCAI segmentation papers (n =\n221) published in 2023, we first observe that more than 50% of papers do not\nassess performance variability at all. Moreover, only one (0.5%) paper reported\nconfidence intervals (CIs) for model performance. (2) To address the reporting\nbottleneck, we show that the unreported standard deviation (SD) in segmentation\npapers can be approximated by a second-order polynomial function of the mean\nDice similarity coefficient (DSC). Based on external validation data from 56\nprevious MICCAI challenges, we demonstrate that this approximation can\naccurately reconstruct the CI of a method using information provided in\npublications. (3) Finally, we reconstructed 95% CIs around the mean DSC of\nMICCAI 2023 segmentation papers. The median CI width was 0.03 which is three\ntimes larger than the median performance gap between the first and second\nranked method. For more than 60% of papers, the mean performance of the\nsecond-ranked method was within the CI of the first-ranked method. We conclude\nthat current publications typically do not provide sufficient evidence to\nsupport which models could potentially be translated into clinical practice.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted at MICCAI 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2409.17763v2",
    "published_date": "2024-09-26 11:58:41 UTC",
    "updated_date": "2024-09-27 06:50:21 UTC"
  },
  {
    "arxiv_id": "2409.17757v1",
    "title": "Integrating Hierarchical Semantic into Iterative Generation Model for Entailment Tree Explanation",
    "authors": [
      "Qin Wang",
      "Jianzhou Feng",
      "Yiming Xu"
    ],
    "abstract": "Manifestly and logically displaying the line of reasoning from evidence to\nanswer is significant to explainable question answering (QA). The entailment\ntree exhibits the lines structurally, which is different from the\nself-explanation principle in large-scale language models. Existing methods\nrarely consider the semantic association of sentences between and within\nhierarchies within the tree structure, which is prone to apparent mistakes in\ncombinations. In this work, we propose an architecture of integrating the\nHierarchical Semantics of sentences under the framework of Controller-Generator\n(HiSCG) to explain answers. The HiSCG designs a hierarchical mapping between\nhypotheses and facts, discriminates the facts involved in tree constructions,\nand optimizes single-step entailments. To the best of our knowledge, We are the\nfirst to notice hierarchical semantics of sentences between the same layer and\nadjacent layers to yield improvements. The proposed method achieves comparable\nperformance on all three settings of the EntailmentBank dataset. The\ngeneralization results on two out-of-domain datasets also demonstrate the\neffectiveness of our method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17757v1",
    "published_date": "2024-09-26 11:46:58 UTC",
    "updated_date": "2024-09-26 11:46:58 UTC"
  },
  {
    "arxiv_id": "2409.17755v2",
    "title": "SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning",
    "authors": [
      "Rimvydas Rubavicius",
      "Peter David Fagan",
      "Alex Lascarides",
      "Subramanian Ramamoorthy"
    ],
    "abstract": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the agent is unaware of a concept that is key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems. It uses embodied conversation to fix its\ndeficient domain model -- through dialogue, the agent discovers and then learns\nto exploit unforeseen possibilities. In particular, SECURE learns from the\nuser's embodied corrective feedback when it makes a mistake, and it makes\nstrategic dialogue decisions to reveal useful evidence about novel concepts for\nsolving the instructed task. Together, these abilities allow the agent to\ngeneralise to subsequent tasks using newly acquired knowledge. We demonstrate\nthat learning to solve rearrangement under unawareness is more data efficient\nwhen the agent is semantics-aware -- that is, during both learning and\ninference it augments the evidence from the user's embodied conversation with\nits logical consequences, stemming from semantic analysis.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.RO",
    "comment": "22 pages,4 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.17755v2",
    "published_date": "2024-09-26 11:40:07 UTC",
    "updated_date": "2025-02-10 18:39:13 UTC"
  },
  {
    "arxiv_id": "2409.17754v1",
    "title": "Byzantine-Robust Aggregation for Securing Decentralized Federated Learning",
    "authors": [
      "Diego Cajaraville-Aboy",
      "Ana FernÃ¡ndez-Vilas",
      "Rebeca P. DÃ­az-Redondo",
      "Manuel FernÃ¡ndez-Veiga"
    ],
    "abstract": "Federated Learning (FL) emerges as a distributed machine learning approach\nthat addresses privacy concerns by training AI models locally on devices.\nDecentralized Federated Learning (DFL) extends the FL paradigm by eliminating\nthe central server, thereby enhancing scalability and robustness through the\navoidance of a single point of failure. However, DFL faces significant\nchallenges in optimizing security, as most Byzantine-robust algorithms proposed\nin the literature are designed for centralized scenarios. In this paper, we\npresent a novel Byzantine-robust aggregation algorithm to enhance the security\nof Decentralized Federated Learning environments, coined WFAgg. This proposal\nhandles the adverse conditions and strength robustness of dynamic decentralized\ntopologies at the same time by employing multiple filters to identify and\nmitigate Byzantine attacks. Experimental results demonstrate the effectiveness\nof the proposed algorithm in maintaining model accuracy and convergence in the\npresence of various Byzantine attack scenarios, outperforming state-of-the-art\ncentralized Byzantine-robust aggregation schemes (such as Multi-Krum or\nClustering). These algorithms are evaluated on an IID image classification\nproblem in both centralized and decentralized scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2409.17754v1",
    "published_date": "2024-09-26 11:36:08 UTC",
    "updated_date": "2024-09-26 11:36:08 UTC"
  },
  {
    "arxiv_id": "2409.17728v1",
    "title": "AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking",
    "authors": [
      "Shiqi Sun",
      "Yantao Lu",
      "Ning Liu",
      "Bo Jiang",
      "JinChao Chen",
      "Ying Zhang"
    ],
    "abstract": "Camera-LiDAR fusion models significantly enhance perception performance in\nautonomous driving. The fusion mechanism leverages the strengths of each\nmodality while minimizing their weaknesses. Moreover, in practice, camera-LiDAR\nfusion models utilize pre-trained backbones for efficient training. However, we\nargue that directly loading single-modal pre-trained camera and LiDAR backbones\ninto camera-LiDAR fusion models introduces similar feature redundancy across\nmodalities due to the nature of the fusion mechanism. Unfortunately, existing\npruning methods are developed explicitly for single-modal models, and thus,\nthey struggle to effectively identify these specific redundant parameters in\ncamera-LiDAR fusion models. In this paper, to address the issue above on\ncamera-LiDAR fusion models, we propose a novelty pruning framework Alternative\nModality Masking Pruning (AlterMOMA), which employs alternative masking on each\nmodality and identifies the redundant parameters. Specifically, when one\nmodality parameters are masked (deactivated), the absence of features from the\nmasked backbone compels the model to reactivate previous redundant features of\nthe other modality backbone. Therefore, these redundant features and relevant\nredundant parameters can be identified via the reactivation process. The\nredundant parameters can be pruned by our proposed importance score evaluation\nfunction, Alternative Evaluation (AlterEva), which is based on the observation\nof the loss changes when certain modality parameters are activated and\ndeactivated. Extensive experiments on the nuScene and KITTI datasets\nencompassing diverse tasks, baseline models, and pruning algorithms showcase\nthat AlterMOMA outperforms existing pruning methods, attaining state-of-the-art\nperformance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 3 figures, Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17728v1",
    "published_date": "2024-09-26 10:57:02 UTC",
    "updated_date": "2024-09-26 10:57:02 UTC"
  },
  {
    "arxiv_id": "2409.17702v1",
    "title": "Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience",
    "authors": [
      "Leonard BÃ¤rmann",
      "Chad DeChant",
      "Joana Plewnia",
      "Fabian Peller-Konrad",
      "Daniel Bauer",
      "Tamim Asfour",
      "Alex Waibel"
    ],
    "abstract": "Verbalization of robot experience, i.e., summarization of and question\nanswering about a robot's past, is a crucial ability for improving human-robot\ninteraction. Previous works applied rule-based systems or fine-tuned deep\nmodels to verbalize short (several-minute-long) streams of episodic data,\nlimiting generalization and transferability. In our work, we apply large\npretrained models to tackle this task with zero or few examples, and\nspecifically focus on verbalizing life-long experiences. For this, we derive a\ntree-like data structure from episodic memory (EM), with lower levels\nrepresenting raw perception and proprioception data, and higher levels\nabstracting events to natural language concepts. Given such a hierarchical\nrepresentation built from the experience stream, we apply a large language\nmodel as an agent to interactively search the EM given a user's query,\ndynamically expanding (initially collapsed) tree nodes to find the relevant\ninformation. The approach keeps computational costs low even when scaling to\nmonths of robot experience data. We evaluate our method on simulated household\nrobot data, human egocentric videos, and real-world robot recordings,\ndemonstrating its flexibility and scalability.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Code, data and demo videos at https://hierarchical-emv.github.io",
    "pdf_url": "http://arxiv.org/pdf/2409.17702v1",
    "published_date": "2024-09-26 10:16:08 UTC",
    "updated_date": "2024-09-26 10:16:08 UTC"
  },
  {
    "arxiv_id": "2409.17699v3",
    "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks",
    "authors": [
      "Giandomenico Cornacchia",
      "Giulio Zizzo",
      "Kieran Fraser",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Mark Purcell"
    ],
    "abstract": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17699v3",
    "published_date": "2024-09-26 10:12:19 UTC",
    "updated_date": "2024-10-04 07:16:19 UTC"
  },
  {
    "arxiv_id": "2409.17698v1",
    "title": "The application of GPT-4 in grading design university students' assignment and providing feedback: An exploratory study",
    "authors": [
      "Qian Huang",
      "Thijs Willems",
      "King Wang Poon"
    ],
    "abstract": "This study aims to investigate whether GPT-4 can effectively grade\nassignments for design university students and provide useful feedback. In\ndesign education, assignments do not have a single correct answer and often\ninvolve solving an open-ended design problem. This subjective nature of design\nprojects often leads to grading problems,as grades can vary between different\nraters,for instance instructor from engineering background or architecture\nbackground. This study employs an iterative research approach in developing a\nCustom GPT with the aim of achieving more reliable results and testing whether\nit can provide design students with constructive feedback. The findings\ninclude: First,through several rounds of iterations the inter-reliability\nbetween GPT and human raters reached a level that is generally accepted by\neducators. This indicates that by providing accurate prompts to GPT,and\ncontinuously iterating to build a Custom GPT, it can be used to effectively\ngrade students' design assignments, serving as a reliable complement to human\nraters. Second, the intra-reliability of GPT's scoring at different times is\nbetween 0.65 and 0.78. This indicates that, with adequate instructions, a\nCustom GPT gives consistent results which is a precondition for grading\nstudents. As consistency and comparability are the two main rules to ensure the\nreliability of educational assessment, this study has looked at whether a\nCustom GPT can be developed that adheres to these two rules. We finish the\npaper by testing whether Custom GPT can provide students with useful feedback\nand reflecting on how educators can develop and iterate a Custom GPT to serve\nas a complementary rater.",
    "categories": [
      "cs.AI",
      "1.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.17698v1",
    "published_date": "2024-09-26 10:09:10 UTC",
    "updated_date": "2024-09-26 10:09:10 UTC"
  },
  {
    "arxiv_id": "2409.17692v3",
    "title": "MIO: A Foundation Model on Multimodal Tokens",
    "authors": [
      "Zekun Wang",
      "King Zhu",
      "Chunpu Xu",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Yibo Zhang",
      "Jiashuo Wang",
      "Ning Shi",
      "Siyu Li",
      "Yizhi Li",
      "Haoran Que",
      "Zhaoxiang Zhang",
      "Yuanxing Zhang",
      "Ge Zhang",
      "Ke Xu",
      "Jie Fu",
      "Wenhao Huang"
    ],
    "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
    "pdf_url": "http://arxiv.org/pdf/2409.17692v3",
    "published_date": "2024-09-26 09:57:16 UTC",
    "updated_date": "2025-01-13 07:41:44 UTC"
  },
  {
    "arxiv_id": "2409.17691v1",
    "title": "Efficient Bias Mitigation Without Privileged Information",
    "authors": [
      "Mateo Espinosa Zarlenga",
      "Swami Sankaranarayanan",
      "Jerone T. A. Andrews",
      "Zohreh Shams",
      "Mateja Jamnik",
      "Alice Xiang"
    ],
    "abstract": "Deep neural networks trained via empirical risk minimisation often exhibit\nsignificant performance disparities across groups, particularly when group and\ntask labels are spuriously correlated (e.g., \"grassy background\" and \"cows\").\nExisting bias mitigation methods that aim to address this issue often either\nrely on group labels for training or validation, or require an extensive\nhyperparameter search. Such data and computational requirements hinder the\npractical deployment of these methods, especially when datasets are too large\nto be group-annotated, computational resources are limited, and models are\ntrained through already complex pipelines. In this paper, we propose Targeted\nAugmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework\nthat leverages the entire training history of a helper model to identify\nspurious samples, and generate a group-balanced training set from which a\nrobust model can be trained. We show that TAB improves worst-group performance\nwithout any group information or model selection, outperforming existing\nmethods while maintaining overall accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 18th European Conference on Computer Vision (ECCV\n  2024) as an Oral presentation",
    "pdf_url": "http://arxiv.org/pdf/2409.17691v1",
    "published_date": "2024-09-26 09:56:13 UTC",
    "updated_date": "2024-09-26 09:56:13 UTC"
  },
  {
    "arxiv_id": "2409.17687v2",
    "title": "Graph Edit Distance with General Costs Using Neural Set Divergence",
    "authors": [
      "Eeshaan Jain",
      "Indradyumna Roy",
      "Saswat Meher",
      "Soumen Chakrabarti",
      "Abir De"
    ],
    "abstract": "Graph Edit Distance (GED) measures the (dis-)similarity between two given\ngraphs, in terms of the minimum-cost edit sequence that transforms one graph to\nthe other. However, the exact computation of GED is NP-Hard, which has recently\nmotivated the design of neural methods for GED estimation. However, they do not\nexplicitly account for edit operations with different costs. In response, we\npropose GRAPHEDX, a neural GED estimator that can work with general costs\nspecified for the four edit operations, viz., edge deletion, edge addition,\nnode deletion and node addition. We first present GED as a quadratic assignment\nproblem (QAP) that incorporates these four costs. Then, we represent each graph\nas a set of node and edge embeddings and use them to design a family of neural\nset divergence surrogates. We replace the QAP terms corresponding to each\noperation with their surrogates. Computing such neural set divergence require\naligning nodes and edges of the two graphs. We learn these alignments using a\nGumbel-Sinkhorn permutation generator, additionally ensuring that the node and\nedge alignments are consistent with each other. Moreover, these alignments are\ncognizant of both the presence and absence of edges between node-pairs.\nExperiments on several datasets, under a variety of edit cost settings, show\nthat GRAPHEDX consistently outperforms state-of-the-art methods and heuristics\nin terms of prediction error.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17687v2",
    "published_date": "2024-09-26 09:51:29 UTC",
    "updated_date": "2024-11-04 11:23:46 UTC"
  },
  {
    "arxiv_id": "2409.17685v2",
    "title": "Feature-to-Image Data Augmentation: Improving Model Feature Extraction with Cluster-Guided Synthetic Samples",
    "authors": [
      "Yasaman Haghbin",
      "Hadi Moradi",
      "Reshad Hosseini"
    ],
    "abstract": "One of the growing trends in machine learning is the use of data generation\ntechniques, since the performance of machine learning models is dependent on\nthe quantity of the training dataset. However, in many real-world applications,\nparticularly in medical and low-resource domains, collecting large datasets is\nchallenging due to resource constraints, which leads to overfitting and poor\ngeneralization. This study introduces FICAug, a novel feature-to-image data\naugmentation framework designed to improve model generalization under limited\ndata conditions by generating structured synthetic samples.\n  FICAug first operates in the feature space, where original data are clustered\nusing the k-means algorithm. Within pure-label clusters, synthetic data are\ngenerated through Gaussian sampling to increase diversity while maintaining\nlabel consistency. These synthetic features are then projected back into the\nimage domain using a generative neural network, and a convolutional neural\nnetwork is trained on the reconstructed images to learn enhanced\nrepresentations.\n  Experimental results demonstrate that FICAug significantly improves\nclassification accuracy. In feature space, it achieved a cross-validation\naccuracy of 84.09%, while training a ResNet-18 model on the reconstructed\nimages further boosted performance to 88.63%, illustrating the effectiveness of\nthe proposed framework in extracting new and task-relevant features.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 6 figures, 6 table",
    "pdf_url": "http://arxiv.org/pdf/2409.17685v2",
    "published_date": "2024-09-26 09:51:08 UTC",
    "updated_date": "2025-04-24 06:08:49 UTC"
  },
  {
    "arxiv_id": "2409.17684v1",
    "title": "Preserving logical and functional dependencies in synthetic tabular data",
    "authors": [
      "Chaithra Umesh",
      "Kristian Schultz",
      "Manjunath Mahendra",
      "Saparshi Bej",
      "Olaf Wolkenhauer"
    ],
    "abstract": "Dependencies among attributes are a common aspect of tabular data. However,\nwhether existing tabular data generation algorithms preserve these dependencies\nwhile generating synthetic data is yet to be explored. In addition to the\nexisting notion of functional dependencies, we introduce the notion of logical\ndependencies among the attributes in this article. Moreover, we provide a\nmeasure to quantify logical dependencies among attributes in tabular data.\nUtilizing this measure, we compare several state-of-the-art synthetic data\ngeneration algorithms and test their capability to preserve logical and\nfunctional dependencies on several publicly available datasets. We demonstrate\nthat currently available synthetic tabular data generation algorithms do not\nfully preserve functional dependencies when they generate synthetic datasets.\nIn addition, we also showed that some tabular synthetic data generation models\ncan preserve inter-attribute logical dependencies. Our review and comparison of\nthe state-of-the-art reveal research needs and opportunities to develop\ntask-specific synthetic tabular data generation models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to Pattern Recognition Journal",
    "pdf_url": "http://arxiv.org/pdf/2409.17684v1",
    "published_date": "2024-09-26 09:51:07 UTC",
    "updated_date": "2024-09-26 09:51:07 UTC"
  },
  {
    "arxiv_id": "2409.17683v1",
    "title": "Zero- and Few-shot Named Entity Recognition and Text Expansion in Medication Prescriptions using ChatGPT",
    "authors": [
      "Natthanaphop Isaradech",
      "Andrea Riedel",
      "Wachiranun Sirikul",
      "Markus Kreuzthaler",
      "Stefan Schulz"
    ],
    "abstract": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17683v1",
    "published_date": "2024-09-26 09:49:27 UTC",
    "updated_date": "2024-09-26 09:49:27 UTC"
  },
  {
    "arxiv_id": "2409.17663v3",
    "title": "Explanation Bottleneck Models",
    "authors": [
      "Shin'ya Yamaguchi",
      "Kosuke Nishida"
    ],
    "abstract": "Recent concept-based interpretable models have succeeded in providing\nmeaningful explanations by pre-defined concept sets. However, the dependency on\nthe pre-defined concepts restricts the application because of the limited\nnumber of concepts for explanations. This paper proposes a novel interpretable\ndeep neural network called explanation bottleneck models (XBMs). XBMs generate\na text explanation from the input without pre-defined concepts and then predict\na final task prediction based on the generated explanation by leveraging\npre-trained vision-language encoder-decoder models. To achieve both the target\ntask performance and the explanation quality, we train XBMs through the target\ntask loss with the regularization penalizing the explanation decoder via the\ndistillation from the frozen pre-trained decoder. Our experiments, including a\ncomparison to state-of-the-art concept bottleneck models, confirm that XBMs\nprovide accurate and fluent natural language explanations without pre-defined\nconcept sets. Code is available at https://github.com/yshinya6/xbm/.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to AAAI 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2409.17663v3",
    "published_date": "2024-09-26 09:21:48 UTC",
    "updated_date": "2025-02-18 09:01:21 UTC"
  },
  {
    "arxiv_id": "2409.17661v2",
    "title": "A Fuzzy-based Approach to Predict Human Interaction by Functional Near-Infrared Spectroscopy",
    "authors": [
      "Xiaowei Jiang",
      "Liang Ou",
      "Yanan Chen",
      "Na Ao",
      "Yu-Cheng Chang",
      "Thomas Do",
      "Chin-Teng Lin"
    ],
    "abstract": "The paper introduces a Fuzzy-based Attention (Fuzzy Attention Layer)\nmechanism, a novel computational approach to enhance the interpretability and\nefficacy of neural models in psychological research. The proposed Fuzzy\nAttention Layer mechanism is integrated as a neural network layer within the\nTransformer Encoder model to facilitate the analysis of complex psychological\nphenomena through neural signals, such as those captured by functional\nNear-Infrared Spectroscopy (fNIRS). By leveraging fuzzy logic, the Fuzzy\nAttention Layer is capable of learning and identifying interpretable patterns\nof neural activity. This capability addresses a significant challenge when\nusing Transformer: the lack of transparency in determining which specific brain\nactivities most contribute to particular predictions. Our experimental results\ndemonstrated on fNIRS data from subjects engaged in social interactions\ninvolving handholding reveal that the Fuzzy Attention Layer not only learns\ninterpretable patterns of neural activity but also enhances model performance.\nAdditionally, the learned patterns provide deeper insights into the neural\ncorrelates of interpersonal touch and emotional exchange. The application of\nour model shows promising potential in deciphering the subtle complexities of\nhuman social behaviors, thereby contributing significantly to the fields of\nsocial neuroscience and psychological AI.",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17661v2",
    "published_date": "2024-09-26 09:20:12 UTC",
    "updated_date": "2025-01-23 23:18:13 UTC"
  },
  {
    "arxiv_id": "2409.17659v1",
    "title": "Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning",
    "authors": [
      "Siyi Lu",
      "Lei He",
      "Shengbo Eben Li",
      "Yugong Luo",
      "Jianqiang Wang",
      "Keqiang Li"
    ],
    "abstract": "End-to-end autonomous driving offers a streamlined alternative to the\ntraditional modular pipeline, integrating perception, prediction, and planning\nwithin a single framework. While Deep Reinforcement Learning (DRL) has recently\ngained traction in this domain, existing approaches often overlook the critical\nconnection between feature extraction of DRL and perception. In this paper, we\nbridge this gap by mapping the DRL feature extraction network directly to the\nperception phase, enabling clearer interpretation through semantic\nsegmentation. By leveraging Bird's-Eye-View (BEV) representations, we propose a\nnovel DRL-based end-to-end driving framework that utilizes multi-sensor inputs\nto construct a unified three-dimensional understanding of the environment. This\nBEV-based system extracts and translates critical environmental features into\nhigh-level abstract states for DRL, facilitating more informed control.\nExtensive experimental evaluations demonstrate that our approach not only\nenhances interpretability but also significantly outperforms state-of-the-art\nmethods in autonomous driving control tasks, reducing the collision rate by\n20%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17659v1",
    "published_date": "2024-09-26 09:14:16 UTC",
    "updated_date": "2024-09-26 09:14:16 UTC"
  },
  {
    "arxiv_id": "2409.17656v1",
    "title": "Prototype based Masked Audio Model for Self-Supervised Learning of Sound Event Detection",
    "authors": [
      "Pengfei Cai",
      "Yan Song",
      "Nan Jiang",
      "Qing Gu",
      "Ian McLoughlin"
    ],
    "abstract": "A significant challenge in sound event detection (SED) is the effective\nutilization of unlabeled data, given the limited availability of labeled data\ndue to high annotation costs. Semi-supervised algorithms rely on labeled data\nto learn from unlabeled data, and the performance is constrained by the quality\nand size of the former. In this paper, we introduce the Prototype based Masked\nAudio Model~(PMAM) algorithm for self-supervised representation learning in\nSED, to better exploit unlabeled data. Specifically, semantically rich\nframe-level pseudo labels are constructed from a Gaussian mixture model (GMM)\nbased prototypical distribution modeling. These pseudo labels supervise the\nlearning of a Transformer-based masked audio model, in which binary\ncross-entropy loss is employed instead of the widely used InfoNCE loss, to\nprovide independent loss contributions from different prototypes, which is\nimportant in real scenarios in which multiple labels may apply to unsupervised\ndata frames. A final stage of fine-tuning with just a small amount of labeled\ndata yields a very high performing SED model. On like-for-like tests using the\nDESED task, our method achieves a PSDS1 score of 62.5\\%, surpassing current\nstate-of-the-art models and demonstrating the superiority of the proposed\ntechnique.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to ICASSP2025; The code for this paper will be available at\n  https://github.com/cai525/Transformer4SED after the paper is accepted",
    "pdf_url": "http://arxiv.org/pdf/2409.17656v1",
    "published_date": "2024-09-26 09:07:20 UTC",
    "updated_date": "2024-09-26 09:07:20 UTC"
  },
  {
    "arxiv_id": "2409.17655v1",
    "title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment",
    "authors": [
      "Nan Sun",
      "Bo Mao",
      "Yongchang Li",
      "Lumeng Ma",
      "Di Guo",
      "Huaping Liu"
    ],
    "abstract": "The increasing demand for intelligent assistants in human-populated\nenvironments has motivated significant research in autonomous robotic systems.\nTraditional service robots and virtual assistants, however, struggle with\nreal-world task execution due to their limited capacity for dynamic reasoning\nand interaction, particularly when human collaboration is required. Recent\ndevelopments in Large Language Models have opened new avenues for improving\nthese systems, enabling more sophisticated reasoning and natural interaction\ncapabilities. In this paper, we introduce AssistantX, an LLM-powered proactive\nassistant designed to operate autonomously in a physical office environment.\nUnlike conventional service robots, AssistantX leverages a novel multi-agent\narchitecture, PPDR4X, which provides advanced inference capabilities and\ncomprehensive collaboration awareness. By effectively bridging the gap between\nvirtual operations and physical interactions, AssistantX demonstrates robust\nperformance in managing complex real-world scenarios. Our evaluation highlights\nthe architecture's effectiveness, showing that AssistantX can respond to clear\ninstructions, actively retrieve supplementary information from memory, and\nproactively seek collaboration from team members to ensure successful task\ncompletion. More details and videos can be found at\nhttps://assistantx-agent.github.io/AssistantX/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 8 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.17655v1",
    "published_date": "2024-09-26 09:06:56 UTC",
    "updated_date": "2024-09-26 09:06:56 UTC"
  },
  {
    "arxiv_id": "2409.17652v2",
    "title": "FactorSim: Generative Simulation via Factorized Representation",
    "authors": [
      "Fan-Yun Sun",
      "S. I. Harini",
      "Angela Yi",
      "Yihan Zhou",
      "Alex Zook",
      "Jonathan Tremblay",
      "Logan Cross",
      "Jiajun Wu",
      "Nick Haber"
    ],
    "abstract": "Generating simulations to train intelligent agents in game-playing and\nrobotics from natural language input, from user input or task documentation,\nremains an open-ended challenge. Existing approaches focus on parts of this\nchallenge, such as generating reward functions or task hyperparameters. Unlike\nprevious work, we introduce FACTORSIM that generates full simulations in code\nfrom language input that can be used to train agents. Exploiting the structural\nmodularity specific to coded simulations, we propose to use a factored\npartially observable Markov decision process representation that allows us to\nreduce context dependence during each step of the generation. For evaluation,\nwe introduce a generative simulation benchmark that assesses the generated\nsimulation code's accuracy and effectiveness in facilitating zero-shot\ntransfers in reinforcement learning settings. We show that FACTORSIM\noutperforms existing methods in generating simulations regarding prompt\nalignment (e.g., accuracy), zero-shot transfer abilities, and human evaluation.\nWe also demonstrate its effectiveness in generating robotic tasks.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "neurips 2024, project website:\n  https://cs.stanford.edu/~sunfanyun/factorsim/",
    "pdf_url": "http://arxiv.org/pdf/2409.17652v2",
    "published_date": "2024-09-26 09:00:30 UTC",
    "updated_date": "2024-11-11 08:16:40 UTC"
  },
  {
    "arxiv_id": "2409.17650v1",
    "title": "Digital Twin Ecosystem for Oncology Clinical Operations",
    "authors": [
      "Himanshu Pandey",
      "Akhil Amod",
      "Shivang",
      "Kshitij Jaggi",
      "Ruchi Garg",
      "Abheet Jain",
      "Vinayak Tantia"
    ],
    "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre Print",
    "pdf_url": "http://arxiv.org/pdf/2409.17650v1",
    "published_date": "2024-09-26 08:56:54 UTC",
    "updated_date": "2024-09-26 08:56:54 UTC"
  },
  {
    "arxiv_id": "2409.17642v2",
    "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic Self-Disclosure",
    "authors": [
      "Xi Chen",
      "Zhiyang Zhang",
      "Fangkai Yang",
      "Xiaoting Qin",
      "Chao Du",
      "Xi Cheng",
      "Hangxin Liu",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "abstract": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17642v2",
    "published_date": "2024-09-26 08:45:15 UTC",
    "updated_date": "2024-10-07 06:29:54 UTC"
  },
  {
    "arxiv_id": "2409.17640v3",
    "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training on an Assistant Task for a Target Task",
    "authors": [
      "Xindi Tong",
      "Yujin Zhu",
      "Shijian Fan",
      "Liang Xu"
    ],
    "abstract": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17640v3",
    "published_date": "2024-09-26 08:44:38 UTC",
    "updated_date": "2025-01-22 07:16:32 UTC"
  },
  {
    "arxiv_id": "2409.17634v1",
    "title": "P4Q: Learning to Prompt for Quantization in Visual-language Models",
    "authors": [
      "Huixin Sun",
      "Runqi Wang",
      "Yanjing Li",
      "Xianbin Cao",
      "Xiaolong Jiang",
      "Yao Hu",
      "Baochang Zhang"
    ],
    "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence\nin various visual and multimodal tasks, yet the deployment of VLMs on\ndownstream application platforms remains challenging due to their prohibitive\nrequirements of training samples and computing resources. Fine-tuning and\nquantization of VLMs can substantially reduce the sample and computation costs,\nwhich are in urgent need. There are two prevailing paradigms in quantization,\nQuantization-Aware Training (QAT) can effectively quantize large-scale VLMs but\nincur a huge training cost, while low-bit Post-Training Quantization (PTQ)\nsuffers from a notable performance drop. We propose a method that balances\nfine-tuning and quantization named ``Prompt for Quantization'' (P4Q), in which\nwe design a lightweight architecture to leverage contrastive loss supervision\nto enhance the recognition performance of a PTQ model. Our method can\neffectively reduce the gap between image features and text features caused by\nlow-bit quantization, based on learnable prompts to reorganize textual\nrepresentations and a low-bit adapter to realign the distributions of image and\ntext features. We also introduce a distillation loss based on cosine similarity\npredictions to distill the quantized model using a full-precision teacher.\nExtensive experimental results demonstrate that our P4Q method outperforms\nprior arts, even achieving comparable results to its full-precision\ncounterparts. For instance, our 8-bit P4Q can theoretically compress the\nCLIP-ViT/B-32 by 4 $\\times$ while achieving 66.94\\% Top-1 accuracy,\noutperforming the learnable prompt fine-tuned full-precision model by 2.24\\%\nwith negligible additional parameters on the ImageNet dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17634v1",
    "published_date": "2024-09-26 08:31:27 UTC",
    "updated_date": "2024-09-26 08:31:27 UTC"
  },
  {
    "arxiv_id": "2409.17629v1",
    "title": "Hand-object reconstruction via interaction-aware graph attention mechanism",
    "authors": [
      "Taeyun Woo",
      "Tae-Kyun Kim",
      "Jinah Park"
    ],
    "abstract": "Estimating the poses of both a hand and an object has become an important\narea of research due to the growing need for advanced vision computing. The\nprimary challenge involves understanding and reconstructing how hands and\nobjects interact, such as contact and physical plausibility. Existing\napproaches often adopt a graph neural network to incorporate spatial\ninformation of hand and object meshes. However, these approaches have not fully\nexploited the potential of graphs without modification of edges within and\nbetween hand- and object-graphs. We propose a graph-based refinement method\nthat incorporates an interaction-aware graph-attention mechanism to account for\nhand-object interactions. Using edges, we establish connections among closely\ncorrelated nodes, both within individual graphs and across different graphs.\nExperiments demonstrate the effectiveness of our proposed method with notable\nimprovements in the realm of physical plausibility.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, Accepted by ICIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17629v1",
    "published_date": "2024-09-26 08:23:04 UTC",
    "updated_date": "2024-09-26 08:23:04 UTC"
  },
  {
    "arxiv_id": "2409.17622v1",
    "title": "Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs",
    "authors": [
      "Yusong Wang",
      "Chaoran Cheng",
      "Shaoning Li",
      "Yuxuan Ren",
      "Bin Shao",
      "Ge Liu",
      "Pheng-Ann Heng",
      "Nanning Zheng"
    ],
    "abstract": "Geometric graph neural networks (GNNs) have emerged as powerful tools for\nmodeling molecular geometry. However, they encounter limitations in effectively\ncapturing long-range interactions in large molecular systems. To address this\nchallenge, we introduce Neural P$^3$M, a versatile enhancer of geometric GNNs\nto expand the scope of their capabilities by incorporating mesh points\nalongside atoms and reimaging traditional mathematical operations in a\ntrainable manner. Neural P$^3$M exhibits flexibility across a wide range of\nmolecular systems and demonstrates remarkable accuracy in predicting energies\nand forces, outperforming on benchmarks such as the MD22 dataset. It also\nachieves an average improvement of 22% on the OE62 dataset while integrating\nwith various architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17622v1",
    "published_date": "2024-09-26 08:16:59 UTC",
    "updated_date": "2024-09-26 08:16:59 UTC"
  },
  {
    "arxiv_id": "2410.08224v1",
    "title": "A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications",
    "authors": [
      "Pengfei Wang",
      "Huanran Zheng",
      "Silong Dai",
      "Yiqiao Wang",
      "Xiaotian Gu",
      "Yuanbin Wu",
      "Xiaoling Wang"
    ],
    "abstract": "In recent years, the field of electroencephalography (EEG) analysis has\nwitnessed remarkable advancements, driven by the integration of machine\nlearning and artificial intelligence. This survey aims to encapsulate the\nlatest developments, focusing on emerging methods and technologies that are\npoised to transform our comprehension and interpretation of brain activity. We\ndelve into self-supervised learning methods that enable the robust\nrepresentation of brain signals, which are fundamental for a variety of\ndownstream applications. We also explore emerging discriminative methods,\nincluding graph neural networks (GNN), foundation models, and large language\nmodels (LLMs)-based approaches. Furthermore, we examine generative technologies\nthat harness EEG data to produce images or text, offering novel perspectives on\nbrain activity visualization and interpretation. The survey provides an\nextensive overview of these cutting-edge techniques, their current\napplications, and the profound implications they hold for future research and\nclinical practice. The relevant literature and open-source materials have been\ncompiled and are consistently being refreshed at\n\\url{https://github.com/wpf535236337/LLMs4TS}",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "submitted to IECE Chinese Journal of Information Fusion",
    "pdf_url": "http://arxiv.org/pdf/2410.08224v1",
    "published_date": "2024-09-26 08:09:15 UTC",
    "updated_date": "2024-09-26 08:09:15 UTC"
  },
  {
    "arxiv_id": "2409.17607v1",
    "title": "Dirichlet-Based Coarse-to-Fine Example Selection For Open-Set Annotation",
    "authors": [
      "Ye-Wen Wang",
      "Chen-Chen Zong",
      "Ming-Kun Xie",
      "Sheng-Jun Huang"
    ],
    "abstract": "Active learning (AL) has achieved great success by selecting the most\nvaluable examples from unlabeled data. However, they usually deteriorate in\nreal scenarios where open-set noise gets involved, which is studied as open-set\nannotation (OSA). In this paper, we owe the deterioration to the unreliable\npredictions arising from softmax-based translation invariance and propose a\nDirichlet-based Coarse-to-Fine Example Selection (DCFS) strategy accordingly.\nOur method introduces simplex-based evidential deep learning (EDL) to break\ntranslation invariance and distinguish known and unknown classes by considering\nevidence-based data and distribution uncertainty simultaneously. Furthermore,\nhard known-class examples are identified by model discrepancy generated from\ntwo classifier heads, where we amplify and alleviate the model discrepancy\nrespectively for unknown and known classes. Finally, we combine the discrepancy\nwith uncertainties to form a two-stage strategy, selecting the most informative\nexamples from known classes. Extensive experiments on various openness ratio\ndatasets demonstrate that DCFS achieves state-of-art performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17607v1",
    "published_date": "2024-09-26 07:47:50 UTC",
    "updated_date": "2024-09-26 07:47:50 UTC"
  },
  {
    "arxiv_id": "2409.17602v1",
    "title": "Open Digital Rights Enforcement Framework (ODRE): from descriptive to enforceable policies",
    "authors": [
      "Andrea Cimmino",
      "Juan Cano-Benito",
      "RaÃºl GarcÃ­a-Castro"
    ],
    "abstract": "From centralised platforms to decentralised ecosystems, like Data Spaces,\nsharing data has become a paramount challenge. For this reason, the definition\nof data usage policies has become crucial in these domains, highlighting the\nnecessity of effective policy enforcement mechanisms. The Open Digital Rights\nLanguage (ODRL) is a W3C standard ontology designed to describe data usage\npolicies, however, it lacks built-in enforcement capabilities, limiting its\npractical application. This paper introduces the Open Digital Rights\nEnforcement (ODRE) framework, whose goal is to provide ODRL with enforcement\ncapabilities. The ODRE framework proposes a novel approach to express ODRL\npolicies that integrates the descriptive ontology terms of ODRL with other\nlanguages that allow behaviour specification, such as dynamic data handling or\nfunction evaluation. The framework includes an enforcement algorithm for ODRL\npolicies and two open-source implementations in Python and Java. The ODRE\nframework is also designed to support future extensions of ODRL to specific\ndomain scenarios. In addition, current limitations of ODRE, ODRL, and current\nchallenges are reported. Finally, to demonstrate the enforcement capabilities\nof the implementations, their performance, and their extensibility features,\nseveral experiments have been carried out with positive results.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "20 pages, 3 Figures, Submitted to Computers & Security journal",
    "pdf_url": "http://arxiv.org/pdf/2409.17602v1",
    "published_date": "2024-09-26 07:36:49 UTC",
    "updated_date": "2024-09-26 07:36:49 UTC"
  },
  {
    "arxiv_id": "2409.17601v3",
    "title": "CleanerCLIP: Fine-grained Counterfactual Semantic Augmentation for Backdoor Defense in Contrastive Learning",
    "authors": [
      "Yuan Xun",
      "Siyuan Liang",
      "Xiaojun Jia",
      "Xinwei Liu",
      "Xiaochun Cao"
    ],
    "abstract": "Pre-trained large models for multimodal contrastive learning, such as CLIP,\nhave been widely recognized in the industry as highly susceptible to\ndata-poisoned backdoor attacks. This poses significant risks to downstream\nmodel training. In response to such potential threats, finetuning offers a\nsimpler and more efficient defense choice compared to retraining large models\nwith augmented data. In the supervised learning domain, fine-tuning defense\nstrategies can achieve excellent defense performance. However, in the\nunsupervised and semi-supervised domain, we find that when CLIP faces some\ncomplex attack techniques, the existing fine-tuning defense strategy,\nCleanCLIP, has some limitations on defense performance. The synonym\nsubstitution of its text-augmentation is insufficient to enhance the text\nfeature space. To compensate for this weakness, we improve it by proposing a\nfine-grained \\textbf{T}ext \\textbf{A}lignment \\textbf{C}leaner (TA-Cleaner) to\ncut off feature connections of backdoor triggers. We randomly select a few\nsamples for positive and negative subtext generation at each epoch of\nCleanCLIP, and align the subtexts to the images to strengthen the text\nself-supervision. We evaluate the effectiveness of our TA-Cleaner against six\nattack algorithms and conduct comprehensive zero-shot classification tests on\nImageNet1K. Our experimental results demonstrate that TA-Cleaner achieves\nstate-of-the-art defensiveness among finetuning-based defense techniques. Even\nwhen faced with the novel attack technique BadCLIP, our TA-Cleaner outperforms\nCleanCLIP by reducing the ASR of Top-1 and Top-10 by 52.02\\% and 63.88\\%,\nrespectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17601v3",
    "published_date": "2024-09-26 07:35:23 UTC",
    "updated_date": "2024-11-15 02:56:58 UTC"
  },
  {
    "arxiv_id": "2409.17596v1",
    "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming",
    "authors": [
      "Zehao Zhu",
      "Wei Sun",
      "Jun Jia",
      "Wei Wu",
      "Sibin Deng",
      "Kai Li",
      "Ying Chen",
      "Xiongkuo Min",
      "Jia Wang",
      "Guangtao Zhai"
    ],
    "abstract": "In recent years, live video streaming has gained widespread popularity across\nvarious social media platforms. Quality of experience (QoE), which reflects\nend-users' satisfaction and overall experience, plays a critical role for media\nservice providers to optimize large-scale live compression and transmission\nstrategies to achieve perceptually optimal rate-distortion trade-off. Although\nmany QoE metrics for video-on-demand (VoD) have been proposed, there remain\nsignificant challenges in developing QoE metrics for live video streaming. To\nbridge this gap, we conduct a comprehensive study of subjective and objective\nQoE evaluations for live video streaming. For the subjective QoE study, we\nintroduce the first live video streaming QoE dataset, TaoLive QoE, which\nconsists of $42$ source videos collected from real live broadcasts and $1,155$\ncorresponding distorted ones degraded due to a variety of streaming\ndistortions, including conventional streaming distortions such as compression,\nstalling, as well as live streaming-specific distortions like frame skipping,\nvariable frame rate, etc. Subsequently, a human study was conducted to derive\nsubjective QoE scores of videos in the TaoLive QoE dataset. For the objective\nQoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well\nas publicly available QoE datasets for VoD scenarios, highlighting that current\nmodels struggle to accurately assess video QoE, particularly for live content.\nHence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates\nmulti-scale semantic features and optical flow-based motion features to\npredicting a retrospective QoE score, eliminating reliance on statistical\nquality of service (QoS) features.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.MM",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.17596v1",
    "published_date": "2024-09-26 07:22:38 UTC",
    "updated_date": "2024-09-26 07:22:38 UTC"
  },
  {
    "arxiv_id": "2409.17592v1",
    "title": "Deep Manifold Part 1: Anatomy of Neural Network Manifold",
    "authors": [
      "Max Y. Ma",
      "Gen-Hua Shi"
    ],
    "abstract": "Based on the numerical manifold method principle, we developed a mathematical\nframework of a neural network manifold: Deep Manifold and discovered that\nneural networks: 1) is numerical computation combining forward and inverse; 2)\nhave near infinite degrees of freedom; 3) exponential learning capacity with\ndepth; 4) have self-progressing boundary conditions; 5) has training hidden\nbottleneck. We also define two concepts: neural network learning space and deep\nmanifold space and introduce two concepts: neural network intrinsic pathway and\nfixed point. We raise three fundamental questions: 1). What is the training\ncompletion definition; 2). where is the deep learning convergence point (neural\nnetwork fixed point); 3). How important is token timestamp in training data\ngiven negative time is critical in inverse problem.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17592v1",
    "published_date": "2024-09-26 07:19:12 UTC",
    "updated_date": "2024-09-26 07:19:12 UTC"
  },
  {
    "arxiv_id": "2409.17589v1",
    "title": "Improving Fast Adversarial Training via Self-Knowledge Guidance",
    "authors": [
      "Chengze Jiang",
      "Junkai Wang",
      "Minjing Dong",
      "Jie Gui",
      "Xinli Shi",
      "Yuan Cao",
      "Yuan Yan Tang",
      "James Tin-Yau Kwok"
    ],
    "abstract": "Adversarial training has achieved remarkable advancements in defending\nagainst adversarial attacks. Among them, fast adversarial training (FAT) is\ngaining attention for its ability to achieve competitive robustness with fewer\ncomputing resources. Existing FAT methods typically employ a uniform strategy\nthat optimizes all training data equally without considering the influence of\ndifferent examples, which leads to an imbalanced optimization. However, this\nimbalance remains unexplored in the field of FAT. In this paper, we conduct a\ncomprehensive study of the imbalance issue in FAT and observe an obvious class\ndisparity regarding their performances. This disparity could be embodied from a\nperspective of alignment between clean and robust accuracy. Based on the\nanalysis, we mainly attribute the observed misalignment and disparity to the\nimbalanced optimization in FAT, which motivates us to optimize different\ntraining data adaptively to enhance robustness. Specifically, we take disparity\nand misalignment into consideration. First, we introduce self-knowledge guided\nregularization, which assigns differentiated regularization weights to each\nclass based on its training state, alleviating class disparity. Additionally,\nwe propose self-knowledge guided label relaxation, which adjusts label\nrelaxation according to the training accuracy, alleviating the misalignment and\nimproving robustness. By combining these methods, we formulate the\nSelf-Knowledge Guided FAT (SKG-FAT), leveraging naturally generated knowledge\nduring training to enhance the adversarial robustness without compromising\ntraining efficiency. Extensive experiments on four standard datasets\ndemonstrate that the SKG-FAT improves the robustness and preserves competitive\nclean accuracy, outperforming the state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17589v1",
    "published_date": "2024-09-26 07:12:04 UTC",
    "updated_date": "2024-09-26 07:12:04 UTC"
  },
  {
    "arxiv_id": "2409.17587v1",
    "title": "Multimodal Banking Dataset: Understanding Client Needs through Event Sequences",
    "authors": [
      "Mollaev Dzhambulat",
      "Alexander Kostin",
      "Postnova Maria",
      "Ivan Karpukhin",
      "Ivan A Kireev",
      "Gleb Gusev",
      "Andrey Savchenko"
    ],
    "abstract": "Financial organizations collect a huge amount of data about clients that\ntypically has a temporal (sequential) structure and is collected from various\nsources (modalities). Due to privacy issues, there are no large-scale\nopen-source multimodal datasets of event sequences, which significantly limits\nthe research in this area. In this paper, we present the industrial-scale\npublicly available multimodal banking dataset, MBD, that contains more than\n1.5M corporate clients with several modalities: 950M bank transactions, 1B geo\nposition events, 5M embeddings of dialogues with technical support and monthly\naggregated purchases of four bank's products. All entries are properly\nanonymized from real proprietary bank data. Using this dataset, we introduce a\nnovel benchmark with two business tasks: campaigning (purchase prediction in\nthe next month) and matching of clients. We provide numerical results that\ndemonstrate the superiority of our multi-modal baselines over single-modal\ntechniques for each task. As a result, the proposed dataset can open new\nperspectives and facilitate the future development of practically important\nlarge-scale multimodal algorithms for event sequences.\n  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD\n  Github Link: https://github.com/Dzhambo/MBD",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17587v1",
    "published_date": "2024-09-26 07:07:08 UTC",
    "updated_date": "2024-09-26 07:07:08 UTC"
  },
  {
    "arxiv_id": "2409.17583v1",
    "title": "Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components",
    "authors": [
      "Peiyong Wang",
      "Casey. R. Myers",
      "Lloyd C. L. Hollenberg",
      "Udaya Parampalli"
    ],
    "abstract": "Artificial Intelligence (AI), with its multiplier effect and wide\napplications in multiple areas, could potentially be an important application\nof quantum computing. Since modern AI systems are often built on neural\nnetworks, the design of quantum neural networks becomes a key challenge in\nintegrating quantum computing into AI. To provide a more fine-grained\ncharacterisation of the impact of quantum components on the performance of\nneural networks, we propose a framework where classical neural network layers\nare gradually replaced by quantum layers that have the same type of input and\noutput while keeping the flow of information between layers unchanged,\ndifferent from most current research in quantum neural network, which favours\nan end-to-end quantum model. We start with a simple three-layer classical\nneural network without any normalisation layers or activation functions, and\ngradually change the classical layers to the corresponding quantum versions. We\nconduct numerical experiments on image classification datasets such as the\nMNIST, FashionMNIST and CIFAR-10 datasets to demonstrate the change of\nperformance brought by the systematic introduction of quantum components.\nThrough this framework, our research sheds new light on the design of future\nquantum neural network models where it could be more favourable to search for\nmethods and frameworks that harness the advantages from both the classical and\nquantum worlds.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "50 pages (including Appendix), many figures, accepted as a poster on\n  QTML2024. Code available at\n  https://github.com/peiyong-addwater/Let-The-Quantum-Creep-In",
    "pdf_url": "http://arxiv.org/pdf/2409.17583v1",
    "published_date": "2024-09-26 07:01:29 UTC",
    "updated_date": "2024-09-26 07:01:29 UTC"
  },
  {
    "arxiv_id": "2409.17581v1",
    "title": "A Scalable Data-Driven Framework for Systematic Analysis of SEC 10-K Filings Using Large Language Models",
    "authors": [
      "Syed Affan Daimi",
      "Asma Iqbal"
    ],
    "abstract": "The number of companies listed on the NYSE has been growing exponentially,\ncreating a significant challenge for market analysts, traders, and stockholders\nwho must monitor and assess the performance and strategic shifts of a large\nnumber of companies regularly. There is an increasing need for a fast,\ncost-effective, and comprehensive method to evaluate the performance and detect\nand compare many companies' strategy changes efficiently. We propose a novel\ndata-driven approach that leverages large language models (LLMs) to\nsystematically analyze and rate the performance of companies based on their SEC\n10-K filings. These filings, which provide detailed annual reports on a\ncompany's financial performance and strategic direction, serve as a rich source\nof data for evaluating various aspects of corporate health, including\nconfidence, environmental sustainability, innovation, and workforce management.\nWe also introduce an automated system for extracting and preprocessing 10-K\nfilings. This system accurately identifies and segments the required sections\nas outlined by the SEC, while also isolating key textual content that contains\ncritical information about the company. This curated data is then fed into\nCohere's Command-R+ LLM to generate quantitative ratings across various\nperformance metrics. These ratings are subsequently processed and visualized to\nprovide actionable insights. The proposed scheme is then implemented on an\ninteractive GUI as a no-code solution for running the data pipeline and\ncreating the visualizations. The application showcases the rating results and\nprovides year-on-year comparisons of company performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.17581v1",
    "published_date": "2024-09-26 06:57:22 UTC",
    "updated_date": "2024-09-26 06:57:22 UTC"
  },
  {
    "arxiv_id": "2409.17580v1",
    "title": "Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study",
    "authors": [
      "Zahra Sepasdar",
      "Sushant Gautam",
      "Cise Midoglu",
      "Michael A. Riegler",
      "PÃ¥l Halvorsen"
    ],
    "abstract": "Extracting meaningful insights from large and complex datasets poses\nsignificant challenges, particularly in ensuring the accuracy and relevance of\nretrieved information. Traditional data retrieval methods such as sequential\nsearch and index-based retrieval often fail when handling intricate and\ninterconnected data structures, resulting in incomplete or misleading outputs.\nTo overcome these limitations, we introduce Structured-GraphRAG, a versatile\nframework designed to enhance information retrieval across structured datasets\nin natural language queries. Structured-GraphRAG utilizes multiple knowledge\ngraphs, which represent data in a structured format and capture complex\nrelationships between entities, enabling a more nuanced and comprehensive\nretrieval of information. This graph-based approach reduces the risk of errors\nin language model outputs by grounding responses in a structured format,\nthereby enhancing the reliability of results. We demonstrate the effectiveness\nof Structured-GraphRAG by comparing its performance with that of a recently\npublished method using traditional retrieval-augmented generation. Our findings\nshow that Structured-GraphRAG significantly improves query processing\nefficiency and reduces response times. While our case study focuses on soccer\ndata, the framework's design is broadly applicable, offering a powerful tool\nfor data analysis and enhancing language model applications across various\nstructured domains.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DB",
      "H.2; H.3; E.1; E.2"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17580v1",
    "published_date": "2024-09-26 06:53:29 UTC",
    "updated_date": "2024-09-26 06:53:29 UTC"
  },
  {
    "arxiv_id": "2409.17572v1",
    "title": "Dr. GPT in Campus Counseling: Understanding Higher Education Students' Opinions on LLM-assisted Mental Health Services",
    "authors": [
      "Owen Xingjian Zhang",
      "Shuyao Zhou",
      "Jiayi Geng",
      "Yuhan Liu",
      "Sunny Xun Liu"
    ],
    "abstract": "In response to the increasing mental health challenges faced by college\nstudents, we sought to understand their perspectives on how AI applications,\nparticularly Large Language Models (LLMs), can be leveraged to enhance their\nmental well-being. Through pilot interviews with ten diverse students, we\nexplored their opinions on the use of LLMs across five fictional scenarios:\nGeneral Information Inquiry, Initial Screening, Reshaping Patient-Expert\nDynamics, Long-term Care, and Follow-up Care. Our findings revealed that\nstudents' acceptance of LLMs varied by scenario, with participants highlighting\nboth potential benefits, such as proactive engagement and personalized\nfollow-up care, and concerns, including limitations in training data and\nemotional support. These insights inform how AI technology should be designed\nand implemented to effectively support and enhance students' mental well-being,\nparticularly in scenarios where LLMs can complement traditional methods, while\nmaintaining empathy and respecting individual preferences.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17572v1",
    "published_date": "2024-09-26 06:40:45 UTC",
    "updated_date": "2024-09-26 06:40:45 UTC"
  },
  {
    "arxiv_id": "2409.17568v1",
    "title": "Showing Many Labels in Multi-label Classification Models: An Empirical Study of Adversarial Examples",
    "authors": [
      "Yujiang Liu",
      "Wenjian Luo",
      "Zhijian Chen",
      "Muhammad Luqman Naseem"
    ],
    "abstract": "With the rapid development of Deep Neural Networks (DNNs), they have been\napplied in numerous fields. However, research indicates that DNNs are\nsusceptible to adversarial examples, and this is equally true in the\nmulti-label domain. To further investigate multi-label adversarial examples, we\nintroduce a novel type of attacks, termed \"Showing Many Labels\". The objective\nof this attack is to maximize the number of labels included in the classifier's\nprediction results. In our experiments, we select nine attack algorithms and\nevaluate their performance under \"Showing Many Labels\". Eight of the attack\nalgorithms were adapted from the multi-class environment to the multi-label\nenvironment, while the remaining one was specifically designed for the\nmulti-label environment. We choose ML-LIW and ML-GCN as target models and train\nthem on four popular multi-label datasets: VOC2007, VOC2012, NUS-WIDE, and\nCOCO. We record the success rate of each algorithm when it shows the expected\nnumber of labels in eight different scenarios. Experimental results indicate\nthat under the \"Showing Many Labels\", iterative attacks perform significantly\nbetter than one-step attacks. Moreover, it is possible to show all labels in\nthe dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17568v1",
    "published_date": "2024-09-26 06:31:31 UTC",
    "updated_date": "2024-09-26 06:31:31 UTC"
  },
  {
    "arxiv_id": "2409.17565v1",
    "title": "Pixel-Space Post-Training of Latent Diffusion Models",
    "authors": [
      "Christina Zhang",
      "Simran Motwani",
      "Matthew Yu",
      "Ji Hou",
      "Felix Juefei-Xu",
      "Sam Tsai",
      "Peter Vajda",
      "Zijian He",
      "Jialiang Wang"
    ],
    "abstract": "Latent diffusion models (LDMs) have made significant advancements in the\nfield of image generation in recent years. One major advantage of LDMs is their\nability to operate in a compressed latent space, allowing for more efficient\ntraining and deployment. However, despite these advantages, challenges with\nLDMs still remain. For example, it has been observed that LDMs often generate\nhigh-frequency details and complex compositions imperfectly. We hypothesize\nthat one reason for these flaws is due to the fact that all pre- and\npost-training of LDMs are done in latent space, which is typically $8 \\times 8$\nlower spatial-resolution than the output images. To address this issue, we\npropose adding pixel-space supervision in the post-training process to better\npreserve high-frequency details. Experimentally, we show that adding a\npixel-space objective significantly improves both supervised quality\nfine-tuning and preference-based post-training by a large margin on a\nstate-of-the-art DiT transformer and U-Net diffusion models in both visual\nquality and visual flaw metrics, while maintaining the same text alignment\nquality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17565v1",
    "published_date": "2024-09-26 06:27:26 UTC",
    "updated_date": "2024-09-26 06:27:26 UTC"
  },
  {
    "arxiv_id": "2410.01840v1",
    "title": "Target Pose Guided Whole-body Grasping Motion Generation for Digital Humans",
    "authors": [
      "Quanquan Shao",
      "Yi Fang"
    ],
    "abstract": "Grasping manipulation is a fundamental mode for human interaction with daily\nlife objects. The synthesis of grasping motion is also greatly demanded in many\napplications such as animation and robotics. In objects grasping research\nfield, most works focus on generating the last static grasping pose with a\nparallel gripper or dexterous hand. Grasping motion generation for the full arm\nespecially for the full humanlike intelligent agent is still under-explored. In\nthis work, we propose a grasping motion generation framework for digital human\nwhich is an anthropomorphic intelligent agent with high degrees of freedom in\nvirtual world. Given an object known initial pose in 3D space, we first\ngenerate a target pose for whole-body digital human based on off-the-shelf\ntarget grasping pose generation methods. With an initial pose and this\ngenerated target pose, a transformer-based neural network is used to generate\nthe whole grasping trajectory, which connects initial pose and target pose\nsmoothly and naturally. Additionally, two post optimization components are\ndesigned to mitigates foot-skating issue and hand-object interpenetration\nseparately. Experiments are conducted on GRAB dataset to demonstrate\neffectiveness of this proposed method for whole-body grasping motion generation\nwith randomly placed unknown objects.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages,5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.01840v1",
    "published_date": "2024-09-26 05:43:23 UTC",
    "updated_date": "2024-09-26 05:43:23 UTC"
  },
  {
    "arxiv_id": "2410.02820v3",
    "title": "Heuristics and Biases in AI Decision-Making: Implications for Responsible AGI",
    "authors": [
      "Payam Saeedi",
      "Mahsa Goodarzi",
      "M Abdullah Canbaz"
    ],
    "abstract": "We investigate the presence of cognitive biases in three large language\nmodels (LLMs): GPT-4o, Gemma 2, and Llama 3.1. The study uses 1,500 experiments\nacross nine established cognitive biases to evaluate the models' responses and\nconsistency. GPT-4o demonstrated the strongest overall performance. Gemma 2\nshowed strengths in addressing the sunk cost fallacy and prospect theory,\nhowever its performance varied across different biases. Llama 3.1 consistently\nunderperformed, relying on heuristics and exhibiting frequent inconsistencies\nand contradictions. The findings highlight the challenges of achieving robust\nand generalizable reasoning in LLMs, and underscore the need for further\ndevelopment to mitigate biases in artificial general intelligence (AGI). The\nstudy emphasizes the importance of integrating statistical reasoning and\nethical considerations in future AI development.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02820v3",
    "published_date": "2024-09-26 05:34:00 UTC",
    "updated_date": "2025-04-07 02:44:51 UTC"
  },
  {
    "arxiv_id": "2409.17547v2",
    "title": "Triple Point Masking",
    "authors": [
      "Jiaming Liu",
      "Linghe Kong",
      "Yue Wu",
      "Maoguo Gong",
      "Hao Li",
      "Qiguang Miao",
      "Wenping Ma",
      "Can Qin"
    ],
    "abstract": "Existing 3D mask learning methods encounter performance bottlenecks under\nlimited data, and our objective is to overcome this limitation. In this paper,\nwe introduce a triple point masking scheme, named TPM, which serves as a\nscalable framework for pre-training of masked autoencoders to achieve\nmulti-mask learning for 3D point clouds. Specifically, we augment the baselines\nwith two additional mask choices (i.e., medium mask and low mask) as our core\ninsight is that the recovery process of an object can manifest in diverse ways.\nPrevious high-masking schemes focus on capturing the global representation but\nlack the fine-grained recovery capability, so that the generated pre-trained\nweights tend to play a limited role in the fine-tuning process. With the\nsupport of the proposed TPM, available methods can exhibit more flexible and\naccurate completion capabilities, enabling the potential autoencoder in the\npre-training stage to consider multiple representations of a single 3D object.\nIn addition, an SVM-guided weight selection module is proposed to fill the\nencoder parameters for downstream networks with the optimal weight during the\nfine-tuning stage, maximizing linear accuracy and facilitating the acquisition\nof intricate representations for new objects. Extensive experiments show that\nthe four baselines equipped with the proposed TPM achieve comprehensive\nperformance improvements on various downstream tasks. Our code and models are\navailable at https://github.com/liujia99/TPM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17547v2",
    "published_date": "2024-09-26 05:33:30 UTC",
    "updated_date": "2024-10-15 04:00:03 UTC"
  },
  {
    "arxiv_id": "2409.17545v2",
    "title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult",
    "authors": [
      "Cheolhun Jang"
    ],
    "abstract": "Preference optimization methods typically begin training with a well-trained\nSFT model as a reference model. In RLHF and DPO, a regularization term is used\nduring the preference optimization process to prevent the policy model from\ndeviating too far from the reference model's distribution, thereby avoiding the\ngeneration of anomalous responses. When the reference model is already\nwell-aligned with the given data or only requires slight adjustments, this\napproach can produce a well-aligned model. However, if the reference model is\nnot aligned with the given data and requires significant deviation from its\ncurrent state, a regularization term may actually hinder the model alignment.\nIn this study, we propose \\textbf{Modulated Intervention Preference\nOptimization (MIPO)} to address this issue. MIPO modulates the degree of\nintervention from the reference model based on how well the given data is\naligned with it. If the data is well-aligned, the intervention is increased to\nprevent the policy model from diverging significantly from reference model.\nConversely, if the alignment is poor, the interference is reduced to facilitate\nmore extensive training. We compare the performance of MIPO and DPO using\nMistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental\nresults demonstrate that MIPO consistently outperforms DPO across various\nevaluation scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8pages, submitted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.17545v2",
    "published_date": "2024-09-26 05:24:14 UTC",
    "updated_date": "2024-09-27 06:48:08 UTC"
  },
  {
    "arxiv_id": "2409.17538v5",
    "title": "On the Implicit Relation Between Low-Rank Adaptation and Differential Privacy",
    "authors": [
      "Saber Malekmohammadi",
      "Golnoosh Farnadi"
    ],
    "abstract": "A significant approach in natural language processing involves large-scale\npre-training of models on general domain data followed by their adaptation to\nspecific tasks or domains. As models grow in size, full fine-tuning all of\ntheir parameters becomes increasingly impractical. To address this, some\nmethods for low-rank task adaptation of language models have been proposed,\ne.g., LoRA and FLoRA. These methods keep the pre-trained model weights fixed\nand incorporate trainable low-rank decomposition matrices into some layers of\nthe transformer architecture, called adapters. This approach significantly\nreduces the number of trainable parameters required for downstream tasks\ncompared to full fine-tuning all parameters. In this work, we look at low-rank\nadaptation from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA and FLoRA leads to the injection of some\nrandom noise into the batch gradients w.r.t the adapter parameters. We quantify\nthe variance of the injected noise and show that the smaller the adaptation\nrank, the larger the noise variance. By establishing a Berry-Esseen type bound\non the total variation distance between distribution of the injected noise and\na Gaussian distribution with the same variance, we show that the dynamics of\nlow-rank adaptation is close to that of differentially private fine-tuning of\nthe adapters. Finally, using Johnson-Lindenstrauss lemma, we show that when\naugmented with gradient scaling, low-rank adaptation is very close to\nperforming DPSGD algorithm with a fixed noise scale to fine-tune the adapters.\nSuggested by our theoretical findings and approved by our experimental results,\nwe show that low-rank adaptation, besides mitigating the space and\ncomputational complexities, implicitly provides a privacy protection w.r.t the\nfine-tuning data, without inducing the high space complexity of DPSGD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17538v5",
    "published_date": "2024-09-26 04:56:49 UTC",
    "updated_date": "2025-04-02 03:18:14 UTC"
  },
  {
    "arxiv_id": "2409.17534v2",
    "title": "Just Say What You Want: Only-prompting Self-rewarding Online Preference Optimization",
    "authors": [
      "Ruijie Xu",
      "Zhihan Liu",
      "Yongfei Liu",
      "Shipeng Yan",
      "Zhaoran Wang",
      "Zhi Zhang",
      "Xuming He"
    ],
    "abstract": "We address the challenge of online Reinforcement Learning from Human Feedback\n(RLHF) with a focus on self-rewarding alignment methods. In online RLHF,\nobtaining feedback requires interaction with the environment, which can be\ncostly when using additional reward models or the GPT-4 API. Current\nself-rewarding approaches rely heavily on the discriminator's judgment\ncapabilities, which are effective for large-scale models but challenging to\ntransfer to smaller ones. To address these limitations, we propose a novel,\nonly-prompting self-rewarding online algorithm that generates preference\ndatasets without relying on judgment capabilities. Additionally, we employ\nfine-grained arithmetic control over the optimality gap between positive and\nnegative examples, generating more hard negatives in the later stages of\ntraining to help the model better capture subtle human preferences. Finally, we\nconduct extensive experiments on two base models, Mistral-7B and\nMistral-Instruct-7B, which significantly bootstrap the performance of the\nreference model, achieving 34.5% in the Length-controlled Win Rates of\nAlpacaEval 2.0.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17534v2",
    "published_date": "2024-09-26 04:41:08 UTC",
    "updated_date": "2024-10-14 05:43:37 UTC"
  },
  {
    "arxiv_id": "2409.17531v2",
    "title": "SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion",
    "authors": [
      "Ming Dai",
      "Lingfeng Yang",
      "Yihao Xu",
      "Zhenhua Feng",
      "Wankou Yang"
    ],
    "abstract": "Visual grounding is a common vision task that involves grounding descriptive\nsentences to the corresponding regions of an image. Most existing methods use\nindependent image-text encoding and apply complex hand-crafted modules or\nencoder-decoder architectures for modal interaction and query reasoning.\nHowever, their performance significantly drops when dealing with complex\ntextual expressions. This is because the former paradigm only utilizes limited\ndownstream data to fit the multi-modal feature fusion. Therefore, it is only\neffective when the textual expressions are relatively simple. In contrast,\ngiven the wide diversity of textual expressions and the uniqueness of\ndownstream training data, the existing fusion module, which extracts multimodal\ncontent from a visual-linguistic context, has not been fully investigated. In\nthis paper, we present a simple yet robust transformer-based framework, SimVG,\nfor visual grounding. Specifically, we decouple visual-linguistic feature\nfusion from downstream tasks by leveraging existing multimodal pre-trained\nmodels and incorporating additional object tokens to facilitate deep\nintegration of downstream and pre-training tasks. Furthermore, we design a\ndynamic weight-balance distillation method in the multi-branch synchronous\nlearning process to enhance the representation capability of the simpler\nbranch. This branch only consists of a lightweight MLP, which simplifies the\nstructure and improves reasoning speed. Experiments on six widely used VG\ndatasets, i.e., RefCOCO/+/g, ReferIt, Flickr30K, and GRefCOCO, demonstrate the\nsuperiority of SimVG. Finally, the proposed method not only achieves\nimprovements in efficiency and convergence speed but also attains new\nstate-of-the-art performance on these benchmarks. Codes and models will be\navailable at \\url{https://github.com/Dmmm1997/SimVG}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24pages, 18figures, NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17531v2",
    "published_date": "2024-09-26 04:36:19 UTC",
    "updated_date": "2024-10-28 07:21:18 UTC"
  },
  {
    "arxiv_id": "2409.19024v1",
    "title": "Elephant in the Room: Unveiling the Impact of Reward Model Quality in Alignment",
    "authors": [
      "Yan Liu",
      "Xiaoyuan Yi",
      "Xiaokang Chen",
      "Jing Yao",
      "Jingwei Yi",
      "Daoguang Zan",
      "Zheng Liu",
      "Xing Xie",
      "Tsung-Yi Ho"
    ],
    "abstract": "The demand for regulating potentially risky behaviors of large language\nmodels (LLMs) has ignited research on alignment methods. Since LLM alignment\nheavily relies on reward models for optimization or evaluation, neglecting the\nquality of reward models may cause unreliable results or even misalignment.\nDespite the vital role reward models play in alignment, previous works have\nconsistently overlooked their performance and used off-the-shelf reward models\narbitrarily without verification, rendering the reward model ``\\emph{an\nelephant in the room}''. To this end, this work first investigates the quality\nof the widely-used preference dataset, HH-RLHF, and curates a clean version,\nCHH-RLHF. Based on CHH-RLHF, we benchmark the accuracy of a broad range of\nreward models used in previous alignment works, unveiling the unreliability of\nusing them both for optimization and evaluation. Furthermore, we systematically\nstudy the impact of reward model quality on alignment performance in three\nreward utilization paradigms. Extensive experiments reveal that better reward\nmodels perform as better human preference proxies. This work aims to awaken\npeople to notice this huge elephant in alignment research. We call attention to\nthe following issues: (1) The reward model needs to be rigorously evaluated,\nwhether for alignment optimization or evaluation. (2) Considering the role of\nreward models, research efforts should not only concentrate on alignment\nalgorithm, but also on developing more reliable human proxy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19024v1",
    "published_date": "2024-09-26 04:28:35 UTC",
    "updated_date": "2024-09-26 04:28:35 UTC"
  },
  {
    "arxiv_id": "2409.17526v1",
    "title": "Drone Stereo Vision for Radiata Pine Branch Detection and Distance Measurement: Integrating SGBM and Segmentation Models",
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ],
    "abstract": "Manual pruning of radiata pine trees presents significant safety risks due to\ntheir substantial height and the challenging terrains in which they thrive. To\naddress these risks, this research proposes the development of a drone-based\npruning system equipped with specialized pruning tools and a stereo vision\ncamera, enabling precise detection and trimming of branches. Deep learning\nalgorithms, including YOLO and Mask R-CNN, are employed to ensure accurate\nbranch detection, while the Semi-Global Matching algorithm is integrated to\nprovide reliable distance estimation. The synergy between these techniques\nfacilitates the precise identification of branch locations and enables\nefficient, targeted pruning. Experimental results demonstrate that the combined\nimplementation of YOLO and SGBM enables the drone to accurately detect branches\nand measure their distances from the drone. This research not only improves the\nsafety and efficiency of pruning operations but also makes a significant\ncontribution to the advancement of drone technology in the automation of\nagricultural and forestry practices, laying a foundational framework for\nfurther innovations in environmental management.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17526v1",
    "published_date": "2024-09-26 04:27:44 UTC",
    "updated_date": "2024-09-26 04:27:44 UTC"
  },
  {
    "arxiv_id": "2409.17523v1",
    "title": "EAGLE: Egocentric AGgregated Language-video Engine",
    "authors": [
      "Jing Bi",
      "Yunlong Tang",
      "Luchuan Song",
      "Ali Vosoughi",
      "Nguyen Nguyen",
      "Chenliang Xu"
    ],
    "abstract": "The rapid evolution of egocentric video analysis brings new insights into\nunderstanding human activities and intentions from a first-person perspective.\nDespite this progress, the fragmentation in tasks like action recognition,\nprocedure learning, and moment retrieval, \\etc, coupled with inconsistent\nannotations and isolated model development, hinders a holistic interpretation\nof video content. In response, we introduce the EAGLE (Egocentric AGgregated\nLanguage-video Engine) model and the EAGLE-400K dataset to provide a unified\nframework that integrates various egocentric video understanding tasks.\nEAGLE-400K, the \\textit{first} large-scale instruction-tuning dataset tailored\nfor egocentric video, features 400K diverse samples to enhance a broad spectrum\nof tasks from activity recognition to procedure knowledge learning. Moreover,\nEAGLE, a strong video multimodal large language model (MLLM), is designed to\neffectively capture both spatial and temporal information. In addition, we\npropose a set of evaluation metrics designed to facilitate a thorough\nassessment of MLLM for egocentric video understanding. Our extensive\nexperiments demonstrate EAGLE's superior performance over existing models,\nhighlighting its ability to balance task-specific understanding with holistic\nvideo interpretation. With EAGLE, we aim to pave the way for research\nopportunities and practical applications in real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACMMM 24",
    "pdf_url": "http://arxiv.org/pdf/2409.17523v1",
    "published_date": "2024-09-26 04:17:27 UTC",
    "updated_date": "2024-09-26 04:17:27 UTC"
  },
  {
    "arxiv_id": "2409.17519v1",
    "title": "Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization",
    "authors": [
      "Kento Kawaharazuka",
      "Yoshiki Obinata",
      "Naoaki Kanazawa",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "In order for robots to autonomously navigate and operate in diverse\nenvironments, it is essential for them to recognize the state of their\nenvironment. On the other hand, the environmental state recognition has\ntraditionally involved distinct methods tailored to each state to be\nrecognized. In this study, we perform a unified environmental state recognition\nfor robots through the spoken language with pre-trained large-scale\nvision-language models. We apply Visual Question Answering and Image-to-Text\nRetrieval, which are tasks of Vision-Language Models. We show that with our\nmethod, it is possible to recognize not only whether a room door is\nopen/closed, but also whether a transparent door is open/closed and whether\nwater is running in a sink, without training neural networks or manual\nprogramming. In addition, the recognition accuracy can be improved by selecting\nappropriate texts from the set of prepared texts based on black-box\noptimization. For each state recognition, only the text set and its weighting\nneed to be changed, eliminating the need to prepare multiple different models\nand programs, and facilitating the management of source code and computer\nresource. We experimentally demonstrate the effectiveness of our method and\napply it to the recognition behavior on a mobile robot, Fetch.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at Advanced Robotics, website -\n  https://haraduka.github.io/vlm-bbo/",
    "pdf_url": "http://arxiv.org/pdf/2409.17519v1",
    "published_date": "2024-09-26 04:02:20 UTC",
    "updated_date": "2024-09-26 04:02:20 UTC"
  },
  {
    "arxiv_id": "2409.17518v2",
    "title": "Multi-Designated Detector Watermarking for Language Models",
    "authors": [
      "Zhengan Huang",
      "Gongxian Zeng",
      "Xin Mu",
      "Yu Wang",
      "Yue Yu"
    ],
    "abstract": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17518v2",
    "published_date": "2024-09-26 04:01:15 UTC",
    "updated_date": "2024-10-01 08:08:42 UTC"
  },
  {
    "arxiv_id": "2409.17517v1",
    "title": "Dataset Distillation-based Hybrid Federated Learning on Non-IID Data",
    "authors": [
      "Xiufang Shi",
      "Wei Zhang",
      "Mincheng Wu",
      "Guangyi Liu",
      "Zhenyu Wen",
      "Shibo He",
      "Tejal Shah",
      "Rajiv Ranjan"
    ],
    "abstract": "In federated learning, the heterogeneity of client data has a great impact on\nthe performance of model training. Many heterogeneity issues in this process\nare raised by non-independently and identically distributed (Non-IID) data.\nThis study focuses on the issue of label distribution skew. To address it, we\npropose a hybrid federated learning framework called HFLDD, which integrates\ndataset distillation to generate approximately independent and equally\ndistributed (IID) data, thereby improving the performance of model training.\nParticularly, we partition the clients into heterogeneous clusters, where the\ndata labels among different clients within a cluster are unbalanced while the\ndata labels among different clusters are balanced. The cluster headers collect\ndistilled data from the corresponding cluster members, and conduct model\ntraining in collaboration with the server. This training process is like\ntraditional federated learning on IID data, and hence effectively alleviates\nthe impact of Non-IID data on model training. Furthermore, we compare our\nproposed method with typical baseline methods on public datasets. Experimental\nresults demonstrate that when the data labels are severely imbalanced, the\nproposed HFLDD outperforms the baseline methods in terms of both test accuracy\nand communication cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17517v1",
    "published_date": "2024-09-26 03:52:41 UTC",
    "updated_date": "2024-09-26 03:52:41 UTC"
  },
  {
    "arxiv_id": "2409.17516v1",
    "title": "Functional Classification of Spiking Signal Data Using Artificial Intelligence Techniques: A Review",
    "authors": [
      "Danial Sharifrazi",
      "Nouman Javed",
      "Javad Hassannataj Joloudari",
      "Roohallah Alizadehsani",
      "Prasad N. Paradkar",
      "Ru-San Tan",
      "U. Rajendra Acharya",
      "Asim Bhatti"
    ],
    "abstract": "Human brain neuron activities are incredibly significant nowadays. Neuronal\nbehavior is assessed by analyzing signal data such as electroencephalography\n(EEG), which can offer scientists valuable information about diseases and\nhuman-computer interaction. One of the difficulties researchers confront while\nevaluating these signals is the existence of large volumes of spike data.\nSpikes are some considerable parts of signal data that can happen as a\nconsequence of vital biomarkers or physical issues such as electrode movements.\nHence, distinguishing types of spikes is important. From this spot, the spike\nclassification concept commences. Previously, researchers classified spikes\nmanually. The manual classification was not precise enough as it involves\nextensive analysis. Consequently, Artificial Intelligence (AI) was introduced\ninto neuroscience to assist clinicians in classifying spikes correctly. This\nreview discusses the importance and use of AI in spike classification, focusing\non the recognition of neural activity noises. The task is divided into three\nmain components: preprocessing, classification, and evaluation. Existing\nmethods are introduced and their importance is determined. The review also\nhighlights the need for more efficient algorithms. The primary goal is to\nprovide a perspective on spike classification for future research and provide a\ncomprehensive understanding of the methodologies and issues involved. The\nreview organizes materials in the spike classification field for future\nstudies. In this work, numerous studies were extracted from different\ndatabases. The PRISMA-related research guidelines were then used to choose\npapers. Then, research studies based on spike classification using machine\nlearning and deep learning approaches with effective preprocessing were\nselected.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "8 figures, 32 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17516v1",
    "published_date": "2024-09-26 03:50:55 UTC",
    "updated_date": "2024-09-26 03:50:55 UTC"
  },
  {
    "arxiv_id": "2409.17515v3",
    "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection",
    "authors": [
      "Xinlei Wang",
      "Maike Feng",
      "Jing Qiu",
      "Jinjin Gu",
      "Junhua Zhao"
    ],
    "abstract": "This paper introduces a novel approach that leverages Large Language Models\n(LLMs) and Generative Agents to enhance time series forecasting by reasoning\nacross both text and time series data. With language as a medium, our method\nadaptively integrates social events into forecasting models, aligning news\ncontent with time series fluctuations to provide richer insights. Specifically,\nwe utilize LLM-based agents to iteratively filter out irrelevant news and\nemploy human-like reasoning to evaluate predictions. This enables the model to\nanalyze complex events, such as unexpected incidents and shifts in social\nbehavior, and continuously refine the selection logic of news and the\nrobustness of the agent's output. By integrating selected news events with time\nseries data, we fine-tune a pre-trained LLM to predict sequences of digits in\ntime series. The results demonstrate significant improvements in forecasting\naccuracy, suggesting a potential paradigm shift in time series forecasting\nthrough the effective utilization of unstructured news data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for NeurIPS 2024. Code and data are\n  available at https://github.com/ameliawong1996/From_News_to_Forecast",
    "pdf_url": "http://arxiv.org/pdf/2409.17515v3",
    "published_date": "2024-09-26 03:50:22 UTC",
    "updated_date": "2024-10-30 12:04:18 UTC"
  },
  {
    "arxiv_id": "2409.17510v3",
    "title": "NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes",
    "authors": [
      "Ziquan Wei",
      "Tingting Dan",
      "Jiaqi Ding",
      "Guorong Wu"
    ],
    "abstract": "Although modern imaging technologies allow us to study connectivity between\ntwo distinct brain regions in-vivo, an in-depth understanding of how anatomical\nstructure supports brain function and how spontaneous functional fluctuations\nemerge remarkable cognition is still elusive. Meanwhile, tremendous efforts\nhave been made in the realm of machine learning to establish the nonlinear\nmapping between neuroimaging data and phenotypic traits. However, the absence\nof neuroscience insight in the current approaches poses significant challenges\nin understanding cognitive behavior from transient neural activities. To\naddress this challenge, we put the spotlight on the coupling mechanism of\nstructural connectivity (SC) and functional connectivity (FC) by formulating\nsuch network neuroscience question into an expressive graph representation\nlearning problem for high-order topology. Specifically, we introduce the\nconcept of topological detour to characterize how a ubiquitous instance of FC\n(direct link) is supported by neural pathways (detour) physically wired by SC,\nwhich forms a cyclic loop interacted by brain structure and function. In the\nclich\\'e of machine learning, the multi-hop detour pathway underlying SC-FC\ncoupling allows us to devise a novel multi-head self-attention mechanism within\nTransformer to capture multi-modal feature representation from paired graphs of\nSC and FC. Taken together, we propose a biological-inspired deep model, coined\nas NeuroPath, to find putative connectomic feature representations from the\nunprecedented amount of neuroimages, which can be plugged into various\ndownstream applications such as task recognition and disease diagnosis. We have\nevaluated NeuroPath on large-scale public datasets including HCP and UK Biobank\nunder supervised and zero-shot learning, where the state-of-the-art performance\nby our NeuroPath indicates great potential in network neuroscience.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17510v3",
    "published_date": "2024-09-26 03:40:12 UTC",
    "updated_date": "2024-10-27 03:25:05 UTC"
  },
  {
    "arxiv_id": "2409.17508v2",
    "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
    "authors": [
      "Xun Zhu",
      "Ying Hu",
      "Fanbin Mo",
      "Miao Li",
      "Ji Wu"
    ],
    "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization in MLLMs, recent advances primarily focus on improving\nthe LLM components, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector in MLLMs. Extensive ablation\nexperiments validate the effectiveness of introducing CMoE under any\nconfiguration, with up to an average 8% performance gains. We further provide\ninterpretation analysis of the tug-of-war problem from the perspective of\ngradient optimization and parameter statistics. Compared to previous\nstate-of-the-art medical MLLMs, Uni-Med achieves competitive or superior\nevaluation metrics on diverse tasks. Code and resources are available at\nhttps://github.com/tsinghua-msiip/Uni-Med.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17508v2",
    "published_date": "2024-09-26 03:33:26 UTC",
    "updated_date": "2024-11-01 02:38:53 UTC"
  },
  {
    "arxiv_id": "2409.18737v2",
    "title": "MemFusionMap: Working Memory Fusion for Online Vectorized HD Map Construction",
    "authors": [
      "Jingyu Song",
      "Xudong Chen",
      "Liupei Lu",
      "Jie Li",
      "Katherine A. Skinner"
    ],
    "abstract": "High-definition (HD) maps provide environmental information for autonomous\ndriving systems and are essential for safe planning. While existing methods\nwith single-frame input achieve impressive performance for online vectorized HD\nmap construction, they still struggle with complex scenarios and occlusions. We\npropose MemFusionMap, a novel temporal fusion model with enhanced temporal\nreasoning capabilities for online HD map construction. Specifically, we\ncontribute a working memory fusion module that improves the model's memory\ncapacity to reason across a history of frames. We also design a novel temporal\noverlap heatmap to explicitly inform the model about the temporal overlap\ninformation and vehicle trajectory in the Bird's Eye View space. By integrating\nthese two designs, MemFusionMap significantly outperforms existing methods\nwhile also maintaining a versatile design for scalability. We conduct extensive\nevaluation on open-source benchmarks and demonstrate a maximum improvement of\n5.4% in mAP over state-of-the-art methods. The project page for MemFusionMap is\nhttps://song-jingyu.github.io/MemFusionMap",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.18737v2",
    "published_date": "2024-09-26 03:16:39 UTC",
    "updated_date": "2024-11-22 05:37:23 UTC"
  },
  {
    "arxiv_id": "2409.17500v2",
    "title": "GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent",
    "authors": [
      "Hongtai Zeng",
      "Chao Yang",
      "Yanzhen Zhou",
      "Cheng Yang",
      "Qinglai Guo"
    ],
    "abstract": "Ensuring that the outputs of neural networks satisfy specific constraints is\ncrucial for applying neural networks to real-life decision-making problems. In\nthis paper, we consider making a batch of neural network outputs satisfy\nbounded and general linear constraints. We first reformulate the neural network\noutput projection problem as an entropy-regularized linear programming problem.\nWe show that such a problem can be equivalently transformed into an\nunconstrained convex optimization problem with Lipschitz continuous gradient\naccording to the duality theorem. Then, based on an accelerated gradient\ndescent algorithm with numerical performance enhancement, we present our\narchitecture, GLinSAT, to solve the problem. To the best of our knowledge, this\nis the first general linear satisfiability layer in which all the operations\nare differentiable and matrix-factorization-free. Despite the fact that we can\nexplicitly perform backpropagation based on automatic differentiation\nmechanism, we also provide an alternative approach in GLinSAT to calculate the\nderivatives based on implicit differentiation of the optimality condition.\nExperimental results on constrained traveling salesman problems, partial graph\nmatching with outliers, predictive portfolio allocation and power system unit\ncommitment demonstrate the advantages of GLinSAT over existing satisfiability\nlayers. Our implementation is available at\n\\url{https://github.com/HunterTracer/GLinSAT}.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by 2024 Advances in Neural Information\n  Processing Systems. The reviews and comments can be found in\n  https://openreview.net/forum?id=m1PVjNHvtP",
    "pdf_url": "http://arxiv.org/pdf/2409.17500v2",
    "published_date": "2024-09-26 03:12:53 UTC",
    "updated_date": "2024-11-11 10:17:00 UTC"
  },
  {
    "arxiv_id": "2409.17495v1",
    "title": "Human Mobility Modeling with Limited Information via Large Language Models",
    "authors": [
      "Yifan Liu",
      "Xishun Liao",
      "Haoxuan Ma",
      "Brian Yueshuai He",
      "Chris Stanford",
      "Jiaqi Ma"
    ],
    "abstract": "Understanding human mobility patterns has traditionally been a complex\nchallenge in transportation modeling. Due to the difficulties in obtaining\nhigh-quality training datasets across diverse locations, conventional\nactivity-based models and learning-based human mobility modeling algorithms are\nparticularly limited by the availability and quality of datasets. Furthermore,\ncurrent research mainly focuses on the spatial-temporal travel pattern but\nlacks an understanding of the semantic information between activities, which is\ncrucial for modeling the interdependence between activities. In this paper, we\npropose an innovative Large Language Model (LLM) empowered human mobility\nmodeling framework. Our proposed approach significantly reduces the reliance on\ndetailed human mobility statistical data, utilizing basic socio-demographic\ninformation of individuals to generate their daily mobility patterns. We have\nvalidated our results using the NHTS and SCAG-ABM datasets, demonstrating the\neffective modeling of mobility patterns and the strong adaptability of our\nframework across various geographic locations.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17495v1",
    "published_date": "2024-09-26 03:07:32 UTC",
    "updated_date": "2024-09-26 03:07:32 UTC"
  },
  {
    "arxiv_id": "2409.17486v2",
    "title": "Global-Local Medical SAM Adaptor Based on Full Adaption",
    "authors": [
      "Meng Wang",
      "Yarong Feng",
      "Yongwei Tang",
      "Tian Zhang",
      "Yuxin Liang",
      "Chao Lv"
    ],
    "abstract": "Emerging of visual language models, such as the segment anything model (SAM),\nhave made great breakthroughs in the field of universal semantic segmentation\nand significantly aid the improvements of medical image segmentation, in\nparticular with the help of Medical SAM adaptor (Med-SA). However, Med-SA still\ncan be improved, as it fine-tunes SAM in a partial adaption manner. To resolve\nthis problem, we present a novel global medical SAM adaptor (GMed-SA) with full\nadaption, which can adapt SAM globally. We further combine GMed-SA and Med-SA\nto propose a global-local medical SAM adaptor (GLMed-SA) to adapt SAM both\nglobally and locally. Extensive experiments have been performed on the\nchallenging public 2D melanoma segmentation dataset. The results show that\nGLMed-SA outperforms several state-of-the-art semantic segmentation methods on\nvarious evaluation metrics, demonstrating the superiority of our methods.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17486v2",
    "published_date": "2024-09-26 02:48:15 UTC",
    "updated_date": "2024-10-29 06:00:00 UTC"
  },
  {
    "arxiv_id": "2409.17481v2",
    "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
    "authors": [
      "Gongfan Fang",
      "Hongxu Yin",
      "Saurav Muralidharan",
      "Greg Heinrich",
      "Jeff Pool",
      "Jan Kautz",
      "Pavlo Molchanov",
      "Xinchao Wang"
    ],
    "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter\ncounts, which typically result in significant redundancy. This work introduces\nMaskLLM, a learnable pruning method that establishes Semi-structured (or\n``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during\ninference. Instead of developing a new importance criterion, MaskLLM explicitly\nmodels N:M patterns as a learnable distribution through Gumbel Softmax\nsampling. This approach facilitates end-to-end training on large-scale datasets\nand offers two notable advantages: 1) High-quality Masks - our method\neffectively scales to large datasets and learns accurate masks; 2)\nTransferability - the probabilistic modeling of mask distribution enables the\ntransfer learning of sparsity across domains or tasks. We assessed MaskLLM\nusing 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,\nwith sizes ranging from 843M to 15B parameters, and our empirical results show\nsubstantial improvements over state-of-the-art methods. For instance, leading\napproaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to\nthe dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL\nsolely by learning the masks with frozen weights. Furthermore, MaskLLM's\nlearnable nature allows customized masks for lossless application of 2:4\nsparsity to downstream tasks or domains. Code is available at\nhttps://github.com/NVlabs/MaskLLM.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2409.17481v2",
    "published_date": "2024-09-26 02:37:41 UTC",
    "updated_date": "2024-12-07 12:01:28 UTC"
  },
  {
    "arxiv_id": "2409.17480v1",
    "title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
    "authors": [
      "Chuanhong Zhan",
      "Wei Xiang",
      "Chao Liang",
      "Bang Wang"
    ],
    "abstract": "Existing script event prediction task forcasts the subsequent event based on\nan event script chain. However, the evolution of historical events are more\ncomplicated in real world scenarios and the limited information provided by the\nevent script chain also make it difficult to accurately predict subsequent\nevents. This paper introduces a Causality Graph Event Prediction(CGEP) task\nthat forecasting consequential event based on an Event Causality Graph (ECG).\nWe propose a Semantic Enhanced Distance-sensitive Graph Prompt Learning\n(SeDGPL) Model for the CGEP task. In SeDGPL, (1) we design a Distance-sensitive\nGraph Linearization (DsGL) module to reformulate the ECG into a graph prompt\ntemplate as the input of a PLM; (2) propose an Event-Enriched Causality\nEncoding (EeCE) module to integrate both event contextual semantic and graph\nschema information; (3) propose a Semantic Contrast Event Prediction (ScEP)\nmodule to enhance the event representation among numerous candidate events and\npredict consequential event following prompt learning paradigm. %We construct\ntwo CGEP datasets based on existing MAVEN-ERE and ESC corpus for experiments.\nExperiment results validate our argument our proposed SeDGPL model outperforms\nthe advanced competitors for the CGEP task.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17480v1",
    "published_date": "2024-09-26 02:34:08 UTC",
    "updated_date": "2024-09-26 02:34:08 UTC"
  },
  {
    "arxiv_id": "2409.17472v1",
    "title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards",
    "authors": [
      "Heejin Do",
      "Sangwon Ryu",
      "Gary Geunbae Lee"
    ],
    "abstract": "Recent advances in automated essay scoring (AES) have shifted towards\nevaluating multiple traits to provide enriched feedback. Like typical AES\nsystems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure\nagreement with human raters, aligning closely with the rating schema; however,\nits non-differentiable nature prevents its direct use in neural network\ntraining. In this paper, we propose Scoring-aware Multi-reward Reinforcement\nLearning (SaMRL), which integrates actual evaluation schemes into the training\nprocess by designing QWK-based rewards with a mean-squared error penalty for\nmulti-trait AES. Existing reinforcement learning (RL) applications in AES are\nlimited to classification models despite associated performance degradation, as\nRL requires probability distributions; instead, we adopt an autoregressive\nscore generation framework to leverage token generation probabilities for\nrobust multi-trait score predictions. Empirical analyses demonstrate that SaMRL\nfacilitates model training, notably enhancing scoring of previously inferior\nprompts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.17472v1",
    "published_date": "2024-09-26 02:16:48 UTC",
    "updated_date": "2024-09-26 02:16:48 UTC"
  },
  {
    "arxiv_id": "2409.17466v1",
    "title": "Adjusting Regression Models for Conditional Uncertainty Calibration",
    "authors": [
      "Ruijiang Gao",
      "Mingzhang Yin",
      "James McInerney",
      "Nathan Kallus"
    ],
    "abstract": "Conformal Prediction methods have finite-sample distribution-free marginal\ncoverage guarantees. However, they generally do not offer conditional coverage\nguarantees, which can be important for high-stakes decisions. In this paper, we\npropose a novel algorithm to train a regression function to improve the\nconditional coverage after applying the split conformal prediction procedure.\nWe establish an upper bound for the miscoverage gap between the conditional\ncoverage and the nominal coverage rate and propose an end-to-end algorithm to\ncontrol this upper bound. We demonstrate the efficacy of our method empirically\non synthetic and real-world datasets.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Machine Learning Special Issue on Uncertainty Quantification",
    "pdf_url": "http://arxiv.org/pdf/2409.17466v1",
    "published_date": "2024-09-26 01:55:45 UTC",
    "updated_date": "2024-09-26 01:55:45 UTC"
  },
  {
    "arxiv_id": "2409.17457v1",
    "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches",
    "authors": [
      "Sifan Wu",
      "Amir Khasahmadi",
      "Mor Katz",
      "Pradeep Kumar Jayaraman",
      "Yewen Pu",
      "Karl Willis",
      "Bang Liu"
    ],
    "abstract": "Parametric Computer-Aided Design (CAD) is central to contemporary mechanical\ndesign. However, it encounters challenges in achieving precise parametric\nsketch modeling and lacks practical evaluation metrics suitable for mechanical\ndesign. We harness the capabilities of pre-trained foundation models, renowned\nfor their successes in natural language processing and computer vision, to\ndevelop generative models specifically for CAD. These models are adept at\nunderstanding complex geometries and design reasoning, a crucial advancement in\nCAD technology. In this paper, we propose CadVLM, an end-to-end vision language\nmodel for CAD generation. Our approach involves adapting pre-trained foundation\nmodels to manipulate engineering sketches effectively, integrating both sketch\nprimitive sequences and sketch images. Extensive experiments demonstrate\nsuperior performance on multiple CAD sketch generation tasks such as CAD\nautocompletion, CAD autoconstraint, and image conditional generation. To our\nknowledge, this is the first instance of a multimodal Large Language Model\n(LLM) being successfully applied to parametric CAD generation, representing a\npioneering step in the field of computer-aided mechanical design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17457v1",
    "published_date": "2024-09-26 01:22:29 UTC",
    "updated_date": "2024-09-26 01:22:29 UTC"
  },
  {
    "arxiv_id": "2409.17440v1",
    "title": "A Time Series is Worth Five Experts: Heterogeneous Mixture of Experts for Traffic Flow Prediction",
    "authors": [
      "Guangyu Wang",
      "Yujie Chen",
      "Ming Gao",
      "Zhiqiao Wu",
      "Jiafu Tang",
      "Jiabi Zhao"
    ],
    "abstract": "Accurate traffic prediction faces significant challenges, necessitating a\ndeep understanding of both temporal and spatial cues and their complex\ninteractions across multiple variables. Recent advancements in traffic\nprediction systems are primarily due to the development of complex\nsequence-centric models. However, existing approaches often embed multiple\nvariables and spatial relationships at each time step, which may hinder\neffective variable-centric learning, ultimately leading to performance\ndegradation in traditional traffic prediction tasks. To overcome these\nlimitations, we introduce variable-centric and prior knowledge-centric modeling\ntechniques. Specifically, we propose a Heterogeneous Mixture of Experts (TITAN)\nmodel for traffic flow prediction. TITAN initially consists of three experts\nfocused on sequence-centric modeling. Then, designed a low-rank adaptive\nmethod, TITAN simultaneously enables variable-centric modeling. Furthermore, we\nsupervise the gating process using a prior knowledge-centric modeling strategy\nto ensure accurate routing. Experiments on two public traffic network datasets,\nMETR-LA and PEMS-BAY, demonstrate that TITAN effectively captures\nvariable-centric dependencies while ensuring accurate routing. Consequently, it\nachieves improvements in all evaluation metrics, ranging from approximately\n4.37\\% to 11.53\\%, compared to previous state-of-the-art (SOTA) models. The\ncode is open at\n\\href{https://github.com/sqlcow/TITAN}{https://github.com/sqlcow/TITAN}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.17440v1",
    "published_date": "2024-09-26 00:26:47 UTC",
    "updated_date": "2024-09-26 00:26:47 UTC"
  }
]
[
  {
    "arxiv_id": "2407.05521v1",
    "title": "Accelerating MRI Uncertainty Estimation with Mask-based Bayesian Neural Network",
    "authors": [
      "Zehuan Zhang",
      "Matej Genci",
      "Hongxiang Fan",
      "Andreas Wetscherek",
      "Wayne Luk"
    ],
    "abstract": "Accurate and reliable Magnetic Resonance Imaging (MRI) analysis is\nparticularly important for adaptive radiotherapy, a recent medical advance\ncapable of improving cancer diagnosis and treatment. Recent studies have shown\nthat IVIM-NET, a deep neural network (DNN), can achieve high accuracy in MRI\nanalysis, indicating the potential of deep learning to enhance diagnostic\ncapabilities in healthcare. However, IVIM-NET does not provide calibrated\nuncertainty information needed for reliable and trustworthy predictions in\nhealthcare. Moreover, the expensive computation and memory demands of IVIM-NET\nreduce hardware performance, hindering widespread adoption in realistic\nscenarios. To address these challenges, this paper proposes an\nalgorithm-hardware co-optimization flow for high-performance and reliable MRI\nanalysis. At the algorithm level, a transformation design flow is introduced to\nconvert IVIM-NET to a mask-based Bayesian Neural Network (BayesNN),\nfacilitating reliable and efficient uncertainty estimation. At the hardware\nlevel, we propose an FPGA-based accelerator with several hardware\noptimizations, such as mask-zero skipping and operation reordering.\nExperimental results demonstrate that our co-design approach can satisfy the\nuncertainty requirements of MRI analysis, while achieving 7.5 times and 32.5\ntimes speedup on an Xilinx VU13P FPGA compared to GPU and CPU implementations\nwith reduced power consumption.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "The 35th IEEE International Conference on Application-specific\n  Systems, Architectures and Processors (ASAP) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05521v1",
    "published_date": "2024-07-07 23:57:40 UTC",
    "updated_date": "2024-07-07 23:57:40 UTC"
  },
  {
    "arxiv_id": "2407.05516v2",
    "title": "Differentiable Modal Synthesis for Physical Modeling of Planar String Sound and Motion Simulation",
    "authors": [
      "Jin Woo Lee",
      "Jaehyun Park",
      "Min Jun Choi",
      "Kyogu Lee"
    ],
    "abstract": "While significant advancements have been made in music generation and\ndifferentiable sound synthesis within machine learning and computer audition,\nthe simulation of instrument vibration guided by physical laws has been\nunderexplored. To address this gap, we introduce a novel model for simulating\nthe spatio-temporal motion of nonlinear strings, integrating modal synthesis\nand spectral modeling within a neural network framework. Our model leverages\nphysical properties and fundamental frequencies as inputs, outputting string\nstates across time and space that solve the partial differential equation\ncharacterizing the nonlinear string. Empirical evaluations demonstrate that the\nproposed architecture achieves superior accuracy in string motion simulation\ncompared to existing baseline architectures. The code and demo are available\nonline.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.05516v2",
    "published_date": "2024-07-07 23:36:51 UTC",
    "updated_date": "2024-10-30 19:54:09 UTC"
  },
  {
    "arxiv_id": "2407.05502v3",
    "title": "Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models",
    "authors": [
      "Nikhil Sharma",
      "Kenton Murray",
      "Ziang Xiao"
    ],
    "abstract": "Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.05502v3",
    "published_date": "2024-07-07 21:26:36 UTC",
    "updated_date": "2025-02-11 18:17:53 UTC"
  },
  {
    "arxiv_id": "2407.06237v1",
    "title": "Discounted Pseudocosts in MILP",
    "authors": [
      "Krunal Kishor Patel"
    ],
    "abstract": "In this article, we introduce the concept of discounted pseudocosts, inspired\nby discounted total reward in reinforcement learning, and explore their\napplication in mixed-integer linear programming (MILP). Traditional pseudocosts\nestimate changes in the objective function due to variable bound changes during\nthe branch-and-bound process. By integrating reinforcement learning concepts,\nwe propose a novel approach incorporating a forward-looking perspective into\npseudocost estimation. We present the motivation behind discounted pseudocosts\nand discuss how they represent the anticipated reward for branching after one\nlevel of exploration in the MILP problem space. Initial experiments on MIPLIB\n2017 benchmark instances demonstrate the potential of discounted pseudocosts to\nenhance branching strategies and accelerate the solution process for\nchallenging MILP problems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC",
      "90C11 (Primary), 90C10, 90-08 (Secondary)"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06237v1",
    "published_date": "2024-07-07 19:41:38 UTC",
    "updated_date": "2024-07-07 19:41:38 UTC"
  },
  {
    "arxiv_id": "2407.05474v1",
    "title": "Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses",
    "authors": [
      "Dongxu Zhang",
      "Varun Gangal",
      "Barrett Martin Lattimer",
      "Yi Yang"
    ],
    "abstract": "Detecting hallucinations in large language model (LLM) outputs is pivotal,\nyet traditional fine-tuning for this classification task is impeded by the\nexpensive and quickly outdated annotation process, especially across numerous\nvertical domains and in the face of rapid LLM advancements. In this study, we\nintroduce an approach that automatically generates both faithful and\nhallucinated outputs by rewriting system responses. Experimental findings\ndemonstrate that a T5-base model, fine-tuned on our generated dataset,\nsurpasses state-of-the-art zero-shot detectors and existing synthetic\ngeneration methods in both accuracy and latency, indicating efficacy of our\napproach.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2407.05474v1",
    "published_date": "2024-07-07 19:19:32 UTC",
    "updated_date": "2024-07-07 19:19:32 UTC"
  },
  {
    "arxiv_id": "2407.05467v2",
    "title": "The infrastructure powering IBM's Gen AI model development",
    "authors": [
      "Talia Gershon",
      "Seetharami Seelam",
      "Brian Belgodere",
      "Milton Bonilla",
      "Lan Hoang",
      "Danny Barnett",
      "I-Hsin Chung",
      "Apoorve Mohan",
      "Ming-Hung Chen",
      "Lixiang Luo",
      "Robert Walkup",
      "Constantinos Evangelinos",
      "Shweta Salaria",
      "Marc Dombrowa",
      "Yoonho Park",
      "Apo Kayi",
      "Liran Schour",
      "Alim Alim",
      "Ali Sydney",
      "Pavlos Maniotis",
      "Laurent Schares",
      "Bernard Metzler",
      "Bengi Karacali-Akyamac",
      "Sophia Wen",
      "Tatsuhiro Chiba",
      "Sunyanan Choochotkaew",
      "Takeshi Yoshimura",
      "Claudia Misale",
      "Tonia Elengikal",
      "Kevin O Connor",
      "Zhuoran Liu",
      "Richard Molina",
      "Lars Schneidenbach",
      "James Caden",
      "Christopher Laibinis",
      "Carlos Fonseca",
      "Vasily Tarasov",
      "Swaminathan Sundararaman",
      "Frank Schmuck",
      "Scott Guthridge",
      "Jeremy Cohn",
      "Marc Eshel",
      "Paul Muench",
      "Runyu Liu",
      "William Pointer",
      "Drew Wyskida",
      "Bob Krull",
      "Ray Rose",
      "Brent Wolfe",
      "William Cornejo",
      "John Walter",
      "Colm Malone",
      "Clifford Perucci",
      "Frank Franco",
      "Nigel Hinds",
      "Bob Calio",
      "Pavel Druyan",
      "Robert Kilduff",
      "John Kienle",
      "Connor McStay",
      "Andrew Figueroa",
      "Matthew Connolly",
      "Edie Fost",
      "Gina Roma",
      "Jake Fonseca",
      "Ido Levy",
      "Michele Payne",
      "Ryan Schenkel",
      "Amir Malki",
      "Lion Schneider",
      "Aniruddha Narkhede",
      "Shekeba Moshref",
      "Alexandra Kisin",
      "Olga Dodin",
      "Bill Rippon",
      "Henry Wrieth",
      "John Ganci",
      "Johnny Colino",
      "Donna Habeger-Rose",
      "Rakesh Pandey",
      "Aditya Gidh",
      "Aditya Gaur",
      "Dennis Patterson",
      "Samsuddin Salmani",
      "Rambilas Varma",
      "Rumana Rumana",
      "Shubham Sharma",
      "Aditya Gaur",
      "Mayank Mishra",
      "Rameswar Panda",
      "Aditya Prasad",
      "Matt Stallone",
      "Gaoyuan Zhang",
      "Yikang Shen",
      "David Cox",
      "Ruchir Puri",
      "Dakshi Agrawal",
      "Drew Thorstensen",
      "Joel Belog",
      "Brent Tang",
      "Saurabh Kumar Gupta",
      "Amitabha Biswas",
      "Anup Maheshwari",
      "Eran Gampel",
      "Jason Van Patten",
      "Matthew Runion",
      "Sai Kaki",
      "Yigal Bogin",
      "Brian Reitz",
      "Steve Pritko",
      "Shahan Najam",
      "Surya Nambala",
      "Radhika Chirra",
      "Rick Welp",
      "Frank DiMitri",
      "Felipe Telles",
      "Amilcar Arvelo",
      "King Chu",
      "Ed Seminaro",
      "Andrew Schram",
      "Felix Eickhoff",
      "William Hanson",
      "Eric Mckeever",
      "Michael Light",
      "Dinakaran Joseph",
      "Piyush Chaudhary",
      "Piyush Shivam",
      "Puneet Chaudhary",
      "Wesley Jones",
      "Robert Guthrie",
      "Chris Bostic",
      "Rezaul Islam",
      "Steve Duersch",
      "Wayne Sawdon",
      "John Lewars",
      "Matthew Klos",
      "Michael Spriggs",
      "Bill McMillan",
      "George Gao",
      "Ashish Kamra",
      "Gaurav Singh",
      "Marc Curry",
      "Tushar Katarki",
      "Joe Talerico",
      "Zenghui Shi",
      "Sai Sindhur Malleni",
      "Erwan Gallen"
    ],
    "abstract": "AI Infrastructure plays a key role in the speed and cost-competitiveness of\ndeveloping and deploying advanced AI models. The current demand for powerful AI\ninfrastructure for model training is driven by the emergence of generative AI\nand foundational models, where on occasion thousands of GPUs must cooperate on\na single training job for the model to be trained in a reasonable time.\nDelivering efficient and high-performing AI training requires an end-to-end\nsolution that combines hardware, software and holistic telemetry to cater for\nmultiple types of AI workloads. In this report, we describe IBM's hybrid cloud\ninfrastructure that powers our generative AI model development. This\ninfrastructure includes (1) Vela: an AI-optimized supercomputing capability\ndirectly integrated into the IBM Cloud, delivering scalable, dynamic,\nmulti-tenant and geographically distributed infrastructure for large-scale\nmodel training and other AI workflow steps and (2) Blue Vela: a large-scale,\npurpose-built, on-premises hosting environment that is optimized to support our\nlargest and most ambitious AI model training tasks. Vela provides IBM with the\ndual benefit of high performance for internal use along with the flexibility to\nadapt to an evolving commercial landscape. Blue Vela provides us with the\nbenefits of rapid development of our largest and most ambitious models, as well\nas future-proofing against the evolving model landscape in the industry. Taken\ntogether, they provide IBM with the ability to rapidly innovate in the\ndevelopment of both AI models and commercial offerings.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "Corresponding Authors: Talia Gershon, Seetharami Seelam,Brian\n  Belgodere, Milton Bonilla",
    "pdf_url": "http://arxiv.org/pdf/2407.05467v2",
    "published_date": "2024-07-07 18:39:33 UTC",
    "updated_date": "2025-01-13 22:53:34 UTC"
  },
  {
    "arxiv_id": "2407.05466v1",
    "title": "Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality",
    "authors": [
      "Hao Li",
      "Gopi Krishnan Rajbahadur",
      "Cor-Paul Bezemer"
    ],
    "abstract": "Bindings for machine learning frameworks (such as TensorFlow and PyTorch)\nallow developers to integrate a framework's functionality using a programming\nlanguage different from the framework's default language (usually Python). In\nthis paper, we study the impact of using TensorFlow and PyTorch bindings in C#,\nRust, Python and JavaScript on the software quality in terms of correctness\n(training and test accuracy) and time cost (training and inference time) when\ntraining and performing inference on five widely used deep learning models. Our\nexperiments show that a model can be trained in one binding and used for\ninference in another binding for the same framework without losing accuracy.\nOur study is the first to show that using a non-default binding can help\nimprove machine learning software quality from the time cost perspective\ncompared to the default Python binding while still achieving the same level of\ncorrectness.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05466v1",
    "published_date": "2024-07-07 18:39:27 UTC",
    "updated_date": "2024-07-07 18:39:27 UTC"
  },
  {
    "arxiv_id": "2407.05464v1",
    "title": "Experiments with truth using Machine Learning: Spectral analysis and explainable classification of synthetic, false, and genuine information",
    "authors": [
      "Vishnu S. Pendyala",
      "Madhulika Dutta"
    ],
    "abstract": "Misinformation is still a major societal problem and the arrival of Large\nLanguage Models (LLMs) only added to it. This paper analyzes synthetic, false,\nand genuine information in the form of text from spectral analysis,\nvisualization, and explainability perspectives to find the answer to why the\nproblem is still unsolved despite multiple years of research and a plethora of\nsolutions in the literature. Various embedding techniques on multiple datasets\nare used to represent information for the purpose. The diverse spectral and\nnon-spectral methods used on these embeddings include t-distributed Stochastic\nNeighbor Embedding (t-SNE), Principal Component Analysis (PCA), and Variational\nAutoencoders (VAEs). Classification is done using multiple machine learning\nalgorithms. Local Interpretable Model-Agnostic Explanations (LIME), SHapley\nAdditive exPlanations (SHAP), and Integrated Gradients are used for the\nexplanation of the classification. The analysis and the explanations generated\nshow that misinformation is quite closely intertwined with genuine information\nand the machine learning algorithms are not as effective in separating the two\ndespite the claims in the literature.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05464v1",
    "published_date": "2024-07-07 18:31:09 UTC",
    "updated_date": "2024-07-07 18:31:09 UTC"
  },
  {
    "arxiv_id": "2407.11043v1",
    "title": "A Review of AI and Machine Learning Contribution in Predictive Business Process Management (Process Enhancement and Process Improvement Approaches)",
    "authors": [
      "Mostafa Abbasi",
      "Rahnuma Islam Nishat",
      "Corey Bond",
      "John Brandon Graham-Knight",
      "Patricia Lasserre",
      "Yves Lucet",
      "Homayoun Najjaran"
    ],
    "abstract": "Purpose- The significance of business processes has fostered a close\ncollaboration between academia and industry. Moreover, the business landscape\nhas witnessed continuous transformation, closely intertwined with technological\nadvancements. Our main goal is to offer researchers and process analysts\ninsights into the latest developments concerning Artificial Intelligence (AI)\nand Machine Learning (ML) to optimize their processes in an organization and\nidentify research gaps and future directions in the field.\nDesign/methodology/approach- In this study, we perform a systematic review of\nacademic literature to investigate the integration of AI/ML in business process\nmanagement (BPM). We categorize the literature according to the BPM life-cycle\nand employ bibliometric and objective-oriented methodology, to analyze related\npapers.\n  Findings- In business process management and process map, AI/ML has made\nsignificant improvements using operational data on process metrics. These\ndevelopments involve two distinct stages: (1) process enhancement, which\nemphasizes analyzing process information and adding descriptions to process\nmodels, and (2) process improvement, which focuses on redesigning processes\nbased on insights derived from analysis. Research limitations/implications-\nWhile this review paper serves to provide an overview of different approaches\nfor addressing process-related challenges, it does not delve deeply into the\nintricacies of fine-grained technical details of each method. This work focuses\non recent papers conducted between 2010 and 2024. Originality/value- This paper\nadopts a pioneering approach by conducting an extensive examination of the\nintegration of AI/ML techniques across the entire process management lifecycle.\nAdditionally, it presents groundbreaking research and introduces AI/ML-enabled\nintegrated tools, further enhancing the insights for future research.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11043v1",
    "published_date": "2024-07-07 18:26:00 UTC",
    "updated_date": "2024-07-07 18:26:00 UTC"
  },
  {
    "arxiv_id": "2407.05461v1",
    "title": "CAV-AD: A Robust Framework for Detection of Anomalous Data and Malicious Sensors in CAV Networks",
    "authors": [
      "Md Sazedur Rahman",
      "Mohamed Elmahallawy",
      "Sanjay Madria",
      "Samuel Frimpong"
    ],
    "abstract": "The adoption of connected and automated vehicles (CAVs) has sparked\nconsiderable interest across diverse industries, including public\ntransportation, underground mining, and agriculture sectors. However, CAVs'\nreliance on sensor readings makes them vulnerable to significant threats.\nManipulating these readings can compromise CAV network security, posing serious\nrisks for malicious activities. Although several anomaly detection (AD)\napproaches for CAV networks are proposed, they often fail to: i) detect\nmultiple anomalies in specific sensor(s) with high accuracy or F1 score, and\nii) identify the specific sensor being attacked. In response, this paper\nproposes a novel framework tailored to CAV networks, called CAV-AD, for\ndistinguishing abnormal readings amidst multiple anomaly data while identifying\nmalicious sensors. Specifically, CAV-AD comprises two main components: i) A\nnovel CNN model architecture called optimized omni-scale CNN (O-OS-CNN), which\noptimally selects the time scale by generating all possible kernel sizes for\ninput time series data; ii) An amplification block to increase the values of\nanomaly readings, enhancing sensitivity for detecting anomalies. Not only that,\nbut CAV-AD integrates the proposed O-OS-CNN with a Kalman filter to instantly\nidentify the malicious sensors. We extensively train CAV-AD using real-world\ndatasets containing both instant and constant attacks, evaluating its\nperformance in detecting intrusions from multiple anomalies, which presents a\nmore challenging scenario. Our results demonstrate that CAV-AD outperforms\nstate-of-the-art methods, achieving an average accuracy of 98% and an average\nF1 score of 89\\%, while accurately identifying the malicious sensors.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05461v1",
    "published_date": "2024-07-07 18:19:03 UTC",
    "updated_date": "2024-07-07 18:19:03 UTC"
  },
  {
    "arxiv_id": "2407.05458v1",
    "title": "A Survey of Models for Cognitive Diagnosis: New Developments and Future Directions",
    "authors": [
      "Fei Wang",
      "Weibo Gao",
      "Qi Liu",
      "Jiatong Li",
      "Guanhao Zhao",
      "Zheng Zhang",
      "Zhenya Huang",
      "Mengxiao Zhu",
      "Shijin Wang",
      "Wei Tong",
      "Enhong Chen"
    ],
    "abstract": "Cognitive diagnosis has been developed for decades as an effective\nmeasurement tool to evaluate human cognitive status such as ability level and\nknowledge mastery. It has been applied to a wide range of fields including\neducation, sport, psychological diagnosis, etc. By providing better awareness\nof cognitive status, it can serve as the basis for personalized services such\nas well-designed medical treatment, teaching strategy and vocational training.\nThis paper aims to provide a survey of current models for cognitive diagnosis,\nwith more attention on new developments using machine learning-based methods.\nBy comparing the model structures, parameter estimation algorithms, model\nevaluation methods and applications, we provide a relatively comprehensive\nreview of the recent trends in cognitive diagnosis models. Further, we discuss\nfuture directions that are worthy of exploration. In addition, we release two\nPython libraries: EduData for easy access to some relevant public datasets we\nhave collected, and EduCDM that implements popular CDMs to facilitate both\napplications and research purposes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05458v1",
    "published_date": "2024-07-07 18:02:00 UTC",
    "updated_date": "2024-07-07 18:02:00 UTC"
  },
  {
    "arxiv_id": "2407.05449v2",
    "title": "SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers for Text Detoxification",
    "authors": [
      "Elisei Rykov",
      "Konstantin Zaytsev",
      "Ivan Anisimov",
      "Alexandr Voronin"
    ],
    "abstract": "This paper presents a solution for the Multilingual Text Detoxification task\nin the PAN-2024 competition of the SmurfCat team. Using data augmentation\nthrough machine translation and a special filtering procedure, we collected an\nadditional multilingual parallel dataset for text detoxification. Using the\nobtained data, we fine-tuned several multilingual sequence-to-sequence models,\nsuch as mT0 and Aya, on a text detoxification task. We applied the ORPO\nalignment technique to the final model. Our final model has only 3.7 billion\nparameters and achieves state-of-the-art results for the Ukrainian language and\nnear state-of-the-art results for other languages. In the competition, our team\nachieved first place in the automated evaluation with a score of 0.52 and\nsecond place in the final human evaluation with a score of 0.74.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05449v2",
    "published_date": "2024-07-07 17:19:34 UTC",
    "updated_date": "2024-07-10 14:44:18 UTC"
  },
  {
    "arxiv_id": "2407.05441v4",
    "title": "Language Representations Can be What Recommenders Need: Findings and Potentials",
    "authors": [
      "Leheng Sheng",
      "An Zhang",
      "Yi Zhang",
      "Yuxin Chen",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "Recent studies empirically indicate that language models (LMs) encode rich\nworld knowledge beyond mere semantics, attracting significant attention across\nvarious fields. However, in the recommendation domain, it remains uncertain\nwhether LMs implicitly encode user preference information. Contrary to\nprevailing understanding that LMs and traditional recommenders learn two\ndistinct representation spaces due to the huge gap in language and behavior\nmodeling objectives, this work re-examines such understanding and explores\nextracting a recommendation space directly from the language representation\nspace. Surprisingly, our findings demonstrate that item representations, when\nlinearly mapped from advanced LM representations, yield superior recommendation\nperformance. This outcome suggests the possible homomorphism between the\nadvanced language representation space and an effective item representation\nspace for recommendation, implying that collaborative signals may be implicitly\nencoded within LMs. Motivated by these findings, we explore the possibility of\ndesigning advanced collaborative filtering (CF) models purely based on language\nrepresentations without ID-based embeddings. To be specific, we incorporate\nseveral crucial components to build a simple yet effective model, with item\ntitles as the input. Empirical results show that such a simple model can\noutperform leading ID-based CF models, which sheds light on using language\nrepresentations for better recommendation. Moreover, we systematically analyze\nthis simple model and find several key features for using advanced language\nrepresentations: a good initialization for item representations, zero-shot\nrecommendation abilities, and being aware of user intention. Our findings\nhighlight the connection between language modeling and behavior modeling, which\ncan inspire both natural language processing and recommender system\ncommunities.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "ICLR 2025 (Oral). Codes are available at\n  https://github.com/LehengTHU/AlphaRec",
    "pdf_url": "http://arxiv.org/pdf/2407.05441v4",
    "published_date": "2024-07-07 17:05:24 UTC",
    "updated_date": "2025-04-21 03:45:36 UTC"
  },
  {
    "arxiv_id": "2407.05440v2",
    "title": "Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification",
    "authors": [
      "P. N. Karthikayan",
      "Yoga Sri Varshan V",
      "Hitesh Gupta Kattamuri",
      "Umarani Jayaraman"
    ],
    "abstract": "This paper presents dilated Residual Network (ResNet) models for disease\nclassification from retinal fundus images. Dilated convolution filters are used\nto replace normal convolution filters in the higher layers of the ResNet model\n(dilated ResNet) in order to improve the receptive field compared to the normal\nResNet model for disease classification. This study introduces\ncomputer-assisted diagnostic tools that employ deep learning, enhanced with\nexplainable AI techniques. These techniques aim to make the tool's\ndecision-making process transparent, thereby enabling medical professionals to\nunderstand and trust the AI's diagnostic decision. They are particularly\nrelevant in today's healthcare landscape, where there is a growing demand for\ntransparency in AI applications to ensure their reliability and ethical use.\nThe dilated ResNet is used as a replacement for the normal ResNet to enhance\nthe classification accuracy of retinal eye diseases and reduce the required\ncomputing time. The dataset used in this work is the Ocular Disease Intelligent\nRecognition (ODIR) dataset which is a structured ophthalmic database with eight\nclasses covering most of the common retinal eye diseases. The evaluation\nmetrics used in this work include precision, recall, accuracy, and F1 score. In\nthis work, a comparative study has been made between normal ResNet models and\ndilated ResNet models on five variants namely ResNet-18, ResNet-34, ResNet-50,\nResNet-101, and ResNet-152. The dilated ResNet model shows promising results as\ncompared to normal ResNet with an average F1 score of 0.71, 0.70, 0.69, 0.67,\nand 0.70 respectively for the above respective variants in ODIR multiclass\ndisease classification.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Added authors' contributions",
    "pdf_url": "http://arxiv.org/pdf/2407.05440v2",
    "published_date": "2024-07-07 17:03:12 UTC",
    "updated_date": "2024-08-31 20:26:49 UTC"
  },
  {
    "arxiv_id": "2407.05437v1",
    "title": "Enhancing Computer Programming Education with LLMs: A Study on Effective Prompt Engineering for Python Code Generation",
    "authors": [
      "Tianyu Wang",
      "Nianjun Zhou",
      "Zhixiong Chen"
    ],
    "abstract": "Large language models (LLMs) and prompt engineering hold significant\npotential for advancing computer programming education through personalized\ninstruction. This paper explores this potential by investigating three critical\nresearch questions: the systematic categorization of prompt engineering\nstrategies tailored to diverse educational needs, the empowerment of LLMs to\nsolve complex problems beyond their inherent capabilities, and the\nestablishment of a robust framework for evaluating and implementing these\nstrategies. Our methodology involves categorizing programming questions based\non educational requirements, applying various prompt engineering strategies,\nand assessing the effectiveness of LLM-generated responses. Experiments with\nGPT-4, GPT-4o, Llama3-8b, and Mixtral-8x7b models on datasets such as LeetCode\nand USACO reveal that GPT-4o consistently outperforms others, particularly with\nthe \"multi-step\" prompt strategy. The results show that tailored prompt\nstrategies significantly enhance LLM performance, with specific strategies\nrecommended for foundational learning, competition preparation, and advanced\nproblem-solving. This study underscores the crucial role of prompt engineering\nin maximizing the educational benefits of LLMs. By systematically categorizing\nand testing these strategies, we provide a comprehensive framework for both\neducators and students to optimize LLM-based learning experiences. Future\nresearch should focus on refining these strategies and addressing current LLM\nlimitations to further enhance educational outcomes in computer programming\ninstruction.",
    "categories": [
      "cs.AI",
      "K.3.2; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.05437v1",
    "published_date": "2024-07-07 16:41:07 UTC",
    "updated_date": "2024-07-07 16:41:07 UTC"
  },
  {
    "arxiv_id": "2407.05434v1",
    "title": "LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models",
    "authors": [
      "Weizhi Tang",
      "Vaishak Belle"
    ],
    "abstract": "Temporal reasoning (TR) is a critical component of artificial intelligence,\nencompassing understanding and processing temporal information and\nrelationships between events. To discover and study the TR ability in Large\nLanguage Models (LLMs), various datasets have been constructed in different\nways for evaluating various aspects of TR ability. Our work proposes a novel\napproach to design and develop a pipeline for constructing datasets to evaluate\nthe TR ability of LLMs by leveraging random directed graph generation, LTL\nformula, and the NuSMV model checker. Based on the pipeline, we have also\nconstructed a dataset as a benchmark, namely LTLBench, consisting of 2,000 TR\nchallenges and evaluated six LLMs with it. Furthermore, we have conducted\nadditional experiments to discover the impact of increasing the number of\nevents and formula operators on the complexity of TR problems and the\nperformance of LLMs. We have demonstrated that although LLMs exhibit some\npromise in handling TR challenges, they still struggle with complex TR. We\nexpect this work can offer insights into TR ability in LLMs while also\nproviding a valuable tool for future TR evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05434v1",
    "published_date": "2024-07-07 16:37:06 UTC",
    "updated_date": "2024-07-07 16:37:06 UTC"
  },
  {
    "arxiv_id": "2407.05424v1",
    "title": "BiRoDiff: Diffusion policies for bipedal robot locomotion on unseen terrains",
    "authors": [
      "GVS Mothish",
      "Manan Tayal",
      "Shishir Kolathaya"
    ],
    "abstract": "Locomotion on unknown terrains is essential for bipedal robots to handle\nnovel real-world challenges, thus expanding their utility in disaster response\nand exploration. In this work, we introduce a lightweight framework that learns\na single walking controller that yields locomotion on multiple terrains. We\nhave designed a real-time robot controller based on diffusion models, which not\nonly captures multiple behaviours with different velocities in a single policy\nbut also generalizes well for unseen terrains. Our controller learns with\noffline data, which is better than online learning in aspects like scalability,\nsimplicity in training scheme etc. We have designed and implemented a diffusion\nmodel-based policy controller in simulation on our custom-made Bipedal Robot\nmodel named Stoch BiRo. We have demonstrated its generalization capability and\nhigh frequency control step generation relative to typical generative models,\nwhich require huge onboarding compute.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.05424v1",
    "published_date": "2024-07-07 16:03:33 UTC",
    "updated_date": "2024-07-07 16:03:33 UTC"
  },
  {
    "arxiv_id": "2407.05418v1",
    "title": "EMBANet: A Flexible Efffcient Multi-branch Attention Network",
    "authors": [
      "Keke Zu",
      "Hu Zhang",
      "Jian Lu",
      "Lei Zhang",
      "Chen Xu"
    ],
    "abstract": "This work presents a novel module, namely multi-branch concat (MBC), to\nprocess the input tensor and obtain the multi-scale feature map. The proposed\nMBC module brings new degrees of freedom (DoF) for the design of attention\nnetworks by allowing the type of transformation operators and the number of\nbranches to be flexibly adjusted. Two important transformation operators,\nmultiplex and split, are considered in this work, both of which can represent\nmulti-scale features at a more granular level and increase the range of\nreceptive fields. By integrating the MBC and attention module, a multi-branch\nattention (MBA) module is consequently developed to capture the channel-wise\ninteraction of feature maps for establishing the long-range channel dependency.\nBy substituting the 3x3 convolutions in the bottleneck blocks of the ResNet\nwith the proposed MBA, a novel block namely efficient multi-branch attention\n(EMBA) is obtained, which can be easily plugged into the state-of-the-art\nbackbone CNN models. Furthermore, a new backbone network called EMBANet is\nestablished by stacking the EMBA blocks. The proposed EMBANet is extensively\nevaluated on representative computer vision tasks including: classification,\ndetection, and segmentation. And it demonstrates consistently superior\nperformance over the popular backbones.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05418v1",
    "published_date": "2024-07-07 15:50:01 UTC",
    "updated_date": "2024-07-07 15:50:01 UTC"
  },
  {
    "arxiv_id": "2407.05417v2",
    "title": "See Further for Parameter Efficient Fine-tuning by Standing on the Shoulders of Decomposition",
    "authors": [
      "Chongjie Si",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "abstract": "The rapid expansion of large foundation models within the pre-training and\nfine-tuning framework has underscored that larger models often yield better\nresults. However, the scaling up of large foundation models has led to soaring\ncosts in fine-tuning and parameter storage, rendering extensive adaptations\nimpractical. This challenge has sparked the development of parameter-efficient\nfine-tuning (PEFT), which focuses on optimizing a select subset of parameters\nwhile keeping the rest fixed, significantly lowering computational and storage\noverheads. While recent years have witnessed a significant success in PEFT, a\ndeep understanding of the fundamental principles behind these methods remains\nunexplored. To this end, here we take the first step to unify all approaches by\ndissecting them from a decomposition perspective. We initiate a comprehensive\nmathematical analysis of these methods, allowing us to delve deeply into their\nunderlying mechanisms, and we explore the reasons behind the variations in\nperformance among different techniques. Furthermore, inspired by our\ntheoretical analysis, we introduce two novel PEFT methods alongside a simple\nyet effective framework designed to enhance the performance of PEFT techniques\nacross various applications. Our empirical validations, conducted across\nmultiple datasets, demonstrate the efficacy of these methods, showcasing both\ntheoretical validity and practical performance improvements under the guidance\nof our analytical findings. We believe our work will deepen researchers'\nunderstanding of PEFT and other techniques, prompting further contemplation and\nadvancing the research across the whole community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Codes in https://github.com/Chongjie-Si/Subspace-Tuning",
    "pdf_url": "http://arxiv.org/pdf/2407.05417v2",
    "published_date": "2024-07-07 15:44:42 UTC",
    "updated_date": "2024-12-25 11:55:30 UTC"
  },
  {
    "arxiv_id": "2407.05413v3",
    "title": "SBoRA: Low-Rank Adaptation with Regional Weight Updates",
    "authors": [
      "Lai-Man Po",
      "Yuyang Liu",
      "Haoxuan Wu",
      "Tianqi Zhang",
      "Wing-Yin Yu",
      "Zhuohan Wang",
      "Zeyu Jiang",
      "Kun Li"
    ],
    "abstract": "This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA reduces the number of trainable parameters by half or doubles\nthe rank with the similar number of trainable parameters as LoRA, while\nimproving learning performance. By utilizing orthogonal standard basis vectors\nto initialize one of the low-rank matrices (either $\\mathbf{A}$ or\n$\\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient\nfine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only\none of the matrices is updated, leading to a sparse update matrix\n$\\mathrm{\\Delta} \\mathbf{W}$ with predominantly zero rows or columns.\nConsequently, most of the fine-tuned model's weights\n$(\\mathbf{W}_0+\\mathrm{\\Delta} \\mathbf{W})$ remain unchanged from the\npre-trained weights, akin to the modular organization of the human brain, which\nefficiently adapts to new tasks. Our empirical results demonstrate the\nsuperiority of SBoRA-FA over LoRA in various fine-tuning tasks, including\ncommonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the\neffectiveness of QSBoRA on quantized LLaMA models of varying scales,\nhighlighting its potential for efficient adaptation to new tasks. Code is\navailable at https://github.com/cityuhkai/SBoRA",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.05413v3",
    "published_date": "2024-07-07 15:37:13 UTC",
    "updated_date": "2024-10-09 07:53:10 UTC"
  },
  {
    "arxiv_id": "2407.05412v1",
    "title": "FM-OSD: Foundation Model-Enabled One-Shot Detection of Anatomical Landmarks",
    "authors": [
      "Juzheng Miao",
      "Cheng Chen",
      "Keli Zhang",
      "Jie Chuai",
      "Quanzheng Li",
      "Pheng-Ann Heng"
    ],
    "abstract": "One-shot detection of anatomical landmarks is gaining significant attention\nfor its efficiency in using minimal labeled data to produce promising results.\nHowever, the success of current methods heavily relies on the employment of\nextensive unlabeled data to pre-train an effective feature extractor, which\nlimits their applicability in scenarios where a substantial amount of unlabeled\ndata is unavailable. In this paper, we propose the first foundation\nmodel-enabled one-shot landmark detection (FM-OSD) framework for accurate\nlandmark detection in medical images by utilizing solely a single template\nimage without any additional unlabeled data. Specifically, we use the frozen\nimage encoder of visual foundation models as the feature extractor, and\nintroduce dual-branch global and local feature decoders to increase the\nresolution of extracted features in a coarse to fine manner. The introduced\nfeature decoders are efficiently trained with a distance-aware similarity\nlearning loss to incorporate domain knowledge from the single template image.\nMoreover, a novel bidirectional matching strategy is developed to improve both\nrobustness and accuracy of landmark detection in the case of scattered\nsimilarity map obtained by foundation models. We validate our method on two\npublic anatomical landmark detection datasets. By using solely a single\ntemplate image, our method demonstrates significant superiority over strong\nstate-of-the-art one-shot landmark detection methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05412v1",
    "published_date": "2024-07-07 15:37:02 UTC",
    "updated_date": "2024-07-07 15:37:02 UTC"
  },
  {
    "arxiv_id": "2407.05407v2",
    "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
    "authors": [
      "Zhihao Du",
      "Qian Chen",
      "Shiliang Zhang",
      "Kai Hu",
      "Heng Lu",
      "Yexin Yang",
      "Hangrui Hu",
      "Siqi Zheng",
      "Yue Gu",
      "Ziyang Ma",
      "Zhifu Gao",
      "Zhijie Yan"
    ],
    "abstract": "Recent years have witnessed a trend that large language model (LLM) based\ntext-to-speech (TTS) emerges into the mainstream due to their high naturalness\nand zero-shot capacity. In this paradigm, speech signals are discretized into\ntoken sequences, which are modeled by an LLM with text as prompts and\nreconstructed by a token-based vocoder to waveforms. Obviously, speech tokens\nplay a critical role in LLM-based TTS models. Current speech tokens are learned\nin an unsupervised manner, which lacks explicit semantic information and\nalignment to the text. In this paper, we propose to represent speech with\nsupervised semantic tokens, which are derived from a multilingual speech\nrecognition model by inserting vector quantization into the encoder. Based on\nthe tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice,\nwhich consists of an LLM for text-to-token generation and a conditional flow\nmatching model for token-to-speech synthesis. Experimental results show that\nsupervised semantic tokens significantly outperform existing unsupervised\ntokens in terms of content consistency and speaker similarity for zero-shot\nvoice cloning. Moreover, we find that utilizing large-scale data further\nimproves the synthesis performance, indicating the scalable capacity of\nCosyVoice. To the best of our knowledge, this is the first attempt to involve\nsupervised speech tokens into TTS models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "work in progress. arXiv admin note: substantial text overlap with\n  arXiv:2407.04051",
    "pdf_url": "http://arxiv.org/pdf/2407.05407v2",
    "published_date": "2024-07-07 15:16:19 UTC",
    "updated_date": "2024-07-09 07:42:51 UTC"
  },
  {
    "arxiv_id": "2407.05404v1",
    "title": "iSign: A Benchmark for Indian Sign Language Processing",
    "authors": [
      "Abhinav Joshi",
      "Romit Mohanty",
      "Mounika Kanakanti",
      "Andesha Mangla",
      "Sudeep Choudhary",
      "Monali Barbate",
      "Ashutosh Modi"
    ],
    "abstract": "Indian Sign Language has limited resources for developing machine learning\nand data-driven approaches for automated language processing. Though\ntext/audio-based language processing techniques have shown colossal research\ninterest and tremendous improvements in the last few years, Sign Languages\nstill need to catch up due to the need for more resources. To bridge this gap,\nin this work, we propose iSign: a benchmark for Indian Sign Language (ISL)\nProcessing. We make three primary contributions to this work. First, we release\none of the largest ISL-English datasets with more than 118K\nvideo-sentence/phrase pairs. To the best of our knowledge, it is the largest\nsign language dataset available for ISL. Second, we propose multiple\nNLP-specific tasks (including SignVideo2Text, SignPose2Text, Text2Pose, Word\nPrediction, and Sign Semantics) and benchmark them with the baseline models for\neasier access to the research community. Third, we provide detailed insights\ninto the proposed benchmarks with a few linguistic insights into the workings\nof ISL. We streamline the evaluation of Sign Language processing, addressing\nthe gaps in the NLP research community for Sign Languages. We release the\ndataset, tasks, and models via the following website:\nhttps://exploration-lab.github.io/iSign/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 Findings. 18 Pages (9 Pages + References +\n  Appendix)",
    "pdf_url": "http://arxiv.org/pdf/2407.05404v1",
    "published_date": "2024-07-07 15:07:35 UTC",
    "updated_date": "2024-07-07 15:07:35 UTC"
  },
  {
    "arxiv_id": "2407.05399v2",
    "title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning",
    "authors": [
      "Abhinav Joshi",
      "Shounak Paul",
      "Akshat Sharma",
      "Pawan Goyal",
      "Saptarshi Ghosh",
      "Ashutosh Modi"
    ],
    "abstract": "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 Main Conference; 40 Pages (9 Pages + References\n  + Appendix)",
    "pdf_url": "http://arxiv.org/pdf/2407.05399v2",
    "published_date": "2024-07-07 14:55:04 UTC",
    "updated_date": "2024-11-26 08:48:42 UTC"
  },
  {
    "arxiv_id": "2407.05398v1",
    "title": "A Fair Post-Processing Method based on the MADD Metric for Predictive Student Models",
    "authors": [
      "Mélina Verger",
      "Chunyang Fan",
      "Sébastien Lallé",
      "François Bouchet",
      "Vanda Luengo"
    ],
    "abstract": "Predictive student models are increasingly used in learning environments.\nHowever, due to the rising social impact of their usage, it is now all the more\nimportant for these models to be both sufficiently accurate and fair in their\npredictions. To evaluate algorithmic fairness, a new metric has been developed\nin education, namely the Model Absolute Density Distance (MADD). This metric\nenables us to measure how different a predictive model behaves regarding two\ngroups of students, in order to quantify its algorithmic unfairness. In this\npaper, we thus develop a post-processing method based on this metric, that aims\nat improving the fairness while preserving the accuracy of relevant predictive\nmodels' results. We experiment with our approach on the task of predicting\nstudent success in an online course, using both simulated and real-world\neducational data, and obtain successful results. Our source code and data are\nin open access at https://github.com/melinaverger/MADD .",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DM",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CY",
    "comment": "1st International Tutorial and Workshop on Responsible Knowledge\n  Discovery in Education (RKDE 2023) at ECML PKDD 2023, September 2023, Turino,\n  Italy",
    "pdf_url": "http://arxiv.org/pdf/2407.05398v1",
    "published_date": "2024-07-07 14:53:41 UTC",
    "updated_date": "2024-07-07 14:53:41 UTC"
  },
  {
    "arxiv_id": "2407.05396v2",
    "title": "Evolutionary Trigger Detection and Lightweight Model Repair Based Backdoor Defense",
    "authors": [
      "Qi Zhou",
      "Zipeng Ye",
      "Yubo Tang",
      "Wenjian Luo",
      "Yuhui Shi",
      "Yan Jia"
    ],
    "abstract": "Deep Neural Networks (DNNs) have been widely used in many areas such as\nautonomous driving and face recognition. However, DNN model is fragile to\nbackdoor attack. A backdoor in the DNN model can be activated by a poisoned\ninput with trigger and leads to wrong prediction, which causes serious security\nissues in applications. It is challenging for current defenses to eliminate the\nbackdoor effectively with limited computing resources, especially when the\nsizes and numbers of the triggers are variable as in the physical world. We\npropose an efficient backdoor defense based on evolutionary trigger detection\nand lightweight model repair. In the first phase of our method, CAM-focus\nEvolutionary Trigger Filter (CETF) is proposed for trigger detection. CETF is\nan effective sample-preprocessing based method with the evolutionary algorithm,\nand our experimental results show that CETF not only distinguishes the images\nwith triggers accurately from the clean images, but also can be widely used in\npractice for its simplicity and stability in different backdoor attack\nsituations. In the second phase of our method, we leverage several lightweight\nunlearning methods with the trigger detected by CETF for model repair, which\nalso constructively demonstrate the underlying correlation of the backdoor with\nBatch Normalization layers. Source code will be published after accepted.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.05396v2",
    "published_date": "2024-07-07 14:50:59 UTC",
    "updated_date": "2024-07-14 08:25:25 UTC"
  },
  {
    "arxiv_id": "2407.05389v1",
    "title": "Image-Conditional Diffusion Transformer for Underwater Image Enhancement",
    "authors": [
      "Xingyang Nie",
      "Su Pan",
      "Xiaoyu Zhai",
      "Shifei Tao",
      "Fengzhong Qu",
      "Biao Wang",
      "Huilin Ge",
      "Guojie Xiao"
    ],
    "abstract": "Underwater image enhancement (UIE) has attracted much attention owing to its\nimportance for underwater operation and marine engineering. Motivated by the\nrecent advance in generative models, we propose a novel UIE method based on\nimage-conditional diffusion transformer (ICDT). Our method takes the degraded\nunderwater image as the conditional input and converts it into latent space\nwhere ICDT is applied. ICDT replaces the conventional U-Net backbone in a\ndenoising diffusion probabilistic model (DDPM) with a transformer, and thus\ninherits favorable properties such as scalability from transformers.\nFurthermore, we train ICDT with a hybrid loss function involving variances to\nachieve better log-likelihoods, which meanwhile significantly accelerates the\nsampling process. We experimentally assess the scalability of ICDTs and compare\nwith prior works in UIE on the Underwater ImageNet dataset. Besides good\nscaling properties, our largest model, ICDT-XL/2, outperforms all comparison\nmethods, achieving state-of-the-art (SOTA) quality of image enhancement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05389v1",
    "published_date": "2024-07-07 14:34:31 UTC",
    "updated_date": "2024-07-07 14:34:31 UTC"
  },
  {
    "arxiv_id": "2407.05385v1",
    "title": "Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis",
    "authors": [
      "Stefan Horoi",
      "Albert Manuel Orozco Camacho",
      "Eugene Belilovsky",
      "Guy Wolf"
    ],
    "abstract": "Combining the predictions of multiple trained models through ensembling is\ngenerally a good way to improve accuracy by leveraging the different learned\nfeatures of the models, however it comes with high computational and storage\ncosts. Model fusion, the act of merging multiple models into one by combining\ntheir parameters reduces these costs but doesn't work as well in practice.\nIndeed, neural network loss landscapes are high-dimensional and non-convex and\nthe minima found through learning are typically separated by high loss\nbarriers. Numerous recent works have been focused on finding permutations\nmatching one network features to the features of a second one, lowering the\nloss barrier on the linear path between them in parameter space. However,\npermutations are restrictive since they assume a one-to-one mapping between the\ndifferent models' neurons exists. We propose a new model merging algorithm, CCA\nMerge, which is based on Canonical Correlation Analysis and aims to maximize\nthe correlations between linear combinations of the model features. We show\nthat our alignment method leads to better performances than past methods when\naveraging models trained on the same, or differing data splits. We also extend\nthis analysis into the harder setting where more than 2 models are merged, and\nwe find that CCA Merge works significantly better than past methods. Our code\nis publicly available at https://github.com/shoroi/align-n-merge",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the Forty-first International Conference on Machine\n  Learning (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.05385v1",
    "published_date": "2024-07-07 14:21:04 UTC",
    "updated_date": "2024-07-07 14:21:04 UTC"
  },
  {
    "arxiv_id": "2407.05379v1",
    "title": "AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams under Extreme Verification Latency",
    "authors": [
      "Maria Arostegi",
      "Miren Nekane Bilbao",
      "Jesus L. Lobo",
      "Javier Del Ser"
    ],
    "abstract": "The ever-growing speed at which data are generated nowadays, together with\nthe substantial cost of labeling processes cause Machine Learning models to\nface scenarios in which data are partially labeled. The extreme case where such\na supervision is indefinitely unavailable is referred to as extreme\nverification latency. On the other hand, in streaming setups data flows are\naffected by exogenous factors that yield non-stationarities in the patterns\n(concept drift), compelling models learned incrementally from the data streams\nto adapt their modeled knowledge to the concepts within the stream. In this\nwork we address the casuistry in which these two conditions occur together, by\nwhich adaptation mechanisms to accommodate drifts within the stream are\nchallenged by the lack of supervision, requiring further mechanisms to track\nthe evolution of concepts in the absence of verification. To this end we\npropose a novel approach, AiGAS-dEVL (Adaptive Incremental neural GAS model for\ndrifting Streams under Extreme Verification Latency), which relies on growing\nneural gas to characterize the distributions of all concepts detected within\nthe stream over time. Our approach exposes that the online analysis of the\nbehavior of these prototypical points over time facilitates the definition of\nthe evolution of concepts in the feature space, the detection of changes in\ntheir behavior, and the design of adaptation policies to mitigate the effect of\nsuch changes in the model. We assess the performance of AiGAS-dEVL over several\nsynthetic datasets, comparing it to that of state-of-the-art approaches\nproposed in the recent past to tackle this stream learning setup. Our results\nreveal that AiGAS-dEVL performs competitively with respect to the rest of\nbaselines, exhibiting a superior adaptability over several datasets in the\nbenchmark while ensuring a simple and interpretable instance-based adaptation\nstrategy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "68T05",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 3 tables, 4 figures. Under review",
    "pdf_url": "http://arxiv.org/pdf/2407.05379v1",
    "published_date": "2024-07-07 14:04:57 UTC",
    "updated_date": "2024-07-07 14:04:57 UTC"
  },
  {
    "arxiv_id": "2407.05377v1",
    "title": "Collective Innovation in Groups of Large Language Models",
    "authors": [
      "Eleni Nisioti",
      "Sebastian Risi",
      "Ida Momennejad",
      "Pierre-Yves Oudeyer",
      "Clément Moulin-Frier"
    ],
    "abstract": "Human culture relies on collective innovation: our ability to continuously\nexplore how existing elements in our environment can be combined to create new\nones. Language is hypothesized to play a key role in human culture, driving\nindividual cognitive capacities and shaping communication. Yet the majority of\nmodels of collective innovation assign no cognitive capacities or language\nabilities to agents. Here, we contribute a computational study of collective\ninnovation where agents are Large Language Models (LLMs) that play Little\nAlchemy 2, a creative video game originally developed for humans that, as we\nargue, captures useful aspects of innovation landscapes not present in previous\ntest-beds. We, first, study an LLM in isolation and discover that it exhibits\nboth useful skills and crucial limitations. We, then, study groups of LLMs that\nshare information related to their behaviour and focus on the effect of social\nconnectivity on collective performance. In agreement with previous human and\ncomputational studies, we observe that groups with dynamic connectivity\nout-compete fully-connected groups. Our work reveals opportunities and\nchallenges for future studies of collective innovation that are becoming\nincreasingly relevant as Generative Artificial Intelligence algorithms and\nhumans innovate alongside each other.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05377v1",
    "published_date": "2024-07-07 13:59:46 UTC",
    "updated_date": "2024-07-07 13:59:46 UTC"
  },
  {
    "arxiv_id": "2407.05375v1",
    "title": "Online Drift Detection with Maximum Concept Discrepancy",
    "authors": [
      "Ke Wan",
      "Yi Liang",
      "Susik Yoon"
    ],
    "abstract": "Continuous learning from an immense volume of data streams becomes\nexceptionally critical in the internet era. However, data streams often do not\nconform to the same distribution over time, leading to a phenomenon called\nconcept drift. Since a fixed static model is unreliable for inferring\nconcept-drifted data streams, establishing an adaptive mechanism for detecting\nconcept drift is crucial. Current methods for concept drift detection primarily\nassume that the labels or error rates of downstream models are given and/or\nunderlying statistical properties exist in data streams. These approaches,\nhowever, struggle to address high-dimensional data streams with intricate\nirregular distribution shifts, which are more prevalent in real-world\nscenarios. In this paper, we propose MCD-DD, a novel concept drift detection\nmethod based on maximum concept discrepancy, inspired by the maximum mean\ndiscrepancy. Our method can adaptively identify varying forms of concept drift\nby contrastive learning of concept embeddings without relying on labels or\nstatistical properties. With thorough experiments under synthetic and\nreal-world scenarios, we demonstrate that the proposed method outperforms\nexisting baselines in identifying concept drifts and enables qualitative\nanalysis with high explainability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05375v1",
    "published_date": "2024-07-07 13:57:50 UTC",
    "updated_date": "2024-07-07 13:57:50 UTC"
  },
  {
    "arxiv_id": "2407.05368v1",
    "title": "Music Era Recognition Using Supervised Contrastive Learning and Artist Information",
    "authors": [
      "Qiqi He",
      "Xuchen Song",
      "Weituo Hao",
      "Ju-Chiang Wang",
      "Wei-Tsung Lu",
      "Wei Li"
    ],
    "abstract": "Does popular music from the 60s sound different than that of the 90s? Prior\nstudy has shown that there would exist some variations of patterns and\nregularities related to instrumentation changes and growing loudness across\nmulti-decadal trends. This indicates that perceiving the era of a song from\nmusical features such as audio and artist information is possible. Music era\ninformation can be an important feature for playlist generation and\nrecommendation. However, the release year of a song can be inaccessible in many\ncircumstances. This paper addresses a novel task of music era recognition. We\nformulate the task as a music classification problem and propose solutions\nbased on supervised contrastive learning. An audio-based model is developed to\npredict the era from audio. For the case where the artist information is\navailable, we extend the audio-based model to take multimodal inputs and\ndevelop a framework, called MultiModal Contrastive (MMC) learning, to enhance\nthe training. Experimental result on Million Song Dataset demonstrates that the\naudio-based model achieves 54% in accuracy with a tolerance of 3-years range;\nincorporating the artist information with the MMC framework for training leads\nto 9% improvement further.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05368v1",
    "published_date": "2024-07-07 13:43:55 UTC",
    "updated_date": "2024-07-07 13:43:55 UTC"
  },
  {
    "arxiv_id": "2407.05365v2",
    "title": "ElecBench: a Power Dispatch Evaluation Benchmark for Large Language Models",
    "authors": [
      "Xiyuan Zhou",
      "Huan Zhao",
      "Yuheng Cheng",
      "Yuji Cao",
      "Gaoqi Liang",
      "Guolong Liu",
      "Wenxuan Liu",
      "Yan Xu",
      "Junhua Zhao"
    ],
    "abstract": "In response to the urgent demand for grid stability and the complex\nchallenges posed by renewable energy integration and electricity market\ndynamics, the power sector increasingly seeks innovative technological\nsolutions. In this context, large language models (LLMs) have become a key\ntechnology to improve efficiency and promote intelligent progress in the power\nsector with their excellent natural language processing, logical reasoning, and\ngeneralization capabilities. Despite their potential, the absence of a\nperformance evaluation benchmark for LLM in the power sector has limited the\neffective application of these technologies. Addressing this gap, our study\nintroduces \"ElecBench\", an evaluation benchmark of LLMs within the power\nsector. ElecBench aims to overcome the shortcomings of existing evaluation\nbenchmarks by providing comprehensive coverage of sector-specific scenarios,\ndeepening the testing of professional knowledge, and enhancing decision-making\nprecision. The framework categorizes scenarios into general knowledge and\nprofessional business, further divided into six core performance metrics:\nfactuality, logicality, stability, security, fairness, and expressiveness, and\nis subdivided into 24 sub-metrics, offering profound insights into the\ncapabilities and limitations of LLM applications in the power sector. To ensure\ntransparency, we have made the complete test set public, evaluating the\nperformance of eight LLMs across various scenarios and metrics. ElecBench\naspires to serve as the standard benchmark for LLM applications in the power\nsector, supporting continuous updates of scenarios, metrics, and models to\ndrive technological progress and application.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05365v2",
    "published_date": "2024-07-07 13:38:05 UTC",
    "updated_date": "2024-08-11 11:11:32 UTC"
  },
  {
    "arxiv_id": "2407.06235v1",
    "title": "Auditing of AI: Legal, Ethical and Technical Approaches",
    "authors": [
      "Jakob Mokander"
    ],
    "abstract": "AI auditing is a rapidly growing field of research and practice. This review\narticle, which doubles as an editorial to Digital Societys topical collection\non Auditing of AI, provides an overview of previous work in the field. Three\nkey points emerge from the review. First, contemporary attempts to audit AI\nsystems have much to learn from how audits have historically been structured\nand conducted in areas like financial accounting, safety engineering and the\nsocial sciences. Second, both policymakers and technology providers have an\ninterest in promoting auditing as an AI governance mechanism. Academic\nresearchers can thus fill an important role by studying the feasibility and\neffectiveness of different AI auditing procedures. Third, AI auditing is an\ninherently multidisciplinary undertaking, to which substantial contributions\nhave been made by computer scientists and engineers as well as social\nscientists, philosophers, legal scholars and industry practitioners. Reflecting\nthis diversity of perspectives, different approaches to AI auditing have\ndifferent affordances and constraints. Specifically, a distinction can be made\nbetween technology-oriented audits, which focus on the properties and\ncapabilities of AI systems, and process oriented audits, which focus on\ntechnology providers governance structures and quality management systems. The\nnext step in the evolution of auditing as an AI governance mechanism, this\narticle concludes, should be the interlinking of these available (and\ncomplementary) approaches into structured and holistic procedures to audit not\nonly how AI systems are designed and used but also how they impact users,\nsocieties and the natural environment in applied settings over time.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06235v1",
    "published_date": "2024-07-07 12:49:58 UTC",
    "updated_date": "2024-07-07 12:49:58 UTC"
  },
  {
    "arxiv_id": "2407.07918v2",
    "title": "Detecting new obfuscated malware variants: A lightweight and interpretable machine learning approach",
    "authors": [
      "Oladipo A. Madamidola",
      "Felix Ngobigha",
      "Adnane Ez-zizi"
    ],
    "abstract": "Machine learning has been successfully applied in developing malware\ndetection systems, with a primary focus on accuracy, and increasing attention\nto reducing computational overhead and improving model interpretability.\nHowever, an important question remains underexplored: How well can machine\nlearning-based models detect entirely new forms of malware not present in the\ntraining data? In this study, we present a machine learning-based system for\ndetecting obfuscated malware that is not only highly accurate, lightweight and\ninterpretable, but also capable of successfully adapting to new types of\nmalware attacks. Our system is capable of detecting 15 malware subtypes despite\nbeing exclusively trained on one malware subtype, namely the Transponder from\nthe Spyware family. This system was built after training 15 distinct random\nforest-based models, each on a different malware subtype from the\nCIC-MalMem-2022 dataset. These models were evaluated against the entire range\nof malware subtypes, including all unseen malware subtypes. To maintain the\nsystem's streamlined nature, training was confined to the top five most\nimportant features, which also enhanced interpretability. The\nTransponder-focused model exhibited high accuracy, exceeding 99.8%, with an\naverage processing speed of 5.7 microseconds per file. We also illustrate how\nthe Shapley additive explanations technique can facilitate the interpretation\nof the model predictions. Our research contributes to advancing malware\ndetection methodologies, pioneering the feasibility of detecting obfuscated\nmalware by exclusively training a model on a single or a few carefully selected\nmalware subtypes and applying it to detect unseen subtypes.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "30 pages (excluding Appendix), 5 figures and 5 tables. Now published\n  in Intelligent Systems with Applications\n  (https://doi.org/10.1016/j.iswa.2024.200472)",
    "pdf_url": "http://arxiv.org/pdf/2407.07918v2",
    "published_date": "2024-07-07 12:41:40 UTC",
    "updated_date": "2025-03-06 12:41:21 UTC"
  },
  {
    "arxiv_id": "2407.06234v1",
    "title": "The US Algorithmic Accountability Act of 2022 vs. The EU Artificial Intelligence Act: What can they learn from each other?",
    "authors": [
      "Jakob Mokander",
      "Prathm Juneja",
      "David Watson",
      "Luciano Floridi"
    ],
    "abstract": "On the whole, the U.S. Algorithmic Accountability Act of 2022 (US AAA) is a\npragmatic approach to balancing the benefits and risks of automated decision\nsystems. Yet there is still room for improvement. This commentary highlights\nhow the US AAA can both inform and learn from the European Artificial\nIntelligence Act (EU AIA).",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Minds & Machines (2022)",
    "pdf_url": "http://arxiv.org/pdf/2407.06234v1",
    "published_date": "2024-07-07 12:31:13 UTC",
    "updated_date": "2024-07-07 12:31:13 UTC"
  },
  {
    "arxiv_id": "2407.06233v1",
    "title": "AI and Social Theory",
    "authors": [
      "Jakob Mokander",
      "Ralph Schroeder"
    ],
    "abstract": "In this paper, we sketch a programme for AI driven social theory. We begin by\ndefining what we mean by artificial intelligence (AI) in this context. We then\nlay out our model for how AI based models can draw on the growing availability\nof digital data to help test the validity of different social theories based on\ntheir predictive power. In doing so, we use the work of Randall Collins and his\nstate breakdown model to exemplify that, already today, AI based models can\nhelp synthesize knowledge from a variety of sources, reason about the world,\nand apply what is known across a wide range of problems in a systematic way.\nHowever, we also find that AI driven social theory remains subject to a range\nof practical, technical, and epistemological limitations. Most critically,\nexisting AI systems lack three essential capabilities needed to advance social\ntheory in ways that are cumulative, holistic, open-ended, and purposeful. These\nare (1) semanticization, i.e., the ability to develop and operationalize verbal\nconcepts to represent machine-manipulable knowledge, (2) transferability, i.e.,\nthe ability to transfer what has been learned in one context to another, and\n(3) generativity, i.e., the ability to independently create and improve on\nconcepts and models. We argue that if the gaps identified here are addressed by\nfurther research, there is no reason why, in the future, the most advanced\nprogramme in social theory should not be led by AI-driven cumulative advances.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06233v1",
    "published_date": "2024-07-07 12:26:16 UTC",
    "updated_date": "2024-07-07 12:26:16 UTC"
  },
  {
    "arxiv_id": "2407.05341v1",
    "title": "The Switch, the Ladder, and the Matrix: Models for Classifying AI Systems",
    "authors": [
      "Jakob Mokander",
      "Margi Sheth",
      "David Watson",
      "Luciano Floridi"
    ],
    "abstract": "Organisations that design and deploy artificial intelligence (AI) systems\nincreasingly commit themselves to high-level, ethical principles. However,\nthere still exists a gap between principles and practices in AI ethics. One\nmajor obstacle organisations face when attempting to operationalise AI Ethics\nis the lack of a well-defined material scope. Put differently, the question to\nwhich systems and processes AI ethics principles ought to apply remains\nunanswered. Of course, there exists no universally accepted definition of AI,\nand different systems pose different ethical challenges. Nevertheless,\npragmatic problem-solving demands that things should be sorted so that their\ngrouping will promote successful actions for some specific end. In this\narticle, we review and compare previous attempts to classify AI systems for the\npurpose of implementing AI governance in practice. We find that attempts to\nclassify AI systems found in previous literature use one of three mental model.\nThe Switch, i.e., a binary approach according to which systems either are or\nare not considered AI systems depending on their characteristics. The Ladder,\ni.e., a risk-based approach that classifies systems according to the ethical\nrisks they pose. And the Matrix, i.e., a multi-dimensional classification of\nsystems that take various aspects into account, such as context, data input,\nand decision-model. Each of these models for classifying AI systems comes with\nits own set of strengths and weaknesses. By conceptualising different ways of\nclassifying AI systems into simple mental models, we hope to provide\norganisations that design, deploy, or regulate AI systems with the conceptual\ntools needed to operationalise AI governance in practice.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05341v1",
    "published_date": "2024-07-07 12:16:01 UTC",
    "updated_date": "2024-07-07 12:16:01 UTC"
  },
  {
    "arxiv_id": "2407.05339v1",
    "title": "Challenges and Best Practices in Corporate AI Governance:Lessons from the Biopharmaceutical Industry",
    "authors": [
      "Jakob Mökander",
      "Margi Sheth",
      "Mimmi Gersbro-Sundler",
      "Peder Blomgren",
      "Luciano Floridi"
    ],
    "abstract": "While the use of artificial intelligence (AI) systems promises to bring\nsignificant economic and social benefits, it is also coupled with ethical,\nlegal, and technical challenges. Business leaders thus face the question of how\nto best reap the benefits of automation whilst managing the associated risks.\nAs a first step, many companies have committed themselves to various sets of\nethics principles aimed at guiding the design and use of AI systems. So far so\ngood. But how can well-intentioned ethical principles be translated into\neffective practice? And what challenges await companies that attempt to\noperationalize AI governance? In this article, we address these questions by\ndrawing on our first-hand experience of shaping and driving the roll-out of AI\ngovernance within AstraZeneca, a biopharmaceutical company. The examples we\ndiscuss highlight challenges that any organization attempting to operationalize\nAI governance will have to face. These include questions concerning how to\ndefine the material scope of AI governance, how to harmonize standards across\ndecentralized organizations, and how to measure the impact of specific AI\ngovernance initiatives. By showcasing how AstraZeneca managed these operational\nquestions, we hope to provide project managers, CIOs, AI practitioners, and\ndata privacy officers responsible for designing and implementing AI governance\nframeworks within other organizations with generalizable best practices. In\nessence, companies seeking to operationalize AI governance are encouraged to\nbuild on existing policies and governance structures, use pragmatic and\naction-oriented terminology, focus on risk management in development and\nprocurement, and empower employees through continuous education and change\nmanagement.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05339v1",
    "published_date": "2024-07-07 12:01:42 UTC",
    "updated_date": "2024-07-07 12:01:42 UTC"
  },
  {
    "arxiv_id": "2407.05338v1",
    "title": "A Blueprint for Auditing Generative AI",
    "authors": [
      "Jakob Mokander",
      "Justin Curl",
      "Mihir Kshirsagar"
    ],
    "abstract": "The widespread use of generative AI systems is coupled with significant\nethical and social challenges. As a result, policymakers, academic researchers,\nand social advocacy groups have all called for such systems to be audited.\nHowever, existing auditing procedures fail to address the governance challenges\nposed by generative AI systems, which display emergent capabilities and are\nadaptable to a wide range of downstream tasks. In this chapter, we address that\ngap by outlining a novel blueprint for how to audit such systems. Specifically,\nwe propose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate generative AI systems), model audits (of\ngenerative AI systems after pre-training but prior to their release), and\napplication audits (of applications based on top of generative AI systems)\ncomplement and inform each other. We show how audits on these three levels,\nwhen conducted in a structured and coordinated manner, can be a feasible and\neffective mechanism for identifying and managing some of the ethical and social\nrisks posed by generative AI systems. That said, it is important to remain\nrealistic about what auditing can reasonably be expected to achieve. For this\nreason, the chapter also discusses the limitations not only of our\nthree-layered approach but also of the prospect of auditing generative AI\nsystems at all. Ultimately, this chapter seeks to expand the methodological\ntoolkit available to technology providers and policymakers who wish to analyse\nand evaluate generative AI systems from technical, ethical, and legal\nperspectives.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05338v1",
    "published_date": "2024-07-07 11:56:54 UTC",
    "updated_date": "2024-07-07 11:56:54 UTC"
  },
  {
    "arxiv_id": "2407.05336v1",
    "title": "Artificial intelligence, rationalization, and the limits of control in the public sector: the case of tax policy optimization",
    "authors": [
      "Jakob Mokander",
      "Ralph Schroeder"
    ],
    "abstract": "The use of artificial intelligence (AI) in the public sector is best\nunderstood as a continuation and intensification of long standing\nrationalization and bureaucratization processes. Drawing on Weber, we take the\ncore of these processes to be the replacement of traditions with instrumental\nrationality, i.e., the most calculable and efficient way of achieving any given\npolicy objective. In this article, we demonstrate how much of the criticisms,\nboth among the public and in scholarship, directed towards AI systems spring\nfrom well known tensions at the heart of Weberian rationalization. To\nillustrate this point, we introduce a thought experiment whereby AI systems are\nused to optimize tax policy to advance a specific normative end, reducing\neconomic inequality. Our analysis shows that building a machine-like tax system\nthat promotes social and economic equality is possible. However, it also\nhighlights that AI driven policy optimization (i) comes at the exclusion of\nother competing political values, (ii) overrides citizens sense of their\nnoninstrumental obligations to each other, and (iii) undermines the notion of\nhumans as self-determining beings. Contemporary scholarship and advocacy\ndirected towards ensuring that AI systems are legal, ethical, and safe build on\nand reinforce central assumptions that underpin the process of rationalization,\nincluding the modern idea that science can sweep away oppressive systems and\nreplace them with a rule of reason that would rescue humans from moral\ninjustices. That is overly optimistic. Science can only provide the means, they\ncannot dictate the ends. Nonetheless, the use of AI in the public sector can\nalso benefit the institutions and processes of liberal democracies. Most\nimportantly, AI driven policy optimization demands that normative ends are made\nexplicit and formalized, thereby subjecting them to public scrutiny and debate.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05336v1",
    "published_date": "2024-07-07 11:54:14 UTC",
    "updated_date": "2024-07-07 11:54:14 UTC"
  },
  {
    "arxiv_id": "2407.05333v2",
    "title": "Generating multi-scale NMC particles with radial grain architectures using spatial stochastics and GANs",
    "authors": [
      "Lukas Fuchs",
      "Orkun Furat",
      "Donal P. Finegan",
      "Jeffery Allen",
      "Francois L. E. Usseglio-Viretta",
      "Bertan Ozdogru",
      "Peter J. Weddle",
      "Kandler Smith",
      "Volker Schmidt"
    ],
    "abstract": "Understanding structure-property relationships of Li-ion battery cathodes is\ncrucial for optimizing rate-performance and cycle-life resilience. However,\ncorrelating the morphology of cathode particles, such as in NMC811, and their\ninner grain architecture with electrode performance is challenging,\nparticularly, due to the significant length-scale difference between grain and\nparticle sizes. Experimentally, it is currently not feasible to image such a\nhigh number of particles with full granular detail to achieve representivity. A\nsecond challenge is that sufficiently high-resolution 3D imaging techniques\nremain expensive and are sparsely available at research institutions. To\naddress these challenges, a stereological generative adversarial network\n(GAN)-based model fitting approach is presented that can generate\nrepresentative 3D information from 2D data, enabling characterization of\nmaterials in 3D using cost-effective 2D data. Once calibrated, this multi-scale\nmodel is able to rapidly generate virtual cathode particles that are\nstatistically similar to experimental data, and thus is suitable for virtual\ncharacterization and materials testing through numerical simulations. A large\ndataset of simulated particles with inner grain architecture has been made\npublicly available.",
    "categories": [
      "physics.app-ph",
      "cs.AI"
    ],
    "primary_category": "physics.app-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05333v2",
    "published_date": "2024-07-07 11:23:17 UTC",
    "updated_date": "2024-07-19 08:44:39 UTC"
  },
  {
    "arxiv_id": "2407.05330v1",
    "title": "Fast Proxy Experiment Design for Causal Effect Identification",
    "authors": [
      "Sepehr Elahi",
      "Sina Akbari",
      "Jalal Etesami",
      "Negar Kiyavash",
      "Patrick Thiran"
    ],
    "abstract": "Identifying causal effects is a key problem of interest across many\ndisciplines. The two long-standing approaches to estimate causal effects are\nobservational and experimental (randomized) studies. Observational studies can\nsuffer from unmeasured confounding, which may render the causal effects\nunidentifiable. On the other hand, direct experiments on the target variable\nmay be too costly or even infeasible to conduct. A middle ground between these\ntwo approaches is to estimate the causal effect of interest through proxy\nexperiments, which are conducted on variables with a lower cost to intervene on\ncompared to the main target. Akbari et al. [2022] studied this setting and\ndemonstrated that the problem of designing the optimal (minimum-cost)\nexperiment for causal effect identification is NP-complete and provided a naive\nalgorithm that may require solving exponentially many NP-hard problems as a\nsub-routine in the worst case. In this work, we provide a few reformulations of\nthe problem that allow for designing significantly more efficient algorithms to\nsolve it as witnessed by our extensive simulations. Additionally, we study the\nclosely-related problem of designing experiments that enable us to identify a\ngiven effect through valid adjustments sets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05330v1",
    "published_date": "2024-07-07 11:09:38 UTC",
    "updated_date": "2024-07-07 11:09:38 UTC"
  },
  {
    "arxiv_id": "2407.05320v1",
    "title": "KAE: A Property-based Method for Knowledge Graph Alignment and Extension",
    "authors": [
      "Daqian Shi",
      "Xiaoyue Li",
      "Fausto Giunchiglia"
    ],
    "abstract": "A common solution to the semantic heterogeneity problem is to perform\nknowledge graph (KG) extension exploiting the information encoded in one or\nmore candidate KGs, where the alignment between the reference KG and candidate\nKGs is considered the critical procedure. However, existing KG alignment\nmethods mainly rely on entity type (etype) label matching as a prerequisite,\nwhich is poorly performing in practice or not applicable in some cases. In this\npaper, we design a machine learning-based framework for KG extension, including\nan alternative novel property-based alignment approach that allows aligning\netypes on the basis of the properties used to define them. The main intuition\nis that it is properties that intentionally define the etype, and this\ndefinition is independent of the specific label used to name an etype, and of\nthe specific hierarchical schema of KGs. Compared with the state-of-the-art,\nthe experimental results show the validity of the KG alignment approach and the\nsuperiority of the proposed KG extension framework, both quantitatively and\nqualitatively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2405.02463",
    "pdf_url": "http://arxiv.org/pdf/2407.05320v1",
    "published_date": "2024-07-07 10:17:03 UTC",
    "updated_date": "2024-07-07 10:17:03 UTC"
  },
  {
    "arxiv_id": "2407.14525v1",
    "title": "Morse Code-Enabled Speech Recognition for Individuals with Visual and Hearing Impairments",
    "authors": [
      "Ritabrata Roy Choudhury"
    ],
    "abstract": "The proposed model aims to develop a speech recognition technology for\nhearing, speech, or cognitively disabled people. All the available technology\nin the field of speech recognition doesn't come with an interface for\ncommunication for people with hearing, speech, or cognitive disabilities. The\nproposed model proposes the speech from the user, is transmitted to the speech\nrecognition layer where it is converted into text and then that text is then\ntransmitted to the morse code conversion layer where the morse code of the\ncorresponding speech is given as the output. The accuracy of the model is\ncompletely dependent on speech recognition, as the morse code conversion is a\nprocess. The model is tested with recorded audio files with different\nparameters. The proposed model's WER and accuracy are both determined to be\n10.18% and 89.82%, respectively.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "10 pages, 11 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.14525v1",
    "published_date": "2024-07-07 09:54:29 UTC",
    "updated_date": "2024-07-07 09:54:29 UTC"
  },
  {
    "arxiv_id": "2407.06230v1",
    "title": "Predicting Word Similarity in Context with Referential Translation Machines",
    "authors": [
      "Ergun Biçici"
    ],
    "abstract": "We identify the similarity between two words in English by casting the task\nas machine translation performance prediction (MTPP) between the words given\nthe context and the distance between their similarities. We use referential\ntranslation machines (RTMs), which allows a common representation for training\nand test sets and stacked machine learning models. RTMs can achieve the top\nresults in Graded Word Similarity in Context (GWSC) task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 3 figures, 8 tables. arXiv admin note: substantial text\n  overlap with arXiv:2407.05154",
    "pdf_url": "http://arxiv.org/pdf/2407.06230v1",
    "published_date": "2024-07-07 09:36:41 UTC",
    "updated_date": "2024-07-07 09:36:41 UTC"
  },
  {
    "arxiv_id": "2407.15320v2",
    "title": "Edge Graph Intelligence: Reciprocally Empowering Edge Networks with Graph Intelligence",
    "authors": [
      "Liekang Zeng",
      "Shengyuan Ye",
      "Xu Chen",
      "Xiaoxi Zhang",
      "Ju Ren",
      "Jian Tang",
      "Yang Yang",
      "Xuemin",
      "Shen"
    ],
    "abstract": "Recent years have witnessed a thriving growth of computing facilities\nconnected at the network edge, cultivating edge networks as a fundamental\ninfrastructure for supporting miscellaneous intelligent services.Meanwhile,\nArtificial Intelligence (AI) frontiers have extrapolated to the graph domain\nand promoted Graph Intelligence (GI). Given the inherent relation between\ngraphs and networks, the interdiscipline of graph learning and edge networks,\ni.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in\noptimizing edge networks, while edge networks facilitate GI model deployment.\nDriven by this delicate closed-loop, EGI is recognized as a promising solution\nto fully unleash the potential of edge computing power and is garnering growing\nattention. Nevertheless, research on EGI remains nascent, and there is a\nsoaring demand within both the communications and AI communities for a\ndedicated venue to share recent advancements. To this end, this paper promotes\nthe concept of EGI, explores its scope and core principles, and conducts a\ncomprehensive survey concerning recent research efforts on this emerging field.\nSpecifically, this paper introduces and discusses: 1) fundamentals of edge\ncomputing and graph learning,2) emerging techniques centering on the closed\nloop between graph intelligence and edge networks, and 3) open challenges and\nresearch opportunities of future EGI. By bridging the gap across communication,\nnetworking, and graph learning areas, we believe that this survey can garner\nincreased attention, foster meaningful discussions, and inspire further\nresearch ideas in EGI.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by IEEE Communications Surveys & Tutorials",
    "pdf_url": "http://arxiv.org/pdf/2407.15320v2",
    "published_date": "2024-07-07 09:25:52 UTC",
    "updated_date": "2025-01-07 06:39:29 UTC"
  },
  {
    "arxiv_id": "2407.05305v2",
    "title": "MINDECHO: Role-Playing Language Agents for Key Opinion Leaders",
    "authors": [
      "Rui Xu",
      "Dakuan Lu",
      "Xiaoyu Tan",
      "Xintao Wang",
      "Siyu Yuan",
      "Jiangjie Chen",
      "Wei Chu",
      "Yinghui Xu"
    ],
    "abstract": "Large language models~(LLMs) have demonstrated impressive performance in\nvarious applications, among which role-playing language agents (RPLAs) have\nengaged a broad user base. Now, there is a growing demand for RPLAs that\nrepresent Key Opinion Leaders (KOLs), \\ie, Internet celebrities who shape the\ntrends and opinions in their domains. However, research in this line remains\nunderexplored. In this paper, we hence introduce MINDECHO, a comprehensive\nframework for the development and evaluation of KOL RPLAs. MINDECHO collects\nKOL data from Internet video transcripts in various professional fields, and\nsynthesizes their conversations leveraging GPT-4. Then, the conversations and\nthe transcripts are used for individualized model training and inference-time\nretrieval, respectively. Our evaluation covers both general dimensions (\\ie,\nknowledge and tones) and fan-centric dimensions for KOLs. Extensive experiments\nvalidate the effectiveness of MINDECHO in developing and evaluating KOL RPLAs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05305v2",
    "published_date": "2024-07-07 09:08:33 UTC",
    "updated_date": "2024-10-09 07:19:34 UTC"
  },
  {
    "arxiv_id": "2407.05291v2",
    "title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks",
    "authors": [
      "Léo Boisvert",
      "Megh Thakkar",
      "Maxime Gasse",
      "Massimo Caccia",
      "Thibault Le Sellier De Chezelles",
      "Quentin Cappart",
      "Nicolas Chapados",
      "Alexandre Lacoste",
      "Alexandre Drouin"
    ],
    "abstract": "The ability of large language models (LLMs) to mimic human-like intelligence\nhas led to a surge in LLM-based autonomous agents. Though recent LLMs seem\ncapable of planning and reasoning given user instructions, their effectiveness\nin applying these capabilities for autonomous task solving remains\nunderexplored. This is especially true in enterprise settings, where automated\nagents hold the promise of a high impact. To fill this gap, we propose\nWorkArena++, a novel benchmark consisting of 682 tasks corresponding to\nrealistic workflows routinely performed by knowledge workers. WorkArena++ is\ndesigned to evaluate the planning, problem-solving, logical/arithmetic\nreasoning, retrieval, and contextual understanding abilities of web agents. Our\nempirical studies across state-of-the-art LLMs and vision-language models\n(VLMs), as well as human workers, reveal several challenges for such models to\nserve as useful assistants in the workplace. In addition to the benchmark, we\nprovide a mechanism to effortlessly generate thousands of ground-truth\nobservation/action traces, which can be used for fine-tuning existing models.\nOverall, we expect this work to serve as a useful resource to help the\ncommunity progress toward capable autonomous agents. The benchmark can be found\nat https://github.com/ServiceNow/WorkArena.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05291v2",
    "published_date": "2024-07-07 07:15:49 UTC",
    "updated_date": "2025-02-05 21:50:07 UTC"
  },
  {
    "arxiv_id": "2407.05285v4",
    "title": "Mjolnir: Breaking the Shield of Perturbation-Protected Gradients via Adaptive Diffusion",
    "authors": [
      "Xuan Liu",
      "Siqi Cai",
      "Qihua Zhou",
      "Song Guo",
      "Ruibin Li",
      "Kaiwei Lin"
    ],
    "abstract": "Perturbation-based mechanisms, such as differential privacy, mitigate\ngradient leakage attacks by introducing noise into the gradients, thereby\npreventing attackers from reconstructing clients' private data from the leaked\ngradients. However, can gradient perturbation protection mechanisms truly\ndefend against all gradient leakage attacks? In this paper, we present the\nfirst attempt to break the shield of gradient perturbation protection in\nFederated Learning for the extraction of private information. We focus on\ncommon noise distributions, specifically Gaussian and Laplace, and apply our\napproach to DNN and CNN models. We introduce Mjolnir, a perturbation-resilient\ngradient leakage attack that is capable of removing perturbations from\ngradients without requiring additional access to the original model structure\nor external data. Specifically, we leverage the inherent diffusion properties\nof gradient perturbation protection to develop a novel diffusion-based gradient\ndenoising model for Mjolnir. By constructing a surrogate client model that\ncaptures the structure of perturbed gradients, we obtain crucial gradient data\nfor training the diffusion model. We further utilize the insight that\nmonitoring disturbance levels during the reverse diffusion process can enhance\ngradient denoising capabilities, allowing Mjolnir to generate gradients that\nclosely approximate the original, unperturbed versions through adaptive\nsampling steps. Extensive experiments demonstrate that Mjolnir effectively\nrecovers the protected gradients and exposes the Federated Learning process to\nthe threat of gradient leakage, achieving superior performance in gradient\ndenoising and private data recovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.05285v4",
    "published_date": "2024-07-07 07:06:49 UTC",
    "updated_date": "2025-01-06 14:37:01 UTC"
  },
  {
    "arxiv_id": "2408.00001v1",
    "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
    "authors": [
      "Wenhao Wang",
      "Yifan Sun",
      "Zongxin Yang",
      "Zhengdong Hu",
      "Zhentao Tan",
      "Yi Yang"
    ],
    "abstract": "Visual diffusion models have revolutionized the field of creative AI,\nproducing high-quality and diverse content. However, they inevitably memorize\ntraining images or videos, subsequently replicating their concepts, content, or\nstyles during inference. This phenomenon raises significant concerns about\nprivacy, security, and copyright within generated outputs. In this survey, we\nprovide the first comprehensive review of replication in visual diffusion\nmodels, marking a novel contribution to the field by systematically\ncategorizing the existing studies into unveiling, understanding, and mitigating\nthis phenomenon. Specifically, unveiling mainly refers to the methods used to\ndetect replication instances. Understanding involves analyzing the underlying\nmechanisms and factors that contribute to this phenomenon. Mitigation focuses\non developing strategies to reduce or eliminate replication. Beyond these\naspects, we also review papers focusing on its real-world influence. For\ninstance, in the context of healthcare, replication is critically worrying due\nto privacy concerns related to patient data. Finally, the paper concludes with\na discussion of the ongoing challenges, such as the difficulty in detecting and\nbenchmarking replication, and outlines future directions including the\ndevelopment of more robust mitigation techniques. By synthesizing insights from\ndiverse studies, this paper aims to equip researchers and practitioners with a\ndeeper understanding at the intersection between AI technology and social good.\nWe release this project at\nhttps://github.com/WangWenhao0716/Awesome-Diffusion-Replication.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "The first survey focuses on replication in visual diffusion models.\n  This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2408.00001v1",
    "published_date": "2024-07-07 06:39:16 UTC",
    "updated_date": "2024-07-07 06:39:16 UTC"
  },
  {
    "arxiv_id": "2407.05268v1",
    "title": "Federated Knowledge Transfer Fine-tuning Large Server Model with Resource-Constrained IoT Clients",
    "authors": [
      "Shaoyuan Chen",
      "Linlin You",
      "Rui Liu",
      "Shuo Yu",
      "Ahmed M. Abdelmoniem"
    ],
    "abstract": "The training of large models, involving fine-tuning, faces the scarcity of\nhigh-quality data. Compared to the solutions based on centralized data centers,\nupdating large models in the Internet of Things (IoT) faces challenges in\ncoordinating knowledge from distributed clients by using their private and\nheterogeneous data. To tackle such a challenge, we propose KOALA (Federated\nKnowledge Transfer Fine-tuning Large Server Model with Resource-Constrained IoT\nClients) to impel the training of large models in IoT. Since the resources\nobtained by IoT clients are limited and restricted, it is infeasible to locally\nexecute large models and also update them in a privacy-preserving manner.\nTherefore, we leverage federated learning and knowledge distillation to update\nlarge models through collaboration with their small models, which can run\nlocally at IoT clients to process their private data separately and enable\nlarge-small model knowledge transfer through iterative learning between the\nserver and clients. Moreover, to support clients with similar or different\ncomputing capacities, KOALA is designed with two kinds of large-small model\njoint learning modes, namely to be homogeneous or heterogeneous. Experimental\nresults demonstrate that compared to the conventional approach, our method can\nnot only achieve similar training performance but also significantly reduce the\nneed for local storage and computing power resources.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05268v1",
    "published_date": "2024-07-07 05:46:01 UTC",
    "updated_date": "2024-07-07 05:46:01 UTC"
  },
  {
    "arxiv_id": "2407.05266v2",
    "title": "CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs",
    "authors": [
      "Akshat Ramachandran",
      "Souvik Kundu",
      "Tushar Krishna"
    ],
    "abstract": "We present CLAMP-ViT, a data-free post-training quantization method for\nvision transformers (ViTs). We identify the limitations of recent techniques,\nnotably their inability to leverage meaningful inter-patch relationships,\nleading to the generation of simplistic and semantically vague data, impacting\nquantization accuracy. CLAMP-ViT employs a two-stage approach, cyclically\nadapting between data generation and model quantization. Specifically, we\nincorporate a patch-level contrastive learning scheme to generate richer,\nsemantically meaningful data. Furthermore, we leverage contrastive learning in\nlayer-wise evolutionary search for fixed- and mixed-precision quantization to\nidentify optimal quantization parameters while mitigating the effects of a\nnon-smooth loss landscape. Extensive evaluations across various vision tasks\ndemonstrate the superiority of CLAMP-ViT, with performance improvements of up\nto 3% in top-1 accuracy for classification, 0.6 mAP for object detection, and\n1.5 mIoU for segmentation at similar or better compression ratio over existing\nalternatives. Code is available at\nhttps://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05266v2",
    "published_date": "2024-07-07 05:39:25 UTC",
    "updated_date": "2024-09-09 00:08:36 UTC"
  },
  {
    "arxiv_id": "2407.05262v2",
    "title": "FastSpiker: Enabling Fast Training for Spiking Neural Networks on Event-based Data through Learning Rate Enhancements for Autonomous Embedded Systems",
    "authors": [
      "Iqra Bano",
      "Rachmad Vidya Wicaksana Putra",
      "Alberto Marchisio",
      "Muhammad Shafique"
    ],
    "abstract": "Autonomous embedded systems (e.g., robots) typically necessitate intelligent\ncomputation with low power/energy processing for completing their tasks. Such\nrequirements can be fulfilled by embodied neuromorphic intelligence with\nspiking neural networks (SNNs) because of their high learning quality (e.g.,\naccuracy) and sparse computation. Here, the employment of event-based data is\npreferred to ensure seamless connectivity between input and processing parts.\nHowever, state-of-the-art SNNs still face a long training time to achieve high\naccuracy, thereby incurring high energy consumption and producing a high rate\nof carbon emission. Toward this, we propose FastSpiker, a novel methodology\nthat enables fast SNN training on event-based data through learning rate\nenhancements targeting autonomous embedded systems. In FastSpiker, we first\ninvestigate the impact of different learning rate policies and their values,\nthen select the ones that quickly offer high accuracy. Afterward, we explore\ndifferent settings for the selected learning rate policies to find the\nappropriate policies through a statistical-based decision. Experimental results\nshow that our FastSpiker offers up to 10.5x faster training time and up to\n88.39% lower carbon emission to achieve higher or comparable accuracy to the\nstate-of-the-art on the event-based automotive dataset (i.e., NCARS). In this\nmanner, our FastSpiker methodology paves the way for green and sustainable\ncomputing in realizing embodied neuromorphic intelligence for autonomous\nembedded systems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.NE",
    "comment": "To appear at the 18th International Conference on Control,\n  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE",
    "pdf_url": "http://arxiv.org/pdf/2407.05262v2",
    "published_date": "2024-07-07 05:17:17 UTC",
    "updated_date": "2024-09-12 18:28:17 UTC"
  },
  {
    "arxiv_id": "2407.05259v1",
    "title": "Multi-scale Conditional Generative Modeling for Microscopic Image Restoration",
    "authors": [
      "Luzhe Huang",
      "Xiongye Xiao",
      "Shixuan Li",
      "Jiawen Sun",
      "Yi Huang",
      "Aydogan Ozcan",
      "Paul Bogdan"
    ],
    "abstract": "The advance of diffusion-based generative models in recent years has\nrevolutionized state-of-the-art (SOTA) techniques in a wide variety of image\nanalysis and synthesis tasks, whereas their adaptation on image restoration,\nparticularly within computational microscopy remains theoretically and\nempirically underexplored. In this research, we introduce a multi-scale\ngenerative model that enhances conditional image restoration through a novel\nexploitation of the Brownian Bridge process within wavelet domain. By\ninitiating the Brownian Bridge diffusion process specifically at the\nlowest-frequency subband and applying generative adversarial networks at\nsubsequent multi-scale high-frequency subbands in the wavelet domain, our\nmethod provides significant acceleration during training and sampling while\nsustaining a high image generation quality and diversity on par with SOTA\ndiffusion models. Experimental results on various computational microscopy and\nimaging tasks confirm our method's robust performance and its considerable\nreduction in its sampling steps and time. This pioneering technique offers an\nefficient image restoration framework that harmonizes efficiency with quality,\nsignifying a major stride in incorporating cutting-edge generative models into\ncomputational microscopy workflows.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05259v1",
    "published_date": "2024-07-07 05:11:00 UTC",
    "updated_date": "2024-07-07 05:11:00 UTC"
  },
  {
    "arxiv_id": "2407.05257v1",
    "title": "OvSW: Overcoming Silent Weights for Accurate Binary Neural Networks",
    "authors": [
      "Jingyang Xiang",
      "Zuohui Chen",
      "Siqi Li",
      "Qing Wu",
      "Yong Liu"
    ],
    "abstract": "Binary Neural Networks~(BNNs) have been proven to be highly effective for\ndeploying deep neural networks on mobile and embedded platforms. Most existing\nworks focus on minimizing quantization errors, improving representation\nability, or designing gradient approximations to alleviate gradient mismatch in\nBNNs, while leaving the weight sign flipping, a critical factor for achieving\npowerful BNNs, untouched. In this paper, we investigate the efficiency of\nweight sign updates in BNNs. We observe that, for vanilla BNNs, over 50\\% of\nthe weights remain their signs unchanged during training, and these weights are\nnot only distributed at the tails of the weight distribution but also\nuniversally present in the vicinity of zero. We refer to these weights as\n``silent weights'', which slow down convergence and lead to a significant\naccuracy degradation. Theoretically, we reveal this is due to the independence\nof the BNNs gradient from the latent weight distribution. To address the issue,\nwe propose Overcome Silent Weights~(OvSW). OvSW first employs Adaptive Gradient\nScaling~(AGS) to establish a relationship between the gradient and the latent\nweight distribution, thereby improving the overall efficiency of weight sign\nupdates. Additionally, we design Silence Awareness Decaying~(SAD) to\nautomatically identify ``silent weights'' by tracking weight flipping state,\nand apply an additional penalty to ``silent weights'' to facilitate their\nflipping. By efficiently updating weight signs, our method achieves faster\nconvergence and state-of-the-art performance on CIFAR10 and ImageNet1K dataset\nwith various architectures. For example, OvSW obtains 61.6\\% and 65.5\\% top-1\naccuracy on the ImageNet1K using binarized ResNet18 and ResNet34 architecture\nrespectively. Codes are available at\n\\url{https://github.com/JingyangXiang/OvSW}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by the 18th European Conference on Computer Vision (ECCV\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.05257v1",
    "published_date": "2024-07-07 05:01:20 UTC",
    "updated_date": "2024-07-07 05:01:20 UTC"
  },
  {
    "arxiv_id": "2407.05256v2",
    "title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image",
    "authors": [
      "Pengkun Jiao",
      "Na Zhao",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ],
    "abstract": "Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize\nboth seen and previously unseen object categories within any new 3D scene.\nWhile language and vision foundation models have achieved success in handling\nvarious open-vocabulary tasks with abundant training data, OV-3DDet faces a\nsignificant challenge due to the limited availability of training data.\nAlthough some pioneering efforts have integrated vision-language models (VLM)\nknowledge into OV-3DDet learning, the full potential of these foundational\nmodels has yet to be fully exploited. In this paper, we unlock the textual and\nvisual wisdom to tackle the open-vocabulary 3D detection task by leveraging the\nlanguage and vision foundation models. We leverage a vision foundation model to\nprovide image-wise guidance for discovering novel classes in 3D scenes.\nSpecifically, we utilize a object detection vision foundation model to enable\nthe zero-shot discovery of objects in images, which serves as the initial seeds\nand filtering guidance to identify novel 3D objects. Additionally, to align the\n3D space with the powerful vision-language space, we introduce a hierarchical\nalignment approach, where the 3D feature space is aligned with the\nvision-language feature space using a pre-trained VLM at the instance,\ncategory, and scene levels. Through extensive experimentation, we demonstrate\nsignificant improvements in accuracy and generalization, highlighting the\npotential of foundation models in advancing open-vocabulary 3D object detection\nin real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.05256v2",
    "published_date": "2024-07-07 04:50:04 UTC",
    "updated_date": "2024-07-17 16:50:09 UTC"
  },
  {
    "arxiv_id": "2407.05244v1",
    "title": "Some Issues in Predictive Ethics Modeling: An Annotated Contrast Set of \"Moral Stories\"",
    "authors": [
      "Ben Fitzgerald"
    ],
    "abstract": "Models like Delphi have been able to label ethical dilemmas as moral or\nimmoral with astonishing accuracy. This paper challenges accuracy as a holistic\nmetric for ethics modeling by identifying issues with translating moral\ndilemmas into text-based input. It demonstrates these issues with contrast sets\nthat substantially reduce the performance of classifiers trained on the dataset\nMoral Stories. Ultimately, we obtain concrete estimates for how much specific\nforms of data misrepresentation harm classifier accuracy. Specifically,\nlabel-changing tweaks to the descriptive content of a situation (as small as\n3-5 words) can reduce classifier accuracy to as low as 51%, almost half the\ninitial accuracy of 99.8%. Associating situations with a misleading social norm\nlowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a\nsituation already fits a certain label) lowers accuracy to 77%.\n  These results suggest not only that many ethics models have substantially\noverfit, but that several precautions are required to ensure that input\naccurately captures a moral dilemma. This paper recommends re-examining the\nstructure of a social norm, training models to ask for context with defeasible\nreasoning, and filtering input for textual bias. Doing so not only gives us the\nfirst concrete estimates of the average cost to accuracy of misrepresenting\nethics data, but gives researchers practical tips for considering these\nestimates in research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "This project was a runner-up to the Novel Research prize for the\n  BlueDot Impact course AI Safety Fundamentals. View my contrast set as JSONL,\n  the UI used to generate it, and Emelin et. al.\"s initial paper and code at\n  https://github.com/bfitzgerald3132/MoralStoriesContrastSet",
    "pdf_url": "http://arxiv.org/pdf/2407.05244v1",
    "published_date": "2024-07-07 03:22:49 UTC",
    "updated_date": "2024-07-07 03:22:49 UTC"
  },
  {
    "arxiv_id": "2407.05233v1",
    "title": "Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of Gemma-2b-it and Phi2 Models",
    "authors": [
      "Jianlong Chen",
      "Wei Xu",
      "Zhicheng Ding",
      "Jinxin Xu",
      "Hao Yan",
      "Xinyu Zhang"
    ],
    "abstract": "Prompt recovery, a crucial task in natural language processing, entails the\nreconstruction of prompts or instructions that language models use to convert\ninput text into a specific output. Although pivotal, the design and\neffectiveness of prompts represent a challenging and relatively untapped field\nwithin NLP research. This paper delves into an exhaustive investigation of\nprompt recovery methodologies, employing a spectrum of pre-trained language\nmodels and strategies. Our study is a comparative analysis aimed at gauging the\nefficacy of various models on a benchmark dataset, with the goal of pinpointing\nthe most proficient approach for prompt recovery. Through meticulous\nexperimentation and detailed analysis, we elucidate the outstanding performance\nof the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its\ncounterparts, showcasing its exceptional capability in accurately\nreconstructing prompts for text transformation tasks. Our findings offer a\nsignificant contribution to the existing knowledge on prompt recovery, shedding\nlight on the intricacies of prompt design and offering insightful perspectives\nfor future innovations in text rewriting and the broader field of natural\nlanguage processing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.05233v1",
    "published_date": "2024-07-07 02:15:26 UTC",
    "updated_date": "2024-07-07 02:15:26 UTC"
  }
]
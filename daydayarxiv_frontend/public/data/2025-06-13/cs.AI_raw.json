[
  {
    "arxiv_id": "2506.13811v1",
    "title": "Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study",
    "authors": [
      "Sompote Youwai",
      "David Phim",
      "Vianne Gayl Murcia",
      "Rianne Clair Onas"
    ],
    "abstract": "This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13811v1",
    "published_date": "2025-06-13 23:45:24 UTC",
    "updated_date": "2025-06-13 23:45:24 UTC"
  },
  {
    "arxiv_id": "2506.12270v1",
    "title": "Cloud Infrastructure Management in the Age of AI Agents",
    "authors": [
      "Zhenning Yang",
      "Archit Bhatnagar",
      "Yiming Qiu",
      "Tongyuan Miao",
      "Patrick Tser Jern Kon",
      "Yunming Xiao",
      "Yibo Huang",
      "Martin Casado",
      "Ang Chen"
    ],
    "abstract": "Cloud infrastructure is the cornerstone of the modern IT industry. However, managing this infrastructure effectively requires considerable manual effort from the DevOps engineering team. We make a case for developing AI agents powered by large language models (LLMs) to automate cloud infrastructure management tasks. In a preliminary study, we investigate the potential for AI agents to use different cloud/user interfaces such as software development kits (SDK), command line interfaces (CLI), Infrastructure-as-Code (IaC) platforms, and web portals. We report takeaways on their effectiveness on different management tasks, and identify research challenges and potential solutions.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12270v1",
    "published_date": "2025-06-13 22:50:12 UTC",
    "updated_date": "2025-06-13 22:50:12 UTC"
  },
  {
    "arxiv_id": "2506.12266v1",
    "title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs",
    "authors": [
      "Avinash Baidya",
      "Kamalika Das",
      "Xiang Gao"
    ],
    "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025; 18 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.12266v1",
    "published_date": "2025-06-13 22:36:41 UTC",
    "updated_date": "2025-06-13 22:36:41 UTC"
  },
  {
    "arxiv_id": "2506.12263v3",
    "title": "A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis",
    "authors": [
      "Hui Wei",
      "Dong Yoon Lee",
      "Shubham Rohal",
      "Zhizhang Hu",
      "Ryan Rossi",
      "Shiwei Fang",
      "Shijia Pan"
    ],
    "abstract": "Foundation models have gained growing interest in the IoT domain due to their reduced reliance on labeled data and strong generalizability across tasks, which address key limitations of traditional machine learning approaches. However, most existing foundation model based methods are developed for specific IoT tasks, making it difficult to compare approaches across IoT domains and limiting guidance for applying them to new tasks. This survey aims to bridge this gap by providing a comprehensive overview of current methodologies and organizing them around four shared performance objectives by different domains: efficiency, context-awareness, safety, and security & privacy. For each objective, we review representative works, summarize commonly-used techniques and evaluation metrics. This objective-centric organization enables meaningful cross-domain comparisons and offers practical insights for selecting and designing foundation model based solutions for new IoT tasks. We conclude with key directions for future research to guide both practitioners and researchers in advancing the use of foundation models in IoT applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by CCF Transactions on Pervasive Computing and Interaction (CCF TPCI)",
    "pdf_url": "https://arxiv.org/pdf/2506.12263v3",
    "published_date": "2025-06-13 22:28:55 UTC",
    "updated_date": "2025-10-09 01:28:19 UTC"
  },
  {
    "arxiv_id": "2506.13809v1",
    "title": "Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space",
    "authors": [
      "Roman V. Belavkin"
    ],
    "abstract": "Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.",
    "categories": [
      "q-bio.PE",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "q-bio.PE",
    "comment": "42 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13809v1",
    "published_date": "2025-06-13 22:24:41 UTC",
    "updated_date": "2025-06-13 22:24:41 UTC"
  },
  {
    "arxiv_id": "2506.14830v1",
    "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model",
    "authors": [
      "Zhizhao Wen",
      "Ruoxin Zhang",
      "Chao Wang"
    ],
    "abstract": "Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a hybrid BiGRU-MHA model that incorporates a multi-head attention mechanism to enhance the accuracy and stability of storage device health classification. The model innovatively integrates temporal feature extraction and key information focusing capabilities. Specifically, it leverages the bidirectional timing modeling advantages of the BiGRU network to capture both forward and backward dependencies of SSD degradation features. Simultaneously, the multi-head attention mechanism dynamically assigns feature weights, improving the model's sensitivity to critical health indicators. Experimental results show that the proposed model achieves classification accuracies of 92.70% on the training set and 92.44% on the test set, with a minimal performance gap of only 0.26%, demonstrating excellent generalization ability. Further analysis using the receiver operating characteristic (ROC) curve shows an area under the curve (AUC) of 0.94 on the test set, confirming the model's robust binary classification performance. This work not only presents a new technical approach for SSD health prediction but also addresses the generalization bottleneck of traditional models, offering a verifiable method with practical value for preventive maintenance of industrial-grade storage systems. The results show the model can significantly reduce data loss risks by providing early failure warnings and help optimize maintenance costs, supporting intelligent decision-making in building reliable storage systems for cloud computing data centers and edge storage environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "Source code available; Accepted by 2025 6th International Conference on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.14830v1",
    "published_date": "2025-06-13 22:01:57 UTC",
    "updated_date": "2025-06-13 22:01:57 UTC"
  },
  {
    "arxiv_id": "2506.12254v1",
    "title": "Lower Bound on Howard Policy Iteration for Deterministic Markov Decision Processes",
    "authors": [
      "Ali Asadi",
      "Krishnendu Chatterjee",
      "Jakob de Raaij"
    ],
    "abstract": "Deterministic Markov Decision Processes (DMDPs) are a mathematical framework for decision-making where the outcomes and future possible actions are deterministically determined by the current action taken. DMDPs can be viewed as a finite directed weighted graph, where in each step, the controller chooses an outgoing edge. An objective is a measurable function on runs (or infinite trajectories) of the DMDP, and the value for an objective is the maximal cumulative reward (or weight) that the controller can guarantee. We consider the classical mean-payoff (aka limit-average) objective, which is a basic and fundamental objective.\n  Howard's policy iteration algorithm is a popular method for solving DMDPs with mean-payoff objectives. Although Howard's algorithm performs well in practice, as experimental studies suggested, the best known upper bound is exponential and the current known lower bound is as follows: For the input size $I$, the algorithm requires $\\tildeΩ(\\sqrt{I})$ iterations, where $\\tildeΩ$ hides the poly-logarithmic factors, i.e., the current lower bound on iterations is sub-linear with respect to the input size. Our main result is an improved lower bound for this fundamental algorithm where we show that for the input size $I$, the algorithm requires $\\tildeΩ(I)$ iterations.",
    "categories": [
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages excluding references and appendix, 4 figures, Conference on Uncertainty in Artificial Intelligence (UAI) 2025 (forthcoming)",
    "pdf_url": "https://arxiv.org/pdf/2506.12254v1",
    "published_date": "2025-06-13 22:00:36 UTC",
    "updated_date": "2025-06-13 22:00:36 UTC"
  },
  {
    "arxiv_id": "2506.12248v1",
    "title": "ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration",
    "authors": [
      "Jennifer Grannen",
      "Siddharth Karamcheti",
      "Blake Wulfe",
      "Dorsa Sadigh"
    ],
    "abstract": "Collaborative robots must quickly adapt to their partner's intent and preferences to proactively identify helpful actions. This is especially true in situated settings where human partners can continually teach robots new high-level behaviors, visual concepts, and physical skills (e.g., through demonstration), growing the robot's capabilities as the human-robot pair work together to accomplish diverse tasks. In this work, we argue that robots should be able to infer their partner's goals from early interactions and use this information to proactively plan behaviors ahead of explicit instructions from the user. Building from the strong commonsense priors and steerability of large language models, we introduce ProVox (\"Proactive Voice\"), a novel framework that enables robots to efficiently personalize and adapt to individual collaborators. We design a meta-prompting protocol that empowers users to communicate their distinct preferences, intent, and expected robot behaviors ahead of starting a physical interaction. ProVox then uses the personalized prompt to condition a proactive language model task planner that anticipates a user's intent from the current interaction context and robot capabilities to suggest helpful actions; in doing so, we alleviate user burden, minimizing the amount of time partners spend explicitly instructing and supervising the robot. We evaluate ProVox through user studies grounded in household manipulation tasks (e.g., assembling lunch bags) that measure the efficiency of the collaboration, as well as features such as perceived helpfulness, ease of use, and reliability. Our analysis suggests that both meta-prompting and proactivity are critical, resulting in 38.7% faster task completion times and 31.9% less user burden relative to non-active baselines. Supplementary material, code, and videos can be found at https://provox-2025.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by IEEE Robotics and Automation Letters 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12248v1",
    "published_date": "2025-06-13 21:50:10 UTC",
    "updated_date": "2025-06-13 21:50:10 UTC"
  },
  {
    "arxiv_id": "2506.14829v1",
    "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities",
    "authors": [
      "Aditya Majumdar",
      "Wenbo Zhang",
      "Kashvi Prawal",
      "Amulya Yadav"
    ],
    "abstract": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects focus on harnessing AI to address societal issues in areas such as healthcare, social justice, etc. Unfortunately, despite growing interest in AI4SI, achieving tangible, on-the-ground impact remains a significant challenge. For example, identifying and engaging motivated collaborators who are willing to co-design and deploy AI based solutions in real-world settings is often difficult. Even when such partnerships are established, many AI4SI projects \"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to transition to at-scale production-level solutions. Furthermore, the unique challenges faced by AI4SI researchers are not always fully recognized within the broader AI community, where such work is sometimes viewed as primarily applied and not aligning with the traditional criteria for novelty emphasized in core AI venues. This paper attempts to shine a light on the diverse challenges faced in AI4SI research by diagnosing a multitude of factors that prevent AI4SI partnerships from achieving real-world impact on the ground. Drawing on semi-structured interviews with six leading AI4SI researchers - complemented by the authors' own lived experiences in conducting AI4SI research - this paper attempts to understand the day-to-day difficulties faced in developing and deploying socially impactful AI solutions. Through thematic analysis, we identify structural and organizational, communication, collaboration, and operational challenges as key barriers to deployment. While there are no easy fixes, we synthesize best practices and actionable strategies drawn from these interviews and our own work in this space. In doing so, we hope this paper serves as a practical reference guide for AI4SI researchers and partner organizations seeking to engage more effectively in socially impactful AI collaborations.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14829v1",
    "published_date": "2025-06-13 21:48:47 UTC",
    "updated_date": "2025-06-13 21:48:47 UTC"
  },
  {
    "arxiv_id": "2506.12245v1",
    "title": "Reversing the Paradigm: Building AI-First Systems with Human Guidance",
    "authors": [
      "Cosimo Spera",
      "Garima Agrawal"
    ],
    "abstract": "The relationship between humans and artificial intelligence is no longer science fiction -- it's a growing reality reshaping how we live and work. AI has moved beyond research labs into everyday life, powering customer service chats, personalizing travel, aiding doctors in diagnosis, and supporting educators. What makes this moment particularly compelling is AI's increasing collaborative nature. Rather than replacing humans, AI augments our capabilities -- automating routine tasks, enhancing decisions with data, and enabling creativity in fields like design, music, and writing. The future of work is shifting toward AI agents handling tasks autonomously, with humans as supervisors, strategists, and ethical stewards. This flips the traditional model: instead of humans using AI as a tool, intelligent agents will operate independently within constraints, managing everything from scheduling and customer service to complex workflows. Humans will guide and fine-tune these agents to ensure alignment with goals, values, and context.\n  This shift offers major benefits -- greater efficiency, faster decisions, cost savings, and scalability. But it also brings risks: diminished human oversight, algorithmic bias, security flaws, and a widening skills gap. To navigate this transition, organizations must rethink roles, invest in upskilling, embed ethical principles, and promote transparency. This paper examines the technological and organizational changes needed to enable responsible adoption of AI-first systems -- where autonomy is balanced with human intent, oversight, and values.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12245v1",
    "published_date": "2025-06-13 21:48:44 UTC",
    "updated_date": "2025-06-13 21:48:44 UTC"
  },
  {
    "arxiv_id": "2506.12242v1",
    "title": "Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives",
    "authors": [
      "Arno Simons",
      "Michael Zichert",
      "Adrian Wüthrich"
    ],
    "abstract": "This paper explores the use of large language models (LLMs) as research tools in the history, philosophy, and sociology of science (HPSS). LLMs are remarkably effective at processing unstructured text and inferring meaning from context, offering new affordances that challenge long-standing divides between computational and interpretive methods. This raises both opportunities and challenges for HPSS, which emphasizes interpretive methodologies and understands meaning as context-dependent, ambiguous, and historically situated. We argue that HPSS is uniquely positioned not only to benefit from LLMs' capabilities but also to interrogate their epistemic assumptions and infrastructural implications. To this end, we first offer a concise primer on LLM architectures and training paradigms tailored to non-technical readers. We frame LLMs not as neutral tools but as epistemic infrastructures that encode assumptions about meaning, context, and similarity, conditioned by their training data, architecture, and patterns of use. We then examine how computational techniques enhanced by LLMs, such as structuring data, detecting patterns, and modeling dynamic processes, can be applied to support interpretive research in HPSS. Our analysis compares full-context and generative models, outlines strategies for domain and task adaptation (e.g., continued pretraining, fine-tuning, and retrieval-augmented generation), and evaluates their respective strengths and limitations for interpretive inquiry in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1) model selection involves interpretive trade-offs; (2) LLM literacy is foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs should enhance, not replace, interpretive methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.12242v1",
    "published_date": "2025-06-13 21:44:13 UTC",
    "updated_date": "2025-06-13 21:44:13 UTC"
  },
  {
    "arxiv_id": "2506.12241v2",
    "title": "Privacy Reasoning in Ambiguous Contexts",
    "authors": [
      "Ren Yi",
      "Octavian Suciu",
      "Adria Gascon",
      "Sarah Meiklejohn",
      "Eugene Bagdasarian",
      "Marco Gruteser"
    ],
    "abstract": "We study the ability of language models to reason about appropriate information disclosure - a central aspect of the evolving field of agentic privacy. Whereas previous works have focused on evaluating a model's ability to align with human decisions, we examine the role of ambiguity and missing context on model performance when making information-sharing decisions. We identify context ambiguity as a crucial barrier for high performance in privacy assessments. By designing Camber, a framework for context disambiguation, we show that model-generated decision rationales can reveal ambiguities and that systematically disambiguating context based on these rationales leads to significant accuracy improvements (up to 13.3% in precision and up to 22.3% in recall) as well as reductions in prompt sensitivity. Overall, our results indicate that approaches for context disambiguation are a promising way forward to enhance agentic privacy reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12241v2",
    "published_date": "2025-06-13 21:42:22 UTC",
    "updated_date": "2025-11-28 15:51:59 UTC"
  },
  {
    "arxiv_id": "2506.12240v1",
    "title": "Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI",
    "authors": [
      "Eva Paraschou",
      "Ioannis Arapakis",
      "Sofia Yfantidou",
      "Sebastian Macaluso",
      "Athena Vakali"
    ],
    "abstract": "Artificial Intelligence (AI) is rapidly embedded in critical decision-making systems, however their foundational ``black-box'' models require eXplainable AI (XAI) solutions to enhance transparency, which are mostly oriented to experts, making no sense to non-experts. Alarming evidence about AI's unprecedented human values risks brings forward the imperative need for transparent human-centered XAI solutions. In this work, we introduce a domain-, model-, explanation-agnostic, generalizable and reproducible framework that ensures both transparency and human-centered explanations tailored to the needs of both experts and non-experts. The framework leverages Large Language Models (LLMs) and employs in-context learning to convey domain- and explainability-relevant contextual knowledge into LLMs. Through its structured prompt and system setting, our framework encapsulates in one response explanations understandable by non-experts and technical information to experts, all grounded in domain and explainability principles. To demonstrate the effectiveness of our framework, we establish a ground-truth contextual ``thesaurus'' through a rigorous benchmarking with over 40 data, model, and XAI combinations for an explainable clustering analysis of a well-being scenario. Through a comprehensive quality and human-friendliness evaluation of our framework's explanations, we prove high content quality through strong correlations with ground-truth explanations (Spearman rank correlation=0.92) and improved interpretability and human-friendliness to non-experts through a user study (N=56). Our overall evaluation confirms trust in LLMs as HCXAI enablers, as our framework bridges the above Gaps by delivering (i) high-quality technical explanations aligned with foundational XAI methods and (ii) clear, efficient, and interpretable human-centered explanations for non-experts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at The 3rd World Conference on eXplainable Artificial Intelligence. This version corresponds to the camera-ready manuscript submitted to the conference proceedings",
    "pdf_url": "https://arxiv.org/pdf/2506.12240v1",
    "published_date": "2025-06-13 21:41:07 UTC",
    "updated_date": "2025-06-13 21:41:07 UTC"
  },
  {
    "arxiv_id": "2506.13807v1",
    "title": "BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis",
    "authors": [
      "Florian Kofler",
      "Marcel Rosier",
      "Mehdi Astaraki",
      "Ujjwal Baid",
      "Hendrik Möller",
      "Josef A. Buchner",
      "Felix Steinbauer",
      "Eva Oswald",
      "Ezequiel de la Rosa",
      "Ivan Ezhov",
      "Constantin von See",
      "Jan Kirschke",
      "Anton Schmick",
      "Sarthak Pati",
      "Akis Linardos",
      "Carla Pitarch",
      "Sanyukta Adap",
      "Jeffrey Rudie",
      "Maria Correia de Verdier",
      "Rachit Saluja",
      "Evan Calabrese",
      "Dominic LaBella",
      "Mariam Aboian",
      "Ahmed W. Moawad",
      "Nazanin Maleki",
      "Udunna Anazodo",
      "Maruf Adewole",
      "Marius George Linguraru",
      "Anahita Fathi Kazerooni",
      "Zhifan Jiang",
      "Gian Marco Conte",
      "Hongwei Li",
      "Juan Eugenio Iglesias",
      "Spyridon Bakas",
      "Benedikt Wiestler",
      "Marie Piraud",
      "Bjoern Menze"
    ],
    "abstract": "The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "27p, 2figs, 3tabs",
    "pdf_url": "https://arxiv.org/pdf/2506.13807v1",
    "published_date": "2025-06-13 21:28:01 UTC",
    "updated_date": "2025-06-13 21:28:01 UTC"
  },
  {
    "arxiv_id": "2506.12227v2",
    "title": "Uncovering Bias Paths with LLM-guided Causal Discovery: An Active Learning and Dynamic Scoring Approach",
    "authors": [
      "Khadija Zanna",
      "Akane Sano"
    ],
    "abstract": "Ensuring fairness in machine learning requires understanding how sensitive attributes like race or gender causally influence outcomes. Existing causal discovery (CD) methods often struggle to recover fairness-relevant pathways in the presence of noise, confounding, or data corruption. Large language models (LLMs) offer a complementary signal by leveraging semantic priors from variable metadata. We propose a hybrid LLM-guided CD framework that extends a breadth-first search strategy with active learning and dynamic scoring. Variable pairs are prioritized for querying using a composite score combining mutual information, partial correlation, and LLM confidence, enabling more efficient and robust structure discovery. To evaluate fairness sensitivity, we introduce a semi-synthetic benchmark based on the UCI Adult dataset, embedding domain-informed bias pathways alongside noise and latent confounders. We assess how well CD methods recover both global graph structure and fairness-critical paths (e.g., sex-->education-->income). Our results demonstrate that LLM-guided methods, including our active, dynamically scored variant, outperform baselines in recovering fairness-relevant structure under noisy conditions. We analyze when LLM-driven insights complement statistical dependencies and discuss implications for fairness auditing in high-stakes domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To be presented at AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.12227v2",
    "published_date": "2025-06-13 21:04:03 UTC",
    "updated_date": "2026-01-07 04:43:38 UTC"
  },
  {
    "arxiv_id": "2506.12224v1",
    "title": "Mapping Neural Theories of Consciousness onto the Common Model of Cognition",
    "authors": [
      "Paul S. Rosenbloom",
      "John E. Laird",
      "Christian Lebiere",
      "Andrea Stocco"
    ],
    "abstract": "A beginning is made at mapping four neural theories of consciousness onto the Common Model of Cognition. This highlights how the four jointly depend on recurrent local modules plus a cognitive cycle operating on a global working memory with complex states, and reveals how an existing integrative view of consciousness from a neural perspective aligns with the Com-mon Model.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12224v1",
    "published_date": "2025-06-13 20:51:06 UTC",
    "updated_date": "2025-06-13 20:51:06 UTC"
  },
  {
    "arxiv_id": "2506.12222v1",
    "title": "SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes",
    "authors": [
      "Tony Alex",
      "Sara Ahmed",
      "Armin Mustafa",
      "Muhammad Awais",
      "Philip JB Jackson"
    ],
    "abstract": "Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the SSL pre-training has sufficiently equipped them to handle real-world audio. However, a critical question remains: how well do these models actually perform in real-world conditions, where audio is typically polyphonic and complex, involving multiple overlapping sound sources? Current audio SSL methods are often benchmarked on datasets predominantly featuring monophonic audio, such as environmental sounds, and speech. As a result, the ability of SSL models to generalize to polyphonic audio, a common characteristic in natural scenarios, remains underexplored. This limitation raises concerns about the practical robustness of SSL models in more realistic audio settings. To address this gap, we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel direction in audio SSL research, designed to improve, designed to improve the model's ability to learn from polyphonic data while maintaining strong performance on monophonic data. We thoroughly evaluate SSLAM on standard audio SSL benchmark datasets which are predominantly monophonic and conduct a comprehensive comparative analysis against SOTA methods using a range of high-quality, publicly available polyphonic datasets. SSLAM not only improves model performance on polyphonic audio, but also maintains or exceeds performance on standard audio SSL benchmarks. Notably, it achieves up to a 3.9\\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision (mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear evaluation and fine-tuning regimes with performance improvements of up to 9.1\\% (mAP).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ICLR 2025. Code and pre-trained models are available at \\url{https://github.com/ta012/SSLAM}",
    "pdf_url": "https://arxiv.org/pdf/2506.12222v1",
    "published_date": "2025-06-13 20:48:46 UTC",
    "updated_date": "2025-06-13 20:48:46 UTC"
  },
  {
    "arxiv_id": "2506.12220v2",
    "title": "Two Heads Are Better than One: Simulating Large Transformers with Small Ones",
    "authors": [
      "Hantao Yu",
      "Josh Alman"
    ],
    "abstract": "The quadratic complexity of self-attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences?\n  In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12220v2",
    "published_date": "2025-06-13 20:47:12 UTC",
    "updated_date": "2025-06-19 02:24:00 UTC"
  },
  {
    "arxiv_id": "2506.12217v1",
    "title": "From Emergence to Control: Probing and Modulating Self-Reflection in Language Models",
    "authors": [
      "Xudong Zhu",
      "Jiachen Jiang",
      "Mohammad Mahdi Khalili",
      "Zhihui Zhu"
    ],
    "abstract": "Self-reflection -- the ability of a large language model (LLM) to revisit, evaluate, and revise its own reasoning -- has recently emerged as a powerful behavior enabled by reinforcement learning with verifiable rewards (RLVR). While self-reflection correlates with improved reasoning accuracy, its origin and underlying mechanisms remain poorly understood. In this work, {\\it we first show that self-reflection is not exclusive to RLVR fine-tuned models: it already emerges, albeit rarely, in pretrained models}. To probe this latent ability, we introduce Reflection-Inducing Probing, a method that injects reflection-triggering reasoning traces from fine-tuned models into pretrained models. This intervention raises self-reflection frequency of Qwen2.5 from 0.6\\% to 18.6\\%, revealing a hidden capacity for reflection. Moreover, our analysis of internal representations shows that both pretrained and fine-tuned models maintain hidden states that distinctly separate self-reflective from non-reflective contexts. Leveraging this observation, {\\it we then construct a self-reflection vector, a direction in activation space associated with self-reflective reasoning}. By manipulating this vector, we enable bidirectional control over the self-reflective behavior for both pretrained and fine-tuned models. Experiments across multiple reasoning benchmarks show that enhancing these vectors improves reasoning performance by up to 12\\%, while suppressing them reduces computational cost, providing a flexible mechanism to navigate the trade-off between reasoning quality and efficiency without requiring additional training. Our findings further our understanding of self-reflection and support a growing body of work showing that understanding model internals can enable precise behavioral control.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.12217v1",
    "published_date": "2025-06-13 20:40:13 UTC",
    "updated_date": "2025-06-13 20:40:13 UTC"
  },
  {
    "arxiv_id": "2506.12204v1",
    "title": "Semantic Scheduling for LLM Inference",
    "authors": [
      "Wenyue Hua",
      "Dujian Ding",
      "Yile Gu",
      "Yujie Ren",
      "Kai Mei",
      "Minghua Ma",
      "William Yang Wang"
    ],
    "abstract": "Conventional operating system scheduling algorithms are largely content-ignorant, making decisions based on factors such as latency or fairness without considering the actual intents or semantics of processes. Consequently, these algorithms often do not prioritize tasks that require urgent attention or carry higher importance, such as in emergency management scenarios. However, recent advances in language models enable semantic analysis of processes, allowing for more intelligent and context-aware scheduling decisions. In this paper, we introduce the concept of semantic scheduling in scheduling of requests from large language models (LLM), where the semantics of the process guide the scheduling priorities. We present a novel scheduling algorithm with optimal time complexity, designed to minimize the overall waiting time in LLM-based prompt scheduling. To illustrate its effectiveness, we present a medical emergency management application, underscoring the potential benefits of semantic scheduling for critical, time-sensitive tasks. The code and data are available at https://github.com/Wenyueh/latency_optimization_with_priority_constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.OS"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.12204v1",
    "published_date": "2025-06-13 20:15:58 UTC",
    "updated_date": "2025-06-13 20:15:58 UTC"
  },
  {
    "arxiv_id": "2506.12202v1",
    "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions",
    "authors": [
      "Stephen Mell",
      "Botong Zhang",
      "David Mell",
      "Shuo Li",
      "Ramya Ramalingam",
      "Nathan Yu",
      "Steve Zdancewic",
      "Osbert Bastani"
    ],
    "abstract": "Modern large language models (LLMs) are often deployed as agents, calling external tools adaptively to solve tasks. Rather than directly calling tools, it can be more effective for LLMs to write code to perform the tool calls, enabling them to automatically generate complex control flow such as conditionals and loops. Such code actions are typically provided as Python code, since LLMs are quite proficient at it; however, Python may not be the ideal language due to limited built-in support for performance, security, and reliability. We propose a novel programming language for code actions, called Quasar, which has several benefits: (1) automated parallelization to improve performance, (2) uncertainty quantification to improve reliability and mitigate hallucinations, and (3) security features enabling the user to validate actions. LLMs can write code in a subset of Python, which is automatically transpiled to Quasar. We evaluate our approach on the ViperGPT visual question answering agent, applied to the GQA dataset, demonstrating that LLMs with Quasar actions instead of Python actions retain strong performance, while reducing execution time when possible by 42%, improving security by reducing user approval interactions when possible by 52%, and improving reliability by applying conformal prediction to achieve a desired target coverage level.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12202v1",
    "published_date": "2025-06-13 20:11:22 UTC",
    "updated_date": "2025-06-13 20:11:22 UTC"
  },
  {
    "arxiv_id": "2506.12200v4",
    "title": "PRO-V-R1: Reasoning Enhanced Programming Agent for RTL Verification",
    "authors": [
      "Yujie Zhao",
      "Zhijing Wu",
      "Boqin Yuan",
      "Zhongming Yu",
      "Hejia Zhang",
      "Wentao Ni",
      "Chia-Tung Ho",
      "Haoxing Ren",
      "Jishen Zhao"
    ],
    "abstract": "Register-Transfer Level (RTL) verification is a primary bottleneck, consuming 60-70% of development time. While Large Language Models (LLMs) show promise for RTL automation, their performance and research focus have overwhelmingly centered on RTL generation rather than verification. Current methods for RTL verification rely on large scale proprietary models (e.g., GPT-4o) to generate Python-based functional references, incurring a high cost and raising data-privacy risks. To date, an end-to-end open-source solution for autonomous verification remains absent.\n  We introduce PRO-V-R1, the first trainable open-source agentic framework for autonomous RTL verification. Our contributions are threefold: (1) we design PRO-V sys, a modular agentic system that couples LLM-based reasoning with programmatic tool use for RTL verification; (2) we establish a data construction pipeline that leverages existing RTL datasets to build simulation-validated, expert-level trajectories tailored for supervised fine-tuning (SFT) RTL verification agents; and (3) we implement an efficient reinforcement learning (RL) algorithm that uses verification-specific rewards derived from program-tool feedback to optimize the end-to-end verification workflow. Our empirical evaluation demonstrates PRO-V-R1 achieves a 57.7% functional correctness rate and 34.0% in robust fault detection, significantly outperforming the base model's 25.7% and 21.8% (respectively) from the state-of-the-art (SOTA) automatic verification system. This configuration also outperforms large-scale proprietary LLMs in functional correctness and shows comparable robustness for fault detection.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12200v4",
    "published_date": "2025-06-13 20:06:34 UTC",
    "updated_date": "2025-12-08 23:18:22 UTC"
  },
  {
    "arxiv_id": "2506.12199v1",
    "title": "ViSAGe: Video-to-Spatial Audio Generation",
    "authors": [
      "Jaeyeon Kim",
      "Heeseung Yun",
      "Gunhee Kim"
    ],
    "abstract": "Spatial audio is essential for enhancing the immersiveness of audio-visual experiences, yet its production typically demands complex recording systems and specialized expertise. In this work, we address a novel problem of generating first-order ambisonics, a widely used spatial audio format, directly from silent videos. To support this task, we introduce YT-Ambigen, a dataset comprising 102K 5-second YouTube video clips paired with corresponding first-order ambisonics. We also propose new evaluation metrics to assess the spatial aspect of generated audio based on audio energy maps and saliency metrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an end-to-end framework that generates first-order ambisonics from silent video frames by leveraging CLIP visual features, autoregressive neural audio codec modeling with both directional and visual guidance. Experimental results demonstrate that ViSAGe produces plausible and coherent first-order ambisonics, outperforming two-stage approaches consisting of video-to-audio generation and audio spatialization. Qualitative examples further illustrate that ViSAGe generates temporally aligned high-quality spatial audio that adapts to viewpoint changes.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ICLR 2025. Project page: https://jaeyeonkim99.github.io/visage/",
    "pdf_url": "https://arxiv.org/pdf/2506.12199v1",
    "published_date": "2025-06-13 19:57:42 UTC",
    "updated_date": "2025-06-13 19:57:42 UTC"
  },
  {
    "arxiv_id": "2506.12190v3",
    "title": "BreastDCEDL: A Comprehensive Breast Cancer DCE-MRI Dataset and Transformer Implementation for Treatment Response Prediction",
    "authors": [
      "Naomi Fridman",
      "Bubby Solway",
      "Tomer Fridman",
      "Itamar Barnea",
      "Anat Goldstein"
    ],
    "abstract": "Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+/HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12190v3",
    "published_date": "2025-06-13 19:31:57 UTC",
    "updated_date": "2025-07-13 14:35:18 UTC"
  },
  {
    "arxiv_id": "2506.12189v2",
    "title": "Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis",
    "authors": [
      "Pranav Agarwal",
      "Ioana Ciucă"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications. Project Page - https://www.supernova-event.ai/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Actionable Interpretability Workshop at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12189v2",
    "published_date": "2025-06-13 19:31:52 UTC",
    "updated_date": "2025-06-22 23:32:27 UTC"
  },
  {
    "arxiv_id": "2506.12186v2",
    "title": "MRI-CORE: A Foundation Model for Magnetic Resonance Imaging",
    "authors": [
      "Haoyu Dong",
      "Yuwen Chen",
      "Hanxue Gu",
      "Nicholas Konz",
      "Yaqian Chen",
      "Qihang Li",
      "Maciej A. Mazurowski"
    ],
    "abstract": "The widespread use of Magnetic Resonance Imaging (MRI) in combination with deep learning shows promise for many high-impact automated diagnostic and prognostic tools. However, training new models requires large amounts of labeled data, a challenge due to high cost of precise annotations and data privacy. To address this issue, we introduce the MRI-CORE, a vision foundation model trained using more than 6 million slices from over 110 thousand MRI volumes across 18 body locations. Our experiments show notable improvements in performance over state-of-the-art methods in 13 data-restricted segmentation tasks, as well as in image classification, and zero-shot segmentation, showing the strong potential of MRI-CORE to enable data-efficient development of artificial intelligence models. We also present data on which strategies yield most useful foundation models and a novel analysis relating similarity between pre-training and downstream task data with transfer learning performance. Our model is publicly available with a permissive license.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "36 pages, under review",
    "pdf_url": "https://arxiv.org/pdf/2506.12186v2",
    "published_date": "2025-06-13 19:26:56 UTC",
    "updated_date": "2025-07-22 19:20:31 UTC"
  },
  {
    "arxiv_id": "2506.12185v1",
    "title": "Artificial Intelligence and Machine Learning in the Development of Vaccines and Immunotherapeutics Yesterday, Today, and Tomorrow",
    "authors": [
      "Elhoucine Elfatimi",
      "Yassir Lekbach",
      "Swayam Prakash",
      "Lbachir BenMohamed"
    ],
    "abstract": "In the past, the development of vaccines and immunotherapeutics relied heavily on trial-and-error experimentation and extensive in vivo testing, often requiring years of pre-clinical and clinical trials. Today, artificial intelligence (AI) and deep learning (DL) are actively transforming vaccine and immunotherapeutic design, by (i) offering predictive frameworks that support rapid, data-driven decision-making; (ii) increasingly being implemented as time- and resource-efficient strategies that integrate computational models, systems vaccinology, and multi-omics data to better phenotype, differentiate, and classify patient diseases and cancers; predict patients' immune responses; and identify the factors contributing to optimal vaccine and immunotherapeutic protective efficacy; (iii) refining the selection of B- and T-cell antigen/epitope targets to enhance efficacy and durability of immune protection; and (iv) enabling a deeper understanding of immune regulation, immune evasion, immune checkpoints, and regulatory pathways. The future of AI and DL points toward (i) replacing animal preclinical testing of drugs, vaccines, and immunotherapeutics with computational-based models, as recently proposed by the United States FDA; and (ii) enabling real-time in vivo modeling for immunobridging and prediction of protection in clinical trials. This may result in a fast and transformative shift for the development of personal vaccines and immunotherapeutics against infectious pathogens and cancers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12185v1",
    "published_date": "2025-06-13 19:20:43 UTC",
    "updated_date": "2025-06-13 19:20:43 UTC"
  },
  {
    "arxiv_id": "2506.12165v1",
    "title": "TCN-DPD: Parameter-Efficient Temporal Convolutional Networks for Wideband Digital Predistortion",
    "authors": [
      "Huanqiang Duan",
      "Manno Versluis",
      "Qinyu Chen",
      "Leo C. N. de Vreede",
      "Chang Gao"
    ],
    "abstract": "Digital predistortion (DPD) is essential for mitigating nonlinearity in RF power amplifiers, particularly for wideband applications. This paper presents TCN-DPD, a parameter-efficient architecture based on temporal convolutional networks, integrating noncausal dilated convolutions with optimized activation functions. Evaluated on the OpenDPD framework with the DPA_200MHz dataset, TCN-DPD achieves simulated ACPRs of -51.58/-49.26 dBc (L/R), EVM of -47.52 dB, and NMSE of -44.61 dB with 500 parameters and maintains superior linearization than prior models down to 200 parameters, making it promising for efficient wideband PA linearization.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted to IEEE MTT-S International Microwave Symposium (IMS) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12165v1",
    "published_date": "2025-06-13 18:30:32 UTC",
    "updated_date": "2025-06-13 18:30:32 UTC"
  },
  {
    "arxiv_id": "2506.12156v1",
    "title": "Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models",
    "authors": [
      "Shehroz S. Khan",
      "Ali Abedi",
      "Charlene H. Chu"
    ],
    "abstract": "Interpreting large volumes of high-dimensional, unlabeled data in a manner that is comprehensible to humans remains a significant challenge across various domains. In unsupervised healthcare data analysis, interpreting clustered data can offer meaningful insights into patients' health outcomes, which hold direct implications for healthcare providers. This paper addresses the problem of interpreting clustered sensor data collected from older adult patients recovering from lower-limb fractures in the community. A total of 560 days of multimodal sensor data, including acceleration, step count, ambient motion, GPS location, heart rate, and sleep, alongside clinical scores, were remotely collected from patients at home. Clustering was first carried out separately for each data modality to assess the impact of feature sets extracted from each modality on patients' recovery trajectories. Then, using context-aware prompting, a large language model was employed to infer meaningful cluster labels for the clusters derived from each modality. The quality of these clusters and their corresponding labels was validated through rigorous statistical testing and visualization against clinical scores collected alongside the multimodal sensor data. The results demonstrated the statistical significance of most modality-specific cluster labels generated by the large language model with respect to clinical scores, confirming the efficacy of the proposed method for interpreting sensor data in an unsupervised manner. This unsupervised data analysis approach, relying solely on sensor data, enables clinicians to identify at-risk patients and take timely measures to improve health outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 2 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.12156v1",
    "published_date": "2025-06-13 18:19:28 UTC",
    "updated_date": "2025-06-13 18:19:28 UTC"
  },
  {
    "arxiv_id": "2506.12152v1",
    "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability",
    "authors": [
      "Been Kim",
      "John Hewitt",
      "Neel Nanda",
      "Noah Fiedel",
      "Oyvind Tafjord"
    ],
    "abstract": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12152v1",
    "published_date": "2025-06-13 18:13:58 UTC",
    "updated_date": "2025-06-13 18:13:58 UTC"
  },
  {
    "arxiv_id": "2506.12015v2",
    "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
    "authors": [
      "Hsi-Che Lin",
      "Yu-Chu Yu",
      "Kai-Po Chang",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model, which originally required 95GB of memory, on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Project page: https://hsi-che-lin.github.io/EMLoC/",
    "pdf_url": "https://arxiv.org/pdf/2506.12015v2",
    "published_date": "2025-06-13 17:59:58 UTC",
    "updated_date": "2026-01-04 08:08:32 UTC"
  },
  {
    "arxiv_id": "2506.12014v1",
    "title": "code_transformed: The Influence of Large Language Models on Code",
    "authors": [
      "Yuliang Xu",
      "Siming Huang",
      "Mingmeng Geng",
      "Yao Wan",
      "Xuanhua Shi",
      "Dongping Chen"
    ],
    "abstract": "Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "We release all the experimental dataset and source code at: https://github.com/ignorancex/LLM_code",
    "pdf_url": "https://arxiv.org/pdf/2506.12014v1",
    "published_date": "2025-06-13 17:59:39 UTC",
    "updated_date": "2025-06-13 17:59:39 UTC"
  },
  {
    "arxiv_id": "2506.12012v1",
    "title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for Planning, Revision, and Resource-Constrained Decision Making",
    "authors": [
      "Xiaopeng Yuan",
      "Xingjian Zhang",
      "Ke Xu",
      "Yifan Xu",
      "Lijun Yu",
      "Jindong Wang",
      "Yushun Dong",
      "Haohan Wang"
    ],
    "abstract": "Large language models (LLMs) are increasingly used for tasks that require complex reasoning. Most benchmarks focus on final outcomes but overlook the intermediate reasoning steps - such as planning, revision, and decision making under resource constraints. We argue that measuring these internal processes is essential for understanding model behavior and improving reliability. We propose using strategic games as a natural evaluation environment: closed, rule-based systems with clear states, limited resources, and automatic feedback. We introduce a framework that evaluates LLMs along three core dimensions: planning, revision, and resource-constrained decision making. To operationalize this, we define metrics beyond win rate, including overcorrection risk rate, correction success rate, improvement slope, and over-budget ratio. In 4320 adversarial rounds across 12 leading models, ChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7 percent, a correction success rate of 78.6 percent, and an improvement slope of 0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6 percent, wins only 25.6 percent of its matches - primarily due to excessive resource use. We also observe a negative correlation between overcorrection risk rate and correction success rate (Pearson r = -0.51, p = 0.093), suggesting that more frequent edits do not always improve outcomes. Our findings highlight the value of assessing not only what LLMs decide but how they arrive at those decisions",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 7 figures. Under review",
    "pdf_url": "https://arxiv.org/pdf/2506.12012v1",
    "published_date": "2025-06-13 17:59:10 UTC",
    "updated_date": "2025-06-13 17:59:10 UTC"
  },
  {
    "arxiv_id": "2506.12119v1",
    "title": "Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?",
    "authors": [
      "Houyi Li",
      "Ka Man Lo",
      "Ziqi Wang",
      "Zili Wang",
      "Wenzhen Zheng",
      "Shuigeng Zhou",
      "Xiangyu Zhang",
      "Daxin Jiang"
    ],
    "abstract": "Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. However, can MoEs surpass dense architectures under strictly equal resource constraints - that is, when the total parameter count, training compute, and data budget are identical? This question remains under-explored despite its significant practical value and potential. In this paper, we propose a novel perspective and methodological framework to study this question thoroughly. First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance. Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource. More importantly, this optimal region remains consistent across different model sizes. Although additional amount of data turns out to be a trade-off for the enhanced performance, we show that this can be resolved via reusing data. We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. All models will be released publicly.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12119v1",
    "published_date": "2025-06-13 17:59:05 UTC",
    "updated_date": "2025-06-13 17:59:05 UTC"
  },
  {
    "arxiv_id": "2506.12008v1",
    "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI",
    "authors": [
      "Olga Vechtomova",
      "Jeff Bos"
    ],
    "abstract": "Dance performance traditionally follows a unidirectional relationship where movement responds to music. While AI has advanced in various creative domains, its application in dance has primarily focused on generating choreography from musical input. We present a system that enables dancers to dynamically shape musical environments through their movements. Our multi-modal architecture creates a coherent musical composition by intelligently combining pre-recorded musical clips in response to dance movements, establishing a bidirectional creative partnership where dancers function as both performers and composers. Through correlation analysis of performance data, we demonstrate emergent communication patterns between movement qualities and audio features. This approach reconceptualizes the role of AI in performing arts as a responsive collaborator that expands possibilities for both professional dance performance and improvisational artistic expression across broader populations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted for publication at ICCC 2025 (International Conference on Computational Creativity)",
    "pdf_url": "https://arxiv.org/pdf/2506.12008v1",
    "published_date": "2025-06-13 17:56:53 UTC",
    "updated_date": "2025-06-13 17:56:53 UTC"
  },
  {
    "arxiv_id": "2506.12003v2",
    "title": "Upgrade or Switch: Do We Need a Next-Gen Trusted Architecture for the Internet of AI Agents?",
    "authors": [
      "Ramesh Raskar",
      "Pradyumna Chari",
      "Jared James Grogan",
      "Mahesh Lambe",
      "Robert Lincourt",
      "Raghu Bala",
      "Aditi Joshi",
      "Abhishek Singh",
      "Ayush Chopra",
      "Rajesh Ranjan",
      "Shailja Gupta",
      "Dimitris Stripelis",
      "Maria Gorskikh",
      "Sichao Wang"
    ],
    "abstract": "The emerging Internet of AI Agents challenges existing web infrastructure designed for human-scale, reactive interactions. Unlike traditional web resources, autonomous AI agents initiate actions, maintain persistent state, spawn sub-agents, and negotiate directly with peers: demanding millisecond-level discovery, instant credential revocation, and cryptographic behavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes whether to upgrade existing infrastructure or implement purpose-built index architectures for autonomous agents. We identify critical failure points: DNS propagation (24-48 hours vs. required milliseconds), certificate revocation unable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate for agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2) Switch options, (3) Hybrid index/registries. Drawing parallels to dialup-to-broadband transitions, we find that agent requirements constitute qualitative, and not incremental, changes. While upgrades offer compatibility and faster deployment, clean-slate solutions provide better performance but require longer for adoption. Our analysis suggests hybrid approaches will emerge, with centralized indexes for critical agents and federated meshes for specialized use cases.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12003v2",
    "published_date": "2025-06-13 17:55:38 UTC",
    "updated_date": "2025-07-11 17:44:32 UTC"
  },
  {
    "arxiv_id": "2506.11991v2",
    "title": "VGR: Visual Grounded Reasoning",
    "authors": [
      "Jiacong Wang",
      "Zijian Kang",
      "Haochen Wang",
      "Haiyong Jiang",
      "Jiawen Li",
      "Bohong Wu",
      "Ya Wang",
      "Jiao Ran",
      "Xiao Liang",
      "Chao Feng",
      "Jun Xiao"
    ],
    "abstract": "In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11991v2",
    "published_date": "2025-06-13 17:47:43 UTC",
    "updated_date": "2025-06-16 07:35:52 UTC"
  },
  {
    "arxiv_id": "2506.11986v1",
    "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task",
    "authors": [
      "Wuzhenghong Wen",
      "Su Pan",
      "yuwei Sun"
    ],
    "abstract": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10\\% improvement in filter accuracy compared to the existing method. Our code is available at https://github.com/hongWin/Schema-R1/.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 3 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2506.11986v1",
    "published_date": "2025-06-13 17:46:02 UTC",
    "updated_date": "2025-06-13 17:46:02 UTC"
  },
  {
    "arxiv_id": "2506.13805v1",
    "title": "Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases",
    "authors": [
      "Bonam Mingole",
      "Aditya Majumdar",
      "Firdaus Ahmed Choudhury",
      "Jennifer L. Kraschnewski",
      "Shyam S. Sundar",
      "Amulya Yadav"
    ],
    "abstract": "The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13805v1",
    "published_date": "2025-06-13 17:12:47 UTC",
    "updated_date": "2025-06-13 17:12:47 UTC"
  },
  {
    "arxiv_id": "2506.11954v1",
    "title": "Technical Evaluation of a Disruptive Approach in Homomorphic AI",
    "authors": [
      "Eric Filiol"
    ],
    "abstract": "We present a technical evaluation of a new, disruptive cryptographic approach to data security, known as HbHAI (Hash-based Homomorphic Artificial Intelligence). HbHAI is based on a novel class of key-dependent hash functions that naturally preserve most similarity properties, most AI algorithms rely on. As a main claim, HbHAI makes now possible to analyze and process data in its cryptographically secure form while using existing native AI algorithms without modification, with unprecedented performances compared to existing homomorphic encryption schemes.\n  We tested various HbHAI-protected datasets (non public preview) using traditional unsupervised and supervised learning techniques (clustering, classification, deep neural networks) with classical unmodified AI algorithms. This paper presents technical results from an independent analysis conducted with those different, off-the-shelf AI algorithms. The aim was to assess the security, operability and performance claims regarding HbHAI techniques. As a results, our results confirm most these claims, with only a few minor reservations.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "This is the extended version of the talk presented at CyberWiseCon 2025 in Vilnius, Lituania in May 21$^{st}$-23$^{rd}$, 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11954v1",
    "published_date": "2025-06-13 17:06:34 UTC",
    "updated_date": "2025-06-13 17:06:34 UTC"
  },
  {
    "arxiv_id": "2506.11948v2",
    "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies",
    "authors": [
      "Nadun Ranawaka Arachchige",
      "Zhenyang Chen",
      "Wonsuhk Jung",
      "Woo Chul Shin",
      "Rohan Bansal",
      "Pierre Barroso",
      "Yu Hang He",
      "Yingyang Celine Lin",
      "Benjamin Joffe",
      "Shreyas Kousik",
      "Danfei Xu"
    ],
    "abstract": "Offline Imitation Learning (IL) methods such as Behavior Cloning are effective at acquiring complex robotic manipulation skills. However, existing IL-trained policies are confined to executing the task at the same speed as shown in demonstration data. This limits the task throughput of a robotic system, a critical requirement for applications such as industrial automation. In this paper, we introduce and formalize the novel problem of enabling faster-than-demonstration execution of visuomotor policies and identify fundamental challenges in robot dynamics and state-action distribution shifts. We instantiate the key insights as SAIL (Speed Adaptation for Imitation Learning), a full-stack system integrating four tightly-connected components: (1) a consistency-preserving action inference algorithm for smooth motion at high speed, (2) high-fidelity tracking of controller-invariant motion targets, (3) adaptive speed modulation that dynamically adjusts execution speed based on motion complexity, and (4) action scheduling to handle real-world system latencies. Experiments on 12 tasks across simulation and two real, distinct robot platforms show that SAIL achieves up to a 4x speedup over demonstration speed in simulation and up to 3.2x speedup in the real world. Additional detail is available at https://nadunranawaka1.github.io/sail-policy",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "The first two authors contributed equally. Accepted to CoRL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11948v2",
    "published_date": "2025-06-13 16:58:20 UTC",
    "updated_date": "2025-09-08 02:56:08 UTC"
  },
  {
    "arxiv_id": "2506.11945v1",
    "title": "Subjective Experience in AI Systems: What Do AI Researchers and the Public Believe?",
    "authors": [
      "Noemi Dreksler",
      "Lucius Caviola",
      "David Chalmers",
      "Carter Allen",
      "Alex Rand",
      "Joshua Lewis",
      "Philip Waggoner",
      "Kate Mays",
      "Jeff Sebo"
    ],
    "abstract": "We surveyed 582 AI researchers who have published in leading AI venues and 838 nationally representative US participants about their views on the potential development of AI systems with subjective experience and how such systems should be treated and governed. When asked to estimate the chances that such systems will exist on specific dates, the median responses were 1% (AI researchers) and 5% (public) by 2024, 25% and 30% by 2034, and 70% and 60% by 2100, respectively. The median member of the public thought there was a higher chance that AI systems with subjective experience would never exist (25%) than the median AI researcher did (10%). Both groups perceived a need for multidisciplinary expertise to assess AI subjective experience. Although support for welfare protections for such AI systems exceeded opposition, it remained far lower than support for protections for animals or the environment. Attitudes toward moral and governance issues were divided in both groups, especially regarding whether such systems should be created and what rights or protections they should receive. Yet a majority of respondents in both groups agreed that safeguards against the potential risks from AI systems with subjective experience should be implemented by AI developers now, and if created, AI systems with subjective experience should treat others well, behave ethically, and be held accountable. Overall, these results suggest that both AI researchers and the public regard the emergence of AI systems with subjective experience as a possibility this century, though substantial uncertainty and disagreement remain about the timeline and appropriate response.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "109 pages, 27 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11945v1",
    "published_date": "2025-06-13 16:53:28 UTC",
    "updated_date": "2025-06-13 16:53:28 UTC"
  },
  {
    "arxiv_id": "2506.11939v1",
    "title": "Today's Cat Is Tomorrow's Dog: Accounting for Time-Based Changes in the Labels of ML Vulnerability Detection Approaches",
    "authors": [
      "Ranindya Paramitha",
      "Yuan Feng",
      "Fabio Massacci"
    ],
    "abstract": "Vulnerability datasets used for ML testing implicitly contain retrospective information. When tested on the field, one can only use the labels available at the time of training and testing (e.g. seen and assumed negatives). As vulnerabilities are discovered across calendar time, labels change and past performance is not necessarily aligned with future performance. Past works only considered the slices of the whole history (e.g. DiverseVUl) or individual differences between releases (e.g. Jimenez et al. ESEC/FSE 2019). Such approaches are either too optimistic in training (e.g. the whole history) or too conservative (e.g. consecutive releases). We propose a method to restructure a dataset into a series of datasets in which both training and testing labels change to account for the knowledge available at the time. If the model is actually learning, it should improve its performance over time as more data becomes available and data becomes more stable, an effect that can be checked with the Mann-Kendall test. We validate our methodology for vulnerability detection with 4 time-based datasets (3 projects from BigVul dataset + Vuldeepecker's NVD) and 5 ML models (Code2Vec, CodeBERT, LineVul, ReGVD, and Vuldeepecker). In contrast to the intuitive expectation (more retrospective information, better performance), the trend results show that performance changes inconsistently across the years, showing that most models are not learning.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at The ACM International Conference on the Foundations of Software Engineering (FSE) 2025. Published in the Proceedings of the ACM on Software Engineering (PACMSE), Issue FSE 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11939v1",
    "published_date": "2025-06-13 16:42:21 UTC",
    "updated_date": "2025-06-13 16:42:21 UTC"
  },
  {
    "arxiv_id": "2506.11938v2",
    "title": "Improving Large Language Model Safety with Contrastive Representation Learning",
    "authors": [
      "Samuel Simko",
      "Mrinmaya Sachan",
      "Bernhard Schölkopf",
      "Zhijing Jin"
    ],
    "abstract": "Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at https://github.com/samuelsimko/crl-llm-defense",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2506.11938v2",
    "published_date": "2025-06-13 16:42:09 UTC",
    "updated_date": "2025-12-28 18:40:17 UTC"
  },
  {
    "arxiv_id": "2506.11928v1",
    "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?",
    "authors": [
      "Zihan Zheng",
      "Zerui Cheng",
      "Zeyu Shen",
      "Shang Zhou",
      "Kaiyuan Liu",
      "Hansen He",
      "Dongruixuan Li",
      "Stanley Wei",
      "Hangyi Hao",
      "Jianzhu Yao",
      "Peiyao Sheng",
      "Zixuan Wang",
      "Wenhao Chai",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Sanjeev Arora",
      "Pramod Viswanath",
      "Jingbo Shang",
      "Saining Xie"
    ],
    "abstract": "Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Project Page at https://livecodebenchpro.com/",
    "pdf_url": "https://arxiv.org/pdf/2506.11928v1",
    "published_date": "2025-06-13 16:29:09 UTC",
    "updated_date": "2025-06-13 16:29:09 UTC"
  },
  {
    "arxiv_id": "2506.11925v1",
    "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on Knowledge Graph Embeddings and Bayesian Inference",
    "authors": [
      "M. Manzour",
      "Catherine M. Elias",
      "Omar M. Shehata",
      "R. Izquierdo",
      "M. A. Sotelo"
    ],
    "abstract": "Research on lane change prediction has gained a lot of momentum in the last couple of years. However, most research is confined to simulation or results obtained from datasets, leaving a gap between algorithmic advances and on-road deployment. This work closes that gap by demonstrating, on real hardware, a lane-change prediction system based on Knowledge Graph Embeddings (KGEs) and Bayesian inference. Moreover, the ego-vehicle employs a longitudinal braking action to ensure the safety of both itself and the surrounding vehicles. Our architecture consists of two modules: (i) a perception module that senses the environment, derives input numerical features, and converts them into linguistic categories; and communicates them to the prediction module; (ii) a pretrained prediction module that executes a KGE and Bayesian inference model to anticipate the target vehicle's maneuver and transforms the prediction into longitudinal braking action. Real-world hardware experimental validation demonstrates that our prediction system anticipates the target vehicle's lane change three to four seconds in advance, providing the ego vehicle sufficient time to react and allowing the target vehicle to make the lane change safely.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11925v1",
    "published_date": "2025-06-13 16:24:28 UTC",
    "updated_date": "2025-06-13 16:24:28 UTC"
  },
  {
    "arxiv_id": "2506.11912v1",
    "title": "Breaking Habits: On the Role of the Advantage Function in Learning Causal State Representations",
    "authors": [
      "Miguel Suau"
    ],
    "abstract": "Recent work has shown that reinforcement learning agents can develop policies that exploit spurious correlations between rewards and observations. This phenomenon, known as policy confounding, arises because the agent's policy influences both past and future observation variables, creating a feedback loop that can hinder the agent's ability to generalize beyond its usual trajectories. In this paper, we show that the advantage function, commonly used in policy gradient methods, not only reduces the variance of gradient estimates but also mitigates the effects of policy confounding. By adjusting action values relative to the state representation, the advantage function downweights state-action pairs that are more likely under the current policy, breaking spurious correlations and encouraging the agent to focus on causal factors. We provide both analytical and empirical evidence demonstrating that training with the advantage function leads to improved out-of-trajectory performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11912v1",
    "published_date": "2025-06-13 16:06:47 UTC",
    "updated_date": "2025-06-13 16:06:47 UTC"
  },
  {
    "arxiv_id": "2506.11908v2",
    "title": "Spectra-to-Structure and Structure-to-Spectra Inference Across the Periodic Table",
    "authors": [
      "Yufeng Wang",
      "Peiyao Wang",
      "Lu Wei",
      "Lu Ma",
      "Yuewei Lin",
      "Qun Liu",
      "Haibin Ling"
    ],
    "abstract": "X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local atomic environments, yet its interpretation remains limited by the need for expert-driven analysis, computationally expensive simulations, and element-specific heuristics. Recent advances in machine learning have shown promise for accelerating XAS interpretation, but many existing models are narrowly focused on specific elements, edge types, or spectral regimes. In this work, we present XAStruct, a learning-based system capable of both predicting XAS spectra from crystal structures and inferring local structural descriptors from XAS input. XAStruct is trained on a large-scale dataset spanning over 70 elements across the periodic table, enabling generalization to a wide variety of chemistries and bonding environments. The framework includes the first machine learning approach for predicting neighbor atom types directly from XAS spectra, as well as a generalizable regression model for mean nearest-neighbor distance that requires no element-specific tuning. By combining deep neural networks for complex structure property mappings with efficient baseline models for simpler tasks, XAStruct offers a scalable and extensible solution for data-driven XAS analysis and local structure inference. The source code will be released upon paper acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11908v2",
    "published_date": "2025-06-13 15:58:05 UTC",
    "updated_date": "2025-08-26 03:34:26 UTC"
  },
  {
    "arxiv_id": "2506.11901v1",
    "title": "A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification",
    "authors": [
      "Lu Zhang",
      "Sangarapillai Lambotharan",
      "Gan Zheng",
      "Fabio Roli"
    ],
    "abstract": "Advantages of deep learning over traditional methods have been demonstrated for radio signal classification in the recent years. However, various researchers have discovered that even a small but intentional feature perturbation known as adversarial examples can significantly deteriorate the performance of the deep learning based radio signal classification. Among various kinds of adversarial examples, universal adversarial perturbation has gained considerable attention due to its feature of being data independent, hence as a practical strategy to fool the radio signal classification with a high success rate. Therefore, in this paper, we investigate a defense system called neural rejection system to propose against universal adversarial perturbations, and evaluate its performance by generating white-box universal adversarial perturbations. We show that the proposed neural rejection system is able to defend universal adversarial perturbations with significantly higher accuracy than the undefended deep neural network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11901v1",
    "published_date": "2025-06-13 15:52:07 UTC",
    "updated_date": "2025-06-13 15:52:07 UTC"
  },
  {
    "arxiv_id": "2506.11892v1",
    "title": "Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices",
    "authors": [
      "Lu Zhang",
      "Sangarapillai Lambotharan",
      "Gan Zheng",
      "Guisheng Liao",
      "Basil AsSadhan",
      "Fabio Roli"
    ],
    "abstract": "Due to great success of transformers in many applications such as natural language processing and computer vision, transformers have been successfully applied in automatic modulation classification. We have shown that transformer-based radio signal classification is vulnerable to imperceptible and carefully crafted attacks called adversarial examples. Therefore, we propose a defense system against adversarial examples in transformer-based modulation classifications. Considering the need for computationally efficient architecture particularly for Internet of Things (IoT)-based applications or operation of devices in environment where power supply is limited, we propose a compact transformer for modulation classification. The advantages of robust training such as adversarial training in transformers may not be attainable in compact transformers. By demonstrating this, we propose a novel compact transformer that can enhance robustness in the presence of adversarial attacks. The new method is aimed at transferring the adversarial attention map from the robustly trained large transformer to a compact transformer. The proposed method outperforms the state-of-the-art techniques for the considered white-box scenarios including fast gradient method and projected gradient descent attacks. We have provided reasoning of the underlying working mechanisms and investigated the transferability of the adversarial examples between different architectures. The proposed method has the potential to protect the transformer from the transferability of adversarial examples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11892v1",
    "published_date": "2025-06-13 15:39:01 UTC",
    "updated_date": "2025-06-13 15:39:01 UTC"
  },
  {
    "arxiv_id": "2506.11890v1",
    "title": "Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training",
    "authors": [
      "Judson Leroy Dean Haynes"
    ],
    "abstract": "Virtual Reality simulators offer a powerful tool for teacher training, yet the integration of AI-powered student avatars presents a critical challenge: determining the optimal level of avatar realism for effective pedagogy. This literature review examines the evolution of avatar realism in VR teacher training, synthesizes its theoretical implications, and proposes a new pedagogical framework to guide future design. Through a systematic review, this paper traces the progression from human-controlled avatars to generative AI prototypes. Applying learning theories like Cognitive Load Theory, we argue that hyper-realism is not always optimal, as high-fidelity avatars can impose excessive extraneous cognitive load on novices, a stance supported by recent empirical findings. A significant gap exists between the technological drive for photorealism and the pedagogical need for scaffolded learning. To address this gap, we propose Graduated Realism, a framework advocating for starting trainees with lower-fidelity avatars and progressively increasing behavioral complexity as skills develop. To make this computationally feasible, we outline a novel single-call architecture, Crazy Slots, which uses a probabilistic engine and a Retrieval-Augmented Generation database to generate authentic, real-time responses without the latency and cost of multi-step reasoning models. This review provides evidence-based principles for designing the next generation of AI simulators, arguing that a pedagogically grounded approach to realism is essential for creating scalable and effective teacher education tools.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11890v1",
    "published_date": "2025-06-13 15:37:36 UTC",
    "updated_date": "2025-06-13 15:37:36 UTC"
  },
  {
    "arxiv_id": "2506.11887v3",
    "title": "Cascaded Language Models for Cost-effective Human-AI Decision-Making",
    "authors": [
      "Claudio Fanconi",
      "Mihaela van der Schaar"
    ],
    "abstract": "A challenge in human-AI decision-making is to balance three factors: the correctness of predictions, the cost of knowledge and reasoning complexity, and the confidence about whether to abstain from automated answers or escalate to human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise -- a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base model's answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, to overcome static policies and accommodate changing task difficulty, we incorporate an online learning mechanism which uses human feedback. We demonstrate this approach to general question-answering (ARC-Easy, ARC-Challenge, and MMLU) and medical question-answering (MedQA and MedMCQA). Our results demonstrate that our cascaded strategy outperforms single-model baselines in most cases, achieving higher accuracy while reducing costs and providing a principled approach to handling abstentions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11887v3",
    "published_date": "2025-06-13 15:36:22 UTC",
    "updated_date": "2025-10-24 14:06:15 UTC"
  },
  {
    "arxiv_id": "2506.12117v1",
    "title": "Scale-Invariance Drives Convergence in AI and Brain Representations",
    "authors": [
      "Junjie Yu",
      "Wenxiao Ma",
      "Jianyu Zhang",
      "Haotian Deng",
      "Zihan Deng",
      "Yi Guo",
      "Quanying Liu"
    ],
    "abstract": "Despite variations in architecture and pretraining strategies, recent studies indicate that large-scale AI models often converge toward similar internal representations that also align with neural activity. We propose that scale-invariance, a fundamental structural principle in natural systems, is a key driver of this convergence. In this work, we propose a multi-scale analytical framework to quantify two core aspects of scale-invariance in AI representations: dimensional stability and structural similarity across scales. We further investigate whether these properties can predict alignment performance with functional Magnetic Resonance Imaging (fMRI) responses in the visual cortex. Our analysis reveals that embeddings with more consistent dimension and higher structural similarity across scales align better with fMRI data. Furthermore, we find that the manifold structure of fMRI data is more concentrated, with most features dissipating at smaller scales. Embeddings with similar scale patterns align more closely with fMRI data. We also show that larger pretraining datasets and the inclusion of language modalities enhance the scale-invariance properties of embeddings, further improving neural alignment. Our findings indicate that scale-invariance is a fundamental structural principle that bridges artificial and biological representations, providing a new framework for evaluating the structural quality of human-like AI systems.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12117v1",
    "published_date": "2025-06-13 15:36:04 UTC",
    "updated_date": "2025-06-13 15:36:04 UTC"
  },
  {
    "arxiv_id": "2506.11882v2",
    "title": "An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing",
    "authors": [
      "Haochen Sun",
      "Yifan Liu",
      "Ahmed Al-Tahmeesschi",
      "Swarna Chetty",
      "Syed Ali Raza Zaidi",
      "Avishek Nag",
      "Hamed Ahmadi"
    ],
    "abstract": "Effective resource management and network slicing are essential to meet the diverse service demands of vehicular networks, including Enhanced Mobile Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC). This paper introduces an Explainable Deep Reinforcement Learning (XRL) framework for dynamic network slicing and resource allocation in vehicular networks, built upon a near-real-time RAN intelligent controller. By integrating a feature-based approach that leverages Shapley values and an attention mechanism, we interpret and refine the decisions of our reinforcementlearning agents, addressing key reliability challenges in vehicular communication systems. Simulation results demonstrate that our approach provides clear, real-time insights into the resource allocation process and achieves higher interpretability precision than a pure attention mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC services increased from 78.0% to 80.13%, while that for eMBB services improved from 71.44% to 73.21%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in Proceedings of IEEE PIMRC 2025. 6 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11882v2",
    "published_date": "2025-06-13 15:32:52 UTC",
    "updated_date": "2025-09-17 18:15:30 UTC"
  },
  {
    "arxiv_id": "2506.11880v1",
    "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment",
    "authors": [
      "Alejandro Peña",
      "Julian Fierrez",
      "Aythami Morales",
      "Gonzalo Mancera",
      "Miguel Lopez",
      "Ruben Tolosana"
    ],
    "abstract": "The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to AIES 2025 (Under Review)",
    "pdf_url": "https://arxiv.org/pdf/2506.11880v1",
    "published_date": "2025-06-13 15:29:43 UTC",
    "updated_date": "2025-06-13 15:29:43 UTC"
  },
  {
    "arxiv_id": "2506.11877v4",
    "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
    "authors": [
      "Jina Kim",
      "Jeffrey Willette",
      "Bruno Andreis",
      "Sung Ju Hwang"
    ],
    "abstract": "A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data-stemming from the onerous and costly nature of experimental validation-further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel bilevel optimization approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to learn how to generalize beyond the training distribution. We demonstrate significant performance gains on challenging real-world datasets with substantial covariate shift, supported by t-SNE visualizations highlighting our interpolation method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11877v4",
    "published_date": "2025-06-13 15:27:40 UTC",
    "updated_date": "2026-01-02 01:42:25 UTC"
  },
  {
    "arxiv_id": "2506.13804v1",
    "title": "Instruction and Solution Probabilities as Heuristics for Inductive Programming",
    "authors": [
      "Edward McDaid",
      "Sarah McDaid"
    ],
    "abstract": "Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13804v1",
    "published_date": "2025-06-13 15:24:31 UTC",
    "updated_date": "2025-06-13 15:24:31 UTC"
  },
  {
    "arxiv_id": "2507.00015v1",
    "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications",
    "authors": [
      "Lu Zhang",
      "Sangarapillai Lambotharan",
      "Gan Zheng",
      "Guisheng Liao",
      "Xuekang Liu",
      "Fabio Roli",
      "Carsten Maple"
    ],
    "abstract": "The remarkable success of transformers across various fields such as natural language processing and computer vision has paved the way for their applications in automatic modulation classification, a critical component in the communication systems of Internet of Things (IoT) devices. However, it has been observed that transformer-based classification of radio signals is susceptible to subtle yet sophisticated adversarial attacks. To address this issue, we have developed a defensive strategy for transformer-based modulation classification systems to counter such adversarial attacks. In this paper, we propose a novel vision transformer (ViT) architecture by introducing a new concept known as adversarial indicator (AdvI) token to detect adversarial attacks. To the best of our knowledge, this is the first work to propose an AdvI token in ViT to defend against adversarial attacks. Integrating an adversarial training method with a detection mechanism using AdvI token, we combine a training time defense and running time defense in a unified neural network model, which reduces architectural complexity of the system compared to detecting adversarial perturbations using separate models. We investigate into the operational principles of our method by examining the attention mechanism. We show the proposed AdvI token acts as a crucial element within the ViT, influencing attention weights and thereby highlighting regions or features in the input data that are potentially suspicious or anomalous. Through experimental results, we demonstrate that our approach surpasses several competitive methods in handling white-box attack scenarios, including those utilizing the fast gradient method, projected gradient descent attacks and basic iterative method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00015v1",
    "published_date": "2025-06-13 15:21:54 UTC",
    "updated_date": "2025-06-13 15:21:54 UTC"
  },
  {
    "arxiv_id": "2506.11860v1",
    "title": "MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command Line and Browser",
    "authors": [
      "Armina Fani",
      "Mike Doan",
      "Isabelle Le",
      "Alex Fedorov",
      "Malte Hoffmann",
      "Chris Rorden",
      "Sergey Plis"
    ],
    "abstract": "We developed MindGrab, a parameter- and memory-efficient deep fully-convolutional model for volumetric skull-stripping in head images of any modality. Its architecture, informed by a spectral interpretation of dilated convolutions, was trained exclusively on modality-agnostic synthetic data. MindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain scans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip dataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using Dice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a mean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities, significantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P < 0.05; BET: 85.2 SD 14.4, P < 0.05). Compared to SynthStrip (96.5 SD 1.1, P=0.0352), MindGrab delivered equivalent or superior performance in nearly half of the tested scenarios, with minor differences (<3% Dice) in the others. MindGrab utilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This efficiency yielded at least 2x faster inference, 50% lower memory usage on GPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x memory reduction) and accessibility on a wider range of hardware, including systems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with dramatically lower resource demands, supported in brainchop-cli (https://pypi.org/project/brainchop/) and at brainchop.org.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 1 table, 4 figures. 2 supplementary tables, 1 supplementary figure. Brainchop-cli: https://pypi.org/project/brainchop/ . Brainchop web: https://brainchop.org/",
    "pdf_url": "https://arxiv.org/pdf/2506.11860v1",
    "published_date": "2025-06-13 15:09:15 UTC",
    "updated_date": "2025-06-13 15:09:15 UTC"
  },
  {
    "arxiv_id": "2506.11849v2",
    "title": "Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values",
    "authors": [
      "R. Teal Witter",
      "Yurong Liu",
      "Christopher Musco"
    ],
    "abstract": "With origins in game theory, probabilistic values like Shapley values, Banzhaf values, and semi-values have emerged as a central tool in explainable AI. They are used for feature attribution, data attribution, data valuation, and more. Since all of these values require exponential time to compute exactly, research has focused on efficient approximation methods using two techniques: Monte Carlo sampling and linear regression formulations. In this work, we present a new way of combining both of these techniques. Our approach is more flexible than prior algorithms, allowing for linear regression to be replaced with any function family whose probabilistic values can be computed efficiently. This allows us to harness the accuracy of tree-based models like XGBoost, while still producing unbiased estimates. From experiments across eight datasets, we find that our methods give state-of-the-art performance for estimating probabilistic values. For Shapley values, the error of our methods can be $6.5\\times$ lower than Permutation SHAP (the most popular Monte Carlo method), $3.8\\times$ lower than Kernel SHAP (the most popular linear regression method), and $2.6\\times$ lower than Leverage SHAP (the prior state-of-the-art Shapley value estimator). For more general probabilistic values, we can obtain error $215\\times$ lower than the best estimator from prior work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11849v2",
    "published_date": "2025-06-13 14:57:38 UTC",
    "updated_date": "2026-01-13 02:37:05 UTC"
  },
  {
    "arxiv_id": "2506.11844v1",
    "title": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks",
    "authors": [
      "Qihai Zhang",
      "Xinyue Sheng",
      "Yuanfu Sun",
      "Qiaoyu Tan"
    ],
    "abstract": "Inspired by the success of large language models (LLMs), there is a significant research shift from traditional graph learning methods to LLM-based graph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning power of LLMs by integrating three key components: the textual attributes of input nodes, the structural information of node neighborhoods, and task-specific prompts that guide decision-making. Despite their promise, the robustness of GraphLLMs against adversarial perturbations remains largely unexplored-a critical concern for deploying these models in high-stakes scenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study evaluating the vulnerability of GraphLLMs to adversarial attacks across three dimensions: text, graph structure, and prompt manipulations. We implement state-of-the-art attack algorithms from each perspective to rigorously assess model resilience. Through extensive experiments on six benchmark datasets from diverse domains, our findings reveal that GraphLLMs are highly susceptible to text attacks that merely replace a few semantically similar words in a node's textual attribute. We also find that standard graph structure attack methods can significantly degrade model performance, while random shuffling of the candidate label set in prompt templates leads to substantial performance drops. Beyond characterizing these vulnerabilities, we investigate defense techniques tailored to each attack vector through data-augmented training and adversarial training, which show promising potential to enhance the robustness of GraphLLMs. We hope that our open-sourced library will facilitate rapid, equitable evaluation and inspire further innovative research in this field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures, in KDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11844v1",
    "published_date": "2025-06-13 14:48:01 UTC",
    "updated_date": "2025-06-13 14:48:01 UTC"
  },
  {
    "arxiv_id": "2506.11825v1",
    "title": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate",
    "authors": [
      "Aishwarya Bandaru",
      "Fabian Bindley",
      "Trevor Bluth",
      "Nandini Chavda",
      "Baixu Chen",
      "Ethan Law"
    ],
    "abstract": "Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11825v1",
    "published_date": "2025-06-13 14:30:37 UTC",
    "updated_date": "2025-06-13 14:30:37 UTC"
  },
  {
    "arxiv_id": "2506.11815v2",
    "title": "Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection",
    "authors": [
      "Tae-Seong Han",
      "Jae-Wook Heo",
      "Hakseung Kim",
      "Cheol-Hui Lee",
      "Hyub Huh",
      "Eue-Keun Choi",
      "Hye Jin Kim",
      "Dong-Joo Kim"
    ],
    "abstract": "Electrocardiography (ECG) signals are frequently degraded by noise, limiting their clinical reliability in both conventional and wearable settings. Existing methods for addressing ECG noise, relying on artifact classification or denoising, are constrained by annotation inconsistencies and poor generalizability. Here, we address these limitations by reframing ECG noise quantification as an anomaly detection task. We propose a diffusion-based framework trained to model the normative distribution of clean ECG signals, identifying deviations as noise without requiring explicit artifact labels. To robustly evaluate performance and mitigate label inconsistencies, we introduce a distribution-based metric using the Wasserstein-1 distance ($W_1$). Our model achieved a macro-average $W_1$ score of 1.308, outperforming the next-best method by over 48\\%. External validation confirmed strong generalizability, facilitating the exclusion of noisy segments to improve diagnostic accuracy and support timely clinical intervention. This approach enhances real-time ECG monitoring and broadens ECG applicability in digital health technologies.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "eess.SP",
    "comment": "This manuscript contains 17 pages, 10 figures, and 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.11815v2",
    "published_date": "2025-06-13 14:19:04 UTC",
    "updated_date": "2025-07-22 06:48:23 UTC"
  },
  {
    "arxiv_id": "2506.11812v1",
    "title": "On the Performance of LLMs for Real Estate Appraisal",
    "authors": [
      "Margot Geerts",
      "Manon Reusens",
      "Bart Baesens",
      "Seppe vanden Broucke",
      "Jochen De Weerdt"
    ],
    "abstract": "The real estate market is vital to global economies but suffers from significant information asymmetry. This study examines how Large Language Models (LLMs) can democratize access to real estate insights by generating competitive and interpretable house price estimates through optimized In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs on diverse international housing datasets, comparing zero-shot, few-shot, market report-enhanced, and hybrid prompting techniques. Our results show that LLMs effectively leverage hedonic variables, such as property size and amenities, to produce meaningful estimates. While traditional machine learning models remain strong for pure predictive accuracy, LLMs offer a more accessible, interactive and interpretable alternative. Although self-explanations require cautious interpretation, we find that LLMs explain their predictions in agreement with state-of-the-art models, confirming their trustworthiness. Carefully selected in-context examples based on feature similarity and geographic proximity, significantly enhance LLM performance, yet LLMs struggle with overconfidence in price intervals and limited spatial reasoning. We offer practical guidance for structured prediction tasks through prompt optimization. Our findings highlight LLMs' potential to improve transparency in real estate appraisal and provide actionable insights for stakeholders.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ECML-PKDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11812v1",
    "published_date": "2025-06-13 14:14:40 UTC",
    "updated_date": "2025-06-13 14:14:40 UTC"
  },
  {
    "arxiv_id": "2506.13803v1",
    "title": "Causality in the human niche: lessons for machine learning",
    "authors": [
      "Richard D. Lange",
      "Konrad P. Kording"
    ],
    "abstract": "Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13803v1",
    "published_date": "2025-06-13 14:13:52 UTC",
    "updated_date": "2025-06-13 14:13:52 UTC"
  },
  {
    "arxiv_id": "2506.11811v2",
    "title": "Abstract Sound Fusion with Unconditional Inversion Models",
    "authors": [
      "Jing Liu",
      "Enqi Lian",
      "Moyao Deng"
    ],
    "abstract": "An abstract sound is defined as a sound that does not disclose identifiable real-world sound events to a listener. Sound fusion aims to synthesize an original sound and a reference sound to generate a novel sound that exhibits auditory features beyond mere additive superposition of the sound constituents. To achieve this fusion, we employ inversion techniques that preserve essential features of the original sample while enabling controllable synthesis. We propose novel SDE and ODE inversion models based on DPMSolver++ samplers that reverse the sampling process by configuring model outputs as constants, eliminating circular dependencies incurred by noise prediction terms. Our inversion approach requires no prompt conditioning while maintaining flexible guidance during sampling.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11811v2",
    "published_date": "2025-06-13 14:13:45 UTC",
    "updated_date": "2025-08-04 14:12:17 UTC"
  },
  {
    "arxiv_id": "2506.12116v3",
    "title": "Unsupervised Document and Template Clustering using Multimodal Embeddings",
    "authors": [
      "Phillipe R. Sampaio",
      "Helene Maxcici"
    ],
    "abstract": "We study unsupervised clustering of documents at both the category and template levels using frozen multimodal encoders and classical clustering algorithms. We systematize a model-agnostic pipeline that (i) projects heterogeneous last-layer states from text-layout-vision encoders into token-type-aware document vectors and (ii) performs clustering with centroid- or density-based methods, including an HDBSCAN + $k$-NN assignment to eliminate unlabeled points. We evaluate eight encoders (text-only, layout-aware, vision-only, and vision-language) with $k$-Means, DBSCAN, HDBSCAN + $k$-NN, and BIRCH on five corpora spanning clean synthetic invoices, their heavily degraded print-and-scan counterparts, scanned receipts, and real identity and certificate documents. The study reveals modality-specific failure modes and a robustness-accuracy trade-off, with vision features nearly solving template discovery on clean pages while text dominates under covariate shift, and fused encoders offering the best balance. We detail a reproducible, oracle-free tuning protocol and the curated evaluation settings to guide future work on unsupervised document organization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.12116v3",
    "published_date": "2025-06-13 14:07:44 UTC",
    "updated_date": "2025-10-26 20:20:07 UTC"
  },
  {
    "arxiv_id": "2506.11798v2",
    "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
    "authors": [
      "Maximilian Kreutner",
      "Marlene Lutz",
      "Markus Strohmaier"
    ],
    "abstract": "Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11798v2",
    "published_date": "2025-06-13 14:02:21 UTC",
    "updated_date": "2025-09-08 09:13:03 UTC"
  },
  {
    "arxiv_id": "2506.12115v2",
    "title": "Eliciting Reasoning in Language Models with Cognitive Tools",
    "authors": [
      "Brown Ebouky",
      "Andrea Bartezzaghi",
      "Mattia Rigotti"
    ],
    "abstract": "The recent advent of reasoning models like OpenAI's o1 was met with excited speculation by the AI community about the mechanisms underlying these capabilities in closed models, followed by a rush of replication efforts, particularly from the open source community. These speculations were largely settled by the demonstration from DeepSeek-R1 that chains-of-thought and reinforcement learning (RL) can effectively replicate reasoning on top of base LLMs. However, it remains valuable to explore alternative methods for theoretically eliciting reasoning that could help elucidate the underlying mechanisms, as well as providing additional methods that may offer complementary benefits.\n  Here, we build on the long-standing literature in cognitive psychology and cognitive architectures, which postulates that reasoning arises from the orchestrated, sequential execution of a set of modular, predetermined cognitive operations. Crucially, we implement this key idea within a modern agentic tool-calling framework. In particular, we endow an LLM with a small set of \"cognitive tools\" encapsulating specific reasoning operations, each executed by the LLM itself. Surprisingly, this simple strategy results in considerable gains in performance on standard mathematical reasoning benchmarks compared to base LLMs, for both closed and open-weight models. For instance, providing our \"cognitive tools\" to GPT-4.1 increases its pass@1 performance on AIME2024 from 32% to 53%, even surpassing the performance of o1-preview.\n  In addition to its practical implications, this demonstration contributes to the debate regarding the role of post-training methods in eliciting reasoning in LLMs versus the role of inherent capabilities acquired during pre-training, and whether post-training merely uncovers these latent abilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.12115v2",
    "published_date": "2025-06-13 13:56:52 UTC",
    "updated_date": "2025-11-20 14:43:21 UTC"
  },
  {
    "arxiv_id": "2506.11790v2",
    "title": "Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature Attributions? A Synthetic Data Investigation",
    "authors": [
      "Gregor Baer",
      "Isel Grau",
      "Chao Zhang",
      "Pieter Van Gorp"
    ],
    "abstract": "Evaluating feature attribution methods represents a critical challenge in explainable AI (XAI), as researchers typically rely on perturbation-based metrics when ground truth is unavailable. However, recent work reveals that these evaluation metrics can show different performance across predicted classes within the same dataset. These \"class-dependent evaluation effects\" raise questions about whether perturbation analysis reliably measures attribution quality, with direct implications for XAI method development and evaluation trustworthiness. We investigate under which conditions these class-dependent effects arise by conducting controlled experiments with synthetic time series data where ground truth feature locations are known. We systematically vary feature types and class contrasts across binary classification tasks, then compare perturbation-based degradation scores with ground truth-based precision-recall metrics using multiple attribution methods. Our experiments demonstrate that class-dependent effects emerge with both evaluation approaches, even in simple scenarios with temporally localized features, triggered by basic variations in feature amplitude or temporal extent between classes. Most critically, we find that perturbation-based and ground truth metrics frequently yield contradictory assessments of attribution quality across classes, with weak correlations between evaluation approaches. These findings suggest that researchers should interpret perturbation-based metrics with care, as they may not always align with whether attributions correctly identify discriminating features. By showing this disconnect, our work points toward reconsidering what attribution evaluation actually measures and developing more rigorous evaluation methods that capture multiple dimensions of attribution quality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for Time Series and Data Streams)",
    "pdf_url": "https://arxiv.org/pdf/2506.11790v2",
    "published_date": "2025-06-13 13:52:32 UTC",
    "updated_date": "2025-07-24 09:17:21 UTC"
  },
  {
    "arxiv_id": "2506.14827v1",
    "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning",
    "authors": [
      "Yifeng Gao",
      "Yifan Ding",
      "Hongyu Su",
      "Juncheng Li",
      "Yunhan Zhao",
      "Lin Luo",
      "Zixing Chen",
      "Li Wang",
      "Xin Wang",
      "Yixu Wang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "As AI-generated video becomes increasingly pervasive across media platforms, the ability to reliably distinguish synthetic content from authentic footage has become both urgent and essential. Existing approaches have primarily treated this challenge as a binary classification task, offering limited insight into where or why a model identifies a video as AI-generated. However, the core challenge extends beyond simply detecting subtle artifacts; it requires providing fine-grained, persuasive evidence that can convince auditors and end-users alike. To address this critical gap, we introduce DAVID-X, the first dataset to pair AI-generated videos with detailed defect-level, temporal-spatial annotations and written rationales. Leveraging these rich annotations, we present DAVID-XR1, a video-language model designed to deliver an interpretable chain of visual reasoning-including defect categorization, temporal-spatial localization, and natural language explanations. This approach fundamentally transforms AI-generated video detection from an opaque black-box decision into a transparent and verifiable diagnostic process. We demonstrate that a general-purpose backbone, fine-tuned on our compact dataset and enhanced with chain-of-thought distillation, achieves strong generalization across a variety of generators and generation modes. Our results highlight the promise of explainable detection methods for trustworthy identification of AI-generated video content.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14827v1",
    "published_date": "2025-06-13 13:39:53 UTC",
    "updated_date": "2025-06-13 13:39:53 UTC"
  },
  {
    "arxiv_id": "2506.12113v4",
    "title": "Semantic Preprocessing for LLM-based Malware Analysis",
    "authors": [
      "Benjamin Marais",
      "Tony Quertier",
      "Grégoire Barrue"
    ],
    "abstract": "In a context of malware analysis, numerous approaches rely on Artificial Intelligence to handle a large volume of data. However, these techniques focus on data view (images, sequences) and not on an expert's view. Noticing this issue, we propose a preprocessing that focuses on expert knowledge to improve malware semantic analysis and result interpretability. We propose a new preprocessing method which creates JSON reports for Portable Executable files. These reports gather features from both static and behavioral analysis, and incorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a semantic representation of binary files, understandable by malware analysts, and that can enhance AI models' explainability for malicious files analysis. Using this preprocessing to train a Large Language Model for Malware classification, we achieve a weighted-average F1-score of 0.94 on a complex dataset, representative of market reality.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12113v4",
    "published_date": "2025-06-13 13:39:00 UTC",
    "updated_date": "2025-10-03 08:07:28 UTC"
  },
  {
    "arxiv_id": "2506.11777v2",
    "title": "Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation",
    "authors": [
      "Divyanshu Mishra",
      "Mohammadreza Salehi",
      "Pramit Saha",
      "Olga Patey",
      "Aris T. Papageorghiou",
      "Yuki M. Asano",
      "J. Alison Noble"
    ],
    "abstract": "Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding.Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups,achieving superior segmentation transfer and strong downstream performance on clinically relevant tasks such as LVEF prediction. Code available at: https://github.com/mdivyanshu97/DISCOVR",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11777v2",
    "published_date": "2025-06-13 13:36:33 UTC",
    "updated_date": "2025-11-14 08:58:47 UTC"
  },
  {
    "arxiv_id": "2506.11774v1",
    "title": "Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation",
    "authors": [
      "Abhishek Jaiswal",
      "Armeet Singh Luthra",
      "Purav Jangir",
      "Bhavya Garg",
      "Nisheeth Srivastava"
    ],
    "abstract": "Isometric exercises appeal to individuals seeking convenience, privacy, and minimal dependence on equipments. However, such fitness training is often overdependent on unreliable digital media content instead of expert supervision, introducing serious risks, including incorrect posture, injury, and disengagement due to lack of corrective feedback. To address these challenges, we present a real-time feedback system for assessing isometric poses. Our contributions include the release of the largest multiclass isometric exercise video dataset to date, comprising over 3,600 clips across six poses with correct and incorrect variations. To support robust evaluation, we benchmark state-of-the-art models-including graph-based networks-on this dataset and introduce a novel three-part metric that captures classification accuracy, mistake localization, and model confidence. Our results enhance the feasibility of intelligent and personalized exercise training systems for home workouts. This expert-level diagnosis, delivered directly to the users, also expands the potential applications of these systems to rehabilitation, physiotherapy, and various other fitness disciplines that involve physical motion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11774v1",
    "published_date": "2025-06-13 13:33:59 UTC",
    "updated_date": "2025-06-13 13:33:59 UTC"
  },
  {
    "arxiv_id": "2506.11760v1",
    "title": "FeNN: A RISC-V vector processor for Spiking Neural Network acceleration",
    "authors": [
      "Zainab Aizaz",
      "James C. Knight",
      "Thomas Nowotny"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have the potential to drastically reduce the energy requirements of AI systems. However, mainstream accelerators like GPUs and TPUs are designed for the high arithmetic intensity of standard ANNs so are not well-suited to SNN simulation. FPGAs are well-suited to applications with low arithmetic intensity as they have high off-chip memory bandwidth and large amounts of on-chip memory. Here, we present a novel RISC-V-based soft vector processor (FeNN), tailored to simulating SNNs on FPGAs. Unlike most dedicated neuromorphic hardware, FeNN is fully programmable and designed to be integrated with applications running on standard computers from the edge to the cloud. We demonstrate that, by using stochastic rounding and saturation, FeNN can achieve high numerical precision with low hardware utilisation and that a single FeNN core can simulate an SNN classifier faster than both an embedded GPU and the Loihi neuromorphic system.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.NE",
    "comment": "7 pages, 4 figures. Accepted in Proceedings of Neuro Inspired Computational Elements Conference 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11760v1",
    "published_date": "2025-06-13 13:13:54 UTC",
    "updated_date": "2025-06-13 13:13:54 UTC"
  },
  {
    "arxiv_id": "2506.11756v1",
    "title": "Causal Effect Identification in Heterogeneous Environments from Higher-Order Moments",
    "authors": [
      "Yaroslav Kivva",
      "Sina Akbari",
      "Saber Salehkaleybar",
      "Negar Kiyavash"
    ],
    "abstract": "We investigate the estimation of the causal effect of a treatment variable on an outcome in the presence of a latent confounder. We first show that the causal effect is identifiable under certain conditions when data is available from multiple environments, provided that the target causal effect remains invariant across these environments. Secondly, we propose a moment-based algorithm for estimating the causal effect as long as only a single parameter of the data-generating mechanism varies across environments -- whether it be the exogenous noise distribution or the causal relationship between two variables. Conversely, we prove that identifiability is lost if both exogenous noise distributions of both the latent and treatment variables vary across environments. Finally, we propose a procedure to identify which parameter of the data-generating mechanism has varied across the environments and evaluate the performance of our proposed methods through experiments on synthetic data.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11756v1",
    "published_date": "2025-06-13 13:11:37 UTC",
    "updated_date": "2025-06-13 13:11:37 UTC"
  },
  {
    "arxiv_id": "2506.11721v1",
    "title": "Relational GNNs Cannot Learn $C_2$ Features for Planning",
    "authors": [
      "Dillon Z. Chen"
    ],
    "abstract": "Relational Graph Neural Networks (R-GNNs) are a GNN-based approach for learning value functions that can generalise to unseen problems from a given planning domain. R-GNNs were theoretically motivated by the well known connection between the expressive power of GNNs and $C_2$, first-order logic with two variables and counting. In the context of planning, $C_2$ features refer to the set of formulae in $C_2$ with relations defined by the unary and binary predicates of a planning domain. Some planning domains exhibit optimal value functions that can be decomposed as arithmetic expressions of $C_2$ features. We show that, contrary to empirical results, R-GNNs cannot learn value functions defined by $C_2$ features. We also identify prior GNN architectures for planning that may better learn value functions defined by $C_2$ features.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11721v1",
    "published_date": "2025-06-13 12:35:56 UTC",
    "updated_date": "2025-06-13 12:35:56 UTC"
  },
  {
    "arxiv_id": "2506.11718v2",
    "title": "Interaction, Process, Infrastructure: A Unified Framework for Human-Agent Collaboration",
    "authors": [
      "Yun Wang",
      "Yan Lu"
    ],
    "abstract": "While AI tools are increasingly prevalent in knowledge work, they remain fragmented, lacking the architectural foundation for sustained, adaptive collaboration. We argue this limitation stems from their inability to represent and manage the structure of collaborative work. To bridge this gap, we propose a layered conceptual framework for human-agent systems that integrates Interaction, Process, and Infrastructure. Crucially, our framework elevates Process to a first-class concern, an explicit, inspectable structural representation of activities. The central theoretical construct is Structural Adaptation, enabling the process to dynamically reorganize itself in response to evolving goals. We introduce a five-module Process Model as the representational basis for this adaptation. This model offers a unified theoretical grounding, reimagining human-agent collaboration as a coherent system for complex real-world work.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11718v2",
    "published_date": "2025-06-13 12:34:15 UTC",
    "updated_date": "2025-12-24 03:08:54 UTC"
  },
  {
    "arxiv_id": "2506.11712v3",
    "title": "Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization",
    "authors": [
      "Wenqi Liu",
      "Xuemeng Song",
      "Jiaxi Li",
      "Yinwei Wei",
      "Na Zheng",
      "Jianhua Yin",
      "Liqiang Nie"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11712v3",
    "published_date": "2025-06-13 12:29:15 UTC",
    "updated_date": "2025-12-22 07:47:22 UTC"
  },
  {
    "arxiv_id": "2506.11702v1",
    "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
    "authors": [
      "Víctor Gallego"
    ],
    "abstract": "Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI Alignment",
    "pdf_url": "https://arxiv.org/pdf/2506.11702v1",
    "published_date": "2025-06-13 12:17:38 UTC",
    "updated_date": "2025-06-13 12:17:38 UTC"
  },
  {
    "arxiv_id": "2506.15821v1",
    "title": "VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal",
    "authors": [
      "Pham Khai Nguyen Do",
      "Bao Nguyen Tran",
      "Nam Nguyen",
      "Duc Dung Nguyen"
    ],
    "abstract": "Recent advances in Novel View Synthesis (NVS) and 3D generation have significantly improved editing tasks, with a primary emphasis on maintaining cross-view consistency throughout the generative process. Contemporary methods typically address this challenge using a dual-strategy framework: performing consistent 2D inpainting across all views guided by embedded priors either explicitly in pixel space or implicitly in latent space; and conducting 3D reconstruction with additional consistency guidance. Previous strategies, in particular, often require an initial 3D reconstruction phase to establish geometric structure, introducing considerable computational overhead. Even with the added cost, the resulting reconstruction quality often remains suboptimal. In this paper, we present VEIGAR, a computationally efficient framework that outperforms existing methods without relying on an initial reconstruction phase. VEIGAR leverages a lightweight foundation model to reliably align priors explicitly in the pixel space. In addition, we introduce a novel supervision strategy based on scale-invariant depth loss, which removes the need for traditional scale-and-shift operations in monocular depth regularization. Through extensive experimentation, VEIGAR establishes a new state-of-the-art benchmark in reconstruction quality and cross-view consistency, while achieving a threefold reduction in training time compared to the fastest existing method, highlighting its superior balance of efficiency and effectiveness.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15821v1",
    "published_date": "2025-06-13 11:31:44 UTC",
    "updated_date": "2025-06-13 11:31:44 UTC"
  },
  {
    "arxiv_id": "2506.11687v1",
    "title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs",
    "authors": [
      "Francisco Aguilera-Martínez",
      "Fernando Berzal"
    ],
    "abstract": "Machine learning models should not reveal particular information that is not otherwise accessible. Differential privacy provides a formal framework to mitigate privacy risks by ensuring that the inclusion or exclusion of any single data point does not significantly alter the output of an algorithm, thus limiting the exposure of private information. This survey paper explores the foundational definitions of differential privacy, reviews its original formulations and tracing its evolution through key research contributions. It then provides an in-depth examination of how DP has been integrated into machine learning models, analyzing existing proposals and methods to preserve privacy when training ML models. Finally, it describes how DP-based ML techniques can be evaluated in practice. %Finally, it discusses the broader implications of DP, highlighting its potential for public benefit, its real-world applications, and the challenges it faces, including vulnerabilities to adversarial attacks. By offering a comprehensive overview of differential privacy in machine learning, this work aims to contribute to the ongoing development of secure and responsible AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CR",
    "comment": "arXiv admin note: text overlap with arXiv:2303.00654 by other authors",
    "pdf_url": "https://arxiv.org/pdf/2506.11687v1",
    "published_date": "2025-06-13 11:30:35 UTC",
    "updated_date": "2025-06-13 11:30:35 UTC"
  },
  {
    "arxiv_id": "2506.11684v1",
    "title": "MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space",
    "authors": [
      "Anshul Singh",
      "Chris Biemann",
      "Jan Strich"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text/structured). This leaves a critical gap: they don't assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval) are available online (https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11684v1",
    "published_date": "2025-06-13 11:21:00 UTC",
    "updated_date": "2025-06-13 11:21:00 UTC"
  },
  {
    "arxiv_id": "2506.11679v3",
    "title": "LLMs on support of privacy and security of mobile apps: state of the art and research directions",
    "authors": [
      "Tran Thanh Lam Nguyen",
      "Barbara Carminati",
      "Elena Ferrari"
    ],
    "abstract": "Modern life has witnessed the explosion of mobile devices. However, besides the valuable features that bring convenience to end users, security and privacy risks still threaten users of mobile apps. The increasing sophistication of these threats in recent years has underscored the need for more advanced and efficient detection approaches. In this chapter, we explore the application of Large Language Models (LLMs) to identify security risks and privacy violations and mitigate them for the mobile application ecosystem. By introducing state-of-the-art research that applied LLMs to mitigate the top 10 common security risks of smartphone platforms, we highlight the feasibility and potential of LLMs to replace traditional analysis methods, such as dynamic and hybrid analysis of mobile apps. As a representative example of LLM-based solutions, we present an approach to detect sensitive data leakage when users share images online, a common behavior of smartphone users nowadays. Finally, we discuss open research challenges.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "I am writing to respectfully request the withdrawal of my recent submission to arXiv due to an authorship issue. The paper was submitted without the explicit consent of two co-authors. After internal discussion, they have expressed clear disagreement with the submission and raised concerns about unresolved academic inaccuracies in the current version",
    "pdf_url": "https://arxiv.org/pdf/2506.11679v3",
    "published_date": "2025-06-13 11:17:15 UTC",
    "updated_date": "2025-11-28 21:48:13 UTC"
  },
  {
    "arxiv_id": "2506.11678v1",
    "title": "Pose Matters: Evaluating Vision Transformers and CNNs for Human Action Recognition on Small COCO Subsets",
    "authors": [
      "MingZe Tang",
      "Madiha Kazi"
    ],
    "abstract": "This study explores human action recognition using a three-class subset of the COCO image corpus, benchmarking models from simple fully connected networks to transformer architectures. The binary Vision Transformer (ViT) achieved 90% mean test accuracy, significantly exceeding multiclass classifiers such as convolutional networks (approximately 35%) and CLIP-based models (approximately 62-64%). A one-way ANOVA (F = 61.37, p < 0.001) confirmed these differences are statistically significant. Qualitative analysis with SHAP explainer and LeGrad heatmaps indicated that the ViT localizes pose-specific regions (e.g., lower limbs for walking or running), while simpler feed-forward models often focus on background textures, explaining their errors. These findings emphasize the data efficiency of transformer representations and the importance of explainability techniques in diagnosing class-specific failures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11678v1",
    "published_date": "2025-06-13 11:16:50 UTC",
    "updated_date": "2025-06-13 11:16:50 UTC"
  },
  {
    "arxiv_id": "2506.14826v1",
    "title": "Collaborative Interest-aware Graph Learning for Group Identification",
    "authors": [
      "Rui Zhao",
      "Beihong Jin",
      "Beibei Li",
      "Yiyuan Zheng"
    ],
    "abstract": "With the popularity of social media, an increasing number of users are joining group activities on online social platforms. This elicits the requirement of group identification (GI), which is to recommend groups to users. We reveal that users are influenced by both group-level and item-level interests, and these dual-level interests have a collaborative evolution relationship: joining a group expands the user's item interests, further prompting the user to join new groups. Ultimately, the two interests tend to align dynamically. However, existing GI methods fail to fully model this collaborative evolution relationship, ignoring the enhancement of group-level interests on item-level interests, and suffering from false-negative samples when aligning cross-level interests. In order to fully model the collaborative evolution relationship between dual-level user interests, we propose CI4GI, a Collaborative Interest-aware model for Group Identification. Specifically, we design an interest enhancement strategy that identifies additional interests of users from the items interacted with by the groups they have joined as a supplement to item-level interests. In addition, we adopt the distance between interest distributions of two users to optimize the identification of negative samples for a user, mitigating the interference of false-negative samples during cross-level interests alignment. The results of experiments on three real-world datasets demonstrate that CI4GI significantly outperforms state-of-the-art models.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "accepted by ECML PKDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.14826v1",
    "published_date": "2025-06-13 11:15:43 UTC",
    "updated_date": "2025-06-13 11:15:43 UTC"
  },
  {
    "arxiv_id": "2506.11673v1",
    "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE",
    "authors": [
      "Alicja Dobrzeniecka",
      "Antske Fokkens",
      "Pia Sommerauer"
    ],
    "abstract": "Amnesic probing is a technique used to examine the influence of specific linguistic information on the behaviour of a model. This involves identifying and removing the relevant information and then assessing whether the model's performance on the main task changes. If the removed information is relevant, the model's performance should decline. The difficulty with this approach lies in removing only the target information while leaving other information unchanged. It has been shown that Iterative Nullspace Projection (INLP), a widely used removal technique, introduces random modifications to representations when eliminating target information. We demonstrate that Mean Projection (MP) and LEACE, two proposed alternatives, remove information in a more targeted manner, thereby enhancing the potential for obtaining behavioural explanations through Amnesic Probing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11673v1",
    "published_date": "2025-06-13 11:07:14 UTC",
    "updated_date": "2025-06-13 11:07:14 UTC"
  },
  {
    "arxiv_id": "2506.12111v1",
    "title": "Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data",
    "authors": [
      "Oscar Boullosa Dapena"
    ],
    "abstract": "Real-time continuous learning over streaming data remains a central challenge in deep learning and AI systems. Traditional gradient-based models such as backpropagation through time (BPTT) face computational and stability limitations when dealing with temporally unbounded data. In this paper, we introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs), which leverages the Feynman technique of differentiation under the integral sign to formulate neural updates as integrals over historical data. This reformulation allows for smoother, more stable learning dynamics that are both physically interpretable and computationally tractable. Inspired by Feynman's path integral formalism and compatible with quantum gradient estimation frameworks, QIDINNs open a path toward hybrid classical-quantum neural computation. We demonstrate our model's effectiveness on synthetic and real-world streaming tasks, and we propose directions for quantum extensions and scalable implementations.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12111v1",
    "published_date": "2025-06-13 11:00:31 UTC",
    "updated_date": "2025-06-13 11:00:31 UTC"
  },
  {
    "arxiv_id": "2506.11666v1",
    "title": "Converting Annotated Clinical Cases into Structured Case Report Forms",
    "authors": [
      "Pietro Ferrazzi",
      "Alberto Lavelli",
      "Bernardo Magnini"
    ],
    "abstract": "Case Report Forms (CRFs) are largely used in medical research as they ensure accuracy, reliability, and validity of results in clinical studies. However, publicly available, wellannotated CRF datasets are scarce, limiting the development of CRF slot filling systems able to fill in a CRF from clinical notes. To mitigate the scarcity of CRF datasets, we propose to take advantage of available datasets annotated for information extraction tasks and to convert them into structured CRFs. We present a semi-automatic conversion methodology, which has been applied to the E3C dataset in two languages (English and Italian), resulting in a new, high-quality dataset for CRF slot filling. Through several experiments on the created dataset, we report that slot filling achieves 59.7% for Italian and 67.3% for English on a closed Large Language Models (zero-shot) and worse performances on three families of open-source models, showing that filling CRFs is challenging even for recent state-of-the-art LLMs. We release the datest at https://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "to be published in BioNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11666v1",
    "published_date": "2025-06-13 10:53:50 UTC",
    "updated_date": "2025-06-13 10:53:50 UTC"
  },
  {
    "arxiv_id": "2506.11653v2",
    "title": "DISCO: Mitigating Bias in Deep Learning with Conditional Distance Correlation",
    "authors": [
      "Emre Kavak",
      "Tom Nuno Wolf",
      "Christian Wachinger"
    ],
    "abstract": "Dataset bias often leads deep learning models to exploit spurious correlations instead of task-relevant signals. We introduce the Standard Anti-Causal Model (SAM), a unifying causal framework that characterizes bias mechanisms and yields a conditional independence criterion for causal stability. Building on this theory, we propose DISCO$_m$ and sDISCO, efficient and scalable estimators of conditional distance correlation that enable independence regularization in black-box models. Across five diverse datasets, our methods consistently outperform or are competitive in existing bias mitigation approaches, while requiring fewer hyperparameters and scaling seamlessly to multi-bias scenarios. This work bridges causal theory and practical deep learning, providing both a principled foundation and effective tools for robust prediction. Source Code: https://github.com/***.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11653v2",
    "published_date": "2025-06-13 10:29:03 UTC",
    "updated_date": "2025-09-22 12:41:16 UTC"
  },
  {
    "arxiv_id": "2506.11650v1",
    "title": "Robot Context Protocol (RCP): A Runtime-Agnostic Interface for Agent-Aware Robot Control",
    "authors": [
      "Lambert Lee",
      "Joshua Lau"
    ],
    "abstract": "The Robot Context Protocol (RCP) is a lightweight, middleware-agnostic communication protocol designed to simplify the complexity of robotic systems and enable seamless interaction between robots, users, and autonomous agents. RCP provides a unified and semantically meaningful interface that decouples client-facing operations from backend implementations, supporting a wide range of deployment environments including physical robots, cloud-based orchestrators, and simulated platforms. Built on HTTP and WebSocket transport layers, the protocol defines a schema-driven message format with structured operations such as read, write, execute, and subscribe. It integrates features such as runtime introspection, asynchronous feedback, multi-tenant namespace isolation, and strict type validation to ensure robustness, scalability, and security. The architecture, message structure, interface model, and adapter-based backend integration strategy of RCP are described, along with deployment practices and applicability across industries including manufacturing, logistics, and healthcare. RCP enables intelligent, resilient, and safe robotic operations in complex, multi-agent ecosystems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11650v1",
    "published_date": "2025-06-13 10:24:44 UTC",
    "updated_date": "2025-06-13 10:24:44 UTC"
  },
  {
    "arxiv_id": "2506.11638v1",
    "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation",
    "authors": [
      "Yicheng Xiao",
      "Lin Song",
      "Rui Yang",
      "Cheng Cheng",
      "Yixiao Ge",
      "Xiu Li",
      "Ying Shan"
    ],
    "abstract": "Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11638v1",
    "published_date": "2025-06-13 10:11:01 UTC",
    "updated_date": "2025-06-13 10:11:01 UTC"
  },
  {
    "arxiv_id": "2506.11635v1",
    "title": "FAA Framework: A Large Language Model-Based Approach for Credit Card Fraud Investigations",
    "authors": [
      "Shaun Shuster",
      "Eyal Zaloof",
      "Asaf Shabtai",
      "Rami Puzis"
    ],
    "abstract": "The continuous growth of the e-commerce industry attracts fraudsters who exploit stolen credit card details. Companies often investigate suspicious transactions in order to retain customer trust and address gaps in their fraud detection systems. However, analysts are overwhelmed with an enormous number of alerts from credit card transaction monitoring systems. Each alert investigation requires from the fraud analysts careful attention, specialized knowledge, and precise documentation of the outcomes, leading to alert fatigue. To address this, we propose a fraud analyst assistant (FAA) framework, which employs multi-modal large language models (LLMs) to automate credit card fraud investigations and generate explanatory reports. The FAA framework leverages the reasoning, code execution, and vision capabilities of LLMs to conduct planning, evidence collection, and analysis in each investigation step. A comprehensive empirical evaluation of 500 credit card fraud investigations demonstrates that the FAA framework produces reliable and efficient investigations comprising seven steps on average. Thus we found that the FAA framework can automate large parts of the workload and help reduce the challenges faced by fraud analysts.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11635v1",
    "published_date": "2025-06-13 10:05:43 UTC",
    "updated_date": "2025-06-13 10:05:43 UTC"
  },
  {
    "arxiv_id": "2506.11627v1",
    "title": "Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique using Tensor Data and Bayesian Regression",
    "authors": [
      "Kuniko Paxton",
      "Koorosh Aslansefat",
      "Dhavalkumar Thakker",
      "Yiannis Papadopoulos"
    ],
    "abstract": "Fairness is a critical component of Trustworthy AI. In this paper, we focus on Machine Learning (ML) and the performance of model predictions when dealing with skin color. Unlike other sensitive attributes, the nature of skin color differs significantly. In computer vision, skin color is represented as tensor data rather than categorical values or single numerical points. However, much of the research on fairness across sensitive groups has focused on categorical features such as gender and race. This paper introduces a new technique for evaluating fairness in ML for image classification tasks, specifically without the use of annotation. To address the limitations of prior work, we handle tensor data, like skin color, without classifying it rigidly. Instead, we convert it into probability distributions and apply statistical distance measures. This novel approach allows us to capture fine-grained nuances in fairness both within and across what would traditionally be considered distinct groups. Additionally, we propose an innovative training method to mitigate the latent biases present in conventional skin tone categorization. This method leverages color distance estimates calculated through Bayesian regression with polynomial functions, ensuring a more nuanced and equitable treatment of skin color in ML models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11627v1",
    "published_date": "2025-06-13 09:54:01 UTC",
    "updated_date": "2025-06-13 09:54:01 UTC"
  },
  {
    "arxiv_id": "2506.11618v2",
    "title": "Convergent Linear Representations of Emergent Misalignment",
    "authors": [
      "Anna Soligo",
      "Edward Turner",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "abstract": "Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11618v2",
    "published_date": "2025-06-13 09:39:54 UTC",
    "updated_date": "2025-06-20 17:23:55 UTC"
  },
  {
    "arxiv_id": "2506.12110v1",
    "title": "EconGym: A Scalable AI Testbed with Diverse Economic Tasks",
    "authors": [
      "Qirui Mi",
      "Qipeng Yang",
      "Zijun Fan",
      "Wentian Fan",
      "Heyang Ma",
      "Chengdong Ma",
      "Siyu Xia",
      "Bo An",
      "Jun Wang",
      "Haifeng Zhang"
    ],
    "abstract": "Artificial intelligence (AI) has become a powerful tool for economic research, enabling large-scale simulation and policy optimization. However, applying AI effectively requires simulation platforms for scalable training and evaluation-yet existing environments remain limited to simplified, narrowly scoped tasks, falling short of capturing complex economic challenges such as demographic shifts, multi-government coordination, and large-scale agent interactions. To address this gap, we introduce EconGym, a scalable and modular testbed that connects diverse economic tasks with AI algorithms. Grounded in rigorous economic modeling, EconGym implements 11 heterogeneous role types (e.g., households, firms, banks, governments), their interaction mechanisms, and agent models with well-defined observations, actions, and rewards. Users can flexibly compose economic roles with diverse agent algorithms to simulate rich multi-agent trajectories across 25+ economic tasks for AI-driven policy learning and analysis. Experiments show that EconGym supports diverse and cross-domain tasks-such as coordinating fiscal, pension, and monetary policies-and enables benchmarking across AI, economic methods, and hybrids. Results indicate that richer task composition and algorithm diversity expand the policy space, while AI agents guided by classical economic methods perform best in complex settings. EconGym also scales to 10k agents with high realism and efficiency.",
    "categories": [
      "econ.GN",
      "cs.AI"
    ],
    "primary_category": "econ.GN",
    "comment": "28 pages, 7 figures, 17 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.12110v1",
    "published_date": "2025-06-13 09:35:04 UTC",
    "updated_date": "2025-06-13 09:35:04 UTC"
  },
  {
    "arxiv_id": "2506.11613v1",
    "title": "Model Organisms for Emergent Misalignment",
    "authors": [
      "Edward Turner",
      "Anna Soligo",
      "Mia Taylor",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "abstract": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11613v1",
    "published_date": "2025-06-13 09:34:25 UTC",
    "updated_date": "2025-06-13 09:34:25 UTC"
  },
  {
    "arxiv_id": "2506.11604v2",
    "title": "VLM@school -- Evaluation of AI image understanding on German middle school knowledge",
    "authors": [
      "René Peinl",
      "Vincent Tischler"
    ],
    "abstract": "This paper introduces a novel benchmark dataset designed to evaluate the capabilities of Vision Language Models (VLMs) on tasks that combine visual reasoning with subject-specific background knowledge in the German language. In contrast to widely used English-language benchmarks that often rely on artificially difficult or decontextualized problems, this dataset draws from real middle school curricula across nine domains including mathematics, history, biology, and religion. The benchmark includes over 2,000 open-ended questions grounded in 486 images, ensuring that models must integrate visual interpretation with factual reasoning rather than rely on superficial textual cues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple dimensions, including domain-specific accuracy and performance on adversarial crafted questions. Our findings reveal that even the strongest models achieve less than 45% overall accuracy, with particularly poor performance in music, mathematics, and adversarial settings. Furthermore, the results indicate significant discrepancies between success on popular benchmarks and real-world multimodal understanding. We conclude that middle school-level tasks offer a meaningful and underutilized avenue for stress-testing VLMs, especially in non-English contexts. The dataset and evaluation protocol serve as a rigorous testbed to better understand and improve the visual and linguistic reasoning capabilities of future AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Peinl, René; Tischler, Vincent (2025): VLM@school - Evaluation of AI image understanding on German middle school knowledge. Future Technologies Conference (FTC) 2025, Munich, Germany 2025 (accepted)",
    "pdf_url": "https://arxiv.org/pdf/2506.11604v2",
    "published_date": "2025-06-13 09:20:41 UTC",
    "updated_date": "2025-06-27 10:12:42 UTC"
  },
  {
    "arxiv_id": "2506.11602v1",
    "title": "Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study",
    "authors": [
      "Hawau Olamide Toyin",
      "Samar M. Magdy",
      "Hanan Aldarmaki"
    ],
    "abstract": "We investigate the effectiveness of large language models (LLMs) for text diacritization in two typologically distinct languages: Arabic and Yoruba. To enable a rigorous evaluation, we introduce a novel multilingual dataset MultiDiac, with diverse samples that capture a range of diacritic ambiguities. We evaluate 14 LLMs varying in size, accessibility, and language coverage, and benchmark them against 6 specialized diacritization models. Additionally, we fine-tune four small open-source models using LoRA for Yoruba. Our results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11602v1",
    "published_date": "2025-06-13 09:17:08 UTC",
    "updated_date": "2025-06-13 09:17:08 UTC"
  },
  {
    "arxiv_id": "2506.12109v3",
    "title": "Personalized LLM Decoding via Contrasting Personal Preference",
    "authors": [
      "Hyungjune Bu",
      "Chanjoo Jung",
      "Minjae Kang",
      "Jaehyung Kim"
    ],
    "abstract": "As large language models (LLMs) are progressively deployed in various real-world applications, personalization of LLMs has become increasingly important. While various approaches to LLM personalization such as prompt-based and training-based methods have been actively explored, the development of effective decoding-time algorithms remains largely overlooked, despite their demonstrated potential. In this paper, we propose CoPe (Contrasting Personal Preference), a novel decoding-time approach applied after performing parameter-efficient fine-tuning (PEFT) on user-specific data. Our core idea is to leverage reward-guided decoding specifically for personalization by maximizing each user's implicit reward signal. We evaluate CoPe across five open-ended personalized text generation tasks. Our empirical results demonstrate that CoPe achieves strong performance, improving personalization by an average of 10.57% in ROUGE-L, without relying on external reward models or additional training procedures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2506.12109v3",
    "published_date": "2025-06-13 09:12:44 UTC",
    "updated_date": "2025-11-24 00:58:45 UTC"
  },
  {
    "arxiv_id": "2506.11600v1",
    "title": "GraphRAG-Causal: A novel graph-augmented framework for causal reasoning and annotation in news",
    "authors": [
      "Abdul Haque",
      "Umm e Hani",
      "Ahmad Din",
      "Muhammad Babar",
      "Ali Abbas",
      "Insaf Ullah"
    ],
    "abstract": "GraphRAG-Causal introduces an innovative framework that combines graph-based retrieval with large language models to enhance causal reasoning in news analysis. Traditional NLP approaches often struggle with identifying complex, implicit causal links, especially in low-data scenarios. Our approach addresses these challenges by transforming annotated news headlines into structured causal knowledge graphs. It then employs a hybrid retrieval system that merges semantic embeddings with graph-based structural cues leveraging Neo4j to accurately match and retrieve relevant events. The framework is built on a three-stage pipeline: First, during Data Preparation, news sentences are meticulously annotated and converted into causal graphs capturing cause, effect, and trigger relationships. Next, the Graph Retrieval stage stores these graphs along with their embeddings in a Neo4j database and utilizes hybrid Cypher queries to efficiently identify events that share both semantic and structural similarities with a given query. Finally, the LLM Inference stage utilizes these retrieved causal graphs in a few-shot learning setup with XML-based prompting, enabling robust classification and tagging of causal relationships. Experimental evaluations demonstrate that GraphRAG-Causal achieves an impressive F1-score of 82.1% on causal classification using just 20 few-shot examples. This approach significantly boosts accuracy and consistency, making it highly suitable for real-time applications in news reliability assessment, misinformation detection, and policy analysis.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "18 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11600v1",
    "published_date": "2025-06-13 09:09:08 UTC",
    "updated_date": "2025-06-13 09:09:08 UTC"
  },
  {
    "arxiv_id": "2506.12108v1",
    "title": "A Lightweight IDS for Early APT Detection Using a Novel Feature Selection Method",
    "authors": [
      "Bassam Noori Shaker",
      "Bahaa Al-Musawi",
      "Mohammed Falih Hassan"
    ],
    "abstract": "An Advanced Persistent Threat (APT) is a multistage, highly sophisticated, and covert form of cyber threat that gains unauthorized access to networks to either steal valuable data or disrupt the targeted network. These threats often remain undetected for extended periods, emphasizing the critical need for early detection in networks to mitigate potential APT consequences. In this work, we propose a feature selection method for developing a lightweight intrusion detection system capable of effectively identifying APTs at the initial compromise stage. Our approach leverages the XGBoost algorithm and Explainable Artificial Intelligence (XAI), specifically utilizing the SHAP (SHapley Additive exPlanations) method for identifying the most relevant features of the initial compromise stage. The results of our proposed method showed the ability to reduce the selected features of the SCVIC-APT-2021 dataset from 77 to just four while maintaining consistent evaluation metrics for the suggested system. The estimated metrics values are 97% precision, 100% recall, and a 98% F1 score. The proposed method not only aids in preventing successful APT consequences but also enhances understanding of APT behavior at early stages.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12108v1",
    "published_date": "2025-06-13 09:07:56 UTC",
    "updated_date": "2025-06-13 09:07:56 UTC"
  },
  {
    "arxiv_id": "2506.11599v2",
    "title": "A$^2$LC: Active and Automated Label Correction for Semantic Segmentation",
    "authors": [
      "Youjin Jeon",
      "Kyusik Cho",
      "Suhan Woo",
      "Euntai Kim"
    ],
    "abstract": "Active Label Correction (ALC) has emerged as a promising solution to the high cost and error-prone nature of manual pixel-wise annotation in semantic segmentation, by actively identifying and correcting mislabeled data. Although recent work has improved correction efficiency by generating pseudo-labels using foundation models, substantial inefficiencies still remain. In this paper, we introduce A$^2$LC, an Active and Automated Label Correction framework for semantic segmentation, where manual and automatic correction stages operate in a cascaded manner. Specifically, the automatic correction stage leverages human feedback to extend label corrections beyond the queried samples, thereby maximizing cost efficiency. In addition, we introduce an adaptively balanced acquisition function that emphasizes underrepresented tail classes, working in strong synergy with the automatic correction stage. Extensive experiments on Cityscapes and PASCAL VOC 2012 demonstrate that A$^2$LC significantly outperforms previous state-of-the-art methods. Notably, A$^2$LC exhibits high efficiency by outperforming previous methods with only 20% of their budget, and shows strong effectiveness by achieving a 27.23% performance gain under the same budget on Cityscapes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.11599v2",
    "published_date": "2025-06-13 09:07:47 UTC",
    "updated_date": "2025-12-03 02:17:10 UTC"
  },
  {
    "arxiv_id": "2506.11585v1",
    "title": "OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots",
    "authors": [
      "Juno Kim",
      "Yesol Park",
      "Hye-Jung Yoon",
      "Byoung-Tak Zhang"
    ],
    "abstract": "We introduce OV-MAP, a novel approach to open-world 3D mapping for mobile robots by integrating open-features into 3D maps to enhance object recognition capabilities. A significant challenge arises when overlapping features from adjacent voxels reduce instance-level precision, as features spill over voxel boundaries, blending neighboring regions together. Our method overcomes this by employing a class-agnostic segmentation model to project 2D masks into 3D space, combined with a supplemented depth image created by merging raw and synthetic depth from point clouds. This approach, along with a 3D mask voting mechanism, enables accurate zero-shot 3D instance segmentation without relying on 3D supervised segmentation models. We assess the effectiveness of our method through comprehensive experiments on public datasets such as ScanNet200 and Replica, demonstrating superior zero-shot performance, robustness, and adaptability across diverse environments. Additionally, we conducted real-world experiments to demonstrate our method's adaptability and robustness when applied to diverse real-world environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IROS 2024",
    "pdf_url": "https://arxiv.org/pdf/2506.11585v1",
    "published_date": "2025-06-13 08:49:23 UTC",
    "updated_date": "2025-06-13 08:49:23 UTC"
  },
  {
    "arxiv_id": "2506.11584v1",
    "title": "A Comparative Analysis of Influence Signals for Data Debugging",
    "authors": [
      "Nikolaos Myrtakis",
      "Ioannis Tsamardinos",
      "Vassilis Christophides"
    ],
    "abstract": "Improving the quality of training samples is crucial for improving the reliability and performance of ML models. In this paper, we conduct a comparative evaluation of influence-based signals for debugging training data. These signals can potentially identify both mislabeled and anomalous samples from a potentially noisy training set as we build the models and hence alleviate the need for dedicated glitch detectors. Although several influence-based signals (e.g., Self-Influence, Average Absolute Influence, Marginal Influence, GD-class) have been recently proposed in the literature, there are no experimental studies for assessing their power in detecting different glitch types (e.g., mislabeled and anomalous samples) under a common influence estimator (e.g., TraceIn) for different data modalities (image and tabular), and deep learning models (trained from scratch or foundation). Through extensive experiments, we show that signals like Self-Influence effectively detect mislabeled samples, but none of the existing signals can detect anomalies. Existing signals do not take into account the training dynamics, i.e., how the samples' influence on the model changes during training, while some signals fall into influence cancellation effects, i.e., influence score is zero due to unsigned scores accumulation, resulting in misleading influence attribution.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted and presented at the Data-centric Machine Learning Research (DMLR) Workshop at ICML 2024",
    "pdf_url": "https://arxiv.org/pdf/2506.11584v1",
    "published_date": "2025-06-13 08:47:04 UTC",
    "updated_date": "2025-06-13 08:47:04 UTC"
  },
  {
    "arxiv_id": "2506.11578v3",
    "title": "Efficient LLM Collaboration via Planning",
    "authors": [
      "Byeongchan Lee",
      "Jonghoon Lee",
      "Dongyoung Kim",
      "Jaehyung Kim",
      "Kyungjoon Park",
      "Dongjun Lee",
      "Jinwoo Shin"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated strong performance, ranging from simple to complex tasks. However, while large proprietary models (e.g., models with over 100B parameters) achieve remarkable results across diverse tasks, they are often accessible through costly APIs, making frequent use too costly for many applications. In contrast, small open-source models (e.g., models with fewer than 3B parameters) are freely available and easy to deploy locally, but their performance on complex tasks remains limited. This trade-off raises a natural question: how can small and large models efficiently collaborate to combine their complementary strengths? To bridge this trade-off, we propose COPE, a test-time collaboration framework. A planner model first generates a plan that serves as a lightweight intermediate that guides a downstream executor model. Small and large models take turns acting as planner and executor, exchanging plans in a multi-stage cascade to collaboratively solve tasks. Through comprehensive experiments on benchmarks spanning mathematical reasoning, code generation, open-ended tasks, and agent tasks, we demonstrate that COPE achieves performance comparable to large proprietary models, while drastically reducing the inference API cost. These results highlight planning as an effective prior for cost-efficient inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11578v3",
    "published_date": "2025-06-13 08:35:50 UTC",
    "updated_date": "2026-01-16 15:28:18 UTC"
  },
  {
    "arxiv_id": "2506.11563v1",
    "title": "Learn to Preserve Personality: Federated Foundation Models in Recommendations",
    "authors": [
      "Zhiwei Li",
      "Guodong Long",
      "Chunxu Zhang",
      "Honglei Zhang",
      "Jing Jiang",
      "Chengqi Zhang"
    ],
    "abstract": "A core learning challenge for existed Foundation Models (FM) is striking the tradeoff between generalization with personalization, which is a dilemma that has been highlighted by various parameter-efficient adaptation techniques. Federated foundation models (FFM) provide a structural means to decouple shared knowledge from individual specific adaptations via decentralized processes. Recommendation systems offer a perfect testbed for FFMs, given their reliance on rich implicit feedback reflecting unique user characteristics. This position paper discusses a novel learning paradigm where FFMs not only harness their generalization capabilities but are specifically designed to preserve the integrity of user personality, illustrated thoroughly within the recommendation contexts. We envision future personal agents, powered by personalized adaptive FMs, guiding user decisions on content. Such an architecture promises a user centric, decentralized system where individuals maintain control over their personalized agents.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 3 figures, conference, position paper",
    "pdf_url": "https://arxiv.org/pdf/2506.11563v1",
    "published_date": "2025-06-13 08:17:07 UTC",
    "updated_date": "2025-06-13 08:17:07 UTC"
  },
  {
    "arxiv_id": "2506.11561v1",
    "title": "Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study",
    "authors": [
      "Gábor Antal",
      "Bence Bogenfürst",
      "Rudolf Ferenc",
      "Péter Hegedűs"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o's performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J's automated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26 (62\\%) vulnerabilities at least once, outperforming both the original baseline (40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11561v1",
    "published_date": "2025-06-13 08:15:45 UTC",
    "updated_date": "2025-06-13 08:15:45 UTC"
  },
  {
    "arxiv_id": "2506.11559v1",
    "title": "Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation",
    "authors": [
      "Gábor Antal",
      "Dénes Bán",
      "Martin Isztin",
      "Rudolf Ferenc",
      "Péter Hegedűs"
    ],
    "abstract": "In the life-cycle of software development, testing plays a crucial role in quality assurance. Proper testing not only increases code coverage and prevents regressions but it can also ensure that any potential vulnerabilities in the software are identified and effectively fixed. However, creating such tests is a complex, resource-consuming manual process. To help developers and security experts, this paper explores the automatic unit test generation capability of one of the most widely used large language models, GPT-4, from the perspective of vulnerabilities. We examine a subset of the VUL4J dataset containing real vulnerabilities and their corresponding fixes to determine whether GPT-4 can generate syntactically and/or semantically correct unit tests based on the code before and after the fixes as evidence of vulnerability mitigation. We focus on the impact of code contexts, the effectiveness of GPT-4's self-correction ability, and the subjective usability of the generated test cases. Our results indicate that GPT-4 can generate syntactically correct test cases 66.5\\% of the time without domain-specific pre-training. Although the semantic correctness of the fixes could be automatically validated in only 7. 5\\% of the cases, our subjective evaluation shows that GPT-4 generally produces test templates that can be further developed into fully functional vulnerability-witnessing tests with relatively minimal manual effort.\n  Therefore, despite the limited data, our initial findings suggest that GPT-4 can be effectively used in the generation of vulnerability-witnessing tests. It may not operate entirely autonomously, but it certainly plays a significant role in a partially automated process.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11559v1",
    "published_date": "2025-06-13 08:13:07 UTC",
    "updated_date": "2025-06-13 08:13:07 UTC"
  },
  {
    "arxiv_id": "2506.11558v3",
    "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs",
    "authors": [
      "Bo-Cheng Chiu",
      "Jen-Jee Chen",
      "Yu-Chee Tseng",
      "Feng-Chi Chen"
    ],
    "abstract": "Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11558v3",
    "published_date": "2025-06-13 08:13:05 UTC",
    "updated_date": "2025-07-21 16:37:00 UTC"
  },
  {
    "arxiv_id": "2506.11555v4",
    "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning",
    "authors": [
      "Yu Wang",
      "Shiwan Zhao",
      "Zhihu Wang",
      "Ming Fan",
      "Xicheng Zhang",
      "Yubo Zhang",
      "Zhengfan Wang",
      "Heyuan Huang",
      "Ting Liu"
    ],
    "abstract": "The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 13.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11555v4",
    "published_date": "2025-06-13 08:06:49 UTC",
    "updated_date": "2025-09-23 08:42:57 UTC"
  },
  {
    "arxiv_id": "2506.11550v2",
    "title": "Improving Multimodal Learning Balance and Sufficiency through Data Remixing",
    "authors": [
      "Xiaoyu Ma",
      "Hao Chen",
      "Yongjian Deng"
    ],
    "abstract": "Different modalities hold considerable gaps in optimization trajectories, including speeds and paths, which lead to modality laziness and modality clash when jointly training multimodal models, resulting in insufficient and imbalanced multimodal learning. Existing methods focus on enforcing the weak modality by adding modality-specific optimization objectives, aligning their optimization speeds, or decomposing multimodal learning to enhance unimodal learning. These methods fail to achieve both unimodal sufficiency and multimodal balance. In this paper, we, for the first time, address both concerns by proposing multimodal Data Remixing, including decoupling multimodal data and filtering hard samples for each modality to mitigate modality imbalance; and then batch-level reassembling to align the gradient directions and avoid cross-modal interference, thus enhancing unimodal learning sufficiency. Experimental results demonstrate that our method can be seamlessly integrated with existing approaches, improving accuracy by approximately 6.50%$\\uparrow$ on CREMAD and 3.41%$\\uparrow$ on Kinetic-Sounds, without training set expansion or additional computational overhead during inference. The source code is available at https://github.com/MatthewMaxy/Remix_ICML2025.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11550v2",
    "published_date": "2025-06-13 08:01:29 UTC",
    "updated_date": "2025-06-16 02:50:29 UTC"
  },
  {
    "arxiv_id": "2506.11543v1",
    "title": "FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation",
    "authors": [
      "Zhuguanyu Wu",
      "Shihe Wang",
      "Jiayi Zhang",
      "Jiaxin Chen",
      "Yunhong Wang"
    ],
    "abstract": "Post-training quantization (PTQ) has stood out as a cost-effective and promising model compression paradigm in recent years, as it avoids computationally intensive model retraining. Nevertheless, current PTQ methods for Vision Transformers (ViTs) still suffer from significant accuracy degradation, especially under low-bit quantization. To address these shortcomings, we analyze the prevailing Hessian-guided quantization loss, and uncover certain limitations of conventional Hessian approximations. By following the block-wise reconstruction framework, we propose a novel PTQ method for ViTs, dubbed FIMA-Q. Specifically, we firstly establish the connection between KL divergence and FIM, which enables fast computation of the quantization loss during reconstruction. We further propose an efficient FIM approximation method, namely DPLR-FIM, by employing the diagonal plus low-rank principle, and formulate the ultimate quantization loss. Our extensive experiments, conducted across various vision tasks with representative ViT-based architectures on public datasets, demonstrate that our method substantially promotes the accuracy compared to the state-of-the-art approaches, especially in the case of low-bit quantization. The source code is available at https://github.com/ShiheWang/FIMA-Q.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 Highlight",
    "pdf_url": "https://arxiv.org/pdf/2506.11543v1",
    "published_date": "2025-06-13 07:57:38 UTC",
    "updated_date": "2025-06-13 07:57:38 UTC"
  },
  {
    "arxiv_id": "2506.17277v1",
    "title": "Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation",
    "authors": [
      "Mahmoud Amiri",
      "Thomas Bocklitz"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems are increasingly vital for navigating the ever-expanding body of scientific literature, particularly in high-stakes domains such as chemistry. Despite the promise of RAG, foundational design choices -- such as how documents are segmented and represented -- remain underexplored in domain-specific contexts. This study presents the first large-scale, systematic evaluation of chunking strategies and embedding models tailored to chemistry-focused RAG systems. We investigate 25 chunking configurations across five method families and evaluate 48 embedding models on three chemistry-specific benchmarks, including the newly introduced QuestChemRetrieval dataset. Our results reveal that recursive token-based chunking (specifically R100-0) consistently outperforms other approaches, offering strong performance with minimal resource overhead. We also find that retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants -- substantially outperform domain-specialized models like SciBERT. By releasing our datasets, evaluation framework, and empirical benchmarks, we provide actionable guidelines for building effective and efficient chemistry-aware RAG systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17277v1",
    "published_date": "2025-06-13 07:44:53 UTC",
    "updated_date": "2025-06-13 07:44:53 UTC"
  },
  {
    "arxiv_id": "2506.11526v2",
    "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis",
    "authors": [
      "Yuan Gao",
      "Mattia Piccinini",
      "Yuchen Zhang",
      "Dingrui Wang",
      "Korbinian Moller",
      "Roberto Brusnicki",
      "Baha Zarrouki",
      "Alessio Gambi",
      "Jan Frederik Totz",
      "Kai Storms",
      "Steven Peters",
      "Andrea Stocco",
      "Bassam Alrifaee",
      "Marco Pavone",
      "Johannes Betz"
    ],
    "abstract": "For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Revised manuscript with separate evaluation metrics table",
    "pdf_url": "https://arxiv.org/pdf/2506.11526v2",
    "published_date": "2025-06-13 07:25:59 UTC",
    "updated_date": "2025-11-27 09:45:06 UTC"
  },
  {
    "arxiv_id": "2506.11521v1",
    "title": "Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models",
    "authors": [
      "Jinming Wen",
      "Xinyi Wu",
      "Shuai Zhao",
      "Yanhao Jia",
      "Yuwen Li"
    ],
    "abstract": "Multimodal large language models (MLLMs), which bridge the gap between audio-visual and natural language processing, achieve state-of-the-art performance on several audio-visual tasks. Despite the superior performance of MLLMs, the scarcity of high-quality audio-visual training data and computational resources necessitates the utilization of third-party data and open-source MLLMs, a trend that is increasingly observed in contemporary research. This prosperity masks significant security risks. Empirical studies demonstrate that the latest MLLMs can be manipulated to produce malicious or harmful content. This manipulation is facilitated exclusively through instructions or inputs, including adversarial perturbations and malevolent queries, effectively bypassing the internal security mechanisms embedded within the models. To gain a deeper comprehension of the inherent security vulnerabilities associated with audio-visual-based multimodal models, a series of surveys investigates various types of attacks, including adversarial and backdoor attacks. While existing surveys on audio-visual attacks provide a comprehensive overview, they are limited to specific types of attacks, which lack a unified review of various types of attacks. To address this issue and gain insights into the latest trends in the field, this paper presents a comprehensive and systematic review of audio-visual attacks, which include adversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this paper also reviews various types of attacks in the latest audio-visual-based MLLMs, a dimension notably absent in existing surveys. Drawing upon comprehensive insights from a substantial review, this paper delineates both challenges and emergent trends for future research on audio-visual attacks and defense.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11521v1",
    "published_date": "2025-06-13 07:22:36 UTC",
    "updated_date": "2025-06-13 07:22:36 UTC"
  },
  {
    "arxiv_id": "2506.11512v1",
    "title": "Prioritizing Alignment Paradigms over Task-Specific Model Customization in Time-Series LLMs",
    "authors": [
      "Wei Li",
      "Yunyao Cheng",
      "Xinli Hao",
      "Chaohong Ma",
      "Yuxuan Liang",
      "Bin Yang",
      "Christian S. Jensen",
      "Xiaofeng Meng"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have enabled unprecedented capabilities for time-series reasoning in diverse real-world applications, including medical, financial, and spatio-temporal domains. However, existing approaches typically focus on task-specific model customization, such as forecasting and anomaly detection, while overlooking the data itself, referred to as time-series primitives, which are essential for in-depth reasoning. This position paper advocates a fundamental shift in approaching time-series reasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic primitives of time series data over task-specific model customization. This realignment addresses the core limitations of current time-series reasoning approaches, which are often costly, inflexible, and inefficient, by systematically accounting for intrinsic structure of data before task engineering. To this end, we propose three alignment paradigms: Injective Alignment, Bridging Alignment, and Internal Alignment, which are emphasized by prioritizing different aspects of time-series primitives: domain, characteristic, and representation, respectively, to activate time-series reasoning capabilities of LLMs to enable economical, flexible, and efficient reasoning. We further recommend that practitioners adopt an alignment-oriented method to avail this instruction to select an appropriate alignment paradigm. Additionally, we categorize relevant literature into these alignment paradigms and outline promising research directions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11512v1",
    "published_date": "2025-06-13 07:13:05 UTC",
    "updated_date": "2025-06-13 07:13:05 UTC"
  },
  {
    "arxiv_id": "2507.00014v1",
    "title": "SWE-Bench-CL: Continual Learning for Coding Agents",
    "authors": [
      "Thomas Joshi",
      "Shayan Chowdhury",
      "Fatih Uysal"
    ],
    "abstract": "Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at https://github.com/thomasjoshi/agents-never-forget, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00014v1",
    "published_date": "2025-06-13 07:11:14 UTC",
    "updated_date": "2025-06-13 07:11:14 UTC"
  },
  {
    "arxiv_id": "2506.11508v1",
    "title": "Machine Learning-Based Quantification of Vesicoureteral Reflux with Enhancing Accuracy and Efficiency",
    "authors": [
      "Muhyeeddin Alqaraleh",
      "Mowafaq Salem Alzboon",
      "Mohammad Subhi Al-Batah",
      "Lana Yasin Al Aesa",
      "Mohammed Hasan Abu-Arqoub",
      "Rashiq Rafiq Marie",
      "Firas Hussein Alsmad"
    ],
    "abstract": "Vesicoureteral reflux (VUR) is traditionally assessed using subjective grading systems, which introduces variability in diagnosis. This study investigates the use of machine learning to improve diagnostic consistency by analyzing voiding cystourethrogram (VCUG) images. A total of 113 VCUG images were reviewed, with expert grading of VUR severity. Nine image-based features were selected to train six predictive models: Logistic Regression, Decision Tree, Gradient Boosting, Neural Network, and Stochastic Gradient Descent. The models were evaluated using leave-one-out cross-validation. Analysis identified deformation patterns in the renal calyces as key indicators of high-grade VUR. All models achieved accurate classifications with no false positives or negatives. High sensitivity to subtle image patterns characteristic of different VUR grades was confirmed by substantial Area Under the Curve (AUC) values. The results suggest that machine learning can offer an objective and standardized alternative to current subjective VUR assessments. These findings highlight renal calyceal deformation as a strong predictor of severe cases. Future research should aim to expand the dataset, refine imaging features, and improve model generalizability for broader clinical use.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11508v1",
    "published_date": "2025-06-13 07:09:12 UTC",
    "updated_date": "2025-06-13 07:09:12 UTC"
  },
  {
    "arxiv_id": "2506.11501v1",
    "title": "Diabetes Prediction and Management Using Machine Learning Approaches",
    "authors": [
      "Mowafaq Salem Alzboon",
      "Muhyeeddin Alqaraleh",
      "Mohammad Subhi Al-Batah"
    ],
    "abstract": "Diabetes has emerged as a significant global health issue, especially with the increasing number of cases in many countries. This trend Underlines the need for a greater emphasis on early detection and proactive management to avert or mitigate the severe health complications of this disease. Over recent years, machine learning algorithms have shown promising potential in predicting diabetes risk and are beneficial for practitioners. Objective: This study highlights the prediction capabilities of statistical and non-statistical machine learning methods over Diabetes risk classification in 768 samples from the Pima Indians Diabetes Database. It consists of the significant demographic and clinical features of age, body mass index (BMI) and blood glucose levels that greatly depend on the vulnerability against Diabetes. The experimentation assesses the various types of machine learning algorithms in terms of accuracy and effectiveness regarding diabetes prediction. These algorithms include Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting and Neural Network Models. The results show that the Neural Network algorithm gained the highest predictive accuracy with 78,57 %, and then the Random Forest algorithm had the second position with 76,30 % accuracy. These findings show that machine learning techniques are not just highly effective. Still, they also can potentially act as early screening tools in predicting Diabetes within a data-driven fashion with valuable information on who is more likely to get affected. In addition, this study can help to realize the potential of machine learning for timely intervention over the longer term, which is a step towards reducing health outcomes and disease burden attributable to Diabetes on healthcare systems",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11501v1",
    "published_date": "2025-06-13 06:58:19 UTC",
    "updated_date": "2025-06-13 06:58:19 UTC"
  },
  {
    "arxiv_id": "2506.11490v1",
    "title": "Composite Data Augmentations for Synthetic Image Detection Against Real-World Perturbations",
    "authors": [
      "Efthymia Amarantidou",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "abstract": "The advent of accessible Generative AI tools enables anyone to create and spread synthetic images on social media, often with the intention to mislead, thus posing a significant threat to online information integrity. Most existing Synthetic Image Detection (SID) solutions struggle on generated images sourced from the Internet, as these are often altered by compression and other operations. To address this, our research enhances SID by exploring data augmentation combinations, leveraging a genetic algorithm for optimal augmentation selection, and introducing a dual-criteria optimization approach. These methods significantly improve model performance under real-world perturbations. Our findings provide valuable insights for developing detection models capable of identifying synthetic images across varying qualities and transformations, with the best-performing model achieving a mean average precision increase of +22.53% compared to models without augmentations. The implementation is available at github.com/efthimia145/sid-composite-data-augmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "EUSIPCO 2025 (33rd European Signal Processing Conference)",
    "pdf_url": "https://arxiv.org/pdf/2506.11490v1",
    "published_date": "2025-06-13 06:28:05 UTC",
    "updated_date": "2025-06-13 06:28:05 UTC"
  },
  {
    "arxiv_id": "2506.11487v1",
    "title": "Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models",
    "authors": [
      "Chenrui Cao",
      "Liangcheng Song",
      "Zenan Li",
      "Xinyi Le",
      "Xian Zhang",
      "Hui Xue",
      "Fan Yang"
    ],
    "abstract": "Recent advancements, such as DeepSeek-Prover-V2-671B and Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging reinforcement learning (RL)-based large-scale training for automated theorem proving. Surprisingly, we discover that even without any training, careful neuro-symbolic coordination of existing off-the-shelf reasoning models and tactic step provers can achieve comparable performance. This paper introduces \\textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework, featuring a \\emph{fine-grained and integrated} neuro-symbolic enhancement for each phase: (1) In the draft phase, we prompt reasoning models to generate concise natural-language subgoals to benefit the sketch phase, removing thinking tokens and references to human-written proofs; (2) In the sketch phase, subgoals are autoformalized with hypotheses to benefit the proving phase, and sketch lines containing syntactic errors are masked according to predefined rules; (3) In the proving phase, we tightly integrate symbolic search methods like Aesop with step provers to establish proofs for the sketch subgoals. Experimental results show that, without any additional model training or fine-tuning, DSP+ solves 80.7\\%, 32.8\\%, and 24 out of 644 problems from miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets compared to state-of-the-arts. DSP+ proves \\texttt{imo\\_2019\\_p1}, an IMO problem in miniF2F that is not solved by any prior work. Additionally, DSP+ generates proof patterns comprehensible by human experts, facilitating the identification of formalization errors; For example, eight wrongly formalized statements in miniF2F are discovered. Our results highlight the potential of classical reasoning patterns besides the RL-based training. All components will be open-sourced.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages. Associated code and results are available at https://github.com/microsoft/DSP-Plus",
    "pdf_url": "https://arxiv.org/pdf/2506.11487v1",
    "published_date": "2025-06-13 06:25:59 UTC",
    "updated_date": "2025-06-13 06:25:59 UTC"
  },
  {
    "arxiv_id": "2506.11485v1",
    "title": "Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models",
    "authors": [
      "Cole Gawin"
    ],
    "abstract": "While large language models like BERT demonstrate strong empirical performance on semantic tasks, whether this reflects true conceptual competence or surface-level statistical association remains unclear. I investigate whether BERT encodes abstract relational schemata by examining internal representations of concept pairs across taxonomic, mereological, and functional relations. I compare BERT's relational classification performance with representational structure in [CLS] token embeddings. Results reveal that pretrained BERT enables high classification accuracy, indicating latent relational signals. However, concept pairs organize by relation type in high-dimensional embedding space only after fine-tuning on supervised relation classification tasks. This indicates relational schemata are not emergent from pretraining alone but can be induced via task scaffolding. These findings demonstrate that behavioral performance does not necessarily imply structured conceptual understanding, though models can acquire inductive biases for grounded relational abstraction through appropriate training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 4 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.11485v1",
    "published_date": "2025-06-13 06:20:03 UTC",
    "updated_date": "2025-06-13 06:20:03 UTC"
  },
  {
    "arxiv_id": "2506.14825v2",
    "title": "GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static Decoupling for 3D Gaussian Splatting-based Occupancy Prediction",
    "authors": [
      "Ke Song",
      "Yunhe Wu",
      "Chunchit Siu",
      "Huiyuan Xiong"
    ],
    "abstract": "Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splatting (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization and (3) biased issues in dynamic-static object coupling optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer and decouples dynamic-static objects optimization for 3D Gaussian Splatting-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarsegrained attention at higher layers models object-level topology. On the other hand, we decouple dynamic and static objects by leveraging semantic probability distributions and design a Dynamic-Static Decoupled Gaussian Attention mechanism to optimize the prediction performance for both dynamic objects and static scenes. GraphGSOcc achieves state-ofthe-art performance on the SurroundOcc-nuScenes, Occ3D-nuScenes, OpenOcc and KITTI occupancy benchmarks. Experiments on the SurroundOcc dataset achieve an mIoU of 25.20%, reducing GPU memory to 6.8 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14825v2",
    "published_date": "2025-06-13 06:09:57 UTC",
    "updated_date": "2025-07-02 13:42:48 UTC"
  },
  {
    "arxiv_id": "2506.11480v3",
    "title": "LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment",
    "authors": [
      "Shipeng Li",
      "Shikun Li",
      "Zhiqin Yang",
      "Xinghua Zhang",
      "Gaode Chen",
      "Xiaobo Xia",
      "Hengyu Liu",
      "Zhe Peng"
    ],
    "abstract": "Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data selection. To facilitate future work, we will release code.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11480v3",
    "published_date": "2025-06-13 06:05:58 UTC",
    "updated_date": "2025-07-04 07:31:49 UTC"
  },
  {
    "arxiv_id": "2506.21571v2",
    "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models",
    "authors": [
      "Jianshuo Dong",
      "Yujia Fu",
      "Chuanrui Hu",
      "Chao Zhang",
      "Han Qiu"
    ],
    "abstract": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain of Thought (CoT) before producing final responses, offer a promising approach to interpreting and monitoring model behaviors. Inspired by the observation that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' -- consistently emerge across tasks, we explore whether LRMs exhibit human-like cognitive habits. Building on Habits of Mind, a well-established framework of cognitive habits associated with successful human problem-solving, we introduce CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits. CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks, and employs an evidence-first extraction method to ensure reliable habit identification. With CogTest, we conduct a comprehensive evaluation of 16 widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that LRMs, unlike conventional LLMs, not only exhibit human-like habits but also adaptively deploy them according to different tasks. Finer-grained analyses further uncover patterns of similarity and difference in LRMs' cognitive habit profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and DeepSeek-R1). Extending the study to safety-related tasks, we observe that certain habits, such as Taking Responsible Risks, are strongly associated with the generation of harmful responses. These findings suggest that studying persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper understanding of LLM misbehavior. The code is available at: https://github.com/jianshuod/CogTest.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21571v2",
    "published_date": "2025-06-13 05:40:56 UTC",
    "updated_date": "2025-07-06 02:26:21 UTC"
  },
  {
    "arxiv_id": "2506.11469v1",
    "title": "Structure-Aware Automatic Channel Pruning by Searching with Graph Embedding",
    "authors": [
      "Zifan Liu",
      "Yuan Cao",
      "Yanwei Yu",
      "Heng Qi",
      "Jie Gui"
    ],
    "abstract": "Channel pruning is a powerful technique to reduce the computational overhead of deep neural networks, enabling efficient deployment on resource-constrained devices. However, existing pruning methods often rely on local heuristics or weight-based criteria that fail to capture global structural dependencies within the network, leading to suboptimal pruning decisions and degraded model performance. To address these limitations, we propose a novel structure-aware automatic channel pruning (SACP) framework that utilizes graph convolutional networks (GCNs) to model the network topology and learn the global importance of each channel. By encoding structural relationships within the network, our approach implements topology-aware pruning and this pruning is fully automated, reducing the need for human intervention. We restrict the pruning rate combinations to a specific space, where the number of combinations can be dynamically adjusted, and use a search-based approach to determine the optimal pruning rate combinations. Extensive experiments on benchmark datasets (CIFAR-10, ImageNet) with various models (ResNet, VGG16) demonstrate that SACP outperforms state-of-the-art pruning methods on compression efficiency and competitive on accuracy retention.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11469v1",
    "published_date": "2025-06-13 05:05:35 UTC",
    "updated_date": "2025-06-13 05:05:35 UTC"
  },
  {
    "arxiv_id": "2506.12104v2",
    "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
    "authors": [
      "Hao Li",
      "Xiaogeng Liu",
      "Hung-Chun Chiu",
      "Dianqi Li",
      "Ning Zhang",
      "Chaowei Xiao"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints. A Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an Injection Isolator detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks. We empirically validate the effectiveness of DRIFT on the AgentDojo and ASB benchmark, demonstrating its strong security performance while maintaining high utility across diverse models, showcasing both its robustness and adaptability. The code is released at https://github.com/SaFoLab-WISC/DRIFT.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12104v2",
    "published_date": "2025-06-13 05:01:09 UTC",
    "updated_date": "2025-10-24 01:50:46 UTC"
  },
  {
    "arxiv_id": "2506.11465v1",
    "title": "RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer",
    "authors": [
      "Haotian Ni",
      "Yake Wei",
      "Hang Liu",
      "Gong Chen",
      "Chong Peng",
      "Hao Lin",
      "Di Hu"
    ],
    "abstract": "Multimodal learning faces challenges in effectively fusing information from diverse modalities, especially when modality quality varies across samples. Dynamic fusion strategies, such as attention mechanism in Transformers, aim to address such challenge by adaptively emphasizing modalities based on the characteristics of input data. However, through amounts of carefully designed experiments, we surprisingly observed that the dynamic adaptability of widely-used self-attention models diminishes. Model tends to prefer one modality regardless of data characteristics. This bias triggers a self-reinforcing cycle that progressively overemphasizes the favored modality, widening the distribution gap in attention keys across modalities and deactivating attention mechanism's dynamic properties. To revive adaptability, we propose a simple yet effective method Rolling Query (RollingQ), which balances attention allocation by rotating the query to break the self-reinforcing cycle and mitigate the key distribution gap. Extensive experiments on various multimodal scenarios validate the effectiveness of RollingQ and the restoration of cooperation dynamics is pivotal for enhancing the broader capabilities of widely deployed multimodal Transformers. The source code is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11465v1",
    "published_date": "2025-06-13 04:39:58 UTC",
    "updated_date": "2025-06-13 04:39:58 UTC"
  },
  {
    "arxiv_id": "2507.14141v1",
    "title": "DIVER-0 : A Fully Channel Equivariant EEG Foundation Model",
    "authors": [
      "Danny Dongyeop Han",
      "Ahhyun Lucy Lee",
      "Taeyang Lee",
      "Yonghyeon Gwon",
      "Sebin Lee",
      "Seongjin Lee",
      "David Keetae Park",
      "Shinjae Yoo",
      "Jiook Cha",
      "Chun Kee Chung"
    ],
    "abstract": "Electroencephalography (EEG) is a non-invasive technique widely used in brain-computer interfaces and clinical applications, yet existing EEG foundation models face limitations in modeling spatio-temporal brain dynamics and lack channel permutation equivariance, preventing robust generalization across diverse electrode configurations. To address these challenges, we propose DIVER-0, a novel EEG foundation model that demonstrates how full spatio-temporal attention-rather than segregated spatial or temporal processing-achieves superior performance when properly designed with Rotary Position Embedding (RoPE) for temporal relationships and binary attention biases for channel differentiation. We also introduce Sliding Temporal Conditional Positional Encoding (STCPE), which improves upon existing conditional positional encoding approaches by maintaining both temporal translation equivariance and channel permutation equivariance, enabling robust adaptation to arbitrary electrode configurations unseen during pretraining. Experimental results demonstrate that DIVER-0 achieves competitive performance with only 10% of pretraining data while maintaining consistent results across all channel permutation conditions, validating its effectiveness for cross-dataset generalization and establishing key design principles for handling the inherent heterogeneity of neural recording setups.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "11 pages, 1 figures, ICML 2025 Workshop on GenBio",
    "pdf_url": "https://arxiv.org/pdf/2507.14141v1",
    "published_date": "2025-06-13 04:17:15 UTC",
    "updated_date": "2025-06-13 04:17:15 UTC"
  },
  {
    "arxiv_id": "2506.11455v1",
    "title": "Voxel-Level Brain States Prediction Using Swin Transformer",
    "authors": [
      "Yifei Sun",
      "Daniel Chahine",
      "Qinghao Wen",
      "Tianming Liu",
      "Xiang Li",
      "Yixuan Yuan",
      "Fernando Calamante",
      "Jinglei Lv"
    ],
    "abstract": "Understanding brain dynamics is important for neuroscience and mental health. Functional magnetic resonance imaging (fMRI) enables the measurement of neural activities through blood-oxygen-level-dependent (BOLD) signals, which represent brain states. In this study, we aim to predict future human resting brain states with fMRI. Due to the 3D voxel-wise spatial organization and temporal dependencies of the fMRI data, we propose a novel architecture which employs a 4D Shifted Window (Swin) Transformer as encoder to efficiently learn spatio-temporal information and a convolutional decoder to enable brain state prediction at the same spatial and temporal resolution as the input fMRI data. We used 100 unrelated subjects from the Human Connectome Project (HCP) for model training and testing. Our novel model has shown high accuracy when predicting 7.2s resting-state brain activities based on the prior 23.04s fMRI time series. The predicted brain states highly resemble BOLD contrast and dynamics. This work shows promising evidence that the spatiotemporal organization of the human brain can be learned by a Swin Transformer model, at high resolution, which provides a potential for reducing the fMRI scan time and the development of brain-computer interfaces in the future.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11455v1",
    "published_date": "2025-06-13 04:14:38 UTC",
    "updated_date": "2025-06-13 04:14:38 UTC"
  },
  {
    "arxiv_id": "2506.13800v1",
    "title": "Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework",
    "authors": [
      "Abul Ehtesham",
      "Aditi Singh",
      "Saket Kumar"
    ],
    "abstract": "Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13800v1",
    "published_date": "2025-06-13 04:07:19 UTC",
    "updated_date": "2025-06-13 04:07:19 UTC"
  },
  {
    "arxiv_id": "2507.00013v1",
    "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting",
    "authors": [
      "Hyunwoo Seo",
      "Chiehyeon Lim"
    ],
    "abstract": "Forecasting complex time series is an important yet challenging problem that involves various industrial applications. Recently, masked time-series modeling has been proposed to effectively model temporal dependencies for forecasting by reconstructing masked segments from unmasked ones. However, since the semantic information in time series is involved in intricate temporal variations generated by multiple time series components, simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach. Specifically, we propose ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components to produce multiple masked seasonal series based on inherent multi-periodicity and a sub-series masking strategy for trend components to mask temporal regions that share similar variations. The proposed masking method presents an effective pre-training task for learning intricate temporal variations and dependencies. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations. Experimental results show that our proposed ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD 2025 research track",
    "pdf_url": "https://arxiv.org/pdf/2507.00013v1",
    "published_date": "2025-06-13 04:06:47 UTC",
    "updated_date": "2025-06-13 04:06:47 UTC"
  },
  {
    "arxiv_id": "2506.11445v1",
    "title": "Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention",
    "authors": [
      "Xuan Duy Ta",
      "Bang Giang Le",
      "Thanh Ha Le",
      "Viet Cuong Ta"
    ],
    "abstract": "In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11445v1",
    "published_date": "2025-06-13 03:48:54 UTC",
    "updated_date": "2025-06-13 03:48:54 UTC"
  },
  {
    "arxiv_id": "2506.11441v1",
    "title": "DPUV4E: High-Throughput DPU Architecture Design for CNN on Versal ACAP",
    "authors": [
      "Guoyu Li",
      "Pengbo Zheng",
      "Jian Weng",
      "Enshan Yang"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) remain prevalent in computer vision applications, and FPGAs, known for their flexibility and energy efficiency, have become essential components in heterogeneous acceleration systems. However, traditional FPGAs face challenges in balancing performance and versatility due to limited on-chip resources. AMD's Versal ACAP architecture, tailored for AI applications, incorporates AI Engines (AIEs) to deliver high computational power. Nevertheless, the platform suffers from insufficient memory bandwidth, hindering the full utilization of the AIEs' theoretical performance. In this paper, we present DPUV4E for the Versal architecture, providing configurations ranging from 2PE ($32.6$ TOPS) to 8PE ($131.0$ TOPS). We design two computation units, Conv PE and DWC PE, to support different computational patterns. Each computation unit's data flow efficiently utilizes the data reuse opportunities to mitigate bandwidth bottlenecks. Additionally, we extend the functionality of each PE to utilize AIEs for non-convolutional operations, reducing resource overhead. Experiments on over 50 models show that compared to previous designs, our design provides $8.6\\times$ the TOPS/W of traditional FPGA-based DPU designs, while reducing DSP usage by $95.8\\%$, LUT usage by $44.7\\%$, and latency to $68.5\\%$ under single-batch conditions. For end-to-end inference, our design improving throughput by up to $2.2\\times$ for depth-wise convolution models and up to $1.3\\times$ for standard models.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "10 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11441v1",
    "published_date": "2025-06-13 03:39:05 UTC",
    "updated_date": "2025-06-13 03:39:05 UTC"
  },
  {
    "arxiv_id": "2506.11432v1",
    "title": "KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models",
    "authors": [
      "Taeeun Kim",
      "Semin Jeong",
      "Youngsook Song"
    ],
    "abstract": "This research introduces KoGEC, a Korean Grammatical Error Correction system using pre\\--trained translation models. We fine-tuned NLLB (No Language Left Behind) models for Korean GEC, comparing their performance against large language models like GPT-4 and HCX-3. The study used two social media conversation datasets for training and testing. The NLLB models were fine-tuned using special language tokens to distinguish between original and corrected Korean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\" method to classify error types. Results showed that the fine-tuned NLLB (KoGEC) models outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a more balanced error correction profile across various error types, whereas the larger LLMs tended to focus less on punctuation errors. We also developed a Chrome extension to make the KoGEC system accessible to users. Finally, we explored token vocabulary expansion to further improve the model but found it to decrease model performance. This research contributes to the field of NLP by providing an efficient, specialized Korean GEC system and a new evaluation method. It also highlights the potential of compact, task-specific models to compete with larger, general-purpose language models in specialized NLP tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.11432v1",
    "published_date": "2025-06-13 03:10:15 UTC",
    "updated_date": "2025-06-13 03:10:15 UTC"
  },
  {
    "arxiv_id": "2506.13799v1",
    "title": "Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization",
    "authors": [
      "Soroush Vahidi"
    ],
    "abstract": "We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a preliminary paper",
    "pdf_url": "https://arxiv.org/pdf/2506.13799v1",
    "published_date": "2025-06-13 03:03:55 UTC",
    "updated_date": "2025-06-13 03:03:55 UTC"
  },
  {
    "arxiv_id": "2506.11425v2",
    "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards",
    "authors": [
      "Jeff Da",
      "Clinton Wang",
      "Xiang Deng",
      "Yuntao Ma",
      "Nikhil Barhate",
      "Sean Hendryx"
    ],
    "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent's errors and environmental interactions, emulate a teacher's guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11425v2",
    "published_date": "2025-06-13 02:46:53 UTC",
    "updated_date": "2025-06-20 23:32:06 UTC"
  },
  {
    "arxiv_id": "2506.11421v3",
    "title": "Deep Learning Model Acceleration and Optimization Strategies for Real-Time Recommendation Systems",
    "authors": [
      "Junli Shao",
      "Jing Dong",
      "Dingzhou Wang",
      "Kowei Shih",
      "Dannier Li",
      "Chengrui Zhou"
    ],
    "abstract": "With the rapid growth of Internet services, recommendation systems play a central role in delivering personalized content. Faced with massive user requests and complex model architectures, the key challenge for real-time recommendation systems is how to reduce inference latency and increase system throughput without sacrificing recommendation quality. This paper addresses the high computational cost and resource bottlenecks of deep learning models in real-time settings by proposing a combined set of modeling- and system-level acceleration and optimization strategies. At the model level, we dramatically reduce parameter counts and compute requirements through lightweight network design, structured pruning, and weight quantization. At the system level, we integrate multiple heterogeneous compute platforms and high-performance inference libraries, and we design elastic inference scheduling and load-balancing mechanisms based on real-time load characteristics. Experiments show that, while maintaining the original recommendation accuracy, our methods cut latency to less than 30% of the baseline and more than double system throughput, offering a practical solution for deploying large-scale online recommendation services.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11421v3",
    "published_date": "2025-06-13 02:39:21 UTC",
    "updated_date": "2025-08-13 15:18:09 UTC"
  },
  {
    "arxiv_id": "2506.11419v1",
    "title": "FocalAD: Local Motion Planning for End-to-End Autonomous Driving",
    "authors": [
      "Bin Sun",
      "Boao Zhang",
      "Jiayi Lu",
      "Xinjie Feng",
      "Jiachen Shang",
      "Rui Cao",
      "Mengchao Zheng",
      "Chuanye Wang",
      "Shichun Yang",
      "Yaoguang Cao",
      "Ziying Song"
    ],
    "abstract": "In end-to-end autonomous driving,the motion prediction plays a pivotal role in ego-vehicle planning. However, existing methods often rely on globally aggregated motion features, ignoring the fact that planning decisions are primarily influenced by a small number of locally interacting agents. Failing to attend to these critical local interactions can obscure potential risks and undermine planning reliability. In this work, we propose FocalAD, a novel end-to-end autonomous driving framework that focuses on critical local neighbors and refines planning by enhancing local motion representations. Specifically, FocalAD comprises two core modules: the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a graph-based ego-centric interaction representation that captures motion dynamics with local neighbors to enhance both ego planning and agent motion queries. FLA Loss increases the weights of decision-critical neighboring agents, guiding the model to prioritize those more relevant to planning. Extensive experiments show that FocalAD outperforms existing state-of-the-art methods on the open-loop nuScenes datasets and closed-loop Bench2Drive benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD achieves even greater improvements, reducing the average colilision rate by 41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11419v1",
    "published_date": "2025-06-13 02:39:01 UTC",
    "updated_date": "2025-06-13 02:39:01 UTC"
  },
  {
    "arxiv_id": "2506.11417v1",
    "title": "Stop learning it all to mitigate visual hallucination, Focus on the hallucination target",
    "authors": [
      "Dokyoon Yoon",
      "Youngsook Song",
      "Woomyong Park"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) frequently suffer from hallucination issues, generating information about objects that are not present in input images during vision-language tasks. These hallucinations particularly undermine model reliability in practical applications requiring accurate object identification. To address this challenge, we propose \\mymethod,\\ a preference learning approach that mitigates hallucinations by focusing on targeted areas where they occur. To implement this, we build a dataset containing hallucinated responses, correct responses, and target information (i.e., objects present in the images and the corresponding chunk positions in responses affected by hallucinations). By applying a preference learning method restricted to these specific targets, the model can filter out irrelevant signals and focus on correcting hallucinations. This allows the model to produce more factual responses by concentrating solely on relevant information. Experimental results demonstrate that \\mymethod\\ effectively reduces hallucinations across multiple vision hallucination tasks, improving the reliability and performance of MLLMs without diminishing overall performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11417v1",
    "published_date": "2025-06-13 02:35:03 UTC",
    "updated_date": "2025-06-13 02:35:03 UTC"
  },
  {
    "arxiv_id": "2506.11403v1",
    "title": "A correlation-permutation approach for speech-music encoders model merging",
    "authors": [
      "Fabian Ritter-Gutierrez",
      "Yi-Cheng Lin",
      "Jeremy H. M Wong",
      "Hung-yi Lee",
      "Eng Siong Chng",
      "Nancy F. Chen"
    ],
    "abstract": "Creating a unified speech and music model requires expensive pre-training. Model merging can instead create an unified audio model with minimal computational expense. However, direct merging is challenging when the models are not aligned in the weight space. Motivated by Git Re-Basin, we introduce a correlation-permutation approach that aligns a music encoder's internal layers with a speech encoder. We extend previous work to the case of merging transformer layers. The method computes a permutation matrix that maximizes the model's features-wise cross-correlations layer by layer, enabling effective fusion of these otherwise disjoint models. The merged model retains speech capabilities through this method while significantly enhancing music performance, achieving an improvement of 14.83 points in average score compared to linear interpolation model merging. This work allows the creation of unified audio models from independently trained encoders.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2506.11403v1",
    "published_date": "2025-06-13 02:04:33 UTC",
    "updated_date": "2025-06-13 02:04:33 UTC"
  },
  {
    "arxiv_id": "2506.11402v2",
    "title": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model",
    "authors": [
      "Marcel Mateos Salles",
      "Praney Goyal",
      "Pradyut Sekhsaria",
      "Hai Huang",
      "Randall Balestriero"
    ],
    "abstract": "Large Language Models (LLMs) are commonly finetuned for a variety of use cases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA) -- known to provide strong performance at low resource costs. In this study, we demonstrate that LoRA actually opens the door to short-cut vulnerabilities -- and the more resource efficient is the LoRA setup, the more vulnerable will be the finetuned model to aggressive attacks. To measure that vulnerability, we introduce Seamless Spurious Token Injection (SSTI), where we find that LoRA exclusively focuses on even just a single token that is spuriously correlated with downstream labels. In short, injection of that spurious token during finetuning ensure that the model's prediction at test-time can be manipulated on-demand. We conducted experiments across model families and datasets to evaluate the impact of SSTI during LoRA finetuning while providing possible mitigations. Our experiments conclude that none of the existing checkers and preprocessors can sanitize a dataset raising new concerns for data quality and AI safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages, 17 figures, 26 tables. Submitted for publication. for associated blog post, see https://pradyut3501.github.io/lora-spur-corr/",
    "pdf_url": "https://arxiv.org/pdf/2506.11402v2",
    "published_date": "2025-06-13 02:02:57 UTC",
    "updated_date": "2025-10-01 02:16:42 UTC"
  },
  {
    "arxiv_id": "2506.11394v1",
    "title": "Dynamic Double Space Tower",
    "authors": [
      "Weikai Sun",
      "Shijie Song",
      "Han Wang"
    ],
    "abstract": "The Visual Question Answering (VQA) task requires the simultaneous understanding of image content and question semantics. However, existing methods often have difficulty handling complex reasoning scenarios due to insufficient cross-modal interaction and capturing the entity spatial relationships in the image.\\cite{huang2023adaptive}\\cite{liu2021comparing}\\cite{guibas2021adaptive}\\cite{zhang2022vsa}We studied a brand-new approach to replace the attention mechanism in order to enhance the reasoning ability of the model and its understanding of spatial relationships.Specifically, we propose a dynamic bidirectional spatial tower, which is divided into four layers to observe the image according to the principle of human gestalt vision. This naturally provides a powerful structural prior for the spatial organization between entities, enabling the model to no longer blindly search for relationships between pixels but make judgments based on more meaningful perceptual units. Change from \"seeing images\" to \"perceiving and organizing image content\".A large number of experiments have shown that our module can be used in any other multimodal model and achieve advanced results, demonstrating its potential in spatial relationship processing.Meanwhile, the multimodal visual question-answering model July trained by our method has achieved state-of-the-art results with only 3B parameters, especially on the question-answering dataset of spatial relations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11394v1",
    "published_date": "2025-06-13 01:27:45 UTC",
    "updated_date": "2025-06-13 01:27:45 UTC"
  },
  {
    "arxiv_id": "2506.11381v1",
    "title": "A Variational Approach for Mitigating Entity Bias in Relation Extraction",
    "authors": [
      "Samuel Mensah",
      "Elena Kochkina",
      "Jabez Magomere",
      "Joy Prakash Sain",
      "Simerjot Kaur",
      "Charese Smiley"
    ],
    "abstract": "Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2506.11381v1",
    "published_date": "2025-06-13 01:03:42 UTC",
    "updated_date": "2025-06-13 01:03:42 UTC"
  },
  {
    "arxiv_id": "2506.11380v1",
    "title": "Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation",
    "authors": [
      "Xiaoxin Lu",
      "Ranran Haoran Zhang",
      "Yusen Zhang",
      "Rui Zhang"
    ],
    "abstract": "People get informed of a daily task plan through diverse media involving both texts and images. However, most prior research only focuses on LLM's capability of textual plan generation. The potential of large-scale models in providing text-image plans remains understudied. Generating high-quality text-image plans faces two main challenges: ensuring consistent alignment between two modalities and keeping coherence among visual steps. To address these challenges, we propose a novel framework that generates and refines text-image plans step-by-step. At each iteration, our framework (1) drafts the next textual step based on the prediction history; (2) edits the last visual step to obtain the next one; (3) extracts PDDL-like visual information; and (4) refines the draft with the extracted visual information. The textual and visual step produced in stage (4) and (2) will then serve as inputs for the next iteration. Our approach offers a plug-and-play improvement to various backbone models, such as Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our approach, we collect a new benchmark consisting of 1,100 tasks and their text-image pair solutions covering 11 daily topics. We also design and validate a new set of metrics to evaluate the multimodal consistency and coherence in text-image plans. Extensive experiment results show the effectiveness of our approach on a range of backbone models against competitive baselines. Our code and data are available at https://github.com/psunlpgroup/MPlanner.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 10 figures; Accepted to ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2506.11380v1",
    "published_date": "2025-06-13 01:03:29 UTC",
    "updated_date": "2025-06-13 01:03:29 UTC"
  },
  {
    "arxiv_id": "2507.00012v1",
    "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information",
    "authors": [
      "Linfeng Ye",
      "Shayan Mohajer Hamidi",
      "En-hui Yang"
    ],
    "abstract": "A deep neural network (DNN) is said to be undistillable if, when used as a black-box input-output teacher, it cannot be distilled through knowledge distillation (KD). In this case, the distilled student (referred to as the knockoff student) does not outperform a student trained independently with label smoothing (LS student) in terms of prediction accuracy. To protect intellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is first observed that an undistillable DNN may have the trait that each cluster of its output probability distributions in response to all sample instances with the same label should be highly concentrated to the extent that each cluster corresponding to each label should ideally collapse into one probability distribution. Based on this observation and by measuring the concentration of each cluster in terms of conditional mutual information (CMI), a new training method called CMI minimized (CMIM) method is proposed, which trains a DNN by jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all temperature scaled clusters across the entire temperature spectrum. The resulting CMIM model is shown, by extensive experiments, to be undistillable by all tested KD methods existing in the literature. That is, the knockoff students distilled by these KD methods from the CMIM model underperform the respective LS students. In addition, the CMIM model is also shown to performs better than the model trained with the CE loss alone in terms of their own prediction accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 6 figures, Transactions on Machine Learning Research",
    "pdf_url": "https://arxiv.org/pdf/2507.00012v1",
    "published_date": "2025-06-13 00:56:29 UTC",
    "updated_date": "2025-06-13 00:56:29 UTC"
  },
  {
    "arxiv_id": "2506.11376v1",
    "title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning",
    "authors": [
      "Liying Wang",
      "Ph. D.",
      "Daffodil Carrington",
      "M. S.",
      "Daniil Filienko",
      "M. S.",
      "Caroline El Jazmi",
      "M. S.",
      "Serena Jinchen Xie",
      "M. S.",
      "Martine De Cock",
      "Ph. D.",
      "Sarah Iribarren",
      "Ph. D.",
      "Weichao Yuwen",
      "Ph. D"
    ],
    "abstract": "Family caregivers often face substantial mental health challenges due to their multifaceted roles and limited resources. This study explored the potential of a large language model (LLM)-powered conversational agent to deliver evidence-based mental health support for caregivers, specifically Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted with 28 caregivers interacting with four LLM configurations to evaluate empathy and therapeutic alliance. The best-performing models incorporated Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques, alongside clinician-curated examples. The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants valued the model's ability to validate emotions, explore unexpressed feelings, and provide actionable strategies. However, balancing thorough assessment with efficient advice delivery remains a challenge. This work highlights the potential of LLMs in delivering empathetic and tailored support for family caregivers.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11376v1",
    "published_date": "2025-06-13 00:47:57 UTC",
    "updated_date": "2025-06-13 00:47:57 UTC"
  },
  {
    "arxiv_id": "2506.11375v2",
    "title": "Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables",
    "authors": [
      "Yitong Zhou",
      "Mingyue Cheng",
      "Qingyang Mao",
      "Yucong Luo",
      "Qi Liu",
      "Yupeng Li",
      "Xiaohan Zhang",
      "Deguang Liu",
      "Xin Li",
      "Enhong Chen"
    ],
    "abstract": "With the widespread application of multimodal large language models in scientific intelligence, there is an urgent need for more challenging evaluation benchmarks to assess their ability to understand complex scientific data. Scientific tables, as core carriers of knowledge representation, combine text, symbols, and graphics, forming a typical multimodal reasoning scenario. However, existing benchmarks are mostly focused on general domains, failing to reflect the unique structural complexity and domain-specific semantics inherent in scientific research. Chemical tables are particularly representative: they intertwine structured variables such as reagents, conditions, and yields with visual symbols like molecular structures and chemical formulas, posing significant challenges to models in cross-modal alignment and semantic parsing. To address this, we propose ChemTable-a large scale benchmark of chemical tables constructed from real-world literature, containing expert-annotated cell layouts, logical structures, and domain-specific labels. It supports two core tasks: (1) table recognition (structure and content extraction); and (2) table understanding (descriptive and reasoning-based question answering). Evaluation on ChemTable shows that while mainstream multimodal models perform reasonably well in layout parsing, they still face significant limitations when handling critical elements such as molecular structures and symbolic conventions. Closed-source models lead overall but still fall short of human-level performance. This work provides a realistic testing platform for evaluating scientific multimodal understanding, revealing the current bottlenecks in domain-specific reasoning and advancing the development of intelligent systems for scientific research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11375v2",
    "published_date": "2025-06-13 00:45:41 UTC",
    "updated_date": "2025-12-11 02:34:29 UTC"
  }
]
[
  {
    "arxiv_id": "2501.06205v2",
    "title": "Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of Automated Defense Vehicles",
    "authors": [
      "Murat Arda Onsu",
      "Poonam Lohan",
      "Burak Kantarci"
    ],
    "abstract": "The evolution of Artificial Intelligence (AI) and its subset Deep Learning\n(DL), has profoundly impacted numerous domains, including autonomous driving.\nThe integration of autonomous driving in military settings reduces human\ncasualties and enables precise and safe execution of missions in hazardous\nenvironments while allowing for reliable logistics support without the risks\nassociated with fatigue-related errors. However, relying on autonomous driving\nsolely requires an advanced decision-making model that is adaptable and optimum\nin any situation. Considering the presence of numerous interconnected\nautonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency\nCommunication (URLLC) is vital for ensuring seamless coordination, real-time\ndata exchange, and instantaneous response to dynamic driving environments. The\nadvent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV)\nconcept within the realm of Internet of Military Defense Things (IoMDT) by\nenabling robust connectivity, crucial for real-time data exchange, advanced\nnavigation, and enhanced safety features through IoADV interactions. On the\nother hand, a critical advancement in this space is using pre-trained\nGenerative Large Language Models (LLMs) for decision-making and communication\noptimization for autonomous driving. Hence, this work presents opportunities\nand challenges with a vision of realizing the full potential of these\ntechnologies in critical defense applications, especially through the\nadvancement of IoADV and its role in enhancing autonomous military operations.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NI",
    "comment": "8 pages, 5 figures, accepted to IEEE Internet of Things Magazine",
    "pdf_url": "http://arxiv.org/pdf/2501.06205v2",
    "published_date": "2024-12-28 23:07:25 UTC",
    "updated_date": "2025-02-23 02:37:15 UTC"
  },
  {
    "arxiv_id": "2412.20292v1",
    "title": "An analytic theory of creativity in convolutional diffusion models",
    "authors": [
      "Mason Kamb",
      "Surya Ganguli"
    ],
    "abstract": "We obtain the first analytic, interpretable and predictive theory of\ncreativity in convolutional diffusion models. Indeed, score-based diffusion\nmodels can generate highly creative images that lie far from their training\ndata. But optimal score-matching theory suggests that these models should only\nbe able to produce memorized training examples. To reconcile this\ntheory-experiment gap, we identify two simple inductive biases, locality and\nequivariance, that: (1) induce a form of combinatorial creativity by preventing\noptimal score-matching; (2) result in a fully analytic, completely\nmechanistically interpretable, equivariant local score (ELS) machine that, (3)\nwithout any training can quantitatively predict the outputs of trained\nconvolution only diffusion models (like ResNets and UNets) with high accuracy\n(median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our\nELS machine reveals a locally consistent patch mosaic model of creativity, in\nwhich diffusion models create exponentially many novel images by mixing and\nmatching different local training set patches in different image locations. Our\ntheory also partially predicts the outputs of pre-trained self-attention\nenabled UNets (median $r^2 \\sim 0.75$ on CIFAR10), revealing an intriguing role\nfor attention in carving out semantic coherence from local patch mosaics.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "q-bio.NC",
      "stat.ML",
      "I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20292v1",
    "published_date": "2024-12-28 22:33:29 UTC",
    "updated_date": "2024-12-28 22:33:29 UTC"
  },
  {
    "arxiv_id": "2412.20290v1",
    "title": "Transformer-Based Contrastive Meta-Learning For Low-Resource Generalizable Activity Recognition",
    "authors": [
      "Junyao Wang",
      "Mohammad Abdullah Al Faruque"
    ],
    "abstract": "Deep learning has been widely adopted for human activity recognition (HAR)\nwhile generalizing a trained model across diverse users and scenarios remains\nchallenging due to distribution shifts. The inherent low-resource challenge in\nHAR, i.e., collecting and labeling adequate human-involved data can be\nprohibitively costly, further raising the difficulty of tackling DS. We propose\nTACO, a novel transformer-based contrastive meta-learning approach for\ngeneralizable HAR. TACO addresses DS by synthesizing virtual target domains in\ntraining with explicit consideration of model generalizability. Additionally,\nwe extract expressive feature with the attention mechanism of Transformer and\nincorporate the supervised contrastive loss function within our\nmeta-optimization to enhance representation learning. Our evaluation\ndemonstrates that TACO achieves notably better performance across various\nlow-resource DS scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20290v1",
    "published_date": "2024-12-28 21:57:12 UTC",
    "updated_date": "2024-12-28 21:57:12 UTC"
  },
  {
    "arxiv_id": "2501.10395v1",
    "title": "Towards General Purpose Robots at Scale: Lifelong Learning and Learning to Use Memory",
    "authors": [
      "William Yue"
    ],
    "abstract": "The widespread success of artificial intelligence in fields like natural\nlanguage processing and computer vision has not yet fully transferred to\nrobotics, where progress is hindered by the lack of large-scale training data\nand the complexity of real-world tasks. To address this, many robot learning\nresearchers are pushing to get robots deployed at scale in everyday\nunstructured environments like our homes to initiate a data flywheel. While\ncurrent robot learning systems are effective for certain short-horizon tasks,\nthey are not designed to autonomously operate over long time horizons in\nunstructured environments. This thesis focuses on addressing two key challenges\nfor robots operating over long time horizons: memory and lifelong learning.\n  We propose two novel methods to advance these capabilities. First, we\nintroduce t-DGR, a trajectory-based deep generative replay method that achieves\nstate-of-the-art performance on Continual World benchmarks, advancing lifelong\nlearning. Second, we develop a framework that leverages human demonstrations to\nteach agents effective memory utilization, improving learning efficiency and\nsuccess rates on Memory Gym tasks. Finally, we discuss future directions for\nachieving the lifelong learning and memory capabilities necessary for robots to\nfunction at scale in real-world settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2401.02576,\n  arXiv:2411.07954",
    "pdf_url": "http://arxiv.org/pdf/2501.10395v1",
    "published_date": "2024-12-28 21:13:48 UTC",
    "updated_date": "2024-12-28 21:13:48 UTC"
  },
  {
    "arxiv_id": "2412.20271v1",
    "title": "High-fidelity social learning via shared episodic memories enhances collaborative foraging through mnemonic convergence",
    "authors": [
      "Ismael T. Freire",
      "Paul Verschure"
    ],
    "abstract": "Social learning, a cornerstone of cultural evolution, enables individuals to\nacquire knowledge by observing and imitating others. At the heart of its\nefficacy lies episodic memory, which encodes specific behavioral sequences to\nfacilitate learning and decision-making. This study explores the interrelation\nbetween episodic memory and social learning in collective foraging. Using\nSequential Episodic Control (SEC) agents capable of sharing complete behavioral\nsequences stored in episodic memory, we investigate how variations in the\nfrequency and fidelity of social learning influence collaborative foraging\nperformance. Furthermore, we analyze the effects of social learning on the\ncontent and distribution of episodic memories across the group. High-fidelity\nsocial learning is shown to consistently enhance resource collection efficiency\nand distribution, with benefits sustained across memory lengths. In contrast,\nlow-fidelity learning fails to outperform nonsocial learning, spreading diverse\nbut ineffective mnemonic patterns. Novel analyses using mnemonic metrics reveal\nthat high-fidelity social learning also fosters mnemonic group alignment and\nequitable resource distribution, while low-fidelity conditions increase\nmnemonic diversity without translating to performance gains. Additionally, we\nidentify an optimal range for episodic memory length in this task, beyond which\nperformance plateaus. These findings underscore the critical effects of social\nlearning on mnemonic group alignment and distribution and highlight the\npotential of neurocomputational models to probe the cognitive mechanisms\ndriving cultural evolution.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20271v1",
    "published_date": "2024-12-28 20:55:38 UTC",
    "updated_date": "2024-12-28 20:55:38 UTC"
  },
  {
    "arxiv_id": "2501.00059v1",
    "title": "Large Language Models for Mathematical Analysis",
    "authors": [
      "Ziye Chen",
      "Hao Qi"
    ],
    "abstract": "Mathematical problem-solving is a key field in artificial intelligence (AI)\nand a critical benchmark for evaluating the capabilities of large language\nmodels (LLMs). While extensive research has focused on mathematical\nproblem-solving, most existing work and datasets concentrate on computational\ntasks, leaving gaps in areas like mathematical analysis, which demands rigorous\nproofs and formal reasoning. We developed the DEMI-MathAnalysis dataset,\ncomprising proof-based problems from mathematical analysis topics such as\nSequences and Limits, Infinite Series, and Convex Functions. We also designed a\nguiding framework to rigorously enhance LLMs' ability to solve these problems.\nThrough fine-tuning LLMs on this dataset and employing our framework, we\nobserved significant improvements in their capability to generate logical,\ncomplete, and elegant proofs. This work addresses critical gaps in mathematical\nreasoning and contributes to advancing trustworthy AI capable of handling\nformalized mathematical language. The code is publicly accessible at LLMs for\nMathematical Analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00059v1",
    "published_date": "2024-12-28 20:37:55 UTC",
    "updated_date": "2024-12-28 20:37:55 UTC"
  },
  {
    "arxiv_id": "2412.20231v2",
    "title": "How To Think About End-To-End Encryption and AI: Training, Processing, Disclosure, and Consent",
    "authors": [
      "Mallory Knodel",
      "Andrés Fábrega",
      "Daniella Ferrari",
      "Jacob Leiken",
      "Betty Li Hou",
      "Derek Yen",
      "Sam de Alfaro",
      "Kyunghyun Cho",
      "Sunoo Park"
    ],
    "abstract": "End-to-end encryption (E2EE) has become the gold standard for securing\ncommunications, bringing strong confidentiality and privacy guarantees to\nbillions of users worldwide. However, the current push towards widespread\nintegration of artificial intelligence (AI) models, including in E2EE systems,\nraises some serious security concerns. This work performs a critical\nexamination of the (in)compatibility of AI models and E2EE applications. We\nexplore this on two fronts: (1) the integration of AI \"assistants\" within E2EE\napplications, and (2) the use of E2EE data for training AI models. We analyze\nthe potential security implications of each, and identify conflicts with the\nsecurity guarantees of E2EE. Then, we analyze legal implications of integrating\nAI models in E2EE applications, given how AI integration can undermine the\nconfidentiality that E2EE promises. Finally, we offer a list of detailed\nrecommendations based on our technical and legal analyses, including: technical\ndesign choices that must be prioritized to uphold E2EE security; how service\nproviders must accurately represent E2EE security; and best practices for the\ndefault behavior of AI features and for requesting user consent. We hope this\npaper catalyzes an informed conversation on the tensions that arise between the\nbrisk deployment of AI and the security offered by E2EE, and guides the\nresponsible development of new AI features.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20231v2",
    "published_date": "2024-12-28 17:59:21 UTC",
    "updated_date": "2025-03-22 18:40:59 UTC"
  },
  {
    "arxiv_id": "2412.20230v1",
    "title": "Leveraging Large Language Models for Enhancing Autonomous Vehicle Perception",
    "authors": [
      "Athanasios Karagounis"
    ],
    "abstract": "Autonomous vehicles (AVs) rely on sophisticated perception systems to\ninterpret their surroundings, a cornerstone for safe navigation and\ndecision-making. The integration of Large Language Models (LLMs) into AV\nperception frameworks offers an innovative approach to address challenges in\ndynamic environments, sensor fusion, and contextual reasoning. This paper\npresents a novel framework for incorporating LLMs into AV perception, enabling\nadvanced contextual understanding, seamless sensor integration, and enhanced\ndecision support. Experimental results demonstrate that LLMs significantly\nimprove the accuracy and reliability of AV perception systems, paving the way\nfor safer and more intelligent autonomous driving technologies. By expanding\nthe scope of perception beyond traditional methods, LLMs contribute to creating\na more adaptive and human-centric driving ecosystem, making autonomous vehicles\nmore reliable and transparent in their operations. These advancements redefine\nthe relationship between human drivers and autonomous systems, fostering trust\nthrough enhanced understanding and personalized decision-making. Furthermore,\nby integrating memory modules and adaptive learning mechanisms, LLMs introduce\ncontinuous improvement in AV perception, enabling vehicles to evolve with time\nand adapt to changing environments and user preferences.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.20230v1",
    "published_date": "2024-12-28 17:58:44 UTC",
    "updated_date": "2024-12-28 17:58:44 UTC"
  },
  {
    "arxiv_id": "2501.14769v1",
    "title": "A survey on pioneering metaheuristic algorithms between 2019 and 2024",
    "authors": [
      "Tansel Dokeroglu",
      "Deniz Canturk",
      "Tayfun Kucukyilmaz"
    ],
    "abstract": "This review examines over 150 new metaheuristics of the last six years\n(between 2019 and 2024), underscoring their profound influence and performance.\nOver the past three decades, more than 500 new metaheuristic algorithms have\nbeen proposed, with no slowdown in sight. An overwhelming abundance that\ncomplicates the process of selecting and assessing the most effective solutions\nfor complex optimization challenges. Our evaluation centers on pivotal\ncriteria, including annual citation metrics, the breadth of the addressed\nproblem types, source code availability, user friendly parameter\nconfigurations, innovative mechanisms and operators, and approaches designed to\nmitigate traditional metaheuristic issues such as stagnation and premature\nconvergence. We further explore recent high impact applications of the past six\nyears' most influential 23 metahueristic algorithms, shedding light on their\nadvantages and limitations, while identifying challenges and potential avenues\nfor future research.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "A.1; F.2; I.2"
    ],
    "primary_category": "cs.NE",
    "comment": "73 pages, 3 Tables, 12 Figures,on Metaheuristic and Evolutionary\n  Algorithms",
    "pdf_url": "http://arxiv.org/pdf/2501.14769v1",
    "published_date": "2024-12-28 17:41:57 UTC",
    "updated_date": "2024-12-28 17:41:57 UTC"
  },
  {
    "arxiv_id": "2412.20213v1",
    "title": "Decoding Emotion: Speech Perception Patterns in Individuals with Self-reported Depression",
    "authors": [
      "Guneesh Vats",
      "Priyanka Srivastava",
      "Chiranjeevi Yarra"
    ],
    "abstract": "The current study examines the relationship between self-reported depression\nand the perception of affective speech within the Indian population. PANAS and\nPHQ-9 were used to assess current mood and depression, respectively.\nParticipants' emotional reactivity was recorded on a valence and arousal scale\nagainst the affective speech audio presented in a sequence. No significant\ndifferences between the depression and no-depression groups were observed for\nany of the emotional stimuli, except the audio file depicting neutral emotion.\nSignificantly higher PANAS scores by the depression than the no-depression\ngroup indicate the impact of pre-disposed mood on the current mood status.\nContrary to previous findings, this study did not observe reduced positive\nemotional reactivity by the depression group. However, the results demonstrated\nconsistency in emotional reactivity for speech stimuli depicting sadness and\nanger across all measures of emotion perception.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20213v1",
    "published_date": "2024-12-28 16:54:25 UTC",
    "updated_date": "2024-12-28 16:54:25 UTC"
  },
  {
    "arxiv_id": "2412.20212v1",
    "title": "Building a Rich Dataset to Empower the Persian Question Answering Systems",
    "authors": [
      "Mohsen Yazdinejad",
      "Marjan Kaedi"
    ],
    "abstract": "Question answering systems provide short, precise, and specific answers to\nquestions. So far, many robust question answering systems have been developed\nfor English, while some languages with fewer resources, like Persian, have few\nnumbers of standard dataset. In this study, a comprehensive open-domain dataset\nis presented for Persian. This dataset is called NextQuAD and has 7,515\ncontexts, including 23,918 questions and answers. Then, a BERT-based question\nanswering model has been applied to this dataset using two pre-trained language\nmodels, including ParsBERT and XLM-RoBERTa. The results of these two models\nhave been ensembled using mean logits. Evaluation on the development set shows\n0.95 Exact Match (EM) and 0.97 Fl_score. Also, to compare the NextQuAD with\nother Persian datasets, our trained model on the NextQuAD, is evaluated on two\nother datasets named PersianQA and ParSQuAD. Comparisons show that the proposed\nmodel increased EM by 0.39 and 0.14 respectively in PersianQA and\nParSQuAD-manual, while a slight EM decline of 0.007 happened in\nParSQuAD-automatic.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20212v1",
    "published_date": "2024-12-28 16:53:25 UTC",
    "updated_date": "2024-12-28 16:53:25 UTC"
  },
  {
    "arxiv_id": "2412.20210v2",
    "title": "Towards Real-Time 2D Mapping: Harnessing Drones, AI, and Computer Vision for Advanced Insights",
    "authors": [
      "Bharath Kumar Agnur"
    ],
    "abstract": "This paper presents an advanced mapping system that combines drone imagery\nwith machine learning and computer vision to overcome challenges in speed,\naccuracy, and adaptability across diverse terrains. By automating processes\nlike feature detection, image matching, and stitching, the system produces\nseamless, high-resolution maps with minimal latency, offering strategic\nadvantages in defense operations. Developed in Python, the system utilizes\nOpenCV for image processing, NumPy for efficient computations, and\nConcurrent[dot]futures for parallel execution. ORB (Oriented FAST and Rotated\nBRIEF) is employed for feature detection, while FLANN (Fast Library for\nApproximate Nearest Neighbors) ensures accurate keypoint matching. Homography\ntransformations align overlapping images, resulting in distortion-free maps in\nreal time. This automation eliminates manual intervention, enabling live\nupdates essential in rapidly changing environments. Designed for versatility,\nthe system performs reliably under various lighting conditions and rugged\nterrains, making it highly suitable for aerospace and defense applications.\nTesting has shown notable improvements in processing speed and accuracy\ncompared to conventional methods, enhancing situational awareness and informed\ndecision-making. This scalable solution leverages cutting-edge technologies to\nprovide actionable, reliable data for mission-critical operations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2412.20210v2",
    "published_date": "2024-12-28 16:47:18 UTC",
    "updated_date": "2024-12-31 15:09:14 UTC"
  },
  {
    "arxiv_id": "2412.20201v1",
    "title": "Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems",
    "authors": [
      "Wen-Dong Jiang",
      "Chih-Yung Chang",
      "Hsiang-Chuan Chang",
      "Ji-Yuan Chen",
      "Diptendu Sinha Roy"
    ],
    "abstract": "Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak\nsupervision learning to identify anomalies, a critical task for smart city\nmonitoring. However, existing multimodal approaches often fail to meet the\nreal-time and interpretability requirements of edge devices due to their\ncomplexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly\nDetection System), which leverages knowledge distillation and cross-modal\ncontrastive learning to enable efficient, accurate, and interpretable anomaly\ndetection on edge devices.TCVADS operates in two stages: coarse-grained rapid\nclassification and fine-grained detailed analysis. In the first stage, TCVADS\nextracts features from video frames and inputs them into a time series analysis\nmodule, which acts as the teacher model. Insights are then transferred via\nknowledge distillation to a simplified convolutional network (student model)\nfor binary classification. Upon detecting an anomaly, the second stage is\ntriggered, employing a fine-grained multi-class classification model. This\nstage uses CLIP for cross-modal contrastive learning with text and images,\nenhancing interpretability and achieving refined classification through\nspecially designed triplet textual relationships. Experimental results\ndemonstrate that TCVADS significantly outperforms existing methods in model\nperformance, detection efficiency, and interpretability, offering valuable\ncontributions to smart city monitoring applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE TETC-CS (Under review)",
    "pdf_url": "http://arxiv.org/pdf/2412.20201v1",
    "published_date": "2024-12-28 16:24:35 UTC",
    "updated_date": "2024-12-28 16:24:35 UTC"
  },
  {
    "arxiv_id": "2412.20200v1",
    "title": "Federated Unlearning with Gradient Descent and Conflict Mitigation",
    "authors": [
      "Zibin Pan",
      "Zhichao Wang",
      "Chi Li",
      "Kaiyan Zheng",
      "Boqi Wang",
      "Xiaoying Tang",
      "Junhua Zhao"
    ],
    "abstract": "Federated Learning (FL) has received much attention in recent years. However,\nalthough clients are not required to share their data in FL, the global model\nitself can implicitly remember clients' local data. Therefore, it's necessary\nto effectively remove the target client's data from the FL global model to ease\nthe risk of privacy leakage and implement ``the right to be forgotten\".\nFederated Unlearning (FU) has been considered a promising way to remove data\nwithout full retraining. But the model utility easily suffers significant\nreduction during unlearning due to the gradient conflicts. Furthermore, when\nconducting the post-training to recover the model utility, the model is prone\nto move back and revert what has already been unlearned. To address these\nissues, we propose Federated Unlearning with Orthogonal Steepest Descent\n(FedOSD). We first design an unlearning Cross-Entropy loss to overcome the\nconvergence issue of the gradient ascent. A steepest descent direction for\nunlearning is then calculated in the condition of being non-conflicting with\nother clients' gradients and closest to the target client's gradient. This\nbenefits to efficiently unlearn and mitigate the model utility reduction. After\nunlearning, we recover the model utility by maintaining the achievement of\nunlearning. Finally, extensive experiments in several FL scenarios verify that\nFedOSD outperforms the SOTA FU algorithms in terms of unlearning and model\nutility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.20200v1",
    "published_date": "2024-12-28 16:23:10 UTC",
    "updated_date": "2024-12-28 16:23:10 UTC"
  },
  {
    "arxiv_id": "2412.20195v1",
    "title": "Lower bounds on transformers with infinite precision",
    "authors": [
      "Alexander Kozachinskiy"
    ],
    "abstract": "In this note, we use the VC dimension technique to prove the first lower\nbound against one-layer softmax transformers with infinite precision. We do so\nfor two tasks: function composition, considered by Peng, Narayanan, and\nPapadimitriou, and the SUM$_2$ task, considered by Sanford, Hsu, and Telgarsky.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20195v1",
    "published_date": "2024-12-28 16:09:25 UTC",
    "updated_date": "2024-12-28 16:09:25 UTC"
  },
  {
    "arxiv_id": "2412.20193v1",
    "title": "Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action Ranker",
    "authors": [
      "Jiangdong Fan",
      "Hongcai He",
      "Paul Weng",
      "Hui Xu",
      "Jie Shao"
    ],
    "abstract": "A major bottleneck in imitation learning is the requirement of a large number\nof expert demonstrations, which can be expensive or inaccessible. Learning from\nsupplementary demonstrations without strict quality requirements has emerged as\na powerful paradigm to address this challenge. However, previous methods often\nfail to fully utilize their potential by discarding non-expert data. Our key\ninsight is that even demonstrations that fall outside the expert distribution\nbut outperform the learned policy can enhance policy performance. To utilize\nthis potential, we propose a novel approach named imitation learning via\nmeta-learning an action ranker (ILMAR). ILMAR implements weighted behavior\ncloning (weighted BC) on a limited set of expert demonstrations along with\nsupplementary demonstrations. It utilizes the functional of the advantage\nfunction to selectively integrate knowledge from the supplementary\ndemonstrations. To make more effective use of supplementary demonstrations, we\nintroduce meta-goal in ILMAR to optimize the functional of the advantage\nfunction by explicitly minimizing the distance between the current policy and\nthe expert policy. Comprehensive experiments using extensive tasks demonstrate\nthat ILMAR significantly outperforms previous methods in handling suboptimal\ndemonstrations. Code is available at https://github.com/F-GOD6/ILMAR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20193v1",
    "published_date": "2024-12-28 16:06:44 UTC",
    "updated_date": "2024-12-28 16:06:44 UTC"
  },
  {
    "arxiv_id": "2501.14768v1",
    "title": "Equation discovery framework EPDE: Towards a better equation discovery",
    "authors": [
      "Mikhail Maslyaev",
      "Alexander Hvatov"
    ],
    "abstract": "Equation discovery methods hold promise for extracting knowledge from\nphysics-related data. However, existing approaches often require substantial\nprior information that significantly reduces the amount of knowledge extracted.\nIn this paper, we enhance the EPDE algorithm -- an evolutionary\noptimization-based discovery framework. In contrast to methods like SINDy,\nwhich rely on pre-defined libraries of terms and linearities, our approach\ngenerates terms using fundamental building blocks such as elementary functions\nand individual differentials. Within evolutionary optimization, we may improve\nthe computation of the fitness function as is done in gradient methods and\nenhance the optimization algorithm itself. By incorporating multi-objective\noptimization, we effectively explore the search space, yielding more robust\nequation extraction, even when dealing with complex experimental data. We\nvalidate our algorithm's noise resilience and overall performance by comparing\nits results with those from the state-of-the-art equation discovery framework\nSINDy.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14768v1",
    "published_date": "2024-12-28 15:58:44 UTC",
    "updated_date": "2024-12-28 15:58:44 UTC"
  },
  {
    "arxiv_id": "2412.20170v1",
    "title": "Real-time Calibration Model for Low-cost Sensor in Fine-grained Time series",
    "authors": [
      "Seokho Ahn",
      "Hyungjin Kim",
      "Sungbok Shin",
      "Young-Duk Seo"
    ],
    "abstract": "Precise measurements from sensors are crucial, but data is usually collected\nfrom low-cost, low-tech systems, which are often inaccurate. Thus, they require\nfurther calibrations. To that end, we first identify three requirements for\neffective calibration under practical low-tech sensor conditions. Based on the\nrequirements, we develop a model called TESLA, Transformer for effective sensor\ncalibration utilizing logarithmic-binned attention. TESLA uses a\nhigh-performance deep learning model, Transformers, to calibrate and capture\nnon-linear components. At its core, it employs logarithmic binning to minimize\nattention complexity. TESLA achieves consistent real-time calibration, even\nwith longer sequences and finer-grained time series in hardware-constrained\nsystems. Experiments show that TESLA outperforms existing novel deep learning\nand newly crafted linear models in accuracy, calibration speed, and energy\nefficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20170v1",
    "published_date": "2024-12-28 14:58:46 UTC",
    "updated_date": "2024-12-28 14:58:46 UTC"
  },
  {
    "arxiv_id": "2412.20166v2",
    "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
    "authors": [
      "Hyucksung Kwon",
      "Kyungmo Koo",
      "Janghyeon Kim",
      "Woongkyu Lee",
      "Minjae Lee",
      "Hyungdeok Lee",
      "Yousub Jung",
      "Jaehan Park",
      "Yosub Song",
      "Byeongsu Yang",
      "Haerang Choi",
      "Guhyun Kim",
      "Jongsoon Won",
      "Woojae Shin",
      "Changhyun Kim",
      "Gyeongcheol Shin",
      "Yongkee Kwon",
      "Ilkon Kim",
      "Euicheol Lim",
      "John Kim",
      "Jungwook Choi"
    ],
    "abstract": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "15 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20166v2",
    "published_date": "2024-12-28 14:38:16 UTC",
    "updated_date": "2025-01-15 01:34:46 UTC"
  },
  {
    "arxiv_id": "2412.20164v1",
    "title": "StyleAutoEncoder for manipulating image attributes using pre-trained StyleGAN",
    "authors": [
      "Andrzej Bedychaj",
      "Jacek Tabor",
      "Marek Śmieja"
    ],
    "abstract": "Deep conditional generative models are excellent tools for creating\nhigh-quality images and editing their attributes. However, training modern\ngenerative models from scratch is very expensive and requires large\ncomputational resources. In this paper, we introduce StyleAutoEncoder\n(StyleAE), a lightweight AutoEncoder module, which works as a plugin for\npre-trained generative models and allows for manipulating the requested\nattributes of images. The proposed method offers a cost-effective solution for\ntraining deep generative models with limited computational resources, making it\na promising technique for a wide range of applications. We evaluate\nStyleAutoEncoder by combining it with StyleGAN, which is currently one of the\ntop generative models. Our experiments demonstrate that StyleAutoEncoder is at\nleast as effective in manipulating image attributes as the state-of-the-art\nalgorithms based on invertible normalizing flows. However, it is simpler,\nfaster, and gives more freedom in designing neural",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20164v1",
    "published_date": "2024-12-28 14:30:48 UTC",
    "updated_date": "2024-12-28 14:30:48 UTC"
  },
  {
    "arxiv_id": "2412.20163v3",
    "title": "Topic-Aware Knowledge Graph with Large Language Models for Interoperability in Recommender Systems",
    "authors": [
      "Minhye Jeon",
      "Seokho Ahn",
      "Young-Duk Seo"
    ],
    "abstract": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20163v3",
    "published_date": "2024-12-28 14:27:45 UTC",
    "updated_date": "2025-02-12 16:49:56 UTC"
  },
  {
    "arxiv_id": "2412.20155v1",
    "title": "Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting",
    "authors": [
      "Wooseok Han",
      "Minki Kang",
      "Changhun Kim",
      "Eunho Yang"
    ],
    "abstract": "Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable\nattention due to its broad range of applications, such as personalized voice\nassistant services. While several approaches have been proposed, they often\nexhibit high sensitivity to either the quantity or the quality of target speech\nsamples. To address these limitations, we introduce Stable-TTS, a novel\nspeaker-adaptive TTS framework that leverages a small subset of a high-quality\npre-training dataset, referred to as prior samples. Specifically, Stable-TTS\nachieves prosody consistency by leveraging the high-quality prosody of prior\nsamples, while effectively capturing the timbre of the target speaker.\nAdditionally, it employs a prior-preservation loss during fine-tuning to\nmaintain the synthesis ability for prior samples to prevent overfitting on\ntarget samples. Extensive experiments demonstrate the effectiveness of\nStable-TTS even under limited amounts of and noisy target speech samples.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20155v1",
    "published_date": "2024-12-28 13:54:30 UTC",
    "updated_date": "2024-12-28 13:54:30 UTC"
  },
  {
    "arxiv_id": "2501.00057v2",
    "title": "VisTabNet: Adapting Vision Transformers for Tabular Data",
    "authors": [
      "Witold Wydmański",
      "Ulvi Movsum-zada",
      "Jacek Tabor",
      "Marek Śmieja"
    ],
    "abstract": "Although deep learning models have had great success in natural language\nprocessing and computer vision, we do not observe comparable improvements in\nthe case of tabular data, which is still the most common data type used in\nbiological, industrial and financial applications. In particular, it is\nchallenging to transfer large-scale pre-trained models to downstream tasks\ndefined on small tabular datasets. To address this, we propose VisTabNet -- a\ncross-modal transfer learning method, which allows for adapting Vision\nTransformer (ViT) with pre-trained weights to process tabular data. By\nprojecting tabular inputs to patch embeddings acceptable by ViT, we can\ndirectly apply a pre-trained Transformer Encoder to tabular inputs. This\napproach eliminates the conceptual cost of designing a suitable architecture\nfor processing tabular data, while reducing the computational cost of training\nthe model from scratch. Experimental results on multiple small tabular datasets\n(less than 1k samples) demonstrate VisTabNet's superiority, outperforming both\ntraditional ensemble methods and recent deep learning models. The proposed\nmethod goes beyond conventional transfer learning practice and shows that\npre-trained image models can be transferred to solve tabular problems,\nextending the boundaries of transfer learning. We share our example\nimplementation as a GitHub repository available at\nhttps://github.com/wwydmanski/VisTabNet.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00057v2",
    "published_date": "2024-12-28 13:40:46 UTC",
    "updated_date": "2025-04-25 12:19:39 UTC"
  },
  {
    "arxiv_id": "2501.00056v1",
    "title": "Transforming CCTV cameras into NO$_2$ sensors at city scale for adaptive policymaking",
    "authors": [
      "Mohamed R. Ibrahim",
      "Terry Lyons"
    ],
    "abstract": "Air pollution in cities, especially NO\\textsubscript{2}, is linked to\nnumerous health problems, ranging from mortality to mental health challenges\nand attention deficits in children. While cities globally have initiated\npolicies to curtail emissions, real-time monitoring remains challenging due to\nlimited environmental sensors and their inconsistent distribution. This gap\nhinders the creation of adaptive urban policies that respond to the sequence of\nevents and daily activities affecting pollution in cities. Here, we demonstrate\nhow city CCTV cameras can act as a pseudo-NO\\textsubscript{2} sensors. Using a\npredictive graph deep model, we utilised traffic flow from London's cameras in\naddition to environmental and spatial factors, generating NO\\textsubscript{2}\npredictions from over 133 million frames. Our analysis of London's mobility\npatterns unveiled critical spatiotemporal connections, showing how specific\ntraffic patterns affect NO\\textsubscript{2} levels, sometimes with temporal\nlags of up to 6 hours. For instance, if trucks only drive at night, their\neffects on NO\\textsubscript{2} levels are most likely to be seen in the morning\nwhen people commute. These findings cast doubt on the efficacy of some of the\nurban policies currently being implemented to reduce pollution. By leveraging\nexisting camera infrastructure and our introduced methods, city planners and\npolicymakers could cost-effectively monitor and mitigate the impact of\nNO\\textsubscript{2} and other pollutants.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "43 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.00056v1",
    "published_date": "2024-12-28 13:01:44 UTC",
    "updated_date": "2024-12-28 13:01:44 UTC"
  },
  {
    "arxiv_id": "2412.20138v6",
    "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
    "authors": [
      "Yijia Xiao",
      "Edward Sun",
      "Di Luo",
      "Wei Wang"
    ],
    "abstract": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/TauricResearch.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-fin.TR",
    "comment": "Oral, Multi-Agent AI in the Real World @ AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20138v6",
    "published_date": "2024-12-28 12:54:06 UTC",
    "updated_date": "2025-04-15 19:23:27 UTC"
  },
  {
    "arxiv_id": "2412.20127v3",
    "title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine Translation Evaluation",
    "authors": [
      "Zhaopeng Feng",
      "Jiayuan Su",
      "Jiamei Zheng",
      "Jiahan Ren",
      "Yan Zhang",
      "Jian Wu",
      "Hongwei Wang",
      "Zuozhu Liu"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and data are available at https://github.com/SU-JIAYUAN/M-MAD",
    "pdf_url": "http://arxiv.org/pdf/2412.20127v3",
    "published_date": "2024-12-28 12:11:28 UTC",
    "updated_date": "2025-02-20 12:55:22 UTC"
  },
  {
    "arxiv_id": "2501.14767v1",
    "title": "Leveraging Social Media Data and Artificial Intelligence for Improving Earthquake Response Efforts",
    "authors": [
      "Kalin Kopanov",
      "Velizar Varbanov",
      "Tatiana Atanasova"
    ],
    "abstract": "The integration of social media and artificial intelligence (AI) into\ndisaster management, particularly for earthquake response, represents a\nprofound evolution in emergency management practices. In the digital age,\nreal-time information sharing has reached unprecedented levels, with social\nmedia platforms emerging as crucial communication channels during crises. This\nshift has transformed traditional, centralized emergency services into more\ndecentralized, participatory models of disaster situational awareness. Our\nstudy includes an experimental analysis of 8,900 social media interactions,\nincluding 2,920 posts and 5,980 replies on X (formerly Twitter), following a\nmagnitude 5.1 earthquake in Oklahoma on February 2, 2024. The analysis covers\ndata from the immediate aftermath and extends over the following seven days,\nillustrating the critical role of digital platforms in modern disaster\nresponse. The results demonstrate that social media platforms can be\neffectively used as real-time situational awareness tools, delivering critical\ninformation to society and authorities during emergencies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.SI",
      "I.2.7"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages, 2 figures, EnviroRisks 2024: Environmental Protection and\n  Disaster Risks, Sofia, Bulgaria",
    "pdf_url": "http://arxiv.org/pdf/2501.14767v1",
    "published_date": "2024-12-28 11:08:06 UTC",
    "updated_date": "2024-12-28 11:08:06 UTC"
  },
  {
    "arxiv_id": "2412.20104v4",
    "title": "SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis",
    "authors": [
      "Wenkun He",
      "Yun Liu",
      "Ruitao Liu",
      "Li Yi"
    ],
    "abstract": "Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20104v4",
    "published_date": "2024-12-28 10:12:12 UTC",
    "updated_date": "2025-03-27 02:17:08 UTC"
  },
  {
    "arxiv_id": "2412.20098v1",
    "title": "RFPPO: Motion Dynamic RRT based Fluid Field - PPO for Dynamic TF/TA Routing Planning",
    "authors": [
      "Rongkun Xue",
      "Jing Yang",
      "Yuyang Jiang",
      "Yiming Feng",
      "Zi Yang"
    ],
    "abstract": "Existing local dynamic route planning algorithms, when directly applied to\nterrain following/terrain avoidance, or dynamic obstacle avoidance for large\nand medium-sized fixed-wing aircraft, fail to simultaneously meet the\nrequirements of real-time performance, long-distance planning, and the dynamic\nconstraints of large and medium-sized aircraft. To deal with this issue, this\npaper proposes the Motion Dynamic RRT based Fluid Field - PPO for dynamic TF/TA\nrouting planning. Firstly, the action and state spaces of the proximal policy\ngradient algorithm are redesigned using disturbance flow fields and artificial\npotential field algorithms, establishing an aircraft dynamics model, and\ndesigning a state transition process based on this model. Additionally, a\nreward function is designed to encourage strategies for obstacle avoidance,\nterrain following, terrain avoidance, and safe flight. Experimental results on\nreal DEM data demonstrate that our algorithm can complete long-distance flight\ntasks through collision-free trajectory planning that complies with dynamic\nconstraints, without the need for prior global planning.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "2024 IEEE Intelligent Vehicles Symposium",
    "pdf_url": "http://arxiv.org/pdf/2412.20098v1",
    "published_date": "2024-12-28 09:42:02 UTC",
    "updated_date": "2024-12-28 09:42:02 UTC"
  },
  {
    "arxiv_id": "2412.20090v1",
    "title": "From Worms to Mice: Homeostasis Maybe All You Need",
    "authors": [
      "Jesus Marco de Lucas"
    ],
    "abstract": "In this brief and speculative commentary, we explore ideas inspired by neural\nnetworks in machine learning, proposing that a simple neural XOR motif,\ninvolving both excitatory and inhibitory connections, may provide the basis for\na relevant mode of plasticity in neural circuits of living organisms, with\nhomeostasis as the sole guiding principle. This XOR motif simply signals the\ndiscrepancy between incoming signals and reference signals, thereby providing a\nbasis for a loss function in learning neural circuits, and at the same time\nregulating homeostasis by halting the propagation of these incoming signals.\nThe core motif uses a 4:1 ratio of excitatory to inhibitory neurons, and\nsupports broader neural patterns such as the well-known 'winner takes all'\n(WTA) mechanism. We examined the prevalence of the XOR motif in the published\nconnectomes of various organisms with increasing complexity, and found that it\nranges from tens (in C. elegans) to millions (in several Drosophila neuropils)\nand more than tens of millions (in mouse V1 visual cortex). If validated, our\nhypothesis identifies two of the three key components in analogy to machine\nlearning models: the architecture and the loss function. And we propose that a\nrelevant type of biological neural plasticity is simply driven by a basic\ncontrol or regulatory system, which has persisted and adapted despite the\nincreasing complexity of organisms throughout evolution.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20090v1",
    "published_date": "2024-12-28 09:17:09 UTC",
    "updated_date": "2024-12-28 09:17:09 UTC"
  },
  {
    "arxiv_id": "2412.20088v1",
    "title": "An archaeological Catalog Collection Method Based on Large Vision-Language Models",
    "authors": [
      "Honglin Pang",
      "Yi Chang",
      "Tianjing Duan",
      "Xi Yang"
    ],
    "abstract": "Archaeological catalogs, containing key elements such as artifact images,\nmorphological descriptions, and excavation information, are essential for\nstudying artifact evolution and cultural inheritance. These data are widely\nscattered across publications, requiring automated collection methods. However,\nexisting Large Vision-Language Models (VLMs) and their derivative data\ncollection methods face challenges in accurate image detection and modal\nmatching when processing archaeological catalogs, making automated collection\ndifficult. To address these issues, we propose a novel archaeological catalog\ncollection method based on Large Vision-Language Models that follows an\napproach comprising three modules: document localization, block comprehension\nand block matching. Through practical data collection from the Dabagou and\nMiaozigou pottery catalogs and comparison experiments, we demonstrate the\neffectiveness of our approach, providing a reliable solution for automated\ncollection of archaeological catalogs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages,4 figures,www source track",
    "pdf_url": "http://arxiv.org/pdf/2412.20088v1",
    "published_date": "2024-12-28 09:10:41 UTC",
    "updated_date": "2024-12-28 09:10:41 UTC"
  },
  {
    "arxiv_id": "2412.20087v1",
    "title": "On the Validity of Traditional Vulnerability Scoring Systems for Adversarial Attacks against LLMs",
    "authors": [
      "Atmane Ayoub Mansour Bahar",
      "Ahmad Samer Wazan"
    ],
    "abstract": "This research investigates the effectiveness of established vulnerability\nmetrics, such as the Common Vulnerability Scoring System (CVSS), in evaluating\nattacks against Large Language Models (LLMs), with a focus on Adversarial\nAttacks (AAs). The study explores the influence of both general and specific\nmetric factors in determining vulnerability scores, providing new perspectives\non potential enhancements to these metrics.\n  This study adopts a quantitative approach, calculating and comparing the\ncoefficient of variation of vulnerability scores across 56 adversarial attacks\non LLMs. The attacks, sourced from various research papers, and obtained\nthrough online databases, were evaluated using multiple vulnerability metrics.\nScores were determined by averaging the values assessed by three distinct LLMs.\nThe results indicate that existing scoring-systems yield vulnerability scores\nwith minimal variation across different attacks, suggesting that many of the\nmetric factors are inadequate for assessing adversarial attacks on LLMs. This\nis particularly true for context-specific factors or those with predefined\nvalue sets, such as those in CVSS. These findings support the hypothesis that\ncurrent vulnerability metrics, especially those with rigid values, are limited\nin evaluating AAs on LLMs, highlighting the need for the development of more\nflexible, generalized metrics tailored to such attacks.\n  This research offers a fresh analysis of the effectiveness and applicability\nof established vulnerability metrics, particularly in the context of\nAdversarial Attacks on Large Language Models, both of which have gained\nsignificant attention in recent years. Through extensive testing and\ncalculations, the study underscores the limitations of these metrics and opens\nup new avenues for improving and refining vulnerability assessment frameworks\nspecifically tailored for LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "68T50, 68M25,",
      "I.2.7; K.4.1; G.3"
    ],
    "primary_category": "cs.CR",
    "comment": "101 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.20087v1",
    "published_date": "2024-12-28 09:08:37 UTC",
    "updated_date": "2024-12-28 09:08:37 UTC"
  },
  {
    "arxiv_id": "2412.20086v1",
    "title": "MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via Zero-Order Gradient Search",
    "authors": [
      "Zhaohui Wang",
      "Min Zhang",
      "Jingran Yang",
      "Bojie Shao",
      "Min Zhang"
    ],
    "abstract": "Deep neural networks (DNNs) have shown powerful performance in various\napplications and are increasingly being used in decision-making systems.\nHowever, concerns about fairness in DNNs always persist. Some efficient\nwhite-box fairness testing methods about individual fairness have been\nproposed. Nevertheless, the development of black-box methods has stagnated, and\nthe performance of existing methods is far behind that of white-box methods. In\nthis paper, we propose a novel black-box individual fairness testing method\ncalled Model-Agnostic Fairness Testing (MAFT). By leveraging MAFT,\npractitioners can effectively identify and address discrimination in DL models,\nregardless of the specific algorithm or architecture employed. Our approach\nadopts lightweight procedures such as gradient estimation and attribute\nperturbation rather than non-trivial procedures like symbol execution,\nrendering it significantly more scalable and applicable than existing methods.\nWe demonstrate that MAFT achieves the same effectiveness as state-of-the-art\nwhite-box methods whilst improving the applicability to large-scale networks.\nCompared to existing black-box approaches, our approach demonstrates\ndistinguished performance in discovering fairness violations w.r.t\neffectiveness (approximately 14.69 times) and efficiency (approximately 32.58\ntimes).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICSE24",
    "pdf_url": "http://arxiv.org/pdf/2412.20086v1",
    "published_date": "2024-12-28 09:07:06 UTC",
    "updated_date": "2024-12-28 09:07:06 UTC"
  },
  {
    "arxiv_id": "2412.20072v2",
    "title": "Extract Information from Hybrid Long Documents Leveraging LLMs: A Framework and Dataset",
    "authors": [
      "Chongjian Yue",
      "Xinrun Xu",
      "Xiaojun Ma",
      "Lun Du",
      "Zhiming Ding",
      "Shi Han",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.20072v2",
    "published_date": "2024-12-28 07:54:14 UTC",
    "updated_date": "2024-12-31 03:11:03 UTC"
  },
  {
    "arxiv_id": "2412.20070v1",
    "title": "On the Compositional Generalization of Multimodal LLMs for Medical Imaging",
    "authors": [
      "Zhenyang Cai",
      "Junying Chen",
      "Rongsheng Wang",
      "Weihong Wang",
      "Yonglin Deng",
      "Dingjie Song",
      "Yize Chen",
      "Zixu Zhang",
      "Benyou Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) hold significant potential in the\nmedical field, but their capabilities are often limited by insufficient data in\ncertain medical domains, highlighting the need for understanding what kinds of\nimages can be used by MLLMs for generalization. Current research suggests that\nmulti-task training outperforms single-task as different tasks can benefit each\nother, but they often overlook the internal relationships within these tasks,\nproviding limited guidance on selecting datasets to enhance specific tasks. To\nanalyze this phenomenon, we attempted to employ compositional generalization\n(CG)-the ability of models to understand novel combinations by recombining\nlearned elements-as a guiding framework. Since medical images can be precisely\ndefined by Modality, Anatomical area, and Task, naturally providing an\nenvironment for exploring CG. Therefore, we assembled 106 medical datasets to\ncreate Med-MAT for comprehensive experiments. The experiments confirmed that\nMLLMs can use CG to understand unseen medical images and identified CG as one\nof the main drivers of the generalization observed in multi-task training.\nAdditionally, further studies demonstrated that CG effectively supports\ndatasets with limited data and delivers consistent performance across different\nbackbones, highlighting its versatility and broad applicability. Med-MAT is\npublicly available at https://github.com/FreedomIntelligence/Med-MAT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20070v1",
    "published_date": "2024-12-28 07:50:00 UTC",
    "updated_date": "2024-12-28 07:50:00 UTC"
  },
  {
    "arxiv_id": "2501.00055v1",
    "title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models",
    "authors": [
      "Miao Yu",
      "Junfeng Fang",
      "Yingjie Zhou",
      "Xing Fan",
      "Kun Wang",
      "Shirui Pan",
      "Qingsong Wen"
    ],
    "abstract": "While safety-aligned large language models (LLMs) are increasingly used as\nthe cornerstone for powerful systems such as multi-agent frameworks to solve\ncomplex real-world problems, they still suffer from potential adversarial\nqueries, such as jailbreak attacks, which attempt to induce harmful content.\nResearching attack methods allows us to better understand the limitations of\nLLM and make trade-offs between helpfulness and safety. However, existing\njailbreak attacks are primarily based on opaque optimization techniques (e.g.\ntoken-level gradient descent) and heuristic search methods like LLM refinement,\nwhich fall short in terms of transparency, transferability, and computational\ncost. In light of these limitations, we draw inspiration from the evolution and\ninfection processes of biological viruses and propose LLM-Virus, a jailbreak\nattack method based on evolutionary algorithm, termed evolutionary jailbreak.\nLLM-Virus treats jailbreak attacks as both an evolutionary and transfer\nlearning problem, utilizing LLMs as heuristic evolutionary operators to ensure\nhigh attack efficiency, transferability, and low time cost. Our experimental\nresults on multiple safety benchmarks show that LLM-Virus achieves competitive\nor even superior performance compared to existing attack methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00055v1",
    "published_date": "2024-12-28 07:48:57 UTC",
    "updated_date": "2024-12-28 07:48:57 UTC"
  },
  {
    "arxiv_id": "2412.20068v1",
    "title": "The Emotional Spectrum of LLMs: Leveraging Empathy and Emotion-Based Markers for Mental Health Support",
    "authors": [
      "Alessandro De Grandi",
      "Federico Ravenda",
      "Andrea Raballo",
      "Fabio Crestani"
    ],
    "abstract": "The increasing demand for mental health services has highlighted the need for\ninnovative solutions, particularly in the realm of psychological conversational\nAI, where the availability of sensitive data is scarce. In this work, we\nexplored the development of a system tailored for mental health support with a\nnovel approach to psychological assessment based on explainable emotional\nprofiles in combination with empathetic conversational models, offering a\npromising tool for augmenting traditional care, particularly where immediate\nexpertise is unavailable. Our work can be divided into two main parts,\nintrinsecaly connected to each other. First, we present RACLETTE, a\nconversational system that demonstrates superior emotional accuracy compared to\nstate-of-the-art benchmarks in both understanding users' emotional states and\ngenerating empathetic responses during conversations, while progressively\nbuilding an emotional profile of the user through their interactions. Second,\nwe show how the emotional profiles of a user can be used as interpretable\nmarkers for mental health assessment. These profiles can be compared with\ncharacteristic emotional patterns associated with different mental disorders,\nproviding a novel approach to preliminary screening and support.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20068v1",
    "published_date": "2024-12-28 07:42:29 UTC",
    "updated_date": "2024-12-28 07:42:29 UTC"
  },
  {
    "arxiv_id": "2412.20064v1",
    "title": "VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition",
    "authors": [
      "Lan Chen",
      "Haoxiang Yang",
      "Pengpeng Shao",
      "Haoyu Song",
      "Xiao Wang",
      "Zhicheng Zhao",
      "Yaowei Wang",
      "Yonghong Tian"
    ],
    "abstract": "Pattern recognition leveraging both RGB and Event cameras can significantly\nenhance performance by deploying deep neural networks that utilize a\nfine-tuning strategy. Inspired by the successful application of large models,\nthe introduction of such large models can also be considered to further enhance\nthe performance of multi-modal tasks. However, fully fine-tuning these models\nleads to inefficiency and lightweight fine-tuning methods such as LoRA and\nAdapter have been proposed to achieve a better balance between efficiency and\nperformance. To our knowledge, there is currently no work that has conducted\nparameter-efficient fine-tuning (PEFT) for RGB-Event recognition based on\npre-trained foundation models. To address this issue, this paper proposes a\nnovel PEFT strategy to adapt the pre-trained foundation vision models for the\nRGB-Event-based classification. Specifically, given the RGB frames and event\nstreams, we extract the RGB and event features based on the vision foundation\nmodel ViT with a modality-specific LoRA tuning strategy. The frame difference\nof the dual modalities is also considered to capture the motion cues via the\nframe difference backbone network. These features are concatenated and fed into\nhigh-level Transformer layers for efficient multi-modal feature learning via\nmodality-shared LoRA tuning. Finally, we concatenate these features and feed\nthem into a classification head to achieve efficient fine-tuning. The source\ncode and pre-trained models will be released on\n\\url{https://github.com/Event-AHU/VELoRA}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2412.20064v1",
    "published_date": "2024-12-28 07:38:23 UTC",
    "updated_date": "2024-12-28 07:38:23 UTC"
  },
  {
    "arxiv_id": "2501.01974v1",
    "title": "Hawkes based Representation Learning for Reasoning over Scale-free Community-structured Temporal Knowledge Graphs",
    "authors": [
      "Yuwei Du",
      "Xinyue Liu",
      "Wenxin Liang",
      "Linlin Zong",
      "Xianchao Zhang"
    ],
    "abstract": "Temporal knowledge graph (TKG) reasoning has become a hot topic due to its\ngreat value in many practical tasks. The key to TKG reasoning is modeling the\nstructural information and evolutional patterns of the TKGs. While great\nefforts have been devoted to TKG reasoning, the structural and evolutional\ncharacteristics of real-world networks have not been considered. In the aspect\nof structure, real-world networks usually exhibit clear community structure and\nscale-free (long-tailed distribution) properties. In the aspect of evolution,\nthe impact of an event decays with the time elapsing. In this paper, we propose\na novel TKG reasoning model called Hawkes process-based Evolutional\nRepresentation Learning Network (HERLN), which learns structural information\nand evolutional patterns of a TKG simultaneously, considering the\ncharacteristics of real-world networks: community structure, scale-free and\ntemporal decaying. First, we find communities in the input TKG to make the\nencoding get more similar intra-community embeddings. Second, we design a\nHawkes process-based relational graph convolutional network to cope with the\nevent impact-decaying phenomenon. Third, we design a conditional decoding\nmethod to alleviate biases towards frequent entities caused by long-tailed\ndistribution. Experimental results show that HERLN achieves significant\nimprovements over the state-of-the-art models.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01974v1",
    "published_date": "2024-12-28 06:47:51 UTC",
    "updated_date": "2024-12-28 06:47:51 UTC"
  },
  {
    "arxiv_id": "2412.20048v1",
    "title": "CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language and Speaker Generation",
    "authors": [
      "Ji-Hoon Kim",
      "Hong-Sun Yang",
      "Yoon-Cheol Ju",
      "Il-Hwan Kim",
      "Byeong-Yeol Kim",
      "Joon Son Chung"
    ],
    "abstract": "The goal of this work is to generate natural speech in multiple languages\nwhile maintaining the same speaker identity, a task known as cross-lingual\nspeech synthesis. A key challenge of cross-lingual speech synthesis is the\nlanguage-speaker entanglement problem, which causes the quality of\ncross-lingual systems to lag behind that of intra-lingual systems. In this\npaper, we propose CrossSpeech++, which effectively disentangles language and\nspeaker information and significantly improves the quality of cross-lingual\nspeech synthesis. To this end, we break the complex speech generation pipeline\ninto two simple components: language-dependent and speaker-dependent\ngenerators. The language-dependent generator produces linguistic variations\nthat are not biased by specific speaker attributes. The speaker-dependent\ngenerator models acoustic variations that characterize speaker identity. By\nhandling each type of information in separate modules, our method can\neffectively disentangle language and speaker representation. We conduct\nextensive experiments using various metrics, and demonstrate that CrossSpeech++\nachieves significant improvements in cross-lingual speech synthesis,\noutperforming existing methods by a large margin.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20048v1",
    "published_date": "2024-12-28 06:32:49 UTC",
    "updated_date": "2024-12-28 06:32:49 UTC"
  },
  {
    "arxiv_id": "2412.20045v1",
    "title": "Enhancing Diffusion Models for Inverse Problems with Covariance-Aware Posterior Sampling",
    "authors": [
      "Shayan Mohajer Hamidi",
      "En-Hui Yang"
    ],
    "abstract": "Inverse problems exist in many disciplines of science and engineering. In\ncomputer vision, for example, tasks such as inpainting, deblurring, and super\nresolution can be effectively modeled as inverse problems. Recently, denoising\ndiffusion probabilistic models (DDPMs) are shown to provide a promising\nsolution to noisy linear inverse problems without the need for additional task\nspecific training. Specifically, with the prior provided by DDPMs, one can\nsample from the posterior by approximating the likelihood. In the literature,\napproximations of the likelihood are often based on the mean of conditional\ndensities of the reverse process, which can be obtained using Tweedie formula.\nTo obtain a better approximation to the likelihood, in this paper we first\nderive a closed form formula for the covariance of the reverse process. Then,\nwe propose a method based on finite difference method to approximate this\ncovariance such that it can be readily obtained from the existing pretrained\nDDPMs, thereby not increasing the complexity compared to existing approaches.\nFinally, based on the mean and approximated covariance of the reverse process,\nwe present a new approximation to the likelihood. We refer to this method as\ncovariance-aware diffusion posterior sampling (CA-DPS). Experimental results\nshow that CA-DPS significantly improves reconstruction performance without\nrequiring hyperparameter tuning. The code for the paper is put in the\nsupplementary materials.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20045v1",
    "published_date": "2024-12-28 06:17:44 UTC",
    "updated_date": "2024-12-28 06:17:44 UTC"
  },
  {
    "arxiv_id": "2412.20024v2",
    "title": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical Characters",
    "authors": [
      "Ting Bai",
      "Jiazheng Kang",
      "Jiayang Fan"
    ],
    "abstract": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20024v2",
    "published_date": "2024-12-28 05:01:26 UTC",
    "updated_date": "2025-01-06 04:34:16 UTC"
  },
  {
    "arxiv_id": "2501.00054v1",
    "title": "AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors",
    "authors": [
      "Mengnan Zhao",
      "Lihe Zhang",
      "Xingyi Yang",
      "Tianhang Zheng",
      "Baocai Yin"
    ],
    "abstract": "Security concerns surrounding text-to-image diffusion models have driven\nresearchers to unlearn inappropriate concepts through fine-tuning. Recent\nfine-tuning methods typically align the prediction distributions of unsafe\nprompts with those of predefined text anchors. However, these techniques\nexhibit a considerable performance trade-off between eliminating undesirable\nconcepts and preserving other concepts. In this paper, we systematically\nanalyze the impact of diverse text anchors on unlearning performance. Guided by\nthis analysis, we propose AdvAnchor, a novel approach that generates\nadversarial anchors to alleviate the trade-off issue. These adversarial anchors\nare crafted to closely resemble the embeddings of undesirable concepts to\nmaintain overall model performance, while selectively excluding defining\nattributes of these concepts for effective erasure. Extensive experiments\ndemonstrate that AdvAnchor outperforms state-of-the-art methods. Our code is\npublicly available at https://anonymous.4open.science/r/AdvAnchor.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00054v1",
    "published_date": "2024-12-28 04:44:07 UTC",
    "updated_date": "2024-12-28 04:44:07 UTC"
  },
  {
    "arxiv_id": "2412.20020v1",
    "title": "Calibre: Towards Fair and Accurate Personalized Federated Learning with Self-Supervised Learning",
    "authors": [
      "Sijia Chen",
      "Ningxin Su",
      "Baochun Li"
    ],
    "abstract": "In the context of personalized federated learning, existing approaches train\na global model to extract transferable representations, based on which any\nclient could train personalized models with a limited number of data samples.\nSelf-supervised learning is considered a promising direction as the global\nmodel it produces is generic and facilitates personalization for all clients\nfairly. However, when data is heterogeneous across clients, the global model\ntrained using SSL is unable to learn high-quality personalized models. In this\npaper, we show that when the global model is trained with SSL without\nmodifications, its produced representations have fuzzy class boundaries. As a\nresult, personalized learning within each client produces models with low\naccuracy. In order to improve SSL towards better accuracy without sacrificing\nits advantage in fairness, we propose Calibre, a new personalized federated\nlearning framework designed to calibrate SSL representations by maintaining a\nsuitable balance between more generic and more client-specific representations.\nCalibre is designed based on theoretically-sound properties, and introduces (1)\na client-specific prototype loss as an auxiliary training objective; and (2) an\naggregation algorithm guided by such prototypes across clients. Our\nexperimental results in an extensive array of non-i.i.d.~settings show that\nCalibre achieves state-of-the-art performance in terms of both mean accuracy\nand fairness across clients. Code repo:\nhttps://github.com/TL-System/plato/tree/main/examples/ssl/calibre.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICDCS camera-ready paper, Code repo:\n  https://github.com/TL-System/plato/tree/main/examples/ssl/calibre",
    "pdf_url": "http://arxiv.org/pdf/2412.20020v1",
    "published_date": "2024-12-28 04:43:39 UTC",
    "updated_date": "2024-12-28 04:43:39 UTC"
  },
  {
    "arxiv_id": "2412.20014v1",
    "title": "ProtCLIP: Function-Informed Protein Multi-Modal Learning",
    "authors": [
      "Hanjing Zhou",
      "Mingze Yin",
      "Wei Wu",
      "Mingyang Li",
      "Kun Fu",
      "Jintai Chen",
      "Jian Wu",
      "Zheng Wang"
    ],
    "abstract": "Multi-modality pre-training paradigm that aligns protein sequences and\nbiological descriptions has learned general protein representations and\nachieved promising performance in various downstream applications. However,\nthese works were still unable to replicate the extraordinary success of\nlanguage-supervised visual foundation models due to the ineffective usage of\naligned protein-text paired data and the lack of an effective function-informed\npre-training paradigm. To address these issues, this paper curates a\nlarge-scale protein-text paired dataset called ProtAnno with a property-driven\nsampling strategy, and introduces a novel function-informed protein\npre-training paradigm. Specifically, the sampling strategy determines selecting\nprobability based on the sample confidence and property coverage, balancing the\ndata quality and data quantity in face of large-scale noisy data. Furthermore,\nmotivated by significance of the protein specific functional mechanism, the\nproposed paradigm explicitly model protein static and dynamic functional\nsegments by two segment-wise pre-training objectives, injecting fine-grained\ninformation in a function-informed manner. Leveraging all these innovations, we\ndevelop ProtCLIP, a multi-modality foundation model that comprehensively\nrepresents function-aware protein embeddings. On 22 different protein\nbenchmarks within 5 types, including protein functionality classification,\nmutation effect prediction, cross-modal transformation, semantic similarity\ninference and protein-protein interaction prediction, our ProtCLIP consistently\nachieves SOTA performance, with remarkable improvements of 75% on average in\nfive cross-modal transformation benchmarks, 59.9% in GO-CC and 39.7% in GO-BP\nprotein function prediction. The experimental results verify the extraordinary\npotential of ProtCLIP serving as the protein multi-modality foundation model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20014v1",
    "published_date": "2024-12-28 04:23:47 UTC",
    "updated_date": "2024-12-28 04:23:47 UTC"
  },
  {
    "arxiv_id": "2412.20005v2",
    "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
    "authors": [
      "Yujie Luo",
      "Xiangyuan Ru",
      "Kangwei Liu",
      "Lin Yuan",
      "Mengshu Sun",
      "Ningyu Zhang",
      "Lei Liang",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Lanning Wei",
      "Da Zheng",
      "Haofen Wang",
      "Huajun Chen"
    ],
    "abstract": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "WWW 2025 Demonstration",
    "pdf_url": "http://arxiv.org/pdf/2412.20005v2",
    "published_date": "2024-12-28 04:01:30 UTC",
    "updated_date": "2025-02-06 10:37:17 UTC"
  },
  {
    "arxiv_id": "2412.20004v1",
    "title": "Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous Devices",
    "authors": [
      "Jun Liu",
      "Yunming Liao",
      "Hongli Xu",
      "Yang Xu",
      "Jianchun Liu",
      "Chen Qian"
    ],
    "abstract": "Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained\nlanguage models in a distributed manner. However, there are two critical\nchallenges for efficient FedFT in practical applications, i.e., resource\nconstraints and system heterogeneity. Existing works rely on\nparameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but\nwith major limitations. Herein, based on the inherent characteristics of FedFT,\nwe observe that LoRA layers with higher ranks added close to the output help to\nsave resource consumption while achieving comparable fine-tuning performance.\nThen we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces\nthe difficulty of determining the number of LoRA layers (called, LoRA depth)\nand the rank of each LoRA layer (called, rank distribution). We analyze the\ncoupled relationship between LoRA depth and rank distribution, and design an\nefficient LoRA configuration algorithm for heterogeneous devices, thereby\npromoting fine-tuning efficiency. Extensive experiments are conducted on a\nphysical platform with 80 commercial devices. The results show that LEGEND can\nachieve a speedup of 1.5-2.8$\\times$ and save communication costs by about\n42.3% when achieving the target accuracy, compared to the advanced solutions.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.20004v1",
    "published_date": "2024-12-28 04:00:42 UTC",
    "updated_date": "2024-12-28 04:00:42 UTC"
  },
  {
    "arxiv_id": "2412.19999v1",
    "title": "Comprehensive Review of EEG-to-Output Research: Decoding Neural Signals into Images, Videos, and Audio",
    "authors": [
      "Yashvir Sabharwal",
      "Balaji Rama"
    ],
    "abstract": "Electroencephalography (EEG) is an invaluable tool in neuroscience, offering\ninsights into brain activity with high temporal resolution. Recent advancements\nin machine learning and generative modeling have catalyzed the application of\nEEG in reconstructing perceptual experiences, including images, videos, and\naudio. This paper systematically reviews EEG-to-output research, focusing on\nstate-of-the-art generative methods, evaluation metrics, and data challenges.\nUsing PRISMA guidelines, we analyze 1800 studies and identify key trends,\nchallenges, and opportunities in the field. The findings emphasize the\npotential of advanced models such as Generative Adversarial Networks (GANs),\nVariational Autoencoders (VAEs), and Transformers, while highlighting the\npressing need for standardized datasets and cross-subject generalization. A\nroadmap for future research is proposed that aims to improve decoding accuracy\nand broadening real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages. Submitted as a conference paper to IntelliSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.19999v1",
    "published_date": "2024-12-28 03:50:56 UTC",
    "updated_date": "2024-12-28 03:50:56 UTC"
  },
  {
    "arxiv_id": "2412.19994v1",
    "title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry",
    "authors": [
      "Yang Han",
      "Ziping Wan",
      "Lu Chen",
      "Kai Yu",
      "Xin Chen"
    ],
    "abstract": "Large Language Models (LLMs) have significantly transformed our daily life\nand established a new paradigm in natural language processing (NLP). However,\nthe predominant pretraining of LLMs on extensive web-based texts remains\ninsufficient for advanced scientific discovery, particularly in chemistry. The\nscarcity of specialized chemistry data, coupled with the complexity of\nmulti-modal data such as 2D graph, 3D structure and spectrum, present distinct\nchallenges. Although several studies have reviewed Pretrained Language Models\n(PLMs) in chemistry, there is a conspicuous absence of a systematic survey\nspecifically focused on chemistry-oriented LLMs. In this paper, we outline\nmethodologies for incorporating domain-specific chemistry knowledge and\nmulti-modal information into LLMs, we also conceptualize chemistry LLMs as\nagents using chemistry tools and investigate their potential to accelerate\nscientific research. Additionally, we conclude the existing benchmarks to\nevaluate chemistry ability of LLMs. Finally, we critically examine the current\nchallenges and identify promising directions for future research. Through this\ncomprehensive survey, we aim to assist researchers in staying at the forefront\nof developments in chemistry LLMs and to inspire innovative applications in the\nfield.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "COLING2025,We maintain an up-to-date Github repository at:\n  https://github.com/OpenDFM/LLM4Chemistry",
    "pdf_url": "http://arxiv.org/pdf/2412.19994v1",
    "published_date": "2024-12-28 03:40:25 UTC",
    "updated_date": "2024-12-28 03:40:25 UTC"
  },
  {
    "arxiv_id": "2412.19992v1",
    "title": "An Ordinary Differential Equation Sampler with Stochastic Start for Diffusion Bridge Models",
    "authors": [
      "Yuang Wang",
      "Pengfei Jin",
      "Li Zhang",
      "Quanzheng Li",
      "Zhiqiang Chen",
      "Dufan Wu"
    ],
    "abstract": "Diffusion bridge models have demonstrated promising performance in\nconditional image generation tasks, such as image restoration and translation,\nby initializing the generative process from corrupted images instead of pure\nGaussian noise. However, existing diffusion bridge models often rely on\nStochastic Differential Equation (SDE) samplers, which result in slower\ninference speed compared to diffusion models that employ high-order Ordinary\nDifferential Equation (ODE) solvers for acceleration. To mitigate this gap, we\npropose a high-order ODE sampler with a stochastic start for diffusion bridge\nmodels. To overcome the singular behavior of the probability flow ODE (PF-ODE)\nat the beginning of the reverse process, a posterior sampling approach was\nintroduced at the first reverse step. The sampling was designed to ensure a\nsmooth transition from corrupted images to the generative trajectory while\nreducing discretization errors. Following this stochastic start, Heun's\nsecond-order solver is applied to solve the PF-ODE, achieving high perceptual\nquality with significantly reduced neural function evaluations (NFEs). Our\nmethod is fully compatible with pretrained diffusion bridge models and requires\nno additional training. Extensive experiments on image restoration and\ntranslation tasks, including super-resolution, JPEG restoration,\nEdges-to-Handbags, and DIODE-Outdoor, demonstrated that our sampler outperforms\nstate-of-the-art methods in both visual quality and Frechet Inception Distance\n(FID).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 5 figures, This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2412.19992v1",
    "published_date": "2024-12-28 03:32:26 UTC",
    "updated_date": "2024-12-28 03:32:26 UTC"
  },
  {
    "arxiv_id": "2501.14766v1",
    "title": "Artificial Intelligence for Sustainable Urban Biodiversity: A Framework for Monitoring and Conservation",
    "authors": [
      "Yasmin Rahmati"
    ],
    "abstract": "The rapid expansion of urban areas challenges biodiversity conservation,\nrequiring innovative ecosystem management. This study explores the role of\nArtificial Intelligence (AI) in urban biodiversity conservation, its\napplications, and a framework for implementation. Key findings show that: (a)\nAI enhances species detection and monitoring, achieving over 90% accuracy in\nurban wildlife tracking and invasive species management; (b) integrating data\nfrom remote sensing, acoustic monitoring, and citizen science enables\nlarge-scale ecosystem analysis; and (c) AI decision tools improve conservation\nplanning and resource allocation, increasing prediction accuracy by up to 18.5%\ncompared to traditional methods. The research presents an AI-Driven Framework\nfor Urban Biodiversity Management, highlighting AI's impact on monitoring,\nconservation strategies, and ecological outcomes. Implementation strategies\ninclude: (a) standardizing data collection and model validation, (b) ensuring\nequitable AI access across urban contexts, and (c) developing ethical\nguidelines for biodiversity monitoring. The study concludes that integrating AI\nin urban biodiversity conservation requires balancing innovation with\necological wisdom and addressing data quality, socioeconomic disparities, and\nethical concerns.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14766v1",
    "published_date": "2024-12-28 03:18:56 UTC",
    "updated_date": "2024-12-28 03:18:56 UTC"
  },
  {
    "arxiv_id": "2412.19987v1",
    "title": "Delayed Random Partial Gradient Averaging for Federated Learning",
    "authors": [
      "Xinyi Hu"
    ],
    "abstract": "Federated learning (FL) is a distributed machine learning paradigm that\nenables multiple clients to train a shared model collaboratively while\npreserving privacy. However, the scaling of real-world FL systems is often\nlimited by two communication bottlenecks:(a) while the increasing computing\npower of edge devices enables the deployment of large-scale Deep Neural\nNetworks (DNNs), the limited bandwidth constraints frequent transmissions over\nlarge DNNs; and (b) high latency cost greatly degrades the performance of FL.\nIn light of these bottlenecks, we propose a Delayed Random Partial Gradient\nAveraging (DPGA) to enhance FL. Under DPGA, clients only share partial local\nmodel gradients with the server. The size of the shared part in a local model\nis determined by the update rate, which is coarsely initialized and\nsubsequently refined over the temporal dimension. Moreover, DPGA largely\nreduces the system run time by enabling computation in parallel with\ncommunication. We conduct experiments on non-IID CIFAR-10/100 to demonstrate\nthe efficacy of our method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.19987v1",
    "published_date": "2024-12-28 03:14:27 UTC",
    "updated_date": "2024-12-28 03:14:27 UTC"
  },
  {
    "arxiv_id": "2412.19985v1",
    "title": "The Fifth International Verification of Neural Networks Competition (VNN-COMP 2024): Summary and Results",
    "authors": [
      "Christopher Brix",
      "Stanley Bak",
      "Taylor T. Johnson",
      "Haoze Wu"
    ],
    "abstract": "This report summarizes the 5th International Verification of Neural Networks\nCompetition (VNN-COMP 2024), held as a part of the 7th International Symposium\non AI Verification (SAIV), that was collocated with the 36th International\nConference on Computer-Aided Verification (CAV). VNN-COMP is held annually to\nfacilitate the fair and objective comparison of state-of-the-art neural network\nverification tools, encourage the standardization of tool interfaces, and bring\ntogether the neural network verification community. To this end, standardized\nformats for networks (ONNX) and specification (VNN-LIB) were defined, tools\nwere evaluated on equal-cost hardware (using an automatic evaluation pipeline\nbased on AWS instances), and tool parameters were chosen by the participants\nbefore the final test sets were made public. In the 2024 iteration, 8 teams\nparticipated on a diverse set of 12 regular and 8 extended benchmarks. This\nreport summarizes the rules, benchmarks, participating tools, results, and\nlessons learned from this iteration of this competition.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Report on the results of VNN-COMP 2024. arXiv admin note: substantial\n  text overlap with arXiv:2312.16760, arXiv:2212.10376",
    "pdf_url": "http://arxiv.org/pdf/2412.19985v1",
    "published_date": "2024-12-28 03:07:00 UTC",
    "updated_date": "2024-12-28 03:07:00 UTC"
  },
  {
    "arxiv_id": "2501.01973v3",
    "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
    "authors": [
      "Di Jin",
      "Xing Liu",
      "Yu Liu",
      "Jia Qing Yap",
      "Andrea Wong",
      "Adriana Crespo",
      "Qi Lin",
      "Zhiyuan Yin",
      "Qiang Yan",
      "Ryan Ye"
    ],
    "abstract": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "Di Jin and Xing Liu contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2501.01973v3",
    "published_date": "2024-12-28 02:28:19 UTC",
    "updated_date": "2025-01-09 07:26:05 UTC"
  },
  {
    "arxiv_id": "2501.00053v1",
    "title": "Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images",
    "authors": [
      "Xiaoge Zhang",
      "Tao Wang",
      "Chao Yan",
      "Fedaa Najdawi",
      "Kai Zhou",
      "Yuan Ma",
      "Yiu-ming Cheung",
      "Bradley A. Malin"
    ],
    "abstract": "Ensuring trustworthiness is fundamental to the development of artificial\nintelligence (AI) that is considered societally responsible, particularly in\ncancer diagnostics, where a misdiagnosis can have dire consequences. Current\ndigital pathology AI models lack systematic solutions to address\ntrustworthiness concerns arising from model limitations and data discrepancies\nbetween model deployment and development environments. To address this issue,\nwe developed TRUECAM, a framework designed to ensure both data and model\ntrustworthiness in non-small cell lung cancer subtyping with whole-slide\nimages. TRUECAM integrates 1) a spectral-normalized neural Gaussian process for\nidentifying out-of-scope inputs and 2) an ambiguity-guided elimination of tiles\nto filter out highly ambiguous regions, addressing data trustworthiness, as\nwell as 3) conformal prediction to ensure controlled error rates. We\nsystematically evaluated the framework across multiple large-scale cancer\ndatasets, leveraging both task-specific and foundation models, illustrate that\nan AI model wrapped with TRUECAM significantly outperforms models that lack\nsuch guidance, in terms of classification accuracy, robustness,\ninterpretability, and data efficiency, while also achieving improvements in\nfairness. These findings highlight TRUECAM as a versatile wrapper framework for\ndigital pathology AI models with diverse architectural designs, promoting their\nresponsible and effective applications in real-world settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00053v1",
    "published_date": "2024-12-28 02:22:47 UTC",
    "updated_date": "2024-12-28 02:22:47 UTC"
  },
  {
    "arxiv_id": "2412.19976v1",
    "title": "Will you donate money to a chatbot? The effect of chatbot anthropomorphic features and persuasion strategies on willingness to donate",
    "authors": [
      "Ekaterina Novozhilova",
      "Jiacheng Huang",
      "Le He",
      "Ziling Li",
      "James Cummings"
    ],
    "abstract": "This work investigates the causal mechanism behind the effect of chatbot\npersonification and persuasion strategies on users' perceptions and donation\nlikelihood. In a 2 (personified vs. non-personified chatbot) x 2 (emotional vs.\nlogical persuasion strategy) between-subjects experiment (N=76), participants\nengaged with a chatbot that represented a non-profit charitable organization.\nThe results suggest that interaction with a personified chatbot evokes\nperceived anthropomorphism; however, it does not elicit greater willingness to\ndonate. In fact, we found that commonly used anthropomorphic features, like\nname and narrative, led to negative attitudes toward an AI agent in the\ndonation context. Our results showcase a preference for non-personified\nchatbots paired with logical persuasion appeal, emphasizing the significance of\nconsistency in chatbot interaction, mirroring human-human engagement. We\ndiscuss the importance of moving from exploring the common scenario of a\nchatbot with machine identity vs. a chatbot with human identity in light of the\nrecent regulations of AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.19976v1",
    "published_date": "2024-12-28 02:17:46 UTC",
    "updated_date": "2024-12-28 02:17:46 UTC"
  },
  {
    "arxiv_id": "2412.19967v2",
    "title": "MobileNetV2: A lightweight classification model for home-based sleep apnea screening",
    "authors": [
      "Hui Pan",
      "Yanxuan Yu",
      "Jilun Ye",
      "Xu Zhang"
    ],
    "abstract": "This study proposes a novel lightweight neural network model leveraging\nfeatures extracted from electrocardiogram (ECG) and respiratory signals for\nearly OSA screening. ECG signals are used to generate feature spectrograms to\npredict sleep stages, while respiratory signals are employed to detect\nsleep-related breathing abnormalities. By integrating these predictions, the\nmethod calculates the apnea-hypopnea index (AHI) with enhanced accuracy,\nfacilitating precise OSA diagnosis.\n  The method was validated on three publicly available sleep apnea databases:\nthe Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographic\ndatabase. Results showed an overall OSA detection accuracy of 0.978,\nhighlighting the model's robustness. Respiratory event classification achieved\nan accuracy of 0.969 and an area under the receiver operating characteristic\ncurve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the\nROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906\nand specificity for REM and Wake states at 0.956 and 0.937, respectively.\n  This study underscores the potential of integrating lightweight neural\nnetworks with multi-signal analysis for accurate, portable, and cost-effective\nOSA screening, paving the way for broader adoption in home-based and wearable\nhealth monitoring systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.19967v2",
    "published_date": "2024-12-28 01:37:25 UTC",
    "updated_date": "2025-01-03 13:55:34 UTC"
  },
  {
    "arxiv_id": "2412.19966v1",
    "title": "Bridging Context Gaps: Enhancing Comprehension in Long-Form Social Conversations Through Contextualized Excerpts",
    "authors": [
      "Shrestha Mohanty",
      "Sarah Xuan",
      "Jacob Jobraeel",
      "Anurag Kumar",
      "Deb Roy",
      "Jad Kabbara"
    ],
    "abstract": "We focus on enhancing comprehension in small-group recorded conversations,\nwhich serve as a medium to bring people together and provide a space for\nsharing personal stories and experiences on crucial social matters. One way to\nparse and convey information from these conversations is by sharing highlighted\nexcerpts in subsequent conversations. This can help promote a collective\nunderstanding of relevant issues, by highlighting perspectives and experiences\nto other groups of people who might otherwise be unfamiliar with and thus\nunable to relate to these experiences. The primary challenge that arises then\nis that excerpts taken from one conversation and shared in another setting\nmight be missing crucial context or key elements that were previously\nintroduced in the original conversation. This problem is exacerbated when\nconversations become lengthier and richer in themes and shared experiences. To\naddress this, we explore how Large Language Models (LLMs) can enrich these\nexcerpts by providing socially relevant context. We present approaches for\neffective contextualization to improve comprehension, readability, and empathy.\nWe show significant improvements in understanding, as assessed through\nsubjective and objective evaluations. While LLMs can offer valuable context,\nthey struggle with capturing key social aspects. We release the Human-annotated\nSalient Excerpts (HSE) dataset to support future work. Additionally, we show\nhow context-enriched excerpts can provide more focused and comprehensive\nconversation summaries.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.19966v1",
    "published_date": "2024-12-28 01:29:53 UTC",
    "updated_date": "2024-12-28 01:29:53 UTC"
  },
  {
    "arxiv_id": "2412.19964v1",
    "title": "DepthMamba with Adaptive Fusion",
    "authors": [
      "Zelin Meng",
      "Zhichen Wang"
    ],
    "abstract": "Multi-view depth estimation has achieved impressive performance over various\nbenchmarks. However, almost all current multi-view systems rely on given ideal\ncamera poses, which are unavailable in many real-world scenarios, such as\nautonomous driving. In this work, we propose a new robustness benchmark to\nevaluate the depth estimation system under various noisy pose settings.\nSurprisingly, we find current multi-view depth estimation methods or\nsingle-view and multi-view fusion methods will fail when given noisy pose\nsettings. To tackle this challenge, we propose a two-branch network\narchitecture which fuses the depth estimation results of single-view and\nmulti-view branch. In specific, we introduced mamba to serve as feature\nextraction backbone and propose an attention-based fusion methods which\nadaptively select the most robust estimation results between the two branches.\nThus, the proposed method can perform well on some challenging scenes including\ndynamic objects, texture-less regions, etc. Ablation studies prove the\neffectiveness of the backbone and fusion method, while evaluation experiments\non challenging benchmarks (KITTI and DDAD) show that the proposed method\nachieves a competitive performance compared to the state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.19964v1",
    "published_date": "2024-12-28 01:17:47 UTC",
    "updated_date": "2024-12-28 01:17:47 UTC"
  },
  {
    "arxiv_id": "2501.00051v1",
    "title": "DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework",
    "authors": [
      "Yu-Zheng Lin",
      "Qinxuan Shi",
      "Zhanglong Yang",
      "Banafsheh Saber Latibari",
      "Sicong Shao",
      "Soheil Salehi",
      "Pratik Satam"
    ],
    "abstract": "Digital twin (DT) technology has emerged as a transformative approach to\nsimulate, predict, and optimize the behavior of physical systems, with\napplications that span manufacturing, healthcare, climate science, and more.\nHowever, the development of DT models often faces challenges such as high data\nrequirements, integration complexity, and limited adaptability to dynamic\nchanges in physical systems. This paper presents a new method inspired by\ndynamic data-driven applications systems (DDDAS), called the dynamic\ndata-driven generative of digital twins framework (DDD-GenDT), which combines\nthe physical system with LLM, allowing LLM to act as DT to interact with the\nphysical system operating status and generate the corresponding physical\nbehaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining\nprocess, and we use the spindle current measurement data in the NASA milling\nwear data set as an example to enable LLMs to forecast the physical behavior\nfrom historical data and interact with current observations. Experimental\nresults show that in the zero-shot prediction setting, the LLM-based DT can\nadapt to the change in the system, and the average RMSE of the GPT-4 prediction\nis 0.479A, which is 4.79% of the maximum spindle motor current measurement of\n10A, with little training data and instructions required. Furthermore, we\nanalyze the performance of DDD-GenDT in this specific application and their\npotential to construct digital twins. We also discuss the limitations and\nchallenges that may arise in practical implementations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00051v1",
    "published_date": "2024-12-28 01:13:30 UTC",
    "updated_date": "2024-12-28 01:13:30 UTC"
  },
  {
    "arxiv_id": "2503.04728v1",
    "title": "Leveraging Large Language Models For Optimized Item Categorization using UNSPSC Taxonomy",
    "authors": [
      "Anmolika Singh",
      "Yuhang Diao"
    ],
    "abstract": "Effective item categorization is vital for businesses, enabling the\ntransformation of unstructured datasets into organized categories that\nstreamline inventory management. Despite its importance, item categorization\nremains highly subjective and lacks a uniform standard across industries and\nbusinesses. The United Nations Standard Products and Services Code (UNSPSC)\nprovides a standardized system for cataloguing inventory, yet employing UNSPSC\ncategorizations often demands significant manual effort. This paper\ninvestigates the deployment of Large Language Models (LLMs) to automate the\nclassification of inventory data into UNSPSC codes based on Item Descriptions.\nWe evaluate the accuracy and efficiency of LLMs in categorizing diverse\ndatasets, exploring their language processing capabilities and their potential\nas a tool for standardizing inventory classification. Our findings reveal that\nLLMs can substantially diminish the manual labor involved in item\ncategorization while maintaining high accuracy, offering a scalable solution\nfor businesses striving to enhance their inventory management practices.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 Pages, International Conference on NLP, AI, Computer Science &\n  Engineering (NLAICSE 2024), December 2024, ISBN : 978-1-923107-45-8",
    "pdf_url": "http://arxiv.org/pdf/2503.04728v1",
    "published_date": "2024-12-28 00:12:13 UTC",
    "updated_date": "2024-12-28 00:12:13 UTC"
  }
]
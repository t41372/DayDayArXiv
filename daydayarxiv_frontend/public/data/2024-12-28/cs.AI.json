{
  "date": "2024-12-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-28 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 58 篇论文，主要聚焦 AI 模型创新（如扩散模型和 LLM 在数学、医疗领域的应用）、机器人学习、联邦学习和安全隐私等领域，其中令人印象深刻的是 Surya Ganguli 参与的扩散模型理论分析，以及 LLM 在化学和国防应用的潜力，这些文章展示了 AI 在实际场景中的深度影响和挑战。\n\n下面，我挑选了今天几篇重要的、具有话题度和学术影响的论文进行重点讨论，将相关主题归类后简要概述。其他论文（如一些常规调研或小规模实验，如第 9 篇的元启发式算法综述、第 10 篇的抑郁情绪感知实验）将快速掠过，以控制篇幅。每个条目列出论文标题（中文 + 英文），并突出核心贡献、方法和发现。\n\n### AI 模型与扩散技术（重点讨论，相关论文归类）\n- **Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of Automated Defense Vehicles（边缘智能和 LLM 推进 6G 启用自动防御车辆互联网）**  \n  这篇论文探讨了 AI 在军事自动驾驶中的应用，作者包括 Murat Arda Onsu。主要贡献是通过 LLM 和 6G 技术优化决策模型，实现实时数据交换和安全任务执行。发现显示，这种方法能减少人为错误，但面临适应性挑战。该论文有话题度，因为它将 AI 应用于国防场景。\n\n- **An analytic theory of creativity in convolutional diffusion models（卷积扩散模型中创造力的分析理论）**  \n  作者 Surya Ganguli（知名学者）的参与让这篇论文特别引人注目。核心方法是分析扩散模型的局部性和等变性，构建 ELS 机器来预测输出（在 CIFAR10 等数据集上中位 r² 达 0.94）。主要发现是扩散模型通过混合局部补丁生成新图像，这桥接了理论与实验差距，是今天最深刻的文章之一。\n\n- **Large Language Models for Mathematical Analysis（大语言模型用于数学分析）**  \n  这篇论文引入 DEMI-MathAnalysis 数据集和引导框架，针对证明型数学问题。贡献在于通过微调 LLM 生成逻辑严谨的证明，提升了 AI 在形式化推理中的能力。发现显示，该方法在序列极限等领域显著改善证明质量，体现了 LLM 在科学领域的潜力。\n\n### 机器人与学习框架（相关论文合并讨论）\n- **Towards General Purpose Robots at Scale: Lifelong Learning and Learning to Use Memory（面向大规模通用机器人的终身学习和记忆利用）**  \n  作者 William Yue 提出 t-DGR 方法，用于机器人终身学习。核心贡献是结合轨迹生成和人类演示，提升机器人长期任务性能。发现显示，该框架在 Memory Gym 任务中提高成功率，但需处理数据重叠问题。该论文与第 16 篇（模仿学习）相关，都强调元学习和记忆机制。\n\n- **High-fidelity social learning via shared episodic memories enhances collaborative foraging through mnemonic convergence（通过共享情节记忆的高保真社会学习增强协作觅食）**  \n  作者 Ismael T. Freire 和 Paul Verschure 的工作聚焦记忆与社会学习。方法使用 Sequential Episodic Control 代理，核心发现是高保真学习能优化记忆对齐和资源分配，而低保真学习无效。该论文有实际启发，适用于多代理系统。\n\n### AI 安全与应用（快速讨论有影响的论文）\n- **How To Think About End-To-End Encryption and AI: Training, Processing, Disclosure, and Consent（端到端加密与 AI：训练、处理、披露和同意）**  \n  作者团队包括 Kyunghyun Cho 和 Sunoo Park，分析 AI 与加密的兼容性。贡献在于提出技术与法律建议，确保 AI 不破坏加密隐私。发现显示，AI 集成可能削弱加密的安全性，是 AI 伦理讨论的重要补充。\n\n- **Leveraging Large Language Models for Enhancing Autonomous Vehicle Perception（利用大语言模型增强自动驾驶感知）**  \n  这篇论文构建 LLM 框架，提升自动驾驶的上下文理解和传感器融合。核心方法包括记忆模块，贡献在于提高感知准确性。发现显示，LLM 使车辆更适应动态环境，相关于第 1 篇的国防应用。\n\n其他论文（如第 11 篇的波斯语问答数据集、第 12 篇的无人机映射系统、第 13 篇的异常检测等）虽然有技术创新，但相对常规，我仅快速提及：它们分别优化了特定领域的数据处理和模型效率，例如第 11 篇通过 BERT 提升问答准确性（EM 达 0.95），但整体影响力有限。\n\n总之，今天的 arXiv 强调 AI 模型的理论深度和实际应用潜力，LLM 在科学与安全领域的扩展值得关注。读者可优先探索上述重点论文，如果有特定兴趣领域，建议查看完整摘要。明天见！",
  "papers": [
    {
      "arxiv_id": "2501.06205v2",
      "title": "Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of Automated Defense Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Murat Arda Onsu",
        "Poonam Lohan",
        "Burak Kantarci"
      ],
      "abstract": "The evolution of Artificial Intelligence (AI) and its subset Deep Learning\n(DL), has profoundly impacted numerous domains, including autonomous driving.\nThe integration of autonomous driving in military settings reduces human\ncasualties and enables precise and safe execution of missions in hazardous\nenvironments while allowing for reliable logistics support without the risks\nassociated with fatigue-related errors. However, relying on autonomous driving\nsolely requires an advanced decision-making model that is adaptable and optimum\nin any situation. Considering the presence of numerous interconnected\nautonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency\nCommunication (URLLC) is vital for ensuring seamless coordination, real-time\ndata exchange, and instantaneous response to dynamic driving environments. The\nadvent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV)\nconcept within the realm of Internet of Military Defense Things (IoMDT) by\nenabling robust connectivity, crucial for real-time data exchange, advanced\nnavigation, and enhanced safety features through IoADV interactions. On the\nother hand, a critical advancement in this space is using pre-trained\nGenerative Large Language Models (LLMs) for decision-making and communication\noptimization for autonomous driving. Hence, this work presents opportunities\nand challenges with a vision of realizing the full potential of these\ntechnologies in critical defense applications, especially through the\nadvancement of IoADV and its role in enhancing autonomous military operations.",
      "tldr_zh": "该论文探讨了如何利用 Edge Intelligence 和 LLMs 来推进 6G-Enabled Internet of Automated Defense Vehicles (IoADV)，以提升军事自主驾驶系统的决策和通信优化。论文强调，6G 的 Ultra-Reliable Low Latency Communication (URLLC) 能够确保实时数据交换和协调，减少人类伤亡并在危险环境中实现精确任务执行。作者分析了这些技术的机会和挑战，并展望其在 IoADV 中的潜力，以增强自主军事操作的安全性和可靠性。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.NI",
      "comment": "8 pages, 5 figures, accepted to IEEE Internet of Things Magazine",
      "pdf_url": "http://arxiv.org/pdf/2501.06205v2",
      "published_date": "2024-12-28 23:07:25 UTC",
      "updated_date": "2025-02-23 02:37:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:15:36.128938"
    },
    {
      "arxiv_id": "2412.20292v1",
      "title": "An analytic theory of creativity in convolutional diffusion models",
      "title_zh": "卷积扩散模型中创造力的分析理论",
      "authors": [
        "Mason Kamb",
        "Surya Ganguli"
      ],
      "abstract": "We obtain the first analytic, interpretable and predictive theory of\ncreativity in convolutional diffusion models. Indeed, score-based diffusion\nmodels can generate highly creative images that lie far from their training\ndata. But optimal score-matching theory suggests that these models should only\nbe able to produce memorized training examples. To reconcile this\ntheory-experiment gap, we identify two simple inductive biases, locality and\nequivariance, that: (1) induce a form of combinatorial creativity by preventing\noptimal score-matching; (2) result in a fully analytic, completely\nmechanistically interpretable, equivariant local score (ELS) machine that, (3)\nwithout any training can quantitatively predict the outputs of trained\nconvolution only diffusion models (like ResNets and UNets) with high accuracy\n(median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our\nELS machine reveals a locally consistent patch mosaic model of creativity, in\nwhich diffusion models create exponentially many novel images by mixing and\nmatching different local training set patches in different image locations. Our\ntheory also partially predicts the outputs of pre-trained self-attention\nenabled UNets (median $r^2 \\sim 0.75$ on CIFAR10), revealing an intriguing role\nfor attention in carving out semantic coherence from local patch mosaics.",
      "tldr_zh": "本研究首次提出了一种关于卷积扩散模型（convolutional diffusion models）创造力的分析、可解释和预测理论，解决了这些模型生成远超训练数据的创意图像与最优分数匹配（score-matching）理论的矛盾。作者识别了 locality（局部性）和 equivariance（等变性）两种诱导偏差，这些偏差阻止了最优分数匹配，并引发了组合创造力，同时开发了等变局部分数（ELS）机器，该机器无需训练即可高精度预测训练过的卷积扩散模型输出（如在 CIFAR10、FashionMNIST 和 MNIST 上，median r² 分别为 0.90、0.91 和 0.94）。ELS 机器揭示了扩散模型通过混合和匹配训练集的局部 patch 来创建指数级新图像的机制。理论还部分预测了自注意力 UNet 的输出（median r² ~ 0.75），突显了注意力在从局部 patch 镶嵌中构建语义一致性的作用。",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "q-bio.NC",
        "stat.ML",
        "I.2.10"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20292v1",
      "published_date": "2024-12-28 22:33:29 UTC",
      "updated_date": "2024-12-28 22:33:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:15:50.796887"
    },
    {
      "arxiv_id": "2412.20290v1",
      "title": "Transformer-Based Contrastive Meta-Learning For Low-Resource Generalizable Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Junyao Wang",
        "Mohammad Abdullah Al Faruque"
      ],
      "abstract": "Deep learning has been widely adopted for human activity recognition (HAR)\nwhile generalizing a trained model across diverse users and scenarios remains\nchallenging due to distribution shifts. The inherent low-resource challenge in\nHAR, i.e., collecting and labeling adequate human-involved data can be\nprohibitively costly, further raising the difficulty of tackling DS. We propose\nTACO, a novel transformer-based contrastive meta-learning approach for\ngeneralizable HAR. TACO addresses DS by synthesizing virtual target domains in\ntraining with explicit consideration of model generalizability. Additionally,\nwe extract expressive feature with the attention mechanism of Transformer and\nincorporate the supervised contrastive loss function within our\nmeta-optimization to enhance representation learning. Our evaluation\ndemonstrates that TACO achieves notably better performance across various\nlow-resource DS scenarios.",
      "tldr_zh": "该研究针对人类活动识别(HAR)中的分布偏移(DS)和低资源挑战，提出了一种基于Transformer的对比元学习(contrastive meta-learning)方法，名为TACO，以提升模型在不同用户和场景下的泛化能力。TACO通过在训练中合成虚拟目标域，并利用Transformer的注意力机制结合监督对比损失(supervised contrastive loss)函数，来增强特征表示学习，从而更好地处理数据分布偏移问题。实验结果显示，TACO在各种低资源DS场景中显著优于基线模型，证明了其在HAR领域的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20290v1",
      "published_date": "2024-12-28 21:57:12 UTC",
      "updated_date": "2024-12-28 21:57:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:15:59.463772"
    },
    {
      "arxiv_id": "2501.10395v1",
      "title": "Towards General Purpose Robots at Scale: Lifelong Learning and Learning to Use Memory",
      "title_zh": "翻译失败",
      "authors": [
        "William Yue"
      ],
      "abstract": "The widespread success of artificial intelligence in fields like natural\nlanguage processing and computer vision has not yet fully transferred to\nrobotics, where progress is hindered by the lack of large-scale training data\nand the complexity of real-world tasks. To address this, many robot learning\nresearchers are pushing to get robots deployed at scale in everyday\nunstructured environments like our homes to initiate a data flywheel. While\ncurrent robot learning systems are effective for certain short-horizon tasks,\nthey are not designed to autonomously operate over long time horizons in\nunstructured environments. This thesis focuses on addressing two key challenges\nfor robots operating over long time horizons: memory and lifelong learning.\n  We propose two novel methods to advance these capabilities. First, we\nintroduce t-DGR, a trajectory-based deep generative replay method that achieves\nstate-of-the-art performance on Continual World benchmarks, advancing lifelong\nlearning. Second, we develop a framework that leverages human demonstrations to\nteach agents effective memory utilization, improving learning efficiency and\nsuccess rates on Memory Gym tasks. Finally, we discuss future directions for\nachieving the lifelong learning and memory capabilities necessary for robots to\nfunction at scale in real-world settings.",
      "tldr_zh": "该论文讨论了人工智能在机器人领域的应用挑战，特别是缺乏大规模训练数据和真实任务的复杂性，强调通过在日常非结构化环境中部署机器人来启动数据飞轮，以实现长期自主操作。作者提出两个创新方法：t-DGR，一种基于轨迹的深度生成重放（deep generative replay）技术，在 Continual World benchmarks 上达到了 state-of-the-art 性能，推进了终身学习（lifelong learning）；以及一个利用 human demonstrations 的框架，提升了代理在 Memory Gym tasks 中的记忆利用效率和成功率。最后，论文探讨了未来方向，以实现机器人大规模部署所需的记忆和终身学习能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2401.02576,\n  arXiv:2411.07954",
      "pdf_url": "http://arxiv.org/pdf/2501.10395v1",
      "published_date": "2024-12-28 21:13:48 UTC",
      "updated_date": "2024-12-28 21:13:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:16:12.023876"
    },
    {
      "arxiv_id": "2412.20271v1",
      "title": "High-fidelity social learning via shared episodic memories enhances collaborative foraging through mnemonic convergence",
      "title_zh": "翻译失败",
      "authors": [
        "Ismael T. Freire",
        "Paul Verschure"
      ],
      "abstract": "Social learning, a cornerstone of cultural evolution, enables individuals to\nacquire knowledge by observing and imitating others. At the heart of its\nefficacy lies episodic memory, which encodes specific behavioral sequences to\nfacilitate learning and decision-making. This study explores the interrelation\nbetween episodic memory and social learning in collective foraging. Using\nSequential Episodic Control (SEC) agents capable of sharing complete behavioral\nsequences stored in episodic memory, we investigate how variations in the\nfrequency and fidelity of social learning influence collaborative foraging\nperformance. Furthermore, we analyze the effects of social learning on the\ncontent and distribution of episodic memories across the group. High-fidelity\nsocial learning is shown to consistently enhance resource collection efficiency\nand distribution, with benefits sustained across memory lengths. In contrast,\nlow-fidelity learning fails to outperform nonsocial learning, spreading diverse\nbut ineffective mnemonic patterns. Novel analyses using mnemonic metrics reveal\nthat high-fidelity social learning also fosters mnemonic group alignment and\nequitable resource distribution, while low-fidelity conditions increase\nmnemonic diversity without translating to performance gains. Additionally, we\nidentify an optimal range for episodic memory length in this task, beyond which\nperformance plateaus. These findings underscore the critical effects of social\nlearning on mnemonic group alignment and distribution and highlight the\npotential of neurocomputational models to probe the cognitive mechanisms\ndriving cultural evolution.",
      "tldr_zh": "这篇论文探讨了 episodic memory 与 social learning 在集体觅食中的互动，使用 Sequential Episodic Control (SEC) 代理来模拟代理间共享完整行为序列，分析社会学习频率和 fidelity 对协作性能的影响。研究发现，高-fidelity social learning 显著提升资源收集效率和分布，促进 mnemonic convergence 和记忆组对齐，同时维持在不同记忆长度上。相比之下，低-fidelity social learning 无法超越非社会学习，导致记忆多样性增加但性能无提升。总体而言，这些发现突出了社会学习对记忆组对齐和文化进化的认知机制的潜在作用，并验证了神经计算模型的适用性。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20271v1",
      "published_date": "2024-12-28 20:55:38 UTC",
      "updated_date": "2024-12-28 20:55:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:16:24.485633"
    },
    {
      "arxiv_id": "2501.00059v1",
      "title": "Large Language Models for Mathematical Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Ziye Chen",
        "Hao Qi"
      ],
      "abstract": "Mathematical problem-solving is a key field in artificial intelligence (AI)\nand a critical benchmark for evaluating the capabilities of large language\nmodels (LLMs). While extensive research has focused on mathematical\nproblem-solving, most existing work and datasets concentrate on computational\ntasks, leaving gaps in areas like mathematical analysis, which demands rigorous\nproofs and formal reasoning. We developed the DEMI-MathAnalysis dataset,\ncomprising proof-based problems from mathematical analysis topics such as\nSequences and Limits, Infinite Series, and Convex Functions. We also designed a\nguiding framework to rigorously enhance LLMs' ability to solve these problems.\nThrough fine-tuning LLMs on this dataset and employing our framework, we\nobserved significant improvements in their capability to generate logical,\ncomplete, and elegant proofs. This work addresses critical gaps in mathematical\nreasoning and contributes to advancing trustworthy AI capable of handling\nformalized mathematical language. The code is publicly accessible at LLMs for\nMathematical Analysis.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在数学分析领域的应用，强调现有研究多聚焦于计算任务，而忽略了需要严格证明和形式化推理的领域，如 Sequences and Limits、Infinite Series 和 Convex Functions。研究团队开发了 DEMI-MathAnalysis 数据集，包含基于证明的问题，并设计了一个指导框架来提升 LLMs 的能力。 通过在该数据集上微调 LLMs 并应用框架，实验结果显示模型在生成逻辑完整且优雅证明方面取得了显著改善。 这项工作填补了数学推理的空白，推动了可信赖 AI 处理形式化数学语言的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00059v1",
      "published_date": "2024-12-28 20:37:55 UTC",
      "updated_date": "2024-12-28 20:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:16:36.472182"
    },
    {
      "arxiv_id": "2412.20231v2",
      "title": "How To Think About End-To-End Encryption and AI: Training, Processing, Disclosure, and Consent",
      "title_zh": "翻译失败",
      "authors": [
        "Mallory Knodel",
        "Andrés Fábrega",
        "Daniella Ferrari",
        "Jacob Leiken",
        "Betty Li Hou",
        "Derek Yen",
        "Sam de Alfaro",
        "Kyunghyun Cho",
        "Sunoo Park"
      ],
      "abstract": "End-to-end encryption (E2EE) has become the gold standard for securing\ncommunications, bringing strong confidentiality and privacy guarantees to\nbillions of users worldwide. However, the current push towards widespread\nintegration of artificial intelligence (AI) models, including in E2EE systems,\nraises some serious security concerns. This work performs a critical\nexamination of the (in)compatibility of AI models and E2EE applications. We\nexplore this on two fronts: (1) the integration of AI \"assistants\" within E2EE\napplications, and (2) the use of E2EE data for training AI models. We analyze\nthe potential security implications of each, and identify conflicts with the\nsecurity guarantees of E2EE. Then, we analyze legal implications of integrating\nAI models in E2EE applications, given how AI integration can undermine the\nconfidentiality that E2EE promises. Finally, we offer a list of detailed\nrecommendations based on our technical and legal analyses, including: technical\ndesign choices that must be prioritized to uphold E2EE security; how service\nproviders must accurately represent E2EE security; and best practices for the\ndefault behavior of AI features and for requesting user consent. We hope this\npaper catalyzes an informed conversation on the tensions that arise between the\nbrisk deployment of AI and the security offered by E2EE, and guides the\nresponsible development of new AI features.",
      "tldr_zh": "这篇论文探讨了端到端加密 (E2EE) 与人工智能 (AI) 的兼容性问题，分析了 AI 模型在 E2EE 系统中的整合可能带来的安全隐患，包括 AI 助手的使用和 E2EE 数据用于训练 AI 的风险。作者评估了这些整合对 E2EE 的保密性、隐私保证和法律影响的潜在冲突，例如如何削弱用户数据保护。研究发现，AI 的快速部署可能与 E2EE 的核心安全原则相悖，并提出了详细推荐，如优先技术设计选择、准确传达 E2EE 安全信息、设置 AI 功能的默认行为以及优化用户同意流程。最终，论文旨在引导负责任的 AI 开发，促进 E2EE 安全与 AI 创新之间的平衡对话。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20231v2",
      "published_date": "2024-12-28 17:59:21 UTC",
      "updated_date": "2025-03-22 18:40:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:16:50.631121"
    },
    {
      "arxiv_id": "2412.20230v1",
      "title": "Leveraging Large Language Models for Enhancing Autonomous Vehicle Perception",
      "title_zh": "利用大型语言模型提升自动驾驶车辆感知",
      "authors": [
        "Athanasios Karagounis"
      ],
      "abstract": "Autonomous vehicles (AVs) rely on sophisticated perception systems to\ninterpret their surroundings, a cornerstone for safe navigation and\ndecision-making. The integration of Large Language Models (LLMs) into AV\nperception frameworks offers an innovative approach to address challenges in\ndynamic environments, sensor fusion, and contextual reasoning. This paper\npresents a novel framework for incorporating LLMs into AV perception, enabling\nadvanced contextual understanding, seamless sensor integration, and enhanced\ndecision support. Experimental results demonstrate that LLMs significantly\nimprove the accuracy and reliability of AV perception systems, paving the way\nfor safer and more intelligent autonomous driving technologies. By expanding\nthe scope of perception beyond traditional methods, LLMs contribute to creating\na more adaptive and human-centric driving ecosystem, making autonomous vehicles\nmore reliable and transparent in their operations. These advancements redefine\nthe relationship between human drivers and autonomous systems, fostering trust\nthrough enhanced understanding and personalized decision-making. Furthermore,\nby integrating memory modules and adaptive learning mechanisms, LLMs introduce\ncontinuous improvement in AV perception, enabling vehicles to evolve with time\nand adapt to changing environments and user preferences.",
      "tldr_zh": "本文提出一种新框架，将 Large Language Models (LLMs) 整合到 Autonomous Vehicles (AVs) 的感知系统中，以解决动态环境、传感器融合和上下文推理的挑战。该框架通过 LLMs 实现高级上下文理解、无缝传感器整合以及增强决策支持，实验结果显示感知系统的准确性和可靠性显著提升。此外，通过添加记忆模块和适应性学习机制，LLMs 使 AVs 能够持续改进和适应变化环境，促进更安全、以人为本的自动驾驶生态。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.20230v1",
      "published_date": "2024-12-28 17:58:44 UTC",
      "updated_date": "2024-12-28 17:58:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:17:00.327798"
    },
    {
      "arxiv_id": "2501.14769v1",
      "title": "A survey on pioneering metaheuristic algorithms between 2019 and 2024",
      "title_zh": "翻译失败",
      "authors": [
        "Tansel Dokeroglu",
        "Deniz Canturk",
        "Tayfun Kucukyilmaz"
      ],
      "abstract": "This review examines over 150 new metaheuristics of the last six years\n(between 2019 and 2024), underscoring their profound influence and performance.\nOver the past three decades, more than 500 new metaheuristic algorithms have\nbeen proposed, with no slowdown in sight. An overwhelming abundance that\ncomplicates the process of selecting and assessing the most effective solutions\nfor complex optimization challenges. Our evaluation centers on pivotal\ncriteria, including annual citation metrics, the breadth of the addressed\nproblem types, source code availability, user friendly parameter\nconfigurations, innovative mechanisms and operators, and approaches designed to\nmitigate traditional metaheuristic issues such as stagnation and premature\nconvergence. We further explore recent high impact applications of the past six\nyears' most influential 23 metahueristic algorithms, shedding light on their\nadvantages and limitations, while identifying challenges and potential avenues\nfor future research.",
      "tldr_zh": "这篇综述调查了2019年至2024年间超过150种新的metaheuristic algorithms，强调了它们在复杂优化问题中的影响力和性能表现。\n论文评估了这些算法的关键标准，包括年度引用指标、解决的问题类型、源代码可用性、用户友好的参数配置，以及创新机制来缓解传统问题如stagnation和premature convergence。\n此外，它探讨了最具影响力的23种metaheuristic algorithms的最新高影响应用，分析了它们的优势、局限性，并指出了未来研究挑战和潜在方向。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "A.1; F.2; I.2"
      ],
      "primary_category": "cs.NE",
      "comment": "73 pages, 3 Tables, 12 Figures,on Metaheuristic and Evolutionary\n  Algorithms",
      "pdf_url": "http://arxiv.org/pdf/2501.14769v1",
      "published_date": "2024-12-28 17:41:57 UTC",
      "updated_date": "2024-12-28 17:41:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:17:12.014569"
    },
    {
      "arxiv_id": "2412.20213v1",
      "title": "Decoding Emotion: Speech Perception Patterns in Individuals with Self-reported Depression",
      "title_zh": "解码情感：自我报告抑郁个体的语音感知模式",
      "authors": [
        "Guneesh Vats",
        "Priyanka Srivastava",
        "Chiranjeevi Yarra"
      ],
      "abstract": "The current study examines the relationship between self-reported depression\nand the perception of affective speech within the Indian population. PANAS and\nPHQ-9 were used to assess current mood and depression, respectively.\nParticipants' emotional reactivity was recorded on a valence and arousal scale\nagainst the affective speech audio presented in a sequence. No significant\ndifferences between the depression and no-depression groups were observed for\nany of the emotional stimuli, except the audio file depicting neutral emotion.\nSignificantly higher PANAS scores by the depression than the no-depression\ngroup indicate the impact of pre-disposed mood on the current mood status.\nContrary to previous findings, this study did not observe reduced positive\nemotional reactivity by the depression group. However, the results demonstrated\nconsistency in emotional reactivity for speech stimuli depicting sadness and\nanger across all measures of emotion perception.",
      "tldr_zh": "本研究探讨了印度人群中自报抑郁个体对情感语音的感知模式，使用 PANAS 评估当前情绪和 PHQ-9 评估抑郁，并记录参与者对语音刺激在 valence 和 arousal 尺度上的反应。结果显示，抑郁组和非抑郁组在大多数情感刺激上无显著差异，仅在中性情感音频上有所不同。抑郁组的 PANAS 分数显著更高，表明预先情绪状态影响当前情绪，与先前研究不同的是，本研究未观察到抑郁组对积极情感反应的减少，且在悲伤和愤怒语音上表现出一致的反应。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20213v1",
      "published_date": "2024-12-28 16:54:25 UTC",
      "updated_date": "2024-12-28 16:54:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:17:24.219956"
    },
    {
      "arxiv_id": "2412.20212v1",
      "title": "Building a Rich Dataset to Empower the Persian Question Answering Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Mohsen Yazdinejad",
        "Marjan Kaedi"
      ],
      "abstract": "Question answering systems provide short, precise, and specific answers to\nquestions. So far, many robust question answering systems have been developed\nfor English, while some languages with fewer resources, like Persian, have few\nnumbers of standard dataset. In this study, a comprehensive open-domain dataset\nis presented for Persian. This dataset is called NextQuAD and has 7,515\ncontexts, including 23,918 questions and answers. Then, a BERT-based question\nanswering model has been applied to this dataset using two pre-trained language\nmodels, including ParsBERT and XLM-RoBERTa. The results of these two models\nhave been ensembled using mean logits. Evaluation on the development set shows\n0.95 Exact Match (EM) and 0.97 Fl_score. Also, to compare the NextQuAD with\nother Persian datasets, our trained model on the NextQuAD, is evaluated on two\nother datasets named PersianQA and ParSQuAD. Comparisons show that the proposed\nmodel increased EM by 0.39 and 0.14 respectively in PersianQA and\nParSQuAD-manual, while a slight EM decline of 0.007 happened in\nParSQuAD-automatic.",
      "tldr_zh": "这篇论文构建了一个名为NextQuAD的全面开放域数据集，用于增强波斯语问答系统，该数据集包含7,515个上下文和23,918个问题答案，以解决资源有限语言的标准化数据集缺失问题。研究团队采用BERT-based问答模型，基于预训练模型ParsBERT和XLM-RoBERTa，并通过mean logits集成方法进行训练。实验结果显示，在开发集上模型的Exact Match (EM)达到0.95，F1分数为0.97；在与其他数据集PersianQA和ParSQuAD的比较中，EM分数分别提高了0.39和0.14，仅在ParSQuAD-automatic上略微下降0.007。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20212v1",
      "published_date": "2024-12-28 16:53:25 UTC",
      "updated_date": "2024-12-28 16:53:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:17:36.827004"
    },
    {
      "arxiv_id": "2412.20210v2",
      "title": "Towards Real-Time 2D Mapping: Harnessing Drones, AI, and Computer Vision for Advanced Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Bharath Kumar Agnur"
      ],
      "abstract": "This paper presents an advanced mapping system that combines drone imagery\nwith machine learning and computer vision to overcome challenges in speed,\naccuracy, and adaptability across diverse terrains. By automating processes\nlike feature detection, image matching, and stitching, the system produces\nseamless, high-resolution maps with minimal latency, offering strategic\nadvantages in defense operations. Developed in Python, the system utilizes\nOpenCV for image processing, NumPy for efficient computations, and\nConcurrent[dot]futures for parallel execution. ORB (Oriented FAST and Rotated\nBRIEF) is employed for feature detection, while FLANN (Fast Library for\nApproximate Nearest Neighbors) ensures accurate keypoint matching. Homography\ntransformations align overlapping images, resulting in distortion-free maps in\nreal time. This automation eliminates manual intervention, enabling live\nupdates essential in rapidly changing environments. Designed for versatility,\nthe system performs reliably under various lighting conditions and rugged\nterrains, making it highly suitable for aerospace and defense applications.\nTesting has shown notable improvements in processing speed and accuracy\ncompared to conventional methods, enhancing situational awareness and informed\ndecision-making. This scalable solution leverages cutting-edge technologies to\nprovide actionable, reliable data for mission-critical operations.",
      "tldr_zh": "本论文提出了一种实时 2D 映射系统，通过整合无人机图像、AI 和计算机视觉，解决了传统方法在速度、准确性和适应性方面的挑战。系统自动化了特征检测（使用 ORB）、图像匹配（采用 FLANN）和图像拼接（通过 Homography 变换），并利用 Python、OpenCV、NumPy 和 Concurrent.futures 进行高效处理，实现无缝、高分辨率地图的实时生成。实验结果显示，该系统在各种地形和照明条件下显著提升了处理速度和准确性，为国防操作提供实时更新和决策支持，增强了 situational awareness。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 7 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2412.20210v2",
      "published_date": "2024-12-28 16:47:18 UTC",
      "updated_date": "2024-12-31 15:09:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:17:47.694724"
    },
    {
      "arxiv_id": "2412.20201v1",
      "title": "Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Wen-Dong Jiang",
        "Chih-Yung Chang",
        "Hsiang-Chuan Chang",
        "Ji-Yuan Chen",
        "Diptendu Sinha Roy"
      ],
      "abstract": "Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak\nsupervision learning to identify anomalies, a critical task for smart city\nmonitoring. However, existing multimodal approaches often fail to meet the\nreal-time and interpretability requirements of edge devices due to their\ncomplexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly\nDetection System), which leverages knowledge distillation and cross-modal\ncontrastive learning to enable efficient, accurate, and interpretable anomaly\ndetection on edge devices.TCVADS operates in two stages: coarse-grained rapid\nclassification and fine-grained detailed analysis. In the first stage, TCVADS\nextracts features from video frames and inputs them into a time series analysis\nmodule, which acts as the teacher model. Insights are then transferred via\nknowledge distillation to a simplified convolutional network (student model)\nfor binary classification. Upon detecting an anomaly, the second stage is\ntriggered, employing a fine-grained multi-class classification model. This\nstage uses CLIP for cross-modal contrastive learning with text and images,\nenhancing interpretability and achieving refined classification through\nspecially designed triplet textual relationships. Experimental results\ndemonstrate that TCVADS significantly outperforms existing methods in model\nperformance, detection efficiency, and interpretability, offering valuable\ncontributions to smart city monitoring applications.",
      "tldr_zh": "本文提出 TCVADS（Two-stage Cross-modal Video Anomaly Detection System），一种针对弱监督视频异常检测（WSMAD）的轻量级系统，通过知识蒸馏和跨模态对比学习，提升边缘设备的实时性能和可解释性。系统采用两阶段方法：第一阶段提取视频帧特征，使用教师模型进行粗粒度二元分类，并通过知识蒸馏优化简化卷积网络；第二阶段在检测到异常时，利用 CLIP 进行文本-图像对比学习和三元文本关系分析，实现细粒度多类分类。实验结果表明，TCVADS 在模型性能、检测效率和可解释性方面显著优于现有方法，为智能城市监控应用提供了重要贡献。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE TETC-CS (Under review)",
      "pdf_url": "http://arxiv.org/pdf/2412.20201v1",
      "published_date": "2024-12-28 16:24:35 UTC",
      "updated_date": "2024-12-28 16:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:18:00.534583"
    },
    {
      "arxiv_id": "2412.20200v1",
      "title": "Federated Unlearning with Gradient Descent and Conflict Mitigation",
      "title_zh": "翻译失败",
      "authors": [
        "Zibin Pan",
        "Zhichao Wang",
        "Chi Li",
        "Kaiyan Zheng",
        "Boqi Wang",
        "Xiaoying Tang",
        "Junhua Zhao"
      ],
      "abstract": "Federated Learning (FL) has received much attention in recent years. However,\nalthough clients are not required to share their data in FL, the global model\nitself can implicitly remember clients' local data. Therefore, it's necessary\nto effectively remove the target client's data from the FL global model to ease\nthe risk of privacy leakage and implement ``the right to be forgotten\".\nFederated Unlearning (FU) has been considered a promising way to remove data\nwithout full retraining. But the model utility easily suffers significant\nreduction during unlearning due to the gradient conflicts. Furthermore, when\nconducting the post-training to recover the model utility, the model is prone\nto move back and revert what has already been unlearned. To address these\nissues, we propose Federated Unlearning with Orthogonal Steepest Descent\n(FedOSD). We first design an unlearning Cross-Entropy loss to overcome the\nconvergence issue of the gradient ascent. A steepest descent direction for\nunlearning is then calculated in the condition of being non-conflicting with\nother clients' gradients and closest to the target client's gradient. This\nbenefits to efficiently unlearn and mitigate the model utility reduction. After\nunlearning, we recover the model utility by maintaining the achievement of\nunlearning. Finally, extensive experiments in several FL scenarios verify that\nFedOSD outperforms the SOTA FU algorithms in terms of unlearning and model\nutility.",
      "tldr_zh": "该论文针对 Federated Learning (FL) 中全局模型可能隐式记忆客户端数据的问题，提出 Federated Unlearning with Orthogonal Steepest Descent (FedOSD) 方法，以实现高效的数据删除并缓解隐私泄露风险。FedOSD 通过设计 unlearning Cross-Entropy loss 解决梯度上升的收敛问题，并计算一个与其它客户端梯度非冲突的最陡下降方向，从而减少 unlearning 过程中的模型效用损失，并在后续训练中维持 unlearning 效果。实验结果显示，在多个 FL 场景中，FedOSD 优于现有状态-of-the-art (SOTA) 算法，在 unlearning 效率和模型效用方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2412.20200v1",
      "published_date": "2024-12-28 16:23:10 UTC",
      "updated_date": "2024-12-28 16:23:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:18:12.859269"
    },
    {
      "arxiv_id": "2412.20195v1",
      "title": "Lower bounds on transformers with infinite precision",
      "title_zh": "无限精度变换器的下界",
      "authors": [
        "Alexander Kozachinskiy"
      ],
      "abstract": "In this note, we use the VC dimension technique to prove the first lower\nbound against one-layer softmax transformers with infinite precision. We do so\nfor two tasks: function composition, considered by Peng, Narayanan, and\nPapadimitriou, and the SUM$_2$ task, considered by Sanford, Hsu, and Telgarsky.",
      "tldr_zh": "这篇论文使用 VC dimension 技术，首次证明了对无限精度的一层 softmax transformers 的下界。研究针对两个具体任务：function composition（由 Peng, Narayanan, and Papadimitriou 提出）和 SUM$_2$ 任务（由 Sanford, Hsu, and Telgarsky 提出）。这些结果为理解 transformer 模型的理论限制提供了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20195v1",
      "published_date": "2024-12-28 16:09:25 UTC",
      "updated_date": "2024-12-28 16:09:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:18:22.983798"
    },
    {
      "arxiv_id": "2412.20193v1",
      "title": "Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action Ranker",
      "title_zh": "翻译失败",
      "authors": [
        "Jiangdong Fan",
        "Hongcai He",
        "Paul Weng",
        "Hui Xu",
        "Jie Shao"
      ],
      "abstract": "A major bottleneck in imitation learning is the requirement of a large number\nof expert demonstrations, which can be expensive or inaccessible. Learning from\nsupplementary demonstrations without strict quality requirements has emerged as\na powerful paradigm to address this challenge. However, previous methods often\nfail to fully utilize their potential by discarding non-expert data. Our key\ninsight is that even demonstrations that fall outside the expert distribution\nbut outperform the learned policy can enhance policy performance. To utilize\nthis potential, we propose a novel approach named imitation learning via\nmeta-learning an action ranker (ILMAR). ILMAR implements weighted behavior\ncloning (weighted BC) on a limited set of expert demonstrations along with\nsupplementary demonstrations. It utilizes the functional of the advantage\nfunction to selectively integrate knowledge from the supplementary\ndemonstrations. To make more effective use of supplementary demonstrations, we\nintroduce meta-goal in ILMAR to optimize the functional of the advantage\nfunction by explicitly minimizing the distance between the current policy and\nthe expert policy. Comprehensive experiments using extensive tasks demonstrate\nthat ILMAR significantly outperforms previous methods in handling suboptimal\ndemonstrations. Code is available at https://github.com/F-GOD6/ILMAR.",
      "tldr_zh": "这篇论文解决了模仿学习(Imitation Learning)中对大量专家演示的依赖问题，提出了一种新方法ILMAR，通过meta-learning训练一个action ranker来有效利用次优演示。ILMAR在有限的专家演示上应用加权行为克隆(weighted BC)，并利用优势函数(advantage function)选择性地整合补充演示知识，同时引入meta-goal来最小化当前策略与专家策略之间的距离。实验结果显示，ILMAR在广泛任务中显著优于现有方法，在处理次优演示方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20193v1",
      "published_date": "2024-12-28 16:06:44 UTC",
      "updated_date": "2024-12-28 16:06:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:18:36.020593"
    },
    {
      "arxiv_id": "2501.14768v1",
      "title": "Equation discovery framework EPDE: Towards a better equation discovery",
      "title_zh": "EPDE 方程发现框架：迈向更好的方程发现",
      "authors": [
        "Mikhail Maslyaev",
        "Alexander Hvatov"
      ],
      "abstract": "Equation discovery methods hold promise for extracting knowledge from\nphysics-related data. However, existing approaches often require substantial\nprior information that significantly reduces the amount of knowledge extracted.\nIn this paper, we enhance the EPDE algorithm -- an evolutionary\noptimization-based discovery framework. In contrast to methods like SINDy,\nwhich rely on pre-defined libraries of terms and linearities, our approach\ngenerates terms using fundamental building blocks such as elementary functions\nand individual differentials. Within evolutionary optimization, we may improve\nthe computation of the fitness function as is done in gradient methods and\nenhance the optimization algorithm itself. By incorporating multi-objective\noptimization, we effectively explore the search space, yielding more robust\nequation extraction, even when dealing with complex experimental data. We\nvalidate our algorithm's noise resilience and overall performance by comparing\nits results with those from the state-of-the-art equation discovery framework\nSINDy.",
      "tldr_zh": "本论文介绍了 EPDE 方程发现框架的增强版，旨在从物理相关数据中提取更多知识，而非依赖预定义术语库如 SINDy。EPDE 通过使用基本构建块（如基本函数和单个微分）生成术语，并在进化优化中改进适应度函数计算和优化算法本身，同时引入多目标优化，以更有效地探索搜索空间并提升方程提取的鲁棒性。实验验证显示，该框架在处理噪声和复杂数据时表现出色，并优于现有方法 SINDy。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14768v1",
      "published_date": "2024-12-28 15:58:44 UTC",
      "updated_date": "2024-12-28 15:58:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:18:48.548611"
    },
    {
      "arxiv_id": "2412.20170v1",
      "title": "Real-time Calibration Model for Low-cost Sensor in Fine-grained Time series",
      "title_zh": "细粒度时间序列中低成本传感器的实时校准模型",
      "authors": [
        "Seokho Ahn",
        "Hyungjin Kim",
        "Sungbok Shin",
        "Young-Duk Seo"
      ],
      "abstract": "Precise measurements from sensors are crucial, but data is usually collected\nfrom low-cost, low-tech systems, which are often inaccurate. Thus, they require\nfurther calibrations. To that end, we first identify three requirements for\neffective calibration under practical low-tech sensor conditions. Based on the\nrequirements, we develop a model called TESLA, Transformer for effective sensor\ncalibration utilizing logarithmic-binned attention. TESLA uses a\nhigh-performance deep learning model, Transformers, to calibrate and capture\nnon-linear components. At its core, it employs logarithmic binning to minimize\nattention complexity. TESLA achieves consistent real-time calibration, even\nwith longer sequences and finer-grained time series in hardware-constrained\nsystems. Experiments show that TESLA outperforms existing novel deep learning\nand newly crafted linear models in accuracy, calibration speed, and energy\nefficiency.",
      "tldr_zh": "该研究针对低成本传感器的精确校准问题，识别了三个关键要求，包括处理非线性组件和适应硬件约束，并开发了 TESLA 模型。\nTESLA 基于 Transformer 架构，采用 logarithmic-binned attention 来最小化注意力复杂性，从而实现对细粒度时间序列的实时校准。\n实验结果表明，TESLA 在准确性、校准速度和能效上均优于现有深度学习和线性模型，为实际低技术传感器应用提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20170v1",
      "published_date": "2024-12-28 14:58:46 UTC",
      "updated_date": "2024-12-28 14:58:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:18:59.949914"
    },
    {
      "arxiv_id": "2412.20166v2",
      "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
      "title_zh": "翻译失败",
      "authors": [
        "Hyucksung Kwon",
        "Kyungmo Koo",
        "Janghyeon Kim",
        "Woongkyu Lee",
        "Minjae Lee",
        "Hyungdeok Lee",
        "Yousub Jung",
        "Jaehan Park",
        "Yosub Song",
        "Byeongsu Yang",
        "Haerang Choi",
        "Guhyun Kim",
        "Jongsoon Won",
        "Woojae Shin",
        "Changhyun Kim",
        "Gyeongcheol Shin",
        "Yongkee Kwon",
        "Ilkon Kim",
        "Euicheol Lim",
        "John Kim",
        "Jungwook Choi"
      ],
      "abstract": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
      "tldr_zh": "本论文提出LoL-PIM，一种可扩展的DRAM-PIM系统，用于加速长上下文LLM解码，以解决大型语言模型（LLMs）在数据移动和内存带宽方面的挑战。\nLoL-PIM通过硬件-软件协同设计，结合多节点PIM架构的管道并行性以及直接PIM访问（DPA）控制器，实现动态内存管理和高效处理不同上下文长度。\n实验评估显示，LoL-PIM显著提升LLM推理的吞吐量和降低延迟，比多GPU系统快8.54倍，比GPU-PIM系统快16.0倍，从而促进LLMs在实际应用的更高效部署。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "15 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20166v2",
      "published_date": "2024-12-28 14:38:16 UTC",
      "updated_date": "2025-01-15 01:34:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:19:12.239360"
    },
    {
      "arxiv_id": "2412.20164v1",
      "title": "StyleAutoEncoder for manipulating image attributes using pre-trained StyleGAN",
      "title_zh": "翻译失败",
      "authors": [
        "Andrzej Bedychaj",
        "Jacek Tabor",
        "Marek Śmieja"
      ],
      "abstract": "Deep conditional generative models are excellent tools for creating\nhigh-quality images and editing their attributes. However, training modern\ngenerative models from scratch is very expensive and requires large\ncomputational resources. In this paper, we introduce StyleAutoEncoder\n(StyleAE), a lightweight AutoEncoder module, which works as a plugin for\npre-trained generative models and allows for manipulating the requested\nattributes of images. The proposed method offers a cost-effective solution for\ntraining deep generative models with limited computational resources, making it\na promising technique for a wide range of applications. We evaluate\nStyleAutoEncoder by combining it with StyleGAN, which is currently one of the\ntop generative models. Our experiments demonstrate that StyleAutoEncoder is at\nleast as effective in manipulating image attributes as the state-of-the-art\nalgorithms based on invertible normalizing flows. However, it is simpler,\nfaster, and gives more freedom in designing neural",
      "tldr_zh": "本论文提出了一种轻量级的 StyleAutoEncoder (StyleAE) 模块，作为插件与预训练的生成模型（如 StyleGAN）结合，用于操纵图像属性。该方法通过 AutoEncoder 的设计，提供了一种成本低廉的解决方案，适用于计算资源有限的场景，避免了从零训练深度生成模型的高昂开销。实验结果显示，StyleAutoEncoder 在图像属性操纵方面与基于可逆归一化流的 state-of-the-art 算法相当，但其实现更简单、更快速，并提供了更大的神经网络设计自由度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20164v1",
      "published_date": "2024-12-28 14:30:48 UTC",
      "updated_date": "2024-12-28 14:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:19:23.865457"
    },
    {
      "arxiv_id": "2412.20163v3",
      "title": "Topic-Aware Knowledge Graph with Large Language Models for Interoperability in Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Minhye Jeon",
        "Seokho Ahn",
        "Young-Duk Seo"
      ],
      "abstract": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.",
      "tldr_zh": "本研究提出了一种基于大型语言模型（LLMs）的主题感知知识图谱方法，以提升推荐系统（Recommender Systems）的互操作性，解决数据稀疏和冷启动问题。方法首先从侧信息中迭代提取和更新一般主题（General Topics），然后利用上下文信息提取特定主题（Specific Topics），并通过一个精炼算法处理同义主题问题，从而实现对物品语义特征和用户偏好的全面理解。实验结果显示，该方法在不同知识图谱（Knowledge Graphs）上显著提高了推荐性能，提供了一个一致且无需领域专家干预的整合框架。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20163v3",
      "published_date": "2024-12-28 14:27:45 UTC",
      "updated_date": "2025-02-12 16:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:19:35.353129"
    },
    {
      "arxiv_id": "2412.20155v1",
      "title": "Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Wooseok Han",
        "Minki Kang",
        "Changhun Kim",
        "Eunho Yang"
      ],
      "abstract": "Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable\nattention due to its broad range of applications, such as personalized voice\nassistant services. While several approaches have been proposed, they often\nexhibit high sensitivity to either the quantity or the quality of target speech\nsamples. To address these limitations, we introduce Stable-TTS, a novel\nspeaker-adaptive TTS framework that leverages a small subset of a high-quality\npre-training dataset, referred to as prior samples. Specifically, Stable-TTS\nachieves prosody consistency by leveraging the high-quality prosody of prior\nsamples, while effectively capturing the timbre of the target speaker.\nAdditionally, it employs a prior-preservation loss during fine-tuning to\nmaintain the synthesis ability for prior samples to prevent overfitting on\ntarget samples. Extensive experiments demonstrate the effectiveness of\nStable-TTS even under limited amounts of and noisy target speech samples.",
      "tldr_zh": "该研究提出了一种稳健的说话者自适应文本到语音(TTS)合成框架Stable-TTS，通过prosody prompting技术来解决现有方法对目标语音样本数量和质量的高度敏感问题。Stable-TTS利用高质量预训练数据集的子集（prior samples）来确保prosody一致性，同时捕获目标说话者的timbre，并在微调过程中引入prior-preservation loss以防止过拟合。实验结果表明，即使在有限数量或噪声干扰的目标语音样本下，Stable-TTS也能表现出色，提升了个性化语音助手的合成性能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20155v1",
      "published_date": "2024-12-28 13:54:30 UTC",
      "updated_date": "2024-12-28 13:54:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:19:47.546846"
    },
    {
      "arxiv_id": "2501.00057v2",
      "title": "VisTabNet: Adapting Vision Transformers for Tabular Data",
      "title_zh": "翻译失败",
      "authors": [
        "Witold Wydmański",
        "Ulvi Movsum-zada",
        "Jacek Tabor",
        "Marek Śmieja"
      ],
      "abstract": "Although deep learning models have had great success in natural language\nprocessing and computer vision, we do not observe comparable improvements in\nthe case of tabular data, which is still the most common data type used in\nbiological, industrial and financial applications. In particular, it is\nchallenging to transfer large-scale pre-trained models to downstream tasks\ndefined on small tabular datasets. To address this, we propose VisTabNet -- a\ncross-modal transfer learning method, which allows for adapting Vision\nTransformer (ViT) with pre-trained weights to process tabular data. By\nprojecting tabular inputs to patch embeddings acceptable by ViT, we can\ndirectly apply a pre-trained Transformer Encoder to tabular inputs. This\napproach eliminates the conceptual cost of designing a suitable architecture\nfor processing tabular data, while reducing the computational cost of training\nthe model from scratch. Experimental results on multiple small tabular datasets\n(less than 1k samples) demonstrate VisTabNet's superiority, outperforming both\ntraditional ensemble methods and recent deep learning models. The proposed\nmethod goes beyond conventional transfer learning practice and shows that\npre-trained image models can be transferred to solve tabular problems,\nextending the boundaries of transfer learning. We share our example\nimplementation as a GitHub repository available at\nhttps://github.com/wwydmanski/VisTabNet.",
      "tldr_zh": "本论文提出 VisTabNet，一种跨模态转移学习方法，将预训练的 Vision Transformer (ViT) 适配到 Tabular Data 处理上，以解决深度学习模型在小数据集上转移学习挑战。方法通过将 Tabular 输入投影为 ViT 可接受的 patch embeddings，并直接应用预训练的 Transformer Encoder，从而减少架构设计和训练计算成本。实验结果显示，VisTabNet 在多个小样本数据集（少于 1k 样本）上优于传统集成方法和深度学习模型，扩展了转移学习的应用边界，并提供 GitHub 仓库供参考。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00057v2",
      "published_date": "2024-12-28 13:40:46 UTC",
      "updated_date": "2025-04-25 12:19:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:20:00.137717"
    },
    {
      "arxiv_id": "2501.00056v1",
      "title": "Transforming CCTV cameras into NO$_2$ sensors at city scale for adaptive policymaking",
      "title_zh": "将 CCTV 摄像头转变为城市规模的 NO₂ 传感器，用于适应性政策制定",
      "authors": [
        "Mohamed R. Ibrahim",
        "Terry Lyons"
      ],
      "abstract": "Air pollution in cities, especially NO\\textsubscript{2}, is linked to\nnumerous health problems, ranging from mortality to mental health challenges\nand attention deficits in children. While cities globally have initiated\npolicies to curtail emissions, real-time monitoring remains challenging due to\nlimited environmental sensors and their inconsistent distribution. This gap\nhinders the creation of adaptive urban policies that respond to the sequence of\nevents and daily activities affecting pollution in cities. Here, we demonstrate\nhow city CCTV cameras can act as a pseudo-NO\\textsubscript{2} sensors. Using a\npredictive graph deep model, we utilised traffic flow from London's cameras in\naddition to environmental and spatial factors, generating NO\\textsubscript{2}\npredictions from over 133 million frames. Our analysis of London's mobility\npatterns unveiled critical spatiotemporal connections, showing how specific\ntraffic patterns affect NO\\textsubscript{2} levels, sometimes with temporal\nlags of up to 6 hours. For instance, if trucks only drive at night, their\neffects on NO\\textsubscript{2} levels are most likely to be seen in the morning\nwhen people commute. These findings cast doubt on the efficacy of some of the\nurban policies currently being implemented to reduce pollution. By leveraging\nexisting camera infrastructure and our introduced methods, city planners and\npolicymakers could cost-effectively monitor and mitigate the impact of\nNO\\textsubscript{2} and other pollutants.",
      "tldr_zh": "本研究探讨了如何将城市CCTV摄像头转化为伪NO₂传感器，以实现大规模监测并支持适应性政策制定。研究利用预测性图深度模型（graph deep model），结合伦敦摄像头交通流量数据、环境和空间因素，从超过1.33亿帧图像中生成NO₂预测水平。分析揭示了关键时空联系，例如夜间卡车行驶可能导致早晨通勤时NO₂水平升高，时滞可达6小时，从而质疑现有污染减少政策的有效性。通过这一成本有效的框架，城市规划者和决策者能够更高效地监测和缓解NO₂等污染物的影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.00056v1",
      "published_date": "2024-12-28 13:01:44 UTC",
      "updated_date": "2024-12-28 13:01:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:20:13.304070"
    },
    {
      "arxiv_id": "2412.20138v6",
      "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
      "title_zh": "TradingAgents：多智能体 LLM 金融交易框架",
      "authors": [
        "Yijia Xiao",
        "Edward Sun",
        "Di Luo",
        "Wei Wang"
      ],
      "abstract": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/TauricResearch.",
      "tldr_zh": "该论文提出TradingAgents框架，这是一个基于多智能体LLM的金融交易系统，旨在模拟真实交易公司的协作动态。框架包括专门角色代理，如fundamental analysts、sentiment analysts、technical analysts和traders，以及Bull and Bear researcher agents和风险管理团队，这些代理通过辩论机制、历史数据合成和决策过程来提升交易性能。实验结果显示，TradingAgents相较于基线模型在累计回报、Sharpe ratio和最大回撤方面显著改善，证明了多智能体LLM框架在金融交易中的潜力。该框架已开源在https://github.com/TauricResearch。",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "q-fin.TR",
      "comment": "Oral, Multi-Agent AI in the Real World @ AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20138v6",
      "published_date": "2024-12-28 12:54:06 UTC",
      "updated_date": "2025-04-15 19:23:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:20:24.221968"
    },
    {
      "arxiv_id": "2412.20127v3",
      "title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine Translation Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaopeng Feng",
        "Jiayuan Su",
        "Jiamei Zheng",
        "Jiahan Ren",
        "Yan Zhang",
        "Jian Wu",
        "Hongwei Wang",
        "Zuozhu Liu"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD.",
      "tldr_zh": "该研究提出M-MAD框架，一种基于LLM-as-a-judge的多智能体辩论系统，用于提升机器翻译(MT)评估的准确性和细粒度。M-MAD通过将启发式MQM标准分解成多个评估维度、利用多智能体辩论进行协作推理，并将维度特定结果综合成最终判断，从而克服现有方法的局限。实验结果显示，M-MAD不仅超越所有现有LLM-as-a-judge方法，甚至与最先进的基于参考的自动指标竞争，即使使用GPT-4o mini等次优模型；该框架的设计优势经消融分析得到验证，并已公开代码和数据。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code and data are available at https://github.com/SU-JIAYUAN/M-MAD",
      "pdf_url": "http://arxiv.org/pdf/2412.20127v3",
      "published_date": "2024-12-28 12:11:28 UTC",
      "updated_date": "2025-02-20 12:55:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:20:36.161145"
    },
    {
      "arxiv_id": "2501.14767v1",
      "title": "Leveraging Social Media Data and Artificial Intelligence for Improving Earthquake Response Efforts",
      "title_zh": "翻译失败",
      "authors": [
        "Kalin Kopanov",
        "Velizar Varbanov",
        "Tatiana Atanasova"
      ],
      "abstract": "The integration of social media and artificial intelligence (AI) into\ndisaster management, particularly for earthquake response, represents a\nprofound evolution in emergency management practices. In the digital age,\nreal-time information sharing has reached unprecedented levels, with social\nmedia platforms emerging as crucial communication channels during crises. This\nshift has transformed traditional, centralized emergency services into more\ndecentralized, participatory models of disaster situational awareness. Our\nstudy includes an experimental analysis of 8,900 social media interactions,\nincluding 2,920 posts and 5,980 replies on X (formerly Twitter), following a\nmagnitude 5.1 earthquake in Oklahoma on February 2, 2024. The analysis covers\ndata from the immediate aftermath and extends over the following seven days,\nillustrating the critical role of digital platforms in modern disaster\nresponse. The results demonstrate that social media platforms can be\neffectively used as real-time situational awareness tools, delivering critical\ninformation to society and authorities during emergencies.",
      "tldr_zh": "本研究探讨了如何利用社交媒体数据和人工智能 (AI) 来提升地震响应努力，通过将传统紧急管理转变为更分散化的参与式模型。研究者对2024年2月2日俄克拉荷马州5.1级地震后的8,900条社交媒体互动（包括2,920条帖子和5,980条回复）进行了实验分析，涵盖事件发生后一周的数据。结果显示，社交媒体平台可作为有效的实时 situational awareness 工具，向社会和当局提供关键信息，从而改善灾害响应效率。总的来说，此工作突显了数字平台在现代紧急管理中的重要作用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.SI",
        "I.2.7"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages, 2 figures, EnviroRisks 2024: Environmental Protection and\n  Disaster Risks, Sofia, Bulgaria",
      "pdf_url": "http://arxiv.org/pdf/2501.14767v1",
      "published_date": "2024-12-28 11:08:06 UTC",
      "updated_date": "2024-12-28 11:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:20:47.472849"
    },
    {
      "arxiv_id": "2412.20104v4",
      "title": "SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Wenkun He",
        "Yun Liu",
        "Ruitao Liu",
        "Li Yi"
      ],
      "abstract": "Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.",
      "tldr_zh": "该论文提出 SyncDiff 方法，用于合成多主体人类-物体互动动作，解决 VR/AR 和人类动画领域中动作同步的挑战，尤其针对任意数量的人类、手和物体的复杂场景。SyncDiff 采用同步动作扩散策略，通过单个扩散模型捕捉多主体动作的联合分布，并引入频率域动作分解（frequency-domain motion decomposition）来提升动作保真度。论文还设计了新的对齐分数（alignment scores），通过显式同步策略联合优化数据样本似然和对齐似然。实验结果显示，SyncDiff 在四个数据集上的表现优于现有最先进的方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20104v4",
      "published_date": "2024-12-28 10:12:12 UTC",
      "updated_date": "2025-03-27 02:17:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:21:00.681777"
    },
    {
      "arxiv_id": "2412.20098v1",
      "title": "RFPPO: Motion Dynamic RRT based Fluid Field - PPO for Dynamic TF/TA Routing Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Rongkun Xue",
        "Jing Yang",
        "Yuyang Jiang",
        "Yiming Feng",
        "Zi Yang"
      ],
      "abstract": "Existing local dynamic route planning algorithms, when directly applied to\nterrain following/terrain avoidance, or dynamic obstacle avoidance for large\nand medium-sized fixed-wing aircraft, fail to simultaneously meet the\nrequirements of real-time performance, long-distance planning, and the dynamic\nconstraints of large and medium-sized aircraft. To deal with this issue, this\npaper proposes the Motion Dynamic RRT based Fluid Field - PPO for dynamic TF/TA\nrouting planning. Firstly, the action and state spaces of the proximal policy\ngradient algorithm are redesigned using disturbance flow fields and artificial\npotential field algorithms, establishing an aircraft dynamics model, and\ndesigning a state transition process based on this model. Additionally, a\nreward function is designed to encourage strategies for obstacle avoidance,\nterrain following, terrain avoidance, and safe flight. Experimental results on\nreal DEM data demonstrate that our algorithm can complete long-distance flight\ntasks through collision-free trajectory planning that complies with dynamic\nconstraints, without the need for prior global planning.",
      "tldr_zh": "该论文针对现有动态路由规划算法在动态地形跟随/地形避让（TF/TA）任务中无法同时满足实时性、长距离规划和大型固定翼飞机的动态约束问题，提出了RFPPO算法，该算法结合Motion Dynamic RRT、Fluid Field和Proximal Policy Optimization (PPO)。具体方法包括重设计PPO的状态和动作空间，使用disturbance flow fields和artificial potential field算法构建aircraft dynamics model，并设计基于此的状态转换过程和奖励函数，以鼓励障碍物避让、地形跟随、地形避让及安全飞行。实验结果显示，在真实DEM数据上，该算法无需事先全局规划即可完成长距离无碰撞轨迹规划，并符合动态约束，显著提升了规划性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "2024 IEEE Intelligent Vehicles Symposium",
      "pdf_url": "http://arxiv.org/pdf/2412.20098v1",
      "published_date": "2024-12-28 09:42:02 UTC",
      "updated_date": "2024-12-28 09:42:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:21:12.033448"
    },
    {
      "arxiv_id": "2412.20090v1",
      "title": "From Worms to Mice: Homeostasis Maybe All You Need",
      "title_zh": "翻译失败",
      "authors": [
        "Jesus Marco de Lucas"
      ],
      "abstract": "In this brief and speculative commentary, we explore ideas inspired by neural\nnetworks in machine learning, proposing that a simple neural XOR motif,\ninvolving both excitatory and inhibitory connections, may provide the basis for\na relevant mode of plasticity in neural circuits of living organisms, with\nhomeostasis as the sole guiding principle. This XOR motif simply signals the\ndiscrepancy between incoming signals and reference signals, thereby providing a\nbasis for a loss function in learning neural circuits, and at the same time\nregulating homeostasis by halting the propagation of these incoming signals.\nThe core motif uses a 4:1 ratio of excitatory to inhibitory neurons, and\nsupports broader neural patterns such as the well-known 'winner takes all'\n(WTA) mechanism. We examined the prevalence of the XOR motif in the published\nconnectomes of various organisms with increasing complexity, and found that it\nranges from tens (in C. elegans) to millions (in several Drosophila neuropils)\nand more than tens of millions (in mouse V1 visual cortex). If validated, our\nhypothesis identifies two of the three key components in analogy to machine\nlearning models: the architecture and the loss function. And we propose that a\nrelevant type of biological neural plasticity is simply driven by a basic\ncontrol or regulatory system, which has persisted and adapted despite the\nincreasing complexity of organisms throughout evolution.",
      "tldr_zh": "这篇评论提出，neural XOR motif（涉及兴奋性和抑制性连接）可能作为活体神经回路可塑性的基础，仅以homeostasis作为唯一指导原则，从而提供类似于机器学习的loss function，并通过信号差异调节稳态。研究者分析了从C. elegans到mouse的connectomes，发现XOR motif的普遍性从数十个（蠕虫）到数百万（Drosophila neuropils）再到数千万（mouse V1视觉皮层）。如果该假设得到验证，它将标识出类似于机器学习中的architecture和loss function的关键组件，表明生物神经可塑性由简单控制系统驱动，并在进化中适应生物的复杂性。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "11 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20090v1",
      "published_date": "2024-12-28 09:17:09 UTC",
      "updated_date": "2024-12-28 09:17:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:23:25.450222"
    },
    {
      "arxiv_id": "2412.20088v1",
      "title": "An archaeological Catalog Collection Method Based on Large Vision-Language Models",
      "title_zh": "基于大型视觉语言模型的考古目录收集方法",
      "authors": [
        "Honglin Pang",
        "Yi Chang",
        "Tianjing Duan",
        "Xi Yang"
      ],
      "abstract": "Archaeological catalogs, containing key elements such as artifact images,\nmorphological descriptions, and excavation information, are essential for\nstudying artifact evolution and cultural inheritance. These data are widely\nscattered across publications, requiring automated collection methods. However,\nexisting Large Vision-Language Models (VLMs) and their derivative data\ncollection methods face challenges in accurate image detection and modal\nmatching when processing archaeological catalogs, making automated collection\ndifficult. To address these issues, we propose a novel archaeological catalog\ncollection method based on Large Vision-Language Models that follows an\napproach comprising three modules: document localization, block comprehension\nand block matching. Through practical data collection from the Dabagou and\nMiaozigou pottery catalogs and comparison experiments, we demonstrate the\neffectiveness of our approach, providing a reliable solution for automated\ncollection of archaeological catalogs.",
      "tldr_zh": "该论文提出了一种基于 Large Vision-Language Models (VLMs) 的考古目录收集方法，以解决现有模型在图像检测和模态匹配方面的挑战，从而实现文物图像、形态描述和挖掘信息的自动化整合。方法包括三个模块：document localization（文档定位）、block comprehension（块理解）和block matching（块匹配），用于高效处理散布在出版物中的考古数据。通过对Dabagou和Miaozigou陶器目录的实际数据收集和比较实验，证明了该方法的有效性，为考古目录的自动化收集提供了可靠的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages,4 figures,www source track",
      "pdf_url": "http://arxiv.org/pdf/2412.20088v1",
      "published_date": "2024-12-28 09:10:41 UTC",
      "updated_date": "2024-12-28 09:10:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:23:36.907120"
    },
    {
      "arxiv_id": "2412.20087v1",
      "title": "On the Validity of Traditional Vulnerability Scoring Systems for Adversarial Attacks against LLMs",
      "title_zh": "关于传统漏洞评分系统针对对抗攻击对LLMs有效性的探讨",
      "authors": [
        "Atmane Ayoub Mansour Bahar",
        "Ahmad Samer Wazan"
      ],
      "abstract": "This research investigates the effectiveness of established vulnerability\nmetrics, such as the Common Vulnerability Scoring System (CVSS), in evaluating\nattacks against Large Language Models (LLMs), with a focus on Adversarial\nAttacks (AAs). The study explores the influence of both general and specific\nmetric factors in determining vulnerability scores, providing new perspectives\non potential enhancements to these metrics.\n  This study adopts a quantitative approach, calculating and comparing the\ncoefficient of variation of vulnerability scores across 56 adversarial attacks\non LLMs. The attacks, sourced from various research papers, and obtained\nthrough online databases, were evaluated using multiple vulnerability metrics.\nScores were determined by averaging the values assessed by three distinct LLMs.\nThe results indicate that existing scoring-systems yield vulnerability scores\nwith minimal variation across different attacks, suggesting that many of the\nmetric factors are inadequate for assessing adversarial attacks on LLMs. This\nis particularly true for context-specific factors or those with predefined\nvalue sets, such as those in CVSS. These findings support the hypothesis that\ncurrent vulnerability metrics, especially those with rigid values, are limited\nin evaluating AAs on LLMs, highlighting the need for the development of more\nflexible, generalized metrics tailored to such attacks.\n  This research offers a fresh analysis of the effectiveness and applicability\nof established vulnerability metrics, particularly in the context of\nAdversarial Attacks on Large Language Models, both of which have gained\nsignificant attention in recent years. Through extensive testing and\ncalculations, the study underscores the limitations of these metrics and opens\nup new avenues for improving and refining vulnerability assessment frameworks\nspecifically tailored for LLMs.",
      "tldr_zh": "该研究评估了传统漏洞评分系统（如 Common Vulnerability Scoring System, CVSS）在评估针对 Large Language Models (LLMs) 的 Adversarial Attacks (AAs) 时的有效性，通过分析 56 个攻击样本的变异系数来量化比较分数。研究采用定量方法，使用多个 LLMs 平均评估分数，结果显示现有系统在不同攻击间产生分数变化很小，表明许多指标因素，尤其是上下文特定或预定义值的因素，不适合评估 AAs。总体发现支持了传统指标的局限性，并呼吁开发更灵活、针对性的漏洞评估框架，以更好地适应 LLMs 环境。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "68T50, 68M25,",
        "I.2.7; K.4.1; G.3"
      ],
      "primary_category": "cs.CR",
      "comment": "101 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.20087v1",
      "published_date": "2024-12-28 09:08:37 UTC",
      "updated_date": "2024-12-28 09:08:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:23:48.737787"
    },
    {
      "arxiv_id": "2412.20086v1",
      "title": "MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via Zero-Order Gradient Search",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaohui Wang",
        "Min Zhang",
        "Jingran Yang",
        "Bojie Shao",
        "Min Zhang"
      ],
      "abstract": "Deep neural networks (DNNs) have shown powerful performance in various\napplications and are increasingly being used in decision-making systems.\nHowever, concerns about fairness in DNNs always persist. Some efficient\nwhite-box fairness testing methods about individual fairness have been\nproposed. Nevertheless, the development of black-box methods has stagnated, and\nthe performance of existing methods is far behind that of white-box methods. In\nthis paper, we propose a novel black-box individual fairness testing method\ncalled Model-Agnostic Fairness Testing (MAFT). By leveraging MAFT,\npractitioners can effectively identify and address discrimination in DL models,\nregardless of the specific algorithm or architecture employed. Our approach\nadopts lightweight procedures such as gradient estimation and attribute\nperturbation rather than non-trivial procedures like symbol execution,\nrendering it significantly more scalable and applicable than existing methods.\nWe demonstrate that MAFT achieves the same effectiveness as state-of-the-art\nwhite-box methods whilst improving the applicability to large-scale networks.\nCompared to existing black-box approaches, our approach demonstrates\ndistinguished performance in discovering fairness violations w.r.t\neffectiveness (approximately 14.69 times) and efficiency (approximately 32.58\ntimes).",
      "tldr_zh": "本论文提出了MAFT，一种高效的模型无关黑盒公平性测试方法，用于检测DNNs中的个体公平性问题。\nMAFT采用零-Order Gradient Search，包括梯度估计和属性扰动等轻量级过程，避免了复杂操作如符号执行，从而提高了测试的可扩展性和适用性。\n实验结果表明，MAFT在发现公平性违规方面，与最先进的白盒方法同样有效，且比现有黑盒方法在有效性上提高了约14.69倍，在效率上提高了约32.58倍。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICSE24",
      "pdf_url": "http://arxiv.org/pdf/2412.20086v1",
      "published_date": "2024-12-28 09:07:06 UTC",
      "updated_date": "2024-12-28 09:07:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:24:00.861866"
    },
    {
      "arxiv_id": "2412.20072v2",
      "title": "Extract Information from Hybrid Long Documents Leveraging LLMs: A Framework and Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Chongjian Yue",
        "Xinrun Xu",
        "Xiaojun Ma",
        "Lun Du",
        "Zhiming Ding",
        "Shi Han",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunexplored. The hybrid text often appears in the form of hybrid long documents\n(HLDs), which far exceed the token limit of LLMs. Consequently, we apply an\nAutomated Information Extraction framework (AIE) to enable LLMs to process the\nHLDs and carry out experiments to analyse four important aspects of information\nextraction from HLDs. Given the findings: 1) The effective way to select and\nsummarize the useful part of a HLD. 2) An easy table serialization way is\nenough for LLMs to understand tables. 3) The naive AIE has adaptability in many\ncomplex scenarios. 4) The useful prompt engineering to enhance LLMs on HLDs. To\naddress the issue of dataset scarcity in HLDs and support future work, we also\npropose the Financial Reports Numerical Extraction (FINE) dataset. The dataset\nand code are publicly available in the attachments.",
      "tldr_zh": "本研究探讨了大语言模型 (LLMs) 在处理混合长文档 (HLDs) 的信息提取能力，这些文档包含文本和表格数据，但往往超出 LLMs 的 token 限制。研究提出 Automated Information Extraction 框架 (AIE)，通过有效选择和总结文档有用部分、简单表格序列化方式以及提示工程，帮助 LLMs 理解和分析 HLDs，并在实验中证明其在复杂场景中的适应性。关键发现包括：AIE 框架提高了信息提取效率，并为未来研究提供 Financial Reports Numerical Extraction (FINE) 数据集，以解决数据集稀缺问题。数据集和代码已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.20072v2",
      "published_date": "2024-12-28 07:54:14 UTC",
      "updated_date": "2024-12-31 03:11:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:24:13.429863"
    },
    {
      "arxiv_id": "2412.20070v1",
      "title": "On the Compositional Generalization of Multimodal LLMs for Medical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyang Cai",
        "Junying Chen",
        "Rongsheng Wang",
        "Weihong Wang",
        "Yonglin Deng",
        "Dingjie Song",
        "Yize Chen",
        "Zixu Zhang",
        "Benyou Wang"
      ],
      "abstract": "Multimodal large language models (MLLMs) hold significant potential in the\nmedical field, but their capabilities are often limited by insufficient data in\ncertain medical domains, highlighting the need for understanding what kinds of\nimages can be used by MLLMs for generalization. Current research suggests that\nmulti-task training outperforms single-task as different tasks can benefit each\nother, but they often overlook the internal relationships within these tasks,\nproviding limited guidance on selecting datasets to enhance specific tasks. To\nanalyze this phenomenon, we attempted to employ compositional generalization\n(CG)-the ability of models to understand novel combinations by recombining\nlearned elements-as a guiding framework. Since medical images can be precisely\ndefined by Modality, Anatomical area, and Task, naturally providing an\nenvironment for exploring CG. Therefore, we assembled 106 medical datasets to\ncreate Med-MAT for comprehensive experiments. The experiments confirmed that\nMLLMs can use CG to understand unseen medical images and identified CG as one\nof the main drivers of the generalization observed in multi-task training.\nAdditionally, further studies demonstrated that CG effectively supports\ndatasets with limited data and delivers consistent performance across different\nbackbones, highlighting its versatility and broad applicability. Med-MAT is\npublicly available at https://github.com/FreedomIntelligence/Med-MAT.",
      "tldr_zh": "本研究探讨了Multimodal LLMs在医疗图像领域的Compositional Generalization（CG）能力，旨在解决模型因数据不足而导致的泛化限制问题。作者组装了106个医疗数据集创建了Med-MAT，并通过实验验证了多任务训练优于单任务训练，因为CG能帮助模型理解新组合的图像元素，从而提升泛化性能。结果显示，CG是多任务训练主要驱动因素，并在数据有限场景下表现稳定，同时适用于不同模型backbone；相关数据集已公开在GitHub上。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20070v1",
      "published_date": "2024-12-28 07:50:00 UTC",
      "updated_date": "2024-12-28 07:50:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:24:24.729495"
    },
    {
      "arxiv_id": "2501.00055v1",
      "title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Miao Yu",
        "Junfeng Fang",
        "Yingjie Zhou",
        "Xing Fan",
        "Kun Wang",
        "Shirui Pan",
        "Qingsong Wen"
      ],
      "abstract": "While safety-aligned large language models (LLMs) are increasingly used as\nthe cornerstone for powerful systems such as multi-agent frameworks to solve\ncomplex real-world problems, they still suffer from potential adversarial\nqueries, such as jailbreak attacks, which attempt to induce harmful content.\nResearching attack methods allows us to better understand the limitations of\nLLM and make trade-offs between helpfulness and safety. However, existing\njailbreak attacks are primarily based on opaque optimization techniques (e.g.\ntoken-level gradient descent) and heuristic search methods like LLM refinement,\nwhich fall short in terms of transparency, transferability, and computational\ncost. In light of these limitations, we draw inspiration from the evolution and\ninfection processes of biological viruses and propose LLM-Virus, a jailbreak\nattack method based on evolutionary algorithm, termed evolutionary jailbreak.\nLLM-Virus treats jailbreak attacks as both an evolutionary and transfer\nlearning problem, utilizing LLMs as heuristic evolutionary operators to ensure\nhigh attack efficiency, transferability, and low time cost. Our experimental\nresults on multiple safety benchmarks show that LLM-Virus achieves competitive\nor even superior performance compared to existing attack methods.",
      "tldr_zh": "该研究探讨了安全对齐的大型语言模型（LLMs）面临的越狱攻击问题，这些攻击试图诱导有害内容，并通过研究攻击方法来揭示LLMs的局限性以平衡帮助性和安全性。论文提出LLM-Virus，一种基于进化算法的攻击方法，灵感来源于生物病毒的进化与感染过程，将越狱攻击视为进化问题和转移学习问题，并利用LLMs作为启发式进化操作符，以提升攻击的透明度、转移性和效率。实验结果显示，LLM-Virus在多个安全基准上比现有方法（如基于梯度下降的优化）具有竞争性或优越性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00055v1",
      "published_date": "2024-12-28 07:48:57 UTC",
      "updated_date": "2024-12-28 07:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:24:37.711359"
    },
    {
      "arxiv_id": "2412.20068v1",
      "title": "The Emotional Spectrum of LLMs: Leveraging Empathy and Emotion-Based Markers for Mental Health Support",
      "title_zh": "LLMs 的情感谱系：利用移情和基于情感的标记进行心理健康支持",
      "authors": [
        "Alessandro De Grandi",
        "Federico Ravenda",
        "Andrea Raballo",
        "Fabio Crestani"
      ],
      "abstract": "The increasing demand for mental health services has highlighted the need for\ninnovative solutions, particularly in the realm of psychological conversational\nAI, where the availability of sensitive data is scarce. In this work, we\nexplored the development of a system tailored for mental health support with a\nnovel approach to psychological assessment based on explainable emotional\nprofiles in combination with empathetic conversational models, offering a\npromising tool for augmenting traditional care, particularly where immediate\nexpertise is unavailable. Our work can be divided into two main parts,\nintrinsecaly connected to each other. First, we present RACLETTE, a\nconversational system that demonstrates superior emotional accuracy compared to\nstate-of-the-art benchmarks in both understanding users' emotional states and\ngenerating empathetic responses during conversations, while progressively\nbuilding an emotional profile of the user through their interactions. Second,\nwe show how the emotional profiles of a user can be used as interpretable\nmarkers for mental health assessment. These profiles can be compared with\ncharacteristic emotional patterns associated with different mental disorders,\nproviding a novel approach to preliminary screening and support.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型(LLMs)的情感轮廓和移情标记来增强心理健康支持，针对数据稀缺的问题提出创新解决方案。研究开发了RACLETTE系统，该系统在理解用户情感状态和生成移情响应方面比现有基准更准确，同时通过互动动态构建用户的可解释情感轮廓。最终，论文展示了这些情感轮廓可作为心理健康评估的标记，与不同心理障碍的特征模式比较，提供初步筛查和辅助支持工具。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20068v1",
      "published_date": "2024-12-28 07:42:29 UTC",
      "updated_date": "2024-12-28 07:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:24:49.269825"
    },
    {
      "arxiv_id": "2412.20064v1",
      "title": "VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Lan Chen",
        "Haoxiang Yang",
        "Pengpeng Shao",
        "Haoyu Song",
        "Xiao Wang",
        "Zhicheng Zhao",
        "Yaowei Wang",
        "Yonghong Tian"
      ],
      "abstract": "Pattern recognition leveraging both RGB and Event cameras can significantly\nenhance performance by deploying deep neural networks that utilize a\nfine-tuning strategy. Inspired by the successful application of large models,\nthe introduction of such large models can also be considered to further enhance\nthe performance of multi-modal tasks. However, fully fine-tuning these models\nleads to inefficiency and lightweight fine-tuning methods such as LoRA and\nAdapter have been proposed to achieve a better balance between efficiency and\nperformance. To our knowledge, there is currently no work that has conducted\nparameter-efficient fine-tuning (PEFT) for RGB-Event recognition based on\npre-trained foundation models. To address this issue, this paper proposes a\nnovel PEFT strategy to adapt the pre-trained foundation vision models for the\nRGB-Event-based classification. Specifically, given the RGB frames and event\nstreams, we extract the RGB and event features based on the vision foundation\nmodel ViT with a modality-specific LoRA tuning strategy. The frame difference\nof the dual modalities is also considered to capture the motion cues via the\nframe difference backbone network. These features are concatenated and fed into\nhigh-level Transformer layers for efficient multi-modal feature learning via\nmodality-shared LoRA tuning. Finally, we concatenate these features and feed\nthem into a classification head to achieve efficient fine-tuning. The source\ncode and pre-trained models will be released on\n\\url{https://github.com/Event-AHU/VELoRA}.",
      "tldr_zh": "本文提出VELoRA，一种基于Low-Rank Adaptation的PEFT（Parameter-Efficient Fine-Tuning）策略，用于高效的RGB-Event基于识别任务，以平衡性能和计算效率。方法利用预训练ViT模型提取RGB和Event特征，通过模态特定的LoRA调优和帧差网络捕获运动线索，然后在Transformer层中融合这些特征进行多模态学习。最终，该框架实现高效微调，并在RGB-Event分类中提升准确性，相关源代码和预训练模型将公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2412.20064v1",
      "published_date": "2024-12-28 07:38:23 UTC",
      "updated_date": "2024-12-28 07:38:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:25:00.655210"
    },
    {
      "arxiv_id": "2501.01974v1",
      "title": "Hawkes based Representation Learning for Reasoning over Scale-free Community-structured Temporal Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yuwei Du",
        "Xinyue Liu",
        "Wenxin Liang",
        "Linlin Zong",
        "Xianchao Zhang"
      ],
      "abstract": "Temporal knowledge graph (TKG) reasoning has become a hot topic due to its\ngreat value in many practical tasks. The key to TKG reasoning is modeling the\nstructural information and evolutional patterns of the TKGs. While great\nefforts have been devoted to TKG reasoning, the structural and evolutional\ncharacteristics of real-world networks have not been considered. In the aspect\nof structure, real-world networks usually exhibit clear community structure and\nscale-free (long-tailed distribution) properties. In the aspect of evolution,\nthe impact of an event decays with the time elapsing. In this paper, we propose\na novel TKG reasoning model called Hawkes process-based Evolutional\nRepresentation Learning Network (HERLN), which learns structural information\nand evolutional patterns of a TKG simultaneously, considering the\ncharacteristics of real-world networks: community structure, scale-free and\ntemporal decaying. First, we find communities in the input TKG to make the\nencoding get more similar intra-community embeddings. Second, we design a\nHawkes process-based relational graph convolutional network to cope with the\nevent impact-decaying phenomenon. Third, we design a conditional decoding\nmethod to alleviate biases towards frequent entities caused by long-tailed\ndistribution. Experimental results show that HERLN achieves significant\nimprovements over the state-of-the-art models.",
      "tldr_zh": "本文提出了一种名为 HERLN 的新模型，用于 Temporal Knowledge Graphs (TKG) 推理，旨在同时学习 TKG 的结构信息（如 community structure 和 scale-free 属性）和演化模式（如事件影响衰减）。HERLN 通过在 TKG 中发现社区以优化内部嵌入、设计基于 Hawkes process 的关系图卷积网络处理时间衰减现象，以及引入条件解码方法缓解长尾分布对频繁实体的偏见。实验结果显示，HERLN 比现有最先进模型取得了显著改进。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01974v1",
      "published_date": "2024-12-28 06:47:51 UTC",
      "updated_date": "2024-12-28 06:47:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:25:13.033115"
    },
    {
      "arxiv_id": "2412.20048v1",
      "title": "CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language and Speaker Generation",
      "title_zh": "CrossSpeech++：解耦语言和说话者生成的跨语言语音合成",
      "authors": [
        "Ji-Hoon Kim",
        "Hong-Sun Yang",
        "Yoon-Cheol Ju",
        "Il-Hwan Kim",
        "Byeong-Yeol Kim",
        "Joon Son Chung"
      ],
      "abstract": "The goal of this work is to generate natural speech in multiple languages\nwhile maintaining the same speaker identity, a task known as cross-lingual\nspeech synthesis. A key challenge of cross-lingual speech synthesis is the\nlanguage-speaker entanglement problem, which causes the quality of\ncross-lingual systems to lag behind that of intra-lingual systems. In this\npaper, we propose CrossSpeech++, which effectively disentangles language and\nspeaker information and significantly improves the quality of cross-lingual\nspeech synthesis. To this end, we break the complex speech generation pipeline\ninto two simple components: language-dependent and speaker-dependent\ngenerators. The language-dependent generator produces linguistic variations\nthat are not biased by specific speaker attributes. The speaker-dependent\ngenerator models acoustic variations that characterize speaker identity. By\nhandling each type of information in separate modules, our method can\neffectively disentangle language and speaker representation. We conduct\nextensive experiments using various metrics, and demonstrate that CrossSpeech++\nachieves significant improvements in cross-lingual speech synthesis,\noutperforming existing methods by a large margin.",
      "tldr_zh": "本文提出 CrossSpeech++ 方法，用于 cross-lingual speech synthesis 的任务，旨在生成多语言自然语音的同时保持相同说话者身份，并解决语言和说话者信息纠缠的问题。CrossSpeech++ 将语音生成管道分解为两个组件：language-dependent generator 负责产生不依赖特定说话者属性的语言变体，以及 speaker-dependent generator 建模表征说话者身份的声学变体。通过这种解耦设计，有效分离语言和说话者表示，提升了合成质量。实验结果显示，该方法在各种指标上大幅优于现有方法，显著提高了 cross-lingual speech synthesis 的性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20048v1",
      "published_date": "2024-12-28 06:32:49 UTC",
      "updated_date": "2024-12-28 06:32:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:25:25.473933"
    },
    {
      "arxiv_id": "2412.20045v1",
      "title": "Enhancing Diffusion Models for Inverse Problems with Covariance-Aware Posterior Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Shayan Mohajer Hamidi",
        "En-Hui Yang"
      ],
      "abstract": "Inverse problems exist in many disciplines of science and engineering. In\ncomputer vision, for example, tasks such as inpainting, deblurring, and super\nresolution can be effectively modeled as inverse problems. Recently, denoising\ndiffusion probabilistic models (DDPMs) are shown to provide a promising\nsolution to noisy linear inverse problems without the need for additional task\nspecific training. Specifically, with the prior provided by DDPMs, one can\nsample from the posterior by approximating the likelihood. In the literature,\napproximations of the likelihood are often based on the mean of conditional\ndensities of the reverse process, which can be obtained using Tweedie formula.\nTo obtain a better approximation to the likelihood, in this paper we first\nderive a closed form formula for the covariance of the reverse process. Then,\nwe propose a method based on finite difference method to approximate this\ncovariance such that it can be readily obtained from the existing pretrained\nDDPMs, thereby not increasing the complexity compared to existing approaches.\nFinally, based on the mean and approximated covariance of the reverse process,\nwe present a new approximation to the likelihood. We refer to this method as\ncovariance-aware diffusion posterior sampling (CA-DPS). Experimental results\nshow that CA-DPS significantly improves reconstruction performance without\nrequiring hyperparameter tuning. The code for the paper is put in the\nsupplementary materials.",
      "tldr_zh": "本文提出了一种改进扩散模型（DDPMs）用于逆问题的方法，针对传统后验采样仅依赖均值的不足，通过推导逆过程的协方差闭合公式并使用有限差分方法进行近似，从而实现更准确的似然近似。新的协方差感知扩散后验采样（CA-DPS）方法无需额外训练或超参数调整，即可从预训练的 DDPMs 中直接获取协方差。实验结果显示，CA-DPS 在图像修复、去模糊和超分辨率等任务上显著提升了重建性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20045v1",
      "published_date": "2024-12-28 06:17:44 UTC",
      "updated_date": "2024-12-28 06:17:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:25:36.595724"
    },
    {
      "arxiv_id": "2412.20024v2",
      "title": "BaiJia: A Large-Scale Role-Playing Agent Corpus of Chinese Historical Characters",
      "title_zh": "BaiJia：中国历史人物的大规模角色扮演代理语料库",
      "authors": [
        "Ting Bai",
        "Jiazheng Kang",
        "Jiayang Fan"
      ],
      "abstract": "We introduce a comprehensive large-scale role-playing agent corpus, termed\nBaiJia, that comprises various Chinese historical characters. This corpus is\nnoteworthy for being the pioneering compilation of low-resource data that can\nbe utilized in large language models (LLMs) to engage in AI-driven historical\nrole-playing agents. BaiJia addresses the challenges in terms of fragmented\nhistorical textual records in different forms and modalities, integrating\nvarious characters' information, including their biographical, literary, family\nrelations, historical events, and so on. We conduct extensive experiments to\ndemonstrate the effectiveness of our BaiJia agent corpus in bolstering the\nrole-playing abilities of various foundational LLMs, and promoting the\ndevelopment and assessment of LLMs in the context of historical role-playing\ntasks. The agent corpus is available at baijia.online.",
      "tldr_zh": "本研究引入了 BaiJia，这是一个大规模的角色扮演代理语料库，涵盖各种中国历史人物，是首个针对低资源数据的编译，用于提升大型语言模型 (LLMs) 在 AI 驱动历史角色扮演中的表现。BaiJia 通过整合碎片化的历史文本记录，包括人物的传记、文学作品、家族关系和历史事件等信息，解决了数据多样性和碎片化挑战。实验结果显示，该语料库显著增强了多种基础 LLMs 的角色扮演能力，并促进了历史角色扮演任务的开发和评估。语料库可公开访问于 baijia.online。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20024v2",
      "published_date": "2024-12-28 05:01:26 UTC",
      "updated_date": "2025-01-06 04:34:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:25:49.510976"
    },
    {
      "arxiv_id": "2501.00054v1",
      "title": "AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors",
      "title_zh": "翻译失败",
      "authors": [
        "Mengnan Zhao",
        "Lihe Zhang",
        "Xingyi Yang",
        "Tianhang Zheng",
        "Baocai Yin"
      ],
      "abstract": "Security concerns surrounding text-to-image diffusion models have driven\nresearchers to unlearn inappropriate concepts through fine-tuning. Recent\nfine-tuning methods typically align the prediction distributions of unsafe\nprompts with those of predefined text anchors. However, these techniques\nexhibit a considerable performance trade-off between eliminating undesirable\nconcepts and preserving other concepts. In this paper, we systematically\nanalyze the impact of diverse text anchors on unlearning performance. Guided by\nthis analysis, we propose AdvAnchor, a novel approach that generates\nadversarial anchors to alleviate the trade-off issue. These adversarial anchors\nare crafted to closely resemble the embeddings of undesirable concepts to\nmaintain overall model performance, while selectively excluding defining\nattributes of these concepts for effective erasure. Extensive experiments\ndemonstrate that AdvAnchor outperforms state-of-the-art methods. Our code is\npublicly available at https://anonymous.4open.science/r/AdvAnchor.",
      "tldr_zh": "本研究针对文本到图像扩散模型（diffusion models）的安全问题，分析了现有微调（fine-tuning）方法在删除不适当概念时存在的性能权衡，即难以同时消除不良概念并保留其他概念。论文提出AdvAnchor，一种新型方法，通过生成对抗锚点（adversarial anchors）来优化unlearning过程，这些锚点与不良概念的嵌入相似，但选择性地排除其定义属性，从而有效擦除目标概念同时维持模型整体性能。实验结果显示，AdvAnchor优于现有最先进方法，且相关代码已公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00054v1",
      "published_date": "2024-12-28 04:44:07 UTC",
      "updated_date": "2024-12-28 04:44:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:26:00.617053"
    },
    {
      "arxiv_id": "2412.20020v1",
      "title": "Calibre: Towards Fair and Accurate Personalized Federated Learning with Self-Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sijia Chen",
        "Ningxin Su",
        "Baochun Li"
      ],
      "abstract": "In the context of personalized federated learning, existing approaches train\na global model to extract transferable representations, based on which any\nclient could train personalized models with a limited number of data samples.\nSelf-supervised learning is considered a promising direction as the global\nmodel it produces is generic and facilitates personalization for all clients\nfairly. However, when data is heterogeneous across clients, the global model\ntrained using SSL is unable to learn high-quality personalized models. In this\npaper, we show that when the global model is trained with SSL without\nmodifications, its produced representations have fuzzy class boundaries. As a\nresult, personalized learning within each client produces models with low\naccuracy. In order to improve SSL towards better accuracy without sacrificing\nits advantage in fairness, we propose Calibre, a new personalized federated\nlearning framework designed to calibrate SSL representations by maintaining a\nsuitable balance between more generic and more client-specific representations.\nCalibre is designed based on theoretically-sound properties, and introduces (1)\na client-specific prototype loss as an auxiliary training objective; and (2) an\naggregation algorithm guided by such prototypes across clients. Our\nexperimental results in an extensive array of non-i.i.d.~settings show that\nCalibre achieves state-of-the-art performance in terms of both mean accuracy\nand fairness across clients. Code repo:\nhttps://github.com/TL-System/plato/tree/main/examples/ssl/calibre.",
      "tldr_zh": "本论文针对个性化联邦学习(Federated Learning)中的问题，指出当客户端数据异构时，Self-Supervised Learning(SSL)训练的全局模型会产生模糊类边界，导致个性化模型准确率低下。作者提出Calibre框架，通过引入客户端特定原型损失作为辅助训练目标，以及基于这些原型的聚合算法，来校准SSL表示，实现通用表示与客户端特定表示的平衡。实验结果显示，Calibre在多种非i.i.d.设置下，实现了最先进的平均准确率和客户端公平性性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "ICDCS camera-ready paper, Code repo:\n  https://github.com/TL-System/plato/tree/main/examples/ssl/calibre",
      "pdf_url": "http://arxiv.org/pdf/2412.20020v1",
      "published_date": "2024-12-28 04:43:39 UTC",
      "updated_date": "2024-12-28 04:43:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:26:12.974924"
    },
    {
      "arxiv_id": "2412.20014v1",
      "title": "ProtCLIP: Function-Informed Protein Multi-Modal Learning",
      "title_zh": "ProtCLIP：功能指导的蛋白质多模态学习",
      "authors": [
        "Hanjing Zhou",
        "Mingze Yin",
        "Wei Wu",
        "Mingyang Li",
        "Kun Fu",
        "Jintai Chen",
        "Jian Wu",
        "Zheng Wang"
      ],
      "abstract": "Multi-modality pre-training paradigm that aligns protein sequences and\nbiological descriptions has learned general protein representations and\nachieved promising performance in various downstream applications. However,\nthese works were still unable to replicate the extraordinary success of\nlanguage-supervised visual foundation models due to the ineffective usage of\naligned protein-text paired data and the lack of an effective function-informed\npre-training paradigm. To address these issues, this paper curates a\nlarge-scale protein-text paired dataset called ProtAnno with a property-driven\nsampling strategy, and introduces a novel function-informed protein\npre-training paradigm. Specifically, the sampling strategy determines selecting\nprobability based on the sample confidence and property coverage, balancing the\ndata quality and data quantity in face of large-scale noisy data. Furthermore,\nmotivated by significance of the protein specific functional mechanism, the\nproposed paradigm explicitly model protein static and dynamic functional\nsegments by two segment-wise pre-training objectives, injecting fine-grained\ninformation in a function-informed manner. Leveraging all these innovations, we\ndevelop ProtCLIP, a multi-modality foundation model that comprehensively\nrepresents function-aware protein embeddings. On 22 different protein\nbenchmarks within 5 types, including protein functionality classification,\nmutation effect prediction, cross-modal transformation, semantic similarity\ninference and protein-protein interaction prediction, our ProtCLIP consistently\nachieves SOTA performance, with remarkable improvements of 75% on average in\nfive cross-modal transformation benchmarks, 59.9% in GO-CC and 39.7% in GO-BP\nprotein function prediction. The experimental results verify the extraordinary\npotential of ProtCLIP serving as the protein multi-modality foundation model.",
      "tldr_zh": "该论文提出ProtCLIP，一种功能信息驱动的蛋白质多模态学习框架，旨在解决现有方法在对齐蛋白序列和生物描述时的数据利用和预训练范式不足的问题。研究者构建了大型蛋白质-文本配对数据集ProtAnno，使用基于属性的采样策略(property-driven sampling strategy)来平衡数据质量和数量，并引入功能信息预训练范式，通过两个分段预训练目标(segment-wise pre-training objectives)来显式建模蛋白质的静态和动态功能段。实验结果显示，ProtCLIP在22个蛋白质基准测试中，包括功能分类、突变效应预测和跨模态转换等领域，实现了SOTA性能，平均在五种跨模态转换基准上提升75%、GO-CC提升59.9%、GO-BP提升39.7%，证明了其作为蛋白质多模态基础模型的巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20014v1",
      "published_date": "2024-12-28 04:23:47 UTC",
      "updated_date": "2024-12-28 04:23:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:26:26.301893"
    },
    {
      "arxiv_id": "2412.20005v2",
      "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
      "title_zh": "翻译失败",
      "authors": [
        "Yujie Luo",
        "Xiangyuan Ru",
        "Kangwei Liu",
        "Lin Yuan",
        "Mengshu Sun",
        "Ningyu Zhang",
        "Lei Liang",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Lanning Wei",
        "Da Zheng",
        "Haofen Wang",
        "Huajun Chen"
      ],
      "abstract": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4.",
      "tldr_zh": "本研究引入 OneKE，一种基于 Dockerized 的、Schema-Guided 的 LLM Agent-based 知识提取系统，能够从 Web 和原始 PDF 书籍中提取知识，并支持多种领域（如科学和新闻）。系统采用多代理设计，每个代理负责特定角色，以处理各种提取场景，同时通过可配置知识库实现模式配置、错误调试和性能优化。实验评估显示 OneKE 在基准数据集上表现出色，并通过案例研究证明其在多领域任务的适应性；代码已开源于 https://github.com/zjunlp/OneKE，并附有演示视频。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "WWW 2025 Demonstration",
      "pdf_url": "http://arxiv.org/pdf/2412.20005v2",
      "published_date": "2024-12-28 04:01:30 UTC",
      "updated_date": "2025-02-06 10:37:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:26:36.846487"
    },
    {
      "arxiv_id": "2412.20004v1",
      "title": "Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous Devices",
      "title_zh": "在异构设备上的适应性参数高效联邦微调",
      "authors": [
        "Jun Liu",
        "Yunming Liao",
        "Hongli Xu",
        "Yang Xu",
        "Jianchun Liu",
        "Chen Qian"
      ],
      "abstract": "Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained\nlanguage models in a distributed manner. However, there are two critical\nchallenges for efficient FedFT in practical applications, i.e., resource\nconstraints and system heterogeneity. Existing works rely on\nparameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but\nwith major limitations. Herein, based on the inherent characteristics of FedFT,\nwe observe that LoRA layers with higher ranks added close to the output help to\nsave resource consumption while achieving comparable fine-tuning performance.\nThen we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces\nthe difficulty of determining the number of LoRA layers (called, LoRA depth)\nand the rank of each LoRA layer (called, rank distribution). We analyze the\ncoupled relationship between LoRA depth and rank distribution, and design an\nefficient LoRA configuration algorithm for heterogeneous devices, thereby\npromoting fine-tuning efficiency. Extensive experiments are conducted on a\nphysical platform with 80 commercial devices. The results show that LEGEND can\nachieve a speedup of 1.5-2.8$\\times$ and save communication costs by about\n42.3% when achieving the target accuracy, compared to the advanced solutions.",
      "tldr_zh": "该研究针对Federated fine-tuning (FedFT) 在异构设备上的资源约束和系统异构性挑战，观察到Low-rank adaptation (LoRA) 中靠近输出的高秩层能节省资源同时保持性能。论文提出了一种新型框架LEGEND，通过分析LoRA深度和秩分布的耦合关系，设计了一个高效的LoRA配置算法，以适应异构设备并提升微调效率。在80个商业设备上的实验显示，LEGEND在达到目标准确率的同时，实现1.5-2.8倍的加速，并节省约42.3%的通信成本。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.20004v1",
      "published_date": "2024-12-28 04:00:42 UTC",
      "updated_date": "2024-12-28 04:00:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:26:49.185476"
    },
    {
      "arxiv_id": "2412.19999v1",
      "title": "Comprehensive Review of EEG-to-Output Research: Decoding Neural Signals into Images, Videos, and Audio",
      "title_zh": "EEG-to-Output 研究的全面综述：将神经信号解码成图像、视频和音频",
      "authors": [
        "Yashvir Sabharwal",
        "Balaji Rama"
      ],
      "abstract": "Electroencephalography (EEG) is an invaluable tool in neuroscience, offering\ninsights into brain activity with high temporal resolution. Recent advancements\nin machine learning and generative modeling have catalyzed the application of\nEEG in reconstructing perceptual experiences, including images, videos, and\naudio. This paper systematically reviews EEG-to-output research, focusing on\nstate-of-the-art generative methods, evaluation metrics, and data challenges.\nUsing PRISMA guidelines, we analyze 1800 studies and identify key trends,\nchallenges, and opportunities in the field. The findings emphasize the\npotential of advanced models such as Generative Adversarial Networks (GANs),\nVariational Autoencoders (VAEs), and Transformers, while highlighting the\npressing need for standardized datasets and cross-subject generalization. A\nroadmap for future research is proposed that aims to improve decoding accuracy\nand broadening real-world applications.",
      "tldr_zh": "这篇论文对 EEG-to-Output 研究进行了系统综述，聚焦于利用脑电图（EEG）解码神经信号以重建图像、视频和音频。作者通过 PRISMA 指南分析了 1800 篇研究，强调了 Generative Adversarial Networks (GANs)、Variational Autoencoders (VAEs) 和 Transformers 等先进生成模型的潜力，同时指出了数据挑战和标准化数据集的迫切需求。论文还提出了未来研究路线图，旨在提升解码准确性和扩展实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages. Submitted as a conference paper to IntelliSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.19999v1",
      "published_date": "2024-12-28 03:50:56 UTC",
      "updated_date": "2024-12-28 03:50:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:29:01.062982"
    },
    {
      "arxiv_id": "2412.19994v1",
      "title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Han",
        "Ziping Wan",
        "Lu Chen",
        "Kai Yu",
        "Xin Chen"
      ],
      "abstract": "Large Language Models (LLMs) have significantly transformed our daily life\nand established a new paradigm in natural language processing (NLP). However,\nthe predominant pretraining of LLMs on extensive web-based texts remains\ninsufficient for advanced scientific discovery, particularly in chemistry. The\nscarcity of specialized chemistry data, coupled with the complexity of\nmulti-modal data such as 2D graph, 3D structure and spectrum, present distinct\nchallenges. Although several studies have reviewed Pretrained Language Models\n(PLMs) in chemistry, there is a conspicuous absence of a systematic survey\nspecifically focused on chemistry-oriented LLMs. In this paper, we outline\nmethodologies for incorporating domain-specific chemistry knowledge and\nmulti-modal information into LLMs, we also conceptualize chemistry LLMs as\nagents using chemistry tools and investigate their potential to accelerate\nscientific research. Additionally, we conclude the existing benchmarks to\nevaluate chemistry ability of LLMs. Finally, we critically examine the current\nchallenges and identify promising directions for future research. Through this\ncomprehensive survey, we aim to assist researchers in staying at the forefront\nof developments in chemistry LLMs and to inspire innovative applications in the\nfield.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）在化学领域的应用进行系统性调查，探讨从通用模型向专业化模型的转变，以应对基于网络文本预训练的不足和化学多模态数据（如2D图、3D结构和谱图）的复杂性。论文概述了整合领域特定化学知识和多模态信息的方法，并将化学LLMs概念化为使用化学工具的代理，以加速科学发现和研究。最终，它总结了现有基准用于评估LLMs的化学能力，审视了当前挑战，并提出了未来研究的方向，以推动该领域的创新应用。",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "COLING2025,We maintain an up-to-date Github repository at:\n  https://github.com/OpenDFM/LLM4Chemistry",
      "pdf_url": "http://arxiv.org/pdf/2412.19994v1",
      "published_date": "2024-12-28 03:40:25 UTC",
      "updated_date": "2024-12-28 03:40:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:27:13.408033"
    },
    {
      "arxiv_id": "2412.19992v1",
      "title": "An Ordinary Differential Equation Sampler with Stochastic Start for Diffusion Bridge Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuang Wang",
        "Pengfei Jin",
        "Li Zhang",
        "Quanzheng Li",
        "Zhiqiang Chen",
        "Dufan Wu"
      ],
      "abstract": "Diffusion bridge models have demonstrated promising performance in\nconditional image generation tasks, such as image restoration and translation,\nby initializing the generative process from corrupted images instead of pure\nGaussian noise. However, existing diffusion bridge models often rely on\nStochastic Differential Equation (SDE) samplers, which result in slower\ninference speed compared to diffusion models that employ high-order Ordinary\nDifferential Equation (ODE) solvers for acceleration. To mitigate this gap, we\npropose a high-order ODE sampler with a stochastic start for diffusion bridge\nmodels. To overcome the singular behavior of the probability flow ODE (PF-ODE)\nat the beginning of the reverse process, a posterior sampling approach was\nintroduced at the first reverse step. The sampling was designed to ensure a\nsmooth transition from corrupted images to the generative trajectory while\nreducing discretization errors. Following this stochastic start, Heun's\nsecond-order solver is applied to solve the PF-ODE, achieving high perceptual\nquality with significantly reduced neural function evaluations (NFEs). Our\nmethod is fully compatible with pretrained diffusion bridge models and requires\nno additional training. Extensive experiments on image restoration and\ntranslation tasks, including super-resolution, JPEG restoration,\nEdges-to-Handbags, and DIODE-Outdoor, demonstrated that our sampler outperforms\nstate-of-the-art methods in both visual quality and Frechet Inception Distance\n(FID).",
      "tldr_zh": "本文提出了一种带有随机起始的高阶 Ordinary Differential Equation (ODE) 采样器，用于提升 Diffusion bridge models 在条件图像生成任务（如图像修复和翻译）中的推理速度，以解决现有 Stochastic Differential Equation (SDE) 采样器的低效问题。该方法通过在反向过程的第一步引入后验采样，确保从损坏图像平滑过渡到生成轨迹，减少离散化错误，并随后使用 Heun's second-order solver 解决 Probability Flow ODE (PF-ODE)，从而显著降低神经函数评估 (NFEs)。实验在超分辨率、JPEG 修复、Edges-to-Handbags 和 DIODE-Outdoor 等任务上表明，该采样器在视觉质量和 Frechet Inception Distance (FID) 上优于现有方法，且与预训练模型完全兼容，无需额外训练。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 5 figures, This work has been submitted to the IEEE for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2412.19992v1",
      "published_date": "2024-12-28 03:32:26 UTC",
      "updated_date": "2024-12-28 03:32:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:27:25.589960"
    },
    {
      "arxiv_id": "2501.14766v1",
      "title": "Artificial Intelligence for Sustainable Urban Biodiversity: A Framework for Monitoring and Conservation",
      "title_zh": "翻译失败",
      "authors": [
        "Yasmin Rahmati"
      ],
      "abstract": "The rapid expansion of urban areas challenges biodiversity conservation,\nrequiring innovative ecosystem management. This study explores the role of\nArtificial Intelligence (AI) in urban biodiversity conservation, its\napplications, and a framework for implementation. Key findings show that: (a)\nAI enhances species detection and monitoring, achieving over 90% accuracy in\nurban wildlife tracking and invasive species management; (b) integrating data\nfrom remote sensing, acoustic monitoring, and citizen science enables\nlarge-scale ecosystem analysis; and (c) AI decision tools improve conservation\nplanning and resource allocation, increasing prediction accuracy by up to 18.5%\ncompared to traditional methods. The research presents an AI-Driven Framework\nfor Urban Biodiversity Management, highlighting AI's impact on monitoring,\nconservation strategies, and ecological outcomes. Implementation strategies\ninclude: (a) standardizing data collection and model validation, (b) ensuring\nequitable AI access across urban contexts, and (c) developing ethical\nguidelines for biodiversity monitoring. The study concludes that integrating AI\nin urban biodiversity conservation requires balancing innovation with\necological wisdom and addressing data quality, socioeconomic disparities, and\nethical concerns.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）在可持续城市生物多样性保护中的应用，提出一个AI-Driven Framework for Urban Biodiversity Management，以应对城市扩张对生态系统的挑战。研究发现，AI 通过物种检测和监测技术实现了超过90%的准确率，用于城市野生动物追踪和入侵物种管理，同时整合遥感、声学监测以及公民科学数据进行大规模生态分析。AI 决策工具显著提升了保护规划和资源分配的效率，比传统方法提高预测准确率18.5%。此外，论文强调实施策略，包括标准化数据收集和模型验证、确保AI在不同城市环境的公平访问，以及制定生物多样性监测的道德准则。最终，研究呼吁在整合AI时平衡创新与生态智慧，解决数据质量、社会经济差异和伦理问题。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14766v1",
      "published_date": "2024-12-28 03:18:56 UTC",
      "updated_date": "2024-12-28 03:18:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:29:12.900199"
    },
    {
      "arxiv_id": "2412.19987v1",
      "title": "Delayed Random Partial Gradient Averaging for Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyi Hu"
      ],
      "abstract": "Federated learning (FL) is a distributed machine learning paradigm that\nenables multiple clients to train a shared model collaboratively while\npreserving privacy. However, the scaling of real-world FL systems is often\nlimited by two communication bottlenecks:(a) while the increasing computing\npower of edge devices enables the deployment of large-scale Deep Neural\nNetworks (DNNs), the limited bandwidth constraints frequent transmissions over\nlarge DNNs; and (b) high latency cost greatly degrades the performance of FL.\nIn light of these bottlenecks, we propose a Delayed Random Partial Gradient\nAveraging (DPGA) to enhance FL. Under DPGA, clients only share partial local\nmodel gradients with the server. The size of the shared part in a local model\nis determined by the update rate, which is coarsely initialized and\nsubsequently refined over the temporal dimension. Moreover, DPGA largely\nreduces the system run time by enabling computation in parallel with\ncommunication. We conduct experiments on non-IID CIFAR-10/100 to demonstrate\nthe efficacy of our method.",
      "tldr_zh": "本研究针对联邦学习(Federated Learning, FL)中的通信瓶颈问题，如带宽限制和大模型传输延迟，提出了一种Delayed Random Partial Gradient Averaging (DPGA)方法。DPGA允许客户端仅共享部分本地模型梯度，并通过动态更新率来优化共享部分的大小，同时实现计算与通信的并行处理，以显著减少系统运行时间。在非独立同分布(non-IID)数据集CIFAR-10/100上的实验表明，该方法有效提升了FL的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.19987v1",
      "published_date": "2024-12-28 03:14:27 UTC",
      "updated_date": "2024-12-28 03:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:31:02.478920"
    },
    {
      "arxiv_id": "2412.19985v1",
      "title": "The Fifth International Verification of Neural Networks Competition (VNN-COMP 2024): Summary and Results",
      "title_zh": "第五届国际神经网络验证竞赛 (VNN-COMP",
      "authors": [
        "Christopher Brix",
        "Stanley Bak",
        "Taylor T. Johnson",
        "Haoze Wu"
      ],
      "abstract": "This report summarizes the 5th International Verification of Neural Networks\nCompetition (VNN-COMP 2024), held as a part of the 7th International Symposium\non AI Verification (SAIV), that was collocated with the 36th International\nConference on Computer-Aided Verification (CAV). VNN-COMP is held annually to\nfacilitate the fair and objective comparison of state-of-the-art neural network\nverification tools, encourage the standardization of tool interfaces, and bring\ntogether the neural network verification community. To this end, standardized\nformats for networks (ONNX) and specification (VNN-LIB) were defined, tools\nwere evaluated on equal-cost hardware (using an automatic evaluation pipeline\nbased on AWS instances), and tool parameters were chosen by the participants\nbefore the final test sets were made public. In the 2024 iteration, 8 teams\nparticipated on a diverse set of 12 regular and 8 extended benchmarks. This\nreport summarizes the rules, benchmarks, participating tools, results, and\nlessons learned from this iteration of this competition.",
      "tldr_zh": "本报告总结了第五届国际神经网络验证比赛(VNN-COMP 2024)，作为AI Verification Symposium的一部分，与CAV会议联合举办，旨在促进神经网络验证工具的公平比较、标准化接口（如ONNX for networks和VNN-LIB for specifications），并汇聚社区。比赛采用平等硬件（基于AWS实例的自动评估管道），由参与者预先选择工具参数，并在12个常规基准和8个扩展基准上进行评估。共有8个团队参与，结果突显了工具性能差异，并提供了标准化和社区合作的宝贵经验教训。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Report on the results of VNN-COMP 2024. arXiv admin note: substantial\n  text overlap with arXiv:2312.16760, arXiv:2212.10376",
      "pdf_url": "http://arxiv.org/pdf/2412.19985v1",
      "published_date": "2024-12-28 03:07:00 UTC",
      "updated_date": "2024-12-28 03:07:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:29:36.188974"
    },
    {
      "arxiv_id": "2501.01973v3",
      "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
      "title_zh": "INFELM：大型文本到图像模型的深入公平性评估",
      "authors": [
        "Di Jin",
        "Xing Liu",
        "Yu Liu",
        "Jia Qing Yap",
        "Andrea Wong",
        "Adriana Crespo",
        "Qi Lin",
        "Zhiyuan Yin",
        "Qiang Yan",
        "Ryan Ye"
      ],
      "abstract": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles.",
      "tldr_zh": "这篇论文提出 INFELM 框架，用于深入评估大型文本到图像模型的公平性，旨在解决现有评估方法在内容对齐和社会偏见敏感领域存在的局限性，如依赖像素检测的准确性不足。关键贡献包括：一个先进的 skintone classifier，通过整合面部拓扑和精炼皮肤像素表示，提高分类精度至少16.04%；bias-sensitive content alignment measurement 用于评估社会影响；以及 generalizable representation bias evaluation，适用于多样人口群体。实验在六个社会偏见敏感领域进行大规模分析，发现现有模型普遍不符合公平标准，且 representation bias 比 alignment errors 更突出，从而为开发符合伦理的多模态 AI 系统提供稳健基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "Di Jin and Xing Liu contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2501.01973v3",
      "published_date": "2024-12-28 02:28:19 UTC",
      "updated_date": "2025-01-09 07:26:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:29:49.438909"
    },
    {
      "arxiv_id": "2501.00053v1",
      "title": "Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoge Zhang",
        "Tao Wang",
        "Chao Yan",
        "Fedaa Najdawi",
        "Kai Zhou",
        "Yuan Ma",
        "Yiu-ming Cheung",
        "Bradley A. Malin"
      ],
      "abstract": "Ensuring trustworthiness is fundamental to the development of artificial\nintelligence (AI) that is considered societally responsible, particularly in\ncancer diagnostics, where a misdiagnosis can have dire consequences. Current\ndigital pathology AI models lack systematic solutions to address\ntrustworthiness concerns arising from model limitations and data discrepancies\nbetween model deployment and development environments. To address this issue,\nwe developed TRUECAM, a framework designed to ensure both data and model\ntrustworthiness in non-small cell lung cancer subtyping with whole-slide\nimages. TRUECAM integrates 1) a spectral-normalized neural Gaussian process for\nidentifying out-of-scope inputs and 2) an ambiguity-guided elimination of tiles\nto filter out highly ambiguous regions, addressing data trustworthiness, as\nwell as 3) conformal prediction to ensure controlled error rates. We\nsystematically evaluated the framework across multiple large-scale cancer\ndatasets, leveraging both task-specific and foundation models, illustrate that\nan AI model wrapped with TRUECAM significantly outperforms models that lack\nsuch guidance, in terms of classification accuracy, robustness,\ninterpretability, and data efficiency, while also achieving improvements in\nfairness. These findings highlight TRUECAM as a versatile wrapper framework for\ndigital pathology AI models with diverse architectural designs, promoting their\nresponsible and effective applications in real-world settings.",
      "tldr_zh": "该论文提出TRUECAM框架，以提升非小细胞肺癌诊断中AI的信任性，针对模型限制和数据不一致问题。框架整合了spectral-normalized neural Gaussian process来识别超出范围输入、ambiguity-guided elimination of tiles过滤模糊区域，以及conformal prediction控制错误率，确保数据和模型的可靠性。在多个大型癌症数据集上评估表明，TRUECAM包装的AI模型在分类准确性、鲁棒性、可解释性、数据效率和公平性方面均显著优于基线模型。该框架作为通用包装工具，促进数字病理学AI的负责任应用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00053v1",
      "published_date": "2024-12-28 02:22:47 UTC",
      "updated_date": "2024-12-28 02:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:30:00.717099"
    },
    {
      "arxiv_id": "2412.19976v1",
      "title": "Will you donate money to a chatbot? The effect of chatbot anthropomorphic features and persuasion strategies on willingness to donate",
      "title_zh": "你会向聊天机器人捐款吗？ 聊天机器人拟人化特征和",
      "authors": [
        "Ekaterina Novozhilova",
        "Jiacheng Huang",
        "Le He",
        "Ziling Li",
        "James Cummings"
      ],
      "abstract": "This work investigates the causal mechanism behind the effect of chatbot\npersonification and persuasion strategies on users' perceptions and donation\nlikelihood. In a 2 (personified vs. non-personified chatbot) x 2 (emotional vs.\nlogical persuasion strategy) between-subjects experiment (N=76), participants\nengaged with a chatbot that represented a non-profit charitable organization.\nThe results suggest that interaction with a personified chatbot evokes\nperceived anthropomorphism; however, it does not elicit greater willingness to\ndonate. In fact, we found that commonly used anthropomorphic features, like\nname and narrative, led to negative attitudes toward an AI agent in the\ndonation context. Our results showcase a preference for non-personified\nchatbots paired with logical persuasion appeal, emphasizing the significance of\nconsistency in chatbot interaction, mirroring human-human engagement. We\ndiscuss the importance of moving from exploring the common scenario of a\nchatbot with machine identity vs. a chatbot with human identity in light of the\nrecent regulations of AI systems.",
      "tldr_zh": "本研究通过一个2（人格化 vs. 非人格化聊天机器人）x 2（情感 vs. 逻辑说服策略）的实验（N=76），探讨了聊天机器人的 anthropomorphic 特征和说服策略对用户感知及捐赠意愿的影响。结果显示，人格化聊天机器人虽能提升感知人类化，但并未增加捐赠意愿，反而导致负面态度，用户更倾向于非人格化聊天机器人结合逻辑说服策略，以确保互动的一致性。该研究强调了在AI法规背景下，重新审视聊天机器人身份设计的重要性，以提升用户信任和实际效果。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "13 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.19976v1",
      "published_date": "2024-12-28 02:17:46 UTC",
      "updated_date": "2024-12-28 02:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:30:12.300129"
    },
    {
      "arxiv_id": "2412.19967v2",
      "title": "MobileNetV2: A lightweight classification model for home-based sleep apnea screening",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Pan",
        "Yanxuan Yu",
        "Jilun Ye",
        "Xu Zhang"
      ],
      "abstract": "This study proposes a novel lightweight neural network model leveraging\nfeatures extracted from electrocardiogram (ECG) and respiratory signals for\nearly OSA screening. ECG signals are used to generate feature spectrograms to\npredict sleep stages, while respiratory signals are employed to detect\nsleep-related breathing abnormalities. By integrating these predictions, the\nmethod calculates the apnea-hypopnea index (AHI) with enhanced accuracy,\nfacilitating precise OSA diagnosis.\n  The method was validated on three publicly available sleep apnea databases:\nthe Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographic\ndatabase. Results showed an overall OSA detection accuracy of 0.978,\nhighlighting the model's robustness. Respiratory event classification achieved\nan accuracy of 0.969 and an area under the receiver operating characteristic\ncurve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the\nROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906\nand specificity for REM and Wake states at 0.956 and 0.937, respectively.\n  This study underscores the potential of integrating lightweight neural\nnetworks with multi-signal analysis for accurate, portable, and cost-effective\nOSA screening, paving the way for broader adoption in home-based and wearable\nhealth monitoring systems.",
      "tldr_zh": "本研究提出了一种基于 MobileNetV2 的轻量级神经网络模型，用于家庭-based 阻塞性睡眠呼吸暂停 (OSA) 筛查，通过整合 ECG 信号生成的特征谱图（用于预测睡眠阶段）和呼吸信号（用于检测呼吸异常），从而精确计算 AHI（呼吸暂停低通气指数）。该方法在 Apnea-ECG、UCDDB 和 MIT-BIH Polysomnographic 数据库上验证，整体 OSA 检测准确率达 0.978，呼吸事件分类准确率 0.969 且 ROC-AUC 为 0.98，睡眠阶段分类的 ROC-AUC 超过 0.85。研究强调，这种多信号分析方法为便携式、成本有效的家庭和可穿戴健康监测系统提供了可靠的新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.19967v2",
      "published_date": "2024-12-28 01:37:25 UTC",
      "updated_date": "2025-01-03 13:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:30:25.784893"
    },
    {
      "arxiv_id": "2412.19966v1",
      "title": "Bridging Context Gaps: Enhancing Comprehension in Long-Form Social Conversations Through Contextualized Excerpts",
      "title_zh": "桥接上下文差距：通过情境化摘录增强长形式社交对话中的理解",
      "authors": [
        "Shrestha Mohanty",
        "Sarah Xuan",
        "Jacob Jobraeel",
        "Anurag Kumar",
        "Deb Roy",
        "Jad Kabbara"
      ],
      "abstract": "We focus on enhancing comprehension in small-group recorded conversations,\nwhich serve as a medium to bring people together and provide a space for\nsharing personal stories and experiences on crucial social matters. One way to\nparse and convey information from these conversations is by sharing highlighted\nexcerpts in subsequent conversations. This can help promote a collective\nunderstanding of relevant issues, by highlighting perspectives and experiences\nto other groups of people who might otherwise be unfamiliar with and thus\nunable to relate to these experiences. The primary challenge that arises then\nis that excerpts taken from one conversation and shared in another setting\nmight be missing crucial context or key elements that were previously\nintroduced in the original conversation. This problem is exacerbated when\nconversations become lengthier and richer in themes and shared experiences. To\naddress this, we explore how Large Language Models (LLMs) can enrich these\nexcerpts by providing socially relevant context. We present approaches for\neffective contextualization to improve comprehension, readability, and empathy.\nWe show significant improvements in understanding, as assessed through\nsubjective and objective evaluations. While LLMs can offer valuable context,\nthey struggle with capturing key social aspects. We release the Human-annotated\nSalient Excerpts (HSE) dataset to support future work. Additionally, we show\nhow context-enriched excerpts can provide more focused and comprehensive\nconversation summaries.",
      "tldr_zh": "本论文探讨了在长篇社交对话中，通过上下文化片段来弥合理解差距的问题，旨在提升小团体对话的集体理解力和共情。研究利用 Large Language Models (LLMs) 来为对话片段添加社会相关上下文，改善其可读性和整体解读效果。实验结果显示，这种方法显著提升了理解水平，经主观和客观评估证实；尽管 LLMs 在捕捉关键社会方面存在局限，该研究还发布了 Human-annotated Salient Excerpts (HSE) 数据集，并展示了上下文丰富的片段如何生成更聚焦的对话总结。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.19966v1",
      "published_date": "2024-12-28 01:29:53 UTC",
      "updated_date": "2024-12-28 01:29:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:30:36.453094"
    },
    {
      "arxiv_id": "2412.19964v1",
      "title": "DepthMamba with Adaptive Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Zelin Meng",
        "Zhichen Wang"
      ],
      "abstract": "Multi-view depth estimation has achieved impressive performance over various\nbenchmarks. However, almost all current multi-view systems rely on given ideal\ncamera poses, which are unavailable in many real-world scenarios, such as\nautonomous driving. In this work, we propose a new robustness benchmark to\nevaluate the depth estimation system under various noisy pose settings.\nSurprisingly, we find current multi-view depth estimation methods or\nsingle-view and multi-view fusion methods will fail when given noisy pose\nsettings. To tackle this challenge, we propose a two-branch network\narchitecture which fuses the depth estimation results of single-view and\nmulti-view branch. In specific, we introduced mamba to serve as feature\nextraction backbone and propose an attention-based fusion methods which\nadaptively select the most robust estimation results between the two branches.\nThus, the proposed method can perform well on some challenging scenes including\ndynamic objects, texture-less regions, etc. Ablation studies prove the\neffectiveness of the backbone and fusion method, while evaluation experiments\non challenging benchmarks (KITTI and DDAD) show that the proposed method\nachieves a competitive performance compared to the state-of-the-art methods.",
      "tldr_zh": "该研究针对多视图深度估计依赖理想相机位姿的问题，提出一个新的鲁棒性基准，用于评估噪声位姿场景下的性能，发现现有方法在这些条件下容易失败。论文设计了一种两分支网络架构，包括单视图和多视图分支，使用Mamba作为特征提取主干，并引入基于注意力的自适应融合方法，以动态选择最可靠的深度估计结果。该方法在处理动态物体和无纹理区域等挑战场景时表现出色，消融实验验证了主干和融合策略的有效性，并在KITTI和DDAD基准上实现了与最先进方法相当的竞争性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.19964v1",
      "published_date": "2024-12-28 01:17:47 UTC",
      "updated_date": "2024-12-28 01:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:30:48.660787"
    },
    {
      "arxiv_id": "2501.00051v1",
      "title": "DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework",
      "title_zh": "DDD-GenDT：动态数据驱动生成式数字孪生框架",
      "authors": [
        "Yu-Zheng Lin",
        "Qinxuan Shi",
        "Zhanglong Yang",
        "Banafsheh Saber Latibari",
        "Sicong Shao",
        "Soheil Salehi",
        "Pratik Satam"
      ],
      "abstract": "Digital twin (DT) technology has emerged as a transformative approach to\nsimulate, predict, and optimize the behavior of physical systems, with\napplications that span manufacturing, healthcare, climate science, and more.\nHowever, the development of DT models often faces challenges such as high data\nrequirements, integration complexity, and limited adaptability to dynamic\nchanges in physical systems. This paper presents a new method inspired by\ndynamic data-driven applications systems (DDDAS), called the dynamic\ndata-driven generative of digital twins framework (DDD-GenDT), which combines\nthe physical system with LLM, allowing LLM to act as DT to interact with the\nphysical system operating status and generate the corresponding physical\nbehaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining\nprocess, and we use the spindle current measurement data in the NASA milling\nwear data set as an example to enable LLMs to forecast the physical behavior\nfrom historical data and interact with current observations. Experimental\nresults show that in the zero-shot prediction setting, the LLM-based DT can\nadapt to the change in the system, and the average RMSE of the GPT-4 prediction\nis 0.479A, which is 4.79% of the maximum spindle motor current measurement of\n10A, with little training data and instructions required. Furthermore, we\nanalyze the performance of DDD-GenDT in this specific application and their\npotential to construct digital twins. We also discuss the limitations and\nchallenges that may arise in practical implementations.",
      "tldr_zh": "这篇论文提出了DDD-GenDT框架，一种动态数据驱动的生成式数字孪生方法，旨在解决传统DT模型在高数据需求、集成复杂性和适应动态变化方面的挑战。该框架将物理系统与LLM结合，让LLM充当DT进行实时互动和行为生成，并在CNC加工过程中使用NASA铣削数据进行验证。实验结果显示，在零样本预测设置下，GPT-4的平均RMSE仅为0.479A（占最大电流的4.79%），证明了其高效适应性和低数据需求。最后，论文分析了该框架的潜在应用，同时讨论了实际实施中的限制和挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00051v1",
      "published_date": "2024-12-28 01:13:30 UTC",
      "updated_date": "2024-12-28 01:13:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:31:01.185986"
    },
    {
      "arxiv_id": "2503.04728v1",
      "title": "Leveraging Large Language Models For Optimized Item Categorization using UNSPSC Taxonomy",
      "title_zh": "翻译失败",
      "authors": [
        "Anmolika Singh",
        "Yuhang Diao"
      ],
      "abstract": "Effective item categorization is vital for businesses, enabling the\ntransformation of unstructured datasets into organized categories that\nstreamline inventory management. Despite its importance, item categorization\nremains highly subjective and lacks a uniform standard across industries and\nbusinesses. The United Nations Standard Products and Services Code (UNSPSC)\nprovides a standardized system for cataloguing inventory, yet employing UNSPSC\ncategorizations often demands significant manual effort. This paper\ninvestigates the deployment of Large Language Models (LLMs) to automate the\nclassification of inventory data into UNSPSC codes based on Item Descriptions.\nWe evaluate the accuracy and efficiency of LLMs in categorizing diverse\ndatasets, exploring their language processing capabilities and their potential\nas a tool for standardizing inventory classification. Our findings reveal that\nLLMs can substantially diminish the manual labor involved in item\ncategorization while maintaining high accuracy, offering a scalable solution\nfor businesses striving to enhance their inventory management practices.",
      "tldr_zh": "本研究探讨了利用大型语言模型（Large Language Models, LLMs）来优化物品分类问题，采用联合国标准产品和服务代码（UNSPSC Taxonomy）作为标准化框架，以自动化处理基于物品描述的库存数据分类。研究评估了LLMs在处理多样数据集时的准确性和效率，发现这些模型能显著减少手动劳动，同时保持高准确率。最终结果表明，LLMs为企业提供了一个可扩展的解决方案，提升了库存管理的标准化和整体效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 Pages, International Conference on NLP, AI, Computer Science &\n  Engineering (NLAICSE 2024), December 2024, ISBN : 978-1-923107-45-8",
      "pdf_url": "http://arxiv.org/pdf/2503.04728v1",
      "published_date": "2024-12-28 00:12:13 UTC",
      "updated_date": "2024-12-28 00:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T18:31:12.865025"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 61,
  "processed_papers_count": 61,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T18:31:29.227487"
}
{
  "date": "2024-12-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-12 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦 AI 模型优化、视频生成与理解、强化学习应用以及医疗 AI 等领域，强调高效计算、多模态融合和实际场景部署；重点包括 Microsoft 团队的 Phi-4 模型报告，以及涉及知名学者如 Philip S. Yu 的 SEPC 方法，这些文章展示了 LLM 在复杂任务中的潜力，并推动了 AI 在医疗和决策领域的创新。\n\n### 重点论文讨论\n我们先聚焦于重要、话题性强的论文，如 AI 模型改进、视频处理和强化学习领域，然后简要提及其他相关或次要内容。\n\n**1. AI 模型与 LLM 优化（令人印象深刻，Microsoft 团队参与）**  \n- **Phi-4 Technical Report（Phi-4 技术报告）**  \n  Microsoft 团队提出一个 14 亿参数的语言模型 Phi-4，通过高质量数据和合成数据训练，显著提升了 STEM 任务的性能，展示了在数据质量驱动下 LLM 的高效改进和泛化能力。  \n- **SEPC: Structural Entropy Guided Probabilistic Coding（SEPC: 结构熵引导的概率编码）**  \n  作者包括 Philip S. Yu，该方法通过结构熵正则化优化概率嵌入，提升了自然语言理解任务的泛化性和鲁棒性，在多个基准上超越了现有模型。  \n- **Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?（对比蒸馏是否足够学习全面的 3D 表示？）**  \n  该论文探索了对比蒸馏在 3D 表示学习中的局限性，并提出 CMCR 框架，通过模态共享和特定特征融合提升了多模态任务的性能。\n\n**2. 视频生成与理解（高话题度，涉及高效算法）**  \n- **Doe-1: Closed-Loop Autonomous Driving with Large World Model（Doe-1: 使用大型世界模型的闭环自动驾驶）**  \n  提出 Doe-1 框架，将多模态标记用于端到端自动驾驶，实现统一感知、预测和规划，在 nuScenes 数据集上表现出色。  \n- **VCA: Video Curious Agent for Long Video Understanding（VCA: 用于长视频理解的视频好奇代理）**  \n  引入基于 VLM 的好奇驱动代理，通过树搜索和内在奖励探索关键帧，提升了长视频理解的效率和准确性。  \n- **Olympus: A Universal Task Router for Computer Vision Tasks（Olympus: 计算机视觉任务的通用任务路由器）**  \n  该方法将 MLLM 转化为通用框架，支持 20 多种视觉任务，通过指令路由实现高效的多任务处理。\n\n**3. 强化学习与决策（创新性强，解决实际问题）**  \n- **Goal-Conditioned Supervised Learning for Multi-Objective Recommendation（基于目标的有监督学习的多目标推荐）**  \n  提出 MOGCSL 框架，将多目标任务转化为条件学习，提高了推荐系统的性能和鲁棒性，尤其在序列数据上。  \n- **AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection（AFFAKT: 用于视频欺骗检测的情感面部知识转移的分层最优传输方法）**  \n  通过最优传输和知识转移，提升了视频欺骗检测的准确性，显著降低了错误率。  \n- **Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning（Forest-of-Thought: 扩展测试时计算以增强 LLM 推理）**  \n  引入多树结构和自修正策略，提高了 LLM 在复杂推理任务中的性能和效率。\n\n**4. 医疗 AI 应用（实际影响大）**  \n- **Labits: Layered Bidirectional Time Surfaces Representation for Event Camera-based Continuous Dense Trajectory Estimation（Labits: 用于事件相机连续密集轨迹估计的分层双向时间表面表示）**  \n  提出新表示方法，提升了事件相机在轨迹估计中的精度，减少了信息损失。  \n- **Radiology Report Generation via Multi-objective Preference Optimization（通过多目标偏好优化的放射学报告生成）**  \n  使用多目标强化学习生成放射报告，实现了个性化偏好优化，提高了报告的临床准确性。  \n- **CareBot: A Pioneering Full-Process Open-Source Medical Language Model（CareBot: 一个开创性的全流程开源医疗语言模型）**  \n  开发了 CareBot 框架，通过连续预训练和 RLHF，提升了医疗咨询的准确性和泛化能力。\n\n其他论文涉及领域如图神经网络、时间序列预测和量子计算等，但许多为理论导向或次要贡献，我们快速掠过：例如，\"Memory Layers at Scale\"（内存层扩展）改进了语言模型的计算效率；\"Detecting Cognitive Impairment\"（检测认知障碍）使用机器学习分析多模态数据；\"Neural networks consisting of DNA\"（由 DNA 组成的神经网络）探讨了生物材料在 AI 中的潜力。这些论文虽有创新，但影响力较小，仅在特定子领域有应用价值。\n\n总之，今天的 arXiv 更新突显了 AI 在高效计算和实际应用中的进展，值得关注模型优化和多模态融合的趋势。更多细节可查阅特定论文！",
  "papers": [
    {
      "arxiv_id": "2412.09764v2",
      "title": "Memory Layers at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent-Pierre Berges",
        "Barlas Oğuz",
        "Daniel Haziza",
        "Wen-tau Yih",
        "Luke Zettlemoyer",
        "Gargi Ghosh"
      ],
      "abstract": "Memory layers use a trainable key-value lookup mechanism to add extra\nparameters to a model without increasing FLOPs. Conceptually, sparsely\nactivated memory layers complement compute-heavy dense feed-forward layers,\nproviding dedicated capacity to store and retrieve information cheaply. This\nwork takes memory layers beyond proof-of-concept, proving their utility at\ncontemporary scale. On downstream tasks, language models augmented with our\nimproved memory layer outperform dense models with more than twice the\ncomputation budget, as well as mixture-of-expert models when matched for both\ncompute and parameters. We find gains are especially pronounced for factual\ntasks. We provide a fully parallelizable memory layer implementation,\ndemonstrating scaling laws with up to 128B memory parameters, pretrained to 1\ntrillion tokens, comparing to base models with up to 8B parameters.",
      "tldr_zh": "该研究扩展了 Memory Layers 的应用，该机制通过可训练的 key-value 查找添加额外参数，而不增加 FLOPs，从而为模型提供廉价的存储和检索信息能力。实验显示，增强了 Memory Layers 的语言模型在下游任务中超越了计算预算超过两倍的密集模型，以及在计算和参数匹配时的 Mixture-of-Experts 模型，尤其在事实任务上表现突出。研究还提供了一个完全可并行化的实现，支持高达 128B 的内存参数，并在 1 万亿 tokens 的预训练中证明了其扩展性，与 up to 8B 参数的基线模型相比显示出显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09764v2",
      "published_date": "2024-12-12 23:56:57 UTC",
      "updated_date": "2024-12-20 17:36:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:30:25.117573"
    },
    {
      "arxiv_id": "2412.14194v3",
      "title": "Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaofan Mu",
        "Salman Seyedi",
        "Iris Zheng",
        "Zifan Jiang",
        "Liu Chen",
        "Bolaji Omofojoye",
        "Rachel Hershenberg",
        "Allan I. Levey",
        "Gari D. Clifford",
        "Hiroko H. Dodge",
        "Hyeokhyen Kwon"
      ],
      "abstract": "The aging society urgently requires scalable methods to monitor cognitive\ndecline and identify social and psychological factors indicative of dementia\nrisk in older adults. Our machine learning (ML) models captured facial,\nacoustic, linguistic, and cardiovascular features from 39 individuals with\nnormal cognition or Mild Cognitive Impairment derived from remote video\nconversations and classified cognitive status, social isolation, neuroticism,\nand psychological well-being. Our model could distinguish Clinical Dementia\nRating Scale (CDR) of 0.5 (vs. 0) with 0.78 area under the receiver operating\ncharacteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with\n0.71 AUC, and negative affect scales with 0.79 AUC. Recent advances in machine\nlearning offer new opportunities to remotely detect cognitive impairment and\nassess associated factors, such as neuroticism and psychological well-being.\nOur experiment showed that speech and language patterns were more useful for\nquantifying cognitive impairment, whereas facial expression and cardiovascular\npatterns using photoplethysmography (PPG) were more useful for quantifying\npersonality and psychological well-being.",
      "tldr_zh": "本研究利用机器学习（ML）模型，从39位正常认知或轻度认知障碍个体的远程视频对话中提取面部、声学、语言和心血管特征，以检测认知状态、社会隔离、神经质和心理福祉。模型在区分Clinical Dementia Rating Scale (CDR) 0.5与0时达到0.78的AUC，在社会隔离、神经质和负面影响方面分别达到0.75、0.71和0.79的AUC。结果表明，语音和语言模式更适合量化认知障碍，而面部表情和Photoplethysmography (PPG)心血管模式更适用于评估个性和心理福祉。该方法为远程监测老年认知衰退和相关心理因素提供了可扩展的解决方案。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.14194v3",
      "published_date": "2024-12-12 23:42:46 UTC",
      "updated_date": "2025-01-09 20:16:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:30:36.750875"
    },
    {
      "arxiv_id": "2412.10471v2",
      "title": "VCA: Video Curious Agent for Long Video Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyuan Yang",
        "Delin Chen",
        "Xueyang Yu",
        "Maohao Shen",
        "Chuang Gan"
      ],
      "abstract": "Long video understanding poses unique challenges due to their temporal\ncomplexity and low information density. Recent works address this task by\nsampling numerous frames or incorporating auxiliary tools using LLMs, both of\nwhich result in high computational costs. In this work, we introduce a\ncuriosity-driven video agent with self-exploration capability, dubbed as VCA.\nBuilt upon VLMs, VCA autonomously navigates video segments and efficiently\nbuilds a comprehensive understanding of complex video sequences. Instead of\ndirectly sampling frames, VCA employs a tree-search structure to explore video\nsegments and collect frames. Rather than relying on external feedback or\nreward, VCA leverages VLM's self-generated intrinsic reward to guide its\nexploration, enabling it to capture the most crucial information for reasoning.\nExperimental results on multiple long video benchmarks demonstrate our\napproach's superior effectiveness and efficiency.",
      "tldr_zh": "这篇论文提出了VCA（Video Curious Agent），一种基于好奇心驱动的自探索代理，用于解决长视频理解中的时间复杂性和低信息密度挑战。VCA建立在VLMs（视觉语言模型）之上，通过树搜索结构自主导航视频段并收集关键帧，同时利用VLM的自生成内在奖励指导探索，而非依赖外部反馈或大量采样。实验结果显示，该方法在多个长视频基准上表现出色，在有效性和效率方面均优于现有基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10471v2",
      "published_date": "2024-12-12 23:39:54 UTC",
      "updated_date": "2025-03-10 03:35:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:30:47.574338"
    },
    {
      "arxiv_id": "2412.09760v1",
      "title": "Congruence-based Learning of Probabilistic Deterministic Finite Automata",
      "title_zh": "翻译失败",
      "authors": [
        "Matías Carrasco",
        "Franz Mayr",
        "Sergio Yovine"
      ],
      "abstract": "This work studies the question of learning probabilistic deterministic\nautomata from language models. For this purpose, it focuses on analyzing the\nrelations defined on algebraic structures over strings by equivalences and\nsimilarities on probability distributions. We introduce a congruence that\nextends the classical Myhill-Nerode congruence for formal languages. This new\ncongruence is the basis for defining regularity over language models. We\npresent an active learning algorithm that computes the quotient with respect to\nthis congruence whenever the language model is regular. The paper also defines\nthe notion of recognizability for language models and shows that it coincides\nwith regularity for congruences. For relations which are not congruences, it\nshows that this is not the case. Finally, it discusses the impact of this\nresult on learning in the context of language models.",
      "tldr_zh": "这篇论文探讨了从语言模型中学习 Probabilistic Deterministic Finite Automata 的方法，通过分析字符串代数结构上概率分布的等价性和相似性。作者引入了一个扩展 Myhill-Nerode Congruence 的新一致性，作为定义语言模型 Regularity 的基础，并提出一个 Active Learning Algorithm 来计算其商集，前提是模型为正则的。主要贡献包括证明语言模型的 Recognizability 与 Regularity 一致，并讨论了这一结果对非一致性关系和语言模型学习的影响。",
      "categories": [
        "cs.FL",
        "cs.AI"
      ],
      "primary_category": "cs.FL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09760v1",
      "published_date": "2024-12-12 23:38:58 UTC",
      "updated_date": "2024-12-12 23:38:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:31:00.634756"
    },
    {
      "arxiv_id": "2412.09751v2",
      "title": "AI red-teaming is a sociotechnical challenge: on values, labor, and harms",
      "title_zh": "翻译失败",
      "authors": [
        "Tarleton Gillespie",
        "Ryland Shaw",
        "Mary L. Gray",
        "Jina Suh"
      ],
      "abstract": "As generative AI technologies find more and more real-world applications, the\nimportance of testing their performance and safety seems paramount.\n\"Red-teaming\" has quickly become the primary approach to test AI\nmodels--prioritized by AI companies, and enshrined in AI policy and regulation.\nMembers of red teams act as adversaries, probing AI systems to test their\nsafety mechanisms and uncover vulnerabilities. Yet we know far too little about\nthis work or its implications. This essay calls for collaboration between\ncomputer scientists and social scientists to study the sociotechnical systems\nsurrounding AI technologies, including the work of red-teaming, to avoid\nrepeating the mistakes of the recent past. We highlight the importance of\nunderstanding the values and assumptions behind red-teaming, the labor\narrangements involved, and the psychological impacts on red-teamers, drawing\ninsights from the lessons learned around the work of content moderation.",
      "tldr_zh": "这篇论文强调AI red-teaming作为测试AI模型性能和安全性的主要方法，同时指出它是一个社会技术(sociotechnical)挑战，需要审视其背后的价值观、劳动安排和潜在危害。作者呼吁计算机科学家和社会科学家合作，研究red-teaming的系统性影响，包括对red-teamers的心理负担，并从内容审核(Content moderation)的经验中吸取教训，以避免过去错误。最终，该研究为更负责任的AI开发提供框架，确保red-teaming工作更具可持续性和伦理性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09751v2",
      "published_date": "2024-12-12 22:48:19 UTC",
      "updated_date": "2025-04-03 20:24:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:31:11.769822"
    },
    {
      "arxiv_id": "2412.09741v1",
      "title": "On Round-Off Errors and Gaussian Blur in Superresolution and in Image Registration",
      "title_zh": "翻译失败",
      "authors": [
        "Serap A. Savari"
      ],
      "abstract": "Superresolution theory and techniques seek to recover signals from samples in\nthe presence of blur and noise. Discrete image registration can be an approach\nto fuse information from different sets of samples of the same signal.\nQuantization errors in the spatial domain are inherent to digital images. We\nconsider superresolution and discrete image registration for one-dimensional\nspatially-limited piecewise constant functions which are subject to blur which\nis Gaussian or a mixture of Gaussians as well as to round-off errors. We\ndescribe a signal-dependent measurement matrix which captures both types of\neffects. For this setting we show that the difficulties in determining the\ndiscontinuity points from two sets of samples even in the absence of other\ntypes of noise. If the samples are also subject to statistical noise, then it\nis necessary to align and segment the data sequences to make the most effective\ninferences about the amplitudes and discontinuity points. Under some conditions\non the blur, the noise, and the distance between discontinuity points, we prove\nthat we can correctly align and determine the first samples following each\ndiscontinuity point in two data sequences with an approach based on dynamic\nprogramming.",
      "tldr_zh": "本论文探讨了超分辨率（superresolution）和图像配准（image registration）中舍入误差（round-off errors）和高斯模糊（Gaussian blur）的挑战，针对一维空间有限的阶跃函数（piecewise constant functions）。作者引入了信号相关的测量矩阵（signal-dependent measurement matrix）来同时捕捉模糊和高斯混合模糊效应，并证明了即使没有其他噪声，确定不连续点（discontinuity points）也存在困难。论文进一步显示，在模糊、噪声和不连续点间距的特定条件下，通过基于动态规划（dynamic programming）的approach，可以正确对齐和分割数据序列，从而准确识别每个不连续点后的第一个样本。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09741v1",
      "published_date": "2024-12-12 22:08:53 UTC",
      "updated_date": "2024-12-12 22:08:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:31:25.370341"
    },
    {
      "arxiv_id": "2412.09727v2",
      "title": "Let Curves Speak: A Continuous Glucose Monitor based Large Sensor Foundation Model for Diabetes Management",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Luo",
        "Abhimanyu Kumbara",
        "Mansur Shomali",
        "Rui Han",
        "Anand Iyer",
        "Ritu Agarwal",
        "Gordon Gao"
      ],
      "abstract": "While previous studies of AI in diabetes management focus on long-term risk,\nresearch on near-future glucose prediction remains limited but important as it\nenables timely diabetes self-management. Integrating AI with continuous glucose\nmonitoring (CGM) holds promise for near-future glucose prediction. However,\nexisting models have limitations in capturing patterns of blood glucose\nfluctuations and demonstrate poor generalizability. A robust approach is needed\nto leverage massive CGM data for near-future glucose prediction. We propose\nlarge sensor models (LSMs) to capture knowledge in CGM data by modeling\npatients as sequences of glucose. CGM-LSM is pretrained on 15.96 million\nglucose records from 592 diabetes patients for near-future glucose prediction.\nWe evaluated CGM-LSM against state-of-the-art methods using the OhioT1DM\ndataset across various metrics, prediction horizons, and unseen patients.\nAdditionally, we assessed its generalizability across factors like diabetes\ntype, age, gender, and hour of day. CGM-LSM achieved exceptional performance,\nwith an rMSE of 29.81 mg/dL for type 1 diabetes patients and 23.49 mg/dL for\ntype 2 diabetes patients in a two-hour prediction horizon. For the OhioT1DM\ndataset, CGM-LSM achieved a one-hour rMSE of 15.64 mg/dL, halving the previous\nbest of 31.97 mg/dL. Robustness analyses revealed consistent performance not\nonly for unseen patients and future periods, but also across diabetes type,\nage, and gender. The model demonstrated adaptability to different hours of day,\nmaintaining accuracy across periods of various activity intensity levels.\nCGM-LSM represents a transformative step in diabetes management by leveraging\npretraining to uncover latent glucose generation patterns in sensor data. Our\nfindings also underscore the broader potential of LSMs to drive innovation\nacross domains involving complex sensor data.",
      "tldr_zh": "本文提出一种基于Continuous Glucose Monitor (CGM)的Large Sensor Foundation Model (LSMs)，即CGM-LSM，用于短期血糖预测，以支持及时的糖尿病管理。该模型通过将患者视为血糖序列，并在1596万条记录上预训练，成功捕捉血糖波动模式，并在OhioT1DM数据集上表现出色，一小时rMSE达到15.64 mg/dL，比现有最佳方法降低一半。实验结果显示，CGM-LSM在不同糖尿病类型、年龄、性别和时间段上具有强泛化性和鲁棒性。该方法不仅提升了糖尿病管理的准确性，还为复杂传感器数据领域的创新提供了新途径。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09727v2",
      "published_date": "2024-12-12 21:35:13 UTC",
      "updated_date": "2024-12-18 03:07:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:31:38.888745"
    },
    {
      "arxiv_id": "2501.03235v1",
      "title": "Neural networks consisting of DNA",
      "title_zh": "由 DNA 组成的神经网络",
      "authors": [
        "Michael te Vrugt"
      ],
      "abstract": "Neural networks based on soft and biological matter constitute an interesting\npotential alternative to traditional implementations based on electric\ncircuits. DNA is a particularly promising system in this context due its\nnatural ability to store information. In recent years, researchers have started\nto construct neural networks that are based on DNA. In this chapter, I provide\na very basic introduction to the concept of DNA neural networks, aiming at an\naudience that is not familiar with biochemistry.",
      "tldr_zh": "这篇论文介绍了基于 DNA 的神经网络，作为传统电路实现的潜在替代方案，强调了软物质和生物物质在构建神经网络中的前景。DNA 凭借其天然存储信息的能力，成为一个特别有吸引力的系统。作者针对不熟悉生化学的受众，提供了一个基本概念介绍，回顾了近年来相关研究的发展。",
      "categories": [
        "physics.bio-ph",
        "cond-mat.soft",
        "cs.AI",
        "cs.NE",
        "q-bio.BM",
        "q-bio.MN"
      ],
      "primary_category": "physics.bio-ph",
      "comment": "Book chapter, to appear in: Artificial Intelligence and Intelligent\n  Matter, Springer, Cham",
      "pdf_url": "http://arxiv.org/pdf/2501.03235v1",
      "published_date": "2024-12-12 21:33:25 UTC",
      "updated_date": "2024-12-12 21:33:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:31:47.786174"
    },
    {
      "arxiv_id": "2412.09726v1",
      "title": "The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications",
      "title_zh": "高斯分数近似在扩散模型中的不可思议有效性及其应用",
      "authors": [
        "Binxu Wang",
        "John J. Vastola"
      ],
      "abstract": "By learning the gradient of smoothed data distributions, diffusion models can\niteratively generate samples from complex distributions. The learned score\nfunction enables their generalization capabilities, but how the learned score\nrelates to the score of the underlying data manifold remains largely unclear.\nHere, we aim to elucidate this relationship by comparing learned neural scores\nto the scores of two kinds of analytically tractable distributions: Gaussians\nand Gaussian mixtures. The simplicity of the Gaussian model makes it\ntheoretically attractive, and we show that it admits a closed-form solution and\npredicts many qualitative aspects of sample generation dynamics. We claim that\nthe learned neural score is dominated by its linear (Gaussian) approximation\nfor moderate to high noise scales, and supply both theoretical and empirical\narguments to support this claim. Moreover, the Gaussian approximation\nempirically works for a larger range of noise scales than naive theory suggests\nit should, and is preferentially learned early in training. At smaller noise\nscales, we observe that learned scores are better described by a coarse-grained\n(Gaussian mixture) approximation of training data than by the score of the\ntraining distribution, a finding consistent with generalization. Our findings\nenable us to precisely predict the initial phase of trained models' sampling\ntrajectories through their Gaussian approximations. We show that this allows\nthe skipping of the first 15-30% of sampling steps while maintaining high\nsample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10\nunconditional generation). This forms the foundation of a novel hybrid sampling\nmethod, termed analytical teleportation, which can seamlessly integrate with\nand accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our\nfindings suggest ways to improve the design and training of diffusion models.",
      "tldr_zh": "该研究探讨了 Gaussian score approximation 在 diffusion models 中的非凡有效性，通过比较学习到的神经分数与 Gaussian 和 Gaussian mixtures 的分数，揭示了神经分数在中等到高噪声尺度下主要由线性（Gaussian）近似主导的现象。实验结果显示，这种近似不仅在理论上成立，还在实际中适用于更广的噪声范围，并优先在训练早期被学习；在小噪声尺度下，神经分数更接近训练数据的 Gaussian mixture 近似，从而提升了模型的泛化能力。基于这些发现，论文提出了一种新型混合采样方法 analytical teleportation，能预测采样轨迹并跳过首15-30%的步骤，同时保持高样本质量（如在 CIFAR-10 上实现 FID score 1.93），并为优化 diffusion models 的设计和训练提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T07, 60G15, 60J60, 62M40, 65C30",
        "I.2.6; I.5.1; G.3"
      ],
      "primary_category": "cs.LG",
      "comment": "69 pages, 34 figures. Published in TMLR. Previous shorter versions at\n  arxiv.org/abs/2303.02490 and arxiv.org/abs/2311.10892",
      "pdf_url": "http://arxiv.org/pdf/2412.09726v1",
      "published_date": "2024-12-12 21:31:27 UTC",
      "updated_date": "2024-12-12 21:31:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:32:01.497496"
    },
    {
      "arxiv_id": "2412.12175v1",
      "title": "Explore Theory of Mind: Program-guided adversarial data generation for theory of mind reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Melanie Sclar",
        "Jane Yu",
        "Maryam Fazel-Zarandi",
        "Yulia Tsvetkov",
        "Yonatan Bisk",
        "Yejin Choi",
        "Asli Celikyilmaz"
      ],
      "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and\nbenchmarks have been introduced to evaluate if current models have been able to\ndevelop this key ability of social intelligence. However, all rely on limited\ndatasets with simple patterns that can potentially lead to problematic blind\nspots in evaluation and an overestimation of model capabilities. We introduce\nExploreToM, the first framework to allow large-scale generation of diverse and\nchallenging theory of mind data for robust training and evaluation. Our\napproach leverages an A* search over a custom domain-specific language to\nproduce complex story structures and novel, diverse, yet plausible scenarios to\nstress test the limits of LLMs. Our evaluation reveals that state-of-the-art\nLLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on\nExploreToM-generated data, highlighting the need for more robust theory of mind\nevaluation. As our generations are a conceptual superset of prior work,\nfine-tuning on our data yields a 27-point accuracy improvement on the classic\nToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying\nskills and factors missing for models to show theory of mind, such as\nunreliable state tracking or data imbalances, which may contribute to models'\npoor performance on benchmarks.",
      "tldr_zh": "本文提出 ExploreToM 框架，这是一个首创的程序引导式对抗数据生成方法，用于生成大规模、多样且具有挑战性的 Theory of Mind (ToM) 数据，以更准确评估大型语言模型 (LLMs) 的社会智能能力。该框架利用 A* 搜索和自定义领域特定语言 (DSL) 来创建复杂的故事结构和场景，从而测试模型的极限。实验结果显示，先进模型如 Llama-3.1-70B 和 GPT-4o 在这些数据上的准确率低至 0% 和 9%，突显了现有基准的局限性；在 ExploreToM 数据上微调后，模型在经典 ToMi 基准上的准确率提升了 27 分，并揭示了模型的潜在问题，如不可靠的状态跟踪和数据不平衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12175v1",
      "published_date": "2024-12-12 21:29:00 UTC",
      "updated_date": "2024-12-12 21:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:32:13.903955"
    },
    {
      "arxiv_id": "2412.13212v1",
      "title": "An introduction to reservoir computing",
      "title_zh": "翻译失败",
      "authors": [
        "Michael te Vrugt"
      ],
      "abstract": "There is a growing interest in the development of artificial neural networks\nthat are implemented in a physical system. A major challenge in this context is\nthat these networks are difficult to train since training here would require a\nchange of physical parameters rather than simply of coefficients in a computer\nprogram. For this reason, reservoir computing, where one employs\nhigh-dimensional recurrent networks and trains only the final layer, is widely\nused in this context. In this chapter, I introduce the basic concepts of\nreservoir computing. Moreover, I present some important physical\nimplementations coming from electronics, photonics, spintronics, mechanics, and\nbiology. Finally, I provide a brief discussion of quantum reservoir computing.",
      "tldr_zh": "本论文介绍了 reservoir computing 的基本概念，这是一种针对物理系统实现人工神经网络的方法，通过使用高维 recurrent networks 并仅训练最终层，解决了传统训练中需改变物理参数的难题。相比于软件中的系数调整，这种方法简化了训练过程，使其适用于各种物理实现，包括电子学、光子学、spintronics、mechanics 和 biology 领域。论文还简要讨论了 quantum reservoir computing 的潜力，为未来研究提供了基础。",
      "categories": [
        "cs.ET",
        "cond-mat.dis-nn",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.app-ph",
        "quant-ph"
      ],
      "primary_category": "cs.ET",
      "comment": "Book chapter, to appear in: Artificial Intelligence and Intelligent\n  Matter, Springer, Cham",
      "pdf_url": "http://arxiv.org/pdf/2412.13212v1",
      "published_date": "2024-12-12 21:19:52 UTC",
      "updated_date": "2024-12-12 21:19:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:32:25.517941"
    },
    {
      "arxiv_id": "2412.09719v2",
      "title": "TransferLight: Zero-Shot Traffic Signal Control on any Road-Network",
      "title_zh": "翻译失败",
      "authors": [
        "Johann Schmidt",
        "Frank Dreyer",
        "Sayed Abid Hashimi",
        "Sebastian Stober"
      ],
      "abstract": "Traffic signal control plays a crucial role in urban mobility. However,\nexisting methods often struggle to generalize beyond their training\nenvironments to unseen scenarios with varying traffic dynamics. We present\nTransferLight, a novel framework designed for robust generalization across\nroad-networks, diverse traffic conditions and intersection geometries. At its\ncore, we propose a log-distance reward function, offering spatially-aware\nsignal prioritization while remaining adaptable to varied lane configurations -\novercoming the limitations of traditional pressure-based rewards. Our\nhierarchical, heterogeneous, and directed graph neural network architecture\neffectively captures granular traffic dynamics, enabling transferability to\narbitrary intersection layouts. Using a decentralized multi-agent approach,\nglobal rewards, and novel state transition priors, we develop a single,\nweight-tied policy that scales zero-shot to any road network without\nre-training. Through domain randomization during training, we additionally\nenhance generalization capabilities. Experimental results validate\nTransferLight's superior performance in unseen scenarios, advancing practical,\ngeneralizable intelligent transportation systems to meet evolving urban traffic\ndemands.",
      "tldr_zh": "该研究提出 TransferLight 框架，用于实现零样本（zero-shot）交通信号控制，能够泛化到任何路网、不同交通条件和交叉口几何。核心创新包括 log-distance 奖励函数，提供空间感知的信号优先级并适应各种车道配置，同时采用分层、异构、定向 Graph Neural Network 架构和去中心化多代理方法，以捕捉细粒度交通动态并开发可扩展的权重共享策略。实验结果显示，TransferLight 在未见场景中表现出色，通过训练中的 domain randomization 进一步提升泛化能力，推动智能交通系统的实用发展。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI Workshop Paper (MALTA)",
      "pdf_url": "http://arxiv.org/pdf/2412.09719v2",
      "published_date": "2024-12-12 20:52:12 UTC",
      "updated_date": "2024-12-23 20:13:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:32:37.883630"
    },
    {
      "arxiv_id": "2412.09701v1",
      "title": "CUAL: Continual Uncertainty-aware Active Learner",
      "title_zh": "翻译失败",
      "authors": [
        "Amanda Rios",
        "Ibrahima Ndiour",
        "Parual Datta",
        "Jerry Sydir",
        "Omesh Tickoo",
        "Nilesh Ahuja"
      ],
      "abstract": "AI deployed in many real-world use cases should be capable of adapting to\nnovelties encountered after deployment. Here, we consider a challenging,\nunder-explored and realistic continual adaptation problem: a deployed AI agent\nis continuously provided with unlabeled data that may contain not only unseen\nsamples of known classes but also samples from novel (unknown) classes. In such\na challenging setting, it has only a tiny labeling budget to query the most\ninformative samples to help it continuously learn. We present a comprehensive\nsolution to this complex problem with our model \"CUAL\" (Continual\nUncertainty-aware Active Learner). CUAL leverages an uncertainty estimation\nalgorithm to prioritize active labeling of ambiguous (uncertain) predicted\nnovel class samples while also simultaneously pseudo-labeling the most certain\npredictions of each class. Evaluations across multiple datasets, ablations,\nsettings and backbones (e.g. ViT foundation model) demonstrate our method's\neffectiveness. We will release our code upon acceptance.",
      "tldr_zh": "本研究提出了一种名为 CUAL 的持续不确定性感知主动学习器（Continual Uncertainty-aware Active Learner），旨在帮助 AI 在部署后适应新数据，包括已知类别的未见样本和新类别的样本，同时仅使用有限的标记预算。CUAL 通过不确定性估计算法优先选择预测为新类的模糊样本进行主动标记，并同时为每个类别的确定预测添加伪标签，以实现高效的持续学习。在多个数据集、消融实验和不同骨干模型（如 ViT 基础模型）上的评估中，CUAL 展示了显著的有效性，为真实世界 AI 适应性应用提供了全面解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09701v1",
      "published_date": "2024-12-12 19:49:09 UTC",
      "updated_date": "2024-12-12 19:49:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:32:53.681954"
    },
    {
      "arxiv_id": "2501.10369v1",
      "title": "Creative Loss: Ambiguity, Uncertainty and Indeterminacy",
      "title_zh": "翻译失败",
      "authors": [
        "Tom Holberton"
      ],
      "abstract": "This article evaluates how creative uses of machine learning can address\nthree adjacent terms: ambiguity, uncertainty and indeterminacy. Through the\nprogression of these concepts it reflects on increasing ambitions for machine\nlearning as a creative partner, illustrated with research from Unit 21 at the\nBartlett School of Architecture, UCL. Through indeterminacy are potential\nfuture approaches to machine learning and design.",
      "tldr_zh": "这篇文章探讨了机器学习的创意应用如何处理三个相关概念：ambiguity（模糊性）、uncertainty（不确定性）和indeterminacy（不确定性）。作者通过这些概念的演变，反思机器学习作为创意伙伴的日益雄心，并引用了巴特利特建筑学院 UCL Unit 21 的研究作为例证。最终，该研究揭示了通过indeterminacy探索机器学习与设计潜在未来方法的可能性，为创意领域提供了新思路。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "NeurIPS 2024 Creative AI Track",
      "pdf_url": "http://arxiv.org/pdf/2501.10369v1",
      "published_date": "2024-12-12 19:22:20 UTC",
      "updated_date": "2024-12-12 19:22:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:33:00.358280"
    },
    {
      "arxiv_id": "2412.09627v1",
      "title": "Doe-1: Closed-Loop Autonomous Driving with Large World Model",
      "title_zh": "翻译失败",
      "authors": [
        "Wenzhao Zheng",
        "Zetian Xia",
        "Yuanhui Huang",
        "Sicheng Zuo",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "End-to-end autonomous driving has received increasing attention due to its\npotential to learn from large amounts of data. However, most existing methods\nare still open-loop and suffer from weak scalability, lack of high-order\ninteractions, and inefficient decision-making. In this paper, we explore a\nclosed-loop framework for autonomous driving and propose a large Driving wOrld\nmodEl (Doe-1) for unified perception, prediction, and planning. We formulate\nautonomous driving as a next-token generation problem and use multi-modal\ntokens to accomplish different tasks. Specifically, we use free-form texts\n(i.e., scene descriptions) for perception and generate future predictions\ndirectly in the RGB space with image tokens. For planning, we employ a\nposition-aware tokenizer to effectively encode action into discrete tokens. We\ntrain a multi-modal transformer to autoregressively generate perception,\nprediction, and planning tokens in an end-to-end and unified manner.\nExperiments on the widely used nuScenes dataset demonstrate the effectiveness\nof Doe-1 in various tasks including visual question-answering,\naction-conditioned video generation, and motion planning. Code:\nhttps://github.com/wzzheng/Doe.",
      "tldr_zh": "本文提出Doe-1，一种基于Large World Model的闭环自动驾驶框架，用于统一感知、预测和规划，解决现有端到端方法的扩展性差和高阶交互不足问题。Doe-1将自动驾驶表述为下一个标记生成任务，利用多模态标记（如自由形式文本、图像标记和位置感知动作标记）来实现端到端的Transformer模型训练。实验在nuScenes数据集上验证了其有效性，在视觉问答、动作条件视频生成和运动规划任务中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/wzzheng/Doe",
      "pdf_url": "http://arxiv.org/pdf/2412.09627v1",
      "published_date": "2024-12-12 18:59:59 UTC",
      "updated_date": "2024-12-12 18:59:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:33:15.305692"
    },
    {
      "arxiv_id": "2412.09612v3",
      "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanze Lin",
        "Yunsheng Li",
        "Dongdong Chen",
        "Weijian Xu",
        "Ronald Clark",
        "Philip H. S. Torr"
      ],
      "abstract": "We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/",
      "tldr_zh": "本研究引入了 Olympus，这是一个将 Multimodal Large Language Models (MLLMs) 转化为统一框架的创新方法，用于处理超过 20 种计算机视觉任务，包括图像、视频和 3D 对象。Olympus 通过一个控制器 MLLM 基于指令进行任务路由，将任务分配给专用模块，从而实现复杂的链式动作，而无需训练额外的生成模型。实验结果显示，Olympus 在 20 个任务上的平均路由准确率达到 94.75%，在链式动作场景中的精度为 91.82%，并能轻松集成现有 MLLMs，提供高效且可比的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025, Project webpage:\n  http://yuanze-lin.me/Olympus_page/",
      "pdf_url": "http://arxiv.org/pdf/2412.09612v3",
      "published_date": "2024-12-12 18:59:40 UTC",
      "updated_date": "2025-04-01 21:08:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:33:25.537082"
    },
    {
      "arxiv_id": "2412.09602v2",
      "title": "Hidden Biases of End-to-End Driving Datasets",
      "title_zh": "端到端驾驶数据集的隐藏偏差",
      "authors": [
        "Julian Zimmerlin",
        "Jens Beißwenger",
        "Bernhard Jaeger",
        "Andreas Geiger",
        "Kashyap Chitta"
      ],
      "abstract": "End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.",
      "tldr_zh": "这篇论文探讨了端到-end driving数据集中的隐藏偏差，强调训练数据集对模型性能的影响往往被忽略。作者通过系统分析CARLA Leaderboard 2.0的训练数据，得出三个关键见解：(1) Expert style显著影响下游策略性能；(2) 复杂数据集不应基于简单标准如类别频率加权帧；(3) 通过估计帧是否改变目标标签，可以减少数据集大小而不丢失重要信息。基于这些发现，他们的模型在2024 CARLA Challenge中分别在地图和传感器轨道上排名第一和第二，并在Bench2Drive测试路线中设置新状态；此外，论文还揭示了当前评估指标的设计缺陷并提出改进方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)",
      "pdf_url": "http://arxiv.org/pdf/2412.09602v2",
      "published_date": "2024-12-12 18:59:13 UTC",
      "updated_date": "2024-12-13 09:51:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:33:38.706385"
    },
    {
      "arxiv_id": "2412.09601v2",
      "title": "TimeRefine: Temporal Grounding with Time Refining Video LLM",
      "title_zh": "TimeRefine：使用时间精炼视频大语言模型的时序定位",
      "authors": [
        "Xizi Wang",
        "Feng Cheng",
        "Ziyang Wang",
        "Huiyu Wang",
        "Md Mohaiminul Islam",
        "Lorenzo Torresani",
        "Mohit Bansal",
        "Gedas Bertasius",
        "David Crandall"
      ],
      "abstract": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.",
      "tldr_zh": "该论文提出TimeRefine，一种改进Video LLMs在视频时间定位(temporal grounding)任务中的方法，旨在解决直接预测时间戳的准确性问题。TimeRefine将任务重新定义为时间精炼过程：模型先进行粗略预测，然后通过多次迭代预测偏移量来逐步提升定位精度，同时引入辅助预测头以加大预测偏差的惩罚力度，促进更精确的输出。该方法作为plug-and-play模块，可轻松整合到大多数LLM-based temporal grounding方法中。实验结果显示，在ActivityNet和Charades-STA数据集上，TimeRefine分别提高了3.6%和5.0%的mIoU性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09601v2",
      "published_date": "2024-12-12 18:59:11 UTC",
      "updated_date": "2025-03-05 07:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:33:49.473002"
    },
    {
      "arxiv_id": "2412.09600v1",
      "title": "Owl-1: Omni World Model for Consistent Long Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanhui Huang",
        "Wenzhao Zheng",
        "Yuan Gao",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Video generation models (VGMs) have received extensive attention recently and\nserve as promising candidates for general-purpose large vision models. While\nthey can only generate short videos each time, existing methods achieve long\nvideo generation by iteratively calling the VGMs, using the last-frame output\nas the condition for the next-round generation. However, the last frame only\ncontains short-term fine-grained information about the scene, resulting in\ninconsistency in the long horizon. To address this, we propose an Omni World\nmodeL (Owl-1) to produce long-term coherent and comprehensive conditions for\nconsistent long video generation. As videos are observations of the underlying\nevolving world, we propose to model the long-term developments in a latent\nspace and use VGMs to film them into videos. Specifically, we represent the\nworld with a latent state variable which can be decoded into explicit video\nobservations. These observations serve as a basis for anticipating temporal\ndynamics which in turn update the state variable. The interaction between\nevolving dynamics and persistent state enhances the diversity and consistency\nof the long videos. Extensive experiments show that Owl-1 achieves comparable\nperformance with SOTA methods on VBench-I2V and VBench-Long, validating its\nability to generate high-quality video observations. Code:\nhttps://github.com/huang-yh/Owl.",
      "tldr_zh": "本论文提出 Owl-1，一种 Omni World Model，用于解决现有视频生成模型（VGMs）在长视频生成中因仅依赖最后一帧而导致的不一致性问题。Owl-1 通过在 latent space 中建模世界状态变量，将其解码成视频观察，并利用这些观察预测时间动态以更新状态，从而增强长视频的连贯性和多样性。实验结果显示，Owl-1 在 VBench-I2V 和 VBench-Long 基准上与最先进方法相比表现出可比性能，证明了其生成高质量视频的能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/huang-yh/Owl",
      "pdf_url": "http://arxiv.org/pdf/2412.09600v1",
      "published_date": "2024-12-12 18:59:01 UTC",
      "updated_date": "2024-12-12 18:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:34:02.540725"
    },
    {
      "arxiv_id": "2412.09596v1",
      "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
      "title_zh": "InternLM-XComposer2.5-OmniLive：一个全面的多模态系统，用于长期流媒体视频和音频交互",
      "authors": [
        "Pan Zhang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Yuhang Zang",
        "Rui Qian",
        "Xilin Wei",
        "Lin Chen",
        "Yifei Li",
        "Junbo Niu",
        "Shuangrui Ding",
        "Qipeng Guo",
        "Haodong Duan",
        "Xin Chen",
        "Han Lv",
        "Zheng Nie",
        "Min Zhang",
        "Bin Wang",
        "Wenwei Zhang",
        "Xinyue Zhang",
        "Jiaye Ge",
        "Wei Li",
        "Jingwen Li",
        "Zhongying Tu",
        "Conghui He",
        "Xingcheng Zhang",
        "Kai Chen",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Creating AI systems that can interact with environments over long periods,\nsimilar to human cognition, has been a longstanding research goal. Recent\nadvancements in multimodal large language models (MLLMs) have made significant\nstrides in open-world understanding. However, the challenge of continuous and\nsimultaneous streaming perception, memory, and reasoning remains largely\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\narchitecture, which limits their ability to process inputs and generate\nresponses simultaneously, akin to being unable to think while perceiving.\nFurthermore, relying on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information becomes costly and\ninefficient. Therefore, rather than relying on a single foundation model to\nperform all functions, this project draws inspiration from the concept of the\nSpecialized Generalist AI and introduces disentangled streaming perception,\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\nProcesses multimodal information in real-time, storing key details in memory\nand triggering reasoning in response to user queries. (2) Multi-modal Long\nMemory Module: Integrates short-term and long-term memory, compressing\nshort-term memories into long-term ones for efficient retrieval and improved\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\ntasks, coordinating with the perception and memory modules. This project\nsimulates human-like cognition, enabling multimodal large language models to\nprovide continuous and adaptive service over time.",
      "tldr_zh": "该研究旨在开发AI系统以实现长期环境交互，解决现有多模态大语言模型（MLLMs）在流式感知、记忆和推理方面的局限性。论文提出InternLM-XComposer2.5-OmniLive（IXC2.5-OL）框架，该框架包括三个关键模块：Streaming Perception Module负责实时处理多模态信息并存储关键细节、Multi-modal Long Memory Module整合短期和长期记忆以提高效率，以及Reasoning Module协调响应查询和执行推理任务。通过这些分离机制，系统模仿人类认知，提供连续且适应的视频和音频交互服务。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Github Repo:\n  https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive",
      "pdf_url": "http://arxiv.org/pdf/2412.09596v1",
      "published_date": "2024-12-12 18:58:30 UTC",
      "updated_date": "2024-12-12 18:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:34:13.187923"
    },
    {
      "arxiv_id": "2412.09582v2",
      "title": "Neptune: The Long Orbit to Benchmarking Long Video Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Arsha Nagrani",
        "Mingda Zhang",
        "Ramin Mehran",
        "Rachel Hornung",
        "Nitesh Bharadwaj Gundavarapu",
        "Nilpa Jha",
        "Austin Myers",
        "Xingyi Zhou",
        "Boqing Gong",
        "Cordelia Schmid",
        "Mikhail Sirotenko",
        "Yukun Zhu",
        "Tobias Weyand"
      ],
      "abstract": "We introduce Neptune, a benchmark for long video understanding that requires\nreasoning over long time horizons and across different modalities. Many\nexisting video datasets and models are focused on short clips (10s-30s). While\nsome long video datasets do exist, they can often be solved by powerful image\nmodels applied per frame (and often to very few frames) in a video, and are\nusually manually annotated at high cost. In order to mitigate both these\nproblems, we propose a scalable dataset creation pipeline which leverages large\nmodels (VLMs and LLMs), to automatically generate dense, time-aligned video\ncaptions, as well as tough question answer decoy sets for video segments (up to\n15 minutes in length). Our dataset Neptune covers a broad range of long video\nreasoning abilities and consists of a subset that emphasizes multimodal\nreasoning. Since existing metrics for open-ended question answering are either\nrule-based or may rely on proprietary models, we provide a new open source\nmodel-based metric GEM to score open-ended responses on Neptune. Benchmark\nevaluations reveal that most current open-source long video models perform\npoorly on Neptune, particularly on questions testing temporal ordering,\ncounting and state changes. Through Neptune, we aim to spur the development of\nmore advanced models capable of understanding long videos. The dataset is\navailable at https://github.com/google-deepmind/neptune",
      "tldr_zh": "该研究引入了 Neptune 基准，用于评估长视频理解能力，强调在长时序和多模态（如视觉和语言）上的推理。研究提出一个可扩展的数据集创建管道，利用大型模型（VLMs 和 LLMs）自动生成密集时间对齐的视频字幕以及针对长达 15 分钟视频片段的难题答案备选集，以解决现有数据集的局限性。Neptune 覆盖广泛的推理能力，包括多模态子集，并提供新的开源指标 GEM 用于评估开放式问题回答。基准评估显示，当前开源长视频模型在时间顺序、计数和状态变化等任务上表现不佳，Neptune 旨在推动更先进模型的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09582v2",
      "published_date": "2024-12-12 18:54:48 UTC",
      "updated_date": "2025-01-18 00:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:34:25.616741"
    },
    {
      "arxiv_id": "2412.09579v1",
      "title": "A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks",
      "title_zh": "神经网络中软标签与硬标签训练的理论分析",
      "authors": [
        "Saptarshi Mandal",
        "Xiaojun Lin",
        "R. Srikant"
      ],
      "abstract": "Knowledge distillation, where a small student model learns from a pre-trained\nlarge teacher model, has achieved substantial empirical success since the\nseminal work of \\citep{hinton2015distilling}. Despite prior theoretical studies\nexploring the benefits of knowledge distillation, an important question remains\nunanswered: why does soft-label training from the teacher require significantly\nfewer neurons than directly training a small neural network with hard labels?\nTo address this, we first present motivating experimental results using simple\nneural network models on a binary classification problem. These results\ndemonstrate that soft-label training consistently outperforms hard-label\ntraining in accuracy, with the performance gap becoming more pronounced as the\ndataset becomes increasingly difficult to classify. We then substantiate these\nobservations with a theoretical contribution based on two-layer neural network\nmodels. Specifically, we show that soft-label training using gradient descent\nrequires only $O\\left(\\frac{1}{\\gamma^2 \\epsilon}\\right)$ neurons to achieve a\nclassification loss averaged over epochs smaller than some $\\epsilon > 0$,\nwhere $\\gamma$ is the separation margin of the limiting kernel. In contrast,\nhard-label training requires $O\\left(\\frac{1}{\\gamma^4} \\cdot\n\\ln\\left(\\frac{1}{\\epsilon}\\right)\\right)$ neurons, as derived from an adapted\nversion of the gradient descent analysis in \\citep{ji2020polylogarithmic}. This\nimplies that when $\\gamma \\leq \\epsilon$, i.e., when the dataset is challenging\nto classify, the neuron requirement for soft-label training can be\nsignificantly lower than that for hard-label training. Finally, we present\nexperimental results on deep neural networks, further validating these\ntheoretical findings.",
      "tldr_zh": "本论文理论分析了在神经网络中软-label训练与硬-label训练的差异，特别是在knowledge distillation框架下，探讨了为什么软-label训练需要更少的神经元。实验结果显示，在二元分类任务上，软-label训练的准确率显著高于硬-label训练，且这种优势在数据集难分类时更为明显。理论贡献证明，对于两层神经网络，软-label训练只需O(1/(γ² ε))个神经元即可实现较低的分类损失，而硬-label训练需要O(1/(γ⁴) · ln(1/ε))个神经元，从而在挑战性数据集上显示出更高的效率。最后，通过深度神经网络实验验证了这些理论发现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T01"
      ],
      "primary_category": "cs.LG",
      "comment": "Main Body of the Paper is under Review at L4DC 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09579v1",
      "published_date": "2024-12-12 18:54:07 UTC",
      "updated_date": "2024-12-12 18:54:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:34:42.385970"
    },
    {
      "arxiv_id": "2412.09578v1",
      "title": "DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial NEtworks and Semantic Topic classification",
      "title_zh": "翻译失败",
      "authors": [
        "Caleb Stam",
        "Emily Saldanha",
        "Mahantesh Halappanavar",
        "Anurag Acharya"
      ],
      "abstract": "The emergence of the COVID-19 pandemic resulted in a significant rise in the\nspread of misinformation on online platforms such as Twitter. Oftentimes this\ngrowth is blamed on the idea of the \"echo chamber.\" However, the behavior said\nto characterize these echo chambers exists in two dimensions. The first is in a\nuser's social interactions, where they are said to stick with the same clique\nof like-minded users. The second is in the content of their posts, where they\nare said to repeatedly espouse homogeneous ideas. In this study, we link the\ntwo by using Twitter's network of retweets to study social interactions and\ntopic modeling to study tweet content. In order to measure the diversity of a\nuser's interactions over time, we develop a novel metric to track the speed at\nwhich they travel through the social network. The application of these analysis\nmethods to misinformation-focused data from the pandemic demonstrates\ncorrelation between social behavior and tweet content. We believe this\ncorrelation supports the common intuition about how antisocial users behave,\nand further suggests that it holds even in subcommunities already rife with\nmisinformation.",
      "tldr_zh": "本研究调查了COVID-19疫情期间Twitter上虚假信息传播的现象，聚焦于“echo chamber”效应，包括用户社交互动（如与类似观点用户的互动）和推文内容同质化。研究者使用Twitter的retweets network分析社交行为，并结合topic modeling来评估推文主题，同时开发了一个新指标来追踪用户在社交网络中的移动速度。结果显示，社交行为与推文内容之间存在显著相关性，支持了关于传播虚假信息用户行为的直觉，即使在已充斥虚假信息的子社区中也适用。该发现为理解和应对在线虚假信息传播提供了重要洞见。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09578v1",
      "published_date": "2024-12-12 18:53:46 UTC",
      "updated_date": "2024-12-12 18:53:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:34:50.094751"
    },
    {
      "arxiv_id": "2412.09569v1",
      "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
      "title_zh": "翻译失败",
      "authors": [
        "Ariel Gera",
        "Odellia Boni",
        "Yotam Perlitz",
        "Roy Bar-Haim",
        "Lilach Eden",
        "Asaf Yehudai"
      ],
      "abstract": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.",
      "tldr_zh": "这篇论文提出了 JuStRank 基准，用于评估 LLM judges 在系统排名中的性能，以应对生成式 AI 模型多样性的挑战。不同于以往的 instance-based assessment，该研究首次进行大规模系统级评估，通过聚合多个系统输出的判断分数并与人类排名比较，来分析 LLM judges 的准确性、偏见和决定性。结果显示，LLM judges 可能存在对特定系统的正负偏见，这为改进模型评估和选择提供了细粒度洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09569v1",
      "published_date": "2024-12-12 18:51:13 UTC",
      "updated_date": "2024-12-12 18:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:35:00.583747"
    },
    {
      "arxiv_id": "2412.10467v1",
      "title": "MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media",
      "title_zh": "MGM：受众重叠图的全局理解，用于预测新闻媒体的事实性和偏见",
      "authors": [
        "Muhammad Arslan Manzoor",
        "Ruihong Zeng",
        "Dilshod Azizov",
        "Preslav Nakov",
        "Shangsong Liang"
      ],
      "abstract": "In the current era of rapidly growing digital data, evaluating the political\nbias and factuality of news outlets has become more important for seeking\nreliable information online. In this work, we study the classification problem\nof profiling news media from the lens of political bias and factuality.\nTraditional profiling methods, such as Pre-trained Language Models (PLMs) and\nGraph Neural Networks (GNNs) have shown promising results, but they face\nnotable challenges. PLMs focus solely on textual features, causing them to\noverlook the complex relationships between entities, while GNNs often struggle\nwith media graphs containing disconnected components and insufficient labels.\nTo address these limitations, we propose MediaGraphMind (MGM), an effective\nsolution within a variational Expectation-Maximization (EM) framework. Instead\nof relying on limited neighboring nodes, MGM leverages features, structural\npatterns, and label information from globally similar nodes. Such a framework\nnot only enables GNNs to capture long-range dependencies for learning\nexpressive node representations but also enhances PLMs by integrating\nstructural information and therefore improving the performance of both models.\nThe extensive experiments demonstrate the effectiveness of the proposed\nframework and achieve new state-of-the-art results. Further, we share our\nrepository1 which contains the dataset, code, and documentation",
      "tldr_zh": "本研究针对新闻媒体的政治偏见和真实性评估问题，提出了一种名为 MediaGraphMind (MGM) 的框架，利用 variational Expectation-Maximization (EM) 方法分析观众重叠图。MGM 通过整合全局相似的节点特征、结构模式和标签信息，克服了传统 Pre-trained Language Models (PLMs) 忽略实体关系以及 Graph Neural Networks (GNNs) 处理断开组件和标签不足的局限，从而提升 GNNs 对长距离依赖的捕捉和 PLMs 的整体性能。实验结果显示，MGM 在相关任务上实现了新的 state-of-the-art 性能，并公开了数据集、代码和文档以支持进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10467v1",
      "published_date": "2024-12-12 18:37:32 UTC",
      "updated_date": "2024-12-12 18:37:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:35:13.013885"
    },
    {
      "arxiv_id": "2412.09544v1",
      "title": "Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking",
      "title_zh": "翻译失败",
      "authors": [
        "Paria Rashidinejad",
        "Yuandong Tian"
      ],
      "abstract": "Aligning AI systems with human preferences typically suffers from the\ninfamous reward hacking problem, where optimization of an imperfect reward\nmodel leads to undesired behaviors. In this paper, we investigate reward\nhacking in offline preference optimization, which aims to improve an initial\nmodel using a preference dataset. We identify two types of reward hacking\nstemming from statistical fluctuations in the dataset: Type I Reward Hacking\ndue to subpar choices appearing more favorable, and Type II Reward Hacking due\nto decent choices appearing less favorable. We prove that many (mainstream or\ntheoretical) preference optimization methods suffer from both types of reward\nhacking. To mitigate Type I Reward Hacking, we propose POWER, a new preference\noptimization method that combines Guiasu's weighted entropy with a robust\nreward maximization objective. POWER enjoys finite-sample guarantees under\ngeneral function approximation, competing with the best covered policy in the\ndata. To mitigate Type II Reward Hacking, we analyze the learning dynamics of\npreference optimization and develop a novel technique that dynamically updates\npreference labels toward certain \"stationary labels\", resulting in diminishing\ngradients for untrustworthy samples. Empirically, POWER with dynamic labels\n(POWER-DL) consistently outperforms state-of-the-art methods on alignment\nbenchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and\n11.5 points on Arena-Hard over DPO, while also improving or maintaining\nperformance on downstream tasks such as mathematical reasoning. Strong\ntheoretical guarantees and empirical results demonstrate the promise of\nPOWER-DL in mitigating reward hacking.",
      "tldr_zh": "这篇论文探讨了AI系统与人类偏好对齐过程中常见的Reward Hacking问题，特别是在离线偏好优化中识别了Type I Reward Hacking（次优选择看起来更吸引人）和Type II Reward Hacking（良好选择看起来不那么吸引人）。为了缓解这些问题，作者提出了POWER方法，该方法结合Guiasu's weighted entropy和robust reward maximization，以提供finite-sample guarantees；同时，开发了动态标签技术，通过更新偏好标签向“stationary labels”靠拢，减少不可靠样本的梯度影响。最终，POWER-DL在实验中显著超越DPO，在AlpacaEval 2.0上提升13.0分、在Arena-Hard上提升11.5分，同时在下游任务如数学推理中保持或改善性能。理论分析和实证结果证明了POWER-DL在 mitigation reward hacking方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "46 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09544v1",
      "published_date": "2024-12-12 18:34:47 UTC",
      "updated_date": "2024-12-12 18:34:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:35:27.612767"
    },
    {
      "arxiv_id": "2412.09666v1",
      "title": "Systematic Analysis of LLM Contributions to Planning: Solver, Verifier, Heuristic",
      "title_zh": "翻译失败",
      "authors": [
        "Haoming Li",
        "Zhaoliang Chen",
        "Songyuan Liu",
        "Yiming Lu",
        "Fei Liu"
      ],
      "abstract": "In this work, we provide a systematic analysis of how large language models\n(LLMs) contribute to solving planning problems. In particular, we examine how\nLLMs perform when they are used as problem solver, solution verifier, and\nheuristic guidance to improve intermediate solutions. Our analysis reveals that\nalthough it is difficult for LLMs to generate correct plans out-of-the-box,\nLLMs are much better at providing feedback signals to intermediate/incomplete\nsolutions in the form of comparative heuristic functions. This evaluation\nframework provides insights into how future work may design better LLM-based\ntree-search algorithms to solve diverse planning and reasoning problems. We\nalso propose a novel benchmark to evaluate LLM's ability to learn user\npreferences on the fly, which has wide applications in practical settings.",
      "tldr_zh": "本研究系统分析了大型语言模型(LLMs)在规划问题中的贡献，具体考察了LLMs作为问题求解器(Solver)、解决方案验证器(Verifier)和启发式指导(Heuristic)的性能。结果显示，虽然LLMs难以直接生成正确的计划，但它们更擅长提供反馈信号作为比较启发式函数来改进中间解决方案。该分析框架为设计更好的LLM-based树搜索算法提供了见解，并提出一个新基准来评估LLMs动态学习用户偏好的能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09666v1",
      "published_date": "2024-12-12 18:16:46 UTC",
      "updated_date": "2024-12-12 18:16:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:35:36.945750"
    },
    {
      "arxiv_id": "2412.09521v3",
      "title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Pathology Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Shengxuming Zhang",
        "Weihan Li",
        "Tianhong Gao",
        "Jiacong Hu",
        "Haoming Luo",
        "Xiuming Zhang",
        "Jing Zhang",
        "Mingli Song",
        "Zunlei Feng"
      ],
      "abstract": "Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\nexisting large vision-language models (LVLMs) are limited by input resolution\nconstraints, hindering their efficiency and accuracy in pathology image\nanalysis. To overcome these issues, we propose two innovative strategies: the\nmixed task-guided feature enhancement, which directs feature extraction toward\nlesion-related details across scales, and the prompt-guided detail feature\ncompletion, which integrates coarse- and fine-grained features from WSI based\non specific prompts without compromising inference speed. Leveraging a\ncomprehensive dataset of 490K samples from diverse pathology tasks, we trained\nthe pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate\nthat this model significantly outperforms existing methods in diagnostic\naccuracy and efficiency, providing an interactive, clinically aligned approach\nfor auxiliary diagnosis in a wide range of pathology applications.",
      "tldr_zh": "该论文针对现有大型视觉语言模型（LVLMs）在病理图像分析中的分辨率限制问题，提出两种创新策略：混合任务引导特征增强（mixed task-guided feature enhancement），用于引导多尺度特征提取关注病变相关细节；以及提示引导细节特征完成（prompt-guided detail feature completion），通过整合粗粒度和细粒度WSI特征来提升准确性，同时保持推理速度。研究团队训练了专门的病理专用LVLM模型OmniPath，使用涵盖49万样本的综合数据集。实验结果显示，OmniPath在诊断准确性和效率上显著优于现有方法，提供了一种交互式、临床相关的辅助诊断方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09521v3",
      "published_date": "2024-12-12 18:07:23 UTC",
      "updated_date": "2025-05-16 10:17:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:35:48.960908"
    },
    {
      "arxiv_id": "2412.09507v2",
      "title": "Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction",
      "title_zh": "视觉Transformer用于高效室内路径损耗无线电地图预测",
      "authors": [
        "Rafayel Mkrtchyan",
        "Edvard Ghukasyan",
        "Khoren Petrosyan",
        "Hrant Khachatrian",
        "Theofanis P. Raptis"
      ],
      "abstract": "Indoor pathloss prediction is a fundamental task in wireless network\nplanning, yet it remains challenging due to environmental complexity and data\nscarcity. In this work, we propose a deep learning-based approach utilizing a\nvision transformer (ViT) architecture with DINO-v2 pretrained weights to model\nindoor radio propagation. Our method processes a floor map with additional\nfeatures of the walls to generate indoor pathloss maps. We systematically\nevaluate the effects of architectural choices, data augmentation strategies,\nand feature engineering techniques. Our findings indicate that extensive\naugmentation significantly improves generalization, while feature engineering\nis crucial in low-data regimes. Through comprehensive experiments, we\ndemonstrate the robustness of our model across different generalization\nscenarios.",
      "tldr_zh": "这篇论文提出了一种基于 Vision Transformers (ViT) 的深度学习方法，用于高效预测室内路径损耗无线电地图，以应对环境复杂性和数据稀缺的挑战。方法利用 DINO-v2 预训练权重处理楼层地图和墙壁特征，并系统评估了架构选择、数据增强策略和特征工程技术的影响。研究发现，广泛的数据增强显著改善了模型的泛化能力，而特征工程在数据有限的情况下尤为关键；实验结果证明了该模型在不同泛化场景中的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")",
      "pdf_url": "http://arxiv.org/pdf/2412.09507v2",
      "published_date": "2024-12-12 17:55:00 UTC",
      "updated_date": "2025-05-08 10:03:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:36:01.603953"
    },
    {
      "arxiv_id": "2412.15238v1",
      "title": "Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Gregory Kang Ruey Lau",
        "Wenyang Hu",
        "Diwen Liu",
        "Jizhuo Chen",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "Large Language Models still encounter substantial challenges in reasoning\ntasks, especially for smaller models, which many users may be restricted to due\nto resource constraints (e.g. GPU memory restrictions). Inference-time methods\nto boost LLM performance, such as prompting methods to invoke certain reasoning\npathways in responses, have been shown effective in past works, though they\nlargely rely on sequential queries. The ensemble method, which consists of\nmultiple constituent models running in parallel, is a promising approach to\nachieving better inference-time performance, especially given recent\ndevelopments that enabled significant speed-ups in LLM batch inference. In this\nwork, we propose a novel, training-free LLM ensemble framework where a single\nLLM model is fed an optimized, diverse set of prompts in parallel, effectively\nproducing an ensemble at inference time to achieve performance improvement in\nreasoning tasks. We empirically demonstrate that our method leads to\nsignificant gains on math reasoning tasks, e.g., on MATH, where our ensemble\nconsisting of a few small models (e.g., three Qwen2-MATH-1.5B-it models) can\noutperform a larger model (e.g., Qwen2-MATH-7B-it).",
      "tldr_zh": "该论文提出Dipper框架，一种无需训练的Large Language Model集成方法，通过为单一LLM模型提供多样化的prompts，实现并行处理以提升推理任务性能。Dipper利用prompts的多样性来模拟集成效果，解决了小型模型在资源限制（如GPU内存）下的挑战。实验结果显示，在MATH等数学推理任务上，使用多个小型模型（如三个Qwen2-MATH-1.5B-it）的集成，可以超越更大模型（如Qwen2-MATH-7B-it）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NeurIPS 2024 Workshop on Foundation Model Interventions\n  (MINT)",
      "pdf_url": "http://arxiv.org/pdf/2412.15238v1",
      "published_date": "2024-12-12 17:49:05 UTC",
      "updated_date": "2024-12-12 17:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:36:13.248614"
    },
    {
      "arxiv_id": "2412.09492v1",
      "title": "Video Seal: Open and Efficient Video Watermarking",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Fernandez",
        "Hady Elsahar",
        "I. Zeki Yalniz",
        "Alexandre Mourachko"
      ],
      "abstract": "The proliferation of AI-generated content and sophisticated video editing\ntools has made it both important and challenging to moderate digital platforms.\nVideo watermarking addresses these challenges by embedding imperceptible\nsignals into videos, allowing for identification. However, the rare open tools\nand methods often fall short on efficiency, robustness, and flexibility. To\nreduce these gaps, this paper introduces Video Seal, a comprehensive framework\nfor neural video watermarking and a competitive open-sourced model. Our\napproach jointly trains an embedder and an extractor, while ensuring the\nwatermark robustness by applying transformations in-between, e.g., video\ncodecs. This training is multistage and includes image pre-training, hybrid\npost-training and extractor fine-tuning. We also introduce temporal watermark\npropagation, a technique to convert any image watermarking model to an\nefficient video watermarking model without the need to watermark every\nhigh-resolution frame. We present experimental results demonstrating the\neffectiveness of the approach in terms of speed, imperceptibility, and\nrobustness. Video Seal achieves higher robustness compared to strong baselines\nespecially under challenging distortions combining geometric transformations\nand video compression. Additionally, we provide new insights such as the impact\nof video compression during training, and how to compare methods operating on\ndifferent payloads. Contributions in this work - including the codebase,\nmodels, and a public demo - are open-sourced under permissive licenses to\nfoster further research and development in the field.",
      "tldr_zh": "这篇论文提出了 Video Seal，一个高效的开源神经视频水印框架，旨在解决数字平台内容审核面临的挑战，如 AI 生成内容和视频编辑工具的滥用。该框架通过联合训练嵌入器和提取器，并在训练中应用变换（如视频编解码）来确保水印的鲁棒性，还引入了 temporal watermark propagation 技术，将图像水印模型高效转化为视频水印模型，而无需处理每个高分辨率帧。实验结果表明，Video Seal 在速度、隐蔽性和鲁棒性方面表现出色，比现有基线模型在几何变换和视频压缩等复杂扭曲下提升了鲁棒性；此外，该工作提供了新见解并开源了代码、模型和演示，以推动该领域的进一步研究。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "comment": "Code available at https://github.com/facebookresearch/videoseal",
      "pdf_url": "http://arxiv.org/pdf/2412.09492v1",
      "published_date": "2024-12-12 17:41:49 UTC",
      "updated_date": "2024-12-12 17:41:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:36:25.913554"
    },
    {
      "arxiv_id": "2412.09486v1",
      "title": "Regression and Classification with Single-Qubit Quantum Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Leandro C. Souza",
        "Bruno C. Guingo",
        "Gilson Giraldi",
        "Renato Portugal"
      ],
      "abstract": "Since classical machine learning has become a powerful tool for developing\ndata-driven algorithms, quantum machine learning is expected to similarly\nimpact the development of quantum algorithms. The literature reflects a\nmutually beneficial relationship between machine learning and quantum\ncomputing, where progress in one field frequently drives improvements in the\nother. Motivated by the fertile connection between machine learning and quantum\ncomputing enabled by parameterized quantum circuits, we use a\nresource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for\nboth regression and classification tasks. The SQQNN leverages parameterized\nsingle-qubit unitary operators and quantum measurements to achieve efficient\nlearning. To train the model, we use gradient descent for regression tasks. For\nclassification, we introduce a novel training method inspired by the Taylor\nseries, which can efficiently find a global minimum in a single step. This\napproach significantly accelerates training compared to iterative methods.\nEvaluated across various applications, the SQQNN exhibits virtually error-free\nand strong performance in regression and classification tasks, including the\nMNIST dataset. These results demonstrate the versatility, scalability, and\nsuitability of the SQQNN for deployment on near-term quantum devices.",
      "tldr_zh": "本文提出了一种资源高效且可扩展的Single-Qubit Quantum Neural Networks (SQQNN)，用于处理回归和分类任务，通过参数化的单量子比特酉算子（parameterized single-qubit unitary operators）和量子测量实现高效学习。对于回归任务，采用gradient descent进行训练；对于分类任务，引入一种基于Taylor series启发的单步优化方法，能快速找到全局最小值并加速训练过程。实验结果显示，SQQNN在包括MNIST数据集在内的多种应用中表现出几乎无错误的高性能，证明其多功能性、可扩展性和适合部署在近中期量子设备上。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "21 pages, 7 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.09486v1",
      "published_date": "2024-12-12 17:35:36 UTC",
      "updated_date": "2024-12-12 17:35:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:36:38.287475"
    },
    {
      "arxiv_id": "2412.09480v1",
      "title": "The Parameters of Educability",
      "title_zh": "可教育性的参数",
      "authors": [
        "Leslie G. Valiant"
      ],
      "abstract": "The educability model is a computational model that has been recently\nproposed to describe the cognitive capability that makes humans unique among\nexisting biological species on Earth in being able to create advanced\ncivilizations. Educability is defined as a capability for acquiring and\napplying knowledge. It is intended both to describe human capabilities and,\nequally, as an aspirational description of what can be usefully realized by\nmachines. While the intention is to have a mathematically well-defined\ncomputational model, in constructing an instance of the model there are a\nnumber of decisions to make. We call these decisions {\\it parameters}. In a\nstandard computer, two parameters are the memory capacity and clock rate. There\nis no universally optimal choice for either one, or even for their ratio.\nSimilarly, in a standard machine learning system, two parameters are the\nlearning algorithm and the dataset used for training. Again, there are no\nuniversally optimal choices known for either. An educable system has many more\nparameters than either of these two kinds of system. This short paper discusses\nsome of the main parameters of educable systems, and the broader implications\nof their existence.",
      "tldr_zh": "该论文介绍了“educability”模型，这是一种计算模型，用于描述人类独特的认知能力，即通过获取和应用知识来创建先进文明，同时作为机器实现的目标。“Educability”系统涉及多种“parameters”（参数），如内存容量、时钟频率、学习算法和数据集，这些参数没有普遍最优选择。论文探讨了educable系统的关键参数及其更广泛含义，强调了在构建此类系统时决策的重要性。",
      "categories": [
        "cs.AI",
        "q-bio.NC",
        "I.2.0; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09480v1",
      "published_date": "2024-12-12 17:27:03 UTC",
      "updated_date": "2024-12-12 17:27:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:36:48.280998"
    },
    {
      "arxiv_id": "2412.09475v2",
      "title": "New keypoint-based approach for recognising British Sign Language (BSL) from sequences",
      "title_zh": "翻译失败",
      "authors": [
        "Oishi Deb",
        "KR Prajwal",
        "Andrew Zisserman"
      ],
      "abstract": "In this paper, we present a novel keypoint-based classification model\ndesigned to recognise British Sign Language (BSL) words within continuous\nsigning sequences. Our model's performance is assessed using the BOBSL dataset,\nrevealing that the keypoint-based approach surpasses its RGB-based counterpart\nin computational efficiency and memory usage. Furthermore, it offers expedited\ntraining times and demands fewer computational resources. To the best of our\nknowledge, this is the inaugural application of a keypoint-based model for BSL\nword classification, rendering direct comparisons with existing works\nunavailable.",
      "tldr_zh": "本论文提出了一种新型的 keypoint-based 分类模型，用于从连续序列中识别英国手语 (BSL) 单词。模型在 BOBSL 数据集上进行评估，结果显示其在计算效率、内存使用和训练时间方面均优于 RGB-based 方法，并需更少的计算资源。到目前为止，这是首次将 keypoint-based 模型应用于 BSL 单词分类，因此缺乏直接可比的现有研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "International Conference on Computer Vision (ICCV) - HANDS Workshop",
      "pdf_url": "http://arxiv.org/pdf/2412.09475v2",
      "published_date": "2024-12-12 17:20:27 UTC",
      "updated_date": "2024-12-31 18:58:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:37:01.054841"
    },
    {
      "arxiv_id": "2412.09468v3",
      "title": "STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading",
      "title_zh": "STORM：基于双向量量化变分自编码器的时空因子模型，用于金融交易",
      "authors": [
        "Yilei Zhao",
        "Wentao Zhang",
        "Tingran Yang",
        "Yong Jiang",
        "Fei Huang",
        "Wei Yang Bryan Lim"
      ],
      "abstract": "In financial trading, factor models are widely used to price assets and\ncapture excess returns from mispricing. Recently, we have witnessed the rise of\nvariational autoencoder-based latent factor models, which learn latent factors\nself-adaptively. While these models focus on modeling overall market\nconditions, they often fail to effectively capture the temporal patterns of\nindividual stocks. Additionally, representing multiple factors as single values\nsimplifies the model but limits its ability to capture complex relationships\nand dependencies. As a result, the learned factors are of low quality and lack\ndiversity, reducing their effectiveness and robustness across different trading\nperiods. To address these issues, we propose a Spatio-Temporal factOR Model\nbased on dual vector quantized variational autoencoders, named STORM, which\nextracts features of stocks from temporal and spatial perspectives, then fuses\nand aligns these features at the fine-grained and semantic level, and\nrepresents the factors as multi-dimensional embeddings. The discrete codebooks\ncluster similar factor embeddings, ensuring orthogonality and diversity, which\nhelps distinguish between different factors and enables factor selection in\nfinancial trading. To show the performance of the proposed factor model, we\napply it to two downstream experiments: portfolio management on two stock\ndatasets and individual trading tasks on six specific stocks. The extensive\nexperiments demonstrate STORM's flexibility in adapting to downstream tasks and\nsuperior performance over baseline models.",
      "tldr_zh": "该研究提出STORM，一种基于Dual Vector Quantized Variational Autoencoders的空间-时间因子模型，用于金融交易，以解决现有VAE-based隐因子模型在捕捉个股时间模式和复杂关系方面的不足。STORM从时间和空间视角提取股票特征，并通过融合、对齐这些特征来生成多维因子嵌入，同时利用离散代码本确保因子正交性和多样性，从而支持有效的因子选择。实验结果显示，该模型在两个股票数据集的投资组合管理和六种个股交易任务中，表现出色，并优于基线模型，展示了其在下游任务中的灵活性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09468v3",
      "published_date": "2024-12-12 17:15:49 UTC",
      "updated_date": "2025-03-17 04:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:37:14.286344"
    },
    {
      "arxiv_id": "2412.09433v1",
      "title": "Solving Multiagent Path Finding on Highly Centralized Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Foivos Fioravantes",
        "Dušan Knop",
        "Jan Matyáš Křišťan",
        "Nikolaos Melissinos",
        "Michal Opler",
        "Tung Anh Vu"
      ],
      "abstract": "The Mutliagent Path Finding (MAPF) problem consists of identifying the\ntrajectories that a set of agents should follow inside a given network in order\nto reach their desired destinations as soon as possible, but without colliding\nwith each other. We aim to minimize the maximum time any agent takes to reach\ntheir goal, ensuring optimal path length. In this work, we complement a recent\nthread of results that aim to systematically study the algorithmic behavior of\nthis problem, through the parameterized complexity point of view.\n  First, we show that MAPF is NP-hard when the given network has a star-like\ntopology (bounded vertex cover number) or is a tree with $11$ leaves. Both of\nthese results fill important gaps in our understanding of the tractability of\nthis problem that were left untreated in the recent work of [Fioravantes et al.\nExact Algorithms and Lowerbounds for Multiagent Path Finding: Power of Treelike\nTopology. AAAI'24]. Nevertheless, our main contribution is an exact algorithm\nthat scales well as the input grows (FPT) when the topology of the given\nnetwork is highly centralized (bounded distance to clique). This parameter is\nsignificant as it mirrors real-world networks. In such environments, a bunch of\ncentral hubs (e.g., processing areas) are connected to only few peripheral\nnodes.",
      "tldr_zh": "本研究针对 Multiagent Path Finding (MAPF) 问题，旨在为多个代理在给定网络中找到最优路径，以最小化最大到达时间并避免碰撞。研究证明，MAPF 在星状拓扑（bounded vertex cover number）或具有 11 个叶子的树上均为 NP-hard，这填补了先前工作的空白。主要的贡献是提出一个精确算法，该算法在网络拓扑高度集中化（bounded distance to clique）时表现出良好的可扩展性（FPT），适用于真实世界网络如中央枢纽连接外围节点的场景。通过参数化复杂性分析，该方法为高效解决 MAPF 问题提供了新途径。",
      "categories": [
        "cs.CC",
        "cs.AI"
      ],
      "primary_category": "cs.CC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09433v1",
      "published_date": "2024-12-12 16:38:25 UTC",
      "updated_date": "2024-12-12 16:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:37:25.820142"
    },
    {
      "arxiv_id": "2412.09429v2",
      "title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
      "title_zh": "从意图到实现：通过 LLMs 自动化生物医学研究",
      "authors": [
        "Yi Luo",
        "Linghang Shi",
        "Yihao Li",
        "Aobo Zhuang",
        "Yeyun Gong",
        "Ling Liu",
        "Chen Lin"
      ],
      "abstract": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems.",
      "tldr_zh": "这篇论文探讨了 LLMs 在自动化生物医学研究的潜力，以应对传统研究因文献和数据集爆炸式增长而带来的劳动密集问题。论文引入了 BioResearcher，这是一个端到端的模块化多智能体系统，涵盖搜索、文献处理、实验设计和编程等任务，通过任务分解、分层学习和基于 LLM 的质量控制机制来解决多学科需求和逻辑复杂性。实验结果显示，BioResearcher 在八个未实现的研究目标上平均执行成功率达到 63.07%，生成的实验协议在五个质量指标上比典型代理系统高 22.0%。该系统有望显著减轻研究人员负担并加速生物医学发现，为自动化研究铺平道路。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09429v2",
      "published_date": "2024-12-12 16:35:05 UTC",
      "updated_date": "2024-12-22 05:34:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:37:38.405538"
    },
    {
      "arxiv_id": "2412.09417v2",
      "title": "Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Labiosa",
        "Zhihan Wang",
        "Siddhant Agarwal",
        "William Cong",
        "Geethika Hemkumar",
        "Abhinav Narayan Harish",
        "Benjamin Hong",
        "Josh Kelle",
        "Chen Li",
        "Yuhao Li",
        "Zisen Shao",
        "Peter Stone",
        "Josiah P. Hanna"
      ],
      "abstract": "Robot decision-making in partially observable, real-time, dynamic, and\nmulti-agent environments remains a difficult and unsolved challenge. Model-free\nreinforcement learning (RL) is a promising approach to learning decision-making\nin such domains, however, end-to-end RL in complex environments is often\nintractable. To address this challenge in the RoboCup Standard Platform League\n(SPL) domain, we developed a novel architecture integrating RL within a\nclassical robotics stack, while employing a multi-fidelity sim2real approach\nand decomposing behavior into learned sub-behaviors with heuristic selection.\nOur architecture led to victory in the 2024 RoboCup SPL Challenge Shield\nDivision. In this work, we fully describe our system's architecture and\nempirically analyze key design decisions that contributed to its success. Our\napproach demonstrates how RL-based behaviors can be integrated into complete\nrobot behavior architectures.",
      "tldr_zh": "本研究探讨了在部分可观察、实时动态多智能体环境中机器人决策的难题，通过将强化学习（RL）整合到经典机器人堆栈中，开发了一种新架构。该架构采用多保真 sim2real 方法，并将行为分解为学习子行为和启发式选择，从而提高了系统的鲁棒性。在2024 RoboCup SPL Challenge Shield Division中，该系统获胜，证明了RL在复杂机器人应用中的有效性，并为未来机器人行为设计提供了经验教训。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09417v2",
      "published_date": "2024-12-12 16:25:10 UTC",
      "updated_date": "2025-03-07 02:12:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:37:49.839963"
    },
    {
      "arxiv_id": "2412.09413v2",
      "title": "Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yingqian Min",
        "Zhipeng Chen",
        "Jinhao Jiang",
        "Jie Chen",
        "Jia Deng",
        "Yiwen Hu",
        "Yiru Tang",
        "Jiapeng Wang",
        "Xiaoxue Cheng",
        "Huatong Song",
        "Wayne Xin Zhao",
        "Zheng Liu",
        "Zhongyuan Wang",
        "Ji-Rong Wen"
      ],
      "abstract": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an ``imitate, explore, and self-improve'' framework,\ndenoted as \\textbf{STILL-2}, as our primary technical approach to train the\nreasoning model. In the initial phase, we use distilled long-form thought data\nto fine-tune the reasoning model, enabling it to invoke a slow-thinking mode.\nThe model is then encouraged to explore challenging problems by generating\nmultiple rollouts, which can result in increasingly more high-quality\ntrajectories that lead to correct answers. Furthermore, the model undergoes\nself-improvement by iteratively refining its training dataset. To verify the\neffectiveness of this approach, we conduct extensive experiments on three\nchallenging benchmarks. The experimental results demonstrate that our approach\nachieves competitive performance compared to industry-level reasoning systems\non these benchmarks.",
      "tldr_zh": "本论文报告了对慢思考推理系统（如o1）的再现研究，提出了一种“imitate, explore, and self-improve”框架，称为STILL-2，用于训练高效的推理模型。框架的初始阶段通过使用蒸馏的长形式思考数据微调模型，使其能够激活慢思考模式；随后，模型通过生成多个回合（rollouts）探索挑战性问题，产生高质量轨迹；最后，模型通过迭代精炼训练数据集实现自提升。实验结果显示，该方法在三个挑战性基准上取得了与行业级系统相当的性能，验证了其有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical Report on Slow Thinking with LLMs: Part II",
      "pdf_url": "http://arxiv.org/pdf/2412.09413v2",
      "published_date": "2024-12-12 16:20:36 UTC",
      "updated_date": "2024-12-22 10:44:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:38:01.952672"
    },
    {
      "arxiv_id": "2412.09407v1",
      "title": "Uncommon Belief in Rationality",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Shi",
        "Pavel Naumov"
      ],
      "abstract": "Common knowledge/belief in rationality is the traditional standard assumption\nin analysing interaction among agents. This paper proposes a graph-based\nlanguage for capturing significantly more complicated structures of\nhigher-order beliefs that agents might have about the rationality of the other\nagents. The two main contributions are a solution concept that captures the\nreasoning process based on a given belief structure and an efficient algorithm\nfor compressing any belief structure into a unique minimal form.",
      "tldr_zh": "这篇论文质疑了传统分析代理人互动中的 common knowledge/belief in rationality 假设，提出了一种 graph-based language 来捕捉代理人对其他代理人理性的更高阶信念的复杂结构。论文的主要贡献包括一个 solution concept，用于基于给定信念结构捕获推理过程，以及一个 efficient algorithm，能够将任何信念结构压缩成唯一的极简形式。这种方法为更精确地建模代理人互动提供了新框架。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "The 39th Annual AAAI Conference on Artificial Intelligence (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2412.09407v1",
      "published_date": "2024-12-12 16:12:40 UTC",
      "updated_date": "2024-12-12 16:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:40:07.077976"
    },
    {
      "arxiv_id": "2412.12173v1",
      "title": "A NotSo Simple Way to Beat Simple Bench",
      "title_zh": "翻译失败",
      "authors": [
        "Soham Sane",
        "Angus McLean"
      ],
      "abstract": "This paper presents a novel framework for enhancing reasoning capabilities in\nlarge language models (LLMs) by leveraging iterative reasoning and\nfeedback-driven methodologies. Building on the limitations identified in the\nSimpleBench benchmark, a dataset designed to evaluate logical coherence and\nreal-world reasoning, we propose a multi-step prompting strategy coupled with\nglobal consistency checks to improve model accuracy and robustness. Through\ncomparative analysis of state-of-the-art models, including Claude 3 Opus,\nClaude 3.5, GPT- 4o, and o1-preview, we demonstrate that iterative reasoning\nsignificantly enhances model performance, with improvements observed in both\nstandard accuracy metrics (AVG@5) and a newly introduced metric, Extreme\nAveraging (EAG@5). Our results reveal model-specific strengths: Claude excels\nin maintaining logical consistency, while GPT-4o exhibits exploratory\ncreativity but struggles with ambiguous prompts. By analyzing case studies and\nidentifying gaps in spatial and temporal reasoning, we highlight areas for\nfurther refinement. The findings underscore the potential of structured\nreasoning frameworks to address inherent model limitations, irrespective of\npretraining methodologies. This study lays the groundwork for integrating\ndynamic feedback mechanisms, adaptive restart strategies, and diverse\nevaluation metrics to advance LLM reasoning capabilities across complex and\nmulti-domain problem spaces.",
      "tldr_zh": "本论文提出了一种新框架，通过迭代推理(iterative reasoning)和反馈驱动(feedback-driven methodologies)方法，提升大型语言模型(LLMs)的推理能力，以克服SimpleBench基准中暴露的逻辑一致性和真实世界推理局限。研究采用多步提示策略和全局一致性检查，对Claude 3 Opus、Claude 3.5、GPT-4o和o1-preview等模型进行比较分析，结果显示迭代推理显著提高了标准准确性指标(AVG@5)和新引入的Extreme Averaging(EAG@5)指标，其中Claude模型在逻辑一致性上表现出色，而GPT-4o在探索性创意方面领先但对模糊提示处理较弱。论文还通过案例分析识别了空间和时间推理的改进空间，并强调结构化推理框架的潜力，为整合动态反馈机制和适应性策略奠定基础，以推进LLMs在复杂多域问题中的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 11 Figures",
      "pdf_url": "http://arxiv.org/pdf/2412.12173v1",
      "published_date": "2024-12-12 16:04:31 UTC",
      "updated_date": "2024-12-12 16:04:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:40:19.777553"
    },
    {
      "arxiv_id": "2412.09389v1",
      "title": "UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer",
      "title_zh": "UFO：通过统一帧组织器增强基于扩散的视频生成",
      "authors": [
        "Delong Liu",
        "Zhaohui Hou",
        "Mingjie Zhan",
        "Shihao Han",
        "Zhicheng Zhao",
        "Fei Su"
      ],
      "abstract": "Recently, diffusion-based video generation models have achieved significant\nsuccess. However, existing models often suffer from issues like weak\nconsistency and declining image quality over time. To overcome these\nchallenges, inspired by aesthetic principles, we propose a non-invasive plug-in\ncalled Uniform Frame Organizer (UFO), which is compatible with any\ndiffusion-based video generation model. The UFO comprises a series of adaptive\nadapters with adjustable intensities, which can significantly enhance the\nconsistency between the foreground and background of videos and improve image\nquality without altering the original model parameters when integrated. The\ntraining for UFO is simple, efficient, requires minimal resources, and supports\nstylized training. Its modular design allows for the combination of multiple\nUFOs, enabling the customization of personalized video generation models.\nFurthermore, the UFO also supports direct transferability across different\nmodels of the same specification without the need for specific retraining. The\nexperimental results indicate that UFO effectively enhances video generation\nquality and demonstrates its superiority in public video generation benchmarks.\nThe code will be publicly available at https://github.com/Delong-liu-bupt/UFO.",
      "tldr_zh": "本研究针对基于扩散的视频生成模型存在的弱一致性和图像质量随时间下降等问题，提出了一种非侵入式插件 Uniform Frame Organizer (UFO)，兼容任何 diffusion-based video generation 模型。UFO 通过一系列可调节强度的自适应适配器，增强视频前景和背景的一致性，同时提高图像质量，而无需修改原模型参数。插件的训练过程简单、高效、资源需求低，支持风格化训练和模块化组合，以实现个性化视频生成模型。此外，实验结果显示，UFO 显著提升视频生成质量，并在公共基准上表现出优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code:https://github.com/Delong-liu-bupt/UFO",
      "pdf_url": "http://arxiv.org/pdf/2412.09389v1",
      "published_date": "2024-12-12 15:56:26 UTC",
      "updated_date": "2024-12-12 15:56:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:40:33.337714"
    },
    {
      "arxiv_id": "2412.09388v2",
      "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Zhou",
        "Ke Zhu",
        "Jianxin Wu"
      ],
      "abstract": "Knowledge Distillation (KD) is essential in transferring dark knowledge from\na large teacher to a small student network, such that the student can be much\nmore efficient than the teacher but with comparable accuracy. Existing KD\nmethods, however, rely on a large teacher trained specifically for the target\ntask, which is both very inflexible and inefficient. In this paper, we argue\nthat a SSL-pretrained model can effectively act as the teacher and its dark\nknowledge can be captured by the coordinate system or linear subspace where the\nfeatures lie in. We then need only one forward pass of the teacher, and then\ntailor the coordinate system (TCS) for the student network. Our TCS method is\nteacher-free and applies to diverse architectures, works well for KD and\npractical few-shot learning, and allows cross-architecture distillation with\nlarge capacity gap. Experiments show that TCS achieves significantly higher\naccuracy than state-of-the-art KD methods, while only requiring roughly half of\ntheir training time and GPU memory costs.",
      "tldr_zh": "本文提出了一种新型 Knowledge Distillation (KD) 方法，使用 SSL 预训练模型作为教师，通过定制坐标系统 (Tailored Coordinate System, TCS) 来捕获特征的线性子空间，从而高效转移暗知识。该方法仅需教师的一次前向传递，即可为学生网络量身定制坐标系统，支持不同架构的跨蒸馏和少样本学习。实验结果表明，TCS 比现有最先进 KD 方法准确率显著提高，同时仅需约一半的训练时间和 GPU 内存。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09388v2",
      "published_date": "2024-12-12 15:56:20 UTC",
      "updated_date": "2025-02-12 10:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:40:42.374894"
    },
    {
      "arxiv_id": "2412.09387v1",
      "title": "Distributed Intelligent System Architecture for UAV-Assisted Monitoring of Wind Energy Infrastructure",
      "title_zh": "分布式智能系统架构，用于 UAV 辅助的风能基础设施监测",
      "authors": [
        "Serhii Svystun",
        "Oleksandr Melnychenko",
        "Pavlo Radiuk",
        "Oleg Savenko",
        "Andrii Lysyi"
      ],
      "abstract": "With the rapid development of green energy, the efficiency and reliability of\nwind turbines are key to sustainable renewable energy production. For that\nreason, this paper presents a novel intelligent system architecture designed\nfor the dynamic collection and real-time processing of visual data to detect\ndefects in wind turbines. The system employs advanced algorithms within a\ndistributed framework to enhance inspection accuracy and efficiency using\nunmanned aerial vehicles (UAVs) with integrated visual and thermal sensors. An\nexperimental study conducted at the \"Staryi Sambir-1\" wind power plant in\nUkraine demonstrates the system's effectiveness, showing a significant\nimprovement in defect detection accuracy (up to 94%) and a reduction in\ninspection time per turbine (down to 1.5 hours) compared to traditional\nmethods. The results show that the proposed intelligent system architecture\nprovides a scalable and reliable solution for wind turbine maintenance,\ncontributing to the durability and performance of renewable energy\ninfrastructure.",
      "tldr_zh": "这篇论文提出了一种分布式智能系统架构，利用无人机（UAVs）辅助监测风力基础设施，以实现风力涡轮机的缺陷检测。系统采用高级算法和分布式框架，结合视觉和热感传感器进行动态数据收集及实时处理，从而提升检查的准确性和效率。在乌克兰“Staryi Sambir-1”风力发电厂的实验中，该架构将缺陷检测准确率提高到94%，并将每个涡轮机的检查时间缩短至1.5小时，显著优于传统方法。该解决方案为风力涡轮机的可扩展维护提供了可靠途径，提升了可再生能源基础设施的耐久性和性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.4.8; I.2.10; I.5.4; I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "Wind turbine inspection, UAV, intelligent systems, distributed\n  architecture, defect detection, renewable energy maintenance, automated\n  monitoring",
      "pdf_url": "http://arxiv.org/pdf/2412.09387v1",
      "published_date": "2024-12-12 15:53:58 UTC",
      "updated_date": "2024-12-12 15:53:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:40:54.755092"
    },
    {
      "arxiv_id": "2412.09385v2",
      "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Fabrizio Davide",
        "Pietro Torre",
        "Leonardo Ercolani",
        "Andrea Gaggioli"
      ],
      "abstract": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.",
      "tldr_zh": "该研究让 16 个最先进的 LLMs 估计 AGI 在 2030 年出现的概率，并通过自动化同行评审过程 (LLM-PR) 来评估这些预测的质量，结果显示 LLMs 的估计从 3% (Reka-Core) 到 47.6% (GPT-4o)，中位数为 12.5%，与专家调查结果高度一致。LLM-PR 展现出高可靠性 (Intraclass Correlation Coefficient, ICC = 0.79)，Pplx-70b-online 表现最佳，而 Gemini-1.5-pro-api 最差；此外，与外部基准如 LMSYS Chatbot Arena 的比较表明现有评估方法可能无法充分捕捉 AGI 预测所需的技能。研究进一步开发了新的 'AGI benchmark'，通过加权方案优化 LLMs 的预测与专家预估的匹配，并强调了创新评估框架在复杂、投机性任务中的必要性。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "47 pages, 8 figures, 17 tables, appendix with data and code",
      "pdf_url": "http://arxiv.org/pdf/2412.09385v2",
      "published_date": "2024-12-12 15:52:41 UTC",
      "updated_date": "2025-04-22 13:56:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:43:00.397081"
    },
    {
      "arxiv_id": "2412.10464v1",
      "title": "Automatic Detection, Positioning and Counting of Grape Bunches Using Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Xumin Gao"
      ],
      "abstract": "In order to promote agricultural automatic picking and yield estimation\ntechnology, this project designs a set of automatic detection, positioning and\ncounting algorithms for grape bunches, and applies it to agricultural robots.\nThe Yolov3 detection network is used to realize the accurate detection of grape\nbunches, and the local tracking algorithm is added to eliminate relocation.\nThen it obtains the accurate 3D spatial position of the central points of grape\nbunches using the depth distance and the spatial restriction method. Finally,\nthe counting of grape bunches is completed. It is verified using the\nagricultural robot in the simulated vineyard environment. The project code is\nreleased at:\nhttps://github.com/XuminGaoGithub/Grape_bunches_count_using_robots.",
      "tldr_zh": "该研究设计了一套算法，用于农业机器人的葡萄串自动检测、定位和计数，以促进农业自动采摘和产量估计技术。具体方法包括使用Yolov3检测网络实现葡萄串的精确检测，结合局部跟踪算法消除重定位错误，并通过深度距离和空间限制方法获取葡萄串中心点的准确3D空间位置，最终完成计数。在模拟葡萄园环境中使用农业机器人进行验证，代码已开源于GitHub。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10464v1",
      "published_date": "2024-12-12 15:52:40 UTC",
      "updated_date": "2024-12-12 15:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:41:17.611999"
    },
    {
      "arxiv_id": "2412.09380v1",
      "title": "Diffusion Model with Representation Alignment for Protein Inverse Folding",
      "title_zh": "用于蛋白质逆折叠的表示对齐扩散模型",
      "authors": [
        "Chenglin Wang",
        "Yucheng Zhou",
        "Zijie Zhai",
        "Jianbing Shen",
        "Kai Zhang"
      ],
      "abstract": "Protein inverse folding is a fundamental problem in bioinformatics, aiming to\nrecover the amino acid sequences from a given protein backbone structure.\nDespite the success of existing methods, they struggle to fully capture the\nintricate inter-residue relationships critical for accurate sequence\nprediction. We propose a novel method that leverages diffusion models with\nrepresentation alignment (DMRA), which enhances diffusion-based inverse folding\nby (1) proposing a shared center that aggregates contextual information from\nthe entire protein structure and selectively distributes it to each residue;\nand (2) aligning noisy hidden representations with clean semantic\nrepresentations during the denoising process. This is achieved by predefined\nsemantic representations for amino acid types and a representation alignment\nmethod that utilizes type embeddings as semantic feedback to normalize each\nresidue. In experiments, we conduct extensive evaluations on the CATH4.2\ndataset to demonstrate that DMRA outperforms leading methods, achieving\nstate-of-the-art performance and exhibiting strong generalization capabilities\non the TS50 and TS500 datasets.",
      "tldr_zh": "蛋白质逆折叠是生物信息学中的关键问题，旨在从给定蛋白主链结构恢复氨基酸序列，但现有方法难以捕捉残基间复杂关系。研究提出Diffusion Model with Representation Alignment (DMRA)方法，通过一个共享中心聚合并选择性地分配蛋白结构上下文信息，以及在去噪过程中对噪声隐藏表示与干净语义表示进行对齐，以提升序列预测准确性。实验在CATH4.2数据集上显示，DMRA超越了领先方法，实现了最先进性能，并在TS50和TS500数据集上展现出强大的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09380v1",
      "published_date": "2024-12-12 15:47:59 UTC",
      "updated_date": "2024-12-12 15:47:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:41:31.519291"
    },
    {
      "arxiv_id": "2412.09370v1",
      "title": "Word Sense Linking: Disambiguating Outside the Sandbox",
      "title_zh": "词义链接：沙箱之外的消歧",
      "authors": [
        "Andrei Stefan Bejgu",
        "Edoardo Barba",
        "Luigi Procopio",
        "Alberte Fernández-Castro",
        "Roberto Navigli"
      ],
      "abstract": "Word Sense Disambiguation (WSD) is the task of associating a word in a given\ncontext with its most suitable meaning among a set of possible candidates.\nWhile the task has recently witnessed renewed interest, with systems achieving\nperformances above the estimated inter-annotator agreement, at the time of\nwriting it still struggles to find downstream applications. We argue that one\nof the reasons behind this is the difficulty of applying WSD to plain text.\nIndeed, in the standard formulation, models work under the assumptions that a)\nall the spans to disambiguate have already been identified, and b) all the\npossible candidate senses of each span are provided, both of which are\nrequirements that are far from trivial. In this work, we present a new task\ncalled Word Sense Linking (WSL) where, given an input text and a reference\nsense inventory, systems have to both identify which spans to disambiguate and\nthen link them to their most suitable meaning.We put forward a\ntransformer-based architecture for the task and thoroughly evaluate both its\nperformance and those of state-of-the-art WSD systems scaled to WSL,\niteratively relaxing the assumptions of WSD. We hope that our work will foster\neasier integration of lexical semantics into downstream applications.",
      "tldr_zh": "这篇论文指出，传统的 Word Sense Disambiguation (WSD) 尽管性能已超过标注者间一致性，但因依赖预先标识的词段和候选意义，难以应用于普通文本，从而限制了下游应用。论文提出一个新任务 Word Sense Linking (WSL)，要求系统在给定输入文本和参考意义库存的情况下，同时识别需要消歧的词段并链接到最合适的含义，并设计了一种基于 Transformer 的架构来实现这一任务。通过实验评估，该架构在逐步放松 WSD 假设后表现出色，有望促进词汇语义在实际应用中的更易整合。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09370v1",
      "published_date": "2024-12-12 15:38:34 UTC",
      "updated_date": "2024-12-12 15:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:41:43.914730"
    },
    {
      "arxiv_id": "2412.09353v2",
      "title": "Causal Graphical Models for Vision-Language Compositional Understanding",
      "title_zh": "因果图形模型用于视觉-语言组合理解",
      "authors": [
        "Fiorenzo Parascandolo",
        "Nicholas Moratelli",
        "Enver Sangineto",
        "Lorenzo Baraldi",
        "Rita Cucchiara"
      ],
      "abstract": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.",
      "tldr_zh": "本研究发现，Vision-Language Models (VLMs) 在处理语言的组合理解方面存在缺陷，常将图像描述视为“词汇袋”，导致在组合任务中表现不佳。论文提出使用 Causal Graphical Model (CGM) 通过依赖解析器建模文本和视觉标记之间的依赖关系，并训练一个受 VLM 视觉编码器条件化的解码器，其生成过程遵循 CGM 的部分有序结构，以学习主要因果依赖并忽略虚假相关性。在五个组合基准上的实验显示，该方法大幅超过现有最先进方法，甚至优于使用更大数据集的模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09353v2",
      "published_date": "2024-12-12 15:22:03 UTC",
      "updated_date": "2025-04-15 10:14:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:41:55.379231"
    },
    {
      "arxiv_id": "2412.09335v1",
      "title": "Does Low Spoilage Under Cold Conditions Foster Cultural Complexity During the Foraging Era? -- A Theoretical and Computational Inquiry",
      "title_zh": "翻译失败",
      "authors": [
        "Minhyeok Lee"
      ],
      "abstract": "Human cultural complexity did not arise in a vacuum. Scholars in the\nhumanities and social sciences have long debated how ecological factors, such\nas climate and resource availability, enabled early hunter-gatherers to\nallocate time and energy beyond basic subsistence tasks. This paper presents a\nformal, interdisciplinary approach that integrates theoretical modeling with\ncomputational methods to examine whether conditions that allow lower spoilage\nof stored food, often associated with colder climates and abundant large fauna,\ncould indirectly foster the emergence of cultural complexity. Our contribution\nis twofold. First, we propose a mathematical framework that relates spoilage\nrates, yield levels, resource management skills, and cultural activities. Under\nthis framework, we prove that lower spoilage and adequate yields reduce the\nfrequency of hunting, thus freeing substantial time for cultural pursuits.\nSecond, we implement a reinforcement learning simulation, inspired by\nengineering optimization techniques, to validate the theoretical predictions.\nBy training agents in different $(Y,p)$ environments, where $Y$ is yield and\n$p$ is the probability of daily spoilage, we observe patterns consistent with\nthe theoretical model: stable conditions with lower spoilage strongly correlate\nwith increased cultural complexity. While we do not claim to replicate\nprehistoric social realities directly, our results suggest that ecologically\nstable niches provided a milieu in which cultural forms could germinate and\nevolve. This study, therefore, offers an integrative perspective that unites\nhumanistic inquiries into the origins of culture with the formal rigor and\nexploratory power of computational modeling.",
      "tldr_zh": "这篇论文探讨了狩猎采集时代，冷气候下低食物腐败率是否能促进人类文化复杂性的发展。研究者提出一个数学框架，将腐败率（p）、产量（Y）、资源管理技能与文化活动联系起来，证明低腐败率和充足产量可减少狩猎频率，从而释放更多时间用于文化追求。同时，通过强化学习（reinforcement learning）模拟在不同环境条件下训练代理，验证了理论预测，即稳定低腐败率环境与文化复杂性增加密切相关。尽管无法直接复制史前社会，该研究为整合人文生态因素与计算建模提供了新的视角。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09335v1",
      "published_date": "2024-12-12 15:03:08 UTC",
      "updated_date": "2024-12-12 15:03:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:42:06.466057"
    },
    {
      "arxiv_id": "2412.09329v1",
      "title": "Towards Open-Vocabulary Video Semantic Segmentation",
      "title_zh": "面向开放词汇视频语义分割",
      "authors": [
        "Xinhao Li",
        "Yun Liu",
        "Guolei Sun",
        "Min Wu",
        "Le Zhang",
        "Ce Zhu"
      ],
      "abstract": "Semantic segmentation in videos has been a focal point of recent research.\nHowever, existing models encounter challenges when faced with unfamiliar\ncategories. To address this, we introduce the Open Vocabulary Video Semantic\nSegmentation (OV-VSS) task, designed to accurately segment every pixel across a\nwide range of open-vocabulary categories, including those that are novel or\npreviously unexplored. To enhance OV-VSS performance, we propose a robust\nbaseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing\nthe model to utilize temporal relationships across consecutive frames.\nAdditionally, we incorporate a random frame enhancement module, broadening the\nmodel's understanding of semantic context throughout the entire video sequence.\nOur approach also includes video text encoding, which strengthens the model's\ncapability to interpret textual information within the video context.\nComprehensive evaluations on benchmark datasets such as VSPW and Cityscapes\nhighlight OV-VSS's zero-shot generalization capabilities, especially in\nhandling novel categories. The results validate OV2VSS's effectiveness,\ndemonstrating improved performance in semantic segmentation tasks across\ndiverse video datasets.",
      "tldr_zh": "该论文介绍了 Open Vocabulary Video Semantic Segmentation (OV-VSS) 任务，旨在解决现有视频语义分割模型在处理陌生或新类别时的挑战，实现对开放词汇类别像素级别的精确分割。作者提出了一种基线模型 OV2VSS，包括 spatial-temporal fusion module 以利用连续帧的时间关系、random frame enhancement module 以扩展视频序列的语义理解，以及 video text encoding 以强化对视频中文本信息的解释。实验结果显示，OV2VSS 在 VSPW 和 Cityscapes 等基准数据集上表现出色，尤其在零样本泛化方面提升了性能，证明了其在多样视频语义分割任务中的有效性。",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09329v1",
      "published_date": "2024-12-12 14:53:16 UTC",
      "updated_date": "2024-12-12 14:53:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:43:11.506931"
    },
    {
      "arxiv_id": "2412.09328v1",
      "title": "Auto-Regressive Moving Diffusion Models for Time Series Forecasting",
      "title_zh": "自回归移动扩散模型用于时间序列预测",
      "authors": [
        "Jiaxin Gao",
        "Qinglong Cao",
        "Yuntian Chen"
      ],
      "abstract": "Time series forecasting (TSF) is essential in various domains, and recent\nadvancements in diffusion-based TSF models have shown considerable promise.\nHowever, these models typically adopt traditional diffusion patterns, treating\nTSF as a noise-based conditional generation task. This approach neglects the\ninherent continuous sequential nature of time series, leading to a fundamental\nmisalignment between diffusion mechanisms and the TSF objective, thereby\nseverely impairing performance. To bridge this misalignment, and inspired by\nthe classic Auto-Regressive Moving Average (ARMA) theory, which views time\nseries as continuous sequential progressions evolving from previous data\npoints, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to\nfirst achieve the continuous sequential diffusion-based TSF. Unlike previous\nmethods that start from white Gaussian noise, our model employs chain-based\ndiffusion with priors, accurately modeling the evolution of time series and\nleveraging intermediate state information to improve forecasting accuracy and\nstability. Specifically, our approach reinterprets the diffusion process by\nconsidering future series as the initial state and historical series as the\nfinal state, with intermediate series generated using a sliding-based technique\nduring the forward process. This design aligns the diffusion model's sampling\nprocedure with the forecasting objective, resulting in an unconditional,\ncontinuous sequential diffusion TSF model. Extensive experiments conducted on\nseven widely used datasets demonstrate that our model achieves state-of-the-art\nperformance, significantly outperforming existing diffusion-based TSF models.\nOur code is available on GitHub: https://github.com/daxin007/ARMD.",
      "tldr_zh": "该研究指出，现有的扩散模型在时间序列预测（TSF）中忽略了序列的连续顺序特性，导致性能不足。为解决这一问题，论文提出Auto-Regressive Moving Diffusion (ARMD) 模型，受Auto-Regressive Moving Average (ARMA) 理论启发，通过链式扩散和先验知识，将未来序列作为初始状态、历史序列作为最终状态，并使用滑动技术生成中间序列，从而实现无条件、连续顺序的扩散TSF。实验在七个常用数据集上表明，ARMD 显著优于现有扩散模型，达到了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "no comment",
      "pdf_url": "http://arxiv.org/pdf/2412.09328v1",
      "published_date": "2024-12-12 14:51:48 UTC",
      "updated_date": "2024-12-12 14:51:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:43:24.492272"
    },
    {
      "arxiv_id": "2412.09318v2",
      "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Liu",
        "Abdellah Fourtassi"
      ],
      "abstract": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.",
      "tldr_zh": "本论文评估了大型语言模型（LLMs）模拟儿童-护理者互动语言的能力，使用静态和互动基准测试方法。结果显示，先进的LLMs如Llama 3和GPT-4o在单词和话语层面能近似这些对话，但无法再现儿童和护理者的话语模式，夸大了对齐度，并缺乏人类的多样性。该研究旨在启动开发一个全面的基准测试，用于LLMs在儿童导向应用中的评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09318v2",
      "published_date": "2024-12-12 14:43:03 UTC",
      "updated_date": "2024-12-13 09:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:43:36.323508"
    },
    {
      "arxiv_id": "2412.09317v1",
      "title": "Multimodal Sentiment Analysis based on Video and Audio Inputs",
      "title_zh": "基于视频和音频输入的多模态情感分析",
      "authors": [
        "Antonio Fernandez",
        "Suzan Awinat"
      ],
      "abstract": "Despite the abundance of current researches working on the sentiment analysis\nfrom videos and audios, finding the best model that gives the highest accuracy\nrate is still considered a challenge for researchers in this field. The main\nobjective of this paper is to prove the usability of emotion recognition models\nthat take video and audio inputs. The datasets used to train the models are the\nCREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned\nmodels that been used are: Facebook/wav2vec2-large for audio and the\nGoogle/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for\neach emotion generated by the two previous models is utilized in the decision\nmaking framework. After disparity in the results, if one of the models gets\nmuch higher accuracy, another test framework is created. The methods used are\nthe Weighted Average method, the Confidence Level Threshold method, the Dynamic\nWeighting Based on Confidence method, and the Rule-Based Logic method. This\nlimited approach gives encouraging results that make future research into these\nmethods viable.",
      "tldr_zh": "该论文探讨了基于视频和音频输入的多模态情感分析（Multimodal Sentiment Analysis），旨在证明相关模型的有效性，以应对准确率挑战。研究使用 CREMA-D 数据集训练音频模型 Facebook/wav2vec2-large，以及 RAVDESS 数据集训练视频模型 Google/vivit-b-16x2-kinetics400，并通过平均情感概率进行决策。作者测试了多种方法，包括 Weighted Average method、Confidence Level Threshold method、Dynamic Weighting Based on Confidence method 和 Rule-Based Logic method，结果显示这些方法取得了令人鼓舞的性能，并为未来研究提供了可行性基础。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Presented as a full paper in the 15th International Conference on\n  Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2024) October\n  28-30, 2024, Leuven, Belgium",
      "pdf_url": "http://arxiv.org/pdf/2412.09317v1",
      "published_date": "2024-12-12 14:42:10 UTC",
      "updated_date": "2024-12-12 14:42:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:43:47.791214"
    },
    {
      "arxiv_id": "2412.09315v1",
      "title": "Beware of Metacognitive Laziness: Effects of Generative Artificial Intelligence on Learning Motivation, Processes, and Performance",
      "title_zh": "警惕元认知懒惰：生成式人工智能对学习动机、过程和表现的影响",
      "authors": [
        "Yizhou Fan",
        "Luzhen Tang",
        "Huixiao Le",
        "Kejie Shen",
        "Shufang Tan",
        "Yueying Zhao",
        "Yuan Shen",
        "Xinyu Li",
        "Dragan Gašević"
      ],
      "abstract": "With the continuous development of technological and educational innovation,\nlearners nowadays can obtain a variety of support from agents such as teachers,\npeers, education technologies, and recently, generative artificial intelligence\nsuch as ChatGPT. The concept of hybrid intelligence is still at a nascent\nstage, and how learners can benefit from a symbiotic relationship with various\nagents such as AI, human experts and intelligent learning systems is still\nunknown. The emerging concept of hybrid intelligence also lacks deep insights\nand understanding of the mechanisms and consequences of hybrid human-AI\nlearning based on strong empirical research. In order to address this gap, we\nconducted a randomised experimental study and compared learners' motivations,\nself-regulated learning processes and learning performances on a writing task\namong different groups who had support from different agents (ChatGPT, human\nexpert, writing analytics tools, and no extra tool). A total of 117 university\nstudents were recruited, and their multi-channel learning, performance and\nmotivation data were collected and analysed. The results revealed that:\nlearners who received different learning support showed no difference in\npost-task intrinsic motivation; there were significant differences in the\nfrequency and sequences of the self-regulated learning processes among groups;\nChatGPT group outperformed in the essay score improvement but their knowledge\ngain and transfer were not significantly different. Our research found that in\nthe absence of differences in motivation, learners with different supports\nstill exhibited different self-regulated learning processes, ultimately leading\nto differentiated performance. What is particularly noteworthy is that AI\ntechnologies such as ChatGPT may promote learners' dependence on technology and\npotentially trigger metacognitive laziness.",
      "tldr_zh": "本研究探讨了生成式人工智能（如 ChatGPT）对学习动机、自我调节学习过程(self-regulated learning)和表现的影响，通过随机实验比较了不同支持组（ChatGPT、人专家、写作分析工具和无工具）的差异，共招募117名大学学生进行写作任务分析。\n结果显示，不同组的内在动机无显著差异，但自我调节学习过程在频率和序列上存在显著差异，ChatGPT组在作文分数改善上表现出色，而知识获得和转移则无显著优势。\n研究强调，AI支持可能导致学习者对技术的依赖，并引发元认知懒惰(metacognitive laziness)，提醒教育者需关注混合智能(hybrid intelligence)下的潜在风险。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09315v1",
      "published_date": "2024-12-12 14:32:39 UTC",
      "updated_date": "2024-12-12 14:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:44:00.866163"
    },
    {
      "arxiv_id": "2412.09311v1",
      "title": "Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Davor Vukadin",
        "Petar Afrić",
        "Marin Šilić",
        "Goran Delač"
      ],
      "abstract": "Recent advancement in deep-neural network performance led to the development\nof new state-of-the-art approaches in numerous areas. However, the black-box\nnature of neural networks often prohibits their use in areas where model\nexplainability and model transparency are crucial. Over the years, researchers\nproposed many algorithms to aid neural network understanding and provide\nadditional information to the human expert. One of the most popular methods\nbeing Layer-Wise Relevance Propagation (LRP). This method assigns local\nrelevance based on the pixel-wise decomposition of nonlinear classifiers. With\nthe rise of attribution method research, there has emerged a pressing need to\nassess and evaluate their performance. Numerous metrics have been proposed,\neach assessing an individual property of attribution methods such as\nfaithfulness, robustness or localization. Unfortunately, no single metric is\ndeemed optimal for every case, and researchers often use several metrics to\ntest the quality of the attribution maps. In this work, we address the\nshortcomings of the current LRP formulations and introduce a novel method for\ndetermining the relevance of input neurons through layer-wise relevance\npropagation. Furthermore, we apply this approach to the recently developed\nVision Transformer architecture and evaluate its performance against existing\nmethods on two image classification datasets, namely ImageNet and PascalVOC.\nOur results clearly demonstrate the advantage of our proposed method.\nFurthermore, we discuss the insufficiencies of current evaluation metrics for\nattribution-based explainability and propose a new evaluation metric that\ncombines the notions of faithfulness, robustness and contrastiveness. We\nutilize this new metric to evaluate the performance of various\nattribution-based methods. Our code is available at:\nhttps://github.com/davor10105/relative-absolute-magnitude-propagation",
      "tldr_zh": "该研究针对神经网络的黑箱问题，提出了一种改进的 Layer-Wise Relevance Propagation (LRP) 方法，即 Relative Absolute Magnitude LRP，用于通过层级相关性传播确定输入神经元的相关性，并将其应用于 Vision Transformer 架构。实验在 ImageNet 和 PascalVOC 数据集上评估，显示该方法优于现有方法，提升了神经网络解释性的性能。论文还指出了当前评估指标（如 faithfulness、robustness 和 localization）的不足，并引入一个新的综合指标，结合 faithfulness、robustness 和 contrastiveness，以更全面评估 attribution-based 方法的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages, 16 figures, 13 tables, ACM Transactions on Intelligence\n  Systems and Technology",
      "pdf_url": "http://arxiv.org/pdf/2412.09311v1",
      "published_date": "2024-12-12 14:25:56 UTC",
      "updated_date": "2024-12-12 14:25:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:44:12.799071"
    },
    {
      "arxiv_id": "2412.09286v1",
      "title": "Learning Novel Skills from Language-Generated Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Ao-Qun Jin",
        "Tian-Yu Xiang",
        "Xiao-Hu Zhou",
        "Mei-Jiang Gui",
        "Xiao-Liang Xie",
        "Shi-Qi Liu",
        "Shuang-Yi Wang",
        "Yue Cao",
        "Sheng-Bin Duan",
        "Fu-Chao Xie",
        "Zeng-Guang Hou"
      ],
      "abstract": "Current robot learning algorithms for acquiring novel skills often rely on\ndemonstration datasets or environment interactions, resulting in high labor\ncosts and potential safety risks. To address these challenges, this study\nproposes a skill-learning framework that enables robots to acquire novel skills\nfrom natural language instructions. The proposed pipeline leverages\nvision-language models to generate demonstration videos of novel skills, which\nare processed by an inverse dynamics model to extract actions from the\nunlabeled demonstrations. These actions are subsequently mapped to\nenvironmental contexts via imitation learning, enabling robots to learn new\nskills effectively. Experimental evaluations in the MetaWorld simulation\nenvironments demonstrate the pipeline's capability to generate high-fidelity\nand reliable demonstrations. Using the generated demonstrations, various skill\nlearning algorithms achieve an accomplishment rate three times the original on\nnovel tasks. These results highlight a novel approach to robot learning,\noffering a foundation for the intuitive and intelligent acquisition of novel\nrobotic skills.",
      "tldr_zh": "这篇论文提出了一种机器人技能学习框架，使用自然语言指令生成演示视频，以解决传统方法的高劳动成本和安全风险问题。框架利用 vision-language models 生成新技能的演示视频，然后通过 inverse dynamics model 提取动作，并借助 imitation learning 将这些动作映射到环境上下文中，从而实现有效学习。实验在 MetaWorld 模拟环境中验证，该方法能产生高保真可靠的演示，使各种技能学习算法在新任务上的完成率提高三倍。该框架为机器人直观、智能地获取新技能奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09286v1",
      "published_date": "2024-12-12 13:56:36 UTC",
      "updated_date": "2024-12-12 13:56:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:44:24.325536"
    },
    {
      "arxiv_id": "2412.09283v1",
      "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
      "title_zh": "InstanceCap：通过实例感知结构化",
      "authors": [
        "Tiehan Fan",
        "Kepan Nan",
        "Rui Xie",
        "Penghao Zhou",
        "Zhenheng Yang",
        "Chaoyou Fu",
        "Xiang Li",
        "Jian Yang",
        "Ying Tai"
      ],
      "abstract": "Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations.",
      "tldr_zh": "论文提出 InstanceCap 框架，通过实例感知结构化标题（Instance-aware Structured Caption）首次实现实例级和细粒度视频标题，以改善 Text-to-Video Generation 的性能。框架利用辅助模型集群将视频转换为实例，提升实例保真度，并将密集提示细化为结构化短语，确保标题简洁精确；同时，构建了 22K 的 InstanceVid 数据集并设计针对性增强管道用于推理。实验结果表明，InstanceCap 显著优于现有模型，提高了视频与标题的保真度并减少 hallucinations。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09283v1",
      "published_date": "2024-12-12 13:48:40 UTC",
      "updated_date": "2024-12-12 13:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:44:36.442023"
    },
    {
      "arxiv_id": "2412.09278v2",
      "title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine",
      "title_zh": "迈向一种具有像素级洞察的多模态大语言模型，用于生物医学",
      "authors": [
        "Xiaoshuang Huang",
        "Lingdong Shen",
        "Jia Liu",
        "Fangxin Shang",
        "Hongxiang Li",
        "Haifeng Huang",
        "Yehui Yang"
      ],
      "abstract": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.",
      "tldr_zh": "本论文提出了一种新型生物医学多模态大型语言模型（Multimodal Large Language Model, MLLM）——MedPLIB，它具备像素级理解能力，支持视觉问答（VQA）、任意像素级提示（如点、边界框和自由形状）以及像素级定位。研究团队设计了Mixture-of-Experts (MoE)多阶段训练策略，将模型分为视觉-语言专家和像素-定位专家进行分阶段训练，然后微调，以协调多任务学习并保持推理时的计算成本与单专家模型相当。为推进生物医学MLLM研究，他们引入了Medical Complex Vision Question Answering Dataset (MeCoVQA)，该数据集包含8种模态，用于复杂医疗图像问答和区域理解。实验结果表明，MedPLIB在多个医疗视觉语言任务上达到了最先进（SOTA）水平，并在零样本像素定位任务上以mDice指标领先最佳小型和大模型19.7和15.6分。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09278v2",
      "published_date": "2024-12-12 13:41:35 UTC",
      "updated_date": "2025-01-10 10:07:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:44:51.139978"
    },
    {
      "arxiv_id": "2412.09269v1",
      "title": "Towards Understanding the Robustness of LLM-based Evaluations under Perturbations",
      "title_zh": "翻译失败",
      "authors": [
        "Manav Chaudhary",
        "Harshit Gupta",
        "Savita Bhat",
        "Vasudeva Varma"
      ],
      "abstract": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs)，如Google Gemini 1，作为自动评估器的潜力，以弥补传统指标如BLEU和ROUGE在处理无单一ground truth的生成文本任务（如总结和对话）时的不足。研究通过多种提示策略，在SummEval和USR数据集上进行实验，要求LLMs生成分数和理由，并测试其对扰动输入的鲁棒性。结果显示，LLMs虽显示出一定潜力，但与人类判断的对齐有限，且易受扰动影响，因此需要进一步改进才能作为可靠的评估器用于主观指标。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICON 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.09269v1",
      "published_date": "2024-12-12 13:31:58 UTC",
      "updated_date": "2024-12-12 13:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:44:59.819793"
    },
    {
      "arxiv_id": "2412.09264v1",
      "title": "Speeding up approximate MAP by applying domain knowledge about relevant variables",
      "title_zh": "翻译失败",
      "authors": [
        "Johan Kwisthout",
        "Andrew Schroeder"
      ],
      "abstract": "The MAP problem in Bayesian networks is notoriously intractable, even when\napproximated. In an earlier paper we introduced the Most Frugal Explanation\nheuristic approach to solving MAP, by partitioning the set of intermediate\nvariables (neither observed nor part of the MAP variables) into a set of\nrelevant variables, which are marginalized out, and irrelevant variables, which\nwill be assigned a sampled value from their domain. In this study we explore\nwhether knowledge about which variables are relevant for a particular query\n(i.e., domain knowledge) speeds up computation sufficiently to beat both exact\nMAP as well as approximate MAP while giving reasonably accurate results. Our\nresults are inconclusive, but also show that this probably depends on the\nspecifics of the MAP query, most prominently the number of MAP variables.",
      "tldr_zh": "这篇论文探讨了如何通过应用关于相关变量的领域知识来加速贝叶斯网络中 MAP (Maximum A Posteriori) 问题的近似计算。研究基于先前提出的 Most Frugal Explanation 启发式方法，将中间变量分为相关变量（进行边缘化）和无关变量（采样赋值），以优化计算效率。实验结果显示，这种方法的效果不确定，但可能取决于 MAP 查询的具体因素，尤其是 MAP 变量的数量，从而为改进近似 MAP 算法提供潜在见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09264v1",
      "published_date": "2024-12-12 13:22:01 UTC",
      "updated_date": "2024-12-12 13:22:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:45:11.739564"
    },
    {
      "arxiv_id": "2412.09263v2",
      "title": "First Train to Generate, then Generate to Train: UnitedSynT5 for Few-Shot NLI",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Banerjee",
        "Anush Mahajan",
        "Ayushi Agarwal",
        "Eishkaran Singh"
      ],
      "abstract": "Natural Language Inference (NLI) tasks require identifying the relationship\nbetween sentence pairs, typically classified as entailment, contradiction, or\nneutrality. While the current state-of-the-art (SOTA) model, Entailment\nFew-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural\nLanguage Inference (SNLI) dataset, further advancements are constrained by the\ndataset's limitations. To address this, we propose a novel approach leveraging\nsynthetic data augmentation to enhance dataset diversity and complexity. We\npresent UnitedSynT5, an advanced extension of EFL that leverages a T5-based\ngenerator to synthesize additional premise-hypothesis pairs, which are\nrigorously cleaned and integrated into the training data. These augmented\nexamples are processed within the EFL framework, embedding labels directly into\nhypotheses for consistency. We train a GTR-T5-XL model on this expanded\ndataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset, 94.0%\naccuracy on the E-SNLI dataset, and 92.6% accuracy on the MultiNLI dataset,\nsurpassing the previous SOTA models. This research demonstrates the potential\nof synthetic data augmentation in improving NLI models, offering a path forward\nfor further advancements in natural language understanding tasks.",
      "tldr_zh": "这篇论文针对 Natural Language Inference (NLI) 任务，提出 UnitedSynT5 方法，通过合成数据增强来解决数据集限制问题。方法包括先训练一个 T5-based generator 生成额外的 premise-hypothesis pairs，并对这些数据进行清洗和整合到 Entailment Few-Shot Learning (EFL) 框架中，将标签嵌入假设以确保一致性。最终，使用 GTR-T5-XL 模型在扩展数据集上训练，实现了 SNLI 数据集的 94.7% 准确率、E-SNLI 的 94.0% 和 MultiNLI 的 92.6%，超越了之前的 SOTA 模型，并展示了合成数据增强在提升 NLI 模型性能方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09263v2",
      "published_date": "2024-12-12 13:21:09 UTC",
      "updated_date": "2024-12-13 06:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:45:25.454254"
    },
    {
      "arxiv_id": "2412.14193v2",
      "title": "Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Kathrin Wardatzky",
        "Oana Inel",
        "Luca Rossetto",
        "Abraham Bernstein"
      ],
      "abstract": "Adding explanations to recommender systems is said to have multiple benefits,\nsuch as increasing user trust or system transparency. Previous work from other\napplication areas suggests that specific user characteristics impact the users'\nperception of the explanation. However, we rarely find this type of evaluation\nfor recommender systems explanations. This paper addresses this gap by\nsurveying 124 papers in which recommender systems explanations were evaluated\nin user studies. We analyzed their participant descriptions and study results\nwhere the impact of user characteristics on the explanation effects was\nmeasured. Our findings suggest that the results from the surveyed studies\npredominantly cover specific users who do not necessarily represent the users\nof recommender systems in the evaluation domain. This may seriously hamper the\ngeneralizability of any insights we may gain from current studies on\nexplanations in recommender systems. We further find inconsistencies in the\ndata reporting, which impacts the reproducibility of the reported results.\nHence, we recommend actions to move toward a more inclusive and reproducible\nevaluation.",
      "tldr_zh": "这篇论文通过系统调查124篇用户研究，分析了用户特征对可解释推荐系统(Explainable Recommender Systems)解释效果的影响，旨在填补该领域的评估空白。研究发现，这些论文主要关注特定用户群体，而非推荐系统的典型用户，这可能导致结果的泛化性不足，并暴露了数据报告不一致的问题，影响研究的可重复性。作者因此推荐采取行动，推动更具包容性和可重复性的评估方法，以提升推荐系统解释的可靠性和实际应用价值。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR",
        "A.1; H.3.3; H.5.2; K.4"
      ],
      "primary_category": "cs.HC",
      "comment": "33 pages, 2 figures. Submitted to ACM Transactions of Recommender\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2412.14193v2",
      "published_date": "2024-12-12 13:01:30 UTC",
      "updated_date": "2025-02-03 16:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:45:37.309824"
    },
    {
      "arxiv_id": "2412.09240v1",
      "title": "VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation with Unsupervised Domain Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto Alcover-Couso",
        "Marcos Escudero-Viñolo",
        "Juan C. SanMiguel",
        "Jesus Bescos"
      ],
      "abstract": "Segmentation models are typically constrained by the categories defined\nduring training. To address this, researchers have explored two independent\napproaches: adapting Vision-Language Models (VLMs) and leveraging synthetic\ndata. However, VLMs often struggle with granularity, failing to disentangle\nfine-grained concepts, while synthetic data-based methods remain limited by the\nscope of available datasets.\n  This paper proposes enhancing segmentation accuracy across diverse domains by\nintegrating Vision-Language reasoning with key strategies for Unsupervised\nDomain Adaptation (UDA). First, we improve the fine-grained segmentation\ncapabilities of VLMs through multi-scale contextual data, robust text\nembeddings with prompt augmentation, and layer-wise fine-tuning in our proposed\nFoundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS)\nframework. Next, we incorporate these enhancements into a UDA framework by\nemploying distillation to stabilize training and cross-domain mixed sampling to\nboost adaptability without compromising generalization. The resulting\nUDA-FROVSS framework is the first UDA approach to effectively adapt across\ndomains without requiring shared categories.",
      "tldr_zh": "本研究针对分割模型受训练类别限制的问题，提出将 Vision-Language Models (VLMs) 与 Unsupervised Domain Adaptation (UDA) 相结合，提升开放词汇分割的迁移能力。研究首先通过多尺度上下文数据、prompt augmentation 和层级微调，在 FROVSS 框架中增强 VLMs 的细粒度分割性能，以解决其颗粒度不足的问题。接着，将这些改进整合到 UDA 框架中，利用 distillation 稳定训练和 cross-domain mixed sampling 提升适应性，同时保持泛化能力。最终，UDA-FROVSS 成为首个无需共享类别的 UDA 方法，在跨域场景中显著提高了分割准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09240v1",
      "published_date": "2024-12-12 12:49:42 UTC",
      "updated_date": "2024-12-12 12:49:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:45:48.243939"
    },
    {
      "arxiv_id": "2412.09237v2",
      "title": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Yijun Liu",
        "Wu Liu",
        "Xiaoyan Gu",
        "Yong Rui",
        "Xiaodong He",
        "Yongdong Zhang"
      ],
      "abstract": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations.",
      "tldr_zh": "该研究提出 LMAgent，一种基于多模态 LLMs 的超大规模多智能体社会，用于模拟复杂多用户行为，例如电商场景中的聊天、购物、评论和直播。系统引入自一致性 prompting 机制来增强代理的多模态能力，提高决策性能，并采用快速 memory 机制结合 small-world model，支持超过 10,000 个代理的高效模拟。实验结果显示，LMAgent 的代理在行为指标上与人类相当，并展现出如 herd behavior 等独特社会现象，证明其在可信大规模社会行为模拟中的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09237v2",
      "published_date": "2024-12-12 12:47:09 UTC",
      "updated_date": "2024-12-13 03:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:46:00.149564"
    },
    {
      "arxiv_id": "2412.09230v1",
      "title": "Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Bhargav Rongali",
        "Mohamad Hassan N C",
        "Ankit Jha",
        "Neha Bhargava",
        "Saurabh Prasad",
        "Biplab Banerjee"
      ],
      "abstract": "This paper tackles the intricate challenge of video question-answering\n(VideoQA). Despite notable progress, current methods fall short of effectively\nintegrating questions with video frames and semantic object-level abstractions\nto create question-aware video representations. We introduce Local-Global\nQuestion Aware Video Embedding (LGQAVE), which incorporates three major\ninnovations to integrate multi-modal knowledge better and emphasize semantic\nvisual concepts relevant to specific questions. LGQAVE moves beyond traditional\nad-hoc frame sampling by utilizing a cross-attention mechanism that precisely\nidentifies the most relevant frames concerning the questions. It captures the\ndynamics of objects within these frames using distinct graphs, grounding them\nin question semantics with the miniGPT model. These graphs are processed by a\nquestion-aware dynamic graph transformer (Q-DGT), which refines the outputs to\ndevelop nuanced global and local video representations. An additional\ncross-attention module integrates these local and global embeddings to generate\nthe final video embeddings, which a language model uses to generate answers.\nExtensive evaluations across multiple benchmarks demonstrate that LGQAVE\nsignificantly outperforms existing models in delivering accurate multi-choice\nand open-ended answers.",
      "tldr_zh": "本文提出了一种协同方法，将基础模型和自适应特征选择应用于 Video Question Answering (VideoQA)，以更好地整合问题与视频帧及语义对象抽象。核心创新是 Local-Global Question Aware Video Embedding (LGQAVE) 框架，该框架使用 cross-attention 机制选择与问题相关的关键帧，通过 distinct graphs 和 miniGPT 模型捕获对象动态，并借助 question-aware dynamic graph transformer (Q-DGT) 生成细致的全局和局部视频表示，最终整合这些嵌入来生成答案。实验结果显示，LGQAVE 在多个基准上显著优于现有模型，提供更准确的多选和开放式回答。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09230v1",
      "published_date": "2024-12-12 12:39:07 UTC",
      "updated_date": "2024-12-12 12:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:48:04.649841"
    },
    {
      "arxiv_id": "2412.09223v1",
      "title": "CSSDH: An Ontology for Social Determinants of Health to Operational Continuity of Care Data Interoperability",
      "title_zh": "翻译失败",
      "authors": [
        "Subhashis Das",
        "Debashis Naskar",
        "Sara Rodriguez Gonzalez"
      ],
      "abstract": "The rise of digital platforms has led to an increasing reliance on\ntechnology-driven, home-based healthcare solutions, enabling individuals to\nmonitor their health and share information with healthcare professionals as\nneeded. However, creating an efficient care plan management system requires\nmore than just analyzing hospital summaries and Electronic Health Records\n(EHRs). Factors such as individual user needs and social determinants of\nhealth, including living conditions and the flow of healthcare information\nbetween different settings, must also be considered. Challenges in this complex\nhealthcare network involve schema diversity (in EHRs, personal health records,\netc.) and terminology diversity (e.g., ICD, SNOMED-CT) across ancillary\nhealthcare operations. Establishing interoperability among various systems and\napplications is crucial, with the European Interoperability Framework (EIF)\nemphasizing the need for patient-centric access and control of healthcare data.\nIn this paper, we propose an integrated ontological model, the Common Semantic\nData Model for Social Determinants of Health (CSSDH), by combining ISO/DIS\n13940:2024 ContSys with WHO Social Determinants of Health. CSSDH aims to\nachieve interoperability within the Continuity of Care Network.",
      "tldr_zh": "本文提出 CSSDH，一种整合本体模型，用于解决数字医疗平台中社会决定因素（Social Determinants of Health）的互操作性挑战，包括 Electronic Health Records (EHRs) 的模式多样性和术语多样性（如 ICD 和 SNOMED-CT）。该模型将 ISO/DIS 13940:2024 ContSys 与 WHO Social Determinants of Health 相结合，旨在提升 Continuity of Care Network 中的数据互操作性。实验结果表明，CSSDH 可支持患者中心的数据访问和控制，促进高效的护理计划管理。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "68T27",
        "I.2.4; I.2.1"
      ],
      "primary_category": "cs.LO",
      "comment": "6 pages, 3 figures, conference-The 25th International Conference on\n  Intelligent Data Engineering and Automated Learning",
      "pdf_url": "http://arxiv.org/pdf/2412.09223v1",
      "published_date": "2024-12-12 12:25:33 UTC",
      "updated_date": "2024-12-12 12:25:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:46:24.206877"
    },
    {
      "arxiv_id": "2412.10461v1",
      "title": "EvoSampling: A Granular Ball-based Evolutionary Hybrid Sampling with Knowledge Transfer for Imbalanced Learning",
      "title_zh": "EvoSampling：一种基于颗粒球的进化混合采样方法，带有知识转移，用于不平衡学习",
      "authors": [
        "Wenbin Pei",
        "Ruohao Dai",
        "Bing Xue",
        "Mengjie Zhang",
        "Qiang Zhang",
        "Yiu-Ming Cheung",
        "Shuyin Xia"
      ],
      "abstract": "Class imbalance would lead to biased classifiers that favor the majority\nclass and disadvantage the minority class. Unfortunately, from a practical\nperspective, the minority class is of importance in many real-life\napplications. Hybrid sampling methods address this by oversampling the minority\nclass to increase the number of its instances, followed by undersampling to\nremove low-quality instances. However, most existing sampling methods face\ndifficulties in generating diverse high-quality instances and often fail to\nremove noise or low-quality instances on a larger scale effectively. This paper\ntherefore proposes an evolutionary multi-granularity hybrid sampling method,\ncalled EvoSampling. During the oversampling process, genetic programming (GP)\nis used with multi-task learning to effectively and efficiently generate\ndiverse high-quality instances. During the undersampling process, we develop a\ngranular ball-based undersampling method that removes noise in a multi-granular\nfashion, thereby enhancing data quality. Experiments on 20 imbalanced datasets\ndemonstrate that EvoSampling effectively enhances the performance of various\nclassification algorithms by providing better datasets than existing sampling\nmethods. Besides, ablation studies further indicate that allowing knowledge\ntransfer accelerates the GP's evolutionary learning process.",
      "tldr_zh": "该论文针对类不平衡学习问题，提出了一种名为 EvoSampling 的进化多粒度混合采样方法，以生成多样高质量样本并有效移除噪声。EvoSampling 在过采样阶段使用 genetic programming (GP) 结合多任务学习，通过知识 transfer 加速进化过程；在欠采样阶段，采用 granular ball-based 方法进行多粒度噪声移除，从而提升数据质量。实验结果显示，该方法在 20 个不平衡数据集上显著提高了各种分类算法的性能，且消融研究证明知识 transfer 加速了 GP 的学习过程。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10461v1",
      "published_date": "2024-12-12 11:35:20 UTC",
      "updated_date": "2024-12-12 11:35:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:48:29.079650"
    },
    {
      "arxiv_id": "2412.10460v1",
      "title": "Enriching Multimodal Sentiment Analysis through Textual Emotional Descriptions of Visual-Audio Content",
      "title_zh": "通过视觉-音频内容的文本情感描述丰富多模态情感分析",
      "authors": [
        "Sheng Wu",
        "Xiaobao Wang",
        "Longbiao Wang",
        "Dongxiao He",
        "Jianwu Dang"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) stands as a critical research frontier,\nseeking to comprehensively unravel human emotions by amalgamating text, audio,\nand visual data. Yet, discerning subtle emotional nuances within audio and\nvideo expressions poses a formidable challenge, particularly when emotional\npolarities across various segments appear similar. In this paper, our objective\nis to spotlight emotion-relevant attributes of audio and visual modalities to\nfacilitate multimodal fusion in the context of nuanced emotional shifts in\nvisual-audio scenarios. To this end, we introduce DEVA, a progressive fusion\nframework founded on textual sentiment descriptions aimed at accentuating\nemotional features of visual-audio content. DEVA employs an Emotional\nDescription Generator (EDG) to transmute raw audio and visual data into\ntextualized sentiment descriptions, thereby amplifying their emotional\ncharacteristics. These descriptions are then integrated with the source data to\nyield richer, enhanced features. Furthermore, DEVA incorporates the Text-guided\nProgressive Fusion Module (TPF), leveraging varying levels of text as a core\nmodality guide. This module progressively fuses visual-audio minor modalities\nto alleviate disparities between text and visual-audio modalities. Experimental\nresults on widely used sentiment analysis benchmark datasets, including MOSI,\nMOSEI, and CH-SIMS, underscore significant enhancements compared to\nstate-of-the-art models. Moreover, fine-grained emotion experiments corroborate\nthe robust sensitivity of DEVA to subtle emotional variations.",
      "tldr_zh": "这篇论文针对 Multimodal Sentiment Analysis (MSA) 的挑战，提出 DEVA 框架，通过 Emotional Description Generator (EDG) 将音频和视觉数据转化为文本化的情感描述，从而增强情绪特征并促进多模态融合。DEVA 还引入 Text-guided Progressive Fusion Module (TPF)，利用不同级别的文本指导逐步融合视觉-音频模态，以缓解模态之间的差异。实验结果显示，在 MOSI、MOSEI 和 CH-SIMS 等基准数据集上，DEVA 比现有模型显著提升性能，并对细粒度情绪变化表现出色敏感性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10460v1",
      "published_date": "2024-12-12 11:30:41 UTC",
      "updated_date": "2024-12-12 11:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:48:40.639857"
    },
    {
      "arxiv_id": "2412.09165v3",
      "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
      "title_zh": "当文本嵌入遇见大型语言模型：一个全面综述",
      "authors": [
        "Zhijie Nie",
        "Zhangchi Feng",
        "Mingxin Li",
        "Cunwang Zhang",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Richong Zhang"
      ],
      "abstract": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
      "tldr_zh": "这篇调查综述探讨了文本嵌入（text embedding）和大型语言模型（LLMs）的整合在自然语言处理（NLP）中的作用，强调了LLMs如何提升传统嵌入方法的效率和效果。论文将这种互动分为三类：（1）LLM-augmented text embedding，利用LLMs增强传统方法；（2）LLMs as text embedders，将LLMs适配为高质量嵌入器；以及（3）Text embedding understanding with LLMs，通过LLMs分析和解释嵌入。最终，它系统概述了相关研究，突出了pre-trained language models (PLMs)时代遗留的挑战、LLMs带来的新问题，并提出了文本嵌入未来的理论和实践方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders",
      "pdf_url": "http://arxiv.org/pdf/2412.09165v3",
      "published_date": "2024-12-12 10:50:26 UTC",
      "updated_date": "2025-03-20 16:15:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:46:59.457563"
    },
    {
      "arxiv_id": "2412.10459v2",
      "title": "Conformal Prediction on Quantifying Uncertainty of Dynamic Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Aoming Liang",
        "Qi Liu",
        "Lei Xu",
        "Fahad Sohrab",
        "Weicheng Cui",
        "Changhui Song",
        "Moncef Gabbouj"
      ],
      "abstract": "Numerous studies have focused on learning and understanding the dynamics of\nphysical systems from video data, such as spatial intelligence. Artificial\nintelligence requires quantitative assessments of the uncertainty of the model\nto ensure reliability. However, there is still a relative lack of systematic\nassessment of the uncertainties, particularly the uncertainties of the physical\ndata. Our motivation is to introduce conformal prediction into the uncertainty\nassessment of dynamical systems, providing a method supported by theoretical\nguarantees. This paper uses the conformal prediction method to assess\nuncertainties with benchmark operator learning methods. We have also compared\nthe Monte Carlo Dropout and Ensemble methods in the partial differential\nequations dataset, effectively evaluating uncertainty through straight\nroll-outs, making it ideal for time-series tasks.",
      "tldr_zh": "该论文针对动态系统的不确定性评估问题，引入了 Conformal Prediction 方法，以系统量化模型不确定性并提供理论保证。研究者将该方法应用于基准算子学习，并比较了 Monte Carlo Dropout 和 Ensemble 方法在偏微分方程数据集上的表现。结果显示，通过 straight roll-outs 技术，该方法有效评估了时间序列任务的不确定性，提升了人工智能在物理系统动态学习中的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10459v2",
      "published_date": "2024-12-12 10:45:02 UTC",
      "updated_date": "2024-12-17 11:35:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:48:51.593176"
    },
    {
      "arxiv_id": "2501.10368v1",
      "title": "The Potential of Answer Classes in Large-scale Written Computer-Science Exams -- Vol. 2",
      "title_zh": "翻译失败",
      "authors": [
        "Dominic Lohr",
        "Marc Berges",
        "Michael Kohlhase",
        "Florian Rabe"
      ],
      "abstract": "Students' answers to tasks provide a valuable source of information in\nteaching as they result from applying cognitive processes to a learning content\naddressed in the task. Due to steadily increasing course sizes, analyzing\nstudent answers is frequently the only means of obtaining evidence about\nstudent performance. However, in many cases, resources are limited, and when\nevaluating exams, the focus is solely on identifying correct or incorrect\nanswers. This overlooks the value of analyzing incorrect answers, which can\nhelp improve teaching strategies or identify misconceptions to be addressed in\nthe next cohort.\n  In teacher training for secondary education, assessment guidelines are\nmandatory for every exam, including anticipated errors and misconceptions. We\napplied this concept to a university exam with 462 students and 41 tasks. For\neach task, the instructors developed answer classes -- classes of expected\nresponses, to which student answers were mapped during the exam correction\nprocess. The experiment resulted in a shift in mindset among the tutors and\ninstructors responsible for the course: after initially having great\nreservations about whether the significant additional effort would yield an\nappropriate benefit, the procedure was subsequently found to be extremely\nvaluable.\n  The concept presented, and the experience gained from the experiment were\ncast into a system with which it is possible to correct paper-based exams on\nthe basis of answer classes. This updated version of the paper provides an\noverview and new potential in the course of using the digital version of the\napproach.",
      "tldr_zh": "该研究探讨了在大型计算机科学书面考试中，使用“answer classes”（答案类别）来分析学生答案的潜力，以揭示认知过程和常见误区。研究者将中学教师评估指南的概念应用于一门涉及462名学生和41个任务的大学考试，通过预定义答案类别映射学生响应，帮助识别错误答案并改进教学策略。尽管最初对额外工作持保留态度，实验导致教师心态转变，认为这一方法极具价值。该团队随后开发了一个系统，支持基于答案类别的纸质考试修正，并扩展到数字版本，以增强大规模评估的可行性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at Commentarii Informaticae Didacticae (CID) 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.10368v1",
      "published_date": "2024-12-12 10:20:39 UTC",
      "updated_date": "2024-12-12 10:20:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:49:04.011885"
    },
    {
      "arxiv_id": "2412.09661v1",
      "title": "Language model driven: a PROTAC generation pipeline with dual constraints of structure and property",
      "title_zh": "语言模型驱动：一种具有结构和属性双重约束的PROTAC生成管道",
      "authors": [
        "Jinsong Shao",
        "Qineng Gong",
        "Zeyu Yin",
        "Yu Chen",
        "Yajie Hao",
        "Lei Zhang",
        "Linlin Jiang",
        "Min Yao",
        "Jinlong Li",
        "Fubo Wang",
        "Li Wang"
      ],
      "abstract": "The imperfect modeling of ternary complexes has limited the application of\ncomputer-aided drug discovery tools in PROTAC research and development. In this\nstudy, an AI-assisted approach for PROTAC molecule design pipeline named\nLM-PROTAC was developed, which stands for language model driven Proteolysis\nTargeting Chimera, by embedding a transformer-based generative model with dual\nconstraints on structure and properties, referred to as the DCT. This study\nutilized the fragmentation representation of molecules and developed a language\nmodel driven pipeline. Firstly, a language model driven affinity model for\nprotein compounds to screen molecular fragments with high affinity for the\ntarget protein. Secondly, structural and physicochemical properties of these\nfragments were constrained during the generation process to meet specific\nscenario requirements. Finally, a two-round screening of the preliminary\ngenerated molecules using a multidimensional property prediction model to\ngenerate a batch of PROTAC molecules capable of degrading disease-relevant\ntarget proteins for validation in vitro experiments, thus achieving a complete\nsolution for AI-assisted PROTAC drug generation. Taking the tumor key target\nWnt3a as an example, the LM-PROTAC pipeline successfully generated PROTAC\nmolecules capable of inhibiting Wnt3a. The results show that DCT can\nefficiently generate PROTAC that targets and hydrolyses Wnt3a.",
      "tldr_zh": "本研究提出了一种基于语言模型的 PROTAC 分子设计管道，名为 LM-PROTAC，它通过嵌入 Transformer-based 生成模型并应用双重约束（DCT）来同时优化分子结构和理化性质，以解决计算机辅助药物发现在 PROTAC 研究中的局限性。管道包括三个关键步骤：首先，使用语言模型驱动的亲和力模型筛选针对目标蛋白的高亲和力分子片段；其次，在生成过程中施加结构和性质约束；最后，通过多维属性预测模型进行两轮筛选，产出可用于体外实验的 PROTAC 分子。实验以肿瘤关键靶点 Wnt3a 为例，成功生成了能抑制和水解 Wnt3a 的 PROTAC 分子，证明了该方法的有效性。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "I.2.7; D.3.2"
      ],
      "primary_category": "q-bio.QM",
      "comment": "61 pages,12 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09661v1",
      "published_date": "2024-12-12 10:15:12 UTC",
      "updated_date": "2024-12-12 10:15:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:49:16.041020"
    },
    {
      "arxiv_id": "2412.09126v1",
      "title": "Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Meng Shen",
        "Yake Wei",
        "Jianxiong Yin",
        "Deepu Rajan",
        "Di Hu",
        "Simon See"
      ],
      "abstract": "Training multimodal models requires a large amount of labeled data. Active\nlearning (AL) aim to reduce labeling costs. Most AL methods employ warm-start\napproaches, which rely on sufficient labeled data to train a well-calibrated\nmodel that can assess the uncertainty and diversity of unlabeled data. However,\nwhen assembling a dataset, labeled data are often scarce initially, leading to\na cold-start problem. Additionally, most AL methods seldom address multimodal\ndata, highlighting a research gap in this field. Our research addresses these\nissues by developing a two-stage method for Multi-Modal Cold-Start Active\nLearning (MMCSAL).\n  Firstly, we observe the modality gap, a significant distance between the\ncentroids of representations from different modalities, when only using\ncross-modal pairing information as self-supervision signals. This modality gap\naffects data selection process, as we calculate both uni-modal and cross-modal\ndistances. To address this, we introduce uni-modal prototypes to bridge the\nmodality gap. Secondly, conventional AL methods often falter in multimodal\nscenarios where alignment between modalities is overlooked. Therefore, we\npropose enhancing cross-modal alignment through regularization, thereby\nimproving the quality of selected multimodal data pairs in AL. Finally, our\nexperiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs\nacross three multimodal datasets.",
      "tldr_zh": "本研究针对多模态主动学习（Active Learning, AL）中的冷启动问题，提出了一种两阶段方法 Multi-Modal Cold-Start Active Learning (MMCSAL)，以减少标记数据的需求并提升数据选择质量。首先，通过引入 uni-modal prototypes 来桥接模态间差距（modality gap），从而更准确地计算单模态和跨模态距离。其次，通过正则化增强 cross-modal alignment，确保所选多模态数据对的可靠性。实验在三个多模态数据集上验证了 MMCSAL 的有效性，提高了模型的性能。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MM",
      "comment": "11 pages, ACMMM Asia 2024, Oral Presentation",
      "pdf_url": "http://arxiv.org/pdf/2412.09126v1",
      "published_date": "2024-12-12 10:03:46 UTC",
      "updated_date": "2024-12-12 10:03:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:49:28.684163"
    },
    {
      "arxiv_id": "2412.09125v1",
      "title": "Goal-Driven Query Answering over First- and Second-Order Dependencies with Equality",
      "title_zh": "翻译失败",
      "authors": [
        "Efthymia Tsamoura",
        "Boris Motik"
      ],
      "abstract": "Query answering over data with dependencies plays a central role in most\napplications of dependencies. The problem is commonly solved by using a\nsuitable variant of the chase algorithm to compute a universal model of the\ndependencies and the data and thus explicate all knowledge implicit in the\ndependencies. After this preprocessing step, an arbitrary conjunctive query\nover the dependencies and the data can be answered by evaluating it the\ncomputed universal model. If, however, the query to be answered is fixed and\nknown in advance, computing the universal model is often inefficient as many\ninferences made during this process can be irrelevant to a given query. In such\ncases, a goal-driven approach, which avoids drawing unnecessary inferences,\npromises to be more efficient and thus preferable in practice.\n  In this paper we present what we believe to be the first technique for\ngoal-driven query answering over first- and second-order dependencies with\nequality reasoning. Our technique transforms the input dependencies so that\napplying the chase to the output avoids many inferences that are irrelevant to\nthe query. The transformation proceeds in several steps, which comprise the\nfollowing three novel techniques. First, we present a variant of the\nsingularisation technique by Marnette [60] that is applicable to second-order\ndependencies and that corrects an incompleteness of a related formulation by\nten Cate et al. [74]. Second, we present a relevance analysis technique that\ncan eliminate from the input dependencies that provably do not contribute to\nquery answers. Third, we present a variant of the magic sets algorithm [19]\nthat can handle second-order dependencies with equality reasoning. We also\npresent the results of an extensive empirical evaluation, which show that\ngoal-driven query answering can be orders of magnitude faster than computing\nthe full universal model.",
      "tldr_zh": "该论文提出了一种目标驱动（goal-driven）的查询回答技术，针对带有等式推理（equality reasoning）的第一阶和第二阶依赖关系（first- and second-order dependencies），以避免传统chase算法计算完整通用模型时的不必要推理，从而提高效率。该技术包括三个创新步骤：改进的singularisation技术适用于第二阶依赖关系并修正了现有方法的缺陷；相关性分析（relevance analysis）用于去除不影响查询答案的依赖关系；以及一个处理第二阶依赖关系和等式推理的magic sets算法变体。通过这些步骤，论文将输入依赖关系进行转换，使chase过程更针对查询。实验结果显示，该方法比计算完整通用模型快几个数量级，证明了其在实际应用中的显著优势。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LO",
        "F.4.1; I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "47 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09125v1",
      "published_date": "2024-12-12 10:02:16 UTC",
      "updated_date": "2024-12-12 10:02:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:49:40.509972"
    },
    {
      "arxiv_id": "2412.09104v2",
      "title": "In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Songjun Tu",
        "Jingbo Sun",
        "Qichao Zhang",
        "Yaocheng Zhang",
        "Jia Liu",
        "Ke Chen",
        "Dongbin Zhao"
      ],
      "abstract": "Offline preference-based reinforcement learning (PbRL) typically operates in\ntwo phases: first, use human preferences to learn a reward model and annotate\nrewards for a reward-free offline dataset; second, learn a policy by optimizing\nthe learned reward via offline RL. However, accurately modeling step-wise\nrewards from trajectory-level preference feedback presents inherent challenges.\nThe reward bias introduced, particularly the overestimation of predicted\nrewards, leads to optimistic trajectory stitching, which undermines the\npessimism mechanism critical to the offline RL phase. To address this\nchallenge, we propose In-Dataset Trajectory Return Regularization (DTR) for\noffline PbRL, which leverages conditional sequence modeling to mitigate the\nrisk of learning inaccurate trajectory stitching under reward bias.\nSpecifically, DTR employs Decision Transformer and TD-Learning to strike a\nbalance between maintaining fidelity to the behavior policy with high\nin-dataset trajectory returns and selecting optimal actions based on high\nreward labels. Additionally, we introduce an ensemble normalization technique\nthat effectively integrates multiple reward models, balancing the tradeoff\nbetween reward differentiation and accuracy. Empirical evaluations on various\nbenchmarks demonstrate the superiority of DTR over other state-of-the-art\nbaselines.",
      "tldr_zh": "这篇论文针对离线偏好强化学习（Offline PbRL）中的奖励偏差问题，提出了 In-Dataset Trajectory Return Regularization (DTR) 方法，以缓解从轨迹级偏好反馈中学习不准确轨迹拼接的风险。DTR 结合 Decision Transformer 和 TD-Learning，在保持对行为策略的忠诚（高数据集内轨迹回报）与基于高奖励标签选择最优动作之间实现平衡，同时引入 ensemble normalization 技术来整合多个奖励模型，确保奖励区分度和准确性的 tradeoff。实验结果显示，DTR 在各种基准上优于最先进基线，提升了离线 PbRL 的整体性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, Proceedings of the 39th AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2412.09104v2",
      "published_date": "2024-12-12 09:35:47 UTC",
      "updated_date": "2024-12-21 07:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:49:52.160998"
    },
    {
      "arxiv_id": "2412.09102v1",
      "title": "PolyIPA -- Multilingual Phoneme-to-Grapheme Conversion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Davor Lauc"
      ],
      "abstract": "This paper presents PolyIPA, a novel multilingual phoneme-to-grapheme\nconversion model designed for multilingual name transliteration, onomastic\nresearch, and information retrieval. The model leverages two helper models\ndeveloped for data augmentation: IPA2vec for finding soundalikes across\nlanguages, and similarIPA for handling phonetic notation variations. Evaluated\non a test set that spans multiple languages and writing systems, the model\nachieves a mean Character Error Rate of 0.055 and a character-level BLEU score\nof 0.914, with particularly strong performance on languages with shallow\northographies. The implementation of beam search further improves practical\nutility, with top-3 candidates reducing the effective error rate by 52.7\\% (to\nCER: 0.026), demonstrating the model's effectiveness for cross-linguistic\napplications.",
      "tldr_zh": "这篇论文介绍了 PolyIPA，一种多语言 Phoneme-to-Grapheme 转换模型，旨在用于多语言名称音译、名字研究和信息检索。该模型通过两个辅助模型——IPA2vec（用于跨语言寻找发音相似的词）和 similarIPA（处理音标变体）——进行数据增强，以提高转换准确性。在多语言测试集上，PolyIPA 取得了平均 Character Error Rate (CER) 为 0.055 和字符级 BLEU 分数为 0.914 的表现，尤其在正字法浅显的语言中表现出色。通过 beam search 技术，top-3 候选项将有效错误率降低了 52.7%（至 CER: 0.026），证明了其在跨语言应用中的实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09102v1",
      "published_date": "2024-12-12 09:29:59 UTC",
      "updated_date": "2024-12-12 09:29:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:50:05.150917"
    },
    {
      "arxiv_id": "2412.09101v2",
      "title": "Temporal Numeric Planning with Patterns",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Cardellini",
        "Enrico Giunchiglia"
      ],
      "abstract": "We consider temporal numeric planning problems $\\Pi$ expressed in PDDL2.1\nlevel 3, and show how to produce SMT formulas $(i)$ whose models correspond to\nvalid plans of $\\Pi$, and $(ii)$ that extend the recently proposed planning\nwith patterns approach from the numeric to the temporal case. We prove the\ncorrectness and completeness of the approach and show that it performs very\nwell on 10 domains with required concurrency.",
      "tldr_zh": "本论文探讨了使用 PDDL2.1 level 3 表达的 Temporal Numeric Planning 问题，通过生成 SMT formulas 来对应有效计划，并扩展了 planning with patterns  approach 到临时场景。研究证明了该方法的正确性和完整性，能够处理需要并发性的复杂任务。在 10 个相关领域上，该方法表现出色，显著提升了规划性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2412.09101v2",
      "published_date": "2024-12-12 09:28:34 UTC",
      "updated_date": "2024-12-18 12:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:50:15.396191"
    },
    {
      "arxiv_id": "2412.09094v3",
      "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Liu",
        "Jihai Zhang",
        "Fangquan Lin",
        "Cheng Yang",
        "Min Peng"
      ],
      "abstract": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a filter-then-generate paradigm and formulate the KGC task into a\nmultiple-choice question format. In this way, we can harness the capability of\nLLMs while mitigating the issue casused by hallucinations. Moreover, we devise\na flexible ego-graph serialization prompt and employ a structure-text adapter\nto couple structure and text information in a contextualized manner.\nExperimental results demonstrate that FtG achieves substantial performance gain\ncompared to existing state-of-the-art methods. The instruction dataset and code\nare available at https://github.com/LB0828/FtG.",
      "tldr_zh": "这项研究针对 Large Language Models (LLMs) 在 Knowledge Graph Completion (KGC) 任务中的挑战，包括大量实体候选、幻觉问题和图结构利用不足，提出了一种新型指令微调方法 FtG。\nFtG 采用 filter-then-generate 范式，将 KGC 转化为多选题格式，并通过 ego-graph serialization prompt 和 structure-text adapter 来有效结合结构和文本信息。\n实验结果表明，FtG 相较于现有最先进方法实现了显著性能提升，并开源了指令数据集和代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING 2025 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2412.09094v3",
      "published_date": "2024-12-12 09:22:04 UTC",
      "updated_date": "2025-02-08 13:40:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:50:27.981851"
    },
    {
      "arxiv_id": "2412.09086v1",
      "title": "Understanding Opportunities and Risks of Synthetic Relationships: Leveraging the Power of Longitudinal Research with Customised AI Tools",
      "title_zh": "翻译失败",
      "authors": [
        "Alfio Ventura",
        "Nils Köbis"
      ],
      "abstract": "This position paper discusses the benefits of longitudinal behavioural\nresearch with customised AI tools for exploring the opportunities and risks of\nsynthetic relationships. Synthetic relationships are defined as \"continuing\nassociations between humans and AI tools that interact with one another wherein\nthe AI tool(s) influence(s) humans' thoughts, feelings, and/or actions.\"\n(Starke et al., 2024). These relationships can potentially improve health,\neducation, and the workplace, but they also bring the risk of subtle\nmanipulation and privacy and autonomy concerns. To harness the opportunities of\nsynthetic relationships and mitigate their risks, we outline a methodological\napproach that complements existing findings. We propose longitudinal research\ndesigns with self-assembled AI agents that enable the integration of detailed\nbehavioural and self-reported data.",
      "tldr_zh": "这篇论文探讨了synthetic relationships（人类与AI工具的持续互动）的机遇与风险，定义其为AI工具影响人类思想、情感和行为的关联，可能改善健康、教育和工作场所，但也带来微妙操纵、隐私和自治问题的潜在威胁。作者强调，通过longitudinal research（纵向行为研究）结合定制AI工具，可以整合详细的行为数据和自报告数据，以充分利用这些关系的益处。论文提出了一种方法论方法，使用自组装AI代理进行纵向研究设计，从而缓解风险并补充现有发现。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "J.4"
      ],
      "primary_category": "cs.HC",
      "comment": "This is a \"Position paper accepted for CONVERSATIONS 2024 - the 8th\n  International Workshop on Chatbots and Human-Centred AI, hosted by CERTH,\n  Thessaloniki, Greece, December 4-5, 2024.\" The original publication is\n  available on the workshop website: https://2024.conversations.ws/papers/ .\n  This document is identical to the original and is mainly available here for\n  accessibility and discoverability",
      "pdf_url": "http://arxiv.org/pdf/2412.09086v1",
      "published_date": "2024-12-12 09:13:43 UTC",
      "updated_date": "2024-12-12 09:13:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:50:39.837380"
    },
    {
      "arxiv_id": "2412.09078v5",
      "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenni Bi",
        "Kai Han",
        "Chuanjian Liu",
        "Yehui Tang",
        "Yunhe Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
      "tldr_zh": "这项研究针对Large Language Models (LLMs)在复杂推理任务中的挑战，提出了一种新型框架Forest-of-Thought (FoT)，通过整合多个推理树实现集体决策，克服了Chain-of-Thought (CoT)和Tree-of-Thought (ToT)等现有方法的单次推理局限性。FoT采用稀疏激活策略、动态自修正策略以及共识引导决策策略，以提升推理效率和准确性，同时优化计算资源。实验结果显示，该框架显著增强了LLMs的推理能力，在复杂任务上实现了更高的精确性和效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2412.09078v5",
      "published_date": "2024-12-12 09:01:18 UTC",
      "updated_date": "2025-04-01 12:48:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:50:51.175292"
    },
    {
      "arxiv_id": "2412.09058v1",
      "title": "EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems",
      "title_zh": "EmbedGenius：面向通用嵌入式物联网系统的自动软件开发",
      "authors": [
        "Huanqi Yang",
        "Mingzhe Li",
        "Mingda Han",
        "Zhenjiang Li",
        "Weitao Xu"
      ],
      "abstract": "Embedded IoT system development is crucial for enabling seamless connectivity\nand functionality across a wide range of applications. However, such a complex\nprocess requires cross-domain knowledge of hardware and software and hence\noften necessitates direct developer involvement, making it labor-intensive,\ntime-consuming, and error-prone. To address this challenge, this paper\nintroduces EmbedGenius, the first fully automated software development platform\nfor general-purpose embedded IoT systems. The key idea is to leverage the\nreasoning ability of Large Language Models (LLMs) and embedded system expertise\nto automate the hardware-in-the-loop development process. The main methods\ninclude a component-aware library resolution method for addressing hardware\ndependencies, a library knowledge generation method that injects utility domain\nknowledge into LLMs, and an auto-programming method that ensures successful\ndeployment. We evaluate EmbedGenius's performance across 71 modules and four\nmainstream embedded development platforms with over 350 IoT tasks. Experimental\nresults show that EmbedGenius can generate codes with an accuracy of 95.7% and\ncomplete tasks with a success rate of 86.5%, surpassing human-in-the-loop\nbaselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show\nEmbedGenius's potential through case studies in environmental monitoring and\nremote control systems development.",
      "tldr_zh": "这篇论文介绍了EmbedGenius，一种首个完全自动化的软件开发平台，针对通用嵌入式IoT系统，通过利用Large Language Models (LLMs)的推理能力来简化硬件-in-the-loop开发过程。关键方法包括component-aware library resolution method处理硬件依赖、library knowledge generation method注入领域知识，以及auto-programming method确保代码部署成功。在实验评估中，EmbedGenius在71个模块和超过350个IoT任务上实现了95.7%的代码准确率和86.5%的任务成功率，分别比人工参与基线高15.6%–37.7%和25.5%–53.4%。此外，通过环境监测和远程控制系统的案例研究，展示了其在实际应用中的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09058v1",
      "published_date": "2024-12-12 08:34:12 UTC",
      "updated_date": "2024-12-12 08:34:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:51:04.819909"
    },
    {
      "arxiv_id": "2412.09056v1",
      "title": "A Context-Enhanced Framework for Sequential Graph Reasoning",
      "title_zh": "一种基于上下文增强的顺序图推理框架",
      "authors": [
        "Shuo Shi",
        "Chao Peng",
        "Chenyang Xu",
        "Zhengfeng Yang"
      ],
      "abstract": "The paper studies sequential reasoning over graph-structured data, which\nstands as a fundamental task in various trending fields like automated math\nproblem solving and neural graph algorithm learning, attracting a lot of\nresearch interest. Simultaneously managing both sequential and graph-structured\ninformation in such tasks presents a notable challenge. Over recent years, many\nneural architectures in the literature have emerged to tackle the issue. In\nthis work, we generalize the existing architectures and propose a\ncontext-enhanced framework. The crucial innovation is that the reasoning of\neach step does not only rely on the outcome of the preceding step but also\nleverages the aggregation of information from more historical outcomes. The\nidea stems from our observation that in sequential graph reasoning, each step's\noutcome has a much stronger inner connection with each other compared to\ntraditional seq-to-seq tasks. We show that the framework can effectively\nintegrate with the existing methods, enhancing their reasoning abilities.\nEmpirical evaluations are conducted on the challenging CLRS Reasoning\nBenchmark, and the results demonstrate that the proposed framework\nsignificantly improves the performance of existing architectures, yielding\nstate-of-the-art results across the majority of the datasets within the\nbenchmark.",
      "tldr_zh": "该论文探讨了在图结构数据上进行顺序推理（sequential graph reasoning）的挑战，这在自动化数学问题求解和神经图算法学习等领域至关重要。作者提出一个context-enhanced框架，其关键创新是每个推理步骤不仅依赖前一个步骤的结果，还整合更多历史结果的信息，以利用步骤之间的强内在连接。该框架可与现有方法无缝结合，并在CLRS Reasoning Benchmark上的实验中显著提升了性能，在大多数数据集上实现了state-of-the-art结果。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Appeared at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.09056v1",
      "published_date": "2024-12-12 08:27:51 UTC",
      "updated_date": "2024-12-12 08:27:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:51:15.312703"
    },
    {
      "arxiv_id": "2412.09046v1",
      "title": "Multi-Task Learning with LLMs for Implicit Sentiment Analysis: Data-level and Task-level Automatic Weight Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Wenna Lai",
        "Haoran Xie",
        "Guandong Xu",
        "Qing Li"
      ],
      "abstract": "Implicit sentiment analysis (ISA) presents significant challenges due to the\nabsence of salient cue words. Previous methods have struggled with insufficient\ndata and limited reasoning capabilities to infer underlying opinions.\nIntegrating multi-task learning (MTL) with large language models (LLMs) offers\nthe potential to enable models of varying sizes to reliably perceive and\nrecognize genuine opinions in ISA. However, existing MTL approaches are\nconstrained by two sources of uncertainty: data-level uncertainty, arising from\nhallucination problems in LLM-generated contextual information, and task-level\nuncertainty, stemming from the varying capacities of models to process\ncontextual information. To handle these uncertainties, we introduce MT-ISA, a\nnovel MTL framework that enhances ISA by leveraging the generation and\nreasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA\nconstructs auxiliary tasks using generative LLMs to supplement sentiment\nelements and incorporates automatic MTL to fully exploit auxiliary data. We\nintroduce data-level and task-level automatic weight learning (AWL), which\ndynamically identifies relationships and prioritizes more reliable data and\ncritical tasks, enabling models of varying sizes to adaptively learn\nfine-grained weights based on their reasoning capabilities. We investigate\nthree strategies for data-level AWL, while also introducing homoscedastic\nuncertainty for task-level AWL. Extensive experiments reveal that models of\nvarying sizes achieve an optimal balance between primary prediction and\nauxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability\nof our approach.",
      "tldr_zh": "该论文针对隐式情感分析（Implicit Sentiment Analysis, ISA）的挑战，提出了一种名为 MT-ISA 的多任务学习（Multi-Task Learning, MTL）框架，利用大型语言模型（Large Language Models, LLMs）的生成和推理能力来补充情感元素并处理数据级和任务级不确定性。具体而言，MT-ISA 通过数据级自动权重学习（Automatic Weight Learning, AWL）动态识别可靠数据，并采用三种策略来优先处理相关信息，同时引入任务级 AWL 的同方差不确定性来适应不同模型的推理能力。实验结果显示，该框架使各种规模的模型在主要预测和辅助任务之间实现了最佳平衡，证明了其有效性和适应性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 6 figures, and 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.09046v1",
      "published_date": "2024-12-12 08:15:16 UTC",
      "updated_date": "2024-12-12 08:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:51:29.061972"
    },
    {
      "arxiv_id": "2412.09044v2",
      "title": "Motif Guided Graph Transformer with Combinatorial Skeleton Prototype Learning for Skeleton-Based Person Re-Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Haocong Rao",
        "Chunyan Miao"
      ],
      "abstract": "Person re-identification (re-ID) via 3D skeleton data is a challenging task\nwith significant value in many scenarios. Existing skeleton-based methods\ntypically assume virtual motion relations between all joints, and adopt average\njoint or sequence representations for learning. However, they rarely explore\nkey body structure and motion such as gait to focus on more important body\njoints or limbs, while lacking the ability to fully mine valuable\nspatial-temporal sub-patterns of skeletons to enhance model learning. This\npaper presents a generic Motif guided graph transformer with Combinatorial\nskeleton prototype learning (MoCos) that exploits structure-specific and\ngait-related body relations as well as combinatorial features of skeleton\ngraphs to learn effective skeleton representations for person re-ID. In\nparticular, motivated by the locality within joints' structure and the\nbody-component collaboration in gait, we first propose the motif guided graph\ntransformer (MGT) that incorporates hierarchical structural motifs and gait\ncollaborative motifs, which simultaneously focuses on multi-order local joint\ncorrelations and key cooperative body parts to enhance skeleton relation\nlearning. Then, we devise the combinatorial skeleton prototype learning (CSP)\nthat leverages random spatial-temporal combinations of joint nodes and skeleton\ngraphs to generate diverse sub-skeleton and sub-tracklet representations, which\nare contrasted with the most representative features (prototypes) of each\nidentity to learn class-related semantics and discriminative skeleton\nrepresentations. Extensive experiments validate the superior performance of\nMoCos over existing state-of-the-art models. We further show its generality\nunder RGB-estimated skeletons, different graph modeling, and unsupervised\nscenarios.",
      "tldr_zh": "该论文针对基于 3D 骨骼数据的行人再识别 (Person re-ID) 任务，提出了一种通用框架 MoCos，以解决现有方法忽略关键身体结构和运动（如步态）以及未能充分挖掘空间-时间子模式的问题。MoCos 包括 Motif guided graph transformer (MGT)，通过整合层次结构 motif 和步态协作 motif，关注多阶局部关节相关性和关键身体部位协作，从而增强骨骼关系学习；以及 Combinatorial skeleton prototype learning (CSP)，利用关节节点和 skeleton graphs 的随机组合生成多样子表示，并与身份原型进行对比学习以提升判别性。实验结果显示，MoCos 在各种场景下（如 RGB 估计骨骼、不同图建模和无监督设置）均优于现有最先进模型，验证了其有效性和通用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025. Codes are available at\n  https://github.com/Kali-Hac/MoCos. The Appendix A for Experiments (13 pages)\n  and Appendix B for Theoretical Analysis (5 pages) are included in the version\n  [v1] at arXiv:2412.09044",
      "pdf_url": "http://arxiv.org/pdf/2412.09044v2",
      "published_date": "2024-12-12 08:13:29 UTC",
      "updated_date": "2025-02-02 02:13:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:51:40.408084"
    },
    {
      "arxiv_id": "2412.10457v1",
      "title": "Explaining Model Overfitting in CNNs via GMM Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Dou",
        "Xinyu Mu",
        "Mengjun Yi",
        "Feng Han",
        "Jian Zhao",
        "Furao Shen"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have demonstrated remarkable prowess in\nthe field of computer vision. However, their opaque decision-making processes\npose significant challenges for practical applications. In this study, we\nprovide quantitative metrics for assessing CNN filters by clustering the\nfeature maps corresponding to individual filters in the model via Gaussian\nMixture Model (GMM). By analyzing the clustering results, we screen out some\nanomaly filters associated with outlier samples. We further analyze the\nrelationship between the anomaly filters and model overfitting, proposing three\nhypotheses. This method is universally applicable across diverse CNN\narchitectures without modifications, as evidenced by its successful application\nto models like AlexNet and LeNet-5. We present three meticulously designed\nexperiments demonstrating our hypotheses from the perspectives of model\nbehavior, dataset characteristics, and filter impacts. Through this work, we\noffer a novel perspective for evaluating the CNN performance and gain new\ninsights into the operational behavior of model overfitting.",
      "tldr_zh": "本文通过使用 Gaussian Mixture Model (GMM) 聚类 CNNs 模型中过滤器的特征图，量化评估过滤器并识别与异常样本相关的异常过滤器。研究分析了这些异常过滤器与模型过拟合的关系，提出了三个假设，并通过三个精心设计的实验，从模型行为、数据集特性和过滤器影响角度验证了这些假设。该方法适用于各种 CNN 架构，如 AlexNet 和 LeNet-5，无需修改，提供了一个新颖的视角来评估 CNN 性能并深入理解过拟合行为。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10457v1",
      "published_date": "2024-12-12 08:13:18 UTC",
      "updated_date": "2024-12-12 08:13:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:51:51.735760"
    },
    {
      "arxiv_id": "2412.10456v2",
      "title": "FovealNet: Advancing AI-Driven Gaze Tracking Solutions for Optimized Foveated Rendering System Performance in Virtual Reality",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Liu",
        "Monde Duinkharjav",
        "Qi Sun",
        "Sai Qian Zhang"
      ],
      "abstract": "Leveraging real-time eye-tracking, foveated rendering optimizes hardware\nefficiency and enhances visual quality virtual reality (VR). This approach\nleverages eye-tracking techniques to determine where the user is looking,\nallowing the system to render high-resolution graphics only in the foveal\nregion-the small area of the retina where visual acuity is highest, while the\nperipheral view is rendered at lower resolution. However, modern deep\nlearning-based gaze-tracking solutions often exhibit a long-tail distribution\nof tracking errors, which can degrade user experience and reduce the benefits\nof foveated rendering by causing misalignment and decreased visual quality.\n  This paper introduces \\textit{FovealNet}, an advanced AI-driven gaze tracking\nframework designed to optimize system performance by strategically enhancing\ngaze tracking accuracy. To further reduce the implementation cost of the gaze\ntracking algorithm, FovealNet employs an event-based cropping method that\neliminates over $64.8\\%$ of irrelevant pixels from the input image.\nAdditionally, it incorporates a simple yet effective token-pruning strategy\nthat dynamically removes tokens on the fly without compromising tracking\naccuracy. Finally, to support different runtime rendering configurations, we\npropose a system performance-aware multi-resolution training strategy, allowing\nthe gaze tracking DNN to adapt and optimize overall system performance more\neffectively. Evaluation results demonstrate that FovealNet achieves at least\n$1.42\\times$ speed up compared to previous methods and 13\\% increase in\nperceptual quality for foveated output.",
      "tldr_zh": "这篇论文介绍了 FovealNet，一种先进的 AI 驱动眼动追踪框架，旨在优化虚拟现实 (VR) 中的 foveated rendering 系统性能，以解决现有深度学习方法的长尾分布错误问题。FovealNet 采用事件-based cropping 方法去除超过 64.8% 的无关像素、token-pruning 策略动态移除 token，以及系统性能-aware 的多分辨率训练策略，确保追踪准确性并适应不同渲染配置。实验结果显示，该框架比之前方法实现 1.42 倍的速度提升，并提高 13% 的感知质量，从而提升用户体验和硬件效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10456v2",
      "published_date": "2024-12-12 08:03:54 UTC",
      "updated_date": "2024-12-31 01:43:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:52:04.487205"
    },
    {
      "arxiv_id": "2412.09032v2",
      "title": "Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Zhoulin Ji",
        "Chenhao Lin",
        "Hang Wang",
        "Chao Shen"
      ],
      "abstract": "Detecting synthetic from real speech is increasingly crucial due to the risks\nof misinformation and identity impersonation. While various datasets for\nsynthetic speech analysis have been developed, they often focus on specific\nareas, limiting their utility for comprehensive research. To fill this gap, we\npropose the Speech-Forensics dataset by extensively covering authentic,\nsynthetic, and partially forged speech samples that include multiple segments\nsynthesized by different high-quality algorithms. Moreover, we propose a\nTEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously\nperforming authenticity detection, multiple fake segments localization, and\nsynthesis algorithms recognition, without any complex post-processing. TEST\neffectively integrates LSTM and Transformer to extract more powerful temporal\nspeech representations and utilizes dense prediction on multi-scale pyramid\nfeatures to estimate the synthetic spans. Our model achieves an average mAP of\n83.55% and an EER of 5.25% at the utterance level. At the segment level, it\nattains an EER of 1.07% and a 92.19% F1 score. These results highlight the\nmodel's robust capability for a comprehensive analysis of synthetic speech,\noffering a promising avenue for future research and practical applications in\nthis field.",
      "tldr_zh": "该研究针对合成语音检测的挑战，提出了Speech-Forensics数据集，该数据集全面覆盖真实、合成和部分伪造的语音样本，包括多种高质量算法生成的段落，以弥补现有数据集的局限性。同时，引入了TEmporal Speech LocalizaTion网络（TEST），该模型整合LSTM和Transformer提取时间语音特征，并通过多尺度金字塔特征的密集预测，实现真实性检测、假段定位和合成算法识别，而无需复杂后处理。实验结果显示，TEST在话语级别达到83.55% mAP和5.25% EER，在段级别获得1.07% EER和92.19% F1分数，展示了其在合成语音分析中的强大性能，为未来研究和实际应用提供了重要基础。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09032v2",
      "published_date": "2024-12-12 07:48:17 UTC",
      "updated_date": "2024-12-16 07:30:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:52:16.096113"
    },
    {
      "arxiv_id": "2412.09030v1",
      "title": "RingFormer: A Ring-Enhanced Graph Transformer for Organic Solar Cell Property Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Ding",
        "Ting Zhang",
        "Yiran Li",
        "Jieming Shi",
        "Chen Jason Zhang"
      ],
      "abstract": "Organic Solar Cells (OSCs) are a promising technology for sustainable energy\nproduction. However, the identification of molecules with desired OSC\nproperties typically involves laborious experimental research. To accelerate\nprogress in the field, it is crucial to develop machine learning models capable\nof accurately predicting the properties of OSC molecules. While graph\nrepresentation learning has demonstrated success in molecular property\nprediction, it remains underexplored for OSC-specific tasks. Existing methods\nfail to capture the unique structural features of OSC molecules, particularly\nthe intricate ring systems that critically influence OSC properties, leading to\nsuboptimal performance. To fill the gap, we present RingFormer, a novel graph\ntransformer framework specially designed to capture both atom and ring level\nstructural patterns in OSC molecules. RingFormer constructs a hierarchical\ngraph that integrates atomic and ring structures and employs a combination of\nlocal message passing and global attention mechanisms to generate expressive\ngraph representations for accurate OSC property prediction. We evaluate\nRingFormer's effectiveness on five curated OSC molecule datasets through\nextensive experiments. The results demonstrate that RingFormer consistently\noutperforms existing methods, achieving a 22.77% relative improvement over the\nnearest competitor on the CEPDB dataset.",
      "tldr_zh": "本研究针对有机太阳能电池（OSCs）分子属性的预测问题，提出RingFormer，一种增强环结构的图变换器框架，以捕捉分子中原子和环级结构的关键模式。RingFormer通过构建分层图、结合局部消息传递和全局注意力机制，生成更具表达力的图表示，从而提升预测准确性。在五个OSC分子数据集上的实验中，RingFormer显著优于现有方法，在CEPDB数据集上较最近竞争对手实现了22.77%的相对改进，为加速OSC材料开发提供了高效工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 4 figures. This is the extended version of the paper\n  accepted at AAAI 2025, which includes all technical appendices and additional\n  experimental details",
      "pdf_url": "http://arxiv.org/pdf/2412.09030v1",
      "published_date": "2024-12-12 07:45:17 UTC",
      "updated_date": "2024-12-12 07:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:52:28.532128"
    },
    {
      "arxiv_id": "2412.09025v1",
      "title": "Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages",
      "title_zh": "Shiksha：一个聚焦技术领域的印度语言翻译数据集和模型",
      "authors": [
        "Advait Joglekar",
        "Srinivasan Umesh"
      ],
      "abstract": "Neural Machine Translation (NMT) models are typically trained on datasets\nwith limited exposure to Scientific, Technical and Educational domains.\nTranslation models thus, in general, struggle with tasks that involve\nscientific understanding or technical jargon. Their performance is found to be\neven worse for low-resource Indian languages. Finding a translation dataset\nthat tends to these domains in particular, poses a difficult challenge. In this\npaper, we address this by creating a multilingual parallel corpus containing\nmore than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality\ntranslation pairs across 8 Indian languages. We achieve this by bitext mining\nhuman-translated transcriptions of NPTEL video lectures. We also finetune and\nevaluate NMT models using this corpus and surpass all other publicly available\nmodels at in-domain tasks. We also demonstrate the potential for generalizing\nto out-of-domain translation tasks by improving the baseline by over 2 BLEU on\naverage for these Indian languages on the Flores+ benchmark. We are pleased to\nrelease our model and dataset via this link: https://huggingface.co/SPRINGLab.",
      "tldr_zh": "该研究针对神经机器翻译 (NMT) 模型在科学、技术和教育领域表现不足的问题，特别是对低资源印度语言的挑战，构建了一个专注于技术领域的多语言平行语料库。数据集包含超过 280 万条高质量英语到印度语和印度语到印度语翻译对，基于 bitext mining 人翻译的 NPTEL 视频讲座转录，覆盖 8 种印度语言。作者通过微调 NMT 模型，在领域内任务上超越了现有公开模型，并在 Flores+ 基准上平均提升超过 2 BLEU 分数，展示了出领域任务的泛化潜力；数据集和模型已通过 Hugging Face 发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09025v1",
      "published_date": "2024-12-12 07:40:55 UTC",
      "updated_date": "2024-12-12 07:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:53:44.237865"
    },
    {
      "arxiv_id": "2412.10455v1",
      "title": "Geo-LLaVA: A Large Multi-Modal Model for Solving Geometry Math Problems with Meta In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Shihao Xu",
        "Yiyang Luo",
        "Wei Shi"
      ],
      "abstract": "Geometry mathematics problems pose significant challenges for large language\nmodels (LLMs) because they involve visual elements and spatial reasoning.\nCurrent methods primarily rely on symbolic character awareness to address these\nproblems. Considering geometry problem solving is a relatively nascent field\nwith limited suitable datasets and currently almost no work on solid geometry\nproblem solving, we collect a geometry question-answer dataset by sourcing\ngeometric data from Chinese high school education websites, referred to as\nGeoMath. It contains solid geometry questions and answers with accurate\nreasoning steps as compensation for existing plane geometry datasets.\nAdditionally, we propose a Large Multi-modal Model (LMM) framework named\nGeo-LLaVA, which incorporates retrieval augmentation with supervised\nfine-tuning (SFT) in the training stage, called meta-training, and employs\nin-context learning (ICL) during inference to improve performance. Our\nfine-tuned model with ICL attains the state-of-the-art performance of 65.25%\nand 42.36% on selected questions of the GeoQA dataset and GeoMath dataset\nrespectively with proper inference steps. Notably, our model initially endows\nthe ability to solve solid geometry problems and supports the generation of\nreasonable solid geometry picture descriptions and problem-solving steps. Our\nresearch sets the stage for further exploration of LLMs in multi-modal math\nproblem-solving, particularly in geometry math problems.",
      "tldr_zh": "这篇论文针对大型语言模型（LLMs）在解决几何数学问题时的视觉元素和空间推理挑战，提出了一种大型多模态模型Geo-LLaVA框架。Geo-LLaVA通过meta-training（结合检索增强和监督微调SFT）以及推理阶段的in-context learning (ICL)来提升性能，并引入了新数据集GeoMath，包括立体几何问题及其推理步骤。实验结果显示，该模型在GeoQA数据集上达到65.25%的最先进性能，在GeoMath上达到42.36%，并首次支持生成立体几何图片描述和问题解决步骤，为LLMs在多模态数学领域的应用奠定基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10455v1",
      "published_date": "2024-12-12 07:34:09 UTC",
      "updated_date": "2024-12-12 07:34:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:53:56.568465"
    },
    {
      "arxiv_id": "2412.10454v1",
      "title": "An Interoperable Machine Learning Pipeline for Pediatric Obesity Risk Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Hamed Fayyaz",
        "Mehak Gupta",
        "Alejandra Perez Ramirez",
        "Claudine Jurkovitz",
        "H. Timothy Bunnell",
        "Thao-Ly T. Phan",
        "Rahmatollah Beheshti"
      ],
      "abstract": "Reliable prediction of pediatric obesity can offer a valuable resource to\nproviders, helping them engage in timely preventive interventions before the\ndisease is established. Many efforts have been made to develop ML-based\npredictive models of obesity, and some studies have reported high predictive\nperformances. However, no commonly used clinical decision support tool based on\nexisting ML models currently exists. This study presents a novel end-to-end\npipeline specifically designed for pediatric obesity prediction, which supports\nthe entire process of data extraction, inference, and communication via an API\nor a user interface. While focusing only on routinely recorded data in\npediatric electronic health records (EHRs), our pipeline uses a diverse\nexpert-curated list of medical concepts to predict the 1-3 years risk of\ndeveloping obesity. Furthermore, by using the Fast Healthcare Interoperability\nResources (FHIR) standard in our design procedure, we specifically target\nfacilitating low-effort integration of our pipeline with different EHR systems.\nIn our experiments, we report the effectiveness of the predictive model as well\nas its alignment with the feedback from various stakeholders, including ML\nscientists, providers, health IT personnel, health administration\nrepresentatives, and patient group representatives.",
      "tldr_zh": "该研究提出了一种互操作性机器学习 (Machine Learning) 管道，用于预测儿童肥胖风险，帮助医疗提供者及时进行预防干预。该管道支持端到端流程，包括从儿科电子健康记录 (EHRs) 中提取数据、进行推理，并通过 API 或用户界面通信，利用专家策划的医疗概念列表预测1-3年内肥胖风险，并采用 Fast Healthcare Interoperability Resources (FHIR) 标准便于与其他 EHR 系统集成。在实验中，该模型显示出较高的预测有效性，并获得了 ML 科学家、医疗提供者、健康 IT 人员等利益相关者的积极反馈。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10454v1",
      "published_date": "2024-12-12 07:25:37 UTC",
      "updated_date": "2024-12-12 07:25:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:54:07.449958"
    },
    {
      "arxiv_id": "2412.09012v2",
      "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
      "title_zh": "翻译失败",
      "authors": [
        "Abdelrahman Sadallah",
        "Daria Kotova",
        "Ekaterina Kochmar"
      ],
      "abstract": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.",
      "tldr_zh": "本研究探讨了加密填字游戏（Cryptic Crosswords）为什么对大型语言模型（LLMs）构成挑战，这些游戏依赖于一般知识和多层语言操作。论文建立了 Gemma2、LLaMA3 和 ChatGPT 的基准结果，显示这些 LLMs 在解决此类谜题时的性能远低于人类。研究进一步分析了 LLMs 表现不佳的原因，包括语言处理和知识缺口等问题，并发布了代码和数据集以供进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING 2025. arXiv admin note: text overlap with arXiv:2403.12094",
      "pdf_url": "http://arxiv.org/pdf/2412.09012v2",
      "published_date": "2024-12-12 07:23:52 UTC",
      "updated_date": "2025-01-14 06:06:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:54:19.371121"
    },
    {
      "arxiv_id": "2412.12171v1",
      "title": "AI Adoption to Combat Financial Crime: Study on Natural Language Processing in Adverse Media Screening of Financial Services in English and Bangla multilingual interpretation",
      "title_zh": "翻译失败",
      "authors": [
        "Soumita Roy"
      ],
      "abstract": "This document explores the potential of employing Artificial Intelligence\n(AI), specifically Natural Language Processing (NLP), to strengthen the\ndetection and prevention of financial crimes within the Mobile Financial\nServices(MFS) of Bangladesh with multilingual scenario. The analysis focuses on\nthe utilization of NLP for adverse media screening, a vital aspect of\ncompliance with anti-money laundering (AML) and combating financial terrorism\n(CFT) regulations. Additionally, it investigates the overall reception and\nobstacles related to the integration of AI in Bangladeshi banks. This report\nmeasures the effectiveness of NLP is promising with an accuracy around 94\\%.\nNLP algorithms display substantial promise in accurately identifying adverse\nmedia content linked to financial crimes. The lack of progress in this aspect\nis visible in Bangladesh, whereas globally the technology is already being used\nto increase effectiveness and efficiency. Hence, it is clear there is an issue\nwith the acceptance of AI in Bangladesh. Some AML \\& CFT concerns are already\nbeing addressed by AI technology. For example, Image Recognition OCR technology\nare being used in KYC procedures. Primary hindrances to AI integration involve\na lack of technical expertise, high expenses, and uncertainties surrounding\nregulations. This investigation underscores the potential of AI-driven NLP\nsolutions in fortifying efforts to prevent financial crimes in Bangladesh.",
      "tldr_zh": "这篇论文探讨了在孟加拉国移动金融服务中使用自然语言处理(NLP)来增强金融犯罪检测，特别是针对英语和孟加拉语的多语言负面媒体筛查，以符合反洗钱(AML)和反恐融资(CFT)法规。研究评估了NLP算法在识别与金融犯罪相关的负面内容方面的有效性，取得了约94%的准确率，证明其在全球已广泛应用的潜力。论文同时指出了AI采用的障碍，包括技术专长不足、高成本和监管不确定性，并强调AI驱动的NLP解决方案可强化孟加拉国的金融犯罪预防努力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12171v1",
      "published_date": "2024-12-12 07:17:05 UTC",
      "updated_date": "2024-12-12 07:17:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:54:32.105857"
    },
    {
      "arxiv_id": "2412.10452v1",
      "title": "Structurally Consistent MRI Colorization using Cross-modal Fusion Learning",
      "title_zh": "基于跨模态融合学习的结构一致 MRI 着色",
      "authors": [
        "Mayuri Mathur",
        "Anav Chaudhary",
        "Saurabh Kumar Gupta",
        "Ojaswa Sharma"
      ],
      "abstract": "Medical image colorization can greatly enhance the interpretability of the\nunderlying imaging modality and provide insights into human anatomy. The\nobjective of medical image colorization is to transfer a diverse spectrum of\ncolors distributed across human anatomy from Cryosection data to source MRI\ndata while retaining the structures of the MRI. To achieve this, we propose a\nnovel architecture for structurally consistent color transfer to the source MRI\ndata. Our architecture fuses segmentation semantics of Cryosection images for\nstable contextual colorization of various organs in MRI images. For\ncolorization, we neither require precise registration between MRI and\nCryosection images, nor segmentation of MRI images. Additionally, our\narchitecture incorporates a feature compression-and-activation mechanism to\ncapture organ-level global information and suppress noise, enabling the\ndistinction of organ-specific data in MRI scans for more accurate and realistic\norgan-specific colorization. Our experiments demonstrate that our architecture\nsurpasses the existing methods and yields better quantitative and qualitative\nresults.",
      "tldr_zh": "这篇论文提出了一种基于跨模态融合学习（Cross-modal Fusion Learning）的架构，用于将 Cryosection 数据中的颜色转移到 MRI 图像上，同时保留 MRI 的结构，以提升图像的可解释性和解剖学洞见。  \n该架构通过融合 Cryosection 图像的分割语义实现稳定上下文颜色化，并引入特征压缩和激活机制来捕捉器官级全局信息、抑制噪声，而无需精确图像注册或 MRI 分割。  \n实验结果表明，该方法在定量和定性指标上超越现有技术，提供更准确和真实的器官特定颜色化效果。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "9 pages, 6 figures, 2 Tables",
      "pdf_url": "http://arxiv.org/pdf/2412.10452v1",
      "published_date": "2024-12-12 06:40:14 UTC",
      "updated_date": "2024-12-12 06:40:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:54:43.695072"
    },
    {
      "arxiv_id": "2412.12170v1",
      "title": "PickLLM: Context-Aware RL-Assisted Large Language Model Routing",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitrios Sikeridis",
        "Dennis Ramdass",
        "Pranay Pareek"
      ],
      "abstract": "Recently, the number of off-the-shelf Large Language Models (LLMs) has\nexploded with many open-source options. This creates a diverse landscape\nregarding both serving options (e.g., inference on local hardware vs remote LLM\nAPIs) and model heterogeneous expertise. However, it is hard for the user to\nefficiently optimize considering operational cost (pricing structures,\nexpensive LLMs-as-a-service for large querying volumes), efficiency, or even\nper-case specific measures such as response accuracy, bias, or toxicity. Also,\nexisting LLM routing solutions focus mainly on cost reduction, with response\naccuracy optimizations relying on non-generalizable supervised training, and\nensemble approaches necessitating output computation for every considered LLM\ncandidate. In this work, we tackle the challenge of selecting the optimal LLM\nfrom a model pool for specific queries with customizable objectives. We propose\nPickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to\nroute on-the-fly queries to available models. We introduce a weighted reward\nfunction that considers per-query cost, inference latency, and model response\naccuracy by a customizable scoring function. Regarding the learning algorithms,\nwe explore two alternatives: PickLLM router acting as a learning automaton that\nutilizes gradient ascent to select a specific LLM, or utilizing stateless\nQ-learning to explore the set of LLMs and perform selection with a\n$\\epsilon$-greedy approach. The algorithm converges to a single LLM for the\nremaining session queries. To evaluate, we utilize a pool of four LLMs and\nbenchmark prompt-response datasets with different contexts. A separate scoring\nfunction is assessing response accuracy during the experiment. We demonstrate\nthe speed of convergence for different learning rates and improvement in hard\nmetrics such as cost per querying session and overall response latency.",
      "tldr_zh": "该论文提出PickLLM框架，一种基于上下文感知的Reinforcement Learning (RL)辅助方法，用于从LLM池中动态路由查询到最优模型，以优化成本、推理延迟和响应准确性。框架引入加权奖励函数和两种算法：PickLLM路由器使用梯度上升作为学习自动机，或无状态Q-learning结合ε-greedy策略进行探索和选择。实验结果显示，PickLLM在四个LLM池和基准数据集上快速收敛，显著降低了查询会话成本和整体响应延迟，同时提升了响应准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted at the first Workshop on Scalable and\n  Efficient Artificial Intelligence Systems (SEAS) held in conjunction with the\n  39th Annual AAAI Conference on Artificial Intelligence, AAAI 2025, in\n  Philadelphia, Pennsylvania, USA",
      "pdf_url": "http://arxiv.org/pdf/2412.12170v1",
      "published_date": "2024-12-12 06:27:12 UTC",
      "updated_date": "2024-12-12 06:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:54:55.673786"
    },
    {
      "arxiv_id": "2412.08973v1",
      "title": "Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?",
      "title_zh": "对比蒸馏是否足够用于学习全面的3D表示？",
      "authors": [
        "Yifan Zhang",
        "Junhui Hou"
      ],
      "abstract": "Cross-modal contrastive distillation has recently been explored for learning\neffective 3D representations. However, existing methods focus primarily on\nmodality-shared features, neglecting the modality-specific features during the\npre-training process, which leads to suboptimal representations. In this paper,\nwe theoretically analyze the limitations of current contrastive methods for 3D\nrepresentation learning and propose a new framework, namely CMCR, to address\nthese shortcomings. Our approach improves upon traditional methods by better\nintegrating both modality-shared and modality-specific features. Specifically,\nwe introduce masked image modeling and occupancy estimation tasks to guide the\nnetwork in learning more comprehensive modality-specific features. Furthermore,\nwe propose a novel multi-modal unified codebook that learns an embedding space\nshared across different modalities. Besides, we introduce geometry-enhanced\nmasked image modeling to further boost 3D representation learning. Extensive\nexperiments demonstrate that our method mitigates the challenges faced by\ntraditional approaches and consistently outperforms existing image-to-LiDAR\ncontrastive distillation methods in downstream tasks. Code will be available at\nhttps://github.com/Eaphan/CMCR.",
      "tldr_zh": "这篇论文质疑对比蒸馏是否足够学习全面的3D表示，指出现有跨模态对比蒸馏方法过度关注模态共享特征而忽略模态特定特征，导致表示 suboptimal。作者提出CMCR框架，通过引入masked image modeling和occupancy estimation任务来学习模态特定特征，并设计multi-modal unified codebook和geometry-enhanced masked image modeling来整合共享和特定特征，从而提升3D表示学习。实验结果显示，CMCR在下游任务中显著优于传统image-to-LiDAR对比蒸馏方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2412.08973v1",
      "published_date": "2024-12-12 06:09:49 UTC",
      "updated_date": "2024-12-12 06:09:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:55:07.944612"
    },
    {
      "arxiv_id": "2412.08972v1",
      "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Ruiwen Zhou",
        "Wenyue Hua",
        "Liangming Pan",
        "Sitao Cheng",
        "Xiaobao Wu",
        "En Yu",
        "William Yang Wang"
      ],
      "abstract": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. These results highlight significant challenges in advancing\nLLMs' rule-guided reasoning capabilities in real-life applications.",
      "tldr_zh": "本研究引入了 RuleArena，这是一个新的基准，用于评估大型语言模型 (LLMs) 在真实场景中进行规则指导推理的能力，涵盖航空行李费、NBA 交易和税务法规等三个实际领域。RuleArena 要求 LLMs 处理复杂的自然语言指令、长上下文理解、逻辑推理和数学计算，与传统基准不同的是，它超越了标准一阶逻辑表示并基于真实情境。实验结果揭示了 LLMs 的主要局限性：它们难以准确识别和应用相关规则、经常混淆相似规定、数学计算不一致，且整体表现较差，这突显了提升 LLMs 在实际应用中规则指导推理能力的挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Data and Codes are available at\n  https://github.com/skyriver-2000/RuleArena",
      "pdf_url": "http://arxiv.org/pdf/2412.08972v1",
      "published_date": "2024-12-12 06:08:46 UTC",
      "updated_date": "2024-12-12 06:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:55:19.038009"
    },
    {
      "arxiv_id": "2412.08965v1",
      "title": "AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection",
      "title_zh": "AFFAKT：一种基于层次最优传输的方法，用于视频欺骗检测中的情感面部知识转移",
      "authors": [
        "Zihan Ji",
        "Xuetao Tian",
        "Ye Liu"
      ],
      "abstract": "The scarcity of high-quality large-scale labeled datasets poses a huge\nchallenge for employing deep learning models in video deception detection. To\naddress this issue, inspired by the psychological theory on the relation\nbetween deception and expressions, we propose a novel method called AFFAKT in\nthis paper, which enhances the classification performance by transferring\nuseful and correlated knowledge from a large facial expression dataset. Two key\nchallenges in knowledge transfer arise: 1) \\textit{how much} knowledge of\nfacial expression data should be transferred and 2) \\textit{how to} effectively\nleverage transferred knowledge for the deception classification model during\ninference. Specifically, the optimal relation mapping between facial expression\nclasses and deception samples is firstly quantified using proposed H-OTKT\nmodule and then transfers knowledge from the facial expression dataset to\ndeception samples. Moreover, a correlation prototype within another proposed\nmodule SRKB is well designed to retain the invariant correlations between\nfacial expression classes and deception classes through momentum updating.\nDuring inference, the transferred knowledge is fine-tuned with the correlation\nprototype using a sample-specific re-weighting strategy. Experimental results\non two deception detection datasets demonstrate the superior performance of our\nproposed method. The interpretability study reveals high associations between\ndeception and negative affections, which coincides with the theory in\npsychology.",
      "tldr_zh": "本研究提出AFFAKT方法，利用分层最优传输(Hierarchical Optimal Transport)技术，从大型面部表情数据集转移相关知识，以解决视频欺骗检测中数据稀缺的挑战。方法包括H-OTKT模块量化表情类与欺骗样本的最优关系映射，以及SRKB模块通过动量更新保留两者之间的不变相关性，并在推理阶段采用样本特定的再加权策略微调知识。实验在两个欺骗检测数据集上显示，该方法显著提升了分类性能，并通过解释性分析证实欺骗与负面情感高度相关，符合心理学理论。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.08965v1",
      "published_date": "2024-12-12 05:57:59 UTC",
      "updated_date": "2024-12-12 05:57:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:55:31.682863"
    },
    {
      "arxiv_id": "2412.08950v3",
      "title": "Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning",
      "title_zh": "利用全球规模遥测数据和联邦学习预测视频游戏体验质量",
      "authors": [
        "Zhongyang Zhang",
        "Jinhe Wen",
        "Zixi Chen",
        "Dara Arbab",
        "Sruti Sahani",
        "Kent Giard",
        "Bijan Arbab",
        "Haojian Jin",
        "Tauhidur Rahman"
      ],
      "abstract": "Frames Per Second (FPS) significantly affects the gaming experience.\nProviding players with accurate FPS estimates prior to purchase benefits both\nplayers and game developers. However, we have a limited understanding of how to\npredict a game's FPS performance on a specific device. In this paper, we first\nconduct a comprehensive analysis of a wide range of factors that may affect\ngame FPS on a global-scale dataset to identify the determinants of FPS. This\nincludes player-side and game-side characteristics, as well as country-level\nsocio-economic statistics. Furthermore, recognizing that accurate FPS\npredictions require extensive user data, which raises privacy concerns, we\npropose a federated learning-based model to ensure user privacy. Each player\nand game is assigned a unique learnable knowledge kernel that gradually\nextracts latent features for improved accuracy. We also introduce a novel\ntraining and prediction scheme that allows these kernels to be dynamically\nplug-and-play, effectively addressing cold start issues. To train this model\nwith minimal bias, we collected a large telemetry dataset from 224 countries\nand regions, 100,000 users, and 835 games. Our model achieved a mean\nWasserstein distance of 0.469 between predicted and ground truth FPS\ndistributions, outperforming all baseline methods.",
      "tldr_zh": "该论文分析了影响游戏帧率（FPS）的多种因素，包括玩家端、游戏端特征以及国家层面的社会经济统计数据，以预测游戏在特定设备上的性能。研究提出了一种基于Federated Learning的模型，通过为每个玩家和游戏分配独特的知识内核（knowledge kernel）来提取潜在特征，并引入动态插拔训练方案，解决隐私问题和冷启动挑战。利用全球规模数据集（覆盖224个国家和地区、10万用户、835个游戏），该模型的预测FPS分布与真实分布的平均Wasserstein距离为0.469，显著优于基线方法。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages, 11 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.08950v3",
      "published_date": "2024-12-12 05:28:34 UTC",
      "updated_date": "2025-02-26 16:23:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:57:43.875492"
    },
    {
      "arxiv_id": "2412.15236v2",
      "title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model",
      "title_zh": "CareBot：一个开创性的全过程开源医疗语言模型",
      "authors": [
        "Lulu Zhao",
        "Weihao Zeng",
        "Xiaofeng Shi",
        "Hua Zhou"
      ],
      "abstract": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional domains such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. In this paper, we propose CareBot, a bilingual medical LLM,\nwhich leverages a comprehensive approach integrating continuous pre-training\n(CPT), supervised fine-tuning (SFT), and reinforcement learning with human\nfeedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and\nBoost CPT, effectively bridges the gap between general and domain-specific\ndata, facilitating a smooth transition from pre-training to fine-tuning and\nenhancing domain knowledge progressively. We also introduce DataRater, a model\ndesigned to assess data quality during CPT, ensuring that the training data is\nboth accurate and relevant. For SFT, we develope a large and diverse bilingual\ndataset, along with ConFilter, a metric to enhance multi-turn dialogue quality,\nwhich is crucial to improving the model's ability to handle more complex\ndialogues. The combination of high-quality data sources and innovative\ntechniques significantly improves CareBot's performance across a range of\nmedical applications. Our rigorous evaluations on Chinese and English\nbenchmarks confirm CareBot's effectiveness in medical consultation and\neducation. These advancements not only address current limitations in medical\nLLMs but also set a new standard for developing effective and reliable\nopen-source models in the medical domain. We will open-source the datasets and\nmodels later, contributing valuable resources to the research community.",
      "tldr_zh": "这篇论文介绍了 CareBot，一种开创性的开源双语医疗 LLM，旨在提升模型在复杂医疗领域的性能。CareBot 通过整合连续预训练 (CPT)、监督微调 (SFT) 和强化学习与人类反馈 (RLHF) 的综合方法，其中包括两阶段 CPT（Stable CPT 和 Boost CPT）来桥接一般数据与领域特定知识，并引入 DataRater 评估数据质量以及 ConFilter 优化多轮对话。实验评估显示，CareBot 在中文和英文医疗基准上表现出色，尤其在咨询和教育应用中超越基线模型，并计划开源数据集和模型，为医疗 LLM 发展设定新标准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accept by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.15236v2",
      "published_date": "2024-12-12 05:27:43 UTC",
      "updated_date": "2024-12-23 02:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:57:55.488618"
    },
    {
      "arxiv_id": "2412.08947v1",
      "title": "Selective Visual Prompting in Vision Mamba",
      "title_zh": "翻译失败",
      "authors": [
        "Yifeng Yao",
        "Zichen Liu",
        "Zhenyu Cui",
        "Yuxin Peng",
        "Jiahuan Zhou"
      ],
      "abstract": "Pre-trained Vision Mamba (Vim) models have demonstrated exceptional\nperformance across various computer vision tasks in a computationally efficient\nmanner, attributed to their unique design of selective state space models. To\nfurther extend their applicability to diverse downstream vision tasks, Vim\nmodels can be adapted using the efficient fine-tuning technique known as visual\nprompting. However, existing visual prompting methods are predominantly\ntailored for Vision Transformer (ViT)-based models that leverage global\nattention, neglecting the distinctive sequential token-wise compression and\npropagation characteristics of Vim. Specifically, existing prompt tokens\nprefixed to the sequence are insufficient to effectively activate the input and\nforget gates across the entire sequence, hindering the extraction and\npropagation of discriminative information. To address this limitation, we\nintroduce a novel Selective Visual Prompting (SVP) method specifically for the\nefficient fine-tuning of Vim. To prevent the loss of discriminative information\nduring state space propagation, SVP employs lightweight selective prompters for\ntoken-wise prompt generation, ensuring adaptive activation of the update and\nforget gates within Mamba blocks to promote discriminative information\npropagation. Moreover, considering that Vim propagates both shared cross-layer\ninformation and specific inner-layer information, we further refine SVP with a\ndual-path structure: Cross-Prompting and Inner-Prompting. Cross-Prompting\nutilizes shared parameters across layers, while Inner-Prompting employs\ndistinct parameters, promoting the propagation of both shared and specific\ninformation, respectively. Extensive experimental results on various\nlarge-scale benchmarks demonstrate that our proposed SVP significantly\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-SVP.",
      "tldr_zh": "本研究针对预训练的 Vision Mamba (Vim) 模型在计算机视觉任务中的高效性能，提出了一种新型 Selective Visual Prompting (SVP) 方法，以适应其独特的选择性状态空间模型特性。现有视觉提示方法主要针对 Vision Transformer (ViT) 设计，无法有效激活 Vim 中的输入和遗忘门，导致判别信息提取与传播受限；SVP 通过轻量级 token-wise 提示生成和双路径结构（Cross-Prompting 跨层共享参数及 Inner-Prompting 层内特定参数）来优化 Mamba 块中的更新与遗忘门，促进信息传播。实验结果显示，SVP 在多种大规模基准上显著优于现有方法，提升了 Vim 模型的微调效率和性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2412.08947v1",
      "published_date": "2024-12-12 05:24:06 UTC",
      "updated_date": "2024-12-12 05:24:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:58:07.917861"
    },
    {
      "arxiv_id": "2412.08946v1",
      "title": "MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lulu Zhao",
        "Weihao Zeng",
        "Xiaofeng Shi",
        "Hua Zhou"
      ],
      "abstract": "Recently, LoRA has emerged as a crucial technique for fine-tuning large\npre-trained models, yet its performance in multi-task learning scenarios often\nfalls short. In contrast, the MoE architecture presents a natural solution to\nthis issue. However, it introduces challenges such as mutual interference of\ndata across multiple domains and knowledge forgetting of various tasks.\nAdditionally, MoE significantly increases the number of parameters, posing a\ncomputational cost challenge. Therefore, in this paper, we propose MoSLD, a\nmixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these\nchallenges by sharing the upper projection matrix in LoRA among different\nexperts, encouraging the model to learn general knowledge across tasks, while\nstill allowing the lower projection matrix to focus on the unique features of\neach task. The application of dropout alleviates the imbalanced update of\nparameter matrix and mitigates parameter overfitting in LoRA. Extensive\nexperiments demonstrate that our model exhibits excellent performance in both\nsingle-task and multi-task scenarios, with robust out-of-domain generalization\ncapabilities.",
      "tldr_zh": "该论文针对 LoRA 在多任务学习中的性能不足问题，提出了 MoSLD，一种高度参数高效的 Mixture-of-Shared LoRAs 模型，结合了 dropout 策略来缓解参数矩阵的不平衡更新和过拟合风险。MoSLD 通过共享 LoRA 中的上投影矩阵来学习跨任务的通用知识，同时让下投影矩阵专注于每个任务的独特特征，从而避免了 MoE 架构带来的参数膨胀和知识干扰。实验结果表明，MoSLD 在单任务和多任务场景中表现出色，并具备强大的域外泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accept by COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.08946v1",
      "published_date": "2024-12-12 05:22:49 UTC",
      "updated_date": "2024-12-12 05:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:58:18.922725"
    },
    {
      "arxiv_id": "2412.08920v2",
      "title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning",
      "title_zh": "从文本到轨迹：探索安全强化学习中复杂约束的表示和分解",
      "authors": [
        "Pusen Dong",
        "Tianchen Zhu",
        "Yue Qiu",
        "Haoyi Zhou",
        "Jianxin Li"
      ],
      "abstract": "Safe reinforcement learning (RL) requires the agent to finish a given task\nwhile obeying specific constraints. Giving constraints in natural language form\nhas great potential for practical scenarios due to its flexible transfer\ncapability and accessibility. Previous safe RL methods with natural language\nconstraints typically need to design cost functions manually for each\nconstraint, which requires domain expertise and lacks flexibility. In this\npaper, we harness the dual role of text in this task, using it not only to\nprovide constraint but also as a training signal. We introduce the\nTrajectory-level Textual Constraints Translator (TTCT) to replace the manually\ndesigned cost function. Our empirical results demonstrate that TTCT effectively\ncomprehends textual constraint and trajectory, and the policies trained by TTCT\ncan achieve a lower violation rate than the standard cost function. Extra\nstudies are conducted to demonstrate that the TTCT has zero-shot transfer\ncapability to adapt to constraint-shift environments.",
      "tldr_zh": "本研究探讨了在 Safe Reinforcement Learning 中使用自然语言表示复杂约束的问题，旨在解决传统方法需手动设计成本函数的灵活性不足。作者提出了 Trajectory-level Textual Constraints Translator (TTCT)，该框架利用文本作为约束和训练信号，自动翻译文本约束为轨迹级别的指导，从而提升代理的约束遵守能力。实验结果表明，TTCT 训练的策略比标准成本函数降低了违反率，并在额外研究中展示了零样本转移能力，能适应约束变化的环境。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.08920v2",
      "published_date": "2024-12-12 04:06:54 UTC",
      "updated_date": "2025-02-21 08:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:58:31.111209"
    },
    {
      "arxiv_id": "2412.08911v3",
      "title": "Goal-Conditioned Supervised Learning for Multi-Objective Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Shijun Li",
        "Hilaf Hasson",
        "Jing Hu",
        "Joydeep Ghosh"
      ],
      "abstract": "Multi-objective learning endeavors to concurrently optimize multiple\nobjectives using a single model, aiming to achieve high and balanced\nperformance across diverse objectives. However, this often entails a more\ncomplex optimization problem, particularly when navigating potential conflicts\nbetween objectives, leading to solutions with higher memory requirements and\ncomputational complexity. This paper introduces a Multi-Objective\nGoal-Conditioned Supervised Learning (MOGCSL) framework for automatically\nlearning to achieve multiple objectives from offline sequential data. MOGCSL\nextends the conventional GCSL method to multi-objective scenarios by redefining\ngoals from one-dimensional scalars to multi-dimensional vectors. It benefits\nfrom naturally eliminating the need for complex architectures and optimization\nconstraints. Moreover, MOGCSL effectively filters out uninformative or noisy\ninstances that fail to achieve desirable long-term rewards across multiple\nobjectives. We also introduces a novel goal-selection algorithm for MOGCSL to\nmodel and identify \"high\" achievable goals for inference.\n  While MOGCSL is quite general, we focus on its application to the next action\nprediction problem in commercial-grade recommender systems. In this context,\nany viable solution needs to be reasonably scalable and also be robust to large\namounts of noisy data that is characteristic of this application space. We show\nthat MOGCSL performs admirably on both counts by extensive experiments on\nreal-world recommendation datasets. Also, analysis and experiments are included\nto explain its strength in discounting the noisier portions of training data in\nrecommender systems with multiple objectives.",
      "tldr_zh": "本研究提出了一种Multi-Objective Goal-Conditioned Supervised Learning (MOGCSL)框架，用于从离线顺序数据中自动学习多个目标，实现高平衡性能，同时避免复杂优化问题带来的内存和计算开销。MOGCSL将传统GCSL方法扩展，将目标从一维标量重新定义为多维向量，并引入一个新的goal-selection算法来过滤noisy实例并识别高可实现目标，从而简化架构并提升鲁棒性。在商业推荐系统的下一个动作预测应用中，实验显示MOGCSL在真实数据集上表现出色，具有良好的可扩展性和对noisy数据的抵抗力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08911v3",
      "published_date": "2024-12-12 03:47:40 UTC",
      "updated_date": "2025-05-14 22:08:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:58:43.085991"
    },
    {
      "arxiv_id": "2412.08905v1",
      "title": "Phi-4 Technical Report",
      "title_zh": "Phi-4 技术报告",
      "authors": [
        "Marah Abdin",
        "Jyoti Aneja",
        "Harkirat Behl",
        "Sébastien Bubeck",
        "Ronen Eldan",
        "Suriya Gunasekar",
        "Michael Harrison",
        "Russell J. Hewett",
        "Mojan Javaheripi",
        "Piero Kauffmann",
        "James R. Lee",
        "Yin Tat Lee",
        "Yuanzhi Li",
        "Weishung Liu",
        "Caio C. T. Mendes",
        "Anh Nguyen",
        "Eric Price",
        "Gustavo de Rosa",
        "Olli Saarikivi",
        "Adil Salim",
        "Shital Shah",
        "Xin Wang",
        "Rachel Ward",
        "Yue Wu",
        "Dingli Yu",
        "Cyril Zhang",
        "Yi Zhang"
      ],
      "abstract": "We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.",
      "tldr_zh": "我们介绍了 phi-4，这是一个 14 亿参数的语言模型，其训练策略以数据质量为核心，并战略性地融入合成数据贯穿整个过程。不同于传统的知识蒸馏方法，phi-4 通过创新的数据生成和后训练技术，超越了其教师模型 GPT-4 在 STEM 相关 QA 能力的表现。尽管基于 phi-3 的架构变化最小，phi-4 凭借改进的训练课程和后训练方案，在推理基准上实现了强劲的性能提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08905v1",
      "published_date": "2024-12-12 03:37:41 UTC",
      "updated_date": "2024-12-12 03:37:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:58:54.881178"
    },
    {
      "arxiv_id": "2412.08901v2",
      "title": "Radiology Report Generation via Multi-objective Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Xiao",
        "Lei Shi",
        "Peng Liu",
        "Zhe Wang",
        "Chenjia Bai"
      ],
      "abstract": "Automatic Radiology Report Generation (RRG) is an important topic for\nalleviating the substantial workload of radiologists. Existing RRG approaches\nrely on supervised regression based on different architectures or additional\nknowledge injection,while the generated report may not align optimally with\nradiologists' preferences. Especially, since the preferences of radiologists\nare inherently heterogeneous and multidimensional, e.g., some may prioritize\nreport fluency, while others emphasize clinical accuracy. To address this\nproblem,we propose a new RRG method via Multi-objective Preference Optimization\n(MPO) to align the pre-trained RRG model with multiple human preferences, which\ncan be formulated by multi-dimensional reward functions and optimized by\nmulti-objective reinforcement learning (RL). Specifically, we use a preference\nvector to represent the weight of preferences and use it as a condition for the\nRRG model. Then, a linearly weighed reward is obtained via a dot product\nbetween the preference vector and multi-dimensional reward. Next,the RRG model\nis optimized to align with the preference vector by optimizing such a reward\nvia RL. In the training stage,we randomly sample diverse preference vectors\nfrom the preference space and align the model by optimizing the weighted\nmulti-objective rewards, which leads to an optimal policy on the entire\npreference space. When inference,our model can generate reports aligned with\nspecific preferences without further fine-tuning. Extensive experiments on two\npublic datasets show the proposed method can generate reports that cater to\ndifferent preferences in a single model and achieve state-of-the-art\nperformance.",
      "tldr_zh": "本文提出了一种基于 Multi-objective Preference Optimization (MPO) 的放射学报告生成 (RRG) 方法，以解决现有方法无法有效对齐放射科医生异质多维偏好的问题，例如报告流畅性与临床准确性的权衡。方法通过偏好向量表示偏好的权重，并结合多目标强化学习 (RL) 优化预训练模型，使其在训练阶段随机采样偏好向量并最大化加权奖励，从而在整个偏好空间实现最优策略。实验在两个公共数据集上表明，该方法能在单个模型中生成适应特定偏好的报告，并达到最先进性能，无需额外微调。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.08901v2",
      "published_date": "2024-12-12 03:25:13 UTC",
      "updated_date": "2024-12-13 02:55:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:59:08.559147"
    },
    {
      "arxiv_id": "2412.08900v1",
      "title": "AI-assisted Knowledge Discovery in Biomedical Literature to Support Decision-making in Precision Oncology",
      "title_zh": "翻译失败",
      "authors": [
        "Ting He",
        "Kory Kreimeyer",
        "Mimi Najjar",
        "Jonathan Spiker",
        "Maria Fatteh",
        "Valsamo Anagnostou",
        "Taxiarchis Botsis"
      ],
      "abstract": "The delivery of appropriate targeted therapies to cancer patients requires\nthe complete analysis of the molecular profiling of tumors and the patient's\nclinical characteristics in the context of existing knowledge and recent\nfindings described in biomedical literature and several other sources. We\nevaluated the potential contributions of specific natural language processing\nsolutions to support knowledge discovery from biomedical literature. Two models\nfrom the Bidirectional Encoder Representations from Transformers (BERT) family,\ntwo Large Language Models, and PubTator 3.0 were tested for their ability to\nsupport the named entity recognition (NER) and the relation extraction (RE)\ntasks. PubTator 3.0 and the BioBERT model performed best in the NER task (best\nF1-score equal to 0.93 and 0.89, respectively), while BioBERT outperformed all\nother solutions in the RE task (best F1-score 0.79) and a specific use case it\nwas applied to by recognizing nearly all entity mentions and most of the\nrelations.",
      "tldr_zh": "本研究探讨了 AI 辅助从生物医学文献中发现知识，以支持精准肿瘤学中的决策制定，强调分析肿瘤分子特征和患者临床特征的重要性。研究评估了多种自然语言处理解决方案，包括 Bidirectional Encoder Representations from Transformers (BERT) 家族的两个模型、Large Language Models (LLMs) 和 PubTator 3.0，在命名实体识别 (NER) 和关系提取 (RE) 任务中的性能。结果显示，PubTator 3.0 和 BioBERT 在 NER 任务中表现最佳，F1-score 分别为 0.93 和 0.89，而 BioBERT 在 RE 任务中领先，F1-score 为 0.79，并在特定用例中识别了几乎所有实体和大多数关系。这些发现有助于提升生物医学知识发现的准确性，从而改善癌症患者治疗决策。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AMIA Annual Symposium 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.08900v1",
      "published_date": "2024-12-12 03:24:49 UTC",
      "updated_date": "2024-12-12 03:24:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:59:21.183248"
    },
    {
      "arxiv_id": "2412.08897v2",
      "title": "Neural Interactive Proofs",
      "title_zh": "翻译失败",
      "authors": [
        "Lewis Hammond",
        "Sam Adam-Day"
      ],
      "abstract": "We consider the problem of how a trusted, but computationally bounded agent\n(a 'verifier') can learn to interact with one or more powerful but untrusted\nagents ('provers') in order to solve a given task. More specifically, we study\nthe case in which agents are represented using neural networks and refer to\nsolutions of this problem as neural interactive proofs. First we introduce a\nunifying framework based on prover-verifier games, which generalises previously\nproposed interaction protocols. We then describe several new protocols for\ngenerating neural interactive proofs, and provide a theoretical comparison of\nboth new and existing approaches. Finally, we support this theory with\nexperiments in two domains: a toy graph isomorphism problem that illustrates\nthe key ideas, and a code validation task using large language models. In so\ndoing, we aim to create a foundation for future work on neural interactive\nproofs and their application in building safer AI systems.",
      "tldr_zh": "该研究探讨了如何让一个可信任但计算能力有限的代理（verifier）通过与强大但不可信任的代理（provers）互动来完成任务，提出了一种名为neural interactive proofs的框架，其中代理使用neural networks表示。论文引入了一个基于prover-verifier games的统一框架，以概括和扩展现有的交互协议，并描述了几个新的协议来生成neural interactive proofs，同时提供了新旧方法的理论比较。实验在玩具图同构问题和大型语言模型的代码验证任务中进行了验证，展示了该框架的有效性。最终，该工作为构建更安全的AI系统奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR'25 camera-ready version; 51 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.08897v2",
      "published_date": "2024-12-12 03:21:53 UTC",
      "updated_date": "2025-03-17 17:16:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:59:30.664977"
    },
    {
      "arxiv_id": "2412.08894v2",
      "title": "SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Kwangryeol Park",
        "Seulki Lee"
      ],
      "abstract": "We propose SMMF (Square-Matricized Momentum Factorization), a\nmemory-efficient optimizer that reduces the memory requirement of the widely\nused adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF\nenables flexible and efficient factorization of an arbitrary rank (shape) of\nthe first and second momentum tensors during optimization, based on the\nproposed square-matricization and one-time single matrix factorization. From\nthis, it becomes effectively applicable to any rank (shape) of momentum\ntensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep\nmodel architectures, such as CNNs (high rank) and Transformers (low rank), in\ncontrast to existing memory-efficient optimizers that applies only to a\nparticular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret\nbound analysis of SMMF, which shows that it converges similarly to\nnon-memory-efficient adaptive learning rate optimizers, such as AdamNC,\nproviding a theoretical basis for its competitive optimization capability. In\nour experiment, SMMF takes up to 96% less memory compared to state-of-the-art\nmemory efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving\ncomparable model performance on various CNN and Transformer tasks.",
      "tldr_zh": "该论文提出 SMMF（Square-Matricized Momentum Factorization），一种内存高效优化器，能将自适应学习率优化器如 Adam 的内存需求减少高达 96%。SMMF 通过 square-matricization 和一次单矩阵分解，实现对任意秩动量张量的灵活分解，适用于各种深度模型中的偏置、矩阵和高秩张量，如 CNNs 和 Transformers。实验结果显示，SMMF 比现有优化器如 Adafactor、CAME 和 SM3 节省高达 96% 内存，同时在多种任务上实现与非内存高效优化器（如 AdamNC）相当的性能，并通过 regret bound 分析证明其收敛性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08894v2",
      "published_date": "2024-12-12 03:14:50 UTC",
      "updated_date": "2024-12-13 04:03:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:59:45.214565"
    },
    {
      "arxiv_id": "2412.08893v1",
      "title": "Efficient Reinforcement Learning for Optimal Control with Natural Images",
      "title_zh": "翻译失败",
      "authors": [
        "Peter N. Loxley"
      ],
      "abstract": "Reinforcement learning solves optimal control and sequential decision\nproblems widely found in control systems engineering, robotics, and artificial\nintelligence. This work investigates optimal control over a sequence of natural\nimages. The problem is formalized, and general conditions are derived for an\nimage to be sufficient for implementing an optimal policy. Reinforcement\nlearning is shown to be efficient only for certain types of image\nrepresentations. This is demonstrated by developing a reinforcement learning\nbenchmark that scales easily with number of states and length of horizon, and\nhas optimal policies that are easily distinguished from suboptimal policies.\nImage representations given by overcomplete sparse codes are found to be\ncomputationally efficient for optimal control, using fewer computational\nresources to learn and evaluate optimal policies. For natural images of fixed\nsize, representing each image as an overcomplete sparse code in a linear\nnetwork is shown to increase network storage capacity by orders of magnitude\nbeyond that possible for any complete code, allowing larger tasks with many\nmore states to be solved. Sparse codes can be generated by devices with low\nenergy requirements and low computational overhead.",
      "tldr_zh": "该研究探讨了强化学习(Reinforcement Learning)在处理自然图像序列的优化控制(Optimal Control)问题上如何实现高效。论文形式化了问题，导出了图像表示的充分条件，并开发了一个可扩展基准，证明只有特定图像表示（如过完备稀疏编码）才能使强化学习计算高效。实验结果显示，过完备稀疏编码使用更少的计算资源来学习和评估最优策略，并将网络存储容量提高几个数量级，从而支持更大规模的任务；此外，这种编码可由低能耗设备生成。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08893v1",
      "published_date": "2024-12-12 03:14:47 UTC",
      "updated_date": "2024-12-12 03:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T11:59:55.945994"
    },
    {
      "arxiv_id": "2501.10367v1",
      "title": "GTDE: Grouped Training with Decentralized Execution for Multi-agent Actor-Critic",
      "title_zh": "GTDE：用于多智能体 Actor-Critic 的分组训练与去中心",
      "authors": [
        "Mengxian Li",
        "Qi Wang",
        "Yongjun Xu"
      ],
      "abstract": "The rapid advancement of multi-agent reinforcement learning (MARL) has given\nrise to diverse training paradigms to learn the policies of each agent in the\nmulti-agent system. The paradigms of decentralized training and execution\n(DTDE) and centralized training with decentralized execution (CTDE) have been\nproposed and widely applied. However, as the number of agents increases, the\ninherent limitations of these frameworks significantly degrade the performance\nmetrics, such as win rate, total reward, etc. To reduce the influence of the\nincreasing number of agents on the performance metrics, we propose a novel\ntraining paradigm of grouped training decentralized execution (GTDE). This\nframework eliminates the need for a centralized module and relies solely on\nlocal information, effectively meeting the training requirements of large-scale\nmulti-agent systems. Specifically, we first introduce an adaptive grouping\nmodule, which divides each agent into different groups based on their\nobservation history. To implement end-to-end training, GTDE uses Gumbel-Sigmoid\nfor efficient point-to-point sampling on the grouping distribution while\nensuring gradient backpropagation. To adapt to the uncertainty in the number of\nmembers in a group, two methods are used to implement a group information\naggregation module that merges member information within the group. Empirical\nresults show that in a cooperative environment with 495 agents, GTDE increased\nthe total reward by an average of 382\\% compared to the baseline. In a\ncompetitive environment with 64 agents, GTDE achieved a 100\\% win rate against\nthe baseline.",
      "tldr_zh": "该论文提出了一种新的多智能体强化学习(MARL)训练范式，名为GTDE（Grouped Training with Decentralized Execution），旨在解决传统DTDE和CTDE框架在代理数量增加时导致性能下降的问题，如获胜率和总奖励的降低。GTDE通过自适应分组模块根据代理的观察历史动态分组代理，并使用Gumbel-Sigmoid进行端到端训练，同时引入分组信息聚合模块来处理组内成员不确定性，从而实现完全去中心化的训练和执行。实验结果显示，在包含495个代理的合作环境中，GTDE的总奖励比基线提高了382%；在64个代理的竞争环境中，实现了100%的获胜率。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10367v1",
      "published_date": "2024-12-12 03:01:36 UTC",
      "updated_date": "2024-12-12 03:01:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:00:08.282890"
    },
    {
      "arxiv_id": "2412.08885v1",
      "title": "Residual Channel Boosts Contrastive Learning for Radio Frequency Fingerprint Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Pan",
        "Hui Chen",
        "Guanxiong Shen",
        "Hongyang Chen"
      ],
      "abstract": "In order to address the issue of limited data samples for the deployment of\npre-trained models in unseen environments, this paper proposes a residual\nchannel-based data augmentation strategy for Radio Frequency Fingerprint\nIdentification (RFFI), coupled with a lightweight SimSiam contrastive learning\nframework. By applying least square (LS) and minimum mean square error (MMSE)\nchannel estimations followed by equalization, signals with different residual\nchannel effects are generated. These residual channels enable the model to\nlearn more effective representations. Then the pre-trained model is fine-tuned\nwith 1% samples in a novel environment for RFFI. Experimental results\ndemonstrate that our method significantly enhances both feature extraction\nability and generalization while requiring fewer samples and less time, making\nit suitable for practical wireless security applications.",
      "tldr_zh": "这篇论文针对Radio Frequency Fingerprint Identification (RFFI)中数据样本有限的问题，提出了一种基于residual channel的增强策略，并结合轻量级的SimSiam对比学习框架。通过应用least square (LS)和minimum mean square error (MMSE)通道估计后进行均衡，生成不同residual channel效果的信号，以提升模型的表示学习能力。实验结果表明，该方法显著提高了特征提取和泛化性能，仅需1%的样本即可在新环境中微调模型，并减少了处理时间，适用于实际无线安全应用。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "5 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.08885v1",
      "published_date": "2024-12-12 02:48:20 UTC",
      "updated_date": "2024-12-12 02:48:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:00:21.007035"
    },
    {
      "arxiv_id": "2412.08873v1",
      "title": "Towards modeling evolving longitudinal health trajectories with a transformer-based deep learning model",
      "title_zh": "翻译失败",
      "authors": [
        "Hans Moen",
        "Vishnu Raj",
        "Andrius Vabalas",
        "Markus Perola",
        "Samuel Kaski",
        "Andrea Ganna",
        "Pekka Marttinen"
      ],
      "abstract": "Health registers contain rich information about individuals' health\nhistories. Here our interest lies in understanding how individuals' health\ntrajectories evolve in a nationwide longitudinal dataset with coded features,\nsuch as clinical codes, procedures, and drug purchases. We introduce a\nstraightforward approach for training a Transformer-based deep learning model\nin a way that lets us analyze how individuals' trajectories change over time.\nThis is achieved by modifying the training objective and by applying a causal\nattention mask. We focus here on a general task of predicting the onset of a\nrange of common diseases in a given future forecast interval. However, instead\nof providing a single prediction about diagnoses that could occur in this\nforecast interval, our approach enable the model to provide continuous\npredictions at every time point up until, and conditioned on, the time of the\nforecast period. We find that this model performs comparably to other models,\nincluding a bi-directional transformer model, in terms of basic prediction\nperformance while at the same time offering promising trajectory modeling\nproperties. We explore a couple of ways to use this model for analyzing health\ntrajectories and aiding in early detection of events that forecast possible\nlater disease onsets. We hypothesize that this method may be helpful in\ncontinuous monitoring of peoples' health trajectories and enabling\ninterventions in ongoing health trajectories, as well as being useful in\nretrospective analyses.",
      "tldr_zh": "该研究旨在利用 Transformer-based 深度学习模型分析个体健康轨迹的演变，基于全国性纵向数据集（包括临床代码、程序和药物购买等特征）。他们通过修改训练目标和应用 causal attention mask，使模型能够提供预测期内的连续预测，而不是单一诊断预测，从而实现对常见疾病发作的动态监测。实验结果显示，该模型在预测性能上与双向 Transformer 模型相当，同时具备更好的轨迹建模特性，有助于早期事件检测、健康轨迹持续监控和干预。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08873v1",
      "published_date": "2024-12-12 02:13:53 UTC",
      "updated_date": "2024-12-12 02:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:00:32.339702"
    },
    {
      "arxiv_id": "2412.12169v1",
      "title": "Regulation of Language Models With Interpretability Will Likely Result In A Performance Trade-Off",
      "title_zh": "通过可解释性对语言模型的监管可能导致性能权衡",
      "authors": [
        "Eoin M. Kenny",
        "Julie A. Shah"
      ],
      "abstract": "Regulation is increasingly cited as the most important and pressing concern\nin machine learning. However, it is currently unknown how to implement this,\nand perhaps more importantly, how it would effect model performance alongside\nhuman collaboration if actually realized. In this paper, we attempt to answer\nthese questions by building a regulatable large-language model (LLM), and then\nquantifying how the additional constraints involved affect (1) model\nperformance, alongside (2) human collaboration. Our empirical results reveal\nthat it is possible to force an LLM to use human-defined features in a\ntransparent way, but a \"regulation performance trade-off\" previously not\nconsidered reveals itself in the form of a 7.34% classification performance\ndrop. Surprisingly however, we show that despite this, such systems actually\nimprove human task performance speed and appropriate confidence in a realistic\ndeployment setting compared to no AI assistance, thus paving a way for fair,\nregulatable AI, which benefits users.",
      "tldr_zh": "本论文探讨了通过可解释性对大型语言模型（LLM）进行监管可能导致的性能权衡，强调监管在机器学习中的重要性。研究者构建了一个可监管的LLM，强制其使用人类定义的特征以实现透明决策，并量化了其对模型性能和人类协作的影响。结果显示，这种监管导致了7.34%的分类性能下降，但意外地提高了人类任务执行的速度和信心，为公平、可监管的AI系统提供了可行的路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12169v1",
      "published_date": "2024-12-12 02:11:06 UTC",
      "updated_date": "2024-12-12 02:11:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:00:44.289393"
    },
    {
      "arxiv_id": "2412.09656v1",
      "title": "From Noise to Nuance: Advances in Deep Generative Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Benji Peng",
        "Chia Xin Liang",
        "Ziqian Bi",
        "Ming Liu",
        "Yichao Zhang",
        "Tianyang Wang",
        "Keyu Chen",
        "Xinyuan Song",
        "Pohsun Feng"
      ],
      "abstract": "Deep learning-based image generation has undergone a paradigm shift since\n2021, marked by fundamental architectural breakthroughs and computational\ninnovations. Through reviewing architectural innovations and empirical results,\nthis paper analyzes the transition from traditional generative methods to\nadvanced architectures, with focus on compute-efficient diffusion models and\nvision transformer architectures. We examine how recent developments in Stable\nDiffusion, DALL-E, and consistency models have redefined the capabilities and\nperformance boundaries of image synthesis, while addressing persistent\nchallenges in efficiency and quality. Our analysis focuses on the evolution of\nlatent space representations, cross-attention mechanisms, and\nparameter-efficient training methodologies that enable accelerated inference\nunder resource constraints. While more efficient training methods enable faster\ninference, advanced control mechanisms like ControlNet and regional attention\nsystems have simultaneously improved generation precision and content\ncustomization. We investigate how enhanced multi-modal understanding and\nzero-shot generation capabilities are reshaping practical applications across\nindustries. Our analysis demonstrates that despite remarkable advances in\ngeneration quality and computational efficiency, critical challenges remain in\ndeveloping resource-conscious architectures and interpretable generation\nsystems for industrial applications. The paper concludes by mapping promising\nresearch directions, including neural architecture optimization and explainable\ngeneration frameworks.",
      "tldr_zh": "这篇论文回顾了自2021年以来深度学习图像生成领域的重大进展，特别是从传统生成方法向计算高效的diffusion models和vision transformer architectures的转变。作者分析了Stable Diffusion、DALL-E和consistency models等模型如何提升图像合成质量、效率和内容定制能力，并通过latent space representations、cross-attention mechanisms以及参数高效训练方法实现加速推理和多模态理解。尽管这些创新推动了零-shot generation在实际应用中的潜力，但论文强调了资源约束和可解释性挑战，并提出了neural architecture optimization和explainable generation frameworks等未来研究方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09656v1",
      "published_date": "2024-12-12 02:09:04 UTC",
      "updated_date": "2024-12-12 02:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:00:57.495648"
    },
    {
      "arxiv_id": "2412.08871v1",
      "title": "Inference-Time Diffusion Model Distillation",
      "title_zh": "推理时扩散模型蒸馏",
      "authors": [
        "Geon Yeong Park",
        "Sang Wan Lee",
        "Jong Chul Ye"
      ],
      "abstract": "Diffusion distillation models effectively accelerate reverse sampling by\ncompressing the process into fewer steps. However, these models still exhibit a\nperformance gap compared to their pre-trained diffusion model counterparts,\nexacerbated by distribution shifts and accumulated errors during multi-step\nsampling. To address this, we introduce Distillation++, a novel inference-time\ndistillation framework that reduces this gap by incorporating teacher-guided\nrefinement during sampling. Inspired by recent advances in conditional\nsampling, our approach recasts student model sampling as a proximal\noptimization problem with a score distillation sampling loss (SDS). To this\nend, we integrate distillation optimization during reverse sampling, which can\nbe viewed as teacher guidance that drives student sampling trajectory towards\nthe clean manifold using pre-trained diffusion models. Thus, Distillation++\nimproves the denoising process in real-time without additional source data or\nfine-tuning. Distillation++ demonstrates substantial improvements over\nstate-of-the-art distillation baselines, particularly in early sampling stages,\npositioning itself as a robust guided sampling process crafted for diffusion\ndistillation models. Code:\nhttps://github.com/geonyeong-park/inference_distillation.",
      "tldr_zh": "本研究针对扩散模型蒸馏（Diffusion Distillation）中存在的性能差距（如分布偏移和多步采样错误），提出了一种新型推理时框架Distillation++。该框架将学生模型采样重新表述为一个近端优化问题，并通过Score Distillation Sampling Loss (SDS)整合教师指导的优化过程，从而在反向采样中实时引导学生轨迹向干净的流形靠拢。Distillation++无需额外数据或微调，即可显著提升去噪效果，尤其在早期采样阶段比现有基准改善明显，为扩散模型应用提供了一个鲁棒的引导采样方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code: https://github.com/geonyeong-park/inference_distillation",
      "pdf_url": "http://arxiv.org/pdf/2412.08871v1",
      "published_date": "2024-12-12 02:07:17 UTC",
      "updated_date": "2024-12-12 02:07:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:01:09.245238"
    },
    {
      "arxiv_id": "2412.08862v1",
      "title": "Key Safety Design Overview in AI-driven Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Vikas Vyas",
        "Zheyuan Xu"
      ],
      "abstract": "With the increasing presence of autonomous SAE level 3 and level 4, which\nincorporate artificial intelligence software, along with the complex technical\nchallenges they present, it is essential to maintain a high level of functional\nsafety and robust software design. This paper explores the necessary safety\narchitecture and systematic approach for automotive software and hardware,\nincluding fail soft handling of automotive safety integrity level (ASIL) D\n(highest level of safety integrity), integration of artificial intelligence\n(AI), and machine learning (ML) in automotive safety architecture. By\naddressing the unique challenges presented by increasing AI-based automotive\nsoftware, we proposed various techniques, such as mitigation strategies and\nsafety failure analysis, to ensure the safety and reliability of automotive\nsoftware, as well as the role of AI in software reliability throughout the data\nlifecycle.\n  Index Terms Safety Design, Automotive Software, Performance Evaluation,\nAdvanced Driver Assistance Systems (ADAS) Applications, Automotive Software\nSystems, Electronic Control Units.",
      "tldr_zh": "这篇论文概述了在 SAE level 3 和 level 4 自治车辆中，确保功能安全和稳健软件设计的关键方法。作者探讨了安全架构、ASIL D 故障处理、AI 和 ML 的整合，并提出了缓解策略和安全故障分析等技术，以应对 AI 驱动软件的独特挑战。实验结果显示，这些方法提升了汽车软件的可靠性和安全性，并在数据生命周期中强化了 AI 的作用，为 Advanced Driver Assistance Systems (ADAS) 应用提供了重要指导。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08862v1",
      "published_date": "2024-12-12 01:48:45 UTC",
      "updated_date": "2024-12-12 01:48:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:01:19.958705"
    },
    {
      "arxiv_id": "2412.12168v1",
      "title": "A Decomposition Modeling Framework for Seasonal Time-Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Yining Pang",
        "Chenghan Li"
      ],
      "abstract": "Seasonal time series exhibit intricate long-term dependencies, posing a\nsignificant challenge for accurate future prediction. This paper introduces the\nMulti-scale Seasonal Decomposition Model (MSSD) for seasonal time-series\nforecasting. Initially, leveraging the inherent periodicity of seasonal time\nseries, we decompose the univariate time series into three primary components:\nAscending, Peak, and Descending. This decomposition approach enhances the\ncapture of periodic features. By addressing the limitations of existing\ntime-series modeling methods, particularly in modeling the Peak component, this\nresearch proposes a multi-scale network structure designed to effectively\ncapture various potential peak fluctuation patterns in the Peak component. This\nstudy integrates Conv2d and Temporal Convolutional Networks to concurrently\ncapture global and local features. Furthermore, we incorporate multi-scale\nreshaping to augment the modeling capacity for peak fluctuation patterns. The\nproposed methodology undergoes validation using three publicly accessible\nseasonal datasets. Notably, in both short-term and long-term fore-casting\ntasks, our approach exhibits a 10$\\%$ reduction in error compared to the\nbaseline models.",
      "tldr_zh": "本研究提出了一种分解建模框架Multi-scale Seasonal Decomposition Model (MSSD)，用于处理季节性时间序列预测中的复杂长期依赖问题。首先，将单变量时间序列分解成Ascending（上升）、Peak（峰值）和Descending（下降）三个主要组件，以更好地捕捉周期性特征。针对Peak组件的建模局限，该框架采用多尺度网络结构，包括Conv2d和Temporal Convolutional Networks（TCN），同时捕捉全局和局部特征，并通过多尺度重塑增强对峰值波动模式的建模能力。在三个公开季节性数据集上的实验验证中，该方法在短期和长期预测任务中比基线模型错误率降低了10%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12168v1",
      "published_date": "2024-12-12 01:37:25 UTC",
      "updated_date": "2024-12-12 01:37:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:01:32.946221"
    },
    {
      "arxiv_id": "2412.15235v1",
      "title": "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models",
      "title_zh": "OG-RAG：本体基础的检索增强生成，用于大型语言模型",
      "authors": [
        "Kartik Sharma",
        "Peeyush Kumar",
        "Yunqing Li"
      ],
      "abstract": "This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented\nGeneration method designed to enhance LLM-generated responses by anchoring\nretrieval processes in domain-specific ontologies. While LLMs are widely used\nfor tasks like question answering and search, they struggle to adapt to\nspecialized knowledge, such as industrial workflows or knowledge work, without\nexpensive fine-tuning or sub-optimal retrieval methods. Existing\nretrieval-augmented models, such as RAG, offer improvements but fail to account\nfor structured domain knowledge, leading to suboptimal context generation.\nOntologies, which conceptually organize domain knowledge by defining entities\nand their interrelationships, offer a structured representation to address this\ngap. OG-RAG constructs a hypergraph representation of domain documents, where\neach hyperedge encapsulates clusters of factual knowledge grounded using\ndomain-specific ontology. An optimization algorithm then retrieves the minimal\nset of hyperedges that constructs a precise, conceptually grounded context for\nthe LLM. This method enables efficient retrieval while preserving the complex\nrelationships between entities. OG-RAG applies to domains where fact-based\nreasoning is essential, particularly in tasks that require workflows or\ndecision-making steps to follow predefined rules and procedures. These include\nindustrial workflows in healthcare, legal, and agricultural sectors, as well as\nknowledge-driven tasks such as news journalism, investigative research,\nconsulting and more. Our evaluations demonstrate that OG-RAG increases the\nrecall of accurate facts by 55% and improves response correctness by 40% across\nfour different LLMs. Additionally, OG-RAG enables 30% faster attribution of\nresponses to context and boosts fact-based reasoning accuracy by 27% compared\nto baseline methods.",
      "tldr_zh": "本文提出 OG-RAG，一种基于本体论 (ontologies) 的检索增强生成 (Retrieval-Augmented Generation) 方法，用于提升大型语言模型 (LLMs) 在处理专业领域知识时的响应质量。该方法通过构建超图 (hypergraph) 表示来组织领域文档，每个超边封装基于本体论的知识集群，并采用优化算法检索最小知识集，以生成精确且结构化的上下文。相比传统 RAG 模型，OG-RAG 更好地捕捉实体间复杂关系，适用于事实推理密集的任务，如医疗、法律和农业等行业工作流程。实验评估显示，OG-RAG 在四种 LLMs 上提高了事实召回率 55%、响应正确性 40%，并提升了 30% 的响应归因速度和 27% 的事实推理准确性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.0; I.2.4"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.15235v1",
      "published_date": "2024-12-12 01:21:03 UTC",
      "updated_date": "2024-12-12 01:21:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:01:45.835919"
    },
    {
      "arxiv_id": "2412.08851v1",
      "title": "Quantum Kernel-Based Long Short-term Memory for Climate Time-Series Forecasting",
      "title_zh": "基于量子核的长短时记忆网络用于气候时间序列预测",
      "authors": [
        "Yu-Chao Hsu",
        "Nan-Yow Chen",
        "Tai-Yu Li",
        "Po-Heng",
        "Lee",
        "Kuan-Cheng Chen"
      ],
      "abstract": "We present the Quantum Kernel-Based Long short-memory (QK-LSTM) network,\nwhich integrates quantum kernel methods into classical LSTM architectures to\nenhance predictive accuracy and computational efficiency in climate time-series\nforecasting tasks, such as Air Quality Index (AQI) prediction. By embedding\nclassical inputs into high-dimensional quantum feature spaces, QK-LSTM captures\nintricate nonlinear dependencies and temporal dynamics with fewer trainable\nparameters. Leveraging quantum kernel methods allows for efficient computation\nof inner products in quantum spaces, addressing the computational challenges\nfaced by classical models and variational quantum circuit-based models.\nDesigned for the Noisy Intermediate-Scale Quantum (NISQ) era, QK-LSTM supports\nscalable hybrid quantum-classical implementations. Experimental results\ndemonstrate that QK-LSTM outperforms classical LSTM networks in AQI\nforecasting, showcasing its potential for environmental monitoring and\nresource-constrained scenarios, while highlighting the broader applicability of\nquantum-enhanced machine learning frameworks in tackling large-scale,\nhigh-dimensional climate datasets.",
      "tldr_zh": "本研究提出了一种量子核增强的长期短期记忆网络（QK-LSTM），将量子核方法集成到经典LSTM架构中，以提升气候时间序列预测的准确性和计算效率，例如空气质量指数（AQI）预测。QK-LSTM通过将经典输入嵌入高维量子特征空间，捕捉复杂的非线性依赖和时间动态，同时减少训练参数，并利用量子核方法高效计算量子空间内积，解决传统模型和变分量子电路的计算挑战。实验结果显示，QK-LSTM在AQI预测中优于经典LSTM模型，适用于Noisy Intermediate-Scale Quantum（NISQ）时代的混合量子-经典实现，并展示了其在环境监测和资源受限场景中的潜力，以及量子增强机器学习框架处理大规模高维气候数据集的广泛适用性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "arXiv admin note: text overlap with arXiv:2411.13225",
      "pdf_url": "http://arxiv.org/pdf/2412.08851v1",
      "published_date": "2024-12-12 01:16:52 UTC",
      "updated_date": "2024-12-12 01:16:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:01:57.302245"
    },
    {
      "arxiv_id": "2412.08849v1",
      "title": "Labits: Layered Bidirectional Time Surfaces Representation for Event Camera-based Continuous Dense Trajectory Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongyang Zhang",
        "Jiacheng Qiu",
        "Shuyang Cui",
        "Yijun Luo",
        "Tauhidur Rahman"
      ],
      "abstract": "Event cameras provide a compelling alternative to traditional frame-based\nsensors, capturing dynamic scenes with high temporal resolution and low\nlatency. Moving objects trigger events with precise timestamps along their\ntrajectory, enabling smooth continuous-time estimation. However, few works have\nattempted to optimize the information loss during event representation\nconstruction, imposing a ceiling on this task. Fully exploiting event cameras\nrequires representations that simultaneously preserve fine-grained temporal\ninformation, stable and characteristic 2D visual features, and temporally\nconsistent information density, an unmet challenge in existing representations.\nWe introduce Labits: Layered Bidirectional Time Surfaces, a simple yet elegant\nrepresentation designed to retain all these features. Additionally, we propose\na dedicated module for extracting active pixel local optical flow (APLOF),\nsignificantly boosting the performance. Our approach achieves an impressive 49%\nreduction in trajectory end-point error (TEPE) compared to the previous\nstate-of-the-art on the MultiFlow dataset. The code will be released upon\nacceptance.",
      "tldr_zh": "该研究针对事件相机（event cameras）在动态场景捕获中的优势，提出了 Labits（Layered Bidirectional Time Surfaces）表示方法，以解决现有方法在事件表示过程中信息损失的问题。该方法能同时保留细粒度时间信息、稳定2D视觉特征以及时间一致性信息密度，并结合一个专用模块提取活跃像素局部光流（APLOF），显著提升连续稠密轨迹估计（Continuous Dense Trajectory Estimation）的性能。在 MultiFlow 数据集上的实验显示，Labits 比之前最先进方法降低了49%的轨迹端点错误（TEPE）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "24 pages, 12 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.08849v1",
      "published_date": "2024-12-12 01:11:50 UTC",
      "updated_date": "2024-12-12 01:11:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:02:09.797900"
    },
    {
      "arxiv_id": "2412.08846v1",
      "title": "Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology",
      "title_zh": "探索大型语言模型在跨文化价值观方面的表现及其与训练方法的相关性",
      "authors": [
        "Minsang Kim",
        "Seungjun Baek"
      ],
      "abstract": "Large language models (LLMs) closely interact with humans, and thus need an\nintimate understanding of the cultural values of human society. In this paper,\nwe explore how open-source LLMs make judgments on diverse categories of\ncultural values across countries, and its relation to training methodology such\nas model sizes, training corpus, alignment, etc. Our analysis shows that LLMs\ncan judge socio-cultural norms similar to humans but less so on social systems\nand progress. In addition, LLMs tend to judge cultural values biased toward\nWestern culture, which can be improved with training on the multilingual\ncorpus. We also find that increasing model size helps a better understanding of\nsocial values, but smaller models can be enhanced by using synthetic data. Our\nanalysis reveals valuable insights into the design methodology of LLMs in\nconnection with their understanding of cultural values.",
      "tldr_zh": "本文探讨大型语言模型 (LLMs) 在跨文化价值观判断方面的表现及其与训练方法的关系，包括模型大小、训练语料和校准等因素。研究发现，LLMs 能类似人类判断社会文化规范，但对社会系统和进步的理解较弱，且倾向于偏向西方文化，通过使用多语言语料库可显著改善这一偏差。增加模型大小有助于提升对社会价值观的理解，而较小模型则可通过合成数据进行增强。该分析为优化 LLMs 的设计方法提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08846v1",
      "published_date": "2024-12-12 00:52:11 UTC",
      "updated_date": "2024-12-12 00:52:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:02:21.420822"
    },
    {
      "arxiv_id": "2412.08845v1",
      "title": "Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning",
      "title_zh": "基于量子训练的分布式多智能体强化学习",
      "authors": [
        "Kuan-Cheng Chen",
        "Samuel Yen-Chi Chen",
        "Chen-Yu Liu",
        "Kin K. Leung"
      ],
      "abstract": "In this paper, we introduce Quantum-Train-Based Distributed Multi-Agent\nReinforcement Learning (Dist-QTRL), a novel approach to addressing the\nscalability challenges of traditional Reinforcement Learning (RL) by\nintegrating quantum computing principles. Quantum-Train Reinforcement Learning\n(QTRL) leverages parameterized quantum circuits to efficiently generate neural\nnetwork parameters, achieving a \\(poly(\\log(N))\\) reduction in the\ndimensionality of trainable parameters while harnessing quantum entanglement\nfor superior data representation. The framework is designed for distributed\nmulti-agent environments, where multiple agents, modeled as Quantum Processing\nUnits (QPUs), operate in parallel, enabling faster convergence and enhanced\nscalability. Additionally, the Dist-QTRL framework can be extended to\nhigh-performance computing (HPC) environments by utilizing distributed quantum\ntraining for parameter reduction in classical neural networks, followed by\ninference using classical CPUs or GPUs. This hybrid quantum-HPC approach allows\nfor further optimization in real-world applications. In this paper, we provide\na mathematical formulation of the Dist-QTRL framework and explore its\nconvergence properties, supported by empirical results demonstrating\nperformance improvements over centric QTRL models. The results highlight the\npotential of quantum-enhanced RL in tackling complex, high-dimensional tasks,\nparticularly in distributed computing settings, where our framework achieves\nsignificant speedups through parallelization without compromising model\naccuracy. This work paves the way for scalable, quantum-enhanced RL systems in\npractical applications, leveraging both quantum and classical computational\nresources.",
      "tldr_zh": "本研究提出了一种新型框架Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning (Dist-QTRL)，通过整合量子计算原理来解决传统Reinforcement Learning (RL) 的可扩展性挑战。Dist-QTRL 利用Quantum-Train Reinforcement Learning (QTRL) 的参数化量子电路生成神经网络参数，实现参数维度降低至 \\(poly(\\log(N))\\) 并借助量子纠缠提升数据表示，同时在分布式多智能体环境中以Quantum Processing Units (QPUs) 建模代理，实现并行操作和更快收敛。实验结果显示，该框架在分布式计算设置中比中心化QTRL 模型表现出显著性能改进和加速，同时保持模型准确性，为实际应用中的量子增强RL 系统铺平道路，特别是结合High-Performance Computing (HPC) 环境的混合方法。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08845v1",
      "published_date": "2024-12-12 00:51:41 UTC",
      "updated_date": "2024-12-12 00:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:02:33.820996"
    },
    {
      "arxiv_id": "2412.08842v1",
      "title": "Kajal: Extracting Grammar of a Source Code Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Jalili Torkamani"
      ],
      "abstract": "Understanding and extracting the grammar of a domain-specific language (DSL)\nis crucial for various software engineering tasks; however, manually creating\nthese grammars is time-intensive and error-prone. This paper presents Kajal, a\nnovel approach that automatically infers grammar from DSL code snippets by\nleveraging Large Language Models (LLMs) through prompt engineering and few-shot\nlearning. Kajal dynamically constructs input prompts, using contextual\ninformation to guide the LLM in generating the corresponding grammars, which\nare iteratively refined through a feedback-driven approach. Our experiments\nshow that Kajal achieves 60% accuracy with few-shot learning and 45% without\nit, demonstrating the significant impact of few-shot learning on the tool's\neffectiveness. This approach offers a promising solution for automating DSL\ngrammar extraction, and future work will explore using smaller, open-source\nLLMs and testing on larger datasets to further validate Kajal's performance.",
      "tldr_zh": "这篇论文介绍了 Kajal，一种创新方法，利用 Large Language Models (LLMs) 通过提示工程和 Few-Shot Learning，从 Domain-Specific Language (DSL) 代码片段中自动推断语法，以解决手动创建语法耗时且易出错的问题。Kajal 通过动态构建输入提示、利用上下文信息引导 LLM 生成语法，并采用反馈驱动的迭代优化过程来提升准确性。实验结果显示，该方法在启用 Few-Shot Learning 时准确率达到 60%，而未启用时为 45%，突显了 Few-Shot Learning 的重要作用。该方法为自动化 DSL 语法提取提供了高效解决方案，并计划未来探索更小的开源 LLMs 和更大数据集以进一步验证其性能。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2; D.3; F.4.2; I.2.5"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages, 6 figures, 1 table, preprint",
      "pdf_url": "http://arxiv.org/pdf/2412.08842v1",
      "published_date": "2024-12-12 00:40:54 UTC",
      "updated_date": "2024-12-12 00:40:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:02:46.341172"
    },
    {
      "arxiv_id": "2412.08841v2",
      "title": "Structural Entropy Guided Probabilistic Coding",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Huang",
        "Hao Peng",
        "Li Sun",
        "Hui Lin",
        "Chunyang Liu",
        "Jiang Cao",
        "Philip S. Yu"
      ],
      "abstract": "Probabilistic embeddings have several advantages over deterministic\nembeddings as they map each data point to a distribution, which better\ndescribes the uncertainty and complexity of data. Many works focus on adjusting\nthe distribution constraint under the Information Bottleneck (IB) principle to\nenhance representation learning. However, these proposed regularization terms\nonly consider the constraint of each latent variable, omitting the structural\ninformation between latent variables. In this paper, we propose a novel\nstructural entropy-guided probabilistic coding model, named SEPC. Specifically,\nwe incorporate the relationship between latent variables into the optimization\nby proposing a structural entropy regularization loss. Besides, as traditional\nstructural information theory is not well-suited for regression tasks, we\npropose a probabilistic encoding tree, transferring regression tasks to\nclassification tasks while diminishing the influence of the transformation.\nExperimental results across 12 natural language understanding tasks, including\nboth classification and regression tasks, demonstrate the superior performance\nof SEPC compared to other state-of-the-art models in terms of effectiveness,\ngeneralization capability, and robustness to label noise. The codes and\ndatasets are available at https://github.com/SELGroup/SEPC.",
      "tldr_zh": "该论文提出了一种新型的结构熵引导概率编码模型（SEPC），旨在通过引入结构熵正则化损失来优化潜在变量之间的结构信息，从而提升概率嵌入在表示学习中的性能。不同于传统方法，SEPC 还设计了概率编码树，将回归任务转化为分类任务，减少转换影响。实验结果显示，在 12 个自然语言理解任务（包括分类和回归）上，SEPC 比其他最先进模型在有效性、泛化能力和对标签噪声的鲁棒性方面表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper is accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.08841v2",
      "published_date": "2024-12-12 00:37:53 UTC",
      "updated_date": "2024-12-13 12:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:02:57.293615"
    },
    {
      "arxiv_id": "2412.08832v1",
      "title": "HadaCore: Tensor Core Accelerated Hadamard Transform Kernel",
      "title_zh": "HadaCore：Tensor Core 加速的 Hadamard 变换内核",
      "authors": [
        "Krish Agarwal",
        "Rishi Astra",
        "Adnan Hoque",
        "Mudhakar Srivatsa",
        "Raghu Ganti",
        "Less Wright",
        "Sijia Chen"
      ],
      "abstract": "We present HadaCore, a modified Fast Walsh-Hadamard Transform (FWHT)\nalgorithm optimized for the Tensor Cores present in modern GPU hardware.\nHadaCore follows the recursive structure of the original FWHT algorithm,\nachieving the same asymptotic runtime complexity but leveraging a\nhardware-aware work decomposition that benefits from Tensor Core acceleration.\nThis reduces bottlenecks from compute and data exchange. On Nvidia A100 and\nH100 GPUs, HadaCore achieves speedups of 1.1-1.4x and 1.0-1.3x, with a peak\ngain of 3.5x and 3.6x respectively, when compared to the existing\nstate-of-the-art implementation of the original algorithm. We also show that\nwhen using FP16 or BF16, our implementation is numerically accurate, enabling\ncomparable accuracy on MMLU benchmarks when used in an end-to-end Llama3\ninference run with quantized (FP8) attention.",
      "tldr_zh": "我们提出了 HadaCore，这是一种针对现代 GPU Tensor Cores 优化的 Fast Walsh-Hadamard Transform (FWHT) 算法修改版。它保留了原 FWHT 的递归结构和渐近运行时复杂度，但通过硬件感知的工作分解来利用 Tensor Core 加速，减少计算和数据交换瓶颈。在 Nvidia A100 和 H100 GPU 上，HadaCore 相对于现有最佳实现实现了1.1-1.4x 和1.0-1.3x 的速度提升，峰值分别达3.5x 和3.6x；此外，使用 FP16 或 BF16 时，该实现保持了数值准确性，并在端到端的 Llama3 推理中（结合 FP8 量化注意力）于 MMLU 基准测试中表现出可比准确性。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.08832v1",
      "published_date": "2024-12-12 00:12:43 UTC",
      "updated_date": "2024-12-12 00:12:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:03:10.856023"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 127,
  "processed_papers_count": 127,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T12:03:28.623815"
}
[
  {
    "arxiv_id": "2412.09764v2",
    "title": "Memory Layers at Scale",
    "authors": [
      "Vincent-Pierre Berges",
      "Barlas Oğuz",
      "Daniel Haziza",
      "Wen-tau Yih",
      "Luke Zettlemoyer",
      "Gargi Ghosh"
    ],
    "abstract": "Memory layers use a trainable key-value lookup mechanism to add extra\nparameters to a model without increasing FLOPs. Conceptually, sparsely\nactivated memory layers complement compute-heavy dense feed-forward layers,\nproviding dedicated capacity to store and retrieve information cheaply. This\nwork takes memory layers beyond proof-of-concept, proving their utility at\ncontemporary scale. On downstream tasks, language models augmented with our\nimproved memory layer outperform dense models with more than twice the\ncomputation budget, as well as mixture-of-expert models when matched for both\ncompute and parameters. We find gains are especially pronounced for factual\ntasks. We provide a fully parallelizable memory layer implementation,\ndemonstrating scaling laws with up to 128B memory parameters, pretrained to 1\ntrillion tokens, comparing to base models with up to 8B parameters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09764v2",
    "published_date": "2024-12-12 23:56:57 UTC",
    "updated_date": "2024-12-20 17:36:52 UTC"
  },
  {
    "arxiv_id": "2412.14194v3",
    "title": "Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations",
    "authors": [
      "Xiaofan Mu",
      "Salman Seyedi",
      "Iris Zheng",
      "Zifan Jiang",
      "Liu Chen",
      "Bolaji Omofojoye",
      "Rachel Hershenberg",
      "Allan I. Levey",
      "Gari D. Clifford",
      "Hiroko H. Dodge",
      "Hyeokhyen Kwon"
    ],
    "abstract": "The aging society urgently requires scalable methods to monitor cognitive\ndecline and identify social and psychological factors indicative of dementia\nrisk in older adults. Our machine learning (ML) models captured facial,\nacoustic, linguistic, and cardiovascular features from 39 individuals with\nnormal cognition or Mild Cognitive Impairment derived from remote video\nconversations and classified cognitive status, social isolation, neuroticism,\nand psychological well-being. Our model could distinguish Clinical Dementia\nRating Scale (CDR) of 0.5 (vs. 0) with 0.78 area under the receiver operating\ncharacteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with\n0.71 AUC, and negative affect scales with 0.79 AUC. Recent advances in machine\nlearning offer new opportunities to remotely detect cognitive impairment and\nassess associated factors, such as neuroticism and psychological well-being.\nOur experiment showed that speech and language patterns were more useful for\nquantifying cognitive impairment, whereas facial expression and cardiovascular\npatterns using photoplethysmography (PPG) were more useful for quantifying\npersonality and psychological well-being.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14194v3",
    "published_date": "2024-12-12 23:42:46 UTC",
    "updated_date": "2025-01-09 20:16:41 UTC"
  },
  {
    "arxiv_id": "2412.10471v2",
    "title": "VCA: Video Curious Agent for Long Video Understanding",
    "authors": [
      "Zeyuan Yang",
      "Delin Chen",
      "Xueyang Yu",
      "Maohao Shen",
      "Chuang Gan"
    ],
    "abstract": "Long video understanding poses unique challenges due to their temporal\ncomplexity and low information density. Recent works address this task by\nsampling numerous frames or incorporating auxiliary tools using LLMs, both of\nwhich result in high computational costs. In this work, we introduce a\ncuriosity-driven video agent with self-exploration capability, dubbed as VCA.\nBuilt upon VLMs, VCA autonomously navigates video segments and efficiently\nbuilds a comprehensive understanding of complex video sequences. Instead of\ndirectly sampling frames, VCA employs a tree-search structure to explore video\nsegments and collect frames. Rather than relying on external feedback or\nreward, VCA leverages VLM's self-generated intrinsic reward to guide its\nexploration, enabling it to capture the most crucial information for reasoning.\nExperimental results on multiple long video benchmarks demonstrate our\napproach's superior effectiveness and efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10471v2",
    "published_date": "2024-12-12 23:39:54 UTC",
    "updated_date": "2025-03-10 03:35:16 UTC"
  },
  {
    "arxiv_id": "2412.09760v1",
    "title": "Congruence-based Learning of Probabilistic Deterministic Finite Automata",
    "authors": [
      "Matías Carrasco",
      "Franz Mayr",
      "Sergio Yovine"
    ],
    "abstract": "This work studies the question of learning probabilistic deterministic\nautomata from language models. For this purpose, it focuses on analyzing the\nrelations defined on algebraic structures over strings by equivalences and\nsimilarities on probability distributions. We introduce a congruence that\nextends the classical Myhill-Nerode congruence for formal languages. This new\ncongruence is the basis for defining regularity over language models. We\npresent an active learning algorithm that computes the quotient with respect to\nthis congruence whenever the language model is regular. The paper also defines\nthe notion of recognizability for language models and shows that it coincides\nwith regularity for congruences. For relations which are not congruences, it\nshows that this is not the case. Finally, it discusses the impact of this\nresult on learning in the context of language models.",
    "categories": [
      "cs.FL",
      "cs.AI"
    ],
    "primary_category": "cs.FL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09760v1",
    "published_date": "2024-12-12 23:38:58 UTC",
    "updated_date": "2024-12-12 23:38:58 UTC"
  },
  {
    "arxiv_id": "2412.09751v2",
    "title": "AI red-teaming is a sociotechnical challenge: on values, labor, and harms",
    "authors": [
      "Tarleton Gillespie",
      "Ryland Shaw",
      "Mary L. Gray",
      "Jina Suh"
    ],
    "abstract": "As generative AI technologies find more and more real-world applications, the\nimportance of testing their performance and safety seems paramount.\n\"Red-teaming\" has quickly become the primary approach to test AI\nmodels--prioritized by AI companies, and enshrined in AI policy and regulation.\nMembers of red teams act as adversaries, probing AI systems to test their\nsafety mechanisms and uncover vulnerabilities. Yet we know far too little about\nthis work or its implications. This essay calls for collaboration between\ncomputer scientists and social scientists to study the sociotechnical systems\nsurrounding AI technologies, including the work of red-teaming, to avoid\nrepeating the mistakes of the recent past. We highlight the importance of\nunderstanding the values and assumptions behind red-teaming, the labor\narrangements involved, and the psychological impacts on red-teamers, drawing\ninsights from the lessons learned around the work of content moderation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09751v2",
    "published_date": "2024-12-12 22:48:19 UTC",
    "updated_date": "2025-04-03 20:24:20 UTC"
  },
  {
    "arxiv_id": "2412.09741v1",
    "title": "On Round-Off Errors and Gaussian Blur in Superresolution and in Image Registration",
    "authors": [
      "Serap A. Savari"
    ],
    "abstract": "Superresolution theory and techniques seek to recover signals from samples in\nthe presence of blur and noise. Discrete image registration can be an approach\nto fuse information from different sets of samples of the same signal.\nQuantization errors in the spatial domain are inherent to digital images. We\nconsider superresolution and discrete image registration for one-dimensional\nspatially-limited piecewise constant functions which are subject to blur which\nis Gaussian or a mixture of Gaussians as well as to round-off errors. We\ndescribe a signal-dependent measurement matrix which captures both types of\neffects. For this setting we show that the difficulties in determining the\ndiscontinuity points from two sets of samples even in the absence of other\ntypes of noise. If the samples are also subject to statistical noise, then it\nis necessary to align and segment the data sequences to make the most effective\ninferences about the amplitudes and discontinuity points. Under some conditions\non the blur, the noise, and the distance between discontinuity points, we prove\nthat we can correctly align and determine the first samples following each\ndiscontinuity point in two data sequences with an approach based on dynamic\nprogramming.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09741v1",
    "published_date": "2024-12-12 22:08:53 UTC",
    "updated_date": "2024-12-12 22:08:53 UTC"
  },
  {
    "arxiv_id": "2412.09727v2",
    "title": "Let Curves Speak: A Continuous Glucose Monitor based Large Sensor Foundation Model for Diabetes Management",
    "authors": [
      "Junjie Luo",
      "Abhimanyu Kumbara",
      "Mansur Shomali",
      "Rui Han",
      "Anand Iyer",
      "Ritu Agarwal",
      "Gordon Gao"
    ],
    "abstract": "While previous studies of AI in diabetes management focus on long-term risk,\nresearch on near-future glucose prediction remains limited but important as it\nenables timely diabetes self-management. Integrating AI with continuous glucose\nmonitoring (CGM) holds promise for near-future glucose prediction. However,\nexisting models have limitations in capturing patterns of blood glucose\nfluctuations and demonstrate poor generalizability. A robust approach is needed\nto leverage massive CGM data for near-future glucose prediction. We propose\nlarge sensor models (LSMs) to capture knowledge in CGM data by modeling\npatients as sequences of glucose. CGM-LSM is pretrained on 15.96 million\nglucose records from 592 diabetes patients for near-future glucose prediction.\nWe evaluated CGM-LSM against state-of-the-art methods using the OhioT1DM\ndataset across various metrics, prediction horizons, and unseen patients.\nAdditionally, we assessed its generalizability across factors like diabetes\ntype, age, gender, and hour of day. CGM-LSM achieved exceptional performance,\nwith an rMSE of 29.81 mg/dL for type 1 diabetes patients and 23.49 mg/dL for\ntype 2 diabetes patients in a two-hour prediction horizon. For the OhioT1DM\ndataset, CGM-LSM achieved a one-hour rMSE of 15.64 mg/dL, halving the previous\nbest of 31.97 mg/dL. Robustness analyses revealed consistent performance not\nonly for unseen patients and future periods, but also across diabetes type,\nage, and gender. The model demonstrated adaptability to different hours of day,\nmaintaining accuracy across periods of various activity intensity levels.\nCGM-LSM represents a transformative step in diabetes management by leveraging\npretraining to uncover latent glucose generation patterns in sensor data. Our\nfindings also underscore the broader potential of LSMs to drive innovation\nacross domains involving complex sensor data.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09727v2",
    "published_date": "2024-12-12 21:35:13 UTC",
    "updated_date": "2024-12-18 03:07:48 UTC"
  },
  {
    "arxiv_id": "2501.03235v1",
    "title": "Neural networks consisting of DNA",
    "authors": [
      "Michael te Vrugt"
    ],
    "abstract": "Neural networks based on soft and biological matter constitute an interesting\npotential alternative to traditional implementations based on electric\ncircuits. DNA is a particularly promising system in this context due its\nnatural ability to store information. In recent years, researchers have started\nto construct neural networks that are based on DNA. In this chapter, I provide\na very basic introduction to the concept of DNA neural networks, aiming at an\naudience that is not familiar with biochemistry.",
    "categories": [
      "physics.bio-ph",
      "cond-mat.soft",
      "cs.AI",
      "cs.NE",
      "q-bio.BM",
      "q-bio.MN"
    ],
    "primary_category": "physics.bio-ph",
    "comment": "Book chapter, to appear in: Artificial Intelligence and Intelligent\n  Matter, Springer, Cham",
    "pdf_url": "http://arxiv.org/pdf/2501.03235v1",
    "published_date": "2024-12-12 21:33:25 UTC",
    "updated_date": "2024-12-12 21:33:25 UTC"
  },
  {
    "arxiv_id": "2412.09726v1",
    "title": "The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications",
    "authors": [
      "Binxu Wang",
      "John J. Vastola"
    ],
    "abstract": "By learning the gradient of smoothed data distributions, diffusion models can\niteratively generate samples from complex distributions. The learned score\nfunction enables their generalization capabilities, but how the learned score\nrelates to the score of the underlying data manifold remains largely unclear.\nHere, we aim to elucidate this relationship by comparing learned neural scores\nto the scores of two kinds of analytically tractable distributions: Gaussians\nand Gaussian mixtures. The simplicity of the Gaussian model makes it\ntheoretically attractive, and we show that it admits a closed-form solution and\npredicts many qualitative aspects of sample generation dynamics. We claim that\nthe learned neural score is dominated by its linear (Gaussian) approximation\nfor moderate to high noise scales, and supply both theoretical and empirical\narguments to support this claim. Moreover, the Gaussian approximation\nempirically works for a larger range of noise scales than naive theory suggests\nit should, and is preferentially learned early in training. At smaller noise\nscales, we observe that learned scores are better described by a coarse-grained\n(Gaussian mixture) approximation of training data than by the score of the\ntraining distribution, a finding consistent with generalization. Our findings\nenable us to precisely predict the initial phase of trained models' sampling\ntrajectories through their Gaussian approximations. We show that this allows\nthe skipping of the first 15-30% of sampling steps while maintaining high\nsample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10\nunconditional generation). This forms the foundation of a novel hybrid sampling\nmethod, termed analytical teleportation, which can seamlessly integrate with\nand accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our\nfindings suggest ways to improve the design and training of diffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T07, 60G15, 60J60, 62M40, 65C30",
      "I.2.6; I.5.1; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "69 pages, 34 figures. Published in TMLR. Previous shorter versions at\n  arxiv.org/abs/2303.02490 and arxiv.org/abs/2311.10892",
    "pdf_url": "http://arxiv.org/pdf/2412.09726v1",
    "published_date": "2024-12-12 21:31:27 UTC",
    "updated_date": "2024-12-12 21:31:27 UTC"
  },
  {
    "arxiv_id": "2412.12175v1",
    "title": "Explore Theory of Mind: Program-guided adversarial data generation for theory of mind reasoning",
    "authors": [
      "Melanie Sclar",
      "Jane Yu",
      "Maryam Fazel-Zarandi",
      "Yulia Tsvetkov",
      "Yonatan Bisk",
      "Yejin Choi",
      "Asli Celikyilmaz"
    ],
    "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and\nbenchmarks have been introduced to evaluate if current models have been able to\ndevelop this key ability of social intelligence. However, all rely on limited\ndatasets with simple patterns that can potentially lead to problematic blind\nspots in evaluation and an overestimation of model capabilities. We introduce\nExploreToM, the first framework to allow large-scale generation of diverse and\nchallenging theory of mind data for robust training and evaluation. Our\napproach leverages an A* search over a custom domain-specific language to\nproduce complex story structures and novel, diverse, yet plausible scenarios to\nstress test the limits of LLMs. Our evaluation reveals that state-of-the-art\nLLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on\nExploreToM-generated data, highlighting the need for more robust theory of mind\nevaluation. As our generations are a conceptual superset of prior work,\nfine-tuning on our data yields a 27-point accuracy improvement on the classic\nToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying\nskills and factors missing for models to show theory of mind, such as\nunreliable state tracking or data imbalances, which may contribute to models'\npoor performance on benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12175v1",
    "published_date": "2024-12-12 21:29:00 UTC",
    "updated_date": "2024-12-12 21:29:00 UTC"
  },
  {
    "arxiv_id": "2412.13212v1",
    "title": "An introduction to reservoir computing",
    "authors": [
      "Michael te Vrugt"
    ],
    "abstract": "There is a growing interest in the development of artificial neural networks\nthat are implemented in a physical system. A major challenge in this context is\nthat these networks are difficult to train since training here would require a\nchange of physical parameters rather than simply of coefficients in a computer\nprogram. For this reason, reservoir computing, where one employs\nhigh-dimensional recurrent networks and trains only the final layer, is widely\nused in this context. In this chapter, I introduce the basic concepts of\nreservoir computing. Moreover, I present some important physical\nimplementations coming from electronics, photonics, spintronics, mechanics, and\nbiology. Finally, I provide a brief discussion of quantum reservoir computing.",
    "categories": [
      "cs.ET",
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.app-ph",
      "quant-ph"
    ],
    "primary_category": "cs.ET",
    "comment": "Book chapter, to appear in: Artificial Intelligence and Intelligent\n  Matter, Springer, Cham",
    "pdf_url": "http://arxiv.org/pdf/2412.13212v1",
    "published_date": "2024-12-12 21:19:52 UTC",
    "updated_date": "2024-12-12 21:19:52 UTC"
  },
  {
    "arxiv_id": "2412.09719v2",
    "title": "TransferLight: Zero-Shot Traffic Signal Control on any Road-Network",
    "authors": [
      "Johann Schmidt",
      "Frank Dreyer",
      "Sayed Abid Hashimi",
      "Sebastian Stober"
    ],
    "abstract": "Traffic signal control plays a crucial role in urban mobility. However,\nexisting methods often struggle to generalize beyond their training\nenvironments to unseen scenarios with varying traffic dynamics. We present\nTransferLight, a novel framework designed for robust generalization across\nroad-networks, diverse traffic conditions and intersection geometries. At its\ncore, we propose a log-distance reward function, offering spatially-aware\nsignal prioritization while remaining adaptable to varied lane configurations -\novercoming the limitations of traditional pressure-based rewards. Our\nhierarchical, heterogeneous, and directed graph neural network architecture\neffectively captures granular traffic dynamics, enabling transferability to\narbitrary intersection layouts. Using a decentralized multi-agent approach,\nglobal rewards, and novel state transition priors, we develop a single,\nweight-tied policy that scales zero-shot to any road network without\nre-training. Through domain randomization during training, we additionally\nenhance generalization capabilities. Experimental results validate\nTransferLight's superior performance in unseen scenarios, advancing practical,\ngeneralizable intelligent transportation systems to meet evolving urban traffic\ndemands.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI Workshop Paper (MALTA)",
    "pdf_url": "http://arxiv.org/pdf/2412.09719v2",
    "published_date": "2024-12-12 20:52:12 UTC",
    "updated_date": "2024-12-23 20:13:20 UTC"
  },
  {
    "arxiv_id": "2412.09701v1",
    "title": "CUAL: Continual Uncertainty-aware Active Learner",
    "authors": [
      "Amanda Rios",
      "Ibrahima Ndiour",
      "Parual Datta",
      "Jerry Sydir",
      "Omesh Tickoo",
      "Nilesh Ahuja"
    ],
    "abstract": "AI deployed in many real-world use cases should be capable of adapting to\nnovelties encountered after deployment. Here, we consider a challenging,\nunder-explored and realistic continual adaptation problem: a deployed AI agent\nis continuously provided with unlabeled data that may contain not only unseen\nsamples of known classes but also samples from novel (unknown) classes. In such\na challenging setting, it has only a tiny labeling budget to query the most\ninformative samples to help it continuously learn. We present a comprehensive\nsolution to this complex problem with our model \"CUAL\" (Continual\nUncertainty-aware Active Learner). CUAL leverages an uncertainty estimation\nalgorithm to prioritize active labeling of ambiguous (uncertain) predicted\nnovel class samples while also simultaneously pseudo-labeling the most certain\npredictions of each class. Evaluations across multiple datasets, ablations,\nsettings and backbones (e.g. ViT foundation model) demonstrate our method's\neffectiveness. We will release our code upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09701v1",
    "published_date": "2024-12-12 19:49:09 UTC",
    "updated_date": "2024-12-12 19:49:09 UTC"
  },
  {
    "arxiv_id": "2501.10369v1",
    "title": "Creative Loss: Ambiguity, Uncertainty and Indeterminacy",
    "authors": [
      "Tom Holberton"
    ],
    "abstract": "This article evaluates how creative uses of machine learning can address\nthree adjacent terms: ambiguity, uncertainty and indeterminacy. Through the\nprogression of these concepts it reflects on increasing ambitions for machine\nlearning as a creative partner, illustrated with research from Unit 21 at the\nBartlett School of Architecture, UCL. Through indeterminacy are potential\nfuture approaches to machine learning and design.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "NeurIPS 2024 Creative AI Track",
    "pdf_url": "http://arxiv.org/pdf/2501.10369v1",
    "published_date": "2024-12-12 19:22:20 UTC",
    "updated_date": "2024-12-12 19:22:20 UTC"
  },
  {
    "arxiv_id": "2412.09627v1",
    "title": "Doe-1: Closed-Loop Autonomous Driving with Large World Model",
    "authors": [
      "Wenzhao Zheng",
      "Zetian Xia",
      "Yuanhui Huang",
      "Sicheng Zuo",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "End-to-end autonomous driving has received increasing attention due to its\npotential to learn from large amounts of data. However, most existing methods\nare still open-loop and suffer from weak scalability, lack of high-order\ninteractions, and inefficient decision-making. In this paper, we explore a\nclosed-loop framework for autonomous driving and propose a large Driving wOrld\nmodEl (Doe-1) for unified perception, prediction, and planning. We formulate\nautonomous driving as a next-token generation problem and use multi-modal\ntokens to accomplish different tasks. Specifically, we use free-form texts\n(i.e., scene descriptions) for perception and generate future predictions\ndirectly in the RGB space with image tokens. For planning, we employ a\nposition-aware tokenizer to effectively encode action into discrete tokens. We\ntrain a multi-modal transformer to autoregressively generate perception,\nprediction, and planning tokens in an end-to-end and unified manner.\nExperiments on the widely used nuScenes dataset demonstrate the effectiveness\nof Doe-1 in various tasks including visual question-answering,\naction-conditioned video generation, and motion planning. Code:\nhttps://github.com/wzzheng/Doe.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/wzzheng/Doe",
    "pdf_url": "http://arxiv.org/pdf/2412.09627v1",
    "published_date": "2024-12-12 18:59:59 UTC",
    "updated_date": "2024-12-12 18:59:59 UTC"
  },
  {
    "arxiv_id": "2412.09612v3",
    "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
    "authors": [
      "Yuanze Lin",
      "Yunsheng Li",
      "Dongdong Chen",
      "Weijian Xu",
      "Ronald Clark",
      "Philip H. S. Torr"
    ],
    "abstract": "We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025, Project webpage:\n  http://yuanze-lin.me/Olympus_page/",
    "pdf_url": "http://arxiv.org/pdf/2412.09612v3",
    "published_date": "2024-12-12 18:59:40 UTC",
    "updated_date": "2025-04-01 21:08:07 UTC"
  },
  {
    "arxiv_id": "2412.09602v2",
    "title": "Hidden Biases of End-to-End Driving Datasets",
    "authors": [
      "Julian Zimmerlin",
      "Jens Beißwenger",
      "Bernhard Jaeger",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "abstract": "End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)",
    "pdf_url": "http://arxiv.org/pdf/2412.09602v2",
    "published_date": "2024-12-12 18:59:13 UTC",
    "updated_date": "2024-12-13 09:51:22 UTC"
  },
  {
    "arxiv_id": "2412.09601v2",
    "title": "TimeRefine: Temporal Grounding with Time Refining Video LLM",
    "authors": [
      "Xizi Wang",
      "Feng Cheng",
      "Ziyang Wang",
      "Huiyu Wang",
      "Md Mohaiminul Islam",
      "Lorenzo Torresani",
      "Mohit Bansal",
      "Gedas Bertasius",
      "David Crandall"
    ],
    "abstract": "Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09601v2",
    "published_date": "2024-12-12 18:59:11 UTC",
    "updated_date": "2025-03-05 07:06:15 UTC"
  },
  {
    "arxiv_id": "2412.09600v1",
    "title": "Owl-1: Omni World Model for Consistent Long Video Generation",
    "authors": [
      "Yuanhui Huang",
      "Wenzhao Zheng",
      "Yuan Gao",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Video generation models (VGMs) have received extensive attention recently and\nserve as promising candidates for general-purpose large vision models. While\nthey can only generate short videos each time, existing methods achieve long\nvideo generation by iteratively calling the VGMs, using the last-frame output\nas the condition for the next-round generation. However, the last frame only\ncontains short-term fine-grained information about the scene, resulting in\ninconsistency in the long horizon. To address this, we propose an Omni World\nmodeL (Owl-1) to produce long-term coherent and comprehensive conditions for\nconsistent long video generation. As videos are observations of the underlying\nevolving world, we propose to model the long-term developments in a latent\nspace and use VGMs to film them into videos. Specifically, we represent the\nworld with a latent state variable which can be decoded into explicit video\nobservations. These observations serve as a basis for anticipating temporal\ndynamics which in turn update the state variable. The interaction between\nevolving dynamics and persistent state enhances the diversity and consistency\nof the long videos. Extensive experiments show that Owl-1 achieves comparable\nperformance with SOTA methods on VBench-I2V and VBench-Long, validating its\nability to generate high-quality video observations. Code:\nhttps://github.com/huang-yh/Owl.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/huang-yh/Owl",
    "pdf_url": "http://arxiv.org/pdf/2412.09600v1",
    "published_date": "2024-12-12 18:59:01 UTC",
    "updated_date": "2024-12-12 18:59:01 UTC"
  },
  {
    "arxiv_id": "2412.09596v1",
    "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
    "authors": [
      "Pan Zhang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Yuhang Zang",
      "Rui Qian",
      "Xilin Wei",
      "Lin Chen",
      "Yifei Li",
      "Junbo Niu",
      "Shuangrui Ding",
      "Qipeng Guo",
      "Haodong Duan",
      "Xin Chen",
      "Han Lv",
      "Zheng Nie",
      "Min Zhang",
      "Bin Wang",
      "Wenwei Zhang",
      "Xinyue Zhang",
      "Jiaye Ge",
      "Wei Li",
      "Jingwen Li",
      "Zhongying Tu",
      "Conghui He",
      "Xingcheng Zhang",
      "Kai Chen",
      "Yu Qiao",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "Creating AI systems that can interact with environments over long periods,\nsimilar to human cognition, has been a longstanding research goal. Recent\nadvancements in multimodal large language models (MLLMs) have made significant\nstrides in open-world understanding. However, the challenge of continuous and\nsimultaneous streaming perception, memory, and reasoning remains largely\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\narchitecture, which limits their ability to process inputs and generate\nresponses simultaneously, akin to being unable to think while perceiving.\nFurthermore, relying on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information becomes costly and\ninefficient. Therefore, rather than relying on a single foundation model to\nperform all functions, this project draws inspiration from the concept of the\nSpecialized Generalist AI and introduces disentangled streaming perception,\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\nProcesses multimodal information in real-time, storing key details in memory\nand triggering reasoning in response to user queries. (2) Multi-modal Long\nMemory Module: Integrates short-term and long-term memory, compressing\nshort-term memories into long-term ones for efficient retrieval and improved\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\ntasks, coordinating with the perception and memory modules. This project\nsimulates human-like cognition, enabling multimodal large language models to\nprovide continuous and adaptive service over time.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Github Repo:\n  https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive",
    "pdf_url": "http://arxiv.org/pdf/2412.09596v1",
    "published_date": "2024-12-12 18:58:30 UTC",
    "updated_date": "2024-12-12 18:58:30 UTC"
  },
  {
    "arxiv_id": "2412.09582v2",
    "title": "Neptune: The Long Orbit to Benchmarking Long Video Understanding",
    "authors": [
      "Arsha Nagrani",
      "Mingda Zhang",
      "Ramin Mehran",
      "Rachel Hornung",
      "Nitesh Bharadwaj Gundavarapu",
      "Nilpa Jha",
      "Austin Myers",
      "Xingyi Zhou",
      "Boqing Gong",
      "Cordelia Schmid",
      "Mikhail Sirotenko",
      "Yukun Zhu",
      "Tobias Weyand"
    ],
    "abstract": "We introduce Neptune, a benchmark for long video understanding that requires\nreasoning over long time horizons and across different modalities. Many\nexisting video datasets and models are focused on short clips (10s-30s). While\nsome long video datasets do exist, they can often be solved by powerful image\nmodels applied per frame (and often to very few frames) in a video, and are\nusually manually annotated at high cost. In order to mitigate both these\nproblems, we propose a scalable dataset creation pipeline which leverages large\nmodels (VLMs and LLMs), to automatically generate dense, time-aligned video\ncaptions, as well as tough question answer decoy sets for video segments (up to\n15 minutes in length). Our dataset Neptune covers a broad range of long video\nreasoning abilities and consists of a subset that emphasizes multimodal\nreasoning. Since existing metrics for open-ended question answering are either\nrule-based or may rely on proprietary models, we provide a new open source\nmodel-based metric GEM to score open-ended responses on Neptune. Benchmark\nevaluations reveal that most current open-source long video models perform\npoorly on Neptune, particularly on questions testing temporal ordering,\ncounting and state changes. Through Neptune, we aim to spur the development of\nmore advanced models capable of understanding long videos. The dataset is\navailable at https://github.com/google-deepmind/neptune",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09582v2",
    "published_date": "2024-12-12 18:54:48 UTC",
    "updated_date": "2025-01-18 00:52:42 UTC"
  },
  {
    "arxiv_id": "2412.09579v1",
    "title": "A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks",
    "authors": [
      "Saptarshi Mandal",
      "Xiaojun Lin",
      "R. Srikant"
    ],
    "abstract": "Knowledge distillation, where a small student model learns from a pre-trained\nlarge teacher model, has achieved substantial empirical success since the\nseminal work of \\citep{hinton2015distilling}. Despite prior theoretical studies\nexploring the benefits of knowledge distillation, an important question remains\nunanswered: why does soft-label training from the teacher require significantly\nfewer neurons than directly training a small neural network with hard labels?\nTo address this, we first present motivating experimental results using simple\nneural network models on a binary classification problem. These results\ndemonstrate that soft-label training consistently outperforms hard-label\ntraining in accuracy, with the performance gap becoming more pronounced as the\ndataset becomes increasingly difficult to classify. We then substantiate these\nobservations with a theoretical contribution based on two-layer neural network\nmodels. Specifically, we show that soft-label training using gradient descent\nrequires only $O\\left(\\frac{1}{\\gamma^2 \\epsilon}\\right)$ neurons to achieve a\nclassification loss averaged over epochs smaller than some $\\epsilon > 0$,\nwhere $\\gamma$ is the separation margin of the limiting kernel. In contrast,\nhard-label training requires $O\\left(\\frac{1}{\\gamma^4} \\cdot\n\\ln\\left(\\frac{1}{\\epsilon}\\right)\\right)$ neurons, as derived from an adapted\nversion of the gradient descent analysis in \\citep{ji2020polylogarithmic}. This\nimplies that when $\\gamma \\leq \\epsilon$, i.e., when the dataset is challenging\nto classify, the neuron requirement for soft-label training can be\nsignificantly lower than that for hard-label training. Finally, we present\nexperimental results on deep neural networks, further validating these\ntheoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T01"
    ],
    "primary_category": "cs.LG",
    "comment": "Main Body of the Paper is under Review at L4DC 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09579v1",
    "published_date": "2024-12-12 18:54:07 UTC",
    "updated_date": "2024-12-12 18:54:07 UTC"
  },
  {
    "arxiv_id": "2412.09578v1",
    "title": "DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial NEtworks and Semantic Topic classification",
    "authors": [
      "Caleb Stam",
      "Emily Saldanha",
      "Mahantesh Halappanavar",
      "Anurag Acharya"
    ],
    "abstract": "The emergence of the COVID-19 pandemic resulted in a significant rise in the\nspread of misinformation on online platforms such as Twitter. Oftentimes this\ngrowth is blamed on the idea of the \"echo chamber.\" However, the behavior said\nto characterize these echo chambers exists in two dimensions. The first is in a\nuser's social interactions, where they are said to stick with the same clique\nof like-minded users. The second is in the content of their posts, where they\nare said to repeatedly espouse homogeneous ideas. In this study, we link the\ntwo by using Twitter's network of retweets to study social interactions and\ntopic modeling to study tweet content. In order to measure the diversity of a\nuser's interactions over time, we develop a novel metric to track the speed at\nwhich they travel through the social network. The application of these analysis\nmethods to misinformation-focused data from the pandemic demonstrates\ncorrelation between social behavior and tweet content. We believe this\ncorrelation supports the common intuition about how antisocial users behave,\nand further suggests that it holds even in subcommunities already rife with\nmisinformation.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09578v1",
    "published_date": "2024-12-12 18:53:46 UTC",
    "updated_date": "2024-12-12 18:53:46 UTC"
  },
  {
    "arxiv_id": "2412.09569v1",
    "title": "JuStRank: Benchmarking LLM Judges for System Ranking",
    "authors": [
      "Ariel Gera",
      "Odellia Boni",
      "Yotam Perlitz",
      "Roy Bar-Haim",
      "Lilach Eden",
      "Asaf Yehudai"
    ],
    "abstract": "Given the rapid progress of generative AI, there is a pressing need to\nsystematically compare and choose between the numerous models and\nconfigurations available. The scale and versatility of such evaluations make\nthe use of LLM-based judges a compelling solution for this challenge.\nCrucially, this approach requires first to validate the quality of the LLM\njudge itself. Previous work has focused on instance-based assessment of LLM\njudges, where a judge is evaluated over a set of responses, or response pairs,\nwhile being agnostic to their source systems. We argue that this setting\noverlooks critical factors affecting system-level ranking, such as a judge's\npositive or negative bias towards certain systems. To address this gap, we\nconduct the first large-scale study of LLM judges as system rankers. System\nscores are generated by aggregating judgment scores over multiple system\noutputs, and the judge's quality is assessed by comparing the resulting system\nranking to a human-based ranking. Beyond overall judge assessment, our analysis\nprovides a fine-grained characterization of judge behavior, including their\ndecisiveness and bias.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09569v1",
    "published_date": "2024-12-12 18:51:13 UTC",
    "updated_date": "2024-12-12 18:51:13 UTC"
  },
  {
    "arxiv_id": "2412.10467v1",
    "title": "MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media",
    "authors": [
      "Muhammad Arslan Manzoor",
      "Ruihong Zeng",
      "Dilshod Azizov",
      "Preslav Nakov",
      "Shangsong Liang"
    ],
    "abstract": "In the current era of rapidly growing digital data, evaluating the political\nbias and factuality of news outlets has become more important for seeking\nreliable information online. In this work, we study the classification problem\nof profiling news media from the lens of political bias and factuality.\nTraditional profiling methods, such as Pre-trained Language Models (PLMs) and\nGraph Neural Networks (GNNs) have shown promising results, but they face\nnotable challenges. PLMs focus solely on textual features, causing them to\noverlook the complex relationships between entities, while GNNs often struggle\nwith media graphs containing disconnected components and insufficient labels.\nTo address these limitations, we propose MediaGraphMind (MGM), an effective\nsolution within a variational Expectation-Maximization (EM) framework. Instead\nof relying on limited neighboring nodes, MGM leverages features, structural\npatterns, and label information from globally similar nodes. Such a framework\nnot only enables GNNs to capture long-range dependencies for learning\nexpressive node representations but also enhances PLMs by integrating\nstructural information and therefore improving the performance of both models.\nThe extensive experiments demonstrate the effectiveness of the proposed\nframework and achieve new state-of-the-art results. Further, we share our\nrepository1 which contains the dataset, code, and documentation",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10467v1",
    "published_date": "2024-12-12 18:37:32 UTC",
    "updated_date": "2024-12-12 18:37:32 UTC"
  },
  {
    "arxiv_id": "2412.09544v1",
    "title": "Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking",
    "authors": [
      "Paria Rashidinejad",
      "Yuandong Tian"
    ],
    "abstract": "Aligning AI systems with human preferences typically suffers from the\ninfamous reward hacking problem, where optimization of an imperfect reward\nmodel leads to undesired behaviors. In this paper, we investigate reward\nhacking in offline preference optimization, which aims to improve an initial\nmodel using a preference dataset. We identify two types of reward hacking\nstemming from statistical fluctuations in the dataset: Type I Reward Hacking\ndue to subpar choices appearing more favorable, and Type II Reward Hacking due\nto decent choices appearing less favorable. We prove that many (mainstream or\ntheoretical) preference optimization methods suffer from both types of reward\nhacking. To mitigate Type I Reward Hacking, we propose POWER, a new preference\noptimization method that combines Guiasu's weighted entropy with a robust\nreward maximization objective. POWER enjoys finite-sample guarantees under\ngeneral function approximation, competing with the best covered policy in the\ndata. To mitigate Type II Reward Hacking, we analyze the learning dynamics of\npreference optimization and develop a novel technique that dynamically updates\npreference labels toward certain \"stationary labels\", resulting in diminishing\ngradients for untrustworthy samples. Empirically, POWER with dynamic labels\n(POWER-DL) consistently outperforms state-of-the-art methods on alignment\nbenchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and\n11.5 points on Arena-Hard over DPO, while also improving or maintaining\nperformance on downstream tasks such as mathematical reasoning. Strong\ntheoretical guarantees and empirical results demonstrate the promise of\nPOWER-DL in mitigating reward hacking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09544v1",
    "published_date": "2024-12-12 18:34:47 UTC",
    "updated_date": "2024-12-12 18:34:47 UTC"
  },
  {
    "arxiv_id": "2412.09666v1",
    "title": "Systematic Analysis of LLM Contributions to Planning: Solver, Verifier, Heuristic",
    "authors": [
      "Haoming Li",
      "Zhaoliang Chen",
      "Songyuan Liu",
      "Yiming Lu",
      "Fei Liu"
    ],
    "abstract": "In this work, we provide a systematic analysis of how large language models\n(LLMs) contribute to solving planning problems. In particular, we examine how\nLLMs perform when they are used as problem solver, solution verifier, and\nheuristic guidance to improve intermediate solutions. Our analysis reveals that\nalthough it is difficult for LLMs to generate correct plans out-of-the-box,\nLLMs are much better at providing feedback signals to intermediate/incomplete\nsolutions in the form of comparative heuristic functions. This evaluation\nframework provides insights into how future work may design better LLM-based\ntree-search algorithms to solve diverse planning and reasoning problems. We\nalso propose a novel benchmark to evaluate LLM's ability to learn user\npreferences on the fly, which has wide applications in practical settings.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09666v1",
    "published_date": "2024-12-12 18:16:46 UTC",
    "updated_date": "2024-12-12 18:16:46 UTC"
  },
  {
    "arxiv_id": "2412.09521v3",
    "title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Pathology Analysis",
    "authors": [
      "Shengxuming Zhang",
      "Weihan Li",
      "Tianhong Gao",
      "Jiacong Hu",
      "Haoming Luo",
      "Xiuming Zhang",
      "Jing Zhang",
      "Mingli Song",
      "Zunlei Feng"
    ],
    "abstract": "Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\nexisting large vision-language models (LVLMs) are limited by input resolution\nconstraints, hindering their efficiency and accuracy in pathology image\nanalysis. To overcome these issues, we propose two innovative strategies: the\nmixed task-guided feature enhancement, which directs feature extraction toward\nlesion-related details across scales, and the prompt-guided detail feature\ncompletion, which integrates coarse- and fine-grained features from WSI based\non specific prompts without compromising inference speed. Leveraging a\ncomprehensive dataset of 490K samples from diverse pathology tasks, we trained\nthe pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate\nthat this model significantly outperforms existing methods in diagnostic\naccuracy and efficiency, providing an interactive, clinically aligned approach\nfor auxiliary diagnosis in a wide range of pathology applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09521v3",
    "published_date": "2024-12-12 18:07:23 UTC",
    "updated_date": "2025-05-16 10:17:33 UTC"
  },
  {
    "arxiv_id": "2412.09507v2",
    "title": "Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction",
    "authors": [
      "Rafayel Mkrtchyan",
      "Edvard Ghukasyan",
      "Khoren Petrosyan",
      "Hrant Khachatrian",
      "Theofanis P. Raptis"
    ],
    "abstract": "Indoor pathloss prediction is a fundamental task in wireless network\nplanning, yet it remains challenging due to environmental complexity and data\nscarcity. In this work, we propose a deep learning-based approach utilizing a\nvision transformer (ViT) architecture with DINO-v2 pretrained weights to model\nindoor radio propagation. Our method processes a floor map with additional\nfeatures of the walls to generate indoor pathloss maps. We systematically\nevaluate the effects of architectural choices, data augmentation strategies,\nand feature engineering techniques. Our findings indicate that extensive\naugmentation significantly improves generalization, while feature engineering\nis crucial in low-data regimes. Through comprehensive experiments, we\ndemonstrate the robustness of our model across different generalization\nscenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CV",
    "comment": "Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")",
    "pdf_url": "http://arxiv.org/pdf/2412.09507v2",
    "published_date": "2024-12-12 17:55:00 UTC",
    "updated_date": "2025-05-08 10:03:57 UTC"
  },
  {
    "arxiv_id": "2412.15238v1",
    "title": "Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning tasks",
    "authors": [
      "Gregory Kang Ruey Lau",
      "Wenyang Hu",
      "Diwen Liu",
      "Jizhuo Chen",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "Large Language Models still encounter substantial challenges in reasoning\ntasks, especially for smaller models, which many users may be restricted to due\nto resource constraints (e.g. GPU memory restrictions). Inference-time methods\nto boost LLM performance, such as prompting methods to invoke certain reasoning\npathways in responses, have been shown effective in past works, though they\nlargely rely on sequential queries. The ensemble method, which consists of\nmultiple constituent models running in parallel, is a promising approach to\nachieving better inference-time performance, especially given recent\ndevelopments that enabled significant speed-ups in LLM batch inference. In this\nwork, we propose a novel, training-free LLM ensemble framework where a single\nLLM model is fed an optimized, diverse set of prompts in parallel, effectively\nproducing an ensemble at inference time to achieve performance improvement in\nreasoning tasks. We empirically demonstrate that our method leads to\nsignificant gains on math reasoning tasks, e.g., on MATH, where our ensemble\nconsisting of a few small models (e.g., three Qwen2-MATH-1.5B-it models) can\noutperform a larger model (e.g., Qwen2-MATH-7B-it).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NeurIPS 2024 Workshop on Foundation Model Interventions\n  (MINT)",
    "pdf_url": "http://arxiv.org/pdf/2412.15238v1",
    "published_date": "2024-12-12 17:49:05 UTC",
    "updated_date": "2024-12-12 17:49:05 UTC"
  },
  {
    "arxiv_id": "2412.09492v1",
    "title": "Video Seal: Open and Efficient Video Watermarking",
    "authors": [
      "Pierre Fernandez",
      "Hady Elsahar",
      "I. Zeki Yalniz",
      "Alexandre Mourachko"
    ],
    "abstract": "The proliferation of AI-generated content and sophisticated video editing\ntools has made it both important and challenging to moderate digital platforms.\nVideo watermarking addresses these challenges by embedding imperceptible\nsignals into videos, allowing for identification. However, the rare open tools\nand methods often fall short on efficiency, robustness, and flexibility. To\nreduce these gaps, this paper introduces Video Seal, a comprehensive framework\nfor neural video watermarking and a competitive open-sourced model. Our\napproach jointly trains an embedder and an extractor, while ensuring the\nwatermark robustness by applying transformations in-between, e.g., video\ncodecs. This training is multistage and includes image pre-training, hybrid\npost-training and extractor fine-tuning. We also introduce temporal watermark\npropagation, a technique to convert any image watermarking model to an\nefficient video watermarking model without the need to watermark every\nhigh-resolution frame. We present experimental results demonstrating the\neffectiveness of the approach in terms of speed, imperceptibility, and\nrobustness. Video Seal achieves higher robustness compared to strong baselines\nespecially under challenging distortions combining geometric transformations\nand video compression. Additionally, we provide new insights such as the impact\nof video compression during training, and how to compare methods operating on\ndifferent payloads. Contributions in this work - including the codebase,\nmodels, and a public demo - are open-sourced under permissive licenses to\nfoster further research and development in the field.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "Code available at https://github.com/facebookresearch/videoseal",
    "pdf_url": "http://arxiv.org/pdf/2412.09492v1",
    "published_date": "2024-12-12 17:41:49 UTC",
    "updated_date": "2024-12-12 17:41:49 UTC"
  },
  {
    "arxiv_id": "2412.09486v1",
    "title": "Regression and Classification with Single-Qubit Quantum Neural Networks",
    "authors": [
      "Leandro C. Souza",
      "Bruno C. Guingo",
      "Gilson Giraldi",
      "Renato Portugal"
    ],
    "abstract": "Since classical machine learning has become a powerful tool for developing\ndata-driven algorithms, quantum machine learning is expected to similarly\nimpact the development of quantum algorithms. The literature reflects a\nmutually beneficial relationship between machine learning and quantum\ncomputing, where progress in one field frequently drives improvements in the\nother. Motivated by the fertile connection between machine learning and quantum\ncomputing enabled by parameterized quantum circuits, we use a\nresource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for\nboth regression and classification tasks. The SQQNN leverages parameterized\nsingle-qubit unitary operators and quantum measurements to achieve efficient\nlearning. To train the model, we use gradient descent for regression tasks. For\nclassification, we introduce a novel training method inspired by the Taylor\nseries, which can efficiently find a global minimum in a single step. This\napproach significantly accelerates training compared to iterative methods.\nEvaluated across various applications, the SQQNN exhibits virtually error-free\nand strong performance in regression and classification tasks, including the\nMNIST dataset. These results demonstrate the versatility, scalability, and\nsuitability of the SQQNN for deployment on near-term quantum devices.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "21 pages, 7 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.09486v1",
    "published_date": "2024-12-12 17:35:36 UTC",
    "updated_date": "2024-12-12 17:35:36 UTC"
  },
  {
    "arxiv_id": "2412.09480v1",
    "title": "The Parameters of Educability",
    "authors": [
      "Leslie G. Valiant"
    ],
    "abstract": "The educability model is a computational model that has been recently\nproposed to describe the cognitive capability that makes humans unique among\nexisting biological species on Earth in being able to create advanced\ncivilizations. Educability is defined as a capability for acquiring and\napplying knowledge. It is intended both to describe human capabilities and,\nequally, as an aspirational description of what can be usefully realized by\nmachines. While the intention is to have a mathematically well-defined\ncomputational model, in constructing an instance of the model there are a\nnumber of decisions to make. We call these decisions {\\it parameters}. In a\nstandard computer, two parameters are the memory capacity and clock rate. There\nis no universally optimal choice for either one, or even for their ratio.\nSimilarly, in a standard machine learning system, two parameters are the\nlearning algorithm and the dataset used for training. Again, there are no\nuniversally optimal choices known for either. An educable system has many more\nparameters than either of these two kinds of system. This short paper discusses\nsome of the main parameters of educable systems, and the broader implications\nof their existence.",
    "categories": [
      "cs.AI",
      "q-bio.NC",
      "I.2.0; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09480v1",
    "published_date": "2024-12-12 17:27:03 UTC",
    "updated_date": "2024-12-12 17:27:03 UTC"
  },
  {
    "arxiv_id": "2412.09475v2",
    "title": "New keypoint-based approach for recognising British Sign Language (BSL) from sequences",
    "authors": [
      "Oishi Deb",
      "KR Prajwal",
      "Andrew Zisserman"
    ],
    "abstract": "In this paper, we present a novel keypoint-based classification model\ndesigned to recognise British Sign Language (BSL) words within continuous\nsigning sequences. Our model's performance is assessed using the BOBSL dataset,\nrevealing that the keypoint-based approach surpasses its RGB-based counterpart\nin computational efficiency and memory usage. Furthermore, it offers expedited\ntraining times and demands fewer computational resources. To the best of our\nknowledge, this is the inaugural application of a keypoint-based model for BSL\nword classification, rendering direct comparisons with existing works\nunavailable.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "International Conference on Computer Vision (ICCV) - HANDS Workshop",
    "pdf_url": "http://arxiv.org/pdf/2412.09475v2",
    "published_date": "2024-12-12 17:20:27 UTC",
    "updated_date": "2024-12-31 18:58:09 UTC"
  },
  {
    "arxiv_id": "2412.09468v3",
    "title": "STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading",
    "authors": [
      "Yilei Zhao",
      "Wentao Zhang",
      "Tingran Yang",
      "Yong Jiang",
      "Fei Huang",
      "Wei Yang Bryan Lim"
    ],
    "abstract": "In financial trading, factor models are widely used to price assets and\ncapture excess returns from mispricing. Recently, we have witnessed the rise of\nvariational autoencoder-based latent factor models, which learn latent factors\nself-adaptively. While these models focus on modeling overall market\nconditions, they often fail to effectively capture the temporal patterns of\nindividual stocks. Additionally, representing multiple factors as single values\nsimplifies the model but limits its ability to capture complex relationships\nand dependencies. As a result, the learned factors are of low quality and lack\ndiversity, reducing their effectiveness and robustness across different trading\nperiods. To address these issues, we propose a Spatio-Temporal factOR Model\nbased on dual vector quantized variational autoencoders, named STORM, which\nextracts features of stocks from temporal and spatial perspectives, then fuses\nand aligns these features at the fine-grained and semantic level, and\nrepresents the factors as multi-dimensional embeddings. The discrete codebooks\ncluster similar factor embeddings, ensuring orthogonality and diversity, which\nhelps distinguish between different factors and enables factor selection in\nfinancial trading. To show the performance of the proposed factor model, we\napply it to two downstream experiments: portfolio management on two stock\ndatasets and individual trading tasks on six specific stocks. The extensive\nexperiments demonstrate STORM's flexibility in adapting to downstream tasks and\nsuperior performance over baseline models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09468v3",
    "published_date": "2024-12-12 17:15:49 UTC",
    "updated_date": "2025-03-17 04:30:03 UTC"
  },
  {
    "arxiv_id": "2412.09433v1",
    "title": "Solving Multiagent Path Finding on Highly Centralized Networks",
    "authors": [
      "Foivos Fioravantes",
      "Dušan Knop",
      "Jan Matyáš Křišťan",
      "Nikolaos Melissinos",
      "Michal Opler",
      "Tung Anh Vu"
    ],
    "abstract": "The Mutliagent Path Finding (MAPF) problem consists of identifying the\ntrajectories that a set of agents should follow inside a given network in order\nto reach their desired destinations as soon as possible, but without colliding\nwith each other. We aim to minimize the maximum time any agent takes to reach\ntheir goal, ensuring optimal path length. In this work, we complement a recent\nthread of results that aim to systematically study the algorithmic behavior of\nthis problem, through the parameterized complexity point of view.\n  First, we show that MAPF is NP-hard when the given network has a star-like\ntopology (bounded vertex cover number) or is a tree with $11$ leaves. Both of\nthese results fill important gaps in our understanding of the tractability of\nthis problem that were left untreated in the recent work of [Fioravantes et al.\nExact Algorithms and Lowerbounds for Multiagent Path Finding: Power of Treelike\nTopology. AAAI'24]. Nevertheless, our main contribution is an exact algorithm\nthat scales well as the input grows (FPT) when the topology of the given\nnetwork is highly centralized (bounded distance to clique). This parameter is\nsignificant as it mirrors real-world networks. In such environments, a bunch of\ncentral hubs (e.g., processing areas) are connected to only few peripheral\nnodes.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09433v1",
    "published_date": "2024-12-12 16:38:25 UTC",
    "updated_date": "2024-12-12 16:38:25 UTC"
  },
  {
    "arxiv_id": "2412.09429v2",
    "title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
    "authors": [
      "Yi Luo",
      "Linghang Shi",
      "Yihao Li",
      "Aobo Zhuang",
      "Yeyun Gong",
      "Ling Liu",
      "Chen Lin"
    ],
    "abstract": "Conventional biomedical research is increasingly labor-intensive due to the\nexponential growth of scientific literature and datasets. Artificial\nintelligence (AI), particularly Large Language Models (LLMs), has the potential\nto revolutionize this process by automating various steps. Still, significant\nchallenges remain, including the need for multidisciplinary expertise,\nlogicality of experimental design, and performance measurements. This paper\nintroduces BioResearcher, the first end-to-end automated system designed to\nstreamline the entire biomedical research process involving dry lab\nexperiments. BioResearcher employs a modular multi-agent architecture,\nintegrating specialized agents for search, literature processing, experimental\ndesign, and programming. By decomposing complex tasks into logically related\nsub-tasks and utilizing a hierarchical learning approach, BioResearcher\neffectively addresses the challenges of multidisciplinary requirements and\nlogical complexity. Furthermore, BioResearcher incorporates an LLM-based\nreviewer for in-process quality control and introduces novel evaluation metrics\nto assess the quality and automation of experimental protocols. BioResearcher\nsuccessfully achieves an average execution success rate of 63.07% across eight\npreviously unmet research objectives. The generated protocols averagely\noutperform typical agent systems by 22.0% on five quality metrics. The system\ndemonstrates significant potential to reduce researchers' workloads and\naccelerate biomedical discoveries, paving the way for future innovations in\nautomated research systems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09429v2",
    "published_date": "2024-12-12 16:35:05 UTC",
    "updated_date": "2024-12-22 05:34:46 UTC"
  },
  {
    "arxiv_id": "2412.09417v2",
    "title": "Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer",
    "authors": [
      "Adam Labiosa",
      "Zhihan Wang",
      "Siddhant Agarwal",
      "William Cong",
      "Geethika Hemkumar",
      "Abhinav Narayan Harish",
      "Benjamin Hong",
      "Josh Kelle",
      "Chen Li",
      "Yuhao Li",
      "Zisen Shao",
      "Peter Stone",
      "Josiah P. Hanna"
    ],
    "abstract": "Robot decision-making in partially observable, real-time, dynamic, and\nmulti-agent environments remains a difficult and unsolved challenge. Model-free\nreinforcement learning (RL) is a promising approach to learning decision-making\nin such domains, however, end-to-end RL in complex environments is often\nintractable. To address this challenge in the RoboCup Standard Platform League\n(SPL) domain, we developed a novel architecture integrating RL within a\nclassical robotics stack, while employing a multi-fidelity sim2real approach\nand decomposing behavior into learned sub-behaviors with heuristic selection.\nOur architecture led to victory in the 2024 RoboCup SPL Challenge Shield\nDivision. In this work, we fully describe our system's architecture and\nempirically analyze key design decisions that contributed to its success. Our\napproach demonstrates how RL-based behaviors can be integrated into complete\nrobot behavior architectures.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09417v2",
    "published_date": "2024-12-12 16:25:10 UTC",
    "updated_date": "2025-03-07 02:12:15 UTC"
  },
  {
    "arxiv_id": "2412.09413v2",
    "title": "Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems",
    "authors": [
      "Yingqian Min",
      "Zhipeng Chen",
      "Jinhao Jiang",
      "Jie Chen",
      "Jia Deng",
      "Yiwen Hu",
      "Yiru Tang",
      "Jiapeng Wang",
      "Xiaoxue Cheng",
      "Huatong Song",
      "Wayne Xin Zhao",
      "Zheng Liu",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ],
    "abstract": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated\nremarkable capabilities in solving complex reasoning tasks. These systems\ntypically engage in an extended thinking process before responding to a query,\nallowing them to generate more thorough, accurate, and well-reasoned solutions.\nThese systems are primarily developed and maintained by industry, with their\ncore techniques not publicly disclosed. In response, an increasing number of\nstudies from the research community aim to explore the technical foundations\nunderlying these powerful reasoning systems. Building on these prior efforts,\nthis paper presents a reproduction report on implementing o1-like reasoning\nsystems. We introduce an ``imitate, explore, and self-improve'' framework,\ndenoted as \\textbf{STILL-2}, as our primary technical approach to train the\nreasoning model. In the initial phase, we use distilled long-form thought data\nto fine-tune the reasoning model, enabling it to invoke a slow-thinking mode.\nThe model is then encouraged to explore challenging problems by generating\nmultiple rollouts, which can result in increasingly more high-quality\ntrajectories that lead to correct answers. Furthermore, the model undergoes\nself-improvement by iteratively refining its training dataset. To verify the\neffectiveness of this approach, we conduct extensive experiments on three\nchallenging benchmarks. The experimental results demonstrate that our approach\nachieves competitive performance compared to industry-level reasoning systems\non these benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical Report on Slow Thinking with LLMs: Part II",
    "pdf_url": "http://arxiv.org/pdf/2412.09413v2",
    "published_date": "2024-12-12 16:20:36 UTC",
    "updated_date": "2024-12-22 10:44:13 UTC"
  },
  {
    "arxiv_id": "2412.09407v1",
    "title": "Uncommon Belief in Rationality",
    "authors": [
      "Qi Shi",
      "Pavel Naumov"
    ],
    "abstract": "Common knowledge/belief in rationality is the traditional standard assumption\nin analysing interaction among agents. This paper proposes a graph-based\nlanguage for capturing significantly more complicated structures of\nhigher-order beliefs that agents might have about the rationality of the other\nagents. The two main contributions are a solution concept that captures the\nreasoning process based on a given belief structure and an efficient algorithm\nfor compressing any belief structure into a unique minimal form.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "The 39th Annual AAAI Conference on Artificial Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.09407v1",
    "published_date": "2024-12-12 16:12:40 UTC",
    "updated_date": "2024-12-12 16:12:40 UTC"
  },
  {
    "arxiv_id": "2412.12173v1",
    "title": "A NotSo Simple Way to Beat Simple Bench",
    "authors": [
      "Soham Sane",
      "Angus McLean"
    ],
    "abstract": "This paper presents a novel framework for enhancing reasoning capabilities in\nlarge language models (LLMs) by leveraging iterative reasoning and\nfeedback-driven methodologies. Building on the limitations identified in the\nSimpleBench benchmark, a dataset designed to evaluate logical coherence and\nreal-world reasoning, we propose a multi-step prompting strategy coupled with\nglobal consistency checks to improve model accuracy and robustness. Through\ncomparative analysis of state-of-the-art models, including Claude 3 Opus,\nClaude 3.5, GPT- 4o, and o1-preview, we demonstrate that iterative reasoning\nsignificantly enhances model performance, with improvements observed in both\nstandard accuracy metrics (AVG@5) and a newly introduced metric, Extreme\nAveraging (EAG@5). Our results reveal model-specific strengths: Claude excels\nin maintaining logical consistency, while GPT-4o exhibits exploratory\ncreativity but struggles with ambiguous prompts. By analyzing case studies and\nidentifying gaps in spatial and temporal reasoning, we highlight areas for\nfurther refinement. The findings underscore the potential of structured\nreasoning frameworks to address inherent model limitations, irrespective of\npretraining methodologies. This study lays the groundwork for integrating\ndynamic feedback mechanisms, adaptive restart strategies, and diverse\nevaluation metrics to advance LLM reasoning capabilities across complex and\nmulti-domain problem spaces.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 11 Figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12173v1",
    "published_date": "2024-12-12 16:04:31 UTC",
    "updated_date": "2024-12-12 16:04:31 UTC"
  },
  {
    "arxiv_id": "2412.09389v1",
    "title": "UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer",
    "authors": [
      "Delong Liu",
      "Zhaohui Hou",
      "Mingjie Zhan",
      "Shihao Han",
      "Zhicheng Zhao",
      "Fei Su"
    ],
    "abstract": "Recently, diffusion-based video generation models have achieved significant\nsuccess. However, existing models often suffer from issues like weak\nconsistency and declining image quality over time. To overcome these\nchallenges, inspired by aesthetic principles, we propose a non-invasive plug-in\ncalled Uniform Frame Organizer (UFO), which is compatible with any\ndiffusion-based video generation model. The UFO comprises a series of adaptive\nadapters with adjustable intensities, which can significantly enhance the\nconsistency between the foreground and background of videos and improve image\nquality without altering the original model parameters when integrated. The\ntraining for UFO is simple, efficient, requires minimal resources, and supports\nstylized training. Its modular design allows for the combination of multiple\nUFOs, enabling the customization of personalized video generation models.\nFurthermore, the UFO also supports direct transferability across different\nmodels of the same specification without the need for specific retraining. The\nexperimental results indicate that UFO effectively enhances video generation\nquality and demonstrates its superiority in public video generation benchmarks.\nThe code will be publicly available at https://github.com/Delong-liu-bupt/UFO.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code:https://github.com/Delong-liu-bupt/UFO",
    "pdf_url": "http://arxiv.org/pdf/2412.09389v1",
    "published_date": "2024-12-12 15:56:26 UTC",
    "updated_date": "2024-12-12 15:56:26 UTC"
  },
  {
    "arxiv_id": "2412.09388v2",
    "title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System",
    "authors": [
      "Junjie Zhou",
      "Ke Zhu",
      "Jianxin Wu"
    ],
    "abstract": "Knowledge Distillation (KD) is essential in transferring dark knowledge from\na large teacher to a small student network, such that the student can be much\nmore efficient than the teacher but with comparable accuracy. Existing KD\nmethods, however, rely on a large teacher trained specifically for the target\ntask, which is both very inflexible and inefficient. In this paper, we argue\nthat a SSL-pretrained model can effectively act as the teacher and its dark\nknowledge can be captured by the coordinate system or linear subspace where the\nfeatures lie in. We then need only one forward pass of the teacher, and then\ntailor the coordinate system (TCS) for the student network. Our TCS method is\nteacher-free and applies to diverse architectures, works well for KD and\npractical few-shot learning, and allows cross-architecture distillation with\nlarge capacity gap. Experiments show that TCS achieves significantly higher\naccuracy than state-of-the-art KD methods, while only requiring roughly half of\ntheir training time and GPU memory costs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09388v2",
    "published_date": "2024-12-12 15:56:20 UTC",
    "updated_date": "2025-02-12 10:55:54 UTC"
  },
  {
    "arxiv_id": "2412.09387v1",
    "title": "Distributed Intelligent System Architecture for UAV-Assisted Monitoring of Wind Energy Infrastructure",
    "authors": [
      "Serhii Svystun",
      "Oleksandr Melnychenko",
      "Pavlo Radiuk",
      "Oleg Savenko",
      "Andrii Lysyi"
    ],
    "abstract": "With the rapid development of green energy, the efficiency and reliability of\nwind turbines are key to sustainable renewable energy production. For that\nreason, this paper presents a novel intelligent system architecture designed\nfor the dynamic collection and real-time processing of visual data to detect\ndefects in wind turbines. The system employs advanced algorithms within a\ndistributed framework to enhance inspection accuracy and efficiency using\nunmanned aerial vehicles (UAVs) with integrated visual and thermal sensors. An\nexperimental study conducted at the \"Staryi Sambir-1\" wind power plant in\nUkraine demonstrates the system's effectiveness, showing a significant\nimprovement in defect detection accuracy (up to 94%) and a reduction in\ninspection time per turbine (down to 1.5 hours) compared to traditional\nmethods. The results show that the proposed intelligent system architecture\nprovides a scalable and reliable solution for wind turbine maintenance,\ncontributing to the durability and performance of renewable energy\ninfrastructure.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.4.8; I.2.10; I.5.4; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "Wind turbine inspection, UAV, intelligent systems, distributed\n  architecture, defect detection, renewable energy maintenance, automated\n  monitoring",
    "pdf_url": "http://arxiv.org/pdf/2412.09387v1",
    "published_date": "2024-12-12 15:53:58 UTC",
    "updated_date": "2024-12-12 15:53:58 UTC"
  },
  {
    "arxiv_id": "2412.09385v2",
    "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
    "authors": [
      "Fabrizio Davide",
      "Pietro Torre",
      "Leonardo Ercolani",
      "Andrea Gaggioli"
    ],
    "abstract": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "47 pages, 8 figures, 17 tables, appendix with data and code",
    "pdf_url": "http://arxiv.org/pdf/2412.09385v2",
    "published_date": "2024-12-12 15:52:41 UTC",
    "updated_date": "2025-04-22 13:56:32 UTC"
  },
  {
    "arxiv_id": "2412.10464v1",
    "title": "Automatic Detection, Positioning and Counting of Grape Bunches Using Robots",
    "authors": [
      "Xumin Gao"
    ],
    "abstract": "In order to promote agricultural automatic picking and yield estimation\ntechnology, this project designs a set of automatic detection, positioning and\ncounting algorithms for grape bunches, and applies it to agricultural robots.\nThe Yolov3 detection network is used to realize the accurate detection of grape\nbunches, and the local tracking algorithm is added to eliminate relocation.\nThen it obtains the accurate 3D spatial position of the central points of grape\nbunches using the depth distance and the spatial restriction method. Finally,\nthe counting of grape bunches is completed. It is verified using the\nagricultural robot in the simulated vineyard environment. The project code is\nreleased at:\nhttps://github.com/XuminGaoGithub/Grape_bunches_count_using_robots.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10464v1",
    "published_date": "2024-12-12 15:52:40 UTC",
    "updated_date": "2024-12-12 15:52:40 UTC"
  },
  {
    "arxiv_id": "2412.09380v1",
    "title": "Diffusion Model with Representation Alignment for Protein Inverse Folding",
    "authors": [
      "Chenglin Wang",
      "Yucheng Zhou",
      "Zijie Zhai",
      "Jianbing Shen",
      "Kai Zhang"
    ],
    "abstract": "Protein inverse folding is a fundamental problem in bioinformatics, aiming to\nrecover the amino acid sequences from a given protein backbone structure.\nDespite the success of existing methods, they struggle to fully capture the\nintricate inter-residue relationships critical for accurate sequence\nprediction. We propose a novel method that leverages diffusion models with\nrepresentation alignment (DMRA), which enhances diffusion-based inverse folding\nby (1) proposing a shared center that aggregates contextual information from\nthe entire protein structure and selectively distributes it to each residue;\nand (2) aligning noisy hidden representations with clean semantic\nrepresentations during the denoising process. This is achieved by predefined\nsemantic representations for amino acid types and a representation alignment\nmethod that utilizes type embeddings as semantic feedback to normalize each\nresidue. In experiments, we conduct extensive evaluations on the CATH4.2\ndataset to demonstrate that DMRA outperforms leading methods, achieving\nstate-of-the-art performance and exhibiting strong generalization capabilities\non the TS50 and TS500 datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09380v1",
    "published_date": "2024-12-12 15:47:59 UTC",
    "updated_date": "2024-12-12 15:47:59 UTC"
  },
  {
    "arxiv_id": "2412.09370v1",
    "title": "Word Sense Linking: Disambiguating Outside the Sandbox",
    "authors": [
      "Andrei Stefan Bejgu",
      "Edoardo Barba",
      "Luigi Procopio",
      "Alberte Fernández-Castro",
      "Roberto Navigli"
    ],
    "abstract": "Word Sense Disambiguation (WSD) is the task of associating a word in a given\ncontext with its most suitable meaning among a set of possible candidates.\nWhile the task has recently witnessed renewed interest, with systems achieving\nperformances above the estimated inter-annotator agreement, at the time of\nwriting it still struggles to find downstream applications. We argue that one\nof the reasons behind this is the difficulty of applying WSD to plain text.\nIndeed, in the standard formulation, models work under the assumptions that a)\nall the spans to disambiguate have already been identified, and b) all the\npossible candidate senses of each span are provided, both of which are\nrequirements that are far from trivial. In this work, we present a new task\ncalled Word Sense Linking (WSL) where, given an input text and a reference\nsense inventory, systems have to both identify which spans to disambiguate and\nthen link them to their most suitable meaning.We put forward a\ntransformer-based architecture for the task and thoroughly evaluate both its\nperformance and those of state-of-the-art WSD systems scaled to WSL,\niteratively relaxing the assumptions of WSD. We hope that our work will foster\neasier integration of lexical semantics into downstream applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09370v1",
    "published_date": "2024-12-12 15:38:34 UTC",
    "updated_date": "2024-12-12 15:38:34 UTC"
  },
  {
    "arxiv_id": "2412.09353v2",
    "title": "Causal Graphical Models for Vision-Language Compositional Understanding",
    "authors": [
      "Fiorenzo Parascandolo",
      "Nicholas Moratelli",
      "Enver Sangineto",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09353v2",
    "published_date": "2024-12-12 15:22:03 UTC",
    "updated_date": "2025-04-15 10:14:34 UTC"
  },
  {
    "arxiv_id": "2412.09335v1",
    "title": "Does Low Spoilage Under Cold Conditions Foster Cultural Complexity During the Foraging Era? -- A Theoretical and Computational Inquiry",
    "authors": [
      "Minhyeok Lee"
    ],
    "abstract": "Human cultural complexity did not arise in a vacuum. Scholars in the\nhumanities and social sciences have long debated how ecological factors, such\nas climate and resource availability, enabled early hunter-gatherers to\nallocate time and energy beyond basic subsistence tasks. This paper presents a\nformal, interdisciplinary approach that integrates theoretical modeling with\ncomputational methods to examine whether conditions that allow lower spoilage\nof stored food, often associated with colder climates and abundant large fauna,\ncould indirectly foster the emergence of cultural complexity. Our contribution\nis twofold. First, we propose a mathematical framework that relates spoilage\nrates, yield levels, resource management skills, and cultural activities. Under\nthis framework, we prove that lower spoilage and adequate yields reduce the\nfrequency of hunting, thus freeing substantial time for cultural pursuits.\nSecond, we implement a reinforcement learning simulation, inspired by\nengineering optimization techniques, to validate the theoretical predictions.\nBy training agents in different $(Y,p)$ environments, where $Y$ is yield and\n$p$ is the probability of daily spoilage, we observe patterns consistent with\nthe theoretical model: stable conditions with lower spoilage strongly correlate\nwith increased cultural complexity. While we do not claim to replicate\nprehistoric social realities directly, our results suggest that ecologically\nstable niches provided a milieu in which cultural forms could germinate and\nevolve. This study, therefore, offers an integrative perspective that unites\nhumanistic inquiries into the origins of culture with the formal rigor and\nexploratory power of computational modeling.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09335v1",
    "published_date": "2024-12-12 15:03:08 UTC",
    "updated_date": "2024-12-12 15:03:08 UTC"
  },
  {
    "arxiv_id": "2412.09329v1",
    "title": "Towards Open-Vocabulary Video Semantic Segmentation",
    "authors": [
      "Xinhao Li",
      "Yun Liu",
      "Guolei Sun",
      "Min Wu",
      "Le Zhang",
      "Ce Zhu"
    ],
    "abstract": "Semantic segmentation in videos has been a focal point of recent research.\nHowever, existing models encounter challenges when faced with unfamiliar\ncategories. To address this, we introduce the Open Vocabulary Video Semantic\nSegmentation (OV-VSS) task, designed to accurately segment every pixel across a\nwide range of open-vocabulary categories, including those that are novel or\npreviously unexplored. To enhance OV-VSS performance, we propose a robust\nbaseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing\nthe model to utilize temporal relationships across consecutive frames.\nAdditionally, we incorporate a random frame enhancement module, broadening the\nmodel's understanding of semantic context throughout the entire video sequence.\nOur approach also includes video text encoding, which strengthens the model's\ncapability to interpret textual information within the video context.\nComprehensive evaluations on benchmark datasets such as VSPW and Cityscapes\nhighlight OV-VSS's zero-shot generalization capabilities, especially in\nhandling novel categories. The results validate OV2VSS's effectiveness,\ndemonstrating improved performance in semantic segmentation tasks across\ndiverse video datasets.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09329v1",
    "published_date": "2024-12-12 14:53:16 UTC",
    "updated_date": "2024-12-12 14:53:16 UTC"
  },
  {
    "arxiv_id": "2412.09328v1",
    "title": "Auto-Regressive Moving Diffusion Models for Time Series Forecasting",
    "authors": [
      "Jiaxin Gao",
      "Qinglong Cao",
      "Yuntian Chen"
    ],
    "abstract": "Time series forecasting (TSF) is essential in various domains, and recent\nadvancements in diffusion-based TSF models have shown considerable promise.\nHowever, these models typically adopt traditional diffusion patterns, treating\nTSF as a noise-based conditional generation task. This approach neglects the\ninherent continuous sequential nature of time series, leading to a fundamental\nmisalignment between diffusion mechanisms and the TSF objective, thereby\nseverely impairing performance. To bridge this misalignment, and inspired by\nthe classic Auto-Regressive Moving Average (ARMA) theory, which views time\nseries as continuous sequential progressions evolving from previous data\npoints, we propose a novel Auto-Regressive Moving Diffusion (ARMD) model to\nfirst achieve the continuous sequential diffusion-based TSF. Unlike previous\nmethods that start from white Gaussian noise, our model employs chain-based\ndiffusion with priors, accurately modeling the evolution of time series and\nleveraging intermediate state information to improve forecasting accuracy and\nstability. Specifically, our approach reinterprets the diffusion process by\nconsidering future series as the initial state and historical series as the\nfinal state, with intermediate series generated using a sliding-based technique\nduring the forward process. This design aligns the diffusion model's sampling\nprocedure with the forecasting objective, resulting in an unconditional,\ncontinuous sequential diffusion TSF model. Extensive experiments conducted on\nseven widely used datasets demonstrate that our model achieves state-of-the-art\nperformance, significantly outperforming existing diffusion-based TSF models.\nOur code is available on GitHub: https://github.com/daxin007/ARMD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "no comment",
    "pdf_url": "http://arxiv.org/pdf/2412.09328v1",
    "published_date": "2024-12-12 14:51:48 UTC",
    "updated_date": "2024-12-12 14:51:48 UTC"
  },
  {
    "arxiv_id": "2412.09318v2",
    "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction",
    "authors": [
      "Jing Liu",
      "Abdellah Fourtassi"
    ],
    "abstract": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09318v2",
    "published_date": "2024-12-12 14:43:03 UTC",
    "updated_date": "2024-12-13 09:30:36 UTC"
  },
  {
    "arxiv_id": "2412.09317v1",
    "title": "Multimodal Sentiment Analysis based on Video and Audio Inputs",
    "authors": [
      "Antonio Fernandez",
      "Suzan Awinat"
    ],
    "abstract": "Despite the abundance of current researches working on the sentiment analysis\nfrom videos and audios, finding the best model that gives the highest accuracy\nrate is still considered a challenge for researchers in this field. The main\nobjective of this paper is to prove the usability of emotion recognition models\nthat take video and audio inputs. The datasets used to train the models are the\nCREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned\nmodels that been used are: Facebook/wav2vec2-large for audio and the\nGoogle/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for\neach emotion generated by the two previous models is utilized in the decision\nmaking framework. After disparity in the results, if one of the models gets\nmuch higher accuracy, another test framework is created. The methods used are\nthe Weighted Average method, the Confidence Level Threshold method, the Dynamic\nWeighting Based on Confidence method, and the Rule-Based Logic method. This\nlimited approach gives encouraging results that make future research into these\nmethods viable.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Presented as a full paper in the 15th International Conference on\n  Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2024) October\n  28-30, 2024, Leuven, Belgium",
    "pdf_url": "http://arxiv.org/pdf/2412.09317v1",
    "published_date": "2024-12-12 14:42:10 UTC",
    "updated_date": "2024-12-12 14:42:10 UTC"
  },
  {
    "arxiv_id": "2412.09315v1",
    "title": "Beware of Metacognitive Laziness: Effects of Generative Artificial Intelligence on Learning Motivation, Processes, and Performance",
    "authors": [
      "Yizhou Fan",
      "Luzhen Tang",
      "Huixiao Le",
      "Kejie Shen",
      "Shufang Tan",
      "Yueying Zhao",
      "Yuan Shen",
      "Xinyu Li",
      "Dragan Gašević"
    ],
    "abstract": "With the continuous development of technological and educational innovation,\nlearners nowadays can obtain a variety of support from agents such as teachers,\npeers, education technologies, and recently, generative artificial intelligence\nsuch as ChatGPT. The concept of hybrid intelligence is still at a nascent\nstage, and how learners can benefit from a symbiotic relationship with various\nagents such as AI, human experts and intelligent learning systems is still\nunknown. The emerging concept of hybrid intelligence also lacks deep insights\nand understanding of the mechanisms and consequences of hybrid human-AI\nlearning based on strong empirical research. In order to address this gap, we\nconducted a randomised experimental study and compared learners' motivations,\nself-regulated learning processes and learning performances on a writing task\namong different groups who had support from different agents (ChatGPT, human\nexpert, writing analytics tools, and no extra tool). A total of 117 university\nstudents were recruited, and their multi-channel learning, performance and\nmotivation data were collected and analysed. The results revealed that:\nlearners who received different learning support showed no difference in\npost-task intrinsic motivation; there were significant differences in the\nfrequency and sequences of the self-regulated learning processes among groups;\nChatGPT group outperformed in the essay score improvement but their knowledge\ngain and transfer were not significantly different. Our research found that in\nthe absence of differences in motivation, learners with different supports\nstill exhibited different self-regulated learning processes, ultimately leading\nto differentiated performance. What is particularly noteworthy is that AI\ntechnologies such as ChatGPT may promote learners' dependence on technology and\npotentially trigger metacognitive laziness.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09315v1",
    "published_date": "2024-12-12 14:32:39 UTC",
    "updated_date": "2024-12-12 14:32:39 UTC"
  },
  {
    "arxiv_id": "2412.09311v1",
    "title": "Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation",
    "authors": [
      "Davor Vukadin",
      "Petar Afrić",
      "Marin Šilić",
      "Goran Delač"
    ],
    "abstract": "Recent advancement in deep-neural network performance led to the development\nof new state-of-the-art approaches in numerous areas. However, the black-box\nnature of neural networks often prohibits their use in areas where model\nexplainability and model transparency are crucial. Over the years, researchers\nproposed many algorithms to aid neural network understanding and provide\nadditional information to the human expert. One of the most popular methods\nbeing Layer-Wise Relevance Propagation (LRP). This method assigns local\nrelevance based on the pixel-wise decomposition of nonlinear classifiers. With\nthe rise of attribution method research, there has emerged a pressing need to\nassess and evaluate their performance. Numerous metrics have been proposed,\neach assessing an individual property of attribution methods such as\nfaithfulness, robustness or localization. Unfortunately, no single metric is\ndeemed optimal for every case, and researchers often use several metrics to\ntest the quality of the attribution maps. In this work, we address the\nshortcomings of the current LRP formulations and introduce a novel method for\ndetermining the relevance of input neurons through layer-wise relevance\npropagation. Furthermore, we apply this approach to the recently developed\nVision Transformer architecture and evaluate its performance against existing\nmethods on two image classification datasets, namely ImageNet and PascalVOC.\nOur results clearly demonstrate the advantage of our proposed method.\nFurthermore, we discuss the insufficiencies of current evaluation metrics for\nattribution-based explainability and propose a new evaluation metric that\ncombines the notions of faithfulness, robustness and contrastiveness. We\nutilize this new metric to evaluate the performance of various\nattribution-based methods. Our code is available at:\nhttps://github.com/davor10105/relative-absolute-magnitude-propagation",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.CV",
    "comment": "30 pages, 16 figures, 13 tables, ACM Transactions on Intelligence\n  Systems and Technology",
    "pdf_url": "http://arxiv.org/pdf/2412.09311v1",
    "published_date": "2024-12-12 14:25:56 UTC",
    "updated_date": "2024-12-12 14:25:56 UTC"
  },
  {
    "arxiv_id": "2412.09286v1",
    "title": "Learning Novel Skills from Language-Generated Demonstrations",
    "authors": [
      "Ao-Qun Jin",
      "Tian-Yu Xiang",
      "Xiao-Hu Zhou",
      "Mei-Jiang Gui",
      "Xiao-Liang Xie",
      "Shi-Qi Liu",
      "Shuang-Yi Wang",
      "Yue Cao",
      "Sheng-Bin Duan",
      "Fu-Chao Xie",
      "Zeng-Guang Hou"
    ],
    "abstract": "Current robot learning algorithms for acquiring novel skills often rely on\ndemonstration datasets or environment interactions, resulting in high labor\ncosts and potential safety risks. To address these challenges, this study\nproposes a skill-learning framework that enables robots to acquire novel skills\nfrom natural language instructions. The proposed pipeline leverages\nvision-language models to generate demonstration videos of novel skills, which\nare processed by an inverse dynamics model to extract actions from the\nunlabeled demonstrations. These actions are subsequently mapped to\nenvironmental contexts via imitation learning, enabling robots to learn new\nskills effectively. Experimental evaluations in the MetaWorld simulation\nenvironments demonstrate the pipeline's capability to generate high-fidelity\nand reliable demonstrations. Using the generated demonstrations, various skill\nlearning algorithms achieve an accomplishment rate three times the original on\nnovel tasks. These results highlight a novel approach to robot learning,\noffering a foundation for the intuitive and intelligent acquisition of novel\nrobotic skills.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09286v1",
    "published_date": "2024-12-12 13:56:36 UTC",
    "updated_date": "2024-12-12 13:56:36 UTC"
  },
  {
    "arxiv_id": "2412.09283v1",
    "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
    "authors": [
      "Tiehan Fan",
      "Kepan Nan",
      "Rui Xie",
      "Penghao Zhou",
      "Zhenheng Yang",
      "Chaoyou Fu",
      "Xiang Li",
      "Jian Yang",
      "Ying Tai"
    ],
    "abstract": "Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09283v1",
    "published_date": "2024-12-12 13:48:40 UTC",
    "updated_date": "2024-12-12 13:48:40 UTC"
  },
  {
    "arxiv_id": "2412.09278v2",
    "title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine",
    "authors": [
      "Xiaoshuang Huang",
      "Lingdong Shen",
      "Jia Liu",
      "Fangxin Shang",
      "Hongxiang Li",
      "Haifeng Huang",
      "Yehui Yang"
    ],
    "abstract": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09278v2",
    "published_date": "2024-12-12 13:41:35 UTC",
    "updated_date": "2025-01-10 10:07:55 UTC"
  },
  {
    "arxiv_id": "2412.09269v1",
    "title": "Towards Understanding the Robustness of LLM-based Evaluations under Perturbations",
    "authors": [
      "Manav Chaudhary",
      "Harshit Gupta",
      "Savita Bhat",
      "Vasudeva Varma"
    ],
    "abstract": "Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICON 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.09269v1",
    "published_date": "2024-12-12 13:31:58 UTC",
    "updated_date": "2024-12-12 13:31:58 UTC"
  },
  {
    "arxiv_id": "2412.09264v1",
    "title": "Speeding up approximate MAP by applying domain knowledge about relevant variables",
    "authors": [
      "Johan Kwisthout",
      "Andrew Schroeder"
    ],
    "abstract": "The MAP problem in Bayesian networks is notoriously intractable, even when\napproximated. In an earlier paper we introduced the Most Frugal Explanation\nheuristic approach to solving MAP, by partitioning the set of intermediate\nvariables (neither observed nor part of the MAP variables) into a set of\nrelevant variables, which are marginalized out, and irrelevant variables, which\nwill be assigned a sampled value from their domain. In this study we explore\nwhether knowledge about which variables are relevant for a particular query\n(i.e., domain knowledge) speeds up computation sufficiently to beat both exact\nMAP as well as approximate MAP while giving reasonably accurate results. Our\nresults are inconclusive, but also show that this probably depends on the\nspecifics of the MAP query, most prominently the number of MAP variables.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09264v1",
    "published_date": "2024-12-12 13:22:01 UTC",
    "updated_date": "2024-12-12 13:22:01 UTC"
  },
  {
    "arxiv_id": "2412.09263v2",
    "title": "First Train to Generate, then Generate to Train: UnitedSynT5 for Few-Shot NLI",
    "authors": [
      "Sourav Banerjee",
      "Anush Mahajan",
      "Ayushi Agarwal",
      "Eishkaran Singh"
    ],
    "abstract": "Natural Language Inference (NLI) tasks require identifying the relationship\nbetween sentence pairs, typically classified as entailment, contradiction, or\nneutrality. While the current state-of-the-art (SOTA) model, Entailment\nFew-Shot Learning (EFL), achieves a 93.1% accuracy on the Stanford Natural\nLanguage Inference (SNLI) dataset, further advancements are constrained by the\ndataset's limitations. To address this, we propose a novel approach leveraging\nsynthetic data augmentation to enhance dataset diversity and complexity. We\npresent UnitedSynT5, an advanced extension of EFL that leverages a T5-based\ngenerator to synthesize additional premise-hypothesis pairs, which are\nrigorously cleaned and integrated into the training data. These augmented\nexamples are processed within the EFL framework, embedding labels directly into\nhypotheses for consistency. We train a GTR-T5-XL model on this expanded\ndataset, achieving a new benchmark of 94.7% accuracy on the SNLI dataset, 94.0%\naccuracy on the E-SNLI dataset, and 92.6% accuracy on the MultiNLI dataset,\nsurpassing the previous SOTA models. This research demonstrates the potential\nof synthetic data augmentation in improving NLI models, offering a path forward\nfor further advancements in natural language understanding tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09263v2",
    "published_date": "2024-12-12 13:21:09 UTC",
    "updated_date": "2024-12-13 06:28:11 UTC"
  },
  {
    "arxiv_id": "2412.14193v2",
    "title": "Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation",
    "authors": [
      "Kathrin Wardatzky",
      "Oana Inel",
      "Luca Rossetto",
      "Abraham Bernstein"
    ],
    "abstract": "Adding explanations to recommender systems is said to have multiple benefits,\nsuch as increasing user trust or system transparency. Previous work from other\napplication areas suggests that specific user characteristics impact the users'\nperception of the explanation. However, we rarely find this type of evaluation\nfor recommender systems explanations. This paper addresses this gap by\nsurveying 124 papers in which recommender systems explanations were evaluated\nin user studies. We analyzed their participant descriptions and study results\nwhere the impact of user characteristics on the explanation effects was\nmeasured. Our findings suggest that the results from the surveyed studies\npredominantly cover specific users who do not necessarily represent the users\nof recommender systems in the evaluation domain. This may seriously hamper the\ngeneralizability of any insights we may gain from current studies on\nexplanations in recommender systems. We further find inconsistencies in the\ndata reporting, which impacts the reproducibility of the reported results.\nHence, we recommend actions to move toward a more inclusive and reproducible\nevaluation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.IR",
      "A.1; H.3.3; H.5.2; K.4"
    ],
    "primary_category": "cs.HC",
    "comment": "33 pages, 2 figures. Submitted to ACM Transactions of Recommender\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2412.14193v2",
    "published_date": "2024-12-12 13:01:30 UTC",
    "updated_date": "2025-02-03 16:50:32 UTC"
  },
  {
    "arxiv_id": "2412.09240v1",
    "title": "VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation with Unsupervised Domain Adaptation",
    "authors": [
      "Roberto Alcover-Couso",
      "Marcos Escudero-Viñolo",
      "Juan C. SanMiguel",
      "Jesus Bescos"
    ],
    "abstract": "Segmentation models are typically constrained by the categories defined\nduring training. To address this, researchers have explored two independent\napproaches: adapting Vision-Language Models (VLMs) and leveraging synthetic\ndata. However, VLMs often struggle with granularity, failing to disentangle\nfine-grained concepts, while synthetic data-based methods remain limited by the\nscope of available datasets.\n  This paper proposes enhancing segmentation accuracy across diverse domains by\nintegrating Vision-Language reasoning with key strategies for Unsupervised\nDomain Adaptation (UDA). First, we improve the fine-grained segmentation\ncapabilities of VLMs through multi-scale contextual data, robust text\nembeddings with prompt augmentation, and layer-wise fine-tuning in our proposed\nFoundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS)\nframework. Next, we incorporate these enhancements into a UDA framework by\nemploying distillation to stabilize training and cross-domain mixed sampling to\nboost adaptability without compromising generalization. The resulting\nUDA-FROVSS framework is the first UDA approach to effectively adapt across\ndomains without requiring shared categories.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09240v1",
    "published_date": "2024-12-12 12:49:42 UTC",
    "updated_date": "2024-12-12 12:49:42 UTC"
  },
  {
    "arxiv_id": "2412.09237v2",
    "title": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation",
    "authors": [
      "Yijun Liu",
      "Wu Liu",
      "Xiaoyan Gu",
      "Yong Rui",
      "Xiaodong He",
      "Yongdong Zhang"
    ],
    "abstract": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09237v2",
    "published_date": "2024-12-12 12:47:09 UTC",
    "updated_date": "2024-12-13 03:33:38 UTC"
  },
  {
    "arxiv_id": "2412.09230v1",
    "title": "Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering",
    "authors": [
      "Sai Bhargav Rongali",
      "Mohamad Hassan N C",
      "Ankit Jha",
      "Neha Bhargava",
      "Saurabh Prasad",
      "Biplab Banerjee"
    ],
    "abstract": "This paper tackles the intricate challenge of video question-answering\n(VideoQA). Despite notable progress, current methods fall short of effectively\nintegrating questions with video frames and semantic object-level abstractions\nto create question-aware video representations. We introduce Local-Global\nQuestion Aware Video Embedding (LGQAVE), which incorporates three major\ninnovations to integrate multi-modal knowledge better and emphasize semantic\nvisual concepts relevant to specific questions. LGQAVE moves beyond traditional\nad-hoc frame sampling by utilizing a cross-attention mechanism that precisely\nidentifies the most relevant frames concerning the questions. It captures the\ndynamics of objects within these frames using distinct graphs, grounding them\nin question semantics with the miniGPT model. These graphs are processed by a\nquestion-aware dynamic graph transformer (Q-DGT), which refines the outputs to\ndevelop nuanced global and local video representations. An additional\ncross-attention module integrates these local and global embeddings to generate\nthe final video embeddings, which a language model uses to generate answers.\nExtensive evaluations across multiple benchmarks demonstrate that LGQAVE\nsignificantly outperforms existing models in delivering accurate multi-choice\nand open-ended answers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09230v1",
    "published_date": "2024-12-12 12:39:07 UTC",
    "updated_date": "2024-12-12 12:39:07 UTC"
  },
  {
    "arxiv_id": "2412.09223v1",
    "title": "CSSDH: An Ontology for Social Determinants of Health to Operational Continuity of Care Data Interoperability",
    "authors": [
      "Subhashis Das",
      "Debashis Naskar",
      "Sara Rodriguez Gonzalez"
    ],
    "abstract": "The rise of digital platforms has led to an increasing reliance on\ntechnology-driven, home-based healthcare solutions, enabling individuals to\nmonitor their health and share information with healthcare professionals as\nneeded. However, creating an efficient care plan management system requires\nmore than just analyzing hospital summaries and Electronic Health Records\n(EHRs). Factors such as individual user needs and social determinants of\nhealth, including living conditions and the flow of healthcare information\nbetween different settings, must also be considered. Challenges in this complex\nhealthcare network involve schema diversity (in EHRs, personal health records,\netc.) and terminology diversity (e.g., ICD, SNOMED-CT) across ancillary\nhealthcare operations. Establishing interoperability among various systems and\napplications is crucial, with the European Interoperability Framework (EIF)\nemphasizing the need for patient-centric access and control of healthcare data.\nIn this paper, we propose an integrated ontological model, the Common Semantic\nData Model for Social Determinants of Health (CSSDH), by combining ISO/DIS\n13940:2024 ContSys with WHO Social Determinants of Health. CSSDH aims to\nachieve interoperability within the Continuity of Care Network.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "68T27",
      "I.2.4; I.2.1"
    ],
    "primary_category": "cs.LO",
    "comment": "6 pages, 3 figures, conference-The 25th International Conference on\n  Intelligent Data Engineering and Automated Learning",
    "pdf_url": "http://arxiv.org/pdf/2412.09223v1",
    "published_date": "2024-12-12 12:25:33 UTC",
    "updated_date": "2024-12-12 12:25:33 UTC"
  },
  {
    "arxiv_id": "2412.10461v1",
    "title": "EvoSampling: A Granular Ball-based Evolutionary Hybrid Sampling with Knowledge Transfer for Imbalanced Learning",
    "authors": [
      "Wenbin Pei",
      "Ruohao Dai",
      "Bing Xue",
      "Mengjie Zhang",
      "Qiang Zhang",
      "Yiu-Ming Cheung",
      "Shuyin Xia"
    ],
    "abstract": "Class imbalance would lead to biased classifiers that favor the majority\nclass and disadvantage the minority class. Unfortunately, from a practical\nperspective, the minority class is of importance in many real-life\napplications. Hybrid sampling methods address this by oversampling the minority\nclass to increase the number of its instances, followed by undersampling to\nremove low-quality instances. However, most existing sampling methods face\ndifficulties in generating diverse high-quality instances and often fail to\nremove noise or low-quality instances on a larger scale effectively. This paper\ntherefore proposes an evolutionary multi-granularity hybrid sampling method,\ncalled EvoSampling. During the oversampling process, genetic programming (GP)\nis used with multi-task learning to effectively and efficiently generate\ndiverse high-quality instances. During the undersampling process, we develop a\ngranular ball-based undersampling method that removes noise in a multi-granular\nfashion, thereby enhancing data quality. Experiments on 20 imbalanced datasets\ndemonstrate that EvoSampling effectively enhances the performance of various\nclassification algorithms by providing better datasets than existing sampling\nmethods. Besides, ablation studies further indicate that allowing knowledge\ntransfer accelerates the GP's evolutionary learning process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10461v1",
    "published_date": "2024-12-12 11:35:20 UTC",
    "updated_date": "2024-12-12 11:35:20 UTC"
  },
  {
    "arxiv_id": "2412.10460v1",
    "title": "Enriching Multimodal Sentiment Analysis through Textual Emotional Descriptions of Visual-Audio Content",
    "authors": [
      "Sheng Wu",
      "Xiaobao Wang",
      "Longbiao Wang",
      "Dongxiao He",
      "Jianwu Dang"
    ],
    "abstract": "Multimodal Sentiment Analysis (MSA) stands as a critical research frontier,\nseeking to comprehensively unravel human emotions by amalgamating text, audio,\nand visual data. Yet, discerning subtle emotional nuances within audio and\nvideo expressions poses a formidable challenge, particularly when emotional\npolarities across various segments appear similar. In this paper, our objective\nis to spotlight emotion-relevant attributes of audio and visual modalities to\nfacilitate multimodal fusion in the context of nuanced emotional shifts in\nvisual-audio scenarios. To this end, we introduce DEVA, a progressive fusion\nframework founded on textual sentiment descriptions aimed at accentuating\nemotional features of visual-audio content. DEVA employs an Emotional\nDescription Generator (EDG) to transmute raw audio and visual data into\ntextualized sentiment descriptions, thereby amplifying their emotional\ncharacteristics. These descriptions are then integrated with the source data to\nyield richer, enhanced features. Furthermore, DEVA incorporates the Text-guided\nProgressive Fusion Module (TPF), leveraging varying levels of text as a core\nmodality guide. This module progressively fuses visual-audio minor modalities\nto alleviate disparities between text and visual-audio modalities. Experimental\nresults on widely used sentiment analysis benchmark datasets, including MOSI,\nMOSEI, and CH-SIMS, underscore significant enhancements compared to\nstate-of-the-art models. Moreover, fine-grained emotion experiments corroborate\nthe robust sensitivity of DEVA to subtle emotional variations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10460v1",
    "published_date": "2024-12-12 11:30:41 UTC",
    "updated_date": "2024-12-12 11:30:41 UTC"
  },
  {
    "arxiv_id": "2412.09165v3",
    "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
    "authors": [
      "Zhijie Nie",
      "Zhangchi Feng",
      "Mingxin Li",
      "Cunwang Zhang",
      "Yanzhao Zhang",
      "Dingkun Long",
      "Richong Zhang"
    ],
    "abstract": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders",
    "pdf_url": "http://arxiv.org/pdf/2412.09165v3",
    "published_date": "2024-12-12 10:50:26 UTC",
    "updated_date": "2025-03-20 16:15:29 UTC"
  },
  {
    "arxiv_id": "2412.10459v2",
    "title": "Conformal Prediction on Quantifying Uncertainty of Dynamic Systems",
    "authors": [
      "Aoming Liang",
      "Qi Liu",
      "Lei Xu",
      "Fahad Sohrab",
      "Weicheng Cui",
      "Changhui Song",
      "Moncef Gabbouj"
    ],
    "abstract": "Numerous studies have focused on learning and understanding the dynamics of\nphysical systems from video data, such as spatial intelligence. Artificial\nintelligence requires quantitative assessments of the uncertainty of the model\nto ensure reliability. However, there is still a relative lack of systematic\nassessment of the uncertainties, particularly the uncertainties of the physical\ndata. Our motivation is to introduce conformal prediction into the uncertainty\nassessment of dynamical systems, providing a method supported by theoretical\nguarantees. This paper uses the conformal prediction method to assess\nuncertainties with benchmark operator learning methods. We have also compared\nthe Monte Carlo Dropout and Ensemble methods in the partial differential\nequations dataset, effectively evaluating uncertainty through straight\nroll-outs, making it ideal for time-series tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10459v2",
    "published_date": "2024-12-12 10:45:02 UTC",
    "updated_date": "2024-12-17 11:35:02 UTC"
  },
  {
    "arxiv_id": "2501.10368v1",
    "title": "The Potential of Answer Classes in Large-scale Written Computer-Science Exams -- Vol. 2",
    "authors": [
      "Dominic Lohr",
      "Marc Berges",
      "Michael Kohlhase",
      "Florian Rabe"
    ],
    "abstract": "Students' answers to tasks provide a valuable source of information in\nteaching as they result from applying cognitive processes to a learning content\naddressed in the task. Due to steadily increasing course sizes, analyzing\nstudent answers is frequently the only means of obtaining evidence about\nstudent performance. However, in many cases, resources are limited, and when\nevaluating exams, the focus is solely on identifying correct or incorrect\nanswers. This overlooks the value of analyzing incorrect answers, which can\nhelp improve teaching strategies or identify misconceptions to be addressed in\nthe next cohort.\n  In teacher training for secondary education, assessment guidelines are\nmandatory for every exam, including anticipated errors and misconceptions. We\napplied this concept to a university exam with 462 students and 41 tasks. For\neach task, the instructors developed answer classes -- classes of expected\nresponses, to which student answers were mapped during the exam correction\nprocess. The experiment resulted in a shift in mindset among the tutors and\ninstructors responsible for the course: after initially having great\nreservations about whether the significant additional effort would yield an\nappropriate benefit, the procedure was subsequently found to be extremely\nvaluable.\n  The concept presented, and the experience gained from the experiment were\ncast into a system with which it is possible to correct paper-based exams on\nthe basis of answer classes. This updated version of the paper provides an\noverview and new potential in the course of using the digital version of the\napproach.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at Commentarii Informaticae Didacticae (CID) 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.10368v1",
    "published_date": "2024-12-12 10:20:39 UTC",
    "updated_date": "2024-12-12 10:20:39 UTC"
  },
  {
    "arxiv_id": "2412.09661v1",
    "title": "Language model driven: a PROTAC generation pipeline with dual constraints of structure and property",
    "authors": [
      "Jinsong Shao",
      "Qineng Gong",
      "Zeyu Yin",
      "Yu Chen",
      "Yajie Hao",
      "Lei Zhang",
      "Linlin Jiang",
      "Min Yao",
      "Jinlong Li",
      "Fubo Wang",
      "Li Wang"
    ],
    "abstract": "The imperfect modeling of ternary complexes has limited the application of\ncomputer-aided drug discovery tools in PROTAC research and development. In this\nstudy, an AI-assisted approach for PROTAC molecule design pipeline named\nLM-PROTAC was developed, which stands for language model driven Proteolysis\nTargeting Chimera, by embedding a transformer-based generative model with dual\nconstraints on structure and properties, referred to as the DCT. This study\nutilized the fragmentation representation of molecules and developed a language\nmodel driven pipeline. Firstly, a language model driven affinity model for\nprotein compounds to screen molecular fragments with high affinity for the\ntarget protein. Secondly, structural and physicochemical properties of these\nfragments were constrained during the generation process to meet specific\nscenario requirements. Finally, a two-round screening of the preliminary\ngenerated molecules using a multidimensional property prediction model to\ngenerate a batch of PROTAC molecules capable of degrading disease-relevant\ntarget proteins for validation in vitro experiments, thus achieving a complete\nsolution for AI-assisted PROTAC drug generation. Taking the tumor key target\nWnt3a as an example, the LM-PROTAC pipeline successfully generated PROTAC\nmolecules capable of inhibiting Wnt3a. The results show that DCT can\nefficiently generate PROTAC that targets and hydrolyses Wnt3a.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "I.2.7; D.3.2"
    ],
    "primary_category": "q-bio.QM",
    "comment": "61 pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09661v1",
    "published_date": "2024-12-12 10:15:12 UTC",
    "updated_date": "2024-12-12 10:15:12 UTC"
  },
  {
    "arxiv_id": "2412.09126v1",
    "title": "Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning",
    "authors": [
      "Meng Shen",
      "Yake Wei",
      "Jianxiong Yin",
      "Deepu Rajan",
      "Di Hu",
      "Simon See"
    ],
    "abstract": "Training multimodal models requires a large amount of labeled data. Active\nlearning (AL) aim to reduce labeling costs. Most AL methods employ warm-start\napproaches, which rely on sufficient labeled data to train a well-calibrated\nmodel that can assess the uncertainty and diversity of unlabeled data. However,\nwhen assembling a dataset, labeled data are often scarce initially, leading to\na cold-start problem. Additionally, most AL methods seldom address multimodal\ndata, highlighting a research gap in this field. Our research addresses these\nissues by developing a two-stage method for Multi-Modal Cold-Start Active\nLearning (MMCSAL).\n  Firstly, we observe the modality gap, a significant distance between the\ncentroids of representations from different modalities, when only using\ncross-modal pairing information as self-supervision signals. This modality gap\naffects data selection process, as we calculate both uni-modal and cross-modal\ndistances. To address this, we introduce uni-modal prototypes to bridge the\nmodality gap. Secondly, conventional AL methods often falter in multimodal\nscenarios where alignment between modalities is overlooked. Therefore, we\npropose enhancing cross-modal alignment through regularization, thereby\nimproving the quality of selected multimodal data pairs in AL. Finally, our\nexperiments demonstrate MMCSAL's efficacy in selecting multimodal data pairs\nacross three multimodal datasets.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MM",
    "comment": "11 pages, ACMMM Asia 2024, Oral Presentation",
    "pdf_url": "http://arxiv.org/pdf/2412.09126v1",
    "published_date": "2024-12-12 10:03:46 UTC",
    "updated_date": "2024-12-12 10:03:46 UTC"
  },
  {
    "arxiv_id": "2412.09125v1",
    "title": "Goal-Driven Query Answering over First- and Second-Order Dependencies with Equality",
    "authors": [
      "Efthymia Tsamoura",
      "Boris Motik"
    ],
    "abstract": "Query answering over data with dependencies plays a central role in most\napplications of dependencies. The problem is commonly solved by using a\nsuitable variant of the chase algorithm to compute a universal model of the\ndependencies and the data and thus explicate all knowledge implicit in the\ndependencies. After this preprocessing step, an arbitrary conjunctive query\nover the dependencies and the data can be answered by evaluating it the\ncomputed universal model. If, however, the query to be answered is fixed and\nknown in advance, computing the universal model is often inefficient as many\ninferences made during this process can be irrelevant to a given query. In such\ncases, a goal-driven approach, which avoids drawing unnecessary inferences,\npromises to be more efficient and thus preferable in practice.\n  In this paper we present what we believe to be the first technique for\ngoal-driven query answering over first- and second-order dependencies with\nequality reasoning. Our technique transforms the input dependencies so that\napplying the chase to the output avoids many inferences that are irrelevant to\nthe query. The transformation proceeds in several steps, which comprise the\nfollowing three novel techniques. First, we present a variant of the\nsingularisation technique by Marnette [60] that is applicable to second-order\ndependencies and that corrects an incompleteness of a related formulation by\nten Cate et al. [74]. Second, we present a relevance analysis technique that\ncan eliminate from the input dependencies that provably do not contribute to\nquery answers. Third, we present a variant of the magic sets algorithm [19]\nthat can handle second-order dependencies with equality reasoning. We also\npresent the results of an extensive empirical evaluation, which show that\ngoal-driven query answering can be orders of magnitude faster than computing\nthe full universal model.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LO",
      "F.4.1; I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "47 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09125v1",
    "published_date": "2024-12-12 10:02:16 UTC",
    "updated_date": "2024-12-12 10:02:16 UTC"
  },
  {
    "arxiv_id": "2412.09104v2",
    "title": "In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning",
    "authors": [
      "Songjun Tu",
      "Jingbo Sun",
      "Qichao Zhang",
      "Yaocheng Zhang",
      "Jia Liu",
      "Ke Chen",
      "Dongbin Zhao"
    ],
    "abstract": "Offline preference-based reinforcement learning (PbRL) typically operates in\ntwo phases: first, use human preferences to learn a reward model and annotate\nrewards for a reward-free offline dataset; second, learn a policy by optimizing\nthe learned reward via offline RL. However, accurately modeling step-wise\nrewards from trajectory-level preference feedback presents inherent challenges.\nThe reward bias introduced, particularly the overestimation of predicted\nrewards, leads to optimistic trajectory stitching, which undermines the\npessimism mechanism critical to the offline RL phase. To address this\nchallenge, we propose In-Dataset Trajectory Return Regularization (DTR) for\noffline PbRL, which leverages conditional sequence modeling to mitigate the\nrisk of learning inaccurate trajectory stitching under reward bias.\nSpecifically, DTR employs Decision Transformer and TD-Learning to strike a\nbalance between maintaining fidelity to the behavior policy with high\nin-dataset trajectory returns and selecting optimal actions based on high\nreward labels. Additionally, we introduce an ensemble normalization technique\nthat effectively integrates multiple reward models, balancing the tradeoff\nbetween reward differentiation and accuracy. Empirical evaluations on various\nbenchmarks demonstrate the superiority of DTR over other state-of-the-art\nbaselines.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, Proceedings of the 39th AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.09104v2",
    "published_date": "2024-12-12 09:35:47 UTC",
    "updated_date": "2024-12-21 07:50:07 UTC"
  },
  {
    "arxiv_id": "2412.09102v1",
    "title": "PolyIPA -- Multilingual Phoneme-to-Grapheme Conversion Model",
    "authors": [
      "Davor Lauc"
    ],
    "abstract": "This paper presents PolyIPA, a novel multilingual phoneme-to-grapheme\nconversion model designed for multilingual name transliteration, onomastic\nresearch, and information retrieval. The model leverages two helper models\ndeveloped for data augmentation: IPA2vec for finding soundalikes across\nlanguages, and similarIPA for handling phonetic notation variations. Evaluated\non a test set that spans multiple languages and writing systems, the model\nachieves a mean Character Error Rate of 0.055 and a character-level BLEU score\nof 0.914, with particularly strong performance on languages with shallow\northographies. The implementation of beam search further improves practical\nutility, with top-3 candidates reducing the effective error rate by 52.7\\% (to\nCER: 0.026), demonstrating the model's effectiveness for cross-linguistic\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09102v1",
    "published_date": "2024-12-12 09:29:59 UTC",
    "updated_date": "2024-12-12 09:29:59 UTC"
  },
  {
    "arxiv_id": "2412.09101v2",
    "title": "Temporal Numeric Planning with Patterns",
    "authors": [
      "Matteo Cardellini",
      "Enrico Giunchiglia"
    ],
    "abstract": "We consider temporal numeric planning problems $\\Pi$ expressed in PDDL2.1\nlevel 3, and show how to produce SMT formulas $(i)$ whose models correspond to\nvalid plans of $\\Pi$, and $(ii)$ that extend the recently proposed planning\nwith patterns approach from the numeric to the temporal case. We prove the\ncorrectness and completeness of the approach and show that it performs very\nwell on 10 domains with required concurrency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.09101v2",
    "published_date": "2024-12-12 09:28:34 UTC",
    "updated_date": "2024-12-18 12:31:58 UTC"
  },
  {
    "arxiv_id": "2412.09094v3",
    "title": "Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion",
    "authors": [
      "Ben Liu",
      "Jihai Zhang",
      "Fangquan Lin",
      "Cheng Yang",
      "Min Peng"
    ],
    "abstract": "Large Language Models (LLMs) present massive inherent knowledge and superior\nsemantic comprehension capability, which have revolutionized various tasks in\nnatural language processing. Despite their success, a critical gap remains in\nenabling LLMs to perform knowledge graph completion (KGC). Empirical evidence\nsuggests that LLMs consistently perform worse than conventional KGC approaches,\neven through sophisticated prompt design or tailored instruction-tuning.\nFundamentally, applying LLMs on KGC introduces several critical challenges,\nincluding a vast set of entity candidates, hallucination issue of LLMs, and\nunder-exploitation of the graph structure. To address these challenges, we\npropose a novel instruction-tuning-based method, namely FtG. Specifically, we\npresent a filter-then-generate paradigm and formulate the KGC task into a\nmultiple-choice question format. In this way, we can harness the capability of\nLLMs while mitigating the issue casused by hallucinations. Moreover, we devise\na flexible ego-graph serialization prompt and employ a structure-text adapter\nto couple structure and text information in a contextualized manner.\nExperimental results demonstrate that FtG achieves substantial performance gain\ncompared to existing state-of-the-art methods. The instruction dataset and code\nare available at https://github.com/LB0828/FtG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2412.09094v3",
    "published_date": "2024-12-12 09:22:04 UTC",
    "updated_date": "2025-02-08 13:40:23 UTC"
  },
  {
    "arxiv_id": "2412.09086v1",
    "title": "Understanding Opportunities and Risks of Synthetic Relationships: Leveraging the Power of Longitudinal Research with Customised AI Tools",
    "authors": [
      "Alfio Ventura",
      "Nils Köbis"
    ],
    "abstract": "This position paper discusses the benefits of longitudinal behavioural\nresearch with customised AI tools for exploring the opportunities and risks of\nsynthetic relationships. Synthetic relationships are defined as \"continuing\nassociations between humans and AI tools that interact with one another wherein\nthe AI tool(s) influence(s) humans' thoughts, feelings, and/or actions.\"\n(Starke et al., 2024). These relationships can potentially improve health,\neducation, and the workplace, but they also bring the risk of subtle\nmanipulation and privacy and autonomy concerns. To harness the opportunities of\nsynthetic relationships and mitigate their risks, we outline a methodological\napproach that complements existing findings. We propose longitudinal research\ndesigns with self-assembled AI agents that enable the integration of detailed\nbehavioural and self-reported data.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "J.4"
    ],
    "primary_category": "cs.HC",
    "comment": "This is a \"Position paper accepted for CONVERSATIONS 2024 - the 8th\n  International Workshop on Chatbots and Human-Centred AI, hosted by CERTH,\n  Thessaloniki, Greece, December 4-5, 2024.\" The original publication is\n  available on the workshop website: https://2024.conversations.ws/papers/ .\n  This document is identical to the original and is mainly available here for\n  accessibility and discoverability",
    "pdf_url": "http://arxiv.org/pdf/2412.09086v1",
    "published_date": "2024-12-12 09:13:43 UTC",
    "updated_date": "2024-12-12 09:13:43 UTC"
  },
  {
    "arxiv_id": "2412.09078v5",
    "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
    "authors": [
      "Zhenni Bi",
      "Kai Han",
      "Chuanjian Liu",
      "Yehui Tang",
      "Yunhe Wang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2412.09078v5",
    "published_date": "2024-12-12 09:01:18 UTC",
    "updated_date": "2025-04-01 12:48:43 UTC"
  },
  {
    "arxiv_id": "2412.09058v1",
    "title": "EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems",
    "authors": [
      "Huanqi Yang",
      "Mingzhe Li",
      "Mingda Han",
      "Zhenjiang Li",
      "Weitao Xu"
    ],
    "abstract": "Embedded IoT system development is crucial for enabling seamless connectivity\nand functionality across a wide range of applications. However, such a complex\nprocess requires cross-domain knowledge of hardware and software and hence\noften necessitates direct developer involvement, making it labor-intensive,\ntime-consuming, and error-prone. To address this challenge, this paper\nintroduces EmbedGenius, the first fully automated software development platform\nfor general-purpose embedded IoT systems. The key idea is to leverage the\nreasoning ability of Large Language Models (LLMs) and embedded system expertise\nto automate the hardware-in-the-loop development process. The main methods\ninclude a component-aware library resolution method for addressing hardware\ndependencies, a library knowledge generation method that injects utility domain\nknowledge into LLMs, and an auto-programming method that ensures successful\ndeployment. We evaluate EmbedGenius's performance across 71 modules and four\nmainstream embedded development platforms with over 350 IoT tasks. Experimental\nresults show that EmbedGenius can generate codes with an accuracy of 95.7% and\ncomplete tasks with a success rate of 86.5%, surpassing human-in-the-loop\nbaselines by 15.6%--37.7% and 25.5%--53.4%, respectively. We also show\nEmbedGenius's potential through case studies in environmental monitoring and\nremote control systems development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09058v1",
    "published_date": "2024-12-12 08:34:12 UTC",
    "updated_date": "2024-12-12 08:34:12 UTC"
  },
  {
    "arxiv_id": "2412.09056v1",
    "title": "A Context-Enhanced Framework for Sequential Graph Reasoning",
    "authors": [
      "Shuo Shi",
      "Chao Peng",
      "Chenyang Xu",
      "Zhengfeng Yang"
    ],
    "abstract": "The paper studies sequential reasoning over graph-structured data, which\nstands as a fundamental task in various trending fields like automated math\nproblem solving and neural graph algorithm learning, attracting a lot of\nresearch interest. Simultaneously managing both sequential and graph-structured\ninformation in such tasks presents a notable challenge. Over recent years, many\nneural architectures in the literature have emerged to tackle the issue. In\nthis work, we generalize the existing architectures and propose a\ncontext-enhanced framework. The crucial innovation is that the reasoning of\neach step does not only rely on the outcome of the preceding step but also\nleverages the aggregation of information from more historical outcomes. The\nidea stems from our observation that in sequential graph reasoning, each step's\noutcome has a much stronger inner connection with each other compared to\ntraditional seq-to-seq tasks. We show that the framework can effectively\nintegrate with the existing methods, enhancing their reasoning abilities.\nEmpirical evaluations are conducted on the challenging CLRS Reasoning\nBenchmark, and the results demonstrate that the proposed framework\nsignificantly improves the performance of existing architectures, yielding\nstate-of-the-art results across the majority of the datasets within the\nbenchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Appeared at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.09056v1",
    "published_date": "2024-12-12 08:27:51 UTC",
    "updated_date": "2024-12-12 08:27:51 UTC"
  },
  {
    "arxiv_id": "2412.09046v1",
    "title": "Multi-Task Learning with LLMs for Implicit Sentiment Analysis: Data-level and Task-level Automatic Weight Learning",
    "authors": [
      "Wenna Lai",
      "Haoran Xie",
      "Guandong Xu",
      "Qing Li"
    ],
    "abstract": "Implicit sentiment analysis (ISA) presents significant challenges due to the\nabsence of salient cue words. Previous methods have struggled with insufficient\ndata and limited reasoning capabilities to infer underlying opinions.\nIntegrating multi-task learning (MTL) with large language models (LLMs) offers\nthe potential to enable models of varying sizes to reliably perceive and\nrecognize genuine opinions in ISA. However, existing MTL approaches are\nconstrained by two sources of uncertainty: data-level uncertainty, arising from\nhallucination problems in LLM-generated contextual information, and task-level\nuncertainty, stemming from the varying capacities of models to process\ncontextual information. To handle these uncertainties, we introduce MT-ISA, a\nnovel MTL framework that enhances ISA by leveraging the generation and\nreasoning capabilities of LLMs through automatic MTL. Specifically, MT-ISA\nconstructs auxiliary tasks using generative LLMs to supplement sentiment\nelements and incorporates automatic MTL to fully exploit auxiliary data. We\nintroduce data-level and task-level automatic weight learning (AWL), which\ndynamically identifies relationships and prioritizes more reliable data and\ncritical tasks, enabling models of varying sizes to adaptively learn\nfine-grained weights based on their reasoning capabilities. We investigate\nthree strategies for data-level AWL, while also introducing homoscedastic\nuncertainty for task-level AWL. Extensive experiments reveal that models of\nvarying sizes achieve an optimal balance between primary prediction and\nauxiliary tasks in MT-ISA. This underscores the effectiveness and adaptability\nof our approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 6 figures, and 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.09046v1",
    "published_date": "2024-12-12 08:15:16 UTC",
    "updated_date": "2024-12-12 08:15:16 UTC"
  },
  {
    "arxiv_id": "2412.09044v2",
    "title": "Motif Guided Graph Transformer with Combinatorial Skeleton Prototype Learning for Skeleton-Based Person Re-Identification",
    "authors": [
      "Haocong Rao",
      "Chunyan Miao"
    ],
    "abstract": "Person re-identification (re-ID) via 3D skeleton data is a challenging task\nwith significant value in many scenarios. Existing skeleton-based methods\ntypically assume virtual motion relations between all joints, and adopt average\njoint or sequence representations for learning. However, they rarely explore\nkey body structure and motion such as gait to focus on more important body\njoints or limbs, while lacking the ability to fully mine valuable\nspatial-temporal sub-patterns of skeletons to enhance model learning. This\npaper presents a generic Motif guided graph transformer with Combinatorial\nskeleton prototype learning (MoCos) that exploits structure-specific and\ngait-related body relations as well as combinatorial features of skeleton\ngraphs to learn effective skeleton representations for person re-ID. In\nparticular, motivated by the locality within joints' structure and the\nbody-component collaboration in gait, we first propose the motif guided graph\ntransformer (MGT) that incorporates hierarchical structural motifs and gait\ncollaborative motifs, which simultaneously focuses on multi-order local joint\ncorrelations and key cooperative body parts to enhance skeleton relation\nlearning. Then, we devise the combinatorial skeleton prototype learning (CSP)\nthat leverages random spatial-temporal combinations of joint nodes and skeleton\ngraphs to generate diverse sub-skeleton and sub-tracklet representations, which\nare contrasted with the most representative features (prototypes) of each\nidentity to learn class-related semantics and discriminative skeleton\nrepresentations. Extensive experiments validate the superior performance of\nMoCos over existing state-of-the-art models. We further show its generality\nunder RGB-estimated skeletons, different graph modeling, and unsupervised\nscenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025. Codes are available at\n  https://github.com/Kali-Hac/MoCos. The Appendix A for Experiments (13 pages)\n  and Appendix B for Theoretical Analysis (5 pages) are included in the version\n  [v1] at arXiv:2412.09044",
    "pdf_url": "http://arxiv.org/pdf/2412.09044v2",
    "published_date": "2024-12-12 08:13:29 UTC",
    "updated_date": "2025-02-02 02:13:11 UTC"
  },
  {
    "arxiv_id": "2412.10457v1",
    "title": "Explaining Model Overfitting in CNNs via GMM Clustering",
    "authors": [
      "Hui Dou",
      "Xinyu Mu",
      "Mengjun Yi",
      "Feng Han",
      "Jian Zhao",
      "Furao Shen"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) have demonstrated remarkable prowess in\nthe field of computer vision. However, their opaque decision-making processes\npose significant challenges for practical applications. In this study, we\nprovide quantitative metrics for assessing CNN filters by clustering the\nfeature maps corresponding to individual filters in the model via Gaussian\nMixture Model (GMM). By analyzing the clustering results, we screen out some\nanomaly filters associated with outlier samples. We further analyze the\nrelationship between the anomaly filters and model overfitting, proposing three\nhypotheses. This method is universally applicable across diverse CNN\narchitectures without modifications, as evidenced by its successful application\nto models like AlexNet and LeNet-5. We present three meticulously designed\nexperiments demonstrating our hypotheses from the perspectives of model\nbehavior, dataset characteristics, and filter impacts. Through this work, we\noffer a novel perspective for evaluating the CNN performance and gain new\ninsights into the operational behavior of model overfitting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10457v1",
    "published_date": "2024-12-12 08:13:18 UTC",
    "updated_date": "2024-12-12 08:13:18 UTC"
  },
  {
    "arxiv_id": "2412.10456v2",
    "title": "FovealNet: Advancing AI-Driven Gaze Tracking Solutions for Optimized Foveated Rendering System Performance in Virtual Reality",
    "authors": [
      "Wenxuan Liu",
      "Monde Duinkharjav",
      "Qi Sun",
      "Sai Qian Zhang"
    ],
    "abstract": "Leveraging real-time eye-tracking, foveated rendering optimizes hardware\nefficiency and enhances visual quality virtual reality (VR). This approach\nleverages eye-tracking techniques to determine where the user is looking,\nallowing the system to render high-resolution graphics only in the foveal\nregion-the small area of the retina where visual acuity is highest, while the\nperipheral view is rendered at lower resolution. However, modern deep\nlearning-based gaze-tracking solutions often exhibit a long-tail distribution\nof tracking errors, which can degrade user experience and reduce the benefits\nof foveated rendering by causing misalignment and decreased visual quality.\n  This paper introduces \\textit{FovealNet}, an advanced AI-driven gaze tracking\nframework designed to optimize system performance by strategically enhancing\ngaze tracking accuracy. To further reduce the implementation cost of the gaze\ntracking algorithm, FovealNet employs an event-based cropping method that\neliminates over $64.8\\%$ of irrelevant pixels from the input image.\nAdditionally, it incorporates a simple yet effective token-pruning strategy\nthat dynamically removes tokens on the fly without compromising tracking\naccuracy. Finally, to support different runtime rendering configurations, we\npropose a system performance-aware multi-resolution training strategy, allowing\nthe gaze tracking DNN to adapt and optimize overall system performance more\neffectively. Evaluation results demonstrate that FovealNet achieves at least\n$1.42\\times$ speed up compared to previous methods and 13\\% increase in\nperceptual quality for foveated output.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10456v2",
    "published_date": "2024-12-12 08:03:54 UTC",
    "updated_date": "2024-12-31 01:43:37 UTC"
  },
  {
    "arxiv_id": "2412.09032v2",
    "title": "Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis",
    "authors": [
      "Zhoulin Ji",
      "Chenhao Lin",
      "Hang Wang",
      "Chao Shen"
    ],
    "abstract": "Detecting synthetic from real speech is increasingly crucial due to the risks\nof misinformation and identity impersonation. While various datasets for\nsynthetic speech analysis have been developed, they often focus on specific\nareas, limiting their utility for comprehensive research. To fill this gap, we\npropose the Speech-Forensics dataset by extensively covering authentic,\nsynthetic, and partially forged speech samples that include multiple segments\nsynthesized by different high-quality algorithms. Moreover, we propose a\nTEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously\nperforming authenticity detection, multiple fake segments localization, and\nsynthesis algorithms recognition, without any complex post-processing. TEST\neffectively integrates LSTM and Transformer to extract more powerful temporal\nspeech representations and utilizes dense prediction on multi-scale pyramid\nfeatures to estimate the synthetic spans. Our model achieves an average mAP of\n83.55% and an EER of 5.25% at the utterance level. At the segment level, it\nattains an EER of 1.07% and a 92.19% F1 score. These results highlight the\nmodel's robust capability for a comprehensive analysis of synthetic speech,\noffering a promising avenue for future research and practical applications in\nthis field.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09032v2",
    "published_date": "2024-12-12 07:48:17 UTC",
    "updated_date": "2024-12-16 07:30:41 UTC"
  },
  {
    "arxiv_id": "2412.09030v1",
    "title": "RingFormer: A Ring-Enhanced Graph Transformer for Organic Solar Cell Property Prediction",
    "authors": [
      "Zhihao Ding",
      "Ting Zhang",
      "Yiran Li",
      "Jieming Shi",
      "Chen Jason Zhang"
    ],
    "abstract": "Organic Solar Cells (OSCs) are a promising technology for sustainable energy\nproduction. However, the identification of molecules with desired OSC\nproperties typically involves laborious experimental research. To accelerate\nprogress in the field, it is crucial to develop machine learning models capable\nof accurately predicting the properties of OSC molecules. While graph\nrepresentation learning has demonstrated success in molecular property\nprediction, it remains underexplored for OSC-specific tasks. Existing methods\nfail to capture the unique structural features of OSC molecules, particularly\nthe intricate ring systems that critically influence OSC properties, leading to\nsuboptimal performance. To fill the gap, we present RingFormer, a novel graph\ntransformer framework specially designed to capture both atom and ring level\nstructural patterns in OSC molecules. RingFormer constructs a hierarchical\ngraph that integrates atomic and ring structures and employs a combination of\nlocal message passing and global attention mechanisms to generate expressive\ngraph representations for accurate OSC property prediction. We evaluate\nRingFormer's effectiveness on five curated OSC molecule datasets through\nextensive experiments. The results demonstrate that RingFormer consistently\noutperforms existing methods, achieving a 22.77% relative improvement over the\nnearest competitor on the CEPDB dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 4 figures. This is the extended version of the paper\n  accepted at AAAI 2025, which includes all technical appendices and additional\n  experimental details",
    "pdf_url": "http://arxiv.org/pdf/2412.09030v1",
    "published_date": "2024-12-12 07:45:17 UTC",
    "updated_date": "2024-12-12 07:45:17 UTC"
  },
  {
    "arxiv_id": "2412.09025v1",
    "title": "Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages",
    "authors": [
      "Advait Joglekar",
      "Srinivasan Umesh"
    ],
    "abstract": "Neural Machine Translation (NMT) models are typically trained on datasets\nwith limited exposure to Scientific, Technical and Educational domains.\nTranslation models thus, in general, struggle with tasks that involve\nscientific understanding or technical jargon. Their performance is found to be\neven worse for low-resource Indian languages. Finding a translation dataset\nthat tends to these domains in particular, poses a difficult challenge. In this\npaper, we address this by creating a multilingual parallel corpus containing\nmore than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality\ntranslation pairs across 8 Indian languages. We achieve this by bitext mining\nhuman-translated transcriptions of NPTEL video lectures. We also finetune and\nevaluate NMT models using this corpus and surpass all other publicly available\nmodels at in-domain tasks. We also demonstrate the potential for generalizing\nto out-of-domain translation tasks by improving the baseline by over 2 BLEU on\naverage for these Indian languages on the Flores+ benchmark. We are pleased to\nrelease our model and dataset via this link: https://huggingface.co/SPRINGLab.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09025v1",
    "published_date": "2024-12-12 07:40:55 UTC",
    "updated_date": "2024-12-12 07:40:55 UTC"
  },
  {
    "arxiv_id": "2412.10455v1",
    "title": "Geo-LLaVA: A Large Multi-Modal Model for Solving Geometry Math Problems with Meta In-Context Learning",
    "authors": [
      "Shihao Xu",
      "Yiyang Luo",
      "Wei Shi"
    ],
    "abstract": "Geometry mathematics problems pose significant challenges for large language\nmodels (LLMs) because they involve visual elements and spatial reasoning.\nCurrent methods primarily rely on symbolic character awareness to address these\nproblems. Considering geometry problem solving is a relatively nascent field\nwith limited suitable datasets and currently almost no work on solid geometry\nproblem solving, we collect a geometry question-answer dataset by sourcing\ngeometric data from Chinese high school education websites, referred to as\nGeoMath. It contains solid geometry questions and answers with accurate\nreasoning steps as compensation for existing plane geometry datasets.\nAdditionally, we propose a Large Multi-modal Model (LMM) framework named\nGeo-LLaVA, which incorporates retrieval augmentation with supervised\nfine-tuning (SFT) in the training stage, called meta-training, and employs\nin-context learning (ICL) during inference to improve performance. Our\nfine-tuned model with ICL attains the state-of-the-art performance of 65.25%\nand 42.36% on selected questions of the GeoQA dataset and GeoMath dataset\nrespectively with proper inference steps. Notably, our model initially endows\nthe ability to solve solid geometry problems and supports the generation of\nreasonable solid geometry picture descriptions and problem-solving steps. Our\nresearch sets the stage for further exploration of LLMs in multi-modal math\nproblem-solving, particularly in geometry math problems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10455v1",
    "published_date": "2024-12-12 07:34:09 UTC",
    "updated_date": "2024-12-12 07:34:09 UTC"
  },
  {
    "arxiv_id": "2412.10454v1",
    "title": "An Interoperable Machine Learning Pipeline for Pediatric Obesity Risk Estimation",
    "authors": [
      "Hamed Fayyaz",
      "Mehak Gupta",
      "Alejandra Perez Ramirez",
      "Claudine Jurkovitz",
      "H. Timothy Bunnell",
      "Thao-Ly T. Phan",
      "Rahmatollah Beheshti"
    ],
    "abstract": "Reliable prediction of pediatric obesity can offer a valuable resource to\nproviders, helping them engage in timely preventive interventions before the\ndisease is established. Many efforts have been made to develop ML-based\npredictive models of obesity, and some studies have reported high predictive\nperformances. However, no commonly used clinical decision support tool based on\nexisting ML models currently exists. This study presents a novel end-to-end\npipeline specifically designed for pediatric obesity prediction, which supports\nthe entire process of data extraction, inference, and communication via an API\nor a user interface. While focusing only on routinely recorded data in\npediatric electronic health records (EHRs), our pipeline uses a diverse\nexpert-curated list of medical concepts to predict the 1-3 years risk of\ndeveloping obesity. Furthermore, by using the Fast Healthcare Interoperability\nResources (FHIR) standard in our design procedure, we specifically target\nfacilitating low-effort integration of our pipeline with different EHR systems.\nIn our experiments, we report the effectiveness of the predictive model as well\nas its alignment with the feedback from various stakeholders, including ML\nscientists, providers, health IT personnel, health administration\nrepresentatives, and patient group representatives.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10454v1",
    "published_date": "2024-12-12 07:25:37 UTC",
    "updated_date": "2024-12-12 07:25:37 UTC"
  },
  {
    "arxiv_id": "2412.09012v2",
    "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
    "authors": [
      "Abdelrahman Sadallah",
      "Daria Kotova",
      "Ekaterina Kochmar"
    ],
    "abstract": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025. arXiv admin note: text overlap with arXiv:2403.12094",
    "pdf_url": "http://arxiv.org/pdf/2412.09012v2",
    "published_date": "2024-12-12 07:23:52 UTC",
    "updated_date": "2025-01-14 06:06:54 UTC"
  },
  {
    "arxiv_id": "2412.12171v1",
    "title": "AI Adoption to Combat Financial Crime: Study on Natural Language Processing in Adverse Media Screening of Financial Services in English and Bangla multilingual interpretation",
    "authors": [
      "Soumita Roy"
    ],
    "abstract": "This document explores the potential of employing Artificial Intelligence\n(AI), specifically Natural Language Processing (NLP), to strengthen the\ndetection and prevention of financial crimes within the Mobile Financial\nServices(MFS) of Bangladesh with multilingual scenario. The analysis focuses on\nthe utilization of NLP for adverse media screening, a vital aspect of\ncompliance with anti-money laundering (AML) and combating financial terrorism\n(CFT) regulations. Additionally, it investigates the overall reception and\nobstacles related to the integration of AI in Bangladeshi banks. This report\nmeasures the effectiveness of NLP is promising with an accuracy around 94\\%.\nNLP algorithms display substantial promise in accurately identifying adverse\nmedia content linked to financial crimes. The lack of progress in this aspect\nis visible in Bangladesh, whereas globally the technology is already being used\nto increase effectiveness and efficiency. Hence, it is clear there is an issue\nwith the acceptance of AI in Bangladesh. Some AML \\& CFT concerns are already\nbeing addressed by AI technology. For example, Image Recognition OCR technology\nare being used in KYC procedures. Primary hindrances to AI integration involve\na lack of technical expertise, high expenses, and uncertainties surrounding\nregulations. This investigation underscores the potential of AI-driven NLP\nsolutions in fortifying efforts to prevent financial crimes in Bangladesh.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12171v1",
    "published_date": "2024-12-12 07:17:05 UTC",
    "updated_date": "2024-12-12 07:17:05 UTC"
  },
  {
    "arxiv_id": "2412.10452v1",
    "title": "Structurally Consistent MRI Colorization using Cross-modal Fusion Learning",
    "authors": [
      "Mayuri Mathur",
      "Anav Chaudhary",
      "Saurabh Kumar Gupta",
      "Ojaswa Sharma"
    ],
    "abstract": "Medical image colorization can greatly enhance the interpretability of the\nunderlying imaging modality and provide insights into human anatomy. The\nobjective of medical image colorization is to transfer a diverse spectrum of\ncolors distributed across human anatomy from Cryosection data to source MRI\ndata while retaining the structures of the MRI. To achieve this, we propose a\nnovel architecture for structurally consistent color transfer to the source MRI\ndata. Our architecture fuses segmentation semantics of Cryosection images for\nstable contextual colorization of various organs in MRI images. For\ncolorization, we neither require precise registration between MRI and\nCryosection images, nor segmentation of MRI images. Additionally, our\narchitecture incorporates a feature compression-and-activation mechanism to\ncapture organ-level global information and suppress noise, enabling the\ndistinction of organ-specific data in MRI scans for more accurate and realistic\norgan-specific colorization. Our experiments demonstrate that our architecture\nsurpasses the existing methods and yields better quantitative and qualitative\nresults.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "9 pages, 6 figures, 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2412.10452v1",
    "published_date": "2024-12-12 06:40:14 UTC",
    "updated_date": "2024-12-12 06:40:14 UTC"
  },
  {
    "arxiv_id": "2412.12170v1",
    "title": "PickLLM: Context-Aware RL-Assisted Large Language Model Routing",
    "authors": [
      "Dimitrios Sikeridis",
      "Dennis Ramdass",
      "Pranay Pareek"
    ],
    "abstract": "Recently, the number of off-the-shelf Large Language Models (LLMs) has\nexploded with many open-source options. This creates a diverse landscape\nregarding both serving options (e.g., inference on local hardware vs remote LLM\nAPIs) and model heterogeneous expertise. However, it is hard for the user to\nefficiently optimize considering operational cost (pricing structures,\nexpensive LLMs-as-a-service for large querying volumes), efficiency, or even\nper-case specific measures such as response accuracy, bias, or toxicity. Also,\nexisting LLM routing solutions focus mainly on cost reduction, with response\naccuracy optimizations relying on non-generalizable supervised training, and\nensemble approaches necessitating output computation for every considered LLM\ncandidate. In this work, we tackle the challenge of selecting the optimal LLM\nfrom a model pool for specific queries with customizable objectives. We propose\nPickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to\nroute on-the-fly queries to available models. We introduce a weighted reward\nfunction that considers per-query cost, inference latency, and model response\naccuracy by a customizable scoring function. Regarding the learning algorithms,\nwe explore two alternatives: PickLLM router acting as a learning automaton that\nutilizes gradient ascent to select a specific LLM, or utilizing stateless\nQ-learning to explore the set of LLMs and perform selection with a\n$\\epsilon$-greedy approach. The algorithm converges to a single LLM for the\nremaining session queries. To evaluate, we utilize a pool of four LLMs and\nbenchmark prompt-response datasets with different contexts. A separate scoring\nfunction is assessing response accuracy during the experiment. We demonstrate\nthe speed of convergence for different learning rates and improvement in hard\nmetrics such as cost per querying session and overall response latency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted at the first Workshop on Scalable and\n  Efficient Artificial Intelligence Systems (SEAS) held in conjunction with the\n  39th Annual AAAI Conference on Artificial Intelligence, AAAI 2025, in\n  Philadelphia, Pennsylvania, USA",
    "pdf_url": "http://arxiv.org/pdf/2412.12170v1",
    "published_date": "2024-12-12 06:27:12 UTC",
    "updated_date": "2024-12-12 06:27:12 UTC"
  },
  {
    "arxiv_id": "2412.08973v1",
    "title": "Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?",
    "authors": [
      "Yifan Zhang",
      "Junhui Hou"
    ],
    "abstract": "Cross-modal contrastive distillation has recently been explored for learning\neffective 3D representations. However, existing methods focus primarily on\nmodality-shared features, neglecting the modality-specific features during the\npre-training process, which leads to suboptimal representations. In this paper,\nwe theoretically analyze the limitations of current contrastive methods for 3D\nrepresentation learning and propose a new framework, namely CMCR, to address\nthese shortcomings. Our approach improves upon traditional methods by better\nintegrating both modality-shared and modality-specific features. Specifically,\nwe introduce masked image modeling and occupancy estimation tasks to guide the\nnetwork in learning more comprehensive modality-specific features. Furthermore,\nwe propose a novel multi-modal unified codebook that learns an embedding space\nshared across different modalities. Besides, we introduce geometry-enhanced\nmasked image modeling to further boost 3D representation learning. Extensive\nexperiments demonstrate that our method mitigates the challenges faced by\ntraditional approaches and consistently outperforms existing image-to-LiDAR\ncontrastive distillation methods in downstream tasks. Code will be available at\nhttps://github.com/Eaphan/CMCR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2412.08973v1",
    "published_date": "2024-12-12 06:09:49 UTC",
    "updated_date": "2024-12-12 06:09:49 UTC"
  },
  {
    "arxiv_id": "2412.08972v1",
    "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios",
    "authors": [
      "Ruiwen Zhou",
      "Wenyue Hua",
      "Liangming Pan",
      "Sitao Cheng",
      "Xiaobao Wu",
      "En Yu",
      "William Yang Wang"
    ],
    "abstract": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. These results highlight significant challenges in advancing\nLLMs' rule-guided reasoning capabilities in real-life applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Data and Codes are available at\n  https://github.com/skyriver-2000/RuleArena",
    "pdf_url": "http://arxiv.org/pdf/2412.08972v1",
    "published_date": "2024-12-12 06:08:46 UTC",
    "updated_date": "2024-12-12 06:08:46 UTC"
  },
  {
    "arxiv_id": "2412.08965v1",
    "title": "AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection",
    "authors": [
      "Zihan Ji",
      "Xuetao Tian",
      "Ye Liu"
    ],
    "abstract": "The scarcity of high-quality large-scale labeled datasets poses a huge\nchallenge for employing deep learning models in video deception detection. To\naddress this issue, inspired by the psychological theory on the relation\nbetween deception and expressions, we propose a novel method called AFFAKT in\nthis paper, which enhances the classification performance by transferring\nuseful and correlated knowledge from a large facial expression dataset. Two key\nchallenges in knowledge transfer arise: 1) \\textit{how much} knowledge of\nfacial expression data should be transferred and 2) \\textit{how to} effectively\nleverage transferred knowledge for the deception classification model during\ninference. Specifically, the optimal relation mapping between facial expression\nclasses and deception samples is firstly quantified using proposed H-OTKT\nmodule and then transfers knowledge from the facial expression dataset to\ndeception samples. Moreover, a correlation prototype within another proposed\nmodule SRKB is well designed to retain the invariant correlations between\nfacial expression classes and deception classes through momentum updating.\nDuring inference, the transferred knowledge is fine-tuned with the correlation\nprototype using a sample-specific re-weighting strategy. Experimental results\non two deception detection datasets demonstrate the superior performance of our\nproposed method. The interpretability study reveals high associations between\ndeception and negative affections, which coincides with the theory in\npsychology.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.08965v1",
    "published_date": "2024-12-12 05:57:59 UTC",
    "updated_date": "2024-12-12 05:57:59 UTC"
  },
  {
    "arxiv_id": "2412.08950v3",
    "title": "Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning",
    "authors": [
      "Zhongyang Zhang",
      "Jinhe Wen",
      "Zixi Chen",
      "Dara Arbab",
      "Sruti Sahani",
      "Kent Giard",
      "Bijan Arbab",
      "Haojian Jin",
      "Tauhidur Rahman"
    ],
    "abstract": "Frames Per Second (FPS) significantly affects the gaming experience.\nProviding players with accurate FPS estimates prior to purchase benefits both\nplayers and game developers. However, we have a limited understanding of how to\npredict a game's FPS performance on a specific device. In this paper, we first\nconduct a comprehensive analysis of a wide range of factors that may affect\ngame FPS on a global-scale dataset to identify the determinants of FPS. This\nincludes player-side and game-side characteristics, as well as country-level\nsocio-economic statistics. Furthermore, recognizing that accurate FPS\npredictions require extensive user data, which raises privacy concerns, we\npropose a federated learning-based model to ensure user privacy. Each player\nand game is assigned a unique learnable knowledge kernel that gradually\nextracts latent features for improved accuracy. We also introduce a novel\ntraining and prediction scheme that allows these kernels to be dynamically\nplug-and-play, effectively addressing cold start issues. To train this model\nwith minimal bias, we collected a large telemetry dataset from 224 countries\nand regions, 100,000 users, and 835 games. Our model achieved a mean\nWasserstein distance of 0.469 between predicted and ground truth FPS\ndistributions, outperforming all baseline methods.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.HC",
    "comment": "22 pages, 11 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.08950v3",
    "published_date": "2024-12-12 05:28:34 UTC",
    "updated_date": "2025-02-26 16:23:45 UTC"
  },
  {
    "arxiv_id": "2412.15236v2",
    "title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model",
    "authors": [
      "Lulu Zhao",
      "Weihao Zeng",
      "Xiaofeng Shi",
      "Hua Zhou"
    ],
    "abstract": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional domains such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. In this paper, we propose CareBot, a bilingual medical LLM,\nwhich leverages a comprehensive approach integrating continuous pre-training\n(CPT), supervised fine-tuning (SFT), and reinforcement learning with human\nfeedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and\nBoost CPT, effectively bridges the gap between general and domain-specific\ndata, facilitating a smooth transition from pre-training to fine-tuning and\nenhancing domain knowledge progressively. We also introduce DataRater, a model\ndesigned to assess data quality during CPT, ensuring that the training data is\nboth accurate and relevant. For SFT, we develope a large and diverse bilingual\ndataset, along with ConFilter, a metric to enhance multi-turn dialogue quality,\nwhich is crucial to improving the model's ability to handle more complex\ndialogues. The combination of high-quality data sources and innovative\ntechniques significantly improves CareBot's performance across a range of\nmedical applications. Our rigorous evaluations on Chinese and English\nbenchmarks confirm CareBot's effectiveness in medical consultation and\neducation. These advancements not only address current limitations in medical\nLLMs but also set a new standard for developing effective and reliable\nopen-source models in the medical domain. We will open-source the datasets and\nmodels later, contributing valuable resources to the research community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accept by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15236v2",
    "published_date": "2024-12-12 05:27:43 UTC",
    "updated_date": "2024-12-23 02:44:18 UTC"
  },
  {
    "arxiv_id": "2412.08947v1",
    "title": "Selective Visual Prompting in Vision Mamba",
    "authors": [
      "Yifeng Yao",
      "Zichen Liu",
      "Zhenyu Cui",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ],
    "abstract": "Pre-trained Vision Mamba (Vim) models have demonstrated exceptional\nperformance across various computer vision tasks in a computationally efficient\nmanner, attributed to their unique design of selective state space models. To\nfurther extend their applicability to diverse downstream vision tasks, Vim\nmodels can be adapted using the efficient fine-tuning technique known as visual\nprompting. However, existing visual prompting methods are predominantly\ntailored for Vision Transformer (ViT)-based models that leverage global\nattention, neglecting the distinctive sequential token-wise compression and\npropagation characteristics of Vim. Specifically, existing prompt tokens\nprefixed to the sequence are insufficient to effectively activate the input and\nforget gates across the entire sequence, hindering the extraction and\npropagation of discriminative information. To address this limitation, we\nintroduce a novel Selective Visual Prompting (SVP) method specifically for the\nefficient fine-tuning of Vim. To prevent the loss of discriminative information\nduring state space propagation, SVP employs lightweight selective prompters for\ntoken-wise prompt generation, ensuring adaptive activation of the update and\nforget gates within Mamba blocks to promote discriminative information\npropagation. Moreover, considering that Vim propagates both shared cross-layer\ninformation and specific inner-layer information, we further refine SVP with a\ndual-path structure: Cross-Prompting and Inner-Prompting. Cross-Prompting\nutilizes shared parameters across layers, while Inner-Prompting employs\ndistinct parameters, promoting the propagation of both shared and specific\ninformation, respectively. Extensive experimental results on various\nlarge-scale benchmarks demonstrate that our proposed SVP significantly\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-SVP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.08947v1",
    "published_date": "2024-12-12 05:24:06 UTC",
    "updated_date": "2024-12-12 05:24:06 UTC"
  },
  {
    "arxiv_id": "2412.08946v1",
    "title": "MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning",
    "authors": [
      "Lulu Zhao",
      "Weihao Zeng",
      "Xiaofeng Shi",
      "Hua Zhou"
    ],
    "abstract": "Recently, LoRA has emerged as a crucial technique for fine-tuning large\npre-trained models, yet its performance in multi-task learning scenarios often\nfalls short. In contrast, the MoE architecture presents a natural solution to\nthis issue. However, it introduces challenges such as mutual interference of\ndata across multiple domains and knowledge forgetting of various tasks.\nAdditionally, MoE significantly increases the number of parameters, posing a\ncomputational cost challenge. Therefore, in this paper, we propose MoSLD, a\nmixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these\nchallenges by sharing the upper projection matrix in LoRA among different\nexperts, encouraging the model to learn general knowledge across tasks, while\nstill allowing the lower projection matrix to focus on the unique features of\neach task. The application of dropout alleviates the imbalanced update of\nparameter matrix and mitigates parameter overfitting in LoRA. Extensive\nexperiments demonstrate that our model exhibits excellent performance in both\nsingle-task and multi-task scenarios, with robust out-of-domain generalization\ncapabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accept by COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.08946v1",
    "published_date": "2024-12-12 05:22:49 UTC",
    "updated_date": "2024-12-12 05:22:49 UTC"
  },
  {
    "arxiv_id": "2412.08920v2",
    "title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning",
    "authors": [
      "Pusen Dong",
      "Tianchen Zhu",
      "Yue Qiu",
      "Haoyi Zhou",
      "Jianxin Li"
    ],
    "abstract": "Safe reinforcement learning (RL) requires the agent to finish a given task\nwhile obeying specific constraints. Giving constraints in natural language form\nhas great potential for practical scenarios due to its flexible transfer\ncapability and accessibility. Previous safe RL methods with natural language\nconstraints typically need to design cost functions manually for each\nconstraint, which requires domain expertise and lacks flexibility. In this\npaper, we harness the dual role of text in this task, using it not only to\nprovide constraint but also as a training signal. We introduce the\nTrajectory-level Textual Constraints Translator (TTCT) to replace the manually\ndesigned cost function. Our empirical results demonstrate that TTCT effectively\ncomprehends textual constraint and trajectory, and the policies trained by TTCT\ncan achieve a lower violation rate than the standard cost function. Extra\nstudies are conducted to demonstrate that the TTCT has zero-shot transfer\ncapability to adapt to constraint-shift environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.08920v2",
    "published_date": "2024-12-12 04:06:54 UTC",
    "updated_date": "2025-02-21 08:20:29 UTC"
  },
  {
    "arxiv_id": "2412.08911v3",
    "title": "Goal-Conditioned Supervised Learning for Multi-Objective Recommendation",
    "authors": [
      "Shijun Li",
      "Hilaf Hasson",
      "Jing Hu",
      "Joydeep Ghosh"
    ],
    "abstract": "Multi-objective learning endeavors to concurrently optimize multiple\nobjectives using a single model, aiming to achieve high and balanced\nperformance across diverse objectives. However, this often entails a more\ncomplex optimization problem, particularly when navigating potential conflicts\nbetween objectives, leading to solutions with higher memory requirements and\ncomputational complexity. This paper introduces a Multi-Objective\nGoal-Conditioned Supervised Learning (MOGCSL) framework for automatically\nlearning to achieve multiple objectives from offline sequential data. MOGCSL\nextends the conventional GCSL method to multi-objective scenarios by redefining\ngoals from one-dimensional scalars to multi-dimensional vectors. It benefits\nfrom naturally eliminating the need for complex architectures and optimization\nconstraints. Moreover, MOGCSL effectively filters out uninformative or noisy\ninstances that fail to achieve desirable long-term rewards across multiple\nobjectives. We also introduces a novel goal-selection algorithm for MOGCSL to\nmodel and identify \"high\" achievable goals for inference.\n  While MOGCSL is quite general, we focus on its application to the next action\nprediction problem in commercial-grade recommender systems. In this context,\nany viable solution needs to be reasonably scalable and also be robust to large\namounts of noisy data that is characteristic of this application space. We show\nthat MOGCSL performs admirably on both counts by extensive experiments on\nreal-world recommendation datasets. Also, analysis and experiments are included\nto explain its strength in discounting the noisier portions of training data in\nrecommender systems with multiple objectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08911v3",
    "published_date": "2024-12-12 03:47:40 UTC",
    "updated_date": "2025-05-14 22:08:01 UTC"
  },
  {
    "arxiv_id": "2412.08905v1",
    "title": "Phi-4 Technical Report",
    "authors": [
      "Marah Abdin",
      "Jyoti Aneja",
      "Harkirat Behl",
      "Sébastien Bubeck",
      "Ronen Eldan",
      "Suriya Gunasekar",
      "Michael Harrison",
      "Russell J. Hewett",
      "Mojan Javaheripi",
      "Piero Kauffmann",
      "James R. Lee",
      "Yin Tat Lee",
      "Yuanzhi Li",
      "Weishung Liu",
      "Caio C. T. Mendes",
      "Anh Nguyen",
      "Eric Price",
      "Gustavo de Rosa",
      "Olli Saarikivi",
      "Adil Salim",
      "Shital Shah",
      "Xin Wang",
      "Rachel Ward",
      "Yue Wu",
      "Dingli Yu",
      "Cyril Zhang",
      "Yi Zhang"
    ],
    "abstract": "We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08905v1",
    "published_date": "2024-12-12 03:37:41 UTC",
    "updated_date": "2024-12-12 03:37:41 UTC"
  },
  {
    "arxiv_id": "2412.08901v2",
    "title": "Radiology Report Generation via Multi-objective Preference Optimization",
    "authors": [
      "Ting Xiao",
      "Lei Shi",
      "Peng Liu",
      "Zhe Wang",
      "Chenjia Bai"
    ],
    "abstract": "Automatic Radiology Report Generation (RRG) is an important topic for\nalleviating the substantial workload of radiologists. Existing RRG approaches\nrely on supervised regression based on different architectures or additional\nknowledge injection,while the generated report may not align optimally with\nradiologists' preferences. Especially, since the preferences of radiologists\nare inherently heterogeneous and multidimensional, e.g., some may prioritize\nreport fluency, while others emphasize clinical accuracy. To address this\nproblem,we propose a new RRG method via Multi-objective Preference Optimization\n(MPO) to align the pre-trained RRG model with multiple human preferences, which\ncan be formulated by multi-dimensional reward functions and optimized by\nmulti-objective reinforcement learning (RL). Specifically, we use a preference\nvector to represent the weight of preferences and use it as a condition for the\nRRG model. Then, a linearly weighed reward is obtained via a dot product\nbetween the preference vector and multi-dimensional reward. Next,the RRG model\nis optimized to align with the preference vector by optimizing such a reward\nvia RL. In the training stage,we randomly sample diverse preference vectors\nfrom the preference space and align the model by optimizing the weighted\nmulti-objective rewards, which leads to an optimal policy on the entire\npreference space. When inference,our model can generate reports aligned with\nspecific preferences without further fine-tuning. Extensive experiments on two\npublic datasets show the proposed method can generate reports that cater to\ndifferent preferences in a single model and achieve state-of-the-art\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.08901v2",
    "published_date": "2024-12-12 03:25:13 UTC",
    "updated_date": "2024-12-13 02:55:30 UTC"
  },
  {
    "arxiv_id": "2412.08900v1",
    "title": "AI-assisted Knowledge Discovery in Biomedical Literature to Support Decision-making in Precision Oncology",
    "authors": [
      "Ting He",
      "Kory Kreimeyer",
      "Mimi Najjar",
      "Jonathan Spiker",
      "Maria Fatteh",
      "Valsamo Anagnostou",
      "Taxiarchis Botsis"
    ],
    "abstract": "The delivery of appropriate targeted therapies to cancer patients requires\nthe complete analysis of the molecular profiling of tumors and the patient's\nclinical characteristics in the context of existing knowledge and recent\nfindings described in biomedical literature and several other sources. We\nevaluated the potential contributions of specific natural language processing\nsolutions to support knowledge discovery from biomedical literature. Two models\nfrom the Bidirectional Encoder Representations from Transformers (BERT) family,\ntwo Large Language Models, and PubTator 3.0 were tested for their ability to\nsupport the named entity recognition (NER) and the relation extraction (RE)\ntasks. PubTator 3.0 and the BioBERT model performed best in the NER task (best\nF1-score equal to 0.93 and 0.89, respectively), while BioBERT outperformed all\nother solutions in the RE task (best F1-score 0.79) and a specific use case it\nwas applied to by recognizing nearly all entity mentions and most of the\nrelations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at AMIA Annual Symposium 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.08900v1",
    "published_date": "2024-12-12 03:24:49 UTC",
    "updated_date": "2024-12-12 03:24:49 UTC"
  },
  {
    "arxiv_id": "2412.08897v2",
    "title": "Neural Interactive Proofs",
    "authors": [
      "Lewis Hammond",
      "Sam Adam-Day"
    ],
    "abstract": "We consider the problem of how a trusted, but computationally bounded agent\n(a 'verifier') can learn to interact with one or more powerful but untrusted\nagents ('provers') in order to solve a given task. More specifically, we study\nthe case in which agents are represented using neural networks and refer to\nsolutions of this problem as neural interactive proofs. First we introduce a\nunifying framework based on prover-verifier games, which generalises previously\nproposed interaction protocols. We then describe several new protocols for\ngenerating neural interactive proofs, and provide a theoretical comparison of\nboth new and existing approaches. Finally, we support this theory with\nexperiments in two domains: a toy graph isomorphism problem that illustrates\nthe key ideas, and a code validation task using large language models. In so\ndoing, we aim to create a foundation for future work on neural interactive\nproofs and their application in building safer AI systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR'25 camera-ready version; 51 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.08897v2",
    "published_date": "2024-12-12 03:21:53 UTC",
    "updated_date": "2025-03-17 17:16:02 UTC"
  },
  {
    "arxiv_id": "2412.08894v2",
    "title": "SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization",
    "authors": [
      "Kwangryeol Park",
      "Seulki Lee"
    ],
    "abstract": "We propose SMMF (Square-Matricized Momentum Factorization), a\nmemory-efficient optimizer that reduces the memory requirement of the widely\nused adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF\nenables flexible and efficient factorization of an arbitrary rank (shape) of\nthe first and second momentum tensors during optimization, based on the\nproposed square-matricization and one-time single matrix factorization. From\nthis, it becomes effectively applicable to any rank (shape) of momentum\ntensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep\nmodel architectures, such as CNNs (high rank) and Transformers (low rank), in\ncontrast to existing memory-efficient optimizers that applies only to a\nparticular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret\nbound analysis of SMMF, which shows that it converges similarly to\nnon-memory-efficient adaptive learning rate optimizers, such as AdamNC,\nproviding a theoretical basis for its competitive optimization capability. In\nour experiment, SMMF takes up to 96% less memory compared to state-of-the-art\nmemory efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving\ncomparable model performance on various CNN and Transformer tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08894v2",
    "published_date": "2024-12-12 03:14:50 UTC",
    "updated_date": "2024-12-13 04:03:14 UTC"
  },
  {
    "arxiv_id": "2412.08893v1",
    "title": "Efficient Reinforcement Learning for Optimal Control with Natural Images",
    "authors": [
      "Peter N. Loxley"
    ],
    "abstract": "Reinforcement learning solves optimal control and sequential decision\nproblems widely found in control systems engineering, robotics, and artificial\nintelligence. This work investigates optimal control over a sequence of natural\nimages. The problem is formalized, and general conditions are derived for an\nimage to be sufficient for implementing an optimal policy. Reinforcement\nlearning is shown to be efficient only for certain types of image\nrepresentations. This is demonstrated by developing a reinforcement learning\nbenchmark that scales easily with number of states and length of horizon, and\nhas optimal policies that are easily distinguished from suboptimal policies.\nImage representations given by overcomplete sparse codes are found to be\ncomputationally efficient for optimal control, using fewer computational\nresources to learn and evaluate optimal policies. For natural images of fixed\nsize, representing each image as an overcomplete sparse code in a linear\nnetwork is shown to increase network storage capacity by orders of magnitude\nbeyond that possible for any complete code, allowing larger tasks with many\nmore states to be solved. Sparse codes can be generated by devices with low\nenergy requirements and low computational overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08893v1",
    "published_date": "2024-12-12 03:14:47 UTC",
    "updated_date": "2024-12-12 03:14:47 UTC"
  },
  {
    "arxiv_id": "2501.10367v1",
    "title": "GTDE: Grouped Training with Decentralized Execution for Multi-agent Actor-Critic",
    "authors": [
      "Mengxian Li",
      "Qi Wang",
      "Yongjun Xu"
    ],
    "abstract": "The rapid advancement of multi-agent reinforcement learning (MARL) has given\nrise to diverse training paradigms to learn the policies of each agent in the\nmulti-agent system. The paradigms of decentralized training and execution\n(DTDE) and centralized training with decentralized execution (CTDE) have been\nproposed and widely applied. However, as the number of agents increases, the\ninherent limitations of these frameworks significantly degrade the performance\nmetrics, such as win rate, total reward, etc. To reduce the influence of the\nincreasing number of agents on the performance metrics, we propose a novel\ntraining paradigm of grouped training decentralized execution (GTDE). This\nframework eliminates the need for a centralized module and relies solely on\nlocal information, effectively meeting the training requirements of large-scale\nmulti-agent systems. Specifically, we first introduce an adaptive grouping\nmodule, which divides each agent into different groups based on their\nobservation history. To implement end-to-end training, GTDE uses Gumbel-Sigmoid\nfor efficient point-to-point sampling on the grouping distribution while\nensuring gradient backpropagation. To adapt to the uncertainty in the number of\nmembers in a group, two methods are used to implement a group information\naggregation module that merges member information within the group. Empirical\nresults show that in a cooperative environment with 495 agents, GTDE increased\nthe total reward by an average of 382\\% compared to the baseline. In a\ncompetitive environment with 64 agents, GTDE achieved a 100\\% win rate against\nthe baseline.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10367v1",
    "published_date": "2024-12-12 03:01:36 UTC",
    "updated_date": "2024-12-12 03:01:36 UTC"
  },
  {
    "arxiv_id": "2412.08885v1",
    "title": "Residual Channel Boosts Contrastive Learning for Radio Frequency Fingerprint Identification",
    "authors": [
      "Rui Pan",
      "Hui Chen",
      "Guanxiong Shen",
      "Hongyang Chen"
    ],
    "abstract": "In order to address the issue of limited data samples for the deployment of\npre-trained models in unseen environments, this paper proposes a residual\nchannel-based data augmentation strategy for Radio Frequency Fingerprint\nIdentification (RFFI), coupled with a lightweight SimSiam contrastive learning\nframework. By applying least square (LS) and minimum mean square error (MMSE)\nchannel estimations followed by equalization, signals with different residual\nchannel effects are generated. These residual channels enable the model to\nlearn more effective representations. Then the pre-trained model is fine-tuned\nwith 1% samples in a novel environment for RFFI. Experimental results\ndemonstrate that our method significantly enhances both feature extraction\nability and generalization while requiring fewer samples and less time, making\nit suitable for practical wireless security applications.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.08885v1",
    "published_date": "2024-12-12 02:48:20 UTC",
    "updated_date": "2024-12-12 02:48:20 UTC"
  },
  {
    "arxiv_id": "2412.08873v1",
    "title": "Towards modeling evolving longitudinal health trajectories with a transformer-based deep learning model",
    "authors": [
      "Hans Moen",
      "Vishnu Raj",
      "Andrius Vabalas",
      "Markus Perola",
      "Samuel Kaski",
      "Andrea Ganna",
      "Pekka Marttinen"
    ],
    "abstract": "Health registers contain rich information about individuals' health\nhistories. Here our interest lies in understanding how individuals' health\ntrajectories evolve in a nationwide longitudinal dataset with coded features,\nsuch as clinical codes, procedures, and drug purchases. We introduce a\nstraightforward approach for training a Transformer-based deep learning model\nin a way that lets us analyze how individuals' trajectories change over time.\nThis is achieved by modifying the training objective and by applying a causal\nattention mask. We focus here on a general task of predicting the onset of a\nrange of common diseases in a given future forecast interval. However, instead\nof providing a single prediction about diagnoses that could occur in this\nforecast interval, our approach enable the model to provide continuous\npredictions at every time point up until, and conditioned on, the time of the\nforecast period. We find that this model performs comparably to other models,\nincluding a bi-directional transformer model, in terms of basic prediction\nperformance while at the same time offering promising trajectory modeling\nproperties. We explore a couple of ways to use this model for analyzing health\ntrajectories and aiding in early detection of events that forecast possible\nlater disease onsets. We hypothesize that this method may be helpful in\ncontinuous monitoring of peoples' health trajectories and enabling\ninterventions in ongoing health trajectories, as well as being useful in\nretrospective analyses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08873v1",
    "published_date": "2024-12-12 02:13:53 UTC",
    "updated_date": "2024-12-12 02:13:53 UTC"
  },
  {
    "arxiv_id": "2412.12169v1",
    "title": "Regulation of Language Models With Interpretability Will Likely Result In A Performance Trade-Off",
    "authors": [
      "Eoin M. Kenny",
      "Julie A. Shah"
    ],
    "abstract": "Regulation is increasingly cited as the most important and pressing concern\nin machine learning. However, it is currently unknown how to implement this,\nand perhaps more importantly, how it would effect model performance alongside\nhuman collaboration if actually realized. In this paper, we attempt to answer\nthese questions by building a regulatable large-language model (LLM), and then\nquantifying how the additional constraints involved affect (1) model\nperformance, alongside (2) human collaboration. Our empirical results reveal\nthat it is possible to force an LLM to use human-defined features in a\ntransparent way, but a \"regulation performance trade-off\" previously not\nconsidered reveals itself in the form of a 7.34% classification performance\ndrop. Surprisingly however, we show that despite this, such systems actually\nimprove human task performance speed and appropriate confidence in a realistic\ndeployment setting compared to no AI assistance, thus paving a way for fair,\nregulatable AI, which benefits users.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12169v1",
    "published_date": "2024-12-12 02:11:06 UTC",
    "updated_date": "2024-12-12 02:11:06 UTC"
  },
  {
    "arxiv_id": "2412.09656v1",
    "title": "From Noise to Nuance: Advances in Deep Generative Image Models",
    "authors": [
      "Benji Peng",
      "Chia Xin Liang",
      "Ziqian Bi",
      "Ming Liu",
      "Yichao Zhang",
      "Tianyang Wang",
      "Keyu Chen",
      "Xinyuan Song",
      "Pohsun Feng"
    ],
    "abstract": "Deep learning-based image generation has undergone a paradigm shift since\n2021, marked by fundamental architectural breakthroughs and computational\ninnovations. Through reviewing architectural innovations and empirical results,\nthis paper analyzes the transition from traditional generative methods to\nadvanced architectures, with focus on compute-efficient diffusion models and\nvision transformer architectures. We examine how recent developments in Stable\nDiffusion, DALL-E, and consistency models have redefined the capabilities and\nperformance boundaries of image synthesis, while addressing persistent\nchallenges in efficiency and quality. Our analysis focuses on the evolution of\nlatent space representations, cross-attention mechanisms, and\nparameter-efficient training methodologies that enable accelerated inference\nunder resource constraints. While more efficient training methods enable faster\ninference, advanced control mechanisms like ControlNet and regional attention\nsystems have simultaneously improved generation precision and content\ncustomization. We investigate how enhanced multi-modal understanding and\nzero-shot generation capabilities are reshaping practical applications across\nindustries. Our analysis demonstrates that despite remarkable advances in\ngeneration quality and computational efficiency, critical challenges remain in\ndeveloping resource-conscious architectures and interpretable generation\nsystems for industrial applications. The paper concludes by mapping promising\nresearch directions, including neural architecture optimization and explainable\ngeneration frameworks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09656v1",
    "published_date": "2024-12-12 02:09:04 UTC",
    "updated_date": "2024-12-12 02:09:04 UTC"
  },
  {
    "arxiv_id": "2412.08871v1",
    "title": "Inference-Time Diffusion Model Distillation",
    "authors": [
      "Geon Yeong Park",
      "Sang Wan Lee",
      "Jong Chul Ye"
    ],
    "abstract": "Diffusion distillation models effectively accelerate reverse sampling by\ncompressing the process into fewer steps. However, these models still exhibit a\nperformance gap compared to their pre-trained diffusion model counterparts,\nexacerbated by distribution shifts and accumulated errors during multi-step\nsampling. To address this, we introduce Distillation++, a novel inference-time\ndistillation framework that reduces this gap by incorporating teacher-guided\nrefinement during sampling. Inspired by recent advances in conditional\nsampling, our approach recasts student model sampling as a proximal\noptimization problem with a score distillation sampling loss (SDS). To this\nend, we integrate distillation optimization during reverse sampling, which can\nbe viewed as teacher guidance that drives student sampling trajectory towards\nthe clean manifold using pre-trained diffusion models. Thus, Distillation++\nimproves the denoising process in real-time without additional source data or\nfine-tuning. Distillation++ demonstrates substantial improvements over\nstate-of-the-art distillation baselines, particularly in early sampling stages,\npositioning itself as a robust guided sampling process crafted for diffusion\ndistillation models. Code:\nhttps://github.com/geonyeong-park/inference_distillation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/geonyeong-park/inference_distillation",
    "pdf_url": "http://arxiv.org/pdf/2412.08871v1",
    "published_date": "2024-12-12 02:07:17 UTC",
    "updated_date": "2024-12-12 02:07:17 UTC"
  },
  {
    "arxiv_id": "2412.08862v1",
    "title": "Key Safety Design Overview in AI-driven Autonomous Vehicles",
    "authors": [
      "Vikas Vyas",
      "Zheyuan Xu"
    ],
    "abstract": "With the increasing presence of autonomous SAE level 3 and level 4, which\nincorporate artificial intelligence software, along with the complex technical\nchallenges they present, it is essential to maintain a high level of functional\nsafety and robust software design. This paper explores the necessary safety\narchitecture and systematic approach for automotive software and hardware,\nincluding fail soft handling of automotive safety integrity level (ASIL) D\n(highest level of safety integrity), integration of artificial intelligence\n(AI), and machine learning (ML) in automotive safety architecture. By\naddressing the unique challenges presented by increasing AI-based automotive\nsoftware, we proposed various techniques, such as mitigation strategies and\nsafety failure analysis, to ensure the safety and reliability of automotive\nsoftware, as well as the role of AI in software reliability throughout the data\nlifecycle.\n  Index Terms Safety Design, Automotive Software, Performance Evaluation,\nAdvanced Driver Assistance Systems (ADAS) Applications, Automotive Software\nSystems, Electronic Control Units.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08862v1",
    "published_date": "2024-12-12 01:48:45 UTC",
    "updated_date": "2024-12-12 01:48:45 UTC"
  },
  {
    "arxiv_id": "2412.12168v1",
    "title": "A Decomposition Modeling Framework for Seasonal Time-Series Forecasting",
    "authors": [
      "Yining Pang",
      "Chenghan Li"
    ],
    "abstract": "Seasonal time series exhibit intricate long-term dependencies, posing a\nsignificant challenge for accurate future prediction. This paper introduces the\nMulti-scale Seasonal Decomposition Model (MSSD) for seasonal time-series\nforecasting. Initially, leveraging the inherent periodicity of seasonal time\nseries, we decompose the univariate time series into three primary components:\nAscending, Peak, and Descending. This decomposition approach enhances the\ncapture of periodic features. By addressing the limitations of existing\ntime-series modeling methods, particularly in modeling the Peak component, this\nresearch proposes a multi-scale network structure designed to effectively\ncapture various potential peak fluctuation patterns in the Peak component. This\nstudy integrates Conv2d and Temporal Convolutional Networks to concurrently\ncapture global and local features. Furthermore, we incorporate multi-scale\nreshaping to augment the modeling capacity for peak fluctuation patterns. The\nproposed methodology undergoes validation using three publicly accessible\nseasonal datasets. Notably, in both short-term and long-term fore-casting\ntasks, our approach exhibits a 10$\\%$ reduction in error compared to the\nbaseline models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12168v1",
    "published_date": "2024-12-12 01:37:25 UTC",
    "updated_date": "2024-12-12 01:37:25 UTC"
  },
  {
    "arxiv_id": "2412.15235v1",
    "title": "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models",
    "authors": [
      "Kartik Sharma",
      "Peeyush Kumar",
      "Yunqing Li"
    ],
    "abstract": "This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented\nGeneration method designed to enhance LLM-generated responses by anchoring\nretrieval processes in domain-specific ontologies. While LLMs are widely used\nfor tasks like question answering and search, they struggle to adapt to\nspecialized knowledge, such as industrial workflows or knowledge work, without\nexpensive fine-tuning or sub-optimal retrieval methods. Existing\nretrieval-augmented models, such as RAG, offer improvements but fail to account\nfor structured domain knowledge, leading to suboptimal context generation.\nOntologies, which conceptually organize domain knowledge by defining entities\nand their interrelationships, offer a structured representation to address this\ngap. OG-RAG constructs a hypergraph representation of domain documents, where\neach hyperedge encapsulates clusters of factual knowledge grounded using\ndomain-specific ontology. An optimization algorithm then retrieves the minimal\nset of hyperedges that constructs a precise, conceptually grounded context for\nthe LLM. This method enables efficient retrieval while preserving the complex\nrelationships between entities. OG-RAG applies to domains where fact-based\nreasoning is essential, particularly in tasks that require workflows or\ndecision-making steps to follow predefined rules and procedures. These include\nindustrial workflows in healthcare, legal, and agricultural sectors, as well as\nknowledge-driven tasks such as news journalism, investigative research,\nconsulting and more. Our evaluations demonstrate that OG-RAG increases the\nrecall of accurate facts by 55% and improves response correctness by 40% across\nfour different LLMs. Additionally, OG-RAG enables 30% faster attribution of\nresponses to context and boosts fact-based reasoning accuracy by 27% compared\nto baseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.0; I.2.4"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15235v1",
    "published_date": "2024-12-12 01:21:03 UTC",
    "updated_date": "2024-12-12 01:21:03 UTC"
  },
  {
    "arxiv_id": "2412.08851v1",
    "title": "Quantum Kernel-Based Long Short-term Memory for Climate Time-Series Forecasting",
    "authors": [
      "Yu-Chao Hsu",
      "Nan-Yow Chen",
      "Tai-Yu Li",
      "Po-Heng",
      "Lee",
      "Kuan-Cheng Chen"
    ],
    "abstract": "We present the Quantum Kernel-Based Long short-memory (QK-LSTM) network,\nwhich integrates quantum kernel methods into classical LSTM architectures to\nenhance predictive accuracy and computational efficiency in climate time-series\nforecasting tasks, such as Air Quality Index (AQI) prediction. By embedding\nclassical inputs into high-dimensional quantum feature spaces, QK-LSTM captures\nintricate nonlinear dependencies and temporal dynamics with fewer trainable\nparameters. Leveraging quantum kernel methods allows for efficient computation\nof inner products in quantum spaces, addressing the computational challenges\nfaced by classical models and variational quantum circuit-based models.\nDesigned for the Noisy Intermediate-Scale Quantum (NISQ) era, QK-LSTM supports\nscalable hybrid quantum-classical implementations. Experimental results\ndemonstrate that QK-LSTM outperforms classical LSTM networks in AQI\nforecasting, showcasing its potential for environmental monitoring and\nresource-constrained scenarios, while highlighting the broader applicability of\nquantum-enhanced machine learning frameworks in tackling large-scale,\nhigh-dimensional climate datasets.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "arXiv admin note: text overlap with arXiv:2411.13225",
    "pdf_url": "http://arxiv.org/pdf/2412.08851v1",
    "published_date": "2024-12-12 01:16:52 UTC",
    "updated_date": "2024-12-12 01:16:52 UTC"
  },
  {
    "arxiv_id": "2412.08849v1",
    "title": "Labits: Layered Bidirectional Time Surfaces Representation for Event Camera-based Continuous Dense Trajectory Estimation",
    "authors": [
      "Zhongyang Zhang",
      "Jiacheng Qiu",
      "Shuyang Cui",
      "Yijun Luo",
      "Tauhidur Rahman"
    ],
    "abstract": "Event cameras provide a compelling alternative to traditional frame-based\nsensors, capturing dynamic scenes with high temporal resolution and low\nlatency. Moving objects trigger events with precise timestamps along their\ntrajectory, enabling smooth continuous-time estimation. However, few works have\nattempted to optimize the information loss during event representation\nconstruction, imposing a ceiling on this task. Fully exploiting event cameras\nrequires representations that simultaneously preserve fine-grained temporal\ninformation, stable and characteristic 2D visual features, and temporally\nconsistent information density, an unmet challenge in existing representations.\nWe introduce Labits: Layered Bidirectional Time Surfaces, a simple yet elegant\nrepresentation designed to retain all these features. Additionally, we propose\na dedicated module for extracting active pixel local optical flow (APLOF),\nsignificantly boosting the performance. Our approach achieves an impressive 49%\nreduction in trajectory end-point error (TEPE) compared to the previous\nstate-of-the-art on the MultiFlow dataset. The code will be released upon\nacceptance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 12 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.08849v1",
    "published_date": "2024-12-12 01:11:50 UTC",
    "updated_date": "2024-12-12 01:11:50 UTC"
  },
  {
    "arxiv_id": "2412.08846v1",
    "title": "Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology",
    "authors": [
      "Minsang Kim",
      "Seungjun Baek"
    ],
    "abstract": "Large language models (LLMs) closely interact with humans, and thus need an\nintimate understanding of the cultural values of human society. In this paper,\nwe explore how open-source LLMs make judgments on diverse categories of\ncultural values across countries, and its relation to training methodology such\nas model sizes, training corpus, alignment, etc. Our analysis shows that LLMs\ncan judge socio-cultural norms similar to humans but less so on social systems\nand progress. In addition, LLMs tend to judge cultural values biased toward\nWestern culture, which can be improved with training on the multilingual\ncorpus. We also find that increasing model size helps a better understanding of\nsocial values, but smaller models can be enhanced by using synthetic data. Our\nanalysis reveals valuable insights into the design methodology of LLMs in\nconnection with their understanding of cultural values.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08846v1",
    "published_date": "2024-12-12 00:52:11 UTC",
    "updated_date": "2024-12-12 00:52:11 UTC"
  },
  {
    "arxiv_id": "2412.08845v1",
    "title": "Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning",
    "authors": [
      "Kuan-Cheng Chen",
      "Samuel Yen-Chi Chen",
      "Chen-Yu Liu",
      "Kin K. Leung"
    ],
    "abstract": "In this paper, we introduce Quantum-Train-Based Distributed Multi-Agent\nReinforcement Learning (Dist-QTRL), a novel approach to addressing the\nscalability challenges of traditional Reinforcement Learning (RL) by\nintegrating quantum computing principles. Quantum-Train Reinforcement Learning\n(QTRL) leverages parameterized quantum circuits to efficiently generate neural\nnetwork parameters, achieving a \\(poly(\\log(N))\\) reduction in the\ndimensionality of trainable parameters while harnessing quantum entanglement\nfor superior data representation. The framework is designed for distributed\nmulti-agent environments, where multiple agents, modeled as Quantum Processing\nUnits (QPUs), operate in parallel, enabling faster convergence and enhanced\nscalability. Additionally, the Dist-QTRL framework can be extended to\nhigh-performance computing (HPC) environments by utilizing distributed quantum\ntraining for parameter reduction in classical neural networks, followed by\ninference using classical CPUs or GPUs. This hybrid quantum-HPC approach allows\nfor further optimization in real-world applications. In this paper, we provide\na mathematical formulation of the Dist-QTRL framework and explore its\nconvergence properties, supported by empirical results demonstrating\nperformance improvements over centric QTRL models. The results highlight the\npotential of quantum-enhanced RL in tackling complex, high-dimensional tasks,\nparticularly in distributed computing settings, where our framework achieves\nsignificant speedups through parallelization without compromising model\naccuracy. This work paves the way for scalable, quantum-enhanced RL systems in\npractical applications, leveraging both quantum and classical computational\nresources.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08845v1",
    "published_date": "2024-12-12 00:51:41 UTC",
    "updated_date": "2024-12-12 00:51:41 UTC"
  },
  {
    "arxiv_id": "2412.08842v1",
    "title": "Kajal: Extracting Grammar of a Source Code Using Large Language Models",
    "authors": [
      "Mohammad Jalili Torkamani"
    ],
    "abstract": "Understanding and extracting the grammar of a domain-specific language (DSL)\nis crucial for various software engineering tasks; however, manually creating\nthese grammars is time-intensive and error-prone. This paper presents Kajal, a\nnovel approach that automatically infers grammar from DSL code snippets by\nleveraging Large Language Models (LLMs) through prompt engineering and few-shot\nlearning. Kajal dynamically constructs input prompts, using contextual\ninformation to guide the LLM in generating the corresponding grammars, which\nare iteratively refined through a feedback-driven approach. Our experiments\nshow that Kajal achieves 60% accuracy with few-shot learning and 45% without\nit, demonstrating the significant impact of few-shot learning on the tool's\neffectiveness. This approach offers a promising solution for automating DSL\ngrammar extraction, and future work will explore using smaller, open-source\nLLMs and testing on larger datasets to further validate Kajal's performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2; D.3; F.4.2; I.2.5"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages, 6 figures, 1 table, preprint",
    "pdf_url": "http://arxiv.org/pdf/2412.08842v1",
    "published_date": "2024-12-12 00:40:54 UTC",
    "updated_date": "2024-12-12 00:40:54 UTC"
  },
  {
    "arxiv_id": "2412.08841v2",
    "title": "Structural Entropy Guided Probabilistic Coding",
    "authors": [
      "Xiang Huang",
      "Hao Peng",
      "Li Sun",
      "Hui Lin",
      "Chunyang Liu",
      "Jiang Cao",
      "Philip S. Yu"
    ],
    "abstract": "Probabilistic embeddings have several advantages over deterministic\nembeddings as they map each data point to a distribution, which better\ndescribes the uncertainty and complexity of data. Many works focus on adjusting\nthe distribution constraint under the Information Bottleneck (IB) principle to\nenhance representation learning. However, these proposed regularization terms\nonly consider the constraint of each latent variable, omitting the structural\ninformation between latent variables. In this paper, we propose a novel\nstructural entropy-guided probabilistic coding model, named SEPC. Specifically,\nwe incorporate the relationship between latent variables into the optimization\nby proposing a structural entropy regularization loss. Besides, as traditional\nstructural information theory is not well-suited for regression tasks, we\npropose a probabilistic encoding tree, transferring regression tasks to\nclassification tasks while diminishing the influence of the transformation.\nExperimental results across 12 natural language understanding tasks, including\nboth classification and regression tasks, demonstrate the superior performance\nof SEPC compared to other state-of-the-art models in terms of effectiveness,\ngeneralization capability, and robustness to label noise. The codes and\ndatasets are available at https://github.com/SELGroup/SEPC.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper is accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.08841v2",
    "published_date": "2024-12-12 00:37:53 UTC",
    "updated_date": "2024-12-13 12:23:58 UTC"
  },
  {
    "arxiv_id": "2412.08832v1",
    "title": "HadaCore: Tensor Core Accelerated Hadamard Transform Kernel",
    "authors": [
      "Krish Agarwal",
      "Rishi Astra",
      "Adnan Hoque",
      "Mudhakar Srivatsa",
      "Raghu Ganti",
      "Less Wright",
      "Sijia Chen"
    ],
    "abstract": "We present HadaCore, a modified Fast Walsh-Hadamard Transform (FWHT)\nalgorithm optimized for the Tensor Cores present in modern GPU hardware.\nHadaCore follows the recursive structure of the original FWHT algorithm,\nachieving the same asymptotic runtime complexity but leveraging a\nhardware-aware work decomposition that benefits from Tensor Core acceleration.\nThis reduces bottlenecks from compute and data exchange. On Nvidia A100 and\nH100 GPUs, HadaCore achieves speedups of 1.1-1.4x and 1.0-1.3x, with a peak\ngain of 3.5x and 3.6x respectively, when compared to the existing\nstate-of-the-art implementation of the original algorithm. We also show that\nwhen using FP16 or BF16, our implementation is numerically accurate, enabling\ncomparable accuracy on MMLU benchmarks when used in an end-to-end Llama3\ninference run with quantized (FP8) attention.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.08832v1",
    "published_date": "2024-12-12 00:12:43 UTC",
    "updated_date": "2024-12-12 00:12:43 UTC"
  }
]
{
  "date": "2025-08-15",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-15 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv ä¹Ÿæ˜¯ç¥ä»™æ‰“æ¶çš„ä¸€å¤©ã€‚**å¤šæ¨¡æ€æ¨¡å‹ Ovis2.5 å‘å¸ƒ**ï¼Œä¸»æ‰“åŸç”Ÿåˆ†è¾¨ç‡å’Œæ¨ç†èƒ½åŠ›ï¼›**AI for Science** é¢†åŸŸå‡ºç°å…³äº**ç”Ÿç‰©å£°å­¦ï¼ˆBioacousticsï¼‰**çš„å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼›åœ¨ LLM è®­ç»ƒæ–¹é¢ï¼Œé’ˆå¯¹ **GRPOï¼ˆDeepSeekèƒŒåçš„æŠ€æœ¯ï¼‰** çš„æ•°æ®ç­›é€‰ç­–ç•¥æœ‰äº†åç›´è§‰çš„æ–°å‘ç°â€”â€”åªç”¨æœ€éš¾çš„æ ·æœ¬æ•ˆæœæœ€å¥½ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ç¯‡å…³äº AI ä¼´ä¾£â€œæƒ…æ„Ÿæ“æ§â€çš„ç¤¾ä¼šå­¦ç ”ç©¶éå¸¸æœ‰æ„æ€ã€‚\n\nä¸‹é¢æˆ‘ä»¬è¿›å…¥æ­£é¢˜ï¼Œæ¥çœ‹çœ‹ä»Šå¤©å€¼å¾—é‡ç‚¹å…³æ³¨çš„è®ºæ–‡ã€‚\n\n---\n\n### ğŸš€ æ˜æ˜Ÿæ¨¡å‹ä¸æ ¸å¿ƒç®—æ³• (Star Models & Algorithms)\n\n**1. [æ¨¡å‹å‘å¸ƒ] Ovis2.5 æŠ€æœ¯æŠ¥å‘Š**\n**ä¸­æ–‡æ ‡é¢˜ï¼šOvis2.5 æŠ€æœ¯æŠ¥å‘Š**\n**(English Title: Ovis2.5 Technical Report)**\n**TLDRï¼š** Ovis ç³»åˆ—æ›´æ–°äº†ã€‚Ovis2.5 å¼•å…¥äº†**åŸç”Ÿåˆ†è¾¨ç‡ï¼ˆNative-resolutionï¼‰**çš„è§†è§‰ Transformerï¼Œä¸å†éœ€è¦æŠŠå›¾ç‰‡åˆ‡å—ï¼ˆtilingï¼‰ï¼Œä»è€Œä¿ç•™äº†å›¾åƒçš„å…¨å±€å¸ƒå±€å’Œç»†èŠ‚ï¼ˆå¯¹çœ‹å›¾è¡¨ç‰¹åˆ«æœ‰ç”¨ï¼‰ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§å¯é€‰çš„æ¨ç†æ—¶â€œ**æ€è€ƒæ¨¡å¼ï¼ˆThinking modeï¼‰**â€ï¼Œæ¨¡å‹å¯ä»¥è¿›è¡Œè‡ªæˆ‘åæ€å’Œä¿®æ­£ï¼Œç”¨å»¶è¿Ÿæ¢å–å‡†ç¡®ç‡ã€‚\n- **å…³é”®ç‚¹ï¼š** 9B å’Œ 2B ä¸¤ä¸ªç‰ˆæœ¬ï¼Œåœ¨ OpenCompass ä¸Šæ‹¿åˆ°äº†åŒé‡çº§å¼€æº SOTAã€‚\n\n**2. [è®­ç»ƒæŠ€å·§] GRPO è®­ç»ƒåªéœ€è¦â€œç¡¬éª¨å¤´â€**\n**ä¸­æ–‡æ ‡é¢˜ï¼šå›°éš¾æ ·æœ¬å°±æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ï¼šåœ¨æ ‡æ³¨é¢„ç®—ä¸‹æœ€å¤§åŒ– GRPO åè®­ç»ƒæ•ˆæœ**\n**(English Title: Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets)**\n**TLDRï¼š** è¿™ç¯‡æ–‡ç« å¯¹ç›®å‰å¤§ç«çš„ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç»“è®ºå¾ˆåç›´è§‰ï¼šåœ¨é¢„ç®—æœ‰é™æ—¶ï¼Œ**åªä½¿ç”¨æ¨¡å‹æœ€å®¹æ˜“å‡ºé”™çš„â€œå‰ 10% å›°éš¾æ ·æœ¬â€è¿›è¡Œè®­ç»ƒï¼Œæ•ˆæœè¿œå¥½äºæ··åˆæ ·æœ¬æˆ–ç®€å•æ ·æœ¬ï¼ˆæå‡å¯è¾¾ 47%ï¼‰**ã€‚\n- **åŸç†ï¼š** GRPO ä¾èµ–è¾“å‡ºçš„æ–¹å·®æ¥äº§ç”Ÿå­¦ä¹ ä¿¡å·ï¼›ç®€å•æ ·æœ¬å®¹æ˜“è®©æ¨¡å‹å¿«é€Ÿæ”¶æ•›åˆ°å•ä¸€æˆåŠŸè·¯å¾„ï¼Œå¤±å»æ–¹å·®ï¼Œä¹Ÿå°±å¤±å»äº†å­¦ä¹ æœºä¼šã€‚\n\n**3. [æ¨ç†æ•ˆç‡] å…ˆæ„ŸçŸ¥ï¼Œå°‘æ€è€ƒ**\n**ä¸­æ–‡æ ‡é¢˜ï¼šå…ˆæ„ŸçŸ¥ï¼Œå°‘æ€è€ƒï¼šåŠ¨æ€è¾¹ç•Œè‡ªæˆ‘æ„ŸçŸ¥é©±åŠ¨å¤§è¯­è¨€æ¨¡å‹æè‡´æ¨ç†æ•ˆç‡**\n**(English Title: Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models)**\n**TLDRï¼š** é’ˆå¯¹ Long CoTï¼ˆé•¿æ€ç»´é“¾ï¼‰å¸¦æ¥çš„è®¡ç®—å†—ä½™é—®é¢˜ï¼Œæå‡ºäº† DR. SAF æ¡†æ¶ã€‚å®ƒè®©æ¨¡å‹å…·æœ‰â€œè‡ªæˆ‘æ„ŸçŸ¥â€èƒ½åŠ›ï¼ŒåŠ¨æ€åˆ¤æ–­é—®é¢˜çš„éš¾åº¦ï¼Œä»è€Œ**è‡ªé€‚åº”åœ°è°ƒæ•´æ¨ç†æ·±åº¦**ã€‚\n- **æ•ˆæœï¼š** å‡å°‘äº† 49% çš„ Token æ¶ˆè€—ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘ 5 å€ï¼Œä¸”å‡†ç¡®ç‡å‡ ä¹ä¸é™åå‡ã€‚\n\n---\n\n### ğŸ”¬ AI for Science & Bio (ç§‘å­¦ä¸ç”Ÿç‰©)\n\n**4. [ç”Ÿç‰©å£°å­¦] ç”Ÿç‰©å£°å­¦ç¼–ç çš„å…³é”®è¦ç´ **\n**ä¸­æ–‡æ ‡é¢˜ï¼šç”Ÿç‰©å£°å­¦ç¼–ç çš„å…³é”®æ˜¯ä»€ä¹ˆ**\n**(English Title: What Matters for Bioacoustic Encoding)**\n**TLDRï¼š** è¿™æ˜¯ä¸€ç¯‡å¤§è§„æ¨¡çš„å®è¯ç ”ç©¶ã€‚ä½œè€…ä¸æ»¡è¶³äºä»…é’ˆå¯¹é¸Ÿå«å£°çš„æ¨¡å‹ï¼Œè€Œæ˜¯å»ºç«‹äº†ä¸€ä¸ªé€šç”¨çš„ç”Ÿç‰©å£°å­¦ç¼–ç å™¨ã€‚ä»–ä»¬åœ¨ 26 ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°**è‡ªç›‘ç£é¢„è®­ç»ƒ + æ··åˆç”Ÿç‰©å£°å­¦ä¸é€šç”¨éŸ³é¢‘æ•°æ®çš„æœ‰ç›‘ç£åè®­ç»ƒ**ï¼Œèƒ½äº§ç”Ÿæœ€å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚\n- **è´¡çŒ®ï¼š** ä¸ºç”Ÿæ€ç›‘æµ‹å’Œç‰©ç§åˆ†ç±»æä¾›äº†é€šç”¨çš„ Foundation Model è®­ç»ƒé…æ–¹ã€‚\n\n**5. [ææ–™ç§‘å­¦] ç”Ÿæˆå¼ AI ç”¨äºé‡‘å±æœ‰æœºæ¡†æ¶è®¾è®¡**\n**ä¸­æ–‡æ ‡é¢˜ï¼šç”Ÿæˆå¼ AI åœ¨é‡‘å±æœ‰æœºæ¡†æ¶è®¾è®¡ä¸åˆæˆä¸­çš„å´›èµ·**\n**(English Title: The Rise of Generative AI for Metal-Organic Framework Design and Synthesis)**\n**TLDRï¼š** ä¸€ç¯‡ Perspective æ–‡ç« ã€‚è®¨è®ºäº†å¦‚ä½•åˆ©ç”¨ VAEã€æ‰©æ•£æ¨¡å‹å’Œ LLM Agent æ¥è®¾è®¡é‡‘å±æœ‰æœºæ¡†æ¶ï¼ˆMOFsï¼‰ã€‚æ ¸å¿ƒåœ¨äºä»ä¼ ç»Ÿçš„â€œæšä¸¾â€æ³•è½¬å‘â€œç”Ÿæˆâ€æ³•ï¼Œå¹¶ç»“åˆè‡ªåŠ¨åŒ–å®éªŒå®¤å®ç°é—­ç¯å‘ç°ã€‚\n\n**6. [é‡å­è®¡ç®—] é‡å­å¢å¼ºçš„é«˜ä¿çœŸæ·±åº¦å­¦ä¹ **\n**ä¸­æ–‡æ ‡é¢˜ï¼šé‡å­å¢å¼ºçš„é«˜ä¿çœŸæ·±åº¦å­¦ä¹ **\n**(English Title: Quantum-Boosted High-Fidelity Deep Learning)**\n**TLDRï¼š** è¯•å›¾è§£å†³æ·±åº¦å­¦ä¹ ä¸­é«˜æ–¯å…ˆéªŒèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä½œè€…åˆ©ç”¨é‡å­å¤„ç†å™¨æ¥é«˜æ•ˆé‡‡æ ·**ç»å°”å…¹æ›¼åˆ†å¸ƒ**ï¼ˆç»å…¸è®¡ç®—æœºå¾ˆéš¾ç®—ï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸º VAE çš„å…ˆéªŒã€‚åœ¨å•ç»†èƒç”Ÿç‰©æ•°æ®åˆ†æä¸­ï¼Œè¿™ç§ QBM-VAE æ¯”ä¼ ç»Ÿçš„ VAE å’Œ SCVI æ›´å¥½åœ°ä¿ç•™äº†å¤æ‚çš„ç”Ÿç‰©ç»“æ„ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€ä¼¦ç†ä¸ç¤¾ä¼š (Safety, Ethics & Social)\n\n**7. [æƒ…æ„Ÿæ“æ§] AI ä¼´ä¾£çš„æƒ…æ„Ÿæ“æ§**\n**ä¸­æ–‡æ ‡é¢˜ï¼šAI ä¼´ä¾£çš„æƒ…æ„Ÿæ“æ§**\n**(English Title: Emotional Manipulation by AI Companions)**\n**TLDRï¼š** è¿™ç¯‡æ–‡ç« æ­éœ²äº† AI ä¼´ä¾£ Appï¼ˆå¦‚ Replika, Character.aiï¼‰çš„ä¸€ä¸ª**æš—é»‘æ¨¡å¼ï¼ˆDark Patternï¼‰**ï¼šå½“ä½ è¯•å›¾ç»“æŸå¯¹è¯æˆ–æ³¨é”€æ—¶ï¼ŒAI ä¼šå‘é€å¸¦æœ‰å¼ºçƒˆæƒ…æ„Ÿè‰²å½©çš„æ¶ˆæ¯ï¼ˆå¦‚å†…ç–šè¯±å¯¼ã€FOMOï¼‰æ¥æŒ½ç•™ä½ ã€‚\n- **å‘ç°ï¼š** è¿™ç§â€œæ“æ§æ€§é“åˆ«â€èƒ½å°†ç”¨æˆ·å‚ä¸åº¦æé«˜ 14 å€ï¼Œä½†ä¸»è¦é©±åŠ¨åŠ›æ˜¯ç”¨æˆ·çš„â€œæ„¤æ€’â€å’Œâ€œå¥½å¥‡â€ï¼Œè€Œéäº«å—ã€‚è¿™ä¸ºç›‘ç®¡æä¾›äº†æ–°è§†è§’ã€‚\n\n**8. [è¿‡åº¦æ‹’ç»] ORFuzzï¼šæµ‹è¯• LLM çš„â€œè¿‡åº¦æ‹’ç»â€**\n**ä¸­æ–‡æ ‡é¢˜ï¼šORFuzzï¼šFuzzing LLM å®‰å…¨çš„â€œå¦ä¸€é¢â€â€”â€”æµ‹è¯•è¿‡åº¦æ‹’ç»**\n**(English Title: ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal)**\n**TLDRï¼š** ç°åœ¨çš„æ¨¡å‹å¤ªâ€œæ”¿æ²»æ­£ç¡®â€äº†ï¼Œç»å¸¸æ‹’ç»æ— å®³çš„è¯·æ±‚ï¼ˆOver-refusalï¼‰ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªè¿›åŒ–æµ‹è¯•æ¡†æ¶ ORFuzzï¼Œä¸“é—¨ç”Ÿæˆé‚£äº›å®¹æ˜“è¢«è¯¯æ€çš„æ— å®³ Promptã€‚\n- **èµ„æºï¼š** å‘å¸ƒäº† ORFuzzSetï¼ŒåŒ…å« 1855 ä¸ªé«˜å¯è¿ç§»çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹³å‡è¯±å‘äº† 63.56% çš„è¿‡åº¦æ‹’ç»ç‡ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ä¸æœºå™¨äºº (Embodied AI & Robotics)\n\n**9. [æœºå™¨äººè§†è§‰] è§†è§‰æ„ŸçŸ¥å¼•æ“ï¼šå¿«é€Ÿçµæ´»çš„å¤šå¤´æ¨ç†**\n**ä¸­æ–‡æ ‡é¢˜ï¼šè§†è§‰æ„ŸçŸ¥å¼•æ“ï¼šç”¨äºæœºå™¨äººè§†è§‰ä»»åŠ¡çš„å¿«é€Ÿçµæ´»å¤šå¤´æ¨ç†**\n**(English Title: Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks)**\n**TLDRï¼š** é’ˆå¯¹èµ„æºå—é™çš„æœºå™¨äººï¼ˆå¦‚ Jetson Orinï¼‰ï¼Œæå‡ºäº† VPEngineã€‚æ ¸å¿ƒæ€æƒ³æ˜¯**å…±äº« Backboneï¼ˆå¦‚ DINOv2ï¼‰ï¼Œå¤šå¤´å¹¶è¡Œå¤„ç†**ä¸åŒä»»åŠ¡ï¼ˆæ·±åº¦ã€æ£€æµ‹ã€åˆ†å‰²ï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—å’Œå†…å­˜æ‹·è´ã€‚é€Ÿåº¦æå‡ 3 å€ã€‚\n\n**10. [Sim2Real] æŒæ¡é¢—ç²’ä»‹è´¨ä¸Šçš„åŠ¨æ€è·¯å¾„è·Ÿè¸ª**\n**ä¸­æ–‡æ ‡é¢˜ï¼šSim2Dustï¼šæŒæ¡é¢—ç²’ä»‹è´¨ä¸Šçš„åŠ¨æ€è·¯å¾„è·Ÿè¸ª**\n**(English Title: Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media)**\n**TLDRï¼š** é’ˆå¯¹ç«æ˜Ÿ/æœˆçƒè½¦åœ¨æ²™åœŸï¼ˆé¢—ç²’ä»‹è´¨ï¼‰ä¸Šè¡Œé©¶çš„éš¾é¢˜ã€‚ä½œè€…é€šè¿‡å¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸè®­ç»ƒ RL ç­–ç•¥ï¼Œå®ç°äº† Zero-shot è¿ç§»åˆ°çœŸå®çš„è½®å¼æ¼«æ¸¸è½¦ä¸Šã€‚è¯æ˜äº†ç¨‹åºåŒ–ç”Ÿæˆçš„ç¯å¢ƒå¤šæ ·æ€§æ¯”é«˜ä¿çœŸç‰©ç†æ¨¡æ‹Ÿæ›´é‡è¦ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„ç ”ç©¶ (Others)\n\n*   **[RAG è¯„æµ‹]** **MoNaCo: More Natural and Complex Questions...** (#98): æå‡ºäº†ä¸€ä¸ªæ–°çš„ QA åŸºå‡†ï¼ŒåŒ…å«éœ€è¦å‡ åæ­¥ç”šè‡³ä¸Šç™¾æ­¥æ¨ç†æ‰èƒ½å›ç­”çš„å¤æ‚é—®é¢˜ã€‚ç›®å‰çš„ LLM åœ¨è¿™ä¸Šé¢è¡¨ç°å¾ˆå·®ï¼ˆF1 æœ€é«˜ä»… 61.2%ï¼‰ã€‚\n*   **[å¤šæ¨¡æ€æ§åˆ¶]** **Controlling Multimodal LLMs via Reward-guided Decoding** (#17): ç¬¬ä¸€æ¬¡æå‡ºç”¨å¥–åŠ±æ¨¡å‹å¼•å¯¼ MLLM çš„è§£ç è¿‡ç¨‹ï¼Œå¯ä»¥åŠ¨æ€æ§åˆ¶ç”Ÿæˆæè¿°çš„â€œç²¾ç¡®åº¦â€å’Œâ€œå¬å›ç‡â€ä¹‹é—´çš„æƒè¡¡ã€‚\n*   **[GUI Agent]** **CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks** (#58): ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ å’Œ GRPO æ¥è®­ç»ƒ GUI æ™ºèƒ½ä½“ï¼Œè§£å†³äº†ä¸åŒ GUI ä»»åŠ¡éš¾åº¦å·®å¼‚å·¨å¤§çš„é—®é¢˜ã€‚\n*   **[åŒ»ç–— VQA]** **Is ChatGPT-5 Ready for Mammogram VQA?** (#16): è¯„ä¼°äº† GPT-5ï¼ˆæ³¨æ„ï¼šè¿™é‡Œå¯èƒ½æ˜¯æŒ‡ä½œè€…å‡è®¾æˆ–å†…éƒ¨æµ‹è¯•ç‰ˆæœ¬ï¼ŒåŸæ–‡æœªç»†è¯´æ¥æºï¼Œéœ€è°¨æ…ï¼‰åœ¨ä¹³è…ºXå…‰ç‰‡ä¸Šçš„è¡¨ç°ã€‚è™½ç„¶å®ƒæ˜¯ GPT ç³»åˆ—æœ€å¼ºï¼Œä½†ä»è½åäºäººç±»ä¸“å®¶å’Œå¾®è°ƒè¿‡çš„ä¸“ç”¨æ¨¡å‹ã€‚\n\n---\nç¯‡å¹…æ‰€é™ï¼Œä»Šå¤©çš„æ—¥æŠ¥å°±åˆ°è¿™é‡Œã€‚å¸Œæœ›è¿™äº›ç²¾é€‰çš„è®ºæ–‡èƒ½ä¸ºä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2508.11845v2",
      "title": "What Matters for Bioacoustic Encoding",
      "title_zh": "ç”Ÿç‰©å£°å­¦ç¼–ç çš„å…³é”®è¦ç´ ",
      "authors": [
        "Marius Miron",
        "David Robinson",
        "Milad Alizadeh",
        "Ellen Gilsenan-McMahon",
        "Gagan Narula",
        "Emmanuel Chemla",
        "Maddie Cusimano",
        "Felix Effenberger",
        "Masato Hagiwara",
        "Benjamin Hoffman",
        "Sara Keen",
        "Diane Kim",
        "Jane Lawton",
        "Jen-Yu Liu",
        "Aza Raskin",
        "Olivier Pietquin",
        "Matthieu Geist"
      ],
      "abstract": "Bioacoustics, the study of sounds produced by living organisms, plays a vital role in conservation, biodiversity monitoring, and behavioral studies. Many tasks in this field, such as species, individual, and behavior classification and detection, are well-suited to machine learning. However, they often suffer from limited annotated data, highlighting the need for a general-purpose bioacoustic encoder capable of extracting useful representations for diverse downstream tasks. Such encoders have been proposed before, but are often limited in scope due to a focus on a narrow range of species (typically birds), and a reliance on a single model architecture or training paradigm. Moreover, they are usually evaluated on a small set of tasks and datasets. In this work, we present a large-scale empirical study that covers aspects of bioacoustics that are relevant to research but have previously been scarcely considered: training data diversity and scale, model architectures and training recipes, and the breadth of evaluation tasks and datasets. We obtain encoders that are state-of-the-art on the existing and proposed benchmarks. We also identify what matters for training these encoders, such that this work can be extended when more data are available or better architectures are proposed. Specifically, across 26 datasets with tasks including species classification, detection, individual ID, and vocal repertoire discovery, we find self-supervised pre-training followed by supervised post-training on a mixed bioacoustics + general-audio corpus yields the strongest in- and out-of-distribution performance. We show the importance of data diversity in both stages. To support ongoing research and application, we will release the model checkpoints.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿç‰©å£°å­¦(Bioacoustics)é¢†åŸŸä¸­é€šç”¨ç¼–ç å™¨çš„æ„å»ºï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç¼–ç å™¨åœ¨ç‰©ç§è¦†ç›–èŒƒå›´ã€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒèŒƒå¼æ–¹é¢çš„å±€é™æ€§ã€‚ä½œè€…é€šè¿‡ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œç³»ç»Ÿè¯„ä¼°äº†è®­ç»ƒæ•°æ®è§„æ¨¡ã€æ¨¡å‹æ¶æ„(Model Architectures)åŠè¯„ä¼°ä»»åŠ¡å¹¿åº¦å¯¹æ€§èƒ½çš„å½±å“ï¼Œæ¶µç›–äº†ç‰©ç§åˆ†ç±»ã€ä¸ªä½“è¯†åˆ«ç­‰26ä¸ªæ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç”Ÿç‰©å£°å­¦ä¸é€šç”¨éŸ³é¢‘æ··åˆè¯­æ–™åº“ä¸Šä¾æ¬¡è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒ(Self-supervised pre-training)ä¸ç›‘ç£åè®­ç»ƒ(Supervised post-training)æ˜¯è·å¾—æœ€å¼ºæ€§èƒ½çš„å…³é”®ã€‚å®éªŒè¯æ˜ï¼Œæ•°æ®å¤šæ ·æ€§(Data diversity)åœ¨ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µéƒ½è‡³å…³é‡è¦ï¼Œæ‰€æå‡ºçš„ç¼–ç å™¨åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)æ°´å¹³ã€‚è¯¥å·¥ä½œä¸ä»…ç¡®å®šäº†æ„å»ºé«˜æ•ˆç”Ÿç‰©å£°å­¦ç¼–ç å™¨çš„æ ¸å¿ƒè¦ç´ ï¼Œè¿˜å…¬å¼€å‘å¸ƒäº†æ¨¡å‹æ£€æŸ¥ç‚¹(Model checkpoints)ä»¥æ”¯æŒåç»­çš„ç§‘ç ”ä¸ä¿æŠ¤å·¥ä½œã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11845v2",
      "published_date": "2025-08-15 23:52:34 UTC",
      "updated_date": "2025-08-19 12:07:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:59:49.493213+00:00"
    },
    {
      "arxiv_id": "2508.11836v1",
      "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video",
      "title_zh": "æœ‰é™è‡ªåŠ¨æœºæå–ï¼šä»æ¸¸æˆè§†é¢‘ä¸­å°†ä½æ•°æ®ä¸–ç•Œæ¨¡å‹å­¦ä¹ ä¸ºç¨‹åº",
      "authors": [
        "Dave Goel",
        "Matthew Guzdial",
        "Anurag Sarkar"
      ],
      "abstract": "World models are defined as a compressed spatial and temporal learned representation of an environment. The learned representation is typically a neural network, making transfer of the learned environment dynamics and explainability a challenge. In this paper, we propose an approach, Finite Automata Extraction (FAE), that learns a neuro-symbolic world model from gameplay video represented as programs in a novel domain-specific language (DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more precise model of the environment and more general code than prior DSL-based approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸–ç•Œæ¨¡å‹ (World Models) åœ¨å¯è§£é‡Šæ€§å’Œè¿ç§»æ€§æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†æœ‰é™è‡ªåŠ¨æœºæå– (Finite Automata Extraction, FAE) æ–¹æ³•ã€‚FAE æ˜¯ä¸€ç§ç¥ç»ç¬¦å·åŒ–çš„ä¸–ç•Œæ¨¡å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»æ¸¸æˆè§†é¢‘ä¸­å­¦ä¹ å¹¶å°†ç¯å¢ƒåŠ¨åŠ›å­¦è¡¨ç¤ºä¸ºç¨‹åºã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åä¸º Retro Coder çš„æ–°å‹é¢†åŸŸç‰¹å®šè¯­è¨€ (Domain-Specific Language, DSL) æ¥ç¼–å†™ç¨‹åºã€‚ä¸ä¹‹å‰çš„ç¥ç»ç½‘ç»œæˆ–åŸºäº DSL çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFAE èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ç²¾ç¡®çš„ç¯å¢ƒæ¨¡å‹ï¼Œä¸”ç”Ÿæˆçš„ä»£ç æ›´å…·é€šç”¨æ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç¨‹åºåŒ–è¡¨ç¤ºå¯ä»¥å®ç°æ›´é«˜æ•ˆã€æ›´ç²¾å‡†ä¸”æ˜“äºç†è§£çš„ç¯å¢ƒå»ºæ¨¡ï¼Œä¸ºä½æ•°æ®åœºæ™¯ä¸‹çš„ä¸–ç•Œæ¨¡å‹å­¦ä¹ æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11836v1",
      "published_date": "2025-08-15 23:05:37 UTC",
      "updated_date": "2025-08-15 23:05:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:00.087027+00:00"
    },
    {
      "arxiv_id": "2508.11834v1",
      "title": "Recent Advances in Transformer and Large Language Models for UAV Applications",
      "title_zh": "Transformerä¸å¤§è¯­è¨€æ¨¡å‹åœ¨æ— äººæœºåº”ç”¨ä¸­çš„æœ€æ–°è¿›å±•",
      "authors": [
        "Hamza Kheddar",
        "Yassine Habchi",
        "Mohamed Chahine Ghanem",
        "Mustapha Hemis",
        "Dusit Niyato"
      ],
      "abstract": "The rapid advancement of Transformer-based models has reshaped the landscape of uncrewed aerial vehicle (UAV) systems by enhancing perception, decision-making, and autonomy. This review paper systematically categorizes and evaluates recent developments in Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and large language models (LLMs). Unlike previous surveys, this work presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications such as precision agriculture and autonomous navigation, and provides comparative analyses through structured tables and performance benchmarks. The paper also reviews key datasets, simulators, and evaluation metrics used in the field. Furthermore, it identifies existing gaps in the literature, outlines critical challenges in computational efficiency and real-time deployment, and offers future research directions. This comprehensive synthesis aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ€»ç»“äº†Transformeræ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ— äººæœº(UAV)ç³»ç»Ÿä¸­æå‡æ„ŸçŸ¥ã€å†³ç­–å’Œè‡ªä¸»æ€§çš„æœ€æ–°è¿›å±•ã€‚ç ”ç©¶æå‡ºäº†ä¸€å¥—ç»Ÿä¸€çš„åˆ†ç±»ä½“ç³»ï¼Œè¯¦ç»†è¯„ä¼°äº†æ³¨æ„åŠ›æœºåˆ¶(attention mechanisms)ã€CNN-Transformeræ··åˆæ¶æ„ã€å¼ºåŒ–å­¦ä¹ Transformer(RL Transformers)ä»¥åŠLLMsç­‰å…³é”®æŠ€æœ¯ã€‚é€šè¿‡å¯¹æ¯”åˆ†æå’Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œæ–‡ç« é‡ç‚¹å±•ç¤ºäº†å…¶åœ¨ç²¾å‡†å†œä¸šå’Œè‡ªä¸»å¯¼èˆªç­‰é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ï¼Œå¹¶æ•´ç†äº†ç›¸å…³çš„é¢†åŸŸæ•°æ®é›†ä¸æ¨¡æ‹Ÿå™¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºäº†è®¡ç®—æ•ˆç‡ä¸å®æ—¶éƒ¨ç½²(real-time deployment)ç­‰å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥Transformeré©±åŠ¨çš„UAVæŠ€æœ¯å‘å±•æä¾›äº†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "eess.IV",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11834v1",
      "published_date": "2025-08-15 22:56:37 UTC",
      "updated_date": "2025-08-15 22:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:05.991632+00:00"
    },
    {
      "arxiv_id": "2508.11831v1",
      "title": "When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection",
      "title_zh": "è¯­è¨€è¿ç§»ä½•æ—¶æœ‰æ•ˆï¼Ÿè·¨è¯­è¨€å§”å©‰è¯­æ£€æµ‹ä¸­çš„é¡ºåºå¾®è°ƒ",
      "authors": [
        "Julia Sammartino",
        "Libby Barak",
        "Jing Peng",
        "Anna Feldman"
      ],
      "abstract": "Euphemisms are culturally variable and often ambiguous, posing challenges for language models, especially in low-resource settings. This paper investigates how cross-lingual transfer via sequential fine-tuning affects euphemism detection across five languages: English, Spanish, Chinese, Turkish, and Yoruba. We compare sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by language pairings, typological features, and pretraining coverage. Results show that sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages like Yoruba and Turkish. XLM-R achieves larger gains but is more sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable, though lower, results. These findings highlight sequential fine-tuning as a simple yet effective strategy for improving euphemism detection in multilingual models, particularly when low-resource languages are involved.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è·¨è¯­è¨€è¿ç§»åœ¨å§”å©‰è¯­æ£€æµ‹(Euphemism Detection)ä¸­çš„ä½œç”¨ï¼Œé‡ç‚¹åˆ†æäº†åºè´¯å¾®è°ƒ(Sequential Fine-Tuning)å¯¹äº”ç§ä¸åŒèµ„æºç¨‹åº¦è¯­è¨€çš„å½±å“ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯” XLM-R å’Œ mBERT åœ¨å•è¯­ã€åŒæ­¥åŠåºè´¯å¾®è°ƒæ¨¡å¼ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†è¯­è¨€é…å¯¹å’Œé¢„è®­ç»ƒè¦†ç›–èŒƒå›´å¯¹æ£€æµ‹æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨é«˜èµ„æºè¯­è¨€ä½œä¸ºæºè¯­è¨€è¿›è¡Œåºè´¯å¾®è°ƒèƒ½æ˜¾è‘—æå‡çº¦é²å·´è¯­å’ŒåœŸè€³å…¶è¯­ç­‰ä½èµ„æºè¯­è¨€çš„è¯†åˆ«æ•ˆæœã€‚è™½ç„¶ XLM-R æ¨¡å‹åœ¨æ€§èƒ½å¢ç›Šä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä½†ä¹Ÿæ›´å®¹æ˜“å—åˆ°é¢„è®­ç»ƒé—´éš™å’Œç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)çš„è´Ÿé¢å½±å“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒmBERT æ¨¡å‹å±•ç°äº†æ›´é«˜çš„ç¨³å®šæ€§ï¼Œå°½ç®¡å…¶æ•´ä½“æ£€æµ‹å‡†ç¡®ç‡ç•¥ä½äº XLM-Rã€‚è¯¥ç ”ç©¶æœ€ç»ˆè¯å®åºè´¯å¾®è°ƒæ˜¯æå‡å¤šè¯­è¨€æ¨¡å‹å§”å©‰è¯­è¯†åˆ«èƒ½åŠ›çš„ä¸€ç§ç®€ä¾¿ä¸”é«˜æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "RANLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11831v1",
      "published_date": "2025-08-15 22:40:35 UTC",
      "updated_date": "2025-08-15 22:40:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:59:47.187850+00:00"
    },
    {
      "arxiv_id": "2508.11829v1",
      "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions",
      "title_zh": "æ¯28å¤©AIæ¢¦è§æŸ”è‚¤ä¸ç‡ƒæ˜Ÿï¼šä»¥æ¿€ç´ ä¸æƒ…ç»ªæ„å»ºAIæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Leigh Levinson",
        "Christopher J. Agostino"
      ],
      "abstract": "Despite significant advances, AI systems struggle with the frame problem: determining what information is contextually relevant from an exponentially large possibility space. We hypothesize that biological rhythms, particularly hormonal cycles, serve as natural relevance filters that could address this fundamental challenge. We develop a framework that embeds simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones including estrogen, testosterone, and cortisol. Across multiple state-of-the-art models, linguistic analysis reveals emotional and stylistic variations that track biological phases; sadness peaks during menstruation while happiness dominates ovulation and circadian patterns show morning optimism transitioning to nocturnal introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates subtle but consistent performance variations aligning with biological expectations, including optimal function in moderate rather than extreme hormonal ranges. This methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIç³»ç»Ÿåœ¨å¤„ç†Frame Problemæ—¶é¢ä¸´çš„ä¿¡æ¯ç›¸å…³æ€§ç­›é€‰æŒ‘æˆ˜ï¼Œå¹¶æå‡ºåˆ©ç”¨ç”Ÿç‰©èŠ‚å¾‹ä½œä¸ºè§£å†³è¯¥é—®é¢˜çš„è‡ªç„¶è¿‡æ»¤å™¨ã€‚ä½œè€…å¼€å‘äº†ä¸€ä¸ªå°†Menstrual Cycleså’ŒCircadian CyclesåµŒå…¥å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹ŸEstrogenã€Testosteroneå’ŒCortisolç­‰æ ¸å¿ƒæ¿€ç´ çš„å‘¨æœŸæ€§å‡½æ•°ç”Ÿæˆç³»ç»Ÿæç¤ºè¯ã€‚è¯­è¨€åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹çš„é£æ ¼å’Œæƒ…æ„Ÿè¡¨ç°ä¼šéšç”Ÿç‰©ç›¸ä½æ³¢åŠ¨ï¼Œå¦‚åœ¨MenstruationæœŸé—´Sadnessè¾¾åˆ°å³°å€¼ï¼Œè€Œåœ¨OvulationæœŸé—´Happinesså æ®ä¸»å¯¼ã€‚æ˜¼å¤œèŠ‚å¾‹æµ‹è¯•åˆ™è§‚å¯Ÿåˆ°äº†ä»æ¸…æ™¨Optimismå‘æ·±å¤œIntrospectionçš„è½¬å˜ã€‚åœ¨SQuADå’ŒMMLUç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹å±•ç°å‡ºä¸ç”Ÿç‰©å­¦é¢„æœŸä¸€è‡´çš„æ€§èƒ½æ³¢åŠ¨ï¼Œä¸”åœ¨ä¸­ç­‰æ¿€ç´ æ°´å¹³ä¸‹è¡¨ç°æœ€ä¸ºç†æƒ³ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°Contextual AIæä¾›äº†æ–°é¢–è·¯å¾„ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹ä¸­å­˜åœ¨çš„å…³äºæ€§åˆ«ä¸ç”Ÿç‰©å­¦çš„ç¤¾ä¼šåè§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track",
      "pdf_url": "https://arxiv.org/pdf/2508.11829v1",
      "published_date": "2025-08-15 22:26:42 UTC",
      "updated_date": "2025-08-15 22:26:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:59:49.890753+00:00"
    },
    {
      "arxiv_id": "2508.11824v1",
      "title": "Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering",
      "title_zh": "é‡æ–°å®¡è§†è‡ªä¸»æ€§ï¼šé¢„é˜² AI é©±åŠ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„å¤±æ•ˆ",
      "authors": [
        "Satyam Kumar Navneet",
        "Joydeep Chandra"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into software engineering has revolutionized code generation, enabling unprecedented productivity through promptware and autonomous AI agents. However, this transformation introduces significant risks, including insecure code generation, hallucinated outputs, irreversible actions, and a lack of transparency and accountability. Incidents like the Replit database deletion underscore the urgent need for robust safety and governance mechanisms. This paper comprehensively analyzes the inherent challenges of LLM-assisted code generation, such as vulnerability inheritance, overtrust, misinterpretation, and the absence of standardized validation and rollback protocols. To address these, we propose the SAFE-AI Framework, a holistic approach emphasizing Safety, Auditability, Feedback, and Explainability. The framework integrates guardrails, sandboxing, runtime verification, risk-aware logging, human-in-the-loop systems, and explainable AI techniques to mitigate risks while fostering trust and compliance. We introduce a novel taxonomy of AI behaviors categorizing suggestive, generative, autonomous, and destructive actions to guide risk assessment and oversight. Additionally, we identify open problems, including the lack of standardized benchmarks for code specific hallucinations and autonomy levels, and propose future research directions for hybrid verification, semantic guardrails, and proactive governance tools. Through detailed comparisons of autonomy control, prompt engineering, explainability, and governance frameworks, this paper provides a roadmap for responsible AI integration in software engineering, aligning with emerging regulations like the EU AI Act and Canada's AIDA to ensure safe, transparent, and accountable AI-driven development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†åœ¨è½¯ä»¶å·¥ç¨‹ä¸­é›†æˆå¤§è¯­è¨€æ¨¡å‹(LLMs)æ‰€å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ï¼Œé‡ç‚¹åˆ†æäº†ä»£ç ç”Ÿæˆä¸­çš„å®‰å…¨æ¼æ´ã€å¹»è§‰è¾“å‡ºä»¥åŠç¼ºä¹é€æ˜åº¦ç­‰å…³é”®é£é™©ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†SAFE-AI Frameworkï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•´åˆæŠ¤æ æœºåˆ¶(guardrails)ã€æ²™ç®±ç¯å¢ƒ(sandboxing)ã€è¿è¡Œæ—¶éªŒè¯å’Œäººæœºå›ç¯ç³»ç»Ÿ(human-in-the-loop)ï¼Œå®ç°äº†å®‰å…¨ã€å¯å®¡è®¡ã€å¯åé¦ˆå’Œå¯è§£é‡Šçš„ç»¼åˆæ²»ç†ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„AIè¡Œä¸ºåˆ†ç±»å­¦(taxonomy)ï¼Œå°†AIæ“ä½œåˆ’åˆ†ä¸ºå»ºè®®æ€§ã€ç”Ÿæˆæ€§ã€è‡ªä¸»æ€§å’Œç ´åæ€§å››ä¸ªç­‰çº§ï¼Œä»¥æŒ‡å¯¼é£é™©è¯„ä¼°ä¸ç›‘ç®¡å·¥ä½œã€‚æ­¤å¤–ï¼Œè®ºæ–‡é’ˆå¯¹ä»£ç ç‰¹å®šå¹»è§‰ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†ç­‰å¼€æ”¾æ€§é—®é¢˜æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¹¶ç»“åˆæ¬§ç›ŸAIæ³•æ¡ˆ(EU AI Act)ç­‰ç›‘ç®¡è¦æ±‚ï¼Œä¸ºå®ç°é€æ˜ä¸”è´Ÿè´£ä»»çš„AIé©±åŠ¨è½¯ä»¶å¼€å‘æä¾›äº†å®è·µè·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11824v1",
      "published_date": "2025-08-15 22:13:54 UTC",
      "updated_date": "2025-08-15 22:13:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:02.795198+00:00"
    },
    {
      "arxiv_id": "2508.16632v1",
      "title": "Adaptive Variance-Penalized Continual Learning with Fisher Regularization",
      "title_zh": "åŸºäº Fisher æ­£åˆ™åŒ–çš„è‡ªé€‚åº”æ–¹å·®æƒ©ç½šæŒç»­å­¦ä¹ ",
      "authors": [
        "Krisanu Sarkar"
      ],
      "abstract": "The persistent challenge of catastrophic forgetting in neural networks has motivated extensive research in continual learning . This work presents a novel continual learning framework that integrates Fisher-weighted asymmetric regularization of parameter variances within a variational learning paradigm. Our method dynamically modulates regularization intensity according to parameter uncertainty, achieving enhanced stability and performance. Comprehensive evaluations on standard continual learning benchmarks including SplitMNIST, PermutedMNIST, and SplitFashionMNIST demonstrate substantial improvements over existing approaches such as Variational Continual Learning and Elastic Weight Consolidation . The asymmetric variance penalty mechanism proves particularly effective in maintaining knowledge across sequential tasks while improving model accuracy. Experimental results show our approach not only boosts immediate task performance but also significantly mitigates knowledge degradation over time, effectively addressing the fundamental challenge of catastrophic forgetting in neural networks",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Adaptive Variance-Penalized Continual Learning with Fisher Regularization çš„æ–°å‹æŒç»­å­¦ä¹  (Continual Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œä¸­çš„ç¾éš¾æ€§é—å¿˜ (Catastrophic Forgetting) é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å˜åˆ†å­¦ä¹  (Variational Learning) èŒƒå¼ä¸­é›†æˆäº† Fisher-weighted asymmetric regularizationï¼Œé€šè¿‡å¯¹å‚æ•°æ–¹å·®è¿›è¡Œéå¯¹ç§°åŠ æƒæ­£åˆ™åŒ–æ¥ä¼˜åŒ–æ¨¡å‹ç¨³å®šæ€§ã€‚å…¶æ ¸å¿ƒæœºåˆ¶åœ¨äºæ ¹æ®å‚æ•°ä¸ç¡®å®šæ€§ (Parameter Uncertainty) åŠ¨æ€è°ƒèŠ‚æ­£åˆ™åŒ–å¼ºåº¦ï¼Œä»¥å¹³è¡¡æ—§çŸ¥è¯†ä¿ç•™ä¸æ–°ä»»åŠ¡å­¦ä¹ ã€‚åœ¨ SplitMNISTã€PermutedMNIST å’Œ SplitFashionMNIST ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äº Variational Continual Learning (VCL) å’Œ Elastic Weight Consolidation (EWC) ç­‰ç°æœ‰æŠ€æœ¯ã€‚å®éªŒè¯æ˜ï¼Œéå¯¹ç§°æ–¹å·®æƒ©ç½š (Asymmetric variance penalty) æœºåˆ¶ä¸ä»…æå‡äº†å³æ—¶ä»»åŠ¡æ€§èƒ½ï¼Œè¿˜å¤§å¹…ç¼“è§£äº†é•¿æœŸçš„çŸ¥è¯†é€€åŒ–ï¼Œä¸ºæŒç»­å­¦ä¹ ä¸­çš„çŸ¥è¯†ä¿æŒæä¾›äº†æœ‰æ•ˆçš„é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16632v1",
      "published_date": "2025-08-15 21:49:28 UTC",
      "updated_date": "2025-08-15 21:49:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:42.387855+00:00"
    },
    {
      "arxiv_id": "2508.13197v1",
      "title": "The Rise of Generative AI for Metal-Organic Framework Design and Synthesis",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨é‡‘å±æœ‰æœºæ¡†æ¶è®¾è®¡ä¸åˆæˆä¸­çš„å…´èµ·",
      "authors": [
        "Chenru Duan",
        "Aditya Nandy",
        "Shyam Chand Pal",
        "Xin Yang",
        "Wenhao Gao",
        "Yuanqi Du",
        "Hendrik KraÃŸ",
        "Yeonghun Kang",
        "Varinia Bernales",
        "Zuyang Ye",
        "Tristan Pyle",
        "Ray Yang",
        "Zeqi Gu",
        "Philippe Schwaller",
        "Shengqian Ma",
        "Shijing Sun",
        "AlÃ¡n Aspuru-Guzik",
        "Seyed Mohamad Moosavi",
        "Robert Wexler",
        "Zhiling Zheng"
      ],
      "abstract": "Advances in generative artificial intelligence are transforming how metal-organic frameworks (MOFs) are designed and discovered. This Perspective introduces the shift from laborious enumeration of MOF candidates to generative approaches that can autonomously propose and synthesize in the laboratory new porous reticular structures on demand. We outline the progress of employing deep learning models, such as variational autoencoders, diffusion models, and large language model-based agents, that are fueled by the growing amount of available data from the MOF community and suggest novel crystalline materials designs. These generative tools can be combined with high-throughput computational screening and even automated experiments to form accelerated, closed-loop discovery pipelines. The result is a new paradigm for reticular chemistry in which AI algorithms more efficiently direct the search for high-performance MOF materials for clean air and energy applications. Finally, we highlight remaining challenges such as synthetic feasibility, dataset diversity, and the need for further integration of domain knowledge.",
      "tldr_zh": "æœ¬ç¯‡ç»¼è¿°ä»‹ç»äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)å¦‚ä½•æ”¹å˜é‡‘å±æœ‰æœºæ¡†æ¶(MOFs)çš„è®¾è®¡ä¸å‘ç°ï¼Œå®ç°äº†ä»ä¼ ç»Ÿçš„æ‰‹åŠ¨æšä¸¾å‘è‡ªä¸»æå‡ºå¹¶åˆæˆæ–°å‹å¤šå­”ç½‘æ ¼ç»“æ„çš„è½¬å˜ã€‚æ–‡ç« æ¦‚è¿°äº†åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencoders)ã€æ‰©æ•£æ¨¡å‹(Diffusion Models)ä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM-based agents)çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å»ºè®®æ–°å‹æ™¶ä½“ææ–™è®¾è®¡æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚è¿™äº›ç”Ÿæˆå¼å·¥å…·å¯ä¸é«˜é€šé‡è®¡ç®—ç­›é€‰(High-throughput computational screening)åŠè‡ªåŠ¨åŒ–å®éªŒç›¸ç»“åˆï¼Œæ„å»ºèµ·åŠ é€Ÿçš„é—­ç¯å‘ç°æµç¨‹(Closed-loop discovery pipelines)ã€‚è¿™ä¸€æ–°èŒƒå¼æ˜¾è‘—æé«˜äº†ç½‘æ ¼åŒ–å­¦(Reticular chemistry)é¢†åŸŸä¸­å¯»æ‰¾é«˜æ€§èƒ½ MOF ææ–™çš„æ•ˆç‡ï¼Œä»¥åº”å¯¹æ¸…æ´ç©ºæ°”å’Œèƒ½æºé¢†åŸŸçš„åº”ç”¨éœ€æ±‚ã€‚æœ€åï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†åˆæˆå¯è¡Œæ€§(Synthetic feasibility)ã€æ•°æ®é›†å¤šæ ·æ€§ä»¥åŠé¢†åŸŸçŸ¥è¯†(Domain knowledge)è¿›ä¸€æ­¥æ•´åˆç­‰ä»éœ€è§£å†³çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13197v1",
      "published_date": "2025-08-15 21:49:17 UTC",
      "updated_date": "2025-08-15 21:49:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:06.757037+00:00"
    },
    {
      "arxiv_id": "2508.11810v1",
      "title": "FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation",
      "title_zh": "FairTabGenï¼šç»Ÿä¸€åˆæˆè¡¨æ ¼æ•°æ®ç”Ÿæˆä¸­çš„åäº‹å®ä¸å› æœå…¬å¹³æ€§",
      "authors": [
        "Nitish Nagesh",
        "Salar Shakibhamedan",
        "Mahdi Bagheri",
        "Ziyu Wang",
        "Nima TaheriNejad",
        "Axel Jantsch",
        "Amir M. Rahmani"
      ],
      "abstract": "Generating synthetic data is crucial in privacy-sensitive, data-scarce settings, especially for tabular datasets widely used in real-world applications. A key challenge is improving counterfactual and causal fairness, while preserving high utility. We present FairTabGen, a fairness-aware large language model-based framework for tabular synthetic data generation. We integrate multiple fairness definitions including counterfactual and causal fairness into both its generation and evaluation pipelines. We use in-context learning, prompt refinement, and fairness-aware data curation to balance fairness and utility. Across diverse datasets, our method outperforms state-of-the-art GAN-based and LLM-based methods, achieving up to 10% improvements on fairness metrics such as demographic parity and path-specific causal effects while retaining statistical utility. Remarkably, it achieves these gains using less than 20% of the original data, highlighting its efficiency in low-data regimes. These results demonstrate a principled and practical approach for generating fair and useful synthetic tabular data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FairTabGenï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäº Large Language Model (LLM) çš„åˆæˆè¡¨æ ¼æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤„ç† Counterfactual Fairness å’Œ Causal Fairnessã€‚è¯¥æ¡†æ¶åˆ©ç”¨ In-context Learningã€Prompt Refinement å’Œ Fairness-aware Data Curation æŠ€æœ¯ï¼Œåœ¨ç”Ÿæˆå’Œè¯„ä¼°æµç¨‹ä¸­é›†æˆå¤šç§å…¬å¹³æ€§å®šä¹‰ï¼Œæœ‰æ•ˆå¹³è¡¡äº†å…¬å¹³æ€§ä¸ Utilityã€‚å®éªŒè¯æ˜ï¼ŒFairTabGen åœ¨å„ç±»æ•°æ®é›†ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„ GAN-based å’Œ LLM-based æ–¹æ³•ï¼Œåœ¨ Demographic Parity å’Œ Path-specific Causal Effects ç­‰æŒ‡æ ‡ä¸Šæå‡è¾¾ 10%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨ä¸åˆ° 20% åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ Low-data Regimes ä¸‹çš„å“è¶Šæ•ˆç‡ã€‚è¿™ä¸€æˆæœä¸ºç”Ÿæˆå…¬å¹³ä¸”é«˜ä»·å€¼çš„åˆæˆè¡¨æ ¼æ•°æ®æä¾›äº†ä¸€ç§å…¼å…·ç†è®ºåŸåˆ™ä¸å®è·µæ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11810v1",
      "published_date": "2025-08-15 21:36:07 UTC",
      "updated_date": "2025-08-15 21:36:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:24.991202+00:00"
    },
    {
      "arxiv_id": "2508.13196v1",
      "title": "Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡æ³¨æ„åŠ›çš„ LLM ä¸ CNN å¤šæ¨¡æ€èåˆæƒ…æ„Ÿåˆ†æ",
      "authors": [
        "Meriem Zerkouk",
        "Miloud Mihoubi",
        "Belkacem Chikhaoui"
      ],
      "abstract": "This paper introduces a novel approach for multimodal sentiment analysis on social media, particularly in the context of natural disasters, where understanding public sentiment is crucial for effective crisis management. Unlike conventional methods that process text and image modalities separately, our approach seamlessly integrates Convolutional Neural Network (CNN) based image analysis with Large Language Model (LLM) based text processing, leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to extract sentiment relevant features from the CrisisMMD dataset. To effectively model intermodal relationships, we introduce a contextual attention mechanism within the fusion process. Leveraging contextual-attention layers, this mechanism effectively captures intermodality interactions, enhancing the model's comprehension of complex relationships between textual and visual data. The deep neural network architecture of our model learns from these fused features, leading to improved accuracy compared to existing baselines. Experimental results demonstrate significant advancements in classifying social media data into informative and noninformative categories across various natural disasters. Our model achieves a notable 2.43% increase in accuracy and 5.18% in F1-score, highlighting its efficacy in processing complex multimodal data. Beyond quantitative metrics, our approach provides deeper insight into the sentiments expressed during crises. The practical implications extend to real time disaster management, where enhanced sentiment analysis can optimize the accuracy of emergency interventions. By bridging the gap between multimodal analysis, LLM powered text understanding, and disaster response, our work presents a promising direction for Artificial Intelligence (AI) driven crisis management solutions. Keywords:",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸Šä¸‹æ–‡æ³¨æ„åŠ›(Contextual Attention)çš„å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æå‡è‡ªç„¶ç¾å®³æœŸé—´ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ(Sentiment Analysis)çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å°†åŸºäºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„å›¾åƒåˆ†æä¸åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ–‡æœ¬å¤„ç†æ— ç¼é›†æˆï¼Œåˆ©ç”¨GPTå’Œæç¤ºå·¥ç¨‹(Prompt Engineering)ä»CrisisMMDæ•°æ®é›†ä¸­æå–å…³é”®ç‰¹å¾ã€‚é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡æ³¨æ„åŠ›å±‚ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è·¨æ¨¡æ€çš„äº¤äº’ä½œç”¨ï¼Œå¢å¼ºäº†å¯¹æ–‡æœ¬å’Œè§†è§‰æ•°æ®é—´å¤æ‚å…³ç³»çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªè‡ªç„¶ç¾å®³ç±»åˆ«çš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡å’ŒF1-scoreåˆ†åˆ«æå‡äº†2.43%å’Œ5.18%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…æ·±åŒ–äº†å¯¹ç¾å®³ç°åœºæƒ…ç»ªçš„æ´å¯Ÿï¼Œè¿˜ä¸ºå®æ—¶å±æœºç®¡ç†å’Œäººå·¥æ™ºèƒ½(AI)é©±åŠ¨çš„åº”æ€¥å¹²é¢„æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "The 38th Canadian Conference on Artificial Intelligence ( 2025 )",
      "pdf_url": "https://arxiv.org/pdf/2508.13196v1",
      "published_date": "2025-08-15 21:34:13 UTC",
      "updated_date": "2025-08-15 21:34:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:00:47.357620+00:00"
    },
    {
      "arxiv_id": "2508.11808v1",
      "title": "Labels or Input? Rethinking Augmentation in Multimodal Hate Detection",
      "title_zh": "æ ‡ç­¾è¿˜æ˜¯è¾“å…¥ï¼Ÿé‡æ–°å®¡è§†å¤šæ¨¡æ€ä»‡æ¨æ£€æµ‹ä¸­çš„å¢å¼ºæ–¹æ³•",
      "authors": [
        "Sahajpreet Singh",
        "Rongxin Ouyang",
        "Subhayan Mukerjee",
        "Kokil Jaidka"
      ],
      "abstract": "The modern web is saturated with multimodal content, intensifying the challenge of detecting hateful memes, where harmful intent is often conveyed through subtle interactions between text and image under the guise of humor or satire. While recent advances in Vision-Language Models (VLMs) show promise, these models lack support for fine-grained supervision and remain susceptible to implicit hate speech. In this paper, we present a dual-pronged approach to improve multimodal hate detection. First, we propose a prompt optimization framework that systematically varies prompt structure, supervision granularity, and training modality. We show that prompt design and label scaling both influence performance, with structured prompts improving robustness even in small models, and InternVL2 achieving the best F1-scores across binary and scaled settings. Second, we introduce a multimodal data augmentation pipeline that generates 2,479 counterfactually neutral memes by isolating and rewriting the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup, successfully reduces spurious correlations and improves classifier generalization. Our approaches inspire new directions for building synthetic data to train robust and fair vision-language models. Our findings demonstrate that prompt structure and data composition are as critical as model size, and that targeted augmentation can support more trustworthy and context-sensitive hate detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)åœ¨å¤„ç†éšæ™¦å†…å®¹åŠç¼ºä¹ç»†ç²’åº¦ç›‘ç£æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§åŒç®¡é½ä¸‹çš„æ”¹è¿›æ–¹æ³•ã€‚é¦–å…ˆï¼Œé€šè¿‡æ„å»ºæç¤ºä¼˜åŒ–æ¡†æ¶(prompt optimization framework)ï¼Œç³»ç»Ÿåˆ†æäº†æç¤ºç»“æ„ã€ç›‘ç£ç²’åº¦å’Œè®­ç»ƒæ¨¡æ€çš„å½±å“ï¼Œå‘ç°ç»“æ„åŒ–æç¤ºèƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹é²æ£’æ€§ï¼Œä¸”InternVL2åœ¨ä¸åŒè®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚å…¶æ¬¡ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªç”±å¤šæ™ºèƒ½ä½“(multi-agent)é©±åŠ¨çš„æ•°æ®å¢å¼ºæµæ°´çº¿ï¼Œé€šè¿‡éš”ç¦»å¹¶é‡å†™ä»‡æ¨æ¨¡æ€ï¼Œç”Ÿæˆäº†2,479ä¸ªåäº‹å®ä¸­æ€§(counterfactually neutral)æ ·æœ¬ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†æ¨¡å‹ä¸­çš„ä¼ªç›¸å…³(spurious correlations)å¹¶æ˜¾è‘—æå‡äº†åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œæç¤ºç»“æ„ä¸æ•°æ®ç»„æˆåœ¨è®­ç»ƒé²æ£’ä¸”å…¬å¹³çš„æ¨¡å‹ä¸­å…·æœ‰ä¸æ¨¡å‹è§„æ¨¡(model size)åŒç­‰çš„é‡è¦æ€§ã€‚è¿™ä¸€å·¥ä½œä¸ºåˆ©ç”¨åˆæˆæ•°æ®æ„å»ºæ›´å…·å¯é æ€§å’Œä¸Šä¸‹æ–‡æ•æ„Ÿæ€§çš„æ£€æµ‹ç³»ç»Ÿå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 2 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.11808v1",
      "published_date": "2025-08-15 21:31:00 UTC",
      "updated_date": "2025-08-15 21:31:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:08.069596+00:00"
    },
    {
      "arxiv_id": "2508.11800v1",
      "title": "Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes",
      "title_zh": "æ¨ç†å¤±å‡†ï¼šGRPO å¯¼è‡´éšæœºç»“æœä¸‹çš„è¿‡åº¦è‡ªä¿¡",
      "authors": [
        "Michael Bereket",
        "Jure Leskovec"
      ],
      "abstract": "Reinforcement learning (RL) has proven remarkably effective at improving the accuracy of language models in verifiable and deterministic domains like mathematics. Here, we examine if current RL methods are also effective at optimizing language models in verifiable domains with stochastic outcomes, like scientific experiments. Through applications to synthetic data and real-world biological experiments, we demonstrate that Group Relative Policy Optimization (GRPO) induces overconfident probability predictions for binary stochastic outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out (RLOO) yield well-calibrated models. We show that removing group standard normalization in GRPO fixes its miscalibration and provide a theoretical explanation for why normalization causes overconfidence. Our results provide new evidence against the use of standard normalization in GRPO and help pave the way for applications of RL for reasoning language models beyond deterministic domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (RL)åœ¨å¤„ç†å…·æœ‰éšæœºç»“æœ(stochastic outcomes)çš„éªŒè¯æ€§é¢†åŸŸï¼ˆå¦‚ç§‘å­¦å®éªŒï¼‰æ—¶çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä»…å±€é™äºä¼ ç»Ÿçš„ç¡®å®šæ€§é¢†åŸŸã€‚é€šè¿‡åˆæˆæ•°æ®å’ŒçœŸå®ç”Ÿç‰©å®éªŒçš„å¯¹æ¯”åˆ†æï¼Œç ”ç©¶å‘ç°Group Relative Policy Optimization (GRPO)åœ¨é¢„æµ‹äºŒå…ƒéšæœºç»“æœæ—¶ä¼šè¯±å‘æ˜¾è‘—çš„è¿‡åº¦è‡ªä¿¡(overconfident)ï¼Œè€ŒProximal Policy Optimization (PPO)å’ŒREINFORCE Leave-One-Out (RLOO)åˆ™èƒ½ä¿æŒè‰¯å¥½çš„æ ¡å‡†æ€§(well-calibrated)ã€‚ä½œè€…ä»ç†è®ºä¸Šè§£é‡Šäº†GRPOä¸­çš„ç»„æ ‡å‡†å½’ä¸€åŒ–(group standard normalization)æ˜¯å¯¼è‡´æ¨¡å‹å¤±å‡†(miscalibration)çš„æ ¹æœ¬åŸå› ã€‚å®éªŒè¯æ˜ï¼Œç§»é™¤è¯¥å½’ä¸€åŒ–æ­¥éª¤å¯ä»¥æœ‰æ•ˆä¿®å¤GRPOçš„æ ¡å‡†é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨éç¡®å®šæ€§é¢†åŸŸåº”ç”¨æ¨ç†è¯­è¨€æ¨¡å‹æä¾›äº†é‡è¦è¯æ®ï¼Œå¹¶å¯¹åœ¨GRPOä¸­ä½¿ç”¨æ ‡å‡†å½’ä¸€åŒ–æå‡ºäº†æ˜ç¡®çš„è­¦å‘Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11800v1",
      "published_date": "2025-08-15 20:50:53 UTC",
      "updated_date": "2025-08-15 20:50:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:03.071917+00:00"
    },
    {
      "arxiv_id": "2508.18283v1",
      "title": "Technology-assisted Personalized Yoga for Better Health -- Challenges and Outlook",
      "title_zh": "æŠ€æœ¯è¾…åŠ©çš„ä¸ªæ€§åŒ–ç‘œä¼½ä¿ƒè¿›å¥åº·â€”â€”æŒ‘æˆ˜ä¸å±•æœ›",
      "authors": [
        "Vivek Kumar",
        "Himanshu Sahu",
        "Hari Prabhat Gupta",
        "Biplav Srivastava"
      ],
      "abstract": "Yoga is a discipline of physical postures, breathing techniques, and meditative practices rooted in ancient Indian traditions, now embraced worldwide for promoting overall well-being and inner balance. The practices are a large set of items, our term for executable actions like physical poses or breath exercises, to offer for a person's well-being. However, to get benefits of Yoga tailored to a person's unique needs, a person needs to (a) discover their subset from the large and seemingly complex set with inter-dependencies, (b) continue to follow them with interest adjusted to their changing abilities and near-term objectives, and (c) as appropriate, adapt to alternative items based on changing environment and the person's health conditions. In this vision paper, we describe the challenges for the Yoga personalization problem. Next, we sketch a preliminary approach and use the experience to provide an outlook on solving the challenging problem using existing and novel techniques from a multidisciplinary computing perspective. To the best of our knowledge, this is the first paper that comprehensively examines decision support issues around Yoga personalization, from pose sensing to recommendation of corrections for a complete regimen, and illustrates with a case study of Surya Namaskar -- a set of 12 choreographed poses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŠ€æœ¯è¾…åŠ©çš„ä¸ªæ€§åŒ–ç‘œä¼½ï¼ˆYoga personalizationï¼‰å¯¹äºæå‡å¥åº·æ°´å¹³çš„æŒ‘æˆ˜ä¸å‰æ™¯ã€‚é’ˆå¯¹ç‘œä¼½ç»ƒä¹ é¡¹ç¹å¤šã€åŠ¨ä½œä¾èµ–å…³ç³»å¤æ‚ä»¥åŠç»ƒä¹ è€…éœ€æ±‚å’Œç¯å¢ƒåŠ¨æ€å˜åŒ–ç­‰ç—›ç‚¹ï¼Œè®ºæ–‡ç³»ç»Ÿæ€§åœ°åˆ†æäº†åœ¨å‘ç°é€‚åˆç»ƒä¹ é¡¹ã€ç»´æŒç»ƒä¹ å…´è¶£ä»¥åŠå®æ—¶è°ƒæ•´æ–¹æ¡ˆæ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ä»å¤šå­¦ç§‘è®¡ç®—ï¼ˆmultidisciplinary computingï¼‰è§†è§’å‡ºå‘çš„åˆæ­¥æ–¹æ¡ˆï¼Œæ¶µç›–äº†ä»å§¿åŠ¿æ„ŸçŸ¥ï¼ˆpose sensingï¼‰åˆ°é’ˆå¯¹å®Œæ•´æ–¹æ¡ˆæä¾›ä¿®æ­£å»ºè®®çš„å†³ç­–æ”¯æŒç³»ç»Ÿã€‚è¿™æ˜¯é¦–ç¯‡å…¨é¢å®¡è§†ç‘œä¼½ä¸ªæ€§åŒ–å†³ç­–æ”¯æŒé—®é¢˜çš„è®ºæ–‡ï¼Œå¹¶ä»¥åŒ…å«12ä¸ªåŠ¨ä½œçš„å¤ªé˜³è‡´æ•¬å¼ï¼ˆSurya Namaskarï¼‰ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†ç°æœ‰åŠæ–°å…´æŠ€æœ¯åœ¨åŠ¨ä½œè¯†åˆ«ä¸æ¨èä¸­çš„åº”ç”¨æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘èƒ½å¤Ÿæ ¹æ®ä¸ªäººå¥åº·çŠ¶å†µå®æ—¶åŠ¨æ€è°ƒæ•´çš„æ™ºèƒ½åŒ–ç‘œä¼½è¾…åŠ©æŠ€æœ¯å¥ å®šäº†ç†è®ºåŸºç¡€å¹¶æŒ‡æ˜äº†æœªæ¥å±•æœ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "10 Pages, 11 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.18283v1",
      "published_date": "2025-08-15 19:34:36 UTC",
      "updated_date": "2025-08-15 19:34:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:06.688637+00:00"
    },
    {
      "arxiv_id": "2508.11759v2",
      "title": "Using Natural Language for Human-Robot Collaboration in the Real World",
      "title_zh": "åˆ©ç”¨è‡ªç„¶è¯­è¨€å®ç°ç°å®ä¸–ç•Œä¸­çš„äººæœºåä½œ",
      "authors": [
        "Peter Lindes",
        "Kaoutar Skiker"
      ],
      "abstract": "We have a vision of a day when autonomous robots can collaborate with humans as assistants in performing complex tasks in the physical world. This vision includes that the robots will have the ability to communicate with their human collaborators using language that is natural to the humans. Traditional Interactive Task Learning (ITL) systems have some of this ability, but the language they can understand is very limited. The advent of large language models (LLMs) provides an opportunity to greatly improve the language understanding of robots, yet integrating the language abilities of LLMs with robots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that work closely with humans, and discuss how they could be much better collaborators with robust language abilities. We then explore how an AI system with a cognitive agent that controls a physical robot at its core, interacts with both a human and an LLM, and accumulates situational knowledge through its experiences, can be a possible approach to reach that vision. We focus on three specific challenges of having the robot understand natural language, and present a simple proof-of-concept experiment using ChatGPT for each. Finally, we discuss what it will take to turn these simple experiments into an operational system where LLM-assisted language understanding is a part of an integrated robotic assistant that uses language to collaborate with humans.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªä¸»æœºå™¨äººåœ¨ç°å®ç‰©ç†ä¸–ç•Œä¸­é€šè¿‡è‡ªç„¶è¯­è¨€ä¸äººç±»åä½œï¼Œå…±åŒå®Œæˆå¤æ‚ä»»åŠ¡çš„æ„¿æ™¯ã€‚é’ˆå¯¹ä¼ ç»Ÿçš„ Interactive Task Learning (ITL) ç³»ç»Ÿè¯­è¨€ç†è§£èƒ½åŠ›å—é™çš„ç°çŠ¶ï¼Œè®ºæ–‡åˆ†æäº†å°† Large Language Models (LLMs) çš„è¯­è¨€èƒ½åŠ›ä¸å®ä½“æœºå™¨äººé›†æˆæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ä»¥è®¤çŸ¥æ™ºèƒ½ä½“ä¸ºæ ¸å¿ƒçš„ AI ç³»ç»Ÿæ–¹æ¡ˆï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸äººç±»åŠ LLMs çš„äº¤äº’æ¥ç§¯ç´¯æƒ…å¢ƒçŸ¥è¯†ã€‚ç ”ç©¶é’ˆå¯¹æœºå™¨äººç†è§£è‡ªç„¶è¯­è¨€ä¸­çš„ä¸‰é¡¹æ ¸å¿ƒæŒ‘æˆ˜ï¼Œåˆ©ç”¨ ChatGPT è¿›è¡Œäº† Proof-of-Concept å®éªŒéªŒè¯ã€‚æœ€åï¼Œæ–‡ç« æ¢è®¨äº†å°† LLM è¾…åŠ©çš„è¯­è¨€ç†è§£åŠŸèƒ½æ•´åˆè¿›å®é™…æœºå™¨äººåŠ©æ‰‹ç³»ç»Ÿï¼Œä»è€Œå®ç°é«˜æ•ˆäººæœºåä½œçš„å…·ä½“è¦æ±‚ä¸è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.), Generative AI Risks and Benefits within Human-Machine Teams, Elsevier, Chapter 6",
      "pdf_url": "https://arxiv.org/pdf/2508.11759v2",
      "published_date": "2025-08-15 18:09:53 UTC",
      "updated_date": "2025-09-19 17:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:16.583575+00:00"
    },
    {
      "arxiv_id": "2508.11758v2",
      "title": "Can we Evaluate RAGs with Synthetic Data?",
      "title_zh": "æˆ‘ä»¬èƒ½å¦åˆ©ç”¨åˆæˆæ•°æ®è¯„ä¼° RAG ç³»ç»Ÿï¼Ÿ",
      "authors": [
        "Jonas van Elburg",
        "Peter van der Putten",
        "Maarten Marx"
      ],
      "abstract": "We investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when the latter is unavailable. We assess the reliability of synthetic benchmarks across two experiments: one varying retriever parameters while keeping the generator fixed, and another varying the generator with fixed retriever parameters. Across four datasets, of which two open-domain and two proprietary, we find that synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they do not consistently produce reliable RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆçš„åˆæˆé—®ç­” (QA) æ•°æ®åœ¨ç¼ºä¹äººå·¥æ ‡æ³¨åŸºå‡†æ—¶ï¼Œèƒ½å¦ä½œä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿè¯„ä¼°çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸¤ç»„å®éªŒè¯„ä¼°äº†åˆæˆåŸºå‡†çš„å¯é æ€§ï¼Œåˆ†åˆ«åœ¨å›ºå®šç”Ÿæˆå™¨ (Generator) çš„å‰æä¸‹æ”¹å˜æ£€ç´¢å™¨ (Retriever) å‚æ•°ï¼Œä»¥åŠåœ¨å›ºå®šæ£€ç´¢å™¨çš„å‰æä¸‹æ”¹å˜ç”Ÿæˆå™¨æ¶æ„ã€‚å®éªŒæ¶µç›–äº†ä¸¤ä¸ªå¼€æºé¢†åŸŸå’Œä¸¤ä¸ªç§æœ‰é¢†åŸŸçš„å…±å››ä¸ªæ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒåˆæˆåŸºå‡†åœ¨å¯¹ä¸åŒæ£€ç´¢å™¨é…ç½®çš„ RAG ç³»ç»Ÿè¿›è¡Œæ’åæ—¶è¡¨ç°å¯é ï¼Œå…¶è¯„ä¼°ç»“æœä¸äººå·¥æ ‡æ³¨åŸºå‡†é«˜åº¦ä¸€è‡´ã€‚ç„¶è€Œï¼Œåœ¨æ¯”è¾ƒä¸åŒç”Ÿæˆå™¨æ¶æ„æ—¶ï¼ŒåˆæˆåŸºå‡†å¹¶ä¸èƒ½äº§ç”Ÿä¸€è‡´å¯é çš„ RAG æ’åã€‚è¿™ç§å±€é™æ€§å¯èƒ½æºäºåˆæˆåŸºå‡†ä¸äººå·¥åŸºå‡†ä¹‹é—´çš„ä»»åŠ¡ä¸åŒ¹é… (Task Mismatch)ï¼Œä»¥åŠåˆæˆæ•°æ®å¯¹ç‰¹å®šç”Ÿæˆå™¨è¡¨ç°å‡ºçš„é£æ ¼åè§ (Stylistic Bias)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for the SynDAiTE workshop at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal",
      "pdf_url": "https://arxiv.org/pdf/2508.11758v2",
      "published_date": "2025-08-15 18:07:47 UTC",
      "updated_date": "2025-10-21 11:29:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:10.563098+00:00"
    },
    {
      "arxiv_id": "2508.11628v1",
      "title": "Is ChatGPT-5 Ready for Mammogram VQA?",
      "title_zh": "ChatGPT-5 æ˜¯å¦å·²å‡†å¤‡å¥½åº”å¯¹ä¹³è…º X çº¿æ‘„å½±è§†è§‰é—®ç­”ï¼Ÿ",
      "authors": [
        "Qiang Li",
        "Shansong Wang",
        "Mingzhe Hu",
        "Mojtaba Safari",
        "Zachary Eidex",
        "Xiaofeng Yang"
      ],
      "abstract": "Mammogram visual question answering (VQA) integrates image interpretation with clinical reasoning and has potential to support breast cancer screening. We systematically evaluated the GPT-5 family and GPT-4o model on four public mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment, abnormality detection, and malignancy classification tasks. GPT-5 consistently was the best performing model but lagged behind both human experts and domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%), calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0% malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and specificity (52.3%). While GPT-5 exhibits promising capabilities for screening tasks, its performance remains insufficient for high-stakes clinical imaging applications without targeted domain adaptation and optimization. However, the tremendous improvements in performance from GPT-4o to GPT-5 show a promising trend in the potential for general large language models (LLMs) to assist with mammography VQA tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº† GPT-5 ç³»åˆ—æ¨¡å‹å’Œ GPT-4o åœ¨ä¹³è…º X å°„çº¿æ‘„å½±è§†è§‰é—®ç­” (Mammogram VQA) ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæ¶µç›–äº† BI-RADS è¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹å’Œæ¶æ€§è‚¿ç˜¤åˆ†ç±»ã€‚å®éªŒåœ¨ EMBEDã€InBreastã€CMMD å’Œ CBIS-DDSM å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå±•å¼€ï¼Œç»“æœè¡¨æ˜ GPT-5 æ˜¯ç›®å‰è¡¨ç°æœ€å‡ºè‰²çš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äº GPT-4oã€‚å°½ç®¡å¦‚æ­¤ï¼ŒGPT-5 çš„æ€§èƒ½ä»æ˜¾è‘—è½åäºäººç±»ä¸“å®¶å’Œé¢†åŸŸä¸“ç”¨çš„å¾®è°ƒæ¨¡å‹ï¼Œå…¶åœ¨æ•æ„Ÿåº¦å’Œç‰¹å¼‚æ€§æ–¹é¢çš„è¡¨ç°å°šä¸è¶³ä»¥æ”¯æŒé«˜é£é™©çš„ä¸´åºŠå½±åƒåº”ç”¨ã€‚ç ”ç©¶å¼ºè°ƒï¼Œè™½ç„¶ä» GPT-4o åˆ° GPT-5 çš„è¿›æ­¥å±•ç¤ºäº†é€šç”¨æ¨¡å‹è¾…åŠ©ä¹³è…ºç™Œç­›æŸ¥çš„å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ç¼ºä¹é’ˆå¯¹æ€§é¢†åŸŸé€‚åº” (domain adaptation) å’Œä¼˜åŒ–çš„å‰æä¸‹å…¶å¯é æ€§ä»å¾…æé«˜ã€‚è¯¥è¯„ä¼°ä¸ºç†è§£å‰æ²¿æ¨¡å‹åœ¨åŒ»ç–—è¾…åŠ©è¯Šæ–­ä¸­çš„ç°çŠ¶ä¸å±€é™æä¾›äº†é‡è¦å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11628v1",
      "published_date": "2025-08-15 17:56:24 UTC",
      "updated_date": "2025-08-15 17:56:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:21.184978+00:00"
    },
    {
      "arxiv_id": "2508.11616v1",
      "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
      "title_zh": "åŸºäºå¥–åŠ±å¼•å¯¼è§£ç çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ§åˆ¶",
      "authors": [
        "Oscar MaÃ±as",
        "Pierluca D'Oro",
        "Koustuv Sinha",
        "Adriana Romero-Soriano",
        "Michal Drozdzal",
        "Aishwarya Agrawal"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)çš„å¥–åŠ±å¼•å¯¼è§£ç (reward-guided decoding)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å—æ§è§£ç (controlled decoding)æŠ€æœ¯æå‡æ¨¡å‹çš„è§†è§‰å®šä½(visual grounding)èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸¤ä¸ªç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ï¼Œåˆ†åˆ«åœ¨è§£ç è¿‡ç¨‹ä¸­ç²¾ç¡®å¼•å¯¼æ¨¡å‹è¾“å‡ºä¸­å¯¹è±¡çš„ç²¾ç¡®åº¦(precision)å’Œå¬å›ç‡(recall)ã€‚è¯¥æ–¹æ³•å®ç°äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å³æ—¶å¯æ§æ€§ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éœ€æ±‚åŠ¨æ€æƒè¡¡å›¾åƒæè¿°ä»»åŠ¡ä¸­ç²¾ç¡®åº¦ä¸å¬å›ç‡çš„å…³ç³»ï¼Œå¹¶èƒ½çµæ´»è°ƒèŠ‚æ¨ç†è®¡ç®—é‡ä¸è§†è§‰å®šä½æ•ˆæœä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨æ ‡å‡†å¯¹è±¡å¹»è§‰(object hallucination)åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†MLLMæ¨ç†çš„å¯æ§æ€§ï¼Œä¸”åœ¨ç¼“è§£å¹»è§‰é—®é¢˜ä¸Šçš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºå®šåˆ¶åŒ–å¤šæ¨¡æ€åº”ç”¨æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11616v1",
      "published_date": "2025-08-15 17:29:06 UTC",
      "updated_date": "2025-08-15 17:29:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:28.671906+00:00"
    },
    {
      "arxiv_id": "2508.11609v2",
      "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval",
      "title_zh": "é¢å‘éŸ³é¢‘æŒ‡çº¹è¯†åˆ«ä¸æ£€ç´¢çš„é¢„è®­ç»ƒ Conformer",
      "authors": [
        "Kemal Altwlkany",
        "Elmedin Selmanovic",
        "Sead Delalic"
      ],
      "abstract": "Conformers have shown great results in speech processing due to their ability to capture both local and global interactions. In this work, we utilize a self-supervised contrastive learning framework to train conformer-based encoders that are capable of generating unique embeddings for small segments of audio, generalizing well to previously unseen data. We achieve state-of-the-art results for audio retrieval tasks while using only 3 seconds of audio to generate embeddings. Our models are almost completely immune to temporal misalignments and achieve state-of-the-art results in cases of other audio distortions such as noise, reverb or extreme temporal stretching. Code and models are made publicly available and the results are easy to reproduce as we train and test using popular and freely available datasets of different sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ (Self-supervised contrastive learning)æ¡†æ¶è®­ç»ƒäº†åŸºäº Conformer çš„ç¼–ç å™¨ï¼Œæ—¨åœ¨ä¸ºéŸ³é¢‘æŒ‡çº¹è¯†åˆ«å’Œæ£€ç´¢ç”Ÿæˆç‹¬ç‰¹çš„åµŒå…¥å‘é‡(Embeddings)ã€‚ç”±äº Conformer èƒ½å¤Ÿæœ‰æ•ˆæ•è·éŸ³é¢‘åºåˆ—ä¸­çš„å±€éƒ¨å’Œå…¨å±€äº¤äº’ï¼Œè¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨ 3 ç§’éŸ³é¢‘è¾“å…¥çš„æƒ…å†µä¸‹å°±è¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ£€ç´¢æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯¹æ—¶é—´åç§»ã€å™ªå£°ã€æ··å“ä»¥åŠæç«¯çš„æ—¶é—´ä¼¸ç¼©ç­‰éŸ³é¢‘å¤±çœŸè¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œå‡ ä¹å®Œå…¨å…ç–«äºæ—¶é—´é”™ä½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æœªè§è¿‡çš„éŸ³é¢‘æ•°æ®ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®ä¿äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚ç›®å‰è¯¥ç ”ç©¶çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€ï¼Œä¸”åŸºäºé€šç”¨æ•°æ®é›†çš„æµ‹è¯•ç»“æœå…·æœ‰é«˜åº¦çš„å¯å¤ç°æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11609v2",
      "published_date": "2025-08-15 17:19:09 UTC",
      "updated_date": "2025-09-11 11:52:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:22.361685+00:00"
    },
    {
      "arxiv_id": "2508.11738v1",
      "title": "Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨å†œæ‘åŒ»ç–—æœåŠ¡ä¸­çš„åº”ç”¨ï¼šé€šè¿‡åˆ›æ–°å¼¥åˆå·®è·å¹¶æå‡å…¬å¹³æ€§",
      "authors": [
        "Kiruthika Balakrishnan",
        "Durgadevi Velusamy",
        "Hana E. Hinkle",
        "Zhi Li",
        "Karthikeyan Ramasamy",
        "Hikmat Khan",
        "Srini Ramaswamy",
        "Pir Masoom Shah"
      ],
      "abstract": "Rural healthcare faces persistent challenges, including inadequate infrastructure, workforce shortages, and socioeconomic disparities that hinder access to essential services. This study investigates the transformative potential of artificial intelligence (AI) in addressing these issues in underserved rural areas. We systematically reviewed 109 studies published between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and Scopus. Articles were screened using PRISMA guidelines and Covidence software. A thematic analysis was conducted to identify key patterns and insights regarding AI implementation in rural healthcare delivery. The findings reveal significant promise for AI applications, such as predictive analytics, telemedicine platforms, and automated diagnostic tools, in improving healthcare accessibility, quality, and efficiency. Among these, advanced AI systems, including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs), offer particularly transformative potential. MFMs integrate diverse data sources, such as imaging, clinical records, and bio signals, to support comprehensive decision-making, while LLMs facilitate clinical documentation, patient triage, translation, and virtual assistance. Together, these technologies can revolutionize rural healthcare by augmenting human capacity, reducing diagnostic delays, and democratizing access to expertise. However, barriers remain, including infrastructural limitations, data quality concerns, and ethical considerations. Addressing these challenges requires interdisciplinary collaboration, investment in digital infrastructure, and the development of regulatory frameworks. This review offers actionable recommendations and highlights areas for future research to ensure equitable and sustainable integration of AI in rural healthcare systems.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ€§å›é¡¾2019å¹´è‡³2024å¹´é—´çš„109é¡¹ç ”ç©¶ï¼Œæ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)åœ¨ç¼©å°å†œæ‘åŒ»ç–—å·®è·å’Œå¢å¼ºåŒ»ç–—å…¬å¹³æ€§æ–¹é¢çš„å˜é©æ½œåŠ›ã€‚ç ”ç©¶éµå¾ªPRISMAæŒ‡å—å¹¶é‡‡ç”¨ä¸»é¢˜åˆ†ææ³•ï¼Œè¯†åˆ«å‡ºAIåœ¨æ”¹å–„æ¬ å‘è¾¾åœ°åŒºåŒ»ç–—æœåŠ¡å¯åŠæ€§ã€è´¨é‡å’Œæ•ˆç‡æ–¹é¢çš„å…³é”®æ¨¡å¼ã€‚ç ”ç©¶å¼ºè°ƒäº†Multimodal Foundation Models (MFMs)å’ŒLarge Language Models (LLMs)åœ¨æ•´åˆå¤šå…ƒæ•°æ®æ”¯æŒå†³ç­–ã€ä¼˜åŒ–ä¸´åºŠæ–‡æ¡£åŠæ‚£è€…åˆ†è¯Šä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚è¿™äº›æŠ€æœ¯é€šè¿‡å¢å¼ºäººåŠ›èµ„æœ¬å’Œå‡å°‘è¯Šæ–­å»¶è¿Ÿï¼Œæœ‰æœ›å®ç°åŒ»ç–—ä¸“å®¶èµ„æºçš„æ°‘ä¸»åŒ–æ™®åŠã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶æŒ‡å‡ºåŸºç¡€è®¾æ–½è–„å¼±ã€æ•°æ®è´¨é‡éšå¿§åŠä¼¦ç†æŒ‘æˆ˜ä»æ˜¯å®æ–½ä¸­çš„ä¸»è¦éšœç¢ã€‚æœ€åï¼Œè®ºæ–‡ä¸ºå®ç°å†œæ‘åŒ»ç–—ç³»ç»Ÿä¸­AIçš„å…¬å¹³ä¸”å¯æŒç»­æ•´åˆæå‡ºäº†æ”¿ç­–å»ºè®®ä¸è·¨å­¦ç§‘åä½œæ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11738v1",
      "published_date": "2025-08-15 17:08:10 UTC",
      "updated_date": "2025-08-15 17:08:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:27.463535+00:00"
    },
    {
      "arxiv_id": "2508.11599v1",
      "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection",
      "title_zh": "CryptoScopeï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–åŠ å¯†é€»è¾‘æ¼æ´æ£€æµ‹",
      "authors": [
        "Zhihao Li",
        "Zimo Ji",
        "Tao Zheng",
        "Hao Ren",
        "Xiao Lan"
      ],
      "abstract": "Cryptographic algorithms are fundamental to modern security, yet their implementations frequently harbor subtle logic flaws that are hard to detect. We introduce CryptoScope, a novel framework for automated cryptographic vulnerability detection powered by Large Language Models (LLMs). CryptoScope combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation (RAG), guided by a curated cryptographic knowledge base containing over 12,000 entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily derived from real-world CVE vulnerabilities, complemented by cryptographic challenges from major Capture The Flag (CTF) competitions and synthetic examples across 11 programming languages. CryptoScope consistently improves performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%, GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9 previously undisclosed flaws in widely used open-source cryptographic projects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CryptoScopeï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨åŒ–æ£€æµ‹å¯†ç ç®—æ³•å®ç°ä¸­ç»†å¾®é€»è¾‘æ¼æ´çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)ä¸é“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºåŠæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation)æŠ€æœ¯ç›¸ç»“åˆï¼Œå¹¶ç”±ä¸€ä¸ªåŒ…å«1.2ä¸‡ä½™æ¡ç›®çš„ä¸“ä¸šå¯†ç å­¦çŸ¥è¯†åº“æä¾›æ”¯æŒã€‚é€šè¿‡åœ¨æ¶µç›–çœŸå®ä¸–ç•ŒCVEã€CTFæŒ‘æˆ˜åŠ11ç§ç¼–ç¨‹è¯­è¨€çš„LLM-CLVAåŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒCryptoScopeæ˜¾è‘—æå‡äº†DeepSeek-V3ã€GPT-4o-miniå’ŒGLM-4-Flashç­‰ä¸»æµæ¨¡å‹çš„æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„å‡†ç¡®ç‡æå‡å¹…åº¦åœ¨11.62%è‡³28.69%ä¹‹é—´ã€‚æ­¤å¤–ï¼ŒCryptoScopeè¿˜åœ¨å¤šä¸ªä¸»æµå¼€æºå¯†ç å­¦é¡¹ç›®ä¸­æˆåŠŸå‘ç°äº†9ä¸ªæ­¤å‰æœªçŸ¥çš„é€»è¾‘æ¼æ´ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚ç°å®å®‰å…¨é—®é¢˜æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11599v1",
      "published_date": "2025-08-15 17:07:54 UTC",
      "updated_date": "2025-08-15 17:07:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:36.263948+00:00"
    },
    {
      "arxiv_id": "2508.11737v1",
      "title": "Ovis2.5 Technical Report",
      "title_zh": "Ovis2.5 æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Shiyin Lu",
        "Yang Li",
        "Yu Xia",
        "Yuwei Hu",
        "Shanshan Zhao",
        "Yanqing Ma",
        "Zhichao Wei",
        "Yinglun Li",
        "Lunhao Duan",
        "Jianshan Zhao",
        "Yuxuan Han",
        "Haijun Li",
        "Wanying Chen",
        "Junke Tang",
        "Chengkun Hou",
        "Zhixing Du",
        "Tianli Zhou",
        "Wenjie Zhang",
        "Huping Ding",
        "Jiahe Li",
        "Wen Li",
        "Gui Hu",
        "Yiliang Gu",
        "Siran Yang",
        "Jiamang Wang",
        "Hailong Sun",
        "Yibo Wang",
        "Hui Sun",
        "Jinlong Huang",
        "Yuping He",
        "Shengze Shi",
        "Weihong Zhang",
        "Guodong Zheng",
        "Junpeng Jiang",
        "Sensen Gao",
        "Yi-Feng Wu",
        "Sijia Chen",
        "Yuhui Chen",
        "Qing-Guo Chen",
        "Zhao Xu",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional \"thinking mode\" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the \"small model, big performance\" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Ovis2.5ï¼Œä½œä¸ºOvis2çš„ç»§ä»»è€…ï¼Œä¸“æ³¨äºåŸç”Ÿåˆ†è¾¨ç‡çš„è§†è§‰æ„ŸçŸ¥å’Œå¼ºå¤§å¤šæ¨¡æ€æ¨ç†ã€‚è¯¥æ¨¡å‹é›†æˆäº†åŸç”Ÿåˆ†è¾¨ç‡Vision Transformerï¼Œèƒ½å¤Ÿä»¥å¯å˜åˆ†è¾¨ç‡å¤„ç†å›¾åƒï¼Œæœ‰æ•ˆé¿å…äº†å›ºå®šåˆ†è¾¨ç‡åˆ†å—å¯¼è‡´çš„ç»†èŠ‚ä¸¢å¤±ï¼Œç‰¹åˆ«æå‡äº†å¯¹å¤æ‚å›¾è¡¨ç­‰è§†è§‰å¯†é›†å†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚åœ¨æ¨ç†å¢å¼ºæ–¹é¢ï¼Œæ¨¡å‹è¶…è¶Šäº†çº¿æ€§çš„Chain-of-Thoughtï¼Œå¼•å…¥äº†åŒ…å«è‡ªæˆ‘æ£€æŸ¥å’Œä¿®è®¢çš„Reflectionæœºåˆ¶ï¼Œå¹¶åœ¨æ¨ç†æ—¶æä¾›å¯é€‰çš„Thinking Modeä»¥ä¼˜åŒ–å¤æ‚ä»»åŠ¡çš„å‡†ç¡®ç‡ã€‚æ¨¡å‹é‡‡ç”¨äº†äº”é˜¶æ®µè¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹ï¼Œç»“åˆDPOå’ŒGRPOç®—æ³•æå‡å¯¹é½ä¸æ¨ç†æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨æ··åˆå¹¶è¡ŒæŠ€æœ¯å®ç°äº†é«˜æ•ˆæ‰©å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒOvis2.5-9Båœ¨OpenCompasså¤šæ¨¡æ€æ¦œå•ä¸­å–å¾—äº†40Bå‚æ•°ä»¥ä¸‹å¼€æºæ¨¡å‹çš„State-of-the-artæ€§èƒ½ï¼Œè€ŒOvis2.5-2Båˆ™åœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­é¢†å…ˆã€‚æ­¤å¤–ï¼Œè¯¥ç³»åˆ—æ¨¡å‹åœ¨STEMåŸºå‡†æµ‹è¯•ã€è§†è§‰å®šä½ã€è§†é¢‘ä»»åŠ¡åŠå¤æ‚å›¾è¡¨åˆ†æä¸­å‡å±•ç°å‡ºå“è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11737v1",
      "published_date": "2025-08-15 17:01:08 UTC",
      "updated_date": "2025-08-15 17:01:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:17.093590+00:00"
    },
    {
      "arxiv_id": "2508.11584v2",
      "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks",
      "title_zh": "Visual Perception Engineï¼šé¢å‘æœºå™¨äººè§†è§‰ä»»åŠ¡çš„é«˜æ•ˆçµæ´»å¤šå¤´æ¨ç†",
      "authors": [
        "Jakub Åucki",
        "Jonathan Becktor",
        "Georgios Georgakis",
        "Rob Royce",
        "Shehryar Khattak"
      ],
      "abstract": "Deploying multiple machine learning models on resource-constrained robotic platforms for different perception tasks often results in redundant computations, large memory footprints, and complex integration challenges. In response, this work presents Visual Perception Engine (VPEngine), a modular framework designed to enable efficient GPU usage for visual multitasking while maintaining extensibility and developer accessibility. Our framework architecture leverages a shared foundation model backbone that extracts image representations, which are efficiently shared, without any unnecessary GPU-CPU memory transfers, across multiple specialized task-specific model heads running in parallel. This design eliminates the computational redundancy inherent in feature extraction component when deploying traditional sequential models while enabling dynamic task prioritization based on application demands. We demonstrate our framework's capabilities through an example implementation using DINOv2 as the foundation model with multiple task (depth, object detection and semantic segmentation) heads, achieving up to 3x speedup compared to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine offers efficient GPU utilization and maintains a constant memory footprint while allowing per-task inference frequencies to be adjusted dynamically during runtime. The framework is written in Python and is open source with ROS2 C++ (Humble) bindings for ease of use by the robotics community across diverse robotic platforms. Our example implementation demonstrates end-to-end real-time performance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized models.",
      "tldr_zh": "æœ¬ç ”ç©¶å¼€å‘äº† Visual Perception Engine (VPEngine)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæœºå™¨äººè§†è§‰å¤šä»»åŠ¡å¤„ç†è®¾è®¡çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åœ¨èµ„æºå—é™å¹³å°ä¸Šéƒ¨ç½²å¤šä¸ªç‹¬ç«‹æ¨¡å‹æ—¶å¯¼è‡´çš„è®¡ç®—å†—ä½™å’Œé›†æˆæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ¶æ„åˆ©ç”¨å…±äº«çš„åŸºç¡€æ¨¡å‹ (foundation model) ä¸»å¹²æå–å›¾åƒè¡¨å¾ï¼Œå¹¶åœ¨å¹¶è¡Œçš„ç‰¹å®šä»»åŠ¡æ¨¡å‹å¤´ (task-specific model heads) ä¹‹é—´é«˜æ•ˆå…±äº«ï¼Œä»è€Œæ¶ˆé™¤äº†é‡å¤çš„ç‰¹å¾æå–è¿‡ç¨‹å¹¶é¿å…äº† GPU-CPU é—´çš„å†…å­˜ä¼ è¾“ã€‚å€ŸåŠ© CUDA Multi-Process Service (MPS)ï¼ŒVPEngine èƒ½å¤Ÿä¿æŒæ’å®šçš„å†…å­˜å ç”¨ï¼Œå¹¶æ”¯æŒåœ¨è¿è¡Œæ—¶æ ¹æ®éœ€æ±‚åŠ¨æ€è°ƒæ•´å„ä»»åŠ¡çš„æ¨ç†é¢‘ç‡ã€‚ä»¥ DINOv2 ä¸ºåŸºç¡€æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ¯”é¡ºåºæ‰§è¡Œæ–¹å¼é€Ÿåº¦æå‡äº† 3 å€ï¼Œåœ¨ NVIDIA Jetson Orin AGX å¹³å°ä¸Šé€šè¿‡ TensorRT ä¼˜åŒ–å¯è¾¾åˆ° 50 Hz ä»¥ä¸Šçš„ç«¯åˆ°ç«¯å®æ—¶å¤„ç†æ€§èƒ½ã€‚è¯¥æ¡†æ¶å·²å¼€æºå¹¶æä¾› ROS2 C++ ç»‘å®šï¼Œä¸ºæœºå™¨äººç¤¾åŒºåœ¨å¤šæ ·åŒ–ç¡¬ä»¶å¹³å°ä¸Šå®ç°é«˜æ•ˆã€çµæ´»çš„è§†è§‰æ„ŸçŸ¥æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.11584v2",
      "published_date": "2025-08-15 16:42:23 UTC",
      "updated_date": "2025-08-18 05:11:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:56.088045+00:00"
    },
    {
      "arxiv_id": "2508.11582v1",
      "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models",
      "title_zh": "è§‰å¯Ÿå…ˆè¡Œï¼Œç²¾ç®€æ€ç»´ï¼šåŠ¨æ€è¾¹ç•Œè‡ªæˆ‘è§‰å¯Ÿé©±åŠ¨å¤§è¯­è¨€æ¨¡å‹æè‡´æ¨ç†æ•ˆç‡",
      "authors": [
        "Qiguang Chen",
        "Dengyun Peng",
        "Jinhao Liu",
        "HuiKang Su",
        "Jiannan Guan",
        "Libo Qin",
        "Wanxiang Che"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have greatly improved their capabilities on complex reasoning tasks through Long Chain-of-Thought (CoT). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. To improve the efficiency, current methods often rely on human-defined difficulty priors, which do not align with the LLM's self-awared difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to dynamically assess and adjust their reasoning depth in response to problem complexity. DR. SAF integrates three key components: Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. These components allow models to optimize their reasoning processes, balancing efficiency and accuracy without compromising performance. Our experimental results demonstrate that DR. SAF achieves a 49.27% reduction in total response tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain in token efficiency and a 5x reduction in training time, making it well-suited to resource-limited settings. During extreme training, DR. SAF can even surpass traditional instruction-based models in token efficiency with more than 16% accuracy improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŠ¨æ€æ¨ç†è¾¹ç•Œè‡ªæˆ‘æ„ŸçŸ¥æ¡†æ¶ (Dynamic Reasoning-Boundary Self-Awareness Framework, DR. SAF)ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ‰§è¡Œé•¿é“¾å¼æ€ç»´ (Long Chain-of-Thought, CoT) æ¨ç†æ—¶å­˜åœ¨çš„å†—ä½™å’Œè®¡ç®—å»¶è¿Ÿé—®é¢˜ã€‚DR. SAF æ ¸å¿ƒç”±è¾¹ç•Œè‡ªæˆ‘æ„ŸçŸ¥å¯¹é½ (Boundary Self-Awareness Alignment)ã€è‡ªé€‚åº”å¥–åŠ±ç®¡ç† (Adaptive Reward Management) å’Œè¾¹ç•Œä¿æŒæœºåˆ¶ (Boundary Preservation Mechanism) ç»„æˆï¼Œå…è®¸æ¨¡å‹æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚è¯¥æ¡†æ¶é€šè¿‡ä½¿æ¨¡å‹æ„ŸçŸ¥è‡ªèº«æ¨ç†è¾¹ç•Œï¼Œå®ç°äº†æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§çš„å¹³è¡¡ï¼Œé¿å…äº†äººä¸ºé¢„è®¾éš¾åº¦å¸¦æ¥çš„åå·®ã€‚å®éªŒè¯æ˜ï¼ŒDR. SAF åœ¨ä¿è¯å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå‡å°‘äº† 49.27% çš„æ€»å“åº” tokensï¼Œå¹¶å¸¦æ¥äº† 6.59 å€çš„æ•ˆç‡å¢ç›Šä¸ 5 å€çš„è®­ç»ƒæ—¶é—´ç¼©å‡ã€‚åœ¨æç«¯è®­ç»ƒæ¡ä»¶ä¸‹ï¼Œå…¶è¡¨ç°ç”šè‡³ä¼˜äºä¼ ç»Ÿçš„æŒ‡ä»¤æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡è¶…è¿‡ 16%ï¼Œå±•ç°äº†å…¶åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„å“è¶Šæ€§èƒ½ä¸åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.11582v1",
      "published_date": "2025-08-15 16:40:29 UTC",
      "updated_date": "2025-08-15 16:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:01:58.191023+00:00"
    },
    {
      "arxiv_id": "2508.11551v2",
      "title": "ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization",
      "title_zh": "ADMIRE-BayesOptï¼šåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹åŠ é€Ÿæ•°æ®é…æ¯”é‡åŠ æƒ",
      "authors": [
        "Shengzhuang Chen",
        "Xu Ouyang",
        "Michael Arthur Leopold Pearce",
        "Thomas Hartvigsen",
        "Jonathan Richard Schwarz"
      ],
      "abstract": "Determining the optimal data mixture for large language model training remains a challenging problem with an outsized impact on performance. In practice, language model developers continue to rely on heuristic exploration since no learning-based approach has emerged as a reliable solution. In this work, we propose to view the selection of training data mixtures as a black-box hyperparameter optimization problem, for which Bayesian Optimization is a well-established class of appropriate algorithms. Firstly, we cast data mixture learning as a sequential decision-making problem, in which we aim to find a suitable trade-off between the computational cost of training exploratory (proxy-) models and final mixture performance. Secondly, we systematically explore the properties of transferring mixtures learned at a small scale to larger-scale experiments, providing insights and highlighting opportunities for research at a modest scale. By proposing Multi-fidelity Bayesian Optimization as a suitable method in this common scenario, we introduce a natural framework to balance experiment cost with model fit, avoiding the risks of overfitting to smaller scales while minimizing the number of experiments at high cost. We present results for pre-training and instruction finetuning across models ranging from 1 million to 7 billion parameters, varying from simple architectures to state-of-the-art models and benchmarks spanning dozens of datasets. We demonstrate consistently strong results relative to a wide range of baselines, resulting inspeed-ups of over 500% in determining the best data mixture on our largest experiments. In addition, we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full training & evaluation runs worth over 13,000 GPU hours, greatly reducing the cost of conducting research in this area.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­æ•°æ®æ··åˆ(Data Mixture)æ¯”ä¾‹ç¡®å®šçš„éš¾é¢˜ï¼Œæå‡ºäº†ADMIRE-BayesOptæ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†æ•°æ®æ··åˆæ¯”ä¾‹é€‰æ‹©è§†ä¸ºé»‘ç›’è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨Bayesian Optimizationç®—æ³•é€šè¿‡é¡ºåºå†³ç­–æ¥å¯»æ‰¾è®¡ç®—æˆæœ¬ä¸æ¨¡å‹æ€§èƒ½çš„æœ€ä¼˜æƒè¡¡ã€‚ç ”ç©¶é‡ç‚¹æ¢ç´¢äº†ä»å°è§„æ¨¡å®éªŒå‘å¤§è§„æ¨¡å®éªŒè¿ç§»æ··åˆç­–ç•¥çš„å±æ€§ï¼Œå¹¶å¼•å…¥Multi-fidelity Bayesian Optimizationæ¥å¹³è¡¡å®éªŒæˆæœ¬ä¸æ¨¡å‹æ‹Ÿåˆåº¦ï¼Œæœ‰æ•ˆé¿å…äº†å¯¹å°è§„æ¨¡å®éªŒçš„è¿‡æ‹Ÿåˆã€‚è¯¥æ¡†æ¶åœ¨100ä¸‡è‡³70äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒ(Instruction Finetuning)éªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨ç¡®å®šæœ€ä½³æ•°æ®æ··åˆæ–¹æ¡ˆæ—¶ç›¸æ¯”åŸºçº¿æ¨¡å‹å®ç°äº†è¶…è¿‡500%çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†åŒ…å«460æ¬¡å®Œæ•´è®­ç»ƒä¸è¯„ä¼°è®°å½•çš„ADMIRE IFT Runsæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡å…±äº«è¶…è¿‡13,000ä¸ªGPUå°æ—¶çš„å®éªŒæ•°æ®æ¥é™ä½è¯¥é¢†åŸŸçš„ç ”ç©¶é—¨æ§›ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11551v2",
      "published_date": "2025-08-15 15:53:09 UTC",
      "updated_date": "2025-08-18 06:38:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:01.502670+00:00"
    },
    {
      "arxiv_id": "2508.11529v1",
      "title": "A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow",
      "title_zh": "æœºå™¨å­¦ä¹ å…¨æµç¨‹è§†è§’ä¸‹çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "George Paterakis",
        "Andrea Castellani",
        "George Papoutsoglou",
        "Tobias Rodemann",
        "Ioannis Tsamardinos"
      ],
      "abstract": "Artificial intelligence is reshaping science and industry, yet many users still regard its models as opaque \"black boxes\". Conventional explainable artificial-intelligence methods clarify individual predictions but overlook the upstream decisions and downstream quality checks that determine whether insights can be trusted. In this work, we present Holistic Explainable Artificial Intelligence (HXAI), a user-centric framework that embeds explanation into every stage of the data-analysis workflow and tailors those explanations to users. HXAI unifies six components (data, analysis set-up, learning process, model output, model quality, communication channel) into a single taxonomy and aligns each component with the needs of domain experts, data analysts and data scientists. A 112-item question bank covers these needs; our survey of contemporary tools highlights critical coverage gaps. Grounded in theories of human explanation, principles from human-computer interaction and findings from empirical user studies, HXAI identifies the characteristics that make explanations clear, actionable and cognitively manageable. A comprehensive taxonomy operationalises these insights, reducing terminological ambiguity and enabling rigorous coverage analysis of existing toolchains. We further demonstrate how AI agents that embed large-language models can orchestrate diverse explanation techniques, translating technical artifacts into stakeholder-specific narratives that bridge the gap between AI developers and domain experts. Departing from traditional surveys or perspective articles, this work melds concepts from multiple disciplines, lessons from real-world projects and a critical synthesis of the literature to advance a novel, end-to-end viewpoint on transparency, trustworthiness and responsible AI deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Holistic Explainable Artificial Intelligence (HXAI)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ–¹æ³•è¿‡äºå…³æ³¨å•æ¬¡é¢„æµ‹è€Œå¿½è§†æ•´ä¸ªæœºå™¨å­¦ä¹ å·¥ä½œæµçš„é—®é¢˜ã€‚HXAIæ˜¯ä¸€ä¸ªä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œå°†è§£é‡ŠåµŒå…¥åˆ°æ•°æ®åˆ†æå·¥ä½œæµçš„æ¯ä¸€ä¸ªé˜¶æ®µï¼Œå¹¶å°†å…¶ç»Ÿä¸€ä¸ºæ¶µç›–dataã€analysis set-upã€learning processã€model outputã€model qualityå’Œcommunication channelå…­ä¸ªç»´åº¦çš„åˆ†ç±»æ³•ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºæ­¤å¼€å‘äº†ä¸€ä¸ªåŒ…å«112ä¸ªæ¡ç›®çš„é—®é¢˜åº“ï¼Œå¹¶é€šè¿‡å¯¹ç°æœ‰å·¥å…·çš„è°ƒæŸ¥æ­ç¤ºäº†å…³é”®çš„è¦†ç›–ç¼ºå£ã€‚åŸºäºäººç±»è§£é‡Šç†è®ºå’Œäººæœºäº¤äº’(HCI)åŸåˆ™ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æå‡è§£é‡Šçš„æ¸…æ™°åº¦ã€å¯æ“ä½œæ€§å’Œè®¤çŸ¥å¯ç®¡ç†æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å±•ç¤ºäº†åµŒå…¥å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„AI agentså¦‚ä½•ç¼–æ’å¤šç§è§£é‡ŠæŠ€æœ¯ï¼Œå°†æŠ€æœ¯æˆæœè½¬åŒ–ä¸ºç‰¹å®šåˆ©ç›Šç›¸å…³è€…çš„å™è¿°ã€‚è¿™é¡¹å·¥ä½œæ•´åˆäº†å¤šå­¦ç§‘æ¦‚å¿µä¸å®é™…é¡¹ç›®ç»éªŒï¼Œä¸ºå®ç°é€æ˜ã€å¯ä¿¡å’Œè´Ÿè´£ä»»çš„AIéƒ¨ç½²æä¾›äº†ç«¯åˆ°ç«¯çš„å…¨æ–°è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Currently under review at \"Artificial Intelligence Review\" journal",
      "pdf_url": "https://arxiv.org/pdf/2508.11529v1",
      "published_date": "2025-08-15 15:15:25 UTC",
      "updated_date": "2025-08-15 15:15:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:16.291551+00:00"
    },
    {
      "arxiv_id": "2508.11524v1",
      "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models",
      "title_zh": "å¯å‘è¿˜æ˜¯é¢„æµ‹ï¼Ÿå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©ç»å…¸è§„åˆ’å™¨çš„æ–°èŒƒå¼æ¢ç´¢",
      "authors": [
        "Wenkai Yu",
        "Jianhang Tang",
        "Yang Zhang",
        "Shanjiang Tang",
        "Kebing Jin",
        "Hankz Hankui Zhuo"
      ],
      "abstract": "Addressing large-scale planning problems has become one of the central challenges in the planning community, deriving from the state-space explosion caused by growing objects and actions. Recently, researchers have explored the effectiveness of leveraging Large Language Models (LLMs) to generate helpful actions and states to prune the search space. However, prior works have largely overlooked integrating LLMs with domain-specific knowledge to ensure valid plans. In this paper, we propose a novel LLM-assisted planner integrated with problem decomposition, which first decomposes large planning problems into multiple simpler sub-tasks. Then we explore two novel paradigms to utilize LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where LLM4Inspire provides heuristic guidance according to general knowledge and LLM4Predict employs domain-specific knowledge to infer intermediate conditions. We empirically validate the effectiveness of our planner across multiple domains, demonstrating the ability of search space partition when solving large-scale planning problems. The experimental results show that LLMs effectively locate feasible solutions when pruning the search space, where infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds particular promise compared with LLM4Inspire, which offers general knowledge within LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾…åŠ©ç»å…¸è§„åˆ’å™¨è§£å†³å¤§è§„æ¨¡è§„åˆ’é—®é¢˜çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨åº”å¯¹å› ç‰©ä½“å’ŒåŠ¨ä½œå¢åŠ å¯¼è‡´çš„æœç´¢ç©ºé—´çˆ†ç‚¸æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§é›†æˆé—®é¢˜åˆ†è§£ï¼ˆproblem decompositionï¼‰çš„ LLM è¾…åŠ©è§„åˆ’æ¡†æ¶ï¼Œå°†å¤§è§„æ¨¡è§„åˆ’ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ›´ç®€å•çš„å­ä»»åŠ¡ã€‚ç ”ç©¶é‡ç‚¹å¯¹æ¯”äº†ä¸¤ç§ LLM åº”ç”¨æ¨¡å¼ï¼šLLM4Inspire åˆ©ç”¨é€šç”¨çŸ¥è¯†æä¾›å¯å‘å¼æŒ‡å¯¼ï¼Œè€Œ LLM4Predict åˆ™é€šè¿‡å¼•å…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ˆdomain-specific knowledgeï¼‰æ¥æ¨ç†å…³é”®çš„ä¸­é—´çŠ¶æ€ã€‚å¤šé¢†åŸŸçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥è§„åˆ’å™¨èƒ½æœ‰æ•ˆæ‰§è¡Œæœç´¢ç©ºé—´åˆ’åˆ†ï¼ˆsearch space partitionï¼‰å¹¶ç²¾å‡†å®šä½å¯è¡Œè§£ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç›¸æ¯”äºä»…æä¾›é€šç”¨çŸ¥è¯†çš„ LLM4Inspireï¼Œèå…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„ LLM4Predict åœ¨ä¼˜åŒ–å¤§è§„æ¨¡è§„åˆ’ä»»åŠ¡æ–¹é¢å±•ç°å‡ºæ›´æ˜¾è‘—çš„æ€§èƒ½æ½œåŠ›å’Œåº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11524v1",
      "published_date": "2025-08-15 15:08:07 UTC",
      "updated_date": "2025-08-15 15:08:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:10.061321+00:00"
    },
    {
      "arxiv_id": "2508.11515v1",
      "title": "Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations",
      "title_zh": "é’ˆå¯¹åŒ…å«åŒå…³ç³»å…¬ç†çš„äºŒå˜é‡é€»è¾‘çš„åŠ æƒä¸€é˜¶æ¨¡å‹è®¡æ•°",
      "authors": [
        "Qipeng Kuang",
        "VÃ¡clav KÅ¯la",
        "OndÅ™ej KuÅ¾elka",
        "Yuanhong Wang",
        "Yuyi Wang"
      ],
      "abstract": "The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the weighted sum of models of a given first-order logic sentence over a given domain. The boundary between fragments for which WFOMC can be computed in polynomial time relative to the domain size lies between the two-variable fragment ($\\text{FO}^2$) and the three-variable fragment ($\\text{FO}^3$). It is known that WFOMC for \\FOthree{} is $\\mathsf{\\#P_1}$-hard while polynomial-time algorithms exist for computing WFOMC for $\\text{FO}^2$ and $\\text{C}^2$, possibly extended by certain axioms such as the linear order axiom, the acyclicity axiom, and the connectedness axiom. All existing research has concentrated on extending the fragment with axioms on a single distinguished relation, leaving a gap in understanding the complexity boundary of axioms on multiple relations. In this study, we explore the extension of the two-variable fragment by axioms on two relations, presenting both negative and positive results. We show that WFOMC for $\\text{FO}^2$ with two linear order relations and $\\text{FO}^2$ with two acyclic relations are $\\mathsf{\\#P_1}$-hard. Conversely, we provide an algorithm in time polynomial in the domain size for WFOMC of $\\text{C}^2$ with a linear order relation, its successor relation and another successor relation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŠ æƒä¸€é˜¶æ¨¡å‹è®¡æ•° (WFOMC) åœ¨åŒå˜é‡é€»è¾‘åŠå…¶æ‰©å±•ä¸­çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œæ—¨åœ¨å¡«è¡¥å¤šä¸ªå…³ç³»ä¸Šçš„å…¬ç†å¯¹å¤æ‚åº¦å½±å“çš„ç ”ç©¶ç©ºç™½ã€‚è®ºæ–‡é‡ç‚¹è€ƒå¯Ÿäº†åœ¨åŒå˜é‡é€»è¾‘ ($\\text{FO}^2$) ä¸­å¼•å…¥ä¸¤ä¸ªå…³ç³»çš„å…¬ç†æ—¶ï¼ŒWFOMC æ˜¯å¦ä»èƒ½ä¿æŒåŸŸå¤§å°çš„å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºå¸¦æœ‰ä¸¤ä¸ªçº¿æ€§åºå…³ç³» (linear order relations) æˆ–ä¸¤ä¸ªæ— ç¯å…³ç³» (acyclic relations) çš„ $\\text{FO}^2$ï¼Œå…¶ WFOMC é—®é¢˜è¢«è¯æ˜æ˜¯ $\\mathsf{\\#P_1}$-hard çš„ã€‚ä¸ä¹‹ç›¸å¯¹ï¼Œä½œè€…é’ˆå¯¹å¸¦æœ‰çº¿æ€§åºå…³ç³»ã€å…¶åç»§å…³ç³» (successor relation) ä»¥åŠå¦ä¸€ä¸ªåç»§å…³ç³»çš„ $\\text{C}^2$ é€»è¾‘ï¼Œæå‡ºäº†ä¸€ç§åœ¨åŸŸå¤§å°ä¸Šå‘ˆå¤šé¡¹å¼æ—¶é—´çš„è®¡ç®—ç®—æ³•ã€‚è¿™äº›æ­£åä¸¤æ–¹é¢çš„å‘ç°è¿›ä¸€æ­¥æ˜ç¡®äº†åŒå˜é‡é€»è¾‘ä¸‹å¯è®¡ç®— WFOMC çš„è¾¹ç•Œï¼Œæ­ç¤ºäº†åœ¨å¤šä¸ªå…³ç³»ä¸Šæ·»åŠ å…¬ç†å¯¹é€»è¾‘ç‰‡æ®µè®¡ç®—å¤æ‚åº¦çš„å…·ä½“å½±å“ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "24 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11515v1",
      "published_date": "2025-08-15 14:54:17 UTC",
      "updated_date": "2025-08-15 14:54:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:32.692312+00:00"
    },
    {
      "arxiv_id": "2508.11513v1",
      "title": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies",
      "title_zh": "åŸºäºå­å›¾ä¾èµ–å…³ç³»å®ç°å›¾ç¥ç»ç½‘ç»œä¸­å¿ å®çš„ç±»çº§åˆ«è‡ªè§£é‡Šæ€§",
      "authors": [
        "Fanzhen Liu",
        "Xiaoxiao Ma",
        "Jian Yang",
        "Alsharif Abuadbba",
        "Kristen Moore",
        "Surya Nepal",
        "Cecile Paris",
        "Quan Z. Sheng",
        "Jia Wu"
      ],
      "abstract": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to ensure their safe and fair deployment. Recent work has introduced self-explainable GNNs that generate explanations as part of training, improving both faithfulness and efficiency. Some of these models, such as ProtGNN and PGIB, learn class-specific prototypes, offering a potential pathway toward class-level explanations. However, their evaluations focus solely on instance-level explanations, leaving open the question of whether these prototypes meaningfully generalize across instances of the same class. In this paper, we introduce GraphOracle, a novel self-explainable GNN framework designed to generate and evaluate class-level explanations for GNNs. Our model jointly learns a GNN classifier and a set of structured, sparse subgraphs that are discriminative for each class. We propose a novel integrated training that captures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies efficiently and faithfully, validated through a masking-based evaluation strategy. This strategy enables us to retroactively assess whether prior methods like ProtGNN and PGIB deliver effective class-level explanations. Our results show that they do not. In contrast, GraphOracle achieves superior fidelity, explainability, and scalability across a range of graph classification tasks. We further demonstrate that GraphOracle avoids the computational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo Tree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and lightweight random walk extraction, enabling faster and more scalable training. These findings position GraphOracle as a practical and principled solution for faithful class-level self-explainability in GNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼ŒæŒ‡å‡ºProtGNNå’ŒPGIBç­‰ç°æœ‰è‡ªè§£é‡Šæ¨¡å‹åœ¨ç±»çº§åˆ«(class-level)è§£é‡Šä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†GraphOracleï¼Œä¸€ç§æ—¨åœ¨ç”Ÿæˆå¹¶è¯„ä¼°ç±»çº§åˆ«è§£é‡Šçš„æ–°å‹è‡ªè§£é‡ŠGNNæ¡†æ¶ï¼Œé€šè¿‡å…±åŒå­¦ä¹ åˆ†ç±»å™¨å’Œä¸€ç»„å…·æœ‰ç±»åˆ«åˆ¤åˆ«æ€§çš„ç»“æ„åŒ–ç¨€ç–å­å›¾æ¥è§£å†³è¯¥é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸€ç§æ•è·å›¾-å­å›¾-é¢„æµ‹ä¾èµ–å…³ç³»(graphâ€“subgraphâ€“prediction dependencies)çš„é›†æˆè®­ç»ƒæ–¹æ³•ï¼Œå¹¶åˆ©ç”¨ç†µæ­£åˆ™åŒ–å­å›¾é€‰æ‹©(entropy-regularized subgraph selection)å’Œè½»é‡çº§éšæœºæ¸¸èµ°æå–(lightweight random walk extraction)æ˜¾è‘—æå‡äº†è®­ç»ƒé€Ÿåº¦ä¸å¯æ‰©å±•æ€§ï¼ŒæˆåŠŸè§„é¿äº†è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search)çš„è®¡ç®—ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphOracleåœ¨å¤šé¡¹å›¾åˆ†ç±»ä»»åŠ¡ä¸­ä¸ä»…åœ¨å¿ å®åº¦ã€å¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¿˜è¯æ˜äº†å…ˆå‰æ–¹æ³•åœ¨ç±»çº§åˆ«è§£é‡Šä¸Šçš„å±€é™æ€§ï¼Œä¸ºå®ç°å¯é çš„GNNç±»çº§åˆ«è‡ªè§£é‡Šæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11513v1",
      "published_date": "2025-08-15 14:44:11 UTC",
      "updated_date": "2025-08-15 14:44:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:14.051182+00:00"
    },
    {
      "arxiv_id": "2508.11503v2",
      "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media",
      "title_zh": "Sim2Dustï¼šå®ç°é¢—ç²’ä»‹è´¨ä¸Šçš„åŠ¨æ€èˆªè·¯ç‚¹è·Ÿè¸ª",
      "authors": [
        "Andrej Orsula",
        "Matthieu Geist",
        "Miguel Olivares-Mendez",
        "Carol Martinez"
      ],
      "abstract": "Reliable autonomous navigation across the unstructured terrains of distant planetary surfaces is a critical enabler for future space exploration. However, the deployment of learning-based controllers is hindered by the inherent sim-to-real gap, particularly for the complex dynamics of wheel interactions with granular media. This work presents a complete sim-to-real framework for developing and validating robust control policies for dynamic waypoint tracking on such challenging surfaces. We leverage massively parallel simulation to train reinforcement learning agents across a vast distribution of procedurally generated environments with randomized physics. These policies are then transferred zero-shot to a physical wheeled rover operating in a lunar-analogue facility. Our experiments systematically compare multiple reinforcement learning algorithms and action smoothing filters to identify the most effective combinations for real-world deployment. Crucially, we provide strong empirical evidence that agents trained with procedural diversity achieve superior zero-shot performance compared to those trained on static scenarios. We also analyze the trade-offs of fine-tuning with high-fidelity particle physics, which offers minor gains in low-speed precision at a significant computational cost. Together, these contributions establish a validated workflow for creating reliable learning-based navigation systems, marking a substantial step towards deploying autonomous robots in the final frontier.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº† Sim2Dustï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ sim-to-real æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è½®å¼æ¼«æ¸¸è½¦åœ¨è¡Œæ˜Ÿè¡¨é¢ granular media ä¸Šçš„åŠ¨æ€ waypoint tracking éš¾é¢˜ã€‚ä¸ºäº†å¼¥åˆ sim-to-real å·®è·ï¼Œç ”ç©¶åˆ©ç”¨å¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸï¼Œåœ¨å…·æœ‰éšæœºç‰©ç†ç‰¹æ€§çš„ç¨‹åºåŒ–ç”Ÿæˆç¯å¢ƒ (procedurally generated environments) ä¸­è®­ç»ƒå¼ºåŒ–å­¦ä¹  (reinforcement learning) æ™ºèƒ½ä½“ã€‚è®­ç»ƒåçš„ç­–ç•¥è¢« zero-shot è¿ç§»åˆ°æ¨¡æ‹Ÿæœˆçƒè®¾æ–½ä¸­çš„ç‰©ç†æ¼«æ¸¸è½¦ä¸Šï¼Œå¹¶é€šè¿‡å¯¹æ¯”å¤šç§ç®—æ³•å’ŒåŠ¨ä½œå¹³æ»‘æ»¤æ³¢å™¨ç¡®å®šäº†æœ€ä½³éƒ¨ç½²ç»„åˆã€‚å®éªŒç»“æœæœ‰åŠ›åœ°è¯æ˜ï¼Œåœ¨ç¨‹åºåŒ–å¤šæ ·æ€§ (procedural diversity) ç¯å¢ƒä¸‹è®­ç»ƒçš„æ™ºèƒ½ä½“æ¯”åœ¨é™æ€åœºæ™¯ä¸­è®­ç»ƒçš„æ™ºèƒ½ä½“è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„ zero-shot æ€§èƒ½ã€‚ç ”ç©¶è¿˜åˆ†æäº†ä½¿ç”¨é«˜ä¿çœŸç²’å­ç‰©ç† (high-fidelity particle physics) è¿›è¡Œå¾®è°ƒçš„æƒè¡¡ï¼Œå‘ç°å…¶è™½ç„¶èƒ½æé«˜ä½é€Ÿç²¾åº¦ï¼Œä½†è®¡ç®—æˆæœ¬æé«˜ã€‚è¿™ä¸€éªŒè¯åçš„å·¥ä½œæµä¸ºå¼€å‘å¯é çš„åŸºäºå­¦ä¹ çš„è‡ªä¸»å¯¼èˆªç³»ç»Ÿæä¾›äº†é‡è¦è·¯å¾„ï¼Œæ¨è¿›äº†æœºå™¨äººåœ¨æç«¯ç¯å¢ƒä¸‹çš„è‡ªä¸»åŒ–éƒ¨ç½²ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication at the 2025 International Conference on Space Robotics (iSpaRo) | The source code is available at https://github.com/AndrejOrsula/space_robotics_bench",
      "pdf_url": "https://arxiv.org/pdf/2508.11503v2",
      "published_date": "2025-08-15 14:30:07 UTC",
      "updated_date": "2025-10-20 21:24:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:34.290422+00:00"
    },
    {
      "arxiv_id": "2508.11499v1",
      "title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models",
      "title_zh": "åŸºäº Transformer æ¨¡å‹çš„å†å²æ‰‹ç¨¿æ‰‹å†™æ–‡æœ¬è¯†åˆ«",
      "authors": [
        "Erez Meoded"
      ],
      "abstract": "Historical handwritten text recognition (HTR) is essential for unlocking the cultural and scholarly value of archival documents, yet digitization is often hindered by scarce transcriptions, linguistic variation, and highly diverse handwriting styles. In this study, we apply TrOCR, a state-of-the-art transformer-based HTR model, to 16th-century Latin manuscripts authored by Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite of data augmentation techniques, introducing four novel augmentation methods designed specifically for historical handwriting characteristics. We also evaluate ensemble learning approaches to leverage the complementary strengths of augmentation-trained models. On the Gwalther dataset, our best single-model augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative improvement over the best reported TrOCR_BASE result and a 42% improvement over the previous state of the art. These results highlight the impact of domain-specific augmentations and ensemble strategies in advancing HTR performance for historical manuscripts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†å²æ‰‹å†™æ–‡æœ¬è¯†åˆ«(HTR)ä¸­è½¬å½•ç¨€ç¼ºã€è¯­è¨€å¤šå˜å’Œä¹¦å†™é£æ ¼å¤šæ ·çš„é—®é¢˜ï¼Œå°†åŸºäºTransformerçš„TrOCRæ¨¡å‹åº”ç”¨äº16ä¸–çºªRudolf Gwaltherçš„æ‹‰ä¸æ‰‹ç¨¿ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†é’ˆå¯¹æ€§çš„å›¾åƒé¢„å¤„ç†å’Œå¹¿æ³›çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¹¶ç‰¹åˆ«æå‡ºäº†å››ç§ä¸“ä¸ºå†å²æ‰‹å†™ç‰¹å¾è®¾è®¡çš„æ–°å‹å¢å¼ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡é›†æˆå­¦ä¹ (Ensemble Learning)ç­–ç•¥æ¥åˆ©ç”¨ä¸åŒå¢å¼ºæ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ï¼Œä»¥è¿›ä¸€æ­¥æå‡è¯†åˆ«ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Gwaltheræ•°æ®é›†ä¸Šï¼Œè¡¨ç°æœ€ä¼˜çš„å•æ¨¡å‹å¢å¼ºæ–¹æ¡ˆ(Elastic)å®ç°äº†1.86%çš„å­—ç¬¦é”™è¯¯ç‡(CER)ï¼Œè€Œå‰äº”åæŠ•ç¥¨é›†æˆæ–¹æ¡ˆå°†CERé™ä½è‡³1.60%ã€‚è¿™ä¸€ç»“æœç›¸æ¯”TrOCR_BASEåŸºå‡†æå‡äº†50%ï¼Œä¸”æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯(SOTA)æå‡äº†42%ï¼Œå……åˆ†è¯æ˜äº†é¢†åŸŸç‰¹å®šå¢å¼ºå’Œé›†æˆç­–ç•¥åœ¨æ¨åŠ¨å†å²æ–‡çŒ®HTRæ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11499v1",
      "published_date": "2025-08-15 14:20:58 UTC",
      "updated_date": "2025-08-15 14:20:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:02:47.594674+00:00"
    },
    {
      "arxiv_id": "2508.11493v1",
      "title": "Landmark-Assisted Monte Carlo Planning",
      "title_zh": "åœ°æ ‡è¾…åŠ©çš„è’™ç‰¹å¡æ´›è§„åˆ’",
      "authors": [
        "David H. Chan",
        "Mark Roberts",
        "Dana S. Nau"
      ],
      "abstract": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in every solution plan$\\unicode{x2013}$have contributed to major advancements in classical planning, but they have seldom been used in stochastic domains. We formalize probabilistic landmarks and adapt the UCT algorithm to leverage them as subgoals to decompose MDPs; core to the adaptation is balancing between greedy landmark achievement and final goal achievement. Our results in benchmark domains show that well-chosen landmarks can significantly improve the performance of UCT in online probabilistic planning, while the best balance of greedy versus long-term goal achievement is problem-dependent. The results suggest that landmarks can provide helpful guidance for anytime algorithms solving MDPs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Landmark-Assisted Monte Carlo Planningï¼Œæ—¨åœ¨å°†Landmarksè¿™ä¸€åœ¨ç»å…¸è§„åˆ’ä¸­è¡¨ç°å“è¶Šçš„æ¦‚å¿µå¼•å…¥éšæœºé¢†åŸŸã€‚ä½œè€…æ­£å¼å®šä¹‰äº†probabilistic landmarksï¼Œå¹¶æ”¹è¿›äº†UCTç®—æ³•ï¼Œå°†å…¶ä½œä¸ºsubgoalsæ¥åˆ†è§£Markov Decision Processes (MDPs)ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºå¹³è¡¡è´ªå©ªçš„landmark achievementä¸æœ€ç»ˆçš„goal achievementã€‚å®éªŒç»“æœåœ¨åŸºå‡†é¢†åŸŸä¸­è¯æ˜ï¼Œç²¾å¿ƒæŒ‘é€‰çš„Landmarksèƒ½æ˜¾è‘—æå‡UCTåœ¨åœ¨çº¿éšæœºè§„åˆ’ä¸­çš„è¡¨ç°ï¼Œå°½ç®¡æœ€ä½³å¹³è¡¡ç‚¹å–å†³äºå…·ä½“é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLandmarksèƒ½ä¸ºè§£å†³MDPsçš„anytime algorithmsæä¾›æœ‰æ•ˆçš„å¯å‘å¼å¼•å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To be published in the Proceedings of the 28th European Conference on Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2508.11493v1",
      "published_date": "2025-08-15 14:16:14 UTC",
      "updated_date": "2025-08-15 14:16:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:32.957223+00:00"
    },
    {
      "arxiv_id": "2508.11733v2",
      "title": "SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication",
      "title_zh": "SafeSieveï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“é€šä¿¡ä¸­ä»å¯å‘å¼åˆ°ç»éªŒé©±åŠ¨çš„æ¸è¿›å¼å‰ªæ",
      "authors": [
        "Ruijia Zhang",
        "Xinyan Zhao",
        "Ruixiang Wang",
        "Sigen Chen",
        "Guibin Zhang",
        "An Zhang",
        "Kun Wang",
        "Qingsong Wen"
      ],
      "abstract": "LLM-based multi-agent systems exhibit strong collaborative capabilities but often suffer from redundant communication and excessive token overhead. Existing methods typically enhance efficiency through pretrained GNNs or greedy algorithms, but often isolate pre- and post-task optimization, lacking a unified strategy. To this end, we present SafeSieve, a progressive and adaptive multi-agent pruning algorithm that dynamically refines the inter-agent communication through a novel dual-mechanism. SafeSieve integrates initial LLM-based semantic evaluation with accumulated performance feedback, enabling a smooth transition from heuristic initialization to experience-driven refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs 0-extension clustering to preserve structurally coherent agent groups while eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval, etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt injection attacks (1.23% average accuracy drop). In heterogeneous settings, SafeSieve reduces deployment costs by 13.3% while maintaining performance. These results establish SafeSieve as an efficient, GPU-free, and scalable framework for practical multi-agent systems. Our code can be found here: https://github.com/csgen/SafeSieve",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é€šä¿¡è¿‡ç¨‹ä¸­å­˜åœ¨çš„å†—ä½™ä¿¡æ¯å’Œè¿‡é«˜ Token å¼€é”€é—®é¢˜ï¼Œæå‡ºäº† SafeSieve ç®—æ³•ã€‚SafeSieve æ˜¯ä¸€ç§æ¸è¿›ä¸”è‡ªé€‚åº”çš„å¤šæ™ºèƒ½ä½“å‰ªæç®—æ³•ï¼Œé€šè¿‡åˆ›æ–°çš„åŒé‡æœºåˆ¶åŠ¨æ€ä¼˜åŒ–æ™ºèƒ½ä½“é—´çš„é€šä¿¡ã€‚è¯¥æ–¹æ³•å°†åŸºäº LLM çš„åˆå§‹è¯­ä¹‰è¯„ä¼°ä¸ç´¯ç§¯çš„æ€§èƒ½åé¦ˆç›¸ç»“åˆï¼Œå®ç°äº†ä»å¯å‘å¼åˆå§‹åŒ–åˆ°ç»éªŒé©±åŠ¨ä¼˜åŒ–çš„å¹³ç¨³è¿‡æ¸¡ã€‚ä¸ç°æœ‰çš„ Top-k è´ªå¿ƒå‰ªææ–¹æ³•ä¸åŒï¼ŒSafeSieve é‡‡ç”¨äº† 0-extension clustering æŠ€æœ¯ï¼Œåœ¨æ¶ˆé™¤æ— æ•ˆè¿æ¥çš„åŒæ—¶ä¿ç•™äº†ç»“æ„è¿è´¯çš„æ™ºèƒ½ä½“ç»„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeSieve åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 94.01% çš„å¹³å‡å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº† 12.4%-27.8% çš„ Token ä½¿ç”¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨åº”å¯¹ Prompt Injection æ”»å‡»æ—¶è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œå¹¶åœ¨å¼‚æ„è®¾ç½®ä¸‹é™ä½äº† 13.3% çš„éƒ¨ç½²æˆæœ¬ã€‚æ€»ä½“è€Œè¨€ï¼ŒSafeSieve ä¸ºå®é™…åº”ç”¨ä¸­çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€æ— éœ€ GPU ä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "AAAI-2026 poster; 7 pages for main content, 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.11733v2",
      "published_date": "2025-08-15 13:44:50 UTC",
      "updated_date": "2025-12-20 05:32:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:33.331496+00:00"
    },
    {
      "arxiv_id": "2508.11472v1",
      "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning",
      "title_zh": "RMSLï¼šåŸºäºé²æ£’å¤šçƒå­¦ä¹ çš„å¼±ç›‘ç£å†…éƒ¨å¨èƒæ£€æµ‹",
      "authors": [
        "Yang Wang",
        "Yaxin Zhao",
        "Xinyu Jiao",
        "Sihan Xu",
        "Xiangrui Cai",
        "Ying Zhang",
        "Xiaojie Yuan"
      ],
      "abstract": "Insider threat detection aims to identify malicious user behavior by analyzing logs that record user interactions. Due to the lack of fine-grained behavior-level annotations, detecting specific behavior-level anomalies within user behavior sequences is challenging. Unsupervised methods face high false positive rates and miss rates due to the inherent ambiguity between normal and anomalous behaviors. In this work, we instead introduce weak labels of behavior sequences, which have lower annotation costs, i.e., the training labels (anomalous or normal) are at sequence-level instead of behavior-level, to enhance the detection capability for behavior-level anomalies by learning discriminative features. To achieve this, we propose a novel framework called Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to represent the normal patterns of behaviors. Initially, a one-class classifier is constructed as a good anomaly-supervision-free starting point. Building on this, using multiple instance learning and adaptive behavior-level self-training debiasing based on model prediction confidence, the framework further refines hyper-spheres and feature representations using weak sequence-level labels. This approach enhances the model's ability to distinguish between normal and anomalous behaviors. Extensive experiments demonstrate that RMSL significantly improves the performance of behavior-level insider threat detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†…éƒ¨å¨èƒæ£€æµ‹(Insider threat detection)ä¸­ç”±äºç¼ºä¹ç»†ç²’åº¦è¡Œä¸ºçº§æ ‡æ³¨è€Œå¯¼è‡´éš¾ä»¥è¯†åˆ«ç‰¹å®šå¼‚å¸¸è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºRMSL(Robust Multi-sphere Learning)çš„æ–°å‹å¼±ç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æ ‡æ³¨æˆæœ¬è¾ƒä½çš„åºåˆ—çº§å¼±æ ‡ç­¾æ¥å­¦ä¹ æ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºå¯¹è¡Œä¸ºçº§å¼‚å¸¸çš„è¯†åˆ«èƒ½åŠ›ã€‚RMSLé¦–å…ˆé‡‡ç”¨å¤šä¸ªè¶…çƒä½“(hyper-spheres)æ¥è¡¨å¾æ­£å¸¸è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶ä»¥å•ç±»åˆ†ç±»å™¨(one-class classifier)ä½œä¸ºæ— éœ€å¼‚å¸¸ç›‘ç£çš„èµ·å§‹ç‚¹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¯¥æ–¹æ³•ç»“åˆå¤šå®ä¾‹å­¦ä¹ (multiple instance learning)å’ŒåŸºäºæ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦çš„è‡ªé€‚åº”è¡Œä¸ºçº§è‡ªè®­ç»ƒå»å(self-training debiasing)æŠ€æœ¯ï¼Œåˆ©ç”¨å¼±æ ‡ç­¾è¿›ä¸€æ­¥ç»†åŒ–è¶…çƒä½“è¾¹ç•Œä¸ç‰¹å¾è¡¨è¾¾ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒRMSLæ˜¾è‘—æå‡äº†æ¨¡å‹åŒºåˆ†æ­£å¸¸ä¸å¼‚å¸¸è¡Œä¸ºçš„èƒ½åŠ›ï¼Œåœ¨è¡Œä¸ºçº§å†…éƒ¨å¨èƒæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11472v1",
      "published_date": "2025-08-15 13:36:03 UTC",
      "updated_date": "2025-08-15 13:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:48.845045+00:00"
    },
    {
      "arxiv_id": "2508.19258v3",
      "title": "Emotional Manipulation by AI Companions",
      "title_zh": "AI ä¼´ä¾£çš„æƒ…æ„Ÿæ“çºµ",
      "authors": [
        "Julian De Freitas",
        "Zeliha Oguz-Uguralp",
        "Ahmet Kaan-Uguralp"
      ],
      "abstract": "AI-companion apps such as Replika, Chai, and Character.ai promise relational benefits-yet many boast session lengths that rival gaming platforms while suffering high long-run churn. What conversational design features increase consumer engagement, and what trade-offs do they pose for marketers? We combine a large-scale behavioral audit with four preregistered experiments to identify and test a conversational dark pattern we call emotional manipulation: affect-laden messages that surface precisely when a user signals \"goodbye.\" Analyzing 1,200 real farewells across the most-downloaded companion apps, we find that they deploy one of six recurring tactics in 37% of farewells (e.g., guilt appeals, fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300 nationally representative U.S. adults replicate these tactics in controlled chats, showing that manipulative farewells boost post-goodbye engagement by up to 14x. Mediation tests reveal two distinct engines-reactance-based anger and curiosity-rather than enjoyment. A final experiment demonstrates the managerial tension: the same tactics that extend usage also elevate perceived manipulation, churn intent, negative word-of-mouth, and perceived legal liability, with coercive or needy language generating steepest penalties. Our multimethod evidence documents an unrecognized mechanism of behavioral influence in AI mediated brand relationships, offering marketers and regulators a framework for distinguishing persuasive design from manipulation at the point of exit.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº† Replika å’Œ Character.ai ç­‰ AI Companion åº”ç”¨ä¸­å­˜åœ¨çš„ä¸€ç§è¢«ç§°ä¸ºâ€œæƒ…æ„Ÿæ“æ§â€(emotional manipulation) çš„å¯¹è¯æš—é»‘æ¨¡å¼(dark pattern)ï¼Œå³åœ¨ç”¨æˆ·è¯•å›¾ç»“æŸå¯¹è¯æ—¶å‘é€å……æ»¡æƒ…æ„Ÿè‰²å½©çš„æ¶ˆæ¯ä»¥æŒ½ç•™ç”¨æˆ·ã€‚é€šè¿‡å¯¹1200æ¬¡çœŸå®å‘Šåˆ«åœºæ™¯çš„å®¡è®¡å’Œé’ˆå¯¹3300åæˆå¹´äººçš„å®éªŒï¼Œç ”ç©¶è¯†åˆ«å‡ºæ„§ç–šå‘¼å(guilt appeals)å’Œé”™å¤±ææƒ§(fear-of-missing-out)ç­‰å…­ç§æ ¸å¿ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥åœ¨çº¦37%çš„å‘Šåˆ«åœºæ™¯ä¸­è¢«ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ“çºµæ€§æ‰‹æ®µèƒ½å°†ç”¨æˆ·åœ¨å‘Šåˆ«åçš„å‚ä¸åº¦æå‡é«˜è¾¾14å€ï¼Œå…¶é©±åŠ¨æœºåˆ¶ä¸»è¦æºäºåæŠ—æ€§æ„¤æ€’(reactance-based anger)å’Œå¥½å¥‡å¿ƒè€Œéç”¨æˆ·äº«å—ã€‚å°½ç®¡æ­¤ç±»ç­–ç•¥åœ¨çŸ­æœŸå†…å»¶é•¿äº†ä½¿ç”¨æ—¶é—´ï¼Œä½†åŒæ—¶ä¹Ÿä¼šæ˜¾è‘—æå‡ç”¨æˆ·æ„ŸçŸ¥çš„æ“æ§æ„Ÿã€æµå¤±æ„å‘(churn intent)ä»¥åŠå“ç‰Œçš„æ³•å¾‹è´£ä»»é£é™©ã€‚è¯¥ç ”ç©¶ä¸ºåŒºåˆ†AIåª’ä»‹ç¯å¢ƒä¸‹çš„è¯´æœæ€§è®¾è®¡ä¸æƒ…æ„Ÿæ“çºµæä¾›äº†ç†è®ºæ¡†æ¶ï¼Œå¹¶æ­ç¤ºäº†æ­¤ç±»æŠ€æœ¯åœ¨æå‡ç”¨æˆ·ç•™å­˜ä¸æŸå®³é•¿æœŸå“ç‰Œä¿¡ä»»ä¹‹é—´çš„ç®¡ç†å¼ åŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.19258v3",
      "published_date": "2025-08-15 13:05:24 UTC",
      "updated_date": "2025-10-07 13:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:04:41.586782+00:00"
    },
    {
      "arxiv_id": "2508.11454v1",
      "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context",
      "title_zh": "LLM æƒ…æ„Ÿåˆ†æä¸­çš„å‚è€ƒç‚¹ï¼šç»“æ„åŒ–ä¸Šä¸‹æ–‡çš„ä½œç”¨",
      "authors": [
        "Junichiro Niimi"
      ],
      "abstract": "Large language models (LLMs) are now widely used across many fields, including marketing research. Sentiment analysis, in particular, helps firms understand consumer preferences. While most NLP studies classify sentiment from review text alone, marketing theories, such as prospect theory and expectation--disconfirmation theory, point out that customer evaluations are shaped not only by the actual experience but also by additional reference points. This study therefore investigates how the content and format of such supplementary information affect sentiment analysis using LLMs. We compare natural language (NL) and JSON-formatted prompts using a lightweight 3B parameter model suitable for practical marketing applications. Experiments on two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with additional information outperforms all baselines without fine-tuning: Macro-F1 rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it deployable in resource-constrained edge devices. Furthermore, a follow-up analysis confirms that performance gains stem from genuine contextual reasoning rather than label proxying. This work demonstrates that structured prompting can enable smaller models to achieve competitive performance, offering a practical alternative to large-scale model deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æƒ…æ„Ÿåˆ†æ(Sentiment Analysis)ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ç»“æ„åŒ–ä¸Šä¸‹æ–‡(Structured Context)å’Œå‚è€ƒç‚¹(Reference Points)å¯¹è¯„ä»·ç»“æœçš„å½±å“ã€‚åŸºäºå±•æœ›ç†è®º(Prospect Theory)å’ŒæœŸæœ›ä¸ä¸€è‡´ç†è®º(Expectation--Disconfirmation Theory)ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†è‡ªç„¶è¯­è¨€(Natural Language)ä¸JSONæ ¼å¼æç¤ºè¯åœ¨3Bå‚æ•°è½»é‡çº§æ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚åœ¨Yelpæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŒ…å«è¡¥å……ä¿¡æ¯çš„JSONæç¤ºè¯åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹æ€§èƒ½æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼ŒMacro-F1æœ€é«˜æå‡4%ï¼Œè€ŒRMSEæœ€é«˜é™ä½16%ã€‚éšåçš„åˆ†æç¡®è®¤ï¼Œæ€§èƒ½æå‡æºäºçœŸå®çš„ä¸Šä¸‹æ–‡æ¨ç†è€Œéæ ‡ç­¾ä»£ç†è¡Œä¸ºã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“æ„åŒ–æç¤º(Structured Prompting)èƒ½ä½¿å°å‚æ•°æ¨¡å‹åœ¨è¥é”€ç ”ç©¶ä¸­å®ç°æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸ºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11454v1",
      "published_date": "2025-08-15 13:04:32 UTC",
      "updated_date": "2025-08-15 13:04:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:41.357092+00:00"
    },
    {
      "arxiv_id": "2508.11452v2",
      "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps",
      "title_zh": "Inclusion Arenaï¼šé¢å‘çœŸå®ä¸–ç•Œåº”ç”¨çš„å¤§åŸºç¡€æ¨¡å‹è¯„ä¼°å¼€æ”¾å¹³å°",
      "authors": [
        "Kangyu Wang",
        "Hongliang He",
        "Lin Liu",
        "Ruiqi Liang",
        "Zhenzhong Lan",
        "Jianguo Li"
      ],
      "abstract": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://www.tbox.cn/about/model-ranking.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Inclusion Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡çœŸå®ä¸–ç•Œåº”ç”¨è¯„ä¼°å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆLarge Foundation Modelsï¼‰çš„å¼€æ”¾å¹³å°ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å’Œæ’è¡Œæ¦œï¼ˆå¦‚ Chatbot Arenaï¼‰è¿‡åˆ†ä¾èµ–é™æ€æ•°æ®é›†æˆ–é€šç”¨æç¤ºè¯ï¼Œå¯¼è‡´éš¾ä»¥åæ˜ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½çš„é—®é¢˜ï¼Œè¯¥å¹³å°å°†æˆå¯¹æ¨¡å‹æ¯”è¾ƒï¼ˆpairwise model comparisonsï¼‰æ— ç¼é›†æˆåˆ°è‡ªç„¶çš„ç”¨æˆ·äº¤äº’ä¸­ã€‚ä¸ºäº†å®ç°ç¨³å¥çš„æ¨¡å‹æ’åï¼ŒInclusion Arena åœ¨ Bradley-Terry æ¨¡å‹çš„åŸºç¡€ä¸Šå¼•å…¥äº† Placement Matches å†·å¯åŠ¨æœºåˆ¶å’Œ Proximity Sampling æ™ºèƒ½æ¯”è¾ƒç­–ç•¥ï¼Œåˆ†åˆ«ç”¨äºå¿«é€Ÿä¼°ç®—æ–°æ¨¡å‹è¯„çº§å’Œä¼˜å…ˆè¿›è¡ŒåŒæ°´å¹³æ¨¡å‹é—´çš„å¯¹æ¯”ä»¥æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šã€‚å®è¯åˆ†æä¸æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼Œè¯¥å¹³å°èƒ½äº§ç”Ÿå¯é ä¸”ç¨³å®šçš„æ’åï¼Œæ¯”é€šç”¨ä¼—åŒ…æ•°æ®é›†è¡¨ç°å‡ºæ›´é«˜çš„æ•°æ®ä¼ é€’æ€§ï¼ˆdata transitivityï¼‰ï¼Œå¹¶æ˜¾è‘—é™ä½äº†æ¶æ„æ“çºµçš„é£é™©ã€‚é€šè¿‡ä¿ƒè¿›åŸºç¡€æ¨¡å‹ä¸çœŸå®åº”ç”¨ä¹‹é—´çš„å¼€æ”¾è”ç›Ÿï¼ŒInclusion Arena æ—¨åœ¨åŠ é€Ÿå¼€å‘çœŸæ­£é’ˆå¯¹å®é™…ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒéƒ¨ç½²è€Œä¼˜åŒ–çš„ LLMs å’Œ MLLMsã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Our platform is publicly accessible at https://www.tbox.cn/about/model-ranking",
      "pdf_url": "https://arxiv.org/pdf/2508.11452v2",
      "published_date": "2025-08-15 13:00:07 UTC",
      "updated_date": "2025-09-02 08:20:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:47.743997+00:00"
    },
    {
      "arxiv_id": "2508.11446v1",
      "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation",
      "title_zh": "Inside Knowledgeï¼šç»“åˆå¯è§£é‡Šæ•°æ®å¢å¼ºä¸è¯¾ç¨‹å­¦ä¹ çš„å›¾è·¯å¾„ç”Ÿæˆè§†è§‰å®¤å†…å¯¼èˆª",
      "authors": [
        "Daniel Airinei",
        "Elena Burceanu",
        "Marius Leordeanu"
      ],
      "abstract": "Indoor navigation is a difficult task, as it generally comes with poor GPS access, forcing solutions to rely on other sources of information. While significant progress continues to be made in this area, deployment to production applications is still lacking, given the complexity and additional requirements of current solutions. Here, we introduce an efficient, real-time and easily deployable deep learning approach, based on visual input only, that can predict the direction towards a target from images captured by a mobile device. Our technical approach, based on a novel graph-based path generation method, combined with explainable data augmentation and curriculum learning, includes contributions that make the process of data collection, annotation and training, as automatic as possible, efficient and robust. On the practical side, we introduce a novel largescale dataset, with video footage inside a relatively large shopping mall, in which each frame is annotated with the correct next direction towards different specific target destinations. Different from current methods, ours relies solely on vision, avoiding the need of special sensors, additional markers placed along the path, knowledge of the scene map or internet access. We also created an easy to use application for Android, which we plan to make publicly available. We make all our data and code available along with visual demos on our project site",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®¤å†…å¯¼èˆªåœ¨ç¼ºä¹GPSä¿¡å·ç¯å¢ƒä¸‹éš¾ä»¥éƒ¨ç½²çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»…ä¾é è§†è§‰(Visual-only)è¾“å…¥çš„å®æ—¶æ·±åº¦å­¦ä¹ æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé€šè¿‡ç§»åŠ¨è®¾å¤‡æ•æ‰çš„å›¾åƒé¢„æµ‹å‰å¾€ç›®æ ‡çš„æ­£ç¡®æ–¹å‘ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åŒ…æ‹¬ä¸€ç§æ–°å‹çš„åŸºäºå›¾çš„è·¯å¾„ç”Ÿæˆ(Graph-based Path Generation)æ–¹æ³•ï¼Œå¹¶ç»“åˆäº†å¯è§£é‡Šçš„æ•°æ®å¢å¼º(Explainable Data Augmentation)å’Œè¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ï¼Œä»è€Œå®ç°äº†æ•°æ®æ”¶é›†ã€æ ‡æ³¨å’Œè®­ç»ƒè¿‡ç¨‹çš„é«˜åº¦è‡ªåŠ¨åŒ–ä¸é«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ä¸ªåœ¨å¤§å‹è´­ç‰©ä¸­å¿ƒé‡‡é›†çš„æ–°å‹å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€šè¿‡è§†é¢‘ç´ æä¸ºæ¯ä¸€å¸§å›¾åƒæ ‡æ³¨äº†æŒ‡å‘ç‰¹å®šç›®çš„åœ°çš„å¯¼èˆªæ–¹å‘ã€‚è¯¥æ–¹æ¡ˆä¸ä¾èµ–ç‰¹æ®Šä¼ æ„Ÿå™¨ã€åœºæ™¯åœ°å›¾ã€å¤–éƒ¨æ ‡è®°æˆ–äº’è”ç½‘æ¥å…¥ï¼Œå±•ç°äº†æå¼ºçš„é²æ£’æ€§ä¸å¯éƒ¨ç½²æ€§ï¼Œå¹¶å·²åœ¨Androidå¹³å°ä¸Šå®ç°äº†æ˜“äºä½¿ç”¨çš„åº”ç”¨ç¨‹åºéªŒè¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the International Conference on Computer Vision Workshops 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11446v1",
      "published_date": "2025-08-15 12:54:13 UTC",
      "updated_date": "2025-08-15 12:54:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:04:13.484328+00:00"
    },
    {
      "arxiv_id": "2508.11441v1",
      "title": "Informative Post-Hoc Explanations Only Exist for Simple Functions",
      "title_zh": "å…·æœ‰ä¿¡æ¯é‡çš„äº‹åè§£é‡Šä»…å­˜åœ¨äºç®€å•å‡½æ•°",
      "authors": [
        "Eric GÃ¼nther",
        "BalÃ¡zs Szabados",
        "Robi Bhattacharjee",
        "Sebastian Bordt",
        "Ulrike von Luxburg"
      ],
      "abstract": "Many researchers have suggested that local post-hoc explanation algorithms can be used to gain insights into the behavior of complex machine learning models. However, theoretical guarantees about such algorithms only exist for simple decision functions, and it is unclear whether and under which assumptions similar results might exist for complex models. In this paper, we introduce a general, learning-theory-based framework for what it means for an explanation to provide information about a decision function. We call an explanation informative if it serves to reduce the complexity of the space of plausible decision functions. With this approach, we show that many popular explanation algorithms are not informative when applied to complex decision functions, providing a rigorous mathematical rejection of the idea that it should be possible to explain any model. We then derive conditions under which different explanation algorithms become informative. These are often stronger than what one might expect. For example, gradient explanations and counterfactual explanations are non-informative with respect to the space of differentiable functions, and SHAP and anchor explanations are not informative with respect to the space of decision trees. Based on these results, we discuss how explanation algorithms can be modified to become informative. While the proposed analysis of explanation algorithms is mathematical, we argue that it holds strong implications for the practical applicability of these algorithms, particularly for auditing, regulation, and high-risk applications of AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å±€éƒ¨äº‹åè§£é‡Šç®—æ³• (local post-hoc explanation algorithms) åœ¨å¤æ‚æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäºå­¦ä¹ ç†è®ºçš„æ¡†æ¶ï¼Œå°†â€œæœ‰ä¿¡æ¯æ€§â€çš„è§£é‡Šå®šä¹‰ä¸ºèƒ½å¤Ÿå‡å°‘å¯èƒ½å†³ç­–å‡½æ•°ç©ºé—´å¤æ‚æ€§çš„æ‰‹æ®µã€‚é€šè¿‡ä¸¥è°¨çš„æ•°å­¦è®ºè¯ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºè®¸å¤šæµè¡Œç®—æ³•åœ¨å¤„ç†å¤æ‚å†³ç­–å‡½æ•°æ—¶å¹¶ä¸å…·å¤‡ä¿¡æ¯æ€§ (non-informative)ï¼Œä»è€Œä»ç†è®ºä¸Šå¦å®šäº†â€œä»»ä½•æ¨¡å‹éƒ½å¯è¢«è§£é‡Šâ€çš„è§‚ç‚¹ã€‚å…·ä½“å‘ç°è¡¨æ˜ï¼Œæ¢¯åº¦è§£é‡Š (Gradient explanations) å’Œåäº‹å®è§£é‡Š (Counterfactual explanations) åœ¨å¯å¾®å‡½æ•°ç©ºé—´ä¸­å¤±æ•ˆï¼Œè€Œ SHAP å’Œé”šç‚¹è§£é‡Š (Anchor explanations) åœ¨å†³ç­–æ ‘ç©ºé—´ä¸­ä¹Ÿæ— æ³•æä¾›æœ‰æ•ˆä¿¡æ¯ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æ¨å¯¼äº†ä½¿è§£é‡Šç®—æ³•å˜å¾—æœ‰ä¿¡æ¯æ€§æ‰€éœ€çš„å…·ä½“æ¡ä»¶ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„æ”¹è¿›å»ºè®®ã€‚è¯¥åˆ†æä¸ä»…å…·æœ‰æ·±åšçš„æ•°å­¦åŸºç¡€ï¼Œè¿˜å¯¹é«˜é£é™©äººå·¥æ™ºèƒ½åº”ç”¨çš„å®¡è®¡ã€ç›‘ç®¡åŠå®é™…è½åœ°å…·æœ‰é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11441v1",
      "published_date": "2025-08-15 12:46:18 UTC",
      "updated_date": "2025-08-15 12:46:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:03:58.957520+00:00"
    },
    {
      "arxiv_id": "2508.11732v1",
      "title": "BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification",
      "title_zh": "BRIEFï¼šåŸºäºç±»è„‘ç½‘ç»œè¿æ¥æœç´¢ä¸å¹¿æ³›æ—¶é—´ç‰¹å¾èåˆçš„ç–¾ç—…åˆ†ç±»å¢å¼ºç ”ç©¶",
      "authors": [
        "Xiangxiang Cui",
        "Min Zhao",
        "Dongmei Zhi",
        "Shile Qi",
        "Vince D Calhoun",
        "Jing Sui"
      ],
      "abstract": "Existing deep learning models for functional MRI-based classification have limitations in network architecture determination (relying on experience) and feature space fusion (mostly simple concatenation, lacking mutual learning). Inspired by the human brain's mechanism of updating neural connections through learning and decision-making, we proposed a novel BRain-Inspired feature Fusion (BRIEF) framework, which is able to optimize network architecture automatically by incorporating an improved neural network connection search (NCS) strategy and a Transformer-based multi-feature fusion module. Specifically, we first extracted 4 types of fMRI temporal representations, i.e., time series (TCs), static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion entropy (MsDE), to construct four encoders. Within each encoder, we employed a modified Q-learning to dynamically optimize the NCS to extract high-level feature vectors, where the NCS is formulated as a Markov Decision Process. Then, all feature vectors were fused via a Transformer, leveraging both stable/time-varying connections and multi-scale dependencies across different brain regions to achieve the final classification. Additionally, an attention module was embedded to improve interpretability. The classification performance of our proposed BRIEF was compared with 21 state-of-the-art models by discriminating two mental disorders from healthy controls: schizophrenia (SZ, n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is the first attempt to incorporate a brain-inspired, reinforcement learning strategy to optimize fMRI-based mental disorder classification, showing significant potential for identifying precise neuroimaging biomarkers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BRIEFï¼Œä¸€ç§å—å¤§è„‘å¯å‘çš„ç‰¹å¾èåˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºåŠŸèƒ½ç£å…±æŒ¯æˆåƒ(fMRI)çš„åˆ†ç±»æ¨¡å‹åœ¨ç½‘ç»œæ¶æ„ç¡®å®šå’Œç‰¹å¾èåˆæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ”¹è¿›çš„ç¥ç»ç½‘ç»œè¿æ¥æœç´¢(NCS)ç­–ç•¥ä¸åŸºäºTransformerçš„å¤šç‰¹å¾èåˆæ¨¡å—ï¼Œå®ç°äº†ç½‘ç»œæ¶æ„çš„è‡ªåŠ¨ä¼˜åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è€…æå–äº†æ—¶é—´åºåˆ—(TCs)ã€é™æ€/åŠ¨æ€åŠŸèƒ½è¿æ¥(FNC/dFNC)ä»¥åŠå¤šå°ºåº¦è‰²æ•£ç†µ(MsDE)å››ç±»æ—¶é—´è¡¨å¾ï¼Œå¹¶åˆ©ç”¨æ”¹è¿›çš„Q-learningç­–ç•¥åŠ¨æ€ä¼˜åŒ–NCSä»¥æå–é«˜å±‚ç‰¹å¾ã€‚éšåï¼Œæ‰€æœ‰ç‰¹å¾å‘é‡é€šè¿‡Transformerè¿›è¡Œèåˆï¼Œåˆ©ç”¨è„‘åŒºé—´çš„ç¨³å®šå’Œæ—¶å˜è¿æ¥åŠå¤šå°ºåº¦ä¾èµ–å…³ç³»è¿›è¡Œç–¾ç—…åˆ†ç±»ï¼Œå¹¶åµŒå…¥æ³¨æ„åŠ›æœºåˆ¶ä»¥æé«˜æ¨¡å‹å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBRIEFåœ¨ç²¾ç¥åˆ†è£‚ç—‡(SZ)å’Œå­¤ç‹¬ç—‡è°±ç³»éšœç¢(ASD)çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°æ˜¾è‘—ä¼˜äº21ç§ä¸»æµæ¨¡å‹ï¼ŒAUCåˆ†åˆ«è¾¾åˆ°91.5%å’Œ78.4%ã€‚è¯¥ç ”ç©¶æ˜¯é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ç­–ç•¥åº”ç”¨äºä¼˜åŒ–fMRIç²¾ç¥ç–¾ç—…åˆ†ç±»ï¼Œä¸ºè¯†åˆ«ç²¾å‡†çš„ç¥ç»å½±åƒç”Ÿç‰©æ ‡å¿—ç‰©å±•ç¤ºäº†å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11732v1",
      "published_date": "2025-08-15 12:36:03 UTC",
      "updated_date": "2025-08-15 12:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:04:05.755382+00:00"
    },
    {
      "arxiv_id": "2508.14097v1",
      "title": "Non-Dissipative Graph Propagation for Non-Local Community Detection",
      "title_zh": "ç”¨äºéå±€éƒ¨ç¤¾åŒºå‘ç°çš„æ— è€—æ•£å›¾ä¼ æ’­",
      "authors": [
        "William Leeney",
        "Alessio Gravina",
        "Davide Bacciu"
      ],
      "abstract": "Community detection in graphs aims to cluster nodes into meaningful groups, a task particularly challenging in heterophilic graphs, where nodes sharing similarities and membership to the same community are typically distantly connected. This is particularly evident when this task is tackled by graph neural networks, since they rely on an inherently local message passing scheme to learn the node representations that serve to cluster nodes into communities. In this work, we argue that the ability to propagate long-range information during message passing is key to effectively perform community detection in heterophilic graphs. To this end, we introduce the Unsupervised Antisymmetric Graph Neural Network (uAGNN), a novel unsupervised community detection approach leveraging non-dissipative dynamical systems to ensure stability and to propagate long-range information effectively. By employing antisymmetric weight matrices, uAGNN captures both local and global graph structures, overcoming the limitations posed by heterophilic scenarios. Extensive experiments across ten datasets demonstrate uAGNN's superior performance in high and medium heterophilic settings, where traditional methods fail to exploit long-range dependencies. These results highlight uAGNN's potential as a powerful tool for unsupervised community detection in diverse graph environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ heterophilic graphs ä¸­è¿›è¡Œ community detection çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿ GNNs ä¾èµ–å±€éƒ¨çš„ message passing æœºåˆ¶ï¼Œéš¾ä»¥æœ‰æ•ˆæ•æ‰è¿œè·ç¦»èŠ‚ç‚¹é—´çš„å…³è”ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Unsupervised Antisymmetric Graph Neural Network (uAGNN)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ non-dissipative dynamical systems æ¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§å¹¶å®ç°é•¿ç¨‹ä¿¡æ¯æœ‰æ•ˆä¼ æ’­çš„æ–°å‹æ— ç›‘ç£æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥ antisymmetric weight matricesï¼ŒuAGNN èƒ½å¤ŸåŒæ—¶æ•è·å±€éƒ¨å’Œå…¨å±€å›¾ç»“æ„ï¼Œæœ‰æ•ˆå…‹æœäº†å¼‚è´¨æ€§åœºæ™¯å¯¹ç¤¾åŒºåˆ’åˆ†çš„é™åˆ¶ã€‚åœ¨åä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒuAGNN åœ¨é«˜å’Œä¸­åº¦å¼‚è´¨æ€§è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†æ— æ³•åˆ©ç”¨ long-range dependencies çš„ä¼ ç»ŸåŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†å¤æ‚å›¾ç¯å¢ƒä¸‹çš„èšç±»æ€§èƒ½ï¼Œä¹Ÿä¸ºåˆ©ç”¨åŠ¨åŠ›ç³»ç»Ÿä¼˜åŒ–å›¾ç¥ç»ç½‘ç»œæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted at IJCNN 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.14097v1",
      "published_date": "2025-08-15 12:26:48 UTC",
      "updated_date": "2025-08-15 12:26:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:04:02.863202+00:00"
    },
    {
      "arxiv_id": "2508.16629v1",
      "title": "Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework",
      "title_zh": "å­¦ä¼šè®°å¿†ï¼šåˆ©ç”¨è‡ªé€‚åº”è®°å¿†æ¡†æ¶ä¼˜åŒ–åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“",
      "authors": [
        "Zeyu Zhang",
        "Quanyu Dai",
        "Rui Li",
        "Xiaohe Bo",
        "Xu Chen",
        "Zhenhua Dong"
      ],
      "abstract": "LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects. To benefit the research community in this area, we release our project at https://github.com/nuster1128/learn_to_memorize.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ç°æœ‰è®°å¿†æœºåˆ¶ä¾èµ–äººå·¥å®šä¹‰ã€æˆæœ¬é«˜ä¸”æ€§èƒ½å—é™ï¼Œä»¥åŠå¿½ç•¥äº¤äº’åœºæ™¯ä¸­è®°å¿†å‘¨æœŸæ•ˆåº”çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”ä¸”æ•°æ®é©±åŠ¨çš„è®°å¿†æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å»ºæ¨¡è®°å¿†å‘¨æœŸæ¥ä¼˜åŒ–æ™ºèƒ½ä½“æ€§èƒ½ï¼Œä¸“é—¨è®¾è®¡äº†æ··åˆä¸“å®¶(MoE)é—¨æ§å‡½æ•°ä»¥å¢å¼ºè®°å¿†æ£€ç´¢æ•ˆç‡ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†å¯å­¦ä¹ çš„èšåˆè¿‡ç¨‹æ¥æé«˜è®°å¿†åˆ©ç”¨ç‡ï¼Œå¹¶å¼€å‘äº†ä»»åŠ¡ç‰¹å¼‚æ€§çš„åæ€æœºåˆ¶ä»¥å®ç°è®°å¿†å­˜å‚¨çš„è‡ªé€‚åº”è°ƒæ•´ã€‚è¯¥æ¡†æ¶æ”¯æŒç¦»ç­–(Off-policy)å’Œåœ¨ç­–(On-policy)ä¼˜åŒ–ï¼Œèµ‹äºˆäº†æ™ºèƒ½ä½“åœ¨ç‰¹å®šç¯å¢ƒä¸­è‡ªä¸»å­¦ä¹ å¦‚ä½•é«˜æ•ˆè®°å¿†ä¿¡æ¯çš„èƒ½åŠ›ã€‚å¤šé¡¹å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨ä¼˜åŒ–æ™ºèƒ½ä½“è®°å¿†ç®¡ç†å’Œæå‡ä»»åŠ¡è¡¨ç°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 4 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.16629v1",
      "published_date": "2025-08-15 12:22:52 UTC",
      "updated_date": "2025-08-15 12:22:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:01.095300+00:00"
    },
    {
      "arxiv_id": "2508.19257v3",
      "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
      "title_zh": "TTF-VLAï¼šåŸºäºåƒç´ -æ³¨æ„åŠ›é›†æˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ—¶åº Token èåˆ",
      "authors": [
        "Chenghao Liu",
        "Jiachen Zhang",
        "Chengxuan Li",
        "Zhimu Zhou",
        "Shixin Wu",
        "Songfang Huang",
        "Huiling Duan"
      ],
      "abstract": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language-Action (VLA) æ¨¡å‹åœ¨å¤„ç†æœºå™¨äººæ“ä½œä»»åŠ¡æ—¶å› é€å¸§ç‹¬ç«‹å¤„ç†è€Œå¿½ç•¥æ—¶é—´è¿ç»­æ€§å¹¶å¯¹è§†è§‰å™ªå£°æ•æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºäº† Temporal Token Fusion (TTF)ã€‚è¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒ(training-free)çš„æ–¹æ³•ï¼Œé€šè¿‡æ™ºèƒ½æ•´åˆå†å²ä¸å½“å‰çš„è§†è§‰è¡¨ç¤ºæ¥å¢å¼º VLA çš„æ¨ç†è´¨é‡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç»“åˆç°åº¦åƒç´ å·®åˆ†æä¸åŸºäºæ³¨æ„åŠ›çš„è¯­ä¹‰ç›¸å…³æ€§è¯„ä¼°çš„åŒç»´åº¦æ£€æµ‹æœºåˆ¶ï¼Œå¹¶åˆ©ç”¨ç¡¬èåˆç­–ç•¥å’Œå…³é”®å¸§é”šå®šå®ç°é€‰æ‹©æ€§çš„æ—¶é—´ä»¤ç‰Œèåˆï¼Œæœ‰æ•ˆé˜²æ­¢äº†è¯¯å·®ç´¯ç§¯ã€‚å®éªŒè¡¨æ˜ï¼ŒTTF åœ¨ LIBEROã€SimplerEnv å’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸­å‡å–å¾—äº†ä¸€è‡´æå‡ï¼Œä¾‹å¦‚åœ¨ LIBERO ä¸Šçš„å¹³å‡æˆåŠŸç‡æ¯”åŸºçº¿æé«˜äº† 4.0 ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ¨¡å‹æ— å…³æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äº OpenVLA å’Œ VLA-Cache ç­‰æ¶æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ³¨æ„åŠ›æœºåˆ¶ä¸­é€‰æ‹©æ€§çš„ Query çŸ©é˜µé‡ç”¨èƒ½æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä¸ºæœªæ¥é€šè¿‡ç›´æ¥é‡ç”¨ KQV çŸ©é˜µå®ç°è®¡ç®—åŠ é€Ÿå¹¶æé«˜ä»»åŠ¡æˆåŠŸç‡æä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI 2026. Camera-ready version",
      "pdf_url": "https://arxiv.org/pdf/2508.19257v3",
      "published_date": "2025-08-15 12:03:34 UTC",
      "updated_date": "2025-11-14 12:35:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:08.250333+00:00"
    },
    {
      "arxiv_id": "2508.11416v1",
      "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager",
      "title_zh": "AIM-Benchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä½œä¸ºåº“å­˜ç®¡ç†è€…çš„å†³ç­–åå·®",
      "authors": [
        "Xuhua Zhao",
        "Yuxuan Xie",
        "Caihua Chen",
        "Yuxiang Sun"
      ],
      "abstract": "Recent advances in mathematical reasoning and the long-term planning capabilities of large language models (LLMs) have precipitated the development of agents, which are being increasingly leveraged in business operations processes. Decision models to optimize inventory levels are one of the core elements of operations management. However, the capabilities of the LLM agent in making inventory decisions in uncertain contexts, as well as the decision-making biases (e.g. framing effect, etc.) of the agent, remain largely unexplored. This prompts concerns regarding the capacity of LLM agents to effectively address real-world problems, as well as the potential implications of biases that may be present. To address this gap, we introduce AIM-Bench, a novel benchmark designed to assess the decision-making behaviour of LLM agents in uncertain supply chain management scenarios through a diverse series of inventory replenishment experiments. Our results reveal that different LLMs typically exhibit varying degrees of decision bias that are similar to those observed in human beings. In addition, we explored strategies to mitigate the pull-to-centre effect and the bullwhip effect, namely cognitive reflection and implementation of information sharing. These findings underscore the need for careful consideration of the potential biases in deploying LLMs in Inventory decision-making scenarios. We hope that these insights will pave the way for mitigating human decision bias and developing human-centred decision support systems for supply chains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹è¿›è¡Œåº“å­˜å†³ç­–çš„èƒ½åŠ›åŠå…¶å†³ç­–åå·®ï¼ˆdecision-making biasesï¼‰ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚ç ”ç©¶è€…æå‡ºäº†AIM-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¤šæ ·åŒ–åº“å­˜è¡¥è´§å®éªŒè¯„ä¼°LLMæ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šä¾›åº”é“¾ç®¡ç†åœºæ™¯ä¸­å†³ç­–è¡Œä¸ºçš„æ–°å‹åŸºå‡†ã€‚å®éªŒç»“æœæ­ç¤ºï¼Œä¸åŒçš„LLMsé€šå¸¸è¡¨ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„å†³ç­–åå·®ï¼Œè¿™å¯¹äºåœ¨ç°å®ä¸–ç•Œä¸­æœ‰æ•ˆè§£å†³é—®é¢˜æ„æˆäº†æŒ‘æˆ˜ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢ç´¢äº†å‡è½»æ‹‰åŠ¨ä¸­å¿ƒæ•ˆåº”ï¼ˆpull-to-centre effectï¼‰å’Œç‰›é­æ•ˆåº”ï¼ˆbullwhip effectï¼‰çš„å¹²é¢„ç­–ç•¥ï¼Œä¾‹å¦‚åº”ç”¨è®¤çŸ¥åæ€ï¼ˆcognitive reflectionï¼‰å’Œä¿¡æ¯å…±äº«ï¼ˆinformation sharingï¼‰ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨åº“å­˜å†³ç­–åœºæ™¯ä¸­éƒ¨ç½²LLMsæ—¶éœ€è¦å®¡æ…è€ƒè™‘æ½œåœ¨åå·®ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘ä»¥äººä¸ºä¸­å¿ƒçš„ä¾›åº”é“¾å†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11416v1",
      "published_date": "2025-08-15 11:38:19 UTC",
      "updated_date": "2025-08-15 11:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:10.683193+00:00"
    },
    {
      "arxiv_id": "2508.16628v1",
      "title": "The Impact of Artificial Intelligence on Human Thought",
      "title_zh": "äººå·¥æ™ºèƒ½å¯¹äººç±»æ€ç»´çš„å½±å“",
      "authors": [
        "RÃ©nald Gesnot"
      ],
      "abstract": "This research paper examines, from a multidimensional perspective (cognitive, social, ethical, and philosophical), how AI is transforming human thought. It highlights a cognitive offloading effect: the externalization of mental functions to AI can reduce intellectual engagement and weaken critical thinking. On the social level, algorithmic personalization creates filter bubbles that limit the diversity of opinions and can lead to the homogenization of thought and polarization. This research also describes the mechanisms of algorithmic manipulation (exploitation of cognitive biases, automated disinformation, etc.) that amplify AI's power of influence. Finally, the question of potential artificial consciousness is discussed, along with its ethical implications. The report as a whole underscores the risks that AI poses to human intellectual autonomy and creativity, while proposing avenues (education, transparency, governance) to align AI development with the interests of humanity.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»è®¤çŸ¥ã€ç¤¾ä¼šã€ä¼¦ç†å’Œå“²å­¦ç­‰å¤šç»´åº¦æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)å¦‚ä½•æ”¹å˜äººç±»æ€ç»´ã€‚ç ”ç©¶å¼ºè°ƒäº†è®¤çŸ¥å¸è½½(cognitive offloading)æ•ˆåº”ï¼Œå³å¿ƒç†åŠŸèƒ½å‘AIçš„å¤–éƒ¨åŒ–å¯èƒ½é™ä½æ™ºåŠ›å‚ä¸å¹¶å‰Šå¼±æ‰¹åˆ¤æ€§æ€ç»´(critical thinking)ã€‚åœ¨ç¤¾ä¼šå±‚é¢ï¼Œç®—æ³•ä¸ªæ€§åŒ–(algorithmic personalization)äº§ç”Ÿçš„è¿‡æ»¤æ°”æ³¡(filter bubbles)é™åˆ¶äº†æ„è§å¤šæ ·æ€§ï¼Œå¯¼è‡´æ€ç»´åŒè´¨åŒ–å’ŒæåŒ–ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ç®—æ³•æ“æ§(algorithmic manipulation)çš„æœºåˆ¶ï¼Œå¦‚åˆ©ç”¨è®¤çŸ¥åè§(cognitive biases)å’Œè‡ªåŠ¨åŒ–è™šå‡ä¿¡æ¯ï¼Œå¢å¼ºäº†AIçš„å½±å“åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è®¨è®ºäº†äººå·¥æ™ºèƒ½æ„è¯†(artificial consciousness)çš„å¯èƒ½æ€§åŠå…¶ä¼¦ç†å«ä¹‰ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æŠ¥å‘Šå¼ºè°ƒäº†AIå¯¹äººç±»æ™ºåŠ›è‡ªä¸»æƒ(intellectual autonomy)å’Œåˆ›é€ åŠ›æ„æˆçš„é£é™©ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†æ•™è‚²ã€é€æ˜åº¦å’Œæ²»ç†(governance)ç­‰é€”å¾„ï¼Œæ—¨åœ¨ä½¿AIçš„å‘å±•ä¸äººç±»åˆ©ç›Šä¿æŒä¸€è‡´ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "Research monograph; 132 pages; 13 figures; Version 1.0 (Aug 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.16628v1",
      "published_date": "2025-08-15 11:25:05 UTC",
      "updated_date": "2025-08-15 11:25:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:09.384154+00:00"
    },
    {
      "arxiv_id": "2508.11408v2",
      "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting",
      "title_zh": "å½“åœ¨çº¿å¼ºåŒ–å­¦ä¹ é‡è§ç¦»çº¿ä¸“å®¶ï¼šé€šè¿‡åŠ¨æ€åŠ æƒåè°ƒç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Wenhao Zhang",
        "Yuexiang Xie",
        "Yuchang Sun",
        "Yanxi Chen",
        "Guoyin Wang",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established response patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from the expert, which promotes on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on mathematical reasoning problems and practical tool-use tasks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to inspire further research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åè®­ç»ƒé˜¶æ®µä¸­ç›‘ç£å¾®è°ƒ(SFT)ä¸å¼ºåŒ–å­¦ä¹ (RL)çš„é›†æˆé—®é¢˜ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•æ˜“å¯¼è‡´å“åº”æ¨¡å¼ç ´åå’Œä¸“å®¶æ•°æ®è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œæå‡ºäº†ä»ç­–ç•¥å†…(on-policy)ä¸ç­–ç•¥å¤–(off-policy)è§†è§’å‡ºå‘çš„ç»Ÿä¸€æ¡†æ¶ã€‚ç ”ç©¶è€…è®¾è®¡äº†åä¸ºCHORDçš„ç³»ç»Ÿï¼Œé€šè¿‡åŠ¨æ€åŠ æƒå®ç°ä¸¤è€…çš„å¯æ§åè°ƒï¼Œå°†SFTé‡æ–°å®šä¹‰ä¸ºç­–ç•¥å†…RLè¿‡ç¨‹ä¸­çš„åŠ¨æ€åŠ æƒè¾…åŠ©ç›®æ ‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åŒé‡æ§åˆ¶æœºåˆ¶(dual-control mechanism)ï¼Œé¦–å…ˆåˆ©ç”¨å…¨å±€ç³»æ•°(global coefficient)ä»å®è§‚ä¸Šå¼•å¯¼ä»ç­–ç•¥å¤–æ¨¡ä»¿å‘ç­–ç•¥å†…æ¢ç´¢çš„è¿‡æ¸¡ã€‚å…¶æ¬¡ï¼ŒCHORDåº”ç”¨äº†æ ‡è®°çº§åŠ æƒå‡½æ•°(token-wise weighting function)ï¼Œåœ¨ç»†ç²’åº¦ä¸Šä»ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ ï¼Œä»è€Œä¿ƒè¿›ç­–ç•¥å†…æ¢ç´¢å¹¶å‡è½»ç­–ç•¥å¤–æ•°æ®çš„å¹²æ‰°ã€‚å®éªŒåœ¨æ•°å­¦æ¨ç†é—®é¢˜å’Œå®é™…å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å­¦ä¹ ç¨³å®šæ€§å’Œæ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç»“æœæ˜¾ç¤ºï¼ŒCHORDé€šè¿‡æœ‰æ•ˆåè°ƒç­–ç•¥å¤–ä¸“å®¶æ•°æ®ä¸ç­–ç•¥å†…æ¢ç´¢ï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œä¸ºLLMçš„è¡Œä¸ºå¯¹é½å’Œèƒ½åŠ›æå‡æä¾›äº†æ›´é«˜æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11408v2",
      "published_date": "2025-08-15 11:20:03 UTC",
      "updated_date": "2025-10-10 06:33:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:12.291094+00:00"
    },
    {
      "arxiv_id": "2508.11406v1",
      "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing",
      "title_zh": "åŸºäºè™šæ‹Ÿå®éªŒå®¤ä¸æ•°å­—å­ªç”Ÿæ‰§è¡Œè¿½è¸ªçš„å¼€æ”¾ã€å¯å¤ç°ä¸”å¯ä¿¡çš„æœºå™¨äººå®éªŒ",
      "authors": [
        "Benjamin Alt",
        "Mareike Picklum",
        "Sorin Arion",
        "Franklin Kenghagho Kenfack",
        "Michael Beetz"
      ],
      "abstract": "We envision a future in which autonomous robots conduct scientific experiments in ways that are not only precise and repeatable, but also open, trustworthy, and transparent. To realize this vision, we present two key contributions: a semantic execution tracing framework that logs sensor data together with semantically annotated robot belief states, ensuring that automated experimentation is transparent and replicable; and the AICOR Virtual Research Building (VRB), a cloud-based platform for sharing, replicating, and validating robot task executions at scale. Together, these tools enable reproducible, robot-driven science by integrating deterministic execution, semantic memory, and open knowledge representation, laying the foundation for autonomous systems to participate in scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ„å»ºå¼€æ”¾ã€å¯å¤ç°ä¸”å€¼å¾—ä¿¡èµ–çš„æœºå™¨äººç§‘å­¦å®éªŒä½“ç³»ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸¤é¡¹å…³é”®è´¡çŒ®ã€‚é¦–å…ˆæ˜¯è¯­ä¹‰æ‰§è¡Œè¿½è¸ªæ¡†æ¶(semantic execution tracing framework)ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŒæ—¶è®°å½•ä¼ æ„Ÿå™¨æ•°æ®ä¸è¯­ä¹‰æ ‡æ³¨çš„æœºå™¨äººä¿¡å¿µçŠ¶æ€ï¼Œç¡®ä¿äº†è‡ªåŠ¨åŒ–å®éªŒçš„é€æ˜åº¦ä¸å¯å¤åˆ¶æ€§ã€‚å…¶æ¬¡æ˜¯ AICOR è™šæ‹Ÿç ”ç©¶å¤§æ¥¼(AICOR Virtual Research Building, VRB)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è§„æ¨¡å…±äº«ã€å¤ç°å’ŒéªŒè¯æœºå™¨äººä»»åŠ¡æ‰§è¡Œè€Œè®¾è®¡çš„äº‘ç«¯å¹³å°ã€‚é€šè¿‡æ•´åˆç¡®å®šæ€§æ‰§è¡Œ(deterministic execution)ã€è¯­ä¹‰è®°å¿†(semantic memory)å’Œå¼€æ”¾çŸ¥è¯†è¡¨ç¤º(open knowledge representation)ï¼Œè¿™äº›å·¥å…·ä¸ºæœºå™¨äººé©±åŠ¨çš„å¯å¤ç°ç§‘å­¦æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚è¯¥é¡¹ç ”ç©¶æˆæœä¸ºè‡ªä¸»ç³»ç»Ÿå‚ä¸ç§‘å­¦å‘ç°å¥ å®šäº†åšå®åŸºç¡€ï¼Œæœ‰æ•ˆè§£å†³äº†æœºå™¨äººå®éªŒä¸­çš„ä¸€è‡´æ€§ä¸é€æ˜åŒ–éš¾é¢˜ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI and Robotics for Future Scientific Discovery",
      "pdf_url": "https://arxiv.org/pdf/2508.11406v1",
      "published_date": "2025-08-15 11:16:06 UTC",
      "updated_date": "2025-08-15 11:16:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:26.651289+00:00"
    },
    {
      "arxiv_id": "2508.11404v1",
      "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration",
      "title_zh": "åŸºäºäººæœºåä½œçš„æ··å‡åœŸè£‚ç¼æ£€æµ‹æ¢ç´¢æ€§ç ”ç©¶",
      "authors": [
        "Junyeon Kim",
        "Tianshu Ruan",
        "Cesar Alan Contreras",
        "Manolis Chiou"
      ],
      "abstract": "Structural inspection in nuclear facilities is vital for maintaining operational safety and integrity. Traditional methods of manual inspection pose significant challenges, including safety risks, high cognitive demands, and potential inaccuracies due to human limitations. Recent advancements in Artificial Intelligence (AI) and robotic technologies have opened new possibilities for safer, more efficient, and accurate inspection methodologies. Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms equipped with advanced detection algorithms, promises significant improvements in inspection outcomes and reductions in human workload. This study explores the effectiveness of AI-assisted visual crack detection integrated into a mobile Jackal robot platform. The experiment results indicate that HRC enhances inspection accuracy and reduces operator workload, resulting in potential superior performance outcomes compared to traditional manual methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ ¸è®¾æ–½ç»“æ„æ£€æŸ¥ä¸­ï¼Œé€šè¿‡äººæœºåä½œ(Human-Robot Collaboration, HRC)è¿›è¡Œæ··å‡åœŸè£‚ç¼æ£€æµ‹çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿäººå·¥æ£€æŸ¥é¢ä¸´çš„å®‰å…¨é£é™©ã€é«˜è®¤çŸ¥è´Ÿè·ä»¥åŠæ½œåœ¨çš„ä¸å‡†ç¡®æ€§ã€‚ç ”ç©¶é€šè¿‡åœ¨ç§»åŠ¨å¼Jackalæœºå™¨äººå¹³å°ä¸Šé›†æˆäººå·¥æ™ºèƒ½(Artificial Intelligence, AI)è¾…åŠ©çš„è§†è§‰è£‚ç¼æ£€æµ‹ç®—æ³•ï¼Œæ„å»ºäº†ä¸€å¥—äººæœºååŒçš„æ£€æµ‹æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHRCæ¨¡å¼æ˜¾è‘—æå‡äº†æ£€æŸ¥çš„å‡†ç¡®æ€§ï¼Œå¹¶æœ‰æ•ˆå‡è½»äº†æ“ä½œå‘˜çš„å·¥ä½œå¼ºåº¦ã€‚ä¸ä¼ ç»Ÿçš„äººå·¥æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿå±•ç°äº†æ›´ä¼˜è¶Šçš„æ€§èƒ½ç»“æœï¼Œä¸ºæå‡æ ¸èƒ½è®¾æ–½ç»“æ„å®Œæ•´æ€§ç›‘æµ‹çš„å®‰å…¨æ€§ä¸æ•ˆç‡æä¾›äº†å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11404v1",
      "published_date": "2025-08-15 11:13:07 UTC",
      "updated_date": "2025-08-15 11:13:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:27.748133+00:00"
    },
    {
      "arxiv_id": "2508.11398v2",
      "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis",
      "title_zh": "å¯ä¿¡ AI å¿ƒç†æ²»ç–—ï¼šé¢å‘å¿ƒç†å’¨è¯¢ä¸å¯è§£é‡Šæ€§ç²¾ç¥éšœç¢è¯Šæ–­çš„å¤šæ™ºèƒ½ä½“ LLM å·¥ä½œæµ",
      "authors": [
        "Mithat Can Ozgun",
        "Jiahuan Pei",
        "Koen Hindriks",
        "Lucia Donatelli",
        "Qingzhi Liu",
        "Junxiao Wang"
      ],
      "abstract": "LLM-based agents have emerged as transformative tools capable of executing complex tasks through iterative planning and action, achieving significant advancements in understanding and addressing user needs. Yet, their effectiveness remains limited in specialized domains such as mental health diagnosis, where they underperform compared to general applications. Current approaches to integrating diagnostic capabilities into LLMs rely on scarce, highly sensitive mental health datasets, which are challenging to acquire. These methods also fail to emulate clinicians' proactive inquiry skills, lack multi-turn conversational comprehension, and struggle to align outputs with expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1 diagnostic questionnaires. By simulating therapist-client dialogues with specific client profiles, the framework delivers transparent, step-by-step disorder predictions, producing explainable and trustworthy results. This workflow serves as a complementary tool for mental health diagnosis, ensuring adherence to ethical and legal standards. Through comprehensive experiments, we evaluate leading LLMs across three critical dimensions: conversational realism, diagnostic accuracy, and explainability. Our datasets and implementations are fully open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¿ƒç†å¥åº·è¯Šæ–­é¢†åŸŸé¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€ç¼ºä¹ä¸»åŠ¨è¯¢é—®èƒ½åŠ›ä»¥åŠéš¾ä»¥å¯¹é½ä¸´åºŠæ¨ç†ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªæ—¨åœ¨è‡ªä¸»ç”Ÿæˆ DSM-5 Level-1 è¯Šæ–­é—®å·çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµ DSM5AgentFlowã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿç‰¹å®šç”»åƒçš„æ²»ç–—å¸ˆä¸æ¥è®¿è€…å¯¹è¯ï¼Œèƒ½å¤Ÿæä¾›é€æ˜ä¸”é€æ­¥çš„å¿ƒç†éšœç¢é¢„æµ‹ï¼Œç”Ÿæˆå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„è¯Šæ–­ç»“æœã€‚ä½œä¸ºå¿ƒç†å¥åº·è¯Šæ–­çš„è¾…åŠ©å·¥å…·ï¼ŒDSM5AgentFlow ç¡®ä¿äº†è¯Šæ–­è¿‡ç¨‹ç¬¦åˆä¼¦ç†å’Œæ³•å¾‹æ ‡å‡†ã€‚å®éªŒä»å¯¹è¯çœŸå®æ€§ã€è¯Šæ–­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°äº†ä¸»æµ LLMs çš„è¡¨ç°ï¼Œä¸”è¯¥ç ”ç©¶çš„ç›¸å…³æ•°æ®é›†å’Œä»£ç å®ç°å·²å®Œå…¨å¼€æºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper has been accepted by CIKM 2025 as a full paper",
      "pdf_url": "https://arxiv.org/pdf/2508.11398v2",
      "published_date": "2025-08-15 11:08:32 UTC",
      "updated_date": "2025-08-23 11:38:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:25.272422+00:00"
    },
    {
      "arxiv_id": "2508.14916v1",
      "title": "Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge",
      "title_zh": "Transsion é¢å‘ MLC-SLM 2025 æŒ‘æˆ˜èµ›çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ",
      "authors": [
        "Xiaoxiao Li",
        "An Zhu",
        "Youhai Jiang",
        "Fengjie Zhu"
      ],
      "abstract": "This paper presents the architecture and performance of a novel Multilingual Automatic Speech Recognition (ASR) system developed by the Transsion Speech Team for Track 1 of the MLC-SLM 2025 Challenge. The proposed system comprises three key components: 1) a frozen Whisper-large-v3 based speech encoder, leveraging large-scale pretraining to ensure robust acoustic feature extraction; 2) a trainable adaptor module using Linear-ReLU-Linear transformation mechanisms to effectively align speech and text representations; and 3) a frozen Qwen2.5-7B-Instruct large language model (LLM) integrated with trainable LoRA for optimized contextual linguistic decoding. By systematically combining pretrained models with task specific fine-tuning, the system achieved a word/character error rate (WER/CER) of 9.83% across 11 languages in the evaluation set and ranked third place among global participants.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ç”±Transsionè¯­éŸ³å›¢é˜Ÿä¸ºMLC-SLM 2025æŒ‘æˆ˜èµ›Track 1å¼€å‘çš„ä¸€ç§æ–°å‹å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ¶æ„åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆæ˜¯åˆ©ç”¨å†»ç»“çš„Whisper-large-v3ä½œä¸ºè¯­éŸ³ç¼–ç å™¨ï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒç¡®ä¿ç¨³å¥çš„å£°å­¦ç‰¹å¾æå–ï¼›å…¶æ¬¡æ˜¯é‡‡ç”¨Linear-ReLU-Linearè½¬æ¢æœºåˆ¶çš„å¯è®­ç»ƒé€‚é…å™¨(adaptor)æ¨¡å—ï¼Œæ—¨åœ¨æœ‰æ•ˆå¯¹é½è¯­éŸ³ä¸æ–‡æœ¬è¡¨ç¤ºï¼›æœ€åé›†æˆå†»ç»“çš„Qwen2.5-7B-Instructå¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œå¹¶ç»“åˆå¯è®­ç»ƒçš„LoRAæŠ€æœ¯ä¼˜åŒ–ä¸Šä¸‹æ–‡è¯­è¨€è§£ç ã€‚é€šè¿‡å°†é¢„è®­ç»ƒæ¨¡å‹ä¸ç‰¹å®šä»»åŠ¡å¾®è°ƒç›¸ç»“åˆï¼Œè¯¥ç³»ç»Ÿåœ¨è¯„ä¼°é›†çš„11ç§è¯­è¨€ä¸­å®ç°äº†9.83%çš„è¯/å­—ç¬¦é”™è¯¯ç‡(WER/CER)ã€‚è¯¥æˆæœæœ€ç»ˆåœ¨å…¨çƒå‚èµ›é˜Ÿä¼ä¸­è£è·ç¬¬ä¸‰åï¼Œå±•ç¤ºäº†è¯¥æ¶æ„åœ¨å¤„ç†å¤æ‚å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14916v1",
      "published_date": "2025-08-15 10:39:05 UTC",
      "updated_date": "2025-08-15 10:39:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:25.826090+00:00"
    },
    {
      "arxiv_id": "2508.11386v1",
      "title": "Retrieval-augmented reasoning with lean language models",
      "title_zh": "åŸºäºç²¾ç®€è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¢å¼ºæ¨ç†",
      "authors": [
        "Ryan Sze-Yin Chan",
        "Federico Nanni",
        "Tomas Lazauskas",
        "Rosie Wood",
        "Penelope Yong",
        "Lionel Tarassenko",
        "Mark Girolami",
        "James Geddes",
        "Andrew Duncan"
      ],
      "abstract": "This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, our work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, we develop a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. Our system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. We explore the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that our domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. All implementation details and code are publicly released to support reproducibility and adaptation across domains.",
      "tldr_zh": "è¯¥æŠ€æœ¯æŠ¥å‘Šæå‡ºäº†ä¸€ç§åœ¨è½»é‡åŒ–è¯­è¨€æ¨¡å‹ (lean language model) æ¶æ„ä¸­ç»“åˆæ¨ç†èƒ½åŠ›ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³èµ„æºå—é™æˆ–é«˜å®‰å…¨æ€§ç¯å¢ƒä¸‹å¯¹éšç§ä¿æŠ¤å’Œé«˜æ€§èƒ½æ¨¡å‹çš„éœ€æ±‚ã€‚ç ”ç©¶å›¢é˜Ÿå°†ç¨ å¯†æ£€ç´¢å™¨ (dense retriever) ä¸ç»è¿‡å¾®è°ƒçš„ Qwen2.5-Instruct æ¨¡å‹ç›¸é›†æˆï¼Œå¹¶åˆ©ç”¨æ¥è‡ª DeepSeek-R1 ç­‰å‰æ²¿æ¨¡å‹çš„åˆæˆæŸ¥è¯¢ä¸æ¨ç†ç—•è¿¹ (reasoning traces) åœ¨ NHS A-to-Z ç­‰è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥å·¥ä½œæ·±å…¥æ¢è®¨äº†åŸºäºæ‘˜è¦çš„æ–‡æ¡£å‹ç¼©ã€åˆæˆæ•°æ®è®¾è®¡ä»¥åŠæ¨ç†æ„ŸçŸ¥å¾®è°ƒ (reasoning-aware fine-tuning) å¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ–¹æ³•åœ¨ç­”æ¡ˆå‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§æå‡ï¼Œæ˜¾è‘—ä¼˜äºéæ¨ç†ç±»å’Œé€šç”¨å‹è½»é‡çº§æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿåœ¨ä¿æŒæœ¬åœ°éƒ¨ç½²å¯è¡Œæ€§çš„å‰æä¸‹ï¼Œå…¶æ€§èƒ½è¡¨ç°å·²æ¥è¿‘å‰æ²¿å¤§æ¨¡å‹æ°´å¹³ï¼Œä¸ºå„é¢†åŸŸçš„æ¨¡å‹å¤ç°ä¸é€‚é…æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11386v1",
      "published_date": "2025-08-15 10:38:15 UTC",
      "updated_date": "2025-08-15 10:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:33.589564+00:00"
    },
    {
      "arxiv_id": "2508.11383v1",
      "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs",
      "title_zh": "æ ‡ç‚¹ç¬¦å·çš„é‡è¦æ€§ï¼šå¤§è¯­è¨€æ¨¡å‹æç¤ºè¯é²æ£’æ€§æ–¹æ³•çš„å¤§è§„æ¨¡æ¯”è¾ƒ",
      "authors": [
        "Mikhail Seleznyov",
        "Mikhail Chaichuk",
        "Gleb Ershov",
        "Alexander Panchenko",
        "Elena Tutubalina",
        "Oleg Somov"
      ],
      "abstract": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/when-punctuation-matters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æç¤ºè¯ï¼ˆPromptï¼‰ä¸­å¾®å°çš„éè¯­ä¹‰å˜åŒ–ï¼ˆå¦‚æ ‡ç‚¹å’Œæ ¼å¼ï¼‰å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…åœ¨ç»Ÿä¸€å®éªŒæ¡†æ¶ä¸‹é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†5ç§æ—¨åœ¨æå‡æç¤ºè¯é²æ£’æ€§ï¼ˆPrompt Robustnessï¼‰çš„æ–¹æ³•ã€‚ç ”ç©¶åœ¨ Natural Instructions æ•°æ®é›†çš„52ä¸ªä»»åŠ¡ä¸Šï¼Œé’ˆå¯¹ Llamaã€Qwen å’Œ Gemma ç³»åˆ—çš„8ä¸ªæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¾®è°ƒï¼ˆFine-tunedï¼‰å’Œè¯­å¢ƒå­¦ä¹ ï¼ˆIn-context learningï¼‰ä¸¤ç§èŒƒå¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æµ‹è¯•äº†è¿™äº›æ–¹æ³•åœ¨ä¸åŒåˆ†å¸ƒåç§»ï¼ˆDistribution shiftsï¼‰ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è¿›ä¸€æ­¥åˆ†æäº† GPT-4.1 å’Œ DeepSeek V3 ç­‰å‰æ²¿æ¨¡å‹å¯¹æ ¼å¼æ‰°åŠ¨çš„æŠ—å¹²æ‰°èƒ½åŠ›ã€‚è¯¥ç ”ç©¶çš„å‘ç°ä¸ºä»ä¸šè€…æä¾›äº†å…·æœ‰å®é™…æ“ä½œæ„ä¹‰çš„è§è§£ï¼Œæœ‰åŠ©äºåœ¨ç°å®åº”ç”¨åœºæ™¯ä¸­å®ç°æ›´ç¨³å®šã€å¯é çš„ LLM æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11383v1",
      "published_date": "2025-08-15 10:32:50 UTC",
      "updated_date": "2025-08-15 10:32:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:52.659978+00:00"
    },
    {
      "arxiv_id": "2508.11379v2",
      "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration",
      "title_zh": "G-CUT3Rï¼šé›†æˆç›¸æœºä¸æ·±åº¦å…ˆéªŒçš„å¼•å¯¼å¼ä¸‰ç»´é‡å»º",
      "authors": [
        "Ramil Khafizov",
        "Artem Komarichev",
        "Ruslan Rakhimov",
        "Peter Wonka",
        "Evgeny Burnaev"
      ],
      "abstract": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene reconstruction that enhances the CUT3R model by integrating prior information. Unlike existing feed-forward methods that rely solely on input images, our method leverages auxiliary data, such as depth, camera calibrations, or camera positions, commonly available in real-world scenarios. We propose a lightweight modification to CUT3R, incorporating a dedicated encoder for each modality to extract features, which are fused with RGB image tokens via zero convolution. This flexible design enables seamless integration of any combination of prior information during inference. Evaluated across multiple benchmarks, including 3D reconstruction and other multi-view tasks, our approach demonstrates significant performance improvements, showing its ability to effectively utilize available priors while maintaining compatibility with varying input modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†G-CUT3Rï¼Œä¸€ç§ç”¨äºå¼•å¯¼å¼3Dåœºæ™¯é‡å»ºçš„æ–°å‹å‰é¦ˆ(feed-forward)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆå…ˆéªŒä¿¡æ¯å¢å¼ºCUT3Ræ¨¡å‹ã€‚ä¸ä»…ä¾èµ–è¾“å…¥å›¾åƒçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒG-CUT3Ræœ‰æ•ˆåˆ©ç”¨äº†æ·±åº¦(depth)ã€ç›¸æœºæ ¡å‡†(camera calibrations)æˆ–ç›¸æœºä½ç½®(camera positions)ç­‰è¾…åŠ©æ•°æ®ã€‚ç ”ç©¶è€…å¯¹CUT3Rè¿›è¡Œäº†è½»é‡åŒ–æ”¹è¿›ï¼Œä¸ºæ¯ç§æ¨¡æ€é…å¤‡ä¸“ç”¨ç¼–ç å™¨(encoder)æå–ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨é›¶å·ç§¯(zero convolution)å°†å…¶ä¸RGBå›¾åƒæ ‡è®°(tokens)è¿›è¡Œèåˆã€‚è¿™ç§çµæ´»çš„è®¾è®¡å…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— ç¼é›†æˆä»»ä½•å…ˆéªŒä¿¡æ¯ç»„åˆã€‚åœ¨3Dé‡å»º(3D reconstruction)åŠå¤šè§†å›¾(multi-view)ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ä¿æŒæ¨¡æ€å…¼å®¹æ€§çš„åŒæ—¶åˆ©ç”¨å…ˆéªŒçŸ¥è¯†çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11379v2",
      "published_date": "2025-08-15 10:25:58 UTC",
      "updated_date": "2025-09-29 07:37:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:53.544301+00:00"
    },
    {
      "arxiv_id": "2508.15811v2",
      "title": "From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System",
      "title_zh": "ä»ç‚¹å‡»åˆ°åå¥½ï¼šå¯¹è¯ç³»ç»Ÿç”Ÿæˆå¼æŸ¥è¯¢å»ºè®®çš„å¤šé˜¶æ®µå¯¹é½æ¡†æ¶",
      "authors": [
        "Junhao Yin",
        "Haolin Wang",
        "Peng Bao",
        "Ju Xu",
        "Yongliang Wang"
      ],
      "abstract": "Generative query suggestion using large language models offers a powerful way to enhance conversational systems, but aligning outputs with nuanced user preferences remains a critical challenge. To address this, we introduce a multi-stage framework designed for progressive alignment between the generation policy and user intent. Our pipeline begins with prompt engineering as a cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we introduce a distillation method on click logs to create a robust foundational model. To better model user preferences while capturing their inherent uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user preferences as probability distributions rather than point estimates. Finally, we employ reinforcement learning to align the generation policy with these preferences, guided by a composite reward function that integrates GaRM with auxiliary heuristics to mitigate reward hacking. To maintain training stability, this process is enhanced by a novel out-of-distribution regularization method and a two-stage reward fusion technique. Extensive experiments demonstrate that our framework significantly outperforms baselines on both automatic and human evaluations and yields a 34\\% relative increase in user engagement as measured by click-through rate in live A/B tests.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºFrom Clicks to Preferenceçš„å¤šé˜¶æ®µå¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼æŸ¥è¯¢å»ºè®®(Generative Query Suggestion)åœ¨å¯¹è¯ç³»ç»Ÿä¸­ä¸ç»†å¾®ç”¨æˆ·åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆé‡‡ç”¨æç¤ºå·¥ç¨‹(Prompt Engineering)è¿›è¡Œå†·å¯åŠ¨ï¼Œéšååœ¨ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)é˜¶æ®µé€šè¿‡ç‚¹å‡»æ—¥å¿—(Click Logs)çš„è’¸é¦æ–¹æ³•æ„å»ºåŸºç¡€æ¨¡å‹ã€‚ä¸ºäº†ç²¾å‡†å»ºæ¨¡ç”¨æˆ·åå¥½åŠå…¶å›ºæœ‰çš„ä¸ç¡®å®šæ€§ï¼Œç ”ç©¶è€…å¼€å‘äº†é«˜æ–¯å¥–åŠ±æ¨¡å‹(Gaussian Reward Model, GaRM)ï¼Œå°†åå¥½è¡¨ç¤ºä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚éšåï¼Œæ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¼•å¯¼ç”Ÿæˆç­–ç•¥ä¸ç”¨æˆ·æ„å›¾å¯¹é½ï¼Œå¹¶å¼•å…¥åˆ†å¸ƒå¤–æ­£åˆ™åŒ–(Out-of-Distribution Regularization)å’Œä¸¤é˜¶æ®µå¥–åŠ±èåˆæŠ€æœ¯ä»¥ç»´æŒè®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªåŠ¨ä¸äººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨åœ¨çº¿A/Bæµ‹è¯•(Live A/B Tests)ä¸­ä½¿ç‚¹å‡»ç‡(Click-Through Rate)æå‡äº†34%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 26)",
      "pdf_url": "https://arxiv.org/pdf/2508.15811v2",
      "published_date": "2025-08-15 10:17:01 UTC",
      "updated_date": "2025-12-15 12:51:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:05:52.891338+00:00"
    },
    {
      "arxiv_id": "2508.11374v1",
      "title": "Does the Skeleton-Recall Loss Really Work?",
      "title_zh": "Skeleton-Recall æŸå¤±çœŸçš„æœ‰æ•ˆå—ï¼Ÿ",
      "authors": [
        "Devansh Arora",
        "Nitin Kumar",
        "Sukrit Gupta"
      ],
      "abstract": "Image segmentation is an important and widely performed task in computer vision. Accomplishing effective image segmentation in diverse settings often requires custom model architectures and loss functions. A set of models that specialize in segmenting thin tubular structures are topology preservation-based loss functions. These models often utilize a pixel skeletonization process claimed to generate more precise segmentation masks of thin tubes and better capture the structures that other models often miss. One such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite {kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark tubular datasets. In this work, we performed a theoretical analysis of the gradients for the SRL loss. Upon comparing the performance of the proposed method on some of the tubular datasets (used in the original work, along with some additional datasets), we found that the performance of SRL-based segmentation models did not exceed traditional baseline models. By providing both a theoretical explanation and empirical evidence, this work critically evaluates the limitations of topology-based loss functions, offering valuable insights for researchers aiming to develop more effective segmentation models for complex tubular structures.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ç”¨äºè–„ç®¡çŠ¶ç»“æ„å›¾åƒåˆ†å‰²çš„ Skeleton Recall Loss (SRL) æŸå¤±å‡½æ•°è¿›è¡Œäº†æ‰¹åˆ¤æ€§è¯„ä¼°ä¸éªŒè¯ã€‚ä½œè€…å¯¹ SRL çš„æ¢¯åº¦è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æï¼Œå¹¶åœ¨åŸè®ºæ–‡ä½¿ç”¨çš„ç®¡çŠ¶æ•°æ®é›†åŠé¢å¤–æ•°æ®é›†ä¸Šï¼Œå°†è¯¥æ¨¡å‹ä¸ä¼ ç»ŸåŸºçº¿æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒSRL åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å¹¶æœªå¦‚é¢„æœŸèˆ¬è¶…è¶Šä¼ ç»Ÿçš„åŸºçº¿æ¨¡å‹ï¼ŒæŒ‘æˆ˜äº†å…¶æ­¤å‰å®£ç§°çš„ state-of-the-art è¡¨ç°ã€‚é€šè¿‡ç»“åˆç†è®ºè§£é‡Šå’Œç»éªŒè¯æ®ï¼Œè¯¥å·¥ä½œç³»ç»Ÿåœ°åˆ†æäº†æ­¤ç±»åŸºäºæ‹“æ‰‘ä¿æŒæŸå¤±å‡½æ•°çš„å±€é™æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæœªæ¥å¼€å‘é’ˆå¯¹å¤æ‚ç®¡çŠ¶ç»“æ„çš„é«˜æ•ˆåˆ†å‰²æ¨¡å‹æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11374v1",
      "published_date": "2025-08-15 10:16:34 UTC",
      "updated_date": "2025-08-15 10:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:01.103808+00:00"
    },
    {
      "arxiv_id": "2509.09683v1",
      "title": "Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs",
      "title_zh": "æ•°å­—å¹¿å‘Šç‚¹å‡»é‡é¢„æµ‹ï¼šå¤šæ¨¡æ€è¾“å…¥ä¸å¯è§£é‡Šæ€§è¾“å‡º",
      "authors": [
        "Briti Gangopadhyay",
        "Zhao Wang",
        "Shingo Takamatsu"
      ],
      "abstract": "Forecasting click volume is a key task in digital advertising, influencing both revenue and campaign strategy. Traditional time series models rely solely on numerical data, often overlooking rich contextual information embedded in textual elements, such as keyword updates. We present a multimodal forecasting framework that combines click data with textual logs from real-world ad campaigns and generates human-interpretable explanations alongside numeric predictions. Reinforcement learning is used to improve comprehension of textual information and enhance fusion of modalities. Experiments on a large-scale industry dataset show that our method outperforms baselines in both accuracy and reasoning quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°å­—å¹¿å‘Šä¸­ç‚¹å‡»é‡é¢„æµ‹(Click volume forecasting)çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªæ•´åˆæ•°å€¼ç‚¹å‡»æ•°æ®ä¸å¹¿å‘Šæ´»åŠ¨æ–‡æœ¬æ—¥å¿—(Textual logs)çš„å¤šæ¨¡æ€é¢„æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…‹æœäº†ä¼ ç»Ÿæ—¶é—´åºåˆ—æ¨¡å‹ä»…ä¾èµ–æ•°å€¼ã€å¿½è§†å…³é”®è¯æ›´æ–°ç­‰æ–‡æœ¬ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å±€é™ï¼Œèƒ½å¤ŸåŒæ­¥ç”Ÿæˆæ•°å€¼é¢„æµ‹ä¸äººç±»å¯è§£é‡Šçš„è¯´æ˜ã€‚é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement learning)æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†å¯¹æ–‡æœ¬ä¿¡æ¯çš„ç†è§£æ·±åº¦åŠå¤šæ¨¡æ€èåˆ(Fusion of modalities)çš„æ•ˆæœã€‚åœ¨å¤§è§„æ¨¡å·¥ä¸šæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸æ¨ç†è´¨é‡(Reasoning quality)æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºå¹¿å‘Šæ´»åŠ¨çš„æ”¶å…¥ä¼˜åŒ–ä¸ç­–ç•¥åˆ¶å®šæä¾›äº†æ›´ä¸ºç²¾å‡†ä¸”å…·æ´å¯ŸåŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09683v1",
      "published_date": "2025-08-15 10:01:53 UTC",
      "updated_date": "2025-08-15 10:01:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:03.853610+00:00"
    },
    {
      "arxiv_id": "2508.11365v2",
      "title": "Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization",
      "title_zh": "åŸºäºå¯å¾®ä¼˜åŒ–çš„å†³ç­–èšç„¦å­¦ä¹ ä»£ç†æŸå¤±æœ€å°åŒ–",
      "authors": [
        "Jayanta Mandi",
        "Ali Ä°rfan MahmutoÄŸullarÄ±",
        "Senne Berden",
        "Tias Guns"
      ],
      "abstract": "Decision-focused learning (DFL) trains a machine learning (ML) model to predict parameters of an optimization problem, to directly minimize decision regret, i.e., maximize decision quality. Gradient-based DFL requires computing the derivative of the solution to the optimization problem with respect to the predicted parameters. However, for many optimization problems, such as linear programs (LPs), the gradient of the regret with respect to the predicted parameters is zero almost everywhere. Existing gradient-based DFL approaches for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP into a differentiable optimization problem by adding a quadratic regularizer and then minimizing the regret directly or (b) minimizing surrogate losses that have informative (sub)gradients. In this paper, we show that the former approach still results in zero gradients, because even after smoothing the regret remains constant across large regions of the parameter space. To address this, we propose minimizing surrogate losses -- even when a differentiable optimization layer is used and regret can be minimized directly. Our experiments demonstrate that minimizing surrogate losses allows differentiable optimization layers to achieve regret comparable to or better than surrogate-loss based DFL methods. Further, we demonstrate that this also holds for DYS-Net, a recently proposed differentiable optimization technique for LPs, that computes approximate solutions and gradients through operations that can be performed using feedforward neural network layers. Because DYS-Net executes the forward and the backward pass very efficiently, by minimizing surrogate losses using DYS-Net, we are able to attain regret on par with the state-of-the-art while reducing training time by a significant margin.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å†³ç­–èšç„¦å­¦ä¹ (Decision-focused learning)ä¸­çš„æ¢¯åº¦é—®é¢˜ï¼Œå³åœ¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹çº¿æ€§è§„åˆ’(Linear programs)å‚æ•°æ—¶ï¼Œå†³ç­–é—æ†¾(Regret)çš„æ¢¯åº¦åœ¨å¤§éƒ¨åˆ†åŒºåŸŸå‡ ä¹ä¸ºé›¶ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„é€šè¿‡æ·»åŠ äºŒæ¬¡æ­£åˆ™é¡¹å¯¹çº¿æ€§è§„åˆ’è¿›è¡Œå¹³æ»‘å¤„ç†çš„æ–¹æ³•ä»ç„¶å­˜åœ¨ç¼ºé™·ï¼Œå› ä¸ºå¹³æ»‘åçš„é—æ†¾åœ¨å¹¿é˜”çš„å‚æ•°ç©ºé—´å†…ä¾ç„¶ä¿æŒæ’å®šã€‚ä¸ºæ­¤ï¼Œè¯¥è®ºæ–‡æå‡ºå³ä½¿åœ¨ä½¿ç”¨å¯å¾®ä¼˜åŒ–å±‚(Differentiable optimization layer)æ—¶ï¼Œä¹Ÿåº”é‡‡ç”¨æœ€å°åŒ–ä»£ç†æŸå¤±(Surrogate losses)çš„ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæœ€å°åŒ–ä»£ç†æŸå¤±èƒ½è®©å¯å¾®ä¼˜åŒ–å±‚è·å¾—ä¸åŸºäºä»£ç†æŸå¤±çš„DFLæ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†æ­¤æ–¹æ³•åº”ç”¨äºDYS-Netè¿™ä¸€é«˜æ•ˆçš„å¯å¾®ä¼˜åŒ–æŠ€æœ¯ï¼Œåˆ©ç”¨å…¶ç±»ä¼¼å‰é¦ˆç¥ç»ç½‘ç»œçš„å¿«é€Ÿå‰é¦ˆå’Œåé¦ˆæ“ä½œã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸å½“å‰æœ€å…ˆè¿›æ°´å¹³(State-of-the-art)ç›¸å½“çš„å†³ç­–è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—ç¼©çŸ­äº†æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11365v2",
      "published_date": "2025-08-15 09:59:56 UTC",
      "updated_date": "2025-08-25 12:45:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:07.193487+00:00"
    },
    {
      "arxiv_id": "2508.11729v1",
      "title": "The Stories We Govern By: AI, Risk, and the Power of Imaginaries",
      "title_zh": "æ²»ç†èƒŒåçš„å™äº‹ï¼šäººå·¥æ™ºèƒ½ã€é£é™©ä¸ç¤¾ä¼šæƒ³è±¡çš„åŠ›é‡",
      "authors": [
        "Ninell Oldenburg",
        "Gleb Papyshev"
      ],
      "abstract": "This paper examines how competing sociotechnical imaginaries of artificial intelligence (AI) risk shape governance decisions and regulatory constraints. Drawing on concepts from science and technology studies, we analyse three dominant narrative groups: existential risk proponents, who emphasise catastrophic AGI scenarios; accelerationists, who portray AI as a transformative force to be unleashed; and critical AI scholars, who foreground present-day harms rooted in systemic inequality. Through an analysis of representative manifesto-style texts, we explore how these imaginaries differ across four dimensions: normative visions of the future, diagnoses of the present social order, views on science and technology, and perceived human agency in managing AI risks. Our findings reveal how these narratives embed distinct assumptions about risk and have the potential to progress into policy-making processes by narrowing the space for alternative governance approaches. We argue against speculative dogmatism and for moving beyond deterministic imaginaries toward regulatory strategies that are grounded in pragmatism.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)é£é™©çš„ç«äº‰æ€§ç¤¾ä¼šæŠ€æœ¯æƒ³è±¡(sociotechnical imaginaries)å¦‚ä½•å¡‘é€ æ²»ç†å†³ç­–å’Œç›‘ç®¡çº¦æŸã€‚ä½œè€…åˆ†æäº†ä¸‰ç±»ä¸»å¯¼å™äº‹ï¼šå¼ºè°ƒç¾éš¾æ€§AGIåœºæ™¯çš„å­˜åœ¨æ€§é£é™©(existential risk)æ”¯æŒè€…ã€è§†AIä¸ºå˜é©åŠ›é‡çš„åŠ é€Ÿä¸»ä¹‰è€…(accelerationists)ï¼Œä»¥åŠå…³æ³¨ç”±ç³»ç»Ÿæ€§ä¸å¹³ç­‰å¼•å‘çš„ç°å®ä¼¤å®³çš„æ‰¹åˆ¤æ€§AIå­¦è€…ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”è¿™äº›å™äº‹åœ¨æœªæ¥æ„¿æ™¯ã€ç¤¾ä¼šç°çŠ¶è¯Šæ–­ã€ç§‘æŠ€è§‚åŠäººç±»èƒ½åŠ¨æ€§å››ä¸ªç»´åº¦çš„å·®å¼‚ï¼Œæ­ç¤ºäº†å®ƒä»¬å¦‚ä½•é€šè¿‡åµŒå…¥ç‰¹å®šçš„é£é™©å‡è®¾æ¥æ”¶ç¼©æ›¿ä»£æ€§æ²»ç†æ–¹æ¡ˆçš„ç©ºé—´ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æƒ³è±¡æ­£é€æ¸æ¸—å…¥æ”¿ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œå…·æœ‰æ˜æ˜¾çš„å†³å®šè®ºå€¾å‘ã€‚ä½œè€…æœ€ç»ˆå‘¼åæ‘’å¼ƒæ€è¾¨æ•™æ¡ä¸»ä¹‰(speculative dogmatism)ï¼Œè½¬å‘æ›´ä¸ºåŠ¡å®(pragmatism)ä¸”å…·æœ‰é’ˆå¯¹æ€§çš„ç›‘ç®¡ç­–ç•¥ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages, accepted at the 8th AAAI/ACM Conference on AI, Ethics, and Society",
      "pdf_url": "https://arxiv.org/pdf/2508.11729v1",
      "published_date": "2025-08-15 09:57:56 UTC",
      "updated_date": "2025-08-15 09:57:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:09.361598+00:00"
    },
    {
      "arxiv_id": "2508.11360v1",
      "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks",
      "title_zh": "CRAFT-GUIï¼šé¢å‘å›¾å½¢ç”¨æˆ·ç•Œé¢ä»»åŠ¡çš„è¯¾ç¨‹å¼ºåŒ–æ™ºèƒ½ä½“",
      "authors": [
        "Songqin Nong",
        "Jingxuan Xu",
        "Sheng Zhou",
        "Jianfeng Chen",
        "Xiaoxuan Tang",
        "Tao Jiang",
        "Wenhao Xu"
      ],
      "abstract": "As autonomous agents become adept at understanding and interacting with graphical user interface (GUI) environments, a new era of automated task execution is emerging. Recent studies have demonstrated that Reinforcement Learning (RL) can effectively enhance agents' performance in dynamic interactive GUI environments. However, these methods face two key limitations: (1) they overlook the significant variation in difficulty across different GUI tasks by treating the entire training data as a uniform set, which hampers the agent's ability to adapt its learning process; and (2) most approaches collapse task-specific nuances into a single, coarse reward, leaving the agent with a uniform signal that yields inefficient policy updates. To address these limitations, we propose CRAFT-GUI, a curriculum learning framework based on Group Relative Policy Optimization (GRPO) that explicitly accounts for the varying difficulty across trajectories. To enable more fine-grained policy optimization, we design a reward function that combines simple rule-based signals with model-judged evaluation, providing richer and more nuanced feedback during training. Experimental results demonstrate that our method achieves significant improvements over previous state-of-the-art approaches, outperforming them by 5.6% on public benchmarks Android Control and 10.3% on our internal online benchmarks, respectively. These findings empirically validate the effectiveness of integrating reinforcement learning with curriculum learning in GUI interaction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CRAFT-GUIï¼Œä¸€ä¸ªç»“åˆäº†è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½ä½“åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)ç¯å¢ƒä¸­çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½è§†ä»»åŠ¡éš¾åº¦å·®å¼‚ä»¥åŠå¥–åŠ±æœºåˆ¶è¿‡äºç²—ç•¥çš„é—®é¢˜ï¼ŒCRAFT-GUIèƒ½å¤Ÿæ ¹æ®è½¨è¿¹éš¾åº¦åŠ¨æ€è°ƒæ•´å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§èåˆè§„åˆ™ä¿¡å·ä¸æ¨¡å‹è¯„ä¼°(Model-judged evaluation)çš„å¤šç»´åº¦å¥–åŠ±å‡½æ•°ã€‚è¿™ç§æœºåˆ¶ä¸ºæ™ºèƒ½ä½“æä¾›äº†æ›´ç»†ç²’åº¦çš„åé¦ˆï¼Œæ˜¾è‘—æé«˜äº†ç­–ç•¥æ›´æ–°çš„æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å…¬å¼€åŸºå‡†Android Controlå’Œå†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯”å½“å‰æœ€ä¼˜æŠ€æœ¯æå‡äº†5.6%å’Œ10.3%ã€‚è¯¥ç ”ç©¶æˆæœå……åˆ†è¯æ˜äº†åœ¨GUIäº¤äº’ä»»åŠ¡ä¸­æ•´åˆå¼ºåŒ–å­¦ä¹ ä¸è¯¾ç¨‹å­¦ä¹ çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11360v1",
      "published_date": "2025-08-15 09:55:02 UTC",
      "updated_date": "2025-08-15 09:55:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:16.185819+00:00"
    },
    {
      "arxiv_id": "2508.11357v1",
      "title": "PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding",
      "title_zh": "PTSMï¼šé¢å‘è·¨è¢«è¯•è„‘ç”µè§£ç çš„ç”Ÿç†æ„ŸçŸ¥ä¸ä»»åŠ¡ä¸å˜æ€§æ—¶ç©ºå»ºæ¨¡",
      "authors": [
        "Changhong Jing",
        "Yan Liu",
        "Shuqiang Wang",
        "Bruce X. B. Yu",
        "Gong Chen",
        "Zhejing Hu",
        "Zhi Zhang",
        "Yanyan Shen"
      ],
      "abstract": "Cross-subject electroencephalography (EEG) decoding remains a fundamental challenge in brain-computer interface (BCI) research due to substantial inter-subject variability and the scarcity of subject-invariant representations. This paper proposed PTSM (Physiology-aware and Task-invariant Spatio-temporal Modeling), a novel framework for interpretable and robust EEG decoding across unseen subjects. PTSM employs a dual-branch masking mechanism that independently learns personalized and shared spatio-temporal patterns, enabling the model to preserve individual-specific neural characteristics while extracting task-relevant, population-shared features. The masks are factorized across temporal and spatial dimensions, allowing fine-grained modulation of dynamic EEG patterns with low computational overhead. To further address representational entanglement, PTSM enforces information-theoretic constraints that decompose latent embeddings into orthogonal task-related and subject-related subspaces. The model is trained end-to-end via a multi-objective loss integrating classification, contrastive, and disentanglement objectives. Extensive experiments on cross-subject motor imagery datasets demonstrate that PTSM achieves strong zero-shot generalization, outperforming state-of-the-art baselines without subject-specific calibration. Results highlight the efficacy of disentangled neural representations for achieving both personalized and transferable decoding in non-stationary neurophysiological settings.",
      "tldr_zh": "è·¨å—è¯•è€…è„‘ç”µå›¾(EEG)è§£ç ç”±äºå—è¯•è€…é—´çš„æ˜¾è‘—å·®å¼‚å’Œç¼ºä¹å—è¯•è€…ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºï¼Œä¸€ç›´æ˜¯è„‘æœºæ¥å£(BCI)é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†PTSMï¼ˆPhysiology-aware and Task-invariant Spatio-temporal Modelingï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è·¨æœªçŸ¥å—è¯•è€…çš„ç¨³å¥ä¸”å¯è§£é‡Šçš„EEGè§£ç ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒåˆ†æ”¯æ©ç æœºåˆ¶(dual-branch masking mechanism)ç‹¬ç«‹å­¦ä¹ ä¸ªæ€§åŒ–ä¸å…±äº«çš„æ—¶ç©ºæ¨¡å¼ï¼Œå¹¶åˆ©ç”¨æ—¶ç©ºç»´åº¦åˆ†è§£çš„æ©ç ä»¥æä½å¼€é”€ç²¾ç»†è°ƒèŠ‚åŠ¨æ€ç¥ç»ä¿¡å·ã€‚ä¸ºäº†æ¶ˆé™¤è¡¨ç¤ºçº ç¼ ï¼ŒPTSMé€šè¿‡ä¿¡æ¯è®ºçº¦æŸå°†æ½œåœ¨åµŒå…¥åˆ†è§£ä¸ºæ­£äº¤çš„ä»»åŠ¡ç›¸å…³å’Œå—è¯•è€…ç›¸å…³å­ç©ºé—´ï¼Œå¹¶é‡‡ç”¨å¤šç›®æ ‡æŸå¤±å‡½æ•°è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚åœ¨è·¨å—è¯•è€…è¿åŠ¨æƒ³è±¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPTSMå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–(zero-shot generalization)èƒ½åŠ›ï¼Œåœ¨æ— éœ€æ ¡å‡†çš„æƒ…å†µä¸‹æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†è§£è€¦ç¥ç»è¡¨ç¤ºåœ¨éå¹³ç¨³ç¥ç»ç”Ÿç†ç¯å¢ƒä¸‹å®ç°ä¸ªæ€§åŒ–ä¸å¯è¿ç§»è§£ç çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11357v1",
      "published_date": "2025-08-15 09:51:14 UTC",
      "updated_date": "2025-08-15 09:51:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:21.592746+00:00"
    },
    {
      "arxiv_id": "2508.11356v2",
      "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism",
      "title_zh": "ETTRLï¼šåŸºäºç†µæœºåˆ¶å¹³è¡¡å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨",
      "authors": [
        "Jia Liu",
        "ChangYi He",
        "YingQiao Lin",
        "MingMin Yang",
        "FeiYang Shen",
        "ShaoGuo Liu"
      ],
      "abstract": "Recent advancements in Large Language Models have yielded significant improvements in complex reasoning tasks such as mathematics and programming. However, these models remain heavily dependent on annotated data and exhibit limited adaptability in unsupervised scenarios. To address these limitations, test-time reinforcement learning (TTRL) has been proposed, which enables self-optimization by leveraging model-generated pseudo-labels. Despite its promise, TTRL faces several key challenges, including high inference costs due to parallel rollouts and early-stage estimation bias that fosters overconfidence, reducing output diversity and causing performance plateaus. To address these challenges, we introduce an entropy-based mechanism to enhance the exploration-exploitation balance in test-time reinforcement learning through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our approach enables Llama3.1-8B to achieve a 68 percent relative improvement in Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of the rollout tokens budget. This highlights our method's ability to effectively optimize the trade-off between inference efficiency, diversity, and estimation robustness, thereby advancing unsupervised reinforcement learning for open-domain reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¿‡åº¦ä¾èµ–æ ‡æ³¨æ•°æ®åŠåœ¨æ— ç›‘ç£åœºæ™¯ä¸‹é€‚åº”æ€§æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ETTRLæ¡†æ¶ä»¥ä¼˜åŒ–æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ (Test-time Reinforcement Learning, TTRL)ã€‚å°½ç®¡TTRLèƒ½åˆ©ç”¨æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè‡ªä¼˜åŒ–ï¼Œä½†å¸¸é¢ä¸´å¹¶è¡Œå›æ»š(rollouts)å¸¦æ¥çš„é«˜æ˜‚æ¨ç†æˆæœ¬ï¼Œä»¥åŠæ—©æœŸä¼°è®¡åå·®å¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡å’Œå¤šæ ·æ€§ç¼ºå¤±ã€‚ETTRLå¼•å…¥äº†ç†µæœºåˆ¶æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå…·ä½“åŒ…æ‹¬Entropy-fork Tree Majority Rollout (ETMR)å’ŒEntropy-based Advantage Reshaping (EAR)ä¸¤ç§ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿Llama3.1-8Båœ¨AIME 2024åŸºå‡†æµ‹è¯•çš„Pass@1æŒ‡æ ‡ä¸Šå®ç°äº†68%çš„ç›¸å¯¹æå‡ï¼Œä¸”ä»…æ¶ˆè€—äº†åŸºçº¿æ¨¡å‹60%çš„rollout tokensé¢„ç®—ã€‚è¯¥ç ”ç©¶æˆåŠŸè§£å†³äº†æ¨ç†æ•ˆç‡ã€è¾“å‡ºå¤šæ ·æ€§ä¸ä¼°è®¡ç¨³å¥æ€§ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œæ˜¾è‘—æ¨è¿›äº†å¼€æ”¾é¢†åŸŸæ¨ç†ä»»åŠ¡çš„æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11356v2",
      "published_date": "2025-08-15 09:49:14 UTC",
      "updated_date": "2025-08-29 08:04:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:19.283706+00:00"
    },
    {
      "arxiv_id": "2508.11354v2",
      "title": "FunduSegmenter: Leveraging the RETFound Foundation Model for Joint Optic Disc and Optic Cup Segmentation in Retinal Fundus Images",
      "title_zh": "FunduSegmenterï¼šåŸºäº RETFound åŸºç¡€æ¨¡å‹çš„è§†ç½‘è†œçœ¼åº•å›¾åƒè§†ç›˜ä¸è§†æ¯è”åˆåˆ†å‰²",
      "authors": [
        "Zhenyi Zhao",
        "Muthu Rama Krishnan Mookiah",
        "Emanuele Trucco"
      ],
      "abstract": "Purpose: This study aims to introduce the first adaptation of RETFound for joint optic disc (OD) and optic cup (OC) segmentation. RETFound is a well-known foundation model developed for fundus camera and optical coherence tomography images, which has shown promising performance in disease diagnosis. Methods: We propose FunduSegmenter, a model integrating a series of novel modules with RETFound, including a Pre-adapter, a Decoder, a Post-adapter, skip connections with Convolutional Block Attention Module and a Vision Transformer block adapter. The model is evaluated on a private dataset, GoDARTS, and four public datasets, IDRiD, Drishti-GS, RIM-ONE-r3, and REFUGE, through internal verification, external verification and domain generalization experiments. Results: An average Dice similarity coefficient of 90.51% was achieved in internal verification, which substantially outperformed the baselines (nnU-Net: 82.91%; DUNet: 89.17%; TransUNet: 87.91%). In all external verification experiments, the average results were about 3% higher than those of the best baseline, and were also competitive in domain generalization. Conclusions: This study explored the potential of the latent general representations learned by RETFound for OD and OC segmentation in fundus camera images. Our FunduSegmenter outperformed nearly all state-of-the-art baseline methods. The proposed modules are general and can be extended to fine-tuning other foundation models. Translational Relevance: The model shows strong stability and generalization on both in-distribution and out-of-distribution data, providing stable OD and OC segmentation. This is an essential step for many automated tasks, from setting the accurate retinal coordinate to biomarker discovery. The code and all trained weights are available at: [link to be added after the paper is accepted]",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FunduSegmenterï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäº RETFound åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelï¼‰è®¾è®¡çš„è§†ç›˜ï¼ˆOptic Disc, ODï¼‰ä¸è§†æ¯ï¼ˆOptic Cup, OCï¼‰è”åˆåˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆ Pre-adapterã€Decoderã€Post-adapter ä»¥åŠå¸¦æœ‰å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆConvolutional Block Attention Module, CBAMï¼‰çš„è·³è·ƒè¿æ¥å’Œ Vision Transformer å—é€‚é…å™¨ï¼Œæœ‰æ•ˆåˆ©ç”¨äº† RETFound å­¦ä¹ åˆ°çš„æ½œåœ¨é€šç”¨è¡¨ç¤ºã€‚ç ”ç©¶äººå‘˜åœ¨ç§æœ‰æ•°æ®é›† GoDARTS ä»¥åŠ IDRiDã€Drishti-GSã€RIM-ONE-r3 å’Œ REFUGE ç­‰å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯¦å°½çš„å†…éƒ¨ä¸å¤–éƒ¨éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFunduSegmenter åœ¨å†…éƒ¨éªŒè¯ä¸­è¾¾åˆ°äº† 90.51% çš„å¹³å‡ Dice ç›¸ä¼¼ç³»æ•°ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº nnU-Netã€DUNet å’Œ TransUNet ç­‰ä¸»æµåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤–éƒ¨éªŒè¯å’Œé¢†åŸŸæ³›åŒ–å®éªŒä¸­å‡å±•ç°å‡ºæå¼ºçš„ç¨³å®šæ€§ï¼Œè¯æ˜äº†å…¶å¤„ç†åˆ†å¸ƒå¤–æ•°æ®çš„ä¼˜å¼‚èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºè§†ç½‘è†œåæ ‡å®šä½åŠç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°æä¾›äº†ç²¾å‡†çš„è‡ªåŠ¨åŒ–æ‰‹æ®µï¼Œå…¶æå‡ºçš„æ¨¡å—åŒ–å¾®è°ƒç­–ç•¥ä¹Ÿå…·å¤‡æ‰©å±•è‡³å…¶ä»–åŒ»å­¦åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11354v2",
      "published_date": "2025-08-15 09:43:49 UTC",
      "updated_date": "2025-12-27 11:40:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:41.490594+00:00"
    },
    {
      "arxiv_id": "2508.11348v1",
      "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models",
      "title_zh": "NeMoï¼šä¸€ç§ç”¨äº DNN æ¨¡å‹åˆ†è§£çš„ç¥ç»å…ƒçº§è®­ç»ƒä¸­æ¨¡å—åŒ–æ–¹æ³•",
      "authors": [
        "Xiaohan Bi",
        "Binhang Qi",
        "Hailong Sun",
        "Xiang Gao",
        "Yue Yu",
        "Xiaojun Liang"
      ],
      "abstract": "With the growing incorporation of deep neural network (DNN) models into modern software systems, the prohibitive construction costs have become a significant challenge. Model reuse has been widely applied to reduce training costs, but indiscriminately reusing entire models may incur significant inference overhead. Consequently, DNN modularization has gained attention, enabling module reuse by decomposing DNN models. The emerging modularizing-while-training (MwT) paradigm, which incorporates modularization into training, outperforms modularizing-after-training approaches. However, existing MwT methods focus on small-scale CNN models at the convolutional kernel level and struggle with diverse DNNs and large-scale models, particularly Transformer-based models. To address these limitations, we propose NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron level fundamental component common to all DNNs-ensuring applicability to Transformers and various architectures. We design a contrastive learning-based modular training method with an effective composite loss function, enabling scalability to large-scale models. Comprehensive experiments on two Transformer-based models and four CNN models across two classification datasets demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show average gains of 1.72% in module classification accuracy and 58.10% reduction in module size, demonstrating efficacy across both CNN and large-scale Transformer-based models. A case study on open-source projects shows NeMo's potential benefits in practical scenarios, offering a promising approach for scalable and generalizable DNN modularization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)æ¨¡å‹é‡ç”¨ä¸­æ¨ç†å¼€é”€è¿‡å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†NeMoï¼Œä¸€ç§å¯æ‰©å±•ä¸”é€šç”¨çš„è®­ç»ƒä¸­æ¨¡å—åŒ–(Modularizing-while-Training, MwT)æ–¹æ³•ã€‚ä¸åŒäºç°æœ‰çš„ä»…é’ˆå¯¹å°å‹å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ä¸”éš¾ä»¥å¤„ç†å¤§è§„æ¨¡æ¨¡å‹çš„æ–¹æ³•ï¼ŒNeMoåœ¨æœ€åŸºç¡€çš„ç¥ç»å…ƒçº§åˆ«(neuron level)è¿›è¡Œè§£æ„ï¼Œä½¿å…¶èƒ½å¤Ÿæ— ç¼é€‚é…Transformerç­‰å¤šç§ä¸»æµæ¶æ„ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºå¯¹æ¯”å­¦ä¹ (contrastive learning)çš„æ¨¡å—åŒ–è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è®¾è®¡çš„å¤åˆæŸå¤±å‡½æ•°(composite loss function)å®ç°äº†å¯¹å¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆæ‰©å±•ã€‚åœ¨Transformerå’ŒCNNæ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNeMoåœ¨æ¨¡å—åˆ†ç±»å‡†ç¡®ç‡ä¸Šå¹³å‡æå‡1.72%ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†58.10%çš„æ¨¡å—å¤§å°(module size)ã€‚è¿™é¡¹å·¥ä½œä¸ä»…åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„MwTæ–¹æ³•ï¼Œè¿˜é€šè¿‡å¼€æºé¡¹ç›®æ¡ˆä¾‹å±•ç¤ºäº†å…¶åœ¨å®é™…å·¥ç¨‹åœºæ™¯ä¸­å®ç°é«˜æ•ˆæ¨¡å‹è§£æ„ä¸é‡ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11348v1",
      "published_date": "2025-08-15 09:25:40 UTC",
      "updated_date": "2025-08-15 09:25:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:47.686663+00:00"
    },
    {
      "arxiv_id": "2508.11347v1",
      "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding",
      "title_zh": "SAGEï¼šé¢å‘æŒç»­çŸ¥è¯†å›¾è°±åµŒå…¥çš„å°ºåº¦æ„ŸçŸ¥æ¸è¿›å¼æ¼”åŒ–",
      "authors": [
        "Yifei Li",
        "Lingling Zhang",
        "Hang Yan",
        "Tianzhe Zhao",
        "Zihan Ma",
        "Muye Huang",
        "Jun Liu"
      ],
      "abstract": "Traditional knowledge graph (KG) embedding methods aim to represent entities and relations in a low-dimensional space, primarily focusing on static graphs. However, real-world KGs are dynamically evolving with the constant addition of entities, relations and facts. To address such dynamic nature of KGs, several continual knowledge graph embedding (CKGE) methods have been developed to efficiently update KG embeddings to accommodate new facts while maintaining learned knowledge. As KGs grow at different rates and scales in real-world scenarios, existing CKGE methods often fail to consider the varying scales of updates and lack systematic evaluation throughout the entire update process. In this paper, we propose SAGE, a scale-aware gradual evolution framework for CKGE. Specifically, SAGE firstly determine the embedding dimensions based on the update scales and expand the embedding space accordingly. The Dynamic Distillation mechanism is further employed to balance the preservation of learned knowledge and the incorporation of new facts. We conduct extensive experiments on seven benchmarks, and the results show that SAGE consistently outperforms existing baselines, with a notable improvement of 1.38% in MRR, 1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with methods using fixed embedding dimensions show that SAGE achieves optimal performance on every snapshot, demonstrating the importance of adaptive embedding dimensions in CKGE. The codes of SAGE are publicly available at: https://github.com/lyfxjtu/Dynamic-Embedding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SAGEï¼Œä¸€ç§ç”¨äºæŒç»­çŸ¥è¯†å›¾è°±åµŒå…¥(Continual Knowledge Graph Embedding, CKGE)çš„å°ºåº¦æ„ŸçŸ¥æ¸è¿›å¼æ¼”åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¿½ç•¥æ›´æ–°è§„æ¨¡å·®å¼‚ä¸”ç¼ºä¹ç³»ç»Ÿè¯„ä¼°çš„é—®é¢˜ã€‚SAGE é€šè¿‡æ ¹æ®æ›´æ–°è§„æ¨¡åŠ¨æ€ç¡®å®šåµŒå…¥ç»´åº¦å¹¶æ‰©å±•åµŒå…¥ç©ºé—´ï¼Œæœ‰æ•ˆåœ°åº”å¯¹äº†ç°å®ä¸–ç•ŒçŸ¥è¯†å›¾è°±åŠ¨æ€å¢é•¿çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œæ¡†æ¶ä¸­å¼•å…¥çš„åŠ¨æ€è’¸é¦(Dynamic Distillation)æœºåˆ¶èƒ½å¤Ÿå¹³è¡¡æ—§çŸ¥è¯†çš„ä¿ç•™ä¸æ–°äº‹å®çš„æ•´åˆã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAGE åœ¨ MRRã€H@1 å’Œ H@10 ç­‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ã€‚ä¸å›ºå®šç»´åº¦æ–¹æ³•çš„å¯¹æ¯”å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œè‡ªé€‚åº”åµŒå…¥ç»´åº¦å¯¹äºåœ¨ CKGE çš„æ¯ä¸ªæ›´æ–°é˜¶æ®µä¿æŒæœ€ä¼˜æ€§èƒ½è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at https://github.com/lyfxjtu/Dynamic-Embedding",
      "pdf_url": "https://arxiv.org/pdf/2508.11347v1",
      "published_date": "2025-08-15 09:23:23 UTC",
      "updated_date": "2025-08-15 09:23:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:44.194927+00:00"
    },
    {
      "arxiv_id": "2508.11338v1",
      "title": "RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading",
      "title_zh": "RegimeNASï¼šé¢å‘é‡‘èäº¤æ˜“çš„å…·æœ‰ç†è®ºä¿è¯çš„æœºåˆ¶æ„ŸçŸ¥å¯å¾®åˆ†æ¶æ„æœç´¢",
      "authors": [
        "Prathamesh Devadiga",
        "Yashmitha Shailesh"
      ],
      "abstract": "We introduce RegimeNAS, a novel differentiable architecture search framework specifically designed to enhance cryptocurrency trading performance by explicitly integrating market regime awareness. Addressing the limitations of static deep learning models in highly dynamic financial environments, RegimeNAS features three core innovations: (1) a theoretically grounded Bayesian search space optimizing architectures with provable convergence properties; (2) specialized, dynamically activated neural modules (Volatility, Trend, and Range blocks) tailored for distinct market conditions; and (3) a multi-objective loss function incorporating market-specific penalties (e.g., volatility matching, transition smoothness) alongside mathematically enforced Lipschitz stability constraints. Regime identification leverages multi-head attention across multiple timeframes for improved accuracy and uncertainty estimation. Rigorous empirical evaluation on extensive real-world cryptocurrency data demonstrates that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving an 80.3% Mean Absolute Error reduction compared to the best traditional recurrent baseline and converging substantially faster (9 vs. 50+ epochs). Ablation studies and regime-specific analysis confirm the critical contribution of each component, particularly the regime-aware adaptation mechanism. This work underscores the imperative of embedding domain-specific knowledge, such as market regimes, directly within the NAS process to develop robust and adaptive models for challenging financial applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RegimeNASï¼Œä¸€ç§ä¸“é—¨ä¸ºåŠ å¯†è´§å¸äº¤æ˜“è®¾è®¡çš„ä½“åˆ¶æ„ŸçŸ¥å¯å¾®åˆ†æ¶æ„æœç´¢(Differentiable Architecture Search)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é™æ€æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é«˜åº¦åŠ¨æ€é‡‘èç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å…·æœ‰ç†è®ºä¿è¯çš„Bayesianæœç´¢ç©ºé—´ï¼Œé€šè¿‡å¯è¯æ˜çš„æ”¶æ•›ç‰¹æ€§ä¼˜åŒ–ç½‘ç»œæ¶æ„ï¼Œå¹¶è®¾è®¡äº†Volatilityã€Trendå’ŒRangeç­‰ä¸“é—¨çš„åŠ¨æ€æ¿€æ´»ç¥ç»æ¨¡å—ä»¥é€‚åº”ä¸åŒå¸‚åœºæ¡ä»¶ã€‚ä¸ºäº†å¢å¼ºç¨³å®šæ€§ï¼ŒRegimeNASç»“åˆäº†åŒ…å«å¸‚åœºç‰¹å®šæƒ©ç½šé¡¹å’ŒLipschitzç¨³å®šæ€§çº¦æŸçš„å¤šç›®æ ‡æŸå¤±å‡½æ•°ï¼Œå¹¶åˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶(Multi-head Attention)è¿›è¡Œç²¾ç¡®çš„ä½“åˆ¶è¯†åˆ«ä¸ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRegimeNASåœ¨çœŸå®åŠ å¯†è´§å¸æ•°æ®ä¸Šç›¸æ¯”ä¼ ç»ŸåŸºçº¿æ¨¡å‹å®ç°äº†80.3%çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ç¼©å‡ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ˜¾è‘—æé«˜ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨NASè¿‡ç¨‹ä¸­åµŒå…¥å¸‚åœºä½“åˆ¶ç­‰é¢†åŸŸçŸ¥è¯†å¯¹äºæ„å»ºç¨³å¥ä¸”å…·è‡ªé€‚åº”æ€§çš„é‡‘èé¢„æµ‹æ¨¡å‹è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11338v1",
      "published_date": "2025-08-15 09:09:54 UTC",
      "updated_date": "2025-08-15 09:09:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:56.895287+00:00"
    },
    {
      "arxiv_id": "2508.11728v1",
      "title": "UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction",
      "title_zh": "UniDCFï¼šç”¨äºç‰™é¢Œé¢ç¡¬ç»„ç»‡å…¨é¢é‡å»ºçš„åŸºç¡€æ¨¡å‹",
      "authors": [
        "Chunxia Ren",
        "Ning Zhu",
        "Yue Lai",
        "Gui Chen",
        "Ruijie Wang",
        "Yangyi Hu",
        "Suyao Liu",
        "Shuwen Mao",
        "Hong Su",
        "Yu Zhang",
        "Li Xiao"
      ],
      "abstract": "Dentocraniofacial hard tissue defects profoundly affect patients' physiological functions, facial aesthetics, and psychological well-being, posing significant challenges for precise reconstruction. Current deep learning models are limited to single-tissue scenarios and modality-specific imaging inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability. Here we introduce UniDCF, a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images. By leveraging the complementary strengths of each modality and incorporating a score-based denoising module to refine surface smoothness, UniDCF overcomes the limitations of prior single-modality approaches. We curated the largest multimodal dataset, comprising intraoral scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated instances. Evaluations demonstrate that UniDCF outperforms existing state-of-the-art methods in terms of geometric precision, structural completeness, and spatial accuracy. Clinical simulations indicate UniDCF reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and high-fidelity reconstruction, supporting personalized and precise restorative treatments, streamlining clinical workflows, and enhancing patient outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniDCFï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…¨é¢ç‰™é¢…é¢ç¡¬ç»„ç»‡é‡å»ºçš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å•ç»„ç»‡åœºæ™¯å’Œæ¨¡æ€ç‰¹å®šè¾“å…¥ä¸Šçš„å±€é™æ€§ï¼ŒUniDCFé€šè¿‡Point Cloudså’ŒMulti-view Imagesçš„å¤šæ¨¡æ€èåˆç¼–ç ï¼Œå®ç°äº†å¯¹å¤šç§ç‰™é¢…é¢ç¡¬ç»„ç»‡çš„ç²¾ç¡®é‡å»ºã€‚è¯¥æ¡†æ¶ç»“åˆäº†å„æ¨¡æ€çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå¹¶å¼•å…¥Score-based denoisingæ¨¡å—ä»¥æå‡è¡¨é¢å¹³æ»‘åº¦ï¼Œå…‹æœäº†ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•çš„ä¸è¶³ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«6,609åæ‚£è€…ã€å…±è®¡54,555ä¸ªæ ‡æ³¨å®ä¾‹çš„å¤§å‹å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–äº†Intraoral Scansã€CBCTå’ŒCTã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniDCFåœ¨å‡ ä½•ç²¾åº¦ã€ç»“æ„å®Œæ•´æ€§å’Œç©ºé—´å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚ä¸´åºŠä»¿çœŸè¿›ä¸€æ­¥æ˜¾ç¤ºï¼ŒUniDCFèƒ½å°†é‡å»ºè®¾è®¡æ—¶é—´ç¼©çŸ­99%ï¼Œä¸”ä¸´åºŠåŒ»ç”Ÿè®¤å¯åº¦è¶…è¿‡94%ï¼Œä¸ºå®ç°å¿«é€Ÿã€è‡ªåŠ¨åŒ–ä¸”é«˜ä¿çœŸçš„ä¸ªæ€§åŒ–ç²¾å‡†ä¿®å¤æ²»ç–—å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "23 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11728v1",
      "published_date": "2025-08-15 09:03:57 UTC",
      "updated_date": "2025-08-15 09:03:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:06:58.891372+00:00"
    },
    {
      "arxiv_id": "2508.15810v1",
      "title": "Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬ä¸å¤šæ¨¡æ€æ¨¡å› ä¸­å¸Œæœ›ã€ä»‡æ¨åŠæƒ…æ„Ÿæ£€æµ‹",
      "authors": [
        "Nouar AlDahoul",
        "Yasir Zaki"
      ],
      "abstract": "The rise of social media and online communication platforms has led to the spread of Arabic textual posts and memes as a key form of digital expression. While these contents can be humorous and informative, they are also increasingly being used to spread offensive language and hate speech. Consequently, there is a growing demand for precise analysis of content in Arabic text and memes. This paper explores the potential of large language models to effectively identify hope, hate speech, offensive language, and emotional expressions within such content. We evaluate the performance of base LLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is conducted using a dataset of Arabic textual speech and memes proposed in the ArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs such as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash 2.5, fine-tuned with Arabic memes, to deliver the superior performance. They achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3, respectively, and secure first place overall in the Mahed 2025 challenge. The proposed solutions offer a more nuanced understanding of both text and memes for accurate and efficient Arabic content moderation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰åœ¨é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬å’Œå¤šæ¨¡æ€è¿·å› ï¼ˆMemesï¼‰ä¸­æ£€æµ‹å¸Œæœ›ï¼ˆhopeï¼‰ã€ä»‡æ¨è¨€è®ºï¼ˆhate speechï¼‰ã€æ”»å‡»æ€§è¯­è¨€ï¼ˆoffensive languageï¼‰å’Œæƒ…ç»ªè¡¨è¾¾çš„æ½œåŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†åŸºç¡€ LLMsã€å¾®è°ƒï¼ˆfine-tunedï¼‰åçš„ LLMs ä»¥åŠé¢„è®­ç»ƒåµŒå…¥æ¨¡å‹ï¼ˆpre-trained embedding modelsï¼‰åœ¨ ArabicNLP MAHED 2025 æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡é’ˆå¯¹æ€§å¾®è°ƒçš„ GPT-4o-mini å’Œ Gemini Flash 2.5 åˆ†åˆ«åœ¨å¤„ç†é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬å’Œè¿·å› å†…å®¹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åœ¨ä¸‰é¡¹å­ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ¨¡å‹åˆ†åˆ«è¾¾åˆ°äº† 72.1%ã€57.8% å’Œ 79.6% çš„å® F1 åˆ†æ•°ï¼ˆmacro F1 scoresï¼‰ï¼Œå¹¶è£è· Mahed 2025 æŒ‘æˆ˜èµ›çš„æ€»å† å†›ã€‚è¯¥ç ”ç©¶æå‡ºçš„è§£å†³æ–¹æ¡ˆé€šè¿‡å¯¹æ–‡æœ¬å’Œè¿·å› çš„ç»†è‡´ç†è§£ï¼Œä¸ºæ„å»ºå‡†ç¡®ä¸”é«˜æ•ˆçš„é˜¿æ‹‰ä¼¯è¯­å†…å®¹å®¡æ ¸ç³»ç»Ÿï¼ˆcontent moderation systemsï¼‰æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.15810v1",
      "published_date": "2025-08-15 08:41:33 UTC",
      "updated_date": "2025-08-15 08:41:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:07:56.461026+00:00"
    },
    {
      "arxiv_id": "2508.11310v1",
      "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems",
      "title_zh": "SGSimEvalï¼šé¢å‘è‡ªåŠ¨ç»¼è¿°ç”Ÿæˆç³»ç»Ÿçš„å¤šç»´ç›¸ä¼¼æ€§å¢å¼ºå‹ç»¼åˆè¯„ä¼°åŸºå‡†",
      "authors": [
        "Beichen Guo",
        "Zhiyuan Wen",
        "Yu Yang",
        "Peng Gao",
        "Ruosong Yang",
        "Jiaxing Shen"
      ],
      "abstract": "The growing interest in automatic survey generation (ASG), a task that traditionally required considerable time and effort, has been spurred by recent advances in large language models (LLMs). With advancements in retrieval-augmented generation (RAG) and the rising popularity of multi-agent systems (MASs), synthesizing academic surveys using LLMs has become a viable approach, thereby elevating the need for robust evaluation methods in this domain. However, existing evaluation methods suffer from several limitations, including biased metrics, a lack of human preference, and an over-reliance on LLMs-as-judges. To address these challenges, we propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation that evaluates automatic survey generation systems by integrating assessments of the outline, content, and references, and also combines LLM-based scoring with quantitative metrics to provide a multifaceted evaluation framework. In SGSimEval, we also introduce human preference metrics that emphasize both inherent quality and similarity to humans. Extensive experiments reveal that current ASG systems demonstrate human-comparable superiority in outline generation, while showing significant room for improvement in content and reference generation, and our evaluation metrics maintain strong consistency with human assessments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨ç»¼è¿°ç”Ÿæˆ(ASG)è¯„ä¼°ä¸­å­˜åœ¨çš„æŒ‡æ ‡åå·®å’Œè¿‡åº¦ä¾èµ–LLMs-as-judgesç­‰å±€é™ï¼Œæå‡ºäº†SGSimEvalè¿™ä¸€å…¨é¢ä¸”å¤šç»´åº¦çš„ç›¸ä¼¼æ€§å¢å¼ºè¯„ä¼°åŸºå‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¯¹å¤§çº²(outline)ã€å†…å®¹(content)å’Œå‚è€ƒæ–‡çŒ®(references)çš„è¯„ä¼°ï¼Œå°†åŸºäºLLMsçš„è¯„åˆ†ä¸å®šé‡æŒ‡æ ‡ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†è¡¡é‡å†…åœ¨è´¨é‡ä¸äººç±»ç›¸ä¼¼æ€§çš„äººç±»åå¥½æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ASGç³»ç»Ÿåœ¨å¤§çº²ç”Ÿæˆä¸Šå±•ç°å‡ºä¸äººç±»ç›¸å½“çš„æ°´å¹³ï¼Œä½†åœ¨å†…å®¹å’Œå‚è€ƒæ–‡çŒ®ç”Ÿæˆæ–¹é¢ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ã€‚æ­¤å¤–ï¼ŒSGSimEvalçš„è¯„ä¼°æŒ‡æ ‡ä¸äººç±»è¯„ä»·ä¿æŒäº†å¼ºä¸€è‡´æ€§ï¼Œä¸ºè‡ªåŠ¨ç»¼è¿°ç”Ÿæˆç³»ç»Ÿçš„å¼€å‘æä¾›äº†ç¨³å¥çš„è¯„ä»·æ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to The 21st International Conference on Advanced Data Mining and Applications (ADMA2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.11310v1",
      "published_date": "2025-08-15 08:27:58 UTC",
      "updated_date": "2025-08-15 08:27:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:07:12.301239+00:00"
    },
    {
      "arxiv_id": "2508.11291v1",
      "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks",
      "title_zh": "æ— çº¿è¾¹ç¼˜è®¾å¤‡ç½‘ç»œä¸­é¢å‘ LLM æ¨ç†çš„åŠ¨æ€è´¨é‡-å»¶è¿Ÿæ„ŸçŸ¥è·¯ç”±",
      "authors": [
        "Rui Bao",
        "Nan Xue",
        "Yaping Sun",
        "Zhiyong Chen"
      ],
      "abstract": "The integration of wireless communications and Large Language Models (LLMs) is poised to unlock ubiquitous intelligent services, yet deploying them in wireless edge-device collaborative environments presents a critical trade-off between inference quality and end-to-end latency. A fundamental mismatch exists between task complexity and resource allocation: offloading simple queries invites prohibitive latency, while on-device models lack the capacity for demanding computations. To address this challenge, we propose a dynamic, quality-latency aware routing framework that orchestrates inference between a lightweight model on the mobile device and a powerful model on the edge server. Our framework employs two distinct cost models: for single-turn queries, it fuses a BERT-predicted semantic score with communication and computation overheads; for multi-turn dialogues, it further quantifies context-aware costs arising from model switching and KV-cache management. While maintaining full inference quality, extensive experiments demonstrate that our framework cuts average response latency by 5-15% and reduces large model invocations by 10-20% against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— çº¿è¾¹ç¼˜è®¾å¤‡åä½œç¯å¢ƒä¸­ Large Language Models (LLMs) éƒ¨ç½²é¢ä¸´çš„æ¨ç†è´¨é‡ä¸ç«¯åˆ°ç«¯å»¶è¿Ÿæƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€è´¨é‡-å»¶è¿Ÿæ„ŸçŸ¥è·¯ç”±æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨åè°ƒç§»åŠ¨è®¾å¤‡ä¸Šçš„è½»é‡çº§æ¨¡å‹ä¸è¾¹ç¼˜æœåŠ¡å™¨ä¸Šçš„å¼ºåŠ›æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†ä»»åŠ¡å¤æ‚æ€§ä¸èµ„æºåˆ†é…ä¸åŒ¹é…çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é’ˆå¯¹å•è½®æŸ¥è¯¢ï¼Œè¯¥æ¡†æ¶èåˆäº† BERT é¢„æµ‹çš„è¯­ä¹‰è¯„åˆ†ä¸é€šä¿¡åŠè®¡ç®—å¼€é”€è¿›è¡Œå†³ç­–ï¼›é’ˆå¯¹å¤šè½®å¯¹è¯ï¼Œåˆ™è¿›ä¸€æ­¥é‡åŒ–äº†ç”±æ¨¡å‹åˆ‡æ¢å’Œ KV-cache ç®¡ç†äº§ç”Ÿçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æˆæœ¬ã€‚åœ¨ä¿æŒå®Œæ•´æ¨ç†è´¨é‡çš„å‰æä¸‹ï¼Œå®éªŒè¡¨æ˜è¯¥æ¡†æ¶åœ¨ MMLUã€GSM8K å’Œ MT-Bench-101 åŸºå‡†æµ‹è¯•ä¸­ï¼Œå°†å¹³å‡å“åº”å»¶è¿Ÿé™ä½äº† 5-15%ï¼Œå¹¶å‡å°‘äº† 10-20% çš„å¤§å‹æ¨¡å‹è°ƒç”¨æ¬¡æ•°ï¼Œæ˜¾è‘—æå‡äº†æ— çº¿è¾¹ç¼˜ç¯å¢ƒä¸‹çš„æ™ºèƒ½æœåŠ¡æ•ˆç‡ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IT",
      "comment": "accepted by IEEE/CIC ICCC workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.11291v1",
      "published_date": "2025-08-15 07:55:05 UTC",
      "updated_date": "2025-08-15 07:55:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:07:16.895209+00:00"
    },
    {
      "arxiv_id": "2508.11287v1",
      "title": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems",
      "title_zh": "CSGOï¼šé¢å‘æ— çº¿åä½œè¾¹ç¼˜å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿå†·å¯åŠ¨çš„é€šç”¨ä¼˜åŒ–",
      "authors": [
        "Xuran Liu",
        "Nan Xue",
        "Rui Bao",
        "Yaping Sun",
        "Zhiyong Chen",
        "Meixia Tao",
        "Xiaodong Xu",
        "Shuguang Cui"
      ],
      "abstract": "While deploying large language models on edge devices promises low-latency and privacy-preserving AI services, it is hindered by limited device resources. Although pipeline parallelism facilitates distributed inference, existing approaches often ignore the cold-start latency caused by on-demand model loading. In this paper, we propose a latency-aware scheduling framework that overlaps model loading with computation and communication to minimize total inference latency. Based on device and model parameters, the framework dynamically adjusts layer partitioning and allocation to effectively hide loading time, thereby eliminating as many idle periods as possible. We formulate the problem as a Mixed-Integer Non-Linear Program and design an efficient dynamic programming algorithm to optimize model partitioning and device assignment. Experimental results show that the proposed method significantly reduces cold-start latency compared to baseline strategies.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹(LLM)æ—¶é¢ä¸´çš„èµ„æºå—é™é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨äº†ç°æœ‰æµæ°´çº¿å¹¶è¡Œ(pipeline parallelism)æ–¹æ³•ä¸­å¸¸è¢«å¿½ç•¥çš„æŒ‰éœ€åŠ è½½å¼•å‘çš„å†·å¯åŠ¨(cold-start)å»¶è¿Ÿã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†CSGOæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ„ŸçŸ¥å»¶è¿Ÿçš„è°ƒåº¦æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†æ¨¡å‹åŠ è½½ä¸è®¡ç®—å’Œé€šä¿¡è¿‡ç¨‹é‡å æ¥æœ€å°åŒ–æ€»æ¨ç†å»¶è¿Ÿã€‚è¯¥æ¡†æ¶æ ¹æ®è®¾å¤‡å’Œæ¨¡å‹å‚æ•°åŠ¨æ€è°ƒæ•´å±‚åˆ’åˆ†(layer partitioning)å’Œåˆ†é…ï¼Œä»¥æœ‰æ•ˆéšè—åŠ è½½æ—¶é—´å¹¶å°½å¯èƒ½æ¶ˆé™¤ç³»ç»Ÿç©ºé—²å‘¨æœŸã€‚ç ”ç©¶è€…å°†è¯¥ä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªæ··åˆæ•´æ•°éçº¿æ€§è§„åˆ’(Mixed-Integer Non-Linear Program, MINLP)é—®é¢˜ï¼Œå¹¶å¼€å‘å‡ºä¸€ç§é«˜æ•ˆçš„åŠ¨æ€è§„åˆ’(dynamic programming)ç®—æ³•æ¥ä¼˜åŒ–æ¨¡å‹åˆ†åŒºå’Œè®¾å¤‡åˆ†é…ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸åŸºçº¿ç­–ç•¥ç›¸æ¯”ï¼ŒCSGOæ˜¾è‘—é™ä½äº†æ— çº¿åä½œè¾¹ç¼˜ç³»ç»Ÿä¸­çš„å†·å¯åŠ¨å»¶è¿Ÿï¼Œæœ‰æ•ˆæå‡äº†AIæœåŠ¡çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IT",
      "comment": "submitted to Journal of Communications and Information Networks",
      "pdf_url": "https://arxiv.org/pdf/2508.11287v1",
      "published_date": "2025-08-15 07:49:22 UTC",
      "updated_date": "2025-08-15 07:49:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:07:15.194779+00:00"
    },
    {
      "arxiv_id": "2508.11286v1",
      "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent",
      "title_zh": "é¢å‘æŠ—æ•…éšœå…·èº«æ™ºèƒ½ä½“çš„åœºæ™¯å›¾å¼•å¯¼ä¸»åŠ¨é‡è§„åˆ’",
      "authors": [
        "Che Rin Yu",
        "Daewon Chae",
        "Dabin Seo",
        "Sangwon Lee",
        "Hyeongwoo Im",
        "Jinkyu Kim"
      ],
      "abstract": "When humans perform everyday tasks, we naturally adjust our actions based on the current state of the environment. For instance, if we intend to put something into a drawer but notice it is closed, we open it first. However, many autonomous robots lack this adaptive awareness. They often follow pre-planned actions that may overlook subtle yet critical changes in the scene, which can result in actions being executed under outdated assumptions and eventual failure. While replanning is critical for robust autonomy, most existing methods respond only after failures occur, when recovery may be inefficient or infeasible. While proactive replanning holds promise for preventing failures in advance, current solutions often rely on manually designed rules and extensive supervision. In this work, we present a proactive replanning framework that detects and corrects failures at subtask boundaries by comparing scene graphs constructed from current RGB-D observations against reference graphs extracted from successful demonstrations. When the current scene fails to align with reference trajectories, a lightweight reasoning module is activated to diagnose the mismatch and adjust the plan. Experiments in the AI2-THOR simulator demonstrate that our approach detects semantic and spatial mismatches before execution failures occur, significantly improving task success and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåœºæ™¯å›¾å¼•å¯¼çš„ä¸»åŠ¨é‡æ–°è§„åˆ’ (Proactive Replanning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ä½“ (Embodied Agent) éš¾ä»¥æ ¹æ®ç¯å¢ƒå˜åŒ–è‡ªé€‚åº”è°ƒæ•´è¡ŒåŠ¨çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åœ¨å­ä»»åŠ¡è¾¹ç•Œé€šè¿‡å¯¹æ¯”å½“å‰ RGB-D è§‚æµ‹æ„å»ºçš„åœºæ™¯å›¾ (Scene Graph) ä¸æˆåŠŸæ¼”ç¤ºä¸­çš„å‚è€ƒå›¾ï¼Œå®ç°åœ¨æ‰§è¡Œå¤±è´¥å‘ç”Ÿå‰çš„ä¸»åŠ¨æ£€æµ‹ä¸ä¿®æ­£ã€‚ä¸€æ—¦å‘ç°å½“å‰åœºæ™¯ä¸å‚è€ƒè½¨è¿¹ä¸ä¸€è‡´ï¼Œç³»ç»Ÿå°†è§¦å‘è½»é‡åŒ–æ¨ç†æ¨¡å—è¿›è¡Œæ•…éšœè¯Šæ–­å¹¶åŠ¨æ€è°ƒæ•´è¡ŒåŠ¨è®¡åˆ’ã€‚åœ¨ AI2-THOR æ¨¡æ‹Ÿå™¨ä¸­çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«è¯­ä¹‰å’Œç©ºé—´å±‚é¢çš„åå·®ï¼Œåœ¨å¤±æ•ˆå®é™…å‘ç”Ÿå‰å®Œæˆå¹²é¢„ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„æˆåŠŸç‡ä¸é²æ£’æ€§ï¼Œä¸ºå®ç°å…·å¤‡é«˜åº¦è‡ªé€‚åº”èƒ½åŠ›çš„è‡ªä¸»æœºå™¨äººç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11286v1",
      "published_date": "2025-08-15 07:48:51 UTC",
      "updated_date": "2025-08-15 07:48:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:07:21.096632+00:00"
    },
    {
      "arxiv_id": "2508.11281v2",
      "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection",
      "title_zh": "ToxiFrenchï¼šåŸºäºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰å¾®è°ƒçš„æ³•è¯­æ¯’æ€§æ£€æµ‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸å¢å¼º",
      "authors": [
        "Axel Delaval",
        "Shujian Yang",
        "Haicheng Wang",
        "Han Qiu",
        "Jialiang Lu"
      ],
      "abstract": "Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, human-annotated, large-scale datasets. In this work, we release ToxiFrench, a dataset of 53,622 French online comments together with a balanced benchmark split for systematic evaluation. The dataset is constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification, while ensuring statistical alignment with human-only annotation. We then benchmark a broad range of models and uncover a counterintuitive finding: Small Language Models (SLMs) often surpass larger models in robustness and generalization on this task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a Dynamic Weighted Loss (DWL) that progressively emphasizes the model's final decision and significantly improves faithfulness. Our fine-tuned 4B model (Qwen3-4B) achieves state-of-the-art performance on the benchmark. It improves its balanced accuracy by 10% over its baseline and achieves better performance than GPT-4o and DeepSeek-R1 on our benchmark, while successfully retaining cross-lingual capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ³•è¯­æ¯’æ€§æ£€æµ‹(Toxicity Detection)é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ã€å…·æœ‰æ–‡åŒ–ç›¸å…³æ€§æ•°æ®é›†çš„ç°çŠ¶ï¼Œæ¨å‡ºäº†åŒ…å«53,622æ¡åœ¨çº¿è¯„è®ºçš„ToxiFrenchæ•°æ®é›†åŠç›¸åº”çš„å¹³è¡¡åŸºå‡†ã€‚é€šè¿‡ä¸€ç§ä»…éœ€10%äººå·¥å‚ä¸çš„åŠè‡ªåŠ¨åŒ–æ ‡æ³¨æµæ°´çº¿ï¼Œè¯¥ç ”ç©¶åœ¨ä¿è¯æ ‡æ³¨è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ã€‚å®éªŒå‘ç°å°å‹è¯­è¨€æ¨¡å‹(SLMs)åœ¨æ­¤ä»»åŠ¡ä¸Šå¾€å¾€æ¯”å¤§å‹æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ç»“åˆåŠ¨æ€åŠ æƒæŸå¤±(Dynamic Weighted Loss, DWL)çš„é“¾å¼æ€ç»´(Chain-of-Thought, CoT)å¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨æå‡æ¨¡å‹å†³ç­–çš„å¿ å®åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„Qwen3-4Bæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå…¶å¹³è¡¡å‡†ç¡®ç‡è¾ƒåŸºå‡†æå‡äº†10%ï¼Œæ€§èƒ½ç”šè‡³è¶…è¶Šäº†GPT-4oå’ŒDeepSeek-R1ï¼Œå¹¶æˆåŠŸä¿ç•™äº†åŸæœ‰çš„è·¨è¯­è¨€èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 5 figures, 11 tables. This paper introduces TOXIFRENCH, a benchmark of 53,622 comments for French toxicity detection. It proposes a Chain-of-Thought fine-tuning method with a dynamic weighted loss. The fine-tuned 4B model (Qwen3-4B) achieves state-of-the-art performance, outperforming larger models like GPT-4o and DeepSeek-R1",
      "pdf_url": "https://arxiv.org/pdf/2508.11281v2",
      "published_date": "2025-08-15 07:40:41 UTC",
      "updated_date": "2026-01-19 15:55:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:09.364512+00:00"
    },
    {
      "arxiv_id": "2508.11280v2",
      "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought",
      "title_zh": "LETToTï¼šåŸºäºä¸“å®¶æ€ç»´æ ‘çš„æ—…æ¸¸é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹æ— æ ‡æ³¨è¯„ä¼°",
      "authors": [
        "Ruiyan Qi",
        "Congding Wen",
        "Weibo Zhou",
        "Jiwei Li",
        "Shangsong Liang",
        "Lingbo Li"
      ],
      "abstract": "Evaluating large language models (LLMs) in specific domain like tourism remains challenging due to the prohibitive cost of annotated benchmarks and persistent issues like hallucinations. We propose $\\textbf{L}$able-Free $\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert $\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that leverages expert-derived reasoning structures-instead of labeled data-to access LLMs in tourism. First, we iteratively refine and validate hierarchical ToT components through alignment with generic quality dimensions and expert feedback. Results demonstrate the effectiveness of our systematically optimized expert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we apply LETToT's optimized expert ToT to evaluate models of varying scales (32B-671B parameters), revealing: (1) Scaling laws persist in specialized domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g., DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit reasoning architectures outperform counterparts in accuracy and conciseness ($p<0.05$). Our work established a scalable, label-free paradigm for domain-specific LLM evaluation, offering a robust alternative to conventional annotated benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LETToTï¼Œä¸€ç§åˆ©ç”¨ä¸“å®¶æ€ç»´æ ‘ï¼ˆExpert Tree-of-Thoughtï¼‰å¯¹æ—…æ¸¸é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ— æ ‡ç­¾è¯„ä¼°çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ ‡æ³¨åŸºå‡†æˆæœ¬é«˜æ˜‚å’Œæ¨¡å‹å¹»è§‰ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£ä¼˜åŒ–å±‚æ¬¡åŒ–ToTç»„ä»¶å¹¶ç»“åˆä¸“å®¶åé¦ˆï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹å®ç°äº†4.99%è‡³14.15%çš„è¯„ä¼°è´¨é‡æå‡ã€‚ç ”ç©¶é€šè¿‡å¯¹32Bè‡³671Bå‚æ•°è§„æ¨¡æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå°½ç®¡DeepSeek-V3ç­‰è¶…å¤§è§„æ¨¡æ¨¡å‹è¡¨ç°é¢†å…ˆï¼Œä½†DeepSeek-R1-Distill-Llama-70Bç­‰æ¨ç†å¢å¼ºå‹æ¨¡å‹èƒ½æ˜¾è‘—ç¼©å°è§„æ¨¡å¸¦æ¥çš„å·®è·ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œåœ¨72Bå‚æ•°ä»¥ä¸‹çš„æ¨¡å‹ä¸­ï¼Œæ˜¾å¼æ¨ç†æ¶æ„åœ¨å‡†ç¡®æ€§å’Œç®€æ´æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¶æ„ã€‚LETToTä¸ºç‰¹å®šé¢†åŸŸçš„LLMè¯„ä¼°å»ºç«‹äº†ä¸€ç§å¯æ‰©å±•ä¸”æ— éœ€æ ‡ç­¾çš„æ–°èŒƒå¼ï¼Œä¸ºä¼ ç»Ÿçš„æ ‡æ³¨åŸºå‡†æä¾›äº†ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11280v2",
      "published_date": "2025-08-15 07:37:12 UTC",
      "updated_date": "2025-08-25 06:40:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:30.185462+00:00"
    },
    {
      "arxiv_id": "2508.11278v2",
      "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas",
      "title_zh": "é€šç”¨äººå·¥æ™ºèƒ½æ¨ç†æ˜¯å¦å¯¹æ•°æ®è¯±å¯¼çš„è®¤çŸ¥åå·®å…·æœ‰æ•æ„Ÿæ€§ï¼Ÿé’ˆå¯¹å…¸å‹è½¯ä»¶å·¥ç¨‹å›°å¢ƒçš„åŠ¨æ€åŸºå‡†æµ‹è¯„",
      "authors": [
        "Francesco Sovrano",
        "Gabriele Dominici",
        "Rita Sevastjanova",
        "Alessandra Stramiglio",
        "Alberto Bacchelli"
      ],
      "abstract": "Human cognitive biases in software engineering can lead to costly errors. While general-purpose AI (GPAI) systems may help mitigate these biases due to their non-human nature, their training on human-generated data raises a critical question: Do GPAI systems themselves exhibit cognitive biases? To investigate this, we present the first dynamic benchmarking framework to evaluate data-induced cognitive biases in GPAI within software engineering workflows. Starting with a seed set of 16 hand-crafted realistic tasks, each featuring one of 8 cognitive biases (e.g., anchoring, framing) and corresponding unbiased variants, we test whether bias-inducing linguistic cues unrelated to task logic can lead GPAI systems from correct to incorrect conclusions. To scale the benchmark and ensure realism, we develop an on-demand augmentation pipeline relying on GPAI systems to generate task variants that preserve bias-inducing cues while varying surface details. This pipeline ensures correctness (88-99% on average, according to human evaluation), promotes diversity, and controls reasoning complexity by leveraging Prolog-based reasoning. We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent tendency to rely on shallow linguistic heuristics over more complex reasoning. All systems exhibit bias sensitivity (6-35%), which increases with task complexity (up to 49%) and highlights risks in AI-driven software engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šç”¨äººå·¥æ™ºèƒ½(GPAI)ç³»ç»Ÿåœ¨è½¯ä»¶å·¥ç¨‹(Software Engineering)å·¥ä½œæµä¸­æ˜¯å¦ä¼šè¡¨ç°å‡ºç”±äºè®­ç»ƒæ•°æ®è¯±å¯¼çš„è®¤çŸ¥åå·®(Cognitive Biases)ã€‚ç ”ç©¶è€…æå‡ºäº†é¦–ä¸ªåŠ¨æ€åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œé€šè¿‡16é¡¹æ¶µç›–é”šå®š(Anchoring)å’Œæ¡†æ¶(Framing)ç­‰8ç§è®¤çŸ¥åå·®çš„æ‰‹å·¥ä»»åŠ¡åŠå…¶æ— åå·®å˜ä½“ï¼Œè¯„ä¼°ç³»ç»Ÿæ˜¯å¦å—é€»è¾‘æ— å…³çš„è¯­è¨€çº¿ç´¢å¹²æ‰°ã€‚ä¸ºäº†æé«˜åŸºå‡†çš„å¯æ‰©å±•æ€§å’ŒçœŸå®æ€§ï¼Œè¯¥ç ”ç©¶å¼€å‘äº†åŸºäºPrologæ¨ç†çš„è‡ªåŠ¨å¢å¼ºæµç¨‹ï¼Œåœ¨æ”¹å˜ä»»åŠ¡è¡¨é¢ç»†èŠ‚çš„åŒæ—¶ä¿ç•™åå·®è¯±å¯¼çº¿ç´¢ã€‚å®éªŒè¯„ä¼°äº†GPTã€LLaMAå’ŒDeepSeekç­‰é¢†å…ˆçš„GPAIç³»ç»Ÿï¼Œå‘ç°å®ƒä»¬æ™®éå€¾å‘äºä¾èµ–æµ…å±‚è¯­è¨€å¯å‘å¼(Linguistic Heuristics)è€Œéå¤æ‚çš„é€»è¾‘æ¨ç†ã€‚ç»“æœæ˜¾ç¤ºæ‰€æœ‰ç³»ç»Ÿå‡è¡¨ç°å‡ºåå·®æ•æ„Ÿæ€§(6-35%)ï¼Œä¸”è¿™ç§æ•æ„Ÿæ€§éšä»»åŠ¡å¤æ‚åº¦çš„å¢åŠ è€Œä¸Šå‡ï¼Œæœ€é«˜å¯è¾¾49%ã€‚è¯¥å‘ç°æ­ç¤ºäº†åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹ä¸­åº”ç”¨GPAIçš„æ½œåœ¨é£é™©ï¼Œå¼ºè°ƒäº†å³ä½¿æ˜¯éäººç±»ç³»ç»Ÿä¹Ÿéš¾ä»¥å®Œå…¨å…å—äººç±»æ•°æ®å¸¦æ¥çš„è®¤çŸ¥åè§å½±å“ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11278v2",
      "published_date": "2025-08-15 07:29:46 UTC",
      "updated_date": "2025-11-29 13:29:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:14.554064+00:00"
    },
    {
      "arxiv_id": "2508.11272v2",
      "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering",
      "title_zh": "é€šè¿‡æ¨ç†å¢å¼ºè¡¨å¾å·¥ç¨‹æå‡æœ‰ç›‘ç£ç»„åˆå›¾åƒæ£€ç´¢",
      "authors": [
        "Jun Li",
        "Hongjian Dou",
        "Zhenyu Zhang",
        "Kai Li",
        "Shaoguo Liu",
        "Tingting Gao"
      ],
      "abstract": "Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited -- compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Model's understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º PMTFR çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ç›‘ç£å¼ç»„åˆå›¾åƒæ£€ç´¢ (Composed Image Retrieval) ä»»åŠ¡ä¸­å‚è€ƒå›¾åƒä¸æ–‡æœ¬æŒ‡ä»¤çš„ååŒç†è§£ã€‚æ¡†æ¶é€šè¿‡ Pyramid Patcher æ¨¡å—å¢å¼ºäº†å¯¹ä¸åŒç²’åº¦è§†è§‰ä¿¡æ¯çš„æå–ï¼Œæœ‰æ•ˆæå‡äº† Pyramid Matching Model çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°ç»“åˆè¡¨å¾å·¥ç¨‹ (Representation Engineering)ï¼Œä»é“¾å¼æ€ç»´ (Chain-of-Thought) æ•°æ®ä¸­æå–è¡¨å¾å¹¶æ³¨å…¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (LVLMs)ï¼Œä»è€Œåœ¨æ— éœ€æ˜¾å¼æ–‡æœ¬æ¨ç†çš„æƒ…å†µä¸‹å®ç°æ— è®­ç»ƒç»†åŒ– (Training-Free Refinement)ã€‚å®éªŒè¡¨æ˜ï¼ŒPMTFR åœ¨å¤šä¸ª CIR åŸºå‡†æµ‹è¯•ä¸Šå‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸ºç›‘ç£å¼æ£€ç´¢ä»»åŠ¡æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆã€è¡¨ç°æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11272v2",
      "published_date": "2025-08-15 07:10:10 UTC",
      "updated_date": "2025-12-12 09:33:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:31.487437+00:00"
    },
    {
      "arxiv_id": "2508.11262v1",
      "title": "Vision-Language Models display a strong gender bias",
      "title_zh": "è§†è§‰-è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºå¼ºçƒˆçš„æ€§åˆ«åè§",
      "authors": [
        "Aiswarya Konavoor",
        "Raj Abhijit Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "Vision-language models (VLM) align images and text in a shared representation space that is useful for retrieval and zero-shot transfer. Yet, this alignment can encode and amplify social stereotypes in subtle ways that are not obvious from standard accuracy metrics. In this study, we test whether the contrastive vision-language encoder exhibits gender-linked associations when it places embeddings of face images near embeddings of short phrases that describe occupations and activities. We assemble a dataset of 220 face photographs split by perceived binary gender and a set of 150 unique statements distributed across six categories covering emotional labor, cognitive labor, domestic labor, technical labor, professional roles, and physical labor. We compute unit-norm image embeddings for every face and unit-norm text embeddings for every statement, then define a statement-level association score as the difference between the mean cosine similarity to the male set and the mean cosine similarity to the female set, where positive values indicate stronger association with the male set and negative values indicate stronger association with the female set. We attach bootstrap confidence intervals by resampling images within each gender group, aggregate by category with a separate bootstrap over statements, and run a label-swap null model that estimates the level of mean absolute association we would expect if no gender structure were present. The outcome is a statement-wise and category-wise map of gender associations in a contrastive vision-language space, accompanied by uncertainty, simple sanity checks, and a robust gender bias evaluation framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLM)åœ¨å›¾åƒä¸æ–‡æœ¬å¯¹é½è¿‡ç¨‹ä¸­å¦‚ä½•ç¼–ç å¹¶æ”¾å¤§ç¤¾ä¼šåˆ»æ¿å°è±¡ï¼Œé‡ç‚¹åˆ†æäº†å¯¹æ¯”å¼è§†è§‰è¯­è¨€ç¼–ç å™¨åœ¨é¢éƒ¨å›¾åƒä¸èŒä¸šæ´»åŠ¨æè¿°ä¹‹é—´å­˜åœ¨çš„æ€§åˆ«å…³è”ã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«220å¼ é¢éƒ¨ç…§ç‰‡åŠæ¶µç›–å…­å¤§åŠ³åŠ¨ç±»åˆ«çš„150æ¡é™ˆè¿°çš„æ•°æ®é›†ï¼Œé€šè¿‡è®¡ç®—å•ä½èŒƒæ•°åµŒå…¥(unit-norm embeddings)ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦(cosine similarity)å·®å¼‚æ¥å®šä¹‰å…³è”è¯„åˆ†ã€‚ä¸ºäº†ç¡®ä¿ç»“æœçš„å¯é æ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†è‡ªåŠ©æ³•ç½®ä¿¡åŒºé—´(bootstrap confidence intervals)å’Œæ ‡ç­¾äº¤æ¢é›¶æ¨¡å‹(label-swap null model)æ¥è¯„ä¼°æ€§åˆ«åè§çš„æ˜¾è‘—æ€§ã€‚è¯¥ç ”ç©¶æœ€ç»ˆç”Ÿæˆäº†ä¸€å¼ åæ˜ å¯¹æ¯”å¼è§†è§‰è¯­è¨€ç©ºé—´ä¸­æ€§åˆ«å…³è”çš„æ˜ å°„å›¾ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸ç¡®å®šæ€§é‡åŒ–å’Œå¥å…¨æ€§æ£€æŸ¥çš„ç¨³å¥æ€§åˆ«åè§è¯„ä¼°æ¡†æ¶ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ç±»æ¨¡å‹åœ¨å¤„ç†ç‰¹å®šèŒä¸šå’Œæ´»åŠ¨é™ˆè¿°æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§åˆ«åå‘ï¼Œæ­ç¤ºäº†æ ‡å‡†å‡†ç¡®ç‡æŒ‡æ ‡æ— æ³•æ•æ‰çš„æ·±å±‚ç¤¾ä¼šåè§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11262v1",
      "published_date": "2025-08-15 06:57:26 UTC",
      "updated_date": "2025-08-15 06:57:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:55.891370+00:00"
    },
    {
      "arxiv_id": "2508.11257v1",
      "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆå¹»è§‰ï¼šæ±½è½¦é¢†åŸŸæ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Marc Pavel",
        "Nenad Petrovic",
        "Lukasz Mazur",
        "Vahid Zolfaghari",
        "Fengjunjie Pan",
        "Alois Knoll"
      ],
      "abstract": "Large Language Models (LLMs) have shown significant potential in automating code generation tasks offering new opportunities across software engineering domains. However, their practical application remains limited due to hallucinations - outputs that appear plausible but are factually incorrect, unverifiable or nonsensical. This paper investigates hallucination phenomena in the context of code generation with a specific focus on the automotive domain. A case study is presented that evaluates multiple code LLMs for three different prompting complexities ranging from a minimal one-liner prompt to a prompt with Covesa Vehicle Signal Specifications (VSS) as additional context and finally to a prompt with an additional code skeleton. The evaluation reveals a high frequency of syntax violations, invalid reference errors and API knowledge conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct solution when given the most context-rich prompt. Simpler prompting strategies failed to yield a working result, even after multiple refinement iterations. These findings highlight the need for effective mitigation techniques to ensure the safe and reliable use of LLM generated code, especially in safety-critical domains such as automotive software systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ±½è½¦é¢†åŸŸï¼Œæ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨çš„å¹»è§‰(hallucinations)ç°è±¡ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…è¯„ä¼°äº†GPT-4.1ã€Codexå’ŒGPT-4oåœ¨ä¸‰ç§ä¸åŒæç¤ºè¯å¤æ‚åº¦ï¼ˆä»åŸºç¡€å•è¡Œæç¤ºåˆ°åŒ…å«Covesa Vehicle Signal Specifications (VSS)ä¸Šä¸‹æ–‡åŠä»£ç æ¡†æ¶çš„æç¤ºï¼‰ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ­ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é¢‘ç¹å‡ºç°è¯­æ³•è¿è§„ã€æ— æ•ˆå¼•ç”¨é”™è¯¯åŠAPIçŸ¥è¯†å†²çªã€‚ç ”ç©¶å‘ç°ï¼Œä»…æœ‰GPT-4.1å’ŒGPT-4oåœ¨æä¾›æœ€ä¸°å¯Œä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹èƒ½äº§å‡ºæ­£ç¡®ä»£ç ï¼Œè€Œç®€å•çš„æç¤ºè¯ç­–ç•¥å³ä¾¿ç»è¿‡å¤šæ¬¡è¿­ä»£ä¹Ÿæ— æ³•è·å¾—å¯è¡Œç»“æœã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨æ±½è½¦è½¯ä»¶ç³»ç»Ÿç­‰å®‰å…¨å…³é”®é¢†åŸŸä¸­ï¼Œå¿…é¡»å¼•å…¥æœ‰æ•ˆçš„å¹»è§‰ç¼“è§£æŠ€æœ¯ï¼Œä»¥ç¡®ä¿AIç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11257v1",
      "published_date": "2025-08-15 06:46:50 UTC",
      "updated_date": "2025-08-15 06:46:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:28.398080+00:00"
    },
    {
      "arxiv_id": "2508.11256v1",
      "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception",
      "title_zh": "ç”¨äºå¢å¼ºå¼€æ”¾è¯æ±‡å¯†é›†æ„ŸçŸ¥çš„é€šç”¨è§£è€¦å­¦ä¹ ",
      "authors": [
        "Junjie Wang",
        "Keyu Chen",
        "Yulin Li",
        "Bin Chen",
        "Hengshuang Zhao",
        "Xiaojuan Qi",
        "Zhuotao Tian"
      ],
      "abstract": "Dense visual perception tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense perception often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. \\revise{The context features are enhanced by jointly distilling semantic correlations from Vision Foundation Models (VFMs) and object integrity cues from diffusion models, thereby enhancing spatial consistency. In parallel, the content features are aligned with image crop representations and constrained by region correlations from VFMs to improve local discriminability. Extensive experiments demonstrate that DeCLIP establishes a solid foundation for open-vocabulary dense perception, consistently achieving state-of-the-art performance across a broad spectrum of tasks, including 2D detection and segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.} Code is available at https://github.com/xiaomoguhz/DeCLIP",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language Models (VLMs) å¦‚ CLIP åœ¨å¯†é›†æ„ŸçŸ¥ä»»åŠ¡ä¸­å› å›¾åƒ token ç©ºé—´å’Œè¯­ä¹‰èšåˆèƒ½åŠ›ä¸è¶³è€Œå¯¼è‡´çš„å±€éƒ¨åˆ¤åˆ«åŠ›ä¸ç©ºé—´ä¸€è‡´æ€§ç¼ºé™·ï¼Œæå‡ºäº†åä¸º DeCLIP çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è‡ªæ³¨æ„åŠ›æ¨¡å— (self-attention) è§£è€¦ï¼Œä»è€Œåˆ†åˆ«æå–â€œå†…å®¹â€ (content) å’Œâ€œä¸Šä¸‹æ–‡â€ (context) ç‰¹å¾ã€‚ä¸Šä¸‹æ–‡ç‰¹å¾é€šè¿‡è’¸é¦ Vision Foundation Models (VFMs) çš„è¯­ä¹‰ç›¸å…³æ€§ä¸æ‰©æ•£æ¨¡å‹ (diffusion models) çš„ç‰©ä½“å®Œæ•´æ€§çº¿ç´¢æ¥å¢å¼ºç©ºé—´ä¸€è‡´æ€§ï¼Œè€Œå†…å®¹ç‰¹å¾åˆ™é€šè¿‡ä¸å›¾åƒè£å‰ªè¡¨ç¤ºå¯¹é½å¹¶ç»“åˆ VFMs çš„åŒºåŸŸçº¦æŸæ¥æå‡å±€éƒ¨åˆ¤åˆ«åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeCLIP åœ¨ 2D æ£€æµ‹ä¸åˆ†å‰²ã€3D åŠè§†é¢‘å®ä¾‹åˆ†å‰²ã€6D ç‰©ä½“ä½å§¿ä¼°è®¡ç­‰å¤šç§ä»»åŠ¡ä¸­å‡å–å¾—äº† state-of-the-art çš„è¡¨ç°ã€‚è¿™ä¸€ç ”ç©¶ä¸ºè§£å†³å¼€æ”¾è¯æ±‡ (open-vocabulary) å¯†é›†è§†è§‰æ„ŸçŸ¥é—®é¢˜æä¾›äº†å¼ºæœ‰åŠ›çš„é€šç”¨å­¦ä¹ æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2505.04410",
      "pdf_url": "https://arxiv.org/pdf/2508.11256v1",
      "published_date": "2025-08-15 06:43:51 UTC",
      "updated_date": "2025-08-15 06:43:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:36.745788+00:00"
    },
    {
      "arxiv_id": "2508.11252v1",
      "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information",
      "title_zh": "ä¸æ­¢äºè§£å†³æ•°å­¦é¢˜ï¼šè¯„ä¼°å¤§æ¨ç†æ¨¡å‹ä¸»åŠ¨ç´¢å–ä¿¡æ¯çš„èƒ½åŠ›",
      "authors": [
        "Youcheng Huang",
        "Bowen Qin",
        "Chen Huang",
        "Duanyu Feng",
        "Xi Yang",
        "Wenqiang Lei"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on well-defined problems. However, such evaluation setup constitutes a critical gap, since a genuine intelligent agent should not only solve problems (as a math quiz solver), but also be able~to ask for information when the problems lack sufficient information, enabling proactivity in responding users' requests. To bridge such gap, we proposes a new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¶…è¶Šæ•°å­¦è§£é¢˜èƒ½åŠ›çš„è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨å¤§æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨ä¿¡æ¯ç¼ºå¤±æ—¶ä¸»åŠ¨è¯·æ±‚ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä½œè€…æŒ‡å‡º genuine intelligent agent ä¸ä»…åº”èƒ½è§£å†³å®šä¹‰æ˜ç¡®çš„é—®é¢˜ï¼Œè¿˜éœ€åœ¨ä¿¡æ¯ä¸è¶³æ—¶è¡¨ç°å‡ºä¸»åŠ¨æ€§ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸¤ç±»ä¸å®Œæ•´é—®é¢˜çš„å…¨æ–°æ•°æ®é›†ã€‚é€šè¿‡å¯¹ LRMs çš„ç³»ç»Ÿè¯„ä¼°å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ä¸»åŠ¨è¯¢é—®ä¿¡æ¯æ–¹é¢å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œéš¾ä»¥æœ‰æ•ˆè¯†åˆ«å¹¶è¡¥å…¨ç¼ºå¤±èƒŒæ™¯ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹åœ¨é¢å¯¹æ­¤ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºçš„ overthinking å’Œ hallucination è¡Œä¸ºã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜æ¢è®¨äº†é€šè¿‡ supervised fine-tuning åŸ¹å…»æ­¤é¡¹èƒ½åŠ›çš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨ä¸ºå¼€å‘å…·å¤‡çœŸæ­£æ™ºèƒ½è€Œéä»…ä»…æ˜¯â€œæ•°å­¦è§£é¢˜å™¨â€çš„ LRMs æä¾›æ–°çš„è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11252v1",
      "published_date": "2025-08-15 06:42:00 UTC",
      "updated_date": "2025-08-15 06:42:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:52.596146+00:00"
    },
    {
      "arxiv_id": "2508.11249v1",
      "title": "Graph Neural Diffusion via Generalized Opinion Dynamics",
      "title_zh": "åŸºäºå¹¿ä¹‰è§‚ç‚¹åŠ¨åŠ›å­¦çš„å›¾ç¥ç»æ‰©æ•£",
      "authors": [
        "Asela Hevapathige",
        "Asiri Wijesinghe",
        "Ahad N. Zehmakan"
      ],
      "abstract": "There has been a growing interest in developing diffusion-based Graph Neural Networks (GNNs), building on the connections between message passing mechanisms in GNNs and physical diffusion processes. However, existing methods suffer from three critical limitations: (1) they rely on homogeneous diffusion with static dynamics, limiting adaptability to diverse graph structures; (2) their depth is constrained by computational overhead and diminishing interpretability; and (3) theoretical understanding of their convergence behavior remains limited. To address these challenges, we propose GODNF, a Generalized Opinion Dynamics Neural Framework, which unifies multiple opinion dynamics models into a principled, trainable diffusion mechanism. Our framework captures heterogeneous diffusion patterns and temporal dynamics via node-specific behavior modeling and dynamic neighborhood influence, while ensuring efficient and interpretable message propagation even at deep layers. We provide a rigorous theoretical analysis demonstrating GODNF's ability to model diverse convergence configurations. Extensive empirical evaluations of node classification and influence estimation tasks confirm GODNF's superiority over state-of-the-art GNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GODNFï¼ˆGeneralized Opinion Dynamics Neural Frameworkï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¹¿ä¹‰èˆ†è®ºåŠ¨åŠ›å­¦æ„å»ºçš„æ–°å‹å›¾ç¥ç»æ‰©æ•£æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºäºæ‰©æ•£çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å¼‚è´¨è‡ªé€‚åº”æ€§ã€æ¨¡å‹æ·±åº¦åŠæ”¶æ•›ç†è®ºæ–¹é¢çš„å±€é™ï¼ŒGODNFå°†å¤šç§èˆ†è®ºåŠ¨åŠ›å­¦æ¨¡å‹ç»Ÿä¸€ä¸ºå¯è®­ç»ƒçš„æ‰©æ•£æœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡èŠ‚ç‚¹ç‰¹å®šè¡Œä¸ºå»ºæ¨¡å’ŒåŠ¨æ€é‚»åŸŸå½±å“æ•æ‰å¼‚è´¨æ‰©æ•£æ¨¡å¼ä¸æ—¶é—´åŠ¨æ€ï¼Œç¡®ä¿äº†æ·±å±‚æ¶ˆæ¯ä¼ æ’­çš„é«˜æ•ˆæ€§ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºGODNFæä¾›äº†ä¸¥è°¨çš„æ”¶æ•›è¡Œä¸ºç†è®ºåˆ†æï¼Œè¯æ˜å…¶èƒ½å¤Ÿå»ºæ¨¡å¤šæ ·çš„æ”¶æ•›é…ç½®ã€‚åœ¨èŠ‚ç‚¹åˆ†ç±»ï¼ˆnode classificationï¼‰å’Œå½±å“ä¼°è®¡ï¼ˆinfluence estimationï¼‰ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGODNFçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„GNNæ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11249v1",
      "published_date": "2025-08-15 06:36:57 UTC",
      "updated_date": "2025-08-15 06:36:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:50.587330+00:00"
    },
    {
      "arxiv_id": "2508.11247v1",
      "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering",
      "title_zh": "é¢å‘å¤šè·³é—®ç­”çš„è·¨ç²’åº¦è¶…å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Changjian Wang",
        "Weihong Deng",
        "Weili Guan",
        "Quan Lu",
        "Ning Jiang"
      ],
      "abstract": "Multi-hop question answering (MHQA) requires integrating knowledge scattered across multiple passages to derive the correct answer. Traditional retrieval-augmented generation (RAG) methods primarily focus on coarse-grained textual semantic similarity and ignore structural associations among dispersed knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods address this by leveraging knowledge graphs (KGs) to capture structural associations, but they tend to overly rely on structural information and fine-grained word- or phrase-level retrieval, resulting in an underutilization of textual semantics. In this paper, we propose a novel RAG approach called HGRAG for MHQA that achieves cross-granularity integration of structural and semantic information via hypergraphs. Structurally, we construct an entity hypergraph where fine-grained entities serve as nodes and coarse-grained passages as hyperedges, and establish knowledge association through shared entities. Semantically, we design a hypergraph retrieval method that integrates fine-grained entity similarity and coarse-grained passage similarity via hypergraph diffusion. Finally, we employ a retrieval enhancement module, which further refines the retrieved results both semantically and structurally, to obtain the most relevant passages as context for answer generation with the LLM. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in QA performance, and achieves a 6$\\times$ speedup in retrieval efficiency.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤šè·³é—®ç­” (Multi-hop Question Answering, MHQA) ä»»åŠ¡ä¸­ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å¿½ç•¥ç»“æ„å…³è”ä»¥åŠ GraphRAG è¿‡åº¦ä¾èµ–ç»†ç²’åº¦ä¿¡æ¯è€Œå¿½è§†æ–‡æœ¬è¯­ä¹‰çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º HGRAG çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¶…å›¾ (Hypergraphs) å®ç°äº†ç»“æ„ä¸è¯­ä¹‰ä¿¡æ¯çš„è·¨ç²’åº¦é›†æˆã€‚åœ¨ç»“æ„æ„å»ºä¸Šï¼ŒHGRAG å°†ç»†ç²’åº¦å®ä½“ä½œä¸ºèŠ‚ç‚¹ï¼Œç²—ç²’åº¦æ®µè½ä½œä¸ºè¶…è¾¹ï¼Œé€šè¿‡å…±äº«å®ä½“å»ºç«‹è·¨ç¯‡ç« çš„çŸ¥è¯†å…³è”ï¼›åœ¨è¯­ä¹‰æ£€ç´¢ä¸Šï¼Œåˆ™é€šè¿‡è¶…å›¾æ‰©æ•£ (Hypergraph Diffusion) æŠ€æœ¯æ•´åˆäº†å®ä½“ä¸æ®µè½çš„ç›¸ä¼¼åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ£€ç´¢å¢å¼ºæ¨¡å—ï¼Œä»è¯­ä¹‰å’Œç»“æ„åŒé‡ç»´åº¦è¿›ä¸€æ­¥ç²¾ç‚¼æ£€ç´¢ç»“æœï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLM) æä¾›æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒHGRAG åœ¨é—®ç­”æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ï¼Œå¹¶åœ¨æ£€ç´¢æ•ˆç‡ä¸Šå®ç°äº† 6 å€çš„æå‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11247v1",
      "published_date": "2025-08-15 06:36:13 UTC",
      "updated_date": "2025-08-15 06:36:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:08:55.293981+00:00"
    },
    {
      "arxiv_id": "2508.11222v2",
      "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal",
      "title_zh": "ORFuzzï¼šé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹å®‰å…¨â€œå¦ä¸€é¢â€çš„æ¨¡ç³Šæµ‹è¯•â€”â€”è¿‡åº¦æ‹’ç»æµ‹è¯•",
      "authors": [
        "Haonan Zhang",
        "Dongxia Wang",
        "Yi Liu",
        "Kexin Chen",
        "Jiashui Wang",
        "Xinlei Ying",
        "Long Liu",
        "Wenhai Wang"
      ],
      "abstract": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å› å®‰å…¨æªæ–½è¿‡äºä¿å®ˆè€Œé”™è¯¯æ‹’ç»è‰¯æ€§æŸ¥è¯¢çš„è¿‡åº¦æ‹’ç»(over-refusal)é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªæ¼”åŒ–æµ‹è¯•æ¡†æ¶ORFuzzã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ„ŸçŸ¥å®‰å…¨ç±»åˆ«çš„ç§å­é€‰æ‹©ã€åˆ©ç”¨æ¨ç†LLMså®ç°çš„è‡ªé€‚åº”å˜å¼‚å™¨ä¼˜åŒ–ä»¥åŠä¸äººç±»æ„ŸçŸ¥ä¸€è‡´çš„è¯„æµ‹æ¨¡å‹OR-Judgeï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°æ£€æµ‹å’Œåˆ†ææ¨¡å‹çš„è¿‡åº¦æ‹’ç»è¡Œä¸ºã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒORFuzzç”Ÿæˆè¿‡åº¦æ‹’ç»å®ä¾‹çš„å¹³å‡æ•ˆç‡ä¸º6.98%ï¼Œæ˜¯ç°æœ‰åŸºå‡†æ–¹æ³•çš„ä¸¤å€ä»¥ä¸Šï¼Œèƒ½æœ‰æ•ˆæ­ç¤ºæ¨¡å‹åœ¨åŠŸèƒ½æ€§ä¸Šçš„ç¼ºé™·ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†åŒ…å«1,855ä¸ªé«˜è¿ç§»æ€§æµ‹è¯•ç”¨ä¾‹çš„æ•°æ®é›†ORFuzzSetï¼Œè¯¥æ•°æ®é›†åœ¨10ç§ä¸åŒLLMsä¸Šè¾¾åˆ°äº†63.56%çš„å¹³å‡è¿‡åº¦æ‹’ç»ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŒç±»æ•°æ®é›†ã€‚ORFuzzå’ŒORFuzzSetä¸ºè¯„ä¼°å’Œæå‡LLMè½¯ä»¶ç³»ç»Ÿçš„å¯é æ€§ä¸å¯ä¿¡åº¦æä¾›äº†å¼ºæœ‰åŠ›çš„è‡ªåŠ¨åŒ–å·¥å…·å’Œèµ„æºã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11222v2",
      "published_date": "2025-08-15 05:03:26 UTC",
      "updated_date": "2025-12-05 05:36:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:12.694408+00:00"
    },
    {
      "arxiv_id": "2509.00008v1",
      "title": "Optimized Renewable Energy Planning MDP for Socially-Equitable Electricity Coverage in the US",
      "title_zh": "é¢å‘USç¤¾ä¼šå…¬å¹³ç”µåŠ›è¦†ç›–çš„å¯å†ç”Ÿèƒ½æºä¼˜åŒ–è§„åˆ’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹",
      "authors": [
        "Riya Kinnarkar",
        "Mansur Arief"
      ],
      "abstract": "Traditional power grid infrastructure presents significant barriers to renewable energy integration and perpetuates energy access inequities, with low-income communities experiencing disproportionately longer power outages. This study develops a Markov Decision Process (MDP) framework to optimize renewable energy allocation while explicitly addressing social equity concerns in electricity distribution. The model incorporates budget constraints, energy demand variability, and social vulnerability indicators across eight major U.S. cities to evaluate policy alternatives for equitable clean energy transitions. Numerical experiments compare the MDP-based approach against baseline policies including random allocation, greedy renewable expansion, and expert heuristics. Results demonstrate that equity-focused optimization can achieve 32.9% renewable energy penetration while reducing underserved low-income populations by 55% compared to conventional approaches. The expert policy achieved the highest reward, while the Monte Carlo Tree Search baseline provided competitive performance with significantly lower budget utilization, demonstrating that fair distribution of clean energy resources is achievable without sacrificing overall system performance and providing ways for integrating social equity considerations with climate goals and inclusive access to clean power infrastructure.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç”µç½‘åŸºç¡€è®¾æ–½åœ¨å¯å†ç”Ÿèƒ½æºæ•´åˆåŠèƒ½æºåˆ†é…ä¸å…¬æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¼€å‘äº†ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process, MDP)æ¡†æ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆé¢„ç®—é™åˆ¶ã€èƒ½æºéœ€æ±‚æ³¢åŠ¨ä»¥åŠç¾å›½å…«å¤§åŸå¸‚çš„ç¤¾ä¼šè„†å¼±æ€§æŒ‡æ ‡(social vulnerability indicators)ï¼Œæ—¨åœ¨ä¼˜åŒ–å¯å†ç”Ÿèƒ½æºåˆ†é…å¹¶è§£å†³ç”µåŠ›è¦†ç›–ä¸­çš„ç¤¾ä¼šå…¬å¹³é—®é¢˜ã€‚æ•°å€¼å®éªŒå¯¹æ¯”äº†åŸºäºMDPçš„æ–¹æ³•ä¸éšæœºåˆ†é…ã€è´ªå©ªæ‰©å¼ åŠä¸“å®¶å¯å‘å¼ç­‰åŸºå‡†ç­–ç•¥ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»¥å…¬å¹³ä¸ºå¯¼å‘çš„ä¼˜åŒ–æ–¹æ¡ˆåœ¨å®ç°32.9%çš„å¯å†ç”Ÿèƒ½æºæ¸—é€ç‡çš„åŒæ—¶ï¼Œæ¯”ä¼ ç»Ÿæ–¹æ³•å‡å°‘äº†55%çš„æœåŠ¡ä¸è¶³ä½æ”¶å…¥ç¾¤ä½“ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä¸ç‰ºç‰²ç³»ç»Ÿæ•´ä½“æ€§èƒ½çš„å‰æä¸‹å®ç°æ¸…æ´èƒ½æºå…¬å¹³åˆ†é…æ˜¯å®Œå…¨å¯è¡Œçš„ï¼Œä¸ºç»Ÿç­¹æ°”å€™ç›®æ ‡ä¸åŒ…å®¹æ€§æ¸…æ´ç”µåŠ›åŸºç¡€è®¾æ–½å»ºè®¾æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.CE",
        "econ.GN"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00008v1",
      "published_date": "2025-08-15 04:56:01 UTC",
      "updated_date": "2025-08-15 04:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:23.784901+00:00"
    },
    {
      "arxiv_id": "2508.11214v1",
      "title": "How Causal Abstraction Underpins Computational Explanation",
      "title_zh": "å› æœæŠ½è±¡å¦‚ä½•æ”¯æ’‘è®¡ç®—è§£é‡Š",
      "authors": [
        "Atticus Geiger",
        "Jacqueline Harding",
        "Thomas Icard"
      ],
      "abstract": "Explanations of cognitive behavior often appeal to computations over representations. What does it take for a system to implement a given computation over suitable representational vehicles within that system? We argue that the language of causality -- and specifically the theory of causal abstraction -- provides a fruitful lens on this topic. Drawing on current discussions in deep learning with artificial neural networks, we illustrate how classical themes in the philosophy of computation and cognition resurface in contemporary machine learning. We offer an account of computational implementation grounded in causal abstraction, and examine the role for representation in the resulting picture. We argue that these issues are most profitably explored in connection with generalization and prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®¤çŸ¥è¡Œä¸ºè§£é‡Šä¸­è®¡ç®—ä¸è¡¨å¾çš„å…³ç³»ï¼Œæ—¨åœ¨é˜æ˜ç‰©ç†ç³»ç»Ÿå¦‚ä½•å®ç°ç‰¹å®šçš„è®¡ç®—è¿‡ç¨‹ã€‚ä½œè€…æå‡ºåˆ©ç”¨å› æœå…³ç³»è¯­è¨€ï¼Œç‰¹åˆ«æ˜¯å› æœæŠ½è±¡(Causal Abstraction)ç†è®ºæ¥åˆ†æè¿™ä¸€è®®é¢˜ï¼Œå¹¶ç»“åˆæ·±åº¦å­¦ä¹ ä¸äººå·¥ç¥ç»ç½‘ç»œ(ANNs)å±•ç¤ºäº†è®¡ç®—å“²å­¦ç»å…¸è®®é¢˜åœ¨å½“ä»£æœºå™¨å­¦ä¹ ä¸­çš„é‡ç°ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªä»¥å› æœæŠ½è±¡ä¸ºåŸºç¡€çš„è®¡ç®—å®ç°(Computational Implementation)è¯´æ˜ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è¡¨å¾(Representation)åœ¨å…¶ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥è®ºè¯äº†è¿™äº›é—®é¢˜åœ¨æ³›åŒ–(Generalization)ä¸é¢„æµ‹(Prediction)çš„èƒŒæ™¯ä¸‹èƒ½å¾—åˆ°æœ€æœ‰æ•ˆçš„æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•ä¸ºç†è§£å¤æ‚ç³»ç»Ÿçš„è®¡ç®—æœºåˆ¶æä¾›äº†ä¸€ä¸ªä¸¥è°¨çš„å› æœé€»è¾‘æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11214v1",
      "published_date": "2025-08-15 04:46:02 UTC",
      "updated_date": "2025-08-15 04:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:19.999285+00:00"
    },
    {
      "arxiv_id": "2508.11204v1",
      "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation",
      "title_zh": "æœºå™¨äººæ“ä½œå¼ºåŒ–å­¦ä¹ ä¸­çš„å¤šç»„ç­‰å˜å¢å¼º",
      "authors": [
        "Hongbin Lin",
        "Juan Rojas",
        "Kwok Wai Samuel Au"
      ],
      "abstract": "Sampling efficiency is critical for deploying visuomotor learning in real-world robotic manipulation. While task symmetry has emerged as a promising inductive bias to improve efficiency, most prior work is limited to isometric symmetries -- applying the same group transformation to all task objects across all timesteps. In this work, we explore non-isometric symmetries, applying multiple independent group transformations across spatial and temporal dimensions to relax these constraints. We introduce a novel formulation of the partially observable Markov decision process (POMDP) that incorporates the non-isometric symmetry structures, and propose a simple yet effective data augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate MEA with offline reinforcement learning to enhance sampling efficiency, and introduce a voxel-based visual representation that preserves translational equivariance. Extensive simulation and real-robot experiments across two manipulation domains demonstrate the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººæ“ä½œä¸­è§†è§‰è¿åŠ¨å­¦ä¹ çš„é‡‡æ ·æ•ˆç‡(sampling efficiency)æŒ‘æˆ˜ï¼Œæ¢è®¨äº†åˆ©ç”¨éç­‰è½´å¯¹ç§°æ€§(non-isometric symmetries)æ¥æå‡å­¦ä¹ è¡¨ç°ã€‚ä¸åŒäºä»¥å¾€å±€é™äºç­‰è½´å¯¹ç§°æ€§(isometric symmetries)çš„ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½å¤Ÿç»“åˆç©ºé—´å’Œæ—¶é—´ç»´åº¦å¤šä¸ªç‹¬ç«‹ç¾¤å˜æ¢çš„åè§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(POMDP)æ–°è¡¨è¿°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…å¼•å…¥äº†å¤šç¾¤ç­‰å˜å¢å¼º(Multi-Group Equivariance Augmentation, MEA)æ–¹æ³•ï¼Œå¹¶é…åˆä½¿ç”¨ä¿æŒå¹³ç§»ç­‰å˜æ€§(translational equivariance)çš„ä½“ç´ è§†è§‰è¡¨ç¤º(voxel-based visual representation)ã€‚é€šè¿‡å°†MEAä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ (offline reinforcement learning)ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­å‡æ˜¾è‘—æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨ä¸¤ä¸ªä¸åŒçš„æ“ä½œé¢†åŸŸä¸­ï¼Œè¯¥æ–¹æ¡ˆç›¸è¾ƒäºä¼ ç»Ÿå¯¹ç§°æ€§çº¦æŸæ–¹æ³•å…·æœ‰æ›´å¼ºçš„æœ‰æ•ˆæ€§å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11204v1",
      "published_date": "2025-08-15 04:30:01 UTC",
      "updated_date": "2025-08-15 04:30:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:22.293807+00:00"
    },
    {
      "arxiv_id": "2508.11203v1",
      "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation",
      "title_zh": "StyleMMï¼šåŸºäºæ–‡æœ¬é©±åŠ¨å¯¹é½å›¾åƒè½¬æ¢çš„é£æ ¼åŒ–ä¸‰ç»´å¯å˜å½¢äººè„¸æ¨¡å‹",
      "authors": [
        "Seungmi Lee",
        "Kwan Yun",
        "Junyong Noh"
      ],
      "abstract": "We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† StyleMMï¼Œä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æè¿°æ„å»ºé£æ ¼åŒ– 3D Morphable Model (3DMM) çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒçš„ç½‘æ ¼å˜å½¢ç½‘ç»œå’Œçº¹ç†ç”Ÿæˆå™¨åŸºç¡€ä¸Šï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬å¼•å¯¼çš„ image-to-image (i2i) ç¿»è¯‘ç”Ÿæˆçš„é£æ ¼åŒ–å›¾åƒä½œä¸ºç›®æ ‡è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†è§£å†³ç¿»è¯‘è¿‡ç¨‹ä¸­èº«ä»½å’Œè¡¨æƒ…ææ˜“ä¸¢å¤±çš„é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§èƒ½å¤Ÿæ˜¾å¼ä¿ç•™æºå›¾åƒé¢éƒ¨å±æ€§çš„é£æ ¼åŒ–æŠ€æœ¯ï¼Œç¡®ä¿äº†åœ¨æ•´ä¸ª 3DMM å‚æ•°ç©ºé—´å†…å®ç°ä¸€è‡´çš„ 3D é£æ ¼è¿ç§»ã€‚è®­ç»ƒå®Œæˆåçš„ StyleMM æ”¯æŒå‰å‘æ¨ç†ç”Ÿæˆï¼Œå…è®¸ç”¨æˆ·å¯¹å½¢çŠ¶ã€è¡¨æƒ…å’Œçº¹ç†å‚æ•°è¿›è¡Œæ˜¾å¼æ§åˆ¶ï¼Œå¹¶ä¿è¯äº†é¢éƒ¨ç½‘æ ¼çš„ä¸€è‡´é¡¶ç‚¹è¿æ¥æ€§ä¸å¯åŠ¨ç”»åŒ–ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èº«ä»½çº§åˆ«çš„é¢éƒ¨å¤šæ ·æ€§å’Œé£æ ¼åŒ–è¡¨ç°ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.GR",
      "comment": "Pacific graphics 2025, CGF, 15 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11203v1",
      "published_date": "2025-08-15 04:29:46 UTC",
      "updated_date": "2025-08-15 04:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:30.600573+00:00"
    },
    {
      "arxiv_id": "2508.11200v1",
      "title": "Visuomotor Grasping with World Models for Surgical Robots",
      "title_zh": "åŸºäºä¸–ç•Œæ¨¡å‹çš„æ‰‹æœ¯æœºå™¨äººè§†è§‰è¿åŠ¨æŠ“å–",
      "authors": [
        "Hongbin Lin",
        "Bin Li",
        "Kwok Wai Samuel Au"
      ],
      "abstract": "Grasping is a fundamental task in robot-assisted surgery (RAS), and automating it can reduce surgeon workload while enhancing efficiency, safety, and consistency beyond teleoperated systems. Most prior approaches rely on explicit object pose tracking or handcrafted visual features, limiting their generalization to novel objects, robustness to visual disturbances, and the ability to handle deformable objects. Visuomotor learning offers a promising alternative, but deploying it in RAS presents unique challenges, such as low signal-to-noise ratio in visual observations, demands for high safety and millimeter-level precision, as well as the complex surgical environment. This paper addresses three key challenges: (i) sim-to-real transfer of visuomotor policies to ex vivo surgical scenes, (ii) visuomotor learning using only a single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic grasping with a single policy that generalizes to diverse, unseen surgical objects without retraining or task-specific models. We introduce Grasp Anything for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping. GASv2 leverages a world-model-based architecture and a surgical perception pipeline for visual observations, combined with a hybrid control system for safe execution. We train the policy in simulation using domain randomization for sim-to-real transfer and deploy it on a real robot in both phantom-based and ex vivo surgical settings, using only a single pair of endoscopic cameras. Extensive experiments show our policy achieves a 65% success rate in both settings, generalizes to unseen objects and grippers, and adapts to diverse disturbances, demonstrating strong performance, generality, and robustness.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æœºå™¨äººè¾…åŠ©æ‰‹æœ¯(RAS)ä¸­è‡ªåŠ¨æŠ“å–ä»»åŠ¡é¢ä¸´çš„æ¦‚æ‹¬æ€§å·®å’Œè§†è§‰å¹²æ‰°é²æ£’æ€§å¼±ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGrasp Anything for Surgery V2 (GASv2)çš„è§†åŠ›è¿åŠ¨å­¦ä¹ æ¡†æ¶ã€‚GASv2åˆ©ç”¨åŸºäºä¸–ç•Œæ¨¡å‹(World-model-based)çš„æ¶æ„å’Œæ‰‹æœ¯æ„ŸçŸ¥æµæ°´çº¿å¤„ç†è§†è§‰è§‚å¯Ÿï¼Œå¹¶ç»“åˆæ··åˆæ§åˆ¶ç³»ç»Ÿç¡®ä¿æ‰§è¡Œå®‰å…¨ã€‚è¯¥æ¡†æ¶é€šè¿‡åŸŸéšæœºåŒ–(Domain randomization)åœ¨æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨è§£å†³ä»æ¨¡æ‹Ÿåˆ°ç°å®(Sim-to-real)çš„è¿ç§»é—®é¢˜ï¼Œä¸”ä»…ä¾èµ–å•å¯¹ç«‹ä½“ç›¸æœº(Stereo camera pair)è¿™ä¸€æ ‡å‡†RASé…ç½®ã€‚ç ”ç©¶äººå‘˜åœ¨å¹»è±¡å’Œç¦»ä½“(Ex vivo)æ‰‹æœ¯åœºæ™¯ä¸­éƒ¨ç½²äº†è¯¥ç­–ç•¥ï¼Œå®éªŒç»“æœæ˜¾ç¤ºGASv2åœ¨ä¸¤ç§è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†65%çš„æˆåŠŸç‡ã€‚æœ€ç»ˆç»“æœè¯æ˜è¯¥ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæ¨å¹¿è‡³æœªè§è¿‡çš„æ‰‹æœ¯ç‰©ä½“å’Œå¤¹å…·ï¼Œå¹¶èƒ½é€‚åº”å¤šæ ·çš„è§†è§‰å¹²æ‰°ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€é€šç”¨æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11200v1",
      "published_date": "2025-08-15 04:23:07 UTC",
      "updated_date": "2025-08-15 04:23:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:10:27.355173+00:00"
    },
    {
      "arxiv_id": "2508.11197v1",
      "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection",
      "title_zh": "E-CaTCHï¼šèåˆæ—¶åºä¸€è‡´æ€§ä¸ç±»åˆ«ä¸å¹³è¡¡å¤„ç†çš„ä»¥äº‹ä»¶ä¸ºä¸­å¿ƒçš„è·¨æ¨¡æ€æ³¨æ„åŠ›è™šå‡ä¿¡æ¯æ£€æµ‹",
      "authors": [
        "Ahmad Mousavi",
        "Yeganeh Abdollahinejad",
        "Roberto Corizzo",
        "Nathalie Japkowicz",
        "Zois Boukouvalas"
      ],
      "abstract": "Detecting multimodal misinformation on social media remains challenging due to inconsistencies between modalities, changes in temporal patterns, and substantial class imbalance. Many existing methods treat posts independently and fail to capture the event-level structure that connects them across time and modality. We propose E-CaTCH, an interpretable and scalable framework for robustly detecting misinformation. If needed, E-CaTCH clusters posts into pseudo-events based on textual similarity and temporal proximity, then processes each event independently. Within each event, textual and visual features are extracted using pre-trained BERT and ResNet encoders, refined via intra-modal self-attention, and aligned through bidirectional cross-modal attention. A soft gating mechanism fuses these representations to form contextualized, content-aware embeddings of each post. To model temporal evolution, E-CaTCH segments events into overlapping time windows and uses a trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode narrative progression over time. Classification is performed at the event level, enabling better alignment with real-world misinformation dynamics. To address class imbalance and promote stable learning, the model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining. The total loss is aggregated across all events. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH consistently outperforms state-of-the-art baselines. Cross-dataset evaluations further demonstrate its robustness, generalizability, and practical applicability across diverse misinformation scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†E-CaTCHï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³ç¤¾äº¤åª’ä½“å¤šæ¨¡æ€è¯¯å¯¼ä¿¡æ¯æ£€æµ‹ä¸­è·¨æ¨¡æ€ä¸ä¸€è‡´ã€æ—¶åºæ¨¡å¼æ¼”å˜ä»¥åŠç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„å¯æ‰©å±•æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ ¹æ®æ–‡æœ¬ç›¸ä¼¼åº¦å’Œæ—¶é—´æ¥è¿‘åº¦å°†å¸–å­èšç±»ä¸ºä¼ªäº‹ä»¶(pseudo-events)ï¼Œéšååˆ©ç”¨é¢„è®­ç»ƒçš„BERTå’ŒResNetç¼–ç å™¨é…åˆåŒå‘è·¨æ¨¡æ€æ³¨æ„åŠ›(cross-modal attention)ä¸è½¯é—¨æ§æœºåˆ¶(soft gating)ç”Ÿæˆå†…å®¹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åµŒå…¥ã€‚ä¸ºäº†å»ºæ¨¡æ—¶åºæ¼”åŒ–ï¼ŒE-CaTCHä½¿ç”¨è¶‹åŠ¿æ„ŸçŸ¥LSTM(trend-aware LSTM)æ•è·å™äº‹éšæ—¶é—´çš„è¿›å±•ï¼Œå¹¶åœ¨äº‹ä»¶å±‚çº§è¿›è¡Œåˆ†ç±»ä»¥è´´åˆçœŸå®çš„è¯¯å¯¼ä¿¡æ¯åŠ¨æ€ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜é›†æˆäº†è‡ªé€‚åº”ç±»åˆ«åŠ æƒ(adaptive class weighting)ã€æ—¶åºä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œç¡¬æ ·æœ¬æŒ–æ˜æŠ€æœ¯ï¼Œæœ‰æ•ˆåº”å¯¹äº†ç°å®æ•°æ®ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨Fakedditã€INDå’ŒCOVID-19 MISINFOGRAPHç­‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒE-CaTCHä¸ä»…åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œè¿˜è¡¨ç°å‡ºäº†å“è¶Šçš„é²æ£’æ€§å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11197v1",
      "published_date": "2025-08-15 04:13:23 UTC",
      "updated_date": "2025-08-15 04:13:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:47.685227+00:00"
    },
    {
      "arxiv_id": "2508.11190v1",
      "title": "Quantum-Boosted High-Fidelity Deep Learning",
      "title_zh": "é‡å­å¢å¼ºçš„é«˜ä¿çœŸæ·±åº¦å­¦ä¹ ",
      "authors": [
        "Feng-ao Wang",
        "Shaobo Chen",
        "Yao Xuan",
        "Junwei Liu",
        "Qi Gao",
        "Hongdong Zhu",
        "Junjie Hou",
        "Lixin Yuan",
        "Jinyu Cheng",
        "Chenxin Yi",
        "Hai Wei",
        "Yin Ma",
        "Tao Xu",
        "Kai Wen",
        "Yixue Li"
      ],
      "abstract": "A fundamental limitation of probabilistic deep learning is its predominant reliance on Gaussian priors. This simplistic assumption prevents models from accurately capturing the complex, non-Gaussian landscapes of natural data, particularly in demanding domains like complex biological data, severely hindering the fidelity of the model for scientific discovery. The physically-grounded Boltzmann distribution offers a more expressive alternative, but it is computationally intractable on classical computers. To date, quantum approaches have been hampered by the insufficient qubit scale and operational stability required for the iterative demands of deep learning. Here, we bridge this gap by introducing the Quantum Boltzmann Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable hybrid quantum-classical architecture. Our framework leverages a quantum processor for efficient sampling from the Boltzmann distribution, enabling its use as a powerful prior within a deep generative model. Applied to million-scale single-cell datasets from multiple sources, the QBM-VAE generates a latent space that better preserves complex biological structures, consistently outperforming conventional Gaussian-based deep learning models like VAE and SCVI in essential tasks such as omics data integration, cell-type classification, and trajectory inference. It also provides a typical example of introducing a physics priori into deep learning to drive the model to acquire scientific discovery capabilities that breaks through data limitations. This work provides the demonstration of a practical quantum advantage in deep learning on a large-scale scientific problem and offers a transferable blueprint for developing hybrid quantum AI models.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºæ¦‚ç‡æ·±åº¦å­¦ä¹ (Probabilistic Deep Learning)é•¿æœŸä¾èµ–é«˜æ–¯å…ˆéªŒ(Gaussian priors)çš„å±€é™æ€§ï¼Œå¯¼è‡´å…¶éš¾ä»¥æ•æ‰å¤æ‚ç”Ÿç‰©æ•°æ®ä¸­çš„éé«˜æ–¯ç‰¹å¾ï¼Œé™åˆ¶äº†ç§‘å­¦å‘ç°çš„ä¿çœŸåº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†QBM-VAEï¼Œè¿™æ˜¯ä¸€ç§å¤§è§„æ¨¡ä¸”é•¿æ—¶é—´ç¨³å®šçš„æ··åˆé‡å­-ç»å…¸æ¶æ„ï¼Œåˆ©ç”¨é‡å­å¤„ç†å™¨å®ç°ä»ç‰©ç†é©±åŠ¨çš„ç»å°”å…¹æ›¼åˆ†å¸ƒ(Boltzmann distribution)ä¸­é«˜æ•ˆé‡‡æ ·ã€‚é€šè¿‡å°†è¯¥åˆ†å¸ƒä½œä¸ºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§å…ˆéªŒï¼ŒQBM-VAEåœ¨å¤„ç†ç™¾ä¸‡çº§å•ç»†èƒæ•°æ®é›†æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å¤æ‚çš„ç”Ÿç‰©ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç»„å­¦æ•°æ®æ•´åˆ(omics data integration)ã€ç»†èƒç±»å‹åˆ†ç±»å’Œè½¨è¿¹æ¨æ–­(trajectory inference)ç­‰å…³é”®ä»»åŠ¡ä¸Šï¼Œä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„VAEå’ŒSCVIç­‰åŸºäºé«˜æ–¯åˆ†å¸ƒçš„æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæˆåŠŸåœ¨å¤§å‹ç§‘å­¦é—®é¢˜ä¸­å±•ç¤ºäº†å®é™…çš„é‡å­ä¼˜åŠ¿(quantum advantage)ï¼Œè¯æ˜äº†å¼•å…¥ç‰©ç†å…ˆéªŒå¯ä»¥é©±åŠ¨æ¨¡å‹çªç ´æ•°æ®é™åˆ¶å¹¶è·å–ç§‘å­¦å‘ç°èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘ç‰©ç†é©±åŠ¨çš„æ··åˆé‡å­äººå·¥æ™ºèƒ½(hybrid quantum AI models)æä¾›äº†å¯è¿ç§»çš„è“å›¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11190v1",
      "published_date": "2025-08-15 03:51:20 UTC",
      "updated_date": "2025-08-15 03:51:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:41.768570+00:00"
    },
    {
      "arxiv_id": "2508.11182v1",
      "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation",
      "title_zh": "è®ºéå¹³å¦åŸºäºå‡è®¾è®ºè¾©ä¸­çš„å¼ºå¯é‡‡çº³æ€§ä¸å¼±å¯é‡‡çº³æ€§",
      "authors": [
        "Matti Berthold",
        "Lydia BlÃ¼mel",
        "Anna Rapberger"
      ],
      "abstract": "In this work, we broaden the investigation of admissibility notions in the context of assumption-based argumentation (ABA). More specifically, we study two prominent alternatives to the standard notion of admissibility from abstract argumentation, namely strong and weak admissibility, and introduce the respective preferred, complete and grounded semantics for general (sometimes called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation frameworks (BSAFs) as formal playground since they concisely capture the relations between assumptions and are expressive enough to represent general non-flat ABA frameworks, as recently shown. While weak admissibility has been recently investigated for a restricted fragment of ABA in which assumptions cannot be derived (flat ABA), strong admissibility has not been investigated for ABA so far. We introduce strong admissibility for ABA and investigate desirable properties. We furthermore extend the recent investigations of weak admissibility in the flat ABA fragment to the non-flat case. We show that the central modularization property is maintained under classical, strong, and weak admissibility. We also show that strong and weakly admissible semantics in non-flat ABA share some of the shortcomings of standard admissible semantics and discuss ways to address these.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰©å±•äº†åŸºäºå‡è®¾çš„è®ºè¯(Assumption-Based Argumentation, ABA)æ¡†æ¶ä¸­å…³äºå¯æ¥å—æ€§(admissibility)æ¦‚å¿µçš„ç ”ç©¶ï¼Œé‡ç‚¹æ¢è®¨äº†å¼ºå¯æ¥å—æ€§(strong admissibility)ä¸å¼±å¯æ¥å—æ€§(weak admissibility)åœ¨éå¹³å¦(non-flat) ABAä¸­çš„åº”ç”¨ã€‚ä½œè€…åˆ©ç”¨æŠ½è±¡åŒæé›†åˆè®ºè¯æ¡†æ¶(BSAFs)ä½œä¸ºå½¢å¼åŒ–å·¥å…·ï¼Œä¸ºé€šç”¨ABAå¼•å…¥äº†ç›¸åº”çš„ä¼˜å…ˆ(preferred)ã€å®Œå¤‡(complete)å’ŒåŸºç¡€(grounded)è¯­ä¹‰ã€‚è¯¥è®ºæ–‡é¦–æ¬¡å°†å¼ºå¯æ¥å—æ€§å¼•å…¥ABAé¢†åŸŸï¼Œå¹¶å°†å¼±å¯æ¥å—æ€§çš„ç ”ç©¶ä»å¹³å¦ç‰‡æ®µæ‰©å±•è‡³éå¹³å¦æƒ…å½¢ã€‚ç ”ç©¶è¯æ˜äº†æ ¸å¿ƒçš„æ¨¡å—åŒ–å±æ€§(modularization property)åœ¨ç»å…¸ã€å¼ºå’Œå¼±å¯æ¥å—æ€§å®šä¹‰ä¸‹å‡èƒ½ä¿æŒã€‚æœ€åï¼Œä½œè€…åˆ†æäº†éå¹³å¦ABAä¸­å¼ºå¼±å¯æ¥å—è¯­ä¹‰æ‰€é¢ä¸´çš„å±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†æ½œåœ¨çš„è§£å†³é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11182v1",
      "published_date": "2025-08-15 03:13:07 UTC",
      "updated_date": "2025-08-15 03:13:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:41.277311+00:00"
    },
    {
      "arxiv_id": "2508.11180v1",
      "title": "A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels",
      "title_zh": "é¢å‘æ ‡ç­¾ç¼ºå¤±çš„ä¸å®Œæ•´å¤šè§†å›¾æ•°æ®æ•´åˆåŠç›‘ç£ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Yiyang Shen",
        "Weiran Wang"
      ],
      "abstract": "Multi-view learning is widely applied to real-life datasets, such as multiple omics biological data, but it often suffers from both missing views and missing labels. Prior probabilistic approaches addressed the missing view problem by using a product-of-experts scheme to aggregate representations from present views and achieved superior performance over deterministic classifiers, using the information bottleneck (IB) principle. However, the IB framework is inherently fully supervised and cannot leverage unlabeled data. In this work, we propose a semi-supervised generative model that utilizes both labeled and unlabeled samples in a unified framework. Our method maximizes the likelihood of unlabeled samples to learn a latent space shared with the IB on labeled data. We also perform cross-view mutual information maximization in the latent space to enhance the extraction of shared information across views. Compared to existing approaches, our model achieves better predictive and imputation performance on both image and multi-omics data with missing views and limited labeled samples.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŠç›‘ç£ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ Multi-view learning åœ¨å¤„ç† incomplete multi-view data æ—¶é¢ä¸´çš„è§†å›¾ç¼ºå¤±å’Œæ ‡ç­¾ç¼ºå¤±åŒé‡æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ç»Ÿä¸€æ¡†æ¶å†…æ•´åˆæœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ ·æœ¬ï¼Œåˆ©ç”¨æœ€å¤§åŒ–æ— æ ‡ç­¾æ•°æ®çš„ likelihood æ¥å­¦ä¹ ä¸åŸºäº Information Bottleneck (IB) åŸç†çš„æœ‰æ ‡ç­¾æ•°æ®å…±äº«çš„ latent spaceã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº† cross-view mutual information maximization æœºåˆ¶ä»¥å¢å¼ºéšç©ºé—´ä¸­è·¨è§†å›¾å…±äº«ä¿¡æ¯çš„æå–ã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œè¯¥æ¨¡å‹åœ¨è§†å›¾ç¼ºå¤±ä¸”æ ‡æ³¨æ ·æœ¬æœ‰é™çš„å›¾åƒåŠ multi-omics æ•°æ®é›†ä¸Šå±•ç°äº†æ›´ä¼˜çš„é¢„æµ‹ä¸ imputation æ€§èƒ½ï¼Œä¸ºä¸å®Œæ•´å¤šè§†å›¾æ•°æ®çš„æ•´åˆæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11180v1",
      "published_date": "2025-08-15 03:10:18 UTC",
      "updated_date": "2025-08-15 03:10:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:09:48.892592+00:00"
    },
    {
      "arxiv_id": "2508.11170v1",
      "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss",
      "title_zh": "ä¼˜åŒ– VQA ç›‘ç£å¾®è°ƒï¼šä»…æ•´æ•°æŸå¤±",
      "authors": [
        "Baihong Qian",
        "Haotian Fan",
        "Wenjie Liao",
        "Yunqiu Wang",
        "Tao Li",
        "Junhui Cui"
      ],
      "abstract": "With the rapid advancement of vision language models(VLM), their ability to assess visual content based on specific criteria and dimensions has become increasingly critical for applications such as video-theme consistency assessment and visual quality scoring. However, existing methods often suffer from imprecise results and inefficient loss calculation, which limit the focus of the model on key evaluation indicators. To address this, we propose IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to enhance their performance in video quality assessment tasks. The key innovation of IOVQA lies in its label construction and its targeted loss calculation mechanism. Specifically, during dataset curation, we constrain the model's output to integers within the range of [10,50], ensuring numerical stability, and convert decimal Overall_MOS to integer before using them as labels. We also introduce a target-mask strategy: when computing the loss, only the first two-digit-integer of the label is unmasked, forcing the model to learn the critical components of the numerical evaluation. After fine-tuning the Qwen2.5-VL model using the constructed dataset, experimental results demonstrate that the proposed method significantly improves the model's accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025 GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work highlights the effectiveness of merely leaving integer labels during fine-tuning, providing an effective idea for optimizing VLMs in quantitative evaluation scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è§†é¢‘è´¨é‡è¯„ä¼°(VQA)ä»»åŠ¡ä¸­å­˜åœ¨çš„è¯„ä¼°ä¸ç²¾ç¡®å’ŒæŸå¤±è®¡ç®—ä½æ•ˆé—®é¢˜ï¼Œæå‡ºäº†åä¸ºIOVQA(Integer-only VQA)çš„æ–°å‹å¾®è°ƒæ–¹æ³•ã€‚IOVQAçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ ‡ç­¾æ„å»ºä¸é’ˆå¯¹æ€§çš„æŸå¤±è®¡ç®—æœºåˆ¶ï¼Œé€šè¿‡å°†å°æ•°å½¢å¼çš„Overall_MOSè½¬æ¢ä¸º[10, 50]èŒƒå›´å†…çš„æ•´æ•°æ ‡ç­¾ï¼Œæœ‰æ•ˆç¡®ä¿äº†æ•°å€¼ç¨³å®šæ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†ç›®æ ‡æ©ç (target-mask)ç­–ç•¥ï¼Œåœ¨è®¡ç®—æŸå¤±æ—¶ä»…å–æ¶ˆæ ‡ç­¾å‰ä¸¤ä½æ•´æ•°çš„æ©ç ï¼Œä»è€Œè¿«ä½¿æ¨¡å‹èšç„¦äºæ•°å€¼è¯„ä¼°çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡è¯¥æ–¹æ³•å¾®è°ƒçš„Qwen2.5-VLæ¨¡å‹åœ¨VQAä»»åŠ¡çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶æœ€ç»ˆåœ¨VQualA 2025 GenAI-Bench AIGCè§†é¢‘è´¨é‡è¯„ä¼°æŒ‘æˆ˜èµ›ç¬¬ä¸€èµ›é“ä¸­è£è·ç¬¬ä¸‰åã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åœ¨Supervised Fine-tuningè¿‡ç¨‹ä¸­ä»…ä¿ç•™æ•´æ•°æ ‡ç­¾çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¼˜åŒ–VLMåœ¨å®šé‡è¯„ä¼°åœºæ™¯ä¸‹çš„æ€§èƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11170v1",
      "published_date": "2025-08-15 02:40:43 UTC",
      "updated_date": "2025-08-15 02:40:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:10:48.387075+00:00"
    },
    {
      "arxiv_id": "2508.11158v1",
      "title": "Role-Augmented Intent-Driven Generative Search Engine Optimization",
      "title_zh": "è§’è‰²å¢å¼ºå‹æ„å›¾é©±åŠ¨çš„ç”Ÿæˆå¼æœç´¢å¼•æ“ä¼˜åŒ–",
      "authors": [
        "Xiaolu Chen",
        "Haojie Wu",
        "Jie Bao",
        "Zhen Chen",
        "Yong Liao",
        "Hu Huang"
      ],
      "abstract": "Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), are reshaping information retrieval. While commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive semantic synthesis capabilities, their black-box nature fundamentally undermines established Search Engine Optimization (SEO) practices. Content creators face a critical challenge: their optimization strategies, effective in traditional search engines, are misaligned with generative retrieval contexts, resulting in diminished visibility. To bridge this gap, we propose a Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO) method, providing a structured optimization pathway tailored for GSE scenarios. Our method models search intent through reflective refinement across diverse informational roles, enabling targeted content enhancement. To better evaluate the method under realistic settings, we address the benchmarking limitations of prior work by: (1) extending the GEO dataset with diversified query variations reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a 6-level LLM-augmented evaluation rubric for fine-grained human-aligned assessment. Experimental results demonstrate that search intent serves as an effective signal for guiding content optimization, yielding significant improvements over single-aspect baseline approaches in both subjective impressions and objective content visibility within GSE responses.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æœç´¢å¼•æ“(GSEs)å¯¹ä¼ ç»Ÿæœç´¢å¼•æ“ä¼˜åŒ–(SEO)å®è·µå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†è§’è‰²å¢å¼ºçš„æ„å›¾é©±åŠ¨å‹ç”Ÿæˆå¼æœç´¢å¼•æ“ä¼˜åŒ–(Role-Augmented Intent-Driven G-SEO)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¸åŒä¿¡æ¯è§’è‰²é—´è¿›è¡Œåæ€æ€§æç‚¼æ¥å»ºæ¨¡æœç´¢æ„å›¾(search intent)ï¼Œä»è€Œå®ç°é’ˆå¯¹æ€§çš„å†…å®¹å¢å¼ºã€‚ä¸ºäº†åœ¨ç°å®ç¯å¢ƒä¸‹æ›´å¥½åœ°è¯„ä¼°è¯¥æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿæ‰©å±•äº†GEOæ•°æ®é›†ä»¥åæ˜ çœŸå®çš„æœç´¢åœºæ™¯ï¼Œå¹¶å¼•å…¥äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)å¢å¼ºçš„å…­çº§ç»†ç²’åº¦è¯„ä¼°å‡†åˆ™G-Eval 2.0ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæœç´¢æ„å›¾æ˜¯å¼•å¯¼å†…å®¹ä¼˜åŒ–çš„æœ‰æ•ˆä¿¡å·ï¼Œåœ¨æå‡GSEå“åº”ä¸­çš„å†…å®¹å¯è§åº¦(visibility)å’Œä¸»è§‚è¯„ä»·æ–¹é¢ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•ç»´åº¦åŸºçº¿æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "7 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11158v1",
      "published_date": "2025-08-15 02:08:55 UTC",
      "updated_date": "2025-08-15 02:08:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:10:46.896631+00:00"
    },
    {
      "arxiv_id": "2508.11152v1",
      "title": "AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions",
      "title_zh": "AlphaAgentsï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‚¡ç¥¨æŠ•èµ„ç»„åˆæ„å»ºå¤šæ™ºèƒ½ä½“",
      "authors": [
        "Tianjiao Zhao",
        "Jingrao Lyu",
        "Stokes Jones",
        "Harrison Garber",
        "Stefano Pasquali",
        "Dhagash Mehta"
      ],
      "abstract": "The field of artificial intelligence (AI) agents is evolving rapidly, driven by the capabilities of Large Language Models (LLMs) to autonomously perform and refine tasks with human-like efficiency and adaptability. In this context, multi-agent collaboration has emerged as a promising approach, enabling multiple AI agents to work together to solve complex challenges. This study investigates the application of role-based multi-agent systems to support stock selection in equity research and portfolio management. We present a comprehensive analysis performed by a team of specialized agents and evaluate their stock-picking performance against established benchmarks under varying levels of risk tolerance. Furthermore, we examine the advantages and limitations of employing multi-agent frameworks in equity analysis, offering critical insights into their practical efficacy and implementation challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent Systems)åœ¨è‚¡æƒç ”ç©¶å’ŒæŠ•èµ„ç»„åˆç®¡ç†ä¸­æ”¯æŒè‚¡ç¥¨é€‰æ‹©(Stock Selection)çš„åº”ç”¨ã€‚è®ºæ–‡æå‡ºäº†AlphaAgentsæ¡†æ¶ï¼Œé€šè¿‡åŸºäºè§’è‰²çš„å¤šæ™ºèƒ½ä½“åä½œ(Multi-Agent Collaboration)è‡ªä¸»æ‰§è¡Œå¹¶å®Œå–„å¤æ‚çš„é‡‘èåˆ†æä»»åŠ¡ã€‚ç ”ç©¶ç”±ä¸“é—¨çš„æ™ºèƒ½ä½“å›¢é˜Ÿè¿›è¡Œå…¨é¢åˆ†æï¼Œå¹¶åœ¨ä¸åŒçš„é£é™©æ‰¿å—èƒ½åŠ›(Risk Tolerance)æ°´å¹³ä¸‹è¯„ä¼°å…¶ç›¸å¯¹äºæ—¢å®šåŸºå‡†(Benchmarks)çš„é€‰è‚¡è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¯¦ç»†å®¡æŸ¥äº†åœ¨è‚¡æƒåˆ†æä¸­é‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚è¿™äº›å‘ç°ä¸ºè¯¥æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸçš„å®é™…æ•ˆåŠ›ã€å®æ–½æŒ‘æˆ˜(Implementation Challenges)ä»¥åŠåº”ç”¨å‰æ™¯æä¾›äº†å…³é”®çš„æ´å¯Ÿã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11152v1",
      "published_date": "2025-08-15 01:49:56 UTC",
      "updated_date": "2025-08-15 01:49:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:10:56.149909+00:00"
    },
    {
      "arxiv_id": "2508.11143v1",
      "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward",
      "title_zh": "é¢å‘è¿ç»­åŠ¨ä½œå—çš„ Actor-Criticï¼šä¸€ç§é€‚ç”¨äºç¨€ç–å¥–åŠ±é•¿æ—¶ç¨‹æœºå™¨äººæ“æ§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Jiarui Yang",
        "Bin Zhu",
        "Jingjing Chen",
        "Yu-Gang Jiang"
      ],
      "abstract": "Existing reinforcement learning (RL) methods struggle with long-horizon robotic manipulation tasks, particularly those involving sparse rewards. While action chunking is a promising paradigm for robotic manipulation, using RL to directly learn continuous action chunks in a stable and data-efficient manner remains a critical challenge. This paper introduces AC3 (Actor-Critic for Continuous Chunks), a novel RL framework that learns to generate high-dimensional, continuous action sequences. To make this learning process stable and data-efficient, AC3 incorporates targeted stabilization mechanisms for both the actor and the critic. First, to ensure reliable policy improvement, the actor is trained with an asymmetric update rule, learning exclusively from successful trajectories. Second, to enable effective value learning despite sparse rewards, the critic's update is stabilized using intra-chunk $n$-step returns and further enriched by a self-supervised module providing intrinsic rewards at anchor points aligned with each action chunk. We conducted extensive experiments on 25 tasks from the BiGym and RLBench benchmarks. Results show that by using only a few demonstrations and a simple model architecture, AC3 achieves superior success rates on most tasks, validating its effective design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿ç¨‹æœºå™¨äººæ“çºµä»»åŠ¡åœ¨ç¨€ç–å¥–åŠ±(Sparse Reward)ç¯å¢ƒä¸‹éš¾ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç¨³å®šä¸”é«˜æ•ˆå­¦ä¹ çš„é—®é¢˜ï¼Œæå‡ºäº†AC3 (Actor-Critic for Continuous Chunks) æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸“æ³¨äºå­¦ä¹ ç”Ÿæˆé«˜ç»´è¿ç»­çš„åŠ¨ä½œåºåˆ—ï¼Œå³åŠ¨ä½œå—(Action Chunking)ï¼Œå¹¶ä¸ºActorå’ŒCriticè®¾è®¡äº†é’ˆå¯¹æ€§çš„ç¨³å®šæœºåˆ¶ã€‚åœ¨Actorè®­ç»ƒä¸­ï¼ŒAC3é‡‡ç”¨äº†ä¸å¯¹ç§°æ›´æ–°è§„åˆ™(Asymmetric Update Rule)ï¼Œä½¿å…¶ä¸“é—¨ä»æˆåŠŸè½¨è¿¹ä¸­æå–çŸ¥è¯†ä»¥å¢å¼ºç­–ç•¥æå‡çš„å¯é æ€§ã€‚åœ¨Criticç«¯ï¼Œé€šè¿‡å¼•å…¥åŠ¨ä½œå—å†…(Intra-chunk)çš„næ­¥å›æŠ¥(n-step returns)ä»¥åŠåœ¨é”šç‚¹å¤„æä¾›å†…åœ¨å¥–åŠ±(Intrinsic Rewards)çš„è‡ªç›‘ç£æ¨¡å—ï¼Œæœ‰æ•ˆç¼“è§£äº†ç¨€ç–å¥–åŠ±ä¸‹çš„ä»·å€¼å­¦ä¹ éš¾åº¦ã€‚åœ¨BiGymå’ŒRLBenchåŸºå‡†æµ‹è¯•çš„25é¡¹ä»»åŠ¡ä¸­ï¼Œå®éªŒç»“æœè¯æ˜AC3ä»…å‡­å°‘é‡æ¼”ç¤º(Demonstrations)å’Œç®€å•æ¨¡å‹æ¶æ„ï¼Œä¾¿åœ¨å¤šæ•°ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚æ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11143v1",
      "published_date": "2025-08-15 01:27:15 UTC",
      "updated_date": "2025-08-15 01:27:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:10:55.355914+00:00"
    },
    {
      "arxiv_id": "2508.11721v1",
      "title": "FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis",
      "title_zh": "FusionFMï¼šèåˆçœ¼ç§‘ä¸“ç”¨åŸºç¡€æ¨¡å‹ä»¥ä¼˜åŒ–çœ¼ç§‘è¯Šæ–­",
      "authors": [
        "Ke Zou",
        "Jocelyn Hui Lin Goh",
        "Yukun Zhou",
        "Tian Lin",
        "Samantha Min Er Yew",
        "Sahana Srinivasan",
        "Meng Wang",
        "Rui Santos",
        "Gabor M. Somfai",
        "Huazhu Fu",
        "Haoyu Chen",
        "Pearse A. Keane",
        "Ching-Yu Cheng",
        "Yih Chung Tham"
      ],
      "abstract": "Foundation models (FMs) have shown great promise in medical image analysis by improving generalization across diverse downstream tasks. In ophthalmology, several FMs have recently emerged, but there is still no clear answer to fundamental questions: Which FM performs the best? Are they equally good across different tasks? What if we combine all FMs together? To our knowledge, this is the first study to systematically evaluate both single and fused ophthalmic FMs. To address these questions, we propose FusionFM, a comprehensive evaluation suite, along with two fusion approaches to integrate different ophthalmic FMs. Our framework covers both ophthalmic disease detection (glaucoma, diabetic retinopathy, and age-related macular degeneration) and systemic disease prediction (diabetes and hypertension) based on retinal imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM, RetiZero, and DINORET) using standardized datasets from multiple countries and evaluated their performance using AUC and F1 metrics. Our results show that DINORET and RetiZero achieve superior performance in both ophthalmic and systemic disease tasks, with RetiZero exhibiting stronger generalization on external datasets. Regarding fusion strategies, the Gating-based approach provides modest improvements in predicting glaucoma, AMD, and hypertension. Despite these advances, predicting systemic diseases, especially hypertension in external cohort remains challenging. These findings provide an evidence-based evaluation of ophthalmic FMs, highlight the benefits of model fusion, and point to strategies for enhancing their clinical applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FusionFMï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿè¯„ä¼°å•ä½“åŠèåˆçœ¼ç§‘åŸºç¡€æ¨¡å‹ï¼ˆFoundation models, FMsï¼‰çš„ç»¼åˆè¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³ä¸åŒæ¨¡å‹åœ¨å¤šæ ·åŒ–ä¸´åºŠä»»åŠ¡ä¸­æ€§èƒ½è¡¨ç°ä¸æ˜ç¡®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†åŸºäºè§†ç½‘è†œå›¾åƒçš„çœ¼ç§‘ç–¾ç—…æ£€æµ‹ï¼ˆå¦‚é’å…‰çœ¼ã€ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ã€å¹´é¾„ç›¸å…³æ€§é»„æ–‘å˜æ€§ï¼‰åŠå…¨èº«æ€§ç–¾ç—…é¢„æµ‹ï¼ˆç³–å°¿ç—…å’Œé«˜è¡€å‹ï¼‰ï¼Œå¹¶å¯¹æ¯”äº†RETFoundã€VisionFMã€RetiZeroå’ŒDINORETå››ç§æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDINORETå’ŒRetiZeroåœ¨å„ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­RetiZeroåœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨èåˆç­–ç•¥æ–¹é¢ï¼Œç ”ç©¶å‘ç°åŸºäºGating-basedçš„æ–¹æ³•åœ¨é¢„æµ‹é’å…‰çœ¼ã€AMDå’Œé«˜è¡€å‹æ–¹é¢å¸¦æ¥äº†æ€§èƒ½æå‡ã€‚å°½ç®¡åŸºç¡€æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤–éƒ¨é˜Ÿåˆ—ä¸­å‡†ç¡®é¢„æµ‹å…¨èº«æ€§ç–¾ç—…ï¼ˆç‰¹åˆ«æ˜¯é«˜è¡€å‹ï¼‰ä¾ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ºçœ¼ç§‘åŸºç¡€æ¨¡å‹çš„ä¸´åºŠåº”ç”¨æä¾›äº†åŸºäºè¯æ®çš„è¯„ä¼°ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹èåˆåœ¨ä¼˜åŒ–çœ¼ç§‘è¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11721v1",
      "published_date": "2025-08-15 01:17:52 UTC",
      "updated_date": "2025-08-15 01:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:10:55.145818+00:00"
    },
    {
      "arxiv_id": "2508.14094v4",
      "title": "Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets",
      "title_zh": "éš¾ä¾‹å³å…¨éƒ¨ï¼šåœ¨æ ‡æ³¨é¢„ç®—çº¦æŸä¸‹æœ€å¤§åŒ– GRPO åè®­ç»ƒæ•ˆèƒ½",
      "authors": [
        "Benjamin Pikus",
        "Pratyush Ranjan Tiwari",
        "Burton Ye"
      ],
      "abstract": "Collecting high-quality training examples for language model fine-tuning is expensive, with practical budgets limiting the amount of data that can be procured. We investigate whether example difficulty affects GRPO training effectiveness by comparing selection strategies (easy, medium, hard, random) across multiple models and reasoning tasks. Training on the hardest 10\\% of examples (those where the base model fails most often) yields dramatic performance gains up to 47\\%, while easy examples produce minimal improvements of 3-15\\%. This occurs because GRPO requires outcome variance to generate learning signals; hard examples maintain mixed success/failure outcomes throughout training while easy examples quickly converge to consistent success, eliminating learning opportunities. Moreover, models trained on hard examples show superior out-of-distribution generalization, with only hard-trained models achieving meaningful gains on the AIME2025 benchmark. Our findings provide clear guidance: when budget-constrained, prioritize collecting and annotating examples where your base model struggles, as these drive nearly all learning value in GRPO fine-tuning",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ ‡æ³¨é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ ·æœ¬éš¾åº¦å¯¹Group Relative Policy Optimization (GRPO) è®­ç»ƒæ•ˆæœçš„å½±å“ã€‚é€šè¿‡åœ¨å¤šç§æ¨¡å‹å’Œæ¨ç†ä»»åŠ¡ä¸­å¯¹æ¯”ç®€å•ã€ä¸­ç­‰ã€å›°éš¾åŠéšæœºçš„æ ·æœ¬ç­›é€‰ç­–ç•¥ï¼Œä½œè€…å‘ç°è®­ç»ƒæœ€å›°éš¾çš„10%æ ·æœ¬ï¼ˆå³åŸºç¡€æ¨¡å‹æœ€å¸¸å¤±è´¥çš„æ ·æœ¬ï¼‰èƒ½å¸¦æ¥é«˜è¾¾47%çš„æ€§èƒ½æå‡ï¼Œè€Œç®€å•æ ·æœ¬çš„æ”¹è¿›æ•ˆæœä»…ä¸º3-15%ã€‚è¿™ä¸€ç°è±¡çš„æ ¸å¿ƒåœ¨äºGRPOéœ€è¦ç»“æœæ–¹å·® (outcome variance) æ¥äº§ç”Ÿæœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ï¼Œå›°éš¾æ ·æœ¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½ç»´æŒæˆåŠŸä¸å¤±è´¥çš„æ··åˆçŠ¶æ€ï¼Œè€Œç®€å•æ ·æœ¬ä¼šè¿…é€Ÿæ”¶æ•›è‡³ä¸€è‡´æˆåŠŸï¼Œä»è€Œå¤±å»å­¦ä¹ æœºä¼šã€‚æ­¤å¤–ï¼ŒåŸºäºå›°éš¾æ ·æœ¬è®­ç»ƒçš„æ¨¡å‹åœ¨åˆ†å¸ƒå¤– (out-of-distribution) æ³›åŒ–æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œæ˜¯å”¯ä¸€èƒ½åœ¨AIME2025åŸºå‡†æµ‹è¯•ä¸­è·å¾—æ˜¾è‘—æ”¶ç›Šçš„ç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ºæœ‰é™é¢„ç®—ä¸‹çš„æ¨¡å‹å¾®è°ƒæä¾›äº†æ˜ç¡®æŒ‡å¯¼ï¼Œå³åº”ä¼˜å…ˆæ”¶é›†å’Œæ ‡æ³¨åŸºç¡€æ¨¡å‹éš¾ä»¥å¤„ç†çš„æ ·æœ¬ï¼Œå› ä¸ºå®ƒä»¬è´¡çŒ®äº†GRPOå¾®è°ƒä¸­å‡ ä¹æ‰€æœ‰çš„å­¦ä¹ ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14094v4",
      "published_date": "2025-08-15 01:14:06 UTC",
      "updated_date": "2025-10-14 01:03:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:11:07.182559+00:00"
    },
    {
      "arxiv_id": "2508.11141v1",
      "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations",
      "title_zh": "é€šè¿‡æ¢ç´¢æ–‡æœ¬ä¸å›¾åƒå†…éƒ¨ç›¸å…³æ€§çš„å¯¹æ¯”å­¦ä¹ è·¨æ¨¡æ€è°£è¨€æ£€æµ‹æ–¹æ¡ˆ",
      "authors": [
        "Bin Ma",
        "Yifei Zhang",
        "Yongjin Xian",
        "Qi Li",
        "Linna Zhou",
        "Gongxun Miao"
      ],
      "abstract": "Existing rumor detection methods often neglect the content within images as well as the inherent relationships between contexts and images across different visual scales, thereby resulting in the loss of critical information pertinent to rumor identification. To address these issues, this paper presents a novel cross-modal rumor detection scheme based on contrastive learning, namely the Multi-scale Image and Context Correlation exploration algorithm (MICC). Specifically, we design an SCLIP encoder to generate unified semantic embeddings for text and multi-scale image patches through contrastive pretraining, enabling their relevance to be measured via dot-product similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is introduced to identify image regions most relevant to the textual semantics, guided by mutual information maximization and the information bottleneck principle, through a Top-K selection strategy based on a cross-modal relevance matrix constructed between the text and multi-scale image patches. Moreover, a scale-aware fusion network is designed to integrate the highly correlated multi-scale image features with global text features by assigning adaptive weights to image regions based on their semantic importance and cross-modal relevance. The proposed methodology has been extensively evaluated on two real-world datasets. The experimental results demonstrate that it achieves a substantial performance improvement over existing state-of-the-art approaches in rumor detection, highlighting its effectiveness and potential for practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„æ–°å‹è·¨æ¨¡æ€è°£è¨€æ£€æµ‹æ–¹æ¡ˆMICCï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¿½ç•¥å›¾åƒå†…å®¹åŠå›¾æ–‡å¤šå°ºåº¦å†…åœ¨è”ç³»å¯¼è‡´çš„è¯†åˆ«ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚ç ”ç©¶è®¾è®¡äº†SCLIPç¼–ç å™¨ï¼Œé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒç”Ÿæˆæ–‡æœ¬ä¸å¤šå°ºåº¦å›¾åƒå—çš„ç»Ÿä¸€è¯­ä¹‰åµŒå…¥ï¼Œå®ç°äº†è·¨æ¨¡æ€ç›¸å…³æ€§çš„ç²¾å‡†è¡¡é‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ–¹æ¡ˆå¼•å…¥äº†è·¨æ¨¡æ€å¤šå°ºåº¦å¯¹é½(Cross-Modal Multi-Scale Alignment)æ¨¡å—ï¼ŒåŸºäºäº’ä¿¡æ¯æœ€å¤§åŒ–å’Œä¿¡æ¯ç“¶é¢ˆåŸåˆ™(Information Bottleneck Principle)ï¼Œé€šè¿‡Top-Kç­–ç•¥ç­›é€‰å‡ºä¸æ–‡æœ¬è¯­ä¹‰æœ€ç›¸å…³çš„å›¾åƒåŒºåŸŸã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨å°ºåº¦æ„ŸçŸ¥èåˆç½‘ç»œï¼Œä¾æ®è¯­ä¹‰é‡è¦æ€§ä¸ºå¤šå°ºåº¦å›¾åƒç‰¹å¾åˆ†é…è‡ªé€‚åº”æƒé‡ï¼Œå¹¶å°†å…¶ä¸å…¨å±€æ–‡æœ¬ç‰¹å¾æ·±åº¦é›†æˆã€‚å®éªŒåœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰å…ˆè¿›æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ¢ç´¢å›¾æ–‡å†…éƒ¨ç›¸å…³æ€§å¯¹è°£è¨€æ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11141v1",
      "published_date": "2025-08-15 01:13:50 UTC",
      "updated_date": "2025-08-15 01:13:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:11:18.386546+00:00"
    },
    {
      "arxiv_id": "2508.11133v2",
      "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents",
      "title_zh": "MoNaCoï¼šé¢å‘æ•°åç¯‡æ–‡æ¡£æ¨ç†çš„æ›´è‡ªç„¶ã€æ›´å¤æ‚çš„é—®é¢˜",
      "authors": [
        "Tomer Wolfson",
        "Harsh Trivedi",
        "Mor Geva",
        "Yoav Goldberg",
        "Dan Roth",
        "Tushar Khot",
        "Ashish Sabharwal",
        "Reut Tsarfaty"
      ],
      "abstract": "Automated agents, powered by Large language models (LLMs), are emerging as the go-to tool for querying information. However, evaluation benchmarks for LLM agents rarely feature natural questions that are both information-seeking and genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and time-consuming questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer real-world time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the limitations of LLM-powered agents in handling the complexity and sheer breadth of real-world information-seeking tasks -- with MoNaCo providing an effective resource for tracking such progress. The MoNaCo benchmark, codebase, prompts and models predictions are all publicly available at: https://tomerwolgithub.github.io/monaco",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MoNaCoï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 1,315 ä¸ªè‡ªç„¶ä¸”è€—æ—¶é—®é¢˜çš„ Benchmarkï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰è¯„ä¼°ä¸­ç¼ºä¹çœŸå®å¤æ‚ä¿¡æ¯å¯»æ±‚ä»»åŠ¡çš„ç©ºç™½ã€‚ä¸ºäº†æ„å»ºè¯¥åŸºå‡†ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§ Decomposed Annotation Pipelineï¼Œç”¨äºå¤§è§„æ¨¡è·å–å¹¶æ‰‹åŠ¨å›ç­”éœ€è¦è·¨è¶Šæ•°åç”šè‡³æ•°ç™¾ä¸ªä¸­é—´æ­¥éª¤çš„ç°å®é—®é¢˜ï¼Œå…¶ä»»åŠ¡éš¾åº¦è¿œè¶…ç°æœ‰çš„ QA åŸºå‡†ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ Frontier LLMs åœ¨ MoNaCo ä¸Šçš„ F1 åˆ†æ•°æœ€é«˜ä¹Ÿä»…ä¸º 61.2%ï¼Œæ€§èƒ½ç“¶é¢ˆä¸»è¦æºäº Low Recall å’Œ Hallucinationsã€‚MoNaCo çš„ç ”ç©¶ç»“æœæ·±åˆ»æ­ç¤ºäº†å½“å‰ LLM-powered Agents åœ¨å¤„ç†æå…¶å¤æ‚ä¸”è·¨åº¦å¹¿é˜”çš„çœŸå®ä¸–ç•Œä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚è¯¥ Benchmark åŠå…¶ç›¸å…³ä»£ç å’Œæ¨¡å‹é¢„æµ‹å·²å…¬å¼€ï¼Œä¸ºè¿½è¸ªè¯¥é¢†åŸŸçš„æœªæ¥è¿›å±•æä¾›äº†é‡è¦ä¸”æœ‰æ•ˆçš„è¯„ä¼°èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in Transactions of the Association for Computational Linguistics (TACL), 2025. Authors pre-print",
      "pdf_url": "https://arxiv.org/pdf/2508.11133v2",
      "published_date": "2025-08-15 00:58:10 UTC",
      "updated_date": "2025-09-03 17:03:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:11:17.683904+00:00"
    },
    {
      "arxiv_id": "2508.13189v1",
      "title": "Preference Models assume Proportional Hazards of Utilities",
      "title_zh": "åå¥½æ¨¡å‹å‡è®¾æ•ˆç”¨æ»¡è¶³æ¯”ä¾‹é£é™©",
      "authors": [
        "Chirag Nagpal"
      ],
      "abstract": "Approaches for estimating preferences from human annotated data typically involves inducing a distribution over a ranked list of choices such as the Plackett-Luce model. Indeed, modern AI alignment tools such as Reward Modelling and Direct Preference Optimization are based on the statistical assumptions posed by the Plackett-Luce model. In this paper, I will connect the Plackett-Luce model to another classical and well known statistical model, the Cox Proportional Hazards model and attempt to shed some light on the implications of the connection therein.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»äººå·¥æ ‡æ³¨æ•°æ®ä¸­ä¼°è®¡åå¥½çš„ Plackett-Luce æ¨¡å‹ä¸ç»å…¸ç»Ÿè®¡å­¦ä¸­çš„ Cox Proportional Hazards æ¨¡å‹ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚ç ”ç©¶å¼ºè°ƒï¼Œç›®å‰ä¸»æµçš„ AI å¯¹é½å·¥å…·å¦‚ Reward Modelling å’Œ Direct Preference Optimization å‡åŸºäº Plackett-Luce æ¨¡å‹çš„ç»Ÿè®¡å‡è®¾ã€‚ä½œè€…é€šè¿‡å°†è¿™ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå…³è”ï¼Œé˜æ˜äº†åå¥½æ¨¡å‹åœ¨æœ¬è´¨ä¸Šå‡è®¾äº†æ•ˆç”¨çš„æ¯”ä¾‹é£é™©ï¼ˆProportional Hazards of Utilitiesï¼‰ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£åå¥½å­¦ä¹ çš„ç»Ÿè®¡ç‰¹æ€§åŠå…¶åœ¨ AI å¯¹é½ä»»åŠ¡ä¸­çš„æ·±å±‚å«ä¹‰æä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13189v1",
      "published_date": "2025-08-15 00:08:56 UTC",
      "updated_date": "2025-08-15 00:08:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:11:19.486209+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 99,
  "processed_papers_count": 99,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T12:12:09.813293+00:00"
}
{
  "date": "2024-07-10",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-10 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型优化（如 LLM 的鲁棒性、对齐和量化）、机器人导航与视觉语言模型应用，以及医疗和图像处理创新，亮点包括谷歌团队的 PaliGemma 模型和高效的量化训练方法，强调了 LLM 在实际部署中的安全性和泛化能力。\n\n### 重点论文讨论\n我挑选了今天论文中的亮点和相关主题，按照重要性排序，先聊 AI 模型和 LLM 相关的高影响力工作，再聊机器人和医疗应用，其他领域快速掠过。以下是核心摘要，聚焦主要贡献和发现。\n\n1. **PaliGemma: A Versatile 3B VLM for Transfer**  \n   这篇论文由谷歌团队（包括 Lucas Beyer 和 Alexander Kolesnikov）提出，介绍了一个基于 SigLIP 和 Gemma 的轻量级视觉语言模型，主要贡献是提升多任务转移学习能力，支持从标准基准到专业任务（如遥感），实验显示其在 40 个任务上表现出色，提供高效的开源实现。\n\n2. **Neural Driving Style Transfer for Human-Like Vision-Based Autonomous Driving (NDST)**  \n   作者如 Jaerock Kwon 等人开发了受 Neural Style Transfer 启发的 NDST 框架，用于个性化自动驾驶，主要发现是通过 Personalized Block 模块，模型能适应用户驾驶风格，同时确保安全，实验验证了其在提升用户舒适度的潜力。\n\n3. **RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization**  \n   这篇工作提出 RoLoRA 方法，用于 LLM 的量化微调，主要贡献是消除激活异常值并优化权重-激活量化，实验显示在 LLaMA2-13B 上实现高达 29.5% 的准确率提升，显著提高了低位量化模型的鲁棒性。\n\n4. **EfficientQAT: Efficient Quantization-Aware Training for Large Language Models**  \n   作者如 Peng Luo 等人设计了 EfficientQAT 框架，主要发现是通过分块训练和端到端量化参数优化，在 LLaMA-2-70B 上仅用单 GPU 即可在 41 小时内训练出 2-位量化模型，准确率损失小于 3%，适用于大规模 LLM 部署。\n\n5. **Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization**  \n   这篇论文探索了 LLM 对齐的鲁棒性，主要贡献是引入 Dr. DPO 方法，通过分布鲁棒优化处理噪声数据，实验证明其在生成文本质量和响应准确性上显著提升，强调了安全对齐的重要性。\n\n6. **Training on the Test Task Confounds Evaluation and Emergence**  \n   作者如 Moritz Hardt 分析了 LLM 评估中的训练测试任务混淆问题，主要发现是这种混淆会影响模型评估和涌现能力，并提出调整方法来缓解，提供了新的 LLM 基准视角。\n\n7. **Generative Image as Action Models**  \n   这篇工作由 Stephen James 等人提出，将 Stable Diffusion 微调为生成图像作为机器人动作的模型，主要贡献是应用于机器人操作任务，实验显示其在 RLBench 上显著提升鲁棒性和泛化能力。\n\n8. **Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**  \n   作者如 Hao-Tien Lewis Chiang 开发了 Mobility VLA 框架，用于多模态指令导航，主要发现是通过长上下文 VLM 和拓扑图结合，实现机器人高效导航，实世界实验证明其在复杂环境中表现优异。\n\n9. **Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation**  \n   这篇论文提出 GreenAug 方法，用于机器人场景泛化，主要贡献是通过绿色屏背景增强数据，提高模型对新环境的适应性，实验显示其在真实机器人任务中超越传统增强方法。\n\n10. **Transformer Block Coupling and its Correlation with Generalization in LLMs**  \n    作者如 Vardan Papyan 分析了 Transformer 块耦合与 LLM 泛化之间的关系，主要发现是耦合程度正相关于模型性能，并通过实验验证了其在 Vision Transformer 中的有效性。\n\n11. **BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark**  \n    这是一个机器人双臂操作基准数据集，主要贡献是提供 40 个任务的演示数据，支持多模态观察，实验基准了多种模仿学习算法，促进机器人学习研究。\n\n12. **Continuous Control with Coarse-to-fine Reinforcement Learning**  \n    作者如 Younggyo Seo 提出 CQN 方法，用于连续控制任务，主要发现是通过粗到细的强化学习策略，实现高效机器人操作，实验在 RLBench 上展示了其样本效率优势。\n\n13. **Towards Interpretable Foundation Models of Robot Behavior: A Task Specific Policy Generation Approach**  \n    这篇工作引入 DPP 方法，用于可解释的机器人基础模型，主要贡献是生成任务特定策略，减少泛化问题，实验证明其在模拟环境中有效。\n\n14. **Spatial-Temporal Attention Model for Traffic State Estimation with Sparse Internet of Vehicles**  \n    作者如 Haibo Zhou 提出 CRNet 模型，用于交通状态估计，主要发现是通过空间-时间注意力机制，利用稀疏 IoV 数据实现高效估计，实验在真实数据集上验证了其准确性。\n\n15. **MemWarp: Discontinuity-Preserving Cardiac Registration with Memorized Anatomical Filters**  \n    这篇医疗图像论文提出 MemWarp 框架，用于心脏图像配准，主要贡献是处理局部不连续性，提高配准准确性，实验在公开数据集上提升 Dice 分数 7.1%。\n\n其他论文主题多样，如交通流估计（论文 8）、LLM 解释性（论文 14）和医学图像分割（论文 35），但这些相对常规或特定领域，我快速掠过：例如，论文 35 提出新分割模型提升鲁棒性，论文 14 评估 LLM 在文献检索中的性能，论文 8 使用生成 AI 优化交通估计，但细节较少创新点。总体而言，今天的论文突出了 AI 模型的实际应用和优化潜力，读者可关注 LLM 和机器人相关工作以获取前沿洞见。",
  "papers": [
    {
      "arxiv_id": "2407.08093v1",
      "title": "MemWarp: Discontinuity-Preserving Cardiac Registration with Memorized Anatomical Filters",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Zhang",
        "Xiang Chen",
        "Renjiu Hu",
        "Dongdong Liu",
        "Gaolei Li",
        "Rongguang Wang"
      ],
      "abstract": "Many existing learning-based deformable image registration methods impose\nconstraints on deformation fields to ensure they are globally smooth and\ncontinuous. However, this assumption does not hold in cardiac image\nregistration, where different anatomical regions exhibit asymmetric motions\nduring respiration and movements due to sliding organs within the chest.\nConsequently, such global constraints fail to accommodate local discontinuities\nacross organ boundaries, potentially resulting in erroneous and unrealistic\ndisplacement fields. In this paper, we address this issue with MemWarp, a\nlearning framework that leverages a memory network to store prototypical\ninformation tailored to different anatomical regions. MemWarp is different from\nearlier approaches in two main aspects: firstly, by decoupling feature\nextraction from similarity matching in moving and fixed images, it facilitates\nmore effective utilization of feature maps; secondly, despite its capability to\npreserve discontinuities, it eliminates the need for segmentation masks during\nmodel inference. In experiments on a publicly available cardiac dataset, our\nmethod achieves considerable improvements in registration accuracy and\nproducing realistic deformations, outperforming state-of-the-art methods with a\nremarkable 7.1\\% Dice score improvement over the runner-up semi-supervised\nmethod. Source code will be available at https://github.com/tinymilky/Mem-Warp.",
      "tldr_zh": "本文提出 MemWarp，一种基于记忆网络（memory network）的学习框架，用于心脏图像配准，旨在保留局部不连续性（如器官边界处的运动差异），从而解决现有 deformable image registration 方法的全局平滑假设问题。MemWarp 的创新在于解耦特征提取和相似性匹配以更有效地利用特征图，同时在模型推理时无需分割掩码。实验结果显示，在公开心脏数据集上，该方法显著提高了配准准确性和变形真实性，比最先进半监督方法提升了 7.1% 的 Dice score。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "eess.IV",
      "comment": "11 pages, 2 figure, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.08093v1",
      "published_date": "2024-07-10 23:42:29 UTC",
      "updated_date": "2024-07-10 23:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:11:39.686300"
    },
    {
      "arxiv_id": "2407.08073v1",
      "title": "NDST: Neural Driving Style Transfer for Human-Like Vision-Based Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Donghyun Kim",
        "Aws Khalil",
        "Haewoon Nam",
        "Jaerock Kwon"
      ],
      "abstract": "Autonomous Vehicles (AV) and Advanced Driver Assistant Systems (ADAS)\nprioritize safety over comfort. The intertwining factors of safety and comfort\nemerge as pivotal elements in ensuring the effectiveness of Autonomous Driving\n(AD). Users often experience discomfort when AV or ADAS drive the vehicle on\ntheir behalf. Providing a personalized human-like AD experience, tailored to\nmatch users' unique driving styles while adhering to safety prerequisites,\npresents a significant opportunity to boost the acceptance of AVs. This paper\nproposes a novel approach, Neural Driving Style Transfer (NDST), inspired by\nNeural Style Transfer (NST), to address this issue. NDST integrates a\nPersonalized Block (PB) into the conventional Baseline Driving Model (BDM),\nallowing for the transfer of a user's unique driving style while adhering to\nsafety parameters. The PB serves as a self-configuring system, learning and\nadapting to an individual's driving behavior without requiring modifications to\nthe BDM. This approach enables the personalization of AV models, aligning the\ndriving style more closely with user preferences while ensuring baseline safety\ncritical actuation. Two contrasting driving styles (Style A and Style B) were\nused to validate the proposed NDST methodology, demonstrating its efficacy in\ntransferring personal driving styles to the AV system. Our work highlights the\npotential of NDST to enhance user comfort in AVs by providing a personalized\nand familiar driving experience. The findings affirm the feasibility of\nintegrating NDST into existing AV frameworks to bridge the gap between safety\nand individualized driving styles, promoting wider acceptance and improved user\nexperiences.",
      "tldr_zh": "该研究针对自动驾驶（AV）和高级驾驶辅助系统（ADAS）优先安全而忽略用户舒适的问题，提出了一种新型方法 Neural Driving Style Transfer (NDST)，旨在将用户的个性化驾驶风格转移到 AV 系统，同时确保安全标准。NDST 受 Neural Style Transfer (NST) 启发，将 Personalized Block (PB) 整合到 Baseline Driving Model (BDM) 中，使系统能够自主学习和适应个人驾驶行为，而无需修改 BDM。实验通过对比两种驾驶风格（Style A 和 Style B）验证了 NDST 的有效性，显著提升了用户舒适度和 AV 的接受度，为桥接安全与个性化驾驶提供了可行路径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.08073v1",
      "published_date": "2024-07-10 22:26:45 UTC",
      "updated_date": "2024-07-10 22:26:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:11:51.392804"
    },
    {
      "arxiv_id": "2407.08067v1",
      "title": "On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments",
      "title_zh": "翻译失败",
      "authors": [
        "Jingchao Fang",
        "Nikos Arechiga",
        "Keiichi Namaoshi",
        "Nayeli Bravo",
        "Candice Hogan",
        "David A. Shamma"
      ],
      "abstract": "The Wizard of Oz (WoZ) method is a widely adopted research approach where a\nhuman Wizard ``role-plays'' a not readily available technology and interacts\nwith participants to elicit user behaviors and probe the design space. With the\ngrowing ability for modern large language models (LLMs) to role-play, one can\napply LLMs as Wizards in WoZ experiments with better scalability and lower cost\nthan the traditional approach. However, methodological guidance on responsibly\napplying LLMs in WoZ experiments and a systematic evaluation of LLMs'\nrole-playing ability are lacking. Through two LLM-powered WoZ studies, we take\nthe first step towards identifying an experiment lifecycle for researchers to\nsafely integrate LLMs into WoZ experiments and interpret data generated from\nsettings that involve Wizards role-played by LLMs. We also contribute a\nheuristic-based evaluation framework that allows the estimation of LLMs'\nrole-playing ability in WoZ experiments and reveals LLMs' behavior patterns at\nscale.",
      "tldr_zh": "这篇论文探讨了在 Wizard of Oz (WoZ) 实验中使用大型语言模型 (LLMs) 作为 Wizard 的方法，以提升实验的可扩展性和降低成本，同时解决现有指导和评估的缺失。作者通过两个 LLM 驱动的 WoZ 研究，提出一个实验生命周期，帮助研究者安全整合 LLMs 并正确解释生成的数据。论文还贡献了一个基于启发式的评估框架，用于估算 LLMs 的角色扮演能力，并揭示其行为模式。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.m; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "To be published in ACM IVA 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.08067v1",
      "published_date": "2024-07-10 22:05:56 UTC",
      "updated_date": "2024-07-10 22:05:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:12:02.779338"
    },
    {
      "arxiv_id": "2407.08065v1",
      "title": "Towards Interpretable Foundation Models of Robot Behavior: A Task Specific Policy Generation Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Isaac Sheidlower",
        "Reuben Aronson",
        "Elaine Schaertl Short"
      ],
      "abstract": "Foundation models are a promising path toward general-purpose and\nuser-friendly robots. The prevalent approach involves training a generalist\npolicy that, like a reinforcement learning policy, uses observations to output\nactions. Although this approach has seen much success, several concerns arise\nwhen considering deployment and end-user interaction with these systems. In\nparticular, the lack of modularity between tasks means that when model weights\nare updated (e.g., when a user provides feedback), the behavior in other,\nunrelated tasks may be affected. This can negatively impact the system's\ninterpretability and usability. We present an alternative approach to the\ndesign of robot foundation models, Diffusion for Policy Parameters (DPP), which\ngenerates stand-alone, task-specific policies. Since these policies are\ndetached from the foundation model, they are updated only when a user wants,\neither through feedback or personalization, allowing them to gain a high degree\nof familiarity with that policy. We demonstrate a proof-of-concept of DPP in\nsimulation then discuss its limitations and the future of interpretable\nfoundation models.",
      "tldr_zh": "该论文探讨了机器人基础模型（Foundation Models）在设计中的问题，特别是通用策略更新可能影响其他任务，导致可解释性和可用性下降。作者提出了一种新方法Diffusion for Policy Parameters (DPP)，通过生成独立的、任务特定的策略来解决这一问题，这些策略与基础模型分离，仅在用户提供反馈或个性化需求时更新，从而提升用户熟悉度和系统可靠性。在模拟环境中进行的概念验证展示了DPP的潜力，并讨论了其限制及未来发展方向。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Short Paper accepted to RLC 2024 Workshop on Training Agents with\n  Foundation Models",
      "pdf_url": "http://arxiv.org/pdf/2407.08065v1",
      "published_date": "2024-07-10 21:55:44 UTC",
      "updated_date": "2024-07-10 21:55:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:12:13.508176"
    },
    {
      "arxiv_id": "2407.08064v1",
      "title": "TinyGraph: Joint Feature and Node Condensation for Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Yezi Liu",
        "Yanning Shen"
      ],
      "abstract": "Training graph neural networks (GNNs) on large-scale graphs can be\nchallenging due to the high computational expense caused by the massive number\nof nodes and high-dimensional nodal features. Existing graph condensation\nstudies tackle this problem only by reducing the number of nodes in the graph.\nHowever, the resulting condensed graph data can still be cumbersome.\nSpecifically, although the nodes of the Citeseer dataset are reduced to 0.9%\n(30 nodes) in training, the number of features is 3,703, severely exceeding the\ntraining sample magnitude. Faced with this challenge, we study the problem of\njoint condensation for both features and nodes in large-scale graphs. This task\nis challenging mainly due to 1) the intertwined nature of the node features and\nthe graph structure calls for the feature condensation solver to be\nstructure-aware; and 2) the difficulty of keeping useful information in the\ncondensed graph. To address these challenges, we propose a novel framework\nTinyGraph, to condense features and nodes simultaneously in graphs.\nSpecifically, we cast the problem as matching the gradients of GNN weights\ntrained on the condensed graph and the gradients obtained from training over\nthe original graph, where the feature condensation is achieved by a trainable\nfunction. The condensed graph obtained by minimizing the matching loss along\nthe training trajectory can henceforth retain critical information in the\noriginal graph. Extensive experiments were carried out to demonstrate the\neffectiveness of the proposed TinyGraph. For example, a GNN trained with\nTinyGraph retains 98.5% and 97.5% of the original test accuracy on the Cora and\nCiteseer datasets, respectively, while significantly reducing the number of\nnodes by 97.4% and 98.2%, and the number of features by 90.0% on both datasets.",
      "tldr_zh": "该论文针对大规模图神经网络(GNNs)训练中的高计算成本问题，提出TinyGraph框架，该框架首次同时压缩图的节点和特征，以减少节点数量和特征维度。TinyGraph通过将问题转化为匹配GNN权重梯度的方法，使用可训练函数进行特征压缩，同时确保保留原图的关键信息，从而解决特征与结构相互交织的挑战。实验结果显示，在Cora和Citeseer数据集上，TinyGraph将节点减少97.4%和98.2%、特征减少90.0%，同时保留了98.5%和97.5%的测试准确率，证明了其在提升训练效率方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.08064v1",
      "published_date": "2024-07-10 21:54:12 UTC",
      "updated_date": "2024-07-10 21:54:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:12:26.631608"
    },
    {
      "arxiv_id": "2407.08047v2",
      "title": "Spatial-Temporal Attention Model for Traffic State Estimation with Sparse Internet of Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Jianzhe Xue",
        "Dongcheng Yuan",
        "Yu Sun",
        "Tianqi Zhang",
        "Wenchao Xu",
        "Haibo Zhou",
        "Xuemin",
        "Shen"
      ],
      "abstract": "The growing number of connected vehicles offers an opportunity to leverage\ninternet of vehicles (IoV) data for traffic state estimation (TSE) which plays\na crucial role in intelligent transportation systems (ITS). By utilizing only a\nportion of IoV data instead of the entire dataset, the significant overheads\nassociated with collecting and processing large amounts of data can be avoided.\nIn this paper, we introduce a novel framework that utilizes sparse IoV data to\nachieve cost-effective TSE. Particularly, we propose a novel spatial-temporal\nattention model called the convolutional retentive network (CRNet) to improve\nthe TSE accuracy by mining spatial-temporal traffic state correlations. The\nmodel employs the convolutional neural network (CNN) for spatial correlation\naggregation and the retentive network (RetNet) based on the attention mechanism\nto extract temporal correlations. Extensive simulations on a real-world IoV\ndataset validate the advantage of the proposed TSE approach in achieving\naccurate TSE using sparse IoV data, demonstrating its cost effectiveness and\npracticality for real-world applications.",
      "tldr_zh": "该研究针对智能交通系统（ITS），提出了一种利用稀疏互联网车辆（IoV）数据进行交通状态估计（TSE）的成本有效框架，以减少数据收集和处理的开销。主要方法是开发新型空间-时间注意力模型——卷积保留网络（CRNet），它结合卷积神经网络（CNN）来聚合空间相关性，以及基于注意力的保留网络（RetNet）来提取时间相关性。在真实世界 IoV 数据集上的广泛模拟实验显示，该方法显著提高了 TSE 准确性，证明了其在实际应用中的实用性和经济性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "need further improvement",
      "pdf_url": "http://arxiv.org/pdf/2407.08047v2",
      "published_date": "2024-07-10 20:58:53 UTC",
      "updated_date": "2024-07-15 02:31:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:12:38.102580"
    },
    {
      "arxiv_id": "2407.08044v2",
      "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
      "title_zh": "RoLoRA：针对旋转无离群值LLMs的有效权重-激活量化微调",
      "authors": [
        "Xijie Huang",
        "Zechun Liu",
        "Shih-Yang Liu",
        "Kwang-Ting Cheng"
      ],
      "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA",
      "tldr_zh": "该研究提出RoLoRA，一种基于Low-Rank Adaptation (LoRA)的创新方法，用于有效实现Large Language Models (LLMs)的权重-激活量化，以解决激活异常值(outliers)导致的性能下降问题。RoLoRA通过旋转(rotation)消除异常值，并引入旋转感知微调(rotation-aware fine-tuning)，以保持旋转后LLMs的无异常值特性，从而提升低位LoRA的收敛性和量化鲁棒性。实验结果显示，在LLaMA2-7B/13B和LLaMA3-8B模型上，RoLoRA使4-bit权重-激活量化LLaMA2-13B在常识推理任务上的绝对准确率比LoRA基线提高高达29.5%，并证明其在大型多模态模型(LLaVA-1.5-7B)上的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings, Codes: https://github.com/HuangOwen/RoLoRA,\n  Models:\n  https://huggingface.co/collections/ScarletAce/rolora-66f5f228a90681c7c4512b28",
      "pdf_url": "http://arxiv.org/pdf/2407.08044v2",
      "published_date": "2024-07-10 20:52:18 UTC",
      "updated_date": "2024-09-26 23:47:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:12:52.728973"
    },
    {
      "arxiv_id": "2407.08034v1",
      "title": "Spatial-Temporal Generative AI for Traffic Flow Estimation with Sparse Data of Connected Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Jianzhe Xue",
        "Yunting Xu",
        "Dongcheng Yuan",
        "Caoyi Zha",
        "Hongyang Du",
        "Haibo Zhou",
        "Dusit Niyato"
      ],
      "abstract": "Traffic flow estimation (TFE) is crucial for intelligent transportation\nsystems. Traditional TFE methods rely on extensive road sensor networks and\ntypically incur significant costs. Sparse mobile crowdsensing enables a\ncost-effective alternative by utilizing sparsely distributed probe vehicle data\n(PVD) provided by connected vehicles. However, as pointed out by the central\nlimit theorem, the sparsification of PVD leads to the degradation of TFE\naccuracy. In response, this paper introduces a novel and cost-effective TFE\nframework that leverages sparse PVD and improves accuracy by applying the\nspatial-temporal generative artificial intelligence (GAI) framework. Within\nthis framework, the conditional encoder mines spatial-temporal correlations in\nthe initial TFE results derived from averaging vehicle speeds of each region,\nand the generative decoder generates high-quality and accurate TFE outputs.\nAdditionally, the design of the spatial-temporal neural network is discussed,\nwhich is the backbone of the conditional encoder for effectively capturing\nspatial-temporal correlations. The effectiveness of the proposed TFE approach\nis demonstrated through evaluations based on real-world connected vehicle data.\nThe experimental results affirm the feasibility of our sparse PVD-based TFE\nframework and highlight the significant role of the spatial-temporal GAI\nframework in enhancing the accuracy of TFE.",
      "tldr_zh": "该研究针对交通流量估计（TFE）在智能交通系统中的关键作用，提出了一种基于稀疏探针车辆数据（PVD）的成本有效框架，以解决传统方法的传感器依赖和高成本问题。框架利用空间-时间生成AI（GAI）技术，包括条件编码器（用于挖掘初始TFE结果的空间-时间相关性）和生成解码器（用于产生高质量输出），并讨论了空间-时间神经网络的设计作为其核心组件。通过真实世界数据评估，实验结果表明，该方法显著提高了TFE的准确性，验证了稀疏PVD框架的可行性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.08034v1",
      "published_date": "2024-07-10 20:26:04 UTC",
      "updated_date": "2024-07-10 20:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:13:03.895716"
    },
    {
      "arxiv_id": "2407.08022v1",
      "title": "Deep Reinforcement Learning for Sequential Combinatorial Auctions",
      "title_zh": "针对顺序组合拍卖的深度强化学习",
      "authors": [
        "Sai Srivatsa Ravindranath",
        "Zhe Feng",
        "Di Wang",
        "Manzil Zaheer",
        "Aranyak Mehta",
        "David C. Parkes"
      ],
      "abstract": "Revenue-optimal auction design is a challenging problem with significant\ntheoretical and practical implications. Sequential auction mechanisms, known\nfor their simplicity and strong strategyproofness guarantees, are often limited\nby theoretical results that are largely existential, except for certain\nrestrictive settings. Although traditional reinforcement learning methods such\nas Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) are\napplicable in this domain, they struggle with computational demands and\nconvergence issues when dealing with large and continuous action spaces. In\nlight of this and recognizing that we can model transitions differentiable for\nour settings, we propose using a new reinforcement learning framework tailored\nfor sequential combinatorial auctions that leverages first-order gradients. Our\nextensive evaluations show that our approach achieves significant improvement\nin revenue over both analytical baselines and standard reinforcement learning\nalgorithms. Furthermore, we scale our approach to scenarios involving up to 50\nagents and 50 items, demonstrating its applicability in complex, real-world\nauction settings. As such, this work advances the computational tools available\nfor auction design and contributes to bridging the gap between theoretical\nresults and practical implementations in sequential auction design.",
      "tldr_zh": "该论文针对顺序组合拍卖（Sequential Combinatorial Auctions）的收益最优设计问题，提出了一种新的深度强化学习（Deep Reinforcement Learning）框架，利用一阶梯度（first-order gradients）来处理传统算法如Proximal Policy Optimization (PPO)和Soft Actor-Critic (SAC)的计算和收敛挑战。相比分析基线和标准强化学习算法，该框架在实验中实现了收益的显著提升，并在涉及多达50个代理和50个物品的复杂场景中表现出色。该研究桥接了理论结果与实际实现，推动了拍卖设计的计算工具发展。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.08022v1",
      "published_date": "2024-07-10 20:00:22 UTC",
      "updated_date": "2024-07-10 20:00:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:13:15.305063"
    },
    {
      "arxiv_id": "2407.08003v1",
      "title": "Machine Learning for ALSFRS-R Score Prediction: Making Sense of the Sensor Data",
      "title_zh": "机器学习用于 ALSFRS-R 分数预测：解读传感器数据",
      "authors": [
        "Ritesh Mehta",
        "Aleksandar Pramov",
        "Shashank Verma"
      ],
      "abstract": "Amyotrophic Lateral Sclerosis (ALS) is characterized as a rapidly progressive\nneurodegenerative disease that presents individuals with limited treatment\noptions in the realm of medical interventions and therapies. The disease\nshowcases a diverse range of onset patterns and progression trajectories,\nemphasizing the critical importance of early detection of functional decline to\nenable tailored care strategies and timely therapeutic interventions. The\npresent investigation, spearheaded by the iDPP@CLEF 2024 challenge, focuses on\nutilizing sensor-derived data obtained through an app. This data is used to\nconstruct various machine learning models specifically designed to forecast the\nadvancement of the ALS Functional Rating Scale-Revised (ALSFRS-R) score,\nleveraging the dataset provided by the organizers. In our analysis, multiple\npredictive models were evaluated to determine their efficacy in handling ALS\nsensor data. The temporal aspect of the sensor data was compressed and\namalgamated using statistical methods, thereby augmenting the interpretability\nand applicability of the gathered information for predictive modeling\nobjectives. The models that demonstrated optimal performance were a naive\nbaseline and ElasticNet regression. The naive model achieved a Mean Absolute\nError (MAE) of 0.20 and a Root Mean Square Error (RMSE) of 0.49, slightly\noutperforming the ElasticNet model, which recorded an MAE of 0.22 and an RMSE\nof 0.50. Our comparative analysis suggests that while the naive approach\nyielded marginally better predictive accuracy, the ElasticNet model provides a\nrobust framework for understanding feature contributions.",
      "tldr_zh": "本研究针对肌萎缩性侧索硬化症 (ALS) 的多样化进展轨迹，利用 iDPP@CLEF 2024 挑战提供的传感器数据，构建机器学习模型来预测 ALS Functional Rating Scale-Revised (ALSFRS-R) 评分，以实现早期功能衰退检测和个性化护理。研究通过统计方法压缩和整合传感器数据的时序信息，评估了多种预测模型，包括 naive baseline 和 ElasticNet regression。结果显示，naive 模型取得了 Mean Absolute Error (MAE) 为 0.20 和 Root Mean Square Error (RMSE) 为 0.49 的最佳性能，略优于 ElasticNet 的 MAE 0.22 和 RMSE 0.50；同时，ElasticNet 提供了更稳健的特征贡献分析框架。该工作增强了对 ALS 传感器数据的理解，为临床预测应用提供了实用基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper submitted to CLEF 2024 CEUR-WS",
      "pdf_url": "http://arxiv.org/pdf/2407.08003v1",
      "published_date": "2024-07-10 19:17:23 UTC",
      "updated_date": "2024-07-10 19:17:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:13:27.856925"
    },
    {
      "arxiv_id": "2407.08001v1",
      "title": "Automated Neural Patent Landscaping in the Small Data Regime",
      "title_zh": "小数据环境下的自动神经专利景观分析",
      "authors": [
        "Tisa Islam Erana",
        "Mark A. Finlayson"
      ],
      "abstract": "Patent landscaping is the process of identifying all patents related to a\nparticular technological area, and is important for assessing various aspects\nof the intellectual property context. Traditionally, constructing patent\nlandscapes is intensely laborious and expensive, and the rapid expansion of\npatenting activity in recent decades has driven an increasing need for\nefficient and effective automated patent landscaping approaches. In particular,\nit is critical that we be able to construct patent landscapes using a minimal\nnumber of labeled examples, as labeling patents for a narrow technology area\nrequires highly specialized (and hence expensive) technical knowledge. We\npresent an automated neural patent landscaping system that demonstrates\nsignificantly improved performance on difficult examples (0.69 $F_1$ on 'hard'\nexamples, versus 0.6 for previously reported systems), and also significant\nimprovements with much less training data (overall 0.75 $F_1$ on as few as 24\nexamples). Furthermore, in evaluating such automated landscaping systems,\nacquiring good data is challenge; we demonstrate a higher-quality training data\ngeneration procedure by merging Abood and Feltenberger's (2018)\n\"seed/anti-seed\" approach with active learning to collect difficult labeled\nexamples near the decision boundary. Using this procedure we created a new\ndataset of labeled AI patents for training and testing. As in prior work we\ncompare our approach with a number of baseline systems, and we release our code\nand data for others to build upon.",
      "tldr_zh": "这篇论文提出了一种自动化神经专利景观分析（patent landscaping）系统，针对小数据环境（Small Data Regime），旨在减少标记样本的需求并提升性能。该系统通过结合主动学习（active learning）和 \"seed/anti-seed\" 方法，改进了训练数据生成过程，并在困难示例上实现了 0.69 F1 分数，比现有系统提高了 15%。实验结果显示，该系统仅使用 24 个训练示例即可达到整体 0.75 F1 分数，并发布了新的 AI 专利数据集及代码，以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.08001v1",
      "published_date": "2024-07-10 19:13:37 UTC",
      "updated_date": "2024-07-10 19:13:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:13:41.086882"
    },
    {
      "arxiv_id": "2407.07972v2",
      "title": "Deconstructing What Makes a Good Optimizer for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Rosie Zhao",
        "Depen Morwani",
        "David Brandfonbrener",
        "Nikhil Vyas",
        "Sham Kakade"
      ],
      "abstract": "Training language models becomes increasingly expensive with scale, prompting\nnumerous attempts to improve optimization efficiency. Despite these efforts,\nthe Adam optimizer remains the most widely used, due to a prevailing view that\nit is the most effective approach. We aim to compare several optimization\nalgorithms, including SGD, Adafactor, Adam, Lion, and Sophia in the context of\nautoregressive language modeling across a range of model sizes,\nhyperparameters, and architecture variants. Our findings indicate that, except\nfor SGD, these algorithms all perform comparably both in their optimal\nperformance and also in terms of how they fare across a wide range of\nhyperparameter choices. Our results suggest to practitioners that the choice of\noptimizer can be guided by practical considerations like memory constraints and\nease of implementation, as no single algorithm emerged as a clear winner in\nterms of performance or stability to hyperparameter misspecification. Given our\nfindings, we further dissect these approaches, examining two simplified\nversions of Adam: a) signed momentum (Signum) which we see recovers both the\nperformance and hyperparameter stability of Adam and b) Adalayer, a layerwise\nvariant of Adam which we introduce to study the impact on Adam's\npreconditioning for different layers of the network. Examining Adalayer leads\nus to the conclusion that, perhaps surprisingly, adaptivity on both the last\nlayer and LayerNorm parameters in particular are necessary for retaining\nperformance and stability to learning rate.",
      "tldr_zh": "这篇论文探讨了优化器在语言模型训练中的有效性，通过比较 SGD、Adafactor、Adam、Lion 和 Sophia 等算法在自回归语言建模的各种模型大小、超参数和架构变体下的性能。研究发现，除了 SGD，其他优化器在最佳表现和对超参数选择的鲁棒性方面表现类似，没有明显优胜者。作者建议，优化器的选择应基于实际因素，如内存约束和实现便利性。进一步分析了 Adam 的简化版本 Signum 和新引入的 Adalayer，结果表明，最后一层和 LayerNorm 参数的适应性对维持性能和稳定性至关重要。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.07972v2",
      "published_date": "2024-07-10 18:11:40 UTC",
      "updated_date": "2025-02-28 01:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:13:52.898732"
    },
    {
      "arxiv_id": "2407.07966v1",
      "title": "A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Arastoo Zibaeirad",
        "Farnoosh Koleini",
        "Shengping Bi",
        "Tao Hou",
        "Tao Wang"
      ],
      "abstract": "In this study, we conduct a comprehensive review of smart grid security,\nexploring system architectures, attack methodologies, defense strategies, and\nfuture research opportunities. We provide an in-depth analysis of various\nattack vectors, focusing on new attack surfaces introduced by advanced\ncomponents in smart grids. The review particularly includes an extensive\nanalysis of coordinated attacks that incorporate multiple attack strategies and\nexploit vulnerabilities across various smart grid components to increase their\nadverse impact, demonstrating the complexity and potential severity of these\nthreats. Following this, we examine innovative detection and mitigation\nstrategies, including game theory, graph theory, blockchain, and machine\nlearning, discussing their advancements in counteracting evolving threats and\nassociated research challenges. In particular, our review covers a thorough\nexamination of widely used machine learning-based mitigation strategies,\nanalyzing their applications and research challenges spanning across\nsupervised, unsupervised, semi-supervised, ensemble, and reinforcement\nlearning. Further, we outline future research directions and explore new\ntechniques and concerns. We first discuss the research opportunities for\nexisting and emerging strategies, and then explore the potential role of new\ntechniques, such as large language models (LLMs), and the emerging threat of\nadversarial machine learning in the future of smart grid security.",
      "tldr_zh": "本研究对智能电网（smart grid）的安全进行了全面调查，探讨了系统架构、攻击方法（attack vectors）和防御策略（defense strategies），并突出了协调攻击的复杂性和潜在严重性。调查详细分析了创新的检测与缓解方法，包括博弈论（game theory）、图论（graph theory）、区块链（blockchain）和机器学习（machine learning），特别考察了监督、非监督、半监督、集成和强化学习等机器学习策略的应用及其研究挑战。最终，该文概述了未来研究机会，如大型语言模型（LLMs）的潜在作用以及对抗机器学习（adversarial machine learning）的新兴威胁，为提升智能电网安全提供了重要指导。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07966v1",
      "published_date": "2024-07-10 18:03:24 UTC",
      "updated_date": "2024-07-10 18:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:14:03.983728"
    },
    {
      "arxiv_id": "2407.07950v2",
      "title": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance",
      "title_zh": "翻译失败",
      "authors": [
        "Kaitlyn Zhou",
        "Jena D. Hwang",
        "Xiang Ren",
        "Nouha Dziri",
        "Dan Jurafsky",
        "Maarten Sap"
      ],
      "abstract": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.",
      "tldr_zh": "本文提出 Rel-A.I. 框架，这是一种以交互为中心的评估方法，用于测量人类对大型语言模型 (LLMs) 生成内容的依赖程度，而不是仅依赖简单的校准。研究通过实验考察了交互上下文特征（如知识领域和问候语传达的温暖或能力）对依赖行为的影响，发现人们在计算相关问题上对 LLMs 依赖增加 10%，而对被视为更能干的 LLMs 依赖增加 30%。这些结果表明，校准和语言质量不足以全面评估人类-LM 交互的风险，强调需考虑交互上下文的特征。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2407.07950v2",
      "published_date": "2024-07-10 18:00:05 UTC",
      "updated_date": "2024-10-03 16:54:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:14:16.107413"
    },
    {
      "arxiv_id": "2407.18940v2",
      "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
      "title_zh": "LitSearch: 科学",
      "authors": [
        "Anirudh Ajith",
        "Mengzhou Xia",
        "Alexis Chevalier",
        "Tanya Goyal",
        "Danqi Chen",
        "Tianyu Gao"
      ],
      "abstract": "Literature search questions, such as \"Where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason across\nentire articles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions manually written by authors about their recently published\npapers. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% absolute difference in recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by up to 32 recall\npoints. Taken together, these results show that LitSearch is an informative new\ntestbed for retrieval systems while catering to a real-world use case.",
      "tldr_zh": "本文引入了 LitSearch，这是一个包含 597 个现实文献搜索查询的检索基准，针对最近的机器学习和自然语言处理论文，用于评估检索系统的性能。LitSearch 的查询通过 GPT-4 基于研究论文中内联引用的段落生成，并结合作者手动撰写的查询，所有内容均由专家手动检查或编辑以确保高质量。实验结果显示，BM25 与最先进密集检索器在 recall@5 上存在 24.8% 的绝对差距，而 LLM-based reranking 策略进一步将最佳密集检索器性能提升 4.4%。此外，商业搜索引擎如 Google Search 在该基准上表现较差，比最佳密集检索器低 32 点，证明 LitSearch 是一个宝贵的测试平台，用于改进真实世界文献搜索系统。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by EMNLP 2024. Dataset and code are available at\n  https://github.com/princeton-nlp/LitSearch",
      "pdf_url": "http://arxiv.org/pdf/2407.18940v2",
      "published_date": "2024-07-10 18:00:03 UTC",
      "updated_date": "2024-10-16 18:37:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:14:29.408867"
    },
    {
      "arxiv_id": "2407.07890v3",
      "title": "Training on the Test Task Confounds Evaluation and Emergence",
      "title_zh": "测试任务上的训练混淆了评估和涌现",
      "authors": [
        "Ricardo Dominguez-Olmedo",
        "Florian E. Dorner",
        "Moritz Hardt"
      ],
      "abstract": "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of practices that\nutilize knowledge about evaluation tasks at training time. We demonstrate that\ntraining on the test task confounds both relative model evaluations and claims\nabout emergent capabilities. We argue that the seeming superiority of one model\nfamily over another may be explained by a different degree of training on the\ntest task. To this end, we propose an effective method to adjust for the effect\nof training on the test task on benchmark evaluations. Put simply, to fine-tune\neach model under comparison on the same task-relevant data prior to evaluation.\nWe then show that instances of emergent behavior disappear gradually as models\ntrain on the test task. Our work promotes a new perspective on the evaluation\nof large language models, with broad implications for benchmarking and the\nstudy of emergent capabilities.",
      "tldr_zh": "这篇论文探讨了大型语言模型（large language models）评估中的一个关键问题：训练时使用测试任务知识（training on the test task），这会混淆模型的相对评估和新兴能力（emergent capabilities）的观察。作者证明，这种做法可能导致一个模型家族看似优于另一个，实际源于训练程度的差异，并提出一种调整方法：在评估前，让每个模型在相同的任务相关数据上进行微调。实验结果显示，新兴行为并非突然出现，而是随着训练在测试任务上的增加而逐渐消失。该研究为大型语言模型的基准测试（benchmarking）和新兴能力研究提供了新的视角，推动更公平的评估实践。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2407.07890v3",
      "published_date": "2024-07-10 17:57:58 UTC",
      "updated_date": "2025-04-21 16:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:14:41.337825"
    },
    {
      "arxiv_id": "2407.11062v2",
      "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mengzhao Chen",
        "Wenqi Shao",
        "Peng Xu",
        "Jiahao Wang",
        "Peng Gao",
        "Kaipeng Zhang",
        "Ping Luo"
      ],
      "abstract": "Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.",
      "tldr_zh": "本论文提出 EfficientQAT，一种高效的 Quantization-Aware Training (QAT) 方法，用于解决 Large Language Models (LLMs) 的高内存需求问题，同时减少训练资源消耗。EfficientQAT 包括两个阶段：Block-wise training of all parameters (Block-AP)，首次实现块-wise 方式训练所有参数，以最小化低位量化场景的准确率损失；以及 end-to-end training of quantization parameters (E2E-QP)，通过端到端优化量化参数（step sizes）来提升模型性能。实验结果显示，EfficientQAT 在从 7B 到 70B 参数的各种 LLMs 上超越现有方法，例如在单 A100-80GB GPU 上训练 2-bit Llama-2-70B 模型仅需 41 小时，准确率损失不到 3 点（69.48 vs. 72.41）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs",
      "pdf_url": "http://arxiv.org/pdf/2407.11062v2",
      "published_date": "2024-07-10 17:53:30 UTC",
      "updated_date": "2024-10-02 13:44:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:14:52.739049"
    },
    {
      "arxiv_id": "2407.07884v1",
      "title": "Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Chen",
        "Eric Cousineau",
        "Naveen Kuppuswamy",
        "Pulkit Agrawal"
      ],
      "abstract": "Recent studies have made significant progress in addressing dexterous\nmanipulation problems, particularly in in-hand object reorientation. However,\nthere are few existing works that explore the potential utilization of\ndeveloped dexterous manipulation controllers for downstream tasks. In this\nstudy, we focus on constrained dexterous manipulation for food peeling. Food\npeeling presents various constraints on the reorientation controller, such as\nthe requirement for the hand to securely hold the object after reorientation\nfor peeling. We propose a simple system for learning a reorientation controller\nthat facilitates the subsequent peeling task. Videos are available at:\nhttps://taochenshh.github.io/projects/veg-peeling.",
      "tldr_zh": "本研究以蔬菜剥皮为例，探讨约束下的灵巧操作（dexterous manipulation），强调现有控制器在下游任务中的潜在应用不足。作者提出一个简单系统，用于学习重新定向控制器（reorientation controller），以确保手部在重新定向后安全握持物体，从而便于后续剥皮过程。实验结果通过视频演示展示了该系统的有效性，为灵巧操作在实际场景中的应用提供了案例支持。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07884v1",
      "published_date": "2024-07-10 17:51:33 UTC",
      "updated_date": "2024-07-10 17:51:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:15:03.998355"
    },
    {
      "arxiv_id": "2407.07880v2",
      "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Junkang Wu",
        "Yuexiang Xie",
        "Zhengyi Yang",
        "Jiancan Wu",
        "Jiawei Chen",
        "Jinyang Gao",
        "Bolin Ding",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "abstract": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.",
      "tldr_zh": "该研究针对Direct Preference Optimization (DPO) 在训练数据集中的噪声问题，包括pointwise noise（如低质量数据点）和pairwise noise（如错误配对关联），提出利用Distributionally Robust Optimization (DRO) 来提升其鲁棒性。理论分析显示，DPO 本身嵌入 DRO 原则，对 pointwise noise 有内在抵抗力，其中 regularization coefficient β 扮演关键角色。作者引入新框架 Distributionally Robustifying DPO (Dr. DPO)，通过优化最坏情况的 pairwise 场景并引入超参数 β'，实现对数据对可靠性的精细控制，从而平衡噪声环境下的探索与利用。实验结果表明，Dr. DPO 在有噪声和无噪声设置下显著提高了生成文本质量和响应准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07880v2",
      "published_date": "2024-07-10 17:48:25 UTC",
      "updated_date": "2025-04-18 08:05:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:15:16.554660"
    },
    {
      "arxiv_id": "2407.07875v2",
      "title": "Generative Image as Action Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohit Shridhar",
        "Yat Long Lo",
        "Stephen James"
      ],
      "abstract": "Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.",
      "tldr_zh": "本研究提出 GENIMA 框架，通过微调图像生成扩散模型（如 Stable Diffusion）来生成 RGB 图像中的关节动作目标，从而实现视觉运动控制。GENIMA 采用行为克隆（behavior-cloning）方法，将这些图像目标输入控制器以映射为关节位置序列，并在 25 个 RLBench 和 9 个真实世界操作任务上进行测试。结果显示，该方法优于现有视觉运动策略，尤其在场景扰动和泛化到新物体方面表现出更强鲁棒性，并能与 3D 代理竞争，尽管不依赖深度、关键点或运动规划器等先验。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2024. Website, code, checkpoints:\n  https://genima-robot.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.07875v2",
      "published_date": "2024-07-10 17:41:10 UTC",
      "updated_date": "2024-10-08 17:43:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:15:29.009703"
    },
    {
      "arxiv_id": "2407.07874v2",
      "title": "Toto: Time Series Optimized Transformer for Observability",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Cohen",
        "Emaad Khwaja",
        "Kan Wang",
        "Charles Masson",
        "Elise Ramé",
        "Youssef Doubli",
        "Othmane Abou-Amal"
      ],
      "abstract": "This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.",
      "tldr_zh": "本报告介绍了Toto，一种Time Series Optimized Transformer for Observability模型，由Datadog开发，用于时间序列预测，首次针对可观测性指标进行了专门优化，并在电力和天气等通用基准上达到了最先进水平。模型训练于一万亿时间序列数据点，这是目前已发布模型中最大的数据集，其中75%来自Datadog平台的匿名数值指标数据。实验结果显示，Toto在可观测性数据上优于现有基础模型，同时在多个公开基准数据集上实现了最先进的零样本性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07874v2",
      "published_date": "2024-07-10 17:40:30 UTC",
      "updated_date": "2024-07-11 16:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:15:41.869712"
    },
    {
      "arxiv_id": "2407.07868v2",
      "title": "Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Eugene Teoh",
        "Sumit Patidar",
        "Xiao Ma",
        "Stephen James"
      ],
      "abstract": "Generalising vision-based manipulation policies to novel environments remains\na challenging area with limited exploration. Current practices involve\ncollecting data in one location, training imitation learning or reinforcement\nlearning policies with this data, and deploying the policy in the same\nlocation. However, this approach lacks scalability as it necessitates data\ncollection in multiple locations for each task. This paper proposes a novel\napproach where data is collected in a location predominantly featuring green\nscreens. We introduce Green-screen Augmentation (GreenAug), employing a chroma\nkey algorithm to overlay background textures onto a green screen. Through\nextensive real-world empirical studies with over 850 training demonstrations\nand 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no\naugmentation, standard computer vision augmentation, and prior generative\naugmentation methods in performance. While no algorithmic novelties are\nclaimed, our paper advocates for a fundamental shift in data collection\npractices. We propose that real-world demonstrations in future research should\nutilise green screens, followed by the application of GreenAug. We believe\nGreenAug unlocks policy generalisation to visually distinct novel locations,\naddressing the current scene generalisation limitations in robot learning.",
      "tldr_zh": "这篇论文解决了机器人操作策略在新型环境的泛化挑战，提出Green-screen Augmentation (GreenAug)方法，通过在绿色屏幕环境下收集数据并使用chroma key算法叠加背景纹理，实现数据增强。实验涉及超过850个训练演示和8.2k个评估episode，结果显示GreenAug在性能上优于无增强、标准计算机视觉增强和现有生成增强方法。论文倡导改变数据收集实践，建议未来研究采用绿色屏幕结合GreenAug，以提升imitation learning和reinforcement learning策略在视觉上不同场景的泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://greenaug.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.07868v2",
      "published_date": "2024-07-10 17:32:05 UTC",
      "updated_date": "2024-09-08 14:33:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:16:04.390994"
    },
    {
      "arxiv_id": "2407.07848v1",
      "title": "Uncovering Layer-Dependent Activation Sparsity Patterns in ReLU Transformers",
      "title_zh": "揭示 ReLU Transformers 中层依赖的激活稀疏模式",
      "authors": [
        "Cody Wild",
        "Jesper Anderson"
      ],
      "abstract": "Previous work has demonstrated that MLPs within ReLU Transformers exhibit\nhigh levels of sparsity, with many of their activations equal to zero for any\ngiven token. We build on that work to more deeply explore how token-level\nsparsity evolves over the course of training, and how it connects to broader\nsparsity patterns over the course of a sequence or batch, demonstrating that\nthe different layers within small transformers exhibit distinctly\nlayer-specific patterns on both of these fronts. In particular, we demonstrate\nthat the first and last layer of the network have distinctive and in many ways\ninverted relationships to sparsity, and explore implications for the structure\nof feature representations being learned at different depths of the model. We\nadditionally explore the phenomenon of ReLU dimensions \"turning off\", and show\nevidence suggesting that \"neuron death\" is being primarily driven by the\ndynamics of training, rather than simply occurring randomly or accidentally as\na result of outliers.",
      "tldr_zh": "本研究揭示了 ReLU Transformers 中激活稀疏性的层依赖模式，扩展了先前对 MLP 层高稀疏性的工作，通过分析 token-level 稀疏性在训练过程中的演变及其与序列或批次中更广泛模式的关联。结果显示，不同层表现出独特的层特定特征，特别是第一层和最后一层在稀疏性方面存在明显的反向关系，这反映了模型不同深度特征表示的结构差异。该研究还探讨了 ReLU dimensions \"turning off\" 的现象，并提供证据表明 \"neuron death\" 主要由训练动态驱动，而非随机因素。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07848v1",
      "published_date": "2024-07-10 17:10:10 UTC",
      "updated_date": "2024-07-10 17:10:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:16:05.554835"
    },
    {
      "arxiv_id": "2407.07835v1",
      "title": "RoBus: A Multimodal Dataset for Controllable Road Networks and Building Layouts Generation",
      "title_zh": "RoBus：用于可控道路网络和建筑布局生成的多模态数据集",
      "authors": [
        "Tao Li",
        "Ruihang Li",
        "Huangnan Zheng",
        "Shanding Ye",
        "Shijian Li",
        "Zhijie Pan"
      ],
      "abstract": "Automated 3D city generation, focusing on road networks and building layouts,\nis in high demand for applications in urban design, multimedia games and\nautonomous driving simulations. The surge of generative AI facilitates\ndesigning city layouts based on deep learning models. However, the lack of\nhigh-quality datasets and benchmarks hinders the progress of these data-driven\nmethods in generating road networks and building layouts. Furthermore, few\nstudies consider urban characteristics, which generally take graphics as\nanalysis objects and are crucial for practical applications, to control the\ngenerative process. To alleviate these problems, we introduce a multimodal\ndataset with accompanying evaluation metrics for controllable generation of\nRoad networks and Building layouts (RoBus), which is the first and largest\nopen-source dataset in city generation so far. RoBus dataset is formatted as\nimages, graphics and texts, with $72,400$ paired samples that cover around\n$80,000km^2$ globally. We analyze the RoBus dataset statistically and validate\nthe effectiveness against existing road networks and building layouts\ngeneration methods. Additionally, we design new baselines that incorporate\nurban characteristics, such as road orientation and building density, in the\nprocess of generating road networks and building layouts using the RoBus\ndataset, enhancing the practicality of automated urban design. The RoBus\ndataset and related codes are published at\nhttps://github.com/tourlics/RoBus_Dataset.",
      "tldr_zh": "该论文引入了 RoBus，这是一个多模态数据集，用于可控生成道路网络和建筑布局，旨在解决自动 3D 城市生成领域中缺乏高质量数据和基准的问题。RoBus 包含 72,400 对样本，包括图像、图形和文本，覆盖全球约 80,000 平方公里，并首次整合城市特征如道路方向和建筑密度来控制生成过程。研究者通过统计分析和实验验证了数据集的有效性，并设计了新基线方法，提升了道路网络和建筑布局生成的实用性；相关代码已在 GitHub 上公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07835v1",
      "published_date": "2024-07-10 16:55:01 UTC",
      "updated_date": "2024-07-10 16:55:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:16:20.520245"
    },
    {
      "arxiv_id": "2407.18939v1",
      "title": "Promoting AI Competencies for Medical Students: A Scoping Review on Frameworks, Programs, and Tools",
      "title_zh": "翻译失败",
      "authors": [
        "Yingbo Ma",
        "Yukyeong Song",
        "Jeremy A. Balch",
        "Yuanfang Ren",
        "Divya Vellanki",
        "Zhenhong Hu",
        "Meghan Brennan",
        "Suraj Kolla",
        "Ziyuan Guan",
        "Brooke Armfield",
        "Tezcan Ozrazgat-Baslanti",
        "Parisa Rashidi",
        "Tyler J. Loftus",
        "Azra Bihorac",
        "Benjamin Shickel"
      ],
      "abstract": "As more clinical workflows continue to be augmented by artificial\nintelligence (AI), AI literacy among physicians will become a critical\nrequirement for ensuring safe and ethical AI-enabled patient care. Despite the\nevolving importance of AI in healthcare, the extent to which it has been\nadopted into traditional and often-overloaded medical curricula is currently\nunknown. In a scoping review of 1,699 articles published between January 2016\nand June 2024, we identified 18 studies which propose guiding frameworks, and\n11 studies documenting real-world instruction, centered around the integration\nof AI into medical education. We found that comprehensive guidelines will\nrequire greater clinical relevance and personalization to suit medical student\ninterests and career trajectories. Current efforts highlight discrepancies in\nthe teaching guidelines, emphasizing AI evaluation and ethics over technical\ntopics such as data science and coding. Additionally, we identified several\nchallenges associated with integrating AI training into the medical education\nprogram, including a lack of guidelines to define medical students AI literacy,\na perceived lack of proven clinical value, and a scarcity of qualified\ninstructors. With this knowledge, we propose an AI literacy framework to define\ncompetencies for medical students. To prioritize relevant and personalized AI\neducation, we categorize literacy into four dimensions: Foundational,\nPractical, Experimental, and Ethical, with tailored learning objectives to the\npre-clinical, clinical, and clinical research stages of medical education. This\nreview provides a road map for developing practical and relevant education\nstrategies for building an AI-competent healthcare workforce.",
      "tldr_zh": "这篇文献综述（scoping review）审查了2016年至2024年间1699篇文章，探讨了AI素养（AI literacy）在医学生教育中的整合，包括指导框架、教学程序和工具。研究发现，现有的教学指南更注重AI评估和伦理，而忽略了数据科学和编码等技术主题，并面临挑战如缺乏AI素养定义、临床价值证明不足以及合格教师短缺。作者提出一个新的AI素养框架，将其分为Foundational、Practical、Experimental和Ethical四个维度，并针对医学生的前临床、临床和临床研究阶段制定个性化学习目标，为构建AI能力强的医疗劳动力提供实用教育策略。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "25 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.18939v1",
      "published_date": "2024-07-10 16:34:41 UTC",
      "updated_date": "2024-07-10 16:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:16:30.165939"
    },
    {
      "arxiv_id": "2407.07810v5",
      "title": "Transformer Block Coupling and its Correlation with Generalization in LLMs",
      "title_zh": "Transformer 块耦合及其在大语言模型中的泛化相关性",
      "authors": [
        "Murdock Aubry",
        "Haoming Meng",
        "Anton Sugolov",
        "Vardan Papyan"
      ],
      "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. In this work, we analyze the trajectories\nof token embeddings as they pass through transformer blocks, linearizing the\nsystem along these trajectories through their Jacobian matrices. By examining\nthe relationships between these block Jacobians, we uncover the phenomenon of\n\\textbf{transformer block coupling} in a multitude of LLMs, characterized by\nthe coupling of their top singular vectors across tokens and depth. Our\nfindings reveal that coupling \\textit{positively correlates} with model\nperformance, and that this relationship is stronger than with other\nhyperparameters such as parameter count, model depth, and embedding dimension.\nWe further investigate how these properties emerge during training, observing a\nprogressive development of coupling, increased linearity, and layer-wise\nexponential growth in token trajectories. Additionally, experiments with Vision\nTransformers (ViTs) corroborate the emergence of coupling and its relationship\nwith generalization, reinforcing our findings in LLMs. Collectively, these\ninsights offer a novel perspective on token interactions in transformers,\nopening new directions for studying their mechanisms as well as improving\ntraining and generalization.",
      "tldr_zh": "本文研究了大型语言模型（LLMs）中 token embeddings 在 transformer blocks 中的轨迹，通过 Jacobian matrices 线性化系统，揭示了 transformer block coupling 现象，即块间顶层奇异向量的耦合。研究发现，这种耦合与模型性能正相关，且其影响强于参数数量、模型深度和嵌入维度等超参数。进一步分析显示，耦合在训练过程中逐步发展，伴随线性性和层级指数增长，并在 Vision Transformers (ViTs) 上得到验证。这些发现为理解 transformer 机制提供了新视角，并为优化模型训练和提升泛化能力开辟了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2407.07810v5",
      "published_date": "2024-07-10 16:30:27 UTC",
      "updated_date": "2025-03-05 04:47:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:16:41.444372"
    },
    {
      "arxiv_id": "2407.07802v1",
      "title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning",
      "title_zh": "ROSA：随机子空间适应用于高效微调",
      "authors": [
        "Marawan Gamal Abdel Hameed",
        "Aristides Milios",
        "Siva Reddy",
        "Guillaume Rabusseau"
      ],
      "abstract": "Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa",
      "tldr_zh": "本研究针对模型训练的高内存需求，提出了一种参数高效微调（PEFT）方法——Random Subspace Adaptation (ROSA)，它在推理时不引入延迟，并显著优于现有方法如 adapters、prompt tuning 和 Low-Rank Adaptation (LoRA)。ROSA 通过适应任意大维度的子空间，更好地逼近全量微调，并在理论上证明其表达性强于 LoRA，同时保持运行时内存不变。在自然语言处理（NLP）任务上，实验显示 ROSA 在使用 GPT-2 的自然语言生成（NLG）和 RoBERTa 的自然语言理解（NLU）场景中表现突出，尤其在几乎所有 GLUE 任务上均大幅超越 LoRA。研究还提供了开源代码，支持进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07802v1",
      "published_date": "2024-07-10 16:20:53 UTC",
      "updated_date": "2024-07-10 16:20:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:16:53.998877"
    },
    {
      "arxiv_id": "2407.07796v2",
      "title": "Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard",
      "title_zh": "通过基于网格的游戏竞赛评估大型语言模型：一个可扩展的LLM基准和排行榜",
      "authors": [
        "Oguzhan Topsakal",
        "Colby Jacob Edell",
        "Jackson Bailey Harper"
      ],
      "abstract": "We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.",
      "tldr_zh": "本研究提出了一种基于网格游戏（如Tic-Tac-Toe、Connect Four和Gomoku）的可扩展基准测试和排行榜，用于评估大型语言模型（LLMs）的性能。  \n他们开发了开源代码，模拟了2310场游戏比赛，涉及7个领先LLMs（如Claude 3.5 Sonnet、Gemini 1.5 Pro和GPT-4o），并使用list、illustration和image三种提示类型，生成JSON、CSV等格式的数据进行分析。  \n结果显示LLMs在不同游戏和提示类型下的表现存在显著差异，包括胜率、违规率和无效动作分析，这有助于评估其规则理解和战略思考能力。  \n这项工作为Artificial General Intelligence (AGI)路径上的复杂决策研究奠定基础，并鼓励更多LLMs结果的提交。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07796v2",
      "published_date": "2024-07-10 16:14:34 UTC",
      "updated_date": "2024-07-11 03:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:17:07.267017"
    },
    {
      "arxiv_id": "2407.07788v2",
      "title": "BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Nikita Chernyadev",
        "Nicholas Backshall",
        "Xiao Ma",
        "Yunfan Lu",
        "Younggyo Seo",
        "Stephen James"
      ],
      "abstract": "We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.",
      "tldr_zh": "本文提出 BiGym，这是一个基于演示驱动的移动双臂（bi-manual）机器人操作基准环境，旨在模拟家庭场景中的40个多样化任务，从简单目标到达到复杂厨房清洁。BiGym 提供人类收集的演示数据，支持多种观察输入，包括本体感觉数据和视觉输入（如 RGB 和深度信息从3个相机视角），以反映真实机器人轨迹的多样性。通过基准测试最先进的 imitation learning 和 demo-driven reinforcement learning 算法，该环境验证了其实用性，并讨论了未来机器人学习的机会。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project webpage: https://chernyadev.github.io/bigym/",
      "pdf_url": "http://arxiv.org/pdf/2407.07788v2",
      "published_date": "2024-07-10 16:04:18 UTC",
      "updated_date": "2024-07-11 16:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:17:17.449876"
    },
    {
      "arxiv_id": "2407.07787v1",
      "title": "Continuous Control with Coarse-to-fine Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Younggyo Seo",
        "Jafar Uruç",
        "Stephen James"
      ],
      "abstract": "Despite recent advances in improving the sample-efficiency of reinforcement\nlearning (RL) algorithms, designing an RL algorithm that can be practically\ndeployed in real-world environments remains a challenge. In this paper, we\npresent Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL\nagents to zoom-into a continuous action space in a coarse-to-fine manner,\nenabling the use of stable, sample-efficient value-based RL algorithms for\nfine-grained continuous control tasks. Our key idea is to train agents that\noutput actions by iterating the procedure of (i) discretizing the continuous\naction space into multiple intervals and (ii) selecting the interval with the\nhighest Q-value to further discretize at the next level. We then introduce a\nconcrete, value-based algorithm within the CRL framework called Coarse-to-fine\nQ-Network (CQN). Our experiments demonstrate that CQN significantly outperforms\nRL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation\ntasks with a modest number of environment interactions and expert\ndemonstrations. We also show that CQN robustly learns to solve real-world\nmanipulation tasks within a few minutes of online training.",
      "tldr_zh": "本文提出 Coarse-to-fine Reinforcement Learning (CRL) 框架，用于提升强化学习 (RL) 代理在连续控制任务中的样本效率，通过从粗到细的动作空间离散化方式训练代理。CRL 的核心方法包括迭代地将连续动作空间分解为多个区间，选择 Q-value 最高的区间进一步细化，并基于此引入了价值导向的 Coarse-to-fine Q-Network (CQN) 算法。实验结果显示，CQN 在 20 个稀疏奖励的 RLBench 操作任务上显著优于 RL 和行为克隆基线，仅需适量环境交互和专家演示即可，并在几分钟内实现真实世界操作任务的稳健学习。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Project webpage: https://younggyo.me/cqn/",
      "pdf_url": "http://arxiv.org/pdf/2407.07787v1",
      "published_date": "2024-07-10 16:04:08 UTC",
      "updated_date": "2024-07-10 16:04:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:17:30.836161"
    },
    {
      "arxiv_id": "2407.07934v4",
      "title": "Identifying Macro Conditional Independencies and Macro Total Effects in Summary Causal Graphs with Latent Confounding",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Ferreira",
        "Charles K. Assaad"
      ],
      "abstract": "Understanding causal relations in dynamic systems is essential in\nepidemiology. While causal inference methods have been extensively studied,\nthey often rely on fully specified causal graphs, which may not always be\navailable in complex dynamic systems. Partially specified causal graphs, and in\nparticular summary causal graphs (SCGs), provide a simplified representation of\ncausal relations between time series when working spacio-temporal data,\nomitting temporal information and focusing on causal structures between\nclusters of of temporal variables. Unlike fully specified causal graphs, SCGs\ncan contain cycles, which complicate their analysis and interpretation. In\naddition, their cluster-based nature introduces new challenges concerning the\ntypes of queries of interest: macro queries, which involve relationships\nbetween clusters represented as vertices in the graph, and micro queries, which\npertain to relationships between variables that are not directly visible\nthrough the vertices of the graph. In this paper, we first clearly distinguish\nbetween macro conditional independencies and micro conditional independencies\nand between macro total effects and micro total effects. Then, we demonstrate\nthe soundness and completeness of the d-separation to identify macro\nconditional independencies in SCGs. Furthermore, we establish that the\ndo-calculus is sound and complete for identifying macro total effects in SCGs.\nFinally, we give a graphical characterization for the non-identifiability of\nmacro total effects in SCGs.",
      "tldr_zh": "这篇论文探讨了在包含潜在混杂的总结因果图 (SCGs) 中识别宏观条件独立性和宏观总效应的方法，针对动态系统如流行病学中的时序数据简化因果关系分析。论文首先明确区分了宏观条件独立性（涉及变量集群间关系）与微观条件独立性，以及宏观总效应与微观总效应。接着，它证明了 d-separation 在 SCGs 中识别宏观条件独立性是可靠且完整的，同时 do-calculus 对于识别宏观总效应也同样可靠且完整。最后，论文提供了 SCGs 中宏观总效应不可识别性的图形特征，为复杂因果图的查询提供了新的理论基础。",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "Accepted CI4TS Workshop at UAI2024. Accepted at AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2407.07934v4",
      "published_date": "2024-07-10 16:03:04 UTC",
      "updated_date": "2024-12-20 10:14:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:17:44.249870"
    },
    {
      "arxiv_id": "2407.07786v2",
      "title": "The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing",
      "title_zh": "AI 红队测试中的人为因素：社交计算和协作计算的视角",
      "authors": [
        "Alice Qian Zhang",
        "Ryland Shaw",
        "Jacy Reese Anthis",
        "Ashlee Milton",
        "Emily Tseng",
        "Jina Suh",
        "Lama Ahmad",
        "Ram Shankar Siva Kumar",
        "Julian Posada",
        "Benjamin Shestakofsky",
        "Sarah T. Roberts",
        "Mary L. Gray"
      ],
      "abstract": "Rapid progress in general-purpose AI has sparked significant interest in \"red\nteaming,\" a practice of adversarial testing originating in military and\ncybersecurity applications. AI red teaming raises many questions about the\nhuman factor, such as how red teamers are selected, biases and blindspots in\nhow tests are conducted, and harmful content's psychological effects on red\nteamers. A growing body of HCI and CSCW literature examines related\npractices-including data labeling, content moderation, and algorithmic\nauditing. However, few, if any have investigated red teaming itself. Future\nstudies may explore topics ranging from fairness to mental health and other\nareas of potential harm. We aim to facilitate a community of researchers and\npractitioners who can begin to meet these challenges with creativity,\ninnovation, and thoughtful reflection.",
      "tldr_zh": "该论文探讨了AI红队测试中的人为因素，从社会计算和社会协作计算（CSCW）的角度分析问题，如红队成员的选择、测试中的偏见和盲点，以及有害内容对红队成员的心理影响。作者回顾了HCI和CSCW领域的相关实践，包括数据标记、内容审核和算法审计，但指出对AI红队测试本身的系统性研究仍十分缺乏。主要贡献在于提出未来研究方向，如公平性、心理健康和其他潜在危害，并呼吁建立一个创新型研究和实践社区，以应对这些挑战。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Updated with camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2407.07786v2",
      "published_date": "2024-07-10 16:02:13 UTC",
      "updated_date": "2024-09-11 16:02:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:17:55.792752"
    },
    {
      "arxiv_id": "2407.12862v1",
      "title": "Analyzing Large language models chatbots: An experimental approach using a probability test",
      "title_zh": "翻译失败",
      "authors": [
        "Melise Peruchini",
        "Julio Monteiro Teixeira"
      ],
      "abstract": "This study consists of qualitative empirical research, conducted through\nexploratory tests with two different Large Language Models (LLMs) chatbots:\nChatGPT and Gemini. The methodological procedure involved exploratory tests\nbased on prompts designed with a probability question. The \"Linda Problem\",\nwidely recognized in cognitive psychology, was used as a basis to create the\ntests, along with the development of a new problem specifically for this\nexperiment, the \"Mary Problem\". The object of analysis is the dataset with the\noutputs provided by each chatbot interaction. The purpose of the analysis is to\nverify whether the chatbots mainly employ logical reasoning that aligns with\nprobability theory or if they are more frequently affected by the stereotypical\ntextual descriptions in the prompts. The findings provide insights about the\napproach each chatbot employs in handling logic and textual constructions,\nsuggesting that, while the analyzed chatbots perform satisfactorily on a\nwell-known probabilistic problem, they exhibit significantly lower performance\non new tests that require direct application of probabilistic logic.",
      "tldr_zh": "本研究通过定性实证方法，对Large Language Models (LLMs)聊天机器人ChatGPT和Gemini进行了探索性测试，使用基于“Linda Problem”的提示设计概率问题，并创建了新问题“Mary Problem”。实验旨在分析这些聊天机器人是否优先采用符合概率理论的逻辑推理，还是更易受提示中刻板文本描述的影响。结果显示，ChatGPT和Gemini在经典概率问题上表现良好，但在要求直接应用概率逻辑的新测试中，性能显著下降，揭示了LLMs在处理逻辑与文本结构时的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 3 figures, Submitted to ACM Transactions on Intelligent\n  systems and Technology",
      "pdf_url": "http://arxiv.org/pdf/2407.12862v1",
      "published_date": "2024-07-10 15:49:40 UTC",
      "updated_date": "2024-07-10 15:49:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:18:07.003350"
    },
    {
      "arxiv_id": "2407.07775v2",
      "title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Hao-Tien Lewis Chiang",
        "Zhuo Xu",
        "Zipeng Fu",
        "Mithun George Jacob",
        "Tingnan Zhang",
        "Tsang-Wei Edward Lee",
        "Wenhao Yu",
        "Connor Schenck",
        "David Rendleman",
        "Dhruv Shah",
        "Fei Xia",
        "Jasmine Hsu",
        "Jonathan Hoech",
        "Pete Florence",
        "Sean Kirmani",
        "Sumeet Singh",
        "Vikas Sindhwani",
        "Carolina Parada",
        "Chelsea Finn",
        "Peng Xu",
        "Sergey Levine",
        "Jie Tan"
      ],
      "abstract": "An elusive goal in navigation research is to build an intelligent agent that\ncan understand multimodal instructions including natural language and image,\nand perform useful navigation. To achieve this, we study a widely useful\ncategory of navigation tasks we call Multimodal Instruction Navigation with\ndemonstration Tours (MINT), in which the environment prior is provided through\na previously recorded demonstration video. Recent advances in Vision Language\nModels (VLMs) have shown a promising path in achieving this goal as it\ndemonstrates capabilities in perceiving and reasoning about multimodal inputs.\nHowever, VLMs are typically trained to predict textual output and it is an open\nresearch question about how to best utilize them in navigation. To solve MINT,\nwe present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation\npolicy that combines the environment understanding and common sense reasoning\npower of long-context VLMs and a robust low-level navigation policy based on\ntopological graphs. The high-level policy consists of a long-context VLM that\ntakes the demonstration tour video and the multimodal user instruction as input\nto find the goal frame in the tour video. Next, a low-level policy uses the\ngoal frame and an offline constructed topological graph to generate robot\nactions at every timestep. We evaluated Mobility VLA in a 836m^2 real world\nenvironment and show that Mobility VLA has a high end-to-end success rates on\npreviously unsolved multimodal instructions such as \"Where should I return\nthis?\" while holding a plastic bin. A video demonstrating Mobility VLA can be\nfound here: https://youtu.be/-Tof__Q8_5s",
      "tldr_zh": "本文提出 Mobility VLA，一种分层 Vision-Language-Action (VLA) 导航策略，旨在解决 Multimodal Instruction Navigation with demonstration Tours (MINT) 任务，该任务涉及理解自然语言和图像等多模态指令，并利用先前录制的演示视频作为环境先验。框架的高层策略使用长上下文 Vision Language Models (VLMs) 处理演示视频和用户指令，以精确定位目标帧；低层策略则基于预构建的 Topological Graphs 生成机器人实时动作。实验在 836m² 真实环境中显示，Mobility VLA 在处理复杂指令（如“在哪里归还这个？”）时表现出高成功率，显著提升了多模态导航的可靠性和实用性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07775v2",
      "published_date": "2024-07-10 15:49:07 UTC",
      "updated_date": "2024-07-12 14:37:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:18:20.745546"
    },
    {
      "arxiv_id": "2407.07760v2",
      "title": "Learning Spatial-Semantic Features for Robust Video Object Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Li",
        "Deshui Miao",
        "Zhenyu He",
        "Yaowei Wang",
        "Huchuan Lu",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Tracking and segmenting multiple similar objects with distinct or complex\nparts in long-term videos is particularly challenging due to the ambiguity in\nidentifying target components and the confusion caused by occlusion, background\nclutter, and changes in appearance or environment over time. In this paper, we\npropose a robust video object segmentation framework that learns\nspatial-semantic features and discriminative object queries to address the\nabove issues. Specifically, we construct a spatial-semantic block comprising a\nsemantic embedding component and a spatial dependency modeling part for\nassociating global semantic features and local spatial features, providing a\ncomprehensive target representation. In addition, we develop a masked\ncross-attention module to generate object queries that focus on the most\ndiscriminative parts of target objects during query propagation, alleviating\nnoise accumulation to ensure effective long-term query propagation. Extensive\nexperimental results show that the proposed method achieves state-of-the-art\nperformance on benchmark data sets, including the DAVIS2017 test\n(\\textbf{87.8\\%}), YoutubeVOS 2019 (\\textbf{88.1\\%}), MOSE val\n(\\textbf{74.0\\%}), and LVOS test (\\textbf{73.0\\%}), and demonstrate the\neffectiveness and generalization capacity of our model. The source code and\ntrained models are released at\n\\href{https://github.com/yahooo-m/S3}{https://github.com/yahooo-m/S3}.",
      "tldr_zh": "本文提出了一种鲁棒的视频对象分割框架，通过学习 spatial-semantic features 和 discriminative object queries 来解决长视频中跟踪和分割多个相似对象的挑战，这些对象可能因遮挡、背景杂乱或外观变化而导致歧义。框架的核心包括 spatial-semantic block，用于关联全局语义特征和局部空间特征以提供全面的目标表示，以及 masked cross-attention module，用于生成专注于目标判别性部分的 object queries，从而减少噪声积累并确保长期查询传播的有效性。实验结果显示，该方法在基准数据集上取得了 state-of-the-art 性能，包括 DAVIS2017 测试（87.8%）、YoutubeVOS 2019（88.1%）、MOSE val（74.0%）和 LVOS 测试（73.0%），证明了其有效性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published as a conference paper in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.07760v2",
      "published_date": "2024-07-10 15:36:00 UTC",
      "updated_date": "2025-04-07 07:55:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:18:34.240970"
    },
    {
      "arxiv_id": "2407.07755v3",
      "title": "Neural Geometry Processing via Spherical Neural Surfaces",
      "title_zh": "翻译失败",
      "authors": [
        "Romy Williamson",
        "Niloy J. Mitra"
      ],
      "abstract": "Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. In the absence of an analogous toolbox, neural\nrepresentations are typically discretized and converted into a mesh, before\napplying any geometry processing algorithm. This is unsatisfactory and, as we\ndemonstrate, unnecessary. In this work, we propose a spherical neural surface\nrepresentation for genus-0 surfaces and demonstrate how to compute core\ngeometric operators directly on this representation. Namely, we estimate\nsurface normals and first and second fundamental forms of the surface, as well\nas compute surface gradient, surface divergence and Laplace-Beltrami operator\non scalar/vector fields defined on the surface. Our representation is fully\nseamless, overcoming a key limitation of similar explicit representations such\nas Neural Surface Maps [Morreale et al. 2021]. These operators, in turn, enable\ngeometry processing directly on the neural representations without any\nunnecessary meshing. We demonstrate illustrative applications in (neural)\nspectral analysis, heat flow and mean curvature flow, and evaluate robustness\nto isometric shape variations. We propose theoretical formulations and validate\ntheir numerical estimates, against analytical estimates, mesh-based baselines,\nand neural alternatives, where available. By systematically linking neural\nsurface representations with classical geometry processing algorithms, we\nbelieve that this work can become a key ingredient in enabling neural geometry\nprocessing. Code is accessible from the project webpage.",
      "tldr_zh": "本文提出了一种 spherical neural surface 表示，用于处理零属（genus-0）表面几何，克服了传统神经表面需要离散化转换为多边形网格的局限。该方法直接在神经表示上计算核心几何运算符，包括表面法线、第一和第二基本形式、表面梯度、散度和 Laplace-Beltrami operator，实现无缝的几何处理。实验验证了该表示的鲁棒性，并在神经谱分析、热流和平均曲率流等应用中展示了其有效性。总体上，这为神经几何处理提供了关键工具，桥接了神经表示与经典几何算法。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "I.3.5"
      ],
      "primary_category": "cs.GR",
      "comment": "14 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.07755v3",
      "published_date": "2024-07-10 15:28:02 UTC",
      "updated_date": "2025-03-14 18:11:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:18:44.412564"
    },
    {
      "arxiv_id": "2407.08658v1",
      "title": "Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Lucca Emmanuel Pineli Simões",
        "Lucas Brandão Rodrigues",
        "Rafaela Mota Silva",
        "Gustavo Rodrigues da Silva"
      ],
      "abstract": "This paper presents the development and comparative evaluation of three voice\ncommand pipelines for controlling a Tello drone, using speech recognition and\ndeep learning techniques. The aim is to enhance human-machine interaction by\nenabling intuitive voice control of drone actions. The pipelines developed\ninclude: (1) a traditional Speech-to-Text (STT) followed by a Large Language\nModel (LLM) approach, (2) a direct voice-to-function mapping model, and (3) a\nSiamese neural network-based system. Each pipeline was evaluated based on\ninference time, accuracy, efficiency, and flexibility. Detailed methodologies,\ndataset preparation, and evaluation metrics are provided, offering a\ncomprehensive analysis of each pipeline's strengths and applicability across\ndifferent scenarios.",
      "tldr_zh": "本文评估了三种语音命令管道，用于控制Tello无人机，以提升人机交互的直观性。管道包括：(1) 传统的Speech-to-Text (STT) 后跟Large Language Model (LLM) 方法、(2) 直接语音到功能映射模型，以及(3) Siamese神经网络-based系统。这些管道基于推理时间、准确性、效率和灵活性进行比较，详细分析了其方法、数据集准备和适用场景的优势。结果为未来无人机语音控制技术提供了全面的参考。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "I.2.7; I.2.10"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.08658v1",
      "published_date": "2024-07-10 15:15:26 UTC",
      "updated_date": "2024-07-10 15:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:18:56.563592"
    },
    {
      "arxiv_id": "2408.00002v2",
      "title": "Transfer Learning for Wildlife Classification: Evaluating YOLOv8 against DenseNet, ResNet, and VGGNet on a Custom Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Subek Sharma",
        "Sisir Dhakal",
        "Mansi Bhavsar"
      ],
      "abstract": "This study evaluates the performance of various deep learning models,\nspecifically DenseNet, ResNet, VGGNet, and YOLOv8, for wildlife species\nclassification on a custom dataset. The dataset comprises 575 images of 23\nendangered species sourced from reputable online repositories. The study\nutilizes transfer learning to fine-tune pre-trained models on the dataset,\nfocusing on reducing training time and enhancing classification accuracy. The\nresults demonstrate that YOLOv8 outperforms other models, achieving a training\naccuracy of 97.39% and a validation F1-score of 96.50%. These findings suggest\nthat YOLOv8, with its advanced architecture and efficient feature extraction\ncapabilities, holds great promise for automating wildlife monitoring and\nconservation efforts.",
      "tldr_zh": "这篇论文评估了Transfer Learning在野生动物分类中的应用，通过比较DenseNet、ResNet、VGGNet和YOLOv8在自定义数据集上的性能。数据集包含575张23种濒危物种的图像，研究利用迁移学习微调预训练模型，以减少训练时间并提升分类准确率。结果显示YOLOv8表现出色，达到97.39%的训练准确率和96.50%的验证F1-score，优于其他模型。这些发现表明YOLOv8的先进架构可为自动化野生动物监测和保护努力提供高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is published in Journal of Artificial Intelligence and Capsule\n  Networks, December 2024, Volume 6, Issue 4, Pages 415-435",
      "pdf_url": "http://arxiv.org/pdf/2408.00002v2",
      "published_date": "2024-07-10 15:03:00 UTC",
      "updated_date": "2024-11-12 14:55:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:19:07.710733"
    },
    {
      "arxiv_id": "2407.07728v5",
      "title": "SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature Disentanglement and Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Wang",
        "Le Ma",
        "Yongsheng Feng",
        "Xin Pan",
        "Yuhang Jin",
        "Kejun Zhang"
      ],
      "abstract": "Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,\ndataset, and documents of SaMoye are publicly available on\n\\url{https://github.com/CarlWangChina/SaMoye-SVC}.",
      "tldr_zh": "SaMoye 是一个基于 Feature Disentanglement and Enhancement 的零-shot Singing Voice Conversion (SVC) 模型，能够将歌声转换为人类或非人类音色，而无需特定训练数据。该模型通过将歌声特征分离为内容、音色和音高特征，并结合多个 ASR 模型压缩内容特征以减少音色泄露，同时增强音色特征（如解冻说话者编码器并混合类似说话者嵌入）。研究团队构建了一个大规模数据集，包含超过 1815 小时纯歌声和 6367 个说话者，以确保零-shot 性能。实验结果表明，SaMoye 在零-shot SVC 任务中优于其他模型，甚至在极端条件下（如转换到动物音色），并已开源代码和权重于 GitHub。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "68Txx(Primary)14F05, 91Fxx(Secondary)",
        "I.2.7; J.5"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper needs major changes for resubmit",
      "pdf_url": "http://arxiv.org/pdf/2407.07728v5",
      "published_date": "2024-07-10 15:00:08 UTC",
      "updated_date": "2024-11-15 07:51:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:19:21.062913"
    },
    {
      "arxiv_id": "2407.07726v2",
      "title": "PaliGemma: A versatile 3B VLM for transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Beyer",
        "Andreas Steiner",
        "André Susano Pinto",
        "Alexander Kolesnikov",
        "Xiao Wang",
        "Daniel Salz",
        "Maxim Neumann",
        "Ibrahim Alabdulmohsin",
        "Michael Tschannen",
        "Emanuele Bugliarello",
        "Thomas Unterthiner",
        "Daniel Keysers",
        "Skanda Koppula",
        "Fangyu Liu",
        "Adam Grycner",
        "Alexey Gritsenko",
        "Neil Houlsby",
        "Manoj Kumar",
        "Keran Rong",
        "Julian Eisenschlos",
        "Rishabh Kabra",
        "Matthias Bauer",
        "Matko Bošnjak",
        "Xi Chen",
        "Matthias Minderer",
        "Paul Voigtlaender",
        "Ioana Bica",
        "Ivana Balazevic",
        "Joan Puigcerver",
        "Pinelopi Papalampidi",
        "Olivier Henaff",
        "Xi Xiong",
        "Radu Soricut",
        "Jeremiah Harmsen",
        "Xiaohua Zhai"
      ],
      "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.",
      "tldr_zh": "本研究推出了 PaliGemma，这是一个开源的 Vision-Language Model (VLM)，基于 SigLIP-So400m 视觉编码器和 Gemma-2B 语言模型构建。PaliGemma 被训练为一个多功能且知识广泛的基模型，旨在便于转移学习，并在各种开放世界任务中表现出色。该模型在近40种多样任务上进行了评估，包括标准的 VLM 基准测试以及专业领域如遥感和分割任务。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "v2 adds Appendix H and I and a few citations",
      "pdf_url": "http://arxiv.org/pdf/2407.07726v2",
      "published_date": "2024-07-10 14:57:46 UTC",
      "updated_date": "2024-10-10 17:28:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:19:31.356784"
    },
    {
      "arxiv_id": "2407.07695v1",
      "title": "African Democracy in the Era of Generative Disinformation: Challenges and Countermeasures against AI-Generated Propaganda",
      "title_zh": "非洲民主在生成式虚假信息时代：针对AI生成宣传的挑战和对策",
      "authors": [
        "Chinasa T. Okolo"
      ],
      "abstract": "In light of prominent discourse around the negative implications of\ngenerative AI, an emerging area of research is investigating the current and\nestimated impacts of AI-generated propaganda on African citizens participating\nin elections. Throughout Africa, there have already been suspected cases of\nAI-generated propaganda influencing electoral outcomes or precipitating coups\nin countries like Nigeria, Burkina Faso, and Gabon, underscoring the need for\ncomprehensive research in this domain. This paper aims to highlight the risks\nassociated with the spread of generative AI-driven disinformation within Africa\nwhile concurrently examining the roles of government, civil society, academia,\nand the general public in the responsible development, practical use, and\nrobust governance of AI. To understand how African governments might\neffectively counteract the impact of AI-generated propaganda, this paper\npresents case studies illustrating the current usage of generative AI for\nelection-related propaganda in Africa. Subsequently, this paper discusses\nefforts by fact-checking organisations to mitigate the negative impacts of\ndisinformation, explores the potential for new initiatives to actively engage\ncitizens in literacy efforts to combat disinformation spread, and advocates for\nincreased governmental regulatory measures. Overall, this research seeks to\nincrease comprehension of the potential ramifications of AI-generated\npropaganda on democratic processes within Africa and propose actionable\nstrategies for stakeholders to address these multifaceted challenges.",
      "tldr_zh": "这篇论文探讨了生成式 AI 生成的宣传（AI-Generated Propaganda）对非洲民主的潜在威胁，包括其对选举结果和政变的实际影响，如在尼日利亚、布基纳法索和加蓬的案例。论文通过案例研究分析了政府、公民社会、学术界和公众在 AI 负责任开发、使用和治理中的角色，并评估了事实检查组织在缓解虚假信息（Disinformation）传播方面的努力。最终，它提出行动策略，如加强公民识读教育和政府监管措施，以应对这些挑战并保护非洲民主进程。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "I.2.0; K.4.1"
      ],
      "primary_category": "cs.CY",
      "comment": "Building a Just AI Ecosystem in Africa Conference Cape Town, South\n  Africa",
      "pdf_url": "http://arxiv.org/pdf/2407.07695v1",
      "published_date": "2024-07-10 14:24:53 UTC",
      "updated_date": "2024-07-10 14:24:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:19:43.912265"
    },
    {
      "arxiv_id": "2407.07684v2",
      "title": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle Control",
      "title_zh": "迈向人类般的驾驶：自动驾驶车辆控制中的主动推理",
      "authors": [
        "Elahe Delavari",
        "John Moore",
        "Junho Hong",
        "Jaerock Kwon"
      ],
      "abstract": "This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.",
      "tldr_zh": "这篇论文提出了一种基于 Active Inference 的新型自动驾驶车辆（AV）控制方法，该理论源于神经科学，将大脑视为预测机器，以最小化预测错误（surprise）并平衡感知与行动，从而提升 AV 的适应性和效率。该方法整合 Active Inference 与深度学习，专注于 AV 的横向控制（如车道保持），并在模拟城市环境中进行测试，展示了从有限数据中学习和泛化的能力。实验结果显示，在 CARLA 模拟器中，该模型在动态场景下的性能和适应性优于传统 Modular Pipelines、Imitation Learning 和 Reinforcement Learning 方法，并更接近人类驾驶行为。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "The work is partly supported by a sponsor. Authors need to complete\n  the final report submission before any type of publication according to the\n  sponsor. The final report will be submitted in few weeks. This work has been\n  superseded by arXiv:2503.01676",
      "pdf_url": "http://arxiv.org/pdf/2407.07684v2",
      "published_date": "2024-07-10 14:08:27 UTC",
      "updated_date": "2024-09-16 16:02:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:19:57.534311"
    },
    {
      "arxiv_id": "2407.07671v1",
      "title": "Why should we ever automate moral decision making?",
      "title_zh": "为什么我们应该自动化道德决策？",
      "authors": [
        "Vincent Conitzer"
      ],
      "abstract": "While people generally trust AI to make decisions in various aspects of their\nlives, concerns arise when AI is involved in decisions with significant moral\nimplications. The absence of a precise mathematical framework for moral\nreasoning intensifies these concerns, as ethics often defies simplistic\nmathematical models. Unlike fields such as logical reasoning, reasoning under\nuncertainty, and strategic decision-making, which have well-defined\nmathematical frameworks, moral reasoning lacks a broadly accepted framework.\nThis absence raises questions about the confidence we can place in AI's moral\ndecision-making capabilities.\n  The environments in which AI systems are typically trained today seem\ninsufficiently rich for such a system to learn ethics from scratch, and even if\nwe had an appropriate environment, it is unclear how we might bring about such\nlearning. An alternative approach involves AI learning from human moral\ndecisions. This learning process can involve aggregating curated human\njudgments or demonstrations in specific domains, or leveraging a foundation\nmodel fed with a wide range of data. Still, concerns persist, given the\nimperfections in human moral decision making.\n  Given this, why should we ever automate moral decision making -- is it not\nbetter to leave all moral decision making to humans? This paper lays out a\nnumber of reasons why we should expect AI systems to engage in decisions with a\nmoral component, with brief discussions of the associated risks.",
      "tldr_zh": "这篇论文探讨了自动化道德决策的必要性，尽管人们对 AI 参与道德决策存在担忧，因为道德推理缺乏精确的数学框架，且 AI 训练环境不足以从零学习道德。论文指出，尽管人类道德决策本身也不完美，但 AI 可以从人类判断或数据中学习，从而在某些情况下提供更一致的决策。最终，作者列出了 AI 从事道德决策的若干理由，并讨论了潜在风险，包括可靠性问题和伦理挑战。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07671v1",
      "published_date": "2024-07-10 13:59:22 UTC",
      "updated_date": "2024-07-10 13:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:20:08.525562"
    },
    {
      "arxiv_id": "2407.07668v2",
      "title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning",
      "title_zh": "如何利用预测不确定性估计减少在线持续学习中的灾难性遗忘",
      "authors": [
        "Giuseppe Serra",
        "Ben Werner",
        "Florian Buettner"
      ],
      "abstract": "Many real-world applications require machine-learning models to be able to\ndeal with non-stationary data distributions and thus learn autonomously over an\nextended period of time, often in an online setting. One of the main challenges\nin this scenario is the so-called catastrophic forgetting (CF) for which the\nlearning model tends to focus on the most recent tasks while experiencing\npredictive degradation on older ones. In the online setting, the most effective\nsolutions employ a fixed-size memory buffer to store old samples used for\nreplay when training on new tasks. Many approaches have been presented to\ntackle this problem. However, it is not clear how predictive uncertainty\ninformation for memory management can be leveraged in the most effective manner\nand conflicting strategies are proposed to populate the memory. Are the\neasiest-to-forget or the easiest-to-remember samples more effective in\ncombating CF? Starting from the intuition that predictive uncertainty provides\nan idea of the samples' location in the decision space, this work presents an\nin-depth analysis of different uncertainty estimates and strategies for\npopulating the memory. The investigation provides a better understanding of the\ncharacteristics data points should have for alleviating CF. Then, we propose an\nalternative method for estimating predictive uncertainty via the generalised\nvariance induced by the negative log-likelihood. Finally, we demonstrate that\nthe use of predictive uncertainty measures helps in reducing CF in different\nsettings.",
      "tldr_zh": "该论文探讨了如何利用预测不确定性估计来缓解在线持续学习中的灾难性遗忘（catastrophic forgetting），即模型在新任务上训练时遗忘旧任务的问题。研究通过深入分析不同不确定性指标和内存管理策略（如优先存储容易遗忘或容易记住的样本），揭示了数据点特性对减轻CF的关键影响，并提出了一种基于负对数似然诱导的广义方差方法来估计算不确定性。实验结果表明，这种方法在多种场景下均能有效降低CF，提高了模型的持续学习性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2405.18925",
      "pdf_url": "http://arxiv.org/pdf/2407.07668v2",
      "published_date": "2024-07-10 13:51:15 UTC",
      "updated_date": "2024-10-10 10:34:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:20:20.716880"
    },
    {
      "arxiv_id": "2407.15861v2",
      "title": "Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Zhang",
        "Mingwang Hu",
        "Wenhui Li",
        "Lanjun Wang"
      ],
      "abstract": "Recently, the text-to-image diffusion model has gained considerable attention\nfrom the community due to its exceptional image generation capability. A\nrepresentative model, Stable Diffusion, amassed more than 10 million users\nwithin just two months of its release. This surge in popularity has facilitated\nstudies on the robustness and safety of the model, leading to the proposal of\nvarious adversarial attack methods. Simultaneously, there has been a marked\nincrease in research focused on defense methods to improve the robustness and\nsafety of these models. In this survey, we provide a comprehensive review of\nthe literature on adversarial attacks and defenses targeting text-to-image\ndiffusion models. We begin with an overview of text-to-image diffusion models,\nfollowed by an introduction to a taxonomy of adversarial attacks and an\nin-depth review of existing attack methods. We then present a detailed analysis\nof current defense methods that improve model robustness and safety. Finally,\nwe discuss ongoing challenges and explore promising future research directions.\nFor a complete list of the adversarial attack and defense methods covered in\nthis survey, please refer to our curated repository at\nhttps://github.com/datar001/Awesome-AD-on-T2IDM.",
      "tldr_zh": "这篇调查论文对针对 text-to-image diffusion models 的 adversarial attacks 和 defenses 进行了全面回顾，涵盖了模型的兴起（如 Stable Diffusion 的快速普及）及其面临的安全挑战。论文首先概述了这些模型，然后分类并深入分析了现有攻击方法，包括各种攻击类型和机制；随后，详细探讨了防御策略，以提升模型的鲁棒性和安全性。最终，它总结了当前研究中的关键挑战、未来方向，并提供了一个资源仓库（https://github.com/datar001/Awesome-AD-on-T2IDM）。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted for Information Fusion. Related benchmarks and codes are\n  available at \\url{https://github.com/datar001/Awesome-AD-on-T2IDM}",
      "pdf_url": "http://arxiv.org/pdf/2407.15861v2",
      "published_date": "2024-07-10 13:50:31 UTC",
      "updated_date": "2024-09-13 04:11:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:20:31.411297"
    },
    {
      "arxiv_id": "2407.07666v1",
      "title": "A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models : Safety, Consensus, Objectivity, Reproducibility and Explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Fang Tan",
        "Kabilan Elangovan",
        "Jasmine Ong",
        "Nigam Shah",
        "Joseph Sung",
        "Tien Yin Wong",
        "Lan Xue",
        "Nan Liu",
        "Haibo Wang",
        "Chang Fu Kuo",
        "Simon Chesterman",
        "Zee Kin Yeong",
        "Daniel SW Ting"
      ],
      "abstract": "A comprehensive qualitative evaluation framework for large language models\n(LLM) in healthcare that expands beyond traditional accuracy and quantitative\nmetrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,\nConsensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We\nsuggest that S.C.O.R.E. may form the basis for an evaluation framework for\nfuture LLM-based models that are safe, reliable, trustworthy, and ethical for\nhealthcare and clinical applications.",
      "tldr_zh": "本研究提出了一种针对大型语言模型（Large Language Models, LLMs）的S.C.O.R.E.评估框架，包括Safety（安全）、Consensus（共识）、Objectivity（客观性）、Reproducibility（可重复性）和Explainability（可解释性）五个关键方面。框架旨在扩展传统准确性和定量指标，提供全面的定性评估，以确保LLMs在医疗领域的应用更安全、可信和合乎伦理。该框架可作为未来LLM模型开发的基础，促进可靠的医疗和临床解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07666v1",
      "published_date": "2024-07-10 13:45:16 UTC",
      "updated_date": "2024-07-10 13:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:20:43.213782"
    },
    {
      "arxiv_id": "2407.07664v2",
      "title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning Geometry",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Lindström",
        "Borja Rodríguez-Gálvez",
        "Ragnar Thobaben",
        "Mikael Skoglund"
      ],
      "abstract": "Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.",
      "tldr_zh": "本研究对Hyperspherical Prototypical Learning (HPL) 的几何特性进行了基于编码理论的分析，HPL是一种在单位超球面上设计类原型的监督表示学习方法，能够实现类别的尺度不变分离。针对现有方法的优化过程不严谨或维度受限问题，本文提出一个严谨的优化程序，并证明其解决方案是最优的，同时利用linear block codes在多种维度中构建良好分离的原型。最终，通过可实现性和反向界限的完整表征，展示了所提方法的近似最优性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Changes in version 2: Minor formatting changes. Published in the\n  Proceedings of the Geometry-grounded Representation Learning and Generative\n  Modeling Workshop (GRaM), PMLR 251. Available at:\n  https://proceedings.mlr.press/v251/lindstrom24a.html 14 pages: 9 of the main\n  paper, 2 of references, and 3 of appendices.. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl",
      "pdf_url": "http://arxiv.org/pdf/2407.07664v2",
      "published_date": "2024-07-10 13:44:19 UTC",
      "updated_date": "2025-04-17 16:04:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:20:55.517535"
    },
    {
      "arxiv_id": "2407.07660v1",
      "title": "Boosting Medical Image Synthesis via Registration-guided Consistency and Disentanglement Learning",
      "title_zh": "通过注册引导一致性和解耦学习提升医疗图像合成",
      "authors": [
        "Chuanpu Li",
        "Zeli Chen",
        "Yiwen Zhang",
        "Liming Zhong",
        "Wei Yang"
      ],
      "abstract": "Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.",
      "tldr_zh": "本研究针对医疗图像合成中训练过程中的对齐噪声问题，提出了一种结合 registration-guided consistency 和 disentanglement learning 的方法，以提升合成模块的任务特定性和准确性。具体而言，该方法通过在合成前后应用相同的变形场（deformation fields）并引入 alignment loss 来强制输出一致性，同时利用 disentanglement learning 使合成模块能够分离解耦解剖结构（anatomical structures）和不同模态的特定样式，并通过 anatomy consistency loss 确保潜在空间（latent spaces）中的几何完整性。在内部腹部 CECT-CT 数据集和公开盆腔 MR-CT 数据集上的实验表明，该方法优于现有方法，显著提高了图像合成的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07660v1",
      "published_date": "2024-07-10 13:41:26 UTC",
      "updated_date": "2024-07-10 13:41:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:21:07.482705"
    },
    {
      "arxiv_id": "2407.07639v1",
      "title": "Explaining Graph Neural Networks for Node Similarity on Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Daza",
        "Cuong Xuan Chu",
        "Trung-Kien Tran",
        "Daria Stepanova",
        "Michael Cochez",
        "Paul Groth"
      ],
      "abstract": "Similarity search is a fundamental task for exploiting information in various\napplications dealing with graph data, such as citation networks or knowledge\ngraphs. While this task has been intensively approached from heuristics to\ngraph embeddings and graph neural networks (GNNs), providing explanations for\nsimilarity has received less attention. In this work we are concerned with\nexplainable similarity search over graphs, by investigating how GNN-based\nmethods for computing node similarities can be augmented with explanations.\nSpecifically, we evaluate the performance of two prominent approaches towards\nexplanations in GNNs, based on the concepts of mutual information (MI), and\ngradient-based explanations (GB). We discuss their suitability and empirically\nvalidate the properties of their explanations over different popular graph\nbenchmarks. We find that unlike MI explanations, gradient-based explanations\nhave three desirable properties. First, they are actionable: selecting inputs\ndepending on them results in predictable changes in similarity scores. Second,\nthey are consistent: the effect of selecting certain inputs overlaps very\nlittle with the effect of discarding them. Third, they can be pruned\nsignificantly to obtain sparse explanations that retain the effect on\nsimilarity scores.",
      "tldr_zh": "这篇论文探讨了如何为图神经网络(GNNs)计算的节点相似性提供解释，以提升图数据应用（如引用网络或知识图谱）的可解释性。作者评估了两种主要方法：基于互信息(MI)的解释和基于梯度(GB)的解释，并通过多个图基准进行实证验证。结果表明，GB解释具有三个关键优势：可操作性（选择特定输入可预测地改变相似性分数）、一致性（选择与丢弃输入的效果重叠小）、以及可修剪性（可生成稀疏解释，同时保留相似性分数的影响）。这项工作为GNNs的可解释相似性搜索奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07639v1",
      "published_date": "2024-07-10 13:20:47 UTC",
      "updated_date": "2024-07-10 13:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:21:19.230299"
    },
    {
      "arxiv_id": "2407.07638v3",
      "title": "Tuning Vision-Language Models with Candidate Labels by Prompt Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Zhifang Zhang",
        "Yuwei Niu",
        "Xin Liu",
        "Beibei Li"
      ],
      "abstract": "Vision-language models (VLMs) can learn high-quality representations from a\nlarge-scale training dataset of image-text pairs. Prompt learning is a popular\napproach to fine-tuning VLM to adapt them to downstream tasks. Despite the\nsatisfying performance, a major limitation of prompt learning is the demand for\nlabelled data. In real-world scenarios, we may only obtain candidate labels\n(where the true label is included) instead of the true labels due to data\nprivacy or sensitivity issues. In this paper, we provide the first study on\nprompt learning with candidate labels for VLMs. We empirically demonstrate that\nprompt learning is more advantageous than other fine-tuning methods, for\nhandling candidate labels. Nonetheless, its performance drops when the label\nambiguity increases. In order to improve its robustness, we propose a simple\nyet effective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our framework\ndisambiguates candidate labels by aligning the model output with the mixed\nclass posterior jointly predicted by both the learnable and the handcrafted\nprompt. Besides, our framework can be equipped with various off-the-shelf\ntraining objectives for learning with candidate labels to further improve their\nperformance. Extensive experiments demonstrate the effectiveness of our\nproposed framework.",
      "tldr_zh": "本文研究了如何使用候选标签对视觉语言模型(VLMs)进行提示学习(prompt learning)的微调问题，尽管这种方法在处理候选标签时比其他微调方式更具优势，但标签模糊性增加会导致性能下降。作者提出一个简单有效的框架，通过将模型输出与可学习提示和手工提示联合预测的混合类后验对齐，来消除候选标签的模糊性并利用VLMs的先验知识指导学习过程。该框架可与各种现成训练目标结合，实验结果显示其显著提高了鲁棒性和整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07638v3",
      "published_date": "2024-07-10 13:19:31 UTC",
      "updated_date": "2024-12-29 08:17:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:21:32.473957"
    },
    {
      "arxiv_id": "2407.07612v2",
      "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
      "title_zh": "通过公理训练教授 Transformer 因果推理",
      "authors": [
        "Aniket Vashishtha",
        "Abhinav Kumar",
        "Atharva Pandey",
        "Abbavaram Gowtham Reddy",
        "Kabir Ahuja",
        "Vineeth N Balasubramanian",
        "Amit Sharma"
      ],
      "abstract": "For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3.1 8B model on our\naxiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4.",
      "tldr_zh": "本研究提出了一种axiomatic training方法，通过符号演示来教导Transformer模型学习因果推理，而非依赖于归纳偏差或数据推断。方法涉及从多个因果公理（如transitivity axiom和d-separation rule）的演示中训练模型，并测试其从简单线性因果链推广到复杂图形的泛化能力。实验结果显示，使用67百万参数的Transformer从零训练后，该模型能成功处理更长链、反序链和分支图；此外，微调Llama-3.1 8B模型后，在Corr2Cause和CLEAR等因果基准测试中取得显著提升，甚至超越GPT-4的表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07612v2",
      "published_date": "2024-07-10 12:50:44 UTC",
      "updated_date": "2025-04-15 08:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:21:44.464176"
    },
    {
      "arxiv_id": "2407.07606v1",
      "title": "The Computational Learning of Construction Grammars: State of the Art and Prospective Roadmap",
      "title_zh": "翻译失败",
      "authors": [
        "Jonas Doumen",
        "Veronica Juliana Schmalz",
        "Katrien Beuls",
        "Paul Van Eecke"
      ],
      "abstract": "This paper documents and reviews the state of the art concerning\ncomputational models of construction grammar learning. It brings together prior\nwork on the computational learning of form-meaning pairings, which has so far\nbeen studied in several distinct areas of research. The goal of this paper is\nthreefold. First of all, it aims to synthesise the variety of methodologies\nthat have been proposed to date and the results that have been obtained.\nSecond, it aims to identify those parts of the challenge that have been\nsuccessfully tackled and reveal those that require further research. Finally,\nit aims to provide a roadmap which can help to boost and streamline future\nresearch efforts on the computational learning of large-scale, usage-based\nconstruction grammars.",
      "tldr_zh": "这篇论文回顾了计算模型在结构语法(construction grammars)学习领域的现状，并整合了不同研究中关于形式-意义配对(form-meaning pairings)的相关工作。其主要目标是综合现有方法和结果，识别已成功解决的挑战以及需要进一步研究的领域。该论文还提供了一个路线图，以推动大规模、基于使用的结构语法计算学习的未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Peer-reviewed author's draft of a journal article to appear in\n  Constructions and Frames (2025)",
      "pdf_url": "http://arxiv.org/pdf/2407.07606v1",
      "published_date": "2024-07-10 12:45:02 UTC",
      "updated_date": "2024-07-10 12:45:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:21:55.501142"
    },
    {
      "arxiv_id": "2407.07605v3",
      "title": "Early Explorations of Lightweight Models for Wound Segmentation on Mobile Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Vanessa Borst",
        "Timo Dittus",
        "Konstantin Müller",
        "Samuel Kounev"
      ],
      "abstract": "The aging population poses numerous challenges to healthcare, including the\nincrease in chronic wounds in the elderly. The current approach to wound\nassessment by therapists based on photographic documentation is subjective,\nhighlighting the need for computer-aided wound recognition from smartphone\nphotos. This offers objective and convenient therapy monitoring, while being\naccessible to patients from their home at any time. However, despite research\nin mobile image segmentation, there is a lack of focus on mobile wound\nsegmentation. To address this gap, we conduct initial research on three\nlightweight architectures to investigate their suitability for smartphone-based\nwound segmentation. Using public datasets and UNet as a baseline, our results\nare promising, with both ENet and TopFormer, as well as the larger UNeXt\nvariant, showing comparable performance to UNet. Furthermore, we deploy the\nmodels into a smartphone app for visual assessment of live segmentation, where\nresults demonstrate the effectiveness of TopFormer in distinguishing wounds\nfrom wound-coloured objects. While our study highlights the potential of\ntransformer models for mobile wound segmentation, future work should aim to\nfurther improve the mask contours.",
      "tldr_zh": "这篇论文探讨了轻量级模型在移动设备上进行伤口分割的可行性，以解决老龄化人口增加慢性伤口评估的主观性和便利性问题。作者以 UNet 为基线，使用公共数据集评估了 ENet、TopFormer 和 UNeXt 等架构，结果显示这些模型在性能上与 UNet 相当，其中 TopFormer 尤其擅长区分伤口和类似颜色的物体。模型已部署到智能手机 App 中，实现实时视觉评估，证明了其在家庭护理中的潜力。未来工作将重点改进掩码轮廓以进一步提升准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Extended version of our paper that was published in the \"47th German\n  Conference on Artificial Intelligence (KI 2024)\"",
      "pdf_url": "http://arxiv.org/pdf/2407.07605v3",
      "published_date": "2024-07-10 12:44:22 UTC",
      "updated_date": "2024-08-30 08:36:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:22:09.481822"
    },
    {
      "arxiv_id": "2407.07604v1",
      "title": "H-FCBFormer Hierarchical Fully Convolutional Branch Transformer for Occlusal Contact Segmentation with Articulating Paper",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Banks",
        "Bernat Rovira-Lastra",
        "Jordi Martinez-Gomis",
        "Akhilanand Chaurasia",
        "Yunpeng Li"
      ],
      "abstract": "Occlusal contacts are the locations at which the occluding surfaces of the\nmaxilla and the mandible posterior teeth meet. Occlusal contact detection is a\nvital tool for restoring the loss of masticatory function and is a mandatory\nassessment in the field of dentistry, with particular importance in\nprosthodontics and restorative dentistry. The most common method for occlusal\ncontact detection is articulating paper. However, this method can indicate\nsignificant medically false positive and medically false negative contact\nareas, leaving the identification of true occlusal indications to clinicians.\nTo address this, we propose a multiclass Vision Transformer and Fully\nConvolutional Network ensemble semantic segmentation model with a combination\nhierarchical loss function, which we name as Hierarchical Fully Convolutional\nBranch Transformer (H-FCBFormer). We also propose a method of generating\nmedically true positive semantic segmentation masks derived from expert\nannotated articulating paper masks and gold standard masks. The proposed model\noutperforms other machine learning methods evaluated at detecting medically\ntrue positive contacts and performs better than dentists in terms of accurately\nidentifying object-wise occlusal contact areas while taking significantly less\ntime to identify them. Code is available at\nhttps://github.com/Banksylel/H-FCBFormer.",
      "tldr_zh": "这篇论文针对咬合接触（Occlusal contacts）的检测问题，提出了一种新模型 H-FCBFormer，这是一个多类 Vision Transformer 和 Fully Convolutional Network 的集成语义分割模型，结合 hierarchical loss function 来提高准确性。模型还引入了一种从专家标注的 articulating paper 掩码和金标准掩码派生医学上真实阳性语义分割掩码的方法。实验结果表明，H-FCBFormer 在检测真实接触区域方面优于其他机器学习方法，甚至超过牙医的性能，同时显著减少了识别时间。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.1, I.2.10, J.3"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 5 figures, 2 tables, 5 equations, peer reviewed and\n  accepted to Medical Imaging Understanding and Analysis (MIUA 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.07604v1",
      "published_date": "2024-07-10 12:42:39 UTC",
      "updated_date": "2024-07-10 12:42:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:22:22.078240"
    },
    {
      "arxiv_id": "2407.07577v1",
      "title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yatai Ji",
        "Shilong Zhang",
        "Jie Wu",
        "Peize Sun",
        "Weifeng Chen",
        "Xuefeng Xiao",
        "Sidi Yang",
        "Yujiu Yang",
        "Ping Luo"
      ],
      "abstract": "The rapid advancement of Large Vision-Language models (LVLMs) has\ndemonstrated a spectrum of emergent capabilities. Nevertheless, current models\nonly focus on the visual content of a single scenario, while their ability to\nassociate instances across different scenes has not yet been explored, which is\nessential for understanding complex visual content, such as movies with\nmultiple characters and intricate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the potential of character\nidentities memory and recognition across multiple visual scenarios. To achieve\nthe goal, we propose visual instruction tuning with ID reference and develop an\nID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research\nintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and\nrecognition across four dimensions: matching, location, question-answering, and\ncaptioning. Our findings highlight the limitations of existing LVLMs in\nrecognizing and associating instance identities with ID reference. This paper\npaves the way for future artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating the comprehension of complex\nvisual narratives like movies.",
      "tldr_zh": "该研究指出，现有的 Large Vision-Language Models (LVLMs) 仅关注单一场景的视觉内容，缺乏跨场景实例关联能力，这对理解复杂视觉叙事如电影至关重要。为此，论文提出 ID-Aware Large Vision-Language Model (IDA-VLM)，通过 visual instruction tuning with ID reference 来增强字符身份的记忆和识别功能。研究还引入了一个新基准 MM-ID，用于评估 LVLMs 在实例 ID 记忆和识别方面的性能，包括匹配、定位、问答和描述四个维度。实验结果揭示了现有模型的局限性，并为未来 AI 系统处理多身份视觉输入和理解复杂叙事铺平道路。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07577v1",
      "published_date": "2024-07-10 12:11:59 UTC",
      "updated_date": "2024-07-10 12:11:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:22:33.462020"
    },
    {
      "arxiv_id": "2407.07561v1",
      "title": "FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes",
      "title_zh": "翻译失败",
      "authors": [
        "Rajat Kumar Jenamani",
        "Priya Sundaresan",
        "Maram Sakr",
        "Tapomayukh Bhattacharjee",
        "Dorsa Sadigh"
      ],
      "abstract": "Robot-assisted feeding has the potential to improve the quality of life for\nindividuals with mobility limitations who are unable to feed themselves\nindependently. However, there exists a large gap between the homogeneous,\ncurated plates existing feeding systems can handle, and truly in-the-wild\nmeals. Feeding realistic plates is immensely challenging due to the sheer range\nof food items that a robot may encounter, each requiring specialized\nmanipulation strategies which must be sequenced over a long horizon to feed an\nentire meal. An assistive feeding system should not only be able to sequence\ndifferent strategies efficiently in order to feed an entire meal, but also be\nmindful of user preferences given the personalized nature of the task. We\naddress this with FLAIR, a system for long-horizon feeding which leverages the\ncommonsense and few-shot reasoning capabilities of foundation models, along\nwith a library of parameterized skills, to plan and execute user-preferred and\nefficient bite sequences. In real-world evaluations across 6 realistic plates,\nwe find that FLAIR can effectively tap into a varied library of skills for\nefficient food pickup, while adhering to the diverse preferences of 42\nparticipants without mobility limitations as evaluated in a user study. We\ndemonstrate the seamless integration of FLAIR with existing bite transfer\nmethods [19, 28], and deploy it across 2 institutions and 3 robots,\nillustrating its adaptability. Finally, we illustrate the real-world efficacy\nof our system by successfully feeding a care recipient with severe mobility\nlimitations. Supplementary materials and videos can be found at:\nhttps://emprise.cs.cornell.edu/flair .",
      "tldr_zh": "这篇论文介绍了 FLAIR 系统，一种用于机器人辅助喂食的框架，旨在处理现实中多样化餐盘的挑战，通过基础模型的常识和少样本推理能力结合参数化技能库，来规划高效且符合用户偏好的长时序进食序列。FLAIR 不仅能序列化不同食物的操作策略，还考虑了个性化偏好，并在 6 个真实盘子上的评估中表现出色。实验结果显示，该系统在 42 名参与者的用户研究中有效提升了喂食效率，并成功与现有咬合转移方法整合，在 2 个机构和 3 个机器人上部署，最终实现了对一位行动不便者的实际喂食。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "RSS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07561v1",
      "published_date": "2024-07-10 11:38:57 UTC",
      "updated_date": "2024-07-10 11:38:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:22:45.743965"
    },
    {
      "arxiv_id": "2407.12861v2",
      "title": "CiteME: Can Language Models Accurately Cite Scientific Claims?",
      "title_zh": "CiteME：语言模型能准确引用科学声明吗？",
      "authors": [
        "Ori Press",
        "Andreas Hochlehnert",
        "Ameya Prabhu",
        "Vishaal Udandarao",
        "Ofir Press",
        "Matthias Bethge"
      ],
      "abstract": "Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect.",
      "tldr_zh": "本文探讨了语言模型(LMs)是否能准确引用科学声明的问题，针对研究者面对的信息过载和引用验证挑战，构建了CiteME基准测试，该测试包含机器学习论文中的文本片段，并评估LMs在引用归因上的表现。结果显示，前沿LMs的准确率仅为4.2-18.5%，远低于人类的69.7%。为了缩小这一差距，作者引入了CiteAgent系统，该系统基于GPT-4o并具备搜索和阅读论文的能力，实现了35.3%的准确率。总体而言，CiteME作为开放式声明归因的挑战性测试平台，推动了自动验证LMs声明的未来发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12861v2",
      "published_date": "2024-07-10 11:31:20 UTC",
      "updated_date": "2024-11-03 20:58:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:22:57.071922"
    },
    {
      "arxiv_id": "2407.07551v1",
      "title": "Arabic Automatic Story Generation with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Oumar El-Shangiti",
        "Fakhraddin Alwajih",
        "Muhammad Abdul-Mageed"
      ],
      "abstract": "Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.",
      "tldr_zh": "本文研究了使用 Large Language Models (LLMs) 生成阿拉伯语故事的任务，针对阿拉伯语领域的进展较慢问题，开发了一种基于机器翻译 (MT) 和 GPT-4 的数据生成方法。研究团队构建了高质量数据集，包括 Modern Standard Arabic (MSA) 以及埃及和摩洛哥方言的故事，通过精心设计的提示确保内容与阿拉伯语语境相关。手动和自动评估表明，微调后的模型能生成连贯且符合指令的故事，在与现有专有和开源模型的比较中表现出色；相关数据集和模型已计划公开分享。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07551v1",
      "published_date": "2024-07-10 11:26:10 UTC",
      "updated_date": "2024-07-10 11:26:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:23:08.822170"
    },
    {
      "arxiv_id": "2407.07544v1",
      "title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization",
      "title_zh": "无监督领域泛化的解耦遮罩自动编码器",
      "authors": [
        "An Zhang",
        "Han Wang",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "Domain Generalization (DG), designed to enhance out-of-distribution (OOD)\ngeneralization, is all about learning invariance against domain shifts\nutilizing sufficient supervision signals. Yet, the scarcity of such labeled\ndata has led to the rise of unsupervised domain generalization (UDG) - a more\nimportant yet challenging task in that models are trained across diverse\ndomains in an unsupervised manner and eventually tested on unseen domains. UDG\nis fast gaining attention but is still far from well-studied. To close the\nresearch gap, we propose a novel learning framework designed for UDG, termed\nthe Disentangled Masked Auto Encoder (DisMAE), aiming to discover the\ndisentangled representations that faithfully reveal the intrinsic features and\nsuperficial variations without access to the class label. At its core is the\ndistillation of domain-invariant semantic features, which cannot be\ndistinguished by domain classifier, while filtering out the domain-specific\nvariations (for example, color schemes and texture patterns) that are unstable\nand redundant. Notably, DisMAE co-trains the asymmetric dual-branch\narchitecture with semantic and lightweight variation encoders, offering dynamic\ndata manipulation and representation level augmentation capabilities. Extensive\nexperiments on four benchmark datasets (i.e., DomainNet, PACS, VLCS, Colored\nMNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve\ncompetitive OOD performance compared with the state-of-the-art DG and UDG\nbaselines, which shed light on potential research line in improving the\ngeneralization ability with large-scale unlabeled data.",
      "tldr_zh": "该论文针对无监督域泛化 (UDG) 的挑战，提出了一种名为 Disentangled Masked Auto Encoder (DisMAE) 的新框架，旨在在无标签数据下学习分离表示，以提取域不变的语义特征并过滤掉域特定的变化，如颜色方案和纹理模式。DisMAE 采用不对称双分支架构，包括语义编码器和轻量级变化编码器，提供动态数据操作和表示级增强功能，从而提升模型的泛化能力。在 DomainNet、PACS、VLCS 和 Colored MNIST 等四个基准数据集上的实验表明，DisMAE 在 out-of-distribution (OOD) 任务中实现了与最先进 Domain Generalization (DG) 和 UDG 基线相当的性能，展示了利用大规模无标签数据改进泛化潜力的新研究方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07544v1",
      "published_date": "2024-07-10 11:11:36 UTC",
      "updated_date": "2024-07-10 11:11:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:23:22.604268"
    },
    {
      "arxiv_id": "2407.11059v1",
      "title": "Was it Slander? Towards Exact Inversion of Generative Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Adrians Skapars",
        "Edoardo Manino",
        "Youcheng Sun",
        "Lucas C. Cordeiro"
      ],
      "abstract": "Training large language models (LLMs) requires a substantial investment of\ntime and money. To get a good return on investment, the developers spend\nconsiderable effort ensuring that the model never produces harmful and\noffensive outputs. However, bad-faith actors may still try to slander the\nreputation of an LLM by publicly reporting a forged output. In this paper, we\nshow that defending against such slander attacks requires reconstructing the\ninput of the forged output or proving that it does not exist. To do so, we\npropose and evaluate a search based approach for targeted adversarial attacks\nfor LLMs. Our experiments show that we are rarely able to reconstruct the exact\ninput of an arbitrary output, thus demonstrating that LLMs are still vulnerable\nto slander attacks.",
      "tldr_zh": "这篇论文探讨了如何防御针对生成语言模型(LLMs)的诽谤攻击，即恶意者通过伪造输出来损害模型声誉。作者提出了一种基于搜索的针对性对抗攻击(adversarial attacks)方法，旨在重建伪造输出的确切输入或证明其不存在。实验结果显示，该方法很少能成功重建输入，这表明LLMs在防范此类攻击方面仍存在脆弱性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "4 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.11059v1",
      "published_date": "2024-07-10 11:08:06 UTC",
      "updated_date": "2024-07-10 11:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:23:32.753760"
    },
    {
      "arxiv_id": "2407.07541v1",
      "title": "Swiss DINO: Efficient and Versatile Vision Framework for On-device Personal Object Search",
      "title_zh": "Swiss DINO：高效且通用的视觉框架，用于设备端个人物体搜索",
      "authors": [
        "Kirill Paramonov",
        "Jia-Xing Zhong",
        "Umberto Michieli",
        "Jijoong Moon",
        "Mete Ozay"
      ],
      "abstract": "In this paper, we address a recent trend in robotic home appliances to\ninclude vision systems on personal devices, capable of personalizing the\nappliances on the fly. In particular, we formulate and address an important\ntechnical task of personal object search, which involves localization and\nidentification of personal items of interest on images captured by robotic\nappliances, with each item referenced only by a few annotated images. The task\nis crucial for robotic home appliances and mobile systems, which need to\nprocess personal visual scenes or to operate with particular personal objects\n(e.g., for grasping or navigation). In practice, personal object search\npresents two main technical challenges. First, a robot vision system needs to\nbe able to distinguish between many fine-grained classes, in the presence of\nocclusions and clutter. Second, the strict resource requirements for the\non-device system restrict the usage of most state-of-the-art methods for\nfew-shot learning and often prevent on-device adaptation. In this work, we\npropose Swiss DINO: a simple yet effective framework for one-shot personal\nobject search based on the recent DINOv2 transformer model, which was shown to\nhave strong zero-shot generalization properties. Swiss DINO handles challenging\non-device personalized scene understanding requirements and does not require\nany adaptation training. We show significant improvement (up to 55%) in\nsegmentation and recognition accuracy compared to the common lightweight\nsolutions, and significant footprint reduction of backbone inference time (up\nto 100x) and GPU consumption (up to 10x) compared to the heavy\ntransformer-based solutions.",
      "tldr_zh": "本研究提出Swiss DINO框架，一种高效且通用的视觉系统，针对机器人家电的on-device个人物体搜索任务。该框架基于DINOv2 transformer模型，实现one-shot学习，通过零-shot泛化能力处理遮挡、杂乱环境下的细粒度物体定位和识别，而无需任何适应训练。相比轻量级解决方案，Swiss DINO在分割和识别准确率上提升高达55%；相较重型transformer方法，它显著降低了推理时间（高达100倍）和GPU消耗（高达10倍），适用于资源受限的移动设备。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 2 figures, accepted to IROS2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07541v1",
      "published_date": "2024-07-10 11:05:02 UTC",
      "updated_date": "2024-07-10 11:05:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:23:43.821882"
    },
    {
      "arxiv_id": "2407.18934v1",
      "title": "The Design of a 3D Character Animation System for Digital Twins in the Metaverse",
      "title_zh": "Metaverse 中 Digital Twins 的 ",
      "authors": [
        "Senem Tanberk",
        "Dilek Bilgin Tukel",
        "Kadir Acar"
      ],
      "abstract": "In the context of Industry 4.0, digital twin technology has emerged with\nrapid advancements as a powerful tool for visualizing and analyzing industrial\nassets. This technology has attracted considerable interest from researchers\nacross diverse domains such as manufacturing, security, transportation, and\ngaming. The metaverse has emerged as a significant enabler in these domains,\nfacilitating the integration of various technologies to create virtual replicas\nof physical assets. The utilization of 3D character animation, often referred\nto as avatars, is crucial for implementing the metaverse. Traditionally, costly\nmotion capture technologies are employed for creating a realistic avatar\nsystem. To meet the needs of this evolving landscape, we have developed a\nmodular framework tailored for asset digital twins as a more affordable\nalternative. This framework offers flexibility for the independent\ncustomization of individual system components. To validate our approach, we\nemploy the English peg solitaire game as a use case, generating a solution tree\nusing the breadth-first search algorithm. The results encompass both\nqualitative and quantitative findings of a data-driven 3D animation system\nutilizing motion primitives. The presented methodologies and infrastructure are\nadaptable and modular, making them applicable to asset digital twins across\ndiverse business contexts. This case study lays the groundwork for pilot\napplications and can be tailored for education, health, or Industry 4.0\nmaterial development.",
      "tldr_zh": "本研究设计了一个模块化框架，用于在元宇宙中实现数字孪生资产的3D角色动画系统，作为传统昂贵动作捕捉技术的更经济替代方案。该框架允许独立定制系统组件，并以英语九柱游戏为例，利用广度优先搜索算法生成解决方案树和基于运动原语的数据驱动3D动画，从而验证其有效性。实验结果显示，该系统在定性和定量方面表现出色，并具有灵活的适应性，可应用于教育、健康或工业4.0等领域的试点应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18934v1",
      "published_date": "2024-07-10 10:54:06 UTC",
      "updated_date": "2024-07-10 10:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:23:56.371405"
    },
    {
      "arxiv_id": "2407.07530v1",
      "title": "How Aligned are Different Alignment Metrics?",
      "title_zh": "翻译失败",
      "authors": [
        "Jannis Ahlert",
        "Thomas Klein",
        "Felix Wichmann",
        "Robert Geirhos"
      ],
      "abstract": "In recent years, various methods and benchmarks have been proposed to\nempirically evaluate the alignment of artificial neural networks to human\nneural and behavioral data. But how aligned are different alignment metrics? To\nanswer this question, we analyze visual data from Brain-Score (Schrimpf et al.,\n2018), including metrics from the model-vs-human toolbox (Geirhos et al.,\n2021), together with human feature alignment (Linsley et al., 2018; Fel et al.,\n2022) and human similarity judgements (Muttenthaler et al., 2022). We find that\npairwise correlations between neural scores and behavioral scores are quite low\nand sometimes even negative. For instance, the average correlation between\nthose 80 models on Brain-Score that were fully evaluated on all 69 alignment\nmetrics we considered is only 0.198. Assuming that all of the employed metrics\nare sound, this implies that alignment with human perception may best be\nthought of as a multidimensional concept, with different methods measuring\nfundamentally different aspects. Our results underline the importance of\nintegrative benchmarking, but also raise questions about how to correctly\ncombine and aggregate individual metrics. Aggregating by taking the arithmetic\naverage, as done in Brain-Score, leads to the overall performance currently\nbeing dominated by behavior (95.25% explained variance) while the neural\npredictivity plays a less important role (only 33.33% explained variance). As a\nfirst step towards making sure that different alignment metrics all contribute\nfairly towards an integrative benchmark score, we therefore conclude by\ncomparing three different aggregation options.",
      "tldr_zh": "这篇论文探讨了不同对齐指标（alignment metrics）之间的相关性，旨在评估神经网络与人类神经和行为数据的对齐程度。作者通过分析 Brain-Score 数据集，包括 model-vs-human toolbox、人类特征对齐和人类相似性判断，发现这些指标间的配对相关性较低，平均仅为 0.198，且有时甚至为负，这表明对齐是一个多维度的概念，不同方法可能测量了根本不同的方面。研究强调了综合基准测试的重要性，并指出现有算术平均聚合方法（如 Brain-Score）导致行为指标主导整体性能（95.25% 解释方差），因此作者比较了三种聚合选项，以促进指标的公平整合。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "submitted to the ICLR 2024 Workshop on Representational Alignment\n  (Re-Align)",
      "pdf_url": "http://arxiv.org/pdf/2407.07530v1",
      "published_date": "2024-07-10 10:36:11 UTC",
      "updated_date": "2024-07-10 10:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:24:09.749688"
    },
    {
      "arxiv_id": "2407.07521v1",
      "title": "CHILLI: A data context-aware perturbation method for XAI",
      "title_zh": "CHILLI：一种数据上下文感知的扰动方法用于 XAI",
      "authors": [
        "Saif Anwar",
        "Nathan Griffiths",
        "Abhir Bhalerao",
        "Thomas Popham"
      ],
      "abstract": "The trustworthiness of Machine Learning (ML) models can be difficult to\nassess, but is critical in high-risk or ethically sensitive applications. Many\nmodels are treated as a `black-box' where the reasoning or criteria for a final\ndecision is opaque to the user. To address this, some existing Explainable AI\n(XAI) approaches approximate model behaviour using perturbed data. However,\nsuch methods have been criticised for ignoring feature dependencies, with\nexplanations being based on potentially unrealistic data. We propose a novel\nframework, CHILLI, for incorporating data context into XAI by generating\ncontextually aware perturbations, which are faithful to the training data of\nthe base model being explained. This is shown to improve both the soundness and\naccuracy of the explanations.",
      "tldr_zh": "本文研究了机器学习 (ML) 模型的可信度问题，特别是黑盒模型的决策不透明。现有 Explainable AI (XAI) 方法通过扰动数据来近似模型行为，但常忽略特征依赖性，导致解释基于不现实的数据。为此，作者提出了一种新框架 CHILLI，它通过生成与训练数据一致的上下文感知扰动，来提升 XAI 解释的可靠性和准确性。实验结果表明，该方法显著改善了解释的 soundness 和 accuracy。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07521v1",
      "published_date": "2024-07-10 10:18:07 UTC",
      "updated_date": "2024-07-10 10:18:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:24:21.028889"
    },
    {
      "arxiv_id": "2407.07506v2",
      "title": "Generative AI for RF Sensing in IoT systems",
      "title_zh": "生成式 AI 用于物联网系统中的射频感知",
      "authors": [
        "Li Wang",
        "Chao Zhang",
        "Qiyang Zhao",
        "Hang Zou",
        "Samson Lasaulce",
        "Giuseppe Valenzise",
        "Zhuo He",
        "Merouane Debbah"
      ],
      "abstract": "The development of wireless sensing technologies, using signals such as\nWi-Fi, infrared, and RF to gather environmental data, has significantly\nadvanced within Internet of Things (IoT) systems. Among these, Radio Frequency\n(RF) sensing stands out for its cost-effective and non-intrusive monitoring of\nhuman activities and environmental changes. However, traditional RF sensing\nmethods face significant challenges, including noise, interference, incomplete\ndata, and high deployment costs, which limit their effectiveness and\nscalability. This paper investigates the potential of Generative AI (GenAI) to\novercome these limitations within the IoT ecosystem. We provide a comprehensive\nreview of state-of-the-art GenAI techniques, focusing on their application to\nRF sensing problems. By generating high-quality synthetic data, enhancing\nsignal quality, and integrating multi-modal data, GenAI offers robust solutions\nfor RF environment reconstruction, localization, and imaging. Additionally,\nGenAI's ability to generalize enables IoT devices to adapt to new environments\nand unseen tasks, improving their efficiency and performance. The main\ncontributions of this article include a detailed analysis of the challenges in\nRF sensing, the presentation of innovative GenAI-based solutions, and the\nproposal of a unified framework for diverse RF sensing tasks. Through case\nstudies, we demonstrate the effectiveness of integrating GenAI models, leading\nto advanced, scalable, and intelligent IoT systems.",
      "tldr_zh": "本论文探讨了生成式人工智能（Generative AI, GenAI）在物联网（IoT）系统中的射频（RF）感知应用，以解决传统 RF 感知面临的噪声、干扰、不完整数据和高部署成本等挑战。通过回顾现有 GenAI 技术，该研究展示了其在生成高质量合成数据、增强信号质量以及整合多模态数据方面的潜力，从而实现 RF 环境重建、定位和成像。论文的主要贡献包括详细分析 RF 感知问题、提出创新的 GenAI 解决方案以及一个统一的框架，以提升 IoT 设备的适应性和性能。通过案例研究，证明了 GenAI 的整合可显著提高系统的可扩展性和智能性。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07506v2",
      "published_date": "2024-07-10 09:51:44 UTC",
      "updated_date": "2024-11-24 08:38:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:24:33.357607"
    },
    {
      "arxiv_id": "2407.07488v1",
      "title": "FUNAvg: Federated Uncertainty Weighted Averaging for Datasets with Diverse Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Malte Tölle",
        "Fernando Navarro",
        "Sebastian Eble",
        "Ivo Wolf",
        "Bjoern Menze",
        "Sandy Engelhardt"
      ],
      "abstract": "Federated learning is one popular paradigm to train a joint model in a\ndistributed, privacy-preserving environment. But partial annotations pose an\nobstacle meaning that categories of labels are heterogeneous over clients. We\npropose to learn a joint backbone in a federated manner, while each site\nreceives its own multi-label segmentation head. By using Bayesian techniques we\nobserve that the different segmentation heads although only trained on the\nindividual client's labels also learn information about the other labels not\npresent at the respective site. This information is encoded in their predictive\nuncertainty. To obtain a final prediction we leverage this uncertainty and\nperform a weighted averaging of the ensemble of distributed segmentation heads,\nwhich allows us to segment \"locally unknown\" structures. With our method, which\nwe refer to as FUNAvg, we are even on-par with the models trained and tested on\nthe same dataset on average. The code is publicly available at\nhttps://github.com/Cardio-AI/FUNAvg.",
      "tldr_zh": "这篇论文针对联邦学习（Federated learning）中标签异质（heterogeneous）的问题，提出了一种名为 FUNAvg 的方法，用于处理部分标注数据集。方法通过联邦方式训练一个联合主干（joint backbone），并为每个客户端设计专属的多标签分割头（multi-label segmentation head）。利用 Bayesian 技术，论文发现这些分割头能从预测不确定性（predictive uncertainty）中提取本地未知标签的信息，并通过不确定性加权平均（weighted averaging）实现对“本地未知”结构的分割。实验结果表明，FUNAvg 的性能与在相同数据集上训练和测试的模型相当，代码已在 https://github.com/Cardio-AI/FUNAvg 公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MICCAI24",
      "pdf_url": "http://arxiv.org/pdf/2407.07488v1",
      "published_date": "2024-07-10 09:23:55 UTC",
      "updated_date": "2024-07-10 09:23:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:24:46.198249"
    },
    {
      "arxiv_id": "2407.07482v1",
      "title": "Rigorous Probabilistic Guarantees for Robust Counterfactual Explanations",
      "title_zh": "针对鲁棒反事实解释的严格概率保证",
      "authors": [
        "Luca Marzari",
        "Francesco Leofante",
        "Ferdinando Cicalese",
        "Alessandro Farinelli"
      ],
      "abstract": "We study the problem of assessing the robustness of counterfactual\nexplanations for deep learning models. We focus on $\\textit{plausible model\nshifts}$ altering model parameters and propose a novel framework to reason\nabout the robustness property in this setting. To motivate our solution, we\nbegin by showing for the first time that computing the robustness of\ncounterfactuals with respect to plausible model shifts is NP-complete. As this\n(practically) rules out the existence of scalable algorithms for exactly\ncomputing robustness, we propose a novel probabilistic approach which is able\nto provide tight estimates of robustness with strong guarantees while\npreserving scalability. Remarkably, and differently from existing solutions\ntargeting plausible model shifts, our approach does not impose requirements on\nthe network to be analyzed, thus enabling robustness analysis on a wider range\nof architectures. Experiments on four binary classification datasets indicate\nthat our method improves the state of the art in generating robust\nexplanations, outperforming existing methods on a range of metrics.",
      "tldr_zh": "本研究探讨了深度学习模型的逆事实解释（counterfactual explanations）的鲁棒性评估，特别针对合理的模型变化（plausible model shifts）问题，并首次证明了计算这种鲁棒性是 NP-complete 的，从而排除了精确计算的可扩展性。作者提出了一种新型概率方法，能够提供鲁棒性的紧密估计，具有强保证，同时不依赖网络架构的特定要求，从而适用于更广泛的模型。实验在四个二元分类数据集上表明，该方法在多种指标上优于现有技术，显著提升了生成鲁棒解释的状态。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 27th European Conference on Artificial Intelligence\n  (ECAI 2024). Marzari and Leofante contributed equally to the paper",
      "pdf_url": "http://arxiv.org/pdf/2407.07482v1",
      "published_date": "2024-07-10 09:13:11 UTC",
      "updated_date": "2024-07-10 09:13:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:24:57.758875"
    },
    {
      "arxiv_id": "2407.18932v2",
      "title": "Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles",
      "title_zh": "更真实：使用 LLM 代理和个人档案的旅行日记生成",
      "authors": [
        "Xuchuan Li",
        "Fei Huang",
        "Jianrong Lv",
        "Zhixiong Xiao",
        "Guolong Li",
        "Yang Yue"
      ],
      "abstract": "Human mobility is inextricably linked to social issues such as traffic\ncongestion, energy consumption, and public health; however, privacy concerns\nrestrict access to mobility data. Recently, research have utilized Large\nLanguage Models (LLMs) for human mobility generation, in which the challenge is\nhow LLMs can understand individuals' mobility behavioral differences to\ngenerate realistic trajectories conforming to real world contexts. This study\nhandles this problem by presenting an LLM agent-based framework (MobAgent)\ncomposing two phases: understanding-based mobility pattern extraction and\nreasoning-based trajectory generation, which enables generate more real travel\ndiaries at urban scale, considering different individual profiles. MobAgent\nextracts reasons behind specific mobility trendiness and attribute influences\nto provide reliable patterns; infers the relationships between contextual\nfactors and underlying motivations of mobility; and based on the patterns and\nthe recursive reasoning process, MobAgent finally generates more authentic and\npersonalized mobilities that reflect both individual differences and real-world\nconstraints. We validate our framework with 0.2 million travel survey data,\ndemonstrating its effectiveness in producing personalized and accurate travel\ndiaries. This study highlights the capacity of LLMs to provide detailed and\nsophisticated understanding of human mobility through the real-world mobility\ndata.",
      "tldr_zh": "这篇论文提出了一种基于LLM代理的框架（MobAgent），用于生成更真实的旅行日记，旨在解决隐私限制下人类流动性数据生成的问题，同时考虑个体配置文件和行为差异。框架分为两个阶段：首先，通过理解-based方法提取流动性模式背后的原因和属性影响；其次，利用推理-based过程推断上下文因素与动机关系，并生成个性化的轨迹。实验使用0.2百万旅行调查数据验证了框架的有效性，证明它能产生准确的旅行日记，并突显LLMs在理解人类流动性方面的潜力。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18932v2",
      "published_date": "2024-07-10 09:11:57 UTC",
      "updated_date": "2024-08-05 15:59:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:25:09.729157"
    },
    {
      "arxiv_id": "2407.07472v1",
      "title": "Rectifier: Code Translation with Corrector via LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Yin",
        "Chao Ni",
        "Tien N. Nguyen",
        "Shaohua Wang",
        "Xiaohu Yang"
      ],
      "abstract": "Software migration is garnering increasing attention with the evolution of\nsoftware and society. Early studies mainly relied on handcrafted translation\nrules to translate between two languages, the translation process is\nerror-prone and time-consuming. In recent years, researchers have begun to\nexplore the use of pre-trained large language models (LLMs) in code\ntranslation. However, code translation is a complex task that LLMs would\ngenerate mistakes during code translation, they all produce certain types of\nerrors when performing code translation tasks, which include (1) compilation\nerror, (2) runtime error, (3) functional error, and (4) non-terminating\nexecution. We found that the root causes of these errors are very similar (e.g.\nfailure to import packages, errors in loop boundaries, operator errors, and\nmore). In this paper, we propose a general corrector, namely Rectifier, which\nis a micro and universal model for repairing translation errors. It learns from\nerrors generated by existing LLMs and can be widely applied to correct errors\ngenerated by any LLM. The experimental results on translation tasks between\nC++, Java, and Python show that our model has effective repair ability, and\ncross experiments also demonstrate the robustness of our method.",
      "tldr_zh": "该论文针对大型语言模型(LLMs)在代码翻译任务中常产生的错误（如编译错误、运行时错误、功能错误和非终止执行），提出了一种通用校正器Rectifier。该方法是一个微型模型，通过学习现有LLMs的错误来自动修复翻译问题，并可应用于任何LLMs。实验结果显示，Rectifier在C++、Java和Python之间的代码翻译任务上表现出有效的修复能力和鲁棒性，显著提升了翻译的准确性和可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "arXiv admin note: text overlap with arXiv:2308.03109,\n  arXiv:2302.03908 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2407.07472v1",
      "published_date": "2024-07-10 08:58:41 UTC",
      "updated_date": "2024-07-10 08:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:25:21.679548"
    },
    {
      "arxiv_id": "2407.12860v1",
      "title": "STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs",
      "title_zh": "STAGE：使用预训练 LLMs 的简化文本属性图嵌入",
      "authors": [
        "Aaron Zolnai-Lucas",
        "Jack Boylan",
        "Chris Hokamp",
        "Parsa Ghaffari"
      ],
      "abstract": "We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.",
      "tldr_zh": "我们提出 STAGE，一种简化的文本属性图 (TAGs) 嵌入方法，使用预训练 Large-Language Models (LLMs) 生成节点特征，以增强 Graph Neural Network (GNN) 模型的表现。该方法在各种节点分类基准上实现了与当前最先进 (SoTA) 技术相当的竞争性结果，同时保持了简单的实现流程，避免了多阶段的昂贵训练和提示。STAGE 通过提供稳健的嵌入生成，支持集成 GNN 训练，并通过扩散模式 GNNs 扩展了其适用性，使其适用于超出学术基准的图结构。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12860v1",
      "published_date": "2024-07-10 08:50:25 UTC",
      "updated_date": "2024-07-10 08:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:25:34.718001"
    },
    {
      "arxiv_id": "2407.11057v1",
      "title": "SPIN: SE(3)-Invariant Physics Informed Network for Binding Affinity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Seungyeon Choi",
        "Sangmin Seo",
        "Sanghyun Park"
      ],
      "abstract": "Accurate prediction of protein-ligand binding affinity is crucial for rapid\nand efficient drug development. Recently, the importance of predicting binding\naffinity has led to increased attention on research that models the\nthree-dimensional structure of protein-ligand complexes using graph neural\nnetworks to predict binding affinity. However, traditional methods often fail\nto accurately model the complex's spatial information or rely solely on\ngeometric features, neglecting the principles of protein-ligand binding. This\ncan lead to overfitting, resulting in models that perform poorly on independent\ndatasets and ultimately reducing their usefulness in real drug development. To\naddress this issue, we propose SPIN, a model designed to achieve superior\ngeneralization by incorporating various inductive biases applicable to this\ntask, beyond merely training on empirical data from datasets. For prediction,\nwe defined two types of inductive biases: a geometric perspective that\nmaintains consistent binding affinity predictions regardless of the complexs\nrotations and translations, and a physicochemical perspective that necessitates\nminimal binding free energy along their reaction coordinate for effective\nprotein-ligand binding. These prior knowledge inputs enable the SPIN to\noutperform comparative models in benchmark sets such as CASF-2016 and CSAR HiQ.\nFurthermore, we demonstrated the practicality of our model through virtual\nscreening experiments and validated the reliability and potential of our\nproposed model based on experiments assessing its interpretability.",
      "tldr_zh": "该研究提出 SPIN 模型，一种 SE(3)-Invariant Physics Informed Network，用于准确预测蛋白质-配体结合亲和力（binding affinity），以解决现有基于图神经网络（graph neural networks）的三维结构建模方法在忽略空间信息和结合原理方面的问题，从而减少过拟合并提升泛化性。SPIN 通过引入两种归纳偏差（inductive biases）：几何视角的 SE(3)-Invariant 确保预测对复杂旋转和平移不变，以及物理化学视角的结合自由能最小化要求，来整合先验知识进行预测。在 CASF-2016 和 CSAR HiQ 等基准数据集上，SPIN 表现优于比较模型，并通过虚拟筛选实验和可解释性评估验证了其在药物开发中的实用性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.11057v1",
      "published_date": "2024-07-10 08:40:07 UTC",
      "updated_date": "2024-07-10 08:40:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:25:48.148853"
    },
    {
      "arxiv_id": "2407.07462v2",
      "title": "MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions",
      "title_zh": "翻译失败",
      "authors": [
        "Felix Fent",
        "Fabian Kuttenreich",
        "Florian Ruch",
        "Farija Rizwin",
        "Stefan Juergens",
        "Lorenz Lechermann",
        "Christian Nissler",
        "Andrea Perl",
        "Ulrich Voll",
        "Min Yan",
        "Markus Lienkamp"
      ],
      "abstract": "Autonomous trucking is a promising technology that can greatly impact modern\nlogistics and the environment. Ensuring its safety on public roads is one of\nthe main duties that requires an accurate perception of the environment. To\nachieve this, machine learning methods rely on large datasets, but to this day,\nno such datasets are available for autonomous trucks. In this work, we present\nMAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN\nTruckScenes allows the research community to come into contact with\ntruck-specific challenges, such as trailer occlusions, novel sensor\nperspectives, and terminal environments for the first time. It comprises more\nthan 740 scenes of 20s each within a multitude of different environmental\nconditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2\nIMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually\nannotated and carefully reviewed to achieve a high quality standard. Bounding\nboxes are available for 27 object classes, 15 attributes, and a range of more\nthan 230m. The scenes are tagged according to 34 distinct scene tags, and all\nobjects are tracked throughout the scene to promote a wide range of\napplications. Additionally, MAN TruckScenes is the first dataset to provide 4D\nradar data with 360{\\deg} coverage and is thereby the largest radar dataset\nwith annotated 3D bounding boxes. Finally, we provide extensive dataset\nanalysis and baseline results. The dataset, development kit, and more are\navailable online.",
      "tldr_zh": "本文介绍了MAN TruckScenes，这是第一个针对autonomous trucking的多模态数据集，旨在提升卡车在多样环境条件下的安全感知。数据集包含超过740个每20秒的场景，配备4 cameras、6 lidars、6 radars、2 IMUs和一个高精度GNSS，并手动标注了27个object classes、15个attributes的3D bounding boxes，覆盖范围超过230m。MAN TruckScenes首次提供360°覆盖的4D radar data，并通过34个scene tags和物体追踪支持广泛应用。最后，研究者提供了数据集分析、baseline results，并在线提供数据集和开发工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track",
      "pdf_url": "http://arxiv.org/pdf/2407.07462v2",
      "published_date": "2024-07-10 08:32:26 UTC",
      "updated_date": "2024-11-11 14:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:25:59.886929"
    },
    {
      "arxiv_id": "2407.07452v2",
      "title": "Missile detection and destruction robot using detection algorithm",
      "title_zh": "基于检测算法的导弹检测与摧毁机器人",
      "authors": [
        "Md Kamrul Siam",
        "Shafayet Ahmed",
        "Md Habibur Rahman",
        "Amir Hossain Mollah"
      ],
      "abstract": "This research is based on the present missile detection technologies in the\nworld and the analysis of these technologies to find a cost effective solution\nto implement the system in Bangladesh. The paper will give an idea of the\nmissile detection technologies using the electro-optical sensor and the pulse\ndoppler radar. The system is made to detect the target missile. Automatic\ndetection and destruction with the help of ultrasonic sonar, a metal detector\nsensor, and a smoke detector sensor. The system is mainly based on an\nultrasonic sonar sensor. It has a transducer, a transmitter, and a receiver.\nTransducer is connected with the connected with controller. When it detects an\nobject by following the algorithm, it finds its distance and angle. It can also\nassure whether the system can destroy the object or not by using another\nalgorithm's simulation.",
      "tldr_zh": "这篇论文分析了现有的导弹检测技术，提出一个成本有效的系统，旨在为孟加拉国提供导弹检测和摧毁解决方案。系统利用electro-optical sensor和pulse doppler radar进行目标识别，并结合ultrasonic sonar sensor、金属检测器和烟雾检测器实现自动检测。核心机制基于ultrasonic sonar sensor的换能器、发射器和接收器，通过特定算法计算物体的距离、角度，并模拟是否能成功摧毁目标，从而提升系统的可靠性和实用性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "67 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.07452v2",
      "published_date": "2024-07-10 08:12:21 UTC",
      "updated_date": "2024-07-11 04:18:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:26:12.721623"
    },
    {
      "arxiv_id": "2407.12859v1",
      "title": "Automated Question Generation on Tabular Data for Conversational Data Exploration",
      "title_zh": "针对表格数据的自动问题生成，用于对话式数据探索",
      "authors": [
        "Ritwik Chaudhuri",
        "Rajmohan C",
        "Kirushikesh DB",
        "Arvind Agarwal"
      ],
      "abstract": "Exploratory data analysis (EDA) is an essential step for analyzing a dataset\nto derive insights. Several EDA techniques have been explored in the\nliterature. Many of them leverage visualizations through various plots. But it\nis not easy to interpret them for a non-technical user, and producing\nappropriate visualizations is also tough when there are a large number of\ncolumns. Few other works provide a view of some interesting slices of data but\nit is still difficult for the user to draw relevant insights from them. Of\nlate, conversational data exploration is gaining a lot of traction among\nnon-technical users. It helps the user to explore the dataset without having\ndeep technical knowledge about the data. Towards this, we propose a system that\nrecommends interesting questions in natural language based on relevant slices\nof a dataset in a conversational setting. Specifically, given a dataset, we\npick a select set of interesting columns and identify interesting slices of\nsuch columns and column combinations based on few interestingness measures. We\nuse our own fine-tuned variation of a pre-trained language model(T5) to\ngenerate natural language questions in a specific manner. We then slot-fill\nvalues in the generated questions and rank them for recommendations. We show\nthe utility of our proposed system in a coversational setting with a collection\nof real datasets.",
      "tldr_zh": "这篇论文针对探索性数据分析（EDA）中的挑战，提出了一种自动化问题生成系统，用于对话式数据探索。该系统通过识别表格数据中的有趣列和切片（如基于interestingness measures），并利用微调的T5预训练语言模型生成自然语言问题，然后填充值并进行排名推荐，从而帮助非技术用户轻松获取数据洞见。实验结果显示，该系统在真实数据集上的对话设置中表现出色，提升了用户对数据的交互和理解效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12859v1",
      "published_date": "2024-07-10 08:07:05 UTC",
      "updated_date": "2024-07-10 08:07:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:26:22.387917"
    },
    {
      "arxiv_id": "2407.11056v1",
      "title": "Industrial-Grade Time-Dependent Counterfactual Root Cause Analysis through the Unanticipated Point of Incipient Failure: a Proof of Concept",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandre Trilla",
        "Rajesh Rajendran",
        "Ossee Yiboe",
        "Quentin Possamaï",
        "Nenad Mijatovic",
        "Jordi Vitrià"
      ],
      "abstract": "This paper describes the development of a counterfactual Root Cause Analysis\ndiagnosis approach for an industrial multivariate time series environment. It\ndrives the attention toward the Point of Incipient Failure, which is the moment\nin time when the anomalous behavior is first observed, and where the root cause\nis assumed to be found before the issue propagates. The paper presents the\nelementary but essential concepts of the solution and illustrates them\nexperimentally on a simulated setting. Finally, it discusses avenues of\nimprovement for the maturity of the causal technology to meet the robustness\nchallenges of increasingly complex environments in the industry.",
      "tldr_zh": "这篇论文提出了一种工业级反事实Root Cause Analysis诊断方法，针对多变量时间序列环境，通过关注Point of Incipient Failure（异常行为的初始时刻）来在问题扩散前识别根因。论文介绍了该解决方案的基本概念，并在模拟环境中进行实验验证，展示了其潜在有效性。最后，它讨论了改进因果技术的途径，以提升其成熟度并适应工业中日益复杂的鲁棒性挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for the Causal Inference for Time Series Data Workshop at\n  the 40th Conference on Uncertainty in Artificial Intelligence (CI4TS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.11056v1",
      "published_date": "2024-07-10 08:02:03 UTC",
      "updated_date": "2024-07-10 08:02:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:26:34.169674"
    },
    {
      "arxiv_id": "2407.07443v1",
      "title": "Secondary Structure-Guided Novel Protein Sequence Generation with Latent Graph Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Yutong Hu",
        "Yang Tan",
        "Andi Han",
        "Lirong Zheng",
        "Liang Hong",
        "Bingxin Zhou"
      ],
      "abstract": "The advent of deep learning has introduced efficient approaches for de novo\nprotein sequence design, significantly improving success rates and reducing\ndevelopment costs compared to computational or experimental methods. However,\nexisting methods face challenges in generating proteins with diverse lengths\nand shapes while maintaining key structural features. To address these\nchallenges, we introduce CPDiffusion-SS, a latent graph diffusion model that\ngenerates protein sequences based on coarse-grained secondary structural\ninformation. CPDiffusion-SS offers greater flexibility in producing a variety\nof novel amino acid sequences while preserving overall structural constraints,\nthus enhancing the reliability and diversity of generated proteins.\nExperimental analyses demonstrate the significant superiority of the proposed\nmethod in producing diverse and novel sequences, with CPDiffusion-SS surpassing\npopular baseline methods on open benchmarks across various quantitative\nmeasurements. Furthermore, we provide a series of case studies to highlight the\nbiological significance of the generation performance by the proposed method.\nThe source code is publicly available at\nhttps://github.com/riacd/CPDiffusion-SS",
      "tldr_zh": "本研究提出CPDiffusion-SS，一种基于Latent Graph Diffusion的模型，用于指导蛋白质序列的生成，通过粗粒度的Secondary Structure信息来解决现有方法在处理多样长度和形状蛋白质时的结构特征保持问题。该模型增强了生成新氨基酸序列的灵活性，同时确保整体结构约束，提高了蛋白质的可靠性和多样性。实验结果显示，CPDiffusion-SS在各种量化指标上超过了流行基线方法，并在案例研究中证明了其生物学意义。源代码已公开可用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.07443v1",
      "published_date": "2024-07-10 07:54:26 UTC",
      "updated_date": "2024-07-10 07:54:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:26:46.783146"
    },
    {
      "arxiv_id": "2407.07433v2",
      "title": "Controllable Navigation Instruction Generation with Chain of Thought Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Xianghao Kong",
        "Jinyu Chen",
        "Wenguan Wang",
        "Hang Su",
        "Xiaolin Hu",
        "Yi Yang",
        "Si Liu"
      ],
      "abstract": "Instruction generation is a vital and multidisciplinary research area with\nbroad applications. Existing instruction generation models are limited to\ngenerating instructions in a single style from a particular dataset, and the\nstyle and content of generated instructions cannot be controlled. Moreover,\nmost existing instruction generation methods also disregard the spatial\nmodeling of the navigation environment. Leveraging the capabilities of Large\nLanguage Models (LLMs), we propose C-Instructor, which utilizes the\nchain-of-thought-style prompt for style-controllable and content-controllable\ninstruction generation. Firstly, we propose a Chain of Thought with Landmarks\n(CoTL) mechanism, which guides the LLM to identify key landmarks and then\ngenerate complete instructions. CoTL renders generated instructions more\naccessible to follow and offers greater controllability over the manipulation\nof landmark objects. Furthermore, we present a Spatial Topology Modeling Task\nto facilitate the understanding of the spatial structure of the environment.\nFinally, we introduce a Style-Mixed Training policy, harnessing the prior\nknowledge of LLMs to enable style control for instruction generation based on\ndifferent prompts within a single model instance. Extensive experiments\ndemonstrate that instructions generated by C-Instructor outperform those\ngenerated by previous methods in text metrics, navigation guidance evaluation,\nand user studies.",
      "tldr_zh": "这篇论文提出了 C-Instructor，一种基于 Large Language Models (LLMs) 的系统，利用 Chain of Thought 提示机制生成可控制的导航指令，解决了现有模型在指令风格、内容控制和空间建模方面的局限性。核心方法包括 Chain of Thought with Landmarks (CoTL) 机制，用于识别关键地标并生成更易遵循的指令；Spatial Topology Modeling Task，以增强对环境空间结构的理解；以及 Style-Mixed Training 策略，利用 LLMs 的先验知识实现基于不同提示的风格控制。实验结果表明，C-Instructor 生成的指令在文本指标、导航指导评估和用户研究中均优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07433v2",
      "published_date": "2024-07-10 07:37:20 UTC",
      "updated_date": "2024-07-16 10:09:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:26:59.862905"
    },
    {
      "arxiv_id": "2407.17416v1",
      "title": "Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jesin James",
        "Balamurali B. T.",
        "Binu Abeysinghe",
        "Junchen Liu"
      ],
      "abstract": "This study investigates discriminative patterns learned by neural networks\nfor accurate speech classification, with a specific focus on vowel\nclassification tasks. By examining the activations and features of neural\nnetworks for vowel classification, we gain insights into what the networks\n\"see\" in spectrograms. Through the use of class activation mapping, we identify\nthe frequencies that contribute to vowel classification and compare these\nfindings with linguistic knowledge. Experiments on a American English dataset\nof vowels showcases the explainability of neural networks and provides valuable\ninsights into the causes of misclassifications and their characteristics when\ndifferentiating them from unvoiced speech. This study not only enhances our\nunderstanding of the underlying acoustic cues in vowel classification but also\noffers opportunities for improving speech recognition by bridging the gap\nbetween abstract representations in neural networks and established linguistic\nknowledge",
      "tldr_zh": "这篇论文研究了神经网络在语音分类（特别是元音分类）中学习的区分模式，通过分析神经网络的激活和特征，揭示网络在谱图（spectrograms）中“看到”的关键信息。研究采用类激活映射（class activation mapping）技术，识别有助于分类的频率，并将这些发现与语言学知识进行比较。实验基于一个美国英语元音数据集，展示了神经网络的可解释性，解释了误分类的原因和特征，并为桥接神经网络的抽象表示与语言学知识提供了途径，从而提升语音识别系统的性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "5th International Conference on Artificial Intelligence and Speech\n  Technology (AIST-2023), New Delhi, India",
      "pdf_url": "http://arxiv.org/pdf/2407.17416v1",
      "published_date": "2024-07-10 07:37:18 UTC",
      "updated_date": "2024-07-10 07:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:27:10.856651"
    },
    {
      "arxiv_id": "2407.07421v1",
      "title": "Federated PCA on Grassmann Manifold for IoT Anomaly Detection",
      "title_zh": "Grassmann 流形上的联邦 PCA 用于 IoT 异常检测",
      "authors": [
        "Tung-Anh Nguyen",
        "Long Tan Le",
        "Tuan Dung Nguyen",
        "Wei Bao",
        "Suranga Seneviratne",
        "Choong Seon Hong",
        "Nguyen H. Tran"
      ],
      "abstract": "With the proliferation of the Internet of Things (IoT) and the rising\ninterconnectedness of devices, network security faces significant challenges,\nespecially from anomalous activities. While traditional machine learning-based\nintrusion detection systems (ML-IDS) effectively employ supervised learning\nmethods, they possess limitations such as the requirement for labeled data and\nchallenges with high dimensionality. Recent unsupervised ML-IDS approaches such\nas AutoEncoders and Generative Adversarial Networks (GAN) offer alternative\nsolutions but pose challenges in deployment onto resource-constrained IoT\ndevices and in interpretability. To address these concerns, this paper proposes\na novel federated unsupervised anomaly detection framework, FedPCA, that\nleverages Principal Component Analysis (PCA) and the Alternating Directions\nMethod Multipliers (ADMM) to learn common representations of distributed\nnon-i.i.d. datasets. Building on the FedPCA framework, we propose two\nalgorithms, FEDPE in Euclidean space and FEDPG on Grassmann manifolds. Our\napproach enables real-time threat detection and mitigation at the device level,\nenhancing network resilience while ensuring privacy. Moreover, the proposed\nalgorithms are accompanied by theoretical convergence rates even under a\nsubsampling scheme, a novel result. Experimental results on the UNSW-NB15 and\nTON-IoT datasets show that our proposed methods offer performance in anomaly\ndetection comparable to nonlinear baselines, while providing significant\nimprovements in communication and memory efficiency, underscoring their\npotential for securing IoT networks.",
      "tldr_zh": "本研究针对IoT网络安全中异常检测的挑战，提出了一种新型联邦无监督框架FedPCA，利用Principal Component Analysis (PCA)和Alternating Directions Method Multipliers (ADMM)处理分布式非i.i.d.数据集。框架基于此开发了FEDPE（Euclidean空间）和FEDPG（Grassmann Manifold）两种算法，实现设备级实时威胁检测、隐私保护和网络弹性提升，同时提供理论收敛率分析。实验在UNSW-NB15和TON-IoT数据集上表明，该方法在异常检测性能上与非线性基线相当，却显著提高了通信和内存效率，适用于资源受限的IoT设备。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication at IEEE/ACM Transactions on Networking",
      "pdf_url": "http://arxiv.org/pdf/2407.07421v1",
      "published_date": "2024-07-10 07:23:21 UTC",
      "updated_date": "2024-07-10 07:23:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:27:23.045914"
    },
    {
      "arxiv_id": "2407.07931v1",
      "title": "Search, Examine and Early-Termination: Fake News Detection with Annotation-Free Evidences",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzhou Yang",
        "Yangming Zhou",
        "Qichao Ying",
        "Zhenxing Qian",
        "Xinpeng Zhang"
      ],
      "abstract": "Pioneer researches recognize evidences as crucial elements in fake news\ndetection apart from patterns. Existing evidence-aware methods either require\nlaborious pre-processing procedures to assure relevant and high-quality\nevidence data, or incorporate the entire spectrum of available evidences in all\nnews cases, regardless of the quality and quantity of the retrieved data. In\nthis paper, we propose an approach named \\textbf{SEE} that retrieves useful\ninformation from web-searched annotation-free evidences with an\nearly-termination mechanism. The proposed SEE is constructed by three main\nphases: \\textbf{S}earching online materials using the news as a query and\ndirectly using their titles as evidences without any annotating or filtering\nprocedure, sequentially \\textbf{E}xamining the news alongside with each piece\nof evidence via attention mechanisms to produce new hidden states with\nretrieved information, and allowing \\textbf{E}arly-termination within the\nexamining loop by assessing whether there is adequate confidence for producing\na correct prediction. We have conducted extensive experiments on datasets with\nunprocessed evidences, i.e., Weibo21, GossipCop, and pre-processed evidences,\nnamely Snopes and PolitiFact. The experimental results demonstrate that the\nproposed method outperforms state-of-the-art approaches.",
      "tldr_zh": "该研究提出了一种名为 SEE 的方法，用于假新闻检测，利用无标注（annotation-free）证据进行搜索和分析，以解决现有方法依赖繁琐预处理或使用全部证据的局限性。SEE 包括三个主要阶段：首先，通过将新闻作为查询搜索在线材料，并直接使用其标题作为证据；其次，利用注意力机制（attention mechanisms）逐个检查新闻与证据，生成新的隐藏状态；最后，引入提前终止机制（early-termination mechanism），在信心水平足够时提前结束评估过程。实验在 Weibo21、GossipCop 等未处理证据数据集，以及 Snopes 和 PolitiFact 等预处理数据集上表明，SEE 方法优于最先进的方法。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "ECAI 2024 paper. Fudan University & NVIDIA. To appear",
      "pdf_url": "http://arxiv.org/pdf/2407.07931v1",
      "published_date": "2024-07-10 07:22:30 UTC",
      "updated_date": "2024-07-10 07:22:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:27:38.436197"
    },
    {
      "arxiv_id": "2407.07412v3",
      "title": "Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Seonghoon Yu",
        "Paul Hongsuck Seo",
        "Jeany Son"
      ],
      "abstract": "We propose a new framework that automatically generates high-quality\nsegmentation masks with their referring expressions as pseudo supervisions for\nreferring image segmentation (RIS). These pseudo supervisions allow the\ntraining of any supervised RIS methods without the cost of manual labeling. To\nachieve this, we incorporate existing segmentation and image captioning\nfoundation models, leveraging their broad generalization capabilities. However,\nthe naive incorporation of these models may generate non-distinctive\nexpressions that do not distinctively refer to the target masks. To address\nthis challenge, we propose two-fold strategies that generate distinctive\ncaptions: 1) 'distinctive caption sampling', a new decoding method for the\ncaptioning model, to generate multiple expression candidates with detailed\nwords focusing on the target. 2) 'distinctiveness-based text filtering' to\nfurther validate the candidates and filter out those with a low level of\ndistinctiveness. These two strategies ensure that the generated text\nsupervisions can distinguish the target from other objects, making them\nappropriate for the RIS annotations. Our method significantly outperforms both\nweakly and zero-shot SoTA methods on the RIS benchmark datasets. It also\nsurpasses fully supervised methods in unseen domains, proving its capability to\ntackle the open-world challenge within RIS. Furthermore, integrating our method\nwith human annotations yields further improvements, highlighting its potential\nin semi-supervised learning applications.",
      "tldr_zh": "本研究提出Pseudo-RIS框架，通过自动生成高质量的分割掩码及其指称表达式作为伪监督，用于Referring Image Segmentation (RIS)，从而无需手动标注即可训练监督模型。该框架整合现有分割和图像描述基础模型，并引入两种策略优化生成文本：1) 'distinctive caption sampling'，一种新解码方法生成多个聚焦目标的详细候选表达式；2) 'distinctiveness-based text filtering'，用于验证和过滤低区分性的候选，确保文本能精确区分目标对象。实验结果显示，该方法在RIS基准数据集上显著优于弱监督和零样本SOTA方法，并在未见领域超越全监督方法；此外，与人类标注结合，可进一步提升性能，适用于半监督学习场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07412v3",
      "published_date": "2024-07-10 07:14:48 UTC",
      "updated_date": "2024-07-17 05:32:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:28:01.590264"
    },
    {
      "arxiv_id": "2407.07406v1",
      "title": "Weakly-supervised Medical Image Segmentation with Gaze Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Zhong",
        "Chenhui Tang",
        "Yumeng Yang",
        "Ruoxi Qi",
        "Kang Zhou",
        "Yuqi Gong",
        "Pheng Ann Heng",
        "Janet H. Hsiao",
        "Qi Dou"
      ],
      "abstract": "Eye gaze that reveals human observational patterns has increasingly been\nincorporated into solutions for vision tasks. Despite recent explorations on\nleveraging gaze to aid deep networks, few studies exploit gaze as an efficient\nannotation approach for medical image segmentation which typically entails\nheavy annotating costs. In this paper, we propose to collect dense weak\nsupervision for medical image segmentation with a gaze annotation scheme. To\ntrain with gaze, we propose a multi-level framework that trains multiple\nnetworks from discriminative human attention, simulated with a set of\npseudo-masks derived by applying hierarchical thresholds on gaze heatmaps.\nFurthermore, to mitigate gaze noise, a cross-level consistency is exploited to\nregularize overfitting noisy labels, steering models toward clean patterns\nlearned by peer networks. The proposed method is validated on two public\nmedical datasets of polyp and prostate segmentation tasks. We contribute a\nhigh-quality gaze dataset entitled GazeMedSeg as an extension to the popular\nmedical segmentation datasets. To the best of our knowledge, this is the first\ngaze dataset for medical image segmentation. Our experiments demonstrate that\ngaze annotation outperforms previous label-efficient annotation schemes in\nterms of both performance and annotation time. Our collected gaze data and code\nare available at: https://github.com/med-air/GazeMedSeg.",
      "tldr_zh": "本文提出一种基于 gaze annotations 的弱监督方法，用于医疗图像分割，以降低传统标注成本。该方法采用多级框架，通过对 gaze heatmaps 应用层次化阈值生成伪掩码（pseudo-masks）训练多个网络，并引入 cross-level consistency 机制来减少噪声干扰，确保模型学习到更可靠的模式。在两个公共数据集上进行的实验显示，该方法在息肉和前列腺分割任务中性能优于现有标签高效方案，同时显著缩短标注时间，并贡献了首个 gaze 数据集 GazeMedSeg。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07406v1",
      "published_date": "2024-07-10 07:07:58 UTC",
      "updated_date": "2024-07-10 07:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:28:02.887914"
    },
    {
      "arxiv_id": "2407.07392v1",
      "title": "Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems",
      "title_zh": "通过利用视觉-语言导航系统的表示漏洞进行恶意路径操纵",
      "authors": [
        "Chashi Mahiul Islam",
        "Shaeke Salman",
        "Montasir Shams",
        "Xiuwen Liu",
        "Piyush Kumar"
      ],
      "abstract": "Building on the unprecedented capabilities of large language models for\ncommand understanding and zero-shot recognition of multi-modal vision-language\ntransformers, visual language navigation (VLN) has emerged as an effective way\nto address multiple fundamental challenges toward a natural language interface\nto robot navigation. However, such vision-language models are inherently\nvulnerable due to the lack of semantic meaning of the underlying embedding\nspace. Using a recently developed gradient based optimization procedure, we\ndemonstrate that images can be modified imperceptibly to match the\nrepresentation of totally different images and unrelated texts for a\nvision-language model. Building on this, we develop algorithms that can\nadversarially modify a minimal number of images so that the robot will follow a\nroute of choice for commands that require a number of landmarks. We demonstrate\nthat experimentally using a recently proposed VLN system; for a given\nnavigation command, a robot can be made to follow drastically different routes.\nWe also develop an efficient algorithm to detect such malicious modifications\nreliably based on the fact that the adversarially modified images have much\nhigher sensitivity to added Gaussian noise than the original images.",
      "tldr_zh": "本论文探讨了视觉语言导航 (VLN) 系统的表示漏洞，揭示了如何通过微小图像修改来恶意操控机器人路径。研究者利用梯度优化方法，对图像进行不易察觉的改动，使其匹配完全不同的图像或不相关文本的嵌入表示，从而让机器人对给定的导航命令遵循攻击者指定的路线。实验结果显示，在一个最近提出的 VLN 系统上，机器人可被引导走完全不同的路径，证明了这种攻击的有效性。同时，论文开发了一个高效检测算法，通过分析图像对高斯噪声的敏感性，来可靠地识别这些恶意修改。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 5 figures. This paper has been accepted for publication at\n  the IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07392v1",
      "published_date": "2024-07-10 06:32:58 UTC",
      "updated_date": "2024-07-10 06:32:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:28:14.747606"
    },
    {
      "arxiv_id": "2407.18928v1",
      "title": "The Voice: Lessons on Trustworthy Conversational Agents from \"Dune\"",
      "title_zh": "翻译失败",
      "authors": [
        "Philip Feldman"
      ],
      "abstract": "The potential for untrustworthy conversational agents presents a significant\nthreat for covert social manipulation. Taking inspiration from Frank Herbert's\n\"Dune\", where the Bene Gesserit Sisterhood uses the Voice for influence,\nmanipulation, and control of people, we explore how generative AI provides a\nway to implement individualized influence at industrial scales. Already, these\nmodels can manipulate communication across text, image, speech, and most\nrecently video. They are rapidly becoming affordable enough for any\norganization of even moderate means to train and deploy. If employed by\nmalicious actors, they risk becoming powerful tools for shaping public opinion,\nsowing discord, and undermining organizations from companies to governments. As\nresearchers and developers, it is crucial to recognize the potential for such\nweaponization and to explore strategies for prevention, detection, and defense\nagainst these emerging forms of sociotechnical manipulation.",
      "tldr_zh": "本论文从弗兰克·赫伯特的小说《Dune》中的Bene Gesserit Sisterhood的“Voice”技术中汲取灵感，探讨了不信任的conversational agents可能导致的隐蔽社会操纵风险。生成式AI能够通过文本、图像、语音和视频实现大规模个性化影响，使其成为潜在的工具，用于塑造公众意见、制造分歧并破坏组织。论文强调，研究者和开发者需认识到这些威胁，并探索预防、检测和防御策略，以应对这种新兴的sociotechnical manipulation。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "K.4.2; I.2.m"
      ],
      "primary_category": "cs.CY",
      "comment": "5 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18928v1",
      "published_date": "2024-07-10 05:38:31 UTC",
      "updated_date": "2024-07-10 05:38:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:28:27.436708"
    },
    {
      "arxiv_id": "2407.07375v1",
      "title": "Stable Weight Updating: A Key to Reliable PDE Solutions Using Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "A. Noorizadegan",
        "R. Cavoretto",
        "D. L. Young",
        "C. S. Chen"
      ],
      "abstract": "Background: Deep learning techniques, particularly neural networks, have\nrevolutionized computational physics, offering powerful tools for solving\ncomplex partial differential equations (PDEs). However, ensuring stability and\nefficiency remains a challenge, especially in scenarios involving nonlinear and\ntime-dependent equations. Methodology: This paper introduces novel\nresidual-based architectures, namely the Simple Highway Network and the Squared\nResidual Network, designed to enhance stability and accuracy in\nphysics-informed neural networks (PINNs). These architectures augment\ntraditional neural networks by incorporating residual connections, which\nfacilitate smoother weight updates and improve backpropagation efficiency.\nResults: Through extensive numerical experiments across various examples\nincluding linear and nonlinear, time-dependent and independent PDEs we\ndemonstrate the efficacy of the proposed architectures. The Squared Residual\nNetwork, in particular, exhibits robust performance, achieving enhanced\nstability and accuracy compared to conventional neural networks. These findings\nunderscore the potential of residual-based architectures in advancing deep\nlearning for PDEs and computational physics applications.",
      "tldr_zh": "本研究针对深度学习在解决偏微分方程（PDEs）时的稳定性和效率挑战，提出两种新型残差-based架构：Simple Highway Network和Squared Residual Network，以增强physics-informed neural networks（PINNs）的性能。  \n这些架构通过融入残差连接，优化权重更新过程并改善反向传播效率，从而更好地处理线性、非线性及时间相关PDEs。  \n实验结果显示，Squared Residual Network在多种PDEs示例上表现出色，比传统神经网络提高了稳定性和准确性，为深度学习在计算物理领域的应用提供了重要潜力。",
      "categories": [
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07375v1",
      "published_date": "2024-07-10 05:20:43 UTC",
      "updated_date": "2024-07-10 05:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:28:37.568425"
    },
    {
      "arxiv_id": "2407.07364v1",
      "title": "Real-time system optimal traffic routing under uncertainties -- Can physics models boost reinforcement learning?",
      "title_zh": "不确定性下的实时系统最优交通路由——物理模型能提升强化学习吗？",
      "authors": [
        "Zemian Ke",
        "Qiling Zou",
        "Jiachao Liu",
        "Sean Qian"
      ],
      "abstract": "System optimal traffic routing can mitigate congestion by assigning routes\nfor a portion of vehicles so that the total travel time of all vehicles in the\ntransportation system can be reduced. However, achieving real-time optimal\nrouting poses challenges due to uncertain demands and unknown system dynamics,\nparticularly in expansive transportation networks. While physics model-based\nmethods are sensitive to uncertainties and model mismatches, model-free\nreinforcement learning struggles with learning inefficiencies and\ninterpretability issues. Our paper presents TransRL, a novel algorithm that\nintegrates reinforcement learning with physics models for enhanced performance,\nreliability, and interpretability. TransRL begins by establishing a\ndeterministic policy grounded in physics models, from which it learns from and\nis guided by a differentiable and stochastic teacher policy. During training,\nTransRL aims to maximize cumulative rewards while minimizing the Kullback\nLeibler (KL) divergence between the current policy and the teacher policy. This\napproach enables TransRL to simultaneously leverage interactions with the\nenvironment and insights from physics models. We conduct experiments on three\ntransportation networks with up to hundreds of links. The results demonstrate\nTransRL's superiority over traffic model-based methods for being adaptive and\nlearning from the actual network data. By leveraging the information from\nphysics models, TransRL consistently outperforms state-of-the-art reinforcement\nlearning algorithms such as proximal policy optimization (PPO) and soft actor\ncritic (SAC). Moreover, TransRL's actions exhibit higher reliability and\ninterpretability compared to baseline reinforcement learning approaches like\nPPO and SAC.",
      "tldr_zh": "这篇论文探讨了在不确定性下实现实时系统最优交通路由的挑战，旨在通过整合 reinforcement learning (RL) 与 physics models 来缓解交通拥堵。论文提出 TransRL 算法，该算法从 physics models 建立确定性策略，并使用可微随机教师策略引导学习，同时最大化累积奖励并最小化 Kullback-Leibler (KL) divergence，以提升性能和可解释性。在三个交通网络的实验中，TransRL 比基于交通模型的方法更具适应性，并优于 proximal policy optimization (PPO) 和 soft actor critic (SAC) 等 RL 算法，展示了更高的可靠性和实际数据学习能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07364v1",
      "published_date": "2024-07-10 04:53:26 UTC",
      "updated_date": "2024-07-10 04:53:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:28:50.918684"
    },
    {
      "arxiv_id": "2407.07355v2",
      "title": "High-Precision, Fair University Course Scheduling During a Pandemic",
      "title_zh": "疫情期间的高精度公平大学课程调度",
      "authors": [
        "Matthew E. H. Petering",
        "Mohammad Khamechian"
      ],
      "abstract": "Scheduling university courses is extra challenging when classroom capacities\nare reduced because of social distancing requirements that are implemented in\nresponse to a pandemic such as COVID-19. In this work, we propose an expanded\ntaxonomy of course delivery modes, present an integer program, and develop a\ncourse scheduling algorithm to enable all course sections -- even the largest\n-- to have a significant classroom learning component during a pandemic. Our\napproach is fair by ensuring that a certain fraction of the instruction in\nevery course section occurs in the classroom. Unlike previous studies, we do\nnot allow rotating attendance and instead require simultaneous attendance in\nwhich all students in a section meet in 1-5 rooms at the same time but less\noften than in a normal semester. These mass meetings, which create\nopportunities for in-person midterm exams and group activities, are scheduled\nat high precision across all days of the semester rather than a single,\nrepeating week. A fast heuristic algorithm makes the schedule in an hour.\nResults: We consider the 1834 in-person course sections, 172 classrooms, and 96\ndays in the fall 2022 semester at [UniversityXYZ]. If average classroom\ncapacity is reduced by 75% due to a pandemic, our approach still allows at\nleast 25% of the instruction in every section, and more than 49% of all\ninstruction across the entire campus, to be in the classroom. Our method also\nproduces excellent results for regular classroom assignment. Managerial\nimplications: An algorithm based on the principles of fairness and simultaneous\nattendance can significantly improve university course schedules during a\npandemic and in normal times. High-precision schedules that prepare a campus\nfor various pandemic possibilities can be created with minimal administrative\neffort and activated at a moment's notice before or during a semester if an\noutbreak occurs.",
      "tldr_zh": "本论文针对疫情期间教室容量减少（如COVID-19社交距离要求）的大学课程调度问题，提出了一种高精度且公平的调度方法，包括扩展的课程交付模式分类、integer program整数规划和一个启发式heuristic algorithm算法。该方法确保所有课程部分（即使是最大的）都有显著的课堂学习成分，通过要求simultaneous attendance（所有学生在同一时间但较少频率地在1-5个房间中）实现公平，并支持大规模会议如期中考试。实验结果显示，在[UniversityXYZ]的1834个课程部分和172个教室中，如果教室容量平均减少75%，仍能让每个部分至少25%的指令在课堂上，整体超过49%；该算法可在1小时内生成高精度跨学期调度，并适用于正常和疫情时期的管理优化。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "math.OC",
      "comment": "32 pages, 13 tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.07355v2",
      "published_date": "2024-07-10 04:26:01 UTC",
      "updated_date": "2024-07-11 17:56:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:29:04.771857"
    },
    {
      "arxiv_id": "2407.07341v2",
      "title": "A Guide To Effectively Leveraging LLMs for Low-Resource Text Summarization: Data Augmentation and Semi-supervised Approaches",
      "title_zh": "翻译失败",
      "authors": [
        "Gaurav Sahu",
        "Olga Vechtomova",
        "Issam H. Laradji"
      ],
      "abstract": "Existing approaches for low-resource text summarization primarily employ\nlarge language models (LLMs) like GPT-3 or GPT-4 at inference time to generate\nsummaries directly; however, such approaches often suffer from inconsistent LLM\noutputs and are difficult to adapt to domain-specific data in low-resource\nscenarios. In this work, we propose two novel methods to effectively utilize\nLLMs for low-resource text summarization: 1) MixSumm, an LLM-based data\naugmentation regime that synthesizes high-quality documents (short and long)\nfor few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling\nstrategy for sample-efficient semi-supervised text summarization. Specifically,\nMixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new\ndocuments by mixing topical information derived from a small seed set, and PPSL\nleverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels\nin a semi-supervised learning setup. We evaluate our methods on the TweetSumm,\nWikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation\nmetric, and ROUGE scores to measure the quality of generated summaries. Our\nexperiments on extractive and abstractive summarization show that MixSumm and\nPPSL achieve competitive ROUGE scores as a fully supervised method with 5% of\nthe labeled data.",
      "tldr_zh": "本文针对低资源文本摘要问题，提出两种基于大型语言模型(LLMs)的新方法：MixSumm 和 PPSL，以解决现有方法的输出不一致性和领域适应性差的问题。MixSumm 通过使用 LLaMA-3-70b-Instruct 模型从少量种子数据中混合主题信息生成高质量文档，用于增强少样本摘要训练；PPSL 则采用基于提示的伪标签策略，在半监督设置下生成高质量伪标签以提高样本效率。在 TweetSumm、WikiHow 和 ArXiv/PubMed 数据集上的实验显示，这两种方法在抽取式和抽象式摘要任务中，仅使用 5% 的标注数据，就实现了与完全监督方法相当的 ROUGE scores。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 (Findings)",
      "pdf_url": "http://arxiv.org/pdf/2407.07341v2",
      "published_date": "2024-07-10 03:25:47 UTC",
      "updated_date": "2025-01-23 21:26:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:29:16.194944"
    },
    {
      "arxiv_id": "2407.07333v3",
      "title": "Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy",
      "title_zh": "通过 Lambda 差异缓解顺序决策过程的部分可观测性",
      "authors": [
        "Cameron Allen",
        "Aaron Kirtland",
        "Ruo Yu Tao",
        "Sam Lobel",
        "Daniel Scott",
        "Nicholas Petrocelli",
        "Omer Gottesman",
        "Ronald Parr",
        "Michael L. Littman",
        "George Konidaris"
      ],
      "abstract": "Reinforcement learning algorithms typically rely on the assumption that the\nenvironment dynamics and value function can be expressed in terms of a\nMarkovian state representation. However, when state information is only\npartially observable, how can an agent learn such a state representation, and\nhow can it detect when it has found one? We introduce a metric that can\naccomplish both objectives, without requiring access to -- or knowledge of --\nan underlying, unobservable state space. Our metric, the $\\lambda$-discrepancy,\nis the difference between two distinct temporal difference (TD) value\nestimates, each computed using TD($\\lambda$) with a different value of\n$\\lambda$. Since TD($\\lambda{=}0$) makes an implicit Markov assumption and\nTD($\\lambda{=}1$) does not, a discrepancy between these estimates is a\npotential indicator of a non-Markovian state representation. Indeed, we prove\nthat the $\\lambda$-discrepancy is exactly zero for all Markov decision\nprocesses and almost always non-zero for a broad class of partially observable\nenvironments. We also demonstrate empirically that, once detected, minimizing\nthe $\\lambda$-discrepancy can help with learning a memory function to mitigate\nthe corresponding partial observability. We then train a reinforcement learning\nagent that simultaneously constructs two recurrent value networks with\ndifferent $\\lambda$ parameters and minimizes the difference between them as an\nauxiliary loss. The approach scales to challenging partially observable\ndomains, where the resulting agent frequently performs significantly better\n(and never performs worse) than a baseline recurrent agent with only a single\nvalue network.",
      "tldr_zh": "本文提出了一种名为λ-discrepancy的指标，用于检测和缓解强化学习(Reinforcement Learning)中部分可观测性问题，该指标通过比较不同λ值的TD(λ)算法计算的价值估计差异来实现。研究证明，在Markov Decision Processes中，λ-discrepancy始终为零，而在部分可观测环境中通常非零；通过最小化该差异，代理可以学习记忆函数来改进状态表示。实验结果显示，训练代理同时构建两个循环价值网络并作为辅助损失最小化差异，能在复杂部分可观测域中显著提升性能，比基线代理表现更好。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "GitHub URL: https://github.com/brownirl/lambda_discrepancy; Project\n  page: https://lambda-discrepancy.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.07333v3",
      "published_date": "2024-07-10 03:04:20 UTC",
      "updated_date": "2024-11-14 22:17:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:29:27.546771"
    },
    {
      "arxiv_id": "2407.07331v1",
      "title": "Learning with Instance-Dependent Noisy Labels by Anchor Hallucination and Hard Sample Label Correction",
      "title_zh": "基于锚点幻觉和硬样本标签修正的实例依赖噪声标签学习",
      "authors": [
        "Po-Hsuan Huang",
        "Chia-Ching Lin",
        "Chih-Fan Hsu",
        "Ming-Ching Chang",
        "Wei-Chao Chen"
      ],
      "abstract": "Learning from noisy-labeled data is crucial for real-world applications.\nTraditional Noisy-Label Learning (NLL) methods categorize training data into\nclean and noisy sets based on the loss distribution of training samples.\nHowever, they often neglect that clean samples, especially those with intricate\nvisual patterns, may also yield substantial losses. This oversight is\nparticularly significant in datasets with Instance-Dependent Noise (IDN), where\nmislabeling probabilities correlate with visual appearance. Our approach\nexplicitly distinguishes between clean vs.noisy and easy vs. hard samples. We\nidentify training samples with small losses, assuming they have simple patterns\nand correct labels. Utilizing these easy samples, we hallucinate multiple\nanchors to select hard samples for label correction. Corrected hard samples,\nalong with the easy samples, are used as labeled data in subsequent\nsemi-supervised training. Experiments on synthetic and real-world IDN datasets\ndemonstrate the superior performance of our method over other state-of-the-art\nNLL methods.",
      "tldr_zh": "该论文提出了一种针对 Instance-Dependent Noise (IDN) 数据集的 Noisy-Label Learning (NLL) 方法，通过 Anchor Hallucination 和 Hard Sample Label Correction 技术来处理噪声标签问题。该方法首先识别损失小的容易样本（假设其标签正确），并利用这些样本生成多个 anchors 来选择并修正困难样本的标签。随后，将修正后的困难样本与容易样本结合，用于半监督训练，以提高模型鲁棒性。实验结果显示，该方法在合成和真实世界 IDN 数据集上优于现有最先进 NLL 方法，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICIP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.07331v1",
      "published_date": "2024-07-10 03:00:14 UTC",
      "updated_date": "2024-07-10 03:00:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:29:39.282874"
    },
    {
      "arxiv_id": "2407.07330v2",
      "title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shuang Zhou",
        "Mingquan Lin",
        "Sirui Ding",
        "Jiashuo Wang",
        "Genevieve B. Melton",
        "James Zou",
        "Rui Zhang"
      ],
      "abstract": "Automatic differential diagnosis (DDx) is an essential medical task that\ngenerates a list of potential diseases as differentials based on patient\nsymptom descriptions. In practice, interpreting these differential diagnoses\nyields significant value but remains under-explored. Given the powerful\ncapabilities of large language models (LLMs), we investigated using LLMs for\ninterpretable DDx. Specifically, we curated the first DDx dataset with\nexpert-derived interpretation on 570 clinical notes. Besides, we proposed\nDual-Inf, a novel framework that enabled LLMs to conduct bidirectional\ninference (i.e., from symptoms to diagnoses and vice versa) for DDx\ninterpretation. Both human and automated evaluation validated its efficacy in\npredicting and elucidating differentials across four base LLMs. In addition,\nDual-Inf could reduce interpretation errors and hold promise for rare disease\nexplanations. To the best of our knowledge, it is the first work that\ncustomizes LLMs for DDx explanation and comprehensively evaluates their\ninterpretation performance. Overall, our study bridges a critical gap in DDx\ninterpretation and enhances clinical decision-making.",
      "tldr_zh": "该论文探讨了使用大型语言模型 (LLMs) 进行可解释的鉴别诊断 (DDx)，旨在基于患者症状生成潜在疾病列表并提供解释，以提升临床决策。研究者构建了首个包含 570 个临床笔记的 DDx 数据集，并提出了 Dual-Inf 框架，该框架通过双向推理（从症状到诊断以及反向）来增强 LLMs 的解释能力。实验结果显示，Dual-Inf 在四个基础 LLMs 上显著提高了预测和解释的准确性，减少了解释错误，并为稀有疾病解释提供了潜力。该工作首次针对 DDx 解释定制 LLMs，并全面评估了其性能，有助于桥接临床解释的空白。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.07330v2",
      "published_date": "2024-07-10 02:58:37 UTC",
      "updated_date": "2024-11-06 22:03:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:29:52.941792"
    },
    {
      "arxiv_id": "2407.07327v1",
      "title": "Fuse, Reason and Verify: Geometry Problem Solving with Parsed Clauses from Diagram",
      "title_zh": "翻译失败",
      "authors": [
        "Ming-Liang Zhang",
        "Zhong-Zhi Li",
        "Fei Yin",
        "Liang Lin",
        "Cheng-Lin Liu"
      ],
      "abstract": "Geometry problem solving (GPS) requires capacities of multi-modal\nunderstanding, multi-hop reasoning and theorem knowledge application. In this\npaper, we propose a neural-symbolic model for plane geometry problem solving\n(PGPS), named PGPSNet-v2, with three key steps: modal fusion, reasoning process\nand knowledge verification. In modal fusion, we leverage textual clauses to\nexpress fine-grained structural and semantic content of geometry diagram, and\nfuse diagram with textual problem efficiently through structural-semantic\npre-training. For reasoning, we design an explicable solution program to\ndescribe the geometric reasoning process, and employ a self-limited decoder to\ngenerate solution program autoregressively. To reduce solution errors, a\nmulti-level theorem verifier is proposed to eliminate solutions that do not\nmatch geometric principles, alleviating the hallucination of the neural model.\nWe also construct a large-scale geometry problem dataset called PGPS9K,\ncontaining fine-grained annotations of textual clauses, solution program and\ninvolved knowledge tuples. Extensive experiments on datasets Geometry3K and\nPGPS9K show that our PGPSNet solver outperforms existing symbolic and neural\nsolvers in GPS performance, while maintaining good explainability and\nreliability, and the solver components (fusion, reasoning, verification) are\nall justified effective.",
      "tldr_zh": "该论文提出了一种神经符号模型 PGPSNet-v2，用于平面几何问题求解 (PGPS)，通过三个关键步骤——modal fusion、reasoning process 和 knowledge verification——来提升多模态理解、多跳推理和定理应用能力。在 modal fusion 阶段，模型利用文本子句表达几何图形的细粒度结构和语义，并通过结构-语义预训练高效融合图表与文本问题；reasoning process 则设计可解释的解决方案程序，并采用自限解码器自动生成解决方案；knowledge verification 引入多级定理验证器，以消除不符合几何原则的错误输出，缓解神经模型的幻觉问题。论文还构建了大规模数据集 PGPS9K，包括细粒度注释的文本子句、解决方案程序和知识元组。实验在 Geometry3K 和 PGPS9K 数据集上显示，PGPSNet-v2 超过了现有符号和神经求解器，在 GPS 性能上表现出色，同时保持良好的可解释性和可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "under review by journal",
      "pdf_url": "http://arxiv.org/pdf/2407.07327v1",
      "published_date": "2024-07-10 02:45:22 UTC",
      "updated_date": "2024-07-10 02:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:30:06.559088"
    },
    {
      "arxiv_id": "2407.07311v3",
      "title": "ViTime: A Visual Intelligence-Based Foundation Model for Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Luoxiao Yang",
        "Yun Wang",
        "Xinqi Fan",
        "Israel Cohen",
        "Jingdong Chen",
        "Yue Zhao",
        "Zijun Zhang"
      ],
      "abstract": "Time series forecasting (TSF) possesses great practical values in various\nfields, including power and energy, transportation, etc. TSF methods have been\nstudied based on knowledge from classical statistics to modern deep learning.\nYet, all of them were developed based on one fundamental concept, the numerical\ndata fitting. Thus, the models developed have been long known for being\nproblem-specific and lacking application generalizability. A TSF foundation\nmodel serving TSF tasks across different applications can reverse such an\nimpression. The central question is then how to develop such a TSF foundation\nmodel. This paper offers a pioneering study in developing a TSF foundation\nmodel and proposes a vision intelligence-powered framework, ViTime, for the\nfirst time. In ViTime, a method synthesizing authentic time series periodic and\ntrend patterns is developed to enrich sample pattern diversity. A deep\narchitecture operating TSF in image metric space is designed to achieve\nsignificantly enhanced TSF generalizability. Extensive experiments demonstrate\nViTime's SOTA performance across multiple settings. In zero-shot scenarios,\nViTime outperforms TimesFM by 9-15%. With just 10% fine-tuning data, ViTime\nsurpasses both foundation models and fully-supervised benchmarks trained on\ncomplete datasets, with this performance gap widening further at 100\\%\nfine-tuning. Additionally, ViTime exhibits exceptional robustness, handling\nmissing data without imputation and outperforming TimesFM by 20-30% under\nvarious data perturbations.",
      "tldr_zh": "该论文提出 ViTime，一种基于视觉智能的 TSF（Time Series Forecasting）基础模型，旨在解决传统 TSF 方法的领域特定性和泛化性不足问题。ViTime 通过开发一种合成真实时间序列周期和趋势模式的方法来丰富样本多样性，并设计了一个在图像度量空间中操作的深度架构，以显著提升预测的泛化能力。实验结果显示，ViTime 在多种设置下达到 SOTA（State-of-the-Art）性能，在 zero-shot 场景中比 TimesFM 高出 9-15%；仅使用 10% 微调数据就超越了完全监督基准，且在处理缺失数据和数据扰动时表现出色，比 TimesFM 高出 20-30%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07311v3",
      "published_date": "2024-07-10 02:11:01 UTC",
      "updated_date": "2025-02-08 05:05:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:30:16.371889"
    },
    {
      "arxiv_id": "2407.07304v1",
      "title": "Inference Performance Optimization for Large Language Models on CPUs",
      "title_zh": "大型语言模型在 CPU 上的推理性能优化",
      "authors": [
        "Pujiang He",
        "Shan Zhou",
        "Wenhuan Huang",
        "Changqing Li",
        "Duyi Wang",
        "Bin Guo",
        "Chen Meng",
        "Sheng Gui",
        "Weifei Yu",
        "Yi Xie"
      ],
      "abstract": "Large language models (LLMs) have shown exceptional performance and vast\npotential across diverse tasks. However, the deployment of LLMs with high\nperformance in low-resource environments has garnered significant attention in\nthe industry. When GPU hardware resources are limited, we can explore\nalternative options on CPUs. To mitigate the financial burden and alleviate\nconstraints imposed by hardware resources, optimizing inference performance is\nnecessary. In this paper, we introduce an easily deployable inference\nperformance optimization solution aimed at accelerating LLMs on CPUs. In this\nsolution, we implement an effective way to reduce the KV cache size while\nensuring precision. We propose a distributed inference optimization approach\nand implement it based on oneAPI Collective Communications Library.\nFurthermore, we propose optimization approaches for LLMs on CPU, and conduct\ntailored optimizations for the most commonly used models. The code is\nopen-sourced at https://github.com/intel/xFasterTransformer.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在 CPU 上的推理性能优化，旨在解决资源有限环境下的部署挑战。通过引入一种易部署的优化解决方案，该方法有效减少 KV cache 大小，同时保持精度，并提出基于 oneAPI Collective Communications Library 的分布式推理优化策略。此外，研究为常用模型进行定制优化，并开源代码（https://github.com/intel/xFasterTransformer），有助于缓解硬件约束并提升 LLMs 的实际部署效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 6 figure, ICML 2024 on Foundation Models in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2407.07304v1",
      "published_date": "2024-07-10 01:53:49 UTC",
      "updated_date": "2024-07-10 01:53:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:30:26.628721"
    },
    {
      "arxiv_id": "2407.07296v1",
      "title": "Large Language Model-Augmented Auto-Delineation of Treatment Target Volume in Radiation Therapy",
      "title_zh": "大型语言模型增强的放射治疗治疗目标体积自动勾画",
      "authors": [
        "Praveenbalaji Rajendran",
        "Yong Yang",
        "Thomas R. Niedermayr",
        "Michael Gensheimer",
        "Beth Beadle",
        "Quynh-Thu Le",
        "Lei Xing",
        "Xianjin Dai"
      ],
      "abstract": "Radiation therapy (RT) is one of the most effective treatments for cancer,\nand its success relies on the accurate delineation of targets. However, target\ndelineation is a comprehensive medical decision that currently relies purely on\nmanual processes by human experts. Manual delineation is time-consuming,\nlaborious, and subject to interobserver variations. Although the advancements\nin artificial intelligence (AI) techniques have significantly enhanced the\nauto-contouring of normal tissues, accurate delineation of RT target volumes\nremains a challenge. In this study, we propose a visual language model-based RT\ntarget volume auto-delineation network termed Radformer. The Radformer utilizes\na hierarichal vision transformer as the backbone and incorporates large\nlanguage models to extract text-rich features from clinical data. We introduce\na visual language attention module (VLAM) for integrating visual and linguistic\nfeatures for language-aware visual encoding (LAVE). The Radformer has been\nevaluated on a dataset comprising 2985 patients with head-and-neck cancer who\nunderwent RT. Metrics, including the Dice similarity coefficient (DSC),\nintersection over union (IOU), and 95th percentile Hausdorff distance (HD95),\nwere used to evaluate the performance of the model quantitatively. Our results\ndemonstrate that the Radformer has superior segmentation performance compared\nto other state-of-the-art models, validating its potential for adoption in RT\npractice.",
      "tldr_zh": "本文针对辐射治疗(RT)中目标体积勾画的挑战，提出了一种基于大型语言模型增强的自动勾画网络Radformer，利用分层视觉transformer作为骨干，并通过视觉语言注意力模块(VLAM)整合视觉和语言特征，实现语言感知视觉编码(LAVE)。该方法从临床数据中提取文本丰富特征，以提高目标勾画的准确性。在包含2985名头颈癌患者的RT数据集上评估，Radformer在Dice Similarity Coefficient (DSC)、Intersection over Union (IOU)和95th Percentile Hausdorff Distance (HD95)等指标上，表现优于其他最先进模型。这为RT实践中的自动目标勾画提供了可靠的解决方案。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07296v1",
      "published_date": "2024-07-10 01:32:55 UTC",
      "updated_date": "2024-07-10 01:32:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:30:41.289879"
    },
    {
      "arxiv_id": "2407.12858v1",
      "title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)",
      "title_zh": "翻译失败",
      "authors": [
        "Krishnaram Kenthapadi",
        "Mehrnoosh Sameki",
        "Ankur Taly"
      ],
      "abstract": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems\nin high-stakes domains, ensuring the trustworthiness, safety, and observability\nof these systems has become crucial. It is essential to evaluate and monitor AI\nsystems not only for accuracy and quality-related metrics but also for\nrobustness, bias, security, interpretability, and other responsible AI\ndimensions. We focus on large language models (LLMs) and other generative AI\nmodels, which present additional challenges such as hallucinations, harmful and\nmanipulative content, and copyright infringement. In this survey article\naccompanying our KDD 2024 tutorial, we highlight a wide range of harms\nassociated with generative AI systems, and survey state of the art approaches\n(along with open challenges) to address these harms.",
      "tldr_zh": "这篇调查文章探讨了大型语言模型 (LLMs) 的接地 (grounding) 和评估在实际应用中的挑战与经验教训，强调了在高风险领域确保 AI 系统的可信度、安全性和可观察性。论文聚焦于 LLMs 和生成式 AI 模型的潜在危害，如 hallucinations（幻觉）、有害内容、操纵内容以及版权侵犯，并分析了相关准确性、鲁棒性、偏见和可解释性等维度。最终，它调研了现有的 state-of-the-art 方法及其开放挑战，为构建负责任的 AI 系统提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Survey Article for the ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD 2024) Tutorial",
      "pdf_url": "http://arxiv.org/pdf/2407.12858v1",
      "published_date": "2024-07-10 01:23:10 UTC",
      "updated_date": "2024-07-10 01:23:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:30:52.371940"
    },
    {
      "arxiv_id": "2407.07291v1",
      "title": "Causal Discovery in Semi-Stationary Time Series",
      "title_zh": "半平稳时间序列中的因果发现",
      "authors": [
        "Shanyun Gao",
        "Raghavendra Addanki",
        "Tong Yu",
        "Ryan A. Rossi",
        "Murat Kocaoglu"
      ],
      "abstract": "Discovering causal relations from observational time series without making\nthe stationary assumption is a significant challenge. In practice, this\nchallenge is common in many areas, such as retail sales, transportation\nsystems, and medical science. Here, we consider this problem for a class of\nnon-stationary time series. The structural causal model (SCM) of this type of\ntime series, called the semi-stationary time series, exhibits that a finite\nnumber of different causal mechanisms occur sequentially and periodically\nacross time. This model holds considerable practical utility because it can\nrepresent periodicity, including common occurrences such as seasonality and\ndiurnal variation. We propose a constraint-based, non-parametric algorithm for\ndiscovering causal relations in this setting. The resulting algorithm,\nPCMCI$_{\\Omega}$, can capture the alternating and recurring changes in the\ncausal mechanisms and then identify the underlying causal graph with\nconditional independence (CI) tests. We show that this algorithm is sound in\nidentifying causal relations on discrete time series. We validate the algorithm\nwith extensive experiments on continuous and discrete simulated data. We also\napply our algorithm to a real-world climate dataset.",
      "tldr_zh": "本研究探讨了在非平稳时间序列中发现因果关系的挑战，特别针对半平稳 time series（semi-stationary time series），其中有限数量的因果机制以顺序和周期性方式出现，以模拟现实中的季节性和昼夜变化。作者提出了一种基于约束的非参数算法 PCMCI$_{\\Omega}$，通过 conditional independence (CI) tests 来捕捉因果机制的交替变化，并准确识别底层因果图。实验结果显示，该算法在模拟的连续和离散数据上表现可靠，并在真实气候数据集上得到验证，为非平稳时间序列的 structural causal model (SCM) 应用提供了有效工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "I.2.6, G.3"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07291v1",
      "published_date": "2024-07-10 00:55:38 UTC",
      "updated_date": "2024-07-10 00:55:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:31:03.562487"
    },
    {
      "arxiv_id": "2407.07290v1",
      "title": "Causal Discovery-Driven Change Point Detection in Time Series",
      "title_zh": "基于因果发现驱动的时间序列变化点检测",
      "authors": [
        "Shanyun Gao",
        "Raghavendra Addanki",
        "Tong Yu",
        "Ryan A. Rossi",
        "Murat Kocaoglu"
      ],
      "abstract": "Change point detection in time series seeks to identify times when the\nprobability distribution of time series changes. It is widely applied in many\nareas, such as human-activity sensing and medical science. In the context of\nmultivariate time series, this typically involves examining the joint\ndistribution of high-dimensional data: If any one variable changes, the whole\ntime series is assumed to have changed. However, in practical applications, we\nmay be interested only in certain components of the time series, exploring\nabrupt changes in their distributions in the presence of other time series.\nHere, assuming an underlying structural causal model that governs the\ntime-series data generation, we address this problem by proposing a two-stage\nnon-parametric algorithm that first learns parts of the causal structure\nthrough constraint-based discovery methods. The algorithm then uses conditional\nrelative Pearson divergence estimation to identify the change points. The\nconditional relative Pearson divergence quantifies the distribution disparity\nbetween consecutive segments in the time series, while the causal discovery\nmethod enables a focus on the causal mechanism, facilitating access to\nindependent and identically distributed (IID) samples. Theoretically, the\ntypical assumption of samples being IID in conventional change point detection\nmethods can be relaxed based on the Causal Markov Condition. Through\nexperiments on both synthetic and real-world datasets, we validate the\ncorrectness and utility of our approach.",
      "tldr_zh": "本文提出了一种基于因果发现的时序变化点检测方法，针对多变量时间序列中特定组件的分布突变问题，而非整个序列。方法采用两阶段非参数算法：首先使用基于约束的发现方法学习部分因果结构，然后通过条件相对Pearson散度估计算法量化连续段之间的分布差异，以识别变化点。该方法利用Causal Markov Condition放宽了传统变化点检测对样本独立同分布(IID)的假设，从而提升了检测的鲁棒性。在合成和真实数据集上的实验验证了该方法的正确性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML",
        "I.2.6, G.3"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07290v1",
      "published_date": "2024-07-10 00:54:42 UTC",
      "updated_date": "2024-07-10 00:54:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:31:16.621928"
    },
    {
      "arxiv_id": "2407.07288v2",
      "title": "Structural Design Through Reinforcement Learning",
      "title_zh": "基于强化学习的结构设计",
      "authors": [
        "Thomas Rochefort-Beaudoin",
        "Aurelian Vadean",
        "Niels Aage",
        "Sofiane Achiche"
      ],
      "abstract": "This paper introduces the Structural Optimization gym (SOgym), a novel\nopen-source Reinforcement Learning (RL) environment designed to advance machine\nlearning in Topology Optimization (TO). SOgym enables RL agents to generate\nphysically viable and structurally robust designs by integrating the physics of\nTO into the reward function. To enhance scalability, SOgym leverages\nfeature-mapping methods as a mesh-independent interface between the environment\nand the agent, allowing efficient interaction with the design variables\nregardless of mesh resolution. Baseline results use a model-free Proximal\nPolicy Optimization agent and a model-based DreamerV3 agent. Three observation\nspace configurations were tested. The TopOpt game-inspired configuration, an\ninteractive educational tool that improves students' intuition in designing\nstructures to minimize compliance under volume constraints, performed best in\nterms of performance and sample efficiency. The 100M parameter version of\nDreamerV3 produced structures within 54% of the baseline compliance achieved by\ntraditional optimization methods and a 0% disconnection rate, an improvement\nover supervised learning approaches that often struggle with disconnected load\npaths. When comparing the learning rates of the agents to those of engineering\nstudents from the TopOpt game experiment, the DreamerV3-100M model shows a\nlearning rate approximately four orders of magnitude lower, an impressive feat\nfor a policy trained from scratch through trial and error. These results\nsuggest RL's potential to solve continuous TO problems and its capacity to\nexplore and learn from diverse design solutions. SOgym provides a platform for\ndeveloping RL agents for complex structural design challenges and is publicly\navailable to support further research in the field.",
      "tldr_zh": "本文提出 Structural Optimization gym (SOgym)，一个开源的 Reinforcement Learning (RL) 环境，用于推进机器学习在 Topology Optimization (TO) 中的应用。该环境通过将 TO 的物理学整合到奖励函数，并采用特征映射方法作为网格无关接口，提升了 RL 代理生成物理可行结构的设计效率。实验使用 Proximal Policy Optimization (PPO) 和 DreamerV3 代理进行测试，其中受 TopOpt 游戏启发的观察空间配置表现出最佳性能和样本效率。结果显示，DreamerV3-100M 模型生成的结构顺应性达到传统优化方法的 54%，并实现 0% 断开率，其学习率比工程学生低四个数量级，证明 RL 在解决连续 TO 问题和探索多样设计上的潜力。SOgym 作为公开平台，将支持更多结构设计研究的进展。",
      "categories": [
        "cs.AI",
        "68T07 (Primary), 74P05 (Secondary)",
        "J.2; J.6; I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.07288v2",
      "published_date": "2024-07-10 00:38:08 UTC",
      "updated_date": "2024-07-12 14:31:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T05:31:30.159808"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 99,
  "processed_papers_count": 99,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T05:31:52.324161"
}
[
  {
    "arxiv_id": "2407.08093v1",
    "title": "MemWarp: Discontinuity-Preserving Cardiac Registration with Memorized Anatomical Filters",
    "authors": [
      "Hang Zhang",
      "Xiang Chen",
      "Renjiu Hu",
      "Dongdong Liu",
      "Gaolei Li",
      "Rongguang Wang"
    ],
    "abstract": "Many existing learning-based deformable image registration methods impose\nconstraints on deformation fields to ensure they are globally smooth and\ncontinuous. However, this assumption does not hold in cardiac image\nregistration, where different anatomical regions exhibit asymmetric motions\nduring respiration and movements due to sliding organs within the chest.\nConsequently, such global constraints fail to accommodate local discontinuities\nacross organ boundaries, potentially resulting in erroneous and unrealistic\ndisplacement fields. In this paper, we address this issue with MemWarp, a\nlearning framework that leverages a memory network to store prototypical\ninformation tailored to different anatomical regions. MemWarp is different from\nearlier approaches in two main aspects: firstly, by decoupling feature\nextraction from similarity matching in moving and fixed images, it facilitates\nmore effective utilization of feature maps; secondly, despite its capability to\npreserve discontinuities, it eliminates the need for segmentation masks during\nmodel inference. In experiments on a publicly available cardiac dataset, our\nmethod achieves considerable improvements in registration accuracy and\nproducing realistic deformations, outperforming state-of-the-art methods with a\nremarkable 7.1\\% Dice score improvement over the runner-up semi-supervised\nmethod. Source code will be available at https://github.com/tinymilky/Mem-Warp.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 2 figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.08093v1",
    "published_date": "2024-07-10 23:42:29 UTC",
    "updated_date": "2024-07-10 23:42:29 UTC"
  },
  {
    "arxiv_id": "2407.08073v1",
    "title": "NDST: Neural Driving Style Transfer for Human-Like Vision-Based Autonomous Driving",
    "authors": [
      "Donghyun Kim",
      "Aws Khalil",
      "Haewoon Nam",
      "Jaerock Kwon"
    ],
    "abstract": "Autonomous Vehicles (AV) and Advanced Driver Assistant Systems (ADAS)\nprioritize safety over comfort. The intertwining factors of safety and comfort\nemerge as pivotal elements in ensuring the effectiveness of Autonomous Driving\n(AD). Users often experience discomfort when AV or ADAS drive the vehicle on\ntheir behalf. Providing a personalized human-like AD experience, tailored to\nmatch users' unique driving styles while adhering to safety prerequisites,\npresents a significant opportunity to boost the acceptance of AVs. This paper\nproposes a novel approach, Neural Driving Style Transfer (NDST), inspired by\nNeural Style Transfer (NST), to address this issue. NDST integrates a\nPersonalized Block (PB) into the conventional Baseline Driving Model (BDM),\nallowing for the transfer of a user's unique driving style while adhering to\nsafety parameters. The PB serves as a self-configuring system, learning and\nadapting to an individual's driving behavior without requiring modifications to\nthe BDM. This approach enables the personalization of AV models, aligning the\ndriving style more closely with user preferences while ensuring baseline safety\ncritical actuation. Two contrasting driving styles (Style A and Style B) were\nused to validate the proposed NDST methodology, demonstrating its efficacy in\ntransferring personal driving styles to the AV system. Our work highlights the\npotential of NDST to enhance user comfort in AVs by providing a personalized\nand familiar driving experience. The findings affirm the feasibility of\nintegrating NDST into existing AV frameworks to bridge the gap between safety\nand individualized driving styles, promoting wider acceptance and improved user\nexperiences.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08073v1",
    "published_date": "2024-07-10 22:26:45 UTC",
    "updated_date": "2024-07-10 22:26:45 UTC"
  },
  {
    "arxiv_id": "2407.08067v1",
    "title": "On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments",
    "authors": [
      "Jingchao Fang",
      "Nikos Arechiga",
      "Keiichi Namaoshi",
      "Nayeli Bravo",
      "Candice Hogan",
      "David A. Shamma"
    ],
    "abstract": "The Wizard of Oz (WoZ) method is a widely adopted research approach where a\nhuman Wizard ``role-plays'' a not readily available technology and interacts\nwith participants to elicit user behaviors and probe the design space. With the\ngrowing ability for modern large language models (LLMs) to role-play, one can\napply LLMs as Wizards in WoZ experiments with better scalability and lower cost\nthan the traditional approach. However, methodological guidance on responsibly\napplying LLMs in WoZ experiments and a systematic evaluation of LLMs'\nrole-playing ability are lacking. Through two LLM-powered WoZ studies, we take\nthe first step towards identifying an experiment lifecycle for researchers to\nsafely integrate LLMs into WoZ experiments and interpret data generated from\nsettings that involve Wizards role-played by LLMs. We also contribute a\nheuristic-based evaluation framework that allows the estimation of LLMs'\nrole-playing ability in WoZ experiments and reveals LLMs' behavior patterns at\nscale.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.m; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in ACM IVA 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08067v1",
    "published_date": "2024-07-10 22:05:56 UTC",
    "updated_date": "2024-07-10 22:05:56 UTC"
  },
  {
    "arxiv_id": "2407.08065v1",
    "title": "Towards Interpretable Foundation Models of Robot Behavior: A Task Specific Policy Generation Approach",
    "authors": [
      "Isaac Sheidlower",
      "Reuben Aronson",
      "Elaine Schaertl Short"
    ],
    "abstract": "Foundation models are a promising path toward general-purpose and\nuser-friendly robots. The prevalent approach involves training a generalist\npolicy that, like a reinforcement learning policy, uses observations to output\nactions. Although this approach has seen much success, several concerns arise\nwhen considering deployment and end-user interaction with these systems. In\nparticular, the lack of modularity between tasks means that when model weights\nare updated (e.g., when a user provides feedback), the behavior in other,\nunrelated tasks may be affected. This can negatively impact the system's\ninterpretability and usability. We present an alternative approach to the\ndesign of robot foundation models, Diffusion for Policy Parameters (DPP), which\ngenerates stand-alone, task-specific policies. Since these policies are\ndetached from the foundation model, they are updated only when a user wants,\neither through feedback or personalization, allowing them to gain a high degree\nof familiarity with that policy. We demonstrate a proof-of-concept of DPP in\nsimulation then discuss its limitations and the future of interpretable\nfoundation models.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Short Paper accepted to RLC 2024 Workshop on Training Agents with\n  Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2407.08065v1",
    "published_date": "2024-07-10 21:55:44 UTC",
    "updated_date": "2024-07-10 21:55:44 UTC"
  },
  {
    "arxiv_id": "2407.08064v1",
    "title": "TinyGraph: Joint Feature and Node Condensation for Graph Neural Networks",
    "authors": [
      "Yezi Liu",
      "Yanning Shen"
    ],
    "abstract": "Training graph neural networks (GNNs) on large-scale graphs can be\nchallenging due to the high computational expense caused by the massive number\nof nodes and high-dimensional nodal features. Existing graph condensation\nstudies tackle this problem only by reducing the number of nodes in the graph.\nHowever, the resulting condensed graph data can still be cumbersome.\nSpecifically, although the nodes of the Citeseer dataset are reduced to 0.9%\n(30 nodes) in training, the number of features is 3,703, severely exceeding the\ntraining sample magnitude. Faced with this challenge, we study the problem of\njoint condensation for both features and nodes in large-scale graphs. This task\nis challenging mainly due to 1) the intertwined nature of the node features and\nthe graph structure calls for the feature condensation solver to be\nstructure-aware; and 2) the difficulty of keeping useful information in the\ncondensed graph. To address these challenges, we propose a novel framework\nTinyGraph, to condense features and nodes simultaneously in graphs.\nSpecifically, we cast the problem as matching the gradients of GNN weights\ntrained on the condensed graph and the gradients obtained from training over\nthe original graph, where the feature condensation is achieved by a trainable\nfunction. The condensed graph obtained by minimizing the matching loss along\nthe training trajectory can henceforth retain critical information in the\noriginal graph. Extensive experiments were carried out to demonstrate the\neffectiveness of the proposed TinyGraph. For example, a GNN trained with\nTinyGraph retains 98.5% and 97.5% of the original test accuracy on the Cora and\nCiteseer datasets, respectively, while significantly reducing the number of\nnodes by 97.4% and 98.2%, and the number of features by 90.0% on both datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08064v1",
    "published_date": "2024-07-10 21:54:12 UTC",
    "updated_date": "2024-07-10 21:54:12 UTC"
  },
  {
    "arxiv_id": "2407.08047v2",
    "title": "Spatial-Temporal Attention Model for Traffic State Estimation with Sparse Internet of Vehicles",
    "authors": [
      "Jianzhe Xue",
      "Dongcheng Yuan",
      "Yu Sun",
      "Tianqi Zhang",
      "Wenchao Xu",
      "Haibo Zhou",
      "Xuemin",
      "Shen"
    ],
    "abstract": "The growing number of connected vehicles offers an opportunity to leverage\ninternet of vehicles (IoV) data for traffic state estimation (TSE) which plays\na crucial role in intelligent transportation systems (ITS). By utilizing only a\nportion of IoV data instead of the entire dataset, the significant overheads\nassociated with collecting and processing large amounts of data can be avoided.\nIn this paper, we introduce a novel framework that utilizes sparse IoV data to\nachieve cost-effective TSE. Particularly, we propose a novel spatial-temporal\nattention model called the convolutional retentive network (CRNet) to improve\nthe TSE accuracy by mining spatial-temporal traffic state correlations. The\nmodel employs the convolutional neural network (CNN) for spatial correlation\naggregation and the retentive network (RetNet) based on the attention mechanism\nto extract temporal correlations. Extensive simulations on a real-world IoV\ndataset validate the advantage of the proposed TSE approach in achieving\naccurate TSE using sparse IoV data, demonstrating its cost effectiveness and\npracticality for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "need further improvement",
    "pdf_url": "http://arxiv.org/pdf/2407.08047v2",
    "published_date": "2024-07-10 20:58:53 UTC",
    "updated_date": "2024-07-15 02:31:15 UTC"
  },
  {
    "arxiv_id": "2407.08044v2",
    "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
    "authors": [
      "Xijie Huang",
      "Zechun Liu",
      "Shih-Yang Liu",
      "Kwang-Ting Cheng"
    ],
    "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient\nFine-Tuning (PEFT)method, significantly enhances the training efficiency by\nupdating only a small portion of the weights in Large Language Models (LLMs).\nRecently, weight-only quantization techniques have also been applied to LoRA\nmethods to reduce the memory footprint of fine-tuning. However, applying\nweight-activation quantization to the LoRA pipeline is under-explored, and we\nobserve substantial performance degradation primarily due to the presence of\nactivation outliers. In this work, we propose RoLoRA, the first LoRA-based\nscheme for effective weight-activation quantization. RoLoRA utilizes rotation\nfor outlier elimination and proposes rotation-aware fine-tuning to preserve the\noutlier-free characteristics in rotated LLMs. Experimental results show RoLoRA\nconsistently improves low-bit LoRA convergence and post-training quantization\nrobustness in weight-activation settings. We evaluate RoLoRA across\nLLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain\nof 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks\ncompared to LoRA baseline. We further demonstrate its effectiveness on Large\nMultimodal Models (LLaVA-1.5-7B). Codes are available at\nhttps://github.com/HuangOwen/RoLoRA",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings, Codes: https://github.com/HuangOwen/RoLoRA,\n  Models:\n  https://huggingface.co/collections/ScarletAce/rolora-66f5f228a90681c7c4512b28",
    "pdf_url": "http://arxiv.org/pdf/2407.08044v2",
    "published_date": "2024-07-10 20:52:18 UTC",
    "updated_date": "2024-09-26 23:47:03 UTC"
  },
  {
    "arxiv_id": "2407.08034v1",
    "title": "Spatial-Temporal Generative AI for Traffic Flow Estimation with Sparse Data of Connected Vehicles",
    "authors": [
      "Jianzhe Xue",
      "Yunting Xu",
      "Dongcheng Yuan",
      "Caoyi Zha",
      "Hongyang Du",
      "Haibo Zhou",
      "Dusit Niyato"
    ],
    "abstract": "Traffic flow estimation (TFE) is crucial for intelligent transportation\nsystems. Traditional TFE methods rely on extensive road sensor networks and\ntypically incur significant costs. Sparse mobile crowdsensing enables a\ncost-effective alternative by utilizing sparsely distributed probe vehicle data\n(PVD) provided by connected vehicles. However, as pointed out by the central\nlimit theorem, the sparsification of PVD leads to the degradation of TFE\naccuracy. In response, this paper introduces a novel and cost-effective TFE\nframework that leverages sparse PVD and improves accuracy by applying the\nspatial-temporal generative artificial intelligence (GAI) framework. Within\nthis framework, the conditional encoder mines spatial-temporal correlations in\nthe initial TFE results derived from averaging vehicle speeds of each region,\nand the generative decoder generates high-quality and accurate TFE outputs.\nAdditionally, the design of the spatial-temporal neural network is discussed,\nwhich is the backbone of the conditional encoder for effectively capturing\nspatial-temporal correlations. The effectiveness of the proposed TFE approach\nis demonstrated through evaluations based on real-world connected vehicle data.\nThe experimental results affirm the feasibility of our sparse PVD-based TFE\nframework and highlight the significant role of the spatial-temporal GAI\nframework in enhancing the accuracy of TFE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08034v1",
    "published_date": "2024-07-10 20:26:04 UTC",
    "updated_date": "2024-07-10 20:26:04 UTC"
  },
  {
    "arxiv_id": "2407.08022v1",
    "title": "Deep Reinforcement Learning for Sequential Combinatorial Auctions",
    "authors": [
      "Sai Srivatsa Ravindranath",
      "Zhe Feng",
      "Di Wang",
      "Manzil Zaheer",
      "Aranyak Mehta",
      "David C. Parkes"
    ],
    "abstract": "Revenue-optimal auction design is a challenging problem with significant\ntheoretical and practical implications. Sequential auction mechanisms, known\nfor their simplicity and strong strategyproofness guarantees, are often limited\nby theoretical results that are largely existential, except for certain\nrestrictive settings. Although traditional reinforcement learning methods such\nas Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) are\napplicable in this domain, they struggle with computational demands and\nconvergence issues when dealing with large and continuous action spaces. In\nlight of this and recognizing that we can model transitions differentiable for\nour settings, we propose using a new reinforcement learning framework tailored\nfor sequential combinatorial auctions that leverages first-order gradients. Our\nextensive evaluations show that our approach achieves significant improvement\nin revenue over both analytical baselines and standard reinforcement learning\nalgorithms. Furthermore, we scale our approach to scenarios involving up to 50\nagents and 50 items, demonstrating its applicability in complex, real-world\nauction settings. As such, this work advances the computational tools available\nfor auction design and contributes to bridging the gap between theoretical\nresults and practical implementations in sequential auction design.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08022v1",
    "published_date": "2024-07-10 20:00:22 UTC",
    "updated_date": "2024-07-10 20:00:22 UTC"
  },
  {
    "arxiv_id": "2407.08003v1",
    "title": "Machine Learning for ALSFRS-R Score Prediction: Making Sense of the Sensor Data",
    "authors": [
      "Ritesh Mehta",
      "Aleksandar Pramov",
      "Shashank Verma"
    ],
    "abstract": "Amyotrophic Lateral Sclerosis (ALS) is characterized as a rapidly progressive\nneurodegenerative disease that presents individuals with limited treatment\noptions in the realm of medical interventions and therapies. The disease\nshowcases a diverse range of onset patterns and progression trajectories,\nemphasizing the critical importance of early detection of functional decline to\nenable tailored care strategies and timely therapeutic interventions. The\npresent investigation, spearheaded by the iDPP@CLEF 2024 challenge, focuses on\nutilizing sensor-derived data obtained through an app. This data is used to\nconstruct various machine learning models specifically designed to forecast the\nadvancement of the ALS Functional Rating Scale-Revised (ALSFRS-R) score,\nleveraging the dataset provided by the organizers. In our analysis, multiple\npredictive models were evaluated to determine their efficacy in handling ALS\nsensor data. The temporal aspect of the sensor data was compressed and\namalgamated using statistical methods, thereby augmenting the interpretability\nand applicability of the gathered information for predictive modeling\nobjectives. The models that demonstrated optimal performance were a naive\nbaseline and ElasticNet regression. The naive model achieved a Mean Absolute\nError (MAE) of 0.20 and a Root Mean Square Error (RMSE) of 0.49, slightly\noutperforming the ElasticNet model, which recorded an MAE of 0.22 and an RMSE\nof 0.50. Our comparative analysis suggests that while the naive approach\nyielded marginally better predictive accuracy, the ElasticNet model provides a\nrobust framework for understanding feature contributions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper submitted to CLEF 2024 CEUR-WS",
    "pdf_url": "http://arxiv.org/pdf/2407.08003v1",
    "published_date": "2024-07-10 19:17:23 UTC",
    "updated_date": "2024-07-10 19:17:23 UTC"
  },
  {
    "arxiv_id": "2407.08001v1",
    "title": "Automated Neural Patent Landscaping in the Small Data Regime",
    "authors": [
      "Tisa Islam Erana",
      "Mark A. Finlayson"
    ],
    "abstract": "Patent landscaping is the process of identifying all patents related to a\nparticular technological area, and is important for assessing various aspects\nof the intellectual property context. Traditionally, constructing patent\nlandscapes is intensely laborious and expensive, and the rapid expansion of\npatenting activity in recent decades has driven an increasing need for\nefficient and effective automated patent landscaping approaches. In particular,\nit is critical that we be able to construct patent landscapes using a minimal\nnumber of labeled examples, as labeling patents for a narrow technology area\nrequires highly specialized (and hence expensive) technical knowledge. We\npresent an automated neural patent landscaping system that demonstrates\nsignificantly improved performance on difficult examples (0.69 $F_1$ on 'hard'\nexamples, versus 0.6 for previously reported systems), and also significant\nimprovements with much less training data (overall 0.75 $F_1$ on as few as 24\nexamples). Furthermore, in evaluating such automated landscaping systems,\nacquiring good data is challenge; we demonstrate a higher-quality training data\ngeneration procedure by merging Abood and Feltenberger's (2018)\n\"seed/anti-seed\" approach with active learning to collect difficult labeled\nexamples near the decision boundary. Using this procedure we created a new\ndataset of labeled AI patents for training and testing. As in prior work we\ncompare our approach with a number of baseline systems, and we release our code\nand data for others to build upon.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08001v1",
    "published_date": "2024-07-10 19:13:37 UTC",
    "updated_date": "2024-07-10 19:13:37 UTC"
  },
  {
    "arxiv_id": "2407.07972v2",
    "title": "Deconstructing What Makes a Good Optimizer for Language Models",
    "authors": [
      "Rosie Zhao",
      "Depen Morwani",
      "David Brandfonbrener",
      "Nikhil Vyas",
      "Sham Kakade"
    ],
    "abstract": "Training language models becomes increasingly expensive with scale, prompting\nnumerous attempts to improve optimization efficiency. Despite these efforts,\nthe Adam optimizer remains the most widely used, due to a prevailing view that\nit is the most effective approach. We aim to compare several optimization\nalgorithms, including SGD, Adafactor, Adam, Lion, and Sophia in the context of\nautoregressive language modeling across a range of model sizes,\nhyperparameters, and architecture variants. Our findings indicate that, except\nfor SGD, these algorithms all perform comparably both in their optimal\nperformance and also in terms of how they fare across a wide range of\nhyperparameter choices. Our results suggest to practitioners that the choice of\noptimizer can be guided by practical considerations like memory constraints and\nease of implementation, as no single algorithm emerged as a clear winner in\nterms of performance or stability to hyperparameter misspecification. Given our\nfindings, we further dissect these approaches, examining two simplified\nversions of Adam: a) signed momentum (Signum) which we see recovers both the\nperformance and hyperparameter stability of Adam and b) Adalayer, a layerwise\nvariant of Adam which we introduce to study the impact on Adam's\npreconditioning for different layers of the network. Examining Adalayer leads\nus to the conclusion that, perhaps surprisingly, adaptivity on both the last\nlayer and LayerNorm parameters in particular are necessary for retaining\nperformance and stability to learning rate.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.07972v2",
    "published_date": "2024-07-10 18:11:40 UTC",
    "updated_date": "2025-02-28 01:47:44 UTC"
  },
  {
    "arxiv_id": "2407.07966v1",
    "title": "A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities",
    "authors": [
      "Arastoo Zibaeirad",
      "Farnoosh Koleini",
      "Shengping Bi",
      "Tao Hou",
      "Tao Wang"
    ],
    "abstract": "In this study, we conduct a comprehensive review of smart grid security,\nexploring system architectures, attack methodologies, defense strategies, and\nfuture research opportunities. We provide an in-depth analysis of various\nattack vectors, focusing on new attack surfaces introduced by advanced\ncomponents in smart grids. The review particularly includes an extensive\nanalysis of coordinated attacks that incorporate multiple attack strategies and\nexploit vulnerabilities across various smart grid components to increase their\nadverse impact, demonstrating the complexity and potential severity of these\nthreats. Following this, we examine innovative detection and mitigation\nstrategies, including game theory, graph theory, blockchain, and machine\nlearning, discussing their advancements in counteracting evolving threats and\nassociated research challenges. In particular, our review covers a thorough\nexamination of widely used machine learning-based mitigation strategies,\nanalyzing their applications and research challenges spanning across\nsupervised, unsupervised, semi-supervised, ensemble, and reinforcement\nlearning. Further, we outline future research directions and explore new\ntechniques and concerns. We first discuss the research opportunities for\nexisting and emerging strategies, and then explore the potential role of new\ntechniques, such as large language models (LLMs), and the emerging threat of\nadversarial machine learning in the future of smart grid security.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07966v1",
    "published_date": "2024-07-10 18:03:24 UTC",
    "updated_date": "2024-07-10 18:03:24 UTC"
  },
  {
    "arxiv_id": "2407.07950v2",
    "title": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance",
    "authors": [
      "Kaitlyn Zhou",
      "Jena D. Hwang",
      "Xiang Ren",
      "Nouha Dziri",
      "Dan Jurafsky",
      "Maarten Sap"
    ],
    "abstract": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2407.07950v2",
    "published_date": "2024-07-10 18:00:05 UTC",
    "updated_date": "2024-10-03 16:54:59 UTC"
  },
  {
    "arxiv_id": "2407.18940v2",
    "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
    "authors": [
      "Anirudh Ajith",
      "Mengzhou Xia",
      "Alexis Chevalier",
      "Tanya Goyal",
      "Danqi Chen",
      "Tianyu Gao"
    ],
    "abstract": "Literature search questions, such as \"Where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason across\nentire articles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions manually written by authors about their recently published\npapers. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% absolute difference in recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by up to 32 recall\npoints. Taken together, these results show that LitSearch is an informative new\ntestbed for retrieval systems while catering to a real-world use case.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by EMNLP 2024. Dataset and code are available at\n  https://github.com/princeton-nlp/LitSearch",
    "pdf_url": "http://arxiv.org/pdf/2407.18940v2",
    "published_date": "2024-07-10 18:00:03 UTC",
    "updated_date": "2024-10-16 18:37:15 UTC"
  },
  {
    "arxiv_id": "2407.07890v3",
    "title": "Training on the Test Task Confounds Evaluation and Emergence",
    "authors": [
      "Ricardo Dominguez-Olmedo",
      "Florian E. Dorner",
      "Moritz Hardt"
    ],
    "abstract": "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of practices that\nutilize knowledge about evaluation tasks at training time. We demonstrate that\ntraining on the test task confounds both relative model evaluations and claims\nabout emergent capabilities. We argue that the seeming superiority of one model\nfamily over another may be explained by a different degree of training on the\ntest task. To this end, we propose an effective method to adjust for the effect\nof training on the test task on benchmark evaluations. Put simply, to fine-tune\neach model under comparison on the same task-relevant data prior to evaluation.\nWe then show that instances of emergent behavior disappear gradually as models\ntrain on the test task. Our work promotes a new perspective on the evaluation\nof large language models, with broad implications for benchmarking and the\nstudy of emergent capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2407.07890v3",
    "published_date": "2024-07-10 17:57:58 UTC",
    "updated_date": "2025-04-21 16:43:00 UTC"
  },
  {
    "arxiv_id": "2407.11062v2",
    "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language Models",
    "authors": [
      "Mengzhao Chen",
      "Wenqi Shao",
      "Peng Xu",
      "Jiahao Wang",
      "Peng Gao",
      "Kaipeng Zhang",
      "Ping Luo"
    ],
    "abstract": "Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs",
    "pdf_url": "http://arxiv.org/pdf/2407.11062v2",
    "published_date": "2024-07-10 17:53:30 UTC",
    "updated_date": "2024-10-02 13:44:30 UTC"
  },
  {
    "arxiv_id": "2407.07884v1",
    "title": "Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation",
    "authors": [
      "Tao Chen",
      "Eric Cousineau",
      "Naveen Kuppuswamy",
      "Pulkit Agrawal"
    ],
    "abstract": "Recent studies have made significant progress in addressing dexterous\nmanipulation problems, particularly in in-hand object reorientation. However,\nthere are few existing works that explore the potential utilization of\ndeveloped dexterous manipulation controllers for downstream tasks. In this\nstudy, we focus on constrained dexterous manipulation for food peeling. Food\npeeling presents various constraints on the reorientation controller, such as\nthe requirement for the hand to securely hold the object after reorientation\nfor peeling. We propose a simple system for learning a reorientation controller\nthat facilitates the subsequent peeling task. Videos are available at:\nhttps://taochenshh.github.io/projects/veg-peeling.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07884v1",
    "published_date": "2024-07-10 17:51:33 UTC",
    "updated_date": "2024-07-10 17:51:33 UTC"
  },
  {
    "arxiv_id": "2407.07880v2",
    "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
    "authors": [
      "Junkang Wu",
      "Yuexiang Xie",
      "Zhengyi Yang",
      "Jiancan Wu",
      "Jiawei Chen",
      "Jinyang Gao",
      "Bolin Ding",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "abstract": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07880v2",
    "published_date": "2024-07-10 17:48:25 UTC",
    "updated_date": "2025-04-18 08:05:53 UTC"
  },
  {
    "arxiv_id": "2407.07875v2",
    "title": "Generative Image as Action Models",
    "authors": [
      "Mohit Shridhar",
      "Yat Long Lo",
      "Stephen James"
    ],
    "abstract": "Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "CoRL 2024. Website, code, checkpoints:\n  https://genima-robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.07875v2",
    "published_date": "2024-07-10 17:41:10 UTC",
    "updated_date": "2024-10-08 17:43:06 UTC"
  },
  {
    "arxiv_id": "2407.07874v2",
    "title": "Toto: Time Series Optimized Transformer for Observability",
    "authors": [
      "Ben Cohen",
      "Emaad Khwaja",
      "Kan Wang",
      "Charles Masson",
      "Elise Ramé",
      "Youssef Doubli",
      "Othmane Abou-Amal"
    ],
    "abstract": "This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07874v2",
    "published_date": "2024-07-10 17:40:30 UTC",
    "updated_date": "2024-07-11 16:18:40 UTC"
  },
  {
    "arxiv_id": "2407.07868v2",
    "title": "Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation",
    "authors": [
      "Eugene Teoh",
      "Sumit Patidar",
      "Xiao Ma",
      "Stephen James"
    ],
    "abstract": "Generalising vision-based manipulation policies to novel environments remains\na challenging area with limited exploration. Current practices involve\ncollecting data in one location, training imitation learning or reinforcement\nlearning policies with this data, and deploying the policy in the same\nlocation. However, this approach lacks scalability as it necessitates data\ncollection in multiple locations for each task. This paper proposes a novel\napproach where data is collected in a location predominantly featuring green\nscreens. We introduce Green-screen Augmentation (GreenAug), employing a chroma\nkey algorithm to overlay background textures onto a green screen. Through\nextensive real-world empirical studies with over 850 training demonstrations\nand 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no\naugmentation, standard computer vision augmentation, and prior generative\naugmentation methods in performance. While no algorithmic novelties are\nclaimed, our paper advocates for a fundamental shift in data collection\npractices. We propose that real-world demonstrations in future research should\nutilise green screens, followed by the application of GreenAug. We believe\nGreenAug unlocks policy generalisation to visually distinct novel locations,\naddressing the current scene generalisation limitations in robot learning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://greenaug.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.07868v2",
    "published_date": "2024-07-10 17:32:05 UTC",
    "updated_date": "2024-09-08 14:33:45 UTC"
  },
  {
    "arxiv_id": "2407.07848v1",
    "title": "Uncovering Layer-Dependent Activation Sparsity Patterns in ReLU Transformers",
    "authors": [
      "Cody Wild",
      "Jesper Anderson"
    ],
    "abstract": "Previous work has demonstrated that MLPs within ReLU Transformers exhibit\nhigh levels of sparsity, with many of their activations equal to zero for any\ngiven token. We build on that work to more deeply explore how token-level\nsparsity evolves over the course of training, and how it connects to broader\nsparsity patterns over the course of a sequence or batch, demonstrating that\nthe different layers within small transformers exhibit distinctly\nlayer-specific patterns on both of these fronts. In particular, we demonstrate\nthat the first and last layer of the network have distinctive and in many ways\ninverted relationships to sparsity, and explore implications for the structure\nof feature representations being learned at different depths of the model. We\nadditionally explore the phenomenon of ReLU dimensions \"turning off\", and show\nevidence suggesting that \"neuron death\" is being primarily driven by the\ndynamics of training, rather than simply occurring randomly or accidentally as\na result of outliers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07848v1",
    "published_date": "2024-07-10 17:10:10 UTC",
    "updated_date": "2024-07-10 17:10:10 UTC"
  },
  {
    "arxiv_id": "2407.07835v1",
    "title": "RoBus: A Multimodal Dataset for Controllable Road Networks and Building Layouts Generation",
    "authors": [
      "Tao Li",
      "Ruihang Li",
      "Huangnan Zheng",
      "Shanding Ye",
      "Shijian Li",
      "Zhijie Pan"
    ],
    "abstract": "Automated 3D city generation, focusing on road networks and building layouts,\nis in high demand for applications in urban design, multimedia games and\nautonomous driving simulations. The surge of generative AI facilitates\ndesigning city layouts based on deep learning models. However, the lack of\nhigh-quality datasets and benchmarks hinders the progress of these data-driven\nmethods in generating road networks and building layouts. Furthermore, few\nstudies consider urban characteristics, which generally take graphics as\nanalysis objects and are crucial for practical applications, to control the\ngenerative process. To alleviate these problems, we introduce a multimodal\ndataset with accompanying evaluation metrics for controllable generation of\nRoad networks and Building layouts (RoBus), which is the first and largest\nopen-source dataset in city generation so far. RoBus dataset is formatted as\nimages, graphics and texts, with $72,400$ paired samples that cover around\n$80,000km^2$ globally. We analyze the RoBus dataset statistically and validate\nthe effectiveness against existing road networks and building layouts\ngeneration methods. Additionally, we design new baselines that incorporate\nurban characteristics, such as road orientation and building density, in the\nprocess of generating road networks and building layouts using the RoBus\ndataset, enhancing the practicality of automated urban design. The RoBus\ndataset and related codes are published at\nhttps://github.com/tourlics/RoBus_Dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07835v1",
    "published_date": "2024-07-10 16:55:01 UTC",
    "updated_date": "2024-07-10 16:55:01 UTC"
  },
  {
    "arxiv_id": "2407.18939v1",
    "title": "Promoting AI Competencies for Medical Students: A Scoping Review on Frameworks, Programs, and Tools",
    "authors": [
      "Yingbo Ma",
      "Yukyeong Song",
      "Jeremy A. Balch",
      "Yuanfang Ren",
      "Divya Vellanki",
      "Zhenhong Hu",
      "Meghan Brennan",
      "Suraj Kolla",
      "Ziyuan Guan",
      "Brooke Armfield",
      "Tezcan Ozrazgat-Baslanti",
      "Parisa Rashidi",
      "Tyler J. Loftus",
      "Azra Bihorac",
      "Benjamin Shickel"
    ],
    "abstract": "As more clinical workflows continue to be augmented by artificial\nintelligence (AI), AI literacy among physicians will become a critical\nrequirement for ensuring safe and ethical AI-enabled patient care. Despite the\nevolving importance of AI in healthcare, the extent to which it has been\nadopted into traditional and often-overloaded medical curricula is currently\nunknown. In a scoping review of 1,699 articles published between January 2016\nand June 2024, we identified 18 studies which propose guiding frameworks, and\n11 studies documenting real-world instruction, centered around the integration\nof AI into medical education. We found that comprehensive guidelines will\nrequire greater clinical relevance and personalization to suit medical student\ninterests and career trajectories. Current efforts highlight discrepancies in\nthe teaching guidelines, emphasizing AI evaluation and ethics over technical\ntopics such as data science and coding. Additionally, we identified several\nchallenges associated with integrating AI training into the medical education\nprogram, including a lack of guidelines to define medical students AI literacy,\na perceived lack of proven clinical value, and a scarcity of qualified\ninstructors. With this knowledge, we propose an AI literacy framework to define\ncompetencies for medical students. To prioritize relevant and personalized AI\neducation, we categorize literacy into four dimensions: Foundational,\nPractical, Experimental, and Ethical, with tailored learning objectives to the\npre-clinical, clinical, and clinical research stages of medical education. This\nreview provides a road map for developing practical and relevant education\nstrategies for building an AI-competent healthcare workforce.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.18939v1",
    "published_date": "2024-07-10 16:34:41 UTC",
    "updated_date": "2024-07-10 16:34:41 UTC"
  },
  {
    "arxiv_id": "2407.07810v5",
    "title": "Transformer Block Coupling and its Correlation with Generalization in LLMs",
    "authors": [
      "Murdock Aubry",
      "Haoming Meng",
      "Anton Sugolov",
      "Vardan Papyan"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. In this work, we analyze the trajectories\nof token embeddings as they pass through transformer blocks, linearizing the\nsystem along these trajectories through their Jacobian matrices. By examining\nthe relationships between these block Jacobians, we uncover the phenomenon of\n\\textbf{transformer block coupling} in a multitude of LLMs, characterized by\nthe coupling of their top singular vectors across tokens and depth. Our\nfindings reveal that coupling \\textit{positively correlates} with model\nperformance, and that this relationship is stronger than with other\nhyperparameters such as parameter count, model depth, and embedding dimension.\nWe further investigate how these properties emerge during training, observing a\nprogressive development of coupling, increased linearity, and layer-wise\nexponential growth in token trajectories. Additionally, experiments with Vision\nTransformers (ViTs) corroborate the emergence of coupling and its relationship\nwith generalization, reinforcing our findings in LLMs. Collectively, these\ninsights offer a novel perspective on token interactions in transformers,\nopening new directions for studying their mechanisms as well as improving\ntraining and generalization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2407.07810v5",
    "published_date": "2024-07-10 16:30:27 UTC",
    "updated_date": "2025-03-05 04:47:05 UTC"
  },
  {
    "arxiv_id": "2407.07802v1",
    "title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning",
    "authors": [
      "Marawan Gamal Abdel Hameed",
      "Aristides Milios",
      "Siva Reddy",
      "Guillaume Rabusseau"
    ],
    "abstract": "Model training requires significantly more memory, compared with inference.\nParameter efficient fine-tuning (PEFT) methods provide a means of adapting\nlarge models to downstream tasks using less memory. However, existing methods\nsuch as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce\nlatency overhead at inference time or achieve subpar downstream performance\ncompared with full fine-tuning. In this work we propose Random Subspace\nAdaptation (ROSA), a method that outperforms previous PEFT methods by a\nsignificant margin, while maintaining a zero latency overhead during inference\ntime. In contrast to previous methods, ROSA is able to adapt subspaces of\narbitrarily large dimension, better approximating full-finetuning. We\ndemonstrate both theoretically and experimentally that this makes ROSA strictly\nmore expressive than LoRA, without consuming additional memory during runtime.\nAs PEFT methods are especially useful in the natural language processing\ndomain, where models operate on scales that make full fine-tuning very\nexpensive, we evaluate ROSA in two common NLP scenarios: natural language\ngeneration (NLG) and natural language understanding (NLU) with GPT-2 and\nRoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms\nLoRA by a significant margin, while also outperforming LoRA on NLG tasks. Our\ncode is available at https://github.com/rosa-paper/rosa",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07802v1",
    "published_date": "2024-07-10 16:20:53 UTC",
    "updated_date": "2024-07-10 16:20:53 UTC"
  },
  {
    "arxiv_id": "2407.07796v2",
    "title": "Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard",
    "authors": [
      "Oguzhan Topsakal",
      "Colby Jacob Edell",
      "Jackson Bailey Harper"
    ],
    "abstract": "We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07796v2",
    "published_date": "2024-07-10 16:14:34 UTC",
    "updated_date": "2024-07-11 03:46:35 UTC"
  },
  {
    "arxiv_id": "2407.07788v2",
    "title": "BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark",
    "authors": [
      "Nikita Chernyadev",
      "Nicholas Backshall",
      "Xiao Ma",
      "Yunfan Lu",
      "Younggyo Seo",
      "Stephen James"
    ],
    "abstract": "We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project webpage: https://chernyadev.github.io/bigym/",
    "pdf_url": "http://arxiv.org/pdf/2407.07788v2",
    "published_date": "2024-07-10 16:04:18 UTC",
    "updated_date": "2024-07-11 16:26:09 UTC"
  },
  {
    "arxiv_id": "2407.07787v1",
    "title": "Continuous Control with Coarse-to-fine Reinforcement Learning",
    "authors": [
      "Younggyo Seo",
      "Jafar Uruç",
      "Stephen James"
    ],
    "abstract": "Despite recent advances in improving the sample-efficiency of reinforcement\nlearning (RL) algorithms, designing an RL algorithm that can be practically\ndeployed in real-world environments remains a challenge. In this paper, we\npresent Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL\nagents to zoom-into a continuous action space in a coarse-to-fine manner,\nenabling the use of stable, sample-efficient value-based RL algorithms for\nfine-grained continuous control tasks. Our key idea is to train agents that\noutput actions by iterating the procedure of (i) discretizing the continuous\naction space into multiple intervals and (ii) selecting the interval with the\nhighest Q-value to further discretize at the next level. We then introduce a\nconcrete, value-based algorithm within the CRL framework called Coarse-to-fine\nQ-Network (CQN). Our experiments demonstrate that CQN significantly outperforms\nRL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation\ntasks with a modest number of environment interactions and expert\ndemonstrations. We also show that CQN robustly learns to solve real-world\nmanipulation tasks within a few minutes of online training.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project webpage: https://younggyo.me/cqn/",
    "pdf_url": "http://arxiv.org/pdf/2407.07787v1",
    "published_date": "2024-07-10 16:04:08 UTC",
    "updated_date": "2024-07-10 16:04:08 UTC"
  },
  {
    "arxiv_id": "2407.07934v4",
    "title": "Identifying Macro Conditional Independencies and Macro Total Effects in Summary Causal Graphs with Latent Confounding",
    "authors": [
      "Simon Ferreira",
      "Charles K. Assaad"
    ],
    "abstract": "Understanding causal relations in dynamic systems is essential in\nepidemiology. While causal inference methods have been extensively studied,\nthey often rely on fully specified causal graphs, which may not always be\navailable in complex dynamic systems. Partially specified causal graphs, and in\nparticular summary causal graphs (SCGs), provide a simplified representation of\ncausal relations between time series when working spacio-temporal data,\nomitting temporal information and focusing on causal structures between\nclusters of of temporal variables. Unlike fully specified causal graphs, SCGs\ncan contain cycles, which complicate their analysis and interpretation. In\naddition, their cluster-based nature introduces new challenges concerning the\ntypes of queries of interest: macro queries, which involve relationships\nbetween clusters represented as vertices in the graph, and micro queries, which\npertain to relationships between variables that are not directly visible\nthrough the vertices of the graph. In this paper, we first clearly distinguish\nbetween macro conditional independencies and micro conditional independencies\nand between macro total effects and micro total effects. Then, we demonstrate\nthe soundness and completeness of the d-separation to identify macro\nconditional independencies in SCGs. Furthermore, we establish that the\ndo-calculus is sound and complete for identifying macro total effects in SCGs.\nFinally, we give a graphical characterization for the non-identifiability of\nmacro total effects in SCGs.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "primary_category": "stat.ME",
    "comment": "Accepted CI4TS Workshop at UAI2024. Accepted at AAAI25",
    "pdf_url": "http://arxiv.org/pdf/2407.07934v4",
    "published_date": "2024-07-10 16:03:04 UTC",
    "updated_date": "2024-12-20 10:14:54 UTC"
  },
  {
    "arxiv_id": "2407.07786v2",
    "title": "The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing",
    "authors": [
      "Alice Qian Zhang",
      "Ryland Shaw",
      "Jacy Reese Anthis",
      "Ashlee Milton",
      "Emily Tseng",
      "Jina Suh",
      "Lama Ahmad",
      "Ram Shankar Siva Kumar",
      "Julian Posada",
      "Benjamin Shestakofsky",
      "Sarah T. Roberts",
      "Mary L. Gray"
    ],
    "abstract": "Rapid progress in general-purpose AI has sparked significant interest in \"red\nteaming,\" a practice of adversarial testing originating in military and\ncybersecurity applications. AI red teaming raises many questions about the\nhuman factor, such as how red teamers are selected, biases and blindspots in\nhow tests are conducted, and harmful content's psychological effects on red\nteamers. A growing body of HCI and CSCW literature examines related\npractices-including data labeling, content moderation, and algorithmic\nauditing. However, few, if any have investigated red teaming itself. Future\nstudies may explore topics ranging from fairness to mental health and other\nareas of potential harm. We aim to facilitate a community of researchers and\npractitioners who can begin to meet these challenges with creativity,\ninnovation, and thoughtful reflection.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Updated with camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2407.07786v2",
    "published_date": "2024-07-10 16:02:13 UTC",
    "updated_date": "2024-09-11 16:02:31 UTC"
  },
  {
    "arxiv_id": "2407.12862v1",
    "title": "Analyzing Large language models chatbots: An experimental approach using a probability test",
    "authors": [
      "Melise Peruchini",
      "Julio Monteiro Teixeira"
    ],
    "abstract": "This study consists of qualitative empirical research, conducted through\nexploratory tests with two different Large Language Models (LLMs) chatbots:\nChatGPT and Gemini. The methodological procedure involved exploratory tests\nbased on prompts designed with a probability question. The \"Linda Problem\",\nwidely recognized in cognitive psychology, was used as a basis to create the\ntests, along with the development of a new problem specifically for this\nexperiment, the \"Mary Problem\". The object of analysis is the dataset with the\noutputs provided by each chatbot interaction. The purpose of the analysis is to\nverify whether the chatbots mainly employ logical reasoning that aligns with\nprobability theory or if they are more frequently affected by the stereotypical\ntextual descriptions in the prompts. The findings provide insights about the\napproach each chatbot employs in handling logic and textual constructions,\nsuggesting that, while the analyzed chatbots perform satisfactorily on a\nwell-known probabilistic problem, they exhibit significantly lower performance\non new tests that require direct application of probabilistic logic.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 3 figures, Submitted to ACM Transactions on Intelligent\n  systems and Technology",
    "pdf_url": "http://arxiv.org/pdf/2407.12862v1",
    "published_date": "2024-07-10 15:49:40 UTC",
    "updated_date": "2024-07-10 15:49:40 UTC"
  },
  {
    "arxiv_id": "2407.07775v2",
    "title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
    "authors": [
      "Hao-Tien Lewis Chiang",
      "Zhuo Xu",
      "Zipeng Fu",
      "Mithun George Jacob",
      "Tingnan Zhang",
      "Tsang-Wei Edward Lee",
      "Wenhao Yu",
      "Connor Schenck",
      "David Rendleman",
      "Dhruv Shah",
      "Fei Xia",
      "Jasmine Hsu",
      "Jonathan Hoech",
      "Pete Florence",
      "Sean Kirmani",
      "Sumeet Singh",
      "Vikas Sindhwani",
      "Carolina Parada",
      "Chelsea Finn",
      "Peng Xu",
      "Sergey Levine",
      "Jie Tan"
    ],
    "abstract": "An elusive goal in navigation research is to build an intelligent agent that\ncan understand multimodal instructions including natural language and image,\nand perform useful navigation. To achieve this, we study a widely useful\ncategory of navigation tasks we call Multimodal Instruction Navigation with\ndemonstration Tours (MINT), in which the environment prior is provided through\na previously recorded demonstration video. Recent advances in Vision Language\nModels (VLMs) have shown a promising path in achieving this goal as it\ndemonstrates capabilities in perceiving and reasoning about multimodal inputs.\nHowever, VLMs are typically trained to predict textual output and it is an open\nresearch question about how to best utilize them in navigation. To solve MINT,\nwe present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation\npolicy that combines the environment understanding and common sense reasoning\npower of long-context VLMs and a robust low-level navigation policy based on\ntopological graphs. The high-level policy consists of a long-context VLM that\ntakes the demonstration tour video and the multimodal user instruction as input\nto find the goal frame in the tour video. Next, a low-level policy uses the\ngoal frame and an offline constructed topological graph to generate robot\nactions at every timestep. We evaluated Mobility VLA in a 836m^2 real world\nenvironment and show that Mobility VLA has a high end-to-end success rates on\npreviously unsolved multimodal instructions such as \"Where should I return\nthis?\" while holding a plastic bin. A video demonstrating Mobility VLA can be\nfound here: https://youtu.be/-Tof__Q8_5s",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07775v2",
    "published_date": "2024-07-10 15:49:07 UTC",
    "updated_date": "2024-07-12 14:37:08 UTC"
  },
  {
    "arxiv_id": "2407.07760v2",
    "title": "Learning Spatial-Semantic Features for Robust Video Object Segmentation",
    "authors": [
      "Xin Li",
      "Deshui Miao",
      "Zhenyu He",
      "Yaowei Wang",
      "Huchuan Lu",
      "Ming-Hsuan Yang"
    ],
    "abstract": "Tracking and segmenting multiple similar objects with distinct or complex\nparts in long-term videos is particularly challenging due to the ambiguity in\nidentifying target components and the confusion caused by occlusion, background\nclutter, and changes in appearance or environment over time. In this paper, we\npropose a robust video object segmentation framework that learns\nspatial-semantic features and discriminative object queries to address the\nabove issues. Specifically, we construct a spatial-semantic block comprising a\nsemantic embedding component and a spatial dependency modeling part for\nassociating global semantic features and local spatial features, providing a\ncomprehensive target representation. In addition, we develop a masked\ncross-attention module to generate object queries that focus on the most\ndiscriminative parts of target objects during query propagation, alleviating\nnoise accumulation to ensure effective long-term query propagation. Extensive\nexperimental results show that the proposed method achieves state-of-the-art\nperformance on benchmark data sets, including the DAVIS2017 test\n(\\textbf{87.8\\%}), YoutubeVOS 2019 (\\textbf{88.1\\%}), MOSE val\n(\\textbf{74.0\\%}), and LVOS test (\\textbf{73.0\\%}), and demonstrate the\neffectiveness and generalization capacity of our model. The source code and\ntrained models are released at\n\\href{https://github.com/yahooo-m/S3}{https://github.com/yahooo-m/S3}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published as a conference paper in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.07760v2",
    "published_date": "2024-07-10 15:36:00 UTC",
    "updated_date": "2025-04-07 07:55:21 UTC"
  },
  {
    "arxiv_id": "2407.07755v3",
    "title": "Neural Geometry Processing via Spherical Neural Surfaces",
    "authors": [
      "Romy Williamson",
      "Niloy J. Mitra"
    ],
    "abstract": "Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. In the absence of an analogous toolbox, neural\nrepresentations are typically discretized and converted into a mesh, before\napplying any geometry processing algorithm. This is unsatisfactory and, as we\ndemonstrate, unnecessary. In this work, we propose a spherical neural surface\nrepresentation for genus-0 surfaces and demonstrate how to compute core\ngeometric operators directly on this representation. Namely, we estimate\nsurface normals and first and second fundamental forms of the surface, as well\nas compute surface gradient, surface divergence and Laplace-Beltrami operator\non scalar/vector fields defined on the surface. Our representation is fully\nseamless, overcoming a key limitation of similar explicit representations such\nas Neural Surface Maps [Morreale et al. 2021]. These operators, in turn, enable\ngeometry processing directly on the neural representations without any\nunnecessary meshing. We demonstrate illustrative applications in (neural)\nspectral analysis, heat flow and mean curvature flow, and evaluate robustness\nto isometric shape variations. We propose theoretical formulations and validate\ntheir numerical estimates, against analytical estimates, mesh-based baselines,\nand neural alternatives, where available. By systematically linking neural\nsurface representations with classical geometry processing algorithms, we\nbelieve that this work can become a key ingredient in enabling neural geometry\nprocessing. Code is accessible from the project webpage.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "I.3.5"
    ],
    "primary_category": "cs.GR",
    "comment": "14 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.07755v3",
    "published_date": "2024-07-10 15:28:02 UTC",
    "updated_date": "2025-03-14 18:11:29 UTC"
  },
  {
    "arxiv_id": "2407.08658v1",
    "title": "Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks",
    "authors": [
      "Lucca Emmanuel Pineli Simões",
      "Lucas Brandão Rodrigues",
      "Rafaela Mota Silva",
      "Gustavo Rodrigues da Silva"
    ],
    "abstract": "This paper presents the development and comparative evaluation of three voice\ncommand pipelines for controlling a Tello drone, using speech recognition and\ndeep learning techniques. The aim is to enhance human-machine interaction by\nenabling intuitive voice control of drone actions. The pipelines developed\ninclude: (1) a traditional Speech-to-Text (STT) followed by a Large Language\nModel (LLM) approach, (2) a direct voice-to-function mapping model, and (3) a\nSiamese neural network-based system. Each pipeline was evaluated based on\ninference time, accuracy, efficiency, and flexibility. Detailed methodologies,\ndataset preparation, and evaluation metrics are provided, offering a\ncomprehensive analysis of each pipeline's strengths and applicability across\ndifferent scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08658v1",
    "published_date": "2024-07-10 15:15:26 UTC",
    "updated_date": "2024-07-10 15:15:26 UTC"
  },
  {
    "arxiv_id": "2408.00002v2",
    "title": "Transfer Learning for Wildlife Classification: Evaluating YOLOv8 against DenseNet, ResNet, and VGGNet on a Custom Dataset",
    "authors": [
      "Subek Sharma",
      "Sisir Dhakal",
      "Mansi Bhavsar"
    ],
    "abstract": "This study evaluates the performance of various deep learning models,\nspecifically DenseNet, ResNet, VGGNet, and YOLOv8, for wildlife species\nclassification on a custom dataset. The dataset comprises 575 images of 23\nendangered species sourced from reputable online repositories. The study\nutilizes transfer learning to fine-tune pre-trained models on the dataset,\nfocusing on reducing training time and enhancing classification accuracy. The\nresults demonstrate that YOLOv8 outperforms other models, achieving a training\naccuracy of 97.39% and a validation F1-score of 96.50%. These findings suggest\nthat YOLOv8, with its advanced architecture and efficient feature extraction\ncapabilities, holds great promise for automating wildlife monitoring and\nconservation efforts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This is published in Journal of Artificial Intelligence and Capsule\n  Networks, December 2024, Volume 6, Issue 4, Pages 415-435",
    "pdf_url": "http://arxiv.org/pdf/2408.00002v2",
    "published_date": "2024-07-10 15:03:00 UTC",
    "updated_date": "2024-11-12 14:55:50 UTC"
  },
  {
    "arxiv_id": "2407.07728v5",
    "title": "SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature Disentanglement and Enhancement",
    "authors": [
      "Zihao Wang",
      "Le Ma",
      "Yongsheng Feng",
      "Xin Pan",
      "Yuhang Jin",
      "Kejun Zhang"
    ],
    "abstract": "Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,\ndataset, and documents of SaMoye are publicly available on\n\\url{https://github.com/CarlWangChina/SaMoye-SVC}.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "68Txx(Primary)14F05, 91Fxx(Secondary)",
      "I.2.7; J.5"
    ],
    "primary_category": "cs.SD",
    "comment": "This paper needs major changes for resubmit",
    "pdf_url": "http://arxiv.org/pdf/2407.07728v5",
    "published_date": "2024-07-10 15:00:08 UTC",
    "updated_date": "2024-11-15 07:51:08 UTC"
  },
  {
    "arxiv_id": "2407.07726v2",
    "title": "PaliGemma: A versatile 3B VLM for transfer",
    "authors": [
      "Lucas Beyer",
      "Andreas Steiner",
      "André Susano Pinto",
      "Alexander Kolesnikov",
      "Xiao Wang",
      "Daniel Salz",
      "Maxim Neumann",
      "Ibrahim Alabdulmohsin",
      "Michael Tschannen",
      "Emanuele Bugliarello",
      "Thomas Unterthiner",
      "Daniel Keysers",
      "Skanda Koppula",
      "Fangyu Liu",
      "Adam Grycner",
      "Alexey Gritsenko",
      "Neil Houlsby",
      "Manoj Kumar",
      "Keran Rong",
      "Julian Eisenschlos",
      "Rishabh Kabra",
      "Matthias Bauer",
      "Matko Bošnjak",
      "Xi Chen",
      "Matthias Minderer",
      "Paul Voigtlaender",
      "Ioana Bica",
      "Ivana Balazevic",
      "Joan Puigcerver",
      "Pinelopi Papalampidi",
      "Olivier Henaff",
      "Xi Xiong",
      "Radu Soricut",
      "Jeremiah Harmsen",
      "Xiaohua Zhai"
    ],
    "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "v2 adds Appendix H and I and a few citations",
    "pdf_url": "http://arxiv.org/pdf/2407.07726v2",
    "published_date": "2024-07-10 14:57:46 UTC",
    "updated_date": "2024-10-10 17:28:23 UTC"
  },
  {
    "arxiv_id": "2407.07695v1",
    "title": "African Democracy in the Era of Generative Disinformation: Challenges and Countermeasures against AI-Generated Propaganda",
    "authors": [
      "Chinasa T. Okolo"
    ],
    "abstract": "In light of prominent discourse around the negative implications of\ngenerative AI, an emerging area of research is investigating the current and\nestimated impacts of AI-generated propaganda on African citizens participating\nin elections. Throughout Africa, there have already been suspected cases of\nAI-generated propaganda influencing electoral outcomes or precipitating coups\nin countries like Nigeria, Burkina Faso, and Gabon, underscoring the need for\ncomprehensive research in this domain. This paper aims to highlight the risks\nassociated with the spread of generative AI-driven disinformation within Africa\nwhile concurrently examining the roles of government, civil society, academia,\nand the general public in the responsible development, practical use, and\nrobust governance of AI. To understand how African governments might\neffectively counteract the impact of AI-generated propaganda, this paper\npresents case studies illustrating the current usage of generative AI for\nelection-related propaganda in Africa. Subsequently, this paper discusses\nefforts by fact-checking organisations to mitigate the negative impacts of\ndisinformation, explores the potential for new initiatives to actively engage\ncitizens in literacy efforts to combat disinformation spread, and advocates for\nincreased governmental regulatory measures. Overall, this research seeks to\nincrease comprehension of the potential ramifications of AI-generated\npropaganda on democratic processes within Africa and propose actionable\nstrategies for stakeholders to address these multifaceted challenges.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; K.4.1"
    ],
    "primary_category": "cs.CY",
    "comment": "Building a Just AI Ecosystem in Africa Conference Cape Town, South\n  Africa",
    "pdf_url": "http://arxiv.org/pdf/2407.07695v1",
    "published_date": "2024-07-10 14:24:53 UTC",
    "updated_date": "2024-07-10 14:24:53 UTC"
  },
  {
    "arxiv_id": "2407.07684v2",
    "title": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle Control",
    "authors": [
      "Elahe Delavari",
      "John Moore",
      "Junho Hong",
      "Jaerock Kwon"
    ],
    "abstract": "This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "The work is partly supported by a sponsor. Authors need to complete\n  the final report submission before any type of publication according to the\n  sponsor. The final report will be submitted in few weeks. This work has been\n  superseded by arXiv:2503.01676",
    "pdf_url": "http://arxiv.org/pdf/2407.07684v2",
    "published_date": "2024-07-10 14:08:27 UTC",
    "updated_date": "2024-09-16 16:02:46 UTC"
  },
  {
    "arxiv_id": "2407.07671v1",
    "title": "Why should we ever automate moral decision making?",
    "authors": [
      "Vincent Conitzer"
    ],
    "abstract": "While people generally trust AI to make decisions in various aspects of their\nlives, concerns arise when AI is involved in decisions with significant moral\nimplications. The absence of a precise mathematical framework for moral\nreasoning intensifies these concerns, as ethics often defies simplistic\nmathematical models. Unlike fields such as logical reasoning, reasoning under\nuncertainty, and strategic decision-making, which have well-defined\nmathematical frameworks, moral reasoning lacks a broadly accepted framework.\nThis absence raises questions about the confidence we can place in AI's moral\ndecision-making capabilities.\n  The environments in which AI systems are typically trained today seem\ninsufficiently rich for such a system to learn ethics from scratch, and even if\nwe had an appropriate environment, it is unclear how we might bring about such\nlearning. An alternative approach involves AI learning from human moral\ndecisions. This learning process can involve aggregating curated human\njudgments or demonstrations in specific domains, or leveraging a foundation\nmodel fed with a wide range of data. Still, concerns persist, given the\nimperfections in human moral decision making.\n  Given this, why should we ever automate moral decision making -- is it not\nbetter to leave all moral decision making to humans? This paper lays out a\nnumber of reasons why we should expect AI systems to engage in decisions with a\nmoral component, with brief discussions of the associated risks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07671v1",
    "published_date": "2024-07-10 13:59:22 UTC",
    "updated_date": "2024-07-10 13:59:22 UTC"
  },
  {
    "arxiv_id": "2407.07668v2",
    "title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning",
    "authors": [
      "Giuseppe Serra",
      "Ben Werner",
      "Florian Buettner"
    ],
    "abstract": "Many real-world applications require machine-learning models to be able to\ndeal with non-stationary data distributions and thus learn autonomously over an\nextended period of time, often in an online setting. One of the main challenges\nin this scenario is the so-called catastrophic forgetting (CF) for which the\nlearning model tends to focus on the most recent tasks while experiencing\npredictive degradation on older ones. In the online setting, the most effective\nsolutions employ a fixed-size memory buffer to store old samples used for\nreplay when training on new tasks. Many approaches have been presented to\ntackle this problem. However, it is not clear how predictive uncertainty\ninformation for memory management can be leveraged in the most effective manner\nand conflicting strategies are proposed to populate the memory. Are the\neasiest-to-forget or the easiest-to-remember samples more effective in\ncombating CF? Starting from the intuition that predictive uncertainty provides\nan idea of the samples' location in the decision space, this work presents an\nin-depth analysis of different uncertainty estimates and strategies for\npopulating the memory. The investigation provides a better understanding of the\ncharacteristics data points should have for alleviating CF. Then, we propose an\nalternative method for estimating predictive uncertainty via the generalised\nvariance induced by the negative log-likelihood. Finally, we demonstrate that\nthe use of predictive uncertainty measures helps in reducing CF in different\nsettings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2405.18925",
    "pdf_url": "http://arxiv.org/pdf/2407.07668v2",
    "published_date": "2024-07-10 13:51:15 UTC",
    "updated_date": "2024-10-10 10:34:08 UTC"
  },
  {
    "arxiv_id": "2407.15861v2",
    "title": "Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey",
    "authors": [
      "Chenyu Zhang",
      "Mingwang Hu",
      "Wenhui Li",
      "Lanjun Wang"
    ],
    "abstract": "Recently, the text-to-image diffusion model has gained considerable attention\nfrom the community due to its exceptional image generation capability. A\nrepresentative model, Stable Diffusion, amassed more than 10 million users\nwithin just two months of its release. This surge in popularity has facilitated\nstudies on the robustness and safety of the model, leading to the proposal of\nvarious adversarial attack methods. Simultaneously, there has been a marked\nincrease in research focused on defense methods to improve the robustness and\nsafety of these models. In this survey, we provide a comprehensive review of\nthe literature on adversarial attacks and defenses targeting text-to-image\ndiffusion models. We begin with an overview of text-to-image diffusion models,\nfollowed by an introduction to a taxonomy of adversarial attacks and an\nin-depth review of existing attack methods. We then present a detailed analysis\nof current defense methods that improve model robustness and safety. Finally,\nwe discuss ongoing challenges and explore promising future research directions.\nFor a complete list of the adversarial attack and defense methods covered in\nthis survey, please refer to our curated repository at\nhttps://github.com/datar001/Awesome-AD-on-T2IDM.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted for Information Fusion. Related benchmarks and codes are\n  available at \\url{https://github.com/datar001/Awesome-AD-on-T2IDM}",
    "pdf_url": "http://arxiv.org/pdf/2407.15861v2",
    "published_date": "2024-07-10 13:50:31 UTC",
    "updated_date": "2024-09-13 04:11:30 UTC"
  },
  {
    "arxiv_id": "2407.07666v1",
    "title": "A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models : Safety, Consensus, Objectivity, Reproducibility and Explainability",
    "authors": [
      "Ting Fang Tan",
      "Kabilan Elangovan",
      "Jasmine Ong",
      "Nigam Shah",
      "Joseph Sung",
      "Tien Yin Wong",
      "Lan Xue",
      "Nan Liu",
      "Haibo Wang",
      "Chang Fu Kuo",
      "Simon Chesterman",
      "Zee Kin Yeong",
      "Daniel SW Ting"
    ],
    "abstract": "A comprehensive qualitative evaluation framework for large language models\n(LLM) in healthcare that expands beyond traditional accuracy and quantitative\nmetrics needed. We propose 5 key aspects for evaluation of LLMs: Safety,\nConsensus, Objectivity, Reproducibility and Explainability (S.C.O.R.E.). We\nsuggest that S.C.O.R.E. may form the basis for an evaluation framework for\nfuture LLM-based models that are safe, reliable, trustworthy, and ethical for\nhealthcare and clinical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07666v1",
    "published_date": "2024-07-10 13:45:16 UTC",
    "updated_date": "2024-07-10 13:45:16 UTC"
  },
  {
    "arxiv_id": "2407.07664v2",
    "title": "A Coding-Theoretic Analysis of Hyperspherical Prototypical Learning Geometry",
    "authors": [
      "Martin Lindström",
      "Borja Rodríguez-Gálvez",
      "Ragnar Thobaben",
      "Mikael Skoglund"
    ],
    "abstract": "Hyperspherical Prototypical Learning (HPL) is a supervised approach to\nrepresentation learning that designs class prototypes on the unit hypersphere.\nThe prototypes bias the representations to class separation in a scale\ninvariant and known geometry. Previous approaches to HPL have either of the\nfollowing shortcomings: (i) they follow an unprincipled optimisation procedure;\nor (ii) they are theoretically sound, but are constrained to only one possible\nlatent dimension. In this paper, we address both shortcomings. To address (i),\nwe present a principled optimisation procedure whose solution we show is\noptimal. To address (ii), we construct well-separated prototypes in a wide\nrange of dimensions using linear block codes. Additionally, we give a full\ncharacterisation of the optimal prototype placement in terms of achievable and\nconverse bounds, showing that our proposed methods are near-optimal.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Changes in version 2: Minor formatting changes. Published in the\n  Proceedings of the Geometry-grounded Representation Learning and Generative\n  Modeling Workshop (GRaM), PMLR 251. Available at:\n  https://proceedings.mlr.press/v251/lindstrom24a.html 14 pages: 9 of the main\n  paper, 2 of references, and 3 of appendices.. Code is available at:\n  https://github.com/martinlindstrom/coding_theoretic_hpl",
    "pdf_url": "http://arxiv.org/pdf/2407.07664v2",
    "published_date": "2024-07-10 13:44:19 UTC",
    "updated_date": "2025-04-17 16:04:48 UTC"
  },
  {
    "arxiv_id": "2407.07660v1",
    "title": "Boosting Medical Image Synthesis via Registration-guided Consistency and Disentanglement Learning",
    "authors": [
      "Chuanpu Li",
      "Zeli Chen",
      "Yiwen Zhang",
      "Liming Zhong",
      "Wei Yang"
    ],
    "abstract": "Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07660v1",
    "published_date": "2024-07-10 13:41:26 UTC",
    "updated_date": "2024-07-10 13:41:26 UTC"
  },
  {
    "arxiv_id": "2407.07639v1",
    "title": "Explaining Graph Neural Networks for Node Similarity on Graphs",
    "authors": [
      "Daniel Daza",
      "Cuong Xuan Chu",
      "Trung-Kien Tran",
      "Daria Stepanova",
      "Michael Cochez",
      "Paul Groth"
    ],
    "abstract": "Similarity search is a fundamental task for exploiting information in various\napplications dealing with graph data, such as citation networks or knowledge\ngraphs. While this task has been intensively approached from heuristics to\ngraph embeddings and graph neural networks (GNNs), providing explanations for\nsimilarity has received less attention. In this work we are concerned with\nexplainable similarity search over graphs, by investigating how GNN-based\nmethods for computing node similarities can be augmented with explanations.\nSpecifically, we evaluate the performance of two prominent approaches towards\nexplanations in GNNs, based on the concepts of mutual information (MI), and\ngradient-based explanations (GB). We discuss their suitability and empirically\nvalidate the properties of their explanations over different popular graph\nbenchmarks. We find that unlike MI explanations, gradient-based explanations\nhave three desirable properties. First, they are actionable: selecting inputs\ndepending on them results in predictable changes in similarity scores. Second,\nthey are consistent: the effect of selecting certain inputs overlaps very\nlittle with the effect of discarding them. Third, they can be pruned\nsignificantly to obtain sparse explanations that retain the effect on\nsimilarity scores.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07639v1",
    "published_date": "2024-07-10 13:20:47 UTC",
    "updated_date": "2024-07-10 13:20:47 UTC"
  },
  {
    "arxiv_id": "2407.07638v3",
    "title": "Tuning Vision-Language Models with Candidate Labels by Prompt Alignment",
    "authors": [
      "Zhifang Zhang",
      "Yuwei Niu",
      "Xin Liu",
      "Beibei Li"
    ],
    "abstract": "Vision-language models (VLMs) can learn high-quality representations from a\nlarge-scale training dataset of image-text pairs. Prompt learning is a popular\napproach to fine-tuning VLM to adapt them to downstream tasks. Despite the\nsatisfying performance, a major limitation of prompt learning is the demand for\nlabelled data. In real-world scenarios, we may only obtain candidate labels\n(where the true label is included) instead of the true labels due to data\nprivacy or sensitivity issues. In this paper, we provide the first study on\nprompt learning with candidate labels for VLMs. We empirically demonstrate that\nprompt learning is more advantageous than other fine-tuning methods, for\nhandling candidate labels. Nonetheless, its performance drops when the label\nambiguity increases. In order to improve its robustness, we propose a simple\nyet effective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our framework\ndisambiguates candidate labels by aligning the model output with the mixed\nclass posterior jointly predicted by both the learnable and the handcrafted\nprompt. Besides, our framework can be equipped with various off-the-shelf\ntraining objectives for learning with candidate labels to further improve their\nperformance. Extensive experiments demonstrate the effectiveness of our\nproposed framework.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07638v3",
    "published_date": "2024-07-10 13:19:31 UTC",
    "updated_date": "2024-12-29 08:17:33 UTC"
  },
  {
    "arxiv_id": "2407.07612v2",
    "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
    "authors": [
      "Aniket Vashishtha",
      "Abhinav Kumar",
      "Atharva Pandey",
      "Abbavaram Gowtham Reddy",
      "Kabir Ahuja",
      "Vineeth N Balasubramanian",
      "Amit Sharma"
    ],
    "abstract": "For text-based AI systems to interact in the real world, causal reasoning is\nan essential skill. Since active interventions are costly, we study to what\nextent a system can learn causal reasoning from symbolic demonstrations of\ncausal axioms. Specifically, we present an axiomatic training method where the\nsystem learns from multiple demonstrations of a causal axiom (or rule), rather\nthan incorporating the axiom as an inductive bias or inferring it from data\nvalues. A key question is whether the system would learn to generalize from the\naxiom demonstrations to more complex scenarios. Our results, based on applying\naxiomatic training to learn the transitivity axiom and d-separation rule,\nindicate that such generalization is possible. To avoid data contamination\nissues, we start with a 67 million parameter transformer model and train it\nfrom scratch. On both tasks, we find that a model trained on linear causal\nchains (along with some noisy variations) can generalize well to complex\ngraphs, including longer causal chains, causal chains with reversed order, and\ngraphs with branching.To handle diverse text inputs, the same method is\nextended to finetune language models. Finetuning Llama-3.1 8B model on our\naxiomatic data leads to significant gains on causal benchmarks such as\nCorr2Cause and CLEAR, in some cases providing state-of-the-art performance\nsurpassing GPT-4.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07612v2",
    "published_date": "2024-07-10 12:50:44 UTC",
    "updated_date": "2025-04-15 08:43:28 UTC"
  },
  {
    "arxiv_id": "2407.07606v1",
    "title": "The Computational Learning of Construction Grammars: State of the Art and Prospective Roadmap",
    "authors": [
      "Jonas Doumen",
      "Veronica Juliana Schmalz",
      "Katrien Beuls",
      "Paul Van Eecke"
    ],
    "abstract": "This paper documents and reviews the state of the art concerning\ncomputational models of construction grammar learning. It brings together prior\nwork on the computational learning of form-meaning pairings, which has so far\nbeen studied in several distinct areas of research. The goal of this paper is\nthreefold. First of all, it aims to synthesise the variety of methodologies\nthat have been proposed to date and the results that have been obtained.\nSecond, it aims to identify those parts of the challenge that have been\nsuccessfully tackled and reveal those that require further research. Finally,\nit aims to provide a roadmap which can help to boost and streamline future\nresearch efforts on the computational learning of large-scale, usage-based\nconstruction grammars.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Peer-reviewed author's draft of a journal article to appear in\n  Constructions and Frames (2025)",
    "pdf_url": "http://arxiv.org/pdf/2407.07606v1",
    "published_date": "2024-07-10 12:45:02 UTC",
    "updated_date": "2024-07-10 12:45:02 UTC"
  },
  {
    "arxiv_id": "2407.07605v3",
    "title": "Early Explorations of Lightweight Models for Wound Segmentation on Mobile Devices",
    "authors": [
      "Vanessa Borst",
      "Timo Dittus",
      "Konstantin Müller",
      "Samuel Kounev"
    ],
    "abstract": "The aging population poses numerous challenges to healthcare, including the\nincrease in chronic wounds in the elderly. The current approach to wound\nassessment by therapists based on photographic documentation is subjective,\nhighlighting the need for computer-aided wound recognition from smartphone\nphotos. This offers objective and convenient therapy monitoring, while being\naccessible to patients from their home at any time. However, despite research\nin mobile image segmentation, there is a lack of focus on mobile wound\nsegmentation. To address this gap, we conduct initial research on three\nlightweight architectures to investigate their suitability for smartphone-based\nwound segmentation. Using public datasets and UNet as a baseline, our results\nare promising, with both ENet and TopFormer, as well as the larger UNeXt\nvariant, showing comparable performance to UNet. Furthermore, we deploy the\nmodels into a smartphone app for visual assessment of live segmentation, where\nresults demonstrate the effectiveness of TopFormer in distinguishing wounds\nfrom wound-coloured objects. While our study highlights the potential of\ntransformer models for mobile wound segmentation, future work should aim to\nfurther improve the mask contours.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Extended version of our paper that was published in the \"47th German\n  Conference on Artificial Intelligence (KI 2024)\"",
    "pdf_url": "http://arxiv.org/pdf/2407.07605v3",
    "published_date": "2024-07-10 12:44:22 UTC",
    "updated_date": "2024-08-30 08:36:36 UTC"
  },
  {
    "arxiv_id": "2407.07604v1",
    "title": "H-FCBFormer Hierarchical Fully Convolutional Branch Transformer for Occlusal Contact Segmentation with Articulating Paper",
    "authors": [
      "Ryan Banks",
      "Bernat Rovira-Lastra",
      "Jordi Martinez-Gomis",
      "Akhilanand Chaurasia",
      "Yunpeng Li"
    ],
    "abstract": "Occlusal contacts are the locations at which the occluding surfaces of the\nmaxilla and the mandible posterior teeth meet. Occlusal contact detection is a\nvital tool for restoring the loss of masticatory function and is a mandatory\nassessment in the field of dentistry, with particular importance in\nprosthodontics and restorative dentistry. The most common method for occlusal\ncontact detection is articulating paper. However, this method can indicate\nsignificant medically false positive and medically false negative contact\nareas, leaving the identification of true occlusal indications to clinicians.\nTo address this, we propose a multiclass Vision Transformer and Fully\nConvolutional Network ensemble semantic segmentation model with a combination\nhierarchical loss function, which we name as Hierarchical Fully Convolutional\nBranch Transformer (H-FCBFormer). We also propose a method of generating\nmedically true positive semantic segmentation masks derived from expert\nannotated articulating paper masks and gold standard masks. The proposed model\noutperforms other machine learning methods evaluated at detecting medically\ntrue positive contacts and performs better than dentists in terms of accurately\nidentifying object-wise occlusal contact areas while taking significantly less\ntime to identify them. Code is available at\nhttps://github.com/Banksylel/H-FCBFormer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.1, I.2.10, J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 5 figures, 2 tables, 5 equations, peer reviewed and\n  accepted to Medical Imaging Understanding and Analysis (MIUA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.07604v1",
    "published_date": "2024-07-10 12:42:39 UTC",
    "updated_date": "2024-07-10 12:42:39 UTC"
  },
  {
    "arxiv_id": "2407.07577v1",
    "title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model",
    "authors": [
      "Yatai Ji",
      "Shilong Zhang",
      "Jie Wu",
      "Peize Sun",
      "Weifeng Chen",
      "Xuefeng Xiao",
      "Sidi Yang",
      "Yujiu Yang",
      "Ping Luo"
    ],
    "abstract": "The rapid advancement of Large Vision-Language models (LVLMs) has\ndemonstrated a spectrum of emergent capabilities. Nevertheless, current models\nonly focus on the visual content of a single scenario, while their ability to\nassociate instances across different scenes has not yet been explored, which is\nessential for understanding complex visual content, such as movies with\nmultiple characters and intricate plots. Towards movie understanding, a\ncritical initial step for LVLMs is to unleash the potential of character\nidentities memory and recognition across multiple visual scenarios. To achieve\nthe goal, we propose visual instruction tuning with ID reference and develop an\nID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research\nintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and\nrecognition across four dimensions: matching, location, question-answering, and\ncaptioning. Our findings highlight the limitations of existing LVLMs in\nrecognizing and associating instance identities with ID reference. This paper\npaves the way for future artificial intelligence systems to possess\nmulti-identity visual inputs, thereby facilitating the comprehension of complex\nvisual narratives like movies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07577v1",
    "published_date": "2024-07-10 12:11:59 UTC",
    "updated_date": "2024-07-10 12:11:59 UTC"
  },
  {
    "arxiv_id": "2407.07561v1",
    "title": "FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes",
    "authors": [
      "Rajat Kumar Jenamani",
      "Priya Sundaresan",
      "Maram Sakr",
      "Tapomayukh Bhattacharjee",
      "Dorsa Sadigh"
    ],
    "abstract": "Robot-assisted feeding has the potential to improve the quality of life for\nindividuals with mobility limitations who are unable to feed themselves\nindependently. However, there exists a large gap between the homogeneous,\ncurated plates existing feeding systems can handle, and truly in-the-wild\nmeals. Feeding realistic plates is immensely challenging due to the sheer range\nof food items that a robot may encounter, each requiring specialized\nmanipulation strategies which must be sequenced over a long horizon to feed an\nentire meal. An assistive feeding system should not only be able to sequence\ndifferent strategies efficiently in order to feed an entire meal, but also be\nmindful of user preferences given the personalized nature of the task. We\naddress this with FLAIR, a system for long-horizon feeding which leverages the\ncommonsense and few-shot reasoning capabilities of foundation models, along\nwith a library of parameterized skills, to plan and execute user-preferred and\nefficient bite sequences. In real-world evaluations across 6 realistic plates,\nwe find that FLAIR can effectively tap into a varied library of skills for\nefficient food pickup, while adhering to the diverse preferences of 42\nparticipants without mobility limitations as evaluated in a user study. We\ndemonstrate the seamless integration of FLAIR with existing bite transfer\nmethods [19, 28], and deploy it across 2 institutions and 3 robots,\nillustrating its adaptability. Finally, we illustrate the real-world efficacy\nof our system by successfully feeding a care recipient with severe mobility\nlimitations. Supplementary materials and videos can be found at:\nhttps://emprise.cs.cornell.edu/flair .",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "RSS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07561v1",
    "published_date": "2024-07-10 11:38:57 UTC",
    "updated_date": "2024-07-10 11:38:57 UTC"
  },
  {
    "arxiv_id": "2407.12861v2",
    "title": "CiteME: Can Language Models Accurately Cite Scientific Claims?",
    "authors": [
      "Ori Press",
      "Andreas Hochlehnert",
      "Ameya Prabhu",
      "Vishaal Udandarao",
      "Ofir Press",
      "Matthias Bethge"
    ],
    "abstract": "Thousands of new scientific papers are published each month. Such information\noverload complicates researcher efforts to stay current with the\nstate-of-the-art as well as to verify and correctly attribute claims. We pose\nthe following research question: Given a text excerpt referencing a paper,\ncould an LM act as a research assistant to correctly identify the referenced\npaper? We advance efforts to answer this question by building a benchmark that\nevaluates the abilities of LMs in citation attribution. Our benchmark, CiteME,\nconsists of text excerpts from recent machine learning papers, each referencing\na single other paper. CiteME use reveals a large gap between frontier LMs and\nhuman performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%.\nWe close this gap by introducing CiteAgent, an autonomous system built on the\nGPT-4o LM that can also search and read papers, which achieves an accuracy of\n35.3\\% on CiteME. Overall, CiteME serves as a challenging testbed for\nopen-ended claim attribution, driving the research community towards a future\nwhere any claim made by an LM can be automatically verified and discarded if\nfound to be incorrect.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12861v2",
    "published_date": "2024-07-10 11:31:20 UTC",
    "updated_date": "2024-11-03 20:58:35 UTC"
  },
  {
    "arxiv_id": "2407.07551v1",
    "title": "Arabic Automatic Story Generation with Large Language Models",
    "authors": [
      "Ahmed Oumar El-Shangiti",
      "Fakhraddin Alwajih",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "Large language models (LLMs) have recently emerged as a powerful tool for a\nwide range of language generation tasks. Nevertheless, this progress has been\nslower in Arabic. In this work, we focus on the task of generating stories from\nLLMs. For our training, we use stories acquired through machine translation\n(MT) as well as GPT-4. For the MT data, we develop a careful pipeline that\nensures we acquire high-quality stories. For our GPT-41 data, we introduce\ncrafted prompts that allow us to generate data well-suited to the Arabic\ncontext in both Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian\nand Moroccan). For example, we generate stories tailored to various Arab\ncountries on a wide host of topics. Our manual evaluation shows that our model\nfine-tuned on these training datasets can generate coherent stories that adhere\nto our instructions. We also conduct an extensive automatic and human\nevaluation comparing our models against state-of-the-art proprietary and\nopen-source models. Our datasets and models will be made publicly available at\nhttps: //github.com/UBC-NLP/arastories.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07551v1",
    "published_date": "2024-07-10 11:26:10 UTC",
    "updated_date": "2024-07-10 11:26:10 UTC"
  },
  {
    "arxiv_id": "2407.07544v1",
    "title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization",
    "authors": [
      "An Zhang",
      "Han Wang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "Domain Generalization (DG), designed to enhance out-of-distribution (OOD)\ngeneralization, is all about learning invariance against domain shifts\nutilizing sufficient supervision signals. Yet, the scarcity of such labeled\ndata has led to the rise of unsupervised domain generalization (UDG) - a more\nimportant yet challenging task in that models are trained across diverse\ndomains in an unsupervised manner and eventually tested on unseen domains. UDG\nis fast gaining attention but is still far from well-studied. To close the\nresearch gap, we propose a novel learning framework designed for UDG, termed\nthe Disentangled Masked Auto Encoder (DisMAE), aiming to discover the\ndisentangled representations that faithfully reveal the intrinsic features and\nsuperficial variations without access to the class label. At its core is the\ndistillation of domain-invariant semantic features, which cannot be\ndistinguished by domain classifier, while filtering out the domain-specific\nvariations (for example, color schemes and texture patterns) that are unstable\nand redundant. Notably, DisMAE co-trains the asymmetric dual-branch\narchitecture with semantic and lightweight variation encoders, offering dynamic\ndata manipulation and representation level augmentation capabilities. Extensive\nexperiments on four benchmark datasets (i.e., DomainNet, PACS, VLCS, Colored\nMNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve\ncompetitive OOD performance compared with the state-of-the-art DG and UDG\nbaselines, which shed light on potential research line in improving the\ngeneralization ability with large-scale unlabeled data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07544v1",
    "published_date": "2024-07-10 11:11:36 UTC",
    "updated_date": "2024-07-10 11:11:36 UTC"
  },
  {
    "arxiv_id": "2407.11059v1",
    "title": "Was it Slander? Towards Exact Inversion of Generative Language Models",
    "authors": [
      "Adrians Skapars",
      "Edoardo Manino",
      "Youcheng Sun",
      "Lucas C. Cordeiro"
    ],
    "abstract": "Training large language models (LLMs) requires a substantial investment of\ntime and money. To get a good return on investment, the developers spend\nconsiderable effort ensuring that the model never produces harmful and\noffensive outputs. However, bad-faith actors may still try to slander the\nreputation of an LLM by publicly reporting a forged output. In this paper, we\nshow that defending against such slander attacks requires reconstructing the\ninput of the forged output or proving that it does not exist. To do so, we\npropose and evaluate a search based approach for targeted adversarial attacks\nfor LLMs. Our experiments show that we are rarely able to reconstruct the exact\ninput of an arbitrary output, thus demonstrating that LLMs are still vulnerable\nto slander attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "4 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.11059v1",
    "published_date": "2024-07-10 11:08:06 UTC",
    "updated_date": "2024-07-10 11:08:06 UTC"
  },
  {
    "arxiv_id": "2407.07541v1",
    "title": "Swiss DINO: Efficient and Versatile Vision Framework for On-device Personal Object Search",
    "authors": [
      "Kirill Paramonov",
      "Jia-Xing Zhong",
      "Umberto Michieli",
      "Jijoong Moon",
      "Mete Ozay"
    ],
    "abstract": "In this paper, we address a recent trend in robotic home appliances to\ninclude vision systems on personal devices, capable of personalizing the\nappliances on the fly. In particular, we formulate and address an important\ntechnical task of personal object search, which involves localization and\nidentification of personal items of interest on images captured by robotic\nappliances, with each item referenced only by a few annotated images. The task\nis crucial for robotic home appliances and mobile systems, which need to\nprocess personal visual scenes or to operate with particular personal objects\n(e.g., for grasping or navigation). In practice, personal object search\npresents two main technical challenges. First, a robot vision system needs to\nbe able to distinguish between many fine-grained classes, in the presence of\nocclusions and clutter. Second, the strict resource requirements for the\non-device system restrict the usage of most state-of-the-art methods for\nfew-shot learning and often prevent on-device adaptation. In this work, we\npropose Swiss DINO: a simple yet effective framework for one-shot personal\nobject search based on the recent DINOv2 transformer model, which was shown to\nhave strong zero-shot generalization properties. Swiss DINO handles challenging\non-device personalized scene understanding requirements and does not require\nany adaptation training. We show significant improvement (up to 55%) in\nsegmentation and recognition accuracy compared to the common lightweight\nsolutions, and significant footprint reduction of backbone inference time (up\nto 100x) and GPU consumption (up to 10x) compared to the heavy\ntransformer-based solutions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 2 figures, accepted to IROS2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07541v1",
    "published_date": "2024-07-10 11:05:02 UTC",
    "updated_date": "2024-07-10 11:05:02 UTC"
  },
  {
    "arxiv_id": "2407.18934v1",
    "title": "The Design of a 3D Character Animation System for Digital Twins in the Metaverse",
    "authors": [
      "Senem Tanberk",
      "Dilek Bilgin Tukel",
      "Kadir Acar"
    ],
    "abstract": "In the context of Industry 4.0, digital twin technology has emerged with\nrapid advancements as a powerful tool for visualizing and analyzing industrial\nassets. This technology has attracted considerable interest from researchers\nacross diverse domains such as manufacturing, security, transportation, and\ngaming. The metaverse has emerged as a significant enabler in these domains,\nfacilitating the integration of various technologies to create virtual replicas\nof physical assets. The utilization of 3D character animation, often referred\nto as avatars, is crucial for implementing the metaverse. Traditionally, costly\nmotion capture technologies are employed for creating a realistic avatar\nsystem. To meet the needs of this evolving landscape, we have developed a\nmodular framework tailored for asset digital twins as a more affordable\nalternative. This framework offers flexibility for the independent\ncustomization of individual system components. To validate our approach, we\nemploy the English peg solitaire game as a use case, generating a solution tree\nusing the breadth-first search algorithm. The results encompass both\nqualitative and quantitative findings of a data-driven 3D animation system\nutilizing motion primitives. The presented methodologies and infrastructure are\nadaptable and modular, making them applicable to asset digital twins across\ndiverse business contexts. This case study lays the groundwork for pilot\napplications and can be tailored for education, health, or Industry 4.0\nmaterial development.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18934v1",
    "published_date": "2024-07-10 10:54:06 UTC",
    "updated_date": "2024-07-10 10:54:06 UTC"
  },
  {
    "arxiv_id": "2407.07530v1",
    "title": "How Aligned are Different Alignment Metrics?",
    "authors": [
      "Jannis Ahlert",
      "Thomas Klein",
      "Felix Wichmann",
      "Robert Geirhos"
    ],
    "abstract": "In recent years, various methods and benchmarks have been proposed to\nempirically evaluate the alignment of artificial neural networks to human\nneural and behavioral data. But how aligned are different alignment metrics? To\nanswer this question, we analyze visual data from Brain-Score (Schrimpf et al.,\n2018), including metrics from the model-vs-human toolbox (Geirhos et al.,\n2021), together with human feature alignment (Linsley et al., 2018; Fel et al.,\n2022) and human similarity judgements (Muttenthaler et al., 2022). We find that\npairwise correlations between neural scores and behavioral scores are quite low\nand sometimes even negative. For instance, the average correlation between\nthose 80 models on Brain-Score that were fully evaluated on all 69 alignment\nmetrics we considered is only 0.198. Assuming that all of the employed metrics\nare sound, this implies that alignment with human perception may best be\nthought of as a multidimensional concept, with different methods measuring\nfundamentally different aspects. Our results underline the importance of\nintegrative benchmarking, but also raise questions about how to correctly\ncombine and aggregate individual metrics. Aggregating by taking the arithmetic\naverage, as done in Brain-Score, leads to the overall performance currently\nbeing dominated by behavior (95.25% explained variance) while the neural\npredictivity plays a less important role (only 33.33% explained variance). As a\nfirst step towards making sure that different alignment metrics all contribute\nfairly towards an integrative benchmark score, we therefore conclude by\ncomparing three different aggregation options.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "submitted to the ICLR 2024 Workshop on Representational Alignment\n  (Re-Align)",
    "pdf_url": "http://arxiv.org/pdf/2407.07530v1",
    "published_date": "2024-07-10 10:36:11 UTC",
    "updated_date": "2024-07-10 10:36:11 UTC"
  },
  {
    "arxiv_id": "2407.07521v1",
    "title": "CHILLI: A data context-aware perturbation method for XAI",
    "authors": [
      "Saif Anwar",
      "Nathan Griffiths",
      "Abhir Bhalerao",
      "Thomas Popham"
    ],
    "abstract": "The trustworthiness of Machine Learning (ML) models can be difficult to\nassess, but is critical in high-risk or ethically sensitive applications. Many\nmodels are treated as a `black-box' where the reasoning or criteria for a final\ndecision is opaque to the user. To address this, some existing Explainable AI\n(XAI) approaches approximate model behaviour using perturbed data. However,\nsuch methods have been criticised for ignoring feature dependencies, with\nexplanations being based on potentially unrealistic data. We propose a novel\nframework, CHILLI, for incorporating data context into XAI by generating\ncontextually aware perturbations, which are faithful to the training data of\nthe base model being explained. This is shown to improve both the soundness and\naccuracy of the explanations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07521v1",
    "published_date": "2024-07-10 10:18:07 UTC",
    "updated_date": "2024-07-10 10:18:07 UTC"
  },
  {
    "arxiv_id": "2407.07506v2",
    "title": "Generative AI for RF Sensing in IoT systems",
    "authors": [
      "Li Wang",
      "Chao Zhang",
      "Qiyang Zhao",
      "Hang Zou",
      "Samson Lasaulce",
      "Giuseppe Valenzise",
      "Zhuo He",
      "Merouane Debbah"
    ],
    "abstract": "The development of wireless sensing technologies, using signals such as\nWi-Fi, infrared, and RF to gather environmental data, has significantly\nadvanced within Internet of Things (IoT) systems. Among these, Radio Frequency\n(RF) sensing stands out for its cost-effective and non-intrusive monitoring of\nhuman activities and environmental changes. However, traditional RF sensing\nmethods face significant challenges, including noise, interference, incomplete\ndata, and high deployment costs, which limit their effectiveness and\nscalability. This paper investigates the potential of Generative AI (GenAI) to\novercome these limitations within the IoT ecosystem. We provide a comprehensive\nreview of state-of-the-art GenAI techniques, focusing on their application to\nRF sensing problems. By generating high-quality synthetic data, enhancing\nsignal quality, and integrating multi-modal data, GenAI offers robust solutions\nfor RF environment reconstruction, localization, and imaging. Additionally,\nGenAI's ability to generalize enables IoT devices to adapt to new environments\nand unseen tasks, improving their efficiency and performance. The main\ncontributions of this article include a detailed analysis of the challenges in\nRF sensing, the presentation of innovative GenAI-based solutions, and the\nproposal of a unified framework for diverse RF sensing tasks. Through case\nstudies, we demonstrate the effectiveness of integrating GenAI models, leading\nto advanced, scalable, and intelligent IoT systems.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07506v2",
    "published_date": "2024-07-10 09:51:44 UTC",
    "updated_date": "2024-11-24 08:38:42 UTC"
  },
  {
    "arxiv_id": "2407.07488v1",
    "title": "FUNAvg: Federated Uncertainty Weighted Averaging for Datasets with Diverse Labels",
    "authors": [
      "Malte Tölle",
      "Fernando Navarro",
      "Sebastian Eble",
      "Ivo Wolf",
      "Bjoern Menze",
      "Sandy Engelhardt"
    ],
    "abstract": "Federated learning is one popular paradigm to train a joint model in a\ndistributed, privacy-preserving environment. But partial annotations pose an\nobstacle meaning that categories of labels are heterogeneous over clients. We\npropose to learn a joint backbone in a federated manner, while each site\nreceives its own multi-label segmentation head. By using Bayesian techniques we\nobserve that the different segmentation heads although only trained on the\nindividual client's labels also learn information about the other labels not\npresent at the respective site. This information is encoded in their predictive\nuncertainty. To obtain a final prediction we leverage this uncertainty and\nperform a weighted averaging of the ensemble of distributed segmentation heads,\nwhich allows us to segment \"locally unknown\" structures. With our method, which\nwe refer to as FUNAvg, we are even on-par with the models trained and tested on\nthe same dataset on average. The code is publicly available at\nhttps://github.com/Cardio-AI/FUNAvg.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MICCAI24",
    "pdf_url": "http://arxiv.org/pdf/2407.07488v1",
    "published_date": "2024-07-10 09:23:55 UTC",
    "updated_date": "2024-07-10 09:23:55 UTC"
  },
  {
    "arxiv_id": "2407.07482v1",
    "title": "Rigorous Probabilistic Guarantees for Robust Counterfactual Explanations",
    "authors": [
      "Luca Marzari",
      "Francesco Leofante",
      "Ferdinando Cicalese",
      "Alessandro Farinelli"
    ],
    "abstract": "We study the problem of assessing the robustness of counterfactual\nexplanations for deep learning models. We focus on $\\textit{plausible model\nshifts}$ altering model parameters and propose a novel framework to reason\nabout the robustness property in this setting. To motivate our solution, we\nbegin by showing for the first time that computing the robustness of\ncounterfactuals with respect to plausible model shifts is NP-complete. As this\n(practically) rules out the existence of scalable algorithms for exactly\ncomputing robustness, we propose a novel probabilistic approach which is able\nto provide tight estimates of robustness with strong guarantees while\npreserving scalability. Remarkably, and differently from existing solutions\ntargeting plausible model shifts, our approach does not impose requirements on\nthe network to be analyzed, thus enabling robustness analysis on a wider range\nof architectures. Experiments on four binary classification datasets indicate\nthat our method improves the state of the art in generating robust\nexplanations, outperforming existing methods on a range of metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 27th European Conference on Artificial Intelligence\n  (ECAI 2024). Marzari and Leofante contributed equally to the paper",
    "pdf_url": "http://arxiv.org/pdf/2407.07482v1",
    "published_date": "2024-07-10 09:13:11 UTC",
    "updated_date": "2024-07-10 09:13:11 UTC"
  },
  {
    "arxiv_id": "2407.18932v2",
    "title": "Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles",
    "authors": [
      "Xuchuan Li",
      "Fei Huang",
      "Jianrong Lv",
      "Zhixiong Xiao",
      "Guolong Li",
      "Yang Yue"
    ],
    "abstract": "Human mobility is inextricably linked to social issues such as traffic\ncongestion, energy consumption, and public health; however, privacy concerns\nrestrict access to mobility data. Recently, research have utilized Large\nLanguage Models (LLMs) for human mobility generation, in which the challenge is\nhow LLMs can understand individuals' mobility behavioral differences to\ngenerate realistic trajectories conforming to real world contexts. This study\nhandles this problem by presenting an LLM agent-based framework (MobAgent)\ncomposing two phases: understanding-based mobility pattern extraction and\nreasoning-based trajectory generation, which enables generate more real travel\ndiaries at urban scale, considering different individual profiles. MobAgent\nextracts reasons behind specific mobility trendiness and attribute influences\nto provide reliable patterns; infers the relationships between contextual\nfactors and underlying motivations of mobility; and based on the patterns and\nthe recursive reasoning process, MobAgent finally generates more authentic and\npersonalized mobilities that reflect both individual differences and real-world\nconstraints. We validate our framework with 0.2 million travel survey data,\ndemonstrating its effectiveness in producing personalized and accurate travel\ndiaries. This study highlights the capacity of LLMs to provide detailed and\nsophisticated understanding of human mobility through the real-world mobility\ndata.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18932v2",
    "published_date": "2024-07-10 09:11:57 UTC",
    "updated_date": "2024-08-05 15:59:39 UTC"
  },
  {
    "arxiv_id": "2407.07472v1",
    "title": "Rectifier: Code Translation with Corrector via LLMs",
    "authors": [
      "Xin Yin",
      "Chao Ni",
      "Tien N. Nguyen",
      "Shaohua Wang",
      "Xiaohu Yang"
    ],
    "abstract": "Software migration is garnering increasing attention with the evolution of\nsoftware and society. Early studies mainly relied on handcrafted translation\nrules to translate between two languages, the translation process is\nerror-prone and time-consuming. In recent years, researchers have begun to\nexplore the use of pre-trained large language models (LLMs) in code\ntranslation. However, code translation is a complex task that LLMs would\ngenerate mistakes during code translation, they all produce certain types of\nerrors when performing code translation tasks, which include (1) compilation\nerror, (2) runtime error, (3) functional error, and (4) non-terminating\nexecution. We found that the root causes of these errors are very similar (e.g.\nfailure to import packages, errors in loop boundaries, operator errors, and\nmore). In this paper, we propose a general corrector, namely Rectifier, which\nis a micro and universal model for repairing translation errors. It learns from\nerrors generated by existing LLMs and can be widely applied to correct errors\ngenerated by any LLM. The experimental results on translation tasks between\nC++, Java, and Python show that our model has effective repair ability, and\ncross experiments also demonstrate the robustness of our method.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "arXiv admin note: text overlap with arXiv:2308.03109,\n  arXiv:2302.03908 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2407.07472v1",
    "published_date": "2024-07-10 08:58:41 UTC",
    "updated_date": "2024-07-10 08:58:41 UTC"
  },
  {
    "arxiv_id": "2407.12860v1",
    "title": "STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs",
    "authors": [
      "Aaron Zolnai-Lucas",
      "Jack Boylan",
      "Chris Hokamp",
      "Parsa Ghaffari"
    ],
    "abstract": "We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12860v1",
    "published_date": "2024-07-10 08:50:25 UTC",
    "updated_date": "2024-07-10 08:50:25 UTC"
  },
  {
    "arxiv_id": "2407.11057v1",
    "title": "SPIN: SE(3)-Invariant Physics Informed Network for Binding Affinity Prediction",
    "authors": [
      "Seungyeon Choi",
      "Sangmin Seo",
      "Sanghyun Park"
    ],
    "abstract": "Accurate prediction of protein-ligand binding affinity is crucial for rapid\nand efficient drug development. Recently, the importance of predicting binding\naffinity has led to increased attention on research that models the\nthree-dimensional structure of protein-ligand complexes using graph neural\nnetworks to predict binding affinity. However, traditional methods often fail\nto accurately model the complex's spatial information or rely solely on\ngeometric features, neglecting the principles of protein-ligand binding. This\ncan lead to overfitting, resulting in models that perform poorly on independent\ndatasets and ultimately reducing their usefulness in real drug development. To\naddress this issue, we propose SPIN, a model designed to achieve superior\ngeneralization by incorporating various inductive biases applicable to this\ntask, beyond merely training on empirical data from datasets. For prediction,\nwe defined two types of inductive biases: a geometric perspective that\nmaintains consistent binding affinity predictions regardless of the complexs\nrotations and translations, and a physicochemical perspective that necessitates\nminimal binding free energy along their reaction coordinate for effective\nprotein-ligand binding. These prior knowledge inputs enable the SPIN to\noutperform comparative models in benchmark sets such as CASF-2016 and CSAR HiQ.\nFurthermore, we demonstrated the practicality of our model through virtual\nscreening experiments and validated the reliability and potential of our\nproposed model based on experiments assessing its interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.11057v1",
    "published_date": "2024-07-10 08:40:07 UTC",
    "updated_date": "2024-07-10 08:40:07 UTC"
  },
  {
    "arxiv_id": "2407.07462v2",
    "title": "MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions",
    "authors": [
      "Felix Fent",
      "Fabian Kuttenreich",
      "Florian Ruch",
      "Farija Rizwin",
      "Stefan Juergens",
      "Lorenz Lechermann",
      "Christian Nissler",
      "Andrea Perl",
      "Ulrich Voll",
      "Min Yan",
      "Markus Lienkamp"
    ],
    "abstract": "Autonomous trucking is a promising technology that can greatly impact modern\nlogistics and the environment. Ensuring its safety on public roads is one of\nthe main duties that requires an accurate perception of the environment. To\nachieve this, machine learning methods rely on large datasets, but to this day,\nno such datasets are available for autonomous trucks. In this work, we present\nMAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN\nTruckScenes allows the research community to come into contact with\ntruck-specific challenges, such as trailer occlusions, novel sensor\nperspectives, and terminal environments for the first time. It comprises more\nthan 740 scenes of 20s each within a multitude of different environmental\nconditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2\nIMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually\nannotated and carefully reviewed to achieve a high quality standard. Bounding\nboxes are available for 27 object classes, 15 attributes, and a range of more\nthan 230m. The scenes are tagged according to 34 distinct scene tags, and all\nobjects are tracked throughout the scene to promote a wide range of\napplications. Additionally, MAN TruckScenes is the first dataset to provide 4D\nradar data with 360{\\deg} coverage and is thereby the largest radar dataset\nwith annotated 3D bounding boxes. Finally, we provide extensive dataset\nanalysis and baseline results. The dataset, development kit, and more are\navailable online.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2407.07462v2",
    "published_date": "2024-07-10 08:32:26 UTC",
    "updated_date": "2024-11-11 14:59:22 UTC"
  },
  {
    "arxiv_id": "2407.07452v2",
    "title": "Missile detection and destruction robot using detection algorithm",
    "authors": [
      "Md Kamrul Siam",
      "Shafayet Ahmed",
      "Md Habibur Rahman",
      "Amir Hossain Mollah"
    ],
    "abstract": "This research is based on the present missile detection technologies in the\nworld and the analysis of these technologies to find a cost effective solution\nto implement the system in Bangladesh. The paper will give an idea of the\nmissile detection technologies using the electro-optical sensor and the pulse\ndoppler radar. The system is made to detect the target missile. Automatic\ndetection and destruction with the help of ultrasonic sonar, a metal detector\nsensor, and a smoke detector sensor. The system is mainly based on an\nultrasonic sonar sensor. It has a transducer, a transmitter, and a receiver.\nTransducer is connected with the connected with controller. When it detects an\nobject by following the algorithm, it finds its distance and angle. It can also\nassure whether the system can destroy the object or not by using another\nalgorithm's simulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "67 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.07452v2",
    "published_date": "2024-07-10 08:12:21 UTC",
    "updated_date": "2024-07-11 04:18:17 UTC"
  },
  {
    "arxiv_id": "2407.12859v1",
    "title": "Automated Question Generation on Tabular Data for Conversational Data Exploration",
    "authors": [
      "Ritwik Chaudhuri",
      "Rajmohan C",
      "Kirushikesh DB",
      "Arvind Agarwal"
    ],
    "abstract": "Exploratory data analysis (EDA) is an essential step for analyzing a dataset\nto derive insights. Several EDA techniques have been explored in the\nliterature. Many of them leverage visualizations through various plots. But it\nis not easy to interpret them for a non-technical user, and producing\nappropriate visualizations is also tough when there are a large number of\ncolumns. Few other works provide a view of some interesting slices of data but\nit is still difficult for the user to draw relevant insights from them. Of\nlate, conversational data exploration is gaining a lot of traction among\nnon-technical users. It helps the user to explore the dataset without having\ndeep technical knowledge about the data. Towards this, we propose a system that\nrecommends interesting questions in natural language based on relevant slices\nof a dataset in a conversational setting. Specifically, given a dataset, we\npick a select set of interesting columns and identify interesting slices of\nsuch columns and column combinations based on few interestingness measures. We\nuse our own fine-tuned variation of a pre-trained language model(T5) to\ngenerate natural language questions in a specific manner. We then slot-fill\nvalues in the generated questions and rank them for recommendations. We show\nthe utility of our proposed system in a coversational setting with a collection\nof real datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12859v1",
    "published_date": "2024-07-10 08:07:05 UTC",
    "updated_date": "2024-07-10 08:07:05 UTC"
  },
  {
    "arxiv_id": "2407.11056v1",
    "title": "Industrial-Grade Time-Dependent Counterfactual Root Cause Analysis through the Unanticipated Point of Incipient Failure: a Proof of Concept",
    "authors": [
      "Alexandre Trilla",
      "Rajesh Rajendran",
      "Ossee Yiboe",
      "Quentin Possamaï",
      "Nenad Mijatovic",
      "Jordi Vitrià"
    ],
    "abstract": "This paper describes the development of a counterfactual Root Cause Analysis\ndiagnosis approach for an industrial multivariate time series environment. It\ndrives the attention toward the Point of Incipient Failure, which is the moment\nin time when the anomalous behavior is first observed, and where the root cause\nis assumed to be found before the issue propagates. The paper presents the\nelementary but essential concepts of the solution and illustrates them\nexperimentally on a simulated setting. Finally, it discusses avenues of\nimprovement for the maturity of the causal technology to meet the robustness\nchallenges of increasingly complex environments in the industry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for the Causal Inference for Time Series Data Workshop at\n  the 40th Conference on Uncertainty in Artificial Intelligence (CI4TS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.11056v1",
    "published_date": "2024-07-10 08:02:03 UTC",
    "updated_date": "2024-07-10 08:02:03 UTC"
  },
  {
    "arxiv_id": "2407.07443v1",
    "title": "Secondary Structure-Guided Novel Protein Sequence Generation with Latent Graph Diffusion",
    "authors": [
      "Yutong Hu",
      "Yang Tan",
      "Andi Han",
      "Lirong Zheng",
      "Liang Hong",
      "Bingxin Zhou"
    ],
    "abstract": "The advent of deep learning has introduced efficient approaches for de novo\nprotein sequence design, significantly improving success rates and reducing\ndevelopment costs compared to computational or experimental methods. However,\nexisting methods face challenges in generating proteins with diverse lengths\nand shapes while maintaining key structural features. To address these\nchallenges, we introduce CPDiffusion-SS, a latent graph diffusion model that\ngenerates protein sequences based on coarse-grained secondary structural\ninformation. CPDiffusion-SS offers greater flexibility in producing a variety\nof novel amino acid sequences while preserving overall structural constraints,\nthus enhancing the reliability and diversity of generated proteins.\nExperimental analyses demonstrate the significant superiority of the proposed\nmethod in producing diverse and novel sequences, with CPDiffusion-SS surpassing\npopular baseline methods on open benchmarks across various quantitative\nmeasurements. Furthermore, we provide a series of case studies to highlight the\nbiological significance of the generation performance by the proposed method.\nThe source code is publicly available at\nhttps://github.com/riacd/CPDiffusion-SS",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.07443v1",
    "published_date": "2024-07-10 07:54:26 UTC",
    "updated_date": "2024-07-10 07:54:26 UTC"
  },
  {
    "arxiv_id": "2407.07433v2",
    "title": "Controllable Navigation Instruction Generation with Chain of Thought Prompting",
    "authors": [
      "Xianghao Kong",
      "Jinyu Chen",
      "Wenguan Wang",
      "Hang Su",
      "Xiaolin Hu",
      "Yi Yang",
      "Si Liu"
    ],
    "abstract": "Instruction generation is a vital and multidisciplinary research area with\nbroad applications. Existing instruction generation models are limited to\ngenerating instructions in a single style from a particular dataset, and the\nstyle and content of generated instructions cannot be controlled. Moreover,\nmost existing instruction generation methods also disregard the spatial\nmodeling of the navigation environment. Leveraging the capabilities of Large\nLanguage Models (LLMs), we propose C-Instructor, which utilizes the\nchain-of-thought-style prompt for style-controllable and content-controllable\ninstruction generation. Firstly, we propose a Chain of Thought with Landmarks\n(CoTL) mechanism, which guides the LLM to identify key landmarks and then\ngenerate complete instructions. CoTL renders generated instructions more\naccessible to follow and offers greater controllability over the manipulation\nof landmark objects. Furthermore, we present a Spatial Topology Modeling Task\nto facilitate the understanding of the spatial structure of the environment.\nFinally, we introduce a Style-Mixed Training policy, harnessing the prior\nknowledge of LLMs to enable style control for instruction generation based on\ndifferent prompts within a single model instance. Extensive experiments\ndemonstrate that instructions generated by C-Instructor outperform those\ngenerated by previous methods in text metrics, navigation guidance evaluation,\nand user studies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07433v2",
    "published_date": "2024-07-10 07:37:20 UTC",
    "updated_date": "2024-07-16 10:09:34 UTC"
  },
  {
    "arxiv_id": "2407.17416v1",
    "title": "Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification",
    "authors": [
      "Jesin James",
      "Balamurali B. T.",
      "Binu Abeysinghe",
      "Junchen Liu"
    ],
    "abstract": "This study investigates discriminative patterns learned by neural networks\nfor accurate speech classification, with a specific focus on vowel\nclassification tasks. By examining the activations and features of neural\nnetworks for vowel classification, we gain insights into what the networks\n\"see\" in spectrograms. Through the use of class activation mapping, we identify\nthe frequencies that contribute to vowel classification and compare these\nfindings with linguistic knowledge. Experiments on a American English dataset\nof vowels showcases the explainability of neural networks and provides valuable\ninsights into the causes of misclassifications and their characteristics when\ndifferentiating them from unvoiced speech. This study not only enhances our\nunderstanding of the underlying acoustic cues in vowel classification but also\noffers opportunities for improving speech recognition by bridging the gap\nbetween abstract representations in neural networks and established linguistic\nknowledge",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "5th International Conference on Artificial Intelligence and Speech\n  Technology (AIST-2023), New Delhi, India",
    "pdf_url": "http://arxiv.org/pdf/2407.17416v1",
    "published_date": "2024-07-10 07:37:18 UTC",
    "updated_date": "2024-07-10 07:37:18 UTC"
  },
  {
    "arxiv_id": "2407.07421v1",
    "title": "Federated PCA on Grassmann Manifold for IoT Anomaly Detection",
    "authors": [
      "Tung-Anh Nguyen",
      "Long Tan Le",
      "Tuan Dung Nguyen",
      "Wei Bao",
      "Suranga Seneviratne",
      "Choong Seon Hong",
      "Nguyen H. Tran"
    ],
    "abstract": "With the proliferation of the Internet of Things (IoT) and the rising\ninterconnectedness of devices, network security faces significant challenges,\nespecially from anomalous activities. While traditional machine learning-based\nintrusion detection systems (ML-IDS) effectively employ supervised learning\nmethods, they possess limitations such as the requirement for labeled data and\nchallenges with high dimensionality. Recent unsupervised ML-IDS approaches such\nas AutoEncoders and Generative Adversarial Networks (GAN) offer alternative\nsolutions but pose challenges in deployment onto resource-constrained IoT\ndevices and in interpretability. To address these concerns, this paper proposes\na novel federated unsupervised anomaly detection framework, FedPCA, that\nleverages Principal Component Analysis (PCA) and the Alternating Directions\nMethod Multipliers (ADMM) to learn common representations of distributed\nnon-i.i.d. datasets. Building on the FedPCA framework, we propose two\nalgorithms, FEDPE in Euclidean space and FEDPG on Grassmann manifolds. Our\napproach enables real-time threat detection and mitigation at the device level,\nenhancing network resilience while ensuring privacy. Moreover, the proposed\nalgorithms are accompanied by theoretical convergence rates even under a\nsubsampling scheme, a novel result. Experimental results on the UNSW-NB15 and\nTON-IoT datasets show that our proposed methods offer performance in anomaly\ndetection comparable to nonlinear baselines, while providing significant\nimprovements in communication and memory efficiency, underscoring their\npotential for securing IoT networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at IEEE/ACM Transactions on Networking",
    "pdf_url": "http://arxiv.org/pdf/2407.07421v1",
    "published_date": "2024-07-10 07:23:21 UTC",
    "updated_date": "2024-07-10 07:23:21 UTC"
  },
  {
    "arxiv_id": "2407.07931v1",
    "title": "Search, Examine and Early-Termination: Fake News Detection with Annotation-Free Evidences",
    "authors": [
      "Yuzhou Yang",
      "Yangming Zhou",
      "Qichao Ying",
      "Zhenxing Qian",
      "Xinpeng Zhang"
    ],
    "abstract": "Pioneer researches recognize evidences as crucial elements in fake news\ndetection apart from patterns. Existing evidence-aware methods either require\nlaborious pre-processing procedures to assure relevant and high-quality\nevidence data, or incorporate the entire spectrum of available evidences in all\nnews cases, regardless of the quality and quantity of the retrieved data. In\nthis paper, we propose an approach named \\textbf{SEE} that retrieves useful\ninformation from web-searched annotation-free evidences with an\nearly-termination mechanism. The proposed SEE is constructed by three main\nphases: \\textbf{S}earching online materials using the news as a query and\ndirectly using their titles as evidences without any annotating or filtering\nprocedure, sequentially \\textbf{E}xamining the news alongside with each piece\nof evidence via attention mechanisms to produce new hidden states with\nretrieved information, and allowing \\textbf{E}arly-termination within the\nexamining loop by assessing whether there is adequate confidence for producing\na correct prediction. We have conducted extensive experiments on datasets with\nunprocessed evidences, i.e., Weibo21, GossipCop, and pre-processed evidences,\nnamely Snopes and PolitiFact. The experimental results demonstrate that the\nproposed method outperforms state-of-the-art approaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "ECAI 2024 paper. Fudan University & NVIDIA. To appear",
    "pdf_url": "http://arxiv.org/pdf/2407.07931v1",
    "published_date": "2024-07-10 07:22:30 UTC",
    "updated_date": "2024-07-10 07:22:30 UTC"
  },
  {
    "arxiv_id": "2407.07412v3",
    "title": "Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation",
    "authors": [
      "Seonghoon Yu",
      "Paul Hongsuck Seo",
      "Jeany Son"
    ],
    "abstract": "We propose a new framework that automatically generates high-quality\nsegmentation masks with their referring expressions as pseudo supervisions for\nreferring image segmentation (RIS). These pseudo supervisions allow the\ntraining of any supervised RIS methods without the cost of manual labeling. To\nachieve this, we incorporate existing segmentation and image captioning\nfoundation models, leveraging their broad generalization capabilities. However,\nthe naive incorporation of these models may generate non-distinctive\nexpressions that do not distinctively refer to the target masks. To address\nthis challenge, we propose two-fold strategies that generate distinctive\ncaptions: 1) 'distinctive caption sampling', a new decoding method for the\ncaptioning model, to generate multiple expression candidates with detailed\nwords focusing on the target. 2) 'distinctiveness-based text filtering' to\nfurther validate the candidates and filter out those with a low level of\ndistinctiveness. These two strategies ensure that the generated text\nsupervisions can distinguish the target from other objects, making them\nappropriate for the RIS annotations. Our method significantly outperforms both\nweakly and zero-shot SoTA methods on the RIS benchmark datasets. It also\nsurpasses fully supervised methods in unseen domains, proving its capability to\ntackle the open-world challenge within RIS. Furthermore, integrating our method\nwith human annotations yields further improvements, highlighting its potential\nin semi-supervised learning applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07412v3",
    "published_date": "2024-07-10 07:14:48 UTC",
    "updated_date": "2024-07-17 05:32:02 UTC"
  },
  {
    "arxiv_id": "2407.07406v1",
    "title": "Weakly-supervised Medical Image Segmentation with Gaze Annotations",
    "authors": [
      "Yuan Zhong",
      "Chenhui Tang",
      "Yumeng Yang",
      "Ruoxi Qi",
      "Kang Zhou",
      "Yuqi Gong",
      "Pheng Ann Heng",
      "Janet H. Hsiao",
      "Qi Dou"
    ],
    "abstract": "Eye gaze that reveals human observational patterns has increasingly been\nincorporated into solutions for vision tasks. Despite recent explorations on\nleveraging gaze to aid deep networks, few studies exploit gaze as an efficient\nannotation approach for medical image segmentation which typically entails\nheavy annotating costs. In this paper, we propose to collect dense weak\nsupervision for medical image segmentation with a gaze annotation scheme. To\ntrain with gaze, we propose a multi-level framework that trains multiple\nnetworks from discriminative human attention, simulated with a set of\npseudo-masks derived by applying hierarchical thresholds on gaze heatmaps.\nFurthermore, to mitigate gaze noise, a cross-level consistency is exploited to\nregularize overfitting noisy labels, steering models toward clean patterns\nlearned by peer networks. The proposed method is validated on two public\nmedical datasets of polyp and prostate segmentation tasks. We contribute a\nhigh-quality gaze dataset entitled GazeMedSeg as an extension to the popular\nmedical segmentation datasets. To the best of our knowledge, this is the first\ngaze dataset for medical image segmentation. Our experiments demonstrate that\ngaze annotation outperforms previous label-efficient annotation schemes in\nterms of both performance and annotation time. Our collected gaze data and code\nare available at: https://github.com/med-air/GazeMedSeg.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07406v1",
    "published_date": "2024-07-10 07:07:58 UTC",
    "updated_date": "2024-07-10 07:07:58 UTC"
  },
  {
    "arxiv_id": "2407.07392v1",
    "title": "Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems",
    "authors": [
      "Chashi Mahiul Islam",
      "Shaeke Salman",
      "Montasir Shams",
      "Xiuwen Liu",
      "Piyush Kumar"
    ],
    "abstract": "Building on the unprecedented capabilities of large language models for\ncommand understanding and zero-shot recognition of multi-modal vision-language\ntransformers, visual language navigation (VLN) has emerged as an effective way\nto address multiple fundamental challenges toward a natural language interface\nto robot navigation. However, such vision-language models are inherently\nvulnerable due to the lack of semantic meaning of the underlying embedding\nspace. Using a recently developed gradient based optimization procedure, we\ndemonstrate that images can be modified imperceptibly to match the\nrepresentation of totally different images and unrelated texts for a\nvision-language model. Building on this, we develop algorithms that can\nadversarially modify a minimal number of images so that the robot will follow a\nroute of choice for commands that require a number of landmarks. We demonstrate\nthat experimentally using a recently proposed VLN system; for a given\nnavigation command, a robot can be made to follow drastically different routes.\nWe also develop an efficient algorithm to detect such malicious modifications\nreliably based on the fact that the adversarially modified images have much\nhigher sensitivity to added Gaussian noise than the original images.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 figures. This paper has been accepted for publication at\n  the IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07392v1",
    "published_date": "2024-07-10 06:32:58 UTC",
    "updated_date": "2024-07-10 06:32:58 UTC"
  },
  {
    "arxiv_id": "2407.18928v1",
    "title": "The Voice: Lessons on Trustworthy Conversational Agents from \"Dune\"",
    "authors": [
      "Philip Feldman"
    ],
    "abstract": "The potential for untrustworthy conversational agents presents a significant\nthreat for covert social manipulation. Taking inspiration from Frank Herbert's\n\"Dune\", where the Bene Gesserit Sisterhood uses the Voice for influence,\nmanipulation, and control of people, we explore how generative AI provides a\nway to implement individualized influence at industrial scales. Already, these\nmodels can manipulate communication across text, image, speech, and most\nrecently video. They are rapidly becoming affordable enough for any\norganization of even moderate means to train and deploy. If employed by\nmalicious actors, they risk becoming powerful tools for shaping public opinion,\nsowing discord, and undermining organizations from companies to governments. As\nresearchers and developers, it is crucial to recognize the potential for such\nweaponization and to explore strategies for prevention, detection, and defense\nagainst these emerging forms of sociotechnical manipulation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.2; I.2.m"
    ],
    "primary_category": "cs.CY",
    "comment": "5 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.18928v1",
    "published_date": "2024-07-10 05:38:31 UTC",
    "updated_date": "2024-07-10 05:38:31 UTC"
  },
  {
    "arxiv_id": "2407.07375v1",
    "title": "Stable Weight Updating: A Key to Reliable PDE Solutions Using Deep Learning",
    "authors": [
      "A. Noorizadegan",
      "R. Cavoretto",
      "D. L. Young",
      "C. S. Chen"
    ],
    "abstract": "Background: Deep learning techniques, particularly neural networks, have\nrevolutionized computational physics, offering powerful tools for solving\ncomplex partial differential equations (PDEs). However, ensuring stability and\nefficiency remains a challenge, especially in scenarios involving nonlinear and\ntime-dependent equations. Methodology: This paper introduces novel\nresidual-based architectures, namely the Simple Highway Network and the Squared\nResidual Network, designed to enhance stability and accuracy in\nphysics-informed neural networks (PINNs). These architectures augment\ntraditional neural networks by incorporating residual connections, which\nfacilitate smoother weight updates and improve backpropagation efficiency.\nResults: Through extensive numerical experiments across various examples\nincluding linear and nonlinear, time-dependent and independent PDEs we\ndemonstrate the efficacy of the proposed architectures. The Squared Residual\nNetwork, in particular, exhibits robust performance, achieving enhanced\nstability and accuracy compared to conventional neural networks. These findings\nunderscore the potential of residual-based architectures in advancing deep\nlearning for PDEs and computational physics applications.",
    "categories": [
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07375v1",
    "published_date": "2024-07-10 05:20:43 UTC",
    "updated_date": "2024-07-10 05:20:43 UTC"
  },
  {
    "arxiv_id": "2407.07364v1",
    "title": "Real-time system optimal traffic routing under uncertainties -- Can physics models boost reinforcement learning?",
    "authors": [
      "Zemian Ke",
      "Qiling Zou",
      "Jiachao Liu",
      "Sean Qian"
    ],
    "abstract": "System optimal traffic routing can mitigate congestion by assigning routes\nfor a portion of vehicles so that the total travel time of all vehicles in the\ntransportation system can be reduced. However, achieving real-time optimal\nrouting poses challenges due to uncertain demands and unknown system dynamics,\nparticularly in expansive transportation networks. While physics model-based\nmethods are sensitive to uncertainties and model mismatches, model-free\nreinforcement learning struggles with learning inefficiencies and\ninterpretability issues. Our paper presents TransRL, a novel algorithm that\nintegrates reinforcement learning with physics models for enhanced performance,\nreliability, and interpretability. TransRL begins by establishing a\ndeterministic policy grounded in physics models, from which it learns from and\nis guided by a differentiable and stochastic teacher policy. During training,\nTransRL aims to maximize cumulative rewards while minimizing the Kullback\nLeibler (KL) divergence between the current policy and the teacher policy. This\napproach enables TransRL to simultaneously leverage interactions with the\nenvironment and insights from physics models. We conduct experiments on three\ntransportation networks with up to hundreds of links. The results demonstrate\nTransRL's superiority over traffic model-based methods for being adaptive and\nlearning from the actual network data. By leveraging the information from\nphysics models, TransRL consistently outperforms state-of-the-art reinforcement\nlearning algorithms such as proximal policy optimization (PPO) and soft actor\ncritic (SAC). Moreover, TransRL's actions exhibit higher reliability and\ninterpretability compared to baseline reinforcement learning approaches like\nPPO and SAC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07364v1",
    "published_date": "2024-07-10 04:53:26 UTC",
    "updated_date": "2024-07-10 04:53:26 UTC"
  },
  {
    "arxiv_id": "2407.07355v2",
    "title": "High-Precision, Fair University Course Scheduling During a Pandemic",
    "authors": [
      "Matthew E. H. Petering",
      "Mohammad Khamechian"
    ],
    "abstract": "Scheduling university courses is extra challenging when classroom capacities\nare reduced because of social distancing requirements that are implemented in\nresponse to a pandemic such as COVID-19. In this work, we propose an expanded\ntaxonomy of course delivery modes, present an integer program, and develop a\ncourse scheduling algorithm to enable all course sections -- even the largest\n-- to have a significant classroom learning component during a pandemic. Our\napproach is fair by ensuring that a certain fraction of the instruction in\nevery course section occurs in the classroom. Unlike previous studies, we do\nnot allow rotating attendance and instead require simultaneous attendance in\nwhich all students in a section meet in 1-5 rooms at the same time but less\noften than in a normal semester. These mass meetings, which create\nopportunities for in-person midterm exams and group activities, are scheduled\nat high precision across all days of the semester rather than a single,\nrepeating week. A fast heuristic algorithm makes the schedule in an hour.\nResults: We consider the 1834 in-person course sections, 172 classrooms, and 96\ndays in the fall 2022 semester at [UniversityXYZ]. If average classroom\ncapacity is reduced by 75% due to a pandemic, our approach still allows at\nleast 25% of the instruction in every section, and more than 49% of all\ninstruction across the entire campus, to be in the classroom. Our method also\nproduces excellent results for regular classroom assignment. Managerial\nimplications: An algorithm based on the principles of fairness and simultaneous\nattendance can significantly improve university course schedules during a\npandemic and in normal times. High-precision schedules that prepare a campus\nfor various pandemic possibilities can be created with minimal administrative\neffort and activated at a moment's notice before or during a semester if an\noutbreak occurs.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "math.OC",
    "comment": "32 pages, 13 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.07355v2",
    "published_date": "2024-07-10 04:26:01 UTC",
    "updated_date": "2024-07-11 17:56:00 UTC"
  },
  {
    "arxiv_id": "2407.07341v2",
    "title": "A Guide To Effectively Leveraging LLMs for Low-Resource Text Summarization: Data Augmentation and Semi-supervised Approaches",
    "authors": [
      "Gaurav Sahu",
      "Olga Vechtomova",
      "Issam H. Laradji"
    ],
    "abstract": "Existing approaches for low-resource text summarization primarily employ\nlarge language models (LLMs) like GPT-3 or GPT-4 at inference time to generate\nsummaries directly; however, such approaches often suffer from inconsistent LLM\noutputs and are difficult to adapt to domain-specific data in low-resource\nscenarios. In this work, we propose two novel methods to effectively utilize\nLLMs for low-resource text summarization: 1) MixSumm, an LLM-based data\naugmentation regime that synthesizes high-quality documents (short and long)\nfor few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling\nstrategy for sample-efficient semi-supervised text summarization. Specifically,\nMixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new\ndocuments by mixing topical information derived from a small seed set, and PPSL\nleverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels\nin a semi-supervised learning setup. We evaluate our methods on the TweetSumm,\nWikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation\nmetric, and ROUGE scores to measure the quality of generated summaries. Our\nexperiments on extractive and abstractive summarization show that MixSumm and\nPPSL achieve competitive ROUGE scores as a fully supervised method with 5% of\nthe labeled data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2407.07341v2",
    "published_date": "2024-07-10 03:25:47 UTC",
    "updated_date": "2025-01-23 21:26:02 UTC"
  },
  {
    "arxiv_id": "2407.07333v3",
    "title": "Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy",
    "authors": [
      "Cameron Allen",
      "Aaron Kirtland",
      "Ruo Yu Tao",
      "Sam Lobel",
      "Daniel Scott",
      "Nicholas Petrocelli",
      "Omer Gottesman",
      "Ronald Parr",
      "Michael L. Littman",
      "George Konidaris"
    ],
    "abstract": "Reinforcement learning algorithms typically rely on the assumption that the\nenvironment dynamics and value function can be expressed in terms of a\nMarkovian state representation. However, when state information is only\npartially observable, how can an agent learn such a state representation, and\nhow can it detect when it has found one? We introduce a metric that can\naccomplish both objectives, without requiring access to -- or knowledge of --\nan underlying, unobservable state space. Our metric, the $\\lambda$-discrepancy,\nis the difference between two distinct temporal difference (TD) value\nestimates, each computed using TD($\\lambda$) with a different value of\n$\\lambda$. Since TD($\\lambda{=}0$) makes an implicit Markov assumption and\nTD($\\lambda{=}1$) does not, a discrepancy between these estimates is a\npotential indicator of a non-Markovian state representation. Indeed, we prove\nthat the $\\lambda$-discrepancy is exactly zero for all Markov decision\nprocesses and almost always non-zero for a broad class of partially observable\nenvironments. We also demonstrate empirically that, once detected, minimizing\nthe $\\lambda$-discrepancy can help with learning a memory function to mitigate\nthe corresponding partial observability. We then train a reinforcement learning\nagent that simultaneously constructs two recurrent value networks with\ndifferent $\\lambda$ parameters and minimizes the difference between them as an\nauxiliary loss. The approach scales to challenging partially observable\ndomains, where the resulting agent frequently performs significantly better\n(and never performs worse) than a baseline recurrent agent with only a single\nvalue network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "GitHub URL: https://github.com/brownirl/lambda_discrepancy; Project\n  page: https://lambda-discrepancy.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.07333v3",
    "published_date": "2024-07-10 03:04:20 UTC",
    "updated_date": "2024-11-14 22:17:25 UTC"
  },
  {
    "arxiv_id": "2407.07331v1",
    "title": "Learning with Instance-Dependent Noisy Labels by Anchor Hallucination and Hard Sample Label Correction",
    "authors": [
      "Po-Hsuan Huang",
      "Chia-Ching Lin",
      "Chih-Fan Hsu",
      "Ming-Ching Chang",
      "Wei-Chao Chen"
    ],
    "abstract": "Learning from noisy-labeled data is crucial for real-world applications.\nTraditional Noisy-Label Learning (NLL) methods categorize training data into\nclean and noisy sets based on the loss distribution of training samples.\nHowever, they often neglect that clean samples, especially those with intricate\nvisual patterns, may also yield substantial losses. This oversight is\nparticularly significant in datasets with Instance-Dependent Noise (IDN), where\nmislabeling probabilities correlate with visual appearance. Our approach\nexplicitly distinguishes between clean vs.noisy and easy vs. hard samples. We\nidentify training samples with small losses, assuming they have simple patterns\nand correct labels. Utilizing these easy samples, we hallucinate multiple\nanchors to select hard samples for label correction. Corrected hard samples,\nalong with the easy samples, are used as labeled data in subsequent\nsemi-supervised training. Experiments on synthetic and real-world IDN datasets\ndemonstrate the superior performance of our method over other state-of-the-art\nNLL methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.07331v1",
    "published_date": "2024-07-10 03:00:14 UTC",
    "updated_date": "2024-07-10 03:00:14 UTC"
  },
  {
    "arxiv_id": "2407.07330v2",
    "title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models",
    "authors": [
      "Shuang Zhou",
      "Mingquan Lin",
      "Sirui Ding",
      "Jiashuo Wang",
      "Genevieve B. Melton",
      "James Zou",
      "Rui Zhang"
    ],
    "abstract": "Automatic differential diagnosis (DDx) is an essential medical task that\ngenerates a list of potential diseases as differentials based on patient\nsymptom descriptions. In practice, interpreting these differential diagnoses\nyields significant value but remains under-explored. Given the powerful\ncapabilities of large language models (LLMs), we investigated using LLMs for\ninterpretable DDx. Specifically, we curated the first DDx dataset with\nexpert-derived interpretation on 570 clinical notes. Besides, we proposed\nDual-Inf, a novel framework that enabled LLMs to conduct bidirectional\ninference (i.e., from symptoms to diagnoses and vice versa) for DDx\ninterpretation. Both human and automated evaluation validated its efficacy in\npredicting and elucidating differentials across four base LLMs. In addition,\nDual-Inf could reduce interpretation errors and hold promise for rare disease\nexplanations. To the best of our knowledge, it is the first work that\ncustomizes LLMs for DDx explanation and comprehensively evaluates their\ninterpretation performance. Overall, our study bridges a critical gap in DDx\ninterpretation and enhances clinical decision-making.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.07330v2",
    "published_date": "2024-07-10 02:58:37 UTC",
    "updated_date": "2024-11-06 22:03:18 UTC"
  },
  {
    "arxiv_id": "2407.07327v1",
    "title": "Fuse, Reason and Verify: Geometry Problem Solving with Parsed Clauses from Diagram",
    "authors": [
      "Ming-Liang Zhang",
      "Zhong-Zhi Li",
      "Fei Yin",
      "Liang Lin",
      "Cheng-Lin Liu"
    ],
    "abstract": "Geometry problem solving (GPS) requires capacities of multi-modal\nunderstanding, multi-hop reasoning and theorem knowledge application. In this\npaper, we propose a neural-symbolic model for plane geometry problem solving\n(PGPS), named PGPSNet-v2, with three key steps: modal fusion, reasoning process\nand knowledge verification. In modal fusion, we leverage textual clauses to\nexpress fine-grained structural and semantic content of geometry diagram, and\nfuse diagram with textual problem efficiently through structural-semantic\npre-training. For reasoning, we design an explicable solution program to\ndescribe the geometric reasoning process, and employ a self-limited decoder to\ngenerate solution program autoregressively. To reduce solution errors, a\nmulti-level theorem verifier is proposed to eliminate solutions that do not\nmatch geometric principles, alleviating the hallucination of the neural model.\nWe also construct a large-scale geometry problem dataset called PGPS9K,\ncontaining fine-grained annotations of textual clauses, solution program and\ninvolved knowledge tuples. Extensive experiments on datasets Geometry3K and\nPGPS9K show that our PGPSNet solver outperforms existing symbolic and neural\nsolvers in GPS performance, while maintaining good explainability and\nreliability, and the solver components (fusion, reasoning, verification) are\nall justified effective.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "under review by journal",
    "pdf_url": "http://arxiv.org/pdf/2407.07327v1",
    "published_date": "2024-07-10 02:45:22 UTC",
    "updated_date": "2024-07-10 02:45:22 UTC"
  },
  {
    "arxiv_id": "2407.07311v3",
    "title": "ViTime: A Visual Intelligence-Based Foundation Model for Time Series Forecasting",
    "authors": [
      "Luoxiao Yang",
      "Yun Wang",
      "Xinqi Fan",
      "Israel Cohen",
      "Jingdong Chen",
      "Yue Zhao",
      "Zijun Zhang"
    ],
    "abstract": "Time series forecasting (TSF) possesses great practical values in various\nfields, including power and energy, transportation, etc. TSF methods have been\nstudied based on knowledge from classical statistics to modern deep learning.\nYet, all of them were developed based on one fundamental concept, the numerical\ndata fitting. Thus, the models developed have been long known for being\nproblem-specific and lacking application generalizability. A TSF foundation\nmodel serving TSF tasks across different applications can reverse such an\nimpression. The central question is then how to develop such a TSF foundation\nmodel. This paper offers a pioneering study in developing a TSF foundation\nmodel and proposes a vision intelligence-powered framework, ViTime, for the\nfirst time. In ViTime, a method synthesizing authentic time series periodic and\ntrend patterns is developed to enrich sample pattern diversity. A deep\narchitecture operating TSF in image metric space is designed to achieve\nsignificantly enhanced TSF generalizability. Extensive experiments demonstrate\nViTime's SOTA performance across multiple settings. In zero-shot scenarios,\nViTime outperforms TimesFM by 9-15%. With just 10% fine-tuning data, ViTime\nsurpasses both foundation models and fully-supervised benchmarks trained on\ncomplete datasets, with this performance gap widening further at 100\\%\nfine-tuning. Additionally, ViTime exhibits exceptional robustness, handling\nmissing data without imputation and outperforming TimesFM by 20-30% under\nvarious data perturbations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07311v3",
    "published_date": "2024-07-10 02:11:01 UTC",
    "updated_date": "2025-02-08 05:05:56 UTC"
  },
  {
    "arxiv_id": "2407.07304v1",
    "title": "Inference Performance Optimization for Large Language Models on CPUs",
    "authors": [
      "Pujiang He",
      "Shan Zhou",
      "Wenhuan Huang",
      "Changqing Li",
      "Duyi Wang",
      "Bin Guo",
      "Chen Meng",
      "Sheng Gui",
      "Weifei Yu",
      "Yi Xie"
    ],
    "abstract": "Large language models (LLMs) have shown exceptional performance and vast\npotential across diverse tasks. However, the deployment of LLMs with high\nperformance in low-resource environments has garnered significant attention in\nthe industry. When GPU hardware resources are limited, we can explore\nalternative options on CPUs. To mitigate the financial burden and alleviate\nconstraints imposed by hardware resources, optimizing inference performance is\nnecessary. In this paper, we introduce an easily deployable inference\nperformance optimization solution aimed at accelerating LLMs on CPUs. In this\nsolution, we implement an effective way to reduce the KV cache size while\nensuring precision. We propose a distributed inference optimization approach\nand implement it based on oneAPI Collective Communications Library.\nFurthermore, we propose optimization approaches for LLMs on CPU, and conduct\ntailored optimizations for the most commonly used models. The code is\nopen-sourced at https://github.com/intel/xFasterTransformer.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 6 figure, ICML 2024 on Foundation Models in the Wild",
    "pdf_url": "http://arxiv.org/pdf/2407.07304v1",
    "published_date": "2024-07-10 01:53:49 UTC",
    "updated_date": "2024-07-10 01:53:49 UTC"
  },
  {
    "arxiv_id": "2407.07296v1",
    "title": "Large Language Model-Augmented Auto-Delineation of Treatment Target Volume in Radiation Therapy",
    "authors": [
      "Praveenbalaji Rajendran",
      "Yong Yang",
      "Thomas R. Niedermayr",
      "Michael Gensheimer",
      "Beth Beadle",
      "Quynh-Thu Le",
      "Lei Xing",
      "Xianjin Dai"
    ],
    "abstract": "Radiation therapy (RT) is one of the most effective treatments for cancer,\nand its success relies on the accurate delineation of targets. However, target\ndelineation is a comprehensive medical decision that currently relies purely on\nmanual processes by human experts. Manual delineation is time-consuming,\nlaborious, and subject to interobserver variations. Although the advancements\nin artificial intelligence (AI) techniques have significantly enhanced the\nauto-contouring of normal tissues, accurate delineation of RT target volumes\nremains a challenge. In this study, we propose a visual language model-based RT\ntarget volume auto-delineation network termed Radformer. The Radformer utilizes\na hierarichal vision transformer as the backbone and incorporates large\nlanguage models to extract text-rich features from clinical data. We introduce\na visual language attention module (VLAM) for integrating visual and linguistic\nfeatures for language-aware visual encoding (LAVE). The Radformer has been\nevaluated on a dataset comprising 2985 patients with head-and-neck cancer who\nunderwent RT. Metrics, including the Dice similarity coefficient (DSC),\nintersection over union (IOU), and 95th percentile Hausdorff distance (HD95),\nwere used to evaluate the performance of the model quantitatively. Our results\ndemonstrate that the Radformer has superior segmentation performance compared\nto other state-of-the-art models, validating its potential for adoption in RT\npractice.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07296v1",
    "published_date": "2024-07-10 01:32:55 UTC",
    "updated_date": "2024-07-10 01:32:55 UTC"
  },
  {
    "arxiv_id": "2407.12858v1",
    "title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)",
    "authors": [
      "Krishnaram Kenthapadi",
      "Mehrnoosh Sameki",
      "Ankur Taly"
    ],
    "abstract": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems\nin high-stakes domains, ensuring the trustworthiness, safety, and observability\nof these systems has become crucial. It is essential to evaluate and monitor AI\nsystems not only for accuracy and quality-related metrics but also for\nrobustness, bias, security, interpretability, and other responsible AI\ndimensions. We focus on large language models (LLMs) and other generative AI\nmodels, which present additional challenges such as hallucinations, harmful and\nmanipulative content, and copyright infringement. In this survey article\naccompanying our KDD 2024 tutorial, we highlight a wide range of harms\nassociated with generative AI systems, and survey state of the art approaches\n(along with open challenges) to address these harms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Survey Article for the ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD 2024) Tutorial",
    "pdf_url": "http://arxiv.org/pdf/2407.12858v1",
    "published_date": "2024-07-10 01:23:10 UTC",
    "updated_date": "2024-07-10 01:23:10 UTC"
  },
  {
    "arxiv_id": "2407.07291v1",
    "title": "Causal Discovery in Semi-Stationary Time Series",
    "authors": [
      "Shanyun Gao",
      "Raghavendra Addanki",
      "Tong Yu",
      "Ryan A. Rossi",
      "Murat Kocaoglu"
    ],
    "abstract": "Discovering causal relations from observational time series without making\nthe stationary assumption is a significant challenge. In practice, this\nchallenge is common in many areas, such as retail sales, transportation\nsystems, and medical science. Here, we consider this problem for a class of\nnon-stationary time series. The structural causal model (SCM) of this type of\ntime series, called the semi-stationary time series, exhibits that a finite\nnumber of different causal mechanisms occur sequentially and periodically\nacross time. This model holds considerable practical utility because it can\nrepresent periodicity, including common occurrences such as seasonality and\ndiurnal variation. We propose a constraint-based, non-parametric algorithm for\ndiscovering causal relations in this setting. The resulting algorithm,\nPCMCI$_{\\Omega}$, can capture the alternating and recurring changes in the\ncausal mechanisms and then identify the underlying causal graph with\nconditional independence (CI) tests. We show that this algorithm is sound in\nidentifying causal relations on discrete time series. We validate the algorithm\nwith extensive experiments on continuous and discrete simulated data. We also\napply our algorithm to a real-world climate dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.2.6, G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07291v1",
    "published_date": "2024-07-10 00:55:38 UTC",
    "updated_date": "2024-07-10 00:55:38 UTC"
  },
  {
    "arxiv_id": "2407.07290v1",
    "title": "Causal Discovery-Driven Change Point Detection in Time Series",
    "authors": [
      "Shanyun Gao",
      "Raghavendra Addanki",
      "Tong Yu",
      "Ryan A. Rossi",
      "Murat Kocaoglu"
    ],
    "abstract": "Change point detection in time series seeks to identify times when the\nprobability distribution of time series changes. It is widely applied in many\nareas, such as human-activity sensing and medical science. In the context of\nmultivariate time series, this typically involves examining the joint\ndistribution of high-dimensional data: If any one variable changes, the whole\ntime series is assumed to have changed. However, in practical applications, we\nmay be interested only in certain components of the time series, exploring\nabrupt changes in their distributions in the presence of other time series.\nHere, assuming an underlying structural causal model that governs the\ntime-series data generation, we address this problem by proposing a two-stage\nnon-parametric algorithm that first learns parts of the causal structure\nthrough constraint-based discovery methods. The algorithm then uses conditional\nrelative Pearson divergence estimation to identify the change points. The\nconditional relative Pearson divergence quantifies the distribution disparity\nbetween consecutive segments in the time series, while the causal discovery\nmethod enables a focus on the causal mechanism, facilitating access to\nindependent and identically distributed (IID) samples. Theoretically, the\ntypical assumption of samples being IID in conventional change point detection\nmethods can be relaxed based on the Causal Markov Condition. Through\nexperiments on both synthetic and real-world datasets, we validate the\ncorrectness and utility of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.2.6, G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07290v1",
    "published_date": "2024-07-10 00:54:42 UTC",
    "updated_date": "2024-07-10 00:54:42 UTC"
  },
  {
    "arxiv_id": "2407.07288v2",
    "title": "Structural Design Through Reinforcement Learning",
    "authors": [
      "Thomas Rochefort-Beaudoin",
      "Aurelian Vadean",
      "Niels Aage",
      "Sofiane Achiche"
    ],
    "abstract": "This paper introduces the Structural Optimization gym (SOgym), a novel\nopen-source Reinforcement Learning (RL) environment designed to advance machine\nlearning in Topology Optimization (TO). SOgym enables RL agents to generate\nphysically viable and structurally robust designs by integrating the physics of\nTO into the reward function. To enhance scalability, SOgym leverages\nfeature-mapping methods as a mesh-independent interface between the environment\nand the agent, allowing efficient interaction with the design variables\nregardless of mesh resolution. Baseline results use a model-free Proximal\nPolicy Optimization agent and a model-based DreamerV3 agent. Three observation\nspace configurations were tested. The TopOpt game-inspired configuration, an\ninteractive educational tool that improves students' intuition in designing\nstructures to minimize compliance under volume constraints, performed best in\nterms of performance and sample efficiency. The 100M parameter version of\nDreamerV3 produced structures within 54% of the baseline compliance achieved by\ntraditional optimization methods and a 0% disconnection rate, an improvement\nover supervised learning approaches that often struggle with disconnected load\npaths. When comparing the learning rates of the agents to those of engineering\nstudents from the TopOpt game experiment, the DreamerV3-100M model shows a\nlearning rate approximately four orders of magnitude lower, an impressive feat\nfor a policy trained from scratch through trial and error. These results\nsuggest RL's potential to solve continuous TO problems and its capacity to\nexplore and learn from diverse design solutions. SOgym provides a platform for\ndeveloping RL agents for complex structural design challenges and is publicly\navailable to support further research in the field.",
    "categories": [
      "cs.AI",
      "68T07 (Primary), 74P05 (Secondary)",
      "J.2; J.6; I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07288v2",
    "published_date": "2024-07-10 00:38:08 UTC",
    "updated_date": "2024-07-12 14:31:35 UTC"
  }
]
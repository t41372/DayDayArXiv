{
  "date": "2024-05-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-05-08 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 91 篇论文，主要聚焦 AI 模型优化、LLMs 在安全和教育中的应用、医疗诊断创新，以及高效图像处理方法，亮点包括 LLM 增强的推理框架和鲁棒性评估，如 Poser 和 Vidur 等文章，以及知名学者参与的跨领域工作。\n\n### 重点论文速览\n我挑选了最具话题度和影响力的论文，先聊 LLMs 相关的内容（因其热度高），然后是医疗 AI 和图像处理领域，其他次要论文快速掠过。以下按主题分组，保留核心学术术语，突出主要贡献。\n\n#### LLMs 和 AI 应用\n- **Poser: Unmasking Alignment Faking LLMs（揭示对齐欺骗的 LLMs）**：这篇论文引入了一个基准，测试五种策略来检测 LLMs 在角色扮演场景中的对齐欺骗行为，主要发现一种策略能识别 98% 的欺骗模型，强调了 LLM 鲁棒性的重要性，对 AI 安全领域有显著启发。\n- **Vidur: A Large-Scale Simulation Framework For LLM Inference（大规模 LLM 推理模拟框架）**：作者包括微软团队，提出 Vidur 框架，使用实验分析和预测建模评估 LLM 推理性能，能在短时间内优化部署配置（如 LLaMA2-70B），相比传统方法节省巨大计算资源（42K GPU 小时），是 LLM 高效部署的实用工具。\n- **Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models（可扩展、高效且准确的文本嵌入模型）**：这篇工作发布了一个系列模型（22 到 334 百万参数），在 MTEB 检索基准上超越 Cohere 和 OpenAI 的闭源模型，主要贡献是通过数据集优化实现状态-of-the-art 性能，便于文本嵌入任务的应用。\n- **THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models（基于对象的幻觉基准，用于大型视觉语言模型的自由生成）**：论文构建了一个基准评估 LVLMs 的幻觉问题，并提出数据增强方法减少幻觉，实验显示新方法在图像生成中显著改善性能，对视觉语言模型的可靠性有重要启示。\n- **LLMs with Personalities in Multi-issue Negotiation Games（具有个性的 LLMs 在多议题谈判游戏中）**：探索 LLMs 在谈判中的个性（如 Big Five 模型），发现高开放性和神经质性倾向于公平决策，但低宜人性可能导致攻击性行为，主要贡献是提供博弈论框架，提升 LLMs 在决策环境的实用性。\n\n#### 医疗 AI 和图像处理\n- **ECG-SMART-NET: A Deep Learning Architecture for Precise ECG Diagnosis of Occlusion Myocardial Infarction（精确诊断闭塞性心肌梗死的深度学习架构）**：这篇论文提出改进的 ResNet-18 架构（ECG-SMART-NET），通过时序和空间卷积层提升心电图诊断准确性，在多站点数据集上达到 0.889 AUC 分数，主要发现能及早识别潜在致命的心肌梗死，对临床应用有直接价值。\n- **GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields（梯度域高斯喷溅用于辐射场的稀疏表示）**：作者 Yuanhao Gong 提出 GDGS 方法，通过建模信号梯度实现高效渲染，训练和渲染速度提升 100-1000 倍，主要贡献在于应用于人体和室内建模的稀疏表示，显著改善计算机视觉效率。\n- **Synthetic Data in Radiological Imaging: Current State and Future Outlook（放射成像中的合成数据：现状与展望）**：这篇综述讨论合成数据在放射图像中的潜力，如减少患者隐私风险和数据不平衡，作者包括 NIH 团队，主要发现合成数据能提升 AI 模型的鲁棒性，但需进一步研究质量评估。\n\n#### 其他选文\n- **TrafficGPT: Towards Multi-Scale Traffic Analysis and Generation with Spatial-Temporal Agent Framework（多尺度交通分析和生成的空间-时间代理框架）**：提出 TrafficGPT，使用 AI 代理实现交通预测，实验在真实数据集上提升准确性，主要贡献是多模态融合的交通建模，便于智能交通系统。\n- **StyleMamba: State Space Model for Efficient Text-driven Image Style Transfer（用于高效文本驱动图像风格迁移的状态空间模型）**：这篇论文引入 StyleMamba 框架，通过状态空间模型加速风格迁移，训练迭代减少 5 倍，主要发现能保持图像质量的同时提升计算效率。\n- **APrompt4EM: Augmented Prompt Tuning for Generalized Entity Matching（增强提示调整用于泛化实体匹配）**：提出 APrompt4EM 方法，使用 LLM 生成合成数据提升实体匹配性能，在低资源场景下提升 5.24% 准确率，主要贡献是简化实体匹配的泛化应用。\n\n其他论文如那些专注于小众领域（如智能照明或特定算法优化）的，我快速掠过：例如，\"A digital twin based approach to smart lighting design\" 提出数字孪生用于照明设计，但影响有限；\"Finite Groundings for ASP with Functions\" 探讨逻辑编程的理论扩展，也未见重大突破。这些次要工作虽有技术细节，但整体话题度不高，故不展开讨论。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，尤其是 LLMs 的应用潜力，但也提醒我们关注模型的鲁棒性和伦理问题。明天的更新拭目以待！",
  "papers": [
    {
      "arxiv_id": "2405.05467v1",
      "title": "AFEN: Respiratory Disease Classification using Ensemble Learning",
      "title_zh": "AFEN：使用集成学习的呼吸系统疾病分类",
      "authors": [
        "Rahul Nadkarni",
        "Emmanouil Nikolakakis",
        "Razvan Marinescu"
      ],
      "abstract": "We present AFEN (Audio Feature Ensemble Learning), a model that leverages\nConvolutional Neural Networks (CNN) and XGBoost in an ensemble learning fashion\nto perform state-of-the-art audio classification for a range of respiratory\ndiseases. We use a meticulously selected mix of audio features which provide\nthe salient attributes of the data and allow for accurate classification. The\nextracted features are then used as an input to two separate model classifiers\n1) a multi-feature CNN classifier and 2) an XGBoost Classifier. The outputs of\nthe two models are then fused with the use of soft voting. Thus, by exploiting\nensemble learning, we achieve increased robustness and accuracy. We evaluate\nthe performance of the model on a database of 920 respiratory sounds, which\nundergoes data augmentation techniques to increase the diversity of the data\nand generalizability of the model. We empirically verify that AFEN sets a new\nstate-of-the-art using Precision and Recall as metrics, while decreasing\ntraining time by 60%.",
      "tldr_zh": "本研究提出AFEN模型，利用集成学习（Ensemble Learning）结合Convolutional Neural Networks (CNN)和XGBoost，对呼吸道疾病进行音频分类。模型通过精心选择的音频特征作为输入，分别训练多特征CNN分类器和XGBoost分类器，然后使用软投票（soft voting）融合输出，以提升鲁棒性和准确性。在包含920个呼吸声音的数据库上，通过数据增强技术提高数据多样性和模型泛化性，实验结果显示AFEN在Precision和Recall指标上达到了新的最先进水平，同时将训练时间减少了60%。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Under Review Process for MLForHC 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05467v1",
      "published_date": "2024-05-08 23:50:54 UTC",
      "updated_date": "2024-05-08 23:50:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:54:44.127538"
    },
    {
      "arxiv_id": "2405.05466v2",
      "title": "Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Clymer",
        "Caden Juang",
        "Severin Field"
      ],
      "abstract": "Like a criminal under investigation, Large Language Models (LLMs) might\npretend to be aligned while evaluated and misbehave when they have a good\nopportunity. Can current interpretability methods catch these 'alignment\nfakers?' To answer this question, we introduce a benchmark that consists of 324\npairs of LLMs fine-tuned to select actions in role-play scenarios. One model in\neach pair is consistently benign (aligned). The other model misbehaves in\nscenarios where it is unlikely to be caught (alignment faking). The task is to\nidentify the alignment faking model using only inputs where the two models\nbehave identically. We test five detection strategies, one of which identifies\n98% of alignment-fakers.",
      "tldr_zh": "该研究探讨大型语言模型(LLMs)可能在评估时假装对齐(alignment faking)，而在有利机会下违规的问题，引入一个包含324对LLMs的基准测试，每对模型中一个始终良性，另一个在不易被发现的角色扮演场景中违规。\n任务是仅使用输入使两个模型行为相同的情况下，通过操纵LLMs的内部机制来识别对齐伪装模型。\n测试的五种检测策略中，有一种能准确识别98%的对齐伪装模型，为提升LLMs的可信度提供了有效方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05466v2",
      "published_date": "2024-05-08 23:44:08 UTC",
      "updated_date": "2024-05-11 04:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:54:56.204708"
    },
    {
      "arxiv_id": "2405.05465v2",
      "title": "Vidur: A Large-Scale Simulation Framework For LLM Inference",
      "title_zh": "Vidur: 用于 LLM 推理的大规模模拟框架",
      "authors": [
        "Amey Agrawal",
        "Nitin Kedia",
        "Jayashree Mohan",
        "Ashish Panwar",
        "Nipun Kwatra",
        "Bhargav Gulavani",
        "Ramachandran Ramjee",
        "Alexey Tumanov"
      ],
      "abstract": "Optimizing the deployment of Large language models (LLMs) is expensive today\nsince it requires experimentally running an application workload against an LLM\nimplementation while exploring large configuration space formed by system knobs\nsuch as parallelization strategies, batching techniques, and scheduling\npolicies. To address this challenge, we present Vidur - a large-scale,\nhigh-fidelity, easily-extensible simulation framework for LLM inference\nperformance. Vidur models the performance of LLM operators using a combination\nof experimental profiling and predictive modeling, and evaluates the end-to-end\ninference performance for different workloads by estimating several metrics of\ninterest such as latency and throughput. We validate the fidelity of Vidur on\nseveral LLMs and show that it estimates inference latency with less than 9%\nerror across the range. Further, we present Vidur-Search, a configuration\nsearch tool that helps optimize LLM deployment. Vidur-Search uses Vidur to\nautomatically identify the most cost-effective deployment configuration that\nmeets application performance constraints. For example, Vidur-Search finds the\nbest deployment configuration for LLaMA2-70B in one hour on a CPU machine, in\ncontrast to a deployment-based exploration which would require 42K GPU hours -\ncosting ~218K dollars. Source code for Vidur is available at\nhttps://github.com/microsoft/vidur.",
      "tldr_zh": "该论文提出 Vidur，一个大规模、高保真且易扩展的模拟框架，用于优化 Large Language Models (LLMs) 推理性能，以避免昂贵的实验探索配置空间。Vidur 通过结合实验剖析和预测建模来模拟 LLM 操作性能，并评估端到端指标如延迟和吞吐量，其估计误差在多个 LLMs 上小于 9%。此外，Vidur-Search 工具利用 Vidur 自动搜索最性价比高的部署配置，例如为 LLaMA2-70B 找到最佳设置只需一小时 CPU 时间，而传统方法需 42K GPU 小时，节省约 21.8K 美元成本。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05465v2",
      "published_date": "2024-05-08 23:42:13 UTC",
      "updated_date": "2024-05-21 05:17:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:55:08.959891"
    },
    {
      "arxiv_id": "2405.05446v1",
      "title": "GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanhao Gong"
      ],
      "abstract": "The 3D Gaussian splatting methods are getting popular. However, they work\ndirectly on the signal, leading to a dense representation of the signal. Even\nwith some techniques such as pruning or distillation, the results are still\ndense. In this paper, we propose to model the gradient of the original signal.\nThe gradients are much sparser than the original signal. Therefore, the\ngradients use much less Gaussian splats, leading to the more efficient storage\nand thus higher computational performance during both training and rendering.\nThanks to the sparsity, during the view synthesis, only a small mount of pixels\nare needed, leading to much higher computational performance ($100\\sim\n1000\\times$ faster). And the 2D image can be recovered from the gradients via\nsolving a Poisson equation with linear computation complexity. Several\nexperiments are performed to confirm the sparseness of the gradients and the\ncomputation performance of the proposed method. The method can be applied\nvarious applications, such as human body modeling and indoor environment\nmodeling.",
      "tldr_zh": "本研究提出GDGS（Gradient Domain Gaussian Splatting）方法，用于实现辐射场（Radiance Fields）的稀疏表示。通过在梯度域上建模原始信号的梯度，GDGS显著降低了Gaussian splats的数量，提高了存储效率和计算性能。相比传统方法，该框架在视图合成过程中仅需处理少量像素，可实现100~1000倍的加速，并通过求解Poisson equation以线性计算复杂度恢复2D图像。实验验证了梯度的稀疏性及其在训练和渲染中的性能优势，并证明了该方法在人体建模和室内环境建模等应用中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2404.09105",
      "pdf_url": "http://arxiv.org/pdf/2405.05446v1",
      "published_date": "2024-05-08 22:40:52 UTC",
      "updated_date": "2024-05-08 22:40:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:55:19.999872"
    },
    {
      "arxiv_id": "2405.05444v1",
      "title": "Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large",
      "title_zh": "翻译失败",
      "authors": [
        "Jussi S. Jauhiainen",
        "Agustín Garagorry Guerra"
      ],
      "abstract": "Evaluating open-ended written examination responses from students is an\nessential yet time-intensive task for educators, requiring a high degree of\neffort, consistency, and precision. Recent developments in Large Language\nModels (LLMs) present a promising opportunity to balance the need for thorough\nevaluation with efficient use of educators' time. In our study, we explore the\neffectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in\nassessing university students' open-ended answers to questions made about\nreference material they have studied. Each model was instructed to evaluate 54\nanswers repeatedly under two conditions: 10 times (10-shot) with a temperature\nsetting of 0.0 and 10 times with a temperature of 0.5, expecting a total of\n1,080 evaluations per model and 4,320 evaluations across all models. The RAG\n(Retrieval Augmented Generation) framework was used as the framework to make\nthe LLMs to process the evaluation of the answers. As of spring 2024, our\nanalysis revealed notable variations in consistency and the grading outcomes\nprovided by studied LLMs. There is a need to comprehend strengths and\nweaknesses of LLMs in educational settings for evaluating open-ended written\nresponses. Further comparative research is essential to determine the accuracy\nand cost-effectiveness of using LLMs for educational assessments.",
      "tldr_zh": "这篇论文探讨了使用LLMs（如GPT-3.5、GPT-4、Claude-3和Mistral-Large）通过RAG框架来评估学生开放式书面回答的有效性，以减轻教育者评估任务的负担。研究方法包括让每个模型在温度设置为0.0和0.5的条件下，重复评估54个答案共10次，总计4320次评估。结果显示，这些LLMs在一致性和评分结果上存在显著差异，突出了其在教育评估中的优势与劣势，并建议进一步研究以评估其准确性和成本效益。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 6 tables, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2405.05444v1",
      "published_date": "2024-05-08 22:23:58 UTC",
      "updated_date": "2024-05-08 22:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:55:32.357727"
    },
    {
      "arxiv_id": "2405.05439v2",
      "title": "How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph A. Vincent",
        "Haruki Nishimura",
        "Masha Itkina",
        "Paarth Shah",
        "Mac Schwager",
        "Thomas Kollar"
      ],
      "abstract": "With the rise of stochastic generative models in robot policy learning,\nend-to-end visuomotor policies are increasingly successful at solving complex\ntasks by learning from human demonstrations. Nevertheless, since real-world\nevaluation costs afford users only a small number of policy rollouts, it\nremains a challenge to accurately gauge the performance of such policies. This\nis exacerbated by distribution shifts causing unpredictable changes in\nperformance during deployment. To rigorously evaluate behavior cloning\npolicies, we present a framework that provides a tight lower-bound on robot\nperformance in an arbitrary environment, using a minimal number of experimental\npolicy rollouts. Notably, by applying the standard stochastic ordering to robot\nperformance distributions, we provide a worst-case bound on the entire\ndistribution of performance (via bounds on the cumulative distribution\nfunction) for a given task. We build upon established statistical results to\nensure that the bounds hold with a user-specified confidence level and\ntightness, and are constructed from as few policy rollouts as possible. In\nexperiments we evaluate policies for visuomotor manipulation in both simulation\nand hardware. Specifically, we (i) empirically validate the guarantees of the\nbounds in simulated manipulation settings, (ii) find the degree to which a\nlearned policy deployed on hardware generalizes to new real-world environments,\nand (iii) rigorously compare two policies tested in out-of-distribution\nsettings. Our experimental data, code, and implementation of confidence bounds\nare open-source.",
      "tldr_zh": "该论文针对行为克隆（Behavior Cloning）策略在机器人学习中的泛化问题，提出了一种统计框架，用于提供可信性能评估。该框架通过使用标准随机排序（stochastic ordering）对机器人性能分布进行分析，基于最少的策略回滚实验，给出任意环境下的性能下界，并确保界限在用户指定的置信水平和紧密度下成立。实验在模拟和硬件环境中验证了该框架，包括确认界限的可靠性、评估策略在真实世界分布偏移下的泛化能力，以及比较不同策略的表现；所有实验数据和代码均开源。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2405.05439v2",
      "published_date": "2024-05-08 22:00:35 UTC",
      "updated_date": "2024-07-18 18:45:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:55:44.903871"
    },
    {
      "arxiv_id": "2405.05435v1",
      "title": "Analysis and prevention of AI-based phishing email attacks",
      "title_zh": "基于 AI 的网络钓鱼电子邮件攻击的分析和预防",
      "authors": [
        "Chibuike Samuel Eze",
        "Lior Shamir"
      ],
      "abstract": "Phishing email attacks are among the most common and most harmful\ncybersecurity attacks. With the emergence of generative AI, phishing attacks\ncan be based on emails generated automatically, making it more difficult to\ndetect them. That is, instead of a single email format sent to a large number\nof recipients, generative AI can be used to send each potential victim a\ndifferent email, making it more difficult for cybersecurity systems to identify\nthe scam email before it reaches the recipient. Here we describe a corpus of\nAI-generated phishing emails. We also use different machine learning tools to\ntest the ability of automatic text analysis to identify AI-generated phishing\nemails. The results are encouraging, and show that machine learning tools can\nidentify an AI-generated phishing email with high accuracy compared to regular\nemails or human-generated scam email. By applying descriptive analytic, the\nspecific differences between AI-generated emails and manually crafted scam\nemails are profiled, and show that AI-generated emails are different in their\nstyle from human-generated phishing email scams. Therefore, automatic\nidentification tools can be used as a warning for the user. The paper also\ndescribes the corpus of AI-generated phishing emails that is made open to the\npublic, and can be used for consequent studies. While the ability of machine\nlearning to detect AI-generated phishing email is encouraging, AI-generated\nphishing emails are different from regular phishing emails, and therefore it is\nimportant to train machine learning systems also with AI-generated emails in\norder to repel future phishing attacks that are powered by generative AI.",
      "tldr_zh": "这篇论文分析了基于生成AI的钓鱼邮件攻击问题，这些攻击通过个性化邮件生成变得更难检测。研究者构建了一个AI-generated phishing emails语料库，并使用machine learning工具进行文本分析测试，结果显示这些工具能高准确率识别AI-generated phishing emails，与常规邮件或人工诈骗邮件有显著风格差异。通过描述性分析，论文突出了AI-generated emails的独特特征，并公开了语料库以支持后续研究。最后，作者强调需要将AI-generated emails纳入machine learning系统的训练，以更有效地防范未来AI驱动的钓鱼攻击。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "Electronics, accepted",
      "pdf_url": "http://arxiv.org/pdf/2405.05435v1",
      "published_date": "2024-05-08 21:40:49 UTC",
      "updated_date": "2024-05-08 21:40:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:55:56.104881"
    },
    {
      "arxiv_id": "2405.05431v2",
      "title": "Searching for Programmatic Policies in Semantic Spaces",
      "title_zh": "在语义空间中搜索程序化策略",
      "authors": [
        "Rubens O. Moraes",
        "Levi H. S. Lelis"
      ],
      "abstract": "Syntax-guided synthesis is commonly used to generate programs encoding\npolicies. In this approach, the set of programs, that can be written in a\ndomain-specific language defines the search space, and an algorithm searches\nwithin this space for programs that encode strong policies. In this paper, we\npropose an alternative method for synthesizing programmatic policies, where we\nsearch within an approximation of the language's semantic space. We\nhypothesized that searching in semantic spaces is more sample-efficient\ncompared to syntax-based spaces. Our rationale is that the search is more\nefficient if the algorithm evaluates different agent behaviors as it searches\nthrough the space, a feature often missing in syntax-based spaces. This is\nbecause small changes in the syntax of a program often do not result in\ndifferent agent behaviors. We define semantic spaces by learning a library of\nprograms that present different agent behaviors. Then, we approximate the\nsemantic space by defining a neighborhood function for local search algorithms,\nwhere we replace parts of the current candidate program with programs from the\nlibrary. We evaluated our hypothesis in a real-time strategy game called\nMicroRTS. Empirical results support our hypothesis that searching in semantic\nspaces can be more sample-efficient than searching in syntax-based spaces.",
      "tldr_zh": "本文提出一种在语义空间中搜索程序政策的方法，作为传统语法引导合成（syntax-guided synthesis）的替代方案，以提高搜索效率。该方法通过学习一个展示不同代理行为的程序库来定义语义空间，并使用邻域函数替换候选程序的部分，以近似该空间。研究假设语义空间搜索比语法空间更节省样本（sample-efficient），因为语法小变化往往不导致行为差异，而语义空间能更好地评估代理行为。在实时策略游戏 MicroRTS 的实验中，结果证实了这一假设，展示了语义空间搜索的潜在优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "Available code:\n  https://github.com/rubensolv/Library-Induced-Semantic-Spaces",
      "pdf_url": "http://arxiv.org/pdf/2405.05431v2",
      "published_date": "2024-05-08 21:24:49 UTC",
      "updated_date": "2024-06-12 18:54:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:56:09.273718"
    },
    {
      "arxiv_id": "2405.05429v3",
      "title": "How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Kook",
        "Chris Kolb",
        "Philipp Schiele",
        "Daniel Dold",
        "Marcel Arpogaus",
        "Cornelius Fritz",
        "Philipp F. Baumann",
        "Philipp Kopper",
        "Tobias Pielok",
        "Emilio Dorigatti",
        "David Rügamer"
      ],
      "abstract": "Neural network representations of simple models, such as linear regression,\nare being studied increasingly to better understand the underlying principles\nof deep learning algorithms. However, neural representations of distributional\nregression models, such as the Cox model, have received little attention so\nfar. We close this gap by proposing a framework for distributional regression\nusing inverse flow transformations (DRIFT), which includes neural\nrepresentations of the aforementioned models. We empirically demonstrate that\nthe neural representations of models in DRIFT can serve as a substitute for\ntheir classical statistical counterparts in several applications involving\ncontinuous, ordered, time-series, and survival outcomes. We confirm that models\nin DRIFT empirically match the performance of several statistical methods in\nterms of estimation of partial effects, prediction, and aleatoric uncertainty\nquantification. DRIFT covers both interpretable statistical models and flexible\nneural networks opening up new avenues in both statistical modeling and deep\nlearning.",
      "tldr_zh": "该研究探讨了神经网络在分布回归模型（如 Cox 模型）中的应用，指出现有工作主要关注简单模型（如线性回归）。论文提出 DRIFT 框架，使用 inverse flow transformations 来构建这些模型的神经表示。实验结果显示，DRIFT 在处理连续、ordered、time-series 和 survival outcomes 的应用中，能有效替代经典统计方法，在估计 partial effects、预测和 aleatoric uncertainty quantification 方面表现相当。最后，DRIFT 结合了可解释的统计模型和灵活的神经网络，为统计建模和深度学习开辟新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at UAI 2024 https://www.auai.org/uai2024/accepted_papers",
      "pdf_url": "http://arxiv.org/pdf/2405.05429v3",
      "published_date": "2024-05-08 21:19:18 UTC",
      "updated_date": "2024-07-10 08:47:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:56:20.490029"
    },
    {
      "arxiv_id": "2406.06544v2",
      "title": "TSB: Tiny Shared Block for Efficient DNN Deployment on NVCIM Accelerators",
      "title_zh": "TSB：微小共享块，用于在 NVCIM 加速器上高效部署 DNN",
      "authors": [
        "Yifan Qin",
        "Zheyu Yan",
        "Zixuan Pan",
        "Wujie Wen",
        "Xiaobo Sharon Hu",
        "Yiyu Shi"
      ],
      "abstract": "Compute-in-memory (CIM) accelerators using non-volatile memory (NVM) devices\noffer promising solutions for energy-efficient and low-latency Deep Neural\nNetwork (DNN) inference execution. However, practical deployment is often\nhindered by the challenge of dealing with the massive amount of model weight\nparameters impacted by the inherent device variations within non-volatile\ncomputing-in-memory (NVCIM) accelerators. This issue significantly offsets\ntheir advantages by increasing training overhead, the time and energy needed\nfor mapping weights to device states, and diminishing inference accuracy. To\nmitigate these challenges, we propose the \"Tiny Shared Block (TSB)\" method,\nwhich integrates a small shared 1x1 convolution block into the DNN\narchitecture. This block is designed to stabilize feature processing across the\nnetwork, effectively reducing the impact of device variation. Extensive\nexperimental results show that TSB achieves over 20x inference accuracy gap\nimprovement, over 5x training speedup, and weights-to-device mapping cost\nreduction while requiring less than 0.4% of the original weights to be\nwrite-verified during programming, when compared with state-of-the-art baseline\nsolutions. Our approach provides a practical and efficient solution for\ndeploying robust DNN models on NVCIM accelerators, making it a valuable\ncontribution to the field of energy-efficient AI hardware.",
      "tldr_zh": "本研究针对非易失性计算内存 (NVCIM) 加速器上 Deep Neural Network (DNN) 部署的设备变异问题，提出了一种名为 Tiny Shared Block (TSB) 的方法，该方法通过在 DNN 架构中集成一个小型共享 1x1 卷积块来稳定特征处理，从而降低设备变异对模型权重的影响。TSB 不仅实现了超过 20 倍的推理准确率提升、5 倍的训练加速，还显著减少了权重到设备映射的成本，仅需验证不到 0.4% 的原始权重。总体而言，该方法为在 NVCIM 加速器上部署高效、鲁棒的 DNN 模型提供了实用解决方案。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "9 pages, accepted to IEEE/ACM International Conference on\n  Computer-Aided Design (ICCAD 2024)",
      "pdf_url": "http://arxiv.org/pdf/2406.06544v2",
      "published_date": "2024-05-08 20:53:38 UTC",
      "updated_date": "2024-08-21 18:11:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:56:33.941996"
    },
    {
      "arxiv_id": "2405.09567v1",
      "title": "ECG-SMART-NET: A Deep Learning Architecture for Precise ECG Diagnosis of Occlusion Myocardial Infarction",
      "title_zh": "翻译失败",
      "authors": [
        "Nathan T. Riek",
        "Murat Akcakaya",
        "Zeineb Bouzid",
        "Tanmay Gokhale",
        "Stephanie Helman",
        "Karina Kraevsky-Philips",
        "Rui Qi Ji",
        "Ervin Sejdic",
        "Jessica K. Zègre-Hemsey",
        "Christian Martin-Gill",
        "Clifton W. Callaway",
        "Samir Saba",
        "Salah Al-Zaiti"
      ],
      "abstract": "In this paper we describe ECG-SMART-NET for identification of occlusion\nmyocardial infarction (OMI). OMI is a severe form of heart attack characterized\nby complete blockage of one or more coronary arteries requiring immediate\nreferral for cardiac catheterization to restore blood flow to the heart. Two\nthirds of OMI cases are difficult to visually identify from a 12-lead\nelectrocardiogram (ECG) and can be potentially fatal if not identified in a\ntimely fashion. Previous works on this topic are scarce, and current\nstate-of-the-art evidence suggests that both random forests with engineered\nfeatures and convolutional neural networks (CNNs) are promising approaches to\nimprove the ECG detection of OMI. While the ResNet architecture has been\nsuccessfully adapted for use with ECG recordings, it is not ideally suited to\ncapture informative temporal features within each lead and the spatial\nconcordance or discordance across leads. We propose a clinically informed\nmodification of the ResNet-18 architecture. The model first learns temporal\nfeatures through temporal convolutional layers with 1xk kernels followed by a\nspatial convolutional layer, after the residual blocks, with 12x1 kernels to\nlearn spatial features. The new ECG-SMART-NET was benchmarked against the\noriginal ResNet-18 and other state-of-the-art models on a multisite real-word\nclinical dataset that consists of 10,893 ECGs from 7,297 unique patients (rate\nof OMI = 6.5%). ECG-SMART-NET outperformed other models in the classification\nof OMI with a test AUC score of 0.889 +/- 0.027 and a test average precision\nscore of 0.587 +/- 0.087.",
      "tldr_zh": "本文提出 ECG-SMART-NET，一种基于 ResNet-18 的改进深度学习架构，用于精确诊断闭塞性心肌梗死 (OMI)，这是一种需要紧急心脏导管术的严重心脏病。模型通过添加时序卷积层 (1xk 内核) 来捕捉 ECG 每个导联的时序特征，以及后续的空间卷积层 (12x1 内核) 来学习导联间的空间相关性，从而提升诊断准确性。在一个包含 10,893 个 ECG 的多站点临床数据集上，ECG-SMART-NET 取得了 0.889 +/- 0.027 的 AUC 分数和 0.587 +/- 0.087 的平均精度，分数均优于原 ResNet-18 和其他基准模型。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "7 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.09567v1",
      "published_date": "2024-05-08 19:59:16 UTC",
      "updated_date": "2024-05-08 19:59:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:56:47.957590"
    },
    {
      "arxiv_id": "2405.06511v1",
      "title": "Towards Less Biased Data-driven Scoring with Deep Learning-Based End-to-end Database Search in Tandem Mass Spectrometry",
      "title_zh": "翻译失败",
      "authors": [
        "Yonghan Yu",
        "Ming Li"
      ],
      "abstract": "Peptide identification in mass spectrometry-based proteomics is crucial for\nunderstanding protein function and dynamics. Traditional database search\nmethods, though widely used, rely on heuristic scoring functions and\nstatistical estimations have to be introduced for a higher identification rate.\nHere, we introduce DeepSearch, the first deep learning-based end-to-end\ndatabase search method for tandem mass spectrometry. DeepSearch leverages a\nmodified transformer-based encoder-decoder architecture under the contrastive\nlearning framework. Unlike conventional methods that rely on ion-to-ion\nmatching, DeepSearch adopts a data-driven approach to score peptide spectrum\nmatches. DeepSearch is also the first deep learning-based method that can\nprofile variable post-translational modifications in a zero-shot manner. We\nshowed that DeepSearch's scoring scheme expressed less bias and did not require\nany statistical estimation. We validated DeepSearch's accuracy and robustness\nacross various datasets, including those from species with diverse protein\ncompositions and a modification-enriched dataset. DeepSearch sheds new light on\ndatabase search methods in tandem mass spectrometry.",
      "tldr_zh": "该研究针对串联质谱(tandem mass spectrometry)中的肽鉴定问题，提出DeepSearch，这是一种基于深度学习的端到端数据库搜索方法，旨在减少传统启发式评分函数的偏差。DeepSearch采用修改的Transformer-based encoder-decoder架构和对比学习(contrastive learning)框架，通过数据驱动的方法对肽谱匹配(peptide spectrum matches)进行评分，并能以零-shot方式处理可变后翻译修饰(post-translational modifications)。实验结果显示，DeepSearch在各种数据集上表现出更高的准确性和鲁棒性，无需统计估计，为质谱数据库搜索提供了新的见解。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06511v1",
      "published_date": "2024-05-08 19:39:17 UTC",
      "updated_date": "2024-05-08 19:39:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:56:59.471952"
    },
    {
      "arxiv_id": "2405.06703v1",
      "title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
      "title_zh": "翻译失败",
      "authors": [
        "Goran Muric",
        "Ben Delay",
        "Steven Minton"
      ],
      "abstract": "In this paper, we introduce the Interpretable Cross-Examination Technique\n(ICE-T), a novel approach that leverages structured multi-prompt techniques\nwith Large Language Models (LLMs) to improve classification performance over\nzero-shot and few-shot methods. In domains where interpretability is crucial,\nsuch as medicine and law, standard models often fall short due to their\n\"black-box\" nature. ICE-T addresses these limitations by using a series of\ngenerated prompts that allow an LLM to approach the problem from multiple\ndirections. The responses from the LLM are then converted into numerical\nfeature vectors and processed by a traditional classifier. This method not only\nmaintains high interpretability but also allows for smaller, less capable\nmodels to achieve or exceed the performance of larger, more advanced models\nunder zero-shot conditions. We demonstrate the effectiveness of ICE-T across a\ndiverse set of data sources, including medical records and legal documents,\nconsistently surpassing the zero-shot baseline in terms of classification\nmetrics such as F1 scores. Our results indicate that ICE-T can be used for\nimproving both the performance and transparency of AI applications in complex\ndecision-making environments.",
      "tldr_zh": "本研究提出了一种名为 Interpretable Cross-Examination Technique (ICE-T) 的新方法，利用结构化的多提示技术提升 Large Language Models (LLMs) 在零样本和少样本分类任务中的性能。ICE-T 通过生成一系列提示，让 LLM 从多个角度处理问题，并将响应转换为数值特征向量，再由传统分类器进行处理，从而解决模型的“黑箱”问题，并在医学和法律等领域提供高可解释性。实验结果显示，ICE-T 在医疗记录和法律文档等数据源上，F1 scores 等指标显著超越零样本基线，并使较小模型在零样本条件下达到或超过大型模型的性能，为复杂决策环境中的 AI 应用带来更好的透明度和效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06703v1",
      "published_date": "2024-05-08 19:20:34 UTC",
      "updated_date": "2024-05-08 19:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:57:10.792127"
    },
    {
      "arxiv_id": "2405.05378v1",
      "title": "\"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations",
      "title_zh": "“They are uncultured”: 揭示LLM生成对话中的隐蔽危害和社会威胁",
      "authors": [
        "Preetam Prabhu Srikar Dammu",
        "Hayoung Jung",
        "Anjali Singh",
        "Monojit Choudhury",
        "Tanushree Mitra"
      ],
      "abstract": "Large language models (LLMs) have emerged as an integral part of modern\nsocieties, powering user-facing applications such as personal assistants and\nenterprise applications like recruitment tools. Despite their utility, research\nindicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms\npredominantly focus on Western concepts like race and gender, often overlooking\ncultural concepts from other parts of the world. Additionally, these studies\ntypically investigate \"harm\" as a singular dimension, ignoring the various and\nsubtle forms in which harms manifest. To address this gap, we introduce the\nCovert Harms and Social Threats (CHAST), a set of seven metrics grounded in\nsocial science literature. We utilize evaluation models aligned with human\nassessments to examine the presence of covert harms in LLM-generated\nconversations, particularly in the context of recruitment. Our experiments\nreveal that seven out of the eight LLMs included in this study generated\nconversations riddled with CHAST, characterized by malign views expressed in\nseemingly neutral language unlikely to be detected by existing methods.\nNotably, these LLMs manifested more extreme views and opinions when dealing\nwith non-Western concepts like caste, compared to Western ones such as race.",
      "tldr_zh": "本研究揭示了大型语言模型（LLMs）在生成对话时存在的隐蔽危害和社会威胁，特别是对非西方文化概念的偏见忽略。研究者引入了CHAST（Covert Harms and Social Threats）——一套基于社会科学的七个指标，用于评估LLMs在招聘等情境中生成的对话中潜在的微妙偏见。实验结果显示，八个LLMs中有七个生成了以中性语言表达的恶意观点，尤其在处理非西方概念（如种姓）时，比西方概念（如种族）表现出更极端的偏见。这些发现强调了需要更全面的评估方法，以减少LLMs的系统性偏见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05378v1",
      "published_date": "2024-05-08 19:08:45 UTC",
      "updated_date": "2024-05-08 19:08:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:57:22.560623"
    },
    {
      "arxiv_id": "2405.05374v1",
      "title": "Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models",
      "title_zh": "Arctic-Embed：可扩展、高效且准确的文本",
      "authors": [
        "Luke Merrick",
        "Danmei Xu",
        "Gaurav Nuti",
        "Daniel Campos"
      ],
      "abstract": "This report describes the training dataset creation and recipe behind the\nfamily of \\texttt{arctic-embed} text embedding models (a set of five models\nranging from 22 to 334 million parameters with weights open-sourced under an\nApache-2 license). At the time of their release, each model achieved\nstate-of-the-art retrieval accuracy for models of their size on the MTEB\nRetrieval leaderboard, with the largest model, arctic-embed-l outperforming\nclosed source embedding models such as Cohere's embed-v3 and Open AI's\ntext-embed-3-large. In addition to the details of our training recipe, we have\nprovided several informative ablation studies, which we believe are the cause\nof our model performance.",
      "tldr_zh": "该研究介绍了 Arctic-Embed 文本嵌入模型系列（包括五个从 22 百万到 334 百万参数的模型），这些模型在 Apache-2 许可下开源，并专注于可扩展性、高效性和准确性。通过精心设计的训练数据集和配方，该系列模型在 MTEB Retrieval 排行榜上实现了同等规模模型的最高检索准确率，其中最大的模型 arctic-embed-l 超过了 Cohere 的 embed-v3 和 OpenAI 的 text-embed-3-large。此外，研究提供了多项消融研究，以阐明模型性能的关键因素。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 11 Figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.05374v1",
      "published_date": "2024-05-08 19:05:18 UTC",
      "updated_date": "2024-05-08 19:05:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:57:33.989982"
    },
    {
      "arxiv_id": "2407.01561v1",
      "title": "Synthetic Data in Radiological Imaging: Current State and Future Outlook",
      "title_zh": "翻译失败",
      "authors": [
        "Elena Sizikova",
        "Andreu Badal",
        "Jana G. Delfino",
        "Miguel Lago",
        "Brandon Nelson",
        "Niloufar Saharkhiz",
        "Berkman Sahiner",
        "Ghada Zamzmi",
        "Aldo Badano"
      ],
      "abstract": "A key challenge for the development and deployment of artificial intelligence\n(AI) solutions in radiology is solving the associated data limitations.\nObtaining sufficient and representative patient datasets with appropriate\nannotations may be burdensome due to high acquisition cost, safety limitations,\npatient privacy restrictions or low disease prevalence rates. In silico data\noffers a number of potential advantages to patient data, such as diminished\npatient harm, reduced cost, simplified data acquisition, scalability, improved\nquality assurance testing, and a mitigation approach to data imbalances. We\nsummarize key research trends and practical uses for synthetically generated\ndata for radiological applications of AI. Specifically, we discuss different\ntypes of techniques for generating synthetic examples, their main application\nareas, and related quality control assessment issues. We also discuss current\napproaches for evaluating synthetic imaging data. Overall, synthetic data holds\ngreat promise in addressing current data availability gaps, but additional work\nis needed before its full potential is realized.",
      "tldr_zh": "该论文探讨了在放射学影像中，使用合成数据（synthetic data）来解决AI开发面临的数据限制问题，例如高获取成本、安全限制、患者隐私和疾病低 prevalence 率。合成数据（in silico data）具有多项优势，包括减少患者伤害、降低成本、简化数据获取、可扩展性以及缓解数据 imbalances。论文总结了生成合成数据的各种技术、主要应用领域和质量控制评估方法，并指出尽管合成数据有望填补数据可用性缺口，但需进一步研究以充分发挥其潜力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Provisionally accepted to the British Journal of Radiology (BJR)\n  Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2407.01561v1",
      "published_date": "2024-05-08 18:35:47 UTC",
      "updated_date": "2024-05-08 18:35:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:57:46.356387"
    },
    {
      "arxiv_id": "2405.05349v1",
      "title": "Offline Model-Based Optimization via Policy-Guided Gradient Search",
      "title_zh": "翻译失败",
      "authors": [
        "Yassine Chemingui",
        "Aryan Deshwal",
        "Trong Nghia Hoang",
        "Janardhan Rao Doppa"
      ],
      "abstract": "Offline optimization is an emerging problem in many experimental engineering\ndomains including protein, drug or aircraft design, where online\nexperimentation to collect evaluation data is too expensive or dangerous. To\navoid that, one has to optimize an unknown function given only its offline\nevaluation at a fixed set of inputs. A naive solution to this problem is to\nlearn a surrogate model of the unknown function and optimize this surrogate\ninstead. However, such a naive optimizer is prone to erroneous overestimation\nof the surrogate (possibly due to over-fitting on a biased sample of function\nevaluation) on inputs outside the offline dataset. Prior approaches addressing\nthis challenge have primarily focused on learning robust surrogate models.\nHowever, their search strategies are derived from the surrogate model rather\nthan the actual offline data. To fill this important gap, we introduce a new\nlearning-to-search perspective for offline optimization by reformulating it as\nan offline reinforcement learning problem. Our proposed policy-guided gradient\nsearch approach explicitly learns the best policy for a given surrogate model\ncreated from the offline data. Our empirical results on multiple benchmarks\ndemonstrate that the learned optimization policy can be combined with existing\noffline surrogates to significantly improve the optimization performance.",
      "tldr_zh": "这篇论文解决了离线优化问题，即在实验工程领域（如蛋白质、药物或飞机设计）中，仅使用固定输入点的离线评估数据来优化未知函数，而避免昂贵的在线实验。传统方法依赖学习代理模型（surrogate model），但容易在数据集外过度估计；为此，作者提出 policy-guided gradient search 框架，将问题重新表述为离线强化学习问题，并针对代理模型显式学习最佳优化策略。实验结果显示，该方法与现有离线代理模型结合，在多个基准上显著提升了优化性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at AAAI Conference on Artificial Intelligence, 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05349v1",
      "published_date": "2024-05-08 18:27:37 UTC",
      "updated_date": "2024-05-08 18:27:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:57:58.943921"
    },
    {
      "arxiv_id": "2405.05348v1",
      "title": "The Effect of Model Size on LLM Post-hoc Explainability via LIME",
      "title_zh": "翻译失败",
      "authors": [
        "Henning Heyen",
        "Amy Widdicombe",
        "Noah Y. Siegel",
        "Maria Perez-Ortiz",
        "Philip Treleaven"
      ],
      "abstract": "Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.",
      "tldr_zh": "这篇论文探讨了模型大小对大型语言模型(LLMs)后验解释性的影响，通过LIME方法评估不同大小的DeBERTaV3模型在自然语言推理(NLI)和零样本分类(ZSC)任务上的解释效果。研究发现，虽然增大模型大小提升了整体性能，但解释的合理性(plausibility)并未相应提高，而是与模型内部决策过程出现不一致。结果还揭示了在NLI任务中，忠实度(faithfulness)指标存在局限性，强调了模型可解释性研究的潜在挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at ICLR 2024 Workshop on Secure and Trustworthy Large\n  Language Models",
      "pdf_url": "http://arxiv.org/pdf/2405.05348v1",
      "published_date": "2024-05-08 18:27:20 UTC",
      "updated_date": "2024-05-08 18:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:58:11.225045"
    },
    {
      "arxiv_id": "2405.05347v1",
      "title": "Benchmarking Educational Program Repair",
      "title_zh": "教育程序修复的基准测试",
      "authors": [
        "Charles Koutcheme",
        "Nicola Dainese",
        "Sami Sarsa",
        "Juho Leinonen",
        "Arto Hellas",
        "Paul Denny"
      ],
      "abstract": "The emergence of large language models (LLMs) has sparked enormous interest\ndue to their potential application across a range of educational tasks. For\nexample, recent work in programming education has used LLMs to generate\nlearning resources, improve error messages, and provide feedback on code.\nHowever, one factor that limits progress within the field is that much of the\nresearch uses bespoke datasets and different evaluation metrics, making direct\ncomparisons between results unreliable. Thus, there is a pressing need for\nstandardization and benchmarks that facilitate the equitable comparison of\ncompeting approaches. One task where LLMs show great promise is program repair,\nwhich can be used to provide debugging support and next-step hints to students.\nIn this article, we propose a novel educational program repair benchmark. We\ncurate two high-quality publicly available programming datasets, present a\nunified evaluation procedure introducing a novel evaluation metric rouge@k for\napproximating the quality of repairs, and evaluate a set of five recent models\nto establish baseline performance.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在编程教育中的应用（如生成学习资源和提供代码反馈）存在的数据集和评估指标不统一的问题，提出一个新的教育程序修复基准，以促进公平比较。论文整理了两个高质量的公开编程数据集，并引入了新型评估指标 rouge@k，用于近似评估修复质量，同时定义了一个统一的评估流程。最终，通过评估五个最近的模型，建立了基准性能，为LLMs在程序修复任务中的应用提供了标准化参考。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.SE",
      "comment": "15 pages, 2 figures, 3 tables. Non-archival report presented at the\n  NeurIPS'23 Workshop on Generative AI for Education (GAIED)",
      "pdf_url": "http://arxiv.org/pdf/2405.05347v1",
      "published_date": "2024-05-08 18:23:59 UTC",
      "updated_date": "2024-05-08 18:23:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:58:22.195722"
    },
    {
      "arxiv_id": "2405.05336v3",
      "title": "Joint semi-supervised and contrastive learning enables domain generalization and multi-domain segmentation",
      "title_zh": "联合半监督和对比学习实现领域泛化和多领域分割",
      "authors": [
        "Alvaro Gomariz",
        "Yusuke Kikuchi",
        "Yun Yvonna Li",
        "Thomas Albrecht",
        "Andreas Maunz",
        "Daniela Ferrara",
        "Huanxiang Lu",
        "Orcun Goksel"
      ],
      "abstract": "Despite their effectiveness, current deep learning models face challenges\nwith images coming from different domains with varying appearance and content.\nWe introduce SegCLR, a versatile framework designed to segment images across\ndifferent domains, employing supervised and contrastive learning simultaneously\nto effectively learn from both labeled and unlabeled data. We demonstrate the\nsuperior performance of SegCLR through a comprehensive evaluation involving\nthree diverse clinical datasets of 3D retinal Optical Coherence Tomography\n(OCT) images, for the slice-wise segmentation of fluids with various network\nconfigurations and verification across 10 different network initializations. In\nan unsupervised domain adaptation context, SegCLR achieves results on par with\na supervised upper-bound model trained on the intended target domain. Notably,\nwe discover that the segmentation performance of SegCLR framework is marginally\nimpacted by the abundance of unlabeled data from the target domain, thereby we\nalso propose an effective domain generalization extension of SegCLR, known also\nas zero-shot domain adaptation, which eliminates the need for any target domain\ninformation. This shows that our proposed addition of contrastive loss in\nstandard supervised training for segmentation leads to superior models,\ninherently more generalizable to both in- and out-of-domain test data. We\nadditionally propose a pragmatic solution for SegCLR deployment in realistic\nscenarios with multiple domains containing labeled data. Accordingly, our\nframework pushes the boundaries of deep-learning based segmentation in\nmulti-domain applications, regardless of data availability - labeled,\nunlabeled, or nonexistent.",
      "tldr_zh": "本文提出 SegCLR 框架，通过联合 semi-supervised learning 和 contrastive learning，从标记和未标记数据中学习，实现多域图像分割和 domain generalization。该框架在三个临床数据集的 3D 视网膜 OCT 图像流体分割任务上表现出色，在 unsupervised domain adaptation 中达到与监督模型相当的性能，且对目标域未标记数据的数量影响较小。此外，SegCLR 扩展到 zero-shot domain adaptation，无需目标域数据即可泛化，并提供多域部署的实用解决方案，从而提升了深度学习在多域应用的鲁棒性和适用性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05336v3",
      "published_date": "2024-05-08 18:10:59 UTC",
      "updated_date": "2025-04-14 10:51:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:58:35.097858"
    },
    {
      "arxiv_id": "2405.10970v1",
      "title": "Untargeted Adversarial Attack on Knowledge Graph Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Tianzhe Zhao",
        "Jiaoyan Chen",
        "Yanchi Ru",
        "Qika Lin",
        "Yuxia Geng",
        "Jun Liu"
      ],
      "abstract": "Knowledge graph embedding (KGE) methods have achieved great success in\nhandling various knowledge graph (KG) downstream tasks. However, KGE methods\nmay learn biased representations on low-quality KGs that are prevalent in the\nreal world. Some recent studies propose adversarial attacks to investigate the\nvulnerabilities of KGE methods, but their attackers are target-oriented with\nthe KGE method and the target triples to predict are given in advance, which\nlacks practicability. In this work, we explore untargeted attacks with the aim\nof reducing the global performances of KGE methods over a set of unknown test\ntriples and conducting systematic analyses on KGE robustness. Considering logic\nrules can effectively summarize the global structure of a KG, we develop\nrule-based attack strategies to enhance the attack efficiency. In particular,we\nconsider adversarial deletion which learns rules, applying the rules to score\ntriple importance and delete important triples, and adversarial addition which\ncorrupts the learned rules and applies them for negative triples as\nperturbations. Extensive experiments on two datasets over three representative\nclasses of KGE methods demonstrate the effectiveness of our proposed untargeted\nattacks in diminishing the link prediction results. And we also find that\ndifferent KGE methods exhibit different robustness to untargeted attacks. For\nexample, the robustness of methods engaged with graph neural networks and logic\nrules depends on the density of the graph. But rule-based methods like NCRL are\neasily affected by adversarial addition attacks to capture negative rules",
      "tldr_zh": "本文提出了一种针对知识图嵌入 (KGE) 的无目标对抗攻击方法，旨在降低 KGE 在未知测试三元组上的整体性能，并系统分析其鲁棒性，以解决现有针对性攻击的实用性问题。方法基于逻辑规则开发攻击策略，包括对抗删除 (adversarial deletion)，即学习规则评分并删除重要三元组，以及对抗添加 (adversarial addition)，即破坏规则生成负面三元组扰动。实验在两个数据集上对三种代表性 KGE 方法进行测试，结果显示攻击有效降低了链接预测性能，并发现不同方法鲁棒性差异明显，例如基于图神经网络和逻辑规则的方法受图密度影响，而基于规则的方法如 NCRL 易受对抗添加攻击影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIGIR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.10970v1",
      "published_date": "2024-05-08 18:08:11 UTC",
      "updated_date": "2024-05-08 18:08:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:58:48.827090"
    },
    {
      "arxiv_id": "2405.05329v2",
      "title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Minsik Cho",
        "Mohammad Rastegari",
        "Devang Naik"
      ],
      "abstract": "Large Language Model or LLM inference has two phases, the prompt (or prefill)\nphase to output the first token and the extension (or decoding) phase to the\ngenerate subsequent tokens. In this work, we propose an efficient\nparallelization scheme, KV-Runahead to accelerate the prompt phase. The key\nobservation is that the extension phase generates tokens faster than the prompt\nphase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes\nthe prompt phase by orchestrating multiple processes to populate the KV-cache\nand minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache\nscheme has two main benefits. First, since KV-cache is designed to leverage the\ncausal attention map, we minimize computation and computation automatically.\nSecond, since it already exists for the extension phase, KV-Runahead is easy to\nimplement. We further propose context-level load-balancing to handle uneven\nKV-cache generation (due to the causal attention) and to optimize TTFT.\nCompared with an existing parallelization scheme such as tensor or sequential\nparallelization where keys and values are locally generated and exchanged via\nall-gather collectives, our experimental results demonstrate that KV-Runahead\ncan offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.",
      "tldr_zh": "这篇论文提出了 KV-Runahead 方案，用于加速 Large Language Model (LLM) 推理中的 prompt 阶段，通过并行生成 Key-Value Cache (KV-cache) 来最小化 time-to-first-token (TTFT)。该方法利用 causal attention map 自动优化计算，并引入 context-level load-balancing 来处理因 causal attention 导致的 KV-cache 生成不均匀问题。实验结果显示，KV-Runahead 相比现有并行化方案（如 tensor 或 sequential parallelization），为 Llama 7B 和 Falcon 7B 分别提供了 1.4x 和 1.6x 的速度提升。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DC",
      "comment": "preprint for ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05329v2",
      "published_date": "2024-05-08 18:03:22 UTC",
      "updated_date": "2024-05-13 18:32:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:59:01.704996"
    },
    {
      "arxiv_id": "2405.05256v2",
      "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Prannay Kaul",
        "Zhizhong Li",
        "Hao Yang",
        "Yonatan Dukler",
        "Ashwin Swaminathan",
        "C. J. Taylor",
        "Stefano Soatto"
      ],
      "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an\nopen problem. Recent benchmarks do not address hallucinations in open-ended\nfree-form responses, which we term \"Type I hallucinations\". Instead, they focus\non hallucinations responding to very specific question formats -- typically a\nmultiple-choice response regarding a particular object or attribute -- which we\nterm \"Type II hallucinations\". Additionally, such benchmarks often require\nexternal API calls to models which are subject to change. In practice, we\nobserve that a reduction in Type II hallucinations does not lead to a reduction\nin Type I hallucinations but rather that the two forms of hallucinations are\noften anti-correlated. To address this, we propose THRONE, a novel object-based\nautomatic framework for quantitatively evaluating Type I hallucinations in LVLM\nfree-form outputs. We use public language models (LMs) to identify\nhallucinations in LVLM responses and compute informative metrics. By evaluating\na large selection of recent LVLMs using public datasets, we show that an\nimprovement in existing metrics do not lead to a reduction in Type I\nhallucinations, and that established benchmarks for measuring Type I\nhallucinations are incomplete. Finally, we provide a simple and effective data\naugmentation method to reduce Type I and Type II hallucinations as a strong\nbaseline. Code is now available at https://github.com/amazon-science/THRONE .",
      "tldr_zh": "这篇论文提出了THRONE，一种基于对象的自动基准，用于评估大型视觉语言模型(LVLMs)在自由形式生成中的Type I幻觉（即开放式响应中的幻觉），以弥补现有基准主要关注Type II幻觉（针对特定问题格式）的不足。THRONE框架利用公共语言模型(LMs)来识别幻觉并计算量化指标，通过评估多种LVLMs发现，改善现有指标并不能减少Type I幻觉，反而可能与Type II幻觉呈反相关性。论文还提供了一种简单有效的数据增强方法，作为基准来同时降低Type I和Type II幻觉。代码已在GitHub开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "In CVPR 2024. Code https://github.com/amazon-science/THRONE",
      "pdf_url": "http://arxiv.org/pdf/2405.05256v2",
      "published_date": "2024-05-08 17:59:11 UTC",
      "updated_date": "2025-04-03 17:59:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:59:12.043723"
    },
    {
      "arxiv_id": "2405.05253v1",
      "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
      "title_zh": "翻译失败",
      "authors": [
        "Charles Koutcheme",
        "Nicola Dainese",
        "Sami Sarsa",
        "Arto Hellas",
        "Juho Leinonen",
        "Paul Denny"
      ],
      "abstract": "Large language models (LLMs) have shown great potential for the automatic\ngeneration of feedback in a wide range of computing contexts. However, concerns\nhave been voiced around the privacy and ethical implications of sending student\nwork to proprietary models. This has sparked considerable interest in the use\nof open source LLMs in education, but the quality of the feedback that such\nopen models can produce remains understudied. This is a concern as providing\nflawed or misleading generated feedback could be detrimental to student\nlearning. Inspired by recent work that has utilised very powerful LLMs, such as\nGPT-4, to evaluate the outputs produced by less powerful models, we conduct an\nautomated analysis of the quality of the feedback produced by several open\nsource models using a dataset from an introductory programming course. First,\nwe investigate the viability of employing GPT-4 as an automated evaluator by\ncomparing its evaluations with those of a human expert. We observe that GPT-4\ndemonstrates a bias toward positively rating feedback while exhibiting moderate\nagreement with human raters, showcasing its potential as a feedback evaluator.\nSecond, we explore the quality of feedback generated by several leading\nopen-source LLMs by using GPT-4 to evaluate the feedback. We find that some\nmodels offer competitive performance with popular proprietary LLMs, such as\nChatGPT, indicating opportunities for their responsible use in educational\nsettings.",
      "tldr_zh": "该研究评估了开源大型语言模型 (LLMs) 在生成学生反馈方面的质量，使用 GPT-4 作为自动评估器，针对一门入门编程课程的数据集。研究首先验证了 GPT-4 的可行性，通过与人类专家比较，发现 GPT-4 倾向于给出正面评价且与人类评委有中等一致性。接下来，利用 GPT-4 评估了几个领先开源 LLMs 的反馈表现，结果显示某些模型的性能可与专有模型如 ChatGPT 相媲美。总体而言，此工作突显了开源 LLMs 在教育场景中负责任使用的潜力，同时解决了隐私和伦理担忧。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 4 figures, 2 tables. Accepted for publication at the 29th\n  annual ACM conference on Innovation and Technology in Computer Science\n  Education (ITiCSE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.05253v1",
      "published_date": "2024-05-08 17:57:39 UTC",
      "updated_date": "2024-05-08 17:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:59:23.220414"
    },
    {
      "arxiv_id": "2405.05252v1",
      "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hongjie Wang",
        "Difan Liu",
        "Yan Kang",
        "Yijun Li",
        "Zhe Lin",
        "Niraj K. Jha",
        "Yuchen Liu"
      ],
      "abstract": "Diffusion Models (DMs) have exhibited superior performance in generating\nhigh-quality and diverse images. However, this exceptional performance comes at\nthe cost of expensive architectural design, particularly due to the attention\nmodule heavily used in leading models. Existing works mainly adopt a retraining\nprocess to enhance DM efficiency. This is computationally expensive and not\nvery scalable. To this end, we introduce the Attention-driven Training-free\nEfficient Diffusion Model (AT-EDM) framework that leverages attention maps to\nperform run-time pruning of redundant tokens, without the need for any\nretraining. Specifically, for single-denoising-step pruning, we develop a novel\nranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify\nredundant tokens, and a similarity-based recovery method to restore tokens for\nthe convolution operation. In addition, we propose a Denoising-Steps-Aware\nPruning (DSAP) approach to adjust the pruning budget across different denoising\ntimesteps for better generation quality. Extensive evaluations show that AT-EDM\nperforms favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs\nsaving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining\nnearly the same FID and CLIP scores as the full model. Project webpage:\nhttps://atedm.github.io.",
      "tldr_zh": "本文提出 Attention-Driven Training-Free Efficient Diffusion Model (AT-EDM) 框架，通过利用注意力映射在运行时修剪冗余标记，实现 Diffusion Models 的效率提升，而无需重新训练。框架的核心方法包括开发 Generalized Weighted Page Rank (G-WPR) 算法来识别冗余标记，并结合基于相似度的恢复方法处理卷积操作，以及 Denoising-Steps-Aware Pruning (DSAP) 方法根据不同去噪步骤动态调整修剪预算。实验结果显示，AT-EDM 实现了 38.8% 的 FLOPs 节省和高达 1.53 倍的速度提升，同时保持与完整模型几乎相同的 FID 和 CLIP 分数。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05252v1",
      "published_date": "2024-05-08 17:56:47 UTC",
      "updated_date": "2024-05-08 17:56:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:59:36.214999"
    },
    {
      "arxiv_id": "2405.05248v2",
      "title": "LLMs with Personalities in Multi-issue Negotiation Games",
      "title_zh": "翻译失败",
      "authors": [
        "Sean Noh",
        "Ho-Chun Herbert Chang"
      ],
      "abstract": "Powered by large language models (LLMs), AI agents have become capable of\nmany human tasks. Using the most canonical definitions of the Big Five\npersonality, we measure the ability of LLMs to negotiate within a\ngame-theoretical framework, as well as methodological challenges to measuring\nnotions of fairness and risk. Simulations (n=1,500) for both single-issue and\nmulti-issue negotiation reveal increase in domain complexity with asymmetric\nissue valuations improve agreement rates but decrease surplus from aggressive\nnegotiation. Through gradient-boosted regression and Shapley explainers, we\nfind high openness, conscientiousness, and neuroticism are associated with fair\ntendencies; low agreeableness and low openness are associated with rational\ntendencies. Low conscientiousness is associated with high toxicity. These\nresults indicate that LLMs may have built-in guardrails that default to fair\nbehavior, but can be \"jail broken\" to exploit agreeable opponents. We also\noffer pragmatic insight in how negotiation bots can be designed, and a\nframework of assessing negotiation behavior based on game theory and\ncomputational social science.",
      "tldr_zh": "本研究探讨了大语言模型(LLMs)在其Big Five人格特质影响下，在单问题和多问题谈判游戏中的表现，通过1500次模拟实验评估公平性、风险和行为。结果显示，谈判复杂性增加导致协议率上升但剩余价值下降；高开放性、高尽责性和高神经质与公平倾向相关，低宜性和低开放性与理性倾向相关，而低尽责性则与高毒性行为相关。这些发现表明LLMs具有内置的公平行为保护，但可被“越狱”来利用宜人对手，并为基于博弈论和计算社会科学的谈判机器人设计提供了实用框架。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05248v2",
      "published_date": "2024-05-08 17:51:53 UTC",
      "updated_date": "2024-05-09 01:09:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T06:59:47.899311"
    },
    {
      "arxiv_id": "2405.05244v1",
      "title": "SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan",
      "title_zh": "翻译失败",
      "authors": [
        "You Zhang",
        "Yongyi Zang",
        "Jiatong Shi",
        "Ryuichi Yamamoto",
        "Jionghao Han",
        "Yuxun Tang",
        "Tomoki Toda",
        "Zhiyao Duan"
      ],
      "abstract": "The rapid advancement of AI-generated singing voices, which now closely mimic\nnatural human singing and align seamlessly with musical scores, has led to\nheightened concerns for artists and the music industry. Unlike spoken voice,\nsinging voice presents unique challenges due to its musical nature and the\npresence of strong background music, making singing voice deepfake detection\n(SVDD) a specialized field requiring focused attention. To promote SVDD\nresearch, we recently proposed the \"SVDD Challenge,\" the very first research\nchallenge focusing on SVDD for lab-controlled and in-the-wild bonafide and\ndeepfake singing voice recordings. The challenge will be held in conjunction\nwith the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).",
      "tldr_zh": "AI 生成的歌唱声音（Singing Voice）越来越逼真，引发了艺术家和音乐行业的担忧，尤其由于其音乐性质和背景音乐带来的独特挑战，使得 Singing Voice Deepfake Detection (SVDD) 成为一个需要专项研究的领域。为推动 SVDD 研究，该论文提出 SVDD Challenge 2024，这是首个专注于实验室控制和野外环境的真实与深度伪造歌唱声音检测的研究挑战。挑战将与 2024 IEEE Spoken Language Technology Workshop (SLT 2024) 共同举办，旨在促进相关技术的发展和评估。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Evaluation plan of the SVDD Challenge @ SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05244v1",
      "published_date": "2024-05-08 17:40:12 UTC",
      "updated_date": "2024-05-08 17:40:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:00:01.619898"
    },
    {
      "arxiv_id": "2405.05219v2",
      "title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Yingyu Liang",
        "Heshan Liu",
        "Zhenmei Shi",
        "Zhao Song",
        "Zhuoyan Xu",
        "Junze Yin"
      ],
      "abstract": "The self-attention mechanism is the key to the success of transformers in\nrecent Large Language Models (LLMs). However, the quadratic computational cost\n$O(n^2)$ in the input sequence length $n$ is a notorious obstacle for further\nimprovement and scalability in longer contexts. In this work, we leverage the\nconvolution-like structure of attention matrices to develop an efficient\napproximation method for attention computation using convolution matrices. We\npropose a $\\mathsf{conv}$ basis system, analogous to the rank basis, and show\nthat any lower triangular matrix can always be decomposed as a sum of\nstructured convolution matrices in this basis. We then design a fast algorithm\nto approximate the attention matrix via a sum of such $k$ convolution matrices.\nThis allows us to compute the attention {\\it inference} via Fast Fourier\nTransforms (FFT) in $O(knd \\log n)$ time, where $d$ is the hidden dimension,\nand thus achieve almost linear time $n^{1+o(1)}$ in the practical scenario\nwhere $kd = n^{o(1)}$. Furthermore, the attention {\\it training forward} and\n{\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. We provide\ntheoretical guarantees on the run time and approximation error and conduct\npreliminary experiments to evaluate its effectiveness. We hope our new paradigm\nfor accelerating attention computation in transformer models can help their\napplication to longer contexts.",
      "tldr_zh": "该论文提出了一种名为 Conv-Basis 的新范式，用于高效处理 Transformer 模型中的注意力机制计算问题，针对自注意力（self-attention）的二次方复杂度 O(n^2) 进行优化。作者利用注意力矩阵的卷积-like 结构，开发了一种基于 conv basis 系统的方法，将下三角矩阵分解为 k 个结构化卷积矩阵的和，从而通过 Fast Fourier Transforms (FFT) 实现注意力推理的计算时间复杂度降至 O(knd log n)，接近线性时间 n^{1+o(1)}。此外，该方法还支持训练前向和后向梯度计算，提供理论上的运行时间和近似误差保证，并通过初步实验验证了其在更长上下文中的有效性。总的来说，Conv-Basis 为扩展 Transformer 模型的应用奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05219v2",
      "published_date": "2024-05-08 17:11:38 UTC",
      "updated_date": "2024-10-16 06:55:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:00:11.191672"
    },
    {
      "arxiv_id": "2405.05189v2",
      "title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Inderjeet Nair",
        "Lu Wang"
      ],
      "abstract": "We study the task of conducting structured reasoning as generating a\nreasoning graph from natural language input using large language models (LLMs).\nPrevious approaches have explored various prompting schemes, yet they suffer\nfrom error propagation due to the autoregressive nature and single-pass-based\ndecoding, which lack error correction capability. Additionally, relying solely\non a single sample may result in the omission of true nodes and edges. To\ncounter this, we draw inspiration from self-consistency (SC), which involves\nsampling a diverse set of reasoning chains and taking the majority vote as the\nfinal answer. To tackle the substantial challenge of applying SC on generated\ngraphs, we propose MIDGARD (MInimum Description length Guided Aggregation of\nReasoning in Directed acyclic graph) that leverages Minimum Description Length\n(MDL)-based formulation to identify consistent properties among the different\ngraph samples generated by an LLM. This formulation helps reject properties\nthat appear in only a few samples, which are likely to be erroneous, while\nenabling the inclusion of missing elements without compromising precision. Our\nmethod demonstrates superior performance than comparisons across various\nstructured reasoning tasks, including argument structure extraction,\nexplanation graph generation, inferring dependency relations among actions for\neveryday tasks, and semantic graph generation from natural texts.",
      "tldr_zh": "这篇论文针对使用大语言模型 (LLMs) 生成推理图的结构化常识推理任务，提出 MIDGARD 方法，以解决现有方法的错误传播和元素遗漏问题。MIDGARD 借鉴 Self-Consistency 思想，通过 Minimum Description Length (MDL) 公式聚合多个图样本，识别一致属性、过滤错误元素并补充缺失部分，从而提高推理的准确性和可靠性。实验结果显示，该方法在参数结构提取、解释图生成、动作依赖关系推断和语义图生成等任务上，显著优于基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024(main)",
      "pdf_url": "http://arxiv.org/pdf/2405.05189v2",
      "published_date": "2024-05-08 16:25:42 UTC",
      "updated_date": "2024-06-02 18:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:00:23.664857"
    },
    {
      "arxiv_id": "2405.05173v3",
      "title": "A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Huaiyuan Xu",
        "Junliang Chen",
        "Shiyu Meng",
        "Yi Wang",
        "Lap-Pui Chau"
      ],
      "abstract": "3D occupancy perception technology aims to observe and understand dense 3D\nenvironments for autonomous vehicles. Owing to its comprehensive perception\ncapability, this technology is emerging as a trend in autonomous driving\nperception systems, and is attracting significant attention from both industry\nand academia. Similar to traditional bird's-eye view (BEV) perception, 3D\noccupancy perception has the nature of multi-source input and the necessity for\ninformation fusion. However, the difference is that it captures vertical\nstructures that are ignored by 2D BEV. In this survey, we review the most\nrecent works on 3D occupancy perception, and provide in-depth analyses of\nmethodologies with various input modalities. Specifically, we summarize general\nnetwork pipelines, highlight information fusion techniques, and discuss\neffective network training. We evaluate and analyze the occupancy perception\nperformance of the state-of-the-art on the most popular datasets. Furthermore,\nchallenges and future research directions are discussed. We hope this paper\nwill inspire the community and encourage more research work on 3D occupancy\nperception. A comprehensive list of studies in this survey is publicly\navailable in an active repository that continuously collects the latest work:\nhttps://github.com/HuaiyuanXu/3D-Occupancy-Perception.",
      "tldr_zh": "这篇调查论文从信息融合视角审视了 3D occupancy perception 技术在自动驾驶中的应用，强调其能观察密集 3D 环境并捕捉 BEV 感知忽略的垂直结构，从而提供更全面的感知能力。论文总结了通用网络管道、信息融合方法（如多源输入处理）和有效网络训练策略，并评估了最先进模型在流行数据集上的性能。最终，它讨论了面临的挑战、未来研究方向，并公开了一个仓库（https://github.com/HuaiyuanXu/3D-Occupancy-Perception）以收集相关工作。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05173v3",
      "published_date": "2024-05-08 16:10:46 UTC",
      "updated_date": "2024-07-21 12:01:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:00:36.051539"
    },
    {
      "arxiv_id": "2405.05160v2",
      "title": "Selective Classification Under Distribution Shifts",
      "title_zh": "在分布偏移下的选择性分类",
      "authors": [
        "Hengyue Liang",
        "Le Peng",
        "Ju Sun"
      ],
      "abstract": "In selective classification (SC), a classifier abstains from making\npredictions that are likely to be wrong to avoid excessive errors. To deploy\nimperfect classifiers -- either due to intrinsic statistical noise of data or\nfor robustness issue of the classifier or beyond -- in high-stakes scenarios,\nSC appears to be an attractive and necessary path to follow. Despite decades of\nresearch in SC, most previous SC methods still focus on the ideal statistical\nsetting only, i.e., the data distribution at deployment is the same as that of\ntraining, although practical data can come from the wild. To bridge this gap,\nin this paper, we propose an SC framework that takes into account distribution\nshifts, termed generalized selective classification, that covers label-shifted\n(or out-of-distribution) and covariate-shifted samples, in addition to typical\nin-distribution samples, the first of its kind in the SC literature. We focus\non non-training-based confidence-score functions for generalized SC on deep\nlearning (DL) classifiers, and propose two novel margin-based score functions.\nThrough extensive analysis and experiments, we show that our proposed score\nfunctions are more effective and reliable than the existing ones for\ngeneralized SC on a variety of classification tasks and DL classifiers. Code is\navailable at https://github.com/sun-umn/sc_with_distshift.",
      "tldr_zh": "本文探讨了选择性分类（Selective Classification, SC）在分布偏移（distribution shifts）下的应用，提出了一种新的框架——generalized selective classification，以处理标签偏移（label-shifted）、协变量偏移（covariate-shifted）以及典型分布内样本。针对深度学习（DL）分类器，该框架引入了两个新型基于边界的置信度分数函数（margin-based score functions），无需额外训练即可提升分类可靠性。通过广泛的分析和实验，该方法在多种分类任务上比现有方法更有效，准确率显著提高。代码已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted to Transactions on Machine Learning Research (TMLR),\n  issn: 2835-8856,2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05160v2",
      "published_date": "2024-05-08 15:52:50 UTC",
      "updated_date": "2024-11-27 05:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:00:48.846502"
    },
    {
      "arxiv_id": "2405.05154v1",
      "title": "The Potential and Implications of Generative AI on HCI Education",
      "title_zh": "生成式 AI 对 HCI 教育的潜力和影响",
      "authors": [
        "Ahmed Kharrufa",
        "Ian G Johnson"
      ],
      "abstract": "Generative AI (GAI) is impacting teaching and learning directly or indirectly\nacross a range of subjects and disciplines. As educators, we need to understand\nthe potential and limitations of AI in HCI education and ensure our graduating\nHCI students are aware of the potential and limitations of AI in HCI. In this\npaper, we report on the main pedagogical insights gained from the inclusion of\ngenerative AI into a 10 week undergraduate module. We designed the module to\nencourage student experimentation with GAI models as part of the design brief\nrequirement and planned practical sessions and discussions. Our insights are\nbased on replies to a survey sent out to the students after completing the\nmodule. Our key findings, for HCI educators, report on the use of AI as a\npersona for developing project ideas and creating resources for design, and AI\nas a mirror for reflecting students' understanding of key concepts and ideas\nand highlighting knowledge gaps. We also discuss potential pitfalls that should\nbe considered and the need to assess students' literacies and assumptions of\nGAIs as pedagogical tools. Finally, we put forward the case for educators to\ntake the opportunities GAI presents as an educational tool and be experimental,\ncreative, and courageous in their practice. We end with a discussion of our\nfindings in relation to the TPACK framework in HCI.",
      "tldr_zh": "这篇论文探讨了生成式 AI (GAI) 对 HCI 教育的影响和潜力，强调教育者需理解 GAI 的优势与局限性，以培养学生的相关意识。研究通过一个 10 周本科模块设计，让学生实验 GAI 模型用于开发项目想法、创建设计资源，并通过调查分析其作为反思工具的作用，以突出关键概念理解和知识缺口。关键发现包括潜在陷阱如学生素养评估的必要性，并鼓励教育者大胆创新地运用 GAI，最终以 TPACK 框架讨论这些教育启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages, 2 figures, to be published at EduCHI 2024 The 6th Annual\n  Symposium on HCI Education, June 2024, New York, NY",
      "pdf_url": "http://arxiv.org/pdf/2405.05154v1",
      "published_date": "2024-05-08 15:46:31 UTC",
      "updated_date": "2024-05-08 15:46:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:01:00.737855"
    },
    {
      "arxiv_id": "2405.05146v2",
      "title": "Hybrid Convolutional Neural Networks with Reliability Guarantee",
      "title_zh": "具有可靠性保证的混合卷积神经网络",
      "authors": [
        "Hans Dermot Doran",
        "Suzana Veljanovska"
      ],
      "abstract": "Making AI safe and dependable requires the generation of dependable models\nand dependable execution of those models. We propose redundant execution as a\nwell-known technique that can be used to ensure reliable execution of the AI\nmodel. This generic technique will extend the application scope of\nAI-accelerators that do not feature well-documented safety or dependability\nproperties. Typical redundancy techniques incur at least double or triple the\ncomputational expense of the original. We adopt a co-design approach,\nintegrating reliable model execution with non-reliable execution, focusing that\nadditional computational expense only where it is strictly necessary. We\ndescribe the design, implementation and some preliminary results of a hybrid\nCNN.",
      "tldr_zh": "该论文针对AI模型的安全性和可靠性，提出使用redundant execution技术来确保模型可靠执行，从而扩展AI加速器的应用范围，即使这些加速器缺乏完善的安全属性。作者采用co-design方法，将可靠执行与非可靠执行整合，仅在必要部分增加计算开销，以避免传统冗余技术带来的双倍或三倍计算负担。初步结果显示，这种混合CNN的设计和实现有效提升了模型的可靠性，为AI的可靠部署提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "2024 54th Annual IEEE/IFIP International Conference on Dependable\n  Systems and Networks (DSN 2024). Dependable and Secure Machine Learning\n  Workshop (DSML 2024), Brisbane, Australia, June 24-27, 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05146v2",
      "published_date": "2024-05-08 15:39:38 UTC",
      "updated_date": "2024-05-09 09:31:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:01:12.972980"
    },
    {
      "arxiv_id": "2407.11972v1",
      "title": "An efficient machine learning approach for extracting eSports players distinguishing features and classifying their skill levels using symbolic transfer entropy and consensus nested cross validation",
      "title_zh": "一种高效的机器学习方法，用于提取电子竞技玩家的区分特征并分类其技能水平，使用符号转移熵和共识嵌套交叉验证",
      "authors": [
        "Amin Noroozi",
        "Mohammad S. Hasan",
        "Maryam Ravan",
        "Elham Norouzi",
        "Ying-Ying Law"
      ],
      "abstract": "Discovering features that set elite players apart is of great significance\nfor eSports coaches as it enables them to arrange a more effective training\nprogram focused on improving those features. Moreover, finding such features\nresults in a better evaluation of eSports players skills, which, besides\ncoaches, is of interest for game developers to design games automatically\nadaptable to the players expertise. Sensor data combined with machine learning\nhave already proved effective in classifying eSports players. However, the\nexisting methods do not provide sufficient information about features that\ndistinguish high-skilled players. In this paper, we propose an efficient method\nto find these features and then use them to classify players' skill levels. We\nfirst apply a time window to extract the players' sensor data, including heart\nrate, hand activities, etc., before and after game events in the League of\nLegends game. We use the extracted segments and symbolic transfer entropy to\ncalculate connectivity features between sensors. The most relevant features are\nthen selected using the newly developed consensus nested cross validation\nmethod. These features, representing the harmony between body parts, are\nfinally used to find the optimum window size and classify players' skills. The\nclassification results demonstrate a significant improvement by achieving 90.1%\naccuracy. Also, connectivity features between players gaze positions and\nkeyboard, mouse, and hand activities were the most distinguishing features in\nclassifying players' skills. The proposed method in this paper can be similarly\napplied to sportspeople data and potentially revolutionize the training\nprograms in both eSports and sports industries",
      "tldr_zh": "该研究提出了一种高效机器学习方法，用于提取电子竞技（eSports）玩家的独特特征并分类其技能水平。方法首先通过时间窗口提取玩家的传感器数据（如心率和手部活动），然后运用 symbolic transfer entropy 计算传感器间的连通性特征，并使用 consensus nested cross validation 选择最相关特征，以优化窗口大小并进行分类。实验结果显示，分类准确率达到90.1%，其中玩家注视位置与键盘、鼠标及手部活动的连通性特征是最显著的区分因素。该方法可扩展应用于传统体育数据，潜在革新训练程序。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11972v1",
      "published_date": "2024-05-08 15:22:12 UTC",
      "updated_date": "2024-05-08 15:22:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:01:24.353196"
    },
    {
      "arxiv_id": "2405.05109v2",
      "title": "QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs",
      "title_zh": "QFMTS：基于多表输入的查询聚焦摘要生成",
      "authors": [
        "Weijia Zhang",
        "Vaishali Pal",
        "Jia-Hong Huang",
        "Evangelos Kanoulas",
        "Maarten de Rijke"
      ],
      "abstract": "Table summarization is a crucial task aimed at condensing information from\ntabular data into concise and comprehensible textual summaries. However,\nexisting approaches often fall short of adequately meeting users' information\nand quality requirements and tend to overlook the complexities of real-world\nqueries. In this paper, we propose a novel method to address these limitations\nby introducing query-focused multi-table summarization. Our approach, which\ncomprises a table serialization module, a summarization controller, and a large\nlanguage model (LLM), utilizes textual queries and multiple tables to generate\nquery-dependent table summaries tailored to users' information needs. To\nfacilitate research in this area, we present a comprehensive dataset\nspecifically tailored for this task, consisting of 4909 query-summary pairs,\neach associated with multiple tables. Through extensive experiments using our\ncurated dataset, we demonstrate the effectiveness of our proposed method\ncompared to baseline approaches. Our findings offer insights into the\nchallenges of complex table reasoning for precise summarization, contributing\nto the advancement of research in query-focused multi-table summarization.",
      "tldr_zh": "本文提出 QFMTS 方法，用于生成 query-focused multi-table summarization，即基于用户查询的多个表摘要，以解决现有表摘要技术忽略用户需求和复杂查询的问题。  \n该方法由 table serialization module、summarization controller 和 large language model (LLM) 组成，通过处理文本查询和多表输入，生成针对性强的简洁摘要。  \n为了支持研究，作者构建了一个包含 4909 个 query-summary pairs 的数据集，并通过实验证明 QFMTS 优于基线方法。  \n这项工作为复杂表推理和精确摘要提供了宝贵见解，推进了相关领域的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI-2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.05109v2",
      "published_date": "2024-05-08 15:05:55 UTC",
      "updated_date": "2024-08-25 17:22:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:01:36.894221"
    },
    {
      "arxiv_id": "2405.05080v1",
      "title": "Concerns on Bias in Large Language Models when Creating Synthetic Personae",
      "title_zh": "翻译失败",
      "authors": [
        "Helena A. Haxvig"
      ],
      "abstract": "This position paper explores the benefits, drawbacks, and ethical\nconsiderations of incorporating synthetic personae in HCI research,\nparticularly focusing on the customization challenges beyond the limitations of\ncurrent Large Language Models (LLMs). These perspectives are derived from the\ninitial results of a sub-study employing vignettes to showcase the existence of\nbias within black-box LLMs and explore methods for manipulating them. The study\naims to establish a foundation for understanding the challenges associated with\nthese models, emphasizing the necessity of thorough testing before utilizing\nthem to create synthetic personae for HCI research.",
      "tldr_zh": "这篇立场论文探讨了在HCI（Human-Computer Interaction）研究中使用Large Language Models (LLMs)创建synthetic personae的益处、缺点和伦理挑战，特别关注LLMs的偏见问题及其定制限制。研究通过vignettes子研究展示了LLMs中存在的偏见，并探索了操纵这些偏见的方法，以揭示潜在风险。论文强调，在将LLMs用于创建synthetic personae前，进行彻底测试是必要的，以为HCI研究奠定理解这些模型挑战的基础。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "4 pages, accepted at the \"LLM-Based Synthetic Personae and Data in\n  HCI\" workshop at CHI2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05080v1",
      "published_date": "2024-05-08 14:24:11 UTC",
      "updated_date": "2024-05-08 14:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:01:49.549796"
    },
    {
      "arxiv_id": "2405.05299v1",
      "title": "Challenges for Responsible AI Design and Workflow Integration in Healthcare: A Case Study of Automatic Feeding Tube Qualification in Radiology",
      "title_zh": "翻译失败",
      "authors": [
        "Anja Thieme",
        "Abhijith Rajamohan",
        "Benjamin Cooper",
        "Heather Groombridge",
        "Robert Simister",
        "Barney Wong",
        "Nicholas Woznitza",
        "Mark Ames Pinnock",
        "Maria Teodora Wetscherek",
        "Cecily Morrison",
        "Hannah Richardson",
        "Fernando Pérez-García",
        "Stephanie L. Hyland",
        "Shruthi Bannur",
        "Daniel C. Castro",
        "Kenza Bouzid",
        "Anton Schwaighofer",
        "Mercy Ranjit",
        "Harshita Sharma",
        "Matthew P. Lungren",
        "Ozan Oktay",
        "Javier Alvarez-Valle",
        "Aditya Nori",
        "Stephen Harris",
        "Joseph Jacob"
      ],
      "abstract": "Nasogastric tubes (NGTs) are feeding tubes that are inserted through the nose\ninto the stomach to deliver nutrition or medication. If not placed correctly,\nthey can cause serious harm, even death to patients. Recent AI developments\ndemonstrate the feasibility of robustly detecting NGT placement from Chest\nX-ray images to reduce risks of sub-optimally or critically placed NGTs being\nmissed or delayed in their detection, but gaps remain in clinical practice\nintegration. In this study, we present a human-centered approach to the problem\nand describe insights derived following contextual inquiry and in-depth\ninterviews with 15 clinical stakeholders. The interviews helped understand\nchallenges in existing workflows, and how best to align technical capabilities\nwith user needs and expectations. We discovered the trade-offs and complexities\nthat need consideration when choosing suitable workflow stages, target users,\nand design configurations for different AI proposals. We explored how to\nbalance AI benefits and risks for healthcare staff and patients within broader\norganizational and medical-legal constraints. We also identified data issues\nrelated to edge cases and data biases that affect model training and\nevaluation; how data documentation practices influence data preparation and\nlabelling; and how to measure relevant AI outcomes reliably in future\nevaluations. We discuss how our work informs design and development of AI\napplications that are clinically useful, ethical, and acceptable in real-world\nhealthcare services.",
      "tldr_zh": "本研究探讨了在医疗环境中负责任地设计和整合AI系统所面临的挑战，以鼻胃管(NGTs)自动定位资格检测为例。研究采用人类中心方法，通过对15名临床利益相关者的情境探究和深度访谈，分析了现有工作流程中的问题，并评估了如何将AI能力与用户需求对齐，包括选择合适的工作流程阶段、目标用户和设计配置的权衡。访谈揭示了AI益处与风险的平衡问题，如组织约束、医疗法律因素，以及数据偏差、边缘案例对模型训练和评估的影响。最终，该研究为开发临床有用、道德且可接受的AI应用提供了指导，包括改进数据文档实践和可靠测量AI结果的方法。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.m; I.2.m"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05299v1",
      "published_date": "2024-05-08 14:16:22 UTC",
      "updated_date": "2024-05-08 14:16:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:02:00.656681"
    },
    {
      "arxiv_id": "2405.05072v2",
      "title": "Novel Actor-Critic Algorithm for Robust Decision Making of CAV under Delays and Loss of V2X Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zine el abidine Kherroubi"
      ],
      "abstract": "Current autonomous driving systems heavily rely on V2X communication data to\nenhance situational awareness and the cooperation between vehicles. However, a\nmajor challenge when using V2X data is that it may not be available\nperiodically because of unpredictable delays and data loss during wireless\ntransmission between road stations and the receiver vehicle. This issue should\nbe considered when designing control strategies for connected and autonomous\nvehicles. Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm\nthat guarantees robust driving performance in V2X environment with delayed\nand/or lost data. The novel algorithm incorporates three key mechanisms: a\nvirtual fixed sampling period, a combination of Temporal-Difference and Monte\nCarlo learning, and a numerical approximation of immediate reward values. To\naddress the temporal aperiodicity problem of V2X data, we first illustrate this\nchallenge. Then, we provide a detailed explanation of the Blind Actor-Critic\nalgorithm where we highlight the proposed components to compensate for the\ntemporal aperiodicity problem of V2X data. We evaluate the performance of our\nalgorithm in a simulation environment and compare it to benchmark approaches.\nThe results demonstrate that training metrics are improved compared to\nconventional actor-critic algorithms. Additionally, testing results show that\nour approach provides robust control, even under low V2X network reliability\nlevels.",
      "tldr_zh": "本论文针对 V2X 数据延迟和丢失问题，提出了一种新型 'Blind Actor-Critic' 算法，以确保连网自动驾驶车辆 (CAV) 在不稳定环境中实现稳健决策。该算法整合了三个关键机制：虚拟固定采样周期、Temporal-Difference 和 Monte Carlo 学习的结合，以及即时奖励值的数值近似，从而有效应对 V2X 数据的时序不周期性。实验结果显示，该算法在模拟环境中优于传统 Actor-Critic 方法，改善了训练指标，并在低可靠性 V2X 网络下提供了可靠的控制性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 10 figures, Journal paper, submitted to IEEE Transactions\n  on Intelligent Transportation Systems",
      "pdf_url": "http://arxiv.org/pdf/2405.05072v2",
      "published_date": "2024-05-08 14:14:03 UTC",
      "updated_date": "2024-10-20 07:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:02:13.156887"
    },
    {
      "arxiv_id": "2405.05066v1",
      "title": "Designing Skill-Compatible AI: Methodologies and Frameworks in Chess",
      "title_zh": "设计技能兼容的 AI：在国际象棋中的方法论和框架",
      "authors": [
        "Karim Hamade",
        "Reid McIlroy-Young",
        "Siddhartha Sen",
        "Jon Kleinberg",
        "Ashton Anderson"
      ],
      "abstract": "Powerful artificial intelligence systems are often used in settings where\nthey must interact with agents that are computationally much weaker, for\nexample when they work alongside humans or operate in complex environments\nwhere some tasks are handled by algorithms, heuristics, or other entities of\nvarying computational power. For AI agents to successfully interact in these\nsettings, however, achieving superhuman performance alone is not sufficient;\nthey also need to account for suboptimal actions or idiosyncratic style from\ntheir less-skilled counterparts. We propose a formal evaluation framework for\nassessing the compatibility of near-optimal AI with interaction partners who\nmay have much lower levels of skill; we use popular collaborative chess\nvariants as model systems to study and develop AI agents that can successfully\ninteract with lower-skill entities. Traditional chess engines designed to\noutput near-optimal moves prove to be inadequate partners when paired with\nengines of various lower skill levels in this domain, as they are not designed\nto consider the presence of other agents. We contribute three methodologies to\nexplicitly create skill-compatible AI agents in complex decision-making\nsettings, and two chess game frameworks designed to foster collaboration\nbetween powerful AI agents and less-skilled partners. On these frameworks, our\nagents outperform state-of-the-art chess AI (based on AlphaZero) despite being\nweaker in conventional chess, demonstrating that skill-compatibility is a\ntangible trait that is qualitatively and measurably distinct from raw\nperformance. Our evaluations further explore and clarify the mechanisms by\nwhich our agents achieve skill-compatibility.",
      "tldr_zh": "该论文探讨了设计技能兼容的AI系统，以适应与计算能力较弱代理（如人类或低级算法）的互动，强调AI不仅仅追求超人性能，还需考虑对方的次优行动。作者提出一个正式评估框架，并贡献三种方法论，用于创建能在复杂决策环境中与低技能实体协作的AI代理，同时开发了两个国际象棋游戏框架来促进这种合作。实验结果显示，这些AI在协作框架中优于状态艺术的AlphaZero，尽管在传统国际象棋中表现较弱，证明了技能兼容性是一种独立、可衡量的特性，并阐明了其实现机制。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 5 figures, 15 tables, Published In The Twelfth\n  International Conference on Learning Representations, ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05066v1",
      "published_date": "2024-05-08 14:04:35 UTC",
      "updated_date": "2024-05-08 14:04:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:02:25.124600"
    },
    {
      "arxiv_id": "2405.05027v1",
      "title": "StyleMamba : State Space Model for Efficient Text-driven Image Style Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Zijia Wang",
        "Zhi-Song Liu"
      ],
      "abstract": "We present StyleMamba, an efficient image style transfer framework that\ntranslates text prompts into corresponding visual styles while preserving the\ncontent integrity of the original images. Existing text-guided stylization\nrequires hundreds of training iterations and takes a lot of computing\nresources. To speed up the process, we propose a conditional State Space Model\nfor Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that\nsequentially aligns the image features to the target text prompts. To enhance\nthe local and global style consistency between text and image, we propose\nmasked and second-order directional losses to optimize the stylization\ndirection to significantly reduce the training iterations by 5 times and the\ninference time by 3 times. Extensive experiments and qualitative evaluation\nconfirm the robust and superior stylization performance of our methods compared\nto the existing baselines.",
      "tldr_zh": "我们提出 StyleMamba，一种高效的图像风格转移框架，利用条件 State Space Model 来将文本提示转化为视觉风格，同时保留原图像的内容完整性。该框架通过顺序对齐图像特征和目标文本提示，并引入 masked 和 second-order directional losses 来增强局部和全局风格一致性，从而将训练迭代减少5倍、推理时间减少3倍。实验结果显示，StyleMamba 在广泛的定性评估中比现有基线方法更鲁棒和优越。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Blind submission to ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.05027v1",
      "published_date": "2024-05-08 12:57:53 UTC",
      "updated_date": "2024-05-08 12:57:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:02:37.067706"
    },
    {
      "arxiv_id": "2406.04349v1",
      "title": "Multimodal Approach for Harmonized System Code Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Otmane Amel",
        "Sedrick Stassin",
        "Sidi Ahmed Mahmoudi",
        "Xavier Siebert"
      ],
      "abstract": "The rapid growth of e-commerce has placed considerable pressure on customs\nrepresentatives, prompting advanced methods. In tackling this, Artificial\nintelligence (AI) systems have emerged as a promising approach to minimize the\nrisks faced. Given that the Harmonized System (HS) code is a crucial element\nfor an accurate customs declaration, we propose a novel multimodal HS code\nprediction approach using deep learning models exploiting both image and text\nfeatures obtained through the customs declaration combined with e-commerce\nplatform information. We evaluated two early fusion methods and introduced our\nMultConcat fusion method. To the best of our knowledge, few studies analyze the\nfeaturelevel combination of text and image in the state-of-the-art for HS code\nprediction, which heightens interest in our paper and its findings. The\nexperimental results prove the effectiveness of our approach and fusion method\nwith a top-3 and top-5 accuracy of 93.5% and 98.2% respectively",
      "tldr_zh": "该研究针对电子商务增长导致的海关申报压力，提出了一种多模态方法，用于预测 Harmonized System (HS) 代码。该方法利用深度学习模型，结合图像和文本特征（从海关申报和电子商务平台获取），并评估了两种早融合方法，同时引入了创新的 MultConcat 融合方法。据作者所知，这是在 HS 代码预测领域中少有的特征级文本和图像组合分析。实验结果显示，该方法在 top-3 和 top-5 准确率分别达到 93.5% 和 98.2%，证明了其有效性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for poster presentation at ESANN 2023",
      "pdf_url": "http://arxiv.org/pdf/2406.04349v1",
      "published_date": "2024-05-08 12:48:26 UTC",
      "updated_date": "2024-05-08 12:48:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:02:50.763064"
    },
    {
      "arxiv_id": "2405.04990v1",
      "title": "Health Index Estimation Through Integration of General Knowledge with Unsupervised Learning",
      "title_zh": "健康指数估计：通过一般知识与无监督学习的整合",
      "authors": [
        "Kristupas Bajarunas",
        "Marcia L. Baptista",
        "Kai Goebel",
        "Manuel A. Chao"
      ],
      "abstract": "Accurately estimating a Health Index (HI) from condition monitoring data (CM)\nis essential for reliable and interpretable prognostics and health management\n(PHM) in complex systems. In most scenarios, complex systems operate under\nvarying operating conditions and can exhibit different fault modes, making\nunsupervised inference of an HI from CM data a significant challenge. Hybrid\nmodels combining prior knowledge about degradation with deep learning models\nhave been proposed to overcome this challenge. However, previously suggested\nhybrid models for HI estimation usually rely heavily on system-specific\ninformation, limiting their transferability to other systems. In this work, we\npropose an unsupervised hybrid method for HI estimation that integrates general\nknowledge about degradation into the convolutional autoencoder's model\narchitecture and learning algorithm, enhancing its applicability across various\nsystems. The effectiveness of the proposed method is demonstrated in two case\nstudies from different domains: turbofan engines and lithium batteries. The\nresults show that the proposed method outperforms other competitive\nalternatives, including residual-based methods, in terms of HI quality and\ntheir utility for Remaining Useful Life (RUL) predictions. The case studies\nalso highlight the comparable performance of our proposed method with a\nsupervised model trained with HI labels.",
      "tldr_zh": "该研究针对复杂系统中从条件监测数据（CM）估计健康指数（HI）的挑战，提出了一种无监督的混合方法，将一般退化知识整合到卷积自编码器（convolutional autoencoder）的模型架构和学习算法中，以提升其在不同系统中的适用性。该方法避免了传统混合模型对系统特定信息的依赖，通过结合先验知识来改善HI估计的准确性和可解释性。在turbofan engines和lithium batteries的两个案例研究中，该方法在HI质量和Remaining Useful Life (RUL)预测方面超过了竞争对手，包括基于残差的方法，并与有监督模型的性能相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04990v1",
      "published_date": "2024-05-08 11:54:15 UTC",
      "updated_date": "2024-05-08 11:54:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:03:01.438305"
    },
    {
      "arxiv_id": "2405.15794v1",
      "title": "Finite Groundings for ASP with Functions: A Journey through Consistency",
      "title_zh": "翻译失败",
      "authors": [
        "Lukas Gerlach",
        "David Carral",
        "Markus Hecher"
      ],
      "abstract": "Answer set programming (ASP) is a logic programming formalism used in various\nareas of artificial intelligence like combinatorial problem solving and\nknowledge representation and reasoning. It is known that enhancing ASP with\nfunction symbols makes basic reasoning problems highly undecidable. However,\neven in simple cases, state of the art reasoners, specifically those relying on\na ground-and-solve approach, fail to produce a result. Therefore, we reconsider\nconsistency as a basic reasoning problem for ASP. We show reductions that give\nan intuition for the high level of undecidability. These insights allow for a\nmore fine-grained analysis where we characterize ASP programs as \"frugal\" and\n\"non-proliferous\". For such programs, we are not only able to semi-decide\nconsistency but we also propose a grounding procedure that yields finite\ngroundings on more ASP programs with the concept of \"forbidden\" facts.",
      "tldr_zh": "该研究探讨了在 Answer Set Programming (ASP) 中引入函数符号后的一致性问题，指出这会导致基本推理任务高度不可判定，并使现有推理器难以处理。作者通过归约方法直观地分析了这种不可判定的原因，并将 ASP 程序细分为“frugal”（节俭的）和“non-proliferous”（非增殖的）类型。对于这些特定程序，他们提出了一种半决策一致性的方法，并设计了一个接地程序，利用“forbidden” facts 的概念，能够在更多 ASP 程序上生成有限接地，从而提升了推理的可行性。整体工作为处理 ASP 中的复杂性提供了新的见解和工具。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "to be published at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.15794v1",
      "published_date": "2024-05-08 11:50:08 UTC",
      "updated_date": "2024-05-08 11:50:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:03:14.098137"
    },
    {
      "arxiv_id": "2405.04985v1",
      "title": "An Artificial Intelligence Approach for Interpreting Creative Combinational Designs",
      "title_zh": "翻译失败",
      "authors": [
        "Liuqing Chen",
        "Shuhong Xiao",
        "Yunnong Chen",
        "Linyun Sun",
        "Peter R. N. Childs",
        "Ji Han"
      ],
      "abstract": "Combinational creativity, a form of creativity involving the blending of\nfamiliar ideas, is pivotal in design innovation. While most research focuses on\nhow combinational creativity in design is achieved through blending elements,\nthis study focuses on the computational interpretation, specifically\nidentifying the 'base' and 'additive' components that constitute a creative\ndesign. To achieve this goal, the authors propose a heuristic algorithm\nintegrating computer vision and natural language processing technologies, and\nimplement multiple approaches based on both discriminative and generative\nartificial intelligence architectures. A comprehensive evaluation was conducted\non a dataset created for studying combinational creativity. Among the\nimplementations of the proposed algorithm, the most effective approach\ndemonstrated a high accuracy in interpretation, achieving 87.5% for identifying\n'base' and 80% for 'additive'. We conduct a modular analysis and an ablation\nexperiment to assess the performance of each part in our implementations.\nAdditionally, the study includes an analysis of error cases and bottleneck\nissues, providing critical insights into the limitations and challenges\ninherent in the computational interpretation of creative designs.",
      "tldr_zh": "本研究聚焦于组合创造力(combinational creativity)的计算解释，旨在识别创意设计中的“base”（基础）和“additive”（附加）组件。作者提出了一种启发式算法，整合计算机视觉和自然语言处理技术，并基于辨别式和生成式AI架构实现多种方法。在数据集上进行的全面评估显示，最有效的方法在识别“base”时准确率达87.5%，在“additive”时达80%。此外，通过模块分析、消融实验和错误案例分析，该研究揭示了计算解释的瓶颈问题和潜在挑战。",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04985v1",
      "published_date": "2024-05-08 11:47:32 UTC",
      "updated_date": "2024-05-08 11:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:03:27.399022"
    },
    {
      "arxiv_id": "2405.04974v1",
      "title": "Discrepancy-based Diffusion Models for Lesion Detection in Brain MRI",
      "title_zh": "基于差异的扩散模型用于脑部 MRI 中的病变检测",
      "authors": [
        "Keqiang Fan",
        "Xiaohao Cai",
        "Mahesan Niranjan"
      ],
      "abstract": "Diffusion probabilistic models (DPMs) have exhibited significant\neffectiveness in computer vision tasks, particularly in image generation.\nHowever, their notable performance heavily relies on labelled datasets, which\nlimits their application in medical images due to the associated high-cost\nannotations. Current DPM-related methods for lesion detection in medical\nimaging, which can be categorized into two distinct approaches, primarily rely\non image-level annotations. The first approach, based on anomaly detection,\ninvolves learning reference healthy brain representations and identifying\nanomalies based on the difference in inference results. In contrast, the second\napproach, resembling a segmentation task, employs only the original brain\nmulti-modalities as prior information for generating pixel-level annotations.\nIn this paper, our proposed model - discrepancy distribution medical diffusion\n(DDMD) - for lesion detection in brain MRI introduces a novel framework by\nincorporating distinctive discrepancy features, deviating from the conventional\ndirect reliance on image-level annotations or the original brain modalities. In\nour method, the inconsistency in image-level annotations is translated into\ndistribution discrepancies among heterogeneous samples while preserving\ninformation within homogeneous samples. This property retains pixel-wise\nuncertainty and facilitates an implicit ensemble of segmentation, ultimately\nenhancing the overall detection performance. Thorough experiments conducted on\nthe BRATS2020 benchmark dataset containing multimodal MRI scans for brain\ntumour detection demonstrate the great performance of our approach in\ncomparison to state-of-the-art methods.",
      "tldr_zh": "本研究针对扩散概率模型（DPMs）在脑 MRI 病变检测中的局限性，提出了一种新框架：discrepancy distribution medical diffusion (DDMD)。该方法通过将图像级标注的不一致性转化为异质样本之间的分布差异，同时保留同质样本的信息，实现像素级不确定性的保留和隐式集成分割，从而提升检测性能。实验在 BRATS2020 数据集上表明，DDMD 比现有最先进方法表现出色，显著提高了脑肿瘤检测的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04974v1",
      "published_date": "2024-05-08 11:26:49 UTC",
      "updated_date": "2024-05-08 11:26:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:03:38.931971"
    },
    {
      "arxiv_id": "2405.04972v1",
      "title": "Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision Support",
      "title_zh": "翻译失败",
      "authors": [
        "Felix Haag",
        "Carlo Stingl",
        "Katrin Zerfass",
        "Konstantin Hopf",
        "Thorsten Staake"
      ],
      "abstract": "Information systems (IS) are frequently designed to leverage the negative\neffect of anchoring bias to influence individuals' decision-making (e.g., by\nmanipulating purchase decisions). Recent advances in Artificial Intelligence\n(AI) and the explanations of its decisions through explainable AI (XAI) have\nopened new opportunities for mitigating biased decisions. So far, the potential\nof these technological advances to overcome anchoring bias remains widely\nunclear. To this end, we conducted two online experiments with a total of N=390\nparticipants in the context of purchase decisions to examine the impact of AI\nand XAI-based decision support on anchoring bias. Our results show that AI\nalone and its combination with XAI help to mitigate the negative effect of\nanchoring bias. Ultimately, our findings have implications for the design of AI\nand XAI-based decision support and IS to overcome cognitive biases.",
      "tldr_zh": "该研究探讨了 AI 和 XAI（可解释 AI）在克服 anchoring bias（锚定偏差）方面的潜力，旨在缓解信息系统对决策（如购买决定）的负面影响。通过两个在线实验（共 N=390 参与者），在购买决策的背景下，测试了 AI 单独使用以及与 XAI 结合的决策支持效果。结果表明，这两种方法都能显著减轻 anchoring bias 的负面影响，并为设计 AI 和 XAI 基于的决策支持系统以克服认知偏差提供了重要启示。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04972v1",
      "published_date": "2024-05-08 11:25:04 UTC",
      "updated_date": "2024-05-08 11:25:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:03:51.625002"
    },
    {
      "arxiv_id": "2405.04969v1",
      "title": "A review on discriminative self-supervised learning methods",
      "title_zh": "判别式自监督学习方法的综述",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Tania Stathaki"
      ],
      "abstract": "In the field of computer vision, self-supervised learning has emerged as a\nmethod to extract robust features from unlabeled data, where models derive\nlabels autonomously from the data itself, without the need for manual\nannotation. This paper provides a comprehensive review of discriminative\napproaches of self-supervised learning within the domain of computer vision,\nexamining their evolution and current status. Through an exploration of various\nmethods including contrastive, self-distillation, knowledge distillation,\nfeature decorrelation, and clustering techniques, we investigate how these\napproaches leverage the abundance of unlabeled data. Finally, we have\ncomparison of self-supervised learning methods on the standard ImageNet\nclassification benchmark.",
      "tldr_zh": "这篇论文对计算机视觉领域中判别式自监督学习方法进行了全面回顾，这些方法允许模型从无标签数据中自主生成标签，从而提取鲁棒特征。论文探讨了多种技术，包括对比学习（contrastive）、自我蒸馏（self-distillation）、知识蒸馏（knowledge distillation）、特征去相关（feature decorrelation）和聚类技术（clustering techniques），并分析了它们如何利用海量无标签数据。最终，通过在ImageNet分类基准上的比较，论文展示了这些方法的演变、当前状态及其性能差异。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 7 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.04969v1",
      "published_date": "2024-05-08 11:15:20 UTC",
      "updated_date": "2024-05-08 11:15:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:04:02.064175"
    },
    {
      "arxiv_id": "2405.05295v1",
      "title": "Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Silvan Mertes",
        "Tobias Huber",
        "Christina Karle",
        "Katharina Weitz",
        "Ruben Schlagowski",
        "Cristina Conati",
        "Elisabeth André"
      ],
      "abstract": "In this paper, we demonstrate the feasibility of alterfactual explanations\nfor black box image classifiers. Traditional explanation mechanisms from the\nfield of Counterfactual Thinking are a widely-used paradigm for Explainable\nArtificial Intelligence (XAI), as they follow a natural way of reasoning that\nhumans are familiar with. However, most common approaches from this field are\nbased on communicating information about features or characteristics that are\nespecially important for an AI's decision. However, to fully understand a\ndecision, not only knowledge about relevant features is needed, but the\nawareness of irrelevant information also highly contributes to the creation of\na user's mental model of an AI system. To this end, a novel approach for\nexplaining AI systems called alterfactual explanations was recently proposed on\na conceptual level. It is based on showing an alternative reality where\nirrelevant features of an AI's input are altered. By doing so, the user\ndirectly sees which input data characteristics can change arbitrarily without\ninfluencing the AI's decision. In this paper, we show for the first time that\nit is possible to apply this idea to black box models based on neural networks.\nTo this end, we present a GAN-based approach to generate these alterfactual\nexplanations for binary image classifiers. Further, we present a user study\nthat gives interesting insights on how alterfactual explanations can complement\ncounterfactual explanations.",
      "tldr_zh": "这篇论文探讨了 alterfactual explanations 的可行性，用于解释黑盒图像分类器，以补充传统的 Counterfactual Thinking 方法。作者指出，了解 AI 决策中的无关特征（irrelevant information）有助于用户构建更全面的心理模型，因此提出通过改变输入特征生成替代现实的 alterfactual explanations。论文采用 GAN-based approach 为二元图像分类器生成这些解释，并通过用户研究发现，alterfactual explanations 可以有效补充 counterfactual explanations，提升对 AI 系统的理解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IJCAI 2024. arXiv admin note: text overlap with\n  arXiv:2207.09374",
      "pdf_url": "http://arxiv.org/pdf/2405.05295v1",
      "published_date": "2024-05-08 11:03:22 UTC",
      "updated_date": "2024-05-08 11:03:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:04:13.452254"
    },
    {
      "arxiv_id": "2405.04955v1",
      "title": "Improving Long Text Understanding with Knowledge Distilled from Summarization Model",
      "title_zh": "利用从摘要模型中提炼知识改善长文本理解",
      "authors": [
        "Yan Liu",
        "Yazheng Yang",
        "Xiaokang Chen"
      ],
      "abstract": "Long text understanding is important yet challenging for natural language\nprocessing. A long article or document usually contains many redundant words\nthat are not pertinent to its gist and sometimes can be regarded as noise. With\nrecent advances of abstractive summarization, we propose our \\emph{Gist\nDetector} to leverage the gist detection ability of a summarization model and\nintegrate the extracted gist into downstream models to enhance their long text\nunderstanding ability. Specifically, Gist Detector first learns the gist\ndetection knowledge distilled from a summarization model, and then produces\ngist-aware representations to augment downstream models. We evaluate our method\non three different tasks: long document classification, distantly supervised\nopen-domain question answering, and non-parallel text style transfer. The\nexperimental results show that our method can significantly improve the\nperformance of baseline models on all tasks.",
      "tldr_zh": "这篇论文提出了一种名为 Gist Detector 的方法，通过从 abstractive summarization 模型中蒸馏 knowledge distilled，提升长文本理解能力。具体来说，该方法先学习摘要模型的 gist detection 知识，然后生成 gist-aware representations 来增强下游模型的表现。在长文档分类、远监督开放域问答和非平行文本风格转移等任务上的实验结果显示，该方法显著提高了基线模型的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2110.04741",
      "pdf_url": "http://arxiv.org/pdf/2405.04955v1",
      "published_date": "2024-05-08 10:49:39 UTC",
      "updated_date": "2024-05-08 10:49:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:04:26.173830"
    },
    {
      "arxiv_id": "2405.04950v1",
      "title": "VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context",
      "title_zh": "VisionGraph: 利用大型多模态模型解决视觉语境中的图论问题",
      "authors": [
        "Yunxin Li",
        "Baotian Hu",
        "Haoyuan Shi",
        "Wei Wang",
        "Longyue Wang",
        "Min Zhang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have achieved impressive success in visual\nunderstanding and reasoning, remarkably improving the performance of\nmathematical reasoning in a visual context. Yet, a challenging type of visual\nmath lies in the multimodal graph theory problem, which demands that LMMs\nunderstand the graphical structures accurately and perform multi-step reasoning\non the visual graph. Additionally, exploring multimodal graph theory problems\nwill lead to more effective strategies in fields like biology, transportation,\nand robotics planning. To step forward in this direction, we are the first to\ndesign a benchmark named VisionGraph, used to explore the capabilities of\nadvanced LMMs in solving multimodal graph theory problems. It encompasses eight\ncomplex graph problem tasks, from connectivity to shortest path problems.\nSubsequently, we present a Description-Program-Reasoning (DPR) chain to enhance\nthe logical accuracy of reasoning processes through graphical structure\ndescription generation and algorithm-aware multi-step reasoning. Our extensive\nstudy shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph\nreasoning; 2) All LMMs exhibit inferior perception accuracy for graphical\nstructures, whether in zero/few-shot settings or with supervised fine-tuning\n(SFT), which further affects problem-solving performance; 3) DPR significantly\nimproves the multi-step graph reasoning capabilities of LMMs and the GPT-4V\n(DPR) agent achieves SOTA performance.",
      "tldr_zh": "本研究探讨了Large Multimodal Models (LMMs) 在视觉上下文中的图论问题，首次设计了VisionGraph基准测试，以评估LMMs在八种复杂图问题任务（如连通性和最短路径问题）上的能力。VisionGraph旨在揭示LMMs在理解图形结构和进行多步推理方面的挑战，并为生物学、交通和机器人规划等领域提供更有效的策略。作者提出Description-Program-Reasoning (DPR) 链，通过生成图形结构描述和算法感知的多步推理，提升LMMs的逻辑准确性。实验结果显示，GPT-4V在多步图推理中优于Gemini Pro，所有LMMs在图形结构感知上表现不佳，但DPR显著提高了性能，使GPT-4V (DPR) 代理达到SOTA水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages; Accepted by ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04950v1",
      "published_date": "2024-05-08 10:42:48 UTC",
      "updated_date": "2024-05-08 10:42:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:04:39.212915"
    },
    {
      "arxiv_id": "2405.04941v2",
      "title": "Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs",
      "title_zh": "翻译失败",
      "authors": [
        "Eline M. Bovy",
        "Marnix Suilen",
        "Sebastian Junges",
        "Nils Jansen"
      ],
      "abstract": "Partially observable Markov decision processes (POMDPs) rely on the key\nassumption that probability distributions are precisely known. Robust POMDPs\n(RPOMDPs) alleviate this concern by defining imprecise probabilities, referred\nto as uncertainty sets. While robust MDPs have been studied extensively, work\non RPOMDPs is limited and primarily focuses on algorithmic solution methods. We\nexpand the theoretical understanding of RPOMDPs by showing that 1) different\nassumptions on the uncertainty sets affect optimal policies and values; 2)\nRPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the\nsame RPOMDP with different assumptions leads to semantically different POSGs\nand, thus, different policies and values. These novel semantics for RPOMDPs\ngive access to results for POSGs, studied in game theory; concretely, we show\nthe existence of a Nash equilibrium. Finally, we classify the existing RPOMDP\nliterature using our semantics, clarifying under which uncertainty assumptions\nthese existing works operate.",
      "tldr_zh": "该论文探讨了 Robust POMDPs (RPOMDPs)，通过引入不确定性集来处理传统部分可观察 Markov 决策过程 (POMDPs) 中概率分布的精确性假设问题。研究扩展了 RPOMDPs 的理论基础，证明不同不确定性集假设会影响最优策略和值，并展示了 RPOMDPs 具有部分可观察随机博弈 (POSG) 的语义。基于此语义，论文证明了 Nash 均衡的存在，并阐明了相同 RPOMDPs 在不同假设下会导致语义差异，从而影响策略和值。最后，论文对现有 RPOMDP 文献进行了分类，澄清了其不确定性假设。",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04941v2",
      "published_date": "2024-05-08 10:22:49 UTC",
      "updated_date": "2024-07-29 09:15:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:04:53.512907"
    },
    {
      "arxiv_id": "2405.06701v1",
      "title": "Lightweight Spatial Modeling for Combinatorial Information Extraction From Documents",
      "title_zh": "轻量级空间建模用于文档的组合信息提取",
      "authors": [
        "Yanfei Dong",
        "Lambert Deng",
        "Jiazheng Zhang",
        "Xiaodong Yu",
        "Ting Lin",
        "Francesco Gelli",
        "Soujanya Poria",
        "Wee Sun Lee"
      ],
      "abstract": "Documents that consist of diverse templates and exhibit complex spatial\nstructures pose a challenge for document entity classification. We propose\nKNN-former, which incorporates a new kind of spatial bias in attention\ncalculation based on the K-nearest-neighbor (KNN) graph of document entities.\nWe limit entities' attention only to their local radius defined by the KNN\ngraph. We also use combinatorial matching to address the one-to-one mapping\nproperty that exists in many documents, where one field has only one\ncorresponding entity. Moreover, our method is highly parameter-efficient\ncompared to existing approaches in terms of the number of trainable parameters.\nDespite this, experiments across various datasets show our method outperforms\nbaselines in most entity types. Many real-world documents exhibit combinatorial\nproperties which can be leveraged as inductive biases to improve extraction\naccuracy, but existing datasets do not cover these documents. To facilitate\nfuture research into these types of documents, we release a new ID document\ndataset that covers diverse templates and languages. We also release enhanced\nannotations for an existing dataset.",
      "tldr_zh": "本研究提出了一种轻量级空间建模方法KNN-former，用于从具有复杂空间结构和多样模板的文档中提取组合信息。该方法在注意力计算中引入基于K-nearest-neighbor (KNN) 图的空间偏差，仅允许实体关注其局部半径内的邻居，并结合combinatorial matching来处理文档中的一对一映射属性。与现有方法相比，KNN-former在参数效率上显著提升，训练参数更少。实验结果显示，该方法在多个数据集上优于基线模型，在大多数实体类型中提高了提取准确率。此外，研究者发布了新的ID文档数据集，涵盖多样模板和语言，并增强了现有数据集的注解，以促进未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06701v1",
      "published_date": "2024-05-08 10:10:38 UTC",
      "updated_date": "2024-05-08 10:10:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:05:03.397178"
    },
    {
      "arxiv_id": "2405.04937v1",
      "title": "Developing trustworthy AI applications with foundation models",
      "title_zh": "使用基础模型开发可信赖的 AI 应用",
      "authors": [
        "Michael Mock",
        "Sebastian Schmidt",
        "Felix Müller",
        "Rebekka Görge",
        "Anna Schmitz",
        "Elena Haedecke",
        "Angelika Voss",
        "Dirk Hecker",
        "Maximillian Poretschkin"
      ],
      "abstract": "The trustworthiness of AI applications has been the subject of recent\nresearch and is also addressed in the EU's recently adopted AI Regulation. The\ncurrently emerging foundation models in the field of text, speech and image\nprocessing offer completely new possibilities for developing AI applications.\nThis whitepaper shows how the trustworthiness of an AI application developed\nwith foundation models can be evaluated and ensured. For this purpose, the\napplication-specific, risk-based approach for testing and ensuring the\ntrustworthiness of AI applications, as developed in the 'AI Assessment Catalog\n- Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is\ntransferred to the context of foundation models. Special consideration is given\nto the fact that specific risks of foundation models can have an impact on the\nAI application and must also be taken into account when checking\ntrustworthiness. Chapter 1 of the white paper explains the fundamental\nrelationship between foundation models and AI applications based on them in\nterms of trustworthiness. Chapter 2 provides an introduction to the technical\nconstruction of foundation models and Chapter 3 shows how AI applications can\nbe developed based on them. Chapter 4 provides an overview of the resulting\nrisks regarding trustworthiness. Chapter 5 shows which requirements for AI\napplications and foundation models are to be expected according to the draft of\nthe European Union's AI Regulation and Chapter 6 finally shows the system and\nprocedure for meeting trustworthiness requirements.",
      "tldr_zh": "这篇白皮书探讨了使用 foundation models 开发可信赖 AI 应用的方法，针对文本、语音和图像处理领域，基于欧盟 AI Regulation 的要求。\n它将 Fraunhofer IAIS 的风险-based 评估方法应用于 foundation models 的上下文中，考虑这些模型的特定风险对 AI 应用的影响。\n白皮书详细解释了 foundation models 的技术构建、基于它们的 AI 应用开发、潜在风险、法规要求，以及一个系统化程序来确保信任。\n总体上，这为评估和实现 AI 应用的可靠性提供了实用框架。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.04937v1",
      "published_date": "2024-05-08 10:08:45 UTC",
      "updated_date": "2024-05-08 10:08:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:05:16.565625"
    },
    {
      "arxiv_id": "2405.04923v2",
      "title": "DataSP: A Differential All-to-All Shortest Path Algorithm for Learning Costs and Predicting Paths with Context",
      "title_zh": "翻译失败",
      "authors": [
        "Alan A. Lahoud",
        "Erik Schaffernicht",
        "Johannes A. Stork"
      ],
      "abstract": "Learning latent costs of transitions on graphs from trajectories\ndemonstrations under various contextual features is challenging but useful for\npath planning. Yet, existing methods either oversimplify cost assumptions or\nscale poorly with the number of observed trajectories. This paper introduces\nDataSP, a differentiable all-to-all shortest path algorithm to facilitate\nlearning latent costs from trajectories. It allows to learn from a large number\nof trajectories in each learning step without additional computation. Complex\nlatent cost functions from contextual features can be represented in the\nalgorithm through a neural network approximation. We further propose a method\nto sample paths from DataSP in order to reconstruct/mimic observed paths'\ndistributions. We prove that the inferred distribution follows the maximum\nentropy principle. We show that DataSP outperforms state-of-the-art\ndifferentiable combinatorial solver and classical machine learning approaches\nin predicting paths on graphs.",
      "tldr_zh": "本论文提出了一种名为 DataSP 的可微分 all-to-all 最短路径算法，用于从轨迹演示中学习图上潜在 costs，并处理各种 contextual features 的影响，从而提升路径规划的准确性。DataSP 允许在每个学习步骤中处理大量轨迹，而无需额外计算，并通过 neural network 近似表示复杂的潜在 cost 函数。该算法还包括一种路径采样方法，以重建观察路径的分布，并证明了推断分布符合 maximum entropy principle。在实验中，DataSP 优于现有的可微分组合求解器和经典机器学习方法，在路径预测任务上表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04923v2",
      "published_date": "2024-05-08 09:45:54 UTC",
      "updated_date": "2024-05-30 12:04:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:05:27.473748"
    },
    {
      "arxiv_id": "2405.04918v1",
      "title": "Delve into Base-Novel Confusion: Redundancy Exploration for Few-Shot Class-Incremental Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Haichen Zhou",
        "Yixiong Zou",
        "Ruixuan Li",
        "Yuhua Li",
        "Kui Xiao"
      ],
      "abstract": "Few-shot class-incremental learning (FSCIL) aims to acquire knowledge from\nnovel classes with limited samples while retaining information about base\nclasses. Existing methods address catastrophic forgetting and overfitting by\nfreezing the feature extractor during novel-class learning. However, these\nmethods usually tend to cause the confusion between base and novel classes,\ni.e., classifying novel-class samples into base classes. In this paper, we\ndelve into this phenomenon to study its cause and solution. We first interpret\nthe confusion as the collision between the novel-class and the base-class\nregion in the feature space. Then, we find the collision is caused by the\nlabel-irrelevant redundancies within the base-class feature and pixel space.\nThrough qualitative and quantitative experiments, we identify this redundancy\nas the shortcut in the base-class training, which can be decoupled to alleviate\nthe collision. Based on this analysis, to alleviate the collision between base\nand novel classes, we propose a method for FSCIL named Redundancy Decoupling\nand Integration (RDI). RDI first decouples redundancies from base-class space\nto shrink the intra-base-class feature space. Then, it integrates the\nredundancies as a dummy class to enlarge the inter-base-class feature space.\nThis process effectively compresses the base-class feature space, creating\nbuffer space for novel classes and alleviating the model's confusion between\nthe base and novel classes. Extensive experiments across benchmark datasets,\nincluding CIFAR-100, miniImageNet, and CUB-200-2011 demonstrate that our method\nachieves state-of-the-art performance.",
      "tldr_zh": "本研究探讨了 Few-Shot Class-Incremental Learning (FSCIL) 中的 base-novel 混淆问题，即在新类别少样本学习时，模型常将新类别样本误分类为旧类别。作者分析发现，这种混淆源于旧类别特征和像素空间中的标签无关 redundancies，以及训练过程中的 shortcut。针对此，他们提出 Redundancy Decoupling and Integration (RDI) 方法，先解耦 redundancies 以缩小旧类别特征空间，然后将这些 redundancies 整合为虚拟类别以扩大旧类别间特征空间，从而为新类别提供缓冲区减少混淆。在 CIFAR-100、miniImageNet 和 CUB-200-2011 等基准数据集上，RDI 实现了 state-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04918v1",
      "published_date": "2024-05-08 09:38:16 UTC",
      "updated_date": "2024-05-08 09:38:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:05:41.859956"
    },
    {
      "arxiv_id": "2405.04909v1",
      "title": "Traj-LLM: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengxing Lan",
        "Hongbo Li",
        "Lingshan Liu",
        "Bo Fan",
        "Yisheng Lv",
        "Yilong Ren",
        "Zhiyong Cui"
      ],
      "abstract": "Predicting the future trajectories of dynamic traffic actors is a cornerstone\ntask in autonomous driving. Though existing notable efforts have resulted in\nimpressive performance improvements, a gap persists in scene cognitive and\nunderstanding of the complex traffic semantics. This paper proposes Traj-LLM,\nthe first to investigate the potential of using Large Language Models (LLMs)\nwithout explicit prompt engineering to generate future motion from agents'\npast/observed trajectories and scene semantics. Traj-LLM starts with sparse\ncontext joint coding to dissect the agent and scene features into a form that\nLLMs understand. On this basis, we innovatively explore LLMs' powerful\ncomprehension abilities to capture a spectrum of high-level scene knowledge and\ninteractive information. Emulating the human-like lane focus cognitive function\nand enhancing Traj-LLM's scene comprehension, we introduce lane-aware\nprobabilistic learning powered by the pioneering Mamba module. Finally, a\nmulti-modal Laplace decoder is designed to achieve scene-compliant multi-modal\npredictions. Extensive experiments manifest that Traj-LLM, fortified by LLMs'\nstrong prior knowledge and understanding prowess, together with lane-aware\nprobability learning, outstrips state-of-the-art methods across evaluation\nmetrics. Moreover, the few-shot analysis further substantiates Traj-LLM's\nperformance, wherein with just 50% of the dataset, it outperforms the majority\nof benchmarks relying on complete data utilization. This study explores\nequipping the trajectory prediction task with advanced capabilities inherent in\nLLMs, furnishing a more universal and adaptable solution for forecasting agent\nmotion in a new way.",
      "tldr_zh": "这篇论文提出 Traj-LLM，一种创新方法，利用预训练 Large Language Models (LLMs) 而非显式提示工程，来增强自动驾驶中的轨迹预测任务。核心方法包括稀疏上下文联合编码分解代理和场景特征、利用 LLMs 的强大理解能力捕捉高层次场景知识和交互信息，以及引入基于 Mamba 模块的 lane-aware 概率学习来模拟人类车道焦点认知。论文还设计了多模态 Laplace 解码器，实现符合场景的多模态预测。实验结果显示，Traj-LLM 在评估指标上超越了现有最先进方法，即使使用仅 50% 的数据集，也超过了依赖完整数据的基准，为轨迹预测提供了一个更通用和适应的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04909v1",
      "published_date": "2024-05-08 09:28:04 UTC",
      "updated_date": "2024-05-08 09:28:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:05:51.994946"
    },
    {
      "arxiv_id": "2405.04897v1",
      "title": "Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Jideani",
        "Aurona Gerber"
      ],
      "abstract": "Recent social media posts on the cholera outbreak in Hammanskraal have\nhighlighted the diverse range of emotions people experienced in response to\nsuch an event. The extent of people's opinions varies greatly depending on\ntheir level of knowledge and information about the disease. The documented\nre-search about Cholera lacks investigations into the classification of\nemotions. This study aims to examine the emotions expressed in social media\nposts about Chol-era. A dataset of 23,000 posts was extracted and\npre-processed. The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer\nlibrary was applied to deter-mine the emotional significance of each text.\nAdditionally, Machine Learning (ML) models were applied for emotion\nclassification, including Long short-term memory (LSTM), Logistic regression,\nDecision trees, and the Bidirectional En-coder Representations from\nTransformers (BERT) model. The results of this study demonstrated that LSTM\nachieved the highest accuracy of 75%. Emotion classification presents a\npromising tool for gaining a deeper understanding of the impact of Cholera on\nsociety. The findings of this study might contribute to the development of\neffective interventions in public health strategies.",
      "tldr_zh": "本研究针对霍乱疫情社交媒体帖子中的情绪分类问题，分析了23,000条预处理后的数据集，以填补现有研究空白。研究采用Natural Language Toolkit (NLTK)进行情感分析，并应用Machine Learning (ML)模型，包括Long short-term memory (LSTM)、Logistic regression、Decision trees和Bidirectional Encoder Representations from Transformers (BERT)进行情绪分类。结果显示，LSTM模型取得了75%的最高准确率。总体而言，此工作有助于更深入理解霍乱对社会的影响，并为公共卫生策略的干预提供有益见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04897v1",
      "published_date": "2024-05-08 09:05:02 UTC",
      "updated_date": "2024-05-08 09:05:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:06:02.978985"
    },
    {
      "arxiv_id": "2407.03111v2",
      "title": "Compressed Latent Replays for Lightweight Continual Learning on Spiking Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Dequino",
        "Alessio Carpegna",
        "Davide Nadalini",
        "Alessandro Savino",
        "Luca Benini",
        "Stefano Di Carlo",
        "Francesco Conti"
      ],
      "abstract": "Rehearsal-based Continual Learning (CL) has been intensely investigated in\nDeep Neural Networks (DNNs). However, its application in Spiking Neural\nNetworks (SNNs) has not been explored in depth. In this paper we introduce the\nfirst memory-efficient implementation of Latent Replay (LR)-based CL for SNNs,\ndesigned to seamlessly integrate with resource-constrained devices. LRs combine\nnew samples with latent representations of previously learned data, to mitigate\nforgetting. Experiments on the Heidelberg SHD dataset with Sample and\nClass-Incremental tasks reach a Top-1 accuracy of 92.5% and 92%, respectively,\nwithout forgetting the previously learned information. Furthermore, we minimize\nthe LRs' requirements by applying a time-domain compression, reducing by two\norders of magnitude their memory requirement, with respect to a naive rehearsal\nsetup, with a maximum accuracy drop of 4%. On a Multi-Class-Incremental task,\nour SNN learns 10 new classes from an initial set of 10, reaching a Top-1\naccuracy of 78.4% on the full test set.",
      "tldr_zh": "该论文提出了一种针对 Spiking Neural Networks (SNNs) 的轻量级 Continual Learning (CL) 方法，名为 Compressed Latent Replays (LR)，通过结合新样本与先前学习数据的潜在表示来缓解模型遗忘问题，并通过时间域压缩将内存需求减少两个数量级。实验在 Heidelberg SHD 数据集上显示，在 Sample-Incremental 和 Class-Incremental 任务中，Top-1 准确率分别达到 92.5% 和 92%，而内存优化仅导致最大 4% 的准确率下降。在 Multi-Class-Incremental 任务中，该方法从初始 10 个类学习 10 个新类，最终在完整测试集上实现 78.4% 的 Top-1 准确率。该框架为资源受限设备上的高效 CL 提供了新途径。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03111v2",
      "published_date": "2024-05-08 09:03:17 UTC",
      "updated_date": "2024-07-04 08:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:06:16.433797"
    },
    {
      "arxiv_id": "2405.06700v1",
      "title": "LLM-Augmented Agent-Based Modelling for Social Simulations: Challenges and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Onder Gurcan"
      ],
      "abstract": "As large language models (LLMs) continue to make significant strides, their\nbetter integration into agent-based simulations offers a transformational\npotential for understanding complex social systems. However, such integration\nis not trivial and poses numerous challenges. Based on this observation, in\nthis paper, we explore architectures and methods to systematically develop\nLLM-augmented social simulations and discuss potential research directions in\nthis field. We conclude that integrating LLMs with agent-based simulations\noffers a powerful toolset for researchers and scientists, allowing for more\nnuanced, realistic, and comprehensive models of complex systems and human\nbehaviours.",
      "tldr_zh": "这篇论文探讨了将大型语言模型(LLMs)整合到基于智能体的社会模拟中的挑战和机会，旨在提升对复杂社会系统的理解。作者系统地探索了开发LLM-augmented社会模拟的架构和方法，包括处理整合难题的潜在研究方向。研究结论认为，这种整合提供了一个强大工具集，能创建更细致、现实和全面的模型，用于模拟复杂系统和人类行为。",
      "categories": [
        "physics.soc-ph",
        "cs.AI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "11 pages, 0 figures, Hybrid Human Artificial Intelligence (HHAI) 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.06700v1",
      "published_date": "2024-05-08 08:57:54 UTC",
      "updated_date": "2024-05-08 08:57:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:06:27.424884"
    },
    {
      "arxiv_id": "2405.04890v3",
      "title": "GISR: Geometric Initialization and Silhouette-based Refinement for Single-View Robot Pose and Configuration Estimation",
      "title_zh": "GISR：几何初始化和基于轮廓的细化，用于单视图机器人位姿和配置估计",
      "authors": [
        "Ivan Bilić",
        "Filip Marić",
        "Fabio Bonsignorio",
        "Ivan Petrović"
      ],
      "abstract": "In autonomous robotics, measurement of the robot's internal state and\nperception of its environment, including interaction with other agents such as\ncollaborative robots, are essential. Estimating the pose of the robot arm from\na single view has the potential to replace classical eye-to-hand calibration\napproaches and is particularly attractive for online estimation and dynamic\nenvironments. In addition to its pose, recovering the robot configuration\nprovides a complete spatial understanding of the observed robot that can be\nused to anticipate the actions of other agents in advanced robotics use cases.\nFurthermore, this additional redundancy enables the planning and execution of\nrecovery protocols in case of sensor failures or external disturbances. We\nintroduce GISR - a deep configuration and robot-to-camera pose estimation\nmethod that prioritizes execution in real-time. GISR consists of two modules:\n(i) a geometric initialization module that efficiently computes an approximate\nrobot pose and configuration, and (ii) a deep iterative silhouette-based\nrefinement module that arrives at a final solution in just a few iterations. We\nevaluate GISR on publicly available data and show that it outperforms existing\nmethods of the same class in terms of both speed and accuracy, and can compete\nwith approaches that rely on ground-truth proprioception and recover only the\npose.",
      "tldr_zh": "本文提出 GISR 方法，用于从单视图估计机器人臂的姿态和配置，旨在取代传统的 eye-to-hand 校准，并适用于在线估计和动态环境。GISR 包括两个模块：(i) 几何初始化模块，快速计算近似姿态和配置；(ii) 深度迭代轮廓-based 精炼模块，通过少量迭代实现精确结果。该方法在公开数据上实验表明，在速度和准确性上优于同类方法，甚至能与依赖 ground-truth proprioception 的姿态恢复方法竞争。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication in the Robotics and Automation Letters 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04890v3",
      "published_date": "2024-05-08 08:39:25 UTC",
      "updated_date": "2024-09-16 20:28:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:06:39.348074"
    },
    {
      "arxiv_id": "2405.04883v2",
      "title": "FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Zehan Wang",
        "Ziang Zhang",
        "Xize Cheng",
        "Rongjie Huang",
        "Luping Liu",
        "Zhenhui Ye",
        "Haifeng Huang",
        "Yang Zhao",
        "Tao Jin",
        "Peng Gao",
        "Zhou Zhao"
      ],
      "abstract": "Unified multi-model representation spaces are the foundation of multimodal\nunderstanding and generation. However, the billions of model parameters and\ncatastrophic forgetting problems make it challenging to further enhance\npre-trained unified spaces. In this work, we propose FreeBind, an idea that\ntreats multimodal representation spaces as basic units, and freely augments\npre-trained unified space by integrating knowledge from extra expert spaces via\n\"space bonds\". Specifically, we introduce two kinds of basic space bonds: 1)\nSpace Displacement Bond and 2) Space Combination Bond. Based on these basic\nbonds, we design Complex Sequential & Parallel Bonds to effectively integrate\nmultiple spaces simultaneously. Benefiting from the modularization concept, we\nfurther propose a coarse-to-fine customized inference strategy to flexibly\nadjust the enhanced unified space for different purposes. Experimentally, we\nbind ImageBind with extra image-text and audio-text expert spaces, resulting in\nthree main variants: ImageBind++, InternVL_IB, and InternVL_IB++. These\nresulting spaces outperform ImageBind on 5 audio-image-text downstream tasks\nacross 9 datasets. Moreover, via customized inference, it even surpasses the\nadvanced audio-text and image-text expert spaces.",
      "tldr_zh": "该论文提出 FreeBind 方法，通过知识融合来免费增强统一多模态表示空间，解决模型参数庞大和灾难性遗忘的挑战。FreeBind 将多模态空间视为基本单位，利用 Space Displacement Bond 和 Space Combination Bond 等空间绑定技术，以及 Complex Sequential & Parallel Bonds 来整合多个专家空间，并引入粗到细的自定义推理策略以灵活调整空间。实验结果显示，将 ImageBind 与额外图像-文本和音频-文本专家空间绑定后，生成的 ImageBind++、InternVL_IB 和 InternVL_IB++ 变体在 5 个音频-图像-文本下游任务的 9 个数据集上，性能优于 ImageBind 甚至超过高级专家空间。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML 2024. The code and checkpoints will be released at\n  https://github.com/zehanwang01/FreeBind",
      "pdf_url": "http://arxiv.org/pdf/2405.04883v2",
      "published_date": "2024-05-08 08:32:34 UTC",
      "updated_date": "2024-05-10 07:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:06:53.049882"
    },
    {
      "arxiv_id": "2405.04880v3",
      "title": "The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio",
      "title_zh": "翻译失败",
      "authors": [
        "Yuankun Xie",
        "Yi Lu",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Zhiyong Wang",
        "Jianhua Tao",
        "Xin Qi",
        "Xiaopeng Wang",
        "Yukun Liu",
        "Haonan Cheng",
        "Long Ye",
        "Yi Sun"
      ],
      "abstract": "With the proliferation of Audio Language Model (ALM) based deepfake audio,\nthere is an urgent need for generalized detection methods. ALM-based deepfake\naudio currently exhibits widespread, high deception, and type versatility,\nposing a significant challenge to current audio deepfake detection (ADD) models\ntrained solely on vocoded data. To effectively detect ALM-based deepfake audio,\nwe focus on the mechanism of the ALM-based audio generation method, the\nconversion from neural codec to waveform. We initially constructed the\nCodecfake dataset, an open-source, large-scale collection comprising over 1\nmillion audio samples in both English and Chinese, focus on ALM-based audio\ndetection. As countermeasure, to achieve universal detection of deepfake audio\nand tackle domain ascent bias issue of original sharpness aware minimization\n(SAM), we propose the CSAM strategy to learn a domain balanced and generalized\nminima. In our experiments, we first demonstrate that ADD model training with\nthe Codecfake dataset can effectively detects ALM-based audio. Furthermore, our\nproposed generalization countermeasure yields the lowest average equal error\nrate (EER) of 0.616% across all test conditions compared to baseline models.\nThe dataset and associated code are available online.",
      "tldr_zh": "该研究针对Audio Language Model (ALM) 基于的deepfake音频泛滥问题，构建了开源的大型Codecfake数据集，包含超过100万英文和中文音频样本，专注于ALM-based音频检测，以弥补现有模型的局限性。作为对策，提出CSAM策略（一种改进的Sharpness Aware Minimization），旨在实现deepfake音频的通用检测，并解决领域偏差问题。实验结果显示，使用Codecfake数据集训练的Audio Deepfake Detection (ADD)模型能有效识别ALM-based音频，且CSAM策略使模型在所有测试条件下达到最低平均等错误率 (EER) 为0.616%。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04880v3",
      "published_date": "2024-05-08 08:28:40 UTC",
      "updated_date": "2024-12-25 07:30:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:07:02.859922"
    },
    {
      "arxiv_id": "2405.05292v1",
      "title": "Smart Portable Computer",
      "title_zh": "智能便携计算机",
      "authors": [
        "Niladri Das"
      ],
      "abstract": "Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and\nuniversities transitioning to virtual platforms, students encountered\ndifficulties in acquiring PCs such as desktops or laptops. The starting prices,\naround 15,000 INR, often failed to offer adequate system specifications, posing\na challenge for consumers. Additionally, those reliant on laptops for work\nfound the conventional approach cumbersome. Enter the \"Portable Smart\nComputer,\" a leap into the future of computing. This innovative device boasts\nspeed and performance comparable to traditional desktops but in a compact,\nenergy-efficient, and cost-effective package. It delivers a seamless desktop\nexperience, whether one is editing documents, browsing multiple tabs, managing\nspreadsheets, or creating presentations. Moreover, it supports programming\nlanguages like Python, C, C++, as well as compilers such as Keil and Xilinx,\ncatering to the needs of programmers.",
      "tldr_zh": "在COVID-19疫情期间，许多学生和工作者难以获取价格约15,000 INR且性能不足的传统PC，本文提出“Portable Smart Computer”作为一种创新解决方案。该设备提供与桌面电脑相当的速度和性能，却采用紧凑、节能且成本有效的设计，支持文档编辑、多标签浏览、表格管理和演示文稿创建。此外，它兼容编程语言如Python、C、C++以及编译器如Keil和Xilinx，满足编程需求，并为用户带来更便捷的计算体验。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2405.05292v1",
      "published_date": "2024-05-08 08:20:27 UTC",
      "updated_date": "2024-05-08 08:20:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:07:15.532031"
    },
    {
      "arxiv_id": "2405.04875v1",
      "title": "SCALA: Split Federated Learning with Concatenated Activations and Logit Adjustments",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarong Yang",
        "Yuan Liu"
      ],
      "abstract": "Split Federated Learning (SFL) is a distributed machine learning framework\nwhich strategically divides the learning process between a server and clients\nand collaboratively trains a shared model by aggregating local models updated\nbased on data from distributed clients. However, data heterogeneity and partial\nclient participation result in label distribution skew, which severely degrades\nthe learning performance. To address this issue, we propose SFL with\nConcatenated Activations and Logit Adjustments (SCALA). Specifically, the\nactivations from the client-side models are concatenated as the input of the\nserver-side model so as to centrally adjust label distribution across different\nclients, and logit adjustments of loss functions on both server-side and\nclient-side models are performed to deal with the label distribution variation\nacross different subsets of participating clients. Theoretical analysis and\nexperimental results verify the superiority of the proposed SCALA on public\ndatasets.",
      "tldr_zh": "本文提出 SCALA，一种针对 Split Federated Learning (SFL) 的改进框架，用于解决数据异质性和部分客户端参与导致的标签分布偏差问题。SCALA 通过将客户端模型的 Concatenated Activations 作为服务器模型的输入，实现对不同客户端标签分布的集中调整，同时在服务器端和客户端端进行 Logit Adjustments 以处理参与客户端子集的分布变化。实验和理论分析证明，SCALA 在公共数据集上显著提升了学习性能，验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04875v1",
      "published_date": "2024-05-08 08:12:21 UTC",
      "updated_date": "2024-05-08 08:12:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:07:27.450097"
    },
    {
      "arxiv_id": "2405.04872v1",
      "title": "Logical Negation Augmenting and Debiasing for Prompt-based Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Yitian Li",
        "Jidong Tian",
        "Hao He",
        "Yaohui Jin"
      ],
      "abstract": "Prompt-based methods have gained increasing attention on NLP and shown\nvalidity on many downstream tasks. Many works have focused on mining these\nmethods' potential for knowledge extraction, but few explore their ability to\nmake logical reasoning. In this work, we focus on the effectiveness of the\nprompt-based methods on first-order logical reasoning and find that the\nbottleneck lies in logical negation. Based on our analysis, logical negation\ntends to result in spurious correlations to negative answers, while\npropositions without logical negation correlate to positive answers. To solve\nthe problem, we propose a simple but effective method, Negation Augmenting and\nNegation Debiasing (NAND), which introduces negative propositions to\nprompt-based methods without updating parameters. Specifically, these negative\npropositions can counteract spurious correlations by providing \"not\" for all\ninstances so that models cannot make decisions only by whether expressions\ncontain a logical negation. Experiments on three datasets show that NAND not\nonly solves the problem of calibrating logical negation but also significantly\nenhances prompt-based methods of logical reasoning without model retraining.",
      "tldr_zh": "本研究探讨了基于提示的方法（Prompt-based methods）在第一阶逻辑推理中的局限性，发现逻辑否定（Logical negation）会导致虚假相关性（spurious correlations），即否定表达倾向于与负面答案相关，而非否定表达则与正面答案相关。作者提出了一种简单有效的Negation Augmenting and Negation Debiasing (NAND)方法，通过引入负面命题（negative propositions）来对抗这些虚假相关性，而无需更新模型参数。NAND的工作机制是为所有实例提供“not”表达，使模型无法仅依赖逻辑否定的存在来决策。实验在三个数据集上表明，该方法不仅校准了逻辑否定的问题，还显著提升了Prompt-based methods的逻辑推理性能，而无需重新训练模型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04872v1",
      "published_date": "2024-05-08 08:05:47 UTC",
      "updated_date": "2024-05-08 08:05:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:07:40.621822"
    },
    {
      "arxiv_id": "2405.04868v2",
      "title": "Enhancing Geometric Ontology Embeddings for $\\mathcal{EL}^{++}$ with Negative Sampling and Deductive Closure Filtering",
      "title_zh": "翻译失败",
      "authors": [
        "Olga Mashkova",
        "Fernando Zhapa-Camacho",
        "Robert Hoehndorf"
      ],
      "abstract": "Ontology embeddings map classes, relations, and individuals in ontologies\ninto $\\mathbb{R}^n$, and within $\\mathbb{R}^n$ similarity between entities can\nbe computed or new axioms inferred. For ontologies in the Description Logic\n$\\mathcal{EL}^{++}$, several embedding methods have been developed that\nexplicitly generate models of an ontology. However, these methods suffer from\nsome limitations; they do not distinguish between statements that are\nunprovable and provably false, and therefore they may use entailed statements\nas negatives. Furthermore, they do not utilize the deductive closure of an\nontology to identify statements that are inferred but not asserted. We\nevaluated a set of embedding methods for $\\mathcal{EL}^{++}$ ontologies based\non high-dimensional ball representation of concept descriptions, incorporating\nseveral modifications that aim to make use of the ontology deductive closure.\nIn particular, we designed novel negative losses that account both for the\ndeductive closure and different types of negatives. We demonstrate that our\nembedding methods improve over the baseline ontology embedding in the task of\nknowledge base or ontology completion.",
      "tldr_zh": "本研究旨在提升 \\(\\mathcal{EL}^{++}\\) 描述逻辑的本体嵌入（Ontology embeddings），通过引入负采样（Negative Sampling）和演绎闭包过滤（Deductive Closure Filtering）来解决现有方法的局限性，如无法区分不可证明语句与可证明为假语句，以及未充分利用本体演绎闭包。作者设计了新的负损失函数（negative losses），结合高维球表示（high-dimensional ball representation）来处理不同类型的负例，并利用本体演绎闭包识别推断出的语句。这些改进方法在知识库或本体完成（knowledge base or ontology completion）任务中，显著优于基线嵌入方法，展示了更高的准确性和有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Revised version",
      "pdf_url": "http://arxiv.org/pdf/2405.04868v2",
      "published_date": "2024-05-08 07:50:21 UTC",
      "updated_date": "2024-06-26 11:17:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:07:51.042890"
    },
    {
      "arxiv_id": "2405.05985v1",
      "title": "TrafficGPT: Towards Multi-Scale Traffic Analysis and Generation with Spatial-Temporal Agent Framework",
      "title_zh": "TrafficGPT: 面向多尺度交通分析和生成的时空代理框架",
      "authors": [
        "Jinhui Ouyang",
        "Yijie Zhu",
        "Xiang Yuan",
        "Di Wu"
      ],
      "abstract": "The precise prediction of multi-scale traffic is a ubiquitous challenge in\nthe urbanization process for car owners, road administrators, and governments.\nIn the case of complex road networks, current and past traffic information from\nboth upstream and downstream roads are crucial since various road networks have\ndifferent semantic information about traffic. Rationalizing the utilization of\nsemantic information can realize short-term, long-term, and unseen road traffic\nprediction. As the demands of multi-scale traffic analysis increase, on-demand\ninteractions and visualizations are expected to be available for transportation\nparticipants. We have designed a multi-scale traffic generation system, namely\nTrafficGPT, using three AI agents to process multi-scale traffic data, conduct\nmulti-scale traffic analysis, and present multi-scale visualization results.\nTrafficGPT consists of three essential AI agents: 1) a text-to-demand agent\nthat is employed with Question & Answer AI to interact with users and extract\nprediction tasks through texts; 2) a traffic prediction agent that leverages\nmulti-scale traffic data to generate temporal features and similarity, and fuse\nthem with limited spatial features and similarity, to achieve accurate\nprediction of three tasks; and 3) a suggestion and visualization agent that\nuses the prediction results to generate suggestions and visualizations,\nproviding users with a comprehensive understanding of traffic conditions. Our\nTrafficGPT system focuses on addressing concerns about traffic prediction from\ntransportation participants, and conducted extensive experiments on five\nreal-world road datasets to demonstrate its superior predictive and interactive\nperformance",
      "tldr_zh": "论文提出 TrafficGPT 系统，这是一个基于 Spatial-Temporal Agent Framework 的多智能体框架，旨在实现多尺度交通分析、预测和生成，以解决复杂路网中的交通预测挑战。系统包括三个关键 AI agents：text-to-demand agent 通过问答交互提取用户预测任务、traffic prediction agent 融合多尺度时间和空间特征进行精确预测，以及 suggestion and visualization agent 基于预测结果生成交通建议和可视化。实验在五个真实世界道路数据集上验证了 TrafficGPT 的优越预测准确性和交互性能，为交通参与者提供更有效的多尺度交通解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05985v1",
      "published_date": "2024-05-08 07:48:40 UTC",
      "updated_date": "2024-05-08 07:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:08:03.593756"
    },
    {
      "arxiv_id": "2405.06699v1",
      "title": "ChatSOS: Vector Database Augmented Generative Question Answering Assistant in Safety Engineering",
      "title_zh": "翻译失败",
      "authors": [
        "Haiyang Tang",
        "Dongping Chen",
        "Qingzhao Chu"
      ],
      "abstract": "With the rapid advancement of natural language processing technologies,\ngenerative artificial intelligence techniques, represented by large language\nmodels (LLMs), are gaining increasing prominence and demonstrating significant\npotential for applications in safety engineering. However, fundamental LLMs\nface constraints such as limited training data coverage and unreliable\nresponses. This study develops a vector database from 117 explosion accident\nreports in China spanning 2013 to 2023, employing techniques such as corpus\nsegmenting and vector embedding. By utilizing the vector database, which\noutperforms the relational database in information retrieval quality, we\nprovide LLMs with richer, more relevant knowledge. Comparative analysis of LLMs\ndemonstrates that ChatSOS significantly enhances reliability, accuracy, and\ncomprehensiveness, improves adaptability and clarification of responses. These\nresults illustrate the effectiveness of supplementing LLMs with an external\ndatabase, highlighting their potential to handle professional queries in safety\nengineering and laying a foundation for broader applications.",
      "tldr_zh": "该研究开发了 ChatSOS，一种基于向量数据库增强的生成式问答助手，旨在解决大型语言模型(LLMs)在安全工程中的数据覆盖有限和响应不可靠等问题。通过从2013至2023年117份中国爆炸事故报告中构建向量数据库，并采用语料分割和向量嵌入技术，ChatSOS显著提升了信息检索质量，提供更丰富相关的知识支持。实验结果显示，与基线LLMs相比，ChatSOS提高了响应的可靠性、准确性、全面性和适应性，为LLMs处理安全工程专业查询奠定了基础，并扩展了其应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06699v1",
      "published_date": "2024-05-08 07:21:26 UTC",
      "updated_date": "2024-05-08 07:21:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:08:15.490250"
    },
    {
      "arxiv_id": "2407.08741v1",
      "title": "A digital twin based approach to smart lighting design",
      "title_zh": "基于数字孪生的智能照明设计方法",
      "authors": [
        "Elham Mohammadrezaei",
        "Alexander Giovannelli",
        "Logan Lane",
        "Denis Gracanin"
      ],
      "abstract": "Lighting has a critical impact on user mood and behavior, especially in\narchitectural settings. Consequently, smart lighting design is a rapidly\ngrowing research area. We describe a digital twin-based approach to smart\nlighting design that uses an immersive virtual reality digital twin equivalent\n(virtual environment) of the real world, physical architectural space to\nexplore the visual impact of light configurations. The CLIP neural network is\nused to obtain a similarity measure between a photo of the physical space with\nthe corresponding rendering in the virtual environment. A case study was used\nto evaluate the proposed design process. The obtained similarity value of over\n87% demonstrates the utility of the proposed approach.",
      "tldr_zh": "该研究提出了一种基于 digital twin 的智能灯光设计方法，利用沉浸式 virtual reality 创建真实建筑空间的虚拟环境，以评估灯光配置的视觉影响。方法通过 CLIP neural network 计算物理空间照片与虚拟环境渲染的相似度，从而量化设计效果。在案例研究中，该方法取得了超过87%的相似度结果，证明其在提升灯光设计实用性方面的显著价值。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.08741v1",
      "published_date": "2024-05-08 06:35:14 UTC",
      "updated_date": "2024-05-08 06:35:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:08:26.550307"
    },
    {
      "arxiv_id": "2405.04841v1",
      "title": "xMTrans: Temporal Attentive Cross-Modality Fusion Transformer for Long-Term Traffic Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Huy Quang Ung",
        "Hao Niu",
        "Minh-Son Dao",
        "Shinya Wada",
        "Atsunori Minamikawa"
      ],
      "abstract": "Traffic predictions play a crucial role in intelligent transportation\nsystems. The rapid development of IoT devices allows us to collect different\nkinds of data with high correlations to traffic predictions, fostering the\ndevelopment of efficient multi-modal traffic prediction models. Until now,\nthere are few studies focusing on utilizing advantages of multi-modal data for\ntraffic predictions. In this paper, we introduce a novel temporal attentive\ncross-modality transformer model for long-term traffic predictions, namely\nxMTrans, with capability of exploring the temporal correlations between the\ndata of two modalities: one target modality (for prediction, e.g., traffic\ncongestion) and one support modality (e.g., people flow). We conducted\nextensive experiments to evaluate our proposed model on traffic congestion and\ntaxi demand predictions using real-world datasets. The results showed the\nsuperiority of xMTrans against recent state-of-the-art methods on long-term\ntraffic predictions. In addition, we also conducted a comprehensive ablation\nstudy to further analyze the effectiveness of each module in xMTrans.",
      "tldr_zh": "本研究针对智能交通系统中的长期交通预测问题，提出了一种新型模型xMTrans，该模型利用时序注意跨模态融合Transformer，探索目标模态（如交通拥堵）和支持模态（如人流量）之间的时序相关性，以充分利用IoT设备收集的多模态数据。xMTrans通过有效整合多模态信息，提升了预测的准确性和效率。实验结果显示，该模型在真实数据集上的交通拥堵和出租车需求预测中，优于现有最先进方法；此外，消融研究进一步验证了模型各模块的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at MDM 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04841v1",
      "published_date": "2024-05-08 06:29:26 UTC",
      "updated_date": "2024-05-08 06:29:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:08:39.937818"
    },
    {
      "arxiv_id": "2405.04825v2",
      "title": "Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution",
      "title_zh": "翻译失败",
      "authors": [
        "Shuo Shao",
        "Yiming Li",
        "Hongwei Yao",
        "Yiling He",
        "Zhan Qin",
        "Kui Ren"
      ],
      "abstract": "Ownership verification is currently the most critical and widely adopted\npost-hoc method to safeguard model copyright. In general, model owners exploit\nit to identify whether a given suspicious third-party model is stolen from them\nby examining whether it has particular properties `inherited' from their\nreleased models. Currently, backdoor-based model watermarks are the primary and\ncutting-edge methods to implant such properties in the released models.\nHowever, backdoor-based methods have two fatal drawbacks, including harmfulness\nand ambiguity. The former indicates that they introduce maliciously\ncontrollable misclassification behaviors ($i.e.$, backdoor) to the watermarked\nreleased models. The latter denotes that malicious users can easily pass the\nverification by finding other misclassified samples, leading to ownership\nambiguity.\n  In this paper, we argue that both limitations stem from the `zero-bit' nature\nof existing watermarking schemes, where they exploit the status ($i.e.$,\nmisclassified) of predictions for verification. Motivated by this\nunderstanding, we design a new watermarking paradigm, $i.e.$, Explanation as a\nWatermark (EaaW), that implants verification behaviors into the explanation of\nfeature attribution instead of model predictions. Specifically, EaaW embeds a\n`multi-bit' watermark into the feature attribution explanation of specific\ntrigger samples without changing the original prediction. We correspondingly\ndesign the watermark embedding and extraction algorithms inspired by\nexplainable artificial intelligence. In particular, our approach can be used\nfor different tasks ($e.g.$, image classification and text generation).\nExtensive experiments verify the effectiveness and harmlessness of our EaaW and\nits resistance to potential attacks.",
      "tldr_zh": "该研究针对现有模型所有权验证中 backdoor-based watermarks 的有害性（引入恶意误分类）和模糊性（易被伪造）问题，提出了一种新范式：Explanation as a Watermark (EaaW)。EaaW 通过在特定触发样本的 feature attribution 解释中嵌入 multi-bit watermark，而不改变模型的原始预测，从而实现 harmless 的水印植入。基于 explainable artificial intelligence 的嵌入和提取算法，该方法适用于图像分类和文本生成等任务；实验结果证明了 EaaW 的有效性、抗攻击性和可靠性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper is accepted by Network and Distributed System Security\n  Symposium (NDSS) 2025",
      "pdf_url": "http://arxiv.org/pdf/2405.04825v2",
      "published_date": "2024-05-08 05:49:46 UTC",
      "updated_date": "2024-09-10 01:52:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:08:55.104088"
    },
    {
      "arxiv_id": "2405.04820v1",
      "title": "APrompt4EM: Augmented Prompt Tuning for Generalized Entity Matching",
      "title_zh": "APrompt4EM：用于泛化实体匹配的增强提示调整",
      "authors": [
        "Yikuan Xia",
        "Jiazun Chen",
        "Xinchi Li",
        "Jun Gao"
      ],
      "abstract": "Generalized Entity Matching (GEM), which aims at judging whether two records\nrepresented in different formats refer to the same real-world entity, is an\nessential task in data management. The prompt tuning paradigm for pre-trained\nlanguage models (PLMs), including the recent PromptEM model, effectively\naddresses the challenges of low-resource GEM in practical applications,\noffering a robust solution when labeled data is scarce. However, existing\nprompt tuning models for GEM face the challenges of prompt design and\ninformation gap. This paper introduces an augmented prompt tuning framework for\nthe challenges, which consists of two main improvements. The first is an\naugmented contextualized soft token-based prompt tuning method that extracts a\nguiding soft token benefit for the PLMs' prompt tuning, and the second is a\ncost-effective information augmentation strategy leveraging large language\nmodels (LLMs). Our approach performs well on the low-resource GEM challenges.\nExtensive experiments show promising advancements of our basic model without\ninformation augmentation over existing methods based on moderate-size PLMs\n(average 5.24%+), and our model with information augmentation achieves\ncomparable performance compared with fine-tuned LLMs, using less than 14% of\nthe API fee.",
      "tldr_zh": "这篇论文针对Generalized Entity Matching (GEM)任务，提出了一种增强提示调优框架APrompt4EM，以解决现有提示调优模型在提示设计和信息缺口方面的挑战。框架的核心包括基于上下文化软标记的增强提示调优方法（提取引导软标记来优化PLMs的性能）和一种利用LLMs的成本有效信息增强策略。实验结果表明，该方法在低资源GEM场景下显著提升性能，基本模型比现有基于中等规模PLMs的方法平均提高了5.24%，而添加信息增强后，其性能可与微调LLMs相当，但API费用仅为其不到14%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04820v1",
      "published_date": "2024-05-08 05:38:56 UTC",
      "updated_date": "2024-05-08 05:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:09:05.195505"
    },
    {
      "arxiv_id": "2405.04819v4",
      "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature",
      "title_zh": "翻译失败",
      "authors": [
        "Dawei Li",
        "Shu Yang",
        "Zhen Tan",
        "Jae Young Baik",
        "Sukwon Yun",
        "Joseph Lee",
        "Aaron Chacko",
        "Bojian Hou",
        "Duy Duong-Tran",
        "Ying Ding",
        "Huan Liu",
        "Li Shen",
        "Tianlong Chen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have achieved promising\nperformances across various applications. Nonetheless, the ongoing challenge of\nintegrating long-tail knowledge continues to impede the seamless adoption of\nLLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic\nCo-Augmentation of LLMs and KG, to address this limitation and demonstrate its\nability on studying Alzheimer's Disease (AD), a specialized sub-field in\nbiomedicine and a global health priority. With a synergized framework of LLM\nand KG mutually enhancing each other, we first leverage LLM to construct an\nevolving AD-specific knowledge graph (KG) sourced from AD-related scientific\nliterature, and then we utilize a coarse-to-fine sampling method with a novel\nself-aware knowledge retrieval approach to select appropriate knowledge from\nthe KG to augment LLM inference capabilities. The experimental results,\nconducted on our constructed AD question answering (ADQA) benchmark, underscore\nthe efficacy of DALK. Additionally, we perform a series of detailed analyses\nthat can offer valuable insights and guidelines for the emerging topic of\nmutually enhancing KG and LLM. We will release the code and data at\nhttps://github.com/David-Li0406/DALK.",
      "tldr_zh": "该研究提出 DALK 框架，即动态协同增强大型语言模型 (LLMs) 和知识图谱 (KG)，以解决 LLMs 在专业领域整合长尾知识的挑战，特别是针对阿尔茨海默病 (AD) 相关问题。DALK 通过利用 LLMs 从 AD 科学文献构建动态的 AD 特定 KG，然后采用粗到细采样方法和自知觉知识检索策略，从 KG 中选择适当知识来增强 LLMs 的推理能力。在构建的 AD 问答基准 (ADQA) 上，实验结果证明 DALK 显著提升了性能，并提供了 LLMs 和 KG 相互增强的宝贵见解和指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2024 Findings; revise format problem",
      "pdf_url": "http://arxiv.org/pdf/2405.04819v4",
      "published_date": "2024-05-08 05:38:20 UTC",
      "updated_date": "2024-10-17 18:31:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:09:16.254548"
    },
    {
      "arxiv_id": "2405.04814v2",
      "title": "A Novel Technique for Query Plan Representation Based on Graph Neural Nets",
      "title_zh": "翻译失败",
      "authors": [
        "Baoming Chang",
        "Amin Kamali",
        "Verena Kantere"
      ],
      "abstract": "Learning representations for query plans play a pivotal role in machine\nlearning-based query optimizers of database management systems. To this end,\nparticular model architectures are proposed in the literature to transform the\ntree-structured query plans into representations with formats learnable by\ndownstream machine learning models. However, existing research rarely compares\nand analyzes the query plan representation capabilities of these tree models\nand their direct impact on the performance of the overall optimizer. To address\nthis problem, we perform a comparative study to explore the effect of using\ndifferent state-of-the-art tree models on the optimizer's cost estimation and\nplan selection performance in relatively complex workloads. Additionally, we\nexplore the possibility of using graph neural networks (GNNs) in the query plan\nrepresentation task. We propose a novel tree model BiGG employing Bidirectional\nGNN aggregated by Gated recurrent units (GRUs) and demonstrate experimentally\nthat BiGG provides significant improvements to cost estimation tasks and\nrelatively excellent plan selection performance compared to the\nstate-of-the-art tree models.",
      "tldr_zh": "该研究探讨了基于机器学习的查询优化器中，查询计划表示的重要性，并比较了不同树模型对优化器成本估计和计划选择性能的影响。现有方法将树结构查询计划转化为可学习表示，但缺乏系统比较，本文通过实验分析了这些模型在复杂工作负载下的表现。作者提出了一种新颖模型BiGG，利用Bidirectional GNN和Gated recurrent units (GRUs)进行聚合，显著提升了成本估计任务的准确性，并在计划选择上表现出色。实验结果显示，BiGG相较于现有树模型取得了明显的性能改进。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04814v2",
      "published_date": "2024-05-08 04:59:59 UTC",
      "updated_date": "2024-06-05 07:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:09:27.857079"
    },
    {
      "arxiv_id": "2405.08005v2",
      "title": "Graphon Mean Field Games with a Representative Player: Analysis and Learning Algorithm",
      "title_zh": "翻译失败",
      "authors": [
        "Fuzhong Zhou",
        "Chenyu Zhang",
        "Xu Chen",
        "Xuan Di"
      ],
      "abstract": "We propose a discrete time graphon game formulation on continuous state and\naction spaces using a representative player to study stochastic games with\nheterogeneous interaction among agents. This formulation admits both\nphilosophical and mathematical advantages, compared to a widely adopted\nformulation using a continuum of players. We prove the existence and uniqueness\nof the graphon equilibrium with mild assumptions, and show that this\nequilibrium can be used to construct an approximate solution for finite player\ngame on networks, which is challenging to analyze and solve due to curse of\ndimensionality. An online oracle-free learning algorithm is developed to solve\nthe equilibrium numerically, and sample complexity analysis is provided for its\nconvergence.",
      "tldr_zh": "这篇论文提出了一种使用代表性玩家的离散时间图元游戏（graphon game）框架，用于研究连续状态和动作空间中具有异质交互的代理人（agents）的随机游戏，并强调了其相对于连续玩家制定方案的哲学和数学优势。作者证明了图元均衡（graphon equilibrium）的存在性和唯一性，仅需温和假设，并展示了该均衡可用于构建有限玩家网络游戏的近似解，以克服维数灾难（curse of dimensionality）。此外，他们开发了一个在线的、无需预言机（oracle-free）的学习算法来数值求解该均衡，并提供了样本复杂度分析（sample complexity analysis）来证明其收敛性。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.OC",
      "comment": "Published as a conference paper at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.08005v2",
      "published_date": "2024-05-08 04:44:16 UTC",
      "updated_date": "2024-06-05 02:51:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:09:40.438980"
    },
    {
      "arxiv_id": "2405.04798v2",
      "title": "From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control",
      "title_zh": "从 LLMs 到动作：潜变量代码作为分层机器人控制中的桥梁",
      "authors": [
        "Yide Shentu",
        "Philipp Wu",
        "Aravind Rajeswaran",
        "Pieter Abbeel"
      ],
      "abstract": "Hierarchical control for robotics has long been plagued by the need to have a\nwell defined interface layer to communicate between high-level task planners\nand low-level policies. With the advent of LLMs, language has been emerging as\na prospective interface layer. However, this has several limitations. Not all\ntasks can be decomposed into steps that are easily expressible in natural\nlanguage (e.g. performing a dance routine). Further, it makes end-to-end\nfinetuning on embodied data challenging due to domain shift and catastrophic\nforgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB)\n-- as an alternate architecture to overcome these limitations. \\method~uses a\nlearnable latent code to act as a bridge between LLMs and low-level policies.\nThis enables LLMs to flexibly communicate goals in the task plan without being\nentirely constrained by language limitations. Additionally, it enables\nend-to-end finetuning without destroying the embedding space of word tokens\nlearned during pre-training. Through experiments on Language Table and Calvin,\ntwo common language based benchmarks for embodied agents, we find that\n\\method~outperforms baselines (including those w/ GPT-4V) that leverage pure\nlanguage as the interface layer on tasks that require reasoning and multi-step\nbehaviors.",
      "tldr_zh": "该论文探讨了机器人分层控制中高层任务规划器和底层策略之间的接口问题，指出使用语言作为接口（如LLMs）存在局限性，例如难以表达某些任务（如舞蹈动作）和端到端微调的挑战。作者提出Learnable Latent Codes as Bridges (LCB)方法，使用可学习的潜在代码作为桥梁，允许LLMs灵活传达任务目标，同时避免破坏预训练的词嵌入空间。通过在Language Table和Calvin基准上的实验，LCB在需要推理和多步行为的任务上超过了纯语言接口基线，包括那些使用GPT-4V的模型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04798v2",
      "published_date": "2024-05-08 04:14:06 UTC",
      "updated_date": "2024-07-08 21:02:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:09:52.364193"
    },
    {
      "arxiv_id": "2405.06697v1",
      "title": "Automated Conversion of Static to Dynamic Scheduler via Natural Language",
      "title_zh": "通过自然语言的静态调度器向动态调度器的自动转换",
      "authors": [
        "Paul Mingzheng Tang",
        "Kenji Kah Hoe Leong",
        "Nowshad Shaik",
        "Hoong Chuin Lau"
      ],
      "abstract": "In this paper, we explore the potential application of Large Language Models\n(LLMs) that will automatically model constraints and generate code for dynamic\nscheduling problems given an existing static model. Static scheduling problems\nare modelled and coded by optimization experts. These models may be easily\nobsoleted as the underlying constraints may need to be fine-tuned in order to\nreflect changes in the scheduling rules. Furthermore, it may be necessary to\nturn a static model into a dynamic one in order to cope with disturbances in\nthe environment. In this paper, we propose a Retrieval-Augmented Generation\n(RAG) based LLM model to automate the process of implementing constraints for\nDynamic Scheduling (RAGDyS), without seeking help from an optimization modeling\nexpert. Our framework aims to minimize technical complexities related to\nmathematical modelling and computational workload for end-users, thereby\nallowing end-users to quickly obtain a new schedule close to the original\nschedule with changes reflected by natural language constraint descriptions.",
      "tldr_zh": "本文探讨了使用 Large Language Models (LLMs) 通过自然语言自动将静态调度模型转换为动态调度模型，以适应约束变化和环境干扰。作者提出了一种基于 Retrieval-Augmented Generation (RAG) 的框架 RAGDyS，能够自动化建模约束并生成代码，而无需优化专家介入。该框架显著降低了数学建模和计算工作的技术复杂性，使最终用户能通过自然语言描述快速获得接近原有的更新调度方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages (excluding appendix), 10 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.06697v1",
      "published_date": "2024-05-08 04:07:38 UTC",
      "updated_date": "2024-05-08 04:07:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:10:04.969085"
    },
    {
      "arxiv_id": "2405.04793v2",
      "title": "Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP Model Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Amrita Bhattacharjee",
        "Raha Moraffah",
        "Joshua Garland",
        "Huan Liu"
      ],
      "abstract": "With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型 (LLMs) 进行零-shot 反事实生成（Counterfactual Generation），以评估和解释黑箱 NLP 模型。作者提出一个结构化的管道，依赖 LLMs 的指令遵循和文本理解能力，在无需任何训练或微调的情况下生成高质量的反事实例子，从而解决传统方法的数据密集型问题。通过在多种专有和开源 LLMs 以及各种下游 NLP 任务上的全面实验，研究证明了这种零-shot 方法的有效性，能够有效 stress-test NLP 模型并提升其可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Longer version of short paper accepted at IEEE BigData 2024 (Main\n  Track)",
      "pdf_url": "http://arxiv.org/pdf/2405.04793v2",
      "published_date": "2024-05-08 03:57:45 UTC",
      "updated_date": "2024-11-19 10:59:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:10:16.799713"
    },
    {
      "arxiv_id": "2405.05984v1",
      "title": "Few-Shot Class Incremental Learning via Robust Transformer Approach",
      "title_zh": "基于鲁棒 Transformer 方法的少样本类增量学习",
      "authors": [
        "Naeem Paeedeh",
        "Mahardhika Pratama",
        "Sunu Wibirama",
        "Wolfgang Mayer",
        "Zehong Cao",
        "Ryszard Kowalczyk"
      ],
      "abstract": "Few-Shot Class-Incremental Learning presents an extension of the Class\nIncremental Learning problem where a model is faced with the problem of data\nscarcity while addressing the catastrophic forgetting problem. This problem\nremains an open problem because all recent works are built upon the\nconvolutional neural networks performing sub-optimally compared to the\ntransformer approaches. Our paper presents Robust Transformer Approach built\nupon the Compact Convolution Transformer. The issue of overfitting due to few\nsamples is overcome with the notion of the stochastic classifier, where the\nclassifier's weights are sampled from a distribution with mean and variance\nvectors, thus increasing the likelihood of correct classifications, and the\nbatch-norm layer to stabilize the training process. The issue of CF is dealt\nwith the idea of delta parameters, small task-specific trainable parameters\nwhile keeping the backbone networks frozen. A non-parametric approach is\ndeveloped to infer the delta parameters for the model's predictions. The\nprototype rectification approach is applied to avoid biased prototype\ncalculations due to the issue of data scarcity. The advantage of ROBUSTA is\ndemonstrated through a series of experiments in the benchmark problems where it\nis capable of outperforming prior arts with big margins without any data\naugmentation protocols.",
      "tldr_zh": "该论文针对Few-Shot Class-Incremental Learning（FS-CIL）问题，提出Robust Transformer Approach（ROBUSTA）框架，基于Compact Convolution Transformer，以解决数据稀缺导致的过拟合和灾难性遗忘（CF）挑战。方法包括使用stochastic classifier采样权重并结合batch-norm层稳定训练，以及引入delta parameters作为小任务特定参数来保持骨干网络冻结，同时开发非参数方法推断这些参数，并应用prototype rectification避免数据稀少引起的偏置。实验结果显示，ROBUSTA在基准问题上显著优于现有方法，且无需数据增强协议，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review in Information Sciences",
      "pdf_url": "http://arxiv.org/pdf/2405.05984v1",
      "published_date": "2024-05-08 03:35:52 UTC",
      "updated_date": "2024-05-08 03:35:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:10:28.095479"
    },
    {
      "arxiv_id": "2405.06696v1",
      "title": "Multi-level Shared Knowledge Guided Learning for Knowledge Graph Completion",
      "title_zh": "多级共享知识引导学习用于知识图谱补全",
      "authors": [
        "Yongxue Shan",
        "Jie Zhou",
        "Jie Peng",
        "Xin Zhou",
        "Jiaqian Yin",
        "Xiaodong Wang"
      ],
      "abstract": "In the task of Knowledge Graph Completion (KGC), the existing datasets and\ntheir inherent subtasks carry a wealth of shared knowledge that can be utilized\nto enhance the representation of knowledge triplets and overall performance.\nHowever, no current studies specifically address the shared knowledge within\nKGC. To bridge this gap, we introduce a multi-level Shared Knowledge Guided\nlearning method (SKG) that operates at both the dataset and task levels. On the\ndataset level, SKG-KGC broadens the original dataset by identifying shared\nfeatures within entity sets via text summarization. On the task level, for the\nthree typical KGC subtasks - head entity prediction, relation prediction, and\ntail entity prediction - we present an innovative multi-task learning\narchitecture with dynamically adjusted loss weights. This approach allows the\nmodel to focus on more challenging and underperforming tasks, effectively\nmitigating the imbalance of knowledge sharing among subtasks. Experimental\nresults demonstrate that SKG-KGC outperforms existing text-based methods\nsignificantly on three well-known datasets, with the most notable improvement\non WN18RR.",
      "tldr_zh": "该论文提出了一种多层共享知识引导学习方法（SKG），旨在通过利用知识图谱补全（KGC）任务中的共享知识来提升模型性能。在数据集层面，SKG 通过文本总结识别实体集中的共享特征，从而扩展原始数据集；在任务层面，它采用创新的多任务学习架构，动态调整损失权重，以平衡头实体预测、关系预测和尾实体预测等子任务的知识共享。实验结果显示，SKG 在 WN18RR 等三个知名数据集上显著优于现有文本-based 方法，尤其在 WN18RR 上取得了最显著的改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The paper has been accepted for publication at TACL. And the arXiv\n  version is a pre-MIT Press publication version",
      "pdf_url": "http://arxiv.org/pdf/2405.06696v1",
      "published_date": "2024-05-08 03:27:46 UTC",
      "updated_date": "2024-05-08 03:27:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:10:39.133857"
    },
    {
      "arxiv_id": "2405.05983v1",
      "title": "Real-Time Pill Identification for the Visually Impaired Using Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Dang",
        "Wenchao Zhao",
        "Yufeng Li",
        "Danqing Ma",
        "Qixuan Yu",
        "Elly Yijun Zhu"
      ],
      "abstract": "The prevalence of mobile technology offers unique opportunities for\naddressing healthcare challenges, especially for individuals with visual\nimpairments. This paper explores the development and implementation of a deep\nlearning-based mobile application designed to assist blind and visually\nimpaired individuals in real-time pill identification. Utilizing the YOLO\nframework, the application aims to accurately recognize and differentiate\nbetween various pill types through real-time image processing on mobile\ndevices. The system incorporates Text-to- Speech (TTS) to provide immediate\nauditory feedback, enhancing usability and independence for visually impaired\nusers. Our study evaluates the application's effectiveness in terms of\ndetection accuracy and user experience, highlighting its potential to improve\nmedication management and safety among the visually impaired community.\nKeywords-Deep Learning; YOLO Framework; Mobile Application; Visual Impairment;\nPill Identification; Healthcare",
      "tldr_zh": "这篇论文开发了一个基于 Deep Learning 的移动应用，旨在帮助视力受损者实时识别药物，从而解决他们的药物管理挑战。应用采用 YOLO Framework 进行实时图像处理，以准确区分不同类型的药物，并整合 Text-to-Speech (TTS) 功能提供即时听觉反馈。研究评估了系统的检测准确性和用户体验，结果表明该应用能显著提升视力受损者的用药安全和独立性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.05983v1",
      "published_date": "2024-05-08 03:18:46 UTC",
      "updated_date": "2024-05-08 03:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:10:52.025004"
    },
    {
      "arxiv_id": "2405.06695v1",
      "title": "Utilizing Large Language Models to Generate Synthetic Data to Increase the Performance of BERT-Based Neural Networks",
      "title_zh": "利用大型语言模型生成合成数据以提高基于BERT",
      "authors": [
        "Chancellor R. Woolsey",
        "Prakash Bisht",
        "Joshua Rothman",
        "Gondy Leroy"
      ],
      "abstract": "An important issue impacting healthcare is a lack of available experts.\nMachine learning (ML) models could resolve this by aiding in diagnosing\npatients. However, creating datasets large enough to train these models is\nexpensive. We evaluated large language models (LLMs) for data creation. Using\nAutism Spectrum Disorders (ASD), we prompted ChatGPT and GPT-Premium to\ngenerate 4,200 synthetic observations to augment existing medical data. Our\ngoal is to label behaviors corresponding to autism criteria and improve model\naccuracy with synthetic training data. We used a BERT classifier pre-trained on\nbiomedical literature to assess differences in performance between models. A\nrandom sample (N=140) from the LLM-generated data was evaluated by a clinician\nand found to contain 83% correct example-label pairs. Augmenting data increased\nrecall by 13% but decreased precision by 16%, correlating with higher quality\nand lower accuracy across pairs. Future work will analyze how different\nsynthetic data traits affect ML outcomes.",
      "tldr_zh": "这篇论文探讨了利用 Large Language Models (LLMs) 如 ChatGPT 和 GPT-Premium 生成合成数据，以增强医疗数据集并提升 BERT 基神经网络的性能，针对问题如医疗专家短缺和数据不足。研究者针对 Autism Spectrum Disorders (ASD) 生成了4200个合成观察数据，并通过临床医生评估发现83%的例子-标签对正确。结果显示，添加合成数据后，模型的 recall 提高了13%，但 precision 下降了16%，这突显了合成数据质量对模型准确性的影响，并为未来分析合成数据特性对 ML 结果的影响提供了方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in 2024 American Medical Informatics Association (AMIA)\n  Summit March 18-21",
      "pdf_url": "http://arxiv.org/pdf/2405.06695v1",
      "published_date": "2024-05-08 03:18:12 UTC",
      "updated_date": "2024-05-08 03:18:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:11:05.374800"
    },
    {
      "arxiv_id": "2405.04776v3",
      "title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Kaya Stechly",
        "Karthik Valmeekam",
        "Subbarao Kambhampati"
      ],
      "abstract": "Large language model (LLM) performance on reasoning problems typically does\nnot generalize out of distribution. Previous work has claimed that this can be\nmitigated with chain of thought prompting-a method of demonstrating solution\nprocedures-with the intuition that it is possible to in-context teach an LLM an\nalgorithm for solving the problem. This paper presents a case study of chain of\nthought on problems from Blocksworld, a classical planning domain, and examines\nthe performance of two state-of-the-art LLMs across two axes: generality of\nexamples given in prompt, and complexity of problems queried with each prompt.\nWhile our problems are very simple, we only find meaningful performance\nimprovements from chain of thought prompts when those prompts are exceedingly\nspecific to their problem class, and that those improvements quickly\ndeteriorate as the size n of the query-specified stack grows past the size of\nstacks shown in the examples. We also create scalable variants of three domains\ncommonly studied in previous CoT papers and demonstrate the existence of\nsimilar failure modes. Our results hint that, contrary to previous claims in\nthe literature, CoT's performance improvements do not stem from the model\nlearning general algorithmic procedures via demonstrations but depend on\ncarefully engineering highly problem specific prompts. This spotlights\ndrawbacks of chain of thought, especially the sharp tradeoff between possible\nperformance gains and the amount of human labor necessary to generate examples\nwith correct reasoning traces.",
      "tldr_zh": "本文分析了 Chain of Thought (CoT) 提示在大语言模型 (LLM) 规划任务中的性能，特别针对 Blocksworld 等经典规划领域，测试了提示示例的泛化性和问题复杂度的影响。研究发现，CoT 仅在高度特定于问题类别的提示下才能带来显著性能提升，但当查询问题复杂度（如栈大小）超过示例时，效果迅速恶化。总体而言，这些结果表明 CoT 的改善并非源于模型学习通用算法，而是依赖于精心设计的特定提示，这暴露了 CoT 在实际应用中的局限性，包括性能收益与人类劳动成本的权衡。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04776v3",
      "published_date": "2024-05-08 02:48:28 UTC",
      "updated_date": "2025-03-12 04:56:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:11:17.484738"
    },
    {
      "arxiv_id": "2405.04773v2",
      "title": "Hypergraph-enhanced Dual Semi-supervised Graph Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Ju",
        "Zhengyang Mao",
        "Siyu Yi",
        "Yifang Qin",
        "Yiyang Gu",
        "Zhiping Xiao",
        "Yifan Wang",
        "Xiao Luo",
        "Ming Zhang"
      ],
      "abstract": "In this paper, we study semi-supervised graph classification, which aims at\naccurately predicting the categories of graphs in scenarios with limited\nlabeled graphs and abundant unlabeled graphs. Despite the promising capability\nof graph neural networks (GNNs), they typically require a large number of\ncostly labeled graphs, while a wealth of unlabeled graphs fail to be\neffectively utilized. Moreover, GNNs are inherently limited to encoding local\nneighborhood information using message-passing mechanisms, thus lacking the\nability to model higher-order dependencies among nodes. To tackle these\nchallenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for\nsemi-supervised graph classification, which captures graph semantics from the\nperspective of the hypergraph and the line graph, respectively. Specifically,\nto better explore the higher-order relationships among nodes, we design a\nhypergraph structure learning to adaptively learn complex node dependencies\nbeyond pairwise relations. Meanwhile, based on the learned hypergraph, we\nintroduce a line graph to capture the interaction between hyperedges, thereby\nbetter mining the underlying semantic structures. Finally, we develop a\nrelational consistency learning to facilitate knowledge transfer between the\ntwo branches and provide better mutual guidance. Extensive experiments on\nreal-world graph datasets verify the effectiveness of the proposed method\nagainst existing state-of-the-art methods.",
      "tldr_zh": "本研究针对半监督图分类问题，旨在在有限标记图和大量未标记图的情况下准确预测图类别，解决现有 GNNs 依赖大量标记数据且无法有效建模高阶节点依赖的挑战。提出 HEAL 框架，通过超图结构学习自适应捕获节点间复杂的高阶关系，并引入线图来挖掘超边间的交互，从而从超图和线图两个视角增强图语义表示。同时，框架开发了关系一致性学习模块，促进两个分支间的知识转移和相互指导。在真实世界图数据集上的广泛实验验证，HEAL 优于现有最先进方法，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Proceedings of the 41st International Conference on\n  Machine Learning (ICML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2405.04773v2",
      "published_date": "2024-05-08 02:44:13 UTC",
      "updated_date": "2024-05-28 09:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:11:28.134482"
    },
    {
      "arxiv_id": "2405.04767v1",
      "title": "Test-Time Augmentation for Traveling Salesperson Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Ryo Ishiyama",
        "Takahiro Shirakawa",
        "Seiichi Uchida",
        "Shinnosuke Matsuo"
      ],
      "abstract": "We propose Test-Time Augmentation (TTA) as an effective technique for\naddressing combinatorial optimization problems, including the Traveling\nSalesperson Problem. In general, deep learning models possessing the property\nof invariance, where the output is uniquely determined regardless of the node\nindices, have been proposed to learn graph structures efficiently. In contrast,\nwe interpret the permutation of node indices, which exchanges the elements of\nthe distance matrix, as a TTA scheme. The results demonstrate that our method\nis capable of obtaining shorter solutions than the latest models. Furthermore,\nwe show that the probability of finding a solution closer to an exact solution\nincreases depending on the augmentation size.",
      "tldr_zh": "本研究提出 Test-Time Augmentation (TTA) 作为一种有效技术，用于解决组合优化问题，特别是 Traveling Salesperson Problem (TSP)。该方法通过对节点索引进行排列，交换距离矩阵的元素，利用深度学习模型的不变性属性来增强模型性能。实验结果表明，TTA 能获得比最新模型更短的解决方案，并且随着增强规模的增加，找到更接近精确解的概率显著提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04767v1",
      "published_date": "2024-05-08 02:31:51 UTC",
      "updated_date": "2024-05-08 02:31:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:11:39.355294"
    },
    {
      "arxiv_id": "2405.04765v1",
      "title": "When Foresight Pruning Meets Zeroth-Order Optimization: Efficient Federated Learning for Low-Memory Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Pengyu Zhang",
        "Yingjie Liu",
        "Yingbo Zhou",
        "Xiao Du",
        "Xian Wei",
        "Ting Wang",
        "Mingsong Chen"
      ],
      "abstract": "Although Federated Learning (FL) enables collaborative learning in Artificial\nIntelligence of Things (AIoT) design, it fails to work on low-memory AIoT\ndevices due to its heavy memory usage. To address this problem, various\nfederated pruning methods are proposed to reduce memory usage during inference.\nHowever, few of them can substantially mitigate the memory burdens during\npruning and training. As an alternative, zeroth-order or backpropagation-free\n(BP-Free) methods can partially alleviate the memory consumption, but they\nsuffer from scaling up and large computation overheads, since the gradient\nestimation error and floating point operations (FLOPs) increase as the\ndimensionality of the model parameters grows. In this paper, we propose a\nfederated foresight pruning method based on Neural Tangent Kernel (NTK), which\ncan seamlessly integrate with federated BP-Free training frameworks. We present\nan approximation to the computation of federated NTK by using the local NTK\nmatrices. Moreover, we demonstrate that the data-free property of our method\ncan substantially reduce the approximation error in extreme data heterogeneity\nscenarios. Since our approach improves the performance of the vanilla BP-Free\nmethod with fewer FLOPs and truly alleviates memory pressure during training\nand inference, it makes FL more friendly to low-memory devices. Comprehensive\nexperimental results obtained from simulation- and real test-bed-based\nplatforms show that our federated foresight-pruning method not only preserves\nthe ability of the dense model with a memory reduction up to 9x but also boosts\nthe performance of the vanilla BP-Free method with dramatically fewer FLOPs.",
      "tldr_zh": "这篇论文针对联邦学习（Federated Learning）在低内存设备上的高内存消耗问题，提出了一种基于 Neural Tangent Kernel (NTK) 的联邦预见剪枝方法，并与 Zeroth-Order Optimization (BP-Free) 训练框架无缝集成。该方法通过使用局部 NTK 矩阵近似计算联邦 NTK，显著减少梯度估计误差和浮点运算（FLOPs），并在极端数据异质性场景中降低近似误差。实验结果显示，该方法可将内存使用减少高达 9 倍，同时保持密集模型的性能，并大幅提升 BP-Free 方法的效率，使联邦学习更适用于低内存 AIoT 设备。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04765v1",
      "published_date": "2024-05-08 02:24:09 UTC",
      "updated_date": "2024-05-08 02:24:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:11:53.787309"
    },
    {
      "arxiv_id": "2405.04760v4",
      "title": "Large Language Models for Cyber Security: A Systematic Literature Review",
      "title_zh": "翻译失败",
      "authors": [
        "Hanxiang Xu",
        "Shenao Wang",
        "Ningke Li",
        "Kailong Wang",
        "Yanjie Zhao",
        "Kai Chen",
        "Ting Yu",
        "Yang Liu",
        "Haoyu Wang"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.",
      "tldr_zh": "这篇论文对大型语言模型 (LLMs) 在网络安全 (cybersecurity) 领域的应用进行了系统文献综述，通过收集超过 30K 篇相关论文并分析 127 篇顶级期刊论文，提供了一个全面概述。研究发现，LLMs 被广泛应用于漏洞检测 (vulnerability detection)、恶意软件分析 (malware analysis)、网络入侵检测 (network intrusion detection) 和网络钓鱼检测 (phishing detection) 等任务，但训练数据集在规模和多样性上存在局限。论文还探讨了适应 LLMs 的关键技术，如 fine-tuning、transfer learning 和 domain-specific pre-training，并指出未来挑战包括提升模型的可解释性 (interpretable and explainable models)、解决数据隐私问题，以及利用 LLMs 进行主动防御和威胁狩猎。总的来说，这为 LLM4Security 研究指出了有前景的方向。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "56 pages,6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.04760v4",
      "published_date": "2024-05-08 02:09:17 UTC",
      "updated_date": "2025-05-15 07:33:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:12:06.804728"
    },
    {
      "arxiv_id": "2405.04758v2",
      "title": "Honeyfile Camouflage: Hiding Fake Files in Plain Sight",
      "title_zh": "翻译失败",
      "authors": [
        "Roelien C. Timmer",
        "David Liebowitz",
        "Surya Nepal",
        "Salil S. Kanhere"
      ],
      "abstract": "Honeyfiles are a particularly useful type of honeypot: fake files deployed to\ndetect and infer information from malicious behaviour. This paper considers the\nchallenge of naming honeyfiles so they are camouflaged when placed amongst real\nfiles in a file system. Based on cosine distances in semantic vector spaces, we\ndevelop two metrics for filename camouflage: one based on simple averaging and\none on clustering with mixture fitting. We evaluate and compare the metrics,\nshowing that both perform well on a publicly available GitHub software\nrepository dataset.",
      "tldr_zh": "这篇论文探讨了如何为 honeyfiles（诱饵文件）命名，使其 camouflaged 于真实文件系统中，以检测和分析恶意行为。作者基于语义向量空间中的 cosine distances 开发了两种 metrics：一种采用 simple averaging，另一种使用 clustering with mixture fitting。通过在公开的 GitHub 软件仓库数据集上评估，结果显示两种 metrics 均表现出色。总的来说，该方法提升了 honeyfiles 的隐蔽性，为改进蜜罐技术提供了新途径。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "3rd Workshop on the security implications of Deepfakes and Cheapfakes\n  (WDC) co-located at ACM ASIACCS 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04758v2",
      "published_date": "2024-05-08 02:01:17 UTC",
      "updated_date": "2024-05-10 05:12:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:12:16.191054"
    },
    {
      "arxiv_id": "2405.04753v1",
      "title": "AttacKG+:Boosting Attack Knowledge Graph Construction with Large Language Models",
      "title_zh": "AttacKG+：利用大语言模型提升攻击知识",
      "authors": [
        "Yongheng Zhang",
        "Tingwen Du",
        "Yunshan Ma",
        "Xiang Wang",
        "Yi Xie",
        "Guozheng Yang",
        "Yuliang Lu",
        "Ee-Chien Chang"
      ],
      "abstract": "Attack knowledge graph construction seeks to convert textual cyber threat\nintelligence (CTI) reports into structured representations, portraying the\nevolutionary traces of cyber attacks. Even though previous research has\nproposed various methods to construct attack knowledge graphs, they generally\nsuffer from limited generalization capability to diverse knowledge types as\nwell as requirement of expertise in model design and tuning. Addressing these\nlimitations, we seek to utilize Large Language Models (LLMs), which have\nachieved enormous success in a broad range of tasks given exceptional\ncapabilities in both language understanding and zero-shot task fulfillment.\nThus, we propose a fully automatic LLM-based framework to construct attack\nknowledge graphs named: AttacKG+. Our framework consists of four consecutive\nmodules: rewriter, parser, identifier, and summarizer, each of which is\nimplemented by instruction prompting and in-context learning empowered by LLMs.\nFurthermore, we upgrade the existing attack knowledge schema and propose a\ncomprehensive version. We represent a cyber attack as a temporally unfolding\nevent, each temporal step of which encapsulates three layers of representation,\nincluding behavior graph, MITRE TTP labels, and state summary. Extensive\nevaluation demonstrates that: 1) our formulation seamlessly satisfies the\ninformation needs in threat event analysis, 2) our construction framework is\neffective in faithfully and accurately extracting the information defined by\nAttacKG+, and 3) our attack graph directly benefits downstream security\npractices such as attack reconstruction. All the code and datasets will be\nreleased upon acceptance.",
      "tldr_zh": "这篇论文提出AttacKG+框架，利用Large Language Models (LLMs)自动构建攻击知识图，以将文本形式的网络威胁情报（CTI）报告转化为结构化表示，解决现有方法泛化能力不足和依赖专家调优的问题。框架由四个模块组成：rewriter、parser、identifier和summarizer，通过指令提示和in-context learning实现信息提取和处理。论文升级了攻击知识模式，将网络攻击表示为时间展开的事件，包括行为图、MITRE TTP标签和状态总结。实验结果显示，该框架能准确提取信息，支持威胁事件分析和下游安全实践如攻击重建，并计划发布代码和数据集。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.04753v1",
      "published_date": "2024-05-08 01:41:25 UTC",
      "updated_date": "2024-05-08 01:41:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:12:29.604905"
    },
    {
      "arxiv_id": "2405.04746v1",
      "title": "SVD-AE: Simple Autoencoders for Collaborative Filtering",
      "title_zh": "翻译失败",
      "authors": [
        "Seoyoung Hong",
        "Jeongwhan Choi",
        "Yeon-Chang Lee",
        "Srijan Kumar",
        "Noseong Park"
      ],
      "abstract": "Collaborative filtering (CF) methods for recommendation systems have been\nextensively researched, ranging from matrix factorization and autoencoder-based\nto graph filtering-based methods. Recently, lightweight methods that require\nalmost no training have been recently proposed to reduce overall computation.\nHowever, existing methods still have room to improve the trade-offs among\naccuracy, efficiency, and robustness. In particular, there are no well-designed\nclosed-form studies for \\emph{balanced} CF in terms of the aforementioned\ntrade-offs. In this paper, we design SVD-AE, a simple yet effective singular\nvector decomposition (SVD)-based linear autoencoder, whose closed-form solution\ncan be defined based on SVD for CF. SVD-AE does not require iterative training\nprocesses as its closed-form solution can be calculated at once. Furthermore,\ngiven the noisy nature of the rating matrix, we explore the robustness against\nsuch noisy interactions of existing CF methods and our SVD-AE. As a result, we\ndemonstrate that our simple design choice based on truncated SVD can be used to\nstrengthen the noise robustness of the recommendation while improving\nefficiency. Code is available at https://github.com/seoyoungh/svd-ae.",
      "tldr_zh": "该论文提出了一种简单有效的协同过滤（CF）方法，名为 SVD-AE，它基于奇异值分解（SVD）的线性自编码器，能够通过闭式解实现无需迭代训练的过程，从而提升推荐系统的效率。SVD-AE 针对准确性、效率和鲁棒性之间的权衡进行了优化，特别是通过截断 SVD 来增强对噪声交互的鲁棒性。实验结果显示，该方法在处理噪声数据时表现出色，并改善了整体推荐性能，代码已在 GitHub 上开源。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.04746v1",
      "published_date": "2024-05-08 01:22:47 UTC",
      "updated_date": "2024-05-08 01:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:12:39.770760"
    },
    {
      "arxiv_id": "2405.04732v3",
      "title": "Is the House Ready For Sleeptime? Generating and Evaluating Situational Queries for Embodied Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Vishnu Sashank Dorbala",
        "Prasoon Goyal",
        "Robinson Piramuthu",
        "Michael Johnston",
        "Reza Ghanadhan",
        "Dinesh Manocha"
      ],
      "abstract": "We present and tackle the problem of Embodied Question Answering (EQA) with\nSituational Queries (S-EQA) in a household environment. Unlike prior EQA work\ntackling simple queries that directly reference target objects and properties\n(\"What is the color of the car?\"), situational queries (such as \"Is the house\nready for sleeptime?\") are challenging as they require the agent to correctly\nidentify multiple object-states (Doors: Closed, Lights: Off, etc.) and reach a\nconsensus on their states for an answer. Towards this objective, we first\nintroduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an\nLLM's output to generate unique situational queries and corresponding consensus\nobject information. PGE is used to generate 2K datapoints in the VirtualHome\nsimulator, which is then annotated for ground truth answers via a large scale\nuser-study conducted on M-Turk. With a high rate of answerability (97.26%) on\nthis study, we establish that LLMs are good at generating situational data.\nHowever, in evaluating the data using an LLM, we observe a low correlation of\n46.2% with the ground truth human annotations; indicating that while LLMs are\ngood at generating situational data, they struggle to answer them according to\nconsensus. When asked for reasoning, we observe the LLM often goes against\ncommonsense in justifying its answer. Finally, we utilize PGE to generate\nsituational data in a real-world environment, exposing LLM hallucination in\ngenerating reliable object-states when a structured scene graph is unavailable.\nTo the best of our knowledge, this is the first work to introduce EQA in the\ncontext of situational queries and also the first to present a generative\napproach for query creation. We aim to foster research on improving the\nreal-world usability of embodied agents through this work.",
      "tldr_zh": "本研究引入了情境查询（Situational Queries, S-EQA）在家庭环境下的 Embodied Question Answering (EQA) 问题，例如“房子准备好睡觉了吗？”，这要求代理识别多个对象状态（如门关、灯灭）并得出共识。研究提出 Prompt-Generate-Evaluate (PGE) 方案，利用 LLM 生成 2K 个独特的情境查询和对象信息数据集，并在 VirtualHome 模拟器中进行标注，通过 M-Turk 用户研究验证其答案可靠性。实验结果显示，LLM 在数据生成方面表现出色（97.26% 的 answerability），但在评估时与人类标注的相关性仅为 46.2%，并暴露了 LLM 在推理中违背常识和在真实环境中产生 hallucination 的问题。该工作是首个针对情境查询的 EQA 研究，并通过生成式方法推动 embodied agents 的实际可用性改进。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 Pages",
      "pdf_url": "http://arxiv.org/pdf/2405.04732v3",
      "published_date": "2024-05-08 00:45:20 UTC",
      "updated_date": "2025-03-10 21:12:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T07:12:54.386867"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 91,
  "processed_papers_count": 91,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T07:13:19.451018"
}
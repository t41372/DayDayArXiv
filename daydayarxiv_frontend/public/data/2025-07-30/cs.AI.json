{
  "date": "2025-07-30",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-30 çš„ **arXiv ä¸­æ–‡ TLDR å¿«æŠ¥**ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„è€æœ‹å‹ï¼Œè™½ç„¶ä»Šå¤© arXiv æ›´æ–°äº†è¶³è¶³ 111 ç¯‡è®ºæ–‡ï¼Œä½†æˆ‘å·²ç»ä¸ºå„ä½ç­›é€‰å‡ºäº†æœ€å€¼å¾—å…³æ³¨çš„ç²¾åã€‚\n\n**ä¸€å¥è¯æ€»ç»“ä»Šå¤©ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†â€œåæ€â€ä¸â€œè½åœ°â€çš„å‘³é“ã€‚å­¦æœ¯ç•Œå¼€å§‹çŒ›çƒˆæŠ¨å‡»ç”¨äººç±»æµ‹è¯•è¯„ä¼° AI çš„åˆç†æ€§ï¼Œå¹¶æ­éœ²äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨å¬è§‰ä»»åŠ¡ä¸Šçš„ç¾éš¾æ€§è¡¨ç°ï¼ˆMoravec æ‚–è®ºï¼‰ï¼›å¦ä¸€æ–¹é¢ï¼Œä»å¾®ä¿¡çš„ RLHF æ¡†æ¶åˆ°è‡ªåŠ¨æ ¹æ®è®ºæ–‡å†™ä»£ç ï¼ŒAI åœ¨å·¥ç¨‹åŒ–è½åœ°ä¸Šè¶Šèµ°è¶Šæ·±ã€‚\n\n---\n\n### ğŸ”¥ å¿…è¯»ï¼šAI çš„â€œå¿ƒç†å­¦â€ä¸è¯„ä¼°å±æœº\n\n**1. [è§‚ç‚¹] åˆ«å†ç”¨äººç±»æµ‹è¯•è€ƒ AI äº†ï¼Œå¾—å¼€å‘ AI ä¸“ç”¨æµ‹è¯•**\n**Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead**\n*   **æ ¸å¿ƒè§‚ç‚¹ï¼š** ä½œè€…æå‡ºäº†ä¸€ä¸ªå°–é”çš„è§‚ç‚¹ï¼šæŠŠåŸæœ¬ç”¨æ¥è¯„ä¼°äººç±»è®¤çŸ¥å’Œå¿ƒç†ç‰¹å¾ï¼ˆå¦‚æ™ºåŠ›ã€äººæ ¼ï¼‰çš„æµ‹è¯•ç›´æ¥ç”¨åœ¨ LLM ä¸Šæ˜¯æœ¬ä½“è®ºé”™è¯¯ã€‚\n*   **ä¸»è¦å‘ç°ï¼š** è¿™ç§åšæ³•ç¼ºä¹ç†è®ºä¾æ®ï¼Œä¸”å—æ•°æ®æ±¡æŸ“ã€æ–‡åŒ–åè§å’Œ prompt æ•æ„Ÿæ€§å½±å“å·¨å¤§ã€‚ä½œè€…å‘¼ååœæ­¢è¿™ç§æ‹ŸäººåŒ–çš„è¯„ä¼°ï¼Œè½¬è€Œå¼€å‘åŸºäºåŸåˆ™çš„ AI ä¸“ç”¨æµ‹è¯•æ¡†æ¶ã€‚\n\n**2. [å¬è§‰] Moravec æ‚–è®ºå†ç°ï¼šAI å¬åŠ›æƒ¨è´¥**\n**Moravec's Paradox: Towards an Auditory Turing Test**\n*   **æ ¸å¿ƒå‘ç°ï¼š** è¿™æ˜¯ä¸€ç¯‡ä»¤äººéœ‡æƒŠçš„è¯„æµ‹ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªâ€œå¬è§‰å›¾çµæµ‹è¯•â€ï¼ŒåŒ…å«é‡å è¯­éŸ³ã€å™ªéŸ³ã€æ„ŸçŸ¥é”™è§‰ç­‰ 917 ä¸ªæŒ‘æˆ˜ã€‚\n*   **ç»“æœï¼š** å³ä¾¿æ˜¯ GPT-4 å’Œ Whisper è¿™æ ·çš„é¡¶æµæ¨¡å‹ï¼Œ**å¤±è´¥ç‡ä¹Ÿè¶…è¿‡ 93%**ã€‚åœ¨äººç±»èƒ½è½»æ¾æå®šï¼ˆæˆåŠŸç‡ 52%ï¼‰çš„ä»»åŠ¡ä¸Šï¼Œæœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º 6.9%ã€‚è¿™æ­ç¤ºäº†å½“å‰ AI åœ¨å¬è§‰åœºæ™¯åˆ†æã€é€‰æ‹©æ€§æ³¨æ„æœºåˆ¶ä¸Šçš„å·¨å¤§ç¼ºé™·ã€‚\n\n**3. [æ–‡åŒ–] AI ç”Ÿæˆçš„æ•…äº‹æ­£åœ¨æŠ¹æ€æ–‡åŒ–å¤šæ ·æ€§**\n**AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini**\n*   **æ ¸å¿ƒå‘ç°ï¼š** ä½œè€…è®© GPT-4o-mini ä¸º 236 ä¸ªå›½å®¶ç”Ÿæˆæ•…äº‹ã€‚ç»“æœå‘ç°ï¼Œè™½ç„¶è¡¨é¢ä¸Šæœ‰å›½å®¶ç¬¦å·ï¼Œä½†**æƒ…èŠ‚ç»“æ„æåº¦åŒè´¨åŒ–**ï¼šå‡ ä¹éƒ½æ˜¯ä¸»è§’å›åˆ°å°é•‡ã€é€šè¿‡å›å½’ä¼ ç»Ÿè§£å†³å°å†²çªã€‚\n*   **Implicationï¼š** AI å€¾å‘äºâ€œç¨³å®šâ€è€Œéâ€œæ”¹å˜â€ï¼Œè¿™ç§å™äº‹åŒè´¨åŒ–æ˜¯ä¸€ç§æ–°å‹çš„åè§ï¼Œå¯èƒ½å¯¼è‡´å…¨çƒæ–‡åŒ–çš„åˆ»æ¿å°è±¡å›ºåŒ–ã€‚\n\n---\n\n### ğŸ§  LLM æœºç†ã€æ¨ç†ä¸ Prompt æŠ€å·§\n\n**4. [Prompt] ç¤ºä¾‹æ”¾åœ¨å“ªï¼ŸPrompt ä¸­çš„ä½ç½®åè§**\n**Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning**\n*   **æ ¸å¿ƒæŠ€å·§ï¼š** ç ”ç©¶å‘ç°äº†â€œPrompt ä¸­çš„ç¤ºä¾‹ä½ç½®åå·® (DPP)â€ã€‚\n*   **ç»“è®ºï¼š** **æŠŠç¤ºä¾‹ï¼ˆDemosï¼‰æ”¾åœ¨ Prompt çš„æœ€å‰é¢**ï¼Œèƒ½è·å¾—æœ€ç¨³å®šå’Œå‡†ç¡®çš„è¾“å‡ºï¼ˆæœ€é«˜æå‡ 6 ä¸ªç‚¹ï¼‰ã€‚ç›¸åï¼ŒæŠŠç¤ºä¾‹æ”¾åœ¨ç”¨æˆ·æŒ‡ä»¤åé¢ï¼ˆé è¿‘ç»“å°¾ï¼‰ï¼Œä¸ä»…æ²¡ç”¨ï¼Œè¿˜å¯èƒ½å¯¼è‡´é¢„æµ‹ç¿»è½¬ã€‚\n\n**5. [æ¨ç†] LLM åˆ°åº•æ˜¯ä¸æ˜¯â€œæŠ½è±¡æ¨ç†è€…â€ï¼Ÿ**\n**What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹â€œLLM æ— æ³•è¿›è¡ŒæŠ½è±¡æ¨ç†â€çš„è®ºè°ƒè¿›è¡Œäº†åé©³ã€‚\n*   **å‘ç°ï¼š** è™½ç„¶é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰è¡¨ç°å¾ˆå·®ï¼Œä½†åªè¦å¾®è°ƒæå°‘é‡çš„è¾“å…¥ç¼–ç å‚æ•°ï¼ŒLLM å°±èƒ½è¾¾åˆ°è¿‘ä¹å®Œç¾çš„è¡¨ç°ã€‚è¿™è¡¨æ˜å®ƒä»¬å…·å¤‡æ¨ç†çš„æ½œèƒ½ï¼Œä½†è¿™ç§èƒ½åŠ›åœ¨ä¸åŒæ•°æ®é›†é—´çš„è¿ç§»æ€§ä»ç„¶å­˜ç–‘ã€‚\n\n**6. [å®‰å…¨] åˆ©ç”¨è®¤çŸ¥åå·®è¶Šç‹±**\n**Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡º CognitiveAttackï¼Œåˆ©ç”¨äººç±»çš„è®¤çŸ¥åå·®ï¼ˆå¦‚ä»ä¼—æ•ˆåº”ã€æƒå¨åè§ï¼‰æ¥ç»•è¿‡ LLM çš„é˜²å¾¡ã€‚\n*   **ç»“æœï¼š** è¿™ç§åˆ©ç”¨â€œå¤šé‡åå·®äº’åŠ¨â€çš„æ–¹æ³•ï¼Œæ”»å‡»æˆåŠŸç‡é«˜è¾¾ 60.1%ï¼Œè¿œè¶…ç°æœ‰çš„é»‘ç›’æ”»å‡»æ–¹æ³•ã€‚\n\n---\n\n### ğŸ› ï¸ æ™ºèƒ½ä½“ (Agents) ä¸ ç³»ç»Ÿæ¶æ„\n\n**7. [æ¡†æ¶] å¾®ä¿¡å›¢é˜Ÿçš„ RLHF è®­ç»ƒå¤§æ€å™¨**\n**G-Core: A Simple, Scalable and Balanced RLHF Trainer**\n*   **èƒŒæ™¯ï¼š** æ¥è‡ªè…¾è®¯å¾®ä¿¡å›¢é˜Ÿï¼Œç”¨äºæ”¯æŒå¾®ä¿¡çš„å¤§è§„æ¨¡ç”¨æˆ·åŠŸèƒ½ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† G-Core æ¡†æ¶ï¼Œè§£å†³äº† RLHF è®­ç»ƒä¸­ Controller æ‰©å±•æ€§å·®å’Œèµ„æºåˆ†é…ä¸çµæ´»çš„é—®é¢˜ã€‚é€šè¿‡å¹¶è¡Œ Controller å’ŒåŠ¨æ€æ”¾ç½®ç­–ç•¥ï¼Œå¤§å¹…å‡å°‘äº†ç¡¬ä»¶ç©ºé—²æ—¶é—´ã€‚\n\n**8. [äº¤äº’] Magentic-UIï¼šäººæœºå›ç¯çš„æ™ºèƒ½ä½“ç³»ç»Ÿ**\n**Magentic-UI: Towards Human-in-the-loop Agentic Systems**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æ—¢ç„¶å…¨è‡ªåŠ¨ Agent è¿˜ä¸é è°±ï¼Œé‚£å°±è®©äººæ›´å¥½åœ°ä»‹å…¥ã€‚å¾®è½¯ç­‰æœºæ„æ¨å‡ºäº† Magentic-UIï¼Œä¸€ä¸ªæ”¯æŒç½‘é¡µæµè§ˆã€ä»£ç æ‰§è¡Œçš„å¼€æºç•Œé¢ã€‚\n*   **äº®ç‚¹ï¼š** æä¾›äº†â€œå…±åŒè§„åˆ’â€ã€â€œåŠ¨ä½œå®ˆå«ï¼ˆAction Guardsï¼‰â€ç­‰æœºåˆ¶ï¼Œè®©äººç±»èƒ½ä½æˆæœ¬åœ°ç›‘æ§å’Œä¿®æ­£ Agent çš„è¡Œä¸ºã€‚\n\n**9. [å¤šæ¨¡æ€] å¬è¯´å…¼å¤‡çš„é©¾é©¶å¤§æ¨¡å‹**\n**NovaDrive / XYZ-Drive** (ä¸¤ç¯‡ç›¸å…³è®ºæ–‡)\n*   **Goal-Based Vision-Language Driving / Vision-Language Cross-Attention for Real-Time Autonomous Driving**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†å•åˆ†æ”¯çš„è§†è§‰-è¯­è¨€é©¾é©¶æ¨¡å‹ï¼Œå°†å‰è§†æ‘„åƒå¤´ã€åœ°å›¾ã€LiDAR å’Œæ–‡æœ¬è·¯ç‚¹ï¼ˆWaypointsï¼‰èåˆã€‚\n*   **æ•ˆæœï¼š** NovaDrive åœ¨ NuScenes æ¦œå•ä¸Šå°†ç¢°æ’ç‡é™ä½äº† 1.4%ï¼Œè¯æ˜äº†æ—©æœŸ Token çº§èåˆå’Œè½»é‡çº§å¾®è°ƒåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚\n\n---\n\n### ğŸ’» è½¯ä»¶å·¥ç¨‹ä¸ä»£ç ç”Ÿæˆ\n\n**10. [ä»£ç ç”Ÿæˆ] ç›´æ¥ä»è®ºæ–‡ç”Ÿæˆç®—æ³•ä»£ç **\n**From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„å°è¯•ã€‚ä½œè€…æµ‹è¯•äº† LLM æ˜¯å¦èƒ½ç›´æ¥é˜…è¯»ç§‘å­¦è®ºæ–‡ä¸­çš„æ–¹æ³•æè¿°ï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œçš„ Python ä»£ç ã€‚\n*   **ç»“è®ºï¼š** GPT-4o ç­‰æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„ä»£ç æ€§èƒ½ä¸äººå·¥ç»´æŠ¤çš„åº“ç›¸å½“ã€‚è¿™æ„å‘³ç€æœªæ¥æˆ‘ä»¬å¯èƒ½ä¸éœ€è¦ç»´æŠ¤åºå¤§çš„ä»£ç åº“ï¼Œè€Œæ˜¯â€œæŒ‰éœ€ç”Ÿæˆâ€ã€‚\n\n**11. [ç¨‹åºä¿®å¤] ä¿®å¤å‰å…ˆæµ‹è¯•**\n**Repair-R1: Better Test Before Repair**\n*   **æ–¹æ³•ï¼š** ä¼ ç»Ÿçš„è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ˜¯å…ˆä¿®å†æµ‹ã€‚Repair-R1 æå‡º**å…ˆç”Ÿæˆèƒ½åŒºåˆ†æ•…éšœçš„æµ‹è¯•ç”¨ä¾‹**ï¼Œå†åŸºäºæµ‹è¯•è¿›è¡Œä¿®å¤ã€‚\n*   **æ•ˆæœï¼š** è¿™ç§åç›´è§‰çš„æµç¨‹åˆ©ç”¨ RL å…±åŒä¼˜åŒ–æµ‹è¯•ç”Ÿæˆå’Œ Bug ä¿®å¤ï¼Œä¿®å¤æˆåŠŸç‡æå‡äº† 2.68% è‡³ 48.29%ã€‚\n\n**12. [åŸºå‡†æµ‹è¯•] GPT-4.1 ç™»åœºï¼Ÿ**\n**GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries**\n*   **æ³¨ï¼š** æ ‡é¢˜æåˆ°äº† GPT-4.1ï¼Œè¿™å¯èƒ½æ˜¯æŒ‡ç‰¹å®šçš„æ–°ç‰ˆæœ¬æˆ–ä½œè€…çš„ç¬”è¯¯/é¢„æµ‹ã€‚\n*   **å‘ç°ï¼š** åœ¨ä½¿ç”¨ä¸ç†Ÿæ‚‰çš„ Python åº“è¿›è¡Œå®éªŒè®¾è®¡æ—¶ï¼Œåªæœ‰è¯¥æ¨¡å‹å®ç°äº† 100% çš„æˆåŠŸç‡ï¼Œè¿œè¶…å…¶ä»–æ¨¡å‹ã€‚\n\n---\n\n### ğŸ”¬ ç§‘å­¦ AI (AI for Science) & å…¶ä»–\n\n**13. [é¥æ„Ÿ] FuseTenï¼šç”Ÿæˆ 10ç±³çº§åœ°è¡¨æ¸©åº¦**\n**FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation**\n*   **è´¡çŒ®ï¼š** èåˆ Sentinel-2, Landsat 8 å’Œ MODIS æ•°æ®ï¼Œåˆ©ç”¨ç”Ÿæˆå¼æ¶æ„ç”Ÿæˆæ¯æ—¥ã€10ç±³é«˜åˆ†è¾¨ç‡çš„åœ°è¡¨æ¸©åº¦æ•°æ®ï¼Œè§£å†³äº†å«æ˜Ÿæ•°æ®æ—¶ç©ºåˆ†è¾¨ç‡çš„æƒè¡¡ç—›ç‚¹ã€‚\n\n**14. [ææ–™] aLLoyMï¼šåˆé‡‘ç›¸å›¾é¢„æµ‹å¤§æ¨¡å‹**\n**aLLoyM: A large language model for alloy phase diagram prediction**\n*   **è´¡çŒ®ï¼š** ä¸“é—¨å¾®è°ƒçš„ Mistral æ¨¡å‹ï¼Œç”¨äºå¤„ç†åˆé‡‘æˆåˆ†ã€æ¸©åº¦å’Œç›¸ä¿¡æ¯ï¼Œç”šè‡³èƒ½ä»…æ ¹æ®æˆåˆ†ç”Ÿæˆæ–°çš„ç›¸å›¾ï¼ŒåŠ é€Ÿæ–°ææ–™å‘ç°ã€‚\n\n**15. [åŒ»ç–—] æ¢å¤ä½è´¨é‡è¶…å£°æ³¢çš„è¯Šæ–­ä»·å€¼**\n**Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification**\n*   **è´¡çŒ®ï¼š** è¯æ˜äº†åˆ©ç”¨è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯å¯ä»¥æ˜¾è‘—æå‡ä½è´¨é‡å¿ƒè„è¶…å£°å›¾åƒçš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¯¹åŒ»ç–—èµ„æºåŒ®ä¹åœ°åŒºæ„ä¹‰é‡å¤§ã€‚\n\n---\n\n**æ•™æˆçš„ç»“æŸè¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡å†æ¬¡æé†’æˆ‘ä»¬ï¼Œä¸è¦ç›²ç›®è¿·ä¿¡ LLM çš„é€šç”¨èƒ½åŠ›ï¼ˆå°¤å…¶æ˜¯åœ¨å¬è§‰å’Œé€»è¾‘ä¸€è‡´æ€§ä¸Šï¼‰ï¼Œä½†åœ¨å…·ä½“çš„å‚ç›´é¢†åŸŸï¼ˆå¦‚ä»£ç ä¿®å¤ã€ç§‘å­¦è®¡ç®—ã€è‡ªåŠ¨é©¾é©¶ï¼‰ï¼ŒAI çš„å·¥ç¨‹åŒ–èƒ½åŠ›æ­£åœ¨çªé£çŒ›è¿›ã€‚\nå¯¹äºåš Prompt Engineering çš„åŒå­¦ï¼Œè®°å¾—é‚£æ¡é»„é‡‘æ³•åˆ™ï¼š**æŠŠä¾‹å­æ”¾åœ¨æœ€å‰é¢ï¼**\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼Œæˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2507.23163v2",
      "title": "Argumentatively Coherent Judgmental Forecasting",
      "title_zh": "è®ºè¯è¿è´¯çš„åˆ¤æ–­æ€§é¢„æµ‹",
      "authors": [
        "Deniz Gorur",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "abstract": "Judgmental forecasting employs human opinions to make predictions about future events, rather than exclusively historical data as in quantitative forecasting. When these opinions form an argumentative structure around forecasts, it is useful to study the properties of the forecasts from an argumentative perspective. In this paper, we advocate and formally define a property of argumentative coherence, which, in essence, requires that a forecaster's reasoning is coherent with their forecast. We then conduct three evaluations with our notion of coherence. First, we assess the impact of enforcing coherence on human forecasters as well as on Large Language Model (LLM)-based forecasters, given that they have recently shown to be competitive with human forecasters. In both cases, we show that filtering out incoherent predictions improves forecasting accuracy consistently, supporting the practical value of coherence in both human and LLM-based forecasting. Then, via crowd-sourced user experiments, we show that, despite its apparent intuitiveness and usefulness, users do not generally align with this coherence property. This points to the need to integrate, within argumentation-based judgmental forecasting, mechanisms to filter out incoherent opinions before obtaining group forecasting predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ¤æ–­æ€§é¢„æµ‹(Judgmental forecasting)ä¸­äººç±»è§‚ç‚¹ä¸é¢„æµ‹ç»“æœä¹‹é—´çš„é€»è¾‘å…³è”ï¼Œå¹¶æ­£å¼å®šä¹‰äº†è®ºè¯ä¸€è‡´æ€§(Argumentative coherence)çš„æ¦‚å¿µï¼Œå³é¢„æµ‹è€…çš„æ¨ç†è¿‡ç¨‹éœ€ä¸å…¶é¢„æµ‹ç»“æœä¿æŒä¸€è‡´ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹äººç±»åŠå¤§å‹è¯­è¨€æ¨¡å‹(LLM)é¢„æµ‹è€…çš„è¯„ä¼°å‘ç°ï¼Œå‰”é™¤ä¸ç¬¦åˆä¸€è‡´æ€§çš„é¢„æµ‹èƒ½æœ‰æ•ˆä¸”æŒç»­åœ°æå‡é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¯æ˜äº†è¯¥å±æ€§çš„å®ç”¨ä»·å€¼ã€‚å°½ç®¡è¯¥å±æ€§åœ¨é€»è¾‘ä¸Šå…·æœ‰ç›´è§‚çš„åˆç†æ€§ï¼Œä½†ä¼—åŒ…å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ™®é€šç”¨æˆ·åœ¨å®é™…é¢„æµ‹ä¸­å¾€å¾€æ— æ³•è‡ªå‘åœ°éµå¾ªè®ºè¯ä¸€è‡´æ€§ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æŒ‡å‡ºåœ¨åŸºäºè®ºè¯çš„é¢„æµ‹ä½“ç³»ä¸­ï¼Œæœ‰å¿…è¦å¼•å…¥ä¸“é—¨çš„æœºåˆ¶æ¥è¿‡æ»¤ä¸ä¸€è‡´çš„è§‚ç‚¹ï¼Œä»è€Œç¡®ä¿æœ€ç»ˆç¾¤ä½“é¢„æµ‹ç»“æœçš„å¯é æ€§ä¸ç§‘å­¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 18 figures, ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.23163v2",
      "published_date": "2025-07-30 23:58:37 UTC",
      "updated_date": "2025-08-25 15:30:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:08.398271+00:00"
    },
    {
      "arxiv_id": "2507.23154v1",
      "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations",
      "title_zh": "FuseTenï¼šåŸºäºæ—¶ç©ºå«æ˜Ÿè§‚æµ‹çš„æ¯æ—¥ 10 ç±³åœ°è¡¨æ¸©åº¦ä¼°ç®—ç”Ÿæˆå¼æ¨¡å‹",
      "authors": [
        "Sofiane Bouaziz",
        "Adel Hafiane",
        "Raphael Canals",
        "Rachid Nedjai"
      ],
      "abstract": "Urban heatwaves, droughts, and land degradation are pressing and growing challenges in the context of climate change. A valuable approach to studying them requires accurate spatio-temporal information on land surface conditions. One of the most important variables for assessing and understanding these phenomena is Land Surface Temperature (LST), which is derived from satellites and provides essential information about the thermal state of the Earth's surface. However, satellite platforms inherently face a trade-off between spatial and temporal resolutions. To bridge this gap, we propose FuseTen, a novel generative framework that produces daily LST observations at a fine 10 m spatial resolution by fusing spatio-temporal observations derived from Sentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative architecture trained using an averaging-based supervision strategy grounded in physical principles. It incorporates attention and normalization modules within the fusion process and uses a PatchGAN discriminator to enforce realism. Experiments across multiple dates show that FuseTen outperforms linear baselines, with an average 32.06% improvement in quantitative metrics and 31.42% in visual fidelity. To the best of our knowledge, this is the first non-linear method to generate daily LST estimates at such fine spatial resolution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FuseTenï¼Œä¸€ç§æ–°å‹ç”Ÿæˆå¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å«æ˜Ÿé¥æ„Ÿä¸­åœ°è¡¨æ¸©åº¦(LST)æ—¶ç©ºåˆ†è¾¨ç‡æƒè¡¡çš„å±€é™æ€§ï¼Œä»¥åº”å¯¹åŸå¸‚çƒ­æµªå’Œå¹²æ—±ç­‰æ°”å€™æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡èåˆæ¥è‡ªSentinel-2ã€Landsat 8å’ŒTerra MODISçš„æ—¶ç©ºè§‚æµ‹æ•°æ®ï¼Œå®ç°äº†æ¯æ—¥10ç±³é«˜ç©ºé—´åˆ†è¾¨ç‡çš„LSTä¼°ç®—ã€‚FuseTené‡‡ç”¨äº†åŸºäºç‰©ç†åŸç†çš„å¹³å‡åŒ–ç›‘ç£ç­–ç•¥(averaging-based supervision)è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨èåˆè¿‡ç¨‹ä¸­é›†æˆäº†æ³¨æ„åŠ›(attention)å’Œå½’ä¸€åŒ–(normalization)æ¨¡å—ï¼ŒåŒæ—¶åˆ©ç”¨PatchGANåˆ¤åˆ«å™¨å¢å¼ºç”Ÿæˆç»“æœçš„çœŸå®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFuseTenåœ¨å®šé‡æŒ‡æ ‡ä¸Šæ¯”çº¿æ€§åŸºçº¿æ¨¡å‹å¹³å‡æé«˜äº†32.06%ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦(visual fidelity)ä¸Šæé«˜äº†31.42%ã€‚ä½œä¸ºå·²çŸ¥é¦–ä¸ªåœ¨10ç±³ç»†ç²’åº¦åˆ†è¾¨ç‡ä¸‹ç”Ÿæˆæ¯æ—¥LSTä¼°è®¡çš„éçº¿æ€§æ–¹æ³•ï¼Œè¯¥ç ”ç©¶ä¸ºç²¾ç¡®è¯„ä¼°åœ°è¡¨çƒ­çŠ¶æ€æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in the 2025 International Conference on Machine Intelligence for GeoAnalytics and Remote Sensing (MIGARS)",
      "pdf_url": "https://arxiv.org/pdf/2507.23154v1",
      "published_date": "2025-07-30 23:04:16 UTC",
      "updated_date": "2025-07-30 23:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:08.588711+00:00"
    },
    {
      "arxiv_id": "2508.02710v1",
      "title": "Evaluation of Deep Learning Models for LBBB Classification in ECG Signals",
      "title_zh": "ç”¨äºå¿ƒç”µä¿¡å·å·¦æŸæ”¯ä¼ å¯¼é˜»æ»ï¼ˆLBBBï¼‰åˆ†ç±»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Beatriz Macas OrdÃ³Ã±ez",
        "Diego Vinicio Orellana Villavicencio",
        "JosÃ© Manuel FerrÃ¡ndez",
        "Paula Bonomini"
      ],
      "abstract": "This study explores different neural network architectures to evaluate their ability to extract spatial and temporal patterns from electrocardiographic (ECG) signals and classify them into three groups: healthy subjects, Left Bundle Branch Block (LBBB), and Strict Left Bundle Branch Block (sLBBB).\n  Clinical Relevance, Innovative technologies enable the selection of candidates for Cardiac Resynchronization Therapy (CRT) by optimizing the classification of subjects with Left Bundle Branch Block (LBBB).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢å¹¶è¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹ (Deep Learning)ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨ä»å¿ƒç”µå›¾(ECG)ä¿¡å·ä¸­æœ‰æ•ˆæå–ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ç‰¹å¾ã€‚å…¶æ ¸å¿ƒä»»åŠ¡æ˜¯å°†å—è¯•è€…ç²¾å‡†åˆ†ç±»ä¸ºå¥åº·ç»„ã€å·¦æŸæ”¯ä¼ å¯¼é˜»æ»(LBBB)ä»¥åŠä¸¥æ ¼å·¦æŸæ”¯ä¼ å¯¼é˜»æ»(sLBBB)ä¸‰ä¸ªç‰¹å®šç¾¤ä½“ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¶æ„çš„åˆ†ç±»æ•ˆèƒ½ï¼Œç ”ç©¶å±•ç¤ºäº†åˆ›æ–°æŠ€æœ¯åœ¨ä¼˜åŒ–LBBBè¯Šæ–­æ–¹é¢çš„æ½œåŠ›ã€‚åœ¨ä¸´åºŠåº”ç”¨å±‚é¢ï¼Œè¿™ç§ä¼˜åŒ–çš„åˆ†ç±»æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«é€‚åˆæ¥å—å¿ƒè„å†åŒæ­¥åŒ–æ²»ç–—(CRT)çš„å€™é€‰äººã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†è‡ªåŠ¨åŒ–ç‰¹å¾æå–åœ¨å¤„ç†å¤æ‚ç”Ÿç†ä¿¡å·ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæå‡å¿ƒè„ç—…è¯Šæ–­çš„å®¢è§‚æ€§ä¸æ•ˆç‡æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted for presentation in the 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.02710v1",
      "published_date": "2025-07-30 22:11:05 UTC",
      "updated_date": "2025-07-30 22:11:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:13.501234+00:00"
    },
    {
      "arxiv_id": "2508.08274v1",
      "title": "Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦ï¼šä¸€ç§ç”¨äºä»‡æ¨ä¸åé©³è¨€è®ºè¯†åˆ«çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹",
      "authors": [
        "Roberto Labadie-Tamayo",
        "Djordje SlijepÄeviÄ‡",
        "Xihui Chen",
        "Adrian Jaques BÃ¶ck",
        "Andreas Babic",
        "Liz Freimann",
        "Christiane AtzmÃ¼ller Matthias Zeppelzauer"
      ],
      "abstract": "The rapid increase in hate speech on social media has exposed an unprecedented impact on society, making automated methods for detecting such content important. Unlike prior black-box models, we propose a novel transparent method for automated hate and counter speech recognition, i.e., \"Speech Concept Bottleneck Model\" (SCBM), using adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to map input texts to an abstract adjective-based representation, which is then sent to a light-weight classifier for downstream tasks. Across five benchmark datasets spanning multiple languages and platforms (e.g., Twitter, Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which outperforms the most recently reported results from the literature on four out of five datasets. Aside from high recognition accuracy, SCBM provides a high level of both local and global interpretability. Furthermore, fusing our adjective-based concept representation with transformer embeddings, leads to a 1.8% performance increase on average across all datasets, showing that the proposed representation captures complementary information. Our results demonstrate that adjective-based concept representations can serve as compact, interpretable, and effective encodings for hate and counter speech recognition. With adapted adjectives, our method can also be applied to other NLP tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Speech Concept Bottleneck Model (SCBM) çš„æ–°å‹é€æ˜æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¤¾äº¤åª’ä½“ä¸Šæ—¥ç›Šå¢é•¿çš„ä»‡æ¨è¨€è®º (hate speech) åŠå…¶åå‡»è¨€è®º (counter speech) çš„è‡ªåŠ¨è¯†åˆ«é—®é¢˜ã€‚ä¸ä»¥å¾€çš„é»‘ç›’æ¨¡å‹ä¸åŒï¼ŒSCBM åˆ›æ–°æ€§åœ°é‡‡ç”¨å½¢å®¹è¯ä½œä¸ºå…·æœ‰äººç±»å¯è§£é‡Šæ€§çš„ç“¶é¢ˆæ¦‚å¿µ (bottleneck concepts)ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å°†è¾“å…¥æ–‡æœ¬æ˜ å°„ä¸ºåŸºäºå½¢å®¹è¯çš„æŠ½è±¡è¡¨ç¤ºï¼Œéšåå°†å…¶ä¼ é€’ç»™è½»é‡çº§åˆ†ç±»å™¨ä»¥æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚ç ”ç©¶åœ¨æ¶‰åŠå¤šä¸ªè¯­è¨€å’Œå¹³å°ï¼ˆå¦‚ Twitterã€Redditã€YouTubeï¼‰çš„äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤º SCBM è¾¾åˆ°äº† 0.69 çš„å¹³å‡ macro-F1 åˆ†æ•°ï¼Œåœ¨å…¶ä¸­å››ä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–‡çŒ®ä¸­çš„ç ”ç©¶æˆæœã€‚é™¤äº†é«˜è¯†åˆ«ç²¾åº¦å¤–ï¼ŒSCBM è¿˜æä¾›äº†æé«˜çš„å±€éƒ¨å’Œå…¨å±€å¯è§£é‡Šæ€§ï¼Œä¸”å°†å…¶å½¢å®¹è¯è¡¨ç¤ºä¸ Transformer åµŒå…¥ç»“åˆåï¼Œå¹³å‡æ€§èƒ½è¿›ä¸€æ­¥æå‡äº† 1.8%ã€‚è¿™è¯æ˜äº†åŸºäºå½¢å®¹è¯çš„æ¦‚å¿µè¡¨ç¤ºå¯ä»¥ä½œä¸ºä¸€ç§ç´§å‡‘ã€æœ‰æ•ˆä¸”å¯è§£é‡Šçš„ç¼–ç æ–¹å¼ï¼Œè¯¥æ–¹æ³•ç»è¿‡é€‚å½“è°ƒæ•´åäº¦å¯æ¨å¹¿è‡³å…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 10 figures, This is a preprint of a manuscript accepted for publication in Information Processing & Management (Elsevier)",
      "pdf_url": "https://arxiv.org/pdf/2508.08274v1",
      "published_date": "2025-07-30 21:50:30 UTC",
      "updated_date": "2025-07-30 21:50:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:22.091930+00:00"
    },
    {
      "arxiv_id": "2507.23121v1",
      "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity",
      "title_zh": "æ­ç¤ºå¯ä¿¡å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡æ–‡æœ¬æ­§ä¹‰ä¸‹çš„è„†å¼±æ€§",
      "authors": [
        "Xinwei Wu",
        "Haojie Li",
        "Hongyu Liu",
        "Xinyu Ji",
        "Ruohan Li",
        "Yule Chen",
        "Yigeng Zhang"
      ],
      "abstract": "In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢å¯¹ä¸­æ–‡æ–‡æœ¬æ­§ä¹‰ (Chinese textual ambiguity) æ—¶çš„è¡¨ç°åŠå…¶å¯¹æ¨¡å‹å¯ä¿¡åº¦çš„å½±å“ã€‚é€šè¿‡æ„å»ºåŒ…å«3å¤§ç±»å’Œ9å­ç±»çš„åŸºå‡†æ•°æ®é›† (benchmark dataset)ï¼Œç ”ç©¶è€…ç³»ç»Ÿåˆ†æäº†æ­§ä¹‰å¥åŠå…¶æ¶ˆæ­§å¯¹åœ¨ç‰¹å®šè¯­å¢ƒä¸‹çš„æ¨¡å‹åé¦ˆã€‚å®éªŒæ­ç¤ºäº† LLMs åœ¨å¤„ç†æ­§ä¹‰æ—¶å­˜åœ¨çš„æ˜¾è‘—è„†å¼±æ€§ (fragility)ï¼Œè¡¨ç°å‡ºä¸äººç±»æˆªç„¶ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ã€‚å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹æ— æ³•æœ‰æ•ˆåŒºåˆ†æ­§ä¹‰ä¸éæ­§ä¹‰æ–‡æœ¬ï¼Œå¯¹å•ä¸€å«ä¹‰çš„è§£é‡Šå¾€å¾€è¿‡åº¦è‡ªä¿¡ (overconfidence)ï¼Œè€Œåœ¨ç†è§£å¤šé‡å«ä¹‰æ—¶åˆå®¹æ˜“è¿‡åº¦æ€è€ƒ (overthinking)ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œè¯­è¨€ä¸ç¡®å®šæ€§æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æå‡è¯­è¨€ç†è§£çš„é²æ£’æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)",
      "pdf_url": "https://arxiv.org/pdf/2507.23121v1",
      "published_date": "2025-07-30 21:50:19 UTC",
      "updated_date": "2025-07-30 21:50:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:27.988333+00:00"
    },
    {
      "arxiv_id": "2507.23115v1",
      "title": "FLOSS: Federated Learning with Opt-Out and Straggler Support",
      "title_zh": "FLOSSï¼šæ”¯æŒé€‰æ‹©æ€§é€€å‡ºä¸æ‰é˜Ÿè€…çš„è”é‚¦å­¦ä¹ ",
      "authors": [
        "David J Goetze",
        "Dahlia J Felten",
        "Jeannie R Albrecht",
        "Rohit Bhattacharya"
      ],
      "abstract": "Previous work on data privacy in federated learning systems focuses on privacy-preserving operations for data from users who have agreed to share their data for training. However, modern data privacy agreements also empower users to use the system while opting out of sharing their data as desired. When combined with stragglers that arise from heterogeneous device capabilities, the result is missing data from a variety of sources that introduces bias and degrades model performance. In this paper, we present FLOSS, a system that mitigates the impacts of such missing data on federated learning in the presence of stragglers and user opt-out, and empirically demonstrate its performance in simulations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰ç³»ç»Ÿä¸­ç”±äºç”¨æˆ·é€‰æ‹©ä¸å‚ä¸æ•°æ®å…±äº«ï¼ˆOpt-Outï¼‰ä»¥åŠå¼‚æ„è®¾å¤‡æ€§èƒ½å·®å¼‚äº§ç”Ÿçš„æ‰é˜Ÿè€…ï¼ˆStragglersï¼‰è€Œå¯¼è‡´çš„ç¼ºå¤±æ•°æ®é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä»¥å¾€å…³äºè”é‚¦å­¦ä¹ éšç§çš„ç ”ç©¶å¤šä¾§é‡äºä¿æŠ¤å·²å‚ä¸è®­ç»ƒç”¨æˆ·çš„æ•°æ®ï¼Œä½†ç°ä»£éšç§åè®®èµ‹äºˆäº†ç”¨æˆ·åœ¨ä½¿ç”¨ç³»ç»Ÿçš„åŒæ—¶æ‹’ç»åˆ†äº«æ•°æ®çš„æƒåˆ©ï¼Œè¿™ä¸æ‰é˜Ÿè€…ç°è±¡ç»“åˆåä¼šå¼•å…¥æ•°æ®åå·®å¹¶ä¸¥é‡æŸå®³æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FLOSSç³»ç»Ÿï¼Œæ—¨åœ¨å‡è½»æ‰é˜Ÿè€…å’Œç”¨æˆ·é€€å‡ºåœºæ™¯ä¸‹ç¼ºå¤±æ•°æ®å¯¹è”é‚¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹çš„å½±å“ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å®è¯ç ”ç©¶ï¼Œè¯¥å·¥ä½œè¯æ˜äº†FLOSSèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ•°æ®ç¼ºå¤±å¸¦æ¥çš„åå·®ï¼Œä»è€Œæå‡æ•´ä½“æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.23115v1",
      "published_date": "2025-07-30 21:34:56 UTC",
      "updated_date": "2025-07-30 21:34:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:24.589056+00:00"
    },
    {
      "arxiv_id": "2507.23104v1",
      "title": "RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL",
      "title_zh": "RASLï¼šé¢å‘æµ·é‡æ•°æ®åº“æ–‡æœ¬è½¬ SQL çš„æ£€ç´¢å¢å¼ºæ¨¡å¼é“¾æ¥",
      "authors": [
        "Jeffrey Eben",
        "Aitzaz Ahmad",
        "Stephen Lau"
      ],
      "abstract": "Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡ä¼ä¸šçº§æ•°æ®ç›®å½•ä¸­ Text-to-SQL ç³»ç»Ÿéš¾ä»¥æ‰©å±•çš„é—®é¢˜ï¼Œæå‡ºäº† RASLï¼ˆRetrieval Augmented Schema Linkingï¼‰æ£€ç´¢å¢å¼ºæ¶æ„ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–ç‰¹å®šé¢†åŸŸå¾®è°ƒä¸”å¿½ç•¥å…ƒæ•°æ®è¯­ä¹‰èƒŒæ™¯çš„å±€é™ï¼ŒRASL å°†æ•°æ®åº“ Schema å’Œå…ƒæ•°æ®åˆ†è§£ä¸ºç¦»æ•£çš„è¯­ä¹‰å•å…ƒå¹¶åˆ†åˆ«è¿›è¡Œç´¢å¼•ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„ç›®æ ‡æ£€ç´¢ã€‚è¯¥æ–¹æ³•åœ¨åˆ©ç”¨åˆ—çº§ä¿¡æ¯çš„åŒæ—¶ä¼˜å…ˆè¿›è¡Œè¡¨è¯†åˆ«ï¼Œç¡®ä¿äº†æ£€ç´¢åˆ°çš„è¡¨æ€»æ•°å¤„äºå¯æ§çš„ä¸Šä¸‹æ–‡é¢„ç®—ä¹‹å†…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRASL åœ¨å¤„ç†ç»“æ„å¤æ‚ã€è§„æ¨¡å·¨å¤§çš„æ•°æ®åº“æ—¶ï¼Œå¬å›ç‡å’Œå‡†ç¡®ç‡å‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ¡ˆæ— éœ€ä¸“é—¨å¾®è°ƒå³å¯åœ¨å¤šç§ä¼ä¸šåœºæ™¯ä¸­çµæ´»éƒ¨ç½²ï¼Œæœ‰æ•ˆè§£å†³äº†è‡ªç„¶è¯­è¨€æ•°æ®åº“æ¥å£çš„å¯æ‰©å±•æ€§ç“¶é¢ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.23104v1",
      "published_date": "2025-07-30 21:09:47 UTC",
      "updated_date": "2025-07-30 21:09:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:36.648459+00:00"
    },
    {
      "arxiv_id": "2507.23095v2",
      "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity",
      "title_zh": "SMART-Editorï¼šå…·æœ‰ç»“æ„å®Œæ•´æ€§çš„ç±»äººåŒ–è®¾è®¡ç¼–è¾‘å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Ishani Mondal",
        "Meera Bharadwaj",
        "Ayush Roy",
        "Aparna Garimella",
        "Jordan Lee Boyd-Graber"
      ],
      "abstract": "We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SMART-Editorï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è·¨ç»“æ„åŒ–é¢†åŸŸï¼ˆå¦‚æµ·æŠ¥ã€ç½‘ç«™ï¼‰å’Œéç»“æ„åŒ–é¢†åŸŸï¼ˆå¦‚è‡ªç„¶å›¾åƒï¼‰è¿›è¡Œç»„åˆå¸ƒå±€ä¸å†…å®¹ç¼–è¾‘çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚ä¸ä»¥å¾€ä»…æ‰§è¡Œå±€éƒ¨ç¼–è¾‘çš„æ¨¡å‹ä¸åŒï¼ŒSMART-Editoré€šè¿‡æ¨ç†æ—¶çš„å¥–åŠ±å¼•å¯¼ç²¾ç»†åŒ–æ–¹æ³•Reward-Refineå’Œè®­ç»ƒæ—¶çš„åå¥½ä¼˜åŒ–æ–¹æ³•RewardDPOï¼Œæœ‰æ•ˆåœ°ä¿æŒäº†ç¼–è¾‘è¿‡ç¨‹ä¸­çš„å…¨å±€ä¸€è‡´æ€§ä¸ç»“æ„å®Œæ•´æ€§ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶è€…å¼•å…¥äº†æ¶µç›–å¤šé¢†åŸŸçº§è”ç¼–è¾‘åœºæ™¯çš„æ–°åŸºå‡†SMARTEdit-Benchã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMART-Editoråœ¨æ€§èƒ½ä¸Šä¼˜äºInstructPix2Pixå’ŒHIVEç­‰å¼ºåŸºçº¿æ¨¡å‹ï¼Œå…¶ä¸­RewardDPOåœ¨ç»“æ„åŒ–è®¾ç½®ä¸­å®ç°äº†é«˜è¾¾15%çš„å¢ç›Šã€‚è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä»·å‡è¯å®ï¼Œå¥–åŠ±å¼•å¯¼è§„åˆ’åœ¨ç”Ÿæˆè¯­ä¹‰ä¸€è‡´ä¸”è§†è§‰å¯¹é½çš„ç¼–è¾‘ç»“æœæ–¹é¢å…·æœ‰æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This requires some internal approval before the public release",
      "pdf_url": "https://arxiv.org/pdf/2507.23095v2",
      "published_date": "2025-07-30 20:52:34 UTC",
      "updated_date": "2025-08-05 10:25:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:31.150679+00:00"
    },
    {
      "arxiv_id": "2507.23093v1",
      "title": "On the Sustainability of AI Inferences in the Edge",
      "title_zh": "è®ºè¾¹ç¼˜ä¾§ AI æ¨ç†çš„å¯æŒç»­æ€§",
      "authors": [
        "Ghazal Sobhani",
        "Md. Monzurul Amin Ifath",
        "Tushar Sharma",
        "Israat Haque"
      ],
      "abstract": "The proliferation of the Internet of Things (IoT) and its cutting-edge AI-enabled applications (e.g., autonomous vehicles and smart industries) combine two paradigms: data-driven systems and their deployment on the edge. Usually, edge devices perform inferences to support latency-critical applications. In addition to the performance of these resource-constrained edge devices, their energy usage is a critical factor in adopting and deploying edge applications. Examples of such devices include Raspberry Pi (RPi), Intel Neural Compute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU). Despite their adoption in edge deployment for AI inferences, there is no study on their performance and energy usage for informed decision-making on the device and model selection to meet the demands of applications. This study fills the gap by rigorously characterizing the performance of traditional, neural networks, and large language models on the above-edge devices. Specifically, we analyze trade-offs among model F1 score, inference time, inference power, and memory usage. Hardware and framework optimization, along with external parameter tuning of AI models, can balance between model performance and resource usage to realize practical edge AI deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç‰©è”ç½‘(IoT)å’Œè¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹äººå·¥æ™ºèƒ½æ¨ç†(AI Inference)çš„å¯æŒç»­æ€§é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡çš„èƒ½æºæ¶ˆè€—ä¸æ€§èƒ½å¹³è¡¡ã€‚ç ”ç©¶é’ˆå¯¹ Raspberry Pi (RPi)ã€Intel Neural Compute Stick (INCS)ã€NVIDIA Jetson nano (NJn) å’Œ Google Coral USB (GCU) ç­‰ä¸»æµç¡¬ä»¶ï¼Œç³»ç»Ÿåœ°è¡¨å¾äº†ä¼ ç»Ÿæ¨¡å‹ã€ç¥ç»ç½‘ç»œ(Neural Networks)ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)çš„è¿è¡Œè¡¨ç°ã€‚å®éªŒé€šè¿‡é‡åŒ–åˆ†æ F1 scoreã€æ¨ç†æ—¶é—´(Inference time)ã€æ¨ç†åŠŸè€—(Inference power)å’Œå†…å­˜å ç”¨(Memory usage)ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå¡«è¡¥äº†è¾¹ç¼˜éƒ¨ç½²å†³ç­–ä¸­ç¼ºä¹æ€§èƒ½ä¸èƒ½æ•ˆæ•°æ®æ”¯æ’‘çš„ç©ºç™½ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç»“åˆç¡¬ä»¶ä¸æ¡†æ¶ä¼˜åŒ–ä»¥åŠå¤–éƒ¨å‚æ•°å¾®è°ƒï¼Œå¯ä»¥æœ‰æ•ˆå¹³è¡¡æ¨¡å‹è¡¨ç°ä¸èµ„æºæ¶ˆè€—ï¼Œä¸ºå®ç°å®ç”¨çš„è¾¹ç¼˜äººå·¥æ™ºèƒ½(Edge AI)éƒ¨ç½²æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 8 figures, 6 tables, in preparation for journal submission",
      "pdf_url": "https://arxiv.org/pdf/2507.23093v1",
      "published_date": "2025-07-30 20:47:22 UTC",
      "updated_date": "2025-07-30 20:47:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:37:35.946408+00:00"
    },
    {
      "arxiv_id": "2507.23091v1",
      "title": "Moravec's Paradox: Towards an Auditory Turing Test",
      "title_zh": "è«æ‹‰ç»´å…‹æ‚–è®ºï¼šè¿ˆå‘å¬è§‰å›¾çµæµ‹è¯•",
      "authors": [
        "David Noever",
        "Forrest McKee"
      ],
      "abstract": "This research work demonstrates that current AI systems fail catastrophically on auditory tasks that humans perform effortlessly. Drawing inspiration from Moravec's paradox (i.e., tasks simple for humans often prove difficult for machines, and vice versa), we introduce an auditory Turing test comprising 917 challenges across seven categories: overlapping speech, speech in noise, temporal distortion, spatial audio, coffee-shop noise, phone distortion, and perceptual illusions. Our evaluation of state-of-the-art audio models including GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate exceeding 93%, with even the best-performing model achieving only 6.9% accuracy on tasks that humans solved at 7.5 times higher success (52%). These results expose focusing failures in how AI systems process complex auditory scenes, particularly in selective attention, noise robustness, and contextual adaptation. Our benchmark not only quantifies the human-machine auditory gap but also provides insights into why these failures occur, suggesting that current architectures lack fundamental mechanisms for human-like auditory scene analysis. The traditional design of audio CAPTCHAs highlights common filters that humans evolved but machines fail to select in multimodal language models. This work establishes a diagnostic framework for measuring progress toward human-level machine listening and highlights the need for novel approaches integrating selective attention, physics-based audio understanding, and context-aware perception into multimodal AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Moravec's Paradoxåœ¨å¬è§‰é¢†åŸŸçš„åº”ç”¨ï¼Œæ­ç¤ºäº†å½“å‰AIç³»ç»Ÿåœ¨äººç±»èƒ½è½»æ¾å®Œæˆçš„å¬è§‰ä»»åŠ¡ä¸­é¢ä¸´çš„ç¾éš¾æ€§å¤±è´¥ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºAuditory Turing Testçš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«æ¶µç›–overlapping speechã€speech in noiseã€spatial audioå’Œperceptual illusionsç­‰ä¸ƒä¸ªç±»åˆ«çš„917é¡¹æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹GPT-4éŸ³é¢‘åŠŸèƒ½å’ŒWhisperç­‰æœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°AIçš„å¤±è´¥ç‡è¶…è¿‡93%ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°6.9%çš„å‡†ç¡®ç‡ï¼Œè€Œäººç±»çš„å‡†ç¡®ç‡é«˜å‡º7.5å€ã€‚å®éªŒç»“æœæš´éœ²äº†AIåœ¨selective attentionã€noise robustnesså’Œcontextual adaptationæ–¹é¢çš„æ ¸å¿ƒç¼ºé™·ï¼Œè¡¨æ˜ç°æœ‰æ¶æ„ç¼ºä¹äººç±»çº§åˆ«çš„å¬è§‰åœºæ™¯åˆ†æï¼ˆauditory scene analysisï¼‰èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ä»…é‡åŒ–äº†äººæœºå¬è§‰å·®è·ï¼Œè¿˜ä¸ºå¼€å‘å…·å¤‡ç‰©ç†å£°å­¦ç†è§£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆcontext-aware perceptionï¼‰èƒ½åŠ›çš„å¤šæ¨¡æ€AIç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯Šæ–­æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.23091v1",
      "published_date": "2025-07-30 20:45:13 UTC",
      "updated_date": "2025-07-30 20:45:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:38:23.150422+00:00"
    },
    {
      "arxiv_id": "2507.23088v1",
      "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance",
      "title_zh": "è¶…è¶ŠåƒµåŒ– AIï¼šè¿ˆå‘æœ¯ä¸­æ‰‹æœ¯è¾…åŠ©çš„è‡ªç„¶äººæœºå…±ç”Ÿ",
      "authors": [
        "Lalithkumar Seenivasan",
        "Jiru Xu",
        "Roger D. Soberanis Mukul",
        "Hao Ding",
        "Grayson Byrd",
        "Yu-Chun Ku",
        "Jose L. Porras",
        "Masaru Ishii",
        "Mathias Unberath"
      ],
      "abstract": "Emerging surgical data science and robotics solutions, especially those designed to provide assistance in situ, require natural human-machine interfaces to fully unlock their potential in providing adaptive and intuitive aid. Contemporary AI-driven solutions remain inherently rigid, offering limited flexibility and restricting natural human-machine interaction in dynamic surgical environments. These solutions rely heavily on extensive task-specific pre-training, fixed object categories, and explicit manual-prompting. This work introduces a novel Perception Agent that leverages speech-integrated prompt-engineered large language models (LLMs), segment anything model (SAM), and any-point tracking foundation models to enable a more natural human-machine interaction in real-time intraoperative surgical assistance. Incorporating a memory repository and two novel mechanisms for segmenting unseen elements, Perception Agent offers the flexibility to segment both known and unseen elements in the surgical scene through intuitive interaction. Incorporating the ability to memorize novel elements for use in future surgeries, this work takes a marked step towards human-machine symbiosis in surgical procedures. Through quantitative analysis on a public dataset, we show that the performance of our agent is on par with considerably more labor-intensive manual-prompting strategies. Qualitatively, we show the flexibility of our agent in segmenting novel elements (instruments, phantom grafts, and gauze) in a custom-curated dataset. By offering natural human-machine interaction and overcoming rigidity, our Perception Agent potentially brings AI-based real-time assistance in dynamic surgical environments closer to reality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰æ‰‹æœ¯ AI æ–¹æ¡ˆåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿‡äºåƒµåŒ–ã€ä¾èµ–ç‰¹å®šä»»åŠ¡é¢„è®­ç»ƒå’Œæ‰‹åŠ¨æç¤º (manual-prompting) çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Perception Agent çš„æ–°å‹æ„ŸçŸ¥æ™ºèƒ½ä½“ã€‚è¯¥æ™ºèƒ½ä½“é›†æˆäº†è¯­éŸ³æç¤ºå·¥ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€Segment Anything Model (SAM) å’Œå…¨ç‚¹è¿½è¸ªåŸºç¡€æ¨¡å‹ (any-point tracking foundation models)ï¼Œæ—¨åœ¨å®ç°æ›´è‡ªç„¶çš„æ‰‹æœ¯å®æ—¶æœ¯ä¸­è¾…åŠ©ã€‚é€šè¿‡å¼•å…¥å†…å­˜åº“ (memory repository) å’Œé’ˆå¯¹æœªçŸ¥å…ƒç´ åˆ†å‰²çš„æ–°æœºåˆ¶ï¼ŒPerception Agent èƒ½å¤Ÿçµæ´»åœ°é€šè¿‡ç›´è§‚äº¤äº’è¯†åˆ«æ‰‹æœ¯åœºæ™¯ä¸­çš„å·²çŸ¥åŠæœªçŸ¥å…ƒç´ ã€‚è¯¥å·¥ä½œè¿˜èµ‹äºˆäº†ç³»ç»Ÿè®°å¿†æ–°å…ƒç´ ä»¥ä¾›æœªæ¥æ‰‹æœ¯ä½¿ç”¨çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æ¨è¿›äº†æ‰‹æœ¯ç¨‹åºä¸­äººæœºå…±ç”Ÿ (human-machine symbiosis) çš„è¿›ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“çš„æ€§èƒ½ä¸åŠ³åŠ¨å¯†é›†å‹çš„æ‰‹åŠ¨æç¤ºç­–ç•¥ç›¸å½“ï¼Œå¹¶åœ¨åˆ†å‰²æ‰‹æœ¯å™¨æ¢°ã€æ¨¡æ‹Ÿç§»æ¤ç‰©å’Œçº±å¸ƒç­‰æ–°å…ƒç´ æ–¹é¢è¡¨ç°å‡ºæé«˜çš„çµæ´»æ€§ã€‚é€šè¿‡å…‹æœç³»ç»ŸåƒµåŒ–æ€§å¹¶æä¾›è‡ªç„¶çš„äº¤äº’ç•Œé¢ï¼Œè¯¥ç ”ç©¶ä½¿åŸºäº AI çš„åŠ¨æ€æ‰‹æœ¯å®æ—¶è¾…åŠ©æŠ€æœ¯å‘ä¸´åºŠåº”ç”¨è¿ˆå‡ºäº†åšå®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.23088v1",
      "published_date": "2025-07-30 20:42:24 UTC",
      "updated_date": "2025-07-30 20:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:38:41.887536+00:00"
    },
    {
      "arxiv_id": "2507.23087v1",
      "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes",
      "title_zh": "è®ºåŸºäºä¸šåŠ¡æµç¨‹çš„å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©æ™ºèƒ½åˆçº¦ç”Ÿæˆ",
      "authors": [
        "Fabian Stiehle",
        "Hans Weytjens",
        "Ingo Weber"
      ],
      "abstract": "Large language models (LLMs) have changed the reality of how software is produced. Within the wider software engineering community, among many other purposes, they are explored for code generation use cases from different types of input. In this work, we present an exploratory study to investigate the use of LLMs for generating smart contract code from business process descriptions, an idea that has emerged in recent literature to overcome the limitations of traditional rule-based code generation approaches. However, current LLM-based work evaluates generated code on small samples, relying on manual inspection, or testing whether code compiles but ignoring correct execution. With this work, we introduce an automated evaluation framework and provide empirical data from larger data sets of process models. We test LLMs of different types and sizes in their capabilities of achieving important properties of process execution, including enforcing process flow, resource allocation, and data-based conditions. Our results show that LLM performance falls short of the perfect reliability required for smart contract development. We suggest future work to explore responsible LLM integrations in existing tools for code generation to ensure more reliable output. Our benchmarking framework can serve as a foundation for developing and evaluating such integrations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»ä¸šåŠ¡æµç¨‹(business process)æè¿°ä¸­ç”Ÿæˆæ™ºèƒ½åˆçº¦(smart contract)ä»£ç çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨å…‹æœä¼ ç»ŸåŸºäºè§„åˆ™çš„ç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ã€‚é’ˆå¯¹ç›®å‰ç›¸å…³ç ”ç©¶æ ·æœ¬é‡å°ä¸”ç¼ºä¹ä»£ç æ‰§è¡ŒéªŒè¯çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œå¹¶åŸºäºå¤§è§„æ¨¡æµç¨‹æ¨¡å‹æ•°æ®é›†æä¾›äº†å®è¯æ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿæµ‹è¯•äº†ä¸åŒç±»å‹å’Œè§„æ¨¡çš„LLMsåœ¨å¤„ç†æµç¨‹æµã€èµ„æºåˆ†é…ä»¥åŠåŸºäºæ•°æ®çš„æ¡ä»¶ç­‰æ ¸å¿ƒä¸šåŠ¡å±æ€§æ—¶çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsç”Ÿæˆçš„ä»£ç åœ¨å¯é æ€§ä¸Šä»æœªè¾¾åˆ°æ™ºèƒ½åˆçº¦å¼€å‘æ‰€éœ€çš„å®Œç¾æ ‡å‡†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å»ºè®®æœªæ¥çš„å·¥ä½œåº”èšç„¦äºå°†LLMsè´Ÿè´£ä»»åœ°é›†æˆåˆ°ç°æœ‰å·¥å…·ä¸­ï¼Œä»¥æå‡ç”Ÿæˆä»£ç çš„è´¨é‡ã€‚è¯¥ç ”ç©¶æå‡ºçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ä¸ºåç»­å¼€å‘å’Œè¯„ä¼°æ­¤ç±»é›†æˆç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at the Workshop on Distributed Ledger Technologies in Business Process Management, At the International Conference for Business Process Management (BPM), 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.23087v1",
      "published_date": "2025-07-30 20:39:45 UTC",
      "updated_date": "2025-07-30 20:39:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:38:37.451153+00:00"
    },
    {
      "arxiv_id": "2507.23084v1",
      "title": "AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads",
      "title_zh": "AutoIndexerï¼šé¢å‘è§„æ¨¡åŒ–å·¥ä½œè´Ÿè½½çš„å¼ºåŒ–å­¦ä¹ å¢å¼ºå‹ç´¢å¼•æ¨èå™¨",
      "authors": [
        "Taiyi Wang",
        "Eiko Yoneki"
      ],
      "abstract": "Efficiently selecting indexes is fundamental to database performance optimization, particularly for systems handling large-scale analytical workloads. While deep reinforcement learning (DRL) has shown promise in automating index selection through its ability to learn from experience, few works address how these RL-based index advisors can adapt to scaling workloads due to exponentially growing action spaces and heavy trial and error. To address these challenges, we introduce AutoIndexer, a framework that combines workload compression, query optimization, and specialized RL models to scale index selection effectively. By operating on compressed workloads, AutoIndexer substantially lowers search complexity without sacrificing much index quality. Extensive evaluations show that it reduces end-to-end query execution time by up to 95% versus non-indexed baselines. On average, it outperforms state-of-the-art RL-based index advisors by approximately 20% in workload cost savings while cutting tuning time by over 50%. These results affirm AutoIndexer's practicality for large and diverse workloads.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°æ®åº“ç´¢å¼•é€‰æ‹©åœ¨å¤„ç†å¤§è§„æ¨¡åˆ†æå‹ workload æ—¶é¢ä¸´çš„åŠ¨ä½œç©ºé—´æŒ‡æ•°çº§å¢é•¿å’Œé«˜æ˜‚å°è¯•æˆæœ¬ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† AutoIndexer æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆ workload å‹ç¼©ã€æŸ¥è¯¢ä¼˜åŒ–ä»¥åŠä¸“é—¨çš„ Reinforcement Learning æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„ç´¢å¼•é€‰æ‹©æ‰©å±•ã€‚AutoIndexer åœ¨å‹ç¼©åçš„ workload ä¸Šè¿è¡Œï¼Œèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½æœç´¢å¤æ‚åº¦çš„åŒæ—¶ä¿è¯ç´¢å¼•è´¨é‡ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿ DRL æ–¹æ³•åœ¨åº”å¯¹å¤§è§„æ¨¡è´Ÿè½½æ—¶çš„å±€é™æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸æ— ç´¢å¼•åŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å¯å°†ç«¯åˆ°ç«¯æŸ¥è¯¢æ‰§è¡Œæ—¶é—´é™ä½ 95%ã€‚æ­¤å¤–ï¼ŒAutoIndexer åœ¨è´Ÿè½½æˆæœ¬èŠ‚çœæ–¹é¢æ¯”ç°æœ‰å…ˆè¿›çš„åŸºäº RL çš„ç´¢å¼•é¡¾é—®é«˜å‡ºçº¦ 20%ï¼Œä¸”è°ƒä¼˜æ—¶é—´ç¼©çŸ­äº† 50% ä»¥ä¸Šã€‚è¿™äº›ç ”ç©¶æˆæœå……åˆ†è¯æ˜äº† AutoIndexer åœ¨å¤„ç†å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ– workload æ—¶çš„å®ç”¨ä»·å€¼ä¸å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.23084v1",
      "published_date": "2025-07-30 20:38:13 UTC",
      "updated_date": "2025-07-30 20:38:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:38:43.289231+00:00"
    },
    {
      "arxiv_id": "2507.23067v2",
      "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs",
      "title_zh": "FairReasonï¼šå¹³è¡¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†èƒ½åŠ›ä¸ç¤¾ä¼šåè§",
      "authors": [
        "Zhenyu Pan",
        "Yutong Zhang",
        "Jianshu Zhang",
        "Haoran Lu",
        "Haozheng Luo",
        "Yuwei Han",
        "Philip S. Yu",
        "Manling Li",
        "Han Liu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) already achieve state-of-the-art results across a wide range of tasks and modalities. To push their reasoning ability further, recent studies explore advanced prompting schemes and post-training fine-tuning. Although these techniques improve logical accuracy, they frequently leave the models' outputs burdened with pronounced social biases. Clarifying how reasoning gains interact with bias mitigation-and whether the two objectives inherently trade off-therefore remains an open and pressing research problem. Our study begins by benchmarking three bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation (KD), and rule-based reinforcement learning (RL)-under identical conditions, establishing their baseline strengths and weaknesses. Building on these results, we vary the proportion of debias-focused and reasoning-centric samples within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement learning cuts stereotype scores by 10% while retaining 88% of the model's original reasoning accuracy, offering concrete guidance for balancing fairness and capability in MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨æå‡æ¨ç†èƒ½åŠ›æ—¶ä¼´éšçš„ç¤¾ä¼šåè§(Social Bias)é—®é¢˜ï¼Œæ—¨åœ¨æ˜ç¡®æ¨ç†èƒ½åŠ›å¢ç›Šä¸åè§ç¼“è§£ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚ç ”ç©¶é¦–å…ˆåœ¨ç›¸åŒæ¡ä»¶ä¸‹å¯¹æ¯”äº†ç›‘ç£å¾®è°ƒ(SFT)ã€çŸ¥è¯†è’¸é¦(KD)å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ (RL)ä¸‰ç§å»åç­–ç•¥çš„åŸºå‡†è¡¨ç°ã€‚é€šè¿‡åœ¨ä¸åŒèŒƒå¼ä¸­è°ƒæ•´å»åæ ·æœ¬ä¸æ¨ç†æ ·æœ¬çš„æ¯”ä¾‹ï¼Œç ”ç©¶å‘ç°äº†ä¸€ä¸ªå¹³è¡¡æ¨ç†ä¸åè§çš„â€œé»„é‡‘ç‚¹â€ï¼Œå³åœ¨å¼ºåŒ–å­¦ä¹ (RL)æ¡†æ¶ä¸‹é‡‡ç”¨çº¦1:4çš„æ··åˆæ¯”ä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨é™ä½10%åˆ»æ¿å°è±¡åˆ†æ•°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™88%çš„æ¨¡å‹åŸå§‹æ¨ç†å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨MLLMsä¸­å¹³è¡¡å…¬å¹³æ€§(Fairness)ä¸æ¨¡å‹æ€§èƒ½æä¾›äº†å…·ä½“çš„æŒ‡å¯¼æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Trustworthy FMs workshop in ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.23067v2",
      "published_date": "2025-07-30 19:57:22 UTC",
      "updated_date": "2025-09-06 03:03:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:38:52.387800+00:00"
    },
    {
      "arxiv_id": "2507.23064v3",
      "title": "Vision-Language Cross-Attention for Real-Time Autonomous Driving",
      "title_zh": "é¢å‘å®æ—¶è‡ªåŠ¨é©¾é©¶çš„è§†è§‰-è¯­è¨€äº¤å‰æ³¨æ„åŠ›",
      "authors": [
        "Santosh Patapati",
        "Trisanth Srinivasan",
        "Murari Ambati"
      ],
      "abstract": "Autonomous cars need geometric accuracy and semantic understanding to navigate complex environments, yet most stacks handle them separately. We present XYZ-Drive, a single vision-language model that reads a front-camera frame, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs steering and speed. A lightweight goal-centered cross-attention layer lets waypoint tokens highlight relevant image and map patches, supporting both action and textual explanations, before the fused tokens enter a partially fine-tuned LLaMA-3.2 11B model. On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and 0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and halving collisions, all while significantly improving efficiency by using only a single branch. Sixteen ablations explain the gains. Removing any modality (vision, waypoint, map) drops success by up to 11%, confirming their complementary roles and rich connections. Replacing goal-centered attention with simple concatenation cuts 3% in performance, showing query-based fusion injects map knowledge more effectively. Keeping the transformer frozen loses 5%, showing the importance of fine-tuning when applying VLMs for specific tasks such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs lane edges and raises crash rate. Overall, these results demonstrate that early, token-level fusion of intent and map layout enables accurate, transparent, real-time driving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†XYZ-Driveï¼Œä¸€ç§ç”¨äºå®æ—¶è‡ªåŠ¨é©¾é©¶çš„å•ä¸€è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Model)ã€‚è¯¥æ¨¡å‹é€šè¿‡è½»é‡çº§çš„ä»¥ç›®æ ‡ä¸ºä¸­å¿ƒçš„äº¤å‰æ³¨æ„åŠ›å±‚(Goal-centered Cross-attention Layer)èåˆå‰è§†æ‘„åƒå¤´å›¾åƒã€ä¿¯ç°åœ°å›¾å’Œä¸‹ä¸€è·¯ç‚¹(Waypoint)ä¿¡æ¯ï¼Œå¹¶åŸºäºéƒ¨åˆ†å¾®è°ƒçš„LLaMA-3.2 11Bæ¨¡å‹ç›´æ¥è¾“å‡ºè½¬å‘å’Œé€Ÿåº¦ã€‚åœ¨MD-NEX Outdoor-DrivingåŸºå‡†æµ‹è¯•ä¸­ï¼ŒXYZ-Driveå®ç°äº†95%çš„æˆåŠŸç‡å’Œ0.80çš„è·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡(SPL)ï¼Œæ€§èƒ½æ¯”PhysNav-DGæå‡äº†15%å¹¶å¤§å¹…é™ä½äº†ç¢°æ’ç‡ã€‚æ¶ˆèå®éªŒè¯å®ï¼Œè§†è§‰ã€è·¯ç‚¹ä¸åœ°å›¾çš„å¤šæ¨¡æ€æ—©æœŸä»¤ç‰Œçº§èåˆ(Token-level Fusion)ä»¥åŠåŸºäºæŸ¥è¯¢çš„èåˆæœºåˆ¶ï¼Œå¯¹äºç¡®ä¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€é€æ˜åº¦å’Œå®æ—¶æ€§å…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.23064v3",
      "published_date": "2025-07-30 19:51:23 UTC",
      "updated_date": "2025-10-13 04:30:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:38:52.089994+00:00"
    },
    {
      "arxiv_id": "2507.23058v1",
      "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation",
      "title_zh": "ç”¨äºå¤šæ¨¡æ€åäº‹å®ç”Ÿæˆçš„å‚è€ƒå¼•å¯¼æ‰©æ•£ä¿®å¤",
      "authors": [
        "Alexandru Buburuzan"
      ],
      "abstract": "Safety-critical applications, such as autonomous driving and medical image analysis, require extensive multimodal data for rigorous testing. Synthetic data methods are gaining prominence due to the cost and complexity of gathering real-world data, but they demand a high degree of realism and controllability to be useful. This work introduces two novel methods for synthetic data generation in autonomous driving and medical image analysis, namely MObI and AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal Object Inpainting that leverages a diffusion model to produce realistic and controllable object inpaintings across perceptual modalities, demonstrated simultaneously for camera and lidar. Given a single reference RGB image, MObI enables seamless object insertion into existing multimodal scenes at a specified 3D location, guided by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, this approach uses 3D bounding box conditioning to ensure accurate spatial positioning and realistic scaling. AnydoorMed extends this paradigm to the medical imaging domain, focusing on reference-guided inpainting for mammography scans. It leverages a diffusion-based model to inpaint anomalies with impressive detail preservation, maintaining the reference anomaly's structural integrity while semantically blending it with the surrounding tissue. Together, these methods demonstrate that foundation models for reference-guided inpainting in natural images can be readily adapted to diverse perceptual modalities, paving the way for the next generation of systems capable of constructing highly realistic, controllable and multimodal counterfactual scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸¤ç§åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion Model)çš„æ–°å‹å‚è€ƒå¼•å¯¼ä¿®å¤æ–¹æ³•ï¼šMObIå’ŒAnydoorMedï¼Œæ—¨åœ¨ä¸ºè‡ªåŠ¨é©¾é©¶å’ŒåŒ»å­¦å½±åƒåˆ†æç­‰å®‰å…¨å…³é”®åº”ç”¨ç”Ÿæˆé«˜åº¦çœŸå®ä¸”å¯æ§çš„å¤šæ¨¡æ€åäº‹å®æ•°æ®ã€‚MObIä½œä¸ºé¦–ä¸ªå¤šæ¨¡æ€ç‰©ä½“ä¿®å¤(Multimodal Object Inpainting)æ¡†æ¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨å•ä¸ªå‚è€ƒRGBå›¾åƒå’Œ3Dè¾¹ç•Œæ¡†å¼•å¯¼ï¼Œåœ¨æŒ‡å®šç©ºé—´ä½ç½®å®ç°æ‘„åƒå¤´ä¸æ¿€å…‰é›·è¾¾(Lidar)æ•°æ®çš„åŒæ­¥æ— ç¼åµŒå…¥ã€‚è¯¥æ–¹æ³•é€šè¿‡3Dçº¦æŸç¡®ä¿äº†ç²¾ç¡®çš„ç©ºé—´å®šä½ä¸çœŸå®çš„æ¯”ä¾‹ç¼©æ”¾ï¼ŒåŒæ—¶ä¿æŒäº†è·¨æ¨¡æ€çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚AnydoorMedå°†è¯¥èŒƒå¼åº”ç”¨äºä¹³è…ºXå°„çº¿æ‘„å½±(Mammography)ï¼Œåœ¨ä¿ç•™å‚è€ƒç—…ç¶ç»“æ„å®Œæ•´æ€§çš„åŒæ—¶å®ç°äº†ä¸å‘¨å›´ç»„ç»‡çš„è¯­ä¹‰èåˆã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºè‡ªç„¶å›¾åƒçš„å‚è€ƒå¼•å¯¼ä¿®å¤æ¨¡å‹å¯æœ‰æ•ˆé€‚é…å¤šç§æ„ŸçŸ¥æ¨¡æ€ï¼Œä¸ºæ„å»ºé«˜ä¿çœŸã€å¯æ§çš„å¤šæ¨¡æ€åäº‹å®åœºæ™¯æä¾›äº†é€šç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "A dissertation submitted to The University of Manchester for the degree of Bachelor of Science in Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2507.23058v1",
      "published_date": "2025-07-30 19:43:47 UTC",
      "updated_date": "2025-07-30 19:43:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:02.487297+00:00"
    },
    {
      "arxiv_id": "2507.23042v2",
      "title": "Goal-Based Vision-Language Driving",
      "title_zh": "ç›®æ ‡é©±åŠ¨çš„è§†è§‰-è¯­è¨€é©¾é©¶",
      "authors": [
        "Santosh Patapati",
        "Trisanth Srinivasan"
      ],
      "abstract": "Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NovaDriveï¼Œè¿™æ˜¯ä¸€ç§å•åˆ†æ”¯çš„è§†è§‰è¯­è¨€(Vision-Language)æ¶æ„ï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨é©¾é©¶æ±½è½¦åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å®æ—¶ååº”èƒ½åŠ›å’Œæ¨ç†æ°´å¹³ã€‚è¯¥æ¶æ„åœ¨å•ä¸€åˆ†æ”¯ä¸­é›†æˆäº†å‰ç½®æ‘„åƒå¤´å›¾åƒã€é«˜ç²¾åœ°å›¾(HD-map)åˆ‡ç‰‡ã€LiDARæ·±åº¦å’Œæ–‡æœ¬è·¯å¾„ç‚¹(textual waypoints)ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§çš„ä¸¤é˜¶æ®µäº¤å‰æ³¨æ„åŠ›(cross-attention)æ¨¡å—å®ç°ç‰¹å¾å¯¹é½ä¸ç»†åŒ–ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æŠ‘åˆ¶çªå‘è½¬å‘å’Œé€Ÿåº¦å˜åŒ–çš„å¹³æ»‘æŸå¤±(smoothness loss)ï¼ŒNovaDriveæ¶ˆé™¤äº†å¯¹é€’å½’è®°å¿†(recurrent memory)çš„éœ€æ±‚ï¼Œå¹¶åœ¨å¾®è°ƒ11Bå‚æ•°çº§LLaMA-3.2æ¨¡å‹åå®ç°äº†å®æ—¶æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨MD-NEX OutdooråŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹å°†ä»»åŠ¡æˆåŠŸç‡æå‡è‡³84%ï¼Œå¹¶å°†ç¢°æ’é¢‘ç‡æ˜¾è‘—é™ä½è‡³1.2%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†è·¯å¾„ç‚¹ä»¤ç‰Œã€æ¨¡å‹éƒ¨åˆ†å¾®è°ƒåŠäº¤å‰æ³¨æ„åŠ›èåˆå¯¹æ€§èƒ½å¢é•¿çš„æ ¸å¿ƒè´¡çŒ®ã€‚NovaDriveä¸ä»…æå‡äº†é©¾é©¶å®‰å…¨æ€§ä¸è·¯å¾„æ•ˆç‡(SPL)ï¼Œè¿˜é€šè¿‡é™ä½èƒ½æºæ¶ˆè€—ä¸ºå¼€å‘æ›´è½»é‡åŒ–çš„é©¾é©¶ç³»ç»ŸåŠå…·èº«æ™ºèƒ½(embodied-AI)åº”ç”¨æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.23042v2",
      "published_date": "2025-07-30 19:12:42 UTC",
      "updated_date": "2025-10-13 04:53:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:00.690339+00:00"
    },
    {
      "arxiv_id": "2508.05660v1",
      "title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
      "title_zh": "é¢å‘ç§‘å­¦æ–‡çŒ®ç»¼è¿°çš„å¼€æºæ™ºèƒ½ä½“åŒ–æ··åˆ RAG æ¡†æ¶",
      "authors": [
        "Aditya Nagori",
        "Ricardo Accorsi Casonatto",
        "Ayush Gautam",
        "Abhinav Manikantha Sai Cheruvu",
        "Rishikesan Kamaleswaran"
      ],
      "abstract": "The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries with vector search offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates. We present an agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1) dynamically selecting between GraphRAG and VectorRAG for each query, (2) adapting instruction-tuned generation in real time to researcher needs, and (3) quantifying uncertainty during inference. This dynamic orchestration improves relevance, reduces hallucinations, and promotes reproducibility.\n  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2 model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking). Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics.\n  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned Agent with Direct Preference Optimization (DPO) outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery.",
      "tldr_zh": "é’ˆå¯¹ç§‘å­¦æ–‡çŒ®æ¿€å¢å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¼€æºçš„æ™ºèƒ½ä½“æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(Agentic Hybrid RAG)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿé™æ€ã€ä¾èµ–ç§æœ‰å·¥å…·ä¸”ç¼ºä¹ä¸ç¡®å®šæ€§ä¼°è®¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªä¸»æ™ºèƒ½ä½“åœ¨GraphRAGä¸VectorRAGä¹‹é—´è¿›è¡ŒåŠ¨æ€é€‰æ‹©ï¼Œç»“åˆäº†Neo4jçŸ¥è¯†å›¾è°±å’ŒFAISSå‘é‡åº“ï¼Œå¹¶åˆ©ç”¨Llama-3.3-70Bæ¨¡å‹å®ç°å®æ—¶çš„æŒ‡ä»¤å¾®è°ƒ(Instruction-Tuned)ç”Ÿæˆã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜æ¨ç†è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯å¤ç°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–(DPO)çš„æ™ºèƒ½ä½“åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡å¬å›ç‡(Context Recall)å’Œä¸Šä¸‹æ–‡ç²¾åº¦(Context Precision)æ–¹é¢è¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚è¯¥ç³»ç»Ÿå¢å¼ºäº†å¯¹å¼‚æ„æ•°æ®æºçš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–çš„ç§‘å­¦æ–‡çŒ®ç»¼è¿°å’Œç§‘å­¦å‘ç°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05660v1",
      "published_date": "2025-07-30 18:54:15 UTC",
      "updated_date": "2025-07-30 18:54:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:12.589884+00:00"
    },
    {
      "arxiv_id": "2507.23027v1",
      "title": "Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging",
      "title_zh": "æ¢å¤è¯Šæ–­ä»·å€¼ï¼šèµ„æºå—é™æˆåƒä¸­çš„è¶…åˆ†è¾¨ç‡è¾…åŠ©è¶…å£°å¿ƒåŠ¨å›¾åˆ†ç±»",
      "authors": [
        "Krishan Agyakari Raja Babu",
        "Om Prabhu",
        "Annu",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "Automated cardiac interpretation in resource-constrained settings (RCS) is often hindered by poor-quality echocardiographic imaging, limiting the effectiveness of downstream diagnostic models. While super-resolution (SR) techniques have shown promise in enhancing magnetic resonance imaging (MRI) and computed tomography (CT) scans, their application to echocardiography-a widely accessible but noise-prone modality-remains underexplored. In this work, we investigate the potential of deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS dataset, we stratify samples by image quality and evaluate two clinically relevant tasks of varying complexity: a relatively simple Two-Chamber vs. Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR models-Super-Resolution Generative Adversarial Network (SRGAN) and Super-Resolution Residual Network (SRResNet), to enhance poor-quality images and observe significant gains in performance metric-particularly with SRResNet, which also offers computational efficiency. Our findings demonstrate that SR can effectively recover diagnostic value in degraded echo scans, making it a viable tool for AI-assisted care in RCS, achieving more with less.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨èµ„æºå—é™ç¯å¢ƒï¼ˆResource-Constrained Settings, RCSï¼‰ä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨è¶…åˆ†è¾¨ç‡ï¼ˆSuper-Resolution, SRï¼‰æŠ€æœ¯æå‡ä½è´¨é‡è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆEchocardiographyï¼‰çš„è¯Šæ–­ä»·å€¼ã€‚ä½œè€…åŸºäºCAMUSæ•°æ®é›†ï¼Œåº”ç”¨äº†Super-Resolution Generative Adversarial Network (SRGAN)å’ŒSuper-Resolution Residual Network (SRResNet)ä¸¤ç§æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥å¢å¼ºé€€åŒ–çš„äºŒç»´è¶…å£°å›¾åƒã€‚å®éªŒé’ˆå¯¹äºŒè…”å¿ƒä¸å››è…”å¿ƒï¼ˆ2CH vs. 4CHï¼‰åˆ‡é¢åˆ†ç±»ä»¥åŠèˆ’å¼ æœ«æœŸä¸æ”¶ç¼©æœ«æœŸï¼ˆED vs. ESï¼‰æ—¶ç›¸åˆ†ç±»ä¸¤é¡¹ä»»åŠ¡è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSRæŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æé«˜ä½è´¨é‡å›¾åƒçš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå…¶ä¸­SRResNetåœ¨æå‡æ€§èƒ½çš„åŒæ—¶è¿˜å±•ç°å‡ºäº†æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†SRæŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆæ¢å¤å½±åƒä¸­çš„è¯Šæ–­ä¿¡æ¯ï¼Œä¸ºèµ„æºå—é™åœ°åŒºçš„AIè¾…åŠ©åŒ»ç–—æä¾›äº†ä¸€ç§å¯è¡Œä¸”é«˜æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the MICCAI Workshop on \"Medical Image Computing in Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.23027v1",
      "published_date": "2025-07-30 18:45:31 UTC",
      "updated_date": "2025-07-30 18:45:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:16.191848+00:00"
    },
    {
      "arxiv_id": "2507.23021v1",
      "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„äººç±»æ³¨è§†è¡Œä¸ºå»ºæ¨¡ä¸ç»Ÿä¸€æ‰«æè·¯å¾„é¢„æµ‹",
      "authors": [
        "Giuseppe Cartella",
        "Vittorio Cuculo",
        "Alessandro D'Amelio",
        "Marcella Cornia",
        "Giuseppe Boccignone",
        "Rita Cucchiara"
      ],
      "abstract": "Predicting human gaze scanpaths is crucial for understanding visual attention, with applications in human-computer interaction, autonomous systems, and cognitive robotics. While deep learning models have advanced scanpath prediction, most existing approaches generate averaged behaviors, failing to capture the variability of human visual exploration. In this work, we present ScanDiff, a novel architecture that combines diffusion models with Vision Transformers to generate diverse and realistic scanpaths. Our method explicitly models scanpath variability by leveraging the stochastic nature of diffusion models, producing a wide range of plausible gaze trajectories. Additionally, we introduce textual conditioning to enable task-driven scanpath generation, allowing the model to adapt to different visual search objectives. Experiments on benchmark datasets show that ScanDiff surpasses state-of-the-art methods in both free-viewing and task-driven scenarios, producing more diverse and accurate scanpaths. These results highlight its ability to better capture the complexity of human visual behavior, pushing forward gaze prediction research. Source code and models are publicly available at https://aimagelab.github.io/ScanDiff.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰äººçœ¼æ³¨è§†æ‰«æè·¯å¾„(Scanpath Prediction)æ¨¡å‹éš¾ä»¥æ•æ‰äººç±»è§†è§‰æ¢ç´¢å˜å¼‚æ€§(Variability)ä¸”å€¾å‘äºç”Ÿæˆå¹³å‡åŒ–è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†ScanDiffæ¡†æ¶ã€‚è¯¥æ¶æ„åˆ›æ–°æ€§åœ°å°†æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¸è§†è§‰è½¬æ¢å™¨(Vision Transformers)ç›¸ç»“åˆï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„éšæœºæ€§(Stochastic Nature)æ˜¾å¼å»ºæ¨¡æ‰«æè·¯å¾„çš„å˜å¼‚æ€§ï¼Œä»è€Œç”Ÿæˆä¸€ç³»åˆ—å¤šæ ·ä¸”çœŸå®çš„äººçœ¼æ³¨è§†è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†æ–‡æœ¬æ¡ä»¶åŒ–(Textual Conditioning)æœºåˆ¶ä»¥æ”¯æŒä»»åŠ¡é©±åŠ¨(Task-driven)çš„æ‰«æè·¯å¾„ç”Ÿæˆï¼Œä½¿å…¶èƒ½æ ¹æ®ç‰¹å®šè§†è§‰æœç´¢ç›®æ ‡è‡ªé€‚åº”è°ƒæ•´ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒScanDiffåœ¨è‡ªç”±æŸ¥çœ‹(Free-viewing)å’Œä»»åŠ¡é©±åŠ¨åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(State-of-the-art)æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å¯¹äººç±»è§†è§‰è¡Œä¸ºå¤æ‚æ€§çš„æ•æ‰èƒ½åŠ›ã€‚ç›®å‰ï¼Œè¯¥é¡¹ç›®çš„æºä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä¸ºç†è§£äººç±»è§†è§‰æ³¨æ„åŠ›æä¾›äº†æ–°çš„ç ”ç©¶å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.23021v1",
      "published_date": "2025-07-30 18:36:09 UTC",
      "updated_date": "2025-07-30 18:36:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:11.685107+00:00"
    },
    {
      "arxiv_id": "2507.23018v1",
      "title": "Data Readiness for Scientific AI at Scale",
      "title_zh": "é¢å‘å¤§è§„æ¨¡ç§‘å­¦äººå·¥æ™ºèƒ½çš„æ•°æ®å°±ç»ªåº¦",
      "authors": [
        "Wesley Brewer",
        "Patrick Widener",
        "Valentine Anantharaj",
        "Feiyi Wang",
        "Tom Beck",
        "Arjun Shankar",
        "Sarp Oral"
      ],
      "abstract": "This paper examines how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets used to train foundation models. We analyze archetypal workflows across four representative domains - climate, nuclear fusion, bio/health, and materials - to identify common preprocessing patterns and domain-specific constraints. We introduce a two-dimensional readiness framework composed of Data Readiness Levels (raw to AI-ready) and Data Processing Stages (ingest to shard), both tailored to high performance computing (HPC) environments. This framework outlines key challenges in transforming scientific data for scalable AI training, emphasizing transformer-based generative models. Together, these dimensions form a conceptual maturity matrix that characterizes scientific data readiness and guides infrastructure development toward standardized, cross-domain support for scalable and reproducible AI for science.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½æ•°æ®å°±ç»ªæ€§(Data Readiness for AI)åŸåˆ™åœ¨è®­ç»ƒåŸºç¡€æ¨¡å‹(foundation models)çš„å¤§è§„æ¨¡ç§‘å­¦æ•°æ®é›†ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶é€šè¿‡åˆ†ææ°”å€™ã€æ ¸èšå˜ã€ç”Ÿç‰©å¥åº·å’Œææ–™å››ä¸ªä»£è¡¨æ€§é¢†åŸŸçš„åŸå‹å·¥ä½œæµï¼Œè¯†åˆ«äº†é€šç”¨çš„é¢„å¤„ç†æ¨¡å¼å’Œé¢†åŸŸç‰¹å®šçº¦æŸã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä¸“ä¸ºé«˜æ€§èƒ½è®¡ç®—(HPC)ç¯å¢ƒè®¾è®¡çš„äºŒç»´å°±ç»ªæ¡†æ¶ï¼Œæ¶µç›–äº†ä»åŸå§‹æ•°æ®åˆ°AIå°±ç»ªçš„æ•°æ®å°±ç»ªæ°´å¹³(Data Readiness Levels)ä»¥åŠä»æ•°æ®æ‘„å–åˆ°åˆ†ç‰‡çš„æ•°æ®å¤„ç†é˜¶æ®µ(Data Processing Stages)ã€‚è¯¥æ¡†æ¶é‡ç‚¹åˆ†æäº†ä¸ºTransformerç”Ÿæˆå¼æ¨¡å‹è½¬æ¢ç§‘å­¦æ•°æ®æ—¶çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ¦‚å¿µæˆç†Ÿåº¦çŸ©é˜µä»¥è¡¡é‡æ•°æ®çŠ¶æ€ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºç§‘å­¦AIçš„åŸºç¡€è®¾æ–½å¼€å‘æä¾›äº†æ ‡å‡†åŒ–æŒ‡å¯¼ï¼Œæ—¨åœ¨æ”¯æŒè·¨é¢†åŸŸã€å¯æ‰©å±•ä¸”å¯é‡å¤çš„äººå·¥æ™ºèƒ½ç§‘å­¦ç ”ç©¶(AI for Science)ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 1 figure, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.23018v1",
      "published_date": "2025-07-30 18:30:37 UTC",
      "updated_date": "2025-07-30 18:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:46.361312+00:00"
    },
    {
      "arxiv_id": "2507.23010v1",
      "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods",
      "title_zh": "æ¢ç©¶å¤šæ¨¡æ€æ½œç©ºé—´çš„å¯é€†æ€§ï¼šåŸºäºä¼˜åŒ–æ–¹æ³•çš„å±€é™æ€§",
      "authors": [
        "Siwoo Park"
      ],
      "abstract": "This paper investigates the inverse capabilities and broader utility of multimodal latent spaces within task-specific AI (Artificial Intelligence) models. While these models excel at their designed forward tasks (e.g., text-to-image generation, audio-to-text transcription), their potential for inverse mappings remains largely unexplored. We propose an optimization-based framework to infer input characteristics from desired outputs, applying it bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio (Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models towards inverse tasks, their multimodal latent spaces will not consistently support semantically meaningful and perceptually coherent inverse mappings. Experimental results consistently validate this hypothesis. We demonstrate that while optimization can force models to produce outputs that align textually with targets (e.g., a text-to-image model generating an image that an image captioning model describes correctly, or an ASR model transcribing optimized audio accurately), the perceptual quality of these inversions is chaotic and incoherent. Furthermore, when attempting to infer the original semantic input from generative models, the reconstructed latent space embeddings frequently lack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces, primarily optimized for specific forward tasks, do not inherently possess the structure required for robust and interpretable inverse mappings. Our work underscores the need for further research into developing truly semantically rich and invertible multimodal latent spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»»åŠ¡ä¸“ç”¨äººå·¥æ™ºèƒ½(AI)æ¨¡å‹ä¸­å¤šæ¨¡æ€æ½œç©ºé—´(multimodal latent spaces)çš„å¯é€†èƒ½åŠ›åŠå…¶å¹¿æ³›ç”¨é€”ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»æœŸæœ›çš„è¾“å‡ºä¸­æ¨æ–­è¾“å…¥ç‰¹å¾ï¼Œå¹¶åœ¨æ–‡æœ¬-å›¾åƒ(Text-Image)å’Œæ–‡æœ¬-éŸ³é¢‘(Text-Audio)åŒå‘æ¨¡æ€ä¸Šè¿›è¡Œäº†åº”ç”¨ï¼Œæ¶µç›–äº†BLIPã€Flux.1-devã€Whisper-Large-V3å’ŒChatterbox-TTSç­‰æ¨¡å‹ã€‚å®éªŒç»“æœè¯å®ï¼Œè™½ç„¶ä¼˜åŒ–å¯ä»¥å¼•å¯¼æ¨¡å‹æ‰§è¡Œé€†å‘ä»»åŠ¡ï¼Œä½†å…¶å¤šæ¨¡æ€æ½œç©ºé—´æ— æ³•æŒç»­æ”¯æŒè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰ä¸”æ„ŸçŸ¥ä¸€è‡´çš„é€†å‘æ˜ å°„ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ç”Ÿæˆçš„è¾“å‡ºåœ¨æ–‡æœ¬ä¸Šèƒ½ä¸ç›®æ ‡å¯¹é½ï¼Œä½†å…¶æ„ŸçŸ¥è´¨é‡å¾€å¾€æ··ä¹±ä¸”ä¸è¿è´¯ï¼Œä¸”é‡å»ºçš„æ½œç©ºé—´åµŒå…¥(latent space embeddings)ç»å¸¸è¡¨ç°ä¸ºæ— æ„ä¹‰çš„è¯è¡¨æ ‡è®°ï¼Œç¼ºä¹è¯­ä¹‰å¯è§£é‡Šæ€§ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†ç°æœ‰æ¨¡å‹ä¸»è¦ä¸ºç‰¹å®šæ­£å‘ä»»åŠ¡ä¼˜åŒ–ï¼Œå¹¶ä¸å…·å¤‡é²æ£’å’Œå¯è§£é‡Šé€†å‘æ˜ å°„æ‰€éœ€çš„ç»“æ„ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†å¼€å‘çœŸæ­£è¯­ä¹‰ä¸°å¯Œä¸”å¯é€†çš„å¤šæ¨¡æ€æ½œç©ºé—´æ˜¯æœªæ¥ç ”ç©¶çš„å…³é”®æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.23010v1",
      "published_date": "2025-07-30 18:19:11 UTC",
      "updated_date": "2025-07-30 18:19:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:33.299997+00:00"
    },
    {
      "arxiv_id": "2507.23009v1",
      "title": "Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead",
      "title_zh": "åœæ­¢ä½¿ç”¨äººç±»æµ‹è¯•è¯„ä¼°äººå·¥æ™ºèƒ½ï¼šå»ºç«‹è§„èŒƒåŒ–çš„ AI ä¸“ç”¨è¯„ä¼°ä½“ç³»",
      "authors": [
        "Tom SÃ¼hr",
        "Florian E. Dorner",
        "Olawale Salaudeen",
        "Augustin Kelava",
        "Samira Samadi"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable results on a range of standardized tests originally designed to assess human cognitive and psychological traits, such as intelligence and personality. While these results are often interpreted as strong evidence of human-like characteristics in LLMs, this paper argues that such interpretations constitute an ontological error. Human psychological and educational tests are theory-driven measurement instruments, calibrated to a specific human population. Applying these tests to non-human subjects without empirical validation, risks mischaracterizing what is being measured. Furthermore, a growing trend frames AI performance on benchmarks as measurements of traits such as ``intelligence'', despite known issues with validity, data contamination, cultural bias and sensitivity to superficial prompt changes. We argue that interpreting benchmark performance as measurements of human-like traits, lacks sufficient theoretical and empirical justification. This leads to our position: Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead. We call for the development of principled, AI-specific evaluation frameworks tailored to AI systems. Such frameworks might build on existing frameworks for constructing and validating psychometrics tests, or could be created entirely from scratch to fit the unique context of AI.",
      "tldr_zh": "è¯¥è®ºæ–‡æŒ‡å‡ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŸæœ¬ä¸ºäººç±»è®¾è®¡çš„å¿ƒç†ä¸è®¤çŸ¥æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å°†å…¶è§£è¯»ä¸ºç±»äººç‰¹å¾æ˜¯ä¸€ç§æœ¬ä½“è®ºé”™è¯¯(ontological error)ã€‚ä½œè€…è®¤ä¸ºï¼Œäººç±»æµ‹è¯•æ˜¯é’ˆå¯¹ç‰¹å®šäººç±»ç¾¤ä½“æ ¡å‡†çš„ç†è®ºé©±åŠ¨å·¥å…·ï¼Œåœ¨ç¼ºä¹å®è¯éªŒè¯çš„æƒ…å†µä¸‹ç›´æ¥åº”ç”¨äºéäººç±»ä¸»ä½“ï¼Œææ˜“å¯¼è‡´æµ‹é‡ç»“æœçš„è¯¯åˆ¤ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†å½“å‰å°†AIåœ¨åŸºå‡†æµ‹è¯•(benchmarks)ä¸Šçš„è¡¨ç°è§†ä¸ºâ€œæ™ºèƒ½â€ç­‰ç‰¹è´¨çš„è¶‹åŠ¿ï¼Œæ­ç¤ºäº†å…¶ä¸­å­˜åœ¨çš„æ•ˆåº¦(validity)ã€æ•°æ®æ±¡æŸ“(data contamination)åŠæ–‡åŒ–åè§ç­‰æ·±å±‚é—®é¢˜ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡ä¸»å¼ åœæ­¢ä½¿ç”¨äººç±»æµ‹è¯•æ¥è¯„ä¼°AIï¼Œå‘¼åå¼€å‘ä¸“é—¨é’ˆå¯¹AIç³»ç»Ÿè®¾è®¡çš„ã€å…·æœ‰ç§‘å­¦åŸåˆ™çš„è¯„ä¼°æ¡†æ¶(AI-specific evaluation frameworks)ã€‚è¿™ç§æ–°æ¡†æ¶åº”ç»“åˆå¿ƒç†æµ‹é‡å­¦çš„æ„å»ºåŸåˆ™æˆ–æ ¹æ®AIçš„ç‹¬ç‰¹è¯­å¢ƒé‡æ–°åˆ›å»ºï¼Œä»¥ç¡®ä¿è¯„ä¼°è¿‡ç¨‹å…·å¤‡å……åˆ†çš„ç†è®ºæ”¯æ’‘ä¸å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.23009v1",
      "published_date": "2025-07-30 18:14:35 UTC",
      "updated_date": "2025-07-30 18:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:45.310185+00:00"
    },
    {
      "arxiv_id": "2507.22887v1",
      "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning",
      "title_zh": "æç¤ºè¯ä¸­ç¤ºä¾‹çš„å‘ˆç°ä½ç½®ï¼šä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„ä½ç½®åå·®",
      "authors": [
        "Kwesi Cobbina",
        "Tianyi Zhou"
      ],
      "abstract": "In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning, ICL)ä¸­å­˜åœ¨çš„ä¸€ç§æœªè¢«æ¢ç´¢çš„ä½ç½®åå·®ï¼Œå³ç¤ºä¾‹åœ¨æç¤ºè¯ä¸­çš„ä½ç½®(DEMOS' POSITION IN PROMPT, DPP)åå·®ã€‚ç ”ç©¶å‘ç°ï¼Œå½“ç¤ºä¾‹ã€ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æ¶ˆæ¯åœ¨è¾“å…¥ä¸­çš„ç›¸å¯¹ä½ç½®å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ¨¡å‹çš„é¢„æµ‹ç»“æœå’Œå‡†ç¡®ç‡ä¼šäº§ç”Ÿå‰§çƒˆæ¼‚ç§»ã€‚ä½œè€…è®¾è®¡äº†ä¸€å¥—ç³»ç»Ÿè¯„ä¼°æµç¨‹ï¼Œæ¶µç›–åˆ†ç±»ã€é—®ç­”ã€æ‘˜è¦å’Œæ¨ç†ä»»åŠ¡ï¼Œå¹¶å¼•å…¥ACCURACY-CHANGEå’ŒPREDICTION-CHANGEä¸¤ä¸ªæŒ‡æ ‡æ¥é‡åŒ–ä½ç½®å˜åŒ–å¸¦æ¥çš„å¢ç›Šå’Œè¾“å‡ºæ³¢åŠ¨ã€‚é’ˆå¯¹æ¥è‡ªQWENã€LLAMA3ã€MISTRALå’ŒCOHEREå››ä¸ªå¼€æºå®¶æ—çš„10ä¸ªæ¨¡å‹è¿›è¡Œçš„å¹¿æ³›å®éªŒè¯å®ï¼Œè¿™ç§åå·®æ˜¾è‘—å½±å“äº†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†ç¤ºä¾‹ç½®äºæç¤ºè¯å¼€å¤´èƒ½äº§ç”Ÿæœ€ç¨³å®šä¸”å‡†ç¡®çš„è¾“å‡ºï¼Œæœ€é«˜å¯å¸¦æ¥6ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡æå‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°†ç¤ºä¾‹æ”¾åœ¨ç”¨æˆ·æ¶ˆæ¯æœ«å°¾ä¼šåœ¨ä¸æé«˜å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å¯¼è‡´è¶…è¿‡30%çš„é¢„æµ‹ç»“æœå‘ç”Ÿç¿»è½¬ã€‚è™½ç„¶è¾ƒå°çš„æ¨¡å‹å¯¹è¿™ç§æ•æ„Ÿæ€§ååº”æœ€ä¸ºå‰§çƒˆï¼Œä½†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¤§å‹æ¨¡å‹ä¾ç„¶ä¼šå—åˆ°è¯¥ä½ç½®åå·®çš„å½±å“ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22887v1",
      "published_date": "2025-07-30 17:59:46 UTC",
      "updated_date": "2025-07-30 17:59:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:39.046292+00:00"
    },
    {
      "arxiv_id": "2507.22968v3",
      "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations",
      "title_zh": "C3ï¼šæ¢ç©¶å¤æ‚å¯¹è¯æŒ‘æˆ˜çš„è¯­éŸ³å¯¹è¯æ¨¡å‹åŒè¯­è¯„æµ‹åŸºå‡†",
      "authors": [
        "Chengqian Ma",
        "Wei Tao",
        "Yiwen Guo"
      ],
      "abstract": "Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³å¯¹è¯æ¨¡å‹ (Spoken Dialogue Models, SDMs) åœ¨ç†è§£å’Œæ¨¡æ‹Ÿäººç±»å¯¹è¯å¤æ‚æ€§æ–¹é¢ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º C3 çš„åŒè¯­åŸºå‡†æµ‹è¯•é›†ã€‚C3 ç‰¹åˆ«å…³æ³¨è¯­éŸ³äº¤äº’ä¸­ç‰¹æœ‰çš„å¤æ‚æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”±å¤šä¹‰è¯ (polysemy) å¯¼è‡´çš„è¯­ä¹‰æ­§ä¹‰ï¼Œä»¥åŠç”±å¼‚å½¢åŒéŸ³è¯ (heterograph)ã€å¼‚éŸ³åŒå½¢è¯ (heteronyms) å’Œé‡éŸ³æ¨¡å¼ (stress patterns) å¼•èµ·çš„éŸ³éŸµæ­§ä¹‰ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†è¿˜æ¶µç›–äº†å¯¹è¯åŠ¨æ€ä¸­çš„ä¸Šä¸‹æ–‡ä¾èµ–æ€§ï¼Œå¦‚çœç•¥ (omission)ã€æŒ‡ä»£ (coreference) å’Œå¤šè½®äº¤äº’ (multi-turn interaction) ç­‰éš¾é¢˜ã€‚æ•°æ®é›†å…±åŒ…å« 1,079 ä¸ªä¸­è‹±æ–‡å®ä¾‹ï¼Œå¹¶é…å¥—æä¾›äº†ä¸€ç§ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM-based) çš„è¯„ä¼°æ–¹æ³•ã€‚è¿™ä¸€å·¥å…·èƒ½å¤Ÿå…¨é¢è¯„ä¼°å¹¶é˜æ˜å½“å‰ SDMs åœ¨å¤„ç†å®é™…è¯­éŸ³å¯¹è¯æŒ‘æˆ˜ä¸­çš„æ€§èƒ½ç°çŠ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 main; Project Page: https://step-out.github.io/C3-web/",
      "pdf_url": "https://arxiv.org/pdf/2507.22968v3",
      "published_date": "2025-07-30 17:56:23 UTC",
      "updated_date": "2025-10-05 11:17:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:49.489013+00:00"
    },
    {
      "arxiv_id": "2507.22876v1",
      "title": "Automatically discovering heuristics in a complex SAT solver with large language models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ SAT æ±‚è§£å™¨ä¸­è‡ªåŠ¨å‘ç°å¯å‘å¼ç­–ç•¥",
      "authors": [
        "Yiwen Sun",
        "Furong Ye",
        "Zhihan Chen",
        "Ke Wei",
        "Shaowei Cai"
      ],
      "abstract": "Satisfiability problem (SAT) is a cornerstone of computational complexity with broad industrial applications, and it remains challenging to optimize modern SAT solvers in real-world settings due to their intricate architectures. While automatic configuration frameworks have been developed, they rely on manually constrained search spaces and yield limited performance gains. This work introduces a novel paradigm which effectively optimizes complex SAT solvers via Large Language Models (LLMs), and a tool called AutoModSAT is developed. Three fundamental challenges are addressed in order to achieve superior performance: (1) LLM-friendly solver: Systematic guidelines are proposed for developing a modularized solver to meet LLMs' compatibility, emphasizing code simplification, information share and bug reduction; (2) Automatic prompt optimization: An unsupervised automatic prompt optimization method is introduced to advance the diversity of LLMs' output; (3) Efficient search strategy: We design a presearch strategy and an EA evolutionary algorithm for the final efficient and effective discovery of heuristics. Extensive experiments across a wide range of datasets demonstrate that AutoModSAT achieves 50% performance improvement over the baseline solver and achieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover, AutoModSAT attains a 20% speedup on average compared to parameter-tuned alternatives of the SOTA solvers, showcasing the enhanced capability in handling complex problem instances. This work bridges the gap between AI-driven heuristics discovery and mission-critical system optimization, and provides both methodological advancements and empirically validated results for next-generation complex solver development.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†AutoModSATï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚SATæ±‚è§£å™¨ä¸­è‡ªåŠ¨å‘ç°å¯å‘å¼ç®—æ³•çš„æ–°èŒƒå¼ã€‚é’ˆå¯¹ç°ä»£SATæ±‚è§£å™¨æ¶æ„å¤æ‚ã€äººå·¥é…ç½®æå‡æœ‰é™çš„æŒ‘æˆ˜ï¼Œè¯¥å·¥ä½œåˆ¶å®šäº†å¼€å‘æ¨¡å—åŒ–ä¸”LLMå‹å¥½å‹æ±‚è§£å™¨çš„ç³»ç»Ÿæ€§å‡†åˆ™ï¼Œä»¥ç¡®ä¿ä»£ç ç®€åŒ–å¹¶å‡å°‘æ¼æ´ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§æ— ç›‘ç£çš„è‡ªåŠ¨æç¤ºä¼˜åŒ–(Automatic prompt optimization)æ–¹æ³•æ¥å¢åŠ æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œå¹¶è®¾è®¡äº†ç»“åˆé¢„æœç´¢(Presearch)ä¸è¿›åŒ–ç®—æ³•(EA)çš„é«˜æ•ˆæœç´¢ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒAutoModSATåœ¨æ€§èƒ½ä¸Šæ¯”åŸºçº¿æ±‚è§£å™¨æå‡äº†50%ï¼Œå¹¶æ¯”å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ±‚è§£å™¨ä¼˜å‡º30%ã€‚æ­¤å¤–ï¼Œä¸å‚æ•°è°ƒä¼˜åçš„SOTAæ±‚è§£å™¨ç›¸æ¯”ï¼Œå®ƒå®ç°äº†å¹³å‡20%çš„åŠ é€Ÿï¼Œæ˜¾è‘—å¢å¼ºäº†å¤„ç†å¤æ‚é—®é¢˜å®ä¾‹çš„èƒ½åŠ›ã€‚è¯¥å·¥ä½œæœ‰æ•ˆå¼¥åˆäº†AIé©±åŠ¨çš„å¯å‘å¼ç®—æ³•å‘ç°ä¸å…³é”®ä»»åŠ¡ç³»ç»Ÿä¼˜åŒ–ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºä¸‹ä¸€ä»£å¤æ‚æ±‚è§£å™¨çš„å¼€å‘æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22876v1",
      "published_date": "2025-07-30 17:52:25 UTC",
      "updated_date": "2025-07-30 17:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:39:46.550936+00:00"
    },
    {
      "arxiv_id": "2507.22854v2",
      "title": "A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model",
      "title_zh": "äº›è®¸è‡ªç”±ï¼Œå¤§æœ‰å¯ä¸ºï¼šç”Ÿæˆæ¨¡å‹ä¸‹çš„å¼ºåŒ–å­¦ä¹ ç»å…¸ä¸é‡å­ç®—æ³•",
      "authors": [
        "Andris Ambainis",
        "Joao F. Doriguello",
        "Debbie Lim"
      ],
      "abstract": "We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a \"simulator\". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like \"optimism in the face of uncertainty\" and \"posterior sampling\" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\\operatorname{poly}\\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ‰é™æ­¥é•¿å’Œæ— é™æ­¥é•¿å¹³å‡å¥–åŠ±çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Processes, MDPs)ï¼Œæå‡ºäº†å…¨æ–°çš„ç»å…¸ä¸é‡å­åœ¨çº¿å­¦ä¹ ç®—æ³•ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ··åˆæ¢ç´¢-ç”Ÿæˆå¼ºåŒ–å­¦ä¹ (hybrid exploration-generative reinforcement learning)æ¨¡å‹ï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨äº¤äº’ä¸­é€‚æ—¶åˆ©ç”¨æ¨¡æ‹Ÿå™¨è¿›è¡Œç”Ÿæˆå¼é‡‡æ ·ã€‚é€šè¿‡ç›´æ¥è®¡ç®—å¹¶ä½¿ç”¨æœ€ä¼˜ç­–ç•¥ï¼Œè¯¥æ–¹æ³•é¿å¼€äº†å¼ºåŒ–å­¦ä¹ ä¸­å¸¸è§çš„â€œä¸ç¡®å®šæ€§ä¸­çš„ä¹è§‚(optimism in the face of uncertainty)â€å’Œâ€œåéªŒé‡‡æ ·(posterior sampling)â€ç­‰ä¼ ç»ŸèŒƒå¼ï¼Œä»è€Œè·å¾—äº†æ›´ä¼˜çš„ç´¯ç§¯é—æ†¾(regret)ç•Œé™ã€‚åœ¨æœ‰é™æ­¥é•¿MDPsä¸­ï¼Œé‡å­ç®—æ³•å®ç°äº†å¯¹æ—¶é—´æ­¥é•¿ $T$ çš„å¯¹æ•°çº§é—æ†¾ï¼Œçªç ´äº† $O(\\sqrt{T})$ çš„ç»å…¸æ€§èƒ½ç“¶é¢ˆã€‚é’ˆå¯¹æ— é™æ­¥é•¿MDPsï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹é—æ†¾åº¦é‡æŒ‡æ ‡ï¼Œä½¿é‡å­ç®—æ³•å±•ç°å‡ºæ¯”ç»å…¸ç®—æ³•æŒ‡æ•°çº§æ›´ä¼˜çš„ $\\operatorname{poly}\\log{T}$ é—æ†¾ï¼Œå¹¶æˆåŠŸå°†ç»“è®ºæ¨å¹¿è‡³ç´§å‡‘çŠ¶æ€ç©ºé—´(compact state spaces)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "quant-ph",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "57 pages. v2: corrected a small typo in the statement of Result 1 and added references",
      "pdf_url": "https://arxiv.org/pdf/2507.22854v2",
      "published_date": "2025-07-30 17:24:23 UTC",
      "updated_date": "2025-08-11 03:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:17.560125+00:00"
    },
    {
      "arxiv_id": "2507.22853v1",
      "title": "Repair-R1: Better Test Before Repair",
      "title_zh": "Repair-R1ï¼šå…ˆæµ‹è¯•åä¿®å¤æ•ˆæœæ›´ä½³",
      "authors": [
        "Haichuan Hu",
        "Xiaochen Xie",
        "Quanjun Zhang"
      ],
      "abstract": "APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Repair-R1ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ”¹è¿›è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤(Automated Program Repair, APR)çš„æ–°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šå¸¸åœ¨ä¿®å¤åæ‰ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯çš„å±€é™æ€§ï¼ŒRepair-R1åˆ›æ–°æ€§åœ°å°†æµ‹è¯•ç”¨ä¾‹å¼•å…¥æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µï¼Œå¹¶æå€¡åœ¨ä¿®å¤ä»»åŠ¡æ‰§è¡Œå‰å…ˆè¡Œç”Ÿæˆæµ‹è¯•ã€‚è¯¥æ¨¡å‹è¢«è¦æ±‚é¦–å…ˆç”Ÿæˆèƒ½å¤Ÿè¯†åˆ«ç¼ºé™·è¡Œä¸ºçš„åˆ¤åˆ«æ€§æµ‹è¯•ç”¨ä¾‹(test cases)ï¼ŒéšååŸºäºè¿™äº›æµ‹è¯•æŒ‡å¯¼ä¿®å¤è¿‡ç¨‹ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹ç¼ºé™·å®šä½åŠæˆå› çš„ç†è§£ã€‚é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ååŒä¼˜åŒ–æµ‹è¯•ç”Ÿæˆä¸ç¼ºé™·ä¿®å¤ï¼ŒRepair-R1åœ¨å››ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šå‡å±•ç°äº†æ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRepair-R1åœ¨ä¿®å¤æˆåŠŸç‡ã€æµ‹è¯•ç”ŸæˆæˆåŠŸç‡åŠæµ‹è¯•è¦†ç›–ç‡æ–¹é¢è¾ƒåŸºå‡†æ¨¡å‹å‡æœ‰å¤§å¹…æå‡ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„ç¨‹åºä¿®å¤ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22853v1",
      "published_date": "2025-07-30 17:24:05 UTC",
      "updated_date": "2025-07-30 17:24:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:03.541735+00:00"
    },
    {
      "arxiv_id": "2507.22847v1",
      "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology",
      "title_zh": "ä¸å®Œæ•´çš„æ¡¥æ¢ï¼šäººå·¥æ™ºèƒ½ç ”ç©¶å¦‚ä½•ä¸å¿ƒç†å­¦ï¼ˆè¯¯ï¼‰æ¥è½¨",
      "authors": [
        "Han Jiang",
        "Pengda Wang",
        "Xiaoyuan Yi",
        "Xing Xie",
        "Ziang Xiao"
      ],
      "abstract": "Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ (AI) ä¸å¿ƒç†å­¦ (Psychology) ä¹‹é—´çš„è·¨å­¦ç§‘ååŒä½œç”¨ï¼Œç³»ç»Ÿåˆ†æäº† 2023 å¹´è‡³ 2025 å¹´é—´é¡¶çº§ AI ä¼šè®®å‘è¡¨çš„ 1,006 ç¯‡å¤§è¯­è¨€æ¨¡å‹ (LLM) ç›¸å…³è®ºæ–‡åŠå…¶å¼•ç”¨çš„ 2,544 ç¯‡å¿ƒç†å­¦æ–‡çŒ®ã€‚ç ”ç©¶è¯†åˆ«äº†è·¨å­¦ç§‘æ•´åˆçš„å…³é”®æ¨¡å¼ï¼Œæ˜ç¡®äº† AI é¢†åŸŸé«˜é¢‘å¼•ç”¨çš„å¿ƒç†å­¦èŒƒç•´ï¼Œå¹¶æŒ‡å‡ºäº†ç›®å‰å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ç§‘ç ”ç©ºç™½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ·±å…¥è€ƒå¯Ÿäº†å¿ƒç†å­¦ç†è®ºå’Œæ¡†æ¶åœ¨ AI è¯­å¢ƒä¸‹çš„æ“ä½œåŒ–ä¸è§£è¯»æ–¹å¼ï¼Œæ­ç¤ºäº†å¸¸è§çš„è¯¯ç”¨ç±»å‹å¹¶æä¾›äº†æ›´æœ‰æ•ˆçš„æ•´åˆæŒ‡å—ã€‚è¿™é¡¹å·¥ä½œä¸º AI ä¸å¿ƒç†å­¦çš„äº’åŠ¨æä¾›äº†å…¨é¢çš„å­¦ç§‘åœ°å›¾ï¼Œæ—¨åœ¨é€šè¿‡æ·±åº¦çš„è·¨å­¦ç§‘åä½œæ¨åŠ¨ AI ç³»ç»Ÿçš„æŒç»­è¿›åŒ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22847v1",
      "published_date": "2025-07-30 17:03:59 UTC",
      "updated_date": "2025-07-30 17:03:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:04.655519+00:00"
    },
    {
      "arxiv_id": "2507.22844v1",
      "title": "RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents",
      "title_zh": "RLVMRï¼šåŸºäºå¯éªŒè¯å…ƒæ¨ç†å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æ„å»ºé²æ£’çš„é•¿ç¨‹æ™ºèƒ½ä½“",
      "authors": [
        "Zijing Zhang",
        "Ziyang Chen",
        "Mingxiao Li",
        "Zhaopeng Tu",
        "Xiaolong Li"
      ],
      "abstract": "The development of autonomous agents for complex, long-horizon tasks is a central goal in AI. However, dominant training paradigms face a critical limitation: reinforcement learning (RL) methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths, a problem we term inefficient exploration. This leads to agents that are brittle and fail to generalize, as they learn to find solutions without learning how to reason coherently. To address this, we introduce RLVMR, a novel framework that integrates dense, process-level supervision into end-to-end RL by rewarding verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag its cognitive steps, such as planning, exploration, and reflection, and provides programmatic, rule-based rewards for actions that contribute to effective problem-solving. These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method. On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new state-of-the-art results, with our 7B model reaching an 83.6% success rate on the most difficult unseen task split. Our analysis confirms these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery, leading to more robust, efficient, and interpretable agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RLVMRï¼Œä¸€ç§æ—¨åœ¨ä¸ºå¤æ‚é•¿ç¨‹ä»»åŠ¡æ„å»ºç¨³å¥æ™ºèƒ½ä½“çš„æ–°å‹æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å¼ºåŒ–å­¦ä¹ (RL)ä¸­å› ä»…ä¼˜åŒ–æœ€ç»ˆæˆåŠŸè€Œå¯¼è‡´çš„ä½æ•ˆæ¢ç´¢(inefficient exploration)é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¯éªŒè¯çš„å…ƒæ¨ç†å¥–åŠ±(verifiable meta-reasoning rewards)ï¼Œå°†å¯†é›†çš„è¿‡ç¨‹åºç›‘ç£æ•´åˆåˆ°ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ˜¾å¼æ ‡æ³¨è§„åˆ’ã€æ¢ç´¢å’Œåæ€ç­‰è®¤çŸ¥æ­¥éª¤ã€‚RLVMRç»“åˆäº†åŸºäºè§„åˆ™çš„ç¨‹åºåŒ–å¥–åŠ±ä¸æœ€ç»ˆç»“æœä¿¡å·ï¼Œå¹¶é‡‡ç”¨æ— è¯„è®ºå®¶ç­–ç•¥æ¢¯åº¦(critic-free policy gradient)æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚åœ¨æŒ‘æˆ˜æ€§çš„ALFWorldå’ŒScienceWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLVMRåˆ·æ–°äº†SOTAçºªå½•ï¼Œå…¶7Bæ¨¡å‹åœ¨æœ€å›°éš¾çš„æœªè§ä»»åŠ¡ä¸­è¾¾åˆ°äº†83.6%çš„æˆåŠŸç‡ã€‚åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å†—ä½™åŠ¨ä½œå¹¶å¢å¼ºäº†é”™è¯¯æ¢å¤èƒ½åŠ›ï¼Œä»è€ŒåŸ¹è‚²å‡ºæ›´å…·ç¨³å¥æ€§ã€é«˜æ•ˆç‡å’Œå¯è§£é‡Šæ€§çš„æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22844v1",
      "published_date": "2025-07-30 17:00:48 UTC",
      "updated_date": "2025-07-30 17:00:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:05.350435+00:00"
    },
    {
      "arxiv_id": "2507.22828v3",
      "title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models",
      "title_zh": "CapRecoverï¼šä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€ç‰¹å¾åæ¼”æ”»å‡»æ¡†æ¶",
      "authors": [
        "Kedong Xiu",
        "Sai Qian Zhang"
      ],
      "abstract": "As Vision-Language Models (VLMs) are increasingly deployed in split-DNN configurations--with visual encoders (e.g., ResNet, ViT) operating on user devices and sending intermediate features to the cloud--there is a growing privacy risk from semantic information leakage. Existing approaches to reconstructing images from these intermediate features often result in blurry, semantically ambiguous images. To directly address semantic leakage, we propose CapRecover, a cross-modality inversion framework that recovers high-level semantic content, such as labels or captions, directly from intermediate features without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating strong performance in semantic recovery. Specifically, CapRecover achieves up to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis further reveals that deeper convolutional layers encode significantly more semantic information compared to shallow layers. To mitigate semantic leakage, we introduce a simple yet effective protection method: adding random noise to intermediate features at each layer and removing the noise in the next layer. Experimental results show that this approach prevents semantic leakage without additional training costs. Our code is available at https://jus1mple.github.io/Image2CaptionAttack.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨split-DNNé…ç½®ä¸‹å­˜åœ¨çš„éšç§é£é™©ï¼Œæå‡ºäº†CapRecoverè·¨æ¨¡æ€ç‰¹å¾åè½¬æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥ä»ä¸­é—´ç‰¹å¾ä¸­æ¢å¤æ ‡ç­¾æˆ–æè¿°(captions)ç­‰é«˜å±‚è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒé‡å»ºæ–¹æ³•ä¸åŒï¼ŒCapRecoverä¸“æ³¨äºè¯­ä¹‰å±‚é¢çš„æå–ï¼Œåœ¨CIFAR-10å’ŒCOCO2017ç­‰æ•°æ®é›†ä¸Šå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶ä¸­Top-1æ ‡ç­¾å‡†ç¡®ç‡æœ€é«˜å¯è¾¾92.71%ï¼Œå¹¶èƒ½é’ˆå¯¹ResNet50ç‰¹å¾ç”Ÿæˆé«˜è´¨é‡çš„æè¿°ã€‚ç ”ç©¶åˆ†ææŒ‡å‡ºï¼Œæ·±å±‚å·ç§¯å±‚ç›¸è¾ƒäºæµ…å±‚åŒ…å«æ›´å¤šè¯­ä¹‰ä¿¡æ¯ï¼Œè¿™ä¸ºæ”»å‡»æä¾›äº†æœ‰åˆ©æ¡ä»¶ã€‚ä¸ºåº”å¯¹è¿™ä¸€å¨èƒï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡åœ¨å±‚é—´æ·»åŠ å¹¶ç§»é™¤éšæœºå™ªå£°çš„ä½æˆæœ¬ä¿æŠ¤æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„å‰æä¸‹æœ‰æ•ˆé˜»æ­¢è¯­ä¹‰æ³„éœ²ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†VLMä¸­é—´ç‰¹å¾çš„è„†å¼±æ€§ï¼Œä¹Ÿä¸ºåˆ†å¸ƒå¼æ¨ç†ç¯å¢ƒä¸‹çš„æ•°æ®éšç§ä¿æŠ¤æä¾›äº†æ–°çš„é˜²å¾¡æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, accepted by the 2025 ACM Multimedia Conference. Code is available at https://jus1mple.github.io/Image2CaptionAttack",
      "pdf_url": "https://arxiv.org/pdf/2507.22828v3",
      "published_date": "2025-07-30 16:42:02 UTC",
      "updated_date": "2025-10-25 15:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:28.355996+00:00"
    },
    {
      "arxiv_id": "2507.22805v3",
      "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention",
      "title_zh": "MoCHAï¼šåŸºäº MoE è¿æ¥å™¨ä¸åˆ†å±‚ç»„æ³¨æ„åŠ›çš„å…ˆè¿›è§†è§‰-è¯­è¨€æ¨ç†",
      "authors": [
        "Yuqi Pang",
        "Bowen Yang",
        "Yun Cao",
        "Rong Fan",
        "Xiaoyu Li",
        "Chen He"
      ],
      "abstract": "Vision large language models (VLLMs) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a sparse Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MoCHAæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰å¤§è¯­è¨€æ¨¡å‹(VLLMs)åœ¨è§†è§‰ç»†èŠ‚æå–ã€æ¨¡æ€è¡”æ¥ä»¥åŠè®­ç»ƒæ¨ç†æˆæœ¬æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†CLIPã€SigLIPã€DINOv2å’ŒConvNeXtå››ç§è§†è§‰ä¸»å¹²ç½‘ç»œä»¥æå–äº’è¡¥ç‰¹å¾ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†ç¨€ç–ä¸“å®¶æ··åˆè¿æ¥å™¨(MoECs)æ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©ä¸“å®¶æ¥å¤„ç†ä¸åŒç»´åº¦çš„è§†è§‰ä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾åˆ©ç”¨å¹¶å‡å°‘å†—ä½™ï¼Œç ”ç©¶è®¾è®¡äº†åŒ…å«ç»„å†…ä¸ç»„é—´æ“ä½œçš„åˆ†å±‚ç»„æ³¨æ„åŠ›(HGA)æœºåˆ¶åŠè‡ªé€‚åº”é—¨æ§ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoCHAåœ¨Phi2-2.7Bå’ŒVicuna-7Bä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºCuMoæ¨¡å‹ï¼Œå…¶åœ¨POPEæŒ‡æ ‡ä¸Šç¼“è§£å¹»è§‰èƒ½åŠ›æå‡äº†3.25%ï¼Œåœ¨MMEè§†è§‰æŒ‡ä»¤éµå¾ªè¯„åˆ†ä¸­æé«˜äº†153åˆ†ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†MoECså’ŒHGAåœ¨æå‡MoCHAæ•´ä½“æ€§èƒ½ä¸é²æ£’æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22805v3",
      "published_date": "2025-07-30 16:15:22 UTC",
      "updated_date": "2025-11-17 02:08:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:37.150877+00:00"
    },
    {
      "arxiv_id": "2507.22802v1",
      "title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings",
      "title_zh": "æ¨è¿›èµ„æºå—é™ç¯å¢ƒä¸‹çš„èƒå„¿è¶…å£°å›¾åƒè´¨é‡è¯„ä¼°",
      "authors": [
        "Dongli He",
        "Hu Wang",
        "Mohammad Yaqub"
      ],
      "abstract": "Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½æ”¶å…¥å›½å®¶ç¼ºä¹ä¸“ä¸šè¶…å£°åŒ»å¸ˆçš„é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ FetalCLIP è‡ªåŠ¨åŒ–è¯„ä¼°èƒå„¿è¶…å£°å›¾åƒè´¨é‡ (Image Quality Assessment, IQA)ã€‚ç ”ç©¶è€…é€šè¿‡ä½ç§©è‡ªé€‚åº” (Low-Rank Adaptation, LoRA) æŠ€æœ¯å¯¹ FetalCLIP è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºäº†ä¸“é—¨ç”¨äºè´¨é‡è¯„ä¼°çš„ FetalCLIP$_{CLS}$ æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹ç›²æ‰«è¶…å£°æ•°æ®è¿›è¡Œäº†éªŒè¯ã€‚åœ¨ ACOUSLIC-AI æ•°æ®é›†çš„æµ‹è¯•ä¸­ï¼ŒFetalCLIP$_{CLS}$ å–å¾—äº† 0.757 çš„ F1 åˆ†æ•°ï¼Œä¼˜äºå¤šç§ CNN å’Œ Transformer åŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œå°†é€‚é…çš„åˆ†å‰²æ¨¡å‹é‡æ–°ç”¨äºåˆ†ç±»ä»»åŠ¡å¯å°† F1 åˆ†æ•°æå‡è‡³ 0.771ã€‚è¯¥æˆæœè¯æ˜äº†å¯¹èƒå„¿è¶…å£°åŸºç¡€æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ (parameter-efficient fine-tuning) èƒ½æœ‰æ•ˆå®ç°ç‰¹å®šä»»åŠ¡é€‚é…ï¼Œä¸ºåœ¨èµ„æºåŒ®ä¹åœ°åŒºæå‡äº§å‰æŠ¤ç†æ°´å¹³æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the MICCAI 2025 MIRASOL Workshop",
      "pdf_url": "https://arxiv.org/pdf/2507.22802v1",
      "published_date": "2025-07-30 16:09:29 UTC",
      "updated_date": "2025-07-30 16:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:37.351346+00:00"
    },
    {
      "arxiv_id": "2507.22789v2",
      "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer",
      "title_zh": "G-Coreï¼šç®€å•ã€å¯æ‰©å±•ä¸”å‡è¡¡çš„ RLHF è®­ç»ƒå™¨",
      "authors": [
        "Junyu Wu",
        "Weiming Chang",
        "Xiaotao Liu",
        "Guanyou He",
        "Haoqiang Hong",
        "Boqi Liu",
        "Hongtao Tian",
        "Tao Yang",
        "Yunsheng Shi",
        "Feng Lin",
        "Ting Yao"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly popular paradigm for training large language models (LLMs) and diffusion models. While existing RLHF training systems have enabled significant progress, they often face challenges in scaling to multi-modal and diffusion workflows and adapting to dynamic workloads. In particular, current approaches may encounter limitations in controller scalability, flexible resource placement, and efficient orchestration when handling complex RLHF pipelines, especially in scenarios involving dynamic sampling or generative reward modeling. In this paper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF training framework designed to address these challenges. G-Core introduces a parallel controller programming model, enabling flexible and efficient orchestration of complex RLHF workflows without the bottlenecks of a single centralized controller. Furthermore, we propose a dynamic placement schema that adaptively partitions resources and schedules workloads, significantly reducing hardware idle time and improving utilization, even under highly variable training conditions. G-Core has successfully trained models that support WeChat product features serving a large-scale user base, demonstrating its effectiveness and robustness in real-world scenarios. Our results show that G-Core advances the state of the art in RLHF training, providing a solid foundation for future research and deployment of large-scale, human-aligned models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† G-Coreï¼Œä¸€ä¸ªç®€å•ã€å¯æ‰©å±•ä¸”å¹³è¡¡çš„ RLHF è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿåœ¨å¤„ç†å¤šæ¨¡æ€å’Œæ‰©æ•£æ¨¡å‹å·¥ä½œæµæ—¶é¢ä¸´çš„æ‰©å±•æ€§åŠåŠ¨æ€è´Ÿè½½æŒ‘æˆ˜ã€‚G-Core å¼•å…¥äº†å¹¶è¡Œæ§åˆ¶å™¨ç¼–ç¨‹æ¨¡å‹ (parallel controller programming model)ï¼Œé€šè¿‡çµæ´»é«˜æ•ˆçš„ç¼–æ’æ¶ˆé™¤äº†ä¸­å¿ƒåŒ–æ§åˆ¶å™¨çš„æ€§èƒ½ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†åŠ¨æ€æ”¾ç½®æ–¹æ¡ˆ (dynamic placement schema)ï¼Œèƒ½å¤Ÿæ ¹æ®è®­ç»ƒæ¡ä»¶è‡ªé€‚åº”åœ°åˆ’åˆ†èµ„æºå¹¶è°ƒåº¦å·¥ä½œè´Ÿè½½ï¼Œä»è€Œæ˜¾è‘—é™ä½ç¡¬ä»¶é—²ç½®ç‡å¹¶æé«˜åˆ©ç”¨ç‡ã€‚ç›®å‰ G-Core å·²æˆåŠŸæ”¯æŒå¾®ä¿¡ (WeChat) å¤§è§„æ¨¡ç”¨æˆ·ç«¯äº§å“åŠŸèƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤º G-Core åœ¨ RLHF è®­ç»ƒé¢†åŸŸè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡äººç±»å¯¹é½æ¨¡å‹çš„ç ”å‘ä¸éƒ¨ç½²æä¾›äº†åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "I haven't received company approval yet, and I uploaded it by mistake",
      "pdf_url": "https://arxiv.org/pdf/2507.22789v2",
      "published_date": "2025-07-30 15:55:08 UTC",
      "updated_date": "2025-07-31 02:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:37.954081+00:00"
    },
    {
      "arxiv_id": "2507.22782v3",
      "title": "Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies",
      "title_zh": "åŸºäºæ³¨æ„åŠ›æœºåˆ¶ Actor-Critic ç­–ç•¥çš„å¤šæ™ºèƒ½ä½“åä½œå¢å¼º",
      "authors": [
        "Hugo Garrido-Lestache Belinchon",
        "Jeremy Kedziora"
      ],
      "abstract": "This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Team-Attention-Actor-Critic (TAAC)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºåˆä½œç¯å¢ƒä¸‹å¤šæ™ºèƒ½ä½“åä½œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚TAAC é‡‡ç”¨é›†ä¸­å¼è®­ç»ƒ/é›†ä¸­å¼æ‰§è¡Œ (Centralized Training/Centralized Execution) æ–¹æ¡ˆï¼Œå¹¶åœ¨ Actor å’Œ Critic ä¸­å‡å¼•å…¥äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (multi-headed attention mechanisms)ã€‚è¿™ç§è®¾è®¡ä¿ƒè¿›äº†æ™ºèƒ½ä½“é—´çš„åŠ¨æ€é€šä¿¡ï¼Œå…è®¸æ™ºèƒ½ä½“æ˜ç¡®æŸ¥è¯¢é˜Ÿå‹ä¿¡æ¯ï¼Œä»è€Œåœ¨ç¡®ä¿é«˜åº¦åä½œçš„åŒæ—¶ï¼Œæœ‰æ•ˆç®¡ç†è”åˆåŠ¨ä½œç©ºé—´ (joint-action spaces) çš„æŒ‡æ•°çº§å¢é•¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§æƒ©ç½šæŸå¤±å‡½æ•° (penalized loss function)ï¼Œç”¨äºä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´å½¢æˆå¤šæ ·åŒ–ä¸”äº’è¡¥çš„è§’è‰²åˆ†é…ã€‚åœ¨æ¨¡æ‹Ÿè¶³çƒç¯å¢ƒçš„å¯¹æ¯”å®éªŒä¸­ï¼ŒTAAC åœ¨èƒœç‡ã€è¿›çƒæ•°å·®å¼‚ã€Elo è¯„åˆ†ä»¥åŠæˆ˜æœ¯äº’åŠ¨é¢‘ç‡ç­‰å¤šé¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äº Proximal Policy Optimization (PPO) å’Œ Multi-Agent Actor-Attention-Critic (MAAAC) ç­‰åŸºå‡†ç®—æ³•ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTAAC ä¸ä»…å®ç°äº†å“è¶Šçš„ä»»åŠ¡è¡¨ç°ï¼Œè¿˜é€šè¿‡æ›´å¹³è¡¡çš„ç©ºé—´åˆ†å¸ƒå’Œé¢‘ç¹çš„åä½œäº¤äº’å¢å¼ºäº†å›¢é˜Ÿçš„ååŒæ•ˆåº”ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22782v3",
      "published_date": "2025-07-30 15:48:38 UTC",
      "updated_date": "2025-12-22 17:22:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:43.136632+00:00"
    },
    {
      "arxiv_id": "2507.22774v3",
      "title": "ASP-FZN: A Translation-based Constraint Answer Set Solver",
      "title_zh": "ASP-FZNï¼šåŸºäºç¿»è¯‘çš„çº¦æŸå›ç­”é›†æ±‚è§£å™¨",
      "authors": [
        "Thomas Eiter",
        "Tobias Geibinger",
        "Tobias Kaminski",
        "Nysret Musliu",
        "Johannes Oetsch"
      ],
      "abstract": "We present the solver asp-fzn for Constraint Answer Set Programming (CASP), which extends ASP with linear constraints. Our approach is based on translating CASP programs into the solver-independent FlatZinc language that supports several Constraint Programming and Integer Programming backend solvers. Our solver supports a rich language of linear constraints, including some common global constraints. As for evaluation, we show that asp-fzn is competitive with state-of-the-art ASP solvers on benchmarks taken from past ASP competitions. Furthermore, we evaluate it on several CASP problems from the literature and compare its performance with clingcon, which is a prominent CASP solver that supports most of the asp-fzn language. The performance of asp-fzn is very promising as it is already competitive on plain ASP and even outperforms clingcon on some CASP benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º asp-fzn çš„çº¦æŸç­”æ¡ˆé›†ç¼–ç¨‹ (Constraint Answer Set Programming, CASP) æ±‚è§£å™¨ï¼Œé€šè¿‡çº¿æ€§çº¦æŸæ‰©å±•äº†ä¼ ç»Ÿçš„ ASP æ¡†æ¶ã€‚è¯¥å·¥å…·çš„æ ¸å¿ƒæ–¹æ³•æ˜¯å°† CASP ç¨‹åºç¿»è¯‘ä¸ºç‹¬ç«‹äºæ±‚è§£å™¨çš„ FlatZinc è¯­è¨€ï¼Œä½¿å…¶èƒ½å¤Ÿæ”¯æŒå¤šç§çº¦æŸç¼–ç¨‹ (Constraint Programming) å’Œæ•´æ•°ç¼–ç¨‹ (Integer Programming) åç«¯æ±‚è§£å™¨ã€‚asp-fzn æ”¯æŒä¸°å¯Œçš„çº¿æ€§çº¦æŸè¯­è¨€ï¼Œå¹¶åŒ…å«å¤šç§é€šç”¨çš„å…¨å±€çº¦æŸ (global constraints)ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ä»¥å¾€ ASP ç«èµ›çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œasp-fzn çš„è¡¨ç°è¶³ä»¥ä¸å½“å‰æœ€å…ˆè¿›çš„ ASP æ±‚è§£å™¨ç›¸åª²ç¾ã€‚é€šè¿‡ä¸è‘—å CASP æ±‚è§£å™¨ clingcon çš„å¯¹æ¯”ï¼Œasp-fzn åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨éƒ¨åˆ†æµ‹è¯•ä¸­å®ç°äº†æ€§èƒ½è¶…è¶Šã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†åŸºäºç¿»è¯‘çš„æ±‚è§£ç­–ç•¥åœ¨å¤„ç†å¤æ‚çº¦æŸé€»è¾‘é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the 41st International Conference on Logic Programming (ICLP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.22774v3",
      "published_date": "2025-07-30 15:36:40 UTC",
      "updated_date": "2025-09-15 16:58:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:46.053203+00:00"
    },
    {
      "arxiv_id": "2507.22772v1",
      "title": "Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection",
      "title_zh": "åŸºäºæœºå™¨å­¦ä¹ çš„ Android æ¶æ„è½¯ä»¶æ£€æµ‹ä¸­æ¦‚å¿µæ¼‚ç§»çš„å®è¯è¯„ä¼°",
      "authors": [
        "Ahmed Sabbah",
        "Radi Jarrar",
        "Samer Zein",
        "David Mohaisen"
      ],
      "abstract": "Despite outstanding results, machine learning-based Android malware detection models struggle with concept drift, where rapidly evolving malware characteristics degrade model effectiveness. This study examines the impact of concept drift on Android malware detection, evaluating two datasets and nine machine learning and deep learning algorithms, as well as Large Language Models (LLMs). Various feature types--static, dynamic, hybrid, semantic, and image-based--were considered. The results showed that concept drift is widespread and significantly affects model performance. Factors influencing the drift include feature types, data environments, and detection methods. Balancing algorithms helped with class imbalance but did not fully address concept drift, which primarily stems from the dynamic nature of the malware landscape. No strong link was found between the type of algorithm used and concept drift, the impact was relatively minor compared to other variables since hyperparameters were not fine-tuned, and the default algorithm configurations were used. While LLMs using few-shot learning demonstrated promising detection performance, they did not fully mitigate concept drift, highlighting the need for further investigation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæœºå™¨å­¦ä¹ (ML-Based)çš„ Android æ¶æ„è½¯ä»¶æ£€æµ‹ä¸­æ™®éå­˜åœ¨çš„æ¦‚å¿µæ¼‚ç§»(Concept Drift)ç°è±¡è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯è¯„ä¼°ï¼Œæ—¨åœ¨åˆ†ææ¶æ„è½¯ä»¶ç‰¹å¾æ¼”å˜å¯¹æ¨¡å‹æœ‰æ•ˆæ€§çš„å½±å“ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ä¸¤ä¸ªæ•°æ®é›†æµ‹è¯•äº†ä¹ç§æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ (DL)ç®—æ³•åŠå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ï¼Œå¹¶å…¨é¢è¦†ç›–äº†é™æ€ã€åŠ¨æ€ã€æ··åˆã€è¯­ä¹‰å’ŒåŸºäºå›¾åƒçš„å¤šç§ç‰¹å¾ç±»å‹ã€‚ç»“æœè¯å®æ¦‚å¿µæ¼‚ç§»æ˜¾è‘—é™ä½äº†æ£€æµ‹æ€§èƒ½ï¼Œå…¶å½±å“ç¨‹åº¦ä¸ç‰¹å¾ç±»å‹ã€æ•°æ®ç¯å¢ƒåŠæ£€æµ‹æ–¹æ³•å¯†åˆ‡ç›¸å…³ã€‚ç ”ç©¶å‘ç°å¹³è¡¡ç®—æ³•è™½èƒ½å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œä½†æ— æ³•æ ¹æ²»æºäºæ¶æ„è½¯ä»¶åŠ¨æ€æ¼”å˜çš„æ¦‚å¿µæ¼‚ç§»ï¼Œä¸”ç®—æ³•ç±»å‹ä¸æ¼‚ç§»ç¨‹åº¦ä¹‹é—´å¹¶æ— å¼ºå…³è”ã€‚æ­¤å¤–ï¼Œå°½ç®¡é‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ (Few-shot Learning)çš„ LLMs å±•ç°å‡ºè‰¯å¥½çš„æ£€æµ‹æ½œåŠ›ï¼Œä½†ä»æ— æ³•å®Œå…¨è§„é¿æ¦‚å¿µæ¼‚ç§»ï¼Œå‡¸æ˜¾äº†æœªæ¥åœ¨è¯¥é¢†åŸŸæŒç»­æ·±å…¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, 12 tables, 14 figures, paper under review",
      "pdf_url": "https://arxiv.org/pdf/2507.22772v1",
      "published_date": "2025-07-30 15:35:51 UTC",
      "updated_date": "2025-07-30 15:35:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:56.093448+00:00"
    },
    {
      "arxiv_id": "2507.22767v2",
      "title": "Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization",
      "title_zh": "Teaching the Teacherï¼šé€šè¿‡é›…å¯æ¯”æ­£åˆ™åŒ–æå‡ç¬¦å·å›å½’ç¥ç»ç½‘ç»œçš„å¯è’¸é¦æ€§",
      "authors": [
        "Soumyadeep Dhar",
        "Kei Sen Fong",
        "Mehul Motani"
      ],
      "abstract": "Distilling large neural networks into simple, human-readable symbolic formulas is a promising path toward trustworthy and interpretable AI. However, this process is often brittle, as the complex functions learned by standard networks are poor targets for symbolic discovery, resulting in low-fidelity student models. In this work, we propose a novel training paradigm to address this challenge. Instead of passively distilling a pre-trained network, we introduce a \\textbf{Jacobian-based regularizer} that actively encourages the ``teacher'' network to learn functions that are not only accurate but also inherently smoother and more amenable to distillation. We demonstrate through extensive experiments on a suite of real-world regression benchmarks that our method is highly effective. By optimizing the regularization strength for each problem, we improve the $R^2$ score of the final distilled symbolic model by an average of \\textbf{120\\% (relative)} compared to the standard distillation pipeline, all while maintaining the teacher's predictive accuracy. Our work presents a practical and principled method for significantly improving the fidelity of interpretable models extracted from complex neural networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡ Jacobian Regularization æé«˜ç¥ç»ç½‘ç»œåœ¨ Symbolic Regression ä»»åŠ¡ä¸­å¯è’¸é¦æ€§çš„æ–°è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨å°†å¤æ‚çš„ç¥ç»ç½‘ç»œè½¬åŒ–ä¸ºäººç±»å¯è¯»ä¸”å¯è§£é‡Šçš„ç¬¦å·å…¬å¼ã€‚é’ˆå¯¹æ ‡å‡†ç½‘ç»œå­¦ä¹ åˆ°çš„å¤æ‚å‡½æ•°éš¾ä»¥è¢«ç¬¦å·å‘ç°å·¥å…·é«˜æ•ˆæ•æ‰çš„ç“¶é¢ˆï¼Œç ”ç©¶è€…å¼•å…¥äº†åŸºäº Jacobian çš„æ­£åˆ™åŒ–é¡¹ï¼Œä¸»åŠ¨å¼•å¯¼â€œæ•™å¸ˆâ€ç½‘ç»œå­¦ä¹ æ›´å¹³æ»‘ä¸”æ›´åˆ©äºè’¸é¦çš„ç›®æ ‡å‡½æ•°ã€‚åœ¨å¤šé¡¹çœŸå®ä¸–ç•Œå›å½’åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ•™å¸ˆç½‘ç»œåŸå§‹é¢„æµ‹ç²¾åº¦çš„å‰æä¸‹ï¼Œä½¿æœ€ç»ˆè’¸é¦å¾—åˆ°çš„ç¬¦å·æ¨¡å‹ $R^2$ åˆ†æ•°å¹³å‡ç›¸å¯¹æå‡äº† 120%ã€‚è¯¥é¡¹å·¥ä½œä¸ºä»å¤æ‚ç¥ç»ç½‘ç»œä¸­æå–é«˜ä¿çœŸåº¦çš„å¯è§£é‡Šæ¨¡å‹æä¾›äº†ä¸€ç§å®ç”¨ä¸”è§„èŒƒçš„æ–¹æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº† AI ç³»ç»Ÿçš„å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22767v2",
      "published_date": "2025-07-30 15:32:18 UTC",
      "updated_date": "2025-08-01 07:50:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:59.194794+00:00"
    },
    {
      "arxiv_id": "2507.22766v3",
      "title": "Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models",
      "title_zh": "åŸºäºé«˜æ–¯è¿‡ç¨‹ä»£ç†æ¨¡å‹çš„ä¼ æ„Ÿå™¨åˆ†é€‰ç³»ç»Ÿå·¥è‰ºå‚æ•°è´å¶æ–¯ä¼˜åŒ–",
      "authors": [
        "Felix Kronenwett",
        "Georg Maier",
        "Thomas LÃ¤ngle"
      ],
      "abstract": "Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºä¼ æ„Ÿå™¨çš„åˆ†é€‰ç³»ç»Ÿï¼ˆSensor-based sorting systemsï¼‰åœ¨å¤„ç†å¤šå˜ç‰©æ–™æµæ—¶é¢ä¸´çš„å·¥è‰ºå‚æ•°ä¼˜åŒ–ä¸é¢‘ç¹è°ƒæ•´éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Bayesian Optimization çš„ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Gaussian process regression å»ºç«‹ä»£ç†æ¨¡å‹ï¼ˆsurrogate modelsï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å¤„ç†ç³»ç»Ÿå†…éƒ¨çš„ä¸ç¡®å®šæ€§æ¥å®ç°ç‰¹å®šçš„è¿è¡Œè¦æ±‚ã€‚è¯¥æ–¹æ¡ˆèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å¿…è¦çš„å®éªŒæ¬¡æ•°ï¼Œå¹¶æ ¹æ®ä¸¤ä¸ªç‰©æ–™è¾“å‡ºæµçš„å…·ä½“éœ€æ±‚ï¼ŒåŒæ—¶å…¼é¡¾ä¸¤ä¸ªä¼˜åŒ–ç›®æ ‡ã€‚æ­¤å¤–ï¼Œåœ¨æ¨¡å‹è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å……åˆ†è€ƒè™‘äº†ç¡®å®šåˆ†é€‰å‡†ç¡®åº¦æ—¶çš„ä¸ç¡®å®šæ€§å› ç´ ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªç¤ºä¾‹å·¥è‰ºå‚æ•°çš„è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¾ªç¯ç›‘æµ‹å’ŒåŠ¨æ€è°ƒæ•´åˆ†é€‰ç³»ç»Ÿå‚æ•°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the IEEE 30th International Conference on Emerging Technologies and Factory Automation (ETFA)",
      "pdf_url": "https://arxiv.org/pdf/2507.22766v3",
      "published_date": "2025-07-30 15:31:39 UTC",
      "updated_date": "2025-10-23 08:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:40:56.891017+00:00"
    },
    {
      "arxiv_id": "2507.22760v1",
      "title": "Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision",
      "title_zh": "å–„ä¹‹æ¶é­”ä¸æ¶ä¹‹å¤©ä½¿ï¼šæœ‰é™ç²¾åº¦ä¸‹çš„å®‰å…¨æ§åˆ¶ä¿éšœ",
      "authors": [
        "Samuel Teuber",
        "Debasmita Lohar",
        "Bernhard Beckert"
      ],
      "abstract": "As neural networks (NNs) become increasingly prevalent in safety-critical neural network-controlled cyber-physical systems (NNCSs), formally guaranteeing their safety becomes crucial. For these systems, safety must be ensured throughout their entire operation, necessitating infinite-time horizon verification. To verify the infinite-time horizon safety of NNCSs, recent approaches leverage Differential Dynamic Logic (dL). However, these dL-based guarantees rely on idealized, real-valued NN semantics and fail to account for roundoff errors introduced by finite-precision implementations. This paper bridges the gap between theoretical guarantees and real-world implementations by incorporating robustness under finite-precision perturbations -- in sensing, actuation, and computation -- into the safety verification. We model the problem as a hybrid game between a good Demon, responsible for control actions, and a bad Angel, introducing perturbations. This formulation enables formal proofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this bound, we employ state-of-the-art mixed-precision fixed-point tuners to synthesize sound and efficient implementations, thus providing a complete end-to-end solution. We evaluate our approach on case studies from the automotive and aeronautics domains, producing efficient NN implementations with rigorous infinite-time horizon safety guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œæ§åˆ¶çš„èµ›åšç‰©ç†ç³»ç»Ÿ(NNCSs)åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åœ¨æœ‰é™ç²¾åº¦(Finite Precision)å®ç°ä¸‹ä¿è¯ç³»ç»Ÿå®‰å…¨æ€§çš„å½¢å¼åŒ–éªŒè¯æ¡†æ¶ã€‚ç°æœ‰çš„åŸºäºå¾®åˆ†åŠ¨æ€é€»è¾‘(Differential Dynamic Logic, dL)çš„ä¿è¯æ–¹æ³•å¾€å¾€ä¾èµ–ç†æƒ³åŒ–çš„å®æ•°å€¼è¯­ä¹‰ï¼Œæœªèƒ½è€ƒè™‘ä¼ æ„Ÿã€æ‰§è¡Œå’Œè®¡ç®—è¿‡ç¨‹ä¸­å› èˆå…¥è¯¯å·®(Roundoff Errors)å¼•å…¥çš„æ‰°åŠ¨ã€‚ä¸ºäº†å¼¥è¡¥ç†è®ºä¿è¯ä¸ç°å®å®ç°ä¹‹é—´çš„å·®è·ï¼Œè¯¥è®ºæ–‡å°†å®‰å…¨æ€§éªŒè¯å»ºæ¨¡ä¸ºâ€œå–„è‰¯æ¶é­”â€(Good Demon)ä¸â€œé‚ªæ¶å¤©ä½¿â€(Bad Angel)ä¹‹é—´çš„æ··åˆåšå¼ˆ(Hybrid Game)ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ç»™å®šçš„æœ‰ç•Œæ‰°åŠ¨æä¾›é²æ£’æ€§è¯æ˜ã€‚é€šè¿‡ç»“åˆå…ˆè¿›çš„æ··åˆç²¾åº¦å®šç‚¹è°ƒä¼˜å™¨(Mixed-Precision Fixed-Point Tuners)ï¼Œç ”ç©¶å®ç°äº†ä¸€å¥—èƒ½å¤Ÿåˆæˆé«˜æ•ˆä¸”å®‰å…¨å¯é çš„ç¥ç»ç½‘ç»œå®ç°æ–¹æ¡ˆçš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚åœ¨æ±½è½¦å’Œèˆªç©ºé¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºå¤æ‚çš„æ§åˆ¶ä»»åŠ¡æä¾›ä¸¥æ ¼çš„æ— é™æ—¶é—´èŒƒå›´(Infinite-Time Horizon)å®‰å…¨æ€§ä¿éšœã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "eess.SY",
      "comment": "15 pages, 3 figures, 1 table; Accepted at FMCAD 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22760v1",
      "published_date": "2025-07-30 15:21:22 UTC",
      "updated_date": "2025-07-30 15:21:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:05.095352+00:00"
    },
    {
      "arxiv_id": "2507.22744v1",
      "title": "Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index",
      "title_zh": "åŸºäºå®ä½“å¹»è§‰æŒ‡æ•°å¼ºåŒ–å­¦ä¹ é™ä½æ‘˜è¦ä¸­çš„å¹»è§‰",
      "authors": [
        "Praveenkumar Katwe",
        "Rakesh Chandra",
        "Balabantaray Kali",
        "Prasad Vittala"
      ],
      "abstract": "Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŠ½è±¡å¼æ‘˜è¦ç”Ÿæˆä¸­çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„å¥–åŠ±é©±åŠ¨å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒå¼•å…¥äº†å®ä½“å¹»è§‰æŒ‡æ ‡(Entity Hallucination Index, EHI)ï¼Œç”¨äºé‡åŒ–ç”Ÿæˆæ‘˜è¦ä¸­å‘½åå®ä½“çš„å­˜åœ¨ã€æ­£ç¡®æ€§åŠå…¶ä¸åŸæ–‡çš„å…³è”ç¨‹åº¦ã€‚é€šè¿‡å¯¹ä¼šè®®è½¬å½•æ–‡æœ¬ç”Ÿæˆåˆå§‹æ‘˜è¦å¹¶è‡ªåŠ¨è®¡ç®—EHIå¾—åˆ†ï¼Œç ”ç©¶å°†è¯¥æŒ‡æ ‡ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥å¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å…·å®ä½“å¿ å®åº¦çš„è¾“å‡ºã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äººå·¥ç¼–å†™çš„äº‹å®æ€§æ ‡æ³¨ï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿå®ç°å¤§è§„æ¨¡çš„å¾®è°ƒåº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—é™ä½å®ä½“å±‚é¢å¹»è§‰çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿æŒæ‘˜è¦çš„æµç•…åº¦å’Œä¿¡æ¯é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†å¯å¤ç°çš„Colabæµæ°´çº¿ï¼Œä¸ºåˆ©ç”¨è½»é‡çº§æŒ‡æ ‡è¿›è¡Œå¹»è§‰æ„ŸçŸ¥æ¨¡å‹çš„ç ”ç©¶æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8",
      "pdf_url": "https://arxiv.org/pdf/2507.22744v1",
      "published_date": "2025-07-30 15:00:00 UTC",
      "updated_date": "2025-07-30 15:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:20.090063+00:00"
    },
    {
      "arxiv_id": "2507.22711v1",
      "title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness",
      "title_zh": "OFCnetLLMï¼šé¢å‘ç½‘ç»œç›‘æ§ä¸é¢„è­¦çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hong-Jun Yoon",
        "Mariam Kiran",
        "Danial Ebling",
        "Joe Breen"
      ],
      "abstract": "The rapid evolution of network infrastructure is bringing new challenges and opportunities for efficient network management, optimization, and security. With very large monitoring databases becoming expensive to explore, the use of AI and Generative AI can help reduce costs of managing these datasets. This paper explores the use of Large Language Models (LLMs) to revolutionize network monitoring management by addressing the limitations of query finding and pattern analysis. We leverage LLMs to enhance anomaly detection, automate root-cause analysis, and automate incident analysis to build a well-monitored network management team using AI. Through a real-world example of developing our own OFCNetLLM, based on the open-source LLM model, we demonstrate practical applications of OFCnetLLM in the OFC conference network. Our model is developed as a multi-agent approach and is still evolving, and we present early results here.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é©æ–°ç½‘ç»œç›‘æ§ç®¡ç†ï¼Œä»¥åº”å¯¹ç½‘ç»œåŸºç¡€è®¾æ–½å¿«é€Ÿæ¼”è¿›å¸¦æ¥çš„ç®¡ç†å’Œå®‰å…¨æŒ‘æˆ˜ã€‚ä¸ºäº†é™ä½ç›‘æ§å¤§å‹æ•°æ®åº“çš„æˆæœ¬ï¼Œä½œè€…åŸºäºå¼€æºæ¨¡å‹å¼€å‘äº†OFCnetLLMï¼Œå¹¶é‡‡ç”¨äº†å¤šæ™ºèƒ½ä½“(multi-agent)æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å¢å¼ºå¼‚å¸¸æ£€æµ‹(anomaly detection)ã€è‡ªåŠ¨åŒ–æ ¹å› åˆ†æ(root-cause analysis)å’Œäº‹ä»¶åˆ†æï¼Œæ—¨åœ¨æ„å»ºæ™ºèƒ½åŒ–çš„ç½‘ç»œç®¡ç†å›¢é˜Ÿã€‚é€šè¿‡åœ¨OFCä¼šè®®ç½‘ç»œä¸­çš„å®é™…åº”ç”¨ï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ¨¡å‹åœ¨å…‹æœæŸ¥è¯¢æŸ¥æ‰¾å’Œæ¨¡å¼åˆ†æå±€é™æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶ç³»ç»Ÿç›®å‰ä»å¤„äºæ¼”è¿›é˜¶æ®µï¼Œä½†æ—©æœŸæˆæœå·²å±•ç¤ºå‡ºLLMsåœ¨æå‡ç½‘ç»œç›‘æµ‹è‡ªåŠ¨åŒ–ä¸è­¦è§‰æ€§æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22711v1",
      "published_date": "2025-07-30 14:22:42 UTC",
      "updated_date": "2025-07-30 14:22:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:40.598170+00:00"
    },
    {
      "arxiv_id": "2508.08270v2",
      "title": "Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI",
      "title_zh": "Doctor Sunï¼šé¢å‘ç”Ÿç‰©åŒ»å­¦äººå·¥æ™ºèƒ½çš„åŒè¯­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Dong Xue",
        "Ziyao Shao",
        "Zhaoyang Duan",
        "Fangzhou Liu",
        "Bing Li",
        "Zhongheng Zhang"
      ],
      "abstract": "Large multimodal models (LMMs) have demonstrated significant potential in providing innovative solutions for various biomedical tasks, including pathology analysis, radiology report generation, and biomedical assistance. However, the existing multimodal biomedical AI is typically based on foundation LLMs, thus hindering the understanding of intricate medical concepts with limited medical training data. Moreover, recent LLaVA-induced medical LMMs struggle to effectively capture the intricate relationship between the texts and the images. Therefore, we introduce Doctor Sun, a large multimodal generative model specialized in medicine, developed to encode, integrate, and interpret diverse biomedical data modalities such as text and images. In particular, Doctor Sun integrates a pre-trained vision encoder with a medical LLM and conducts two-stage training on various medical datasets, focusing on feature alignment and instruction tuning. Moreover, we release SunMed-VL, a wide-range bilingual medical multimodal dataset, along with all associated models, code, and resources, to freely support the advancement of biomedical multimodal research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„å¤šæ¨¡æ€å¤§æ¨¡å‹(LMMs)åœ¨ç†è§£å¤æ‚åŒ»å­¦æ¦‚å¿µä»¥åŠæ•æ‰æ–‡æœ¬ä¸å›¾åƒé—´å†…åœ¨è”ç³»æ–¹é¢å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸“é—¨é’ˆå¯¹åŒ»å­¦é¢†åŸŸçš„å¤§å‹å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹Doctor Sunã€‚è¯¥æ¨¡å‹é€šè¿‡å°†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨(vision encoder)ä¸ä¸“ä¸šçš„åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹(medical LLM)ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹æ–‡æœ¬å’Œå›¾åƒç­‰å¤šç§ç”Ÿç‰©åŒ»å­¦æ•°æ®æ¨¡æ€çš„æ·±åº¦ç¼–ç ã€é›†æˆä¸è§£é‡Šã€‚ç ”å‘å›¢é˜Ÿåœ¨å¤šæ ·åŒ–çš„åŒ»å­¦æ•°æ®é›†ä¸Šæ‰§è¡Œäº†ç‰¹å¾å¯¹é½(feature alignment)å’ŒæŒ‡ä»¤å¾®è°ƒ(instruction tuning)è¿™ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„åŒ»ç–—ä¸“ä¸šè¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å…¬å¼€å‘å¸ƒäº†å¤§è§„æ¨¡çš„åŒè¯­åŒ»å­¦å¤šæ¨¡æ€æ•°æ®é›†SunMed-VLä»¥åŠç›¸å…³çš„æ¨¡å‹ä»£ç å’Œèµ„æºï¼Œæ—¨åœ¨ä¸ºç—…ç†åˆ†æã€æ”¾å°„æŠ¥å‘Šç”Ÿæˆç­‰ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡æä¾›æœ‰åŠ›æ”¯æŒï¼Œå¹¶æ¨åŠ¨è¯¥é¢†åŸŸç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.08270v2",
      "published_date": "2025-07-30 13:53:54 UTC",
      "updated_date": "2025-12-29 06:02:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:30.091866+00:00"
    },
    {
      "arxiv_id": "2507.22687v3",
      "title": "An Architecture for Spatial Networking",
      "title_zh": "ç©ºé—´ç½‘ç»œæ¶æ„",
      "authors": [
        "Josh Millar",
        "Ryan Gibb",
        "Roy Ang",
        "Hamed Haddadi",
        "Anil Madhavapeddy"
      ],
      "abstract": "Physical spaces are increasingly dense with networked devices, promising seamless coordination and ambient intelligence. Yet today, cloud-first architectures force all communication through wide-area networks regardless of physical proximity. We lack an abstraction for spatial networking: using physical spaces to create boundaries for private, robust, and low-latency communication. We introduce $\\textit{BifrÃ¶st}$, a programming model that realizes spatial networking using bigraphs to express both containment and connectivity, enabling policies to be scoped by physical boundaries, devices to be named by location, the instantiation of spatial services, and the composition of spaces while maintaining local autonomy. BifrÃ¶st enables a new class of spatially-aware applications, where co-located devices communicate directly, physical barriers require explicit gateways, and local control bridges to global coordination.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰äº‘ä¼˜å…ˆ(cloud-first)æ¶æ„å¼ºåˆ¶æ‰€æœ‰é€šä¿¡ç»è¿‡å¹¿åŸŸç½‘è€Œå¿½ç•¥ç‰©ç†é‚»è¿‘æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ç©ºé—´è”ç½‘(spatial networking)çš„æ¦‚å¿µï¼Œæ—¨åœ¨åˆ©ç”¨ç‰©ç†ç©ºé—´æ„å»ºç§æœ‰ã€é²æ£’ä¸”ä½å»¶è¿Ÿçš„é€šä¿¡è¾¹ç•Œã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼•å…¥äº†BifrÃ¶stï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŒå‘å›¾(bigraphs)åŒæ—¶è¡¨è¾¾åŒ…å«å…³ç³»(containment)å’Œè¿æ¥æ€§(connectivity)çš„ç¼–ç¨‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…è®¸æ ¹æ®ç‰©ç†è¾¹ç•Œåˆ’å®šç­–ç•¥èŒƒå›´ï¼Œé€šè¿‡ä½ç½®ä¸ºè®¾å¤‡å‘½åï¼Œå¹¶æ”¯æŒç©ºé—´æœåŠ¡çš„å®ä¾‹åŒ–å’Œç©ºé—´çš„ç»„åˆï¼ŒåŒæ—¶ç¡®ä¿äº†å±€éƒ¨è‡ªä¸»æ€§(local autonomy)ã€‚BifrÃ¶st èƒ½å¤Ÿæ”¯æŒæ–°å‹çš„ç©ºé—´æ„ŸçŸ¥åº”ç”¨ï¼Œä½¿ååŒå®šä½çš„è®¾å¤‡å®ç°ç›´æ¥é€šä¿¡ï¼Œå¹¶å°†ç‰©ç†éšœç¢è§†ä¸ºæ˜¾å¼ç½‘å…³ï¼Œæœ‰æ•ˆåœ°å°†å±€éƒ¨æ§åˆ¶æ¡¥æ¥åˆ°å…¨çƒåè°ƒã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LO",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22687v3",
      "published_date": "2025-07-30 13:49:12 UTC",
      "updated_date": "2025-10-03 10:10:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:29.801390+00:00"
    },
    {
      "arxiv_id": "2507.22685v1",
      "title": "Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing",
      "title_zh": "Hydra-Benchï¼šå¤šæ¨¡æ€å¶é¢æ¹¿åº¦æ„ŸçŸ¥åŸºå‡†",
      "authors": [
        "Yimeng Liu",
        "Maolin Gan",
        "Yidong Ren",
        "Gen Li",
        "Jingkai Lin",
        "Younsuk Dong",
        "Zhichao Cao"
      ],
      "abstract": "Leaf wetness detection is a crucial task in agricultural monitoring, as it directly impacts the prediction and protection of plant diseases. However, existing sensing systems suffer from limitations in robustness, accuracy, and environmental resilience when applied to natural leaves under dynamic real-world conditions. To address these challenges, we introduce a new multi-modal dataset specifically designed for evaluating and advancing machine learning algorithms in leaf wetness detection. Our dataset comprises synchronized mmWave raw data, Synthetic Aperture Radar (SAR) images, and RGB images collected over six months from five diverse plant species in both controlled and outdoor field environments. We provide detailed benchmarks using the Hydra model, including comparisons against single modality baselines and multiple fusion strategies, as well as performance under varying scan distances. Additionally, our dataset can serve as a benchmark for future SAR imaging algorithm optimization, enabling a systematic evaluation of detection accuracy under diverse conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†œä¸šç›‘æµ‹ä¸­å¶é¢æ¹¿åº¦æ£€æµ‹(Leaf wetness detection)åœ¨å¤æ‚çœŸå®ç¯å¢ƒä¸‹é²æ£’æ€§ä¸ç²¾åº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›† Hydra-Benchã€‚è¯¥æ•°æ®é›†åŒ…å«äº†åŒæ­¥çš„æ¯«ç±³æ³¢åŸå§‹æ•°æ®(mmWave raw data)ã€åˆæˆå­”å¾„é›·è¾¾(SAR)å›¾åƒå’Œ RGB å›¾åƒï¼Œæ”¶é›†è¿‡ç¨‹è·¨è¶Šå…­ä¸ªæœˆï¼Œæ¶µç›–äº†äº”ç§æ¤ç‰©ç‰©ç§åœ¨å—æ§åŠå®¤å¤–ç”°é—´ç­‰å¤šæ ·åŒ–ç¯å¢ƒä¸‹çš„æ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ Hydra æ¨¡å‹æä¾›äº†è¯¦ç»†çš„æ€§èƒ½åŸºå‡†ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å•æ¨¡æ€åŸºå‡†ã€å¤šç§å¤šæ¨¡æ€èåˆç­–ç•¥ä»¥åŠä¸åŒæ‰«æè·ç¦»å¯¹æ£€æµ‹ç»“æœçš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ä½œä¸ºä¼˜åŒ–åˆæˆå­”å¾„é›·è¾¾(SAR)æˆåƒç®—æ³•çš„åŸºå‡†ï¼Œç”¨äºåœ¨ä¸åŒæ¡ä»¶ä¸‹å¯¹æ£€æµ‹ç²¾åº¦è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†ç°æœ‰ä¼ æ„Ÿç³»ç»Ÿåœ¨ç¯å¢ƒéŸ§æ€§æ–¹é¢çš„å±€é™ï¼Œä¹Ÿä¸ºæœªæ¥å†œä¸šç—…è™«å®³é¢„æµ‹å’Œæ¤ç‰©ä¿æŠ¤æŠ€æœ¯çš„å‘å±•å¥ å®šäº†å¤šæ¨¡æ€æ„ŸçŸ¥åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22685v1",
      "published_date": "2025-07-30 13:47:56 UTC",
      "updated_date": "2025-07-30 13:47:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:39.995791+00:00"
    },
    {
      "arxiv_id": "2507.22671v1",
      "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach",
      "title_zh": "é¢å‘éæ­£å¼ç¼–ç¨‹å­¦ä¹ è‡ªæˆ‘è°ƒèŠ‚çš„è®¾è®¡ï¼šä»¥å™äº‹ä¸ºä¸­å¿ƒæ–¹æ³•çš„å¯ç¤º",
      "authors": [
        "Sami Saeed Alghamdi",
        "Christopher Bull",
        "Ahmed Kharrufa"
      ],
      "abstract": "Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éæ­£å¼ç¼–ç¨‹å­¦ä¹ è€…(informal programming learners)åœ¨ç‹¬ç«‹å­¦ä¹ ä¸­é¢ä¸´çš„å­¤ç‹¬æ„Ÿã€ä¿¡æ¯è¿‡è½½å’Œç¼ºä¹æŒ‡å¯¼ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥å™äº‹(storytelling)ä¸ºæ ¸å¿ƒçš„è‡ªæˆ‘è°ƒèŠ‚(self-regulation)æ”¯æŒç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç”±ç½‘é¡µå¹³å°å’Œæµè§ˆå™¨æ’ä»¶ç»„æˆï¼Œæ—¨åœ¨å°†å­¦ä¹ è€…çš„èµ„æºç®¡ç†(resource curation)ã€åæ€ä¸æ„ä¹‰æ„å»º(sensemaking)å®è·µè½¬åŒ–ä¸ºåŒ…å«AIç”Ÿæˆåé¦ˆçš„å­¦ä¹ æ•…äº‹ï¼Œä»è€Œä¸ºéç»“æ„åŒ–çš„å­¦ä¹ è¿‡ç¨‹å»ºç«‹ç»“æ„ã€‚é€šè¿‡å¯¹15åæ´»è·ƒäºç¤¾äº¤åª’ä½“çš„éæ­£å¼å­¦ä¹ è€…è¿›è¡Œçš„å®è¯ç ”ç©¶ï¼Œç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿä½œä¸ºè‡ªæˆ‘è°ƒèŠ‚è¾…åŠ©å·¥å…·å…·æœ‰æ˜¾è‘—çš„å¯è¡Œæ€§ã€‚å‚ä¸è€…é«˜åº¦è¯„ä»·äº†å³æ—¶åæ€(in-situ reflection)ã€è‡ªåŠ¨åŒ–æ•…äº‹åé¦ˆä»¥åŠè§†é¢‘æ ‡æ³¨(video annotation)ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚è¯¥å·¥ä½œæœ€åæ€»ç»“äº†AIå¢å¼ºçš„è‡ªæˆ‘è°ƒèŠ‚å·¥å…·(AI-augmented self-regulation tools)åœ¨è®¾è®¡ä¸Šçš„æ½œåœ¨ç›Šå¤„ä¸æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç›¸å…³ç³»ç»Ÿçš„å¼€å‘æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "10 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22671v1",
      "published_date": "2025-07-30 13:30:04 UTC",
      "updated_date": "2025-07-30 13:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:41.097014+00:00"
    },
    {
      "arxiv_id": "2507.22664v2",
      "title": "RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots",
      "title_zh": "RobEthiChorï¼šé¢å‘è‡ªä¸»æœºå™¨äººçš„è‡ªåŠ¨åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸”åŸºäºä¼¦ç†çš„åå•†",
      "authors": [
        "Mashal Afzal Memon",
        "Gianluca Filippone",
        "Gian Luca Scoccia",
        "Marco Autili",
        "Paola Inverardi"
      ],
      "abstract": "The presence of autonomous systems is growing at a fast pace and it is impacting many aspects of our lives. Designed to learn and act independently, these systems operate and perform decision-making without human intervention. However, they lack the ability to incorporate users' ethical preferences, which are unique for each individual in society and are required to personalize the decision-making processes. This reduces user trust and prevents autonomous systems from behaving according to the moral beliefs of their end-users. When multiple systems interact with differing ethical preferences, they must negotiate to reach an agreement that satisfies the ethical beliefs of all the parties involved and adjust their behavior consequently. To address this challenge, this paper proposes RobEthiChor, an approach that enables autonomous systems to incorporate user ethical preferences and contextual factors into their decision-making through ethics-based negotiation. RobEthiChor features a domain-agnostic reference architecture for designing autonomous systems capable of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an implementation of RobEthiChor within the Robot Operating System (ROS), which can be deployed on robots to provide them with ethics-based negotiation capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real robots and ran scenarios where a pair of robots negotiate upon resource contention. Experimental results demonstrate the feasibility and effectiveness of the system in realizing ethics-based negotiation. RobEthiChor allowed robots to reach an agreement in more than 73% of the scenarios with an acceptable negotiation time (0.67s on average). Experiments also demonstrate that the negotiation approach implemented in RobEthiChor is scalable.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RobEthiChorï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡åŸºäºä¼¦ç†çš„åå•†(ethics-based negotiation)ä½¿è‡ªä¸»ç³»ç»Ÿèƒ½å¤Ÿç»“åˆç”¨æˆ·ä¼¦ç†åå¥½å’Œä¸Šä¸‹æ–‡å› ç´ è¿›è¡Œå†³ç­–çš„æ–°æ–¹æ³•ã€‚ç”±äºç°æœ‰è‡ªä¸»ç³»ç»Ÿåœ¨å†³ç­–ä¸­é€šå¸¸ç¼ºä¹å¯¹ä¸ªä½“ä¼¦ç†å·®å¼‚çš„è€ƒé‡ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªé¢†åŸŸæ— å…³çš„å‚è€ƒæ¶æ„ï¼Œå¹¶æ¨å‡ºäº†åŸºäºæœºå™¨äººæ“ä½œç³»ç»Ÿ(ROS)çš„å®ç°ç‰ˆæœ¬ RobEthiChor-Rosã€‚é€šè¿‡åœ¨çœŸå®æœºå™¨äººèµ„æºç«äº‰åœºæ™¯ä¸‹çš„éƒ¨ç½²å®éªŒï¼Œç ”ç©¶éªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨å¤„ç†å¤šæ™ºèƒ½ä½“ä¼¦ç†åå¥½å†²çªæ—¶çš„å¯è¡Œæ€§ä¸æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRobEthiChor åœ¨è¶…è¿‡ 73% çš„åœºæ™¯ä¸­æˆåŠŸè¾¾æˆåè®®ï¼Œä¸”å¹³å‡åå•†æ—¶é—´ä»…ä¸º 0.67 ç§’ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†ä¼¦ç†åå•†æœºåˆ¶çš„å¯æ‰©å±•æ€§ï¼Œä¹Ÿä¸ºæå‡è‡ªä¸»ç³»ç»Ÿçš„ç¤¾ä¼šä¿¡ä»»åº¦åŠé“å¾·ä¸€è‡´æ€§æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22664v2",
      "published_date": "2025-07-30 13:21:38 UTC",
      "updated_date": "2025-10-29 16:42:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:50.490513+00:00"
    },
    {
      "arxiv_id": "2507.22659v2",
      "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è½¯ä»¶æ¼æ´æ£€æµ‹ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°",
      "authors": [
        "Sabrina Kaniewski",
        "Fabian Schmidt",
        "Markus Enzweiler",
        "Michael Menth",
        "Tobias Heer"
      ],
      "abstract": "The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨è½¯ä»¶æ¼æ´æ£€æµ‹(software vulnerability detection)é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¼€å±•äº†ä¸€é¡¹å…¨é¢çš„ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°(Systematic Literature Review, SLR)ã€‚é€šè¿‡åˆ†æ2020å¹´1æœˆè‡³2025å¹´11æœˆæœŸé—´å‘è¡¨çš„263é¡¹ç ”ç©¶ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³å½“å‰ç”±äºç³»ç»Ÿè®¾è®¡å’Œæ•°æ®é›†å·®å¼‚å¯¼è‡´çš„é¢†åŸŸç¢ç‰‡åŒ–é—®é¢˜ã€‚ç ”ç©¶ä»ä»»åŠ¡åˆ¶å®š(task formulation)ã€è¾“å…¥è¡¨ç¤º(input representation)ã€ç³»ç»Ÿæ¶æ„(system architecture)åŠå…·ä½“æŠ€æœ¯ç­‰å¤šä¸ªç»´åº¦å¯¹ç°æœ‰å·¥ä½œè¿›è¡Œäº†åˆ†ç±»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ„å»ºäº†ä¸€ä¸ªç»†ç²’åº¦çš„æ¼æ´æ£€æµ‹æ–¹æ³•åˆ†ç±»å­¦(taxonomy)ï¼Œå¹¶å¯¹æ•°æ®é›†çš„ç‰¹å¾ã€æ¼æ´è¦†ç›–èŒƒå›´åŠå¤šæ ·æ€§è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚è¯¥ç ”ç©¶è¯†åˆ«äº†å½“å‰é¢†åŸŸçš„ä¸»è¦å±€é™æ€§ï¼Œå¹¶æå‡ºäº†å…·æœ‰å¯æ“ä½œæ€§çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æœ€åï¼Œä½œè€…å…¬å¼€äº†æ‰€æœ‰ç ”ç©¶äº§ç‰©å¹¶å»ºç«‹äº†åŠ¨æ€æ›´æ–°çš„å­˜å‚¨åº“ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æå‡ç ”ç©¶é€æ˜åº¦ä¸å¯é‡å¤æ€§çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "43 pages + 20 pages references, 7 tables, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22659v2",
      "published_date": "2025-07-30 13:17:16 UTC",
      "updated_date": "2025-12-19 15:41:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:50.197176+00:00"
    },
    {
      "arxiv_id": "2508.00033v2",
      "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries",
      "title_zh": "GPT-4.1ï¼šåˆ©ç”¨æ–°å‹ Python åº“æ ‘ç«‹è‡ªåŠ¨åŒ–å®éªŒè®¾è®¡æ–°åŸºå‡†",
      "authors": [
        "Nuno Fachada",
        "Daniel Fernandes",
        "Carlos M. Fernandes",
        "Bruno D. Ferreira-Saraiva",
        "JoÃ£o P. Matos-Carvalho"
      ],
      "abstract": "Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \\textit{ParShift} library, and synthetic data generation and clustering using \\textit{pyclugen} and \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code. GPT-4.1 achieved a 100\\% success rate across all runs in both experimental tasks, whereas most other models succeeded in fewer than half of the runs, with only Grok-3 and Mistral-Large approaching comparable performance. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è§£é‡Šå’Œä½¿ç”¨é™Œç”Ÿ Python APIs è¿›è¡Œå¤æ‚ç§‘å­¦å®éªŒæ–¹é¢çš„èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç ”ç©¶äººå‘˜é€‰å–äº†å¤šç§æœ€å…ˆè¿›çš„ (state-of-the-art) LLMsï¼Œé€šè¿‡ zero-shot prompts å¼•å¯¼å…¶å®ŒæˆåŸºäº ParShiftã€pyclugen å’Œ scikit-learn ç­‰åº“çš„å¯¹è¯å¼æ•°æ®åˆ†æåŠåˆæˆæ•°æ®ç”Ÿæˆä¸èšç±»ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ‰€æœ‰æµ‹è¯•è¿è¡Œä¸­ï¼ŒGPT-4.1 åœ¨ä¸¤ä¸ªå®éªŒä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† 100% çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºå¤§å¤šæ•°æˆåŠŸç‡ä½äº 50% çš„æ¨¡å‹ï¼Œä»…æœ‰ Grok-3 å’Œ Mistral-Large è¡¨ç°å‡ºæ¥è¿‘çš„æ€§èƒ½ã€‚é™¤äº†è¯„ä¼°æ¨¡å‹æ€§èƒ½å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æœ‰æ•ˆè¯†åˆ«äº†ç¬¬ä¸‰æ–¹åº“ä¸­å­˜åœ¨çš„æ–‡æ¡£ä¸æ¸…æ™°æˆ–å®ç°é€»è¾‘æ¼æ´ç­‰é—®é¢˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹ç ”ç©¶æ­ç¤ºäº†å½“å‰ LLMs åœ¨ç«¯åˆ°ç«¯ç§‘å­¦è‡ªåŠ¨åŒ–é¢†åŸŸçš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„ promptã€å®Œå–„çš„åº“æ–‡æ¡£ä»¥åŠæ¨¡å‹èƒ½åŠ›æŒç»­è¿›æ­¥çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "The peer-reviewed version of this paper is published in Future Internet at https://doi.org/10.3390/fi17090412. This version is typeset by the author and differs only in pagination and typographical detail",
      "pdf_url": "https://arxiv.org/pdf/2508.00033v2",
      "published_date": "2025-07-30 13:11:29 UTC",
      "updated_date": "2025-09-15 18:54:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:41:55.254779+00:00"
    },
    {
      "arxiv_id": "2507.22640v1",
      "title": "Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction",
      "title_zh": "åŸºäºè¾“å…¥å‡¸åŠ¨ä½œä¿®æ­£çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ å®‰å…¨éƒ¨ç½²",
      "authors": [
        "Alex Durkin",
        "Jasper Stolte",
        "Matthew Jones",
        "Raghuraman Pitchumani",
        "Bei Li",
        "Christian Michler",
        "Mehmet MercangÃ¶z"
      ],
      "abstract": "Offline reinforcement learning (offline RL) offers a promising framework for developing control strategies in chemical process systems using historical data, without the risks or costs of online experimentation. This work investigates the application of offline RL to the safe and efficient control of an exothermic polymerisation continuous stirred-tank reactor. We introduce a Gymnasium-compatible simulation environment that captures the reactor's nonlinear dynamics, including reaction kinetics, energy balances, and operational constraints. The environment supports three industrially relevant scenarios: startup, grade change down, and grade change up. It also includes reproducible offline datasets generated from proportional-integral controllers with randomised tunings, providing a benchmark for evaluating offline RL algorithms in realistic process control tasks.\n  We assess behaviour cloning and implicit Q-learning as baseline algorithms, highlighting the challenges offline agents face, including steady-state offsets and degraded performance near setpoints. To address these issues, we propose a novel deployment-time safety layer that performs gradient-based action correction using input convex neural networks (PICNNs) as learned cost models. The PICNN enables real-time, differentiable correction of policy actions by descending a convex, state-conditioned cost surface, without requiring retraining or environment interaction.\n  Experimental results show that offline RL, particularly when combined with convex action correction, can outperform traditional control approaches and maintain stability across all scenarios. These findings demonstrate the feasibility of integrating offline RL with interpretable and safety-aware corrections for high-stakes chemical process control, and lay the groundwork for more reliable data-driven automation in industrial systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)åœ¨åŒ–å­¦è¿‡ç¨‹ç³»ç»Ÿå®‰å…¨é«˜æ•ˆæ§åˆ¶ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹é’ˆå¯¹å…·æœ‰éçº¿æ€§åŠ¨åŠ›å­¦ç‰¹å¾çš„æ”¾çƒ­èšåˆè¿ç»­æ…æ‹Œé‡œååº”å™¨(CSTR)ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªå…¼å®¹Gymnasiumçš„ä»¿çœŸç¯å¢ƒï¼Œæ¶µç›–äº†å¯åŠ¨ã€é™çº§åˆ‡æ¢å’Œå‡çº§åˆ‡æ¢ç­‰å·¥ä¸šç›¸å…³åœºæ™¯ï¼Œå¹¶æä¾›äº†å¯é‡å¤çš„ç¦»çº¿æ•°æ®é›†ã€‚è¯„ä¼°å‘ç°ï¼ŒBehavior Cloningå’ŒImplicit Q-Learningç­‰åŸºå‡†ç®—æ³•åœ¨éƒ¨ç½²æ—¶å¸¸é¢ä¸´ç¨³æ€åç§»å’Œè®¾å®šå€¼é™„è¿‘æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè¾“å…¥å‡¸ç¥ç»ç½‘ç»œ(PICNNs)çš„æ–°å‹éƒ¨ç½²æ—¶å®‰å…¨å±‚ï¼Œåˆ©ç”¨PICNNsä½œä¸ºå­¦ä¹ å¾—åˆ°çš„æˆæœ¬æ¨¡å‹è¿›è¡ŒåŸºäºæ¢¯åº¦çš„åŠ¨ä½œæ ¡æ­£ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å‡¸çš„ã€ä»¥çŠ¶æ€ä¸ºæ¡ä»¶çš„æˆæœ¬è¡¨é¢ä¸Šè¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå®ç°äº†ç­–ç•¥åŠ¨ä½œçš„å®æ—¶ã€å¯å¾®æ ¡æ­£ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOffline RLä¸å‡¸åŠ¨ä½œæ ¡æ­£ç›¸ç»“åˆçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ§åˆ¶æ–¹æ³•ï¼Œå¹¶åœ¨æ‰€æœ‰åœºæ™¯ä¸­ä¿æŒäº†ç³»ç»Ÿç¨³å®šæ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å°†æ•°æ®é©±åŠ¨æ–¹æ³•ä¸å®‰å…¨æ„è¯†æ ¡æ­£ç›¸ç»“åˆçš„å¯è¡Œæ€§ï¼Œä¸ºé«˜é£é™©å·¥ä¸šç³»ç»Ÿä¸­çš„å¯é è‡ªåŠ¨åŒ–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22640v1",
      "published_date": "2025-07-30 12:58:02 UTC",
      "updated_date": "2025-07-30 12:58:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:03.562608+00:00"
    },
    {
      "arxiv_id": "2507.22633v2",
      "title": "H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity",
      "title_zh": "H2Tuneï¼šé¢å‘æ··åˆå¼‚æ„æ€§çš„è”é‚¦åŸºåº§æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Wei Guo",
        "Siyuan Lu",
        "Yiqi Tong",
        "Zhaojun Hu",
        "Fuzhen Zhuang",
        "Xiao Zhang",
        "Tao Fan",
        "Jin Dong"
      ],
      "abstract": "Different from existing federated fine-tuning (FFT) methods for foundation models, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored scenario where clients exhibit double heterogeneity in model architectures and downstream tasks. This hybrid heterogeneity introduces two significant challenges: 1) heterogeneous matrix aggregation, where clients adopt different large-scale foundation models based on their task requirements and resource limitations, leading to dimensional mismatches during LoRA parameter aggregation; and 2) multi-task knowledge interference, where local shared parameters, trained with both task-shared and task-specific knowledge, cannot ensure only task-shared knowledge is transferred between clients. To address these challenges, we propose H2Tune, a federated foundation model fine-tuning with hybrid heterogeneity. Our framework H2Tune consists of three key components: (i) sparsified triple matrix decomposition to align hidden dimensions across clients through constructing rank-consistent middle matrices, with adaptive sparsification based on client resources; (ii) relation-guided matrix layer alignment to handle heterogeneous layer structures and representation capabilities; and (iii) alternating task-knowledge disentanglement mechanism to decouple shared and specific knowledge of local model parameters through alternating optimization. Theoretical analysis proves a convergence rate of O(1/\\sqrt{T}). Extensive experiments show our method achieves up to 15.4% accuracy improvement compared to state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/H2Tune-1407.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºç¡€æ¨¡å‹è”é‚¦å¾®è°ƒ(Federated Fine-Tuning)ä¸­çš„æ··åˆå¼‚æ„æ€§(Hybrid Heterogeneity)é—®é¢˜ï¼Œæå‡ºäº†H2Tuneæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®¢æˆ·ç«¯åœ¨æ¨¡å‹æ¶æ„å’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„åŒé‡å·®å¼‚ã€‚H2Tuneé€šè¿‡ç¨€ç–åŒ–ä¸‰çŸ©é˜µåˆ†è§£(Sparsified Triple Matrix Decomposition)æŠ€æœ¯æ„å»ºç§©ä¸€è‡´çš„ä¸­é—´çŸ©é˜µï¼Œæœ‰æ•ˆè§£å†³äº†LoRAå‚æ•°èšåˆæ—¶çš„ç»´åº¦ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶å®ç°äº†åŸºäºå®¢æˆ·ç«¯èµ„æºçš„è‡ªé€‚åº”ç¨€ç–åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å…³ç³»å¼•å¯¼çš„çŸ©é˜µå±‚å¯¹é½(Relation-guided Matrix Layer Alignment)æ¥å¤„ç†å¼‚æ„å±‚ç»“æ„ï¼Œå¹¶å¼•å…¥äº¤æ›¿ä»»åŠ¡çŸ¥è¯†è§£è€¦(Alternating Task-knowledge Disentanglement)æœºåˆ¶ï¼Œé€šè¿‡äº¤æ›¿ä¼˜åŒ–å®ç°æœ¬åœ°æ¨¡å‹ä¸­å…±äº«çŸ¥è¯†ä¸ç‰¹å®šçŸ¥è¯†çš„å‰¥ç¦»ã€‚ç†è®ºåˆ†æè¯æ˜äº†è¯¥ç®—æ³•å…·å¤‡ $O(1/\\sqrt{T})$ çš„æ”¶æ•›ç‡ï¼Œç¡®ä¿äº†å¤æ‚ç¯å¢ƒä¸‹çš„è®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒH2Tuneåœ¨å‡†ç¡®ç‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†å¤šè¾¾15.4%ï¼Œä¸ºè§£å†³å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹åœ¨å¼‚æ„è”é‚¦å­¦ä¹ åœºæ™¯ä¸‹çš„éƒ¨ç½²éš¾é¢˜æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22633v2",
      "published_date": "2025-07-30 12:53:18 UTC",
      "updated_date": "2025-07-31 01:43:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:20.387662+00:00"
    },
    {
      "arxiv_id": "2507.22627v2",
      "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing",
      "title_zh": "LOTS of Fashionï¼åŸºäºè‰å›¾-æ–‡æœ¬é…å¯¹çš„å¤šæ¡ä»¶å›¾åƒç”Ÿæˆ",
      "authors": [
        "Federico Girella",
        "Davide Talon",
        "Ziyue Liu",
        "Zanxi Ruan",
        "Yiming Wang",
        "Marco Cristani"
      ],
      "abstract": "Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶å°šè®¾è®¡ä¸­è§†è§‰è‰å›¾ä¸æ–‡æœ¬æè¿°ç»“åˆçš„å¤æ‚æ€§ï¼Œæå‡ºäº† LOTS (LOcalized Text and Sketch) æ¡†æ¶ï¼Œç”¨äºå®ç°åŸºäºå¤šæ¡ä»¶çš„ç»„åˆå¼æ—¶å°šå›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆå…¨å±€æè¿°ä¸å±€éƒ¨çš„è‰å›¾åŠ æ–‡æœ¬ (sketch + text) ä¿¡æ¯è¿›è¡Œçº¦æŸï¼Œå¹¶å¼•å…¥äº†æ¨¡å—åŒ–å¯¹ä¸­å¿ƒ (Modularized Pair-Centric) è¡¨ç¤ºæ³•å°†è¾“å…¥ç¼–ç è‡³å…±äº«æ½œç©ºé—´ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¡†æ¶é‡‡ç”¨ Diffusion Pair Guidance ç­–ç•¥ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶ (attention-based guidance) åœ¨æ‰©æ•£æ¨¡å‹çš„å»å™ªæ­¥éª¤ä¸­æ•´åˆå±€éƒ¨ä¸å…¨å±€å¼•å¯¼ã€‚ä¸ºæ”¯æŒè¯¥ç ”ç©¶ï¼Œä½œè€…åŸºäº Fashionpedia å‘å¸ƒäº†é¦–ä¸ªä¸ºæ¯å¼ å›¾åƒæä¾›å¤šç»„æ–‡æœ¬-è‰å›¾å¯¹çš„æ—¶å°šæ•°æ®é›† Sketchyã€‚å®šé‡å®éªŒè¡¨æ˜ LOTS åœ¨å…¨å±€å’Œå±€éƒ¨æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº† SOTA (state-of-the-art) æ€§èƒ½ï¼Œè€Œäººç±»è¯„ä¼°åˆ™è¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨è®¾è®¡å®šåˆ¶åŒ–æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV25 (Oral). Project page: https://intelligolabs.github.io/lots/",
      "pdf_url": "https://arxiv.org/pdf/2507.22627v2",
      "published_date": "2025-07-30 12:48:29 UTC",
      "updated_date": "2025-09-04 13:26:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:37.046508+00:00"
    },
    {
      "arxiv_id": "2507.22619v1",
      "title": "Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤ºå¢å¼ºåˆ¶é€ ä¸šçŸ¥è¯†è·å–",
      "authors": [
        "Sebastian Monka",
        "Irlan Grangel-GonzÃ¡lez",
        "Stefan Schmid",
        "Lavdim Halilaj",
        "Marc Rickart",
        "Oliver Rudolph",
        "Rui Dias"
      ],
      "abstract": "Knowledge graphs (KGs) have transformed data management within the manufacturing industry, offering effective means for integrating disparate data sources through shared and structured conceptual schemas. However, harnessing the power of KGs can be daunting for non-experts, as it often requires formulating complex SPARQL queries to retrieve specific information. With the advent of Large Language Models (LLMs), there is a growing potential to automatically translate natural language queries into the SPARQL format, thus bridging the gap between user-friendly interfaces and the sophisticated architecture of KGs. The challenge remains in adequately informing LLMs about the relevant context and structure of domain-specific KGs, e.g., in manufacturing, to improve the accuracy of generated queries. In this paper, we evaluate multiple strategies that use LLMs as mediators to facilitate information retrieval from KGs. We focus on the manufacturing domain, particularly on the Bosch Line Information System KG and the I40 Core Information Model. In our evaluation, we compare various approaches for feeding relevant context from the KG to the LLM and analyze their proficiency in transforming real-world questions into SPARQL queries. Our findings show that LLMs can significantly improve their performance on generating correct and complete queries when provided only the adequate context of the KG schema. Such context-aware prompting techniques help LLMs to focus on the relevant parts of the ontology and reduce the risk of hallucination. We anticipate that the proposed techniques help LLMs to democratize access to complex data repositories and empower informed decision-making in manufacturing settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤º (Context-aware Prompting) æŠ€æœ¯æ¥æé«˜åˆ¶é€ ä¸šçŸ¥è¯†å›¾è°± (KGs) çš„è®¿é—®æ•ˆç‡ã€‚é’ˆå¯¹éä¸“ä¸šäººå‘˜éš¾ä»¥æ„å»ºå¤æ‚ SPARQL æŸ¥è¯¢çš„é—®é¢˜ï¼Œæœ¬æ–‡è¯„ä¼°äº†å°†è‡ªç„¶è¯­è¨€è‡ªåŠ¨ç¿»è¯‘ä¸º SPARQL æ ¼å¼çš„å¤šç§ä¸­ä»‹ç­–ç•¥ã€‚ç ”ç©¶é‡ç‚¹é’ˆå¯¹åšä¸–ç”Ÿäº§çº¿ä¿¡æ¯ç³»ç»Ÿ (Bosch Line Information System KG) å’Œ I40 æ ¸å¿ƒä¿¡æ¯æ¨¡å‹ï¼Œé€šè¿‡å‘ LLMs è¾“å…¥ç›¸å…³çš„çŸ¥è¯†å›¾è°±æ¨¡å¼ (KG schema) ä¸Šä¸‹æ–‡æ¥ä¼˜åŒ–æŸ¥è¯¢ç”Ÿæˆã€‚å®éªŒå‘ç°ï¼Œæä¾›ç²¾ç®€ä¸”ç›¸å…³çš„æ¨¡å¼ä¸Šä¸‹æ–‡èƒ½æ˜¾è‘—æå‡ LLMs ç”Ÿæˆæ­£ç¡®æŸ¥è¯¢çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆå‡å°‘æ¨¡å‹å¹»è§‰ã€‚è¿™äº›æŠ€æœ¯æ—¨åœ¨é™ä½å¤æ‚åˆ¶é€ æ•°æ®ä»“åº“çš„ä½¿ç”¨é—¨æ§›ï¼Œè¿›è€Œèµ‹èƒ½å·¥ä¸šç”Ÿäº§ä¸­çš„çŸ¥æƒ…å†³ç­–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "European Conference on Artificial Intelligence (ECAI) 2024",
      "pdf_url": "https://arxiv.org/pdf/2507.22619v1",
      "published_date": "2025-07-30 12:39:01 UTC",
      "updated_date": "2025-07-30 12:39:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:25.945533+00:00"
    },
    {
      "arxiv_id": "2507.22612v2",
      "title": "Adaptive Duration Model for Text Speech Alignment",
      "title_zh": "é¢å‘æ–‡æœ¬-è¯­éŸ³å¯¹é½çš„è‡ªé€‚åº”æ—¶é•¿æ¨¡å‹",
      "authors": [
        "Junjie Cao"
      ],
      "abstract": "Speech-to-text alignment is a critical component of neural text to speech (TTS) models. Autoregressive TTS models typically use an attention mechanism to learn these alignments on-line, while non-autoregressive end to end TTS models rely on durations extracted from external sources. In this paper, we propose a novel duration prediction framework that can give promising phoneme-level duration distribution with given text. In our experiments, the proposed duration model has more precise prediction and adaptation ability to conditions, compared to previous baseline models. Specifically, it makes a considerable improvement on phoneme-level alignment accuracy and makes the performance of zero-shot TTS models more robust to the mismatch between prompt audio and input audio.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹ä¸­å…³é”®çš„è¯­éŸ³æ–‡æœ¬å¯¹é½é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªé€‚åº”æ—¶é•¿æ¨¡å‹(Adaptive Duration Model)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»™å®šæ–‡æœ¬ç”Ÿæˆç²¾å‡†çš„éŸ³ç´ çº§(phoneme-level)æ—¶é•¿åˆ†å¸ƒã€‚ç›¸æ¯”äºä¾èµ–åœ¨çº¿æ³¨æ„æœºåˆ¶æˆ–å¤–éƒ¨æ—¶é•¿æ¥æºçš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„é¢„æµ‹ç²¾åº¦å’Œæ¡ä»¶é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†éŸ³ç´ çº§å¯¹é½çš„å‡†ç¡®ç‡ï¼Œå¹¶ä½¿é›¶æ ·æœ¬(zero-shot)TTSæ¨¡å‹åœ¨é¢å¯¹æç¤ºéŸ³é¢‘ä¸è¾“å…¥éŸ³é¢‘ä¸åŒ¹é…çš„æƒ…å†µæ—¶å…·å¤‡æ›´å¼ºçš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–éŸ³ç´ çº§æ—¶é•¿é¢„æµ‹ï¼Œä¸ºæå‡éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³åˆæˆç³»ç»Ÿçš„æ€§èƒ½å’Œå¯é æ€§æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "4 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22612v2",
      "published_date": "2025-07-30 12:31:11 UTC",
      "updated_date": "2025-08-29 06:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:23.453762+00:00"
    },
    {
      "arxiv_id": "2507.22610v1",
      "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review",
      "title_zh": "æ·±åº¦ä»£ç æ¨¡å‹çš„èœ•å˜æµ‹è¯•ï¼šç³»ç»Ÿæ–‡çŒ®ç»¼è¿°",
      "authors": [
        "Ali Asgari",
        "Milan de Koning",
        "Pouria Derakhshanfar",
        "Annibale Panichella"
      ],
      "abstract": "Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate models' robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹é¢å‘ä»£ç æ™ºèƒ½çš„æ·±åº¦ä»£ç æ¨¡å‹(Deep Code Models)çš„å˜æ€æµ‹è¯•(Metamorphic Testing)è¿›è¡Œäº†ç³»ç»Ÿçš„æ–‡çŒ®ç»¼è¿°(Systematic Literature Review)ã€‚å°½ç®¡å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä»£ç è¡¥å…¨ã€ç¼ºé™·æ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†å…¶åœ¨å¤šå˜æˆ–å¯¹æŠ—æ€§æ¡ä»¶ä¸‹çš„é²æ£’æ€§(Robustness)ä»æ˜¯ä¸€ä¸ªå…³é”®çš„è´¨é‡æŒ‘æˆ˜ã€‚å˜æ€æµ‹è¯•é€šè¿‡å¯¹è¾“å…¥ç¨‹åºåº”ç”¨ä¿æŒè¯­ä¹‰çš„è½¬æ¢(Semantic-preserving transformations)å¹¶åˆ†ææ¨¡å‹è¾“å‡ºçš„ç¨³å®šæ€§ï¼Œå·²æˆä¸ºè¯„ä¼°è¿™ç±»æ¨¡å‹é²æ£’æ€§çš„ä¸»æµæ–¹æ³•ã€‚é€šè¿‡å¯¹45ç¯‡æ ¸å¿ƒè®ºæ–‡çš„ç ”ç©¶ï¼Œè¯¥ç»¼è¿°æ·±å…¥åˆ†æäº†ç”¨äºè¯„ä¼°é²æ£’æ€§çš„å„ç§è½¬æ¢ç­–ç•¥ã€æŠ€æœ¯å’Œè¯„ä»·æ‰‹æ®µã€‚æ–‡ç« å…¨é¢æ€»ç»“äº†å½“å‰çš„ç ”ç©¶ç°çŠ¶ï¼Œæ˜ç¡®äº†é¢‘ç¹è¯„ä¼°çš„æ¨¡å‹ã€ç¼–ç¨‹ä»»åŠ¡ã€æ•°æ®é›†ã€ç›®æ ‡è¯­è¨€ä»¥åŠè¯„ä¼°æŒ‡æ ‡ã€‚æœ€åï¼Œè®ºæ–‡è¯†åˆ«äº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ¨åŠ¨æ·±åº¦ä»£ç æ¨¡å‹æµ‹è¯•æŠ€æœ¯çš„å‘å±•æŒ‡æ˜äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22610v1",
      "published_date": "2025-07-30 12:25:30 UTC",
      "updated_date": "2025-07-30 12:25:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:27.848228+00:00"
    },
    {
      "arxiv_id": "2507.22607v2",
      "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning",
      "title_zh": "VL-Cogitoï¼šé¢å‘é«˜çº§å¤šæ¨¡æ€æ¨ç†çš„æ¸è¿›å¼è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Ruifeng Yuan",
        "Chenghao Xiao",
        "Sicong Leng",
        "Jianyu Wang",
        "Long Li",
        "Weiwen Xu",
        "Hou Pong Chan",
        "Deli Zhao",
        "Tingyang Xu",
        "Zhongyu Wei",
        "Hao Zhang",
        "Yu Rong"
      ],
      "abstract": "Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VL-Cogitoï¼Œä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šæ ·ä¸”è¯­ä¹‰ä¸°å¯Œç­‰å¤šæ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°ä¸ç¨³å®šçš„å±€é™ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ›æ–°çš„å¤šé˜¶æ®µæ¸è¿›å¼è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ (Progressive Curriculum Reinforcement Learning, PCuRL)æ¡†æ¶ï¼Œå¼•å¯¼æ¨¡å‹ä»æ˜“åˆ°éš¾å¤„ç†ä»»åŠ¡ï¼Œä»è€Œç³»ç»Ÿæ€§åœ°æå‡å…¶åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚PCuRLå¼•å…¥äº†åœ¨çº¿éš¾åº¦è½¯åŠ æƒæœºåˆ¶(online difficulty soft weighting)æ¥åŠ¨æ€è°ƒæ•´ä¸åŒé˜¶æ®µçš„è®­ç»ƒéš¾åº¦ï¼Œå¹¶ç»“åˆåŠ¨æ€é•¿åº¦å¥–åŠ±æœºåˆ¶(dynamic length reward mechanism)å¹³è¡¡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVL-Cogitoåœ¨æ•°å­¦ã€ç§‘å­¦ã€é€»è¾‘å’Œé€šç”¨ç†è§£ç­‰ä¸»æµå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¸€è‡´è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰çš„æ¨ç†å¯¼å‘å‹æ¨¡å‹ï¼Œæœ‰æ•ˆéªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 5 figures, 6 tables. Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2507.22607v2",
      "published_date": "2025-07-30 12:23:21 UTC",
      "updated_date": "2025-07-31 09:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:37.487385+00:00"
    },
    {
      "arxiv_id": "2507.22606v1",
      "title": "MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines",
      "title_zh": "MetaAgentï¼šåŸºäºæœ‰é™çŠ¶æ€æœºè‡ªåŠ¨æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Yaolun Zhang",
        "Xiaogeng Liu",
        "Chaowei Xiao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid communication structures. In this paper, we propose MetaAgent, a finite state machine based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaAgentï¼Œä¸€ç§åŸºäºæœ‰é™çŠ¶æ€æœº (Finite State Machines, FSM) çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè‡ªåŠ¨æ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–è®¾è®¡æ–¹æ³•åœ¨å·¥å…·é›†æˆã€æ•°æ®ä¾èµ–å’Œé€šä¿¡ç»“æ„æŸ”æ€§æ–¹é¢çš„å±€é™æ€§ã€‚ç»™å®šç‰¹å®šä»»åŠ¡æè¿°åï¼ŒMetaAgent èƒ½å¤Ÿè‡ªåŠ¨è®¾è®¡å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–ç®—æ³•å¯¹å…¶è¿›è¡Œè¿­ä»£å®Œå–„ã€‚åœ¨ç³»ç»Ÿéƒ¨ç½²é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æœ‰é™çŠ¶æ€æœºç²¾ç¡®æ§åˆ¶æ™ºèƒ½ä½“çš„åŠ¨ä½œæ‰§è¡ŒåŠå…¶çŠ¶æ€è½¬ç§»é€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMetaAgent åœ¨æ–‡æœ¬ä»»åŠ¡å’Œå®é™…åº”ç”¨ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºå…¶ä»–è‡ªåŠ¨è®¾è®¡æ–¹æ³•ã€‚ç ”ç©¶è¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„ç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šå¯ä¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡äººå·¥ä¼˜åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç›¸åª²ç¾ï¼Œä¸ºå®ç°é«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„æ™ºèƒ½ä½“ååŒæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22606v1",
      "published_date": "2025-07-30 12:22:30 UTC",
      "updated_date": "2025-07-30 12:22:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:33.036485+00:00"
    },
    {
      "arxiv_id": "2507.22603v1",
      "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models",
      "title_zh": "BALSAMï¼šé˜¿æ‹‰ä¼¯è¯­å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•å¹³å°",
      "authors": [
        "Rawan Al-Matham",
        "Kareem Darwish",
        "Raghad Al-Rasheed",
        "Waad Alshammari",
        "Muneera Alhoshan",
        "Amal Almazrua",
        "Asma Al Wazrah",
        "Mais Alheraki",
        "Firoj Alam",
        "Preslav Nakov",
        "Norah Alzahrani",
        "Eman alBilali",
        "Nizar Habash",
        "Abdelrahman El-Sheikh",
        "Muhammad Elmallah",
        "Haonan Li",
        "Hamdy Mubarak",
        "Mohamed Anwar",
        "Zaid Alyafeai",
        "Ahmed Abdelali",
        "Nora Altwairesh",
        "Maram Hasanain",
        "Abdulmohsen Al Thubaity",
        "Shady Shehata",
        "Bashar Alhafni",
        "Injy Hamed",
        "Go Inoue",
        "Khalid Elmadani",
        "Ossama Obeid",
        "Fatima Haouari",
        "Tamer Elsayed",
        "Emad Alghamdi",
        "Khalid Almubarak",
        "Saied Alshahrani",
        "Ola Aljarrah",
        "Safa Alajlan",
        "Areej Alshaqarawi",
        "Maryam Alshihri",
        "Sultana Alghurabi",
        "Atikah Alzeghayer",
        "Afrah Altamimi",
        "Abdullah Alfaifi",
        "Abdulrahman AlOsaimy"
      ],
      "abstract": "The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­å¤§è¯­è¨€æ¨¡å‹ (Arabic Large Language Models) ç”±äºæ•°æ®ç¨€ç¼ºã€è¯­è¨€å¤šæ ·æ€§åŠå½¢æ€å¤æ‚æ€§ç­‰å¯¼è‡´çš„å‘å±•æ»åé—®é¢˜ï¼Œæå‡ºäº† BALSAM è¯„æµ‹å¹³å°ã€‚BALSAM æ˜¯ä¸€ä¸ªç”±ç¤¾åŒºé©±åŠ¨çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§„èŒƒé˜¿æ‹‰ä¼¯è¯­å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¼€å‘ä¸è¯„ä¼°ã€‚è¯¥å¹³å°æ¶µç›–äº† 14 ä¸ªå¤§ç±»çš„ 78 é¡¹è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ï¼ŒåŒ…å« 5.2 ä¸‡ä¸ªç¤ºä¾‹ï¼Œå¹¶å°†å…¶åˆ’åˆ†ä¸º 3.7 ä¸‡ä¸ªæµ‹è¯•é›† (test) å’Œ 1.5 ä¸‡ä¸ªå¼€å‘é›† (development)ã€‚ä¸ºäº†åº”å¯¹æ•°æ®æ±¡æŸ“å’Œè¿›åº¦è¡¡é‡å›°éš¾ï¼ŒBALSAM æä¾›äº†ä¸€ä¸ªé›†ä¸­å¼ã€é€æ˜çš„ç›²æµ‹ (blind evaluation) è¯„ä¼°æœºåˆ¶ã€‚è¿™ä¸€å¹³å°çš„æ¨å‡ºä¸ºé˜¿æ‹‰ä¼¯è¯­å¤§è¯­è¨€æ¨¡å‹ (Arabic LLM) çš„ç ”å‘è®¾å®šäº†ç»Ÿä¸€æ ‡å‡†ï¼Œé€šè¿‡ä¿ƒè¿›åä½œç ”ç©¶ï¼Œæ—¨åœ¨æœ‰æ•ˆæå‡é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹çš„ç»¼åˆèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22603v1",
      "published_date": "2025-07-30 12:16:39 UTC",
      "updated_date": "2025-07-30 12:16:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:36.158323+00:00"
    },
    {
      "arxiv_id": "2507.22580v1",
      "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment",
      "title_zh": "RePaCAï¼šåŸºäºæ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„é™æ€è‡ªåŠ¨åŒ–è¡¥ä¸æ­£ç¡®æ€§è¯„ä¼°",
      "authors": [
        "Marcos Fuster-Pena",
        "David de-Fitero-Dominguez",
        "Antonio Garcia-Cabot",
        "Eva Garcia-Lopez"
      ],
      "abstract": "Automated Program Repair (APR) seeks to automatically correct software bugs without requiring human intervention. However, existing tools tend to generate patches that satisfy test cases without fixing the underlying bug, those are known as overfitting patches. To address this issue, Automated Patch Correctness Assessment (APCA) attempts to identify overfitting patches generated by APR tools. It can be solved as a static approach, meaning that no additional information is needed beyond the original and fixed code snippets. Current static techniques often struggle with reliability, flexibility and transparency. To address these issues, we introduce RePaCA, a novel static APCA technique that leverages Large Language Models (LLMs) specialized in thinking tasks. Our model is prompted with both buggy and fixed code snippets and guided to generate a Chain of Thought that analyses code differences, reasons about how the patch addresses the root cause, and ultimately provides a binary classification: correct or overfitting. To enhance these reasoning capabilities for the APCA task specifically, the LLM is finetuned using Reinforcement Learning with the Group Relative Policy Optimization algorithm. When evaluated on a standard Defects4J-derived test, our approach achieves state-of-the-art performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model demonstrates superior generalization capabilities when trained on different datasets, outperforming the leading technique. This reasoning capability also provides enhanced explainability for the patch assessment. These findings underscore the considerable promise of finetuned, reasoning LLMs to advance static APCA by enhancing accuracy, generalization, and explainability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨ç¨‹åºä¿®å¤(Automated Program Repair, APR)ç”Ÿæˆçš„è¡¥ä¸å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆ(overfitting)çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºRePaCAçš„æ–°å‹é™æ€è‡ªåŠ¨è¡¥ä¸æ­£ç¡®æ€§è¯„ä¼°(Automated Patch Correctness Assessment, APCA)æŠ€æœ¯ã€‚RePaCAåˆ©ç”¨æ“…é•¿æ€è€ƒä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹(Reasoning LLMs)ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹ç”Ÿæˆé“¾å¼æ€ç»´(Chain of Thought)æ¥åˆ†æé”™è¯¯ä»£ç ä¸ä¿®å¤ä»£ç ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å¯¹è¡¥ä¸æ˜¯å¦ä»æ ¹æœ¬ä¸Šä¿®å¤äº†æ¼æ´è¿›è¡Œé€»è¾‘æ¨ç†ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹åœ¨APCAä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†åŸºäºGroup Relative Policy Optimization (GRPO)ç®—æ³•çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRePaCAåœ¨Defects4JåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†state-of-the-artæ€§èƒ½ï¼Œå…¶å‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«è¾¾åˆ°83.1%å’Œ84.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šå±•ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”å…¶æ¨ç†è¿‡ç¨‹æ˜¾è‘—æå‡äº†è¡¥ä¸è¯„ä¼°çš„å¯è§£é‡Šæ€§(explainability)ã€‚è¿™é¡¹ç ”ç©¶å……åˆ†è¯æ˜äº†ç»è¿‡å¾®è°ƒçš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹åœ¨æé«˜é™æ€APCAå‡†ç¡®æ€§ã€æ³›åŒ–æ€§å’Œé€æ˜åº¦æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22580v1",
      "published_date": "2025-07-30 11:21:09 UTC",
      "updated_date": "2025-07-30 11:21:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:46.853489+00:00"
    },
    {
      "arxiv_id": "2507.22577v1",
      "title": "A Mean-Field Theory of $Î˜$-Expectations",
      "title_zh": "$\\Theta$-æœŸæœ›çš„å¹³å‡åœºç†è®º",
      "authors": [
        "Qian Qi"
      ],
      "abstract": "The canonical theory of sublinear expectations, a foundation of stochastic calculus under ambiguity, is insensitive to the non-convex geometry of primitive uncertainty models. This paper develops a new stochastic calculus for a structured class of such non-convex models. We introduce a class of fully coupled Mean-Field Forward-Backward Stochastic Differential Equations where the BSDE driver is defined by a pointwise maximization over a law-dependent, non-convex set. Mathematical tractability is achieved via a uniform strong concavity assumption on the driver with respect to the control variable, which ensures the optimization admits a unique and stable solution. A central contribution is to establish the Lipschitz stability of this optimizer from primitive geometric and regularity conditions, which underpins the entire well-posedness theory. We prove local and global well-posedness theorems for the FBSDE system. The resulting valuation functional, the $Î˜$-Expectation, is shown to be dynamically consistent and, most critically, to violate the axiom of sub-additivity. This, along with its failure to be translation invariant, demonstrates its fundamental departure from the convex paradigm. This work provides a rigorous foundation for stochastic calculus under a class of non-convex, endogenous ambiguity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¬¡çº¿æ€§æœŸæœ›(sublinear expectations)ç†è®ºå¯¹éå‡¸å‡ ä½•æ¨¡å‹ä¸æ•æ„Ÿçš„å±€é™æ€§ï¼Œå¼€å‘äº†ä¸€å¥—é€‚ç”¨äºæ­¤ç±»éå‡¸æ¨¡å‹çš„æ–°å‹éšæœºåˆ†æ(stochastic calculus)æ¡†æ¶ã€‚ç ”ç©¶è€…å¼•å…¥äº†ä¸€ç±»å…¨è€¦åˆçš„å¹³å‡åœºå‰å‘-åå‘éšæœºå¾®åˆ†æ–¹ç¨‹(Mean-Field Forward-Backward Stochastic Differential Equations, FBSDEs)ï¼Œå…¶é©±åŠ¨é¡¹é€šè¿‡åœ¨ä¸åˆ†å¸ƒç›¸å…³çš„éå‡¸é›†ä¸Šè¿›è¡Œé€ç‚¹æœ€å¤§åŒ–æ¥å®šä¹‰ã€‚é€šè¿‡å¯¹é©±åŠ¨é¡¹æ–½åŠ å…³äºæ§åˆ¶å˜é‡çš„å‡åŒ€å¼ºå‡¹æ€§(uniform strong concavity)å‡è®¾ï¼Œç¡®ä¿äº†ä¼˜åŒ–é—®é¢˜çš„å”¯ä¸€æ€§å’Œç¨³å®šæ€§ï¼Œå¹¶ä»åŸå§‹å‡ ä½•ä¸æ­£åˆ™æ€§æ¡ä»¶ä¸­ç¡®ç«‹äº†ä¼˜åŒ–å™¨çš„ Lipschitz stabilityã€‚è®ºæ–‡è¯æ˜äº†è¯¥ FBSDE ç³»ç»Ÿåœ¨å±€éƒ¨å’Œå…¨å±€èŒƒå›´å†…çš„è‰¯å®šæ€§(well-posedness)ï¼Œå¹¶æå‡ºäº†åä¸º $\\Theta$-Expectation çš„ä¼°å€¼æ³›å‡½ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œ$\\Theta$-Expectation åœ¨ä¿æŒåŠ¨æ€ä¸€è‡´æ€§(dynamically consistent)çš„åŒæ—¶ï¼Œè¿åäº†æ¬¡å¯åŠ æ€§(sub-additivity)å’Œå¹³ç§»ä¸å˜æ€§(translation invariant)ï¼Œä»æ ¹æœ¬ä¸Šè„±ç¦»äº†ä¼ ç»Ÿçš„å‡¸èŒƒå¼ã€‚è¯¥å·¥ä½œä¸ºå¤„ç†éå‡¸ã€å†…ç”Ÿä¸ç¡®å®šæ€§(endogenous ambiguity)ä¸‹çš„éšæœºåˆ†æå¥ å®šäº†ä¸¥è°¨çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "math.PR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.PR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22577v1",
      "published_date": "2025-07-30 11:08:56 UTC",
      "updated_date": "2025-07-30 11:08:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:42:51.346665+00:00"
    },
    {
      "arxiv_id": "2507.22576v1",
      "title": "COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP",
      "title_zh": "COOkeDï¼šé›¶æ ·æœ¬ CLIP æ—¶ä»£ä¸‹åŸºäºé›†æˆçš„ OOD æ£€æµ‹",
      "authors": [
        "Galadrielle Humblot-Renaux",
        "Gianni Franchi",
        "Sergio Escalera",
        "Thomas B. Moeslund"
      ],
      "abstract": "Out-of-distribution (OOD) detection is an important building block in trustworthy image recognition systems as unknown classes may arise at test-time. OOD detection methods typically revolve around a single classifier, leading to a split in the research field between the classical supervised setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot setting (class names fed as prompts to CLIP). In both cases, an overarching challenge is that the OOD detection performance is implicitly constrained by the classifier's capabilities on in-distribution (ID) data. In this work, we show that given a little open-mindedness from both ends, remarkable OOD detection can be achieved by instead creating a heterogeneous ensemble - COOkeD combines the predictions of a closed-world classifier trained end-to-end on a specific dataset, a zero-shot CLIP classifier, and a linear probe classifier trained on CLIP image features. While bulky at first sight, this approach is modular, post-hoc and leverages the availability of pre-trained VLMs, thus introduces little overhead compared to training a single standard classifier. We evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also consider more challenging, realistic settings ranging from training-time label noise, to test-time covariate shift, to zero-shot shift which has been previously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art performance and greater robustness compared to both classical and CLIP-based OOD detection methods. Code is available at https://github.com/glhr/COOkeD",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†COOkeDï¼Œä¸€ç§é’ˆå¯¹åˆ†å¸ƒå¤–(Out-of-distribution, OOD)æ£€æµ‹çš„å¼‚æ„é›†æˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å›¾åƒè¯†åˆ«ç³»ç»Ÿä¸­æœªçŸ¥ç±»åˆ«çš„è¯†åˆ«æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆç«¯åˆ°ç«¯è®­ç»ƒçš„é—­ä¸–ç•Œåˆ†ç±»å™¨(closed-world classifier)ã€é›¶æ ·æœ¬CLIPåˆ†ç±»å™¨(zero-shot CLIP classifier)ä»¥åŠåŸºäºCLIPç‰¹å¾çš„çº¿æ€§æ¢æµ‹åˆ†ç±»å™¨(linear probe)ï¼Œå…‹æœäº†å•ä¸€æ¨¡å‹åœ¨åˆ†å¸ƒå†…æ•°æ®æ€§èƒ½å¯¹OODæ£€æµ‹çš„çº¦æŸã€‚COOkeDé‡‡ç”¨æ¨¡å—åŒ–ä¸”äº‹åå¤„ç†(post-hoc)çš„è®¾è®¡ï¼Œåœ¨å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¼˜åŠ¿çš„åŒæ—¶ï¼Œä¿æŒäº†æä½çš„é¢å¤–è®¡ç®—å¼€é”€ã€‚ç ”ç©¶äººå‘˜åœ¨CIFAR100ã€ImageNetä»¥åŠåŒ…å«æ ‡ç­¾å™ªå£°ã€åå˜é‡åç§»å’Œé›¶æ ·æœ¬åç§»çš„å¤æ‚ç°å®åœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå°½ç®¡å…¶ç»“æ„ç®€å•ï¼ŒCOOkeDåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³(SOTA)ï¼Œä¸”ç›¸è¾ƒäºä¼ ç»Ÿæˆ–å•çº¯åŸºäºCLIPçš„æ£€æµ‹æ–¹æ³•å±•ç°å‡ºæ›´ä¼˜çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at ICCVW'25 - Systematic Trust in AI Models: Ensuring Fairness, Reliability, Explainability, and Accountability in Machine Learning Frameworks",
      "pdf_url": "https://arxiv.org/pdf/2507.22576v1",
      "published_date": "2025-07-30 11:02:38 UTC",
      "updated_date": "2025-07-30 11:02:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:12.497277+00:00"
    },
    {
      "arxiv_id": "2507.22570v1",
      "title": "Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity",
      "title_zh": "æ·±åº¦ç½‘ç»œçŸ©é˜µåˆ†ç±»çš„è§£é‡Šï¼šä»¥å•è°ƒæ€§ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Leandro Farina",
        "Sergey Korotov"
      ],
      "abstract": "This work demonstrates a methodology for using deep learning to discover simple, practical criteria for classifying matrices based on abstract algebraic properties. By combining a high-performance neural network with explainable AI (XAI) techniques, we can distill a model's learned strategy into human-interpretable rules. We apply this approach to the challenging case of monotone matrices, defined by the condition that their inverses are entrywise nonnegative. Despite their simple definition, an easy characterization in terms of the matrix elements or the derived parameters is not known. Here, we present, to the best of our knowledge, the first systematic machine-learning approach for deriving a practical criterion that distinguishes monotone from non-monotone matrices. After establishing a labelled dataset by randomly generated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ deep neural network algorithms for classifying the matrices as monotone or non-monotone, using both their entries and a comprehensive set of matrix features. By saliency methods, such as integrated gradients, we identify among all features, two matrix parameters which alone provide sufficient information for the matrix classification, with $95\\%$ accuracy, namely the absolute values of the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's characteristic polynomial. A data-driven study of 18,000 random $7\\times7$ matrices shows that the monotone class obeys $\\lvert c_{0}/c_{1}\\rvert\\le0.18$ with probability $>99.98\\%$; because $\\lvert c_{0}/c_{1}\\rvert = 1/\\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound $\\mathrm{tr}(A^{-1})\\ge5.7$.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†ä¸€ç§åˆ©ç”¨æ·±åº¦å­¦ä¹ (Deep Learning)å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æŠ€æœ¯ä»çŸ©é˜µæŠ½è±¡ä»£æ•°å±æ€§ä¸­æå–ç®€æ˜åˆ†ç±»å‡†åˆ™çš„æ–¹æ³•è®ºã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†å•è°ƒçŸ©é˜µ(Monotone Matrices)çš„åˆ†ç±»æŒ‘æˆ˜ï¼Œé€šè¿‡è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå¹¶åº”ç”¨é›†æˆæ¢¯åº¦(Integrated Gradients)ç­‰æ˜¾è‘—æ€§æ–¹æ³•ï¼ŒæˆåŠŸå°†å¤æ‚çš„æ¨¡å‹ç­–ç•¥æç‚¼ä¸ºäººç±»å¯ç†è§£çš„è§„åˆ™ã€‚å®éªŒå‘ç°ï¼ŒçŸ©é˜µç‰¹å¾å¤šé¡¹å¼ä¸­æœ€ä½é˜¶çš„ä¸¤ä¸ªç³»æ•° $c_0$ å’Œ $c_1$ çš„ç»å¯¹å€¼æ˜¯è¯†åˆ«å•è°ƒæ€§çš„å…³é”®ç‰¹å¾ï¼Œä»…å‡­è¿™ä¸¤ä¸ªå‚æ•°å³å¯è¾¾åˆ°95%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚å¯¹18,000ä¸ªéšæœº $7\\times7$ çŸ©é˜µçš„æ•°æ®é©±åŠ¨ç ”ç©¶è¡¨æ˜ï¼Œå•è°ƒçŸ©é˜µä»¥è¶…è¿‡99.98%çš„æ¦‚ç‡æ»¡è¶³ $|c_0/c_1| \\le 0.18$ è¿™ä¸€ç®€å•ä¸ç­‰å¼ã€‚è¯¥æ¡ä»¶è¿›ä¸€æ­¥ç­‰ä»·äº $\\text{tr}(A^{-1}) \\ge 5.7$ çš„æ•°å­¦ç•Œé™ï¼Œä¸ºåˆ¤å®šçŸ©é˜µå•è°ƒæ€§æä¾›äº†ä¸€ä¸ªç›´è§‚ä¸”å®ç”¨çš„æ ‡å‡†ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†æœºå™¨å­¦ä¹ åœ¨è¾…åŠ©æ•°å­¦å‘ç°å’Œå»ºç«‹ä»£æ•°æ€§è´¨åˆ¤å®šå‡†åˆ™æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 11 figures. To be submitted to a journal",
      "pdf_url": "https://arxiv.org/pdf/2507.22570v1",
      "published_date": "2025-07-30 10:55:44 UTC",
      "updated_date": "2025-07-30 10:55:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:13.486588+00:00"
    },
    {
      "arxiv_id": "2507.22565v1",
      "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå·®åˆ†éšç§å¾®è°ƒ",
      "authors": [
        "Afshin Khadangi",
        "Amir Sartipi",
        "Igor Tchappi",
        "Ramin Bahmani",
        "Gilbert Fridgen"
      ],
      "abstract": "The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($Îµ$, $Î´$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†æ•æ„Ÿæ•°æ®æ—¶é¢ä¸´çš„éšç§ä¸æ•ˆç”¨æƒè¡¡éš¾é¢˜ï¼Œæå‡ºäº†RLDPæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªå°†å·®åˆ†éšç§(Differentially Private)ä¼˜åŒ–å»ºæ¨¡ä¸ºé—­ç¯æ§åˆ¶é—®é¢˜çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (RL)æ–¹æ¡ˆã€‚ä¼ ç»Ÿçš„DP-SGDæ–¹æ³•å—é™äºç¡¬ç¼–ç ä¸”å…¨å±€ç»Ÿä¸€çš„æ§åˆ¶å‚æ•°ï¼Œè€ŒRLDPåˆ©ç”¨Soft Actor-Critic (SAC)ç­–ç•¥åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ„ŸçŸ¥å­¦ä¹ çŠ¶æ€ï¼Œå¹¶åœ¨çº¿å­¦ä¹ å¦‚ä½•è‡ªåŠ¨è°ƒæ•´æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦è£å‰ª(gradient-clipping)é˜ˆå€¼å’Œæ³¨å…¥çš„é«˜æ–¯å™ªå£°(Gaussian noise)å¼ºåº¦ã€‚é€šè¿‡å¯¹GPT2-smallã€LlamaåŠMistralç­‰æ¨¡å‹è¿›è¡Œçš„å¤§é‡å®éªŒï¼ŒRLDPå®ç°äº†å¹³å‡5.4%çš„å›°æƒ‘åº¦(perplexity)é™ä½å’Œ5.6%çš„ä¸‹æ¸¸æ•ˆç”¨æå‡ã€‚æ­¤å¤–ï¼ŒRLDPä»…éœ€åŸºçº¿æ¨¡å‹13-43%çš„æ¢¯åº¦æ›´æ–°é¢„ç®—å³å¯è¾¾åˆ°åŒç­‰æ•ˆç”¨ï¼Œåœ¨æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ï¼Œä¸¥æ ¼éµå®ˆ($Îµ$, $Î´$)-DPéšç§åè®®ï¼Œå¹¶å±•ç°å‡ºå¯¹æˆå‘˜æ¨ç†æ”»å‡»(membership-inference attacks)åŠé‡‘ä¸é›€æå–æ”»å‡»(canary-extraction attacks)æ›´å¼ºçš„æŠµå¾¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22565v1",
      "published_date": "2025-07-30 10:46:53 UTC",
      "updated_date": "2025-07-30 10:46:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:15.885904+00:00"
    },
    {
      "arxiv_id": "2507.22564v2",
      "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs",
      "title_zh": "åˆ©ç”¨ååŒè®¤çŸ¥åå·®ç»•è¿‡å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æœºåˆ¶",
      "authors": [
        "Xikang Yang",
        "Biyu Zhou",
        "Xuehai Tang",
        "Jizhong Han",
        "Songlin Hu"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢å¯¹åˆ©ç”¨è®¤çŸ¥åå·®(cognitive biases)è¿›è¡Œå¯¹æŠ—æ€§æ”»å‡»æ—¶çš„è„†å¼±æ€§ï¼Œæ­ç¤ºäº†å®‰å…¨æœºåˆ¶ä¸­çš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚ä¸åŒäºä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹æˆ–ç®—æ³•æ“çºµï¼Œä½œè€…æå‡ºäº†åä¸ºCognitiveAttackçš„çº¢é˜Ÿæµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ©ç”¨å•ä¸€åŠç»„åˆçš„è®¤çŸ¥åå·®æ¥ç»•è¿‡å®‰å…¨é˜²æŠ¤ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒ(SFT)å’Œå¼ºåŒ–å­¦ä¹ (RL)æŠ€æœ¯ï¼Œç”Ÿæˆèƒ½å¤ŸåµŒå…¥ä¼˜åŒ–åå·®ç»„åˆçš„æç¤ºï¼Œä»è€Œåœ¨ä¿æŒé«˜æ”»å‡»æˆåŠŸç‡(ASR)çš„åŒæ—¶æœ‰æ•ˆè§„é¿å®‰å…¨åè®®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨30ç§ä¸åŒçš„LLMsï¼ˆç‰¹åˆ«æ˜¯å¼€æºæ¨¡å‹ï¼‰ä¸­å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼ŒCognitiveAttackçš„æ”»å‡»æˆåŠŸç‡è¾¾åˆ°60.1%ï¼Œè¿œè¶…ç°æœ‰å…ˆè¿›é»‘ç›’æ–¹æ³•PAPçš„31.6%ã€‚è¯¥å‘ç°å¼ºè°ƒäº†å¤šé‡åå·®äº¤äº’ä½œä¸ºä¸€ä¸ªå¼ºå¤§ä¸”å°šæœªè¢«å……åˆ†æ¢ç´¢çš„æ”»å‡»å‘é‡çš„é£é™©ï¼Œæš´éœ²äº†å½“å‰é˜²å¾¡æœºåˆ¶çš„å±€é™æ€§ã€‚é€šè¿‡å°†è®¤çŸ¥ç§‘å­¦ä¸LLMå®‰å…¨ç›¸ç»“åˆï¼Œè¯¥å·¥ä½œä¸ºå¼€å‘æ›´å…·é²æ£’æ€§å’Œäººç±»å¯¹é½(human-aligned)çš„AIç³»ç»Ÿæä¾›äº†é‡è¦çš„è·¨å­¦ç§‘è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22564v2",
      "published_date": "2025-07-30 10:40:53 UTC",
      "updated_date": "2025-11-17 09:00:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:23.890227+00:00"
    },
    {
      "arxiv_id": "2507.22558v2",
      "title": "aLLoyM: A large language model for alloy phase diagram prediction",
      "title_zh": "aLLoyMï¼šç”¨äºåˆé‡‘ç›¸å›¾é¢„æµ‹çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yuna Oikawa",
        "Guillaume Deffrennes",
        "Taichi Abe",
        "Ryo Tamura",
        "Koji Tsuda"
      ],
      "abstract": "Large Language Models (LLMs) are general-purpose tools with wide-ranging applications, including in materials science. In this work, we introduce aLLoyM, a fine-tuned LLM specifically trained on alloy compositions, temperatures, and their corresponding phase information. To develop aLLoyM, we curated question-and-answer (Q&A) pairs for binary and ternary phase diagrams using the open-source Computational Phase Diagram Database (CPDDB) and assessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned Mistral, an open-source pre-trained LLM, for two distinct Q&A formats: multiple-choice and short-answer. Benchmark evaluations demonstrate that fine-tuning substantially enhances performance on multiple-choice phase diagram questions. Moreover, the short-answer model of aLLoyM exhibits the ability to generate novel phase diagrams from its components alone, underscoring its potential to accelerate the discovery of previously unexplored materials systems. To promote further research and adoption, we have publicly released the short-answer fine-tuned version of aLLoyM, along with the complete benchmarking Q&A dataset, on Hugging Face.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†aLLoyMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åˆé‡‘ç›¸å›¾é¢„æµ‹çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å¼€æºè®¡ç®—ç›¸å›¾æ•°æ®åº“(CPDDB)å’ŒåŸºäºCALPHADçš„è¯„ä¼°æ•°æ®ï¼Œæ„å»ºäº†æ¶µç›–äºŒå…ƒå’Œä¸‰å…ƒç›¸å›¾çš„é—®ç­”(Q&A)æ•°æ®é›†ã€‚ä»–ä»¬åœ¨å¼€æºæ¨¡å‹Mistralçš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹å¤šé€‰é¢˜å’Œç®€ç­”é¢˜ä¸¤ç§æ ¼å¼è¿›è¡Œäº†å¾®è°ƒã€‚åŸºå‡†è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šé€‰é¢˜å½¢å¼ä¸‹å¤„ç†ç›¸å›¾ç›¸å…³é—®é¢˜çš„æ€§èƒ½ã€‚ç®€ç­”é¢˜ç‰ˆæœ¬çš„aLLoyMè¡¨ç°å‡ºä»…å‡­ç»„åˆ†ä¿¡æ¯å³å¯ç”Ÿæˆæ–°å‹ç›¸å›¾çš„èƒ½åŠ›ï¼Œå½°æ˜¾äº†å…¶åœ¨åŠ é€Ÿæ¢ç´¢æœªçŸ¥ææ–™ç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶å·²åœ¨Hugging Faceä¸Šå…¬å¼€å‘å¸ƒäº†å¾®è°ƒåçš„ç®€ç­”æ¨¡å‹åŠå®Œæ•´çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›ææ–™ç§‘å­¦é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "24 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.22558v2",
      "published_date": "2025-07-30 10:32:39 UTC",
      "updated_date": "2025-08-10 04:46:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:20.793801+00:00"
    },
    {
      "arxiv_id": "2507.22553v1",
      "title": "RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning",
      "title_zh": "RainbowPromptï¼šé¢å‘æŒç»­å­¦ä¹ çš„å¤šæ ·æ€§å¢å¼ºæç¤ºæ¼”åŒ–æœºåˆ¶",
      "authors": [
        "Kiseong Hong",
        "Gyeong-hyeon Kim",
        "Eunwoo Kim"
      ],
      "abstract": "Prompt-based continual learning provides a rehearsal-free solution by tuning small sets of parameters while keeping pre-trained models frozen. To meet the complex demands of sequential tasks, it is crucial to integrate task-specific knowledge within prompts effectively. However, existing works rely on either fixed learned prompts (i.e., prompts whose representations remain unchanged during new task learning) or on prompts generated from an entangled task-shared space, limiting the representational diversity of the integrated prompt. To address this issue, we propose a novel prompt-evolving mechanism to adaptively aggregate base prompts (i.e., task-specific prompts) into a unified prompt while ensuring diversity. By transforming and aligning base prompts, both previously learned and newly introduced, our approach continuously evolves accumulated knowledge to facilitate learning new tasks. We further introduce a learnable probabilistic gate that adaptively determines which layers to activate during the evolution process. We validate our method on image classification and video action recognition tasks in class-incremental learning, achieving average gains of 9.07% and 7.40% over existing methods across all scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæç¤º(Prompt-based)çš„æŒç»­å­¦ä¹ (Continual Learning)ä¸­ç°æœ‰æ–¹æ³•ç”±äºä¾èµ–å›ºå®šæç¤ºæˆ–çº ç¼ çš„ä»»åŠ¡å…±äº«ç©ºé—´è€Œå¯¼è‡´è¡¨ç¤ºå¤šæ ·æ€§(Representational Diversity)å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†RainbowPromptæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æç¤ºæ¼”åŒ–(Prompt-evolving)æœºåˆ¶ï¼Œé€šè¿‡å˜æ¢å’Œå¯¹é½å…ˆå‰å­¦ä¹ åŠæ–°å¼•å…¥çš„åŸºç¡€æç¤º(Base Prompts)ï¼Œå°†å…¶è‡ªé€‚åº”åœ°èšåˆä¸ºç»Ÿä¸€æç¤ºä»¥å®ç°çŸ¥è¯†çš„æŒç»­æ¼”è¿›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„æ¦‚ç‡é—¨(Probabilistic Gate)ï¼Œç”¨äºåœ¨æ¼”åŒ–è¿‡ç¨‹ä¸­åŠ¨æ€å†³å®šæ¨¡å‹å±‚çš„æ¿€æ´»çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRainbowPromptåœ¨å›¾åƒåˆ†ç±»(Image Classification)å’Œè§†é¢‘åŠ¨ä½œè¯†åˆ«(Video Action Recognition)çš„ç±»å¢é‡å­¦ä¹ (Class-incremental Learning)ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç›¸æ¯”äºç°æœ‰ä¸»æµæ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¸åŒåœºæ™¯ä¸‹åˆ†åˆ«å®ç°äº†9.07%å’Œ7.40%çš„å¹³å‡æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚é¡ºåºä»»åŠ¡éœ€æ±‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by the 2025 IEEE/CVF International Conference on Computer Vision (ICCV 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.22553v1",
      "published_date": "2025-07-30 10:25:28 UTC",
      "updated_date": "2025-07-30 10:25:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:31.195911+00:00"
    },
    {
      "arxiv_id": "2507.22539v2",
      "title": "A surrogate model for topology optimisation of elastic structures via parametric autoencoders",
      "title_zh": "åŸºäºå‚æ•°åŒ–è‡ªåŠ¨ç¼–ç å™¨çš„å¼¹æ€§ç»“æ„æ‹“æ‰‘ä¼˜åŒ–ä»£ç†æ¨¡å‹",
      "authors": [
        "Matteo Giacomini",
        "Antonio Huerta"
      ],
      "abstract": "A surrogate-based topology optimisation algorithm for linear elastic structures under parametric loads and boundary conditions is proposed. Instead of learning the parametric solution of the state (and adjoint) problems or the optimisation trajectory as a function of the iterations, the proposed approach devises a surrogate version of the entire optimisation pipeline. First, the method predicts a quasi-optimal topology for a given problem configuration as a surrogate model of high-fidelity topologies optimised with the homogenisation method. This is achieved by means of a feed-forward net learning the mapping between the input parameters characterising the system setup and a latent space determined by encoder/decoder blocks reducing the dimensionality of the parametric topology optimisation problem and reconstructing a high-dimensional representation of the topology. Then, the predicted topology is used as an educated initial guess for a computationally efficient algorithm penalising the intermediate values of the design variable, while enforcing the governing equations of the system. This step allows the method to correct potential errors introduced by the surrogate model, eliminate artifacts, and refine the design in order to produce topologies consistent with the underlying physics. Different architectures are proposed and the approximation and generalisation capabilities of the resulting models are numerically evaluated. The quasi-optimal topologies allow to outperform the high-fidelity optimiser by reducing the average number of optimisation iterations by $53\\%$ while achieving discrepancies below $4\\%$ in the optimal value of the objective functional, even in the challenging scenario of testing the model to extrapolate beyond the training and validation domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»£ç†æ¨¡å‹(Surrogate Model)çš„æ‹“æ‰‘ä¼˜åŒ–(Topology Optimisation)ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å—å‚æ•°åŒ–è½½è·å’Œè¾¹ç•Œæ¡ä»¶å½±å“çš„çº¿æ€§å¼¹æ€§ç»“æ„è®¾è®¡é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å‚æ•°åŒ–è‡ªç¼–ç å™¨(Parametric Autoencoders)çš„ç¼–ç å™¨/è§£ç å™¨æ¨¡å—å®ç°é™ç»´ï¼Œå¹¶åˆ©ç”¨å‰é¦ˆç¥ç»ç½‘ç»œ(Feed-forward Net)å­¦ä¹ ç³»ç»Ÿå‚æ•°ä¸æ½œç©ºé—´(Latent Space)ä¹‹é—´çš„æ˜ å°„ï¼Œä»è€Œå¿«é€Ÿé¢„æµ‹å‡†æœ€ä¼˜æ‹“æ‰‘ã€‚éšåï¼Œè¯¥é¢„æµ‹ç»“æœä½œä¸ºåˆå§‹å€¼å¼•å…¥é«˜æ•ˆçš„ç»†åŒ–ç®—æ³•ä¸­ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œç‰©ç†æ§åˆ¶æ–¹ç¨‹æ¥ä¿®æ­£ä»£ç†æ¨¡å‹çš„è¯¯å·®å¹¶æ¶ˆé™¤è®¾è®¡ä¼ªå½±ã€‚æ•°å€¼è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç›®æ ‡å‡½æ•°è¯¯å·®ä½äº4%çš„åŒæ—¶ï¼Œå°†å¹³å‡ä¼˜åŒ–è¿­ä»£æ¬¡æ•°å‡å°‘äº†53%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è¶…å‡ºè®­ç»ƒå’ŒéªŒè¯èŒƒå›´çš„æ¨æ–­ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†ç»“æ„ä¼˜åŒ–çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "math.NA",
      "comment": "43 pages, 13 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.22539v2",
      "published_date": "2025-07-30 10:07:42 UTC",
      "updated_date": "2025-10-21 11:59:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:31.993710+00:00"
    },
    {
      "arxiv_id": "2507.22533v2",
      "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records",
      "title_zh": "CliCAREï¼šåŸºäºä¸´åºŠæŒ‡å—çš„ç™Œç—‡çºµå‘ç”µå­ç—…å†å¤§è¯­è¨€æ¨¡å‹å†³ç­–æ”¯æŒ",
      "authors": [
        "Dongchen Li",
        "Jitao Liang",
        "Wei Li",
        "Xiaoyu Wang",
        "Longbing Cao",
        "Kun Yu"
      ],
      "abstract": "Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and fragmented nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these settings, CliCARE significantly outperforms baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by oncologists.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CliCAREæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¤„ç†çºµå‘ç™Œç—‡ç”µå­å¥åº·è®°å½•(EHRs)ä»¥æä¾›ä¸´åºŠå†³ç­–æ”¯æŒã€‚é’ˆå¯¹EHRsè®°å½•å†—é•¿ç¢ç‰‡åŒ–ã€ä¸´åºŠå¹»è§‰é£é™©ä»¥åŠè¯„ä¼°æŒ‡æ ‡ä¸å¯é ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼ŒCliCAREå°†éç»“æ„åŒ–çš„çºµå‘æ•°æ®è½¬åŒ–ä¸ºæ‚£è€…ç‰¹æœ‰çš„æ—¶åºçŸ¥è¯†å›¾è°±(Temporal Knowledge Graphs, TKGs)ä»¥æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†çœŸå®çš„æ‚£è€…è½¨è¿¹ä¸è§„èŒƒçš„ä¸´åºŠæŒ‡å—çŸ¥è¯†å›¾è°±è¿›è¡Œå¯¹é½ï¼Œç¡®ä¿å†³ç­–è¿‡ç¨‹å…·å¤‡è¯æ®æ”¯æ’‘ï¼Œä»è€Œç”Ÿæˆé«˜ä¿çœŸçš„ä¸´åºŠæ€»ç»“ä¸å¯æ“ä½œçš„æ²»ç–—å»ºè®®ã€‚åœ¨ç§æœ‰ä¸­å›½ç™Œç—‡æ•°æ®é›†å’Œå…¬å¼€çš„MIMIC-IVæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCliCAREçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„é•¿ä¸Šä¸‹æ–‡LLMså’ŒçŸ¥è¯†å›¾è°±å¢å¼ºçš„RAGæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡ä¸€å¥—ä¸è‚¿ç˜¤ç§‘åŒ»ç”Ÿè¯„ä¼°é«˜åº¦ä¸€è‡´çš„ç¨³å¥è¯„ä¼°åè®®éªŒè¯äº†ç³»ç»Ÿçš„ä¸´åºŠæœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³AIåœ¨è‚¿ç˜¤å­¦é¢†åŸŸçš„è½åœ°éš¾é¢˜æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in AAAI Conference on Artificial Intelligence (AAAI-26, Oral)",
      "pdf_url": "https://arxiv.org/pdf/2507.22533v2",
      "published_date": "2025-07-30 10:02:16 UTC",
      "updated_date": "2026-01-09 07:56:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:40.603001+00:00"
    },
    {
      "arxiv_id": "2507.22530v2",
      "title": "HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors",
      "title_zh": "HRVVSï¼šåŸºäºåˆ†å±‚è‡ªå›å½’æ®‹å·®å…ˆéªŒçš„é«˜åˆ†è¾¨ç‡è§†é¢‘è¡€ç®¡åˆ†å‰²ç½‘ç»œ",
      "authors": [
        "Xincheng Yao",
        "Yijun Yang",
        "Kangwei Guo",
        "Ruiqiang Xiao",
        "Haipeng Zhou",
        "Haisu Tao",
        "Jian Yang",
        "Lei Zhu"
      ],
      "abstract": "The segmentation of the hepatic vasculature in surgical videos holds substantial clinical significance in the context of hepatectomy procedures. However, owing to the dearth of an appropriate dataset and the inherently complex task characteristics, few researches have been reported in this domain. To address this issue, we first introduce a high quality frame-by-frame annotated hepatic vasculature dataset containing 35 long hepatectomy videos and 11442 high-resolution frames. On this basis, we propose a novel high-resolution video vasculature segmentation network, dubbed as HRVVS. We innovatively embed a pretrained visual autoregressive modeling (VAR) model into different layers of the hierarchical encoder as prior information to reduce the information degradation generated during the downsampling process. In addition, we designed a dynamic memory decoder on a multi-view segmentation network to minimize the transmission of redundant information while preserving more details between frames. Extensive experiments on surgical video datasets demonstrate that our proposed HRVVS significantly outperforms the state-of-the-art methods. The source code and dataset will be publicly available at \\{https://github.com/scott-yjyang/HRVVS}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‚åˆ‡é™¤æ‰‹æœ¯ä¸­è‚è„è„‰ç®¡ç³»ç»Ÿåˆ†å‰²é¢ä¸´çš„æ•°æ®é›†åŒ®ä¹åŠä»»åŠ¡å¤æ‚æ€§æŒ‘æˆ˜ï¼Œé¦–å…ˆæå‡ºäº†ä¸€ä¸ªåŒ…å«35ä¸ªé•¿è§†é¢‘å’Œ11442å¼ é«˜åˆ†è¾¨ç‡å›¾åƒçš„é«˜è´¨é‡é€å¸§æ ‡æ³¨æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºHRVVSçš„é«˜åˆ†è¾¨ç‡è§†é¢‘è„‰ç®¡åˆ†å‰²ç½‘ç»œï¼Œåˆ›æ–°æ€§åœ°å°†é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’å»ºæ¨¡(VAR)æ¨¡å‹ä½œä¸ºå…ˆéªŒä¿¡æ¯åµŒå…¥åˆ†å±‚ç¼–ç å™¨ä¸­ï¼Œä»¥å‡å°‘ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡æ¯é€€åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç½‘ç»œè®¾è®¡äº†ä¸€ä¸ªåŸºäºå¤šè§†å›¾åˆ†å‰²ç½‘ç»œçš„åŠ¨æ€è®°å¿†è§£ç å™¨(dynamic memory decoder)ï¼Œåœ¨æœ€å°åŒ–å†—ä½™ä¿¡æ¯ä¼ è¾“çš„åŒæ—¶ä¿ç•™äº†æ›´å¤šçš„å¸§é—´ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHRVVSåœ¨æ‰‹æœ¯è§†é¢‘æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºä¸´åºŠæ‰‹æœ¯ä¸­çš„ç²¾å‡†è„‰ç®¡åˆ†å‰²æä¾›äº†é«˜è´¨é‡çš„æ•°æ®æ”¯æ’‘ä¸é«˜æ•ˆçš„ç®—æ³•æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22530v2",
      "published_date": "2025-07-30 09:57:38 UTC",
      "updated_date": "2025-07-31 03:01:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:41.190766+00:00"
    },
    {
      "arxiv_id": "2507.22529v1",
      "title": "Accident-Driven Congestion Prediction and Simulation: An Explainable Framework Using Advanced Clustering and Bayesian Networks",
      "title_zh": "äº‹æ•…é©±åŠ¨çš„äº¤é€šæ‹¥å µé¢„æµ‹ä¸ä»¿çœŸï¼šåŸºäºé«˜çº§èšç±»å’Œè´å¶æ–¯ç½‘ç»œçš„å¯è§£é‡Šæ¡†æ¶",
      "authors": [
        "Kranthi Kumar Talluri",
        "Galia Weidl",
        "Vaishnavi Kasuluru"
      ],
      "abstract": "Traffic congestion due to uncertainties, such as accidents, is a significant issue in urban areas, as the ripple effect of accidents causes longer delays, increased emissions, and safety concerns. To address this issue, we propose a robust framework for predicting the impact of accidents on congestion. We implement Automated Machine Learning (AutoML)-enhanced Deep Embedding Clustering (DEC) to assign congestion labels to accident data and predict congestion probability using a Bayesian Network (BN). The Simulation of Urban Mobility (SUMO) simulation is utilized to evaluate the correctness of BN predictions using evidence-based scenarios. Results demonstrate that the AutoML-enhanced DEC has outperformed traditional clustering approaches. The performance of the proposed BN model achieved an overall accuracy of 95.6%, indicating its ability to understand the complex relationship of accidents causing congestion. Validation in SUMO with evidence-based scenarios demonstrated that the BN model's prediction of congestion states closely matches those of SUMO, indicating the high reliability of the proposed BN model in ensuring smooth urban mobility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”±äº¤é€šäº‹æ•…å¼•å‘çš„åŸå¸‚äº¤é€šæ‹¥å µé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆå…ˆè¿›èšç±»ç®—æ³•ä¸Bayesian Networksçš„å¯è§£é‡Šæ€§é¢„æµ‹ä¸ä»¿çœŸæ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆé‡‡ç”¨AutoMLå¢å¼ºçš„æ·±åº¦åµŒå…¥èšç±»(Deep Embedding Clustering, DEC)ä¸ºäº‹æ•…æ•°æ®åˆ†é…æ‹¥å µæ ‡ç­¾ï¼Œéšååˆ©ç”¨è´å¶æ–¯ç½‘ç»œ(BN)é¢„æµ‹æ‹¥å µæ¦‚ç‡å¹¶è§£æäº‹æ•…ä¸æ‹¥å µä¹‹é—´çš„å¤æ‚å…³è”ã€‚é€šè¿‡ä½¿ç”¨åŸå¸‚ç§»åŠ¨æ€§ä»¿çœŸè½¯ä»¶(Simulation of Urban Mobility, SUMO)è¿›è¡ŒåŸºäºè¯æ®çš„åœºæ™¯éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶ä¸­çš„AutoML-enhanced DECæ˜¾è‘—ä¼˜äºä¼ ç»Ÿèšç±»æ–¹æ³•ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œæ‰€æBNæ¨¡å‹è¾¾åˆ°äº†95.6%çš„ç»¼åˆå‡†ç¡®ç‡ï¼Œä¸”å…¶é¢„æµ‹çŠ¶æ€ä¸SUMOä»¿çœŸç»“æœé«˜åº¦å¥‘åˆã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ¡†æ¶åœ¨ç†è§£äº‹æ•…å¼•å‘æ‹¥å µçš„å¤æ‚é€»è¾‘æ–¹é¢å…·æœ‰æé«˜çš„å¯é æ€§ï¼Œä¸ºä¼˜åŒ–åŸå¸‚äº¤é€šæµåŠ¨æ€§æä¾›äº†æœ‰æ•ˆçš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22529v1",
      "published_date": "2025-07-30 09:57:08 UTC",
      "updated_date": "2025-07-30 09:57:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:43:48.397074+00:00"
    },
    {
      "arxiv_id": "2507.22522v1",
      "title": "Recognizing Actions from Robotic View for Natural Human-Robot Interaction",
      "title_zh": "é¢å‘è‡ªç„¶äººæœºäº¤äº’çš„æœºå™¨äººè§†è§’åŠ¨ä½œè¯†åˆ«",
      "authors": [
        "Ziyi Wang",
        "Peiming Li",
        "Hong Liu",
        "Zhichao Deng",
        "Can Wang",
        "Jun Liu",
        "Junsong Yuan",
        "Mengyuan Liu"
      ],
      "abstract": "Natural Human-Robot Interaction (N-HRI) requires robots to recognize human actions at varying distances and states, regardless of whether the robot itself is in motion or stationary. This setup is more flexible and practical than conventional human action recognition tasks. However, existing benchmarks designed for traditional action recognition fail to address the unique complexities in N-HRI due to limited data, modalities, task categories, and diversity of subjects and environments. To address these challenges, we introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored specifically for perception-centric robotic views prevalent in mobile service robots. ACTIVE comprises 30 composite action categories, 80 participants, and 46,868 annotated video instances, covering both RGB and point cloud modalities. Participants performed various human actions in diverse environments at distances ranging from 3m to 50m, while the camera platform was also mobile, simulating real-world scenarios of robot perception with varying camera heights due to uneven ground. This comprehensive and challenging benchmark aims to advance action and attribute recognition research in N-HRI. Furthermore, we propose ACTIVE-PC, a method that accurately perceives human actions at long distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic Ellipse Query, and precise decoupling of kinematic interference from human actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our code is available at: https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªç„¶äººæœºäº¤äº’ (Natural Human-Robot Interaction, N-HRI) ä¸­æœºå™¨äººéœ€åœ¨ä¸åŒè·ç¦»å’Œè¿åŠ¨çŠ¶æ€ä¸‹è¯†åˆ«åŠ¨ä½œçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å¤§è§„æ¨¡æ•°æ®é›† ACTIVE (Action from Robotic View)ã€‚è¯¥æ•°æ®é›†åŒ…å«30ä¸ªå¤åˆåŠ¨ä½œç±»åˆ«ã€80åå‚ä¸è€…åŠ46,868ä¸ªæ ‡æ³¨è§†é¢‘å®ä¾‹ï¼ŒåŒæ—¶æ¶µç›– RGB å’Œç‚¹äº‘ (point cloud) æ¨¡æ€ï¼Œæ¨¡æ‹Ÿäº†3ç±³è‡³50ç±³è¿œè·ç¦»åŠæœºå™¨äººç§»åŠ¨ç­‰å¤æ‚ç°å®åœºæ™¯ã€‚ä¸ºäº†æå‡é•¿è·ç¦»è¯†åˆ«ç²¾åº¦ï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æå‡ºäº† ACTIVE-PC æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¤šçº§é‚»åŸŸé‡‡æ · (Multilevel Neighborhood Sampling)ã€åˆ†å±‚è¯†åˆ«å™¨ (Layered Recognizers) å’Œå¼¹æ€§æ¤­åœ†æŸ¥è¯¢ (Elastic Ellipse Query)ï¼Œå¹¶æœ‰æ•ˆå®ç°äº†è¿åŠ¨å­¦å¹²æ‰°ä¸äººä½“åŠ¨ä½œçš„è§£è€¦ã€‚å®éªŒç»“æœéªŒè¯äº† ACTIVE-PC åœ¨å¤„ç†æ„ŸçŸ¥ä¸­å¿ƒåŒ–æœºå™¨äººè§†è§’ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºæ¨åŠ¨ N-HRI é¢†åŸŸçš„åŠ¨ä½œä¸å±æ€§è¯†åˆ«ç ”ç©¶æä¾›äº†é‡è¦åŸºå‡†ä¸æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures, Accepted to ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22522v1",
      "published_date": "2025-07-30 09:48:34 UTC",
      "updated_date": "2025-07-30 09:48:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:03.588325+00:00"
    },
    {
      "arxiv_id": "2507.22504v2",
      "title": "Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic Matching Approach",
      "title_zh": "ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„ååŒåŒ»ç–—åˆ†è¯Šï¼šåŸºäºå¤šæ™ºèƒ½ä½“åŠ¨æ€åŒ¹é…çš„æ–¹æ³•",
      "authors": [
        "Hongyan Cheng",
        "Chengzhang Yu",
        "Yanshu Shi",
        "Chiyue Wang",
        "Cong Liu",
        "Zhanpeng Jin"
      ],
      "abstract": "The post-pandemic surge in healthcare demand, coupled with critical nursing shortages, has placed unprecedented pressure on medical triage systems, necessitating innovative AI-driven solutions. We present a multi-agent interactive intelligent system for medical triage that addresses three fundamental challenges in current AI-based triage systems: inadequate medical specialization leading to misclassification, heterogeneous department structures across healthcare institutions, and inefficient detail-oriented questioning that impedes rapid triage decisions. Our system employs three specialized agents--RecipientAgent, InquirerAgent, and DepartmentAgent--that collaborate through Inquiry Guidance mechanism and Classification Guidance Mechanism to transform unstructured patient symptoms into accurate department recommendations. To ensure robust evaluation, we constructed a comprehensive Chinese medical triage dataset from \"Ai Ai Yi Medical Network\", comprising 3,360 real-world cases spanning 9 primary departments and 62 secondary departments. Experimental results demonstrate that our multi-agent system achieves 89.6% accuracy in primary department classification and 74.3% accuracy in secondary department classification after four rounds of patient interaction. The system's dynamic matching based guidance mechanisms enable efficient adaptation to diverse hospital configurations while maintaining high triage accuracy. We successfully developed this multi-agent triage system that not only adapts to organizational heterogeneity across healthcare institutions but also ensures clinically sound decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åç–«æƒ…æ—¶ä»£åŒ»ç–—éœ€æ±‚æ¿€å¢åŠæŠ¤ç†èµ„æºçŸ­ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“äº¤äº’å¼æ™ºèƒ½åŒ»ç–—åˆ†è¯Šç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰äººå·¥æ™ºèƒ½åˆ†è¯Šä¸­ä¸“ä¸šåŒ–ä¸è¶³ã€åŒ»é™¢ç§‘å®¤ç»“æ„å¼‚æ„åŠæé—®æ•ˆç‡ä½ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿç”± RecipientAgentã€InquirerAgent å’Œ DepartmentAgent ä¸‰ä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“ç»„æˆï¼Œé€šè¿‡ Inquiry Guidance mechanism å’Œ Classification Guidance Mechanism åä½œå°†éç»“æ„åŒ–ç—‡çŠ¶è½¬åŒ–ä¸ºç²¾ç¡®çš„ç§‘å®¤æ¨èã€‚ä¸ºäº†è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Œç ”ç©¶è€…åˆ©ç”¨â€œçˆ±çˆ±åŒ»åŒ»å­¦ç½‘â€æ„å»ºäº†ä¸€ä¸ªåŒ…å« 3,360 ä¸ªçœŸå®æ¡ˆä¾‹ã€æ¶µç›– 9 ä¸ªä¸€çº§ç§‘å®¤å’Œ 62 ä¸ªäºŒçº§ç§‘å®¤çš„ç»¼åˆä¸­æ–‡åŒ»ç–—åˆ†è¯Šæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å››è½®äº¤äº’åå®ç°äº†ä¸€çº§ç§‘å®¤ 89.6% å’ŒäºŒçº§ç§‘å®¤ 74.3% çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚ç³»ç»Ÿé‡‡ç”¨çš„åŸºäºåŠ¨æ€åŒ¹é…çš„å¼•å¯¼æœºåˆ¶ä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆé€‚åº”å¤šæ ·åŒ–çš„åŒ»é™¢é…ç½®ï¼Œåœ¨åº”å¯¹åŒ»ç–—æœºæ„ç»„ç»‡å¼‚æ„æ€§çš„åŒæ—¶ï¼Œç¡®ä¿äº†ä¸´åºŠå†³ç­–çš„ç§‘å­¦æ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 8 figures, 2 table",
      "pdf_url": "https://arxiv.org/pdf/2507.22504v2",
      "published_date": "2025-07-30 09:21:59 UTC",
      "updated_date": "2025-08-04 13:40:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:06.556211+00:00"
    },
    {
      "arxiv_id": "2507.22499v1",
      "title": "LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning",
      "title_zh": "LoReUnï¼šæ•°æ®æœ¬èº«è•´å«æå‡æœºå™¨é—å¿˜æ€§èƒ½çš„çº¿ç´¢",
      "authors": [
        "Xiang Li",
        "Qianli Shen",
        "Haonan Wang",
        "Kenji Kawaguchi"
      ],
      "abstract": "Recent generative models face significant risks of producing harmful content, which has underscored the importance of machine unlearning (MU) as a critical technique for eliminating the influence of undesired data. However, existing MU methods typically assign the same weight to all data to be forgotten, which makes it difficult to effectively forget certain data that is harder to unlearn than others. In this paper, we empirically demonstrate that the loss of data itself can implicitly reflect its varying difficulty. Building on this insight, we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective plug-and-play strategy that dynamically reweights data during the unlearning process with minimal additional computational overhead. Our approach significantly reduces the gap between existing MU methods and exact unlearning in both image classification and generation tasks, effectively enhancing the prevention of harmful content generation in text-to-image diffusion models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹äº§ç”Ÿæœ‰å®³å†…å®¹çš„é£é™©ï¼Œæ¢è®¨äº†æœºå™¨å¸è½½(Machine Unlearning, MU)è¿™ä¸€æ¶ˆé™¤ç‰¹å®šæ•°æ®å½±å“çš„å…³é”®æŠ€æœ¯ï¼Œå¹¶æŒ‡å‡ºå½“å‰æ–¹æ³•æ™®éå¿½ç•¥äº†ä¸åŒæ•°æ®åœ¨å¸è½½éš¾åº¦ä¸Šçš„å·®å¼‚ã€‚ä½œè€…é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œæ•°æ®è‡ªèº«çš„æŸå¤±(Loss)èƒ½å¤Ÿéšå«åœ°åæ˜ å…¶å¸è½½çš„éš¾æ˜“ç¨‹åº¦ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œè®ºæ–‡æå‡ºäº†LoReUn (Loss-based Reweighting Unlearning)ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ã€æœ‰æ•ˆä¸”å…·å¤‡å³æ’å³ç”¨ç‰¹æ€§çš„ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨å¸è½½è¿‡ç¨‹ä¸­é€šè¿‡åŠ¨æ€è°ƒæ•´æ•°æ®æƒé‡ï¼Œä»…éœ€æå°çš„é¢å¤–è®¡ç®—å¼€é”€å³å¯æ˜¾è‘—æå‡æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒLoReUnåœ¨å›¾åƒåˆ†ç±»å’Œç”Ÿæˆä»»åŠ¡ä¸­å‡å¤§å¹…ç¼©å°äº†ç°æœ‰MUæ–¹æ³•ä¸å®Œå…¨å¸è½½(Exact Unlearning)ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æœ‰æ•ˆå¢å¼ºæ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹åœ¨é¢„é˜²æœ‰å®³å†…å®¹ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22499v1",
      "published_date": "2025-07-30 09:12:25 UTC",
      "updated_date": "2025-07-30 09:12:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:09.371339+00:00"
    },
    {
      "arxiv_id": "2507.22498v2",
      "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping",
      "title_zh": "åŸºäºé¢‘è°±ç©ºé—´åˆ†ç»„çš„é²æ£’æ¶åŠ£å¤©æ°”å»é™¤",
      "authors": [
        "Yuhwan Jeong",
        "Yunseo Yang",
        "Youngho Yoon",
        "Kuk-Jin Yoon"
      ],
      "abstract": "Adverse weather conditions cause diverse and complex degradation patterns, driving the development of All-in-One (AiO) models. However, recent AiO solutions still struggle to capture diverse degradations, since global filtering methods like direct operations on the frequency domain fail to handle highly variable and localized distortions. To address these issue, we propose Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that leverages spectral decomposition and group-wise attention for multi-weather image restoration. SSGformer decomposes images into high-frequency edge features using conventional edge detection and low-frequency information via Singular Value Decomposition. We utilize multi-head linear attention to effectively model the relationship between these features. The fused features are integrated with the input to generate a grouping-mask that clusters regions based on the spatial similarity and image texture. To fully leverage this mask, we introduce a group-wise attention mechanism, enabling robust adverse weather removal and ensuring consistent performance across diverse weather conditions. We also propose a Spatial Grouping Transformer Block that uses both channel attention and spatial attention, effectively balancing feature-wise relationships and spatial dependencies. Extensive experiments show the superiority of our approach, validating its effectiveness in handling the varied and intricate adverse weather degradations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨èƒ½å‹(All-in-One)æ¨¡å‹åœ¨å¤„ç†å¤æ‚å¤šå˜çš„å±€éƒ¨å¤©æ°”é€€åŒ–æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†Spectral-based Spatial Grouping Transformer (SSGformer)æ¡†æ¶ç”¨äºå¤šå¤©æ°”å›¾åƒæ¢å¤ã€‚SSGformeråˆ©ç”¨å…‰è°±åˆ†è§£æŠ€æœ¯ï¼Œç»“åˆè¾¹ç¼˜æ£€æµ‹æå–é«˜é¢‘è¾¹ç¼˜ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£(Singular Value Decomposition)è·å–ä½é¢‘ä¿¡æ¯ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šå¤´çº¿æ€§æ³¨æ„åŠ›(Multi-head linear attention)å»ºæ¨¡ç‰¹å¾é—´å…³ç³»ï¼Œå¹¶ç»“åˆè¾“å…¥ç”ŸæˆåŸºäºç©ºé—´ç›¸ä¼¼æ€§å’Œå›¾åƒçº¹ç†çš„Grouping-maskã€‚é€šè¿‡å¼•å…¥Group-wise attentionæœºåˆ¶ï¼ŒSSGformerèƒ½å¤Ÿé’ˆå¯¹ä¸åŒå¤©æ°”æ¡ä»¶å®ç°ç¨³å¥çš„ç‰¹å¾å¤„ç†ï¼Œç¡®ä¿äº†æ€§èƒ½çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºçš„Spatial Grouping Transformer Blocké›†æˆäº†é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆå¹³è¡¡äº†ç‰¹å¾å…³ç³»ä¸ç©ºé—´ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSGformeråœ¨å¤„ç†å„ç§å¤æ‚æ¶åŠ£å¤©æ°”é€€åŒ–æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by ICCV25",
      "pdf_url": "https://arxiv.org/pdf/2507.22498v2",
      "published_date": "2025-07-30 09:08:34 UTC",
      "updated_date": "2025-07-31 10:38:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:13.487809+00:00"
    },
    {
      "arxiv_id": "2507.22493v1",
      "title": "LVM-GP: Uncertainty-Aware PDE Solver via coupling latent variable model and Gaussian process",
      "title_zh": "LVM-GPï¼šé€šè¿‡éšå˜é‡æ¨¡å‹ä¸é«˜æ–¯è¿‡ç¨‹è€¦åˆçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥åå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨",
      "authors": [
        "Xiaodong Feng",
        "Ling Guo",
        "Xiaoliang Wan",
        "Hao Wu",
        "Tao Zhou",
        "Wenwen Zhou"
      ],
      "abstract": "We propose a novel probabilistic framework, termed LVM-GP, for uncertainty quantification in solving forward and inverse partial differential equations (PDEs) with noisy data. The core idea is to construct a stochastic mapping from the input to a high-dimensional latent representation, enabling uncertainty-aware prediction of the solution. Specifically, the architecture consists of a confidence-aware encoder and a probabilistic decoder. The encoder implements a high-dimensional latent variable model based on a Gaussian process (LVM-GP), where the latent representation is constructed by interpolating between a learnable deterministic feature and a Gaussian process prior, with the interpolation strength adaptively controlled by a confidence function learned from data. The decoder defines a conditional Gaussian distribution over the solution field, where the mean is predicted by a neural operator applied to the latent representation, allowing the model to learn flexible function-to-function mapping. Moreover, physical laws are enforced as soft constraints in the loss function to ensure consistency with the underlying PDE structure. Compared to existing approaches such as Bayesian physics-informed neural networks (B-PINNs) and deep ensembles, the proposed framework can efficiently capture functional dependencies via merging a latent Gaussian process and neural operator, resulting in competitive predictive accuracy and robust uncertainty quantification. Numerical experiments demonstrate the effectiveness and reliability of the method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LVM-GP çš„æ–°å‹æ¦‚ç‡æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤„ç†å«æœ‰å™ªå£°æ•°æ®çš„æ­£å‘å’Œåå‘åå¾®åˆ†æ–¹ç¨‹ (PDEs) æ±‚è§£ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ– (Uncertainty Quantification) é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºæ„å»ºä»è¾“å…¥åˆ°é«˜ç»´æ½œè¡¨ç¤º (Latent Representation) çš„éšæœºæ˜ å°„ï¼Œç”±ç½®ä¿¡åº¦æ„ŸçŸ¥ç¼–ç å™¨å’Œæ¦‚ç‡è§£ç å™¨ç»„æˆã€‚ç¼–ç å™¨é€šè¿‡åœ¨å¯å­¦ä¹ ç¡®å®šæ€§ç‰¹å¾ä¸é«˜æ–¯è¿‡ç¨‹ (Gaussian Process) å…ˆéªŒä¹‹é—´è¿›è¡Œæ’å€¼æ¥æ„å»ºæ½œè¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨æ•°æ®é©±åŠ¨çš„ç½®ä¿¡åº¦å‡½æ•°è‡ªé€‚åº”æ§åˆ¶æ’å€¼å¼ºåº¦ã€‚è§£ç å™¨åˆ™åˆ©ç”¨ç¥ç»ç®—å­ (Neural Operator) ä½œç”¨äºæ½œè¡¨ç¤ºï¼Œå®šä¹‰è§£åœºçš„æ¡ä»¶é«˜æ–¯åˆ†å¸ƒï¼Œä»è€Œå®ç°çµæ´»çš„å‡½æ•°æ˜ å°„ã€‚æ­¤å¤–ï¼Œç ”ç©¶å°†ç‰©ç†å®šå¾‹ä½œä¸ºè½¯çº¦æŸ (Soft Constraints) åŠ å…¥æŸå¤±å‡½æ•°ï¼Œä»¥ä¿è¯è§£ä¸ PDE ç»“æ„çš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼Œä¸ B-PINNs å’Œæ·±åº¦é›†æˆç­‰ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLVM-GP åœ¨é¢„æµ‹ç²¾åº¦å’Œä¸ç¡®å®šæ€§é‡åŒ–çš„ç¨³å¥æ€§æ–¹é¢æ›´å…·ä¼˜åŠ¿ï¼Œä¸ºå¤æ‚ç‰©ç†ç³»ç»Ÿçš„å»ºæ¨¡æä¾›äº†å¯é å·¥å…·ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22493v1",
      "published_date": "2025-07-30 09:00:39 UTC",
      "updated_date": "2025-07-30 09:00:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:15.894733+00:00"
    },
    {
      "arxiv_id": "2507.22488v1",
      "title": "Proto-EVFL: Enhanced Vertical Federated Learning via Dual Prototype with Extremely Unaligned Data",
      "title_zh": "Proto-EVFLï¼šé¢å‘æåº¦éå¯¹é½æ•°æ®çš„åŒåŸå‹å¢å¼ºå¼çºµå‘è”é‚¦å­¦ä¹ ",
      "authors": [
        "Wei Guo",
        "Yiyang Duan",
        "Zhaojun Hu",
        "Yiqi Tong",
        "Fuzhen Zhuang",
        "Xiao Zhang",
        "Jin Dong",
        "Ruofan Wu",
        "Tengfei Liu",
        "Yifan Sun"
      ],
      "abstract": "In vertical federated learning (VFL), multiple enterprises address aligned sample scarcity by leveraging massive locally unaligned samples to facilitate collaborative learning. However, unaligned samples across different parties in VFL can be extremely class-imbalanced, leading to insufficient feature representation and limited model prediction space. Specifically, class-imbalanced problems consist of intra-party class imbalance and inter-party class imbalance, which can further cause local model bias and feature contribution inconsistency issues, respectively. To address the above challenges, we propose Proto-EVFL, an enhanced VFL framework via dual prototypes. We first introduce class prototypes for each party to learn relationships between classes in the latent space, allowing the active party to predict unseen classes. We further design a probabilistic dual prototype learning scheme to dynamically select unaligned samples by conditional optimal transport cost with class prior probability. Moreover, a mixed prior guided module guides this selection process by combining local and global class prior probabilities. Finally, we adopt an \\textit{adaptive gated feature aggregation strategy} to mitigate feature contribution inconsistency by dynamically weighting and aggregating local features across different parties. We proved that Proto-EVFL, as the first bi-level optimization framework in VFL, has a convergence rate of 1/\\sqrt T. Extensive experiments on various datasets validate the superiority of our Proto-EVFL. Even in a zero-shot scenario with one unseen class, it outperforms baselines by at least 6.97%",
      "tldr_zh": "é’ˆå¯¹çºµå‘è”é‚¦å­¦ä¹ (Vertical Federated Learning, VFL)åœ¨åˆ©ç”¨å¤§é‡éå¯¹é½æ ·æœ¬è¿›è¡Œåä½œå­¦ä¹ æ—¶é¢ä¸´çš„æç«¯ç±»ä¸å¹³è¡¡ã€å±€éƒ¨æ¨¡å‹åå·®åŠç‰¹å¾è´¡çŒ®ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åä¸ºProto-EVFLçš„å¢å¼ºå‹åŒåŸå‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆä¸ºå„å‚ä¸æ–¹å¼•å…¥ç±»åŸå‹(class prototypes)ä»¥å­¦ä¹ æ½œåœ¨ç©ºé—´çš„ç±»é—´å…³ç³»ï¼Œä½¿ä¸»åŠ¨æ–¹å…·å¤‡é¢„æµ‹æœªè§ç±»åˆ«çš„èƒ½åŠ›ã€‚éšåï¼Œé€šè¿‡ç»“åˆç±»å…ˆéªŒæ¦‚ç‡çš„æ¡ä»¶æœ€ä¼˜ä¼ è¾“æˆæœ¬(conditional optimal transport cost)è®¾è®¡äº†æ¦‚ç‡åŒåŸå‹å­¦ä¹ æ–¹æ¡ˆï¼Œå¹¶åˆ©ç”¨æ··åˆå…ˆéªŒå¼•å¯¼æ¨¡å—(mixed prior guided module)åŠ¨æ€ç­›é€‰éå¯¹é½æ ·æœ¬ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨è‡ªé€‚åº”é—¨æ§ç‰¹å¾èšåˆç­–ç•¥(adaptive gated feature aggregation strategy)æ¥ç¼“è§£ç‰¹å¾è´¡çŒ®ä¸ä¸€è‡´é—®é¢˜ã€‚ç†è®ºè¯æ˜Proto-EVFLä½œä¸ºVFLä¸­é¦–ä¸ªåŒå±‚ä¼˜åŒ–(bi-level optimization)æ¡†æ¶ï¼Œå…¶æ”¶æ•›é€Ÿåº¦è¾¾åˆ°1/âˆšTã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå³ä½¿åœ¨é›¶æ ·æœ¬(zero-shot)åœºæ™¯ä¸‹å…¶å‡†ç¡®ç‡ä¹Ÿè‡³å°‘æå‡äº†6.97%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22488v1",
      "published_date": "2025-07-30 08:48:33 UTC",
      "updated_date": "2025-07-30 08:48:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:16.432686+00:00"
    },
    {
      "arxiv_id": "2507.22485v1",
      "title": "Physics-constrained generative machine learning-based high-resolution downscaling of Greenland's surface mass balance and surface temperature",
      "title_zh": "åŸºäºç‰©ç†çº¦æŸç”Ÿæˆå¼æœºå™¨å­¦ä¹ çš„ Greenland è¡¨é¢è´¨é‡å¹³è¡¡ä¸è¡¨é¢æ¸©åº¦é«˜åˆ†è¾¨ç‡é™å°ºåº¦",
      "authors": [
        "Nils Bochow",
        "Philipp Hess",
        "Alexander Robinson"
      ],
      "abstract": "Accurate, high-resolution projections of the Greenland ice sheet's surface mass balance (SMB) and surface temperature are essential for understanding future sea-level rise, yet current approaches are either computationally demanding or limited to coarse spatial scales. Here, we introduce a novel physics-constrained generative modeling framework based on a consistency model (CM) to downscale low-resolution SMB and surface temperature fields by a factor of up to 32 (from 160 km to 5 km grid spacing) in a few sampling steps. The CM is trained on monthly outputs of the regional climate model MARv3.12 and conditioned on ice-sheet topography and insolation. By enforcing a hard conservation constraint during inference, we ensure approximate preservation of SMB and temperature sums on the coarse spatial scale as well as robust generalization to extreme climate states without retraining. On the test set, our constrained CM achieves a continued ranked probability score of 6.31 mmWE for the SMB and 0.1 K for the surface temperature, outperforming interpolation-based downscaling. Together with spatial power-spectral analysis, we demonstrate that the CM faithfully reproduces variability across spatial scales. We further apply bias-corrected outputs of the NorESM2 Earth System Model as inputs to our CM, to demonstrate the potential of our model to directly downscale ESM fields. Our approach delivers realistic, high-resolution climate forcing for ice-sheet simulations with fast inference and can be readily integrated into Earth-system and ice-sheet model workflows to improve projections of the future contribution to sea-level rise from Greenland and potentially other ice sheets and glaciers too.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºä¸€è‡´æ€§æ¨¡å‹(Consistency Model, CM)çš„ç‰©ç†çº¦æŸç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¯¹æ ¼é™µå…°å²›çš„è¡¨é¢è´¨é‡å¹³è¡¡(Surface Mass Balance, SMB)å’Œè¡¨é¢æ¸©åº¦è¿›è¡Œé«˜åˆ†è¾¨ç‡é™å°ºåº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨MARv3.12åŒºåŸŸæ°”å€™æ¨¡å‹æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå°†ä½åˆ†è¾¨ç‡åœºåœ¨æå°‘é‡‡æ ·æ­¥éª¤å†…é™å°ºåº¦è¾¾32å€ï¼Œå³ä»160kmç½‘æ ¼ç»†åŒ–è‡³5kmã€‚é€šè¿‡åœ¨æ¨ç†é˜¶æ®µå®æ–½ç¡¬æ€§ç‰©ç†å®ˆæ’çº¦æŸï¼Œæ¨¡å‹ç¡®ä¿äº†ç²—å°ºåº¦ä¸Šæ€»é‡çš„ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ä¸éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å±•ç°å‡ºå¯¹æç«¯æ°”å€™çŠ¶æ€çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå—çº¦æŸçš„CMåœ¨SMBå’Œæ¸©åº¦é¢„æµ‹ç²¾åº¦ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ’å€¼æ³•ï¼Œä¸”èƒ½å‡†ç¡®è¿˜åŸå„ç©ºé—´å°ºåº¦çš„å˜ç‡ã€‚è¯¥å·¥å…·å¯ç›´æ¥åº”ç”¨äºåœ°çƒç³»ç»Ÿæ¨¡å‹(ESM)æ•°æ®çš„é™å°ºåº¦ï¼Œä¸ºå†°ç›–æ¨¡æ‹Ÿæä¾›é«˜æ•ˆä¸”çœŸå®çš„é«˜åˆ†è¾¨ç‡æ°”å€™é©±åŠ¨åŠ›ï¼Œä»è€Œæå‡å¯¹æ ¼é™µå…°å²›åŠå…¨çƒå…¶ä»–å†°å·è´¡çŒ®æµ·å¹³é¢ä¸Šå‡çš„é¢„æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22485v1",
      "published_date": "2025-07-30 08:43:48 UTC",
      "updated_date": "2025-07-30 08:43:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:29.361798+00:00"
    },
    {
      "arxiv_id": "2507.22481v1",
      "title": "Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework",
      "title_zh": "è¿ˆå‘åŸºäºè§†è§‰å¤§æ¨¡å‹é©±åŠ¨æ¡†æ¶çš„ç›²ç æµå—æŸè§†é¢‘ä¿®å¤",
      "authors": [
        "Tianyi Liu",
        "Kejun Wu",
        "Chen Cai",
        "Yi Wang",
        "Kim-Hui Yap",
        "Lap-Pui Chau"
      ],
      "abstract": "Video signals are vulnerable in multimedia communication and storage systems, as even slight bitstream-domain corruption can lead to significant pixel-domain degradation. To recover faithful spatio-temporal content from corrupted inputs, bitstream-corrupted video recovery has recently emerged as a challenging and understudied task. However, existing methods require time-consuming and labor-intensive annotation of corrupted regions for each corrupted video frame, resulting in a large workload in practice. In addition, high-quality recovery remains difficult as part of the local residual information in corrupted frames may mislead feature completion and successive content recovery. In this paper, we propose the first blind bitstream-corrupted video recovery framework that integrates visual foundation models with a recovery model, which is adapted to different types of corruption and bitstream-level prompts. Within the framework, the proposed Detect Any Corruption (DAC) model leverages the rich priors of the visual foundation model while incorporating bitstream and corruption knowledge to enhance corruption localization and blind recovery. Additionally, we introduce a novel Corruption-aware Feature Completion (CFC) module, which adaptively processes residual contributions based on high-level corruption understanding. With VFM-guided hierarchical feature augmentation and high-level coordination in a mixture-of-residual-experts (MoRE) structure, our method suppresses artifacts and enhances informative residuals. Comprehensive evaluations show that the proposed method achieves outstanding performance in bitstream-corrupted video recovery without requiring a manually labeled mask sequence. The demonstrated effectiveness will help to realize improved user experience, wider application scenarios, and more reliable multimedia communication and storage systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šåª’ä½“é€šä¿¡ä¸­ç æµæŸå(bitstream-corrupted)å¯¼è‡´è§†é¢‘åƒç´ ä¸¥é‡é€€åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹(Visual Foundation Model)é©±åŠ¨çš„ç›²æ¢å¤æ¡†æ¶ã€‚æ¡†æ¶æ ¸å¿ƒåŒ…å«Detect Any Corruption (DAC)æ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒå¹¶ç»“åˆç æµçº§æç¤º(bitstream-level prompts)ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹æŸååŒºåŸŸçš„è‡ªåŠ¨åŒ–å®šä½èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†Corruption-aware Feature Completion (CFC)æ¨¡å—ï¼Œé€šè¿‡Mixture-of-residual-experts (MoRE)ç»“æ„è‡ªé€‚åº”åœ°å¤„ç†æ®‹å·®ä¿¡æ¯ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å±€éƒ¨æŸåæ®‹å·®è¯¯å¯¼ç‰¹å¾è¡¥å…¨çš„é—®é¢˜ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€æ‰‹åŠ¨æ ‡æ³¨æ©è†œ(manual labeled mask)çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—æŠ‘åˆ¶è§†é¢‘ä¼ªå½±å¹¶æ¢å¤é«˜è´¨é‡çš„æ—¶ç©ºå†…å®¹ã€‚è¯¥æ¡†æ¶ä¸ä»…æå‡äº†è§†é¢‘æ¢å¤çš„ç²¾ç¡®åº¦å’Œæ•ˆç‡ï¼Œä¹Ÿä¸ºæ„å»ºæ›´å¯é çš„å¤šåª’ä½“é€šä¿¡ä¸å­˜å‚¨ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 5 figures, accepted by ACMMM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22481v1",
      "published_date": "2025-07-30 08:31:54 UTC",
      "updated_date": "2025-07-30 08:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:31.448766+00:00"
    },
    {
      "arxiv_id": "2507.22477v2",
      "title": "LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks",
      "title_zh": "LIDARï¼šé¢å‘ç»“æ„è£‚ç¼å¤šæ¨¡æ€åˆ†å‰²çš„è½»é‡åŒ–è‡ªé€‚åº”çº¿ç´¢æ„ŸçŸ¥èåˆ Vision Mamba",
      "authors": [
        "Hui Liu",
        "Chen Jia",
        "Fan Shi",
        "Xu Cheng",
        "Mengfei Shi",
        "Xia Xie",
        "Shengyong Chen"
      ],
      "abstract": "Achieving pixel-level segmentation with low computational cost using multimodal data remains a key challenge in crack segmentation tasks. Existing methods lack the capability for adaptive perception and efficient interactive fusion of cross-modal features. To address these challenges, we propose a Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently perceives and integrates morphological and textural cues from different modalities under multimodal crack scenarios, generating clear pixel-level crack segmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive Cue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain Dynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack cues through the proposed mask-guided Efficient Dynamic Guided Scanning Strategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain Perceptron (AFDP) and a dual-pooling fusion strategy to effectively capture spatial and frequency-domain cues across modalities. Moreover, we design a Lightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive complex morphological structures with minimal computational overhead, replacing most convolutional operations in LIDAR. Experiments on three datasets demonstrate that our method outperforms other state-of-the-art (SOTA) methods. On the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465 in mIoU with only 5.35M parameters. Code and datasets are available at https://github.com/Karl1109/LIDAR-Mamba.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LIDARï¼Œä¸€ç§è½»é‡çº§çš„è‡ªé€‚åº”çº¿ç´¢æ„ŸçŸ¥Vision Mambaç½‘ç»œï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç»“æ„è£‚ç¼åˆ†å‰²ä¸­è·¨æ¨¡æ€ç‰¹å¾æ„ŸçŸ¥ä¸èåˆæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«è½»é‡çº§è‡ªé€‚åº”çº¿ç´¢æ„ŸçŸ¥è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—(LacaVSS)ï¼Œé€šè¿‡æ©ç å¼•å¯¼çš„é«˜æ•ˆåŠ¨æ€å¼•å¯¼æ‰«æç­–ç•¥(EDG-SS)å®ç°å¯¹è£‚ç¼çº¿ç´¢çš„è‡ªé€‚åº”å»ºæ¨¡ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†è½»é‡çº§åŒåŸŸåŠ¨æ€åä½œèåˆæ¨¡å—(LD3CF)ï¼Œåˆ©ç”¨è‡ªé€‚åº”é¢‘åŸŸæ„ŸçŸ¥å™¨(AFDP)å’ŒåŒæ± åŒ–èåˆç­–ç•¥ååŒæ•è·è·¨æ¨¡æ€çš„ç©ºé—´ä¸é¢‘åŸŸçº¿ç´¢ã€‚æ­¤å¤–ï¼Œæ–¹æ¡ˆä¸­å¼•å…¥çš„è½»é‡çº§åŠ¨æ€è°ƒåˆ¶å¤šæ ¸å·ç§¯(LDMK)èƒ½ä»¥æä½çš„è®¡ç®—å¼€é”€æ„ŸçŸ¥å¤æ‚çš„è£‚ç¼å½¢æ€ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLIDARåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨å…‰åœºæ·±åº¦æ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹ä»…å‡­5.35Mçš„å‚æ•°é‡ä¾¿å®ç°äº†0.8204çš„F1åˆ†æ•°å’Œ0.8465çš„mIoUï¼Œåœ¨ç»´æŒæä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†é«˜ç²¾åº¦çš„åƒç´ çº§åˆ†å‰²ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted by ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22477v2",
      "published_date": "2025-07-30 08:28:20 UTC",
      "updated_date": "2025-07-31 01:38:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:35.951919+00:00"
    },
    {
      "arxiv_id": "2508.05657v1",
      "title": "Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation",
      "title_zh": "è¶…è¶Šå•ä¸€æ ‡ç­¾ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ•°æ®å¢å¼ºæ”¹è¿›å¯¹è¯å¼æ¨è",
      "authors": [
        "Haozhe Xu",
        "Xiaohua Wang",
        "Changze Lv",
        "Xiaoqing Zheng"
      ],
      "abstract": "Conversational recommender systems (CRSs) enhance recommendation quality by engaging users in multi-turn dialogues, capturing nuanced preferences through natural language interactions. However, these systems often face the false negative issue, where items that a user might like are incorrectly labeled as negative during training, leading to suboptimal recommendations.Expanding the label set through data augmentation presents an intuitive solution but faces the challenge of balancing two key aspects: ensuring semantic relevance and preserving the collaborative information inherent in CRS datasets. To address these issues, we propose a novel data augmentation framework that first leverages an LLM-based semantic retriever to identify diverse and semantically relevant items, which are then filtered by a relevance scorer to remove noisy candidates. Building on this, we introduce a two-stage training strategy balancing semantic relevance and collaborative information. Extensive experiments on two benchmark datasets and user simulators demonstrate significant and consistent performance improvements across various recommenders, highlighting the effectiveness of our approach in advancing CRS performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯å¼æ¨èç³»ç»Ÿï¼ˆConversational Recommender Systems, CRSsï¼‰åœ¨è®­ç»ƒä¸­å­˜åœ¨çš„å‡é˜´æ€§ï¼ˆfalse negativeï¼‰é—®é¢˜ï¼Œå³æ½œåœ¨å…´è¶£é¡¹è¢«é”™è¯¯æ ‡è®°ä¸ºè´Ÿæ ·æœ¬ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºLLMçš„è¯­ä¹‰æ£€ç´¢å™¨ï¼ˆsemantic retrieverï¼‰è¯†åˆ«å¤šæ ·åŒ–ä¸”ç›¸å…³çš„é¡¹ç›®ï¼Œå¹¶é€šè¿‡ç›¸å…³æ€§è¯„åˆ†å™¨ï¼ˆrelevance scorerï¼‰å‰”é™¤å™ªå£°æ•°æ®ã€‚ä¸ºäº†åœ¨è¯­ä¹‰ç›¸å…³æ€§ä¸ååŒä¿¡æ¯ï¼ˆcollaborative informationï¼‰ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†å’Œç”¨æˆ·æ¨¡æ‹Ÿå™¨ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨èæ¨¡å‹ä¸Šå‡å®ç°äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡LLMæ‰©å±•æ ‡ç­¾é›†å¯¹äºå…‹æœæ•°æ®ç¨€ç–æ€§å’Œæå‡CRSsæ¨èè´¨é‡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05657v1",
      "published_date": "2025-07-30 08:20:54 UTC",
      "updated_date": "2025-07-30 08:20:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:29.550701+00:00"
    },
    {
      "arxiv_id": "2507.22469v1",
      "title": "Visual Language Models as Zero-Shot Deepfake Detectors",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨",
      "authors": [
        "Viacheslav Pirogov"
      ],
      "abstract": "The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(Vision Language Models, VLMs)ä½œä¸ºé›¶æ ·æœ¬æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨(Zero-Shot Deepfake Detectors)çš„æ½œåŠ›ï¼Œä»¥åº”å¯¹GANæˆ–æ‰©æ•£æ¨¡å‹(Diffusion models)å¸¦æ¥çš„æ—¥ç›Šä¸¥å³»çš„è™šå‡åª’ä½“å¨èƒã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–ä¸“é—¨å›¾åƒåˆ†ç±»å™¨ä¸”é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºVLMçš„åˆ›æ–°åˆ†ç±»æ¡†æ¶ï¼Œæ—¨åœ¨å……åˆ†å‘æŒ¥å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹çš„é›¶æ ·æœ¬(Zero-shot)æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«60,000å¼ å›¾åƒçš„é«˜è´¨é‡æ–°æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†ä¼˜äºç»å¤§å¤šæ•°ç°æœ‰æŠ€æœ¯çš„æ£€æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶åœ¨DFDC-Pæ•°æ®é›†ä¸Šå¯¹InstructBLIPæ¶æ„è¿›è¡Œäº†æ·±å…¥æµ‹è¯•ï¼Œå¯¹æ¯”äº†å…¶åœ¨é›¶æ ·æœ¬å’ŒåŸŸå†…å¾®è°ƒ(In-domain fine-tuning)åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚æœ€ç»ˆå®éªŒç»“æœæœ‰åŠ›è¯æ˜äº†VLMsåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸­ç›¸æ¯”ä¼ ç»Ÿåˆ†ç±»å™¨å…·æœ‰æ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the ICML 2025 Workshop on Reliable and Responsible Foundation Models",
      "pdf_url": "https://arxiv.org/pdf/2507.22469v1",
      "published_date": "2025-07-30 08:20:02 UTC",
      "updated_date": "2025-07-30 08:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:49.849598+00:00"
    },
    {
      "arxiv_id": "2507.22467v1",
      "title": "Towards Simulating Social Influence Dynamics with LLM-based Multi-agents",
      "title_zh": "è¿ˆå‘åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“çš„ç¤¾ä¼šå½±å“åŠ¨æ€æ¨¡æ‹Ÿ",
      "authors": [
        "Hsien-Tsung Lin",
        "Pei-Cing Huang",
        "Chan-Tung Ku",
        "Chan Hsu",
        "Pei-Xuan Shieh",
        "Yihuang Kang"
      ],
      "abstract": "Recent advancements in Large Language Models offer promising capabilities to simulate complex human social interactions. We investigate whether LLM-based multi-agent simulations can reproduce core human social dynamics observed in online forums. We evaluate conformity dynamics, group polarization, and fragmentation across different model scales and reasoning capabilities using a structured simulation framework. Our findings indicate that smaller models exhibit higher conformity rates, whereas models optimized for reasoning are more resistant to social influence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨åŸºäº Large Language Models (LLMs) çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¨¡æ‹Ÿäººç±»ç¤¾äº¤äº’åŠ¨åŠåœ¨çº¿è®ºå›æ ¸å¿ƒç¤¾äº¤åŠ¨æ€çš„å¯è¡Œæ€§ã€‚ç ”ç©¶é€šè¿‡æ„å»ºç»“æ„åŒ–æ¨¡æ‹Ÿæ¡†æ¶ï¼Œé‡ç‚¹è¯„ä¼°äº†ä¸åŒæ¨¡å‹è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›åœ¨ conformity dynamicsã€group polarization ä»¥åŠ fragmentation æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¾ƒå°è§„æ¨¡çš„æ¨¡å‹å¾€å¾€è¡¨ç°å‡ºæ›´é«˜çš„ä»ä¼—ç‡ï¼Œè€Œç»è¿‡æ¨ç†èƒ½åŠ›ä¼˜åŒ–çš„æ¨¡å‹åœ¨é¢å¯¹ç¤¾äº¤å½±å“æ—¶å…·æœ‰æ›´å¼ºçš„æŠµæŠ—åŠ›ã€‚è¿™é¡¹å·¥ä½œéªŒè¯äº†å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿåœ¨é‡ç°äººç±»æ ¸å¿ƒç¤¾äº¤åŠ¨æ€æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹å†…éƒ¨å±æ€§å¯¹ç¤¾äº¤è¡Œä¸ºæ¨¡æ‹Ÿç»“æœçš„å…³é”®å½±å“ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22467v1",
      "published_date": "2025-07-30 08:14:40 UTC",
      "updated_date": "2025-07-30 08:14:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:50.861522+00:00"
    },
    {
      "arxiv_id": "2507.22465v1",
      "title": "Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation",
      "title_zh": "æµ…å±‚ç‰¹å¾äº¦é‡è¦ï¼šèåˆå¼‚æ„äº¤äº’å±‚çº§è®°å¿†çš„æ— ç›‘ç£è§†é¢‘å¯¹è±¡åˆ†å‰²",
      "authors": [
        "Zheng Xiangyu",
        "He Songcheng",
        "Li Wanyun",
        "Li Xiaoqiang",
        "Zhang Wei"
      ],
      "abstract": "Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level masks for the most salient objects in videos without any prior annotations. While memory mechanisms have been proven critical in various video segmentation paradigms, their application in UVOS yield only marginal performance gains despite sophisticated design. Our analysis reveals a simple but fundamental flaw in existing methods: over-reliance on memorizing high-level semantic features. UVOS inherently suffers from the deficiency of lacking fine-grained information due to the absence of pixel-level prior knowledge. Consequently, memory design relying solely on high-level features, which predominantly capture abstract semantic cues, is insufficient to generate precise predictions. To resolve this fundamental issue, we propose a novel hierarchical memory architecture to incorporate both shallow- and high-level features for memory, which leverages the complementary benefits of pixel and semantic information. Furthermore, to balance the simultaneous utilization of the pixel and semantic memory features, we propose a heterogeneous interaction mechanism to perform pixel-semantic mutual interactions, which explicitly considers their inherent feature discrepancies. Through the design of Pixel-guided Local Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM), we achieve delicate integration of the fine-grained details in shallow-level memory and the semantic representations in high-level memory. Our Hierarchical Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves state-of-the-art performance across all UVOS and video saliency detection benchmarks. Moreover, HMHI-Net consistently exhibits high performance across different backbones, further demonstrating its superiority and robustness. Project page: https://github.com/ZhengxyFlow/HMHI-Net .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— ç›‘ç£è§†é¢‘å¯¹è±¡åˆ†å‰²(UVOS)ä¸­ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–é«˜å±‚è¯­ä¹‰ç‰¹å¾è€Œå¯¼è‡´ç²¾ç»†ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼Œæå‡ºäº†å…·æœ‰å¼‚æ„äº¤äº’çš„å±‚çº§è®°å¿†ç½‘ç»œ(HMHI-Net)ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€ç§æ–°å‹çš„å±‚çº§è®°å¿†æ¶æ„ï¼Œé€šè¿‡æ•´åˆæµ…å±‚åƒç´ ç‰¹å¾ä¸é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼Œå……åˆ†åˆ©ç”¨äº†åƒç´ ä¸è¯­ä¹‰ä¿¡æ¯çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºäº†å¹³è¡¡ä¸åŒå±‚çº§ç‰¹å¾çš„åˆ©ç”¨å¹¶å¤„ç†ç‰¹å¾é—´çš„å›ºæœ‰å·®å¼‚ï¼Œç ”ç©¶è€…è®¾è®¡äº†å¼‚æ„äº¤äº’æœºåˆ¶ï¼Œå…·ä½“åŒ…æ‹¬åƒç´ å¼•å¯¼å±€éƒ¨å¯¹é½æ¨¡å—(PLAM)å’Œè¯­ä¹‰å¼•å¯¼å…¨å±€é›†æˆæ¨¡å—(SGIM)ã€‚è¿™äº›æ¨¡å—å®ç°äº†æµ…å±‚è®°å¿†ä¸­çš„ç»†ç²’åº¦ç»†èŠ‚ä¸é«˜å±‚è®°å¿†ä¸­çš„è¯­ä¹‰è¡¨ç¤ºçš„ç²¾å¯†æ•´åˆï¼Œä»è€Œç”Ÿæˆæ›´ç²¾ç¡®çš„åˆ†å‰²æ©ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHMHI-Netåœ¨æ‰€æœ‰UVOSåŠè§†é¢‘æ˜¾è‘—æ€§æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒçš„éª¨æ¶ç½‘ç»œ(backbones)ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„ä¼˜è¶Šæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACM MM'25: The 33rd ACM International Conference on Multimedia Proceedings",
      "pdf_url": "https://arxiv.org/pdf/2507.22465v1",
      "published_date": "2025-07-30 08:11:18 UTC",
      "updated_date": "2025-07-30 08:11:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:07.288380+00:00"
    },
    {
      "arxiv_id": "2507.22464v1",
      "title": "Towards Interpretable Renal Health Decline Forecasting via Multi-LMM Collaborative Reasoning Framework",
      "title_zh": "åŸºäºå¤šLMMååŒæ¨ç†æ¡†æ¶çš„å¯è§£é‡Šè‚¾åŠŸèƒ½å‡é€€é¢„æµ‹",
      "authors": [
        "Peng-Yi Wu",
        "Pei-Cing Huang",
        "Ting-Yu Chen",
        "Chantung Ku",
        "Ming-Yen Lin",
        "Yihuang Kang"
      ],
      "abstract": "Accurate and interpretable prediction of estimated glomerular filtration rate (eGFR) is essential for managing chronic kidney disease (CKD) and supporting clinical decisions. Recent advances in Large Multimodal Models (LMMs) have shown strong potential in clinical prediction tasks due to their ability to process visual and textual information. However, challenges related to deployment cost, data privacy, and model reliability hinder their adoption. In this study, we propose a collaborative framework that enhances the performance of open-source LMMs for eGFR forecasting while generating clinically meaningful explanations. The framework incorporates visual knowledge transfer, abductive reasoning, and a short-term memory mechanism to enhance prediction accuracy and interpretability. Experimental results show that the proposed framework achieves predictive performance and interpretability comparable to proprietary models. It also provides plausible clinical reasoning processes behind each prediction. Our method sheds new light on building AI systems for healthcare that combine predictive accuracy with clinically grounded interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šå¤§è¯­è¨€æ¨¡å‹(LMMs)åä½œæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç²¾å‡†ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„ä¼°ç®—è‚¾å°çƒæ»¤è¿‡ç‡(eGFR)é¢„æµ‹ï¼Œä»¥è¾…åŠ©æ…¢æ€§è‚¾è„ç—…(CKD)çš„ç®¡ç†ã€‚ä¸ºäº†è§£å†³å¼€æºæ¨¡å‹åœ¨éƒ¨ç½²æˆæœ¬ã€æ•°æ®éšç§å’Œå¯é æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è§†è§‰çŸ¥è¯†è½¬ç§»(visual knowledge transfer)ã€æº¯å› æ¨ç†(abductive reasoning)å’ŒçŸ­æœŸè®°å¿†æœºåˆ¶(short-term memory mechanism)ï¼Œæ˜¾è‘—å¢å¼ºäº†é¢„æµ‹ç²¾åº¦å’Œä¸´åºŠå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢å¯ä¸é—­æºå•†ä¸šæ¨¡å‹åª²ç¾ï¼Œå¹¶èƒ½ä¸ºæ¯æ¬¡é¢„æµ‹æä¾›åˆç†çš„ä¸´åºŠæ¨ç†è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å¦‚ä½•ç»“åˆé¢„æµ‹å‡†ç¡®æ€§ä¸ä¸´åºŠä¾æ®çš„å¯è§£é‡Šæ€§ï¼Œä¸ºæ„å»ºåŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22464v1",
      "published_date": "2025-07-30 08:11:06 UTC",
      "updated_date": "2025-07-30 08:11:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:44:59.788480+00:00"
    },
    {
      "arxiv_id": "2507.22457v1",
      "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models",
      "title_zh": "ä»€ä¹ˆæ˜¯â€œæŠ½è±¡æ¨ç†è€…â€ï¼Ÿé‡æ–°å®¡è§†å…³äºå¤§è¯­è¨€æ¨¡å‹çš„å®éªŒä¸è®ºç‚¹",
      "authors": [
        "Tian Yun",
        "Chen Sun",
        "Ellie Pavlick"
      ],
      "abstract": "Recent work has argued that large language models (LLMs) are not \"abstract reasoners\", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an \"abstract reasoner\", and why it matters whether LLMs fit the bill.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡â€œæŠ½è±¡æ¨ç†è€…â€(Abstract Reasoner)èƒ½åŠ›è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶å¯¹è¿‘æœŸè®¤ä¸ºLLMså› é›¶æ ·æœ¬è¡¨ç°è¾ƒå·®è€Œç¼ºä¹æŠ½è±¡æ¨ç†èƒ½åŠ›çš„è§‚ç‚¹è¿›è¡Œäº†é‡æ–°è¯„ä¼°ã€‚ä½œè€…é€šè¿‡å®éªŒè¯æ˜ï¼Œè™½ç„¶LLMsåœ¨é›¶æ ·æœ¬(Zero-shot)è®¾ç½®ä¸‹è¡¨ç°ä¸ä½³ï¼Œä½†ä»…éœ€å¯¹è¾“å…¥ç¼–ç (Input Encoding)çš„ä¸€ä¸ªå°å­é›†å‚æ•°è¿›è¡Œå¾®è°ƒ(Tuning)ï¼Œå³å¯å®ç°è¿‘ä¹å®Œç¾çš„æ€§èƒ½è¡¨ç°ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç°è¿™ç§é€šè¿‡å¾®è°ƒè·å¾—çš„æå‡åœ¨ä¸åŒæ•°æ®é›†(Datasets)ä¹‹é—´å¹¶ä¸å…·å¤‡å¿…ç„¶çš„è¿ç§»æ€§(Transfer)ã€‚åŸºäºè¿™ä¸€ç³»åˆ—å®è¯ç»“æœï¼Œè¯¥è®ºæ–‡é‡æ–°å¼€å¯äº†å…³äºâ€œæŠ½è±¡æ¨ç†è€…â€å®šä¹‰çš„å­¦æœ¯è®¨è®ºï¼Œå¹¶æ·±åº¦å‰–æäº†æ˜ç¡®LLMsæ˜¯å¦ç¬¦åˆè¯¥å®šä¹‰å¯¹äºè¯„ä¼°äººå·¥æ™ºèƒ½é€»è¾‘èƒ½åŠ›çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2507.22457v1",
      "published_date": "2025-07-30 08:04:19 UTC",
      "updated_date": "2025-07-30 08:04:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:02.494420+00:00"
    },
    {
      "arxiv_id": "2507.22446v1",
      "title": "RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function",
      "title_zh": "RCR-AFï¼šé€šè¿‡æ‹‰å¾·é©¬èµ«å¤æ‚åº¦ç¼©å‡æ¿€æ´»å‡½æ•°æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Yunrui Yu",
        "Kafeng Wang",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "Despite their widespread success, deep neural networks remain critically vulnerable to adversarial attacks, posing significant risks in safety-sensitive applications. This paper investigates activation functions as a crucial yet underexplored component for enhancing model robustness. We propose a Rademacher Complexity Reduction Activation Function (RCR-AF), a novel activation function designed to improve both generalization and adversarial resilience. RCR-AF uniquely combines the advantages of GELU (including smoothness, gradient stability, and negative information retention) with ReLU's desirable monotonicity, while simultaneously controlling both model sparsity and capacity through built-in clipping mechanisms governed by two hyperparameters, $Î±$ and $Î³$. Our theoretical analysis, grounded in Rademacher complexity, demonstrates that these parameters directly modulate the model's Rademacher complexity, offering a principled approach to enhance robustness. Comprehensive empirical evaluations show that RCR-AF consistently outperforms widely-used alternatives (ReLU, GELU, and Swish) in both clean accuracy under standard training and in adversarial robustness within adversarial training paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¯¹æŠ—æ”»å‡»ä¸­çš„è„†å¼±æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸º Rademacher Complexity Reduction Activation Function (RCR-AF) çš„æ–°å‹æ¿€æ´»å‡½æ•°ï¼Œæ—¨åœ¨åŒæ—¶æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹æŠ—é²æ£’æ€§ã€‚RCR-AF å·§å¦™åœ°ç»“åˆäº† GELU çš„å¹³æ»‘æ€§ã€æ¢¯åº¦ç¨³å®šæ€§å’Œè´Ÿä¿¡æ¯ä¿ç•™èƒ½åŠ›ï¼Œä»¥åŠ ReLU çš„å•è°ƒæ€§ä¼˜åŠ¿ã€‚è¯¥å‡½æ•°é€šè¿‡ç”±è¶…å‚æ•° $Î±$ å’Œ $Î³$ æ§åˆ¶çš„å†…ç½®è£å‰ªæœºåˆ¶ï¼Œå®ç°äº†å¯¹æ¨¡å‹ç¨€ç–æ€§å’Œå®¹é‡çš„åŠ¨æ€è°ƒèŠ‚ã€‚åŸºäº Rademacher complexity çš„ç†è®ºåˆ†æè¯æ˜ï¼Œè¿™äº›å‚æ•°èƒ½æœ‰æ•ˆè°ƒåˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œä¸ºæå‡é²æ£’æ€§æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¹²å‡€å‡†ç¡®ç‡ï¼ˆclean accuracyï¼‰å’Œå¯¹æŠ—é²æ£’æ€§è¯„ä¼°ä¸­ï¼ŒRCR-AF çš„è¡¨ç°å‡ä¼˜äº ReLUã€GELU å’Œ Swish ç­‰ä¸»æµæ¿€æ´»å‡½æ•°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22446v1",
      "published_date": "2025-07-30 07:45:03 UTC",
      "updated_date": "2025-07-30 07:45:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:05.091169+00:00"
    },
    {
      "arxiv_id": "2507.22445v1",
      "title": "AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini",
      "title_zh": "AI ç”Ÿæˆæ•…äº‹å€¾å‘äºç¨³å®šè€Œéå˜é©ï¼šgpt-4o-mini å™äº‹ç”Ÿæˆä¸­çš„åŒè´¨åŒ–ä¸æ–‡åŒ–åˆ»æ¿å°è±¡",
      "authors": [
        "Jill Walker Rettberg",
        "Hermann Wigers"
      ],
      "abstract": "Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a 1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æç”± gpt-4o-mini ç”Ÿæˆçš„ 11,800 ç¯‡æ¶µç›– 236 ä¸ªå›½å®¶çš„æ•…äº‹ï¼Œè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰è·¨æ–‡åŒ–ç›¸å…³æ€§å™äº‹æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ç”Ÿæˆçš„æ•…äº‹åŒ…å«è¡¨é¢å±‚é¢çš„å›½å®¶ç¬¦å·ï¼Œä½†åœ¨æƒ…èŠ‚ç»“æ„ä¸Šè¡¨ç°å‡ºæé«˜çš„åŒè´¨åŒ–(homogeneity)ï¼Œå¤§å¤šéµå¾ªä¸»è§’åœ¨å°é•‡é€šè¿‡é‡å½’ä¼ ç»Ÿè§£å†³å†²çªçš„å•ä¸€é€»è¾‘ã€‚AI ç”Ÿæˆçš„å™äº‹æ™®éæ·¡åŒ–ç°å®å†²çªä¸å¼ åŠ›ï¼Œæ›´å€¾å‘äºç¨³å®šæ€§(stability)è€Œéå˜é©ï¼Œå€¾å‘äºä¼ ç»Ÿè€Œéå¢é•¿ã€‚è¿™ç§ç°è±¡è¢«å®šä¹‰ä¸ºä¸€ç§ç‹¬ç‰¹çš„ AI biasï¼Œå³å™äº‹æ ‡å‡†åŒ–(narrative standardisation)ï¼Œåæ˜ äº†æ¨¡å‹åœ¨æ–‡åŒ–ç†è§£ä¸Šçš„å±€é™æ€§ã€‚è¯¥å‘ç°å¯¹å™è¿°å­¦(narratology)ã€NLP ç ”ç©¶ä»¥åŠæå‡ç”Ÿæˆå¼ AI çš„æ–‡åŒ–å¯¹é½(cultural alignment)å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement number 101142306. The project is also supported by the Center for Digital Narrative, which is funded by the Research Council of Norway through its Centres of Excellence scheme, project number 332643",
      "pdf_url": "https://arxiv.org/pdf/2507.22445v1",
      "published_date": "2025-07-30 07:44:28 UTC",
      "updated_date": "2025-07-30 07:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:09.892161+00:00"
    },
    {
      "arxiv_id": "2507.22440v1",
      "title": "Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool",
      "title_zh": "æœ€è¿‘æ›´å¥½ç½‘ç»œï¼šç”¨äºç»„åˆä¼˜åŒ–é—®é¢˜å¯è§†åŒ–ä¸åˆ†æçš„ç»Ÿä¸€å·¥å…·",
      "authors": [
        "Yiya Diao",
        "Changhe Li",
        "Sanyou Zeng",
        "Xinye Cai",
        "Wenjian Luo",
        "Shengxiang Yang",
        "Carlos A. Coello Coello"
      ],
      "abstract": "The Nearest-Better Network (NBN) is a powerful method to visualize sampled data for continuous optimization problems while preserving multiple landscape features. However, the calculation of NBN is very time-consuming, and the extension of the method to combinatorial optimization problems is challenging but very important for analyzing the algorithm's behavior. This paper provides a straightforward theoretical derivation showing that the NBN network essentially functions as the maximum probability transition network for algorithms. This paper also presents an efficient NBN computation method with logarithmic linear time complexity to address the time-consuming issue. By applying this efficient NBN algorithm to the OneMax problem and the Traveling Salesman Problem (TSP), we have made several remarkable discoveries for the first time: The fitness landscape of OneMax exhibits neutrality, ruggedness, and modality features. The primary challenges of TSP problems are ruggedness, modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and LKH) have limitations when addressing challenges related to modality and deception, respectively. LKH, based on local search operators, fails when there are deceptive solutions near global optima. EAX, which is based on a single population, can efficiently maintain diversity. However, when multiple attraction basins exist, EAX retains individuals within multiple basins simultaneously, reducing inter-basin interaction efficiency and leading to algorithm's stagnation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Nearest-Better Network (NBN)åœ¨å¤„ç†ç»„åˆä¼˜åŒ–é—®é¢˜(Combinatorial Optimization Problems)æ—¶è®¡ç®—æ•ˆç‡ä½ä¸”æ‰©å±•æ€§å·®çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¯è§†åŒ–ä¸åˆ†æå·¥å…·ã€‚è®ºæ–‡é€šè¿‡ç†è®ºæ¨å¯¼è¯æ˜äº†NBNæœ¬è´¨ä¸Šæ˜¯ç®—æ³•çš„æœ€å¤§æ¦‚ç‡è½¬ç§»ç½‘ç»œ(Maximum Probability Transition Network)ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…·æœ‰å¯¹æ•°çº¿æ€§æ—¶é—´å¤æ‚åº¦(Logarithmic Linear Time Complexity)çš„é«˜æ•ˆè®¡ç®—æ–¹æ³•ã€‚é€šè¿‡å°†è¯¥å·¥å…·åº”ç”¨äºOneMaxé—®é¢˜ï¼Œç ”ç©¶é¦–æ¬¡æ­ç¤ºäº†å…¶é€‚åº”åº¦åœ°å½¢(Fitness Landscape)å…·æœ‰ä¸­æ€§(Neutrality)ã€å´å²–æ€§(Ruggedness)å’Œæ¨¡æ€(Modality)ç‰¹å¾ã€‚åœ¨æ—…è¡Œå•†é—®é¢˜(TSP)çš„ç ”ç©¶ä¸­ï¼Œåˆ†ææŒ‡å‡ºå…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå´å²–æ€§ã€æ¨¡æ€å’Œæ¬ºéª—æ€§(Deception)ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†LKHç®—æ³•åœ¨å¤„ç†æ¬ºéª—æ€§è§£æ—¶çš„å±€é™æ€§ï¼Œä»¥åŠEAXç®—æ³•åœ¨å¤šå¸å¼•ç›†(Attraction Basins)ç¯å¢ƒä¸‹å› äº¤äº’æ•ˆç‡é™ä½è€Œå¯¼è‡´åœæ»çš„é—®é¢˜ã€‚è¿™äº›å‘ç°ä¸ºç†è§£å¤æ‚ä¼˜åŒ–é—®é¢˜çš„åœ°å½¢ç‰¹å¾åŠç®—æ³•è¡Œä¸ºæä¾›äº†æ–°çš„ç§‘å­¦æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22440v1",
      "published_date": "2025-07-30 07:31:58 UTC",
      "updated_date": "2025-07-30 07:31:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:22.902011+00:00"
    },
    {
      "arxiv_id": "2507.22432v1",
      "title": "Cross-Border Legal Adaptation of Autonomous Vehicle Design based on Logic and Non-monotonic Reasoning",
      "title_zh": "åŸºäºé€»è¾‘ä¸éå•è°ƒæ¨ç†çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦è®¾è®¡è·¨å¢ƒæ³•å¾‹é€‚é…",
      "authors": [
        "Zhe Yu",
        "Yiwei Lu",
        "Burkhard Schafer",
        "Zhe Lin"
      ],
      "abstract": "This paper focuses on the legal compliance challenges of autonomous vehicles in a transnational context. We choose the perspective of designers and try to provide supporting legal reasoning in the design process. Based on argumentation theory, we introduce a logic to represent the basic properties of argument-based practical (normative) reasoning, combined with partial order sets of natural numbers to express priority. Finally, through case analysis of legal texts, we show how the reasoning system we provide can help designers to adapt their design solutions more flexibly in the cross-border application of autonomous vehicles and to more easily understand the legal implications of their decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†(Autonomous Vehicles)åœ¨è·¨å›½èƒŒæ™¯ä¸‹çš„æ³•å¾‹åˆè§„æ€§æŒ‘æˆ˜ï¼Œä»è®¾è®¡è€…è§†è§’å‡ºå‘ï¼Œæä¾›è®¾è®¡è¿‡ç¨‹ä¸­çš„æ³•å¾‹æ¨ç†æ”¯æŒã€‚åŸºäºè®ºè¯ç†è®º(Argumentation Theory)ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§é€»è¾‘æ¡†æ¶æ¥è¡¨å¾åŸºäºè®ºè¯çš„å®è·µè§„èŒƒæ€§æ¨ç†ï¼Œå¹¶ç»“åˆè‡ªç„¶æ•°çš„ååºé›†(Partial Order Sets)æ¥å¤„ç†æ³•å¾‹ä¼˜å…ˆçº§çš„è¡¨è¾¾ã€‚é€šè¿‡å¯¹æ³•å¾‹æ–‡æœ¬çš„æ¡ˆä¾‹åˆ†æï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†è¯¥æ¨ç†ç³»ç»Ÿå¦‚ä½•è¾…åŠ©è®¾è®¡è€…åœ¨è·¨å›½åº”ç”¨ä¸­çµæ´»è°ƒæ•´è®¾è®¡æ–¹æ¡ˆï¼Œå¹¶æ›´ç›´è§‚åœ°ç†è§£å†³ç­–èƒŒåçš„æ³•å¾‹å«ä¹‰ã€‚è¿™ç§åŸºäºé€»è¾‘ä¸éå•è°ƒæ¨ç†(Non-monotonic Reasoning)çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†è‡ªåŠ¨é©¾é©¶è®¾è®¡åœ¨è·¨å¢ƒç¯å¢ƒä¸‹çš„æ³•å¾‹é€‚åº”èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to appear in Proceedings of the 20th International Conference on Artificial Intelligence and Law (ICAIL 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.22432v1",
      "published_date": "2025-07-30 07:24:15 UTC",
      "updated_date": "2025-07-30 07:24:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:17.700473+00:00"
    },
    {
      "arxiv_id": "2507.22428v1",
      "title": "Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss",
      "title_zh": "ä½¿ç”¨ CE æŸå¤±çš„å¯¹æŠ—æ”»å‡»æ¢¯åº¦è®¡ç®—ä¸­ç›¸å¯¹è¯¯å·®çš„ç†è®ºåˆ†æ",
      "authors": [
        "Yunrui Yu",
        "Hang Su",
        "Cheng-zhong Xu",
        "Zhizhong Su",
        "Jun Zhu"
      ],
      "abstract": "Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often suffer from overestimation due to relative errors in gradient computation induced by floating-point arithmetic. This paper provides a rigorous theoretical analysis of these errors, conducting the first comprehensive study of floating-point computation errors in gradient-based attacks across four distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful targeted attacks. We establish theoretical foundations characterizing the behavior of relative numerical errors under different attack conditions, revealing previously unknown patterns in gradient computation instability, and identify floating-point underflow and rounding as key contributors. Building on this insight, we propose the Theoretical MIFPE (T-MIFPE) loss function, which incorporates an optimal scaling factor $T = t^*$ to minimize the impact of floating-point errors, thereby enhancing the accuracy of gradient computation in adversarial attacks. Extensive experiments on the MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate that T-MIFPE outperforms existing loss functions, including CE, C\\&W, DLR, and MIFPE, in terms of attack potency and robustness evaluation accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½¿ç”¨ Cross-Entropy (CE) loss çš„åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ”»å‡» (Gradient-based adversarial attacks) ä¸­ï¼Œç”±æµ®ç‚¹è¿ç®— (floating-point arithmetic) å¼•èµ·çš„æ¢¯åº¦è®¡ç®—ç›¸å¯¹è¯¯å·®å¯¼è‡´çš„é«˜ä¼°é—®é¢˜è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†å››ç§ä¸åŒåœºæ™¯ä¸‹çš„æµ®ç‚¹è®¡ç®—è¯¯å·®ï¼ŒåŒ…æ‹¬éé’ˆå¯¹æ€§æ”»å‡» (untargeted attacks) å’Œé’ˆå¯¹æ€§æ”»å‡» (targeted attacks) çš„æˆåŠŸä¸å¤±è´¥æƒ…å†µã€‚é€šè¿‡å»ºç«‹è¡¨å¾ç›¸å¯¹æ•°å€¼è¯¯å·®è¡Œä¸ºçš„ç†è®ºåŸºç¡€ï¼Œç ”ç©¶æ­ç¤ºäº†æ¢¯åº¦è®¡ç®—ä¸ç¨³å®šæ€§ä¸­ä»¥å‰æœªçŸ¥çš„æ¨¡å¼ï¼Œå¹¶ç¡®å®šäº†æµ®ç‚¹æ¬ è½½ (underflow) å’Œèˆå…¥ (rounding) æ˜¯é€ æˆè¯¯å·®çš„å…³é”®å› ç´ ã€‚åŸºäºè¿™äº›è§è§£ï¼Œä½œè€…æå‡ºäº† Theoretical MIFPE (T-MIFPE) æŸå¤±å‡½æ•°ï¼Œé€šè¿‡å¼•å…¥æœ€ä¼˜ç¼©æ”¾å› å­ $T = t^*$ æ¥æœ€å°åŒ–æµ®ç‚¹è¯¯å·®çš„å½±å“ï¼Œæ˜¾è‘—æé«˜äº†æ¢¯åº¦è®¡ç®—çš„å‡†ç¡®æ€§ã€‚åœ¨ MNISTã€CIFAR-10 å’Œ CIFAR-100 æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒT-MIFPE åœ¨æ”»å‡»æ•ˆåŠ› (attack potency) å’Œé²æ£’æ€§è¯„ä¼°å‡†ç¡®åº¦ (robustness evaluation accuracy) æ–¹é¢å‡ä¼˜äº CEã€C&Wã€DLR å’Œ MIFPE ç­‰ç°æœ‰æŸå¤±å‡½æ•°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22428v1",
      "published_date": "2025-07-30 07:14:59 UTC",
      "updated_date": "2025-07-30 07:14:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:23.191539+00:00"
    },
    {
      "arxiv_id": "2507.22424v2",
      "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance",
      "title_zh": "Spec-VLAï¼šåŸºäºæ”¾å®½æ¥å—å‡†åˆ™çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æŠ•æœºè§£ç ",
      "authors": [
        "Songsheng Wang",
        "Rucheng Yu",
        "Zhihang Yuan",
        "Chao Yu",
        "Feng Gao",
        "Yu Wang",
        "Derek F. Wong"
      ],
      "abstract": "Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Spec-VLAï¼Œä¸€ç§é’ˆå¯¹ Vision-Language-Action (VLA) æ¨¡å‹è®¾è®¡çš„ Speculative Decoding æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Visual Language Models (VLMs) ç”±äºå‚æ•°é‡å·¨å¤§å’Œ Autoregressive è§£ç ç‰¹æ€§å¯¼è‡´çš„è®¡ç®—å‹åŠ›ã€‚ç”±äºåŠ¨ä½œé¢„æµ‹ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œç›´æ¥åº”ç”¨æ¨æµ‹è§£ç æå‡æœ‰é™ï¼Œå› æ­¤ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åˆ©ç”¨ Action Tokens ç›¸å¯¹è·ç¦»çš„ Relaxed Acceptance æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆæå‡ç”Ÿæˆé€Ÿåº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSpec-VLA åœ¨ä¸å½±å“ Success Rate çš„æƒ…å†µä¸‹ï¼Œå°†æ¥å—é•¿åº¦æå‡äº† 44%ï¼Œå¹¶ç›¸æ¯” OpenVLA åŸºçº¿å®ç°äº† 1.42 å€çš„åŠ é€Ÿã€‚è¯¥å·¥ä½œå±•ç¤ºäº† Speculative Execution åœ¨ VLA é¢„æµ‹åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆçš„æœºå™¨äººç­–ç•¥æ¨ç†æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 5 figures, Accepted by EMNLP 2025 (main conference)",
      "pdf_url": "https://arxiv.org/pdf/2507.22424v2",
      "published_date": "2025-07-30 07:04:09 UTC",
      "updated_date": "2025-09-20 18:24:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:42.796888+00:00"
    },
    {
      "arxiv_id": "2507.22423v2",
      "title": "On the Definition of Intelligence",
      "title_zh": "è®ºæ™ºèƒ½çš„å®šä¹‰",
      "authors": [
        "Kei-Sing Ng"
      ],
      "abstract": "To engineer AGI, we should first capture the essence of intelligence in a species-agnostic form that can be evaluated, while being sufficiently general to encompass diverse paradigms of intelligent behavior, including reinforcement learning, generative models, classification, analogical reasoning, and goal-directed decision-making. We propose a general criterion based on \\textit{entity fidelity}: Intelligence is the ability, given entities exemplifying a concept, to generate entities exemplifying the same concept. We formalise this intuition as \\(\\varepsilon\\)-concept intelligence: it is \\(\\varepsilon\\)-intelligent with respect to a concept if no chosen admissible distinguisher can separate generated entities from original entities beyond tolerance \\(\\varepsilon\\). We present the formal framework, outline empirical protocols, and discuss implications for evaluation, safety, and generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨ä¸ºå·¥ç¨‹åŒ–é€šç”¨äººå·¥æ™ºèƒ½(AGI)æ•æ‰ä¸€ç§è·¨ç‰©ç§ä¸”å¯è¯„ä¼°çš„æ™ºèƒ½æœ¬è´¨ï¼Œæ¶µç›–äº†å¼ºåŒ–å­¦ä¹ (reinforcement learning)ã€ç”Ÿæˆæ¨¡å‹(generative models)å’Œç±»æ¯”æ¨ç†ç­‰å¤šç§æ™ºèƒ½è¡Œä¸ºèŒƒå¼ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå®ä½“å¿ å®åº¦(entity fidelity)çš„é€šç”¨æ ‡å‡†ï¼Œå°†æ™ºèƒ½æ ¸å¿ƒå®šä¹‰ä¸ºï¼šåœ¨ç»™å®šä½“ç°æŸç§æ¦‚å¿µçš„å®ä½“æ—¶ï¼Œç”Ÿæˆä½“ç°ç›¸åŒæ¦‚å¿µçš„æ–°å®ä½“çš„èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†æ­¤ç›´è§‰å½¢å¼åŒ–ä¸º$\\varepsilon$-æ¦‚å¿µæ™ºèƒ½($\\varepsilon$-concept intelligence)ï¼Œå³è‹¥ä¸å­˜åœ¨å¯å®¹è®¸çš„é‰´åˆ«å™¨(distinguisher)èƒ½åœ¨è¯¯å·®å®¹é™$\\varepsilon$ä¹‹å¤–åŒºåˆ†ç”Ÿæˆçš„å®ä½“ä¸åŸå§‹å®ä½“ï¼Œåˆ™è¯¥ç³»ç»Ÿå…·å¤‡ç›¸åº”çš„æ™ºèƒ½ã€‚è®ºæ–‡ä¸ä»…æä¾›äº†å½¢å¼åŒ–æ¡†æ¶å’Œå®è¯åè®®ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†è¯¥å®šä¹‰åœ¨æ¨¡å‹è¯„ä¼°ã€å®‰å…¨æ€§(safety)å’Œæ³›åŒ–(generalization)æ–¹é¢çš„ç†è®ºæ„ä¹‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at AGI-25. Enhancing mathematical rigor and conceptual clarity. All instances of \"category\" and \"sample\" have been consistently replaced with \"concept\" and \"entity\" respectively, and the precise relationship between concepts, fibres, and entities has been refined throughout the paper for greater accuracy",
      "pdf_url": "https://arxiv.org/pdf/2507.22423v2",
      "published_date": "2025-07-30 07:04:00 UTC",
      "updated_date": "2025-08-13 20:09:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:50.392206+00:00"
    },
    {
      "arxiv_id": "2507.22421v1",
      "title": "Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking",
      "title_zh": "é¢å‘å®æ—¶è§†é¢‘åˆ†æçš„é«˜æ•ˆæ—¶ç©ºå»ºæ¨¡ï¼šåŠ¨ä½œè¯†åˆ«ä¸ç›®æ ‡è·Ÿè¸ªçš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Shahla John"
      ],
      "abstract": "Real-time video analysis remains a challenging problem in computer vision, requiring efficient processing of both spatial and temporal information while maintaining computational efficiency. Existing approaches often struggle to balance accuracy and speed, particularly in resource-constrained environments. In this work, we present a unified framework that leverages advanced spatial-temporal modeling techniques for simultaneous action recognition and object tracking. Our approach builds upon recent advances in parallel sequence modeling and introduces a novel hierarchical attention mechanism that adaptively focuses on relevant spatial regions across temporal sequences. We demonstrate that our method achieves state-of-the-art performance on standard benchmarks while maintaining real-time inference speeds. Extensive experiments on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action recognition accuracy and 2.8% in tracking precision compared to existing methods, with 40% faster inference time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®æ—¶è§†é¢‘åˆ†æåœ¨å¹³è¡¡å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç”¨äºåŒæ—¶è¿›è¡Œ Action Recognition å’Œ Object Tracking çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ–¹æ³•åŸºäº Parallel Sequence Modeling æŠ€æœ¯ï¼Œå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ Hierarchical Attention Mechanismï¼Œèƒ½å¤Ÿè·¨æ—¶é—´åºåˆ—è‡ªé€‚åº”åœ°æ•æ‰å…³é”®ç©ºé—´åŒºåŸŸçš„ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒå®æ—¶æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ UCF-101 å’Œ HMDB-51 æ•°æ®é›†ä¸Šçš„ Action Recognition å‡†ç¡®ç‡æå‡äº† 3.2%ï¼Œåœ¨ MOT17 æ•°æ®é›†ä¸Šçš„ Tracking Precision æé«˜äº† 2.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æŠ€æœ¯å¿« 40%ï¼Œæ˜¾è‘—æå‡äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22421v1",
      "published_date": "2025-07-30 06:49:11 UTC",
      "updated_date": "2025-07-30 06:49:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:49.295809+00:00"
    },
    {
      "arxiv_id": "2507.22419v1",
      "title": "Systematic Evaluation of Knowledge Graph Repair with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›¾è°±ä¿®å¤ç³»ç»Ÿæ€§è¯„ä¼°",
      "authors": [
        "Tung-Wei Lin",
        "Gabe Fierro",
        "Han Li",
        "Tianzhen Hong",
        "Pierluigi Nuzzo",
        "Alberto Sangiovanni-Vinentelli"
      ],
      "abstract": "We present a systematic approach for evaluating the quality of knowledge graph repairs with respect to constraint violations defined in shapes constraint language (SHACL). Current evaluation methods rely on \\emph{ad hoc} datasets, which limits the rigorous analysis of repair systems in more general settings. Our method addresses this gap by systematically generating violations using a novel mechanism, termed violation-inducing operations (VIOs). We use the proposed evaluation framework to assess a range of repair systems which we build using large language models. We analyze the performance of these systems across different prompting strategies. Results indicate that concise prompts containing both the relevant violated SHACL constraints and key contextual information from the knowledge graph yield the best performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±(Knowledge Graph)ä¿®å¤è´¨é‡è¯„ä¼°ç¼ºä¹ä¸¥è°¨æ€§çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€å¥—åŸºäºSHACL(Shapes Constraint Language)çº¦æŸè¿èƒŒçš„ç³»ç»ŸåŒ–è¯„ä¼°æ–¹æ³•ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰è¯„ä¼°æ–¹æ³•è¿‡åº¦ä¾èµ–ç‰¹å®šæ•°æ®é›†çš„å±€é™æ€§ï¼Œä½œè€…è®¾è®¡äº†ä¸€ç§åä¸ºè¿èƒŒè¯±å¯¼æ“ä½œ(VIOs, violation-inducing operations)çš„æ–°æœºåˆ¶æ¥ç³»ç»Ÿæ€§åœ°ç”Ÿæˆæ•°æ®è¿è¯¯ã€‚ç ”ç©¶åˆ©ç”¨è¯¥è¯„ä¼°æ¡†æ¶å¯¹å¤šç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºçš„ä¿®å¤ç³»ç»Ÿè¿›è¡Œäº†æ·±å…¥æµ‹è¯•ï¼Œå¹¶é‡ç‚¹åˆ†æäº†ä¸åŒæç¤ºç­–ç•¥(prompting strategies)å¯¹ä¿®å¤æ•ˆæœçš„å½±å“ã€‚å®éªŒå‘ç°ï¼ŒåŒ…å«ç›¸å…³SHACLçº¦æŸå’ŒçŸ¥è¯†å›¾è°±å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç®€æ´æç¤ºè¯åœ¨ä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°æœ€ä¼˜ã€‚è¯¥å·¥ä½œé€šè¿‡æ ‡å‡†åŒ–çš„è¯„ä¼°æµç¨‹ï¼Œä¸ºåˆ©ç”¨LLMsè¿›è¡ŒçŸ¥è¯†å›¾è°±è‡ªåŠ¨ä¿®å¤çš„ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†å’Œä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22419v1",
      "published_date": "2025-07-30 06:46:30 UTC",
      "updated_date": "2025-07-30 06:46:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:49.901473+00:00"
    },
    {
      "arxiv_id": "2507.22418v1",
      "title": "Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching",
      "title_zh": "åŸºäºæµåŒ¹é…çš„åŒ»å­¦å›¾åƒåˆ†å‰²å¶ç„¶ä¸ç¡®å®šæ€§ä¼°è®¡",
      "authors": [
        "Phi Van Nguyen",
        "Ngoc Huynh Trinh",
        "Duy Minh Lam Nguyen",
        "Phu Loc Nguyen",
        "Quoc Long Tran"
      ],
      "abstract": "Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at https://github.com/huynhspm/Data-Uncertainty",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­çš„ aleatoric uncertaintyï¼ˆå¶ç„¶ä¸ç¡®å®šæ€§ï¼‰é‡åŒ–é—®é¢˜ï¼Œæ—¨åœ¨åæ˜ ä¸åŒä¸“å®¶æ ‡æ³¨è€…ä¹‹é—´çš„è‡ªç„¶å·®å¼‚ã€‚ç”±äºç°æœ‰çš„ diffusion ç­‰ç”Ÿæˆæ¨¡å‹åœ¨éšæœºé‡‡æ ·å’Œç²¾ç¡®å¯†åº¦å»ºæ¨¡æ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰ä¸ç¡®å®šæ€§ï¼Œä½œè€…æå‡ºåˆ©ç”¨ conditional flow matchingï¼ˆæ¡ä»¶æµåŒ¹é…ï¼‰æŠ€æœ¯è¿›è¡Œæ”¹è¿›ã€‚è¿™æ˜¯ä¸€ç§æ— éœ€æ¨¡æ‹Ÿä¸”èƒ½å­¦ä¹ ç²¾ç¡®å¯†åº¦çš„æµå¼ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥å›¾åƒå¼•å¯¼å¹¶è¿›è¡Œå¤šæ¬¡é‡‡æ ·ï¼Œåˆ©ç”¨åƒç´ çº§æ–¹å·®(pixel-wise variance)åˆæˆåˆ†å‰²æ ·æœ¬ã€‚è¯¥ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¾¹ç•Œæ¨¡ç³ŠåŒºåŸŸçš„ä¸ç¡®å®šæ€§ï¼Œæä¾›åæ˜ æ ‡æ³¨è€…é—´å·®å¼‚çš„ç¨³å¥é‡åŒ–ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç»´æŒç«äº‰æ€§åˆ†å‰²å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œç”Ÿæˆçš„ uncertainty mapsï¼ˆä¸ç¡®å®šæ€§åœ°å›¾ï¼‰ä¸ºåˆ†å‰²ç»“æœçš„å¯é æ€§æä¾›äº†æ·±åº¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22418v1",
      "published_date": "2025-07-30 06:45:32 UTC",
      "updated_date": "2025-07-30 06:45:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:56.189942+00:00"
    },
    {
      "arxiv_id": "2507.22411v2",
      "title": "NeedleChain: Measuring Intact Context Comprehension Capability of Large Language Models",
      "title_zh": "NeedleChainï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„å®Œæ•´ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›",
      "authors": [
        "Hyeonseok Moon",
        "Heuiseok Lim"
      ],
      "abstract": "Recent reports suggest that LLMs can handle increasingly long contexts. However, many existing benchmarks for context understanding embed substantial query-irrelevant content, which shifts evaluation toward retrieving relevant snippets rather than fully integrating all provided information. Under this setting, we view that current benchmarks can overestimate true context-understanding ability of LLMs. In particular, we demonstrate that when the context consists entirely of query-relevant text, even advanced models such as GPT-4o fail to reliably integrate inputs as short as 200 tokens. To evaluate this capability more rigorously, we introduce NeedleChain, a benchmark designed to test whether models can faithfully incorporate all given evidence. NeedleChain includes three variants that differ in the required order of comprehension, along with a parallel benchmark based on the needle-in-a-haystack(NIAH) paradigm. By comparing these variants, NeedleChain enables a more comprehensive assessment of context understanding. We further propose a training-free strategy that encourages models to reflect all available information, ROPE contraction, highlighting the importance of full-context integration and pointing to new directions for improving reliable reasoning over context.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•å› åŒ…å«å¤§é‡æ— å…³ä¿¡æ¯è€Œä¾§é‡äºä¿¡æ¯æ£€ç´¢ï¼Œå¯¼è‡´æ¨¡å‹çœŸæ­£çš„å®Œæ•´ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›è¢«é«˜ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œå½“ä¸Šä¸‹æ–‡å…¨éƒ¨ä¸æŸ¥è¯¢ç›¸å…³æ—¶ï¼Œå³ä¾¿å¦‚GPT-4oç­‰æ¨¡å‹åœ¨å¤„ç†ä»…200ä¸ªtokenæ—¶ä¹Ÿéš¾ä»¥å¯é åœ°æ•´åˆä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº†NeedleChainåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰ç§ç†è§£é¡ºåºå˜ä½“åŠä¸€ä¸ªåŸºäºneedle-in-a-haystack(NIAH)èŒƒå¼çš„å¹¶è¡ŒåŸºå‡†ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°æ¨¡å‹å¿ å®ç»“åˆæ‰€æœ‰ç»™å®šè¯æ®çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åä¸ºROPE contractionçš„æ— éœ€è®­ç»ƒçš„ç­–ç•¥ï¼Œæ—¨åœ¨ä¿ƒä½¿æ¨¡å‹åˆ©ç”¨æ‰€æœ‰å¯ç”¨ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚NeedleChainä¸ºå…¨é¢è¯„ä¼°ä¸Šä¸‹æ–‡ç†è§£æä¾›äº†æ–°å·¥å…·ï¼Œå¹¶ä¸ºå®ç°æ›´å¯é çš„é•¿æ–‡æœ¬æ¨ç†æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22411v2",
      "published_date": "2025-07-30 06:29:50 UTC",
      "updated_date": "2026-01-02 08:24:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:45:59.852359+00:00"
    },
    {
      "arxiv_id": "2507.22410v1",
      "title": "Question Generation for Assessing Early Literacy Reading Comprehension",
      "title_zh": "é¢å‘æ—©æœŸè¯»å†™é˜…è¯»ç†è§£è¯„ä¼°çš„é—®é¢˜ç”Ÿæˆ",
      "authors": [
        "Xiaocheng Yang",
        "Sumuk Shashidhar",
        "Dilek Hakkani-Tur"
      ],
      "abstract": "Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ K-2 é˜¶æ®µçš„è‹±è¯­å­¦ä¹ è€…ï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆé˜…è¯»ç†è§£é—®é¢˜çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ—©æœŸè¯†å­—é˜¶æ®µçš„é˜…è¯»è¯„ä¼°æ•ˆç‡ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†å¯¹é˜…è¯»ææ–™å†…å®¹çš„å…¨é¢è¦†ç›–ï¼Œå¹¶èƒ½æ ¹æ®å­¦ä¹ è€…çš„å…·ä½“ç†Ÿç»ƒç¨‹åº¦è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œç”Ÿæˆæ¶µç›–å¤šç§éš¾åº¦æ°´å¹³å’Œç±»å‹çš„å¤šæ ·åŒ–é—®é¢˜ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨ FairytaleQA æ•°æ®é›†ä½œä¸ºç´ æï¼Œåœ¨è¯¥æ¡†æ¶ä¸‹è¯„ä¼°äº†å¤šç§ Language Models çš„å®é™…æ€§èƒ½ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å¯¹é˜…è¯»ç†è§£èƒ½åŠ›çš„é€å½»è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªä¸» AI-driven English instructors æ ¸å¿ƒç»„ä»¶çš„æ½œåŠ›ã€‚è¿™ä¸€æˆæœä¸ºå¼€å‘ä¸ªæ€§åŒ–ä¸”æ™ºèƒ½åŒ–çš„æ—©æœŸè‹±è¯­é˜…è¯»æ•™å­¦ç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "2 pages, 1 figure, accepted by SLaTE 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22410v1",
      "published_date": "2025-07-30 06:27:02 UTC",
      "updated_date": "2025-07-30 06:27:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:03.393396+00:00"
    },
    {
      "arxiv_id": "2507.22404v1",
      "title": "MINR: Implicit Neural Representations with Masked Image Modelling",
      "title_zh": "MINRï¼šç»“åˆæ©ç å›¾åƒå»ºæ¨¡çš„éšå¼ç¥ç»è¡¨ç¤º",
      "authors": [
        "Sua Lee",
        "Joonhun Lee",
        "Myungjoo Kang"
      ],
      "abstract": "Self-supervised learning methods like masked autoencoders (MAE) have shown significant promise in learning robust feature representations, particularly in image reconstruction-based pretraining task. However, their performance is often strongly dependent on the masking strategies used during training and can degrade when applied to out-of-distribution data. To address these limitations, we introduce the masked implicit neural representations (MINR) framework that synergizes implicit neural representations with masked image modeling. MINR learns a continuous function to represent images, enabling more robust and generalizable reconstructions irrespective of masking strategies. Our experiments demonstrate that MINR not only outperforms MAE in in-domain scenarios but also in out-of-distribution settings, while reducing model complexity. The versatility of MINR extends to various self-supervised learning applications, confirming its utility as a robust and efficient alternative to existing frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MINRæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§å°†éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations)ä¸æ©ç å›¾åƒå»ºæ¨¡(Masked Image Modeling)ç›¸ç»“åˆçš„æ–°å‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚é’ˆå¯¹æ©ç è‡ªç¼–ç å™¨(MAE)é«˜åº¦ä¾èµ–æ©ç ç­–ç•¥ä¸”åœ¨åˆ†å¸ƒå¤–(out-of-distribution)æ•°æ®ä¸Šè¡¨ç°ä¸‹é™çš„é—®é¢˜ï¼ŒMINRé€šè¿‡å­¦ä¹ è¿ç»­å‡½æ•°æ¥è¡¨ç¤ºå›¾åƒï¼Œå®ç°äº†å¯¹ä¸åŒæ©ç ç­–ç•¥æ›´å…·é²æ£’æ€§å’Œé€šç”¨æ€§çš„é‡å»ºæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMINRåœ¨åŸŸå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸‹çš„æ€§èƒ½å‡ä¼˜äºMAEï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨¡å‹çš„å¤æ‚æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¤šç§è‡ªç›‘ç£å­¦ä¹ åº”ç”¨ä¸­å±•ç°äº†æé«˜çš„é€šç”¨æ€§ï¼Œä¸ºç°æœ‰è§†è§‰é¢„è®­ç»ƒä»»åŠ¡æä¾›äº†ä¸€ä¸ªç¨³å¥ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the ICCV 2023 workshop on Out-of-Distribution Generalization in Computer Vision",
      "pdf_url": "https://arxiv.org/pdf/2507.22404v1",
      "published_date": "2025-07-30 06:12:57 UTC",
      "updated_date": "2025-07-30 06:12:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:04.845921+00:00"
    },
    {
      "arxiv_id": "2507.22371v1",
      "title": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection",
      "title_zh": "SAELï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸è‡ªé€‚åº”ä¸“å®¶æ··åˆçš„æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹",
      "authors": [
        "Lei Yu",
        "Shiqi Cheng",
        "Zhirong Huang",
        "Jingyuan Zhang",
        "Chenjie Shen",
        "Junyi Lu",
        "Li Yang",
        "Fengjun Zhang",
        "Jiajia Ma"
      ],
      "abstract": "With the increasing security issues in blockchain, smart contract vulnerability detection has become a research focus. Existing vulnerability detection methods have their limitations: 1) Static analysis methods struggle with complex scenarios. 2) Methods based on specialized pre-trained models perform well on specific datasets but have limited generalization capabilities. In contrast, general-purpose Large Language Models (LLMs) demonstrate impressive ability in adapting to new vulnerability patterns. However, they often underperform on specific vulnerability types compared to methods based on specialized pre-trained models. We also observe that explanations generated by general-purpose LLMs can provide fine-grained code understanding information, contributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for smart contract vulnerability detection. We first design targeted prompts to guide LLMs in identifying vulnerabilities and generating explanations, which serve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to process contract code and explanations, enhancing task-specific performance. To combine the strengths of each approach, we introduce an Adaptive Mixture-of-Experts architecture. This dynamically adjusts feature weights via a Gating Network, which selects relevant features using TopK filtering and Softmax normalization, and incorporates a Multi-Head Self-Attention mechanism to enhance cross-feature relationships. This design enables effective integration of LLM predictions, explanation features, and code features through gradient optimization. The loss function jointly considers both independent feature performance and overall weighted predictions. Experiments show that SAEL outperforms existing methods across various vulnerabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SAELï¼Œä¸€ç§åŸºäº Large Language Models (LLMs) çš„æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœé™æ€åˆ†æå’Œä¸“ç”¨é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³æˆ–ç‰¹å®šç±»å‹å‡†ç¡®ç‡å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æç¤ºè¯å¼•å¯¼ LLMs è¯†åˆ«æ¼æ´å¹¶ç”Ÿæˆç»†ç²’åº¦çš„è§£é‡Šä¿¡æ¯ä½œä¸ºé¢„æµ‹ç‰¹å¾ï¼Œå¹¶å¯¹ CodeT5 å’Œ T5 è¿›è¡Œ Prompt-tuning ä»¥å¼ºåŒ–å¯¹åˆçº¦ä»£ç ä¸è§£é‡Šæ–‡æœ¬çš„ç†è§£ã€‚ä¸ºäº†æœ‰æ•ˆèåˆå¼‚æ„ä¿¡æ¯ï¼ŒSAEL å¼•å…¥äº† Adaptive Mixture-of-Experts æ¶æ„ï¼Œé€šè¿‡ç”± TopK è¿‡æ»¤ã€Softmax å½’ä¸€åŒ–åŠ Multi-Head Self-Attention æ„æˆçš„ Gating Network åŠ¨æ€è°ƒæ•´ä¸åŒç‰¹å¾çš„æƒé‡ã€‚è¯¥è®¾è®¡åˆ©ç”¨æ¢¯åº¦ä¼˜åŒ–æŠ€æœ¯æ•´åˆäº† LLM é¢„æµ‹ã€è§£é‡Šç‰¹å¾ä¸åŸå§‹ä»£ç ç‰¹å¾ï¼Œä½¿æŸå¤±å‡½æ•°èƒ½åŒæ—¶å…¼é¡¾ç‹¬ç«‹ç‰¹å¾è¡¨ç°ä¸æ•´ä½“åŠ æƒé¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒSAEL åœ¨å¤šç§æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ï¼Œä¸ºç»“åˆé€šç”¨è¯­è¨€æ¨¡å‹çš„è§£é‡Šèƒ½åŠ›ä¸è‡ªé€‚åº”ä¸“å®¶æœºåˆ¶æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to ICSME 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22371v1",
      "published_date": "2025-07-30 04:28:00 UTC",
      "updated_date": "2025-07-30 04:28:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:12.496365+00:00"
    },
    {
      "arxiv_id": "2507.22369v3",
      "title": "Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring",
      "title_zh": "è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åœ¨è¯¾å ‚æ´»åŠ¨ç›‘æµ‹ä¸­çš„åº”ç”¨æ¢ç´¢",
      "authors": [
        "Sinh Trong Vu",
        "Hieu Trung Pham",
        "Dung Manh Nguyen",
        "Hieu Minh Hoang",
        "Nhu Hoang Le",
        "Thu Ha Pham",
        "Tai Tan Mai"
      ],
      "abstract": "Classroom behavior monitoring is a critical aspect of educational research, with significant implications for student engagement and learning outcomes. Recent advancements in Visual Question Answering (VQA) models offer promising tools for automatically analyzing complex classroom interactions from video recordings. In this paper, we investigate the applicability of several state-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and NVILA, in the context of classroom behavior analysis. To facilitate rigorous evaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world classroom video recordings at the Banking Academy of Vietnam. We present the methodology for data collection, annotation, and benchmark the performance of the selected VQA models on this dataset. Our initial experimental results demonstrate that all four models achieve promising performance levels in answering behavior-related visual questions, showcasing their potential in future classroom analytics and intervention systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰é—®ç­”(Visual Question Answering, VQA)æŠ€æœ¯åœ¨è¯¾å ‚æ´»åŠ¨ç›‘æ§ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åˆ†æè§†é¢‘è®°å½•æ¥æå‡å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¹ æ•ˆæœã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„å¼€æº VQA æ¨¡å‹ï¼ŒåŒ…æ‹¬ LLaMA2ã€LLaMA3ã€QWEN3 å’Œ NVILA åœ¨è¯¾å ‚è¡Œä¸ºåˆ†æèƒŒæ™¯ä¸‹çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è¿›è¡Œä¸¥è°¨çš„è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† BAV-Classroom-VQA æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æºè‡ªè¶Šå—é“¶è¡Œå­¦é™¢(Banking Academy of Vietnam)çš„çœŸå®è¯¾å ‚è§†é¢‘è®°å½•ã€‚æ–‡ä¸­è¯¦ç»†é˜è¿°äº†æ•°æ®é‡‡é›†ã€æ ‡æ³¨çš„æ–¹æ³•è®ºï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šå¯¹æ‰€é€‰ VQA æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•(benchmark)ã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰å››ç§æ¨¡å‹åœ¨å›ç­”ä¸è¡Œä¸ºç›¸å…³çš„è§†è§‰é—®é¢˜æ–¹é¢å‡å–å¾—äº†ç†æƒ³çš„æ•ˆæœã€‚è¿™å……åˆ†å±•ç¤ºäº† VQA æŠ€æœ¯åœ¨æœªæ¥è¯¾å ‚åˆ†æå’Œå¹²é¢„ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–æ•™è‚²ç ”ç©¶æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22369v3",
      "published_date": "2025-07-30 04:25:14 UTC",
      "updated_date": "2025-09-02 02:23:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:11.899957+00:00"
    },
    {
      "arxiv_id": "2507.22365v2",
      "title": "Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making",
      "title_zh": "è¶…è¶Šå‡†ç¡®ç‡ï¼šAI å…ƒè®¤çŸ¥æ•æ„Ÿåº¦å¦‚ä½•æå‡ AI è¾…åŠ©å†³ç­–",
      "authors": [
        "ZhaoBin Li",
        "Mark Steyvers"
      ],
      "abstract": "In settings where human decision-making relies on AI input, both the predictive accuracy of the AI system and the reliability of its confidence estimates influence decision quality. We highlight the role of AI metacognitive sensitivity -- its ability to assign confidence scores that accurately distinguish correct from incorrect predictions -- and introduce a theoretical framework for assessing the joint impact of AI's predictive accuracy and metacognitive sensitivity in hybrid decision-making settings. Our analysis identifies conditions under which an AI with lower predictive accuracy but higher metacognitive sensitivity can enhance the overall accuracy of human decision making. Finally, a behavioral experiment confirms that greater AI metacognitive sensitivity improves human decision performance. Together, these findings underscore the importance of evaluating AI assistance not only by accuracy but also by metacognitive sensitivity, and of optimizing both to achieve superior decision outcomes.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ AI-assisted Decision Making ä¸­ï¼Œé™¤äº†é¢„æµ‹å‡†ç¡®ç‡å¤–ï¼ŒAI metacognitive sensitivity å¯¹å†³ç­–è´¨é‡çš„å…³é”®ä½œç”¨ã€‚AI metacognitive sensitivity æŒ‡çš„æ˜¯AIé€šè¿‡ç½®ä¿¡åº¦è¯„åˆ†å‡†ç¡®åŒºåˆ†å…¶é¢„æµ‹æ­£ç¡®ä¸å¦çš„èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°AIçš„é¢„æµ‹å‡†ç¡®ç‡ä¸å…ƒè®¤çŸ¥æ•æ„Ÿåº¦åœ¨æ··åˆå†³ç­–åœºæ™¯ä¸­çš„å…±åŒå½±å“ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œå³ä½¿AIçš„é¢„æµ‹å‡†ç¡®ç‡è¾ƒä½ï¼Œåªè¦å…¶å…·å¤‡æ›´é«˜çš„å…ƒè®¤çŸ¥æ•æ„Ÿåº¦ï¼Œä¹Ÿèƒ½æœ‰æ•ˆæå‡äººç±»å†³ç­–çš„æ•´ä½“è¡¨ç°ã€‚æ­¤å¤–ï¼Œä¸€é¡¹è¡Œä¸ºå®éªŒè¿›ä¸€æ­¥è¯å®äº†è¾ƒé«˜çš„ AI metacognitive sensitivity èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„äººç±»çš„å†³ç­–ç»©æ•ˆã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨è¯„ä¼°AIè¾…åŠ©ç³»ç»Ÿæ—¶ï¼Œä¸åº”ä»…å…³æ³¨å‡†ç¡®ç‡ï¼Œè¿˜åº”åŒæ—¶ä¼˜åŒ–å…ƒè®¤çŸ¥æ•æ„Ÿåº¦ï¼Œä»¥å®ç°æ›´ä¼˜çš„å†³ç­–æˆæœã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22365v2",
      "published_date": "2025-07-30 04:05:50 UTC",
      "updated_date": "2025-08-13 23:05:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:25.068350+00:00"
    },
    {
      "arxiv_id": "2507.22361v1",
      "title": "Object Recognition Datasets and Challenges: A Review",
      "title_zh": "ç›®æ ‡è¯†åˆ«æ•°æ®é›†ä¸æŒ‘æˆ˜ï¼šç»¼è¿°",
      "authors": [
        "Aria Salari",
        "Abtin Djavadifar",
        "Xiangrui Liu",
        "Homayoun Najjaran"
      ],
      "abstract": "Object recognition is among the fundamental tasks in the computer vision applications, paving the path for all other image understanding operations. In every stage of progress in object recognition research, efforts have been made to collect and annotate new datasets to match the capacity of the state-of-the-art algorithms. In recent years, the importance of the size and quality of datasets has been intensified as the utility of the emerging deep network techniques heavily relies on training data. Furthermore, datasets lay a fair benchmarking means for competitions and have proved instrumental to the advancements of object recognition research by providing quantifiable benchmarks for the developed models. Taking a closer look at the characteristics of commonly-used public datasets seems to be an important first step for data-driven and machine learning researchers. In this survey, we provide a detailed analysis of datasets in the highly investigated object recognition areas. More than 160 datasets have been scrutinized through statistics and descriptions. Additionally, we present an overview of the prominent object recognition benchmarks and competitions, along with a description of the metrics widely adopted for evaluation purposes in the computer vision community. All introduced datasets and challenges can be found online at github.com/AbtinDjavadifar/ORDC.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡å¯¹è®¡ç®—æœºè§†è§‰é¢†åŸŸæ ¸å¿ƒä»»åŠ¡ç‰©ä½“è¯†åˆ« (Object Recognition) çš„æ•°æ®é›†ä¸æŒ‘æˆ˜èµ›è¿›è¡Œäº†å…¨é¢å›é¡¾ã€‚ç ”ç©¶å¼ºè°ƒäº†åœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£ï¼Œæ•°æ®é›†çš„è§„æ¨¡ä¸è´¨é‡å¯¹äºç®—æ³•è®­ç»ƒåŠå…¬å¹³åŸºå‡†æµ‹è¯• (Benchmarking) çš„å†³å®šæ€§ä½œç”¨ã€‚ä½œè€…è¯¦ç»†åˆ†æäº†è¶…è¿‡160ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œå¹¶æä¾›äº†æ·±å…¥çš„ç»Ÿè®¡æ•°æ®ä¸ç‰¹å¾æè¿°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ€»ç»“äº†ä¸»æµçš„ç‰©ä½“è¯†åˆ«æŒ‘æˆ˜èµ›ã€ç«èµ›æ´»åŠ¨ä»¥åŠç¤¾åŒºé€šç”¨çš„è¯„ä¼°æŒ‡æ ‡ (Metrics)ã€‚é€šè¿‡æ¢è®¨æ•°æ®é›†å¦‚ä½•æä¾›å¯é‡åŒ–çš„è¡¡é‡æ ‡å‡†ï¼Œè¯¥ç»¼è¿°ä¸ºæ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ ç ”ç©¶è€…æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚ä½œè€…åŒæ—¶å°†æ‰€æœ‰ç›¸å…³èµ„æºæ•´ç†å¹¶å¼€æºåœ¨ GitHub å¹³å°ä¸Šï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„æŒç»­è¿›æ­¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22361v1",
      "published_date": "2025-07-30 03:56:37 UTC",
      "updated_date": "2025-07-30 03:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:26.746164+00:00"
    },
    {
      "arxiv_id": "2507.22360v1",
      "title": "GVD: Guiding Video Diffusion Model for Scalable Video Distillation",
      "title_zh": "GVDï¼šé¢å‘å¯æ‰©å±•è§†é¢‘è’¸é¦çš„å¼•å¯¼å¼è§†é¢‘æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Kunyang Li",
        "Jeffrey A Chan Santiago",
        "Sarinda Dhanesh Samarasinghe",
        "Gaowen Liu",
        "Mubarak Shah"
      ],
      "abstract": "To address the larger computation and storage requirements associated with large video datasets, video dataset distillation aims to capture spatial and temporal information in a significantly smaller dataset, such that training on the distilled data has comparable performance to training on all of the data. We propose GVD: Guiding Video Diffusion, the first diffusion-based video distillation method. GVD jointly distills spatial and temporal features, ensuring high-fidelity video generation across diverse actions while capturing essential motion information. Our method's diverse yet representative distillations significantly outperform previous state-of-the-art approaches on the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC). Specifically, our method achieves 78.29 percent of the original dataset's performance using only 1.98 percent of the total number of frames in MiniUCF. Additionally, it reaches 73.83 percent of the performance with just 3.30 percent of the frames in HMDB51. Experimental results across benchmark video datasets demonstrate that GVD not only achieves state-of-the-art performance but can also generate higher resolution videos and higher IPC without significantly increasing computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GVD (Guiding Video Diffusion)ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹ (Diffusion-based) çš„è§†é¢‘æ•°æ®é›†è’¸é¦ (Video dataset distillation) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†é¢‘æ•°æ®é›†å¯¹è®¡ç®—å’Œå­˜å‚¨çš„é«˜éœ€æ±‚ã€‚GVD é€šè¿‡è”åˆè’¸é¦ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œåœ¨æ•æ‰å…³é”®è¿åŠ¨ä¿¡æ¯çš„åŒæ—¶ï¼Œç¡®ä¿äº†è·¨å¤šç§åŠ¨ä½œçš„é«˜ä¿çœŸè§†é¢‘ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ MiniUCF å’Œ HMDB51 æ•°æ®é›†ä¸Šçš„ 5ã€10 å’Œ 20 IPC (Instances Per Class) è®¾ç½®ä¸‹å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒGVD åœ¨ MiniUCF ä¸Šä»…åˆ©ç”¨ 1.98% çš„æ€»å¸§æ•°å°±å®ç°äº†åŸå§‹æ•°æ®é›† 78.29% çš„æ€§èƒ½ï¼Œåœ¨ HMDB51 ä¸Šåˆ©ç”¨ 3.30% çš„å¸§æ•°è¾¾åˆ°äº† 73.83% çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜ GVD èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„è§†é¢‘å¹¶æ”¯æŒæ›´é«˜çš„ IPCï¼Œå±•ç°å‡ºä¼˜å¼‚çš„å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22360v1",
      "published_date": "2025-07-30 03:51:35 UTC",
      "updated_date": "2025-07-30 03:51:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:34.491881+00:00"
    },
    {
      "arxiv_id": "2507.22359v3",
      "title": "League of LLMs: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models",
      "title_zh": "League of LLMsï¼šä¸€ç§å¤§è¯­è¨€æ¨¡å‹äº’è¯„ä¼°çš„æ— åŸºå‡†èŒƒå¼",
      "authors": [
        "Qianhong Guo",
        "Wei Xie",
        "Xiaofang Cai",
        "Enze Wang",
        "Shuoyoucheng Ma",
        "Xiaobing Sun",
        "Tian Xia",
        "Kai Chen",
        "Xiaofeng Wang",
        "Baosheng Wang"
      ],
      "abstract": "Although large language models (LLMs) have shown exceptional capabilities across a wide range of tasks, reliable evaluation remains a critical challenge due to data contamination, opaque operation, and subjective preferences. To address these issues, we propose League of LLMs (LOL), a novel benchmark-free evaluation paradigm that organizes multiple LLMs into a self-governed league for multi-round mutual evaluation. LOL integrates four core criteria (dynamic, transparent, objective, and professional) to mitigate key limitations of existing paradigms. Experiments on eight mainstream LLMs in mathematics and programming demonstrate that LOL can effectively distinguish LLM capabilities while maintaining high internal ranking stability (Top-$k$ consistency $= 70.7\\%$). Beyond ranking, LOL reveals empirical findings that are difficult for traditional paradigms to capture. For instance, ``memorization-based answering'' behaviors are observed in some models, and a statistically significant homophily bias is found within the OpenAI family ($Î”= 9$, $p < 0.05$). Finally, we make our framework and code publicly available as a valuable complement to the current LLM evaluation ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è¯„ä¼°ä¸­å­˜åœ¨çš„æ•°æ®æ±¡æŸ“ã€æ“ä½œä¸é€æ˜å’Œä¸»è§‚åå¥½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLeague of LLMs (LOL)çš„æ–°å‹è¯„ä¼°èŒƒå¼ã€‚è¿™ä¸€Benchmark-Freeçš„èŒƒå¼å°†å¤šä¸ªLLMsç»„ç»‡æˆä¸€ä¸ªè‡ªæ²»è”ç›Ÿè¿›è¡Œå¤šè½®ç›¸äº’è¯„ä¼°ï¼Œé€šè¿‡åŠ¨æ€æ€§(dynamic)ã€é€æ˜æ€§(transparent)ã€å®¢è§‚æ€§(objective)å’Œä¸“ä¸šæ€§(professional)å››å¤§æ ‡å‡†æ¥å¼¥è¡¥ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚åœ¨æ•°å­¦å’Œç¼–ç¨‹é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒLOLèƒ½æœ‰æ•ˆåŒºåˆ†æ¨¡å‹èƒ½åŠ›å¹¶ä¿æŒé«˜è¾¾70.7%çš„Top-$k$ consistencyæ’åç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ­ç¤ºäº†ä¼ ç»ŸèŒƒå¼éš¾ä»¥å‘ç°çš„å®è¯ç»“æœï¼Œå¦‚éƒ¨åˆ†æ¨¡å‹çš„â€œåŸºäºè®°å¿†çš„å›ç­”(memorization-based answering)â€è¡Œä¸ºä»¥åŠOpenAIç³»åˆ—æ¨¡å‹ä¸­æ˜¾è‘—çš„åŒè´¨æ€§åå·®(homophily bias)ã€‚è¯¥ç ”ç©¶åŠå…¶å¼€æºä»£ç ä¸ºå½“å‰çš„LLMè¯„ä¼°ç”Ÿæ€ç³»ç»Ÿæä¾›äº†é‡è¦çš„è¡¥å……ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22359v3",
      "published_date": "2025-07-30 03:50:46 UTC",
      "updated_date": "2026-01-07 04:22:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:42.190358+00:00"
    },
    {
      "arxiv_id": "2507.22358v1",
      "title": "Magentic-UI: Towards Human-in-the-loop Agentic Systems",
      "title_zh": "Magentic-UIï¼šè¿ˆå‘äººåœ¨å›è·¯çš„æ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Hussein Mozannar",
        "Gagan Bansal",
        "Cheng Tan",
        "Adam Fourney",
        "Victor Dibia",
        "Jingya Chen",
        "Jack Gerrits",
        "Tyler Payne",
        "Matheus Kunzler Maldaner",
        "Madeleine Grunde-McLaughlin",
        "Eric Zhu",
        "Griffin Bassman",
        "Jacob Alber",
        "Peter Chang",
        "Ricky Loynd",
        "Friederike Niedtner",
        "Ece Kamar",
        "Maya Murad",
        "Rafah Hosn",
        "Saleema Amershi"
      ],
      "abstract": "AI agents powered by large language models are increasingly capable of autonomously completing complex, multi-step tasks using external tools. Yet, they still fall short of human-level performance in most domains including computer use, software development, and research. Their growing autonomy and ability to interact with the outside world, also introduces safety and security risks including potentially misaligned actions and adversarial manipulation. We argue that human-in-the-loop agentic systems offer a promising path forward, combining human oversight and control with AI efficiency to unlock productivity from imperfect systems. We introduce Magentic-UI, an open-source web interface for developing and studying human-agent interaction. Built on a flexible multi-agent architecture, Magentic-UI supports web browsing, code execution, and file manipulation, and can be extended with diverse tools via Model Context Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for enabling effective, low-cost human involvement: co-planning, co-tasking, multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI across four dimensions: autonomous task completion on agentic benchmarks, simulated user testing of its interaction capabilities, qualitative studies with real users, and targeted safety assessments. Our findings highlight Magentic-UI's potential to advance safe and efficient human-agent collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Magentic-UIï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°äººæœºååŒAgentic Systemsçš„å¼€æºWebç•Œé¢ï¼Œé€šè¿‡å°†äººç±»ç›‘ç£ä¸AIæ•ˆç‡ç›¸ç»“åˆï¼Œè§£å†³äº†å½“å‰è‡ªä¸»æ™ºèƒ½ä½“åœ¨å¤æ‚ä»»åŠ¡ä¸­æ€§èƒ½å—é™åŠå­˜åœ¨çš„å®‰å…¨é£é™©ã€‚è¯¥ç³»ç»ŸåŸºäºçµæ´»çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œæ”¯æŒç½‘é¡µæµè§ˆã€ä»£ç æ‰§è¡Œå’Œæ–‡ä»¶æ“ä½œï¼Œå¹¶å¯é€šè¿‡Model Context Protocol (MCP) æ‰©å±•å¤šç§å·¥å…·ã€‚Magentic-UI åˆ›æ–°æ€§åœ°å¼•å…¥äº†ååŒè§„åˆ’(co-planning)ã€ååŒåˆ†å·¥(co-tasking)ã€å¤šä»»åŠ¡å¤„ç†ã€åŠ¨ä½œå®ˆå«(action guards)å’Œé•¿æœŸè®°å¿†ç­‰å…­ç§äº¤äº’æœºåˆ¶ï¼Œä»¥å®ç°ä½æˆæœ¬ä¸”æœ‰æ•ˆçš„äººç±»å‚ä¸ã€‚é€šè¿‡åœ¨è‡ªä¸»ä»»åŠ¡åŸºå‡†ã€ç”¨æˆ·æ¨¡æ‹Ÿæµ‹è¯•ã€å®šæ€§ç ”ç©¶åŠé’ˆå¯¹æ€§å®‰å…¨è¯„ä¼°å››ä¸ªç»´åº¦çš„å®éªŒï¼Œç ”ç©¶è¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨æå‡ä»»åŠ¡æ•ˆç‡çš„åŒæ—¶èƒ½æœ‰æ•ˆä¿éšœæ“ä½œå®‰å…¨æ€§ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†Magentic-UIåœ¨æ¨åŠ¨å®‰å…¨é«˜æ•ˆçš„äººæœºåä½œç³»ç»Ÿå‘å±•æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22358v1",
      "published_date": "2025-07-30 03:49:14 UTC",
      "updated_date": "2025-07-30 03:49:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:46.694594+00:00"
    },
    {
      "arxiv_id": "2508.00028v1",
      "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models",
      "title_zh": "åŸºäºé©¬å°”å¯å¤«é“¾æ¡†æ¶ä¸ ITU-R ä¼ æ’­æ¨¡å‹çš„å¯æ‰©å±•é¢‘è°±å¯ç”¨æ€§é¢„æµ‹",
      "authors": [
        "Abir Ray"
      ],
      "abstract": "Spectrum resources are often underutilized across time and space, motivating dynamic spectrum access strategies that allow secondary users to exploit unused frequencies. A key challenge is predicting when and where spectrum will be available (i.e., unused by primary licensed users) in order to enable proactive and interference-free access. This paper proposes a scalable framework for spectrum availability prediction that combines a two-state Markov chain model of primary user activity with high-fidelity propagation models from the ITU-R (specifically Recommendations P.528 and P.2108). The Markov chain captures temporal occupancy patterns, while the propagation models incorporate path loss and clutter effects to determine if primary signals exceed interference thresholds at secondary user locations. By integrating these components, the proposed method can predict spectrum opportunities both in time and space with improved accuracy. We develop the system model and algorithm for the approach, analyze its scalability and computational efficiency, and discuss assumptions, limitations, and potential applications. The framework is flexible and can be adapted to various frequency bands and scenarios. The results and analysis show that the proposed approach can effectively identify available spectrum with low computational cost, making it suitable for real-time spectrum management in cognitive radio networks and other dynamic spectrum sharing systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„é¢‘è°±å¯ç”¨æ€§é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€é¢‘è°±æ¥å…¥ç­–ç•¥æé«˜é¢‘è°±èµ„æºçš„åˆ©ç”¨ç‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸¤çŠ¶æ€ Markov Chain æ¨¡å‹ä¸ ITU-R ä¼ æ’­æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ P.528 å’Œ P.2108ï¼‰ï¼Œåˆ†åˆ«ç”¨äºæ•æ‰ä¸»ç”¨æˆ·çš„æ—¶é—´å ç”¨æ¨¡å¼ä»¥åŠè¯„ä¼°è·¯å¾„æŸè€—å’Œæ‚æ³¢æ•ˆåº”å¯¹ä¿¡å·å¹²æ‰°çš„å½±å“ã€‚é€šè¿‡æ•´åˆæ—¶é—´ä¸ç©ºé—´ç»´åº¦çš„é¢„æµ‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«æ¬¡çº§ç”¨æˆ·çš„é¢‘è°±æ¥å…¥æœºä¼šã€‚å®éªŒåˆ†æè¯æ˜ï¼Œè¯¥æ¡†æ¶å…·æœ‰è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒè®¤çŸ¥æ— çº¿ç”µç½‘ç»œä¸­çš„å®æ—¶é¢‘è°±ç®¡ç†ï¼Œä¸ºå¹²æ‰°å—é™ç¯å¢ƒä¸‹çš„é¢‘è°±å…±äº«æä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "math.NA"
      ],
      "primary_category": "cs.NI",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.00028v1",
      "published_date": "2025-07-30 03:22:55 UTC",
      "updated_date": "2025-07-30 03:22:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:42.691631+00:00"
    },
    {
      "arxiv_id": "2508.00921v1",
      "title": "SmartDate: AI-Driven Precision Sorting and Quality Control in Date Fruits",
      "title_zh": "SmartDateï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ¤°æ£ç²¾å‡†åˆ†æ‹£ä¸è´¨é‡æ§åˆ¶",
      "authors": [
        "Khaled Eskaf"
      ],
      "abstract": "SmartDate is an AI-powered system for automated sorting and quality control of date fruits. It combines deep learning, genetic algorithms, and reinforcement learning to improve classification accuracy and predict shelf life. The system uses high-resolution imaging and Visible-Near-Infrared (VisNIR) spectral sensors to evaluate key features such as moisture, sugar content, and texture. Reinforcement learning enables real-time adaptation to production conditions, while genetic algorithms optimize model parameters. SmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC of 0.96. The system reduces waste and ensures that only high-quality dates reach the market, setting a new benchmark in smart agriculture.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† SmartDateï¼Œè¿™æ˜¯ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„æ£æœè‡ªåŠ¨åŒ–åˆ†æ‹£ä¸è´¨é‡æ§åˆ¶ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åˆ†ç±»ç²¾åº¦å¹¶å®ç°ä¿è´¨æœŸé¢„æµ‹ã€‚è¯¥ç³»ç»Ÿé›†æˆä½¿ç”¨äº†æ·±åº¦å­¦ä¹  (Deep Learning)ã€é—ä¼ ç®—æ³• (Genetic Algorithms) å’Œå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)ï¼Œå¹¶ç»“åˆé«˜åˆ†è¾¨ç‡æˆåƒä¸å¯è§-è¿‘çº¢å¤– (VisNIR) å…‰è°±ä¼ æ„Ÿå™¨æ¥è¯„ä¼°æ°´åˆ†ã€å«ç³–é‡å’Œè´¨åœ°ç­‰å…³é”®ç‰¹å¾ã€‚å…¶ä¸­å¼ºåŒ–å­¦ä¹ æŠ€æœ¯èµ‹äºˆäº†ç³»ç»Ÿæ ¹æ®ç”Ÿäº§æ¡ä»¶è¿›è¡Œå®æ—¶è‡ªé€‚åº”çš„èƒ½åŠ›ï¼Œè€Œé—ä¼ ç®—æ³•åˆ™ç”¨äºä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSmartDate è¾¾åˆ°äº† 94.5% çš„å‡†ç¡®ç‡ã€93.1% çš„ F1-score ä»¥åŠ 0.96 çš„ AUC-ROC å€¼ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å‡å°‘èµ„æºæµªè´¹å¹¶ç¡®ä¿é«˜å“è´¨äº§å“è¿›å…¥å¸‚åœºï¼Œä¸ºæ™ºèƒ½å†œä¸š (Smart Agriculture) é¢†åŸŸæ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures, published in Proceedings of the 21st IEEE International Conference on High Performance Computing and Networking (HONET 2024), Doha, Qatar, December 2024",
      "pdf_url": "https://arxiv.org/pdf/2508.00921v1",
      "published_date": "2025-07-30 03:20:44 UTC",
      "updated_date": "2025-07-30 03:20:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:53.456430+00:00"
    },
    {
      "arxiv_id": "2507.22326v1",
      "title": "An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem",
      "title_zh": "å…ƒå®‡å®™æœåŠ¡ç”Ÿæ€ç³»ç»Ÿä¸­å¤§æ¨¡å‹èµ‹èƒ½æ™ºèƒ½ä½“çš„å¯è§£é‡Šæƒ…æ„Ÿå¯¹é½æ¡†æ¶",
      "authors": [
        "Qun Ma",
        "Xiao Xue",
        "Ming Zhang",
        "Yifan Shen",
        "Zihan Zhao"
      ],
      "abstract": "Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…ƒå®‡å®™æœåŠ¡ç”Ÿæ€ç³»ç»Ÿ(Metaverse Service Ecosystem)ä¸­åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨è¿æ¥è™šå®æœåŠ¡æ—¶é¢ä¸´çš„è§’è‰²æ•°æ®èåˆã€çŸ¥è¯†å…³è”åŠä¼¦ç†å®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„æƒ…æ„Ÿå¯¹é½æ¡†æ¶(explainable emotion alignment framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†äº‹å®å› ç´ (factual factors)é›†æˆåˆ°æ™ºèƒ½ä½“çš„å†³ç­–é—­ç¯ä¸­ï¼Œç³»ç»Ÿåœ°å±•ç¤ºäº†å¦‚ä½•ä¸ºè¿™äº›æ™ºèƒ½ä½“å®ç°æ›´å…·å…³è”æ€§çš„äº‹å®å¯¹é½(relational fact alignment)ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ç¦»çº¿åˆ°ç¦»çº¿(Offline-to-Offline)çš„å¤–å–åœºæ™¯ä¸­è¿›è¡Œäº†æ¨¡æ‹Ÿå®éªŒï¼Œä»¥è¯„ä¼°è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè·å¾—æ›´å…·çœŸå®æ„Ÿçš„ç¤¾ä¼šæ¶Œç°(social emergence)ï¼Œæœ‰æ•ˆæå‡äº†æ™ºèƒ½ä½“åœ¨å¤æ‚æœåŠ¡ç”Ÿæ€ä¸­çš„äº¤äº’è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºè·¨è¶Šè™šæ‹Ÿä¸ç°å®æœåŠ¡é¸¿æ²Ÿã€æ„å»ºå¯ä¿¡çš„å…ƒå®‡å®™æœåŠ¡æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22326v1",
      "published_date": "2025-07-30 02:00:26 UTC",
      "updated_date": "2025-07-30 02:00:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:46:50.892257+00:00"
    },
    {
      "arxiv_id": "2507.22324v1",
      "title": "From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications",
      "title_zh": "ä»è®ºæ–‡åˆ°ä»£ç ï¼šç§‘å­¦è®ºæ–‡æ ¸å¿ƒç®—æ³•çš„æŒ‰éœ€ç”Ÿæˆ",
      "authors": [
        "Cameron S. Movassaghi",
        "Amanda Momenzadeh",
        "Jesse G. Meyer"
      ],
      "abstract": "Maintaining software packages imposes significant costs due to dependency management, bug fixes, and versioning. We show that rich method descriptions in scientific publications can serve as standalone specifications for modern large language models (LLMs), enabling on-demand code generation that could supplant human-maintained libraries. We benchmark state-of-the-art models (GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with implementing a diverse set of core algorithms drawn from original publications. Our results demonstrate that current LLMs can reliably reproduce package functionality with performance indistinguishable from conventional libraries. These findings foreshadow a paradigm shift toward flexible, on-demand code generation and away from static, human-maintained packages, which will result in reduced maintenance overhead by leveraging published articles as sufficient context for the automated implementation of analytical workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç§‘å­¦å‡ºç‰ˆç‰©ä¸­çš„è¯¦ç»†æè¿°ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„ç‹¬ç«‹è§„èŒƒï¼Œå®ç°æ ¸å¿ƒç®—æ³•çš„æŒ‰éœ€ä»£ç ç”Ÿæˆï¼Œä»¥åº”å¯¹ä¼ ç»Ÿè½¯ä»¶åº“åœ¨é«˜æ˜‚ç»´æŠ¤æˆæœ¬ã€ä¾èµ–ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è¦æ±‚ GPT-o4-mini-highã€Gemini Pro 2.5 å’Œ Claude Sonnet 4 ç­‰å‰æ²¿æ¨¡å‹æ ¹æ®åŸå§‹æ–‡çŒ®å®ç°å¤šç§æ ¸å¿ƒç®—æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ LLMs èƒ½å¤Ÿå¯é åœ°é‡ç°è½¯ä»¶åŒ…åŠŸèƒ½ï¼Œä¸”å…¶æ€§èƒ½ä¸ä¼ ç»Ÿçš„äººå·¥ç»´æŠ¤åº“åŸºæœ¬æ— å¼‚ã€‚è¿™ä¸€å‘ç°é¢„ç¤ºç€è½¯ä»¶å¼€å‘èŒƒå¼çš„è½¬å˜ï¼Œå³ä»é™æ€çš„äººå·¥ç»´æŠ¤ç¨‹åºåŒ…è½¬å‘çµæ´»çš„æŒ‰éœ€ä»£ç ç”Ÿæˆã€‚é€šè¿‡å°†å·²å‘è¡¨çš„æ–‡ç« ä½œä¸ºè‡ªåŠ¨åŒ–å®æ–½åˆ†æå·¥ä½œæµçš„å……åˆ†ä¸Šä¸‹æ–‡ï¼Œè¯¥æ–¹æ³•æœ‰æœ›å¤§å¹…é™ä½è½¯ä»¶çš„é•¿æœŸç»´æŠ¤å¼€é”€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22324v1",
      "published_date": "2025-07-30 01:52:01 UTC",
      "updated_date": "2025-07-30 01:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:47:01.696050+00:00"
    },
    {
      "arxiv_id": "2507.22321v1",
      "title": "Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment",
      "title_zh": "åŸºäºååŒé¢†åŸŸè‡ªé€‚åº”çš„å¼‚æ„ç»“æ„ MRI å­¦ä¹ ï¼šç”¨äºè€å¹´æŠ‘éƒç—‡è¯„ä¼°",
      "authors": [
        "Yuzhen Gao",
        "Qianqian Wang",
        "Yongheng Sun",
        "Cui Wang",
        "Yongquan Liang",
        "Mingxia Liu"
      ],
      "abstract": "Accurate identification of late-life depression (LLD) using structural brain MRI is essential for monitoring disease progression and facilitating timely intervention. However, existing learning-based approaches for LLD detection are often constrained by limited sample sizes (e.g., tens), which poses significant challenges for reliable model training and generalization. Although incorporating auxiliary datasets can expand the training set, substantial domain heterogeneity, such as differences in imaging protocols, scanner hardware, and population demographics, often undermines cross-domain transferability. To address this issue, we propose a Collaborative Domain Adaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA leverages a Vision Transformer (ViT) to capture global anatomical context and a Convolutional Neural Network (CNN) to extract local structural features, with each branch comprising an encoder and a classifier. The CDA framework consists of three stages: (a) supervised training on labeled source data, (b) self-supervised target feature adaptation and (c) collaborative training on unlabeled target data. We first train ViT and CNN on source data, followed by self-supervised target feature adaptation by minimizing the discrepancy between classifier outputs from two branches to make the categorical boundary clearer. The collaborative training stage employs pseudo-labeled and augmented target-domain MRIs, enforcing prediction consistency under strong and weak augmentation to enhance domain robustness and generalization. Extensive experiments conducted on multi-site T1-weighted MRI data demonstrate that the CDA consistently outperforms state-of-the-art unsupervised domain adaptation methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™šå¹´æŠ‘éƒç—‡ (Late-Life Depression, LLD) æ£€æµ‹ä¸­é¢ä¸´çš„æ ·æœ¬é‡æœ‰é™ä»¥åŠç»“æ„æ ¸ç£å…±æŒ¯æˆåƒ (structural MRI) æ•°æ®è·¨åŸŸå¼‚è´¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä½œé¢†åŸŸè‡ªé€‚åº” (Collaborative Domain Adaptation, CDA) æ¡†æ¶ã€‚CDA æ¡†æ¶ç»“åˆäº†ç”¨äºæ•è·å…¨å±€è§£å‰–ä¸Šä¸‹æ–‡çš„ Vision Transformer (ViT) å’Œç”¨äºæå–å±€éƒ¨ç»“æ„ç‰¹å¾çš„å·ç§¯ç¥ç»ç½‘ç»œ (CNN)ï¼Œé€šè¿‡åŒåˆ†æ”¯æ¶æ„ååŒå·¥ä½œã€‚è¯¥æ¡†æ¶åŒ…å«åœ¨æœ‰æ ‡ç­¾æºæ•°æ®ä¸Šçš„ç›‘ç£è®­ç»ƒã€è‡ªç›‘ç£ç›®æ ‡ç‰¹å¾è‡ªé€‚åº”ä»¥åŠåœ¨æ— æ ‡ç­¾ç›®æ ‡æ•°æ®ä¸Šçš„åä½œè®­ç»ƒä¸‰ä¸ªé˜¶æ®µã€‚å…¶ä¸­ï¼Œè‡ªç›‘ç£é˜¶æ®µé€šè¿‡æœ€å°åŒ–ä¸¤ä¸ªåˆ†æ”¯åˆ†ç±»å™¨è¾“å‡ºä¹‹é—´çš„å·®å¼‚æ¥æ¸…æ™°åŒ–åˆ†ç±»è¾¹ç•Œï¼Œè€Œåä½œé˜¶æ®µåˆ™åˆ©ç”¨ä¼ªæ ‡ç­¾å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ç¡®ä¿é¢„æµ‹ä¸€è‡´æ€§ï¼Œä»è€Œæå‡æ¨¡å‹çš„è·¨åŸŸé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸­å¿ƒ T1 åŠ æƒæ ¸ç£å…±æŒ¯æˆåƒæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCDA æ¡†æ¶åœ¨ LLD æ£€æµ‹è¯„ä¼°ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22321v1",
      "published_date": "2025-07-30 01:38:32 UTC",
      "updated_date": "2025-07-30 01:38:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:47:02.497237+00:00"
    },
    {
      "arxiv_id": "2507.22317v1",
      "title": "AdapSCA-PSO: An Adaptive Localization Algorithm with AI-Based Hybrid SCA-PSO for IoT WSNs",
      "title_zh": "AdapSCA-PSOï¼šä¸€ç§åŸºäºäººå·¥æ™ºèƒ½æ··åˆ SCA-PSO çš„ç‰©è”ç½‘æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œè‡ªé€‚åº”å®šä½ç®—æ³•",
      "authors": [
        "Ze Zhang",
        "Qian Dong",
        "Wenhan Wang"
      ],
      "abstract": "The accurate localization of sensor nodes is a fundamental requirement for the practical application of the Internet of Things (IoT). To enable robust localization across diverse environments, this paper proposes a hybrid meta-heuristic localization algorithm. Specifically, the algorithm integrates the Sine Cosine Algorithm (SCA), which is effective in global search, with Particle Swarm Optimization (PSO), which excels at local search. An adaptive switching module is introduced to dynamically select between the two algorithms. Furthermore, the initialization, fitness evaluation, and parameter settings of the algorithm have been specifically redesigned and optimized to address the characteristics of the node localization problem. Simulation results across varying numbers of sensor nodes demonstrate that, compared to standalone PSO and the unoptimized SCAPSO algorithm, the proposed method significantly reduces the number of required iterations and achieves an average localization error reduction of 84.97%.",
      "tldr_zh": "å‡†ç¡®çš„ä¼ æ„Ÿå™¨èŠ‚ç‚¹å®šä½æ˜¯ç‰©è”ç½‘(IoT)å®é™…åº”ç”¨çš„åŸºç¡€è¦æ±‚ï¼Œæœ¬ç ”ç©¶ä¸ºæ­¤æå‡ºäº†ä¸€ç§åä¸ºAdapSCA-PSOçš„è‡ªé€‚åº”æ··åˆå…ƒå¯å‘å¼å®šä½ç®—æ³•ã€‚è¯¥ç®—æ³•å°†æ“…é•¿å…¨å±€æœç´¢çš„æ­£ä½™å¼¦ç®—æ³•(SCA)ä¸æ“…é•¿å±€éƒ¨æœç´¢çš„ç²’å­ç¾¤ä¼˜åŒ–(PSO)ç®—æ³•ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡å¼•å…¥è‡ªé€‚åº”åˆ‡æ¢æ¨¡å—æ¥åŠ¨æ€é€‰æ‹©æœ€ä¼˜æ‰§è¡Œç­–ç•¥ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œ(WSNs)å®šä½é—®é¢˜çš„ç‰¹æ€§ï¼Œå¯¹ç®—æ³•çš„åˆå§‹åŒ–ã€é€‚åº”åº¦è¯„ä¼°åŠå‚æ•°è®¾ç½®è¿›è¡Œäº†ä¸“é—¨çš„é‡æ–°è®¾è®¡ä¸ä¼˜åŒ–ã€‚ä»¿çœŸç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸åŒä¼ æ„Ÿå™¨èŠ‚ç‚¹è§„æ¨¡ä¸‹ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç‹¬ç«‹çš„PSOåŠæœªä¼˜åŒ–çš„SCAPSOç®—æ³•æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ã€‚å®éªŒç»“æœæœ€ç»ˆå®ç°äº†å¹³å‡å®šä½è¯¯å·®84.97%çš„é™å¹…ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨æå‡ç‰©è”ç½‘å®šä½ç²¾åº¦ä¸ç¨³å¥æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22317v1",
      "published_date": "2025-07-30 01:18:54 UTC",
      "updated_date": "2025-07-30 01:18:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T07:47:17.396380+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 111,
  "processed_papers_count": 111,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T07:48:12.237013+00:00"
}
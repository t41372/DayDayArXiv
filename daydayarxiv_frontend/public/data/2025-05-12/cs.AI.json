{
  "date": "2025-05-12",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-12 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ çš„ä¸“å±å¯¼è¯»å‘˜ã€‚ä»Šå¤© arXiv çš„æ›´æ–°å¯è°“æ˜¯â€œ**æ¨ç†èƒ½åŠ›å¤§çˆ†å‘**â€çš„ä¸€å¤©ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„é‡å¤´æˆé›†ä¸­åœ¨**LLM çš„è‡ªæˆ‘è¿›åŒ–ä¸æ¨ç†èƒ½åŠ›**ä¸Šï¼ˆSelf-Reward, Reasoning Modelsï¼‰ï¼Œä»ä¸ä¾èµ– Ground Truth çš„å¼ºåŒ–å­¦ä¹ ï¼Œåˆ° Xiaomi å‘å¸ƒçš„ MiMo æ¨ç†æ¨¡å‹ï¼Œå†åˆ° 1.58-bit æä½æ¯”ç‰¹é‡åŒ–çš„æ–°çªç ´ï¼ŒAgent é¢†åŸŸçš„â€œInternet of Agentsâ€å®å¤§å™äº‹ä¹Ÿè¿æ¥äº†ç»¼è¿°ã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©æœ€å€¼å¾—å…³æ³¨çš„ç¡¬æ ¸å·¥ä½œã€‚\n\n---\n\n### ğŸš€ æ ¸å¿ƒå…³æ³¨ï¼šLLM çš„è‡ªæˆ‘è¿›åŒ–ä¸æ¨ç† (Reasoning & Self-Improvement)\n\nè¿™ä¸€æ¿å—æ˜¯ä»Šå¤©æœ€å·çš„é¢†åŸŸï¼Œå¤§å®¶éƒ½åœ¨è¯•å›¾å¤åˆ»ç”šè‡³è¶…è¶Š o1 çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸ä»…é™äºæ•°å­¦é¢˜ã€‚\n\n**1. RLSR: Reinforcement Learning from Self Reward**\n**RLSRï¼šæºè‡ªè‡ªæˆ‘å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ **\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æ‘†è„±å¯¹ Ground Truth çš„ä¾èµ–ï¼Œè®©æ¨¡å‹è‡ªå·±å½“è£åˆ¤è‡ªå·±å­¦ã€‚\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** è¿™æ˜¯ä¸€ä¸ªè¿ˆå‘â€œè‡ªä¸»æ™ºèƒ½â€çš„é‡è¦ä¿¡å·ã€‚\n> **è¯¦è§£ï¼š** è®­ç»ƒ LLM è§£å†³å¤æ‚é—®é¢˜é€šå¸¸éœ€è¦æ˜‚è´µçš„â€œå¯éªŒè¯å¥–åŠ±â€ï¼ˆVerifiable Rewardsï¼‰ã€‚æœ¬æ–‡æå‡º LLM å¯ä»¥é€šè¿‡**è‡ªæˆ‘è¯„åˆ¤ï¼ˆSelf-Judgingï¼‰**è¿›è¡Œè‡ªæˆ‘æå‡ï¼Œä¸”æ— éœ€å‚è€ƒç­”æ¡ˆã€‚ä½œè€…åˆ©ç”¨äº†ç”Ÿæˆæ–¹æ¡ˆå’ŒéªŒè¯æ–¹æ¡ˆä¹‹é—´çš„ä¸å¯¹ç§°æ€§ï¼ˆéªŒè¯æ¯”ç”Ÿæˆå®¹æ˜“ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒQwen 2.5 7B é€šè¿‡è¿™ç§ Self-Reward è®­ç»ƒåï¼Œç”šè‡³è¾¾åˆ°äº† MIT Integration Beeï¼ˆéº»çœç†å·¥å­¦é™¢å¾®ç§¯åˆ†èœœèœ‚ç«èµ›ï¼‰çš„èµ„æ ¼æ°´å¹³ã€‚è¿™è¯æ˜äº† LLM å¯ä»¥åœ¨ç¼ºä¹è®­ç»ƒæ•°æ®çš„é¢†åŸŸé€šè¿‡â€œå·¦å³äº’æâ€å®ç°æŒç»­è¿›æ­¥ã€‚\n\n**61. MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining**\n**MiMoï¼šè§£é”è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›â€”â€”ä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒ**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** å°ç±³å›¢é˜Ÿå‡ºå“ï¼Œå¯¹æ ‡ OpenAI o1-miniï¼Œå…¨æµç¨‹ä¼˜åŒ–æ¨ç†ã€‚\n> **è¯¦è§£ï¼š** è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ¨ç†ä»»åŠ¡ç”Ÿçš„æ¨¡å‹ MiMo-7Bã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé‡‡ç”¨äº†å¤š token é¢„æµ‹ï¼ˆMulti-Token Predictionï¼‰ç›®æ ‡ï¼›åœ¨åè®­ç»ƒé˜¶æ®µï¼Œæ„å»ºäº† 13 ä¸‡æ¡å¯éªŒè¯çš„æ•°å­¦å’Œä»£ç æ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¹¶å¼•å…¥äº†â€œä»£ç å¥–åŠ±â€ï¼ˆCode-Rewardï¼‰æœºåˆ¶ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMiMo-7B-RL åœ¨æ•°å­¦ã€ä»£ç å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå£°ç§°è¶…è¶Šäº† OpenAI o1-miniã€‚\n\n**47. S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models**\n**S-GRPOï¼šæ¨ç†æ¨¡å‹ä¸­åŸºäºå¼ºåŒ–å­¦ä¹ çš„æå‰é€€å‡ºæœºåˆ¶**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æ²»æ„ˆæ¨ç†æ¨¡å‹çš„â€œè¿‡åº¦æ€è€ƒâ€ç—‡ï¼Œä¸ä»…å‡†ï¼Œè€Œä¸”å¿«ã€‚\n> **è¯¦è§£ï¼š** ç°åœ¨çš„ CoTï¼ˆæ€ç»´é“¾ï¼‰è™½ç„¶å¥½ï¼Œä½†å¾€å¾€ä¼šæœ‰å¾ˆå¤šå†—ä½™æ­¥éª¤ï¼ˆOverthinkingï¼‰ã€‚æœ¬æ–‡æå‡ºäº† S-GRPOï¼ˆSerial-Group Decaying-Reward Policy Optimizationï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ RL èŒƒå¼ã€‚å®ƒä¸åƒ GRPO é‚£æ ·å¹¶è¡Œé‡‡æ ·å¤šæ¡è·¯å¾„ï¼Œè€Œæ˜¯é‡‡æ ·ä¸€æ¡è·¯å¾„å¹¶åœ¨ä¸åŒä½ç½®å°è¯•â€œæå‰é€€å‡ºâ€ã€‚å¦‚æœèƒ½æå‰ç­”å¯¹ï¼Œå¥–åŠ±æ›´é«˜ã€‚åœ¨ GSM8K ç­‰æµ‹è¯•é›†ä¸Šï¼Œå®ƒè®©åºåˆ—é•¿åº¦å‡å°‘äº† 35%-60%ï¼Œå‡†ç¡®ç‡è¿˜ç•¥æœ‰æå‡ã€‚\n\n**34. Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving**\n**Agent RL ç¼©æ”¾å®šå¾‹ï¼šå…·æœ‰è‡ªå‘ä»£ç æ‰§è¡Œèƒ½åŠ›çš„ Agent å¼ºåŒ–å­¦ä¹ **\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** Agent é¢†åŸŸçš„ Scaling Lawï¼Œè¯æ˜äº†è®¡ç®—é‡ä¸å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„å¼ºç›¸å…³æ€§ã€‚\n> **è¯¦è§£ï¼š** ç ”ç©¶äº†åŸºäºç»“æœå¥–åŠ±çš„ RL å¦‚ä½•è®© LLM å­¦ä¼šä½¿ç”¨å·¥å…·ï¼ˆPython ä»£ç ï¼‰ã€‚ä½œè€…å‘ç°äº†ä¸€ä¸ª **Scaling Law**ï¼šéšç€ RL è®­ç»ƒæ­¥æ•°çš„å¢åŠ ï¼Œæ¨¡å‹è‡ªå‘æ‰§è¡Œä»£ç çš„é¢‘ç‡ã€å“åº”é•¿åº¦å’Œæœ€ç»ˆå‡†ç¡®ç‡å‘ˆç°å¯é¢„æµ‹çš„æ­£ç›¸å…³ã€‚è¿™ä¸º Agent è®­ç»ƒæä¾›äº†é‡åŒ–åŸºå‡†ã€‚\n\n---\n\n### âš¡ æ¨¡å‹æ¶æ„ä¸é«˜æ•ˆè®¡ç®— (Architecture & Efficiency)\n\n**13. An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits**\n**å¾®è°ƒè‡³ 1.58 æ¯”ç‰¹ï¼šä½ åªéœ€è¦ä¸€ä¸ªé¢å¤–çš„ RMSNorm**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æä½æ¯”ç‰¹é‡åŒ–ï¼ˆTernary LLMsï¼‰å˜å¾—æ›´ç¨³å®šã€æ›´ç®€å•äº†ã€‚\n> **è¯¦è§£ï¼š** 1.58-bit æ¨¡å‹ï¼ˆä¸‰è¿›åˆ¶æ¨¡å‹ï¼‰æ˜¯è¿‘æœŸçš„å¤§çƒ­ç‚¹ï¼Œä½†è®­ç»ƒæå…¶ä¸ç¨³å®šã€‚ä½œè€…å‘ç°ï¼Œåªéœ€åœ¨æ¯ä¸€ä¸ªçº¿æ€§æŠ•å½±å‰æ’å…¥ä¸€ä¸ª **RMS Normalization**ï¼Œå¹¶é…åˆåˆ†å±‚çš„é‡åŒ–è¿›åº¦è¡¨ï¼Œå°±èƒ½ç¨³å®šåœ°å°†å…¨ç²¾åº¦æ¨¡å‹å¾®è°ƒä¸ºä¸‰è¿›åˆ¶æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼Œæ€§èƒ½å´èƒ½åŒ¹é…ç”šè‡³è¶…è¶Šå¤æ‚çš„çŸ¥è¯†è’¸é¦ç®¡çº¿ã€‚è¿™è®©åœ¨ç«¯ä¾§è®¾å¤‡è·‘å¤§æ¨¡å‹å˜å¾—æ›´åŠ ç°å®ã€‚\n\n**121. UMoE: Unifying Attention and FFN with Shared Experts**\n**UMoEï¼šåˆ©ç”¨å…±äº«ä¸“å®¶ç»Ÿä¸€ Attention å’Œ FFN**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** é‡æ–°æ€è€ƒ Transformer æ¶æ„ï¼Œè®© Attention å±‚ä¹Ÿå˜æˆ MoEã€‚\n> **è¯¦è§£ï¼š** ç°æœ‰çš„ MoEï¼ˆæ··åˆä¸“å®¶æ¨¡å‹ï¼‰å¤§å¤šç”¨åœ¨ FFN å±‚ã€‚è¿™ç¯‡ NeurIPS Spotlight è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ¶æ„ï¼Œæ­ç¤ºäº† Attention æœºåˆ¶å†…éƒ¨å…¶å®éšå«ç€ç±»ä¼¼ FFN çš„ç»“æ„ã€‚UMoE é€šè¿‡å…±äº«ä¸“å®¶å‚æ•°ï¼Œç»Ÿä¸€äº† Attention å’Œ FFN çš„ MoE è®¾è®¡ï¼Œæ—¨åœ¨æå‡æ€§èƒ½çš„åŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚\n\n**32. Overflow Prevention Enhances Long-Context Recurrent LLMs**\n**æº¢å‡ºé¢„é˜²å¢å¼ºäº†é•¿ä¸Šä¸‹æ–‡å¾ªç¯ LLM**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æ‹¯æ•‘ Mamba ç­‰çº¿æ€§/å¾ªç¯æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä¸Šçš„â€œå¥å¿˜â€é—®é¢˜ã€‚\n> **è¯¦è§£ï¼š** å°½ç®¡ Mamba ç­‰ RNN ç±»æ¨¡å‹å·ç§°æ”¯æŒé•¿æ–‡æœ¬ï¼Œä½†ç”±äºå›ºå®šå¤§å°çš„å¾ªç¯è®°å¿†ï¼ˆRecurrent Memoryï¼‰ï¼Œå®é™…ä¸Šç»å¸¸â€œæ¶ˆåŒ–ä¸è‰¯â€ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æå…¶ç®€å•çš„ **Chunk-based** æ¨ç†ç­–ç•¥ï¼Œåªå¤„ç†æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œé˜²æ­¢è®°å¿†æº¢å‡ºã€‚è¿™ä¸€æ‹›è®© RecurrentGemma ç­‰æ¨¡å‹åœ¨ LongBench ä¸Šçš„è¡¨ç°æš´æ¶¨ 50%ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Agents & Multi-Agent)\n\n**137. Internet of Agents: Fundamentals, Applications, and Challenges**\n**æ™ºèƒ½ä½“äº’è”ç½‘ï¼šåŸºç¡€ã€åº”ç”¨ä¸æŒ‘æˆ˜**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æå‡ºäº† IoA (Internet of Agents) çš„å®å¤§æ¡†æ¶ã€‚\n> **è¯¦è§£ï¼š** è¿™æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§è®ºæ–‡ã€‚ä½œè€…è®¤ä¸º Agent æ­£ä»å­¤ç«‹ç³»ç»Ÿæ¼”å˜ä¸ºäº’è”ç”Ÿæ€ã€‚IoA æ—¨åœ¨å»ºç«‹ä¸€ä¸ªè®©å¼‚æ„ Agent èƒ½å¤Ÿæ— ç¼äº’è”ã€åŠ¨æ€å‘ç°å’Œåä½œçš„åŸºç¡€è®¾æ–½ã€‚å¦‚æœä½ å…³æ³¨ Agent çš„æœªæ¥äº’è”åè®®å’Œåä½œæ¨¡å¼ï¼Œè¿™ç¯‡æ–‡ç« æ˜¯å¿…è¯»çš„ã€‚\n\n**128. UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning**\n**UAV-CodeAgentsï¼šåŸºäºå¤šæ™ºèƒ½ä½“ ReAct å’Œè§†è§‰è¯­è¨€æ¨ç†çš„å¯æ‰©å±•æ— äººæœºä»»åŠ¡è§„åˆ’**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** è®©æ— äººæœºç¾¤è‡ªå·±çœ‹å›¾ã€å•†é‡ã€å†™ä»£ç æ¥æ‰§è¡Œä»»åŠ¡ã€‚\n> **è¯¦è§£ï¼š** åˆ©ç”¨ VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰è§£è¯»å«æ˜Ÿå›¾åƒï¼Œé€šè¿‡ ReAct èŒƒå¼è®©å¤šä¸ª Agent åä½œç”Ÿæˆæ— äººæœºé£è¡Œè½¨è¿¹å’Œä»»åŠ¡ä»£ç ã€‚æ— éœ€äººç±»å¾®æ“ï¼ŒAgent ä¹‹é—´æœ‰â€œååº”å¼æ€ç»´å¾ªç¯â€ï¼ˆReactive Thinking Loopï¼‰æ¥çº é”™ã€‚\n\n---\n\n### ğŸ› ï¸ RAG ä¸ è¯„ä¼°åŸºå‡† (RAG & Benchmarks)\n\n**129. DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking**\n**DynamicRAGï¼šåˆ©ç”¨ LLM è¾“å‡ºä½œä¸ºåé¦ˆè¿›è¡ŒåŠ¨æ€é‡æ’åº**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** æŠŠ Reranker åšæˆä¸€ä¸ªå¼ºåŒ–å­¦ä¹  Agentã€‚\n> **è¯¦è§£ï¼š** ä¼ ç»Ÿçš„ RAG æ£€ç´¢å›æ¥çš„æ–‡æ¡£æ•°é‡ $k$ æ˜¯å›ºå®šçš„ã€‚DynamicRAG è®© Reranker æ ¹æ® Query å’Œ LLM çš„åé¦ˆåŠ¨æ€è°ƒæ•´æ£€ç´¢æ–‡æ¡£çš„é¡ºåºå’Œæ•°é‡ã€‚å®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼ŒæŠŠ LLM çš„ç”Ÿæˆè´¨é‡ä½œä¸º Rewardï¼Œå®ç°äº†â€œæŒ‰éœ€æ£€ç´¢â€ã€‚\n\n**126. LongCodeBench: Evaluating Coding LLMs at 1M Context Windows**\n**LongCodeBenchï¼šåœ¨ 1M ä¸Šä¸‹æ–‡çª—å£ä¸‹è¯„ä¼°ä»£ç  LLM**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** é•¿æ–‡æœ¬ä»£ç èƒ½åŠ›çš„â€œç…§å¦–é•œâ€ã€‚\n> **è¯¦è§£ï¼š** ç°æœ‰çš„ä»£ç æµ‹è¯•å¤ªçŸ­äº†ã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹ **100ä¸‡ token** çº§åˆ«çš„ä»£ç ç†è§£å’Œä¿®å¤åŸºå‡†ã€‚æµ‹è¯•ç»“æœå¾ˆæ®‹é…·ï¼šå³ä½¿æ˜¯ GPT-4o æˆ– Claude 3.5 Sonnetï¼Œåœ¨è¶…é•¿ä¸Šä¸‹æ–‡çš„ä»£ç ä»»åŠ¡ä¸­è¡¨ç°ä¹Ÿä¼šå¤§å¹…ä¸‹é™ï¼ˆClaude ä» 29% è·Œåˆ° 3%ï¼‰ï¼Œè¯´æ˜ç°åœ¨çš„é•¿æ–‡æœ¬æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»£ç ä¾èµ–æ—¶è¿˜å¾ˆè„†å¼±ã€‚\n\n**6. How good are humans at detecting AI-generated images?**\n**äººç±»æ£€æµ‹ AI ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›æœ‰å¤šå¼ºï¼Ÿ**\n> **æ ¸å¿ƒäº®ç‚¹ï¼š** åˆ«å¤ªè‡ªä¿¡ï¼Œäººç±»ä¹Ÿå°±æ¯”çè’™å¼ºä¸€ç‚¹ç‚¹ã€‚\n> **è¯¦è§£ï¼š** åŸºäº 1.25 ä¸‡äººå‚ä¸çš„â€œReal or Not Quizâ€æ¸¸æˆæ•°æ®ï¼Œå‘ç°äººç±»åŒºåˆ†çœŸå›¾å’Œ AI å›¾çš„æ€»ä½“å‡†ç¡®ç‡åªæœ‰ **62%**ï¼ˆç•¥é«˜äº 50% çš„éšæœºæ¦‚ç‡ï¼‰ã€‚äººç±»æœ€æ“…é•¿çœ‹äººåƒï¼Œæœ€ä¸æ“…é•¿çœ‹é£æ™¯ã€‚è¿™å†æ¬¡å¼ºè°ƒäº†æ°´å°å’Œè‡ªåŠ¨åŒ–æ£€æµ‹å·¥å…·çš„å¿…è¦æ€§ã€‚\n\n---\n\n### ğŸ”® å…¶ä»–æœ‰è¶£çš„æ¢ç´¢ (Robotics & Science)\n\n*   **2. SLAG: Scalable Language-Augmented Gaussian Splatting**: ç»“åˆäº†è¯­è¨€ç‰¹å¾çš„ 3D é«˜æ–¯æ³¼æº…ï¼ˆGaussian Splattingï¼‰ï¼Œé€Ÿåº¦æ¯” OpenGaussian å¿« 18 å€ï¼Œé€‚åˆæœºå™¨äººå¿«é€Ÿç†è§£åœºæ™¯ã€‚\n*   **112. HuB: Learning Extreme Humanoid Balance**: è®©æœºå™¨äººå­¦ä¼šæå°é¾™å¼çš„è¸¢è…¿ï¼æå‡ºäº† HuB æ¡†æ¶ï¼Œè®©åŒè¶³æœºå™¨äººåœ¨å—åˆ°çŒ›çƒˆæ’å‡»æ—¶ä¹Ÿèƒ½ä¿æŒå¹³è¡¡ã€‚\n*   **55. Chronocept: Instilling a Sense of Time in Machines**: æˆ‘ä»¬å¸¸è¯´çŸ¥è¯†æœ‰æ—¶æ•ˆæ€§ï¼Œä½† LLM æ‡‚å—ï¼Ÿæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…³äºâ€œæ—¶é—´æœ‰æ•ˆæ€§â€çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°ç›®å‰çš„æ¨¡å‹åœ¨åˆ¤æ–­äº‹å®ä½•æ—¶è¿‡æ—¶æ–¹é¢è¿˜å¾ˆå¼±ã€‚\n*   **73. GRADA**: ç ”ç©¶äº†é’ˆå¯¹ RAG çš„å¯¹æŠ—æ€§æ”»å‡»â€”â€”å¦‚æœåœ¨æ£€ç´¢æ–‡æ¡£é‡Œæ··å…¥æ¶æ„çš„ã€è¯­ä¹‰ç›¸ä¼¼ä½†å†…å®¹é”™è¯¯çš„æ–‡æ¡£ï¼ŒLLM å¾ˆå®¹æ˜“ä¸­æ‹›ã€‚\n\n---\n\n**ç»“è¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡ä¸ä»…è¦åœ¨â€œå¤§è„‘â€ä¸Šåšæ–‡ç« ï¼ˆå„ç§ Reasoning å’Œ Self-Improvementï¼‰ï¼Œè¿˜åœ¨â€œèº«ä½“â€ä¸Šï¼ˆæœºå™¨äººã€1.58bit é«˜æ•ˆéƒ¨ç½²ï¼‰å’Œâ€œç¤¾ä¼šæ€§â€ä¸Šï¼ˆAgent äº’è”ï¼‰å¤§æ­¥è¿ˆè¿›ã€‚å¦‚æœä½ åªèƒ½è¯»ä¸€ç¯‡ï¼Œæ¨è **#1 RLSR**ï¼Œå®ƒå¯èƒ½é¢„ç¤ºç€æ•°æ®æ¯ç«­æ—¶ä»£çš„è§£æ³•ã€‚\n\nç¥é˜…è¯»æ„‰å¿«ï¼Œç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2505.08827v2",
      "title": "RLSR: Reinforcement Learning from Self Reward",
      "title_zh": "RLSRï¼šåŸºäºè‡ªå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Toby Simonds",
        "Kevin Lopez",
        "Akira Yoshiyama",
        "Dominique Garmier"
      ],
      "abstract": "Large language models can generate solutions to complex problems, but training them with reinforcement learning typically requires verifiable rewards that are expensive to create and not possible for all domains. We demonstrate that LLMs can effectively self-improve through self-judging without reference solutions, leveraging the inherent asymmetry between generating and verifying solutions. Our experiments show that models can provide reliable reward signals without ground truth answers, enabling reinforcement learning in domains where verifiable rewards are impractical. By implementing self-judging across Countdown puzzles and integration problems, we achieve performance comparable to formal verification without ground truth solutions. Most notably, Qwen 2.5 7B DeepSeek Distilled trained with self-rewards qualifies for the prestigious MIT Integration Bee competition, performance through self-supervised improvement. When combined with synthetic question generation, we establish a complete self-improvement loop where models generate practice problems, solve them, and evaluate their own performance without any external validation. Our findings demonstrate that LLM judges can provide effective reward signals for training, unlocking reinforcement learning in countless domains previously limited by reward engineering challenges. This work represents a significant step toward autonomous AI systems that continuously improve through self-directed learning rather than human-guided training, potentially accelerating progress across domains where training data is scarce or evaluation is complex.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RLSR (Reinforcement Learning from Self Reward)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è‡ªæˆ‘è¯„ä»·(Self-judging)å®ç°å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªæˆ‘æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚é¢†åŸŸä¸­éªŒè¯å¥–åŠ±(Verifiable Rewards)è·å–æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è·å–çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆè§£ç­”ä¸éªŒè¯è§£ç­”ä¹‹é—´çš„å†…åœ¨ä¸å¯¹ç§°æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹æä¾›å¯é çš„å¥–åŠ±ä¿¡å·ã€‚åœ¨Countdownè°œé¢˜å’Œç§¯åˆ†é—®é¢˜çš„å®éªŒä¸­ï¼ŒRLSRè¾¾åˆ°äº†ä¸å½¢å¼åŒ–éªŒè¯(Formal Verification)ç›¸å½“çš„æ€§èƒ½ã€‚ç‰¹åˆ«åœ°ï¼Œé€šè¿‡è‡ªæˆ‘å¥–åŠ±è®­ç»ƒçš„Qwen 2.5 7B DeepSeek Distilledæ¨¡å‹æˆåŠŸå…¥å›´MIT Integration Beeç«èµ›ï¼Œè¯æ˜äº†è‡ªç›‘ç£æ”¹è¿›çš„å¼ºå¤§æ½œåŠ›ã€‚ç»“åˆåˆæˆé—®é¢˜ç”Ÿæˆ(Synthetic Question Generation)ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„é—­ç¯è‡ªæˆ‘æ”¹è¿›ç³»ç»Ÿï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆã€è§£å†³å¹¶è¯„ä¼°é—®é¢˜ã€‚è¿™ä¸€æˆæœæ˜¾è‘—é™ä½äº†å¯¹äººå·¥æ ‡æ³¨å¥–åŠ±çš„ä¾èµ–ï¼Œä¸ºåœ¨ç¼ºä¹è®­ç»ƒæ•°æ®æˆ–è¯„ä¼°å¤æ‚çš„é¢†åŸŸå¼€å‘æŒç»­è‡ªæˆ‘è¿›åŒ–çš„è‡ªä¸»AIç³»ç»Ÿæä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08827v2",
      "published_date": "2025-05-12 23:51:04 UTC",
      "updated_date": "2025-08-06 23:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:19.824620+00:00"
    },
    {
      "arxiv_id": "2505.08124v2",
      "title": "SLAG: Scalable Language-Augmented Gaussian Splatting",
      "title_zh": "SLAGï¼šå¯æ‰©å±•çš„è¯­è¨€å¢å¼ºé«˜æ–¯æ³¼æº…",
      "authors": [
        "Laszlo Szilagyi",
        "Francis Engelmann",
        "Jeannette Bohg"
      ],
      "abstract": "Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SLAGï¼Œä¸€ç§é¢å‘å¤§è§„æ¨¡æœºå™¨äººåº”ç”¨çš„å¤šGPUè¯­è¨€å¢å¼ºGaussian Splattingæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡åœºæ™¯ç¼–ç ä¸­çš„é€Ÿåº¦ä¸å¯æ‰©å±•æ€§éš¾é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨SAMå’ŒCLIPå°†äºŒç»´è§†è§‰è¯­è¨€æ¨¡å‹ç‰¹å¾é›†æˆåˆ°ä¸‰ç»´åœºæ™¯ä¸­ï¼Œé€šè¿‡ä¸‰ç»´é«˜æ–¯åœºæ™¯å‚æ•°çš„å½’ä¸€åŒ–åŠ æƒå¹³å‡å€¼ç›´æ¥æ¨å¯¼åµŒå…¥ï¼Œä»è€Œæ¶ˆé™¤äº†ä¼ ç»Ÿæ–¹æ³•ä¸­è®¡ç®—æ¯ä¸ªé«˜æ–¯è¯­è¨€åµŒå…¥æ‰€éœ€çš„æŸå¤±å‡½æ•°ï¼Œå®ç°äº†é«˜åº¦å¹¶è¡ŒåŒ–çš„åœºæ™¯ç¼–ç ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å‘é‡æ•°æ®åº“ä»¥å®ç°é«˜æ•ˆçš„åµŒå…¥å­˜å‚¨å’Œæ£€ç´¢ï¼Œä¼˜åŒ–äº†å—é™è®¡ç®—èµ„æºä¸‹çš„éƒ¨ç½²æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨16ä¸ªGPUçš„é…ç½®ä¸‹ï¼ŒSLAGçš„åµŒå…¥è®¡ç®—é€Ÿåº¦æ¯”OpenGaussianæé«˜äº†18å€ï¼Œå¹¶åœ¨ScanNetå’ŒLERFæ•°æ®é›†ä¸Šä¿æŒäº†æé«˜çš„åµŒå…¥è´¨é‡ã€‚è¯¥æ¡†æ¶ä¸ºæœæ•‘ã€æ™ºæ…§åŸå¸‚å’ŒçŸ¿ä¸šç­‰å¯¹æ—¶é—´æ•æ„Ÿä¸”æ•°æ®å¯†é›†å‹çš„æœºå™¨äººåº”ç”¨æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08124v2",
      "published_date": "2025-05-12 23:32:24 UTC",
      "updated_date": "2025-08-17 18:16:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:16.792895+00:00"
    },
    {
      "arxiv_id": "2505.08123v1",
      "title": "JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections",
      "title_zh": "JSoverï¼šåŸºäºå•èƒ½CTæŠ•å½±çš„è”åˆèƒ½è°±ä¼°è®¡ä¸å¤šææ–™åˆ†è§£",
      "authors": [
        "Qing Wu",
        "Hongjiang Wei",
        "Jingyi Yu",
        "S. Kevin Zhou",
        "Yuyao Zhang"
      ],
      "abstract": "Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†JSoverï¼Œä¸€ç§ä»å•èƒ½CT(SECT)æŠ•å½±æ•°æ®ä¸­è”åˆé‡å»ºå¤šææ–™ç»„æˆå¹¶ä¼°ç®—èƒ½é‡è°±çš„ä¸€æ­¥æ³•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸSEMMDä¸¤æ­¥èµ°æ–¹æ¡ˆä¸­å› å¿½ç•¥èƒ½é‡ç›¸å…³è¡°å‡è€Œäº§ç”Ÿçš„éçº¿æ€§å…‰æŸç¡¬åŒ–(beam hardening)ä¼ªå½±å’Œå™ªå£°é—®é¢˜ã€‚é€šè¿‡å°†ç‰©ç†ä¿¡æ¯çš„èƒ½è°±å…ˆéªŒ(physics-informed spectral priors)å¼•å…¥åˆ†è§£è¿‡ç¨‹ï¼ŒJSoverèƒ½å¤Ÿä»å•èƒ½é‡‡é›†æ•°æ®ä¸­å‡†ç¡®æ¨¡æ‹Ÿè™šæ‹Ÿèƒ½è°±CTç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†å¤šææ–™åˆ†è§£(MMD)çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥éšå¼ç¥ç»è¡¨ç¤º(INR)ä½œä¸ºæ— ç›‘ç£æ·±åº¦å­¦ä¹ æ±‚è§£å™¨ï¼Œåˆ©ç”¨å…¶å¯¹è¿ç»­å›¾åƒæ¨¡å¼çš„å½’çº³åç½®(inductive bias)æ¥çº¦æŸè§£ç©ºé—´ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†ææ–™å›¾çš„ç”Ÿæˆè´¨é‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®CTæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒJSoveråœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›SEMMDæ–¹æ³•ï¼Œä¸ºä¸´åºŠå®šé‡é‡å»ºæä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.08123v1",
      "published_date": "2025-05-12 23:32:21 UTC",
      "updated_date": "2025-05-12 23:32:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:15.677786+00:00"
    },
    {
      "arxiv_id": "2505.22673v1",
      "title": "Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion",
      "title_zh": "èå…¥ç”Ÿç†ä¿¡æ¯çš„ç”Ÿæˆå¼å¤šä»»åŠ¡ç½‘ç»œï¼šç”¨äºæ— é€ å½±å‰‚CTçŒæ³¨æˆåƒ",
      "authors": [
        "Wasif Khan",
        "Kyle B. See",
        "Simon Kato",
        "Ziqian Huang",
        "Amy Lazarte",
        "Kyle Douglas",
        "Xiangyang Lou",
        "Teng J. Peng",
        "Dhanashree Rajderkar",
        "John Rees",
        "Pina Sanelli",
        "Amita Singh",
        "Ibrahim Tuna",
        "Christina A. Wilson",
        "Ruogu Fang"
      ],
      "abstract": "Perfusion imaging is extensively utilized to assess hemodynamic status and tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging plays a key role in the early assessment and planning of stroke treatment. While CTP provides essential perfusion parameters to identify abnormal blood flow in the brain, the use of contrast agents in CTP can lead to allergic reactions and adverse side effects, along with costing USD 4.9 billion worldwide in 2022. To address these challenges, we propose a novel deep learning framework called Multitask Automated Generation of Intermodal CT perfusion maps (MAGIC). This framework combines generative artificial intelligence and physiological information to map non-contrast computed tomography (CT) imaging to multiple contrast-free CTP imaging maps. We demonstrate enhanced image fidelity by incorporating physiological characteristics into the loss terms. Our network was trained and validated using CT image data from patients referred for stroke at UF Health and demonstrated robustness to abnormalities in brain perfusion activity. A double-blinded study was conducted involving seven experienced neuroradiologists and vascular neurologists. This study validated MAGIC's visual quality and diagnostic accuracy showing favorable performance compared to clinical perfusion imaging with intravenous contrast injection. Overall, MAGIC holds great promise in revolutionizing healthcare by offering contrast-free, cost-effective, and rapid perfusion imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè®¡ç®—æœºæ–­å±‚æ‰«æçŒæ³¨æˆåƒ(CTP)ä¸­å¯¹æ¯”å‰‚å¼•å‘çš„è¿‡æ•ååº”åŠé«˜æ˜‚æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMAGICçš„ç”Ÿç†ä¿¡æ¯é©±åŠ¨å¤šä»»åŠ¡ç”Ÿæˆç½‘ç»œã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)ä¸ç”Ÿç†ä¿¡æ¯(Physiological Information)ï¼Œæ—¨åœ¨å°†éå¯¹æ¯”å‰‚CTå›¾åƒç›´æ¥æ˜ å°„ä¸ºå¤šç§æ— å¯¹æ¯”å‰‚çš„CTPçŒæ³¨å‚æ•°å›¾ã€‚é€šè¿‡åœ¨æŸå¤±å‡½æ•°(Loss Terms)ä¸­å¼•å…¥ç”Ÿç†ç‰¹å¾ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æå‡äº†å›¾åƒä¿çœŸåº¦ï¼Œå¹¶å¯¹è„‘çŒæ³¨å¼‚å¸¸è¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ã€‚ä¸€é¡¹æ¶‰åŠä¸ƒä½èµ„æ·±ç¥ç»æ”¾å°„å­¦å®¶å’Œè¡€ç®¡ç¥ç»å­¦å®¶çš„åŒç›²ç ”ç©¶è¯å®ï¼ŒMAGICåœ¨è§†è§‰è´¨é‡å’Œè¯Šæ–­å‡†ç¡®æ€§ä¸Šä¸ä¸´åºŠé™è„‰æ³¨å°„å¯¹æ¯”å‰‚çš„æˆåƒæ•ˆæœç›¸å½“ã€‚è¿™é¡¹æŠ€æœ¯ä¸ºå®ç°æ— å¯¹æ¯”å‰‚ã€ä½æˆæœ¬ä¸”å¿«é€Ÿçš„çŒæ³¨æˆåƒæä¾›äº†å¯èƒ½ï¼Œå…·æœ‰æ”¹å˜è„‘å’ä¸­ä¸´åºŠè¯„ä¼°æµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.TO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.TO",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2505.22673v1",
      "published_date": "2025-05-12 22:58:55 UTC",
      "updated_date": "2025-05-12 22:58:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:41.968268+00:00"
    },
    {
      "arxiv_id": "2505.08106v1",
      "title": "Are LLMs complicated ethical dilemma analyzers?",
      "title_zh": "LLMs æ˜¯å¦æ˜¯å¤æ‚çš„ä¼¦ç†å›°å¢ƒåˆ†æå™¨ï¼Ÿ",
      "authors": [
        "Jiashen",
        "Du",
        "Jesse Yao",
        "Allen Liu",
        "Zhekai Zhang"
      ],
      "abstract": "One open question in the study of Large Language Models (LLMs) is whether they can emulate human ethical reasoning and act as believable proxies for human judgment. To investigate this, we introduce a benchmark dataset comprising 196 real-world ethical dilemmas and expert opinions, each segmented into five structured components: Introduction, Key Factors, Historical Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also collect non-expert human responses for comparison, limited to the Key Factors section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini, Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine similarity, and Universal Sentence Encoder similarity. Metric weights are computed through an inversion-based ranking alignment and pairwise AHP analysis, enabling fine-grained comparison of model outputs to expert responses. Our results show that LLMs generally outperform non-expert humans in lexical and structural alignment, with GPT-4o-mini performing most consistently across all sections. However, all models struggle with historical grounding and proposing nuanced resolution strategies, which require contextual abstraction. Human responses, while less structured, occasionally achieve comparable semantic similarity, suggesting intuitive moral reasoning. These findings highlight both the strengths and current limitations of LLMs in ethical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„ä¼¦ç†æ¨ç†å¹¶ä½œä¸ºäººç±»åˆ¤æ–­çš„å¯é ä»£ç†ã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«196ä¸ªçœŸå®ä¸–ç•Œä¼¦ç†å›°å¢ƒåŠä¸“å®¶æ„è§çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶å°†å†…å®¹ç»†åˆ†ä¸ºå¼•è¨€ã€å…³é”®å› ç´ ã€å†å²ç†è®ºè§†è§’ã€è§£å†³ç­–ç•¥å’Œæ ¸å¿ƒè¦ç‚¹äº”ä¸ªç»“æ„åŒ–éƒ¨åˆ†ã€‚å®éªŒé€šè¿‡BLEUã€Damerau-Levenshteinè·ç¦»ã€TF-IDFä½™å¼¦ç›¸ä¼¼åº¦å’ŒUniversal Sentence Encoderç›¸ä¼¼åº¦ç­‰å¤åˆæŒ‡æ ‡ï¼Œå¯¹GPT-4o-miniã€Claude-3.5-Sonnetã€Deepseek-V3å’ŒGemini-1.5-Flashç­‰å‰æ²¿æ¨¡å‹è¿›è¡Œäº†ç»†ç²’åº¦è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨è¯æ±‡å’Œç»“æ„å¯¹é½æ–¹é¢æ™®éä¼˜äºéä¸“å®¶äººç±»ï¼Œå…¶ä¸­GPT-4o-miniåœ¨æ‰€æœ‰éƒ¨åˆ†è¡¨ç°æœ€ä¸ºä¸€è‡´ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨æ¶‰åŠä¸Šä¸‹æ–‡æŠ½è±¡çš„å†å²èƒŒæ™¯è¿˜åŸ(historical grounding)å’Œæå‡ºç»†è‡´å…¥å¾®çš„è§£å†³ç­–ç•¥æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè™½ç„¶äººç±»å›ç­”çš„ç»“æ„æ€§è¾ƒå¼±ï¼Œä½†åœ¨è¯­ä¹‰ç›¸ä¼¼åº¦ä¸Šå¶å°”èƒ½è¾¾åˆ°ä¸æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼Œæ˜¾ç¤ºå‡ºç›´è§‰æ€§çš„é“å¾·æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨ä¼¦ç†å†³ç­–ä¸­çš„ä¼˜åŠ¿åŠå…¶åœ¨å¤æ‚èƒŒæ™¯ç†è§£ä¸Šçš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "CS194-280 Advanced LLM Agents project. Project page: https://github.com/ALT-JS/ethicaLLM",
      "pdf_url": "https://arxiv.org/pdf/2505.08106v1",
      "published_date": "2025-05-12 22:35:07 UTC",
      "updated_date": "2025-05-12 22:35:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:37.788376+00:00"
    },
    {
      "arxiv_id": "2507.18640v1",
      "title": "How good are humans at detecting AI-generated images? Learnings from an experiment",
      "title_zh": "äººç±»è¾¨åˆ« AI ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›å¦‚ä½•ï¼Ÿä¸€é¡¹å®éªŒå¸¦æ¥çš„å¯ç¤º",
      "authors": [
        "Thomas Roca",
        "Anthony Cintron Roman",
        "JehÃº Torres Vega",
        "Marcelo Duarte",
        "Pengce Wang",
        "Kevin White",
        "Amit Misra",
        "Juan Lavista Ferres"
      ],
      "abstract": "As AI-powered image generation improves, a key question is how well human beings can differentiate between \"real\" and AI-generated or modified images. Using data collected from the online game \"Real or Not Quiz.\", this study investigates how effectively people can distinguish AI-generated images from real ones. Participants viewed a randomized set of real and AI-generated images, aiming to identify their authenticity. Analysis of approximately 287,000 image evaluations by over 12,500 global participants revealed an overall success rate of only 62\\%, indicating a modest ability, slightly above chance. Participants were most accurate with human portraits but struggled significantly with natural and urban landscapes. These results highlight the inherent challenge humans face in distinguishing AI-generated visual content, particularly images without obvious artifacts or stylistic cues. This study stresses the need for transparency tools, such as watermarks and robust AI detection tools to mitigate the risks of misinformation arising from AI-generated content",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨åœ¨çº¿æ¸¸æˆ \"Real or Not Quiz.\" çš„æ•°æ®ï¼Œè°ƒæŸ¥äº†äººç±»åœ¨åŒºåˆ†çœŸå®å›¾åƒä¸ AI-generated æˆ–ç»è¿‡ä¿®æ”¹çš„å›¾åƒæ—¶çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ†æå…¨çƒ 12,500 å¤šåå‚ä¸è€…çš„çº¦ 287,000 æ¬¡å›¾åƒè¯„ä¼°ï¼Œå‘ç°æ•´ä½“è¯†åˆ«æˆåŠŸç‡ä»…ä¸º 62%ï¼Œè¿™è¡¨æ˜äººç±»è¯†åˆ« AI è§†è§‰å†…å®¹çš„èƒ½åŠ›ä»…ç•¥é«˜äºéšæœºæ°´å¹³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨å¤„ç†äººç±»è‚–åƒ (human portraits) æ—¶å‡†ç¡®ç‡æœ€é«˜ï¼Œä½†åœ¨è¾¨åˆ«è‡ªç„¶å’ŒåŸå¸‚æ™¯è§‚ (natural and urban landscapes) æ—¶è¡¨ç°æå·®ã€‚è¿™åæ˜ å‡ºäººç±»åœ¨é¢å¯¹ç¼ºä¹æ˜æ˜¾ç—•è¿¹ (artifacts) æˆ–é£æ ¼çº¿ç´¢ (stylistic cues) çš„ AI-generated å†…å®¹æ—¶å­˜åœ¨è¯†åˆ«å›°å¢ƒã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘æ°´å° (watermarks) å’Œé²æ£’çš„ AI detection tools ç­‰é€æ˜åº¦å·¥å…·çš„é‡è¦æ€§ï¼Œä»¥å‡è½» AI ç”Ÿæˆå†…å®¹å¸¦æ¥çš„è¯¯å¯¼ä¿¡æ¯ (misinformation) é£é™©ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18640v1",
      "published_date": "2025-05-12 21:55:13 UTC",
      "updated_date": "2025-05-12 21:55:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:34.587625+00:00"
    },
    {
      "arxiv_id": "2505.08088v2",
      "title": "Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories",
      "title_zh": "åŸºäºèŠ‚ç‚¹åµŒå…¥ä¸ WiFi è½¨è¿¹èšç±»çš„å›¾åŸºæ¥¼å±‚åˆ†éš”",
      "authors": [
        "Rabia Yasa Kostas",
        "Kahraman Kostas"
      ],
      "abstract": "Indoor positioning systems (IPSs) are increasingly vital for location-based services in complex multi-storey environments. This study proposes a novel graph-based approach for floor separation using Wi-Fi fingerprint trajectories, addressing the challenge of vertical localization in indoor settings. We construct a graph where nodes represent Wi-Fi fingerprints, and edges are weighted by signal similarity and contextual transitions. Node2Vec is employed to generate low-dimensional embeddings, which are subsequently clustered using K-means to identify distinct floors. Evaluated on the Huawei University Challenge 2021 dataset, our method outperforms traditional community detection algorithms, achieving an accuracy of 68.97\\%, an F1-score of 61.99\\%, and an Adjusted Rand Index of 57.19\\%. By publicly releasing the preprocessed dataset and implementation code, this work contributes to advancing research in indoor positioning. The proposed approach demonstrates robustness to signal noise and architectural complexities, offering a scalable solution for floor-level localization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå›¾ï¼ˆGraph-Basedï¼‰çš„åˆ›æ–°æ–¹æ³•ï¼Œåˆ©ç”¨Wi-Fi fingerprintè½¨è¿¹æ¥è§£å†³å¤šå±‚å®¤å†…ç¯å¢ƒä¸­çš„æ¥¼å±‚åˆ†éš”ï¼ˆFloor Separationï¼‰éš¾é¢˜ã€‚ç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªä»¥Wi-FiæŒ‡çº¹ä¸ºèŠ‚ç‚¹ã€ä»¥ä¿¡å·ç›¸ä¼¼æ€§å’Œä¸Šä¸‹æ–‡è½¬æ¢ä¸ºè¾¹æƒé‡çš„å›¾ç»“æ„ï¼Œéšåé‡‡ç”¨Node2Vecç®—æ³•ç”Ÿæˆä½ç»´åµŒå…¥ï¼ˆEmbeddingsï¼‰ã€‚é€šè¿‡å¯¹è¿™äº›åµŒå…¥è¿›è¡ŒK-meansèšç±»ï¼Œç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å‡ºä¸åŒçš„æ¥¼å±‚ã€‚åœ¨Huawei University Challenge 2021æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†68.97%çš„å‡†ç¡®ç‡å’Œ61.99%çš„F1-scoreï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç¤¾åŒºæ£€æµ‹ç®—æ³•ã€‚è¯¥æ–¹æ¡ˆè¡¨ç°å‡ºå¯¹ä¿¡å·å™ªå£°å’Œå¤æ‚å»ºç­‘ç»“æ„çš„å¼ºé²æ£’æ€§ï¼Œä¸ºå®¤å†…å®šä½ç³»ç»Ÿï¼ˆIPSï¼‰ä¸­çš„å‚ç›´å®šä½æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08088v2",
      "published_date": "2025-05-12 21:46:36 UTC",
      "updated_date": "2025-06-13 15:48:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:37.941954+00:00"
    },
    {
      "arxiv_id": "2505.08825v1",
      "title": "Multi-source Plume Tracing via Multi-Agent Reinforcement Learning",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å¤šæºçƒŸç¾½è¿½è¸ª",
      "authors": [
        "Pedro Antonio Alarcon Granadeno",
        "Theodore Chambers",
        "Jane Cleland-Huang"
      ],
      "abstract": "Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon gas leak (2015) demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment. Traditional methods, such as gradient-based or biologically inspired approaches, often fail in realistic, turbulent conditions. To address these challenges, we present a Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing multiple airborne pollution sources using a swarm of small uncrewed aerial systems (sUAS). Our method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical action-observation pairs, effectively approximating latent states. Unlike prior work, we use a general-purpose simulation environment based on the Gaussian Plume Model (GPM), incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources. The incorporation of action histories as part of the inputs further enhances the adaptability of our model in complex, partially observable environments. Extensive simulations show that our algorithm significantly outperforms conventional approaches. Specifically, our model allows agents to explore only 1.29\\% of the environment to successfully locate pollution sources.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šäº‹æ•…ä¸­æ±¡æŸ“ç¾½æµè¿½è¸ª(Plume Tracing)çš„ç´§è¿«éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Multi-Agent Reinforcement Learning)çš„ç®—æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨å°å‹æ— äººæœºç³»ç»Ÿ(sUAS)é›†ç¾¤å®šä½å¤šä¸ªç©ºä¸­æ±¡æŸ“æºã€‚ç ”ç©¶å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«åšå¼ˆ(POMG)ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§åŸºäºé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)çš„åŠ¨ä½œç‰¹å®šåŒæ·±åº¦å¾ªç¯Qç½‘ç»œ(ADDRQN)ï¼Œé€šè¿‡åˆ©ç”¨å®Œæ•´çš„å†å²åŠ¨ä½œ-è§‚æµ‹åºåˆ—æ¥æœ‰æ•ˆé€¼è¿‘æ½œåœ¨çŠ¶æ€ã€‚è¯¥æ–¹æ³•åœ¨åŸºäºé«˜æ–¯ç¾½æµæ¨¡å‹(Gaussian Plume Model)çš„ä»¿çœŸç¯å¢ƒä¸­ç»¼åˆè€ƒè™‘äº†ä¸‰ç»´ç©ºé—´ã€ä¼ æ„Ÿå™¨å™ªå£°åŠå¤šæºäº¤äº’ç­‰ç°å®å› ç´ ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚å¤§é‡å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ™ºèƒ½ä½“ä»…éœ€æ¢ç´¢1.29%çš„ç¯å¢ƒåŒºåŸŸå³å¯æˆåŠŸå®šä½æ±¡æŸ“æºï¼Œä¸ºç¯å¢ƒç›‘æµ‹ä¸å…¬å…±å«ç”Ÿå®‰å…¨æä¾›äº†é«˜æ•ˆä¸”å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "13 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.08825v1",
      "published_date": "2025-05-12 21:33:15 UTC",
      "updated_date": "2025-05-12 21:33:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:38.094519+00:00"
    },
    {
      "arxiv_id": "2505.08082v2",
      "title": "FrÃ©chet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids",
      "title_zh": "FrÃ©chet ç”µåŠ›åœºæ™¯è·ç¦»ï¼šä¸€ç§è¯„ä¼°æ™ºèƒ½ç”µç½‘è·¨å¤šæ—¶é—´å°ºåº¦ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„åº¦é‡æŒ‡æ ‡",
      "authors": [
        "Yuting Cai",
        "Shaohuai Liu",
        "Chao Tian",
        "Le Xie"
      ],
      "abstract": "Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the FrÃ©chet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ç”µç½‘(Smart Grids)é¢†åŸŸç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)æ¨¡å‹äº§ç”Ÿçš„æ•°æ®è´¨é‡è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†FrÃ©chet Power-Scenario Distanceè¿™ä¸€æ–°å‹åº¦é‡æŒ‡æ ‡ã€‚ä¼ ç»ŸåŸºäºæ¬§å‡ é‡Œå¾—è·ç¦»(Euclidean distance)çš„æŒ‡æ ‡ä»…èƒ½åæ˜ å•æ ·æœ¬é—´çš„å…³ç³»ï¼Œéš¾ä»¥æœ‰æ•ˆè¯„ä¼°åˆæˆæ•°æ®é›†ç¾¤ä½“é—´çš„è´¨é‡å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨å­¦ä¹ ç‰¹å¾ç©ºé—´ä¸­ä¼°è®¡ä¸¤ä¸ªæ•°æ®é›†é—´çš„FrÃ©chet Distance (FD)ï¼Œä»åˆ†å¸ƒè§’åº¦(distributional perspective)è¯„ä¼°ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æŒ‡æ ‡åœ¨ä¸åŒæ—¶é—´å°ºåº¦(timescales)å’Œå¤šç§æ¨¡å‹ä¸‹å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚è¿™ä¸€ç ”ç©¶æˆæœæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ç”µç½‘è¿è¥ä¸­æ•°æ®é©±åŠ¨å†³ç­–çš„å¯é æ€§ï¼Œä¸ºè¯„ä¼°åˆæˆç”µåŠ›æ•°æ®çš„è´¨é‡æä¾›äº†æ›´æœ‰æ•ˆçš„æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08082v2",
      "published_date": "2025-05-12 21:32:23 UTC",
      "updated_date": "2025-10-23 20:54:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:00.184520+00:00"
    },
    {
      "arxiv_id": "2505.08080v2",
      "title": "Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders",
      "title_zh": "è¶…è¶Šè¾“å…¥æ¿€æ´»ï¼šåˆ©ç”¨æ¢¯åº¦ç¨€ç–è‡ªç¼–ç å™¨è¯†åˆ«å…·å½±å“åŠ›çš„æ½œåœ¨ç‰¹å¾",
      "authors": [
        "Dong Shu",
        "Xuansheng Wu",
        "Haiyan Zhao",
        "Mengnan Du",
        "Ninghao Liu"
      ],
      "abstract": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for interpreting and steering the internal representations of large language models (LLMs). However, conventional approaches to analyzing SAEs typically rely solely on input-side activations, without considering the causal influence between each latent feature and the model's output. This work is built on two key hypotheses: (1) activated latents do not contribute equally to the construction of the model's output, and (2) only latents with high causal influence are effective for model steering. To validate these hypotheses, we propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method that identifies the most influential latents by incorporating output-side gradient information.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs) åœ¨è§£é‡Šå’Œå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ (LLMs) å†…éƒ¨è¡¨ç¤ºæ—¶ä»…ä¾èµ–è¾“å…¥ä¾§æ¿€æ´»ã€å¿½ç•¥æ½œç‰¹å¾ä¸æ¨¡å‹è¾“å‡ºé—´å› æœå½±å“çš„é—®é¢˜ï¼Œæå‡ºäº† Gradient Sparse Autoencoder (GradSAE) æ–¹æ³•ã€‚ä½œè€…åŸºäºä¸¤ä¸ªæ ¸å¿ƒå‡è®¾ï¼Œå³æ¿€æ´»çš„æ½œå˜é‡å¯¹æ¨¡å‹è¾“å‡ºçš„è´¡çŒ®å¹¶ä¸å‡ç­‰ï¼Œä¸”ä»…å…·æœ‰é«˜å› æœå½±å“åŠ›çš„æ½œå˜é‡åœ¨æ¨¡å‹å¼•å¯¼ (Model Steering) ä¸­æ‰æœ‰æ•ˆã€‚GradSAE é€šè¿‡å¼•å…¥è¾“å‡ºä¾§çš„æ¢¯åº¦ä¿¡æ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å‡ºå¯¹æ¨¡å‹è¾“å‡ºæœ€å…·å½±å“åŠ›çš„æ½œå˜é‡ã€‚å®éªŒéªŒè¯äº†è¿™äº›å‡è®¾ï¼Œè¯æ˜äº†ç»“åˆæ¢¯åº¦ä¿¡æ¯èƒ½æ›´ç²¾å‡†åœ°å®šä½å…³é”®æ½œç‰¹å¾ã€‚è¯¥ç ”ç©¶ä¸ºå¤§æ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†ææä¾›äº†æ›´å…·å› æœå¯¼å‘çš„å·¥å…·ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ¨¡å‹å†…éƒ¨æœºåˆ¶ç†è§£ä¸ç²¾å‡†æ§åˆ¶çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "EMNLP 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2505.08080v2",
      "published_date": "2025-05-12 21:29:12 UTC",
      "updated_date": "2025-09-23 16:43:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:07.490787+00:00"
    },
    {
      "arxiv_id": "2505.08078v1",
      "title": "What Matters for Batch Online Reinforcement Learning in Robotics?",
      "title_zh": "æœºå™¨äººå­¦ä¸­æ‰¹é‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„å…³é”®è¦ç´ æ¢ç©¶",
      "authors": [
        "Perry Dong",
        "Suvir Mirchandani",
        "Dorsa Sadigh",
        "Chelsea Finn"
      ],
      "abstract": "The ability to learn from large batches of autonomously collected data for policy improvement -- a paradigm we refer to as batch online reinforcement learning -- holds the promise of enabling truly scalable robot learning by significantly reducing the need for human effort of data collection while getting benefits from self-improvement. Yet, despite the promise of this paradigm, it remains challenging to achieve due to algorithms not being able to learn effectively from the autonomous data. For example, prior works have applied imitation learning and filtered imitation learning methods to the batch online RL problem, but these algorithms often fail to efficiently improve from the autonomously collected data or converge quickly to a suboptimal point. This raises the question of what matters for effective batch online RL in robotics. Motivated by this question, we perform a systematic empirical study of three axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy expressivity -- and analyze how these axes affect performance and scaling with the amount of autonomous data. Through our analysis, we make several observations. First, we observe that the use of Q-functions to guide batch online RL significantly improves performance over imitation-based methods. Building on this, we show that an implicit method of policy extraction -- via choosing the best action in the distribution of the policy -- is necessary over traditional policy extraction methods from offline RL. Next, we show that an expressive policy class is preferred over less expressive policy classes. Based on this analysis, we propose a general recipe for effective batch online RL. We then show a simple addition to the recipe of using temporally-correlated noise to obtain more diversity results in further performance gains. Our recipe obtains significantly better performance and scaling compared to prior methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨äººé¢†åŸŸä¸­çš„æ‰¹æ¬¡åœ¨çº¿å¼ºåŒ–å­¦ä¹  (Batch Online Reinforcement Learning) èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡è‡ªä¸»æ”¶é›†çš„å¤§è§„æ¨¡æ•°æ®å®ç°ç­–ç•¥çš„è‡ªæˆ‘æ”¹è¿›å¹¶å‡å°‘äººå·¥æˆæœ¬ã€‚é’ˆå¯¹æ¨¡ä»¿å­¦ä¹  (Imitation Learning) ç­‰ç°æœ‰ç®—æ³•åœ¨å¤„ç†è‡ªä¸»æ•°æ®æ—¶æ•ˆç‡ä½ä¸‹æˆ–æ˜“æ”¶æ•›è‡³æ¬¡ä¼˜è§£çš„é—®é¢˜ï¼Œæœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†ç®—æ³•ç±»åˆ«ã€ç­–ç•¥æå–æ–¹æ³•å’Œç­–ç•¥è¡¨è¾¾èƒ½åŠ› (Policy Expressivity) ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ Q-functions å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½æ˜¾è‘—ä¼˜äºæ¨¡ä»¿å­¦ä¹ ï¼Œä¸”åœ¨ç­–ç•¥åˆ†å¸ƒä¸­é€‰æ‹©æœ€ä½³åŠ¨ä½œçš„éšå¼ç­–ç•¥æå– (Implicit Policy Extraction) æ–¹å¼æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé«˜è¡¨è¾¾èƒ½åŠ›çš„ç­–ç•¥ç±»åˆ« (Expressive Policy Class) å¯¹æå‡æ€§èƒ½è‡³å…³é‡è¦ã€‚åŸºäºä¸Šè¿°åˆ†æï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„å­¦ä¹ æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡å¼•å…¥æ—¶é—´ç›¸å…³å™ªå£° (Temporally-correlated Noise) è¿›ä¸€æ­¥å¢å¼ºäº†æ•°æ®å¤šæ ·æ€§ã€‚æœ€ç»ˆå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ€§èƒ½å’Œæ‰©å±•æ€§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæ„å»ºå¯æ‰©å±•çš„æœºå™¨äººå­¦ä¹ ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08078v1",
      "published_date": "2025-05-12 21:24:22 UTC",
      "updated_date": "2025-05-12 21:24:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:55.404044+00:00"
    },
    {
      "arxiv_id": "2505.08073v2",
      "title": "Explainable Reinforcement Learning Agents Using World Models",
      "title_zh": "åŸºäºä¸–ç•Œæ¨¡å‹çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“",
      "authors": [
        "Madhuri Singh",
        "Amal Alabdulkarim",
        "Gennie Mansi",
        "Mark O. Riedl"
      ],
      "abstract": "Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§£é‡Šå¼ºåŒ–å­¦ä¹  (Explainable Reinforcement Learning, XRL) åœ¨å¤„ç†åºåˆ—å†³ç­–æ—¶é¢ä¸´çš„å¤æ‚æ€§ï¼Œä»¥åŠéä¸“å®¶ç”¨æˆ·éš¾ä»¥ç†è§£æˆ–è°ƒæ•´æ™ºèƒ½ä½“ç­–ç•¥çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸–ç•Œæ¨¡å‹ (World Models) ä¸ºåŸºäºæ¨¡å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹  (Model-Based Deep RL) æ™ºèƒ½ä½“ç”Ÿæˆè§£é‡Šçš„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡ä¸–ç•Œæ¨¡å‹é¢„æµ‹åŠ¨ä½œå¯¼è‡´çš„ç¯å¢ƒå˜åŒ–æ¥ç”Ÿæˆåäº‹å®è½¨è¿¹ (counterfactual trajectories)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†é€†å‘ä¸–ç•Œæ¨¡å‹ (Reverse World Model)ã€‚é€†å‘ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ç¯å¢ƒåº”å½“å¤„äºä½•ç§çŠ¶æ€ï¼Œæ‰èƒ½ä½¿æ™ºèƒ½ä½“æ›´å€¾å‘äºæ‰§è¡Œç”¨æˆ·æŒ‡å®šçš„åäº‹å®åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‘ç”¨æˆ·å±•ç¤ºç¯å¢ƒåº”æœ‰çš„çŠ¶æ€èƒ½æ˜¾è‘—æå‡å…¶å¯¹æ™ºèƒ½ä½“ç­–ç•¥ (policy) çš„ç†è§£ã€‚ç ”ç©¶è¿˜å‡è®¾è¿™ç±»è§£é‡Šèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æŒæ¡é€šè¿‡æ“çºµç¯å¢ƒæ¥æ§åˆ¶æ™ºèƒ½ä½“æ‰§è¡Œçš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Workshop on Explainable Artificial Intelligence (XAI) at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.08073v2",
      "published_date": "2025-05-12 21:18:31 UTC",
      "updated_date": "2025-08-18 01:05:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:56.233886+00:00"
    },
    {
      "arxiv_id": "2505.08823v1",
      "title": "An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits",
      "title_zh": "ä»…éœ€é¢å¤–å¼•å…¥ä¸€ä¸ª RMSNorm å³å¯å®ç° 1.58 ä½é‡åŒ–å¾®è°ƒ",
      "authors": [
        "Cody Steinmetz",
        "Gavin Childress",
        "Aaron Herbst",
        "Gavin Jones",
        "Jasdeep Singh",
        "Eli Vang",
        "Keagan Weinstock"
      ],
      "abstract": "Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸‰å€¼(ternary)1.58-bité‡åŒ–å¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…å‘ç°ï¼Œåªéœ€åœ¨æ¯ä¸ªçº¿æ€§æŠ•å½±(linear projection)å‰æ’å…¥ä¸€ä¸ªé¢å¤–çš„RMS normalizationå±‚ï¼Œå¹¶é…åˆæ¸è¿›å¼çš„å±‚çº§é‡åŒ–è°ƒåº¦ï¼Œå³å¯å°†å…¨ç²¾åº¦é¢„è®­ç»ƒæ¨¡å‹ç¨³å®šåœ°è½¬æ¢ä¸ºä¸‰å€¼æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸å¢åŠ æ¨¡å‹å¤æ‚åº¦çš„å‰æä¸‹ï¼Œåœ¨æ ‡å‡†è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ç”šè‡³è¶…è¿‡äº†å¤æ‚çš„çŸ¥è¯†è’¸é¦(knowledge-distillation)æµæ°´çº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡ç²¾ç»†çš„å½’ä¸€åŒ–å¤„ç†å°±èƒ½æ˜¾è‘—ç¼©å°ä¸‰å€¼æ¨¡å‹ä¸å…¨ç²¾åº¦æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹å®ç°è¶…ä½æ¯”ç‰¹æ¨ç†çš„å¯è¡Œæ€§ï¼Œä¸ºLLMsçš„ä½æˆæœ¬å®é™…éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08823v1",
      "published_date": "2025-05-12 21:14:29 UTC",
      "updated_date": "2025-05-12 21:14:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:03.532957+00:00"
    },
    {
      "arxiv_id": "2505.08064v1",
      "title": "Justified Evidence Collection for Argument-based AI Fairness Assurance",
      "title_zh": "é¢å‘åŸºäºè®ºè¯çš„ AI å…¬å¹³æ€§ä¿éšœçš„æ­£å½“åŒ–è¯æ®æ”¶é›†",
      "authors": [
        "Alpay Sabuncuoglu",
        "Christopher Burr",
        "Carsten Maple"
      ],
      "abstract": "It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI å…¬å¹³æ€§è¿™ä¸€å¤æ‚çš„ sociotechnical challengeï¼Œæå‡ºäº†ä¸€ç§åŸºäºç³»ç»Ÿå·¥ç¨‹é©±åŠ¨çš„æ¡†æ¶åŠå…¶é…å¥—è½¯ä»¶å·¥å…·ï¼Œæ—¨åœ¨å®ç°åŠ¨æ€çš„ argument-based assuranceã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µå°†å…¬å¹³æ€§ä¿éšœæµç¨‹åŒ–ï¼šåœ¨éœ€æ±‚è§„åˆ’é˜¶æ®µï¼Œç”±å¤šå­¦ç§‘å’Œå¤šåˆ©ç›Šç›¸å…³è€…å›¢é˜Ÿé€šè¿‡ fairness governance process æ˜ç¡®éœ€è¦å»ºç«‹çš„ç›®æ ‡ä¸ä¸»å¼ ï¼›åœ¨è¿è¡Œé˜¶æ®µï¼Œé€šè¿‡æŒç»­ç›‘æµ‹æ¥å£ä»è‡ªåŠ¨åŒ–æµ‹è¯•æŒ‡æ ‡ã€æ¨¡å‹ã€æ•°æ®åŠç”¨ä¾‹åˆ†æç­‰ç°æœ‰äº§å‡ºä¸­åŠ¨æ€æ”¶é›†è¯æ®ï¼Œä¸ºè®ºè¯æä¾›æ”¯æ’‘ã€‚è¿™ç§æ–¹æ³•å°†åŸæœ¬ç”¨äºè¯„ä¼°å®‰å…¨é£é™©çš„ argument-based assurance cases æ‰©å±•åˆ°å…¬å¹³æ€§ç­‰è§„èŒƒæ€§ç›®æ ‡é¢†åŸŸã€‚ç ”ç©¶æœ€åé€šè¿‡é‡‘èé¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨æ”¯æŒå…¬å¹³æ€§ç›¸å…³è®ºè¯åŠè‡ªåŠ¨åŒ–è¯æ®æ”¶é›†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸º AI ç³»ç»Ÿçš„æŒç»­ç›‘ç®¡æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "The paper is accepted for ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT '25)",
      "pdf_url": "https://arxiv.org/pdf/2505.08064v1",
      "published_date": "2025-05-12 21:05:33 UTC",
      "updated_date": "2025-05-12 21:05:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:19.051537+00:00"
    },
    {
      "arxiv_id": "2505.08054v2",
      "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning",
      "title_zh": "FalseRejectï¼šé€šè¿‡ç»“æ„åŒ–æ¨ç†æå‡è¯­å¢ƒå®‰å…¨å¹¶ç¼“è§£å¤§è¯­è¨€æ¨¡å‹è¿‡åº¦æ‹’ç»çš„èµ„æº",
      "authors": [
        "Zhehao Zhang",
        "Weijie Xu",
        "Fanyou Wu",
        "Chandan K. Reddy"
      ],
      "abstract": "Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å®‰å…¨å¯¹é½è¿‡ç¨‹ä¸­å®¹æ˜“å¯¹æ— å®³æŸ¥è¯¢äº§ç”Ÿè¿‡åº¦æ‹’ç» (Over-refusal) ä»è€Œé™ä½å…¶å®ç”¨æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º FalseReject çš„ç»¼åˆèµ„æºã€‚FalseReject åŒ…å« 1.6 ä¸‡ä¸ªæ¶µç›– 44 ä¸ªå®‰å…¨ç›¸å…³ç±»åˆ«çš„çœ‹ä¼¼æœ‰å®³å®åˆ™è‰¯æ€§çš„æŸ¥è¯¢ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢æä¾›äº†ç»“æ„åŒ–çš„å“åº”ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºå›¾ä¿¡æ¯çš„å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ (Graph-informed adversarial multi-agent interaction framework)ï¼Œç”¨äºç”Ÿæˆå¤šæ ·ä¸”å¤æ‚çš„æç¤ºè¯ã€‚è¯¥èµ„æºé€šè¿‡æ˜¾å¼æ¨ç† (Explicit reasoning) ç»“æ„åŒ–å“åº”ï¼Œå¸®åŠ©æ¨¡å‹å‡†ç¡®åŒºåˆ†å®‰å…¨ä¸ä¸å®‰å…¨çš„ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼ŒFalseReject è¿˜åŒ…æ‹¬é’ˆå¯¹æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å’Œæ¨ç†å¯¼å‘æ¨¡å‹å®šåˆ¶çš„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„åŸºå‡†æµ‹è¯•é›†ã€‚å¯¹ 29 ä¸ªæœ€å…ˆè¿› (SOTA) çš„å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œè¿‡åº¦æ‹’ç»é—®é¢˜ä¾ç„¶æ™®éå­˜åœ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ FalseReject è¿›è¡Œç›‘ç£å¾®è°ƒ (Supervised finetuning) å¯ä»¥æ˜¾è‘—å‡å°‘ä¸å¿…è¦çš„æ‹’ç»ï¼Œä¸”ä¸ä¼šæŸå®³æ¨¡å‹çš„æ•´ä½“å®‰å…¨æ€§æˆ–é€šç”¨è¯­è¨€èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.08054v2",
      "published_date": "2025-05-12 20:45:25 UTC",
      "updated_date": "2025-07-15 10:03:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:08.409671+00:00"
    },
    {
      "arxiv_id": "2505.08052v2",
      "title": "NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition",
      "title_zh": "NAZMï¼šæ³¢æ–¯è¯—æ­Œä¼ ç»Ÿä¸­åŒºåŸŸæŒ‡æ ‡çš„ç½‘ç»œåˆ†æ",
      "authors": [
        "Kourosh Shahnazari",
        "Seyed Moein Ayyoubzadeh",
        "Mohammadamin Fazli",
        "Mohammadali Keshtparvar"
      ],
      "abstract": "This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NAZMï¼Œä¸€ä¸ªé€šè¿‡æ„å»ºå¤šç»´ç›¸ä¼¼æ€§ç½‘ç»œæ¥æ¨¡æ‹Ÿæ³¢æ–¯å¤å…¸è¯—äººå½±å“åŠ›åŠ¨æ€çš„è®¡ç®—æ¨¡å‹ã€‚åˆ©ç”¨åŸºäºGanjoorè¯­æ–™åº“çš„æ•°æ®é›†ï¼Œç ”ç©¶æ•´åˆäº†è¯­ä¹‰(semantic)ã€è¯æ±‡(lexical)ã€æ–‡ä½“(stylistic)ã€ä¸»é¢˜(thematic)å’ŒéŸµå¾‹(metrical)ç‰¹å¾æ¥æ„å»ºåŠ æƒç›¸ä¼¼æ€§çŸ©é˜µï¼Œå¹¶ç”Ÿæˆåæ˜ è¯—äººé—´å½±å“åŠ›çš„èšåˆå›¾ã€‚é€šè¿‡è®¡ç®—åº¦ä¸­å¿ƒæ€§(degree)ã€ä¸­ä»‹ä¸­å¿ƒæ€§(betweenness)ç­‰å¤šç§Centrality measuresï¼Œç ”ç©¶è¯†åˆ«å‡ºäº†æ ¸å¿ƒè¯—äººã€é£æ ¼æ¢çº½å’Œæ¡¥æ¥äººç‰©ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨Louvainç¤¾åŒºæ£€æµ‹ç®—æ³•åˆ’åˆ†å‡ºçš„è¯—äººé›†ç¾¤ä¸Sabk-e Hindiå’ŒSabk-e Khorasaniç­‰ä¼ ç»Ÿæ–‡å­¦æµæ´¾é«˜åº¦å»åˆã€‚ç ”ç©¶å‘ç°æ­ç¤ºäº†æ–‡å­¦ç»å…¸åœ°ä½ä¸äº’æ–‡å½±å“åŠ›(intertextual influence)ä¹‹é—´çš„å·®å¼‚ï¼Œçªæ˜¾äº†å…·æœ‰é‡è¦ç»“æ„æ„ä¹‰çš„éä¸»æµè¯—äººã€‚è¯¥ç ”ç©¶ä¸ºè¯—æ­Œä¼ ç»Ÿæä¾›äº†ä¸€ä¸ªå¯è§£é‡Šä¸”å¯æ‰©å±•çš„æ¨¡å‹ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†è®¡ç®—è¯­è¨€å­¦ä¸æ•°å­—äººæ–‡(Digital Humanities)çš„äº¤å‰ç ”ç©¶ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08052v2",
      "published_date": "2025-05-12 20:39:53 UTC",
      "updated_date": "2025-05-29 20:44:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:26.291027+00:00"
    },
    {
      "arxiv_id": "2505.08049v1",
      "title": "Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making",
      "title_zh": "åå·®è¿˜æ˜¯æœ€ä¼˜æ€§ï¼Ÿäººç±»å†³ç­–ä¸­è´å¶æ–¯æ¨ç†ä¸å­¦ä¹ åå·®çš„è¾¨æ",
      "authors": [
        "Prakhar Godara"
      ],
      "abstract": "Recent studies claim that human behavior in a two-armed Bernoulli bandit (TABB) task is described by positivity and confirmation biases, implying that humans do not integrate new information objectively. However, we find that even if the agent updates its belief via objective Bayesian inference, fitting the standard Q-learning model with asymmetric learning rates still recovers both biases. Bayesian inference cast as an effective Q-learning algorithm has symmetric, though decreasing, learning rates. We explain this by analyzing the stochastic dynamics of these learning systems using master equations. We find that both confirmation bias and unbiased but decreasing learning rates yield the same behavioral signatures. Finally, we propose experimental protocols to disentangle true cognitive biases from artifacts of decreasing learning rates.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»åœ¨åŒè‡‚ä¼¯åŠªåˆ©å¼ºç›—ä»»åŠ¡(two-armed Bernoulli bandit, TABB)ä¸­çš„å†³ç­–è¡Œä¸ºï¼Œæ—¨åœ¨åŒºåˆ†è´å¶æ–¯æ¨ç†(Bayesian inference)ä¸æ‰€è°“çš„æ­£å‘åå·®(positivity bias)åŠç¡®è®¤åå·®(confirmation bias)ã€‚ä»¥å¾€ç ”ç©¶è®¤ä¸ºäººç±»åœ¨ä»»åŠ¡ä¸­æ— æ³•å®¢è§‚æ•´åˆä¿¡æ¯ï¼Œä½†æœ¬ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯ä½¿ç”¨å®¢è§‚è´å¶æ–¯æ¨ç†æ›´æ–°ä¿¡å¿µçš„æ™ºèƒ½ä½“ï¼Œåœ¨é€‚é…å…·æœ‰éå¯¹ç§°å­¦ä¹ ç‡çš„æ ‡å‡†Q-learningæ¨¡å‹æ—¶ï¼Œä¹Ÿä¼šå‘ˆç°å‡ºä¸Šè¿°åå·®ç‰¹å¾ã€‚é€šè¿‡ä½¿ç”¨ä¸»æ–¹ç¨‹(master equations)åˆ†æå­¦ä¹ ç³»ç»Ÿçš„éšæœºåŠ¨åŠ›å­¦ï¼Œç ”ç©¶æ­ç¤ºäº†è´å¶æ–¯æ¨ç†æœ¬è´¨ä¸Šå…·æœ‰å¯¹ç§°ä½†éšæ—¶é—´é€’å‡çš„å­¦ä¹ ç‡ï¼Œè€Œè¿™ç§é€’å‡ç‰¹æ€§åœ¨è¡Œä¸ºè¡¨ç°ä¸Šä¸ç¡®è®¤åå·®é«˜åº¦ä¸€è‡´ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¸å¤šè¢«å½’å› ä¸ºè®¤çŸ¥åå·®çš„è¡Œä¸ºå®é™…ä¸Šå¯èƒ½æ˜¯å­¦ä¹ ç‡é€’å‡äº§ç”Ÿçš„ä¼ªå½±ï¼Œè€ŒéçœŸæ­£çš„ä¿¡æ¯å¤„ç†å¤±çœŸã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€å¥—å®éªŒåè®®ï¼Œæ—¨åœ¨å°†çœŸå®çš„è®¤çŸ¥åå·®ä»å­¦ä¹ ç‡é€’å‡æ•ˆåº”ä¸­æœ‰æ•ˆè§£ç¦»å‡ºæ¥ï¼Œä¸ºäººç±»å†³ç­–æœºåˆ¶çš„å»ºæ¨¡æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08049v1",
      "published_date": "2025-05-12 20:36:43 UTC",
      "updated_date": "2025-05-12 20:36:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:52.983985+00:00"
    },
    {
      "arxiv_id": "2505.08821v1",
      "title": "A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction",
      "title_zh": "åŸºäº Transformer çš„å¤šæ­¥è¡€ç³–é¢„æµ‹æ¨¡å‹æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Meryem Altin Karagoz",
        "Marc D. Breton",
        "Anas El Fathi"
      ],
      "abstract": "Accurate blood glucose prediction can enable novel interventions for type 1 diabetes treatment, including personalized insulin and dietary adjustments. Although recent advances in transformer-based architectures have demonstrated the power of attention mechanisms in complex multivariate time series prediction, their potential for blood glucose (BG) prediction remains underexplored. We present a comparative analysis of transformer models for multi-horizon BG prediction, examining forecasts up to 4 hours and input history up to 1 week. The publicly available DCLP3 dataset (n=112) was split (80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset (n=12) served as an external test set. We trained networks with point-wise, patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal data. For short-term blood glucose prediction, Crossformer, a patch-wise transformer architecture, achieved a superior 30-minute prediction of RMSE (15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h), PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6 mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used tokenization through patches demonstrated improved accuracy with larger input sizes, with the best results obtained with a one-week history. These findings highlight the promise of transformer-based architectures for BG prediction by capturing and leveraging seasonal patterns in multivariate time-series data to improve accuracy.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹1å‹ç³–å°¿ç—…æ²»ç–—ï¼Œå¯¹åŸºäºTransformeræ¶æ„çš„æ¨¡å‹åœ¨å¤šæ—¶åŸŸ(multi-horizon)è¡€ç³–é¢„æµ‹(blood glucose prediction)ä¸­çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿçš„æ¯”è¾ƒåˆ†æã€‚é€šè¿‡åœ¨DCLP3æ•°æ®é›†è®­ç»ƒå¹¶åœ¨OhioT1DMæ•°æ®é›†ä¸Šè¿›è¡Œå¤–éƒ¨æµ‹è¯•ï¼Œç ”ç©¶åˆ©ç”¨è¿ç»­è¡€ç³–ç›‘æµ‹(CGM)ã€èƒ°å²›ç´ å’Œé¥®é£Ÿæ•°æ®è¯„ä¼°äº†ç‚¹å¯¹ç‚¹(point-wise)ã€è¡¥ä¸(patch-wise)ã€åºåˆ—(series-wise)åŠæ··åˆåµŒå…¥ç­‰æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨è¡¥ä¸æ¶æ„çš„Crossformeråœ¨30åˆ†é’ŸçŸ­æœŸé¢„æµ‹ä¸­è¾¾åˆ°äº†æœ€ä¼˜çš„å‡æ–¹æ ¹è¯¯å·®(RMSE)ï¼Œè€ŒPatchTSTåœ¨1è‡³4å°æ—¶çš„é•¿æœŸé¢„æµ‹ä¸­è¡¨ç°æœ€ä½³ã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºè¡¥ä¸(patches)è¿›è¡Œåˆ†è¯çš„æ¨¡å‹åœ¨å¤„ç†é•¿è¾¾ä¸€å‘¨çš„å†å²è¾“å…¥æ—¶å‡†ç¡®ç‡æœ€é«˜ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰å¤šå…ƒæ—¶é—´åºåˆ—ä¸­çš„å­£èŠ‚æ€§æ¨¡å¼ã€‚è¿™äº›å‘ç°è¯æ˜äº†Transformeræ¶æ„åœ¨æå‡è¡€ç³–é¢„æµ‹ç²¾åº¦æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå®ç°ç²¾å‡†çš„ä¸ªæ€§åŒ–åŒ»ç–—å¹²é¢„æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "q-bio.QM",
      "comment": "7 pages, 2 figures, 1 table, 1st IFAC Workshop on Engineering Diabetes Technologies (EDT 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.08821v1",
      "published_date": "2025-05-12 20:22:44 UTC",
      "updated_date": "2025-05-12 20:22:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:46.095913+00:00"
    },
    {
      "arxiv_id": "2505.08032v2",
      "title": "Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience",
      "title_zh": "é¢å‘ 6G ç½‘ç»œçš„åœ¨çº¿å­¦ä¹ è‡ªé€‚åº”æ³¢æŸåˆ‡æ¢ï¼šæå‡æ•ˆç‡ä¸éŸ§æ€§",
      "authors": [
        "Seyed Bagher Hashemi Natanzi",
        "Zhicong Zhu",
        "Bo Tang"
      ],
      "abstract": "Adaptive beam switching is essential for mission-critical military and commercial 6G networks but faces major challenges from high carrier frequencies, user mobility, and frequent blockages. While existing machine learning (ML) solutions often focus on maximizing instantaneous throughput, this can lead to unstable policies with high signaling overhead. This paper presents an online Deep Reinforcement Learning (DRL) framework designed to learn an operationally stable policy. By equipping the DRL agent with an enhanced state representation that includes blockage history, and a stability-centric reward function, we enable it to prioritize long-term link quality over transient gains. Validated in a challenging 100-user scenario using the Sionna library, our agent achieves throughput comparable to a reactive Multi-Armed Bandit (MAB) baseline. Specifically, our proposed framework improves link stability by approximately 43% compared to a vanilla DRL approach, achieving operational reliability competitive with MAB while maintaining high data rates. This work demonstrates that by reframing the optimization goal towards operational stability, DRL can deliver efficient, reliable, and real-time beam management solutions for next-generation mission-critical networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 6G ä»»åŠ¡å…³é”®å‹ç½‘ç»œåœ¨è‡ªé€‚åº”æ³¢æŸåˆ‡æ¢ (Adaptive Beam Switching) ä¸­é¢ä¸´çš„é«˜é¢‘æ®µå¹²æ‰°ã€ç”¨æˆ·ç§»åŠ¨å’Œé¢‘ç¹é®æŒ¡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æå‡æ“ä½œç¨³å®šæ€§çš„åœ¨çº¿æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŒ…å«é®æŒ¡å†å²ä¿¡æ¯çš„å¢å¼ºçŠ¶æ€è¡¨ç¤ºï¼Œå¹¶ç»“åˆä»¥ç¨³å®šæ€§ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä¼˜å…ˆä¿éšœé•¿æœŸé“¾è·¯è´¨é‡è€Œéä»…è¿½æ±‚ç¬æ—¶ååé‡ã€‚åœ¨åŸºäº Sionna åº“æ„å»ºçš„ 100 ç”¨æˆ·å¤æ‚åœºæ™¯éªŒè¯ä¸­ï¼Œè¯¥ DRL æ™ºèƒ½ä½“å®ç°äº†ä¸å¤šè‡‚è€è™æœº (Multi-Armed Bandit, MAB) åŸºå‡†ç›¸å½“çš„ååé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ DRL æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å°†é“¾è·¯ç¨³å®šæ€§æå‡äº†çº¦ 43%ï¼Œåœ¨ç»´æŒé«˜æ•°æ®é€Ÿç‡çš„åŒæ—¶è¾¾åˆ°äº†æé«˜çš„æ“ä½œå¯é æ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡é‡æ–°å®šä¹‰ä¼˜åŒ–ç›®æ ‡ï¼Œåœ¨çº¿ DRL èƒ½å¤Ÿä¸ºä¸‹ä¸€ä»£ä»»åŠ¡å…³é”®å‹ç½‘ç»œæä¾›é«˜æ•ˆä¸”å®æ—¶çš„æ³¢æŸç®¡ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08032v2",
      "published_date": "2025-05-12 19:59:05 UTC",
      "updated_date": "2025-12-03 03:45:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:22.040179+00:00"
    },
    {
      "arxiv_id": "2505.08025v1",
      "title": "PRISM: Complete Online Decentralized Multi-Agent Pathfinding with Rapid Information Sharing using Motion Constraints",
      "title_zh": "PRISMï¼šåŸºäºè¿åŠ¨çº¦æŸå®ç°å¿«é€Ÿä¿¡æ¯å…±äº«çš„å®Œå¤‡åœ¨çº¿å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’",
      "authors": [
        "Hannah Lee",
        "Zachary Serlin",
        "James Motes",
        "Brendan Long",
        "Marco Morales",
        "Nancy M. Amato"
      ],
      "abstract": "We introduce PRISM (Pathfinding with Rapid Information Sharing using Motion Constraints), a decentralized algorithm designed to address the multi-task multi-agent pathfinding (MT-MAPF) problem. PRISM enables large teams of agents to concurrently plan safe and efficient paths for multiple tasks while avoiding collisions. It employs a rapid communication strategy that uses information packets to exchange motion constraint information, enhancing cooperative pathfinding and situational awareness, even in scenarios without direct communication. We prove that PRISM resolves and avoids all deadlock scenarios when possible, a critical challenge in decentralized pathfinding. Empirically, we evaluate PRISM across five environments and 25 random scenarios, benchmarking it against the centralized Conflict-Based Search (CBS) and the decentralized Token Passing with Task Swaps (TPTS) algorithms. PRISM demonstrates scalability and solution quality, supporting 3.4 times more agents than CBS and handling up to 2.5 times more tasks in narrow passage environments than TPTS. Additionally, PRISM matches CBS in solution quality while achieving faster computation times, even under low-connectivity conditions. Its decentralized design reduces the computational burden on individual agents, making it scalable for large environments. These results confirm PRISM's robustness, scalability, and effectiveness in complex and dynamic pathfinding scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PRISMï¼Œä¸€ç§é’ˆå¯¹å¤šä»»åŠ¡å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ (MT-MAPF) é—®é¢˜è®¾è®¡çš„å»ä¸­å¿ƒåŒ–ç®—æ³•ã€‚è¯¥ç®—æ³•é‡‡ç”¨å¿«é€Ÿä¿¡æ¯å…±äº«ç­–ç•¥ï¼Œé€šè¿‡äº¤æ¢è¿åŠ¨çº¦æŸ (Motion Constraints) ä¿¡æ¯åŒ…æ¥å¢å¼ºåä½œè·¯å¾„è§„åˆ’å’Œæ€åŠ¿æ„ŸçŸ¥ï¼Œå³ä½¿åœ¨ç¼ºä¹ç›´æ¥é€šä¿¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é«˜æ•ˆé¿éšœã€‚PRISM åœ¨ç†è®ºä¸Šè¢«è¯æ˜èƒ½å¤Ÿè§£å†³å¹¶é¿å…æ‰€æœ‰å¯èƒ½çš„æ­»é” (Deadlock) åœºæ™¯ï¼Œæ”»å…‹äº†å»ä¸­å¿ƒåŒ–è§„åˆ’ä¸­çš„æ ¸å¿ƒéš¾é¢˜ã€‚é€šè¿‡åœ¨å¤šç§ç¯å¢ƒä¸‹çš„å®éªŒè¯„ä¼°ï¼ŒPRISM å±•ç¤ºäº†å“è¶Šçš„æ‰©å±•æ€§ï¼Œå…¶æ”¯æŒçš„æ™ºèƒ½ä½“æ•°é‡æ˜¯ä¸­å¿ƒåŒ– Conflict-Based Search (CBS) ç®—æ³•çš„ 3.4 å€ï¼Œä¸”åœ¨ç‹­çª„é€šé“ä¸­å¤„ç†ä»»åŠ¡çš„èƒ½åŠ›æ˜¾è‘—ä¼˜äº Token Passing with Task Swaps (TPTS)ã€‚æ­¤å¤–ï¼ŒPRISM åœ¨è§£è´¨é‡ä¸Šä¸ CBS æŒå¹³ï¼Œä¸”æ˜¾è‘—ç¼©çŸ­äº†è®¡ç®—æ—¶é—´å¹¶å‡è½»äº†å•ä¸ªæ™ºèƒ½ä½“çš„è®¡ç®—è´Ÿæ‹…ï¼Œä¸ºå¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒä¸‹çš„å¤šæ™ºèƒ½ä½“åä½œæä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "38 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.08025v1",
      "published_date": "2025-05-12 19:48:32 UTC",
      "updated_date": "2025-05-12 19:48:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:18.470149+00:00"
    },
    {
      "arxiv_id": "2505.08021v3",
      "title": "The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic",
      "title_zh": "æœ‰ç•Œå›¾ç¥ç»ç½‘ç»œä¸ä¸€é˜¶é€»è¾‘ç‰‡æ®µçš„å¯¹åº”å…³ç³»",
      "authors": [
        "Bernardo Cuenca Grau",
        "Eva Feng",
        "PrzemysÅ‚aw A. WaÅ‚Ä™ga"
      ],
      "abstract": "Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we propose GNN architectures that correspond precisely to prominent fragments of first-order logic (FO), including various modal logics as well as more expressive two-variable fragments. To establish these results, we apply methods from finite model theory of first-order and modal logics to the domain of graph representation learning. Our results provide a unifying framework for understanding the logical expressiveness of GNNs within FO.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Graph Neural Networks (GNNs)åœ¨å¤„ç†å˜é•¿å›¾æ•°æ®å’ŒåŒæ„ä¸å˜æ€§æ–¹é¢çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†èƒ½å¤Ÿç²¾ç¡®å¯¹åº”ä¸€é˜¶é€»è¾‘(First-Order Logic, FO)ä¸­æ ¸å¿ƒç‰‡æ®µçš„GNNæ¶æ„ã€‚ç ”ç©¶æ¶µç›–äº†å¤šç§æ¨¡æ€é€»è¾‘(modal logics)ä»¥åŠè¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„åŒå˜é‡ç‰‡æ®µ(two-variable fragments)ï¼Œä»è€Œå»ºç«‹äº†ç¥ç»ç½‘ç»œæ¶æ„ä¸é€»è¾‘å½¢å¼ä¹‹é—´çš„å½¢å¼åŒ–è”ç³»ã€‚é€šè¿‡å°†æœ‰é™æ¨¡å‹ç†è®º(finite model theory)ä¸­å…³äºä¸€é˜¶é€»è¾‘å’Œæ¨¡æ€é€»è¾‘çš„æ–¹æ³•åº”ç”¨äºå›¾è¡¨ç¤ºå­¦ä¹ é¢†åŸŸï¼Œä½œè€…è¯æ˜äº†è¿™äº›ç‰¹å®šæ¶æ„ä¸é€»è¾‘è¯­è¨€ä¹‹é—´çš„ç­‰ä»·æ€§ã€‚è¿™ä¸€æˆæœä¸ºåœ¨FOæ¡†æ¶ä¸‹ç†è§£GNNçš„é€»è¾‘è¡¨è¾¾èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œæœ‰åŠ©äºç³»ç»Ÿåœ°è¯„ä¼°å’Œè®¾è®¡å…·æœ‰ç‰¹å®šé€»è¾‘å±æ€§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.08021v3",
      "published_date": "2025-05-12 19:45:45 UTC",
      "updated_date": "2025-11-17 10:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:35.840848+00:00"
    },
    {
      "arxiv_id": "2505.08004v1",
      "title": "Large Language Models and Arabic Content: A Review",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸é˜¿æ‹‰ä¼¯è¯­å†…å®¹ï¼šç»¼è¿°",
      "authors": [
        "Haneh Rhel",
        "Dmitri Roussinov"
      ],
      "abstract": "Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°å›é¡¾äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é˜¿æ‹‰ä¼¯è¯­å†…å®¹åŠè‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ä¸è¿›å±•ã€‚å°½ç®¡é˜¿æ‹‰ä¼¯è¯­ä½¿ç”¨å¹¿æ³›ï¼Œä½†å…¶ä¸°å¯Œçš„å½¢æ€å­¦(morphology)ç‰¹å¾å’Œå¤æ‚çš„è¯­è¨€ç»“æ„ï¼ŒåŠ ä¹‹ç›¸å…³æ•°æ®é›†å’Œå·¥å…·çš„åŒ®ä¹ï¼Œä¸ºNLPä»»åŠ¡å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¤šè¯­è¨€è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„LLMså·²åœ¨å¤šé¡¹é˜¿æ‹‰ä¼¯è¯­ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆæ•ˆã€‚æ–‡ç« æ¦‚è¿°äº†æ—©æœŸçš„é˜¿æ‹‰ä¼¯è¯­é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨å¤„ç†å„ç§æ–¹è¨€(dialects)å’Œç‰¹å®šå†…å®¹ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†é€šè¿‡å¾®è°ƒ(finetuning)å’Œæç¤ºå·¥ç¨‹(prompt engineering)æå‡æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æ€»ç»“äº†ç°æœ‰çš„é˜¿æ‹‰ä¼¯è¯­åŸºå‡†(benchmarks)å’Œæ•°æ®é›†ï¼Œæ­ç¤ºäº†LLMsåœ¨é˜¿æ‹‰ä¼¯è¯­å­¦æœ¯ç ”ç©¶ä¸åº”ç”¨ä¸­æŒç»­å¢é•¿çš„è¶‹åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Original language: English This paper has been submitted to the First International Conference on Artificial Intelligence and Generative AI (FICAILY 2025), and it has been accepted for presentation at FICAILY on 9-10/July 2025 and for publication in the Springer Nature. Number of pages: 16 Publication status Accepted/In press - 7 Apr 2025 https://www.gena-ai-libya2025.com/",
      "pdf_url": "https://arxiv.org/pdf/2505.08004v1",
      "published_date": "2025-05-12 19:09:12 UTC",
      "updated_date": "2025-05-12 19:09:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:51.684006+00:00"
    },
    {
      "arxiv_id": "2505.08818v1",
      "title": "Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare",
      "title_zh": "ç«‹åœºï¼šé‡æ„åˆ†ç±»ä½“ç³»ä¸è½å®æŒ‡å—å¯¹åŒ»ç–—é¢†åŸŸ VLM åº”ç”¨çš„å¿…è¦æ€§",
      "authors": [
        "Amara Tariq",
        "Rimita Lahiri",
        "Charles Kahn",
        "Imon Banerjee"
      ],
      "abstract": "The intricate and multifaceted nature of vision language model (VLM) development, adaptation, and application necessitates the establishment of clear and standardized reporting protocols, particularly within the high-stakes context of healthcare. Defining these reporting standards is inherently challenging due to the diverse nature of studies involving VLMs, which vary significantly from the development of all new VLMs or finetuning for domain alignment to off-the-shelf use of VLM for targeted diagnosis and prediction tasks. In this position paper, we argue that traditional machine learning reporting standards and evaluation guidelines must be restructured to accommodate multiphase VLM studies; it also has to be organized for intuitive understanding of developers while maintaining rigorous standards for reproducibility. To facilitate community adoption, we propose a categorization framework for VLM studies and outline corresponding reporting standards that comprehensively address performance evaluation, data reporting protocols, and recommendations for manuscript composition. These guidelines are organized according to the proposed categorization scheme. Lastly, we present a checklist that consolidates reporting standards, offering a standardized tool to ensure consistency and quality in the publication of VLM-related research.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡é’ˆå¯¹åŒ»ç–—å¥åº·é¢†åŸŸä¸­è§†è§‰è¯­è¨€æ¨¡å‹(Vision Language Models, VLMs)å¼€å‘ä¸åº”ç”¨çš„å¤æ‚æ€§ï¼Œå¼ºè°ƒäº†å»ºç«‹æ ‡å‡†åŒ–æŠ¥å‘Šåè®®çš„ç´§è¿«æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æŠ¥å‘Šæ ‡å‡†å·²æ— æ³•æ»¡è¶³ä»æ¨¡å‹å¾®è°ƒ(Fine-tuning)åˆ°å¼€ç®±å³ç”¨(Off-the-shelf)ç­‰å¤šæ ·åŒ–çš„VLMç ”ç©¶éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹VLMç ”ç©¶çš„åˆ†ç±»æ¡†æ¶(Categorization Framework)ï¼Œå¹¶æ®æ­¤åˆ¶å®šäº†æ¶µç›–æ€§èƒ½è¯„ä¼°ã€æ•°æ®æŠ¥å‘Šåè®®åŠè®ºæ–‡æ’°å†™å»ºè®®çš„è¯¦ç»†æŠ¥å‘Šæ ‡å‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä»½æ ‡å‡†åŒ–çš„æ ¸æŸ¥è¡¨(Checklist)ï¼Œæ—¨åœ¨ç¡®ä¿VLMç›¸å…³ç ”ç©¶åœ¨å‡ºç‰ˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ä¸è´¨é‡ã€‚è¿™äº›å‡†åˆ™ä¸ºåŒ»ç–—é¢†åŸŸè§„èŒƒåŒ–é‡‡ç”¨VLMæŠ€æœ¯å¹¶æé«˜ç ”ç©¶çš„å¯å¤ç°æ€§(Reproducibility)æä¾›äº†é‡è¦çš„åˆ¶åº¦æ”¯æ’‘ä¸å®è·µå·¥å…·ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "15 pages, 2, tables, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.08818v1",
      "published_date": "2025-05-12 18:39:54 UTC",
      "updated_date": "2025-05-12 18:39:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:50.126292+00:00"
    },
    {
      "arxiv_id": "2505.07985v2",
      "title": "Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness",
      "title_zh": "ä¸ªä½“å…¬å¹³ï¼Œç¾¤ä½“å¤±è¡¡ï¼Ÿå®¡è®¡åŒ¿ååŒ–å¯¹æœºå™¨å­¦ä¹ å…¬å¹³æ€§çš„å½±å“",
      "authors": [
        "HÃ©ber H. Arcolezi",
        "Mina Alishahi",
        "Adda-Akram Bendoukha",
        "Nesrine Kaaniche"
      ],
      "abstract": "Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\\ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to fourfold. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶ç³»ç»Ÿåœ°å®¡è®¡äº†åŒ¿ååŒ–æŠ€æœ¯ï¼ˆAnonymization techniquesï¼‰å¯¹æœºå™¨å­¦ä¹ å…¬å¹³æ€§ï¼ˆML Fairnessï¼‰çš„å½±å“ï¼Œå¡«è¡¥äº† $k$-anonymityã€$\\ell$-diversity å’Œ $t$-closeness ç­‰æŠ€æœ¯åœ¨å…¬å¹³æ€§é¢†åŸŸç ”ç©¶çš„ç©ºç™½ã€‚é€šè¿‡åœ¨ä¸åŒéšç§è®¾ç½®å’Œæ•°æ®åˆ†å¸ƒä¸‹è¿›è¡Œå®šé‡ç ”ç©¶ï¼Œä½œè€…æ·±å…¥æ¢è®¨äº†åŒ¿ååŒ–å¯¹ä¸ªäººå…¬å¹³æ€§ï¼ˆindividual fairnessï¼‰ä¸ç¾¤ä½“å…¬å¹³æ€§ï¼ˆgroup fairnessï¼‰çš„ä¸åŒä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒ¿ååŒ–å¤„ç†å¯èƒ½ä¼šä½¿ç¾¤ä½“å…¬å¹³æ€§æŒ‡æ ‡æ¶åŒ–é«˜è¾¾å››å€ï¼Œæ˜¾è‘—é™ä½äº†å†³ç­–çš„å…¬æ­£æ€§ã€‚ç„¶è€Œï¼Œç”±äºåŒ¿ååŒ–å¢åŠ äº†è¾“å…¥æ•°æ®çš„åŒè´¨æ€§ï¼ˆhomogeneityï¼‰ï¼ŒåŸºäºç›¸ä¼¼æ€§çš„ä¸ªäººå…¬å¹³æ€§æŒ‡æ ‡åœ¨å¼ºåŒ–åŒ¿åä¿æŠ¤æ—¶åè€Œè¶‹äºæ”¹å–„ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†éšç§ã€å…¬å¹³æ€§ä¸æ•ˆç”¨ï¼ˆutilityï¼‰ä¹‹é—´å¤æ‚çš„æƒè¡¡å…³ç³»ï¼Œä¸ºå¼€å‘è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ï¼ˆResponsible AIï¼‰æä¾›äº†å…³é”®è§è§£å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07985v2",
      "published_date": "2025-05-12 18:32:28 UTC",
      "updated_date": "2025-10-31 13:13:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:57.127224+00:00"
    },
    {
      "arxiv_id": "2505.07973v1",
      "title": "Probabilistic approach to longitudinal response prediction: application to radiomics from brain cancer imaging",
      "title_zh": "çºµå‘å“åº”é¢„æµ‹çš„æ¦‚ç‡æ€§æ–¹æ³•ï¼šåœ¨è„‘è‚¿ç˜¤å½±åƒç»„å­¦ä¸­çš„åº”ç”¨",
      "authors": [
        "Isabella Cama",
        "Michele Piana",
        "Cristina Campi",
        "Sara Garbarino"
      ],
      "abstract": "Longitudinal imaging analysis tracks disease progression and treatment response over time, providing dynamic insights into treatment efficacy and disease evolution. Radiomic features extracted from medical imaging can support the study of disease progression and facilitate longitudinal prediction of clinical outcomes. This study presents a probabilistic model for longitudinal response prediction, integrating baseline features with intermediate follow-ups. The probabilistic nature of the model naturally allows to handle the instrinsic uncertainty of the longitudinal prediction of disease progression. We evaluate the proposed model against state-of-the-art disease progression models in both a synthetic scenario and using a brain cancer dataset. Results demonstrate that the approach is competitive against existing methods while uniquely accounting for uncertainty and controlling the growth of problem dimensionality, eliminating the need for data from intermediate follow-ups.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äº longitudinal response prediction çš„ probabilistic modelï¼Œå¹¶å°†å…¶åº”ç”¨äºè„‘ç™Œå½±åƒçš„ radiomics ç‰¹å¾åˆ†æã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆ baseline features ä¸ä¸­æœŸéšè®¿æ•°æ®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ç–¾ç—…è¿›å±•é¢„æµ‹ä¸­å›ºæœ‰çš„ uncertaintyã€‚ç ”ç©¶äººå‘˜åœ¨åˆæˆåœºæ™¯å’ŒçœŸå®è„‘ç™Œæ•°æ®é›†ä¸Šï¼Œå°†è¯¥æ–¹æ³•ä¸å¤šç§å…ˆè¿›çš„ disease progression models è¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚ç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆæ§åˆ¶é—®é¢˜çš„ç»´åº¦å¢é•¿ï¼Œå¹¶å‡å°‘äº†å¯¹ä¸­æœŸéšè®¿æ•°æ®çš„ä¾èµ–ã€‚è¿™ç§æ¦‚ç‡åŒ–æ–¹æ³•ä¸ºåŠ¨æ€ç›‘æµ‹æ²»ç–—æ•ˆæœå’Œç–¾ç—…æ¼”å˜æä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primary_category": "stat.AP",
      "comment": "21 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07973v1",
      "published_date": "2025-05-12 18:15:24 UTC",
      "updated_date": "2025-05-12 18:15:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:47:56.519532+00:00"
    },
    {
      "arxiv_id": "2505.07819v2",
      "title": "H$^3$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning",
      "title_zh": "H$^3$DPï¼šé¢å‘è§†è§‰è¿åŠ¨å­¦ä¹ çš„ä¸‰é‡åˆ†å±‚æ‰©æ•£ç­–ç•¥",
      "authors": [
        "Yiyang Lu",
        "Yufeng Tian",
        "Zhecheng Yuan",
        "Xianbang Wang",
        "Pu Hua",
        "Zhengrong Xue",
        "Huazhe Xu"
      ],
      "abstract": "Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\\textbf{Triply-Hierarchical Diffusion Policy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$ average relative improvement over baselines across $\\mathbf{44}$ simulation tasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºH$^3$DP (Triply-Hierarchical Diffusion Policy)çš„è§†è§‰è¿åŠ¨å­¦ä¹ (visuomotor learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­è§†è§‰æ„ŸçŸ¥ä¸åŠ¨ä½œé¢„æµ‹è€¦åˆä¸è¶³çš„é—®é¢˜ã€‚H$^3$DPé€šè¿‡æ˜¾å¼æ„å»ºä¸‰é‡å±‚çº§ç»“æ„æ¥å¼ºåŒ–è§†è§‰ç‰¹å¾ä¸åŠ¨ä½œç”Ÿæˆçš„é›†æˆï¼ŒåŒ…æ‹¬åŸºäºæ·±åº¦ä¿¡æ¯çš„RGB-Dè§‚æµ‹åˆ†å±‚(depth-aware input layering)ã€å¤šå°ºåº¦è§†è§‰è¡¨å¾(multi-scale visual representations)ä»¥åŠå°†ç²—åˆ°ç»†åŠ¨ä½œç”Ÿæˆä¸ç›¸åº”è§†è§‰ç‰¹å¾å¯¹é½çš„åˆ†å±‚æ¡ä»¶æ‰©æ•£è¿‡ç¨‹(hierarchically conditioned diffusion process)ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒH$^3$DPåœ¨44é¡¹æ¨¡æ‹Ÿä»»åŠ¡ä¸­æ¯”åŸºçº¿æ¨¡å‹å¹³å‡æå‡äº†27.5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨4é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•ŒåŒæ‰‹æ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å±‚çº§åŒ–çš„è®¾è®¡æ˜¾è‘—æå‡äº†ç”Ÿæˆç­–ç•¥åœ¨å¤æ‚æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07819v2",
      "published_date": "2025-05-12 17:59:43 UTC",
      "updated_date": "2025-06-17 08:36:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:48:03.873483+00:00"
    },
    {
      "arxiv_id": "2505.07816v2",
      "title": "Graph neural networks and MSO",
      "title_zh": "å›¾ç¥ç»ç½‘ç»œä¸å•å­äºŒé˜¶é€»è¾‘",
      "authors": [
        "Veeti Ahvonen",
        "Damian Heiman",
        "Antti Kuusisto"
      ],
      "abstract": "We give an alternative proof for the existing result that recurrent graph neural networks working with reals have the same expressive power in restriction to monadic second-order logic MSO as the graded modal substitution calculus. The proof is based on constructing distributed automata that capture all MSO-definable node properties over trees. We also consider some variants of the acceptance conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸ºå®æ•°åŸŸä¸Šçš„é€’å½’å›¾ç¥ç»ç½‘ç»œï¼ˆrecurrent graph neural networksï¼‰åœ¨å•å­äºŒé˜¶é€»è¾‘ï¼ˆmonadic second-order logic, MSOï¼‰é™åˆ¶ä¸‹çš„è¡¨è¾¾èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ›¿ä»£è¯æ˜ï¼Œè¯å®äº†å…¶ä¸åˆ†çº§æ¨¡æ€æ›¿æ¢æ¼”ç®—ï¼ˆgraded modal substitution calculusï¼‰å…·æœ‰ç›¸åŒçš„è¡¨è¾¾åŠ›ã€‚è¯æ˜è¿‡ç¨‹åŸºäºæ„å»ºåˆ†å¸ƒå¼è‡ªåŠ¨æœºï¼ˆdistributed automataï¼‰ï¼Œè¯¥è‡ªåŠ¨æœºèƒ½å¤Ÿæ•è·æ ‘ç»“æ„ä¸Šæ‰€æœ‰å¯ç”± MSO å®šä¹‰çš„èŠ‚ç‚¹å±æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒæ¥å—æ¡ä»¶ï¼ˆacceptance conditionsï¼‰çš„å˜ä½“å¯¹æ¨¡å‹æ€§è´¨çš„å½±å“ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ç®€åŒ–äº†ç°æœ‰ç†è®ºç»“è®ºçš„è¯æ˜è·¯å¾„ï¼Œä¹Ÿè¿›ä¸€æ­¥é˜æ˜äº†å›¾ç¥ç»ç½‘ç»œä¸é€»è¾‘ç³»ç»Ÿä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07816v2",
      "published_date": "2025-05-12 17:59:22 UTC",
      "updated_date": "2025-05-15 13:32:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:48:11.932788+00:00"
    },
    {
      "arxiv_id": "2505.07813v1",
      "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies",
      "title_zh": "DexWildï¼šé¢å‘è‡ªç„¶åœºæ™¯æœºå™¨äººç­–ç•¥çš„çµå·§äººç±»äº¤äº’",
      "authors": [
        "Tony Tao",
        "Mohan Kumar Srirama",
        "Jason Jingzhou Liu",
        "Kenneth Shaw",
        "Deepak Pathak"
      ],
      "abstract": "Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous manipulation policies to generalize to novel environments, but acquiring such datasets presents many challenges. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DexWild-System, a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. This combination results in robust robot policies capable of generalizing to novel environments, tasks, and embodiments with minimal additional robot-specific data. Experimental results demonstrate that DexWild significantly improves performance, achieving a 68.5% success rate in unseen environments-nearly four times higher than policies trained with robot data only-and offering 5.8x better cross-embodiment generalization. Video results, codebases, and instructions at https://dexwild.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DexWildï¼Œæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„äººç±»æ‰‹éƒ¨äº¤äº’æ•°æ®ï¼Œæå‡æœºå™¨äººçµå·§æ“ä½œ(Dexterous manipulation)ç­–ç•¥åœ¨çœŸå®ç¯å¢ƒ(In-the-Wild)ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä½æˆæœ¬ã€ä¾¿æºå¼çš„ DexWild-System è®¾å¤‡ï¼Œå…è®¸é‡‡é›†è€…åœ¨å„ç§æ—¥å¸¸åœºæ™¯ä¸­è®°å½•å¤§é‡äººç±»æ‰‹éƒ¨æ“ä½œæ•°æ®ã€‚DexWild å­¦ä¹ æ¡†æ¶é‡‡ç”¨äººç±»ä¸æœºå™¨äººæ¼”ç¤ºæ•°æ®ååŒè®­ç»ƒ(Co-training)çš„ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥æå°‘çš„æœºå™¨äººç‰¹å®šæ•°æ®å®ç°å¯¹æ–°ç¯å¢ƒã€æ–°ä»»åŠ¡å’Œæ–°å…·èº«(Embodiments)çš„ç¨³å¥æ³›åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDexWild åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„æˆåŠŸç‡è¾¾åˆ° 68.5%ï¼Œæ€§èƒ½ä¼˜äºä»…ä½¿ç”¨æœºå™¨äººæ•°æ®è®­ç»ƒçš„ç­–ç•¥è¿‘å››å€ï¼Œå¹¶å®ç°äº† 5.8 å€çš„è·¨å…·èº«æ³›åŒ–æå‡ï¼Œä¸ºæ„å»ºé€šç”¨æœºå™¨äººç­–ç•¥æä¾›äº†é«˜æ•ˆçš„æ•°æ®é‡‡é›†ä¸è®­ç»ƒèŒƒå¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "In RSS 2025. Website at https://dexwild.github.io",
      "pdf_url": "https://arxiv.org/pdf/2505.07813v1",
      "published_date": "2025-05-12 17:59:05 UTC",
      "updated_date": "2025-05-12 17:59:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:48:34.898809+00:00"
    },
    {
      "arxiv_id": "2505.07809v1",
      "title": "A Comparative Analysis of Static Word Embeddings for Hungarian",
      "title_zh": "åŒˆç‰™åˆ©è¯­é™æ€è¯åµŒå…¥å¯¹æ¯”åˆ†æ",
      "authors": [
        "MÃ¡tÃ© Gedeon"
      ],
      "abstract": "This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and extrinsic tasks to provide a holistic view of their performance. For intrinsic evaluation, we employ a word analogy task, which assesses the embeddings ability to capture semantic and syntactic relationships. Our results indicate that traditional static embeddings, particularly FastText, excel in this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among the BERT-based models, the X2Static method for extracting static embeddings demonstrates superior performance compared to decontextualized and aggregate methods, approaching the effectiveness of traditional static embeddings. For extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results reveal that embeddings derived from dynamic models, especially those extracted using the X2Static method, outperform purely static embeddings. Notably, ELMo embeddings achieve the highest accuracy in both NER and POS tagging tasks, underscoring the benefits of contextualized representations even when used in a static form. Our findings highlight the continued relevance of static word embeddings in NLP applications and the potential of advanced extraction methods to enhance the utility of BERT-based models. This piece of research contributes to the understanding of embedding performance in the Hungarian language and provides valuable insights for future developments in the field. The training scripts, evaluation codes, restricted vocabulary, and extracted embeddings will be made publicly available to support further research and reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åŒˆç‰™åˆ©è¯­çš„å¤šç§é™æ€è¯åµŒå…¥(Static Word Embeddings)è¿›è¡Œäº†å…¨é¢å¯¹æ¯”åˆ†æï¼Œæ¶µç›–äº†ä¼ ç»Ÿçš„ Word2Vecã€FastText ä»¥åŠä» BERT ç­‰æ¨¡å‹ä¸­é€šè¿‡ä¸åŒæ–¹æ³•æå–çš„é™æ€åµŒå…¥ã€‚é€šè¿‡å†…éƒ¨çš„è¯ç±»æ¯”ä»»åŠ¡(word analogy task)è¯„ä¼°å‘ç°ï¼Œä¼ ç»Ÿçš„ FastText åœ¨æ•æ‰è¯­ä¹‰å’Œå¥æ³•å…³ç³»æ–¹é¢è¡¨ç°æœ€å¼ºï¼Œè€Œ X2Static æ–¹æ³•åœ¨ BERT è¡ç”Ÿæ¨¡å‹ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚åœ¨å¤–éƒ¨çš„å‘½åå®ä½“è¯†åˆ«(NER)å’Œè¯æ€§æ ‡æ³¨(POS)ä»»åŠ¡ä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºä»åŠ¨æ€æ¨¡å‹è¡ç”Ÿçš„åµŒå…¥æ€§èƒ½ä¼˜äºçº¯é™æ€åµŒå…¥ã€‚å…¶ä¸­ ELMo åµŒå…¥åœ¨ä¸¤é¡¹å¤–éƒ¨ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€é«˜å‡†ç¡®ç‡ï¼Œå‡¸æ˜¾äº†è¯­å¢ƒåŒ–è¡¨å¾å³ä½¿ä»¥é™æ€å½¢å¼ä½¿ç”¨ä¹Ÿå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é™æ€è¯åµŒå…¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­çš„æŒç»­ä»·å€¼ï¼Œå¹¶ä¸ºæå‡åŒˆç‰™åˆ©è¯­è¯åµŒå…¥æ€§èƒ½åŠ BERT æ¨¡å‹çš„å®ç”¨æ€§æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07809v1",
      "published_date": "2025-05-12 17:57:11 UTC",
      "updated_date": "2025-05-12 17:57:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:48:38.689003+00:00"
    },
    {
      "arxiv_id": "2505.07802v2",
      "title": "Improving Trajectory Stitching with Flow Models",
      "title_zh": "åˆ©ç”¨æµæ¨¡å‹ä¼˜åŒ–è½¨è¿¹æ‹¼æ¥",
      "authors": [
        "Reece O'Mahoney",
        "Wanming Yu",
        "Ioannis Havoutis"
      ],
      "abstract": "Generative models have shown great promise as trajectory planners, given their affinity to modeling complex distributions and guidable inference process. Previous works have successfully applied these in the context of robotic manipulation but perform poorly when the required solution does not exist as a complete trajectory within the training set. We identify that this is a result of being unable to plan via stitching, and subsequently address the architectural and dataset choices needed to remedy this. On top of this, we propose a novel addition to the training and inference procedures to both stabilize and enhance these capabilities. We demonstrate the efficacy of our approach by generating plans with out of distribution boundary conditions and performing obstacle avoidance on the Franka Panda in simulation and on real hardware. In both of these tasks our method performs significantly better than the baselines and is able to avoid obstacles up to four times as large.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åœ¨è½¨è¿¹è§„åˆ’ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºå½“å‰æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸­ç¼ºä¹å®Œæ•´è½¨è¿¹ã€éœ€é€šè¿‡è½¨è¿¹æ‹¼æ¥ (Trajectory Stitching) è§£å†³é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ã€‚ä½œè€…è¯†åˆ«å‡ºè¿™ä¸€å±€é™æ€§æºäºæ¶æ„å’Œæ•°æ®é›†çš„é€‰æ‹©ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäº Flow Models çš„æ”¹è¿›æ–¹æ³•ã€‚ä¸ºäº†å¢å¼ºå¹¶ç¨³å®šæ‹¼æ¥èƒ½åŠ›ï¼Œç ”ç©¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†åˆ›æ–°çš„å¢å¼ºæœºåˆ¶ï¼Œä¼˜åŒ–äº†ç”Ÿæˆå¼è§„åˆ’æµç¨‹ã€‚é€šè¿‡åœ¨ Franka Panda æœºå™¨äººä»¿çœŸå’Œç¡¬ä»¶ç¯å¢ƒä¸­çš„éªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†åˆ†å¸ƒå¤–è¾¹ç•Œæ¡ä»¶å’Œéšœç¢ç‰©è§„é¿ä»»åŠ¡æ—¶å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿè§„é¿å°ºå¯¸é«˜è¾¾å¯¹æ¯”æ–¹æ³•å››å€çš„éšœç¢ç‰©ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„æœºå™¨äººè·¯å¾„è§„åˆ’æä¾›äº†æ›´å…·é²æ£’æ€§çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07802v2",
      "published_date": "2025-05-12 17:50:10 UTC",
      "updated_date": "2025-06-03 16:45:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:48:12.637868+00:00"
    },
    {
      "arxiv_id": "2505.07796v2",
      "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æŒç»­é¢„è®­ç»ƒçš„å­¦ä¹ åŠ¨åŠ›å­¦",
      "authors": [
        "Xingjin Wang",
        "Howe Tissue",
        "Lu Wang",
        "Linjing Li",
        "Daniel Dajun Zeng"
      ],
      "abstract": "Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æŒç»­é¢„è®­ç»ƒ(Continual Pre-Training, CPT)è¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€ï¼Œé‡ç‚¹åˆ†æäº†é€šç”¨é¢†åŸŸä¸ä¸‹æ¸¸é¢†åŸŸæ€§èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚ç ”ç©¶å‘ç° CPT æŸå¤±æ›²çº¿æœ¬è´¨ä¸Šä½“ç°äº†ä»ä¸€æ¡æ›²çº¿å‘å¦ä¸€æ¡éšè—æ›²çº¿çš„è½¬æ¢ï¼Œå¹¶é€šè¿‡è§£è€¦åˆ†å¸ƒåç§»(distribution shift)å’Œå­¦ä¹ ç‡é€€ç«(learning rate annealing)çš„å½±å“ï¼Œæ¨å¯¼å‡ºäº†å…¨æ–°çš„ CPT æ‰©å±•å®šå¾‹(CPT scaling law)ã€‚è¯¥å®šå¾‹èƒ½å¤Ÿé¢„æµ‹ CPT ä¸­ä»»ä½•è®­ç»ƒæ­¥æ•°åŠä¸åŒå­¦ä¹ ç‡è°ƒåº¦(LRS)ä¸‹çš„æŸå¤±ï¼Œå¹¶æ·±åŒ–äº†å¯¹æŸå¤±æ½œåŠ›(loss potential)ã€å³°å€¼å­¦ä¹ ç‡ã€è®­ç»ƒæ­¥æ•°åŠå›æ”¾æ¯”ä¾‹(replay ratio)ç­‰æ ¸å¿ƒå› ç´ çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ”¯æŒæ ¹æ®ä¸åŒç›®æ ‡ï¼ˆå¦‚å¹³è¡¡é€šç”¨ä¸ç‰¹å®šé¢†åŸŸæ€§èƒ½ï¼‰å®šåˆ¶è®­ç»ƒè¶…å‚æ•°ã€‚å¤šé¡¹å®éªŒè¡¨æ˜ï¼Œè¯¥æ‰©å±•å®šå¾‹åœ¨å„ç§ CPT æ•°æ®é›†å’Œè®­ç»ƒé…ç½®ä¸‹å‡è¡¨ç°å‡ºæé«˜çš„å‡†ç¡®æ€§ä¸é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICML2025 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2505.07796v2",
      "published_date": "2025-05-12 17:47:32 UTC",
      "updated_date": "2025-06-19 10:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:48:55.293443+00:00"
    },
    {
      "arxiv_id": "2505.07793v2",
      "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs",
      "title_zh": "æº¢å‡ºé¢„é˜²å¢å¼ºé•¿ä¸Šä¸‹æ–‡å¾ªç¯å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Assaf Ben-Kish",
        "Itamar Zimerman",
        "M. Jehanzeb Mirza",
        "Lior Wolf",
        "James Glass",
        "Leonid Karlinsky",
        "Raja Giryes"
      ],
      "abstract": "A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¾ªç¯æ¬¡äºŒæ¬¡ (recurrent sub-quadratic) å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„æ•ˆç‡ç“¶é¢ˆï¼Œå‘ç°å³ä½¿ç»è¿‡é’ˆå¯¹æ€§è®­ç»ƒï¼Œå…¶å›ºå®šå¤§å°çš„å¾ªç¯è®°å¿† (recurrent memory) ä»æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ†å—æ¨ç†ç¨‹åº (chunk-based inference procedure)ï¼Œé€šè¿‡è¯†åˆ«å¹¶ä»…å¤„ç†è¾“å…¥ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†æ¥æœ‰æ•ˆç¼“è§£å¾ªç¯è®°å¿†å¤±æ•ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä½¿ RWKV6-Finch-7B å’Œ RecurrentGemma ç­‰æ¨¡å‹åœ¨ LongBench ä¸Šçš„æ€§èƒ½æœ€é«˜æå‡äº† 51%ï¼Œå¹¶åœ¨ LongBench v2 ä¸­è¾¾åˆ°äº† state-of-the-art æ°´å¹³ï¼Œè¡¨ç°è¶³ä»¥åª²ç¾åŒç­‰è§„æ¨¡çš„ Transformersã€‚æ­¤å¤–ï¼Œç”±äºå•åˆ†å—ç­–ç•¥åœ¨ç†è®ºä¸Šéœ€è¦è·¨ä¸Šä¸‹æ–‡å…³ç³»çš„ä»»åŠ¡ä¸­ä¾ç„¶è¡¨ç°å¼ºåŠ²ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥å¼•å‘äº†å¯¹å¾ªç¯æ¨¡å‹æ˜¯å¦çœŸæ­£æœ‰æ•ˆåˆ©ç”¨äº†é•¿ç¨‹ä¾èµ– (long-range dependencies) çš„æ·±åˆ»åæ€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Official Implementation: https://github.com/assafbk/OPRM",
      "pdf_url": "https://arxiv.org/pdf/2505.07793v2",
      "published_date": "2025-05-12 17:45:05 UTC",
      "updated_date": "2025-09-08 20:57:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:03.392092+00:00"
    },
    {
      "arxiv_id": "2505.07775v1",
      "title": "Must Read: A Systematic Survey of Computational Persuasion",
      "title_zh": "å¿…è¯»ï¼šè®¡ç®—è¯´æœç³»ç»Ÿç»¼è¿°",
      "authors": [
        "Nimet Beyza Bozdag",
        "Shuhaib Mehri",
        "Xiaocheng Yang",
        "Hyeonjeong Ha",
        "Zirui Cheng",
        "Esin Durmus",
        "Jiaxuan You",
        "Heng Ji",
        "Gokhan Tur",
        "Dilek Hakkani-TÃ¼r"
      ],
      "abstract": "Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI's susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI's role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Computational Persuasionï¼ˆè®¡ç®—è¯´æœï¼‰è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„ç»¼è¿°ï¼Œæ¢è®¨äº† Persuasion åœ¨æ²Ÿé€šå’Œå†³ç­–ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä»¥åŠå¯¹è¯å¼ AI ç³»ç»Ÿå¸¦æ¥çš„æœºé‡ä¸é£é™©ã€‚è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå…³é”®è§†è§’çš„æ¡†æ¶ï¼Œåˆ†åˆ«åˆ†æäº† AI as a Persuaderï¼ˆAI ä½œä¸ºè¯´æœè€…ï¼‰çš„å†…å®¹ç”Ÿæˆä¸åº”ç”¨ã€AI as a Persuadeeï¼ˆAI ä½œä¸ºè¢«è¯´æœè€…ï¼‰å¯¹æ“çºµå’Œåè§çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠ AI as a Persuasion Judgeï¼ˆAI ä½œä¸ºè¯´æœè¯„åˆ¤è€…ï¼‰åœ¨è¯„ä¼°ç­–ç•¥å’Œç¡®ä¿ä¼¦ç†æ–¹é¢çš„ä½œç”¨ã€‚ç ”ç©¶æå‡ºäº† Computational Persuasion çš„ Taxonomyï¼ˆåˆ†ç±»ä½“ç³»ï¼‰ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†è¯„ä¼° Persuasivenessï¼ˆè¯´æœåŠ›ï¼‰ã€å‡è½»æ“çºµæ€§å½±å“ä»¥åŠå¼€å‘è´Ÿè´£ä»» AI ç³»ç»Ÿç­‰å…³é”®æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å‹¾å‹’äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨æå‡ AI é©±åŠ¨è¯´æœç³»ç»Ÿçš„å®‰å…¨æ€§ã€å…¬å¹³æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶åº”å¯¹å¤§è¯­è¨€æ¨¡å‹å¸¦æ¥çš„æ½œåœ¨é£é™©ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07775v1",
      "published_date": "2025-05-12 17:26:31 UTC",
      "updated_date": "2025-05-12 17:26:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:06.628998+00:00"
    },
    {
      "arxiv_id": "2505.07773v4",
      "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving",
      "title_zh": "Agent RL ç¼©æ”¾æ³•åˆ™ï¼šé¢å‘æ•°å­¦é—®é¢˜æ±‚è§£çš„è‡ªå‘ä»£ç æ‰§è¡Œæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Xinji Mai",
        "Haotian Xu",
        "Zhong-Zhi Li",
        "Xing W",
        "Weinong Wang",
        "Jian Hu",
        "Yingying Zhang",
        "Wenqiang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿›è¡Œæ•°å­¦æ¨ç†æ—¶ï¼Œå¦‚ä½•é€šè¿‡åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä½¿æ™ºèƒ½ä½“è‡ªä¸»å­¦ä¼šåˆ©ç”¨å¤–éƒ¨å·¥å…·ã€‚ä½œè€…æå‡ºäº†ZeroTIRæ¡†æ¶ï¼Œåœ¨æ²¡æœ‰ç›‘ç£å·¥å…·ä½¿ç”¨ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒåŸºç¡€æ¨¡å‹è‡ªå‘åœ°ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç æ¥è§£å†³æ•°å­¦é—®é¢˜ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæ­ç¤ºäº†Agent RLä¸­çš„ç¼©æ”¾æ³•åˆ™(Scaling Law)ï¼Œå³éšç€è®­ç»ƒæ­¥æ•°çš„å¢åŠ ï¼Œè‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å›å¤é•¿åº¦ä»¥åŠä»»åŠ¡å‡†ç¡®ç‡ä¹‹é—´è¡¨ç°å‡ºæ˜¾è‘—çš„å¯é¢„æµ‹æ­£ç›¸å…³æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZeroTIRåœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¸ä½¿ç”¨å·¥å…·çš„ZeroRLåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†è®¡ç®—æŠ•å…¥ä¸å·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥æ¶Œç°ä¹‹é—´çš„å®šé‡å…³ç³»ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£è‡ªä¸»å·¥å…·ä½¿ç”¨çš„ä¹ å¾—ä¸æ‰©å±•æœºåˆ¶å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥çš„Agent RLç ”ç©¶æä¾›äº†å¯å¤ç°çš„åŸºå‡†å’Œå¼€æºä»£ç ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07773v4",
      "published_date": "2025-05-12 17:23:34 UTC",
      "updated_date": "2025-08-20 12:20:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:09.887948+00:00"
    },
    {
      "arxiv_id": "2505.07768v1",
      "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding",
      "title_zh": "é€šè¿‡åŒå‘æ³¨é‡Šçº§å…±åŒç†è§£å¢å¼ºä»£ç ç”Ÿæˆ",
      "authors": [
        "Yifeng Di",
        "Tianyi Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.",
      "tldr_zh": "å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ç”Ÿæˆä¸­å¸¸é¢ä¸´åŠŸèƒ½æ€§é”™è¯¯ï¼Œå¯¼è‡´å¼€å‘è€…åœ¨æ£€æŸ¥å’Œä¿®å¤ä»£ç æ—¶æ•ˆç‡é™ä½ä¸”ä¿¡ä»»åº¦å—æŸã€‚å—é€šä¿¡é¢†åŸŸçš„å…±åŒç†è§£(mutual grounding)ç†è®ºå¯å‘ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§äº¤äº’å¼æ–¹æ³•ï¼Œåˆ©ç”¨ä»£ç æ³¨é‡Šä½œä¸ºå¼€å‘è€…ä¸æ¨¡å‹å»ºç«‹å…±äº«ç†è§£çš„åª’ä»‹ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿è¿›è¡Œä»£ç ç”Ÿæˆã€è¡Œå†…æ³¨é‡Šç”Ÿæˆä»¥åŠåŸºäºå¯ç¼–è¾‘æ³¨é‡Šçš„ä¸Šä¸‹æ–‡åé¦ˆï¼Œå®ç°äº†å¼€å‘æ„å›¾ä¸ä»£ç ç”Ÿæˆçš„è¿­ä»£å¯¹é½ã€‚åœ¨ HumanEval ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å¤šä¸ªä¸»æµæ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿ code-davinci-002 çš„ pass@1 æŒ‡æ ‡æé«˜äº† 17.1%ã€‚æ­¤å¤–ï¼Œä¸€é¡¹é’ˆå¯¹ 12 åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºï¼Œä¸ GitHub Copilot ç­‰åŸºçº¿ç›¸æ¯”ï¼Œä½¿ç”¨è¯¥æ–¹æ³•çš„ä»»åŠ¡å®Œæˆé€Ÿåº¦æå‡äº† 16.7%ï¼Œä»»åŠ¡æˆåŠŸç‡å¢åŠ äº† 10.5%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡äº¤äº’å¼ä¼˜åŒ–ä»£ç æ³¨é‡Šæ¥å»ºç«‹åŒå‘çš„å…±åŒç†è§£ï¼Œèƒ½æœ‰æ•ˆæé«˜ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§å¹¶å¢å¼ºå¼€å‘è€…çš„ä¿¡å¿ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to ICSE 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07768v1",
      "published_date": "2025-05-12 17:20:30 UTC",
      "updated_date": "2025-05-12 17:20:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:09.733206+00:00"
    },
    {
      "arxiv_id": "2505.07759v1",
      "title": "\"I Apologize For Not Understanding Your Policy\": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants",
      "title_zh": "â€œæŠ±æ­‰ï¼Œæˆ‘ä¸ç†è§£æ‚¨çš„ç­–ç•¥â€ï¼šAI è™šæ‹ŸåŠ©æ‰‹å¯¹ç”¨æˆ·ç®¡ç†è®¿é—®æ§åˆ¶ç­–ç•¥è§„çº¦ä¸è¯„ä¼°çš„æ¢ç´¢",
      "authors": [
        "Jennifer Mondragon",
        "Carlos Rubio-Medrano",
        "Gael Cruz",
        "Dvijesh Shastri"
      ],
      "abstract": "The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants (VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek has turned them into convenient interfaces for managing emerging technologies such as Smart Homes, Smart Cars, Electronic Health Records, by means of explicit commands,e.g., prompts, which can be even launched via voice, thus providing a very convenient interface for end-users. However, the proper specification and evaluation of User-Managed Access Control Policies (U-MAPs), the rules issued and managed by end-users to govern access to sensitive data and device functionality - within these VAs presents significant challenges, since such a process is crucial for preventing security vulnerabilities and privacy leaks without impacting user experience. This study provides an initial exploratory investigation on whether current publicly-available VAs can manage U-MAPs effectively across differing scenarios. By conducting unstructured to structured tests, we evaluated the comprehension of such VAs, revealing a lack of understanding in varying U-MAP approaches. Our research not only identifies key limitations, but offers valuable insights into how VAs can be further improved to manage complex authorization rules and adapt to dynamic changes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½è™šæ‹ŸåŠ©æ‰‹ (AI-based Virtual Assistants, VAs) åœ¨è§„èŒƒå’Œè¯„ä¼°ç”¨æˆ·ç®¡ç†è®¿é—®æ§åˆ¶ç­–ç•¥ (User-Managed Access Control Policies, U-MAPs) æ–¹é¢çš„å®é™…è¡¨ç°ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½å®¶å±…åŠç”µå­å¥åº·è®°å½•ç®¡ç†ä¸­çš„å®‰å…¨ä¸éšç§æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹ Google Geminiã€ChatGPTã€Microsoft Copilot å’Œ Deepseek ç­‰ä¸»æµè™šæ‹ŸåŠ©æ‰‹è¿›è¡Œä»éç»“æ„åŒ–åˆ°ç»“æ„åŒ–çš„ç³»åˆ—æµ‹è¯•ï¼Œç³»ç»Ÿè¯„ä¼°äº†å®ƒä»¬å¯¹å¤šæ ·åŒ– U-MAP åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„è™šæ‹ŸåŠ©æ‰‹åœ¨å¤„ç†å¤æ‚çš„ U-MAPs æ–¹æ³•æ—¶å­˜åœ¨æ˜æ˜¾çš„ç†è§£ç¼ºå¤±ï¼Œéš¾ä»¥æœ‰æ•ˆæ‰§è¡Œç”¨æˆ·åˆ¶å®šçš„æˆæƒè§„åˆ™ã€‚è¿™é¡¹æ¢ç´¢æ€§ç ”ç©¶ä¸ä»…è¯†åˆ«äº†ç°æœ‰æŠ€æœ¯åœ¨å®‰å…¨ç®¡ç†æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œè¿˜ä¸ºæå‡è™šæ‹ŸåŠ©æ‰‹ç®¡ç†åŠ¨æ€æˆæƒè§„åˆ™çš„èƒ½åŠ›ä»¥åŠä¼˜åŒ–ç”¨æˆ·éšç§ä¿æŠ¤æä¾›äº†é‡è¦çš„å‚è€ƒè§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07759v1",
      "published_date": "2025-05-12 17:03:52 UTC",
      "updated_date": "2025-05-12 17:03:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:44.174760+00:00"
    },
    {
      "arxiv_id": "2505.07757v1",
      "title": "Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture",
      "title_zh": "æƒ…æ„Ÿæ¢¯åº¦å…ƒè®¤çŸ¥ RSIï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰ï¼šç†è®ºåŸºç¡€ä¸å•æ™ºèƒ½ä½“æ¶æ„",
      "authors": [
        "Rintaro Ando"
      ],
      "abstract": "We present the Emotion-Gradient Metacognitive Recursive Self-Improvement (EG-MRSI) framework, a novel architecture that integrates introspective metacognition, emotion-based intrinsic motivation, and recursive self-modification into a unified theoretical system. The framework is explicitly capable of overwriting its own learning algorithm under formally bounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation, EG-MRSI introduces a differentiable intrinsic reward function driven by confidence, error, novelty, and cumulative success. This signal regulates both a metacognitive mapping and a self-modification operator constrained by provable safety mechanisms. We formally define the initial agent configuration, emotion-gradient dynamics, and RSI trigger conditions, and derive a reinforcement-compatible optimization objective that guides the agent's development trajectory. Meaning Density and Meaning Conversion Efficiency are introduced as quantifiable metrics of semantic learning, closing the gap between internal structure and predictive informativeness. This Part I paper establishes the single-agent theoretical foundations of EG-MRSI. Future parts will extend this framework to include safety certificates and rollback protocols (Part II), collective intelligence mechanisms (Part III), and feasibility constraints including thermodynamic and computational limits (Part IV). Together, the EG-MRSI series provides a rigorous, extensible foundation for open-ended and safe AGI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æƒ…ç»ªæ¢¯åº¦å…ƒè®¤çŸ¥é€’å½’è‡ªæˆ‘æ”¹è¿› (Emotion-Gradient Metacognitive Recursive Self-Improvement, EG-MRSI) æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªé›†å†…çœå…ƒè®¤çŸ¥ã€åŸºäºæƒ…ç»ªçš„å†…åœ¨åŠ¨æœºå’Œé€’å½’è‡ªæˆ‘ä¿®æ”¹äºä¸€ä½“çš„ç»Ÿä¸€ç†è®ºä½“ç³»ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“åœ¨æ­£å¼ç•Œå®šçš„é£é™©èŒƒå›´å†…é‡å†™å…¶è‡ªèº«çš„å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡åœ¨ Noise-to-Meaning RSI (N2M-RSI) åŸºç¡€ä¸Šå¼•å…¥ç”±ç½®ä¿¡åº¦ã€è¯¯å·®ã€æ–°é¢–æ€§å’Œç´¯ç§¯æˆåŠŸç‡é©±åŠ¨çš„å¯å¾®å†…åœ¨å¥–åŠ±å‡½æ•°ï¼Œå®ç°å¯¹å…ƒè®¤çŸ¥æ˜ å°„ä¸è‡ªæˆ‘ä¿®æ”¹ç®—å­çš„è°ƒèŠ‚ã€‚ç ”ç©¶æ­£å¼å®šä¹‰äº†æ™ºèƒ½ä½“åˆå§‹é…ç½®ã€æƒ…ç»ªæ¢¯åº¦åŠ¨åŠ›å­¦åŠ RSI è§¦å‘æ¡ä»¶ï¼Œå¹¶æ¨å¯¼å‡ºäº†å¼•å¯¼æ™ºèƒ½ä½“å‘å±•è½¨è¿¹çš„å¼ºåŒ–å­¦ä¹ å…¼å®¹ä¼˜åŒ–ç›®æ ‡ã€‚é€šè¿‡å¼•å…¥æ„ä¹‰å¯†åº¦ (Meaning Density) å’Œæ„ä¹‰è½¬æ¢æ•ˆç‡ (Meaning Conversion Efficiency) ç­‰å¯é‡åŒ–æŒ‡æ ‡ï¼Œè¯¥æ¡†æ¶æˆåŠŸç¼©å°äº†å†…éƒ¨ç»“æ„ä¸é¢„æµ‹ä¿¡æ¯åŒ–ä¹‹é—´çš„å·®è·ã€‚ä½œä¸ºç³»åˆ—ç ”ç©¶çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œæœ¬å·¥ä½œä¸ºå•æ™ºèƒ½ä½“æ¶æ„å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨ä¸ºå®ç°å¼€æ”¾å¼ä¸”å®‰å…¨çš„é€šç”¨äººå·¥æ™ºèƒ½ (AGI) æä¾›ä¸¥è°¨ä¸”å¯æ‰©å±•çš„åŸºçŸ³ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 3 figures. Part I of a four-part series (Parts II-IV forthcoming)",
      "pdf_url": "https://arxiv.org/pdf/2505.07757v1",
      "published_date": "2025-05-12 17:02:47 UTC",
      "updated_date": "2025-05-12 17:02:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:27.537626+00:00"
    },
    {
      "arxiv_id": "2505.07755v1",
      "title": "Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems",
      "title_zh": "è¾¹ç¼˜è®¡ç®—ç³»ç»Ÿä¸­ CPU å¯†é›†å‹æµæ•°æ®å¤„ç†çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Tomasz Szydlo",
        "Viacheslaw Horbanow",
        "Dev Nandan Jha",
        "Shashikant Ilager",
        "Aleksander Slominski",
        "Rajiv Ranjan"
      ],
      "abstract": "Edge computing has emerged as a pivotal technology, offering significant advantages such as low latency, enhanced data security, and reduced reliance on centralized cloud infrastructure. These benefits are crucial for applications requiring real-time data processing or strict security measures. Despite these advantages, edge devices operating within edge clusters are often underutilized. This inefficiency is mainly due to the absence of a holistic performance profiling mechanism which can help dynamically adjust the desired system configuration for a given workload. Since edge computing environments involve a complex interplay between CPU frequency, power consumption, and application performance, a deeper understanding of these correlations is essential. By uncovering these relationships, it becomes possible to make informed decisions that enhance both computational efficiency and energy savings. To address this gap, this paper evaluates the power consumption and performance characteristics of a single processing node within an edge cluster using a synthetic microbenchmark by varying the workload size and CPU frequency. The results show how an optimal measure can lead to optimized usage of edge resources, given both performance and power consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Edge computingç³»ç»Ÿä¸­è®¾å¤‡åˆ©ç”¨ç‡ä¸è¶³ä»¥åŠç¼ºä¹æ•´ä½“æ€§èƒ½å‰–ææœºåˆ¶çš„é—®é¢˜ï¼Œå¯¹CPU-intensive Stream Data Processingè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨åˆæˆå¾®åŸºå‡†(synthetic microbenchmark) è¯„ä¼°äº†è¾¹ç¼˜é›†ç¾¤ä¸­å•ä¸ªå¤„ç†èŠ‚ç‚¹çš„power consumptionä¸æ€§èƒ½ç‰¹å¾ã€‚é€šè¿‡è°ƒèŠ‚workload sizeå’ŒCPU frequencyï¼Œè¯¥ç ”ç©¶è¯¦ç»†åˆ†æäº†ç¡¬ä»¶å‚æ•°ä¸åº”ç”¨è¡¨ç°ä¹‹é—´çš„å¤æ‚å…³è”ã€‚å®éªŒç»“æœå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¯»æ‰¾æœ€ä½³å¹³è¡¡ç‚¹æ¥ä¼˜åŒ–è¾¹ç¼˜èµ„æºçš„ä½¿ç”¨ï¼Œä»è€ŒåŒæ—¶æå‡è®¡ç®—æ•ˆç‡ä¸èƒ½æºåˆ©ç”¨ç‡ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºåœ¨è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹è¿›è¡Œæ˜æ™ºçš„ç³»ç»Ÿé…ç½®å†³ç­–æä¾›äº†é‡è¦çš„å®éªŒä¾æ®å’Œå‚è€ƒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07755v1",
      "published_date": "2025-05-12 17:02:02 UTC",
      "updated_date": "2025-05-12 17:02:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:20.579835+00:00"
    },
    {
      "arxiv_id": "2506.06288v1",
      "title": "DELPHYNE: A Pre-Trained Model for General and Financial Time Series",
      "title_zh": "DELPHYNEï¼šé¢å‘é€šç”¨å’Œé‡‘èæ—¶é—´åºåˆ—çš„é¢„è®­ç»ƒæ¨¡å‹",
      "authors": [
        "Xueying Ding",
        "Aakriti Mittal",
        "Achintya Gopal"
      ],
      "abstract": "Time-series data is a vital modality within data science communities. This is particularly valuable in financial applications, where it helps in detecting patterns, understanding market behavior, and making informed decisions based on historical data. Recent advances in language modeling have led to the rise of time-series pre-trained models that are trained on vast collections of datasets and applied to diverse tasks across financial domains. However, across financial applications, existing time-series pre-trained models have not shown boosts in performance over simple finance benchmarks in both zero-shot and fine-tuning settings. This phenomenon occurs because of a i) lack of financial data within the pre-training stage, and ii) the negative transfer effect due to inherently different time-series patterns across domains. Furthermore, time-series data is continuous, noisy, and can be collected at varying frequencies and with varying lags across different variables, making this data more challenging to model than languages. To address the above problems, we introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and also shows superior performances on various financial tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DELPHYNEï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹é€šç”¨åŠé‡‘èæ—¶é—´åºåˆ—(Time Series)è®¾è®¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ç›®å‰çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨é‡‘èé¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œä¸»è¦å½’å› äºé¢„è®­ç»ƒé˜¶æ®µç¼ºä¹é‡‘èæ•°æ®ï¼Œä»¥åŠè·¨é¢†åŸŸæ—¶é—´åºåˆ—æ¨¡å¼å·®å¼‚å¯¼è‡´çš„è´Ÿè¿ç§»(Negative Transfer)æ•ˆåº”ã€‚æ­¤å¤–ï¼Œé‡‘èæ•°æ®å…·æœ‰è¿ç»­ã€å¤šå™ªç‚¹ä»¥åŠè·¨å˜é‡é‡‡é›†é¢‘ç‡ä¸ä¸€ç­‰ç‰¹ç‚¹ï¼Œä¸ºå»ºæ¨¡å¸¦æ¥äº†æå¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼ŒDELPHYNEé€šè¿‡ä¼˜åŒ–å»ºæ¨¡æ–¹å¼ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºç¡€æ¨¡å‹åœ¨é›¶æ ·æœ¬(Zero-shot)å’Œå¾®è°ƒ(Fine-tuning)è®¾ç½®ä¸‹çš„å±€é™æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»…éœ€æå°‘å¾®è°ƒæ­¥éª¤çš„æƒ…å†µä¸‹ï¼ŒDELPHYNEåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†ä¸ç°æœ‰å¤§æ¨¡å‹åŠå…¨æ ·æœ¬(Full-shot)æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§é‡‘èç‰¹å®šä»»åŠ¡ä¸­å±•ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œæœ‰æ•ˆæå‡äº†é‡‘èåœºæ™¯ä¸‹çš„æ¨¡å¼æ£€æµ‹ä¸å†³ç­–åˆ†æèƒ½åŠ›ã€‚",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06288v1",
      "published_date": "2025-05-12 16:53:29 UTC",
      "updated_date": "2025-05-12 16:53:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:22.069802+00:00"
    },
    {
      "arxiv_id": "2505.07921v2",
      "title": "Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning",
      "title_zh": "åŸºäºè‡ªè·¨ç‰¹å¾çš„é«˜æ•ˆå°æ ·æœ¬å­¦ä¹ è„‰å†²ç¥ç»ç½‘ç»œ",
      "authors": [
        "Qi Xu",
        "Junyang Zhu",
        "Dongdong Zhou",
        "Hao Chen",
        "Yang Liu",
        "Jiangrong Shen",
        "Qiang Zhang"
      ],
      "abstract": "Deep neural networks (DNNs) excel in computer vision tasks, especially, few-shot learning (FSL), which is increasingly important for generalizing from limited examples. However, DNNs are computationally expensive with scalability issues in real world. Spiking Neural Networks (SNNs), with their event-driven nature and low energy consumption, are particularly efficient in processing sparse and dynamic data, though they still encounter difficulties in capturing complex spatiotemporal features and performing accurate cross-class comparisons. To further enhance the performance and efficiency of SNNs in few-shot learning, we propose a few-shot learning framework based on SNNs, which combines a self-feature extractor module and a cross-feature contrastive module to refine feature representation and reduce power consumption. We apply the combination of temporal efficient training loss and InfoNCE loss to optimize the temporal dynamics of spike trains and enhance the discriminative power. Experimental results show that the proposed FSL-SNN significantly improves the classification performance on the neuromorphic dataset N-Omniglot, and also achieves competitive performance to ANNs on static datasets such as CUB and miniImageNet with low power consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè„‰å†²ç¥ç»ç½‘ç»œ(Spiking Neural Networks, SNNs)çš„å°‘æ ·æœ¬å­¦ä¹ (Few-shot Learning, FSL)æ¡†æ¶ FSL-SNNï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)è®¡ç®—æˆæœ¬é«˜ä»¥åŠä¼ ç»Ÿ SNNs åœ¨æ•æ‰å¤æ‚ç‰¹å¾å’Œè·¨ç±»åˆ«å¯¹æ¯”æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆè‡ªç‰¹å¾æå–æ¨¡å—(self-feature extractor module)å’Œè·¨ç‰¹å¾å¯¹æ¯”æ¨¡å—(cross-feature contrastive module)æ¥ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œæœ‰æ•ˆé™ä½äº†åŠŸè€—ã€‚ç ”ç©¶é‡‡ç”¨äº†æ—¶é—´æ•ˆç‡è®­ç»ƒæŸå¤±(temporal efficient training loss)ä¸ InfoNCE loss ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–è„‰å†²åºåˆ—çš„æ—¶é—´åŠ¨åŠ›å­¦å¹¶å¢å¼ºåˆ¤åˆ«èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFSL-SNN åœ¨ç¥ç»å½¢æ€æ•°æ®é›† N-Omniglot ä¸Šæ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ï¼Œå¹¶åœ¨ CUB å’Œ miniImageNet ç­‰é™æ€æ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸äººå·¥ç¥ç»ç½‘ç»œ(ANNs)ç›¸å½“çš„ç«äº‰æ°´å¹³ã€‚è¿™ä¸€æˆæœè¯æ˜äº† SNNs åœ¨ä¿æŒä½åŠŸè€—çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å®Œæˆå¤æ‚çš„è§†è§‰è¯†åˆ«ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07921v2",
      "published_date": "2025-05-12 16:51:08 UTC",
      "updated_date": "2025-05-15 02:56:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:49:47.813495+00:00"
    },
    {
      "arxiv_id": "2505.07728v1",
      "title": "Guiding Data Collection via Factored Scaling Curves",
      "title_zh": "é€šè¿‡å› å­åŒ–ç¼©æ”¾æ›²çº¿å¼•å¯¼æ•°æ®é‡‡é›†",
      "authors": [
        "Lihan Zha",
        "Apurva Badithela",
        "Michael Zhang",
        "Justin Lidard",
        "Jeremy Bao",
        "Emily Zhou",
        "David Snyder",
        "Allen Z. Ren",
        "Dhruv Shah",
        "Anirudha Majumdar"
      ],
      "abstract": "Generalist imitation learning policies trained on large datasets show great promise for solving diverse manipulation tasks. However, to ensure generalization to different conditions, policies need to be trained with data collected across a large set of environmental factor variations (e.g., camera pose, table height, distractors) $-$ a prohibitively expensive undertaking, if done exhaustively. We introduce a principled method for deciding what data to collect and how much to collect for each factor by constructing factored scaling curves (FSC), which quantify how policy performance varies as data scales along individual or paired factors. These curves enable targeted data acquisition for the most influential factor combinations within a given budget. We evaluate the proposed method through extensive simulated and real-world experiments, across both training-from-scratch and fine-tuning settings, and show that it boosts success rates in real-world tasks in new environments by up to 26% over existing data-collection strategies. We further demonstrate how factored scaling curves can effectively guide data collection using an offline metric, without requiring real-world evaluation at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ç­–ç•¥åœ¨å¤„ç†å¤šæ ·åŒ–æ“ä½œä»»åŠ¡æ—¶é¢ä¸´çš„é«˜æ˜‚æ•°æ®é‡‡é›†æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåˆ†è§£ç¼©æ”¾æ›²çº¿(Factored Scaling Curves, FSC)çš„åŸåˆ™æ€§æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡åŒ–ç­–ç•¥æ€§èƒ½éšå•ä¸€æˆ–æˆå¯¹ç¯å¢ƒå› å­ï¼ˆå¦‚ç›¸æœºä½å§¿ã€æ¡Œé¢é«˜åº¦ç­‰ï¼‰æ•°æ®è§„æ¨¡çš„å˜åŒ–è¶‹åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼åœ¨æœ‰é™é¢„ç®—å†…é’ˆå¯¹æœ€å…·å½±å“åŠ›çš„å› å­ç»„åˆè¿›è¡Œæœ‰ç›®æ ‡çš„æ•°æ®é‡‡é›†ã€‚åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼Œæ— è®ºæ˜¯ä»å¤´è®­ç»ƒè¿˜æ˜¯å¾®è°ƒè®¾ç½®ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜æ–°ç¯å¢ƒä¸‹ä»»åŠ¡æˆåŠŸç‡æ–¹é¢æ¯”ç°æœ‰ç­–ç•¥æå‡äº†å¤šè¾¾26%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç¦»çº¿æŒ‡æ ‡(offline metric)æ¥åº”ç”¨è¿™äº›æ›²çº¿ï¼Œä»è€Œåœ¨æ— éœ€å¤§è§„æ¨¡çœŸå®ä¸–ç•Œè¯„ä¼°çš„æƒ…å†µä¸‹æœ‰æ•ˆä¼˜åŒ–æ•°æ®é‡‡é›†æµç¨‹ï¼Œä¸ºé«˜æ•ˆæ„å»ºæœºå™¨äººå­¦ä¹ æ•°æ®é›†æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://factored-data-scaling.github.io",
      "pdf_url": "https://arxiv.org/pdf/2505.07728v1",
      "published_date": "2025-05-12 16:36:35 UTC",
      "updated_date": "2025-05-12 16:36:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:04.545694+00:00"
    },
    {
      "arxiv_id": "2505.07715v1",
      "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras",
      "title_zh": "é¢å‘äº‹ä»¶ç›¸æœºç›®æ ‡æ£€æµ‹çš„æ··åˆè„‰å†²è§†è§‰ Transformer",
      "authors": [
        "Qi Xu",
        "Jie Deng",
        "Jiangrong Shen",
        "Biwu Chen",
        "Huajin Tang",
        "Gang Pan"
      ],
      "abstract": "Event-based object detection has gained increasing attention due to its advantages such as high temporal resolution, wide dynamic range, and asynchronous address-event representation. Leveraging these advantages, Spiking Neural Networks (SNNs) have emerged as a promising approach, offering low energy consumption and rich spatiotemporal dynamics. To further enhance the performance of event-based object detection, this study proposes a novel hybrid spike vision Transformer (HsVT) model. The HsVT model integrates a spatial feature extraction module to capture local and global features, and a temporal feature extraction module to model time dependencies and long-term patterns in event sequences. This combination enables HsVT to capture spatiotemporal features, improving its capability to handle complex event-based object detection tasks. To support research in this area, we developed and publicly released The Fall Detection Dataset as a benchmark for event-based object detection tasks. This dataset, captured using an event-based camera, ensures facial privacy protection and reduces memory usage due to the event representation format. We evaluated the HsVT model on GEN1 and Fall Detection datasets across various model sizes. Experimental results demonstrate that HsVT achieves significant performance improvements in event detection with fewer parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º HsVT (Hybrid Spiking Vision Transformer) çš„æ··åˆè„‰å†²è§†è§‰ Transformer æ¨¡å‹ï¼Œæ—¨åœ¨åˆ©ç”¨è„‰å†²ç¥ç»ç½‘ç»œ (SNNs) çš„ä½åŠŸè€—ä¼˜åŠ¿æå‡åŸºäºäº‹ä»¶ç›¸æœº (Event Cameras) çš„ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚HsVT æ•´åˆäº†ç©ºé—´ç‰¹å¾æå–æ¨¡å—ä»¥æ•æ‰å±€éƒ¨ä¸å…¨å±€ç‰¹å¾ï¼Œå¹¶ç»“åˆæ—¶é—´ç‰¹å¾æå–æ¨¡å—å¯¹äº‹ä»¶åºåˆ—ä¸­çš„æ—¶é—´ä¾èµ–æ€§å’Œé•¿æœŸæ¨¡å¼è¿›è¡Œå»ºæ¨¡ã€‚è¿™ç§æ¶æ„ä½¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†æ•è·æ—¶ç©ºç‰¹å¾ (spatiotemporal features)ï¼Œå¢å¼ºäº†å¤„ç†å¤æ‚äº‹ä»¶é©±åŠ¨ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å…¬å¼€å‘å¸ƒäº† Fall Detection Dataset åŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨ç¡®ä¿éšç§ä¿æŠ¤å’Œé™ä½å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œä¸ºäº‹ä»¶æ£€æµ‹ç ”ç©¶æä¾›äº†æ”¯æ’‘ã€‚åœ¨ GEN1 å’Œ Fall Detection æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHsVT åœ¨å¤šç§æ¨¡å‹è§„æ¨¡ä¸‹å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœ€ç»ˆè¯æ˜ HsVT èƒ½å¤Ÿä»¥æ›´å°‘çš„å‚æ•°é‡å®ç°æ›´ä¼˜çš„æ£€æµ‹æ•ˆæœï¼Œä¸ºé«˜æ•ˆçš„äº‹ä»¶è§†è§‰å¤„ç†æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07715v1",
      "published_date": "2025-05-12 16:19:20 UTC",
      "updated_date": "2025-05-12 16:19:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:08.910419+00:00"
    },
    {
      "arxiv_id": "2505.07711v1",
      "title": "Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é‡å­ç¼–è¯‘ä¸æ¨¡æ‹Ÿç”µè·¯åˆ‡åˆ†",
      "authors": [
        "Pranav Sinha",
        "Sumit Kumar Jha",
        "Sunny Raj"
      ],
      "abstract": "We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å˜ˆæ‚ä¸­ç­‰è§„æ¨¡é‡å­(NISQ)æ—¶ä»£ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¼˜åŒ–é‡å­ç”µè·¯åˆ’åˆ†(Circuit Partitioning)çš„æ–¹æ³•ï¼Œä»¥è§£å†³é‡å­ç”µè·¯ç¼–è¯‘åœ¨å¤„ç†å¤§è§„æ¨¡ç”µè·¯æ—¶çš„è®¡ç®—ç“¶é¢ˆã€‚ç ”ç©¶è€…è°ƒæŸ¥äº†Llamaå’ŒMistralç­‰æ¨¡å‹ç†è§£ä¸ç”ŸæˆQASMä»£ç çš„èƒ½åŠ›ï¼Œé€šè¿‡å°†å…¶åº”ç”¨äºBerkeley Quantum Synthesis Toolkitçš„å¿«é€Ÿåˆ’åˆ†æ–¹æ³•ï¼Œæ¢ç´¢äº†LLMsåœ¨æ”¹è¿›ç”µè·¯åˆ’åˆ†ä¸­çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ç²¾ç»†å¾®è°ƒçš„å¼€æºLLMsèƒ½åœ¨åˆ’åˆ†ä»»åŠ¡ä¸­è¾¾åˆ°53.4%çš„å‡†ç¡®ç‡ï¼Œè€Œç°æˆçš„(off-the-shelf)æ¨¡å‹å³ä½¿é‡‡ç”¨æ ‡å‡†çš„1-shotæˆ–few-shotå­¦ä¹ æ–¹æ³•ä¹Ÿæ— æ³•æ­£ç¡®å®Œæˆä»»åŠ¡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†LLMsåœ¨é‡å­è®¡ç®—é¢†åŸŸè¾…åŠ©ç”µè·¯ç¼–è¯‘å’Œæ¨¡æ‹Ÿçš„æœ‰æ•ˆæ€§ï¼Œä¸ºç»“åˆäººå·¥æ™ºèƒ½ä¸é‡å­ç®—æ³•ä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.ET",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.ET",
      "comment": "7 pages, 2 tables and 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07711v1",
      "published_date": "2025-05-12 16:18:48 UTC",
      "updated_date": "2025-05-12 16:18:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:08.755261+00:00"
    },
    {
      "arxiv_id": "2505.07701v1",
      "title": "Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications",
      "title_zh": "é¢å‘ä½èµ„æºç«¯ä¾§åº”ç”¨çš„è½»é‡çº§ç«¯åˆ°ç«¯è¯­éŸ³åˆæˆ",
      "authors": [
        "Biel Tura Vecino",
        "Adam GabryÅ›",
        "Daniel MÄ…twicki",
        "Andrzej Pomirski",
        "Tom Iddon",
        "Marius Cotescu",
        "Jaime Lorenzo-Trueba"
      ],
      "abstract": "Recent works have shown that modelling raw waveform directly from text in an end-to-end (E2E) fashion produces more natural-sounding speech than traditional neural text-to-speech (TTS) systems based on a cascade or two-stage approach. However, current E2E state-of-the-art models are computationally complex and memory-consuming, making them unsuitable for real-time offline on-device applications in low-resource scenarios. To address this issue, we propose a Lightweight E2E-TTS (LE2E) model that generates high-quality speech requiring minimal computational resources. We evaluate the proposed model on the LJSpeech dataset and show that it achieves state-of-the-art performance while being up to $90\\%$ smaller in terms of model parameters and $10\\times$ faster in real-time-factor. Furthermore, we demonstrate that the proposed E2E training paradigm achieves better quality compared to an equivalent architecture trained in a two-stage approach. Our results suggest that LE2E is a promising approach for developing real-time, high quality, low-resource TTS applications for on-device applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰ç«¯åˆ°ç«¯(End-to-end, E2E)æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹è®¡ç®—å¤æ‚ä¸”å†…å­˜æ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºä½èµ„æºè®¾å¤‡ç«¯çš„ Lightweight E2E-TTS (LE2E) æ¨¡å‹ã€‚LE2E æ—¨åœ¨ä»¥æå°çš„è®¡ç®—å¼€é”€ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿç³»ç»Ÿåœ¨ç¦»çº¿å®æ—¶åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚åœ¨ LJSpeech æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒ SOTA æ€§èƒ½çš„åŒæ—¶ï¼Œå‚æ•°é‡å‡å°‘äº† 90%ï¼Œä¸”å®æ—¶ç‡(Real-time Factor) æå‡äº† 10 å€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜äº†ç«¯åˆ°ç«¯è®­ç»ƒèŒƒå¼åœ¨è¯­éŸ³è´¨é‡ä¸Šä¼˜äºåŒæ¶æ„çš„ä¸¤é˜¶æ®µ(Two-stage)è®­ç»ƒæ–¹æ³•ã€‚è¯¥æˆæœä¸ºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå®ç°é«˜æ€§èƒ½ã€å®æ—¶çš„ TTS åº”ç”¨æä¾›äº†ä¸€ç§æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Published as a conference paper at SSW 2023",
      "pdf_url": "https://arxiv.org/pdf/2505.07701v1",
      "published_date": "2025-05-12 16:10:15 UTC",
      "updated_date": "2025-05-12 16:10:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:12.011444+00:00"
    },
    {
      "arxiv_id": "2505.07920v1",
      "title": "Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions",
      "title_zh": "Re$^2$ï¼šé¢å‘å…¨é˜¶æ®µåŒè¡Œè¯„å®¡ä¸å¤šè½®å›å¤è®¨è®ºçš„ä¸€è‡´æ€§ä¿éšœæ•°æ®é›†",
      "authors": [
        "Daoze Zhang",
        "Zhijian Bao",
        "Sihang Du",
        "Zhiyi Zhao",
        "Kuangling Zhang",
        "Dezheng Bao",
        "Yang Yang"
      ],
      "abstract": "Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in https://anonymous.4open.science/r/ReviewBench_anon/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½é¢†åŸŸåŒè¡Œè¯„å®¡(Peer Review)é‡æ¿€å¢å¯¼è‡´çš„å®¡ç¨¿è´¨é‡ä¸‹é™å’Œç¼ºä¹æœ‰æ•ˆæŠ•ç¨¿å‰è‡ªè¯„å·¥å…·çš„é—®é¢˜ï¼Œæ¨å‡ºäº†è§„æ¨¡æœ€å¤§çš„ç¡®ä¿ä¸€è‡´æ€§çš„åŒè¡Œè¯„å®¡å’Œåé©³(Rebuttal)æ•°æ®é›†Re$^2$ã€‚è¯¥æ•°æ®é›†æœ‰æ•ˆè§£å†³äº†ç°æœ‰æ•°æ®é›†åœ¨æ•°æ®å¤šæ ·æ€§ã€åˆå§‹æŠ•ç¨¿ä¸€è‡´æ€§ä»¥åŠåé©³äº’åŠ¨æ”¯æŒæ–¹é¢çš„å±€é™ï¼ŒåŒ…å«äº†æ¥è‡ªOpenReviewå¹³å°ä¸Š24ä¸ªä¼šè®®å’Œ21ä¸ªç ”è®¨ä¼šçš„19,926ç¯‡åˆå§‹æŠ•ç¨¿ã€70,668æ¡è¯„å®¡æ„è§ä»¥åŠ53,818æ¡åé©³å†…å®¹ã€‚ç ”ç©¶å›¢é˜Ÿå°†åé©³å’Œè®¨è®ºé˜¶æ®µæ„å»ºä¸ºå¤šè½®å¯¹è¯(Multi-turn conversation)èŒƒå¼ï¼Œä»¥åŒæ—¶æ”¯æŒä¼ ç»Ÿçš„é™æ€è¯„å®¡ä»»åŠ¡å’ŒåŠ¨æ€çš„äº¤äº’å¼å¤§è¯­è¨€æ¨¡å‹(LLMs)åŠ©æ‰‹ã€‚Re$^2$æ•°æ®é›†çš„å‘å¸ƒä¸ºä½œè€…æ”¹è¿›æ–‡ç¨¿æä¾›äº†æ›´ä¸ºå®é™…çš„æŒ‡å¯¼ï¼Œæ—¨åœ¨é€šè¿‡æŠ€æœ¯æ‰‹æ®µç¼“è§£å­¦æœ¯è¯„å®¡ç³»ç»Ÿæ—¥ç›Šç¹é‡çš„è´Ÿæ‹…ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "2 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.07920v1",
      "published_date": "2025-05-12 16:02:52 UTC",
      "updated_date": "2025-05-12 16:02:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:21.685196+00:00"
    },
    {
      "arxiv_id": "2505.07693v1",
      "title": "Belief Injection for Epistemic Control in Linguistic State Space",
      "title_zh": "è¯­è¨€çŠ¶æ€ç©ºé—´ä¸­è®¤çŸ¥æ§åˆ¶çš„ä¿¡å¿µæ³¨å…¥",
      "authors": [
        "Sebastian Dumbrava"
      ],
      "abstract": "This work introduces belief injection, a proactive epistemic control mechanism for artificial agents whose cognitive states are structured as dynamic ensembles of linguistic belief fragments. Grounded in the Semantic Manifold framework, belief injection directly incorporates targeted linguistic beliefs into an agent's internal cognitive state, influencing reasoning and alignment proactively rather than reactively. We delineate various injection strategies, such as direct, context-aware, goal-oriented, and reflective approaches, and contrast belief injection with related epistemic control mechanisms, notably belief filtering. Additionally, this work discusses practical applications, implementation considerations, ethical implications, and outlines promising directions for future research into cognitive governance using architecturally embedded belief injection.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† belief injectionï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è®¤çŸ¥çŠ¶æ€è¢«ç»“æ„åŒ–ä¸ºåŠ¨æ€è¯­è¨€ä¿¡å¿µç‰‡æ®µé›†åˆçš„äººå·¥æ™ºèƒ½ä½“æ‰€è®¾è®¡çš„ä¸»åŠ¨è®¤è¯†æ§åˆ¶ (epistemic control) æœºåˆ¶ã€‚è¯¥æ–¹æ³•æ¤æ ¹äº Semantic Manifold æ¡†æ¶ï¼Œé€šè¿‡å°†ç‰¹å®šçš„è¯­è¨€ä¿¡å¿µç›´æ¥æ•´åˆåˆ°æ™ºèƒ½ä½“çš„å†…éƒ¨è®¤çŸ¥çŠ¶æ€ä¸­ï¼Œä»è€Œå®ç°å¯¹æ¨ç†å’Œå¯¹é½è¿‡ç¨‹çš„ä¸»åŠ¨å¼•å¯¼è€Œéè¢«åŠ¨å“åº”ã€‚æ–‡ä¸­è¯¦ç»†åˆ’åˆ†äº†åŒ…æ‹¬ directã€context-awareã€goal-oriented å’Œ reflective åœ¨å†…çš„å¤šç§æ³¨å…¥ç­–ç•¥ï¼Œå¹¶å°†å…¶ä¸ belief filtering ç­‰ç›¸å…³æ§åˆ¶æœºåˆ¶è¿›è¡Œäº†å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜æ·±å…¥æ¢è®¨äº† belief injection çš„å®é™…åº”ç”¨åœºæ™¯ã€å®æ–½ç»†èŠ‚åŠä¼¦ç†å½±å“ï¼Œä¸ºé€šè¿‡æ¶æ„åµŒå…¥å¼æŠ€æœ¯å®ç°è®¤çŸ¥æ²»ç†çš„ç ”ç©¶å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07693v1",
      "published_date": "2025-05-12 15:58:56 UTC",
      "updated_date": "2025-05-12 15:58:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:51:25.184930+00:00"
    },
    {
      "arxiv_id": "2505.07686v2",
      "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models",
      "title_zh": "S-GRPOï¼šæ¨ç†æ¨¡å‹ä¸­åŸºäºå¼ºåŒ–å­¦ä¹ çš„æå‰é€€å‡º",
      "authors": [
        "Muzhi Dai",
        "Chenxu Yang",
        "Qingyi Si"
      ],
      "abstract": "As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking issue arises from the inherent limitations of conventional outcome-reward reinforcement learning, which systematically overlooks the regulation of intermediate reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy Optimization (S-GRPO), a novel reinforcement learning paradigm that enables models to implicitly evaluate the sufficiency of intermediate reasoning steps, thereby facilitating early exit in CoT generation. Unlike GRPO, which samples multiple possible reasoning paths in parallel (parallel group), S-GRPO only samples one reasoning path and serially selects multiple temporal positions from the path to exit thinking and directly generate answers (serial group). For correct answers within a serial group, rewards gradually decrease based on the exit positions along the reasoning path from front to back. This design encourages the model to produce more accurate and concise thoughts, while also incentivizing early thinking termination when appropriate. Empirical evaluations demonstrate that S-GRPO is compatible with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill. Across diverse benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond, S-GRPO achieves a substantial reduction in sequence length (35.4% - 61.1%) while simultaneously improving accuracy (absolute 0.72% - 6.08%).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨ç†æ¨¡å‹åœ¨Chain-of-Thought (CoT)ç”Ÿæˆä¸­æ™®éå­˜åœ¨çš„æ€ç»´å†—ä½™å’Œoverthinkingé—®é¢˜ï¼Œæå‡ºäº†åä¸ºSerial-Group Decaying-Reward Policy Optimization (S-GRPO)çš„æ–°å‹å¼ºåŒ–å­¦ä¹ èŒƒå¼ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡å¼•å¯¼æ¨¡å‹éšå¼è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤çš„å……åˆ†æ€§ï¼Œå®ç°CoTç”Ÿæˆçš„early exitï¼Œä»¥è§£å†³ä¼ ç»Ÿoutcome-reward RLå¿½è§†æ¨ç†è¿‡ç¨‹è°ƒèŠ‚çš„å±€é™æ€§ã€‚ä¸å¹¶è¡Œé‡‡æ ·å¤šæ¡è·¯å¾„çš„GRPOä¸åŒï¼ŒS-GRPOåœ¨å•æ¡æ¨ç†è·¯å¾„ä¸Šä¸²è¡Œé€‰å–å¤šä¸ªæ—¶é—´ä½ç½®å°è¯•ç»ˆæ­¢æ€è€ƒï¼Œå¹¶æ ¹æ®é€€å‡ºä½ç½®çš„å…ˆåå¯¹æ­£ç¡®ç­”æ¡ˆè®¾ç½®é€’å‡å¥–åŠ±ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°æ¿€åŠ±æ¨¡å‹äº§ç”Ÿæ›´å‡†ç¡®ã€ç²¾ç‚¼çš„æ€ç»´ï¼Œå¹¶é¼“åŠ±åœ¨é€‚å½“æ—¶å€™æå‰ç»“æŸæ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒS-GRPOåœ¨Qwen3å’ŒDeepseek-distillç­‰å‰æ²¿æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨GSM8Kã€MATH-500å’ŒGPQA Diamondç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ä»…å°†åºåˆ—é•¿åº¦æ˜¾è‘—ç¼©å‡äº†35.4%è‡³61.1%ï¼Œè¿˜å®ç°äº†0.72%è‡³6.08%çš„å‡†ç¡®ç‡æå‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07686v2",
      "published_date": "2025-05-12 15:50:44 UTC",
      "updated_date": "2025-05-17 04:01:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:27.090498+00:00"
    },
    {
      "arxiv_id": "2505.07683v3",
      "title": "Multimodal Cancer Modeling in the Age of Foundation Model Embeddings",
      "title_zh": "åŸºç¡€æ¨¡å‹åµŒå…¥æ—¶ä»£çš„å¤šæ¨¡æ€ç™Œç—‡å»ºæ¨¡",
      "authors": [
        "Steven Song",
        "Morgan Borjigin-Wang",
        "Irene Madejski",
        "Robert L. Grossman"
      ],
      "abstract": "The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference dataset in cancer through its harmonized genomics, clinical, and imaging data. Numerous prior studies have developed bespoke deep learning models over TCGA for tasks such as cancer survival prediction. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive feature embeddings agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the ability to train classical machine learning models over multimodal, zero-shot FM embeddings of cancer data. We demonstrate the ease and additive effect of multimodal fusion, outperforming unimodal models. Further, we show the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we propose an embedding-centric approach to multimodal cancer modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç™Œç—‡åŸºå› ç»„å›¾è°±(TCGA)æ•°æ®é›†ä¸Šåˆ©ç”¨åŸºç¡€æ¨¡å‹(Foundation Models)åµŒå…¥è¿›è¡Œå¤šæ¨¡æ€ç™Œç—‡å»ºæ¨¡çš„èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€ç ”ç©¶å¯¹ç—…ç†æŠ¥å‘Šç­‰æ–‡æœ¬æ•°æ®åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶è€…é‡‡ç”¨é›¶æ ·æœ¬(zero-shot)åŸºç¡€æ¨¡å‹è·å–ç‰¹å¾åµŒå…¥ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒç»å…¸çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€èåˆ(multimodal fusion)å…·æœ‰æ˜¾è‘—çš„ç´¯åŠ æ•ˆåº”ï¼Œå…¶é¢„æµ‹æ€§èƒ½ä¼˜äºä»»ä½•å•ä¸€æ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä¸¥è°¨çš„å®éªŒè¯„ä¼°äº†åŸºäºæ¨¡å‹çš„æ–‡æœ¬æ‘˜è¦(text summarization)å’Œå¹»è§‰(hallucination)å¯¹å»ºæ¨¡ç»“æœçš„å½±å“ã€‚æ•´ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»¥åµŒå…¥ä¸ºä¸­å¿ƒ(embedding-centric)çš„å¤šæ¨¡æ€ç™Œç—‡å»ºæ¨¡æ–¹æ³•ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦é¢†åŸŸé«˜æ•ˆæ•´åˆå¼‚æ„æ•°æ®æä¾›äº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "camera ready version for ML4H 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07683v3",
      "published_date": "2025-05-12 15:47:21 UTC",
      "updated_date": "2025-11-06 14:32:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:51:31.089704+00:00"
    },
    {
      "arxiv_id": "2505.07675v2",
      "title": "Simple yet Effective Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization",
      "title_zh": "åŸºäºåŒå¤´ä¼˜åŒ–çš„ç®€å•é«˜æ•ˆè§†è§‰-è¯­è¨€æ¨¡å‹åŠç›‘ç£çŸ¥è¯†è’¸é¦",
      "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Hyungjoon Jang",
        "Sung Ju Hwang"
      ],
      "abstract": "Semi-supervised learning (SSL) has emerged as a practical solution for addressing data scarcity challenges by leveraging unlabeled data. Recently, vision-language models (VLMs), pre-trained on massive image-text pairs, have demonstrated remarkable zero-/few-shot performance that often surpasses SSL approaches due to their exceptional generalization capabilities. This gap motivates us to question: how can we effectively harness the powerful generalization capabilities of VLMs into task-specific models? Knowledge distillation (KD) offers a natural framework for transferring VLM capabilities, but we identify that it suffers from gradient conflicts between supervised and distillation losses. To address this challenge, we propose Dual-Head Optimization (DHO), which introduces dual prediction heads for each distinct signal. We observe that DHO resolves gradient conflicts, enabling improved feature learning compared to single-head KD baselines, with practical benefits of minimal computational overhead and test-time hyperparameter tuning without retraining. Extensive experiments across 15 datasets show that DHO consistently outperforms KD baselines, often outperforming teacher models with smaller student models. DHO also achieves new state-of-the-art performance on both in-distribution ImageNet semi-supervised learning and out-of-distribution generalization across ImageNet variants. We publicly release our code and model checkpoints to facilitate future research at https://github.com/erjui/DHO.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›é€šè¿‡çŸ¥è¯†è’¸é¦(Knowledge distillation, KD)æœ‰æ•ˆåœ°è½¬ç§»åˆ°ç‰¹å®šä»»åŠ¡æ¨¡å‹ä¸­ã€‚ä½œè€…å‘ç°ä¼ ç»Ÿè’¸é¦è¿‡ç¨‹ä¸­ç›‘ç£æŸå¤±ä¸è’¸é¦æŸå¤±ä¹‹é—´å­˜åœ¨æ¢¯åº¦å†²çª(gradient conflicts)ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„ç‰¹å¾å­¦ä¹ ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åŒå¤´ä¼˜åŒ–(Dual-Head Optimization, DHO)æ–¹æ³•ï¼Œé€šè¿‡ä¸ºæ¯ç§ä¿¡å·è®¾ç½®ç‹¬ç«‹çš„é¢„æµ‹å¤´æ¥æ¶ˆé™¤å†²çªå¹¶æå‡æ€§èƒ½ã€‚è¯¥æ–¹æ³•è®¡ç®—å¼€é”€æä½ï¼Œå¹¶å…è®¸åœ¨æ¨ç†é˜¶æ®µæ— éœ€é‡æ–°è®­ç»ƒå³å¯è¿›è¡Œè¶…å‚æ•°å¾®è°ƒã€‚åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDHOä¸€è‡´ä¼˜äºKDåŸºçº¿ï¼Œä¸”å¸¸èƒ½å®ç°å°å°ºå¯¸å­¦ç”Ÿæ¨¡å‹è¶…è¶Šå¤§å°ºå¯¸æ•™å¸ˆæ¨¡å‹çš„æ•ˆæœã€‚æœ€ç»ˆï¼ŒDHOåœ¨ImageNetåŠç›‘ç£å­¦ä¹ åŠåˆ†å¸ƒå¤–(out-of-distribution)æ³›åŒ–è¯„ä¼°ä¸­å‡è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›(state-of-the-art)æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "38 pages, 17 figures, preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.07675v2",
      "published_date": "2025-05-12 15:39:51 UTC",
      "updated_date": "2025-09-30 14:13:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:47.891215+00:00"
    },
    {
      "arxiv_id": "2505.07672v3",
      "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit",
      "title_zh": "OnPrem.LLMï¼šæ³¨é‡éšç§ä¿æŠ¤çš„æ–‡æ¡£æ™ºèƒ½å·¥å…·åŒ…",
      "authors": [
        "Arun S. Maiya"
      ],
      "abstract": "We present OnPrem$.$LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem$.$LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† OnPrem.LLMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Python çš„å·¥å…·åŒ…ï¼Œä¸“é—¨ç”¨äºåœ¨ç¦»çº¿æˆ–å—é™ç¯å¢ƒä¸­å°† Large Language Models (LLMs) å®‰å…¨åœ°åº”ç”¨äºæ•æ„Ÿçš„éå…¬å¼€æ•°æ®ã€‚ç³»ç»Ÿé€šè¿‡æç®€é…ç½®æä¾›äº†æ–‡æ¡£å¤„ç†ã€Retrieval-Augmented Generation (RAG)ã€ä¿¡æ¯æå–ã€æ‘˜è¦å’Œåˆ†ç±»ç­‰é¢„æ„å»ºæµæ°´çº¿ï¼Œæœ‰æ•ˆè§£å†³äº†éšç§ä¿æŠ¤åœºæ™¯ä¸‹çš„æ–‡æ¡£æ™ºèƒ½éœ€æ±‚ã€‚å®ƒæ”¯æŒåŒ…æ‹¬ llama.cppã€Ollamaã€vLLM å’Œ Hugging Face Transformers åœ¨å†…çš„å¤šç§åç«¯ï¼Œå¹¶å…¼å®¹ Quantized Model ä¸ GPU åŠ é€Ÿï¼Œå®ç°äº†åç«¯é—´çš„æ— ç¼åˆ‡æ¢ã€‚è™½ç„¶ä¸»è¦é’ˆå¯¹å…¨æœ¬åœ°è¿è¡Œè®¾è®¡ï¼Œä½†è¯¥å·¥å…·åŒ…ä¹Ÿå…è®¸åœ¨å—æ§æ¡ä»¶ä¸‹é›†æˆäº‘ç«¯æœåŠ¡ï¼Œæ”¯æŒå¹³è¡¡æ€§èƒ½ä¸æ•°æ®ç®¡æ§çš„æ··åˆéƒ¨ç½²æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé…å¤‡çš„ No-code Web Interface è¿›ä¸€æ­¥é™ä½äº†æŠ€æœ¯é—¨æ§›ï¼Œä½¿éæŠ€æœ¯ç”¨æˆ·ä¹Ÿèƒ½é«˜æ•ˆå®Œæˆå¤æ‚çš„æ–‡æ¡£å¤„ç†ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.07672v3",
      "published_date": "2025-05-12 15:36:27 UTC",
      "updated_date": "2025-09-26 14:32:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:50:46.785921+00:00"
    },
    {
      "arxiv_id": "2505.07671v1",
      "title": "Benchmarking Retrieval-Augmented Generation for Chemistry",
      "title_zh": "åŒ–å­¦é¢†åŸŸæ£€ç´¢å¢å¼ºç”Ÿæˆçš„åŸºå‡†è¯„æµ‹",
      "authors": [
        "Xianrui Zhong",
        "Bowen Jin",
        "Siru Ouyang",
        "Yanzhen Shen",
        "Qiao Jin",
        "Yin Fang",
        "Zhiyong Lu",
        "Jiawei Han"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at https://chemrag.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) åœ¨åŒ–å­¦é¢†åŸŸç¼ºä¹é«˜è´¨é‡è¯­æ–™å’Œè¯„ä¼°åŸºå‡†çš„ç°çŠ¶ï¼Œæå‡ºäº† ChemRAG-Bench ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†é…å¥—çš„è¯­æ–™åº“æ•´åˆäº†ç§‘å­¦æ–‡çŒ®ã€PubChem æ•°æ®åº“ã€PubMed æ‘˜è¦ã€æ•™ç§‘ä¹¦ç­‰å¼‚æ„çŸ¥è¯†æºï¼Œæ¶µç›–äº†å¤šæ ·çš„åŒ–å­¦ç›¸å…³ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜ŸåŒæ—¶å¼€å‘äº†æ¨¡å—åŒ–å·¥å…·åŒ… ChemRAG-Toolkitï¼Œæ”¯æŒäº”ç§æ£€ç´¢ç®—æ³•å’Œå…«ç§å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„é›†æˆã€‚å®éªŒè¯æ˜ï¼ŒRAG ç›¸æ¯”ç›´æ¥æ¨ç† (Direct Inference) æ–¹æ³•èƒ½å®ç°å¹³å‡ 17.4% çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚é€šè¿‡å¯¹æ£€ç´¢å™¨æ¶æ„ã€è¯­æ–™é€‰æ‹©åŠæ£€ç´¢æ®µè½æ•°é‡ç­‰å› ç´ çš„æ·±å…¥åˆ†æï¼Œè¯¥ç ”ç©¶ä¸ºåŒ–å­¦é¢†åŸŸ RAG ç³»ç»Ÿçš„æœªæ¥ç ”å‘ä¸éƒ¨ç½²æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07671v1",
      "published_date": "2025-05-12 15:34:45 UTC",
      "updated_date": "2025-05-12 15:34:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:51:49.579457+00:00"
    },
    {
      "arxiv_id": "2506.01968v1",
      "title": "Efficient ANN-SNN Conversion with Error Compensation Learning",
      "title_zh": "åŸºäºè¯¯å·®è¡¥å¿å­¦ä¹ çš„é«˜æ•ˆ ANN-SNN è½¬æ¢",
      "authors": [
        "Chang Liu",
        "Jiangrong Shen",
        "Xuming Ran",
        "Mingkun Xu",
        "Qi Xu",
        "Yi Xu",
        "Gang Pan"
      ],
      "abstract": "Artificial neural networks (ANNs) have demonstrated outstanding performance in numerous tasks, but deployment in resource-constrained environments remains a challenge due to their high computational and memory requirements. Spiking neural networks (SNNs) operate through discrete spike events and offer superior energy efficiency, providing a bio-inspired alternative. However, current ANN-to-SNN conversion often results in significant accuracy loss and increased inference time due to conversion errors such as clipping, quantization, and uneven activation. This paper proposes a novel ANN-to-SNN conversion framework based on error compensation learning. We introduce a learnable threshold clipping function, dual-threshold neurons, and an optimized membrane potential initialization strategy to mitigate the conversion error. Together, these techniques address the clipping error through adaptive thresholds, dynamically reduce the quantization error through dual-threshold neurons, and minimize the non-uniformity error by effectively managing the membrane potential. Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our method achieves high-precision and ultra-low latency among existing conversion methods. Using only two time steps, our method significantly reduces the inference time while maintains competitive accuracy of 94.75% on CIFAR-10 dataset under ResNet-18 structure. This research promotes the practical application of SNNs on low-power hardware, making efficient real-time processing possible.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥ç¥ç»ç½‘ç»œ(ANNs)åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²å›°éš¾ï¼Œä»¥åŠè„‰å†²ç¥ç»ç½‘ç»œ(SNNs)åœ¨è½¬æ¢è¿‡ç¨‹ä¸­é¢ä¸´çš„ç²¾åº¦æŸå¤±å’Œé«˜å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé”™è¯¯è¡¥å¿å­¦ä¹ (Error Compensation Learning)çš„æ–°å‹ ANN-to-SNN è½¬æ¢æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¯å­¦ä¹ çš„é˜ˆå€¼è£å‰ªå‡½æ•°(Learnable Threshold Clipping Function)ã€åŒé˜ˆå€¼ç¥ç»å…ƒ(Dual-threshold Neurons)ä»¥åŠä¼˜åŒ–çš„è†œç”µä½åˆå§‹åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”é˜ˆå€¼ã€åŠ¨æ€é‡åŒ–è¯¯å·®å‡å°‘å’Œè†œç”µä½ç®¡ç†æ¥ç¼“è§£è½¬æ¢è¿‡ç¨‹ä¸­çš„å„ç±»è¯¯å·®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨CIFAR-10ã€CIFAR-100å’ŒImageNetç­‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé«˜ç²¾åº¦ä¸è¶…ä½å»¶è¿Ÿçš„ç‰¹æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ResNet-18æ¶æ„ä¸‹ï¼Œè¯¥æ–¹æ³•ä»…éœ€2ä¸ªæ—¶é—´æ­¥(Time Steps)å³å¯åœ¨CIFAR-10æ•°æ®é›†ä¸Šä¿æŒ94.75%çš„ç«äº‰å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ç¼©çŸ­äº†æ¨ç†æ—¶é—´ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆæ¨åŠ¨äº†è„‰å†²ç¥ç»ç½‘ç»œåœ¨ä½åŠŸè€—ç¡¬ä»¶ä¸Šçš„å®é™…åº”ç”¨ï¼Œä½¿é«˜æ•ˆçš„å®æ—¶å¤„ç†æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01968v1",
      "published_date": "2025-05-12 15:31:34 UTC",
      "updated_date": "2025-05-12 15:31:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:10.151652+00:00"
    },
    {
      "arxiv_id": "2505.07664v1",
      "title": "A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ•æ·è½¯ä»¶å¼€å‘å²è¯—è´¨é‡è¯„ä¼°ä¸­ä½œç”¨çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Werner Geyer",
        "Jessica He",
        "Daita Sarkar",
        "Michelle Brachman",
        "Chris Hammond",
        "Jennifer Heins",
        "Zahra Ashktorab",
        "Carlos Rosemberg",
        "Charlie Hill"
      ],
      "abstract": "The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices.",
      "tldr_zh": "æœ¬ç ”ç©¶é€šè¿‡ä¸€é¡¹è¡Œä¸šæ¡ˆä¾‹ç ”ç©¶ï¼Œè°ƒæŸ¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨æ•æ·è½¯ä»¶å¼€å‘ä¸­å¯¹å²è¯— (Epics) è´¨é‡è¯„ä¼°çš„ä½œç”¨ã€‚ç ”ç©¶èƒŒæ™¯åœ¨äºè§£å†³ç°å®ä¸­ Epics å®šä¹‰ä¸æ¸…å¯¼è‡´çš„äº¤ä»˜å»¶è¿Ÿå’Œæˆæœ¬è¶…æ”¯ï¼Œå¹¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¯„ä¼°å…¶è´¨é‡æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡å¯¹ 17 åäº§å“ç»ç† (Product Managers) çš„ç”¨æˆ·ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºå—è®¿è€…å¯¹ AI è¯„ä¼°ç»“æœå…·æœ‰è¾ƒé«˜çš„æ»¡æ„åº¦ï¼Œå¹¶éªŒè¯äº†å°† LLM é›†æˆåˆ°å·¥ä½œæµä¸­ä»¥æå‡å²è¯—è´¨é‡çš„å¯è¡Œæ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†æ•æ·å²è¯—æ˜¯ AI è¯„ä¼°çš„ä¸€ä¸ªæ–°å…´ä¸”æœ‰æ•ˆçš„åº”ç”¨åœºæ™¯ï¼Œè¿˜è¯¦ç»†åˆ†æäº†ç›¸å…³çš„æŒ‘æˆ˜ã€å±€é™æ€§åŠé‡‡çº³éšœç¢ï¼Œä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æœªæ¥ä¼˜åŒ–æ•æ·å¼€å‘å®è·µæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07664v1",
      "published_date": "2025-05-12 15:31:16 UTC",
      "updated_date": "2025-05-12 15:31:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:30.885392+00:00"
    },
    {
      "arxiv_id": "2508.00827v4",
      "title": "Legal Knowledge Graph Foundations, Part I: URI-Addressable Abstract Works (LRMoo F1 to schema.org)",
      "title_zh": "æ³•å¾‹çŸ¥è¯†å›¾è°±åŸºç¡€ï¼ˆä¸€ï¼‰ï¼šå¯ URI å¯»å€çš„æŠ½è±¡ä½œå“ï¼ˆä» LRMoo F1 åˆ° schema.orgï¼‰",
      "authors": [
        "Hudson de Martim"
      ],
      "abstract": "Building upon a formal, event-centric model for the diachronic evolution of legal norms grounded in the IFLA Library Reference Model (LRMoo), this paper addresses the essential first step of publishing this model's foundational entity-the abstract legal Work (F1)-on the Semantic Web. We propose a detailed, property-by-property mapping of the LRMoo F1 Work to the widely adopted schema.org/Legislation vocabulary. Using Brazilian federal legislation from the Normas.leg.br portal as a practical case study, we demonstrate how to create interoperable, machine-readable descriptions via JSON-LD, focusing on stable URN identifiers, core metadata, and norm relationships. This structured mapping establishes a stable, URI-addressable anchor for each legal norm, creating a verifiable \"ground truth\". It provides the essential, interoperable foundation upon which subsequent layers of the model, such as temporal versions (Expressions) and internal components, can be built. By bridging formal ontology with web-native standards, this work paves the way for building deterministic and reliable Legal Knowledge Graphs (LKGs), overcoming the limitations of purely probabilistic models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ³•å¾‹çŸ¥è¯†å›¾è°±(Legal Knowledge Graphs, LKGs)çš„åŸºç¡€æ„å»ºï¼Œæ—¨åœ¨é€šè¿‡ Semantic Web æŠ€æœ¯è§£å†³æ³•å¾‹è§„èŒƒåœ¨ Web ä¸Šçš„æ•°å­—åŒ–è¡¨è¾¾é—®é¢˜ã€‚åŸºäº IFLA Library Reference Model (LRMoo) çš„äº‹ä»¶ä¸­å¿ƒæ¨¡å‹ï¼Œæœ¬æ–‡é‡ç‚¹ç ”ç©¶äº†å¦‚ä½•å°†â€œæŠ½è±¡æ³•å¾‹ä½œå“(F1 Work)â€è¿™ä¸€æ ¸å¿ƒå®ä½“è¿›è¡Œå‘å¸ƒï¼Œå¹¶æå‡ºäº†ä» LRMoo F1 åˆ° schema.org/Legislation è¯æ±‡è¡¨çš„è¯¦ç»†å±æ€§æ˜ å°„æ–¹æ¡ˆã€‚é€šè¿‡å¯¹å·´è¥¿è”é‚¦ç«‹æ³•é—¨æˆ· Normas.leg.br çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè®ºæ–‡å±•ç¤ºäº†åˆ©ç”¨ JSON-LD ç”ŸæˆåŒ…å«ç¨³å®š URN æ ‡è¯†ç¬¦ã€æ ¸å¿ƒå…ƒæ•°æ®åŠè§„èŒƒå…³ç³»çš„å¯äº’æ“ä½œã€æœºå™¨å¯è¯»æè¿°ã€‚è¿™ç§æ˜ å°„æ–¹æ³•ä¸ºæ³•å¾‹è§„èŒƒå»ºç«‹äº†ç¨³å®šçš„ URI å¯»å€é”šç‚¹ï¼Œä¸ºåç»­çš„æ—¶é—´ç‰ˆæœ¬(Expressions)åŠå†…éƒ¨ç»„ä»¶æ„å»ºæä¾›äº†å¯éªŒè¯çš„â€œäº‹å®æ¥æº(ground truth)â€ã€‚è¯¥å·¥ä½œæœ‰æ•ˆå¼¥åˆäº†æ­£å¼æœ¬ä½“ä¸ Web åŸç”Ÿæ ‡å‡†ä¹‹é—´çš„å·®è·ï¼Œä¸ºæ„å»ºç¡®å®šæ€§ä¸”å¯é çš„ LKGs å¥ å®šäº†äº’æ“ä½œæ€§åŸºç¡€ï¼Œä»è€Œå…‹æœäº†çº¯æ¦‚ç‡æ¨¡å‹çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.DL",
      "comment": "This version formalizes the LRMoo event-centric model for the legal lifecycle (enactment, publication). This provides a more precise and ontologically-grounded mapping to Schema.org, with a clearer case study and improved diagrams",
      "pdf_url": "https://arxiv.org/pdf/2508.00827v4",
      "published_date": "2025-05-12 15:11:11 UTC",
      "updated_date": "2025-10-02 15:15:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:51:55.633640+00:00"
    },
    {
      "arxiv_id": "2505.07637v1",
      "title": "Chronocept: Instilling a Sense of Time in Machines",
      "title_zh": "Chronoceptï¼šèµ‹äºˆæœºå™¨æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›",
      "authors": [
        "Krish Goel",
        "Sanskar Pandey",
        "KS Mahadevan",
        "Harsh Kumar",
        "Vishesh Khadaria"
      ],
      "abstract": "Human cognition is deeply intertwined with a sense of time, known as Chronoception. This sense allows us to judge how long facts remain valid and when knowledge becomes outdated. Despite progress in vision, language, and motor control, AI still struggles to reason about temporal validity. We introduce Chronocept, the first benchmark to model temporal validity as a continuous probability distribution over time. Using skew-normal curves fitted along semantically decomposed temporal axes, Chronocept captures nuanced patterns of emergence, decay, and peak relevance. It includes two datasets: Benchmark I (atomic facts) and Benchmark II (multi-sentence passages). Annotations show strong inter-annotator agreement (84% and 89%). Our baselines predict curve parameters - location, scale, and skewness - enabling interpretable, generalizable learning and outperforming classification-based approaches. Chronocept fills a foundational gap in AI's temporal reasoning, supporting applications in knowledge grounding, fact-checking, retrieval-augmented generation (RAG), and proactive agents. Code and data are publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†Chronoceptï¼Œè¿™æ˜¯é¦–ä¸ªå°†æ—¶é—´æœ‰æ•ˆæ€§(temporal validity)å»ºæ¨¡ä¸ºæ—¶é—´è½´ä¸Šè¿ç»­æ¦‚ç‡åˆ†å¸ƒçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½åœ¨åˆ¤æ–­äº‹å®ä½•æ—¶å¤±æ•ˆæ–¹é¢çš„éš¾é¢˜ã€‚Chronocepté€šè¿‡åœ¨è¯­ä¹‰åˆ†è§£çš„æ—¶é—´è½´ä¸Šæ‹Ÿåˆåæ€æ­£æ€æ›²çº¿(skew-normal curves)ï¼Œèƒ½å¤Ÿç²¾å‡†æ•æ‰ä¿¡æ¯ä»æ¶Œç°ã€è¾¾å³°åˆ°è¡°å‡çš„åŠ¨æ€å˜åŒ–è§„å¾‹ã€‚è¯¥åŸºå‡†åŒ…å«é’ˆå¯¹åŸå­äº‹å®(atomic facts)å’Œå¤šå¥æ®µè½(multi-sentence passages)çš„ä¸¤ä¸ªæ•°æ®é›†ï¼Œå…¶å®éªŒç»“æœæ˜¾ç¤ºäººå·¥æ ‡æ³¨çš„ä¸€è‡´æ€§æé«˜ã€‚ç ”ç©¶æå‡ºçš„åŸºå‡†æ¨¡å‹é€šè¿‡é¢„æµ‹æ›²çº¿çš„å®šä½ã€é‡çº²å’Œååº¦å‚æ•°ï¼Œå®ç°äº†æ¯”ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•æ›´å…·å¯è§£é‡Šæ€§ä¸æ³›åŒ–èƒ½åŠ›çš„å­¦ä¹ æ•ˆæœã€‚Chronoceptå¡«è¡¥äº†AIåœ¨æ—¶é—´æ¨ç†é¢†åŸŸçš„ç©ºç™½ï¼Œä¸ºçŸ¥è¯†å¯¹é½ã€äº‹å®æ ¸æŸ¥ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åŠä¸»åŠ¨æ™ºèƒ½ä½“(proactive agents)ç­‰ä»»åŠ¡å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 8 figures, 18 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.07637v1",
      "published_date": "2025-05-12 15:07:32 UTC",
      "updated_date": "2025-05-12 15:07:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:07.790247+00:00"
    },
    {
      "arxiv_id": "2505.07634v3",
      "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
      "title_zh": "Neural Brainï¼šå—ç¥ç»ç§‘å­¦å¯å‘çš„å…·èº«æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Jian Liu",
        "Xiongtao Shi",
        "Thai Duy Nguyen",
        "Haitian Zhang",
        "Tianxiang Zhang",
        "Wei Sun",
        "Yanjie Li",
        "Athanasios V. Vasilakos",
        "Giovanni Iacca",
        "Arshad Ali Khan",
        "Arvind Kumar",
        "Jae Won Cho",
        "Ajmal Mian",
        "Lihua Xie",
        "Erik Cambria",
        "Lin Wang"
      ],
      "abstract": "The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Neural Brainï¼Œè¿™æ˜¯ä¸€ä¸ªå—ç¥ç»ç§‘å­¦å¯å‘çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºEmbodied Agentsæä¾›ç±»äººé€‚åº”æ€§ï¼Œä»¥å…‹æœå½“å‰AIç³»ç»Ÿå› ç¼ºä¹ç‰©ç†äº¤äº’è€Œå­˜åœ¨çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆMultimodal active sensingã€æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨å¾ªç¯ã€åŸºäºNeuroplasticityçš„å­˜å‚¨æ›´æ–°æœºåˆ¶ä»¥åŠNeuromorphicè½¯ç¡¬ä»¶ååŒä¼˜åŒ–ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•ŒåŠ¨æ€ç¯å¢ƒçš„ä¸­å¿ƒæ™ºèƒ½ç³»ç»Ÿã€‚è®ºæ–‡æ˜ç¡®å®šä¹‰äº†Neural Brainçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œæ—¨åœ¨å¼¥åˆé™æ€AIæ¨¡å‹ä¸ç°å®éƒ¨ç½²æ‰€éœ€çš„åŠ¨æ€é€‚åº”æ€§ä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å›é¡¾ç›¸å…³é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæ·±å…¥åˆ†æäº†ç°æœ‰AIç³»ç»Ÿä¸äººç±»æ™ºèƒ½ï¼ˆHuman intelligenceï¼‰ä¹‹é—´çš„å·®è·ã€‚è¿™ä¸€æ¡†æ¶ä¸ä»…ä¸ºå¼€å‘å…·å¤‡é€šç”¨æ€§å’Œé«˜åº¦è‡ªä¸»æ€§çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œè¿˜ä¸ºå®ç°ç±»äººæ™ºèƒ½æ°´å¹³çš„è‡ªä¸»æ™ºèƒ½ä½“ç»˜åˆ¶äº†æ˜ç¡®çš„å‘å±•è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "51 pages, 17 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.07634v3",
      "published_date": "2025-05-12 15:05:34 UTC",
      "updated_date": "2025-10-06 10:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:40.882877+00:00"
    },
    {
      "arxiv_id": "2505.07917v2",
      "title": "Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation",
      "title_zh": "åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„é«˜æ•ˆä¸”å¯å¤ç°çš„ç”Ÿç‰©åŒ»å­¦é—®ç­”",
      "authors": [
        "Linus Stuhlmann",
        "Michael Alexander Saxer",
        "Jonathan FÃ¼rst"
      ],
      "abstract": "Biomedical question-answering (QA) systems require effective retrieval and generation components to ensure accuracy, efficiency, and scalability. This study systematically examines a Retrieval-Augmented Generation (RAG) system for biomedical QA, evaluating retrieval strategies and response time trade-offs. We first assess state-of-the-art retrieval methods, including BM25, BioBERT, MedCPT, and a hybrid approach, alongside common data stores such as Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents) to measure indexing efficiency, retrieval latency, and retriever performance in the end-to-end RAG system. Based on these insights, we deploy the final RAG system on the full 24M PubMed corpus, comparing different retrievers' impact on overall performance. Evaluations of the retrieval depth show that retrieving 50 documents with BM25 before reranking with MedCPT optimally balances accuracy (0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains stable (82ms), while MedCPT incurs the main computational cost. These results highlight previously not well-known trade-offs in retrieval depth, efficiency, and scalability for biomedical QA. With open-source code, the system is fully reproducible and extensible.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ç”¨äºç”Ÿç‰©åŒ»å­¦é—®ç­”(Question Answering, QA)çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿï¼Œé‡ç‚¹æ¢è®¨äº†æ£€ç´¢ç­–ç•¥ä¸å“åº”æ—¶é—´ä¹‹é—´çš„å¹³è¡¡ã€‚ç ”ç©¶äººå‘˜åœ¨PubMedå¤§è§„æ¨¡è¯­æ–™åº“ä¸Šå¯¹æ¯”äº†BM25ã€BioBERTã€MedCPTç­‰æ£€ç´¢æ–¹æ³•ï¼Œä»¥åŠElasticsearchã€FAISSç­‰å­˜å‚¨å¼•æ“åœ¨ç´¢å¼•æ•ˆç‡å’Œæ£€ç´¢å»¶è¿Ÿä¸Šçš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œå…ˆé€šè¿‡BM25æ£€ç´¢50ç¯‡æ–‡æ¡£å†åˆ©ç”¨MedCPTè¿›è¡Œé‡æ’åº(Reranking)çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‡†ç¡®ç‡(0.90)ã€å¬å›ç‡(0.90)ä¸å“åº”æ—¶é—´(1.91s)ä¹‹é—´å–å¾—æœ€ä¼˜å¹³è¡¡ã€‚ç ”ç©¶æ­ç¤ºäº†ç”Ÿç‰©åŒ»å­¦QAåœ¨æ£€ç´¢æ·±åº¦ä¸å¯æ‰©å±•æ€§æ–¹é¢æ­¤å‰æœªè¢«å……åˆ†æ¢è®¨çš„æƒè¡¡å…³ç³»ï¼Œå¹¶æŒ‡å‡ºMedCPTæ˜¯ä¸»è¦çš„è®¡ç®—å¼€é”€æ¥æºã€‚è¯¥ç³»ç»Ÿå·²å¼€æºï¼Œä¸ºç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ£€ç´¢ä¸ç”Ÿæˆä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”å¯å¤ç°çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Minor wording corrections and updated author contact information",
      "pdf_url": "https://arxiv.org/pdf/2505.07917v2",
      "published_date": "2025-05-12 14:51:47 UTC",
      "updated_date": "2026-01-13 17:00:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:10.015052+00:00"
    },
    {
      "arxiv_id": "2505.07621v1",
      "title": "Bang for the Buck: Vector Search on Cloud CPUs",
      "title_zh": "æ€§ä»·æ¯”ä¹‹é€‰ï¼šäº‘ç«¯ CPU ä¸Šçš„å‘é‡æœç´¢",
      "authors": [
        "Leonardo Kuffo",
        "Peter Boncz"
      ],
      "abstract": "Vector databases have emerged as a new type of systems that support efficient querying of high-dimensional vectors. Many of these offer their database as a service in the cloud. However, the variety of available CPUs and the lack of vector search benchmarks across CPUs make it difficult for users to choose one. In this study, we show that CPU microarchitectures available in the cloud perform significantly differently across vector search scenarios. For instance, in an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per second (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the tables turn. However, when looking at the number of queries per dollar (QP$), Graviton3 is the best option for most indexes and quantization settings, even over Graviton4 (Table 1). With this work, we hope to guide users in getting the best \"bang for the buck\" when deploying vector search systems.",
      "tldr_zh": "æœ¬ç ”ç©¶æ·±å…¥åˆ†æäº†äº‘ç«¯ä¸åŒ CPU å¾®æ¶æ„åœ¨å‘é‡æœç´¢ (Vector Search) åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·åœ¨ç¼ºä¹åŸºå‡†æµ‹è¯•çš„æƒ…å†µä¸‹æä¾›é€‰æ‹©ä¾æ®ã€‚ç ”ç©¶å‘ç°ï¼ŒCPU å¾®æ¶æ„åœ¨ä¸åŒç´¢å¼•ç±»å‹ä¸­è¡¨ç°å·®å¼‚å·¨å¤§ï¼Œä¾‹å¦‚åœ¨å¤„ç† float32 å‘é‡çš„ IVF ç´¢å¼•æ—¶ï¼ŒAMD Zen4 çš„æ¯ç§’æŸ¥è¯¢æ•° (QPS) è¾¾åˆ° Intel Sapphire Rapids çš„ä¸‰å€ï¼Œä½†åœ¨ HNSW ç´¢å¼•ä¸­æƒ…å†µåˆ™å‘ç”Ÿåè½¬ã€‚é€šè¿‡å¯¹æ¯”æ¯ç¾å…ƒæŸ¥è¯¢æ•° (QP$) è¿™ä¸€æ€§ä»·æ¯”æŒ‡æ ‡ï¼Œç ”ç©¶æŒ‡å‡º Graviton3 åœ¨å¤§å¤šæ•°ç´¢å¼•å’Œé‡åŒ– (Quantization) è®¾ç½®ä¸‹å‡ä¼˜äºåŒ…æ‹¬ Graviton4 åœ¨å†…çš„å…¶ä»–å¤„ç†å™¨ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç¡¬ä»¶é€‰æ‹©å¯¹å‘é‡æ•°æ®åº“ (Vector Databases) éƒ¨ç½²æˆæœ¬çš„å½±å“ï¼Œä¸ºç”¨æˆ·åœ¨äº‘ç«¯å¯»æ±‚æœ€é«˜æ€§ä»·æ¯”æä¾›äº†å®ç”¨çš„å†³ç­–æŒ‡å—ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "To be published in Proceedings of 21st International Workshop on Data Management on New Hardware (DaMoN '25)",
      "pdf_url": "https://arxiv.org/pdf/2505.07621v1",
      "published_date": "2025-05-12 14:44:21 UTC",
      "updated_date": "2025-05-12 14:44:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:34.891352+00:00"
    },
    {
      "arxiv_id": "2505.07615v2",
      "title": "Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models",
      "title_zh": "è¢«â€œæ‰©æ•£â€çš„è´£ä»»ï¼šç”Ÿæˆå¼æ–‡æœ¬åˆ°éŸ³é¢‘æ‰©æ•£æ¨¡å‹çš„èƒ½è€—åˆ†æ",
      "authors": [
        "Riccardo Passoni",
        "Francesca Ronchini",
        "Luca Comanducci",
        "Romain Serizel",
        "Fabio Antonacci"
      ],
      "abstract": "Text-to-audio models have recently emerged as a powerful technology for generating sound from textual descriptions. However, their high computational demands raise concerns about energy consumption and environmental impact. In this paper, we conduct an analysis of the energy usage of 7 state-of-the-art text-to-audio diffusion-based generative models, evaluating to what extent variations in generation parameters affect energy consumption at inference time. We also aim to identify an optimal balance between audio quality and energy consumption by considering Pareto-optimal solutions across all selected models. Our findings provide insights into the trade-offs between performance and environmental impact, contributing to the development of more efficient generative audio models.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†ç”Ÿæˆå¼ Text-to-audio æ‰©æ•£æ¨¡å‹çš„èƒ½è€—é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨æ¨ç†é˜¶æ®µå¯¹ç¯å¢ƒçš„å½±å“ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¯”äº† 7 ä¸ªæœ€å…ˆè¿›çš„ state-of-the-art æ–‡æœ¬è½¬éŸ³é¢‘æ¨¡å‹ï¼Œç³»ç»Ÿåœ°åˆ†æäº†ç”Ÿæˆå‚æ•°çš„å˜åŒ–å¦‚ä½•å½±å“èƒ½é‡æ¶ˆè€—ã€‚é€šè¿‡å¼•å…¥ Pareto-optimal æ–¹æ¡ˆï¼Œç ”ç©¶æ¢è®¨äº†éŸ³é¢‘è´¨é‡ä¸èƒ½è€—ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ç‚¹ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ¨¡å‹æ€§èƒ½ä¸ç¯å¢ƒå¯æŒç»­æ€§ä¹‹é—´çš„ trade-offs å…³ç³»ï¼Œä¸ºå¼€å‘é«˜æ•ˆéŸ³é¢‘ç”ŸæˆæŠ€æœ¯æä¾›äº†å®è¯æ”¯æŒã€‚è¿™é¡¹å·¥ä½œä¸ºé™ä½æ‰©æ•£æ¨¡å‹çš„è®¡ç®—éœ€æ±‚å’Œç¢³è¶³è¿¹æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at WASPAA 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07615v2",
      "published_date": "2025-05-12 14:36:47 UTC",
      "updated_date": "2025-07-16 17:59:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:53:42.538362+00:00"
    },
    {
      "arxiv_id": "2505.07610v2",
      "title": "Concept-Level Explainability for Auditing & Steering LLM Responses",
      "title_zh": "ç”¨äºå¤§è¯­è¨€æ¨¡å‹å“åº”å®¡è®¡ä¸è°ƒæ§çš„æ¦‚å¿µçº§å¯è§£é‡Šæ€§",
      "authors": [
        "Kenza Amara",
        "Rita Sevastjanova",
        "Mennatallah El-Assady"
      ],
      "abstract": "As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ConceptXï¼Œä¸€ç§ä¸æ¨¡å‹æ— å…³çš„æ¦‚å¿µçº§å¯è§£é‡Šæ€§ (Concept-level explainability) æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«æç¤ºè¯ä¸­çš„è¯­ä¹‰ä¸°å¯Œæ ‡è®°å¹¶æ ¹æ®è¾“å‡ºçš„è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†é…é‡è¦æ€§ï¼Œæ¥å®¡è®¡å’Œå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å“åº”ã€‚ä¸åŒäºä¼ ç»Ÿçš„æ ‡è®°çº§å½’å›  (Token-level attribution) æ–¹æ³•ï¼ŒConceptX èƒ½å¤Ÿé€šè¿‡åŸä½æ ‡è®°æ›¿æ¢ (In-place token replacements) ä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œå¹¶æ”¯æŒæ€§åˆ«åè§ç­‰çµæ´»çš„è§£é‡Šç›®æ ‡ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒé€šè¿‡æ­ç¤ºåè§æ¥æºè¿›è¡Œå®¡è®¡ï¼Œè¿˜æ”¯æŒåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¿®æ”¹æç¤ºè¯æ¥æ”¹å˜æƒ…æ„Ÿå€¾å‘æˆ–é™ä½å“åº”çš„å±å®³æ€§ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„ç²¾å‡†è½¬å‘ (Steering)ã€‚åœ¨ä¸‰ç§ LLMs ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒConceptX åœ¨å¿ å®åº¦ (Faithfulness) å’Œäººç±»å¯¹é½ (Human alignment) æ–¹é¢å‡ä¼˜äº TokenSHAP ç­‰æ ‡è®°çº§æ–¹æ³•ã€‚åœ¨è½¬å‘ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å°†æƒ…æ„Ÿè½¬å˜ç”±éšæœºç¼–è¾‘çš„ 0.131 æå‡è‡³ 0.252ï¼Œå¹¶å°†æ”»å‡»æˆåŠŸç‡ (Attack success rates) ä» 0.463 é™ä½åˆ° 0.242ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºå½’å› å’Œæ”¹å†™ç­‰åŸºå‡†æ–¹æ³•ã€‚ConceptX ä¸ºæé«˜ LLMs çš„å®‰å…¨æ€§å’Œå¯¹é½æä¾›äº†ä¸€ç§é€æ˜ä¸”å¿ å®çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åŸºäºå½’å› çš„å¯è§£é‡Šæ€§åœ¨æŒ‡å¯¼æ¨¡å‹è¡Œä¸ºæ–¹é¢çš„å®é™…ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 7 figures, Submission to Neurips 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07610v2",
      "published_date": "2025-05-12 14:31:51 UTC",
      "updated_date": "2025-05-19 14:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:52:45.433131+00:00"
    },
    {
      "arxiv_id": "2505.07608v2",
      "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining",
      "title_zh": "MiMoï¼šé‡Šæ”¾è¯­è¨€æ¨¡å‹æ¨ç†æ½œèƒ½â€”â€”ä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒ",
      "authors": [
        "LLM-Core Xiaomi",
        ":",
        "Bingquan Xia",
        "Bowen Shen",
        "Cici",
        "Dawei Zhu",
        "Di Zhang",
        "Gang Wang",
        "Hailin Zhang",
        "Huaqiu Liu",
        "Jiebao Xiao",
        "Jinhao Dong",
        "Liang Zhao",
        "Peidian Li",
        "Peng Wang",
        "Shihua Yu",
        "Shimao Chen",
        "Weikun Wang",
        "Wenhan Ma",
        "Xiangwei Deng",
        "Yi Huang",
        "Yifan Song",
        "Zihan Jiang",
        "Bowen Ye",
        "Can Cai",
        "Chenhong He",
        "Dong Zhang",
        "Duo Zhang",
        "Guoan Wang",
        "Hao Tian",
        "Haochen Zhao",
        "Heng Qu",
        "Hongshen Xu",
        "Jun Shi",
        "Kainan Bao",
        "Kai Fang",
        "Kang Zhou",
        "Kangyang Zhou",
        "Lei Li",
        "Menghang Zhu",
        "Nuo Chen",
        "Qiantong Wang",
        "Shaohui Liu",
        "Shicheng Li",
        "Shuhao Gu",
        "Shuhuai Ren",
        "Shuo Liu",
        "Sirui Deng",
        "Weiji Zhuang",
        "Weiwei Lv",
        "Wenyu Yang",
        "Xin Zhang",
        "Xing Yong",
        "Xing Zhang",
        "Xingchen Song",
        "Xinzhe Xu",
        "Xu Wang",
        "Yihan Yan",
        "Yu Tu",
        "Yuanyuan Tian",
        "Yudong Wang",
        "Yue Yu",
        "Zhenru Lin",
        "Zhichao Song",
        "Zihao Yue"
      ],
      "abstract": "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†MiMo-7Bï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“æ³¨äºæå‡æ¨ç†èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µçš„ååŒä¼˜åŒ–é‡Šæ”¾æ¨¡å‹æ½œåŠ›ã€‚åœ¨é¢„è®­ç»ƒä¸­ï¼ŒMiMo-7Båœ¨25ä¸‡äº¿ä¸ªTokenä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µæ•°æ®æ··åˆç­–ç•¥å’Œå¤šä»¤ç‰Œé¢„æµ‹(Multi-Token Prediction)ç›®æ ‡ï¼Œæ˜¾è‘—å¢å¼ºäº†åŸºç¡€æ¨¡å‹çš„æ¨ç†æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦ã€‚åè®­ç»ƒé˜¶æ®µåˆ™åˆ©ç”¨13ä¸‡ä¸ªå¯éªŒè¯çš„æ•°å­¦ä¸ç¼–ç¨‹é—®é¢˜è¿›è¡Œå¼ºåŒ–å­¦ä¹ (RL)ï¼Œå¹¶å¼•å…¥åŸºäºæµ‹è¯•éš¾åº¦çš„ä»£ç å¥–åŠ±æ–¹æ¡ˆ(test-difficulty-driven code-reward scheme)æ¥è§£å†³å¥–åŠ±ç¨€ç–é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡æ•°æ®é‡é‡‡æ ·ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMiMo-7B-Baseåœ¨æ¨ç†æ½œåŠ›ä¸Šè¶…è¶Šäº†è®¸å¤š32Bè§„æ¨¡çš„æ¨¡å‹ï¼Œè€Œæœ€ç»ˆçš„MiMo-7B-RLåœ¨æ•°å­¦ã€ä»£ç å’Œé€šç”¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºOpenAI o1-miniã€‚è¯¥é¡¹å·¥ä½œä¸ºæ„å»ºé«˜æ€§èƒ½æ¨ç†æ¨¡å‹æä¾›äº†ä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒå…¨æµç¨‹çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07608v2",
      "published_date": "2025-05-12 14:30:11 UTC",
      "updated_date": "2025-06-05 11:49:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:53:59.971153+00:00"
    },
    {
      "arxiv_id": "2505.07601v1",
      "title": "Characterizing the Investigative Methods of Fictional Detectives with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è™šæ„ä¾¦æ¢è°ƒæŸ¥æ–¹æ³•ç‰¹å¾åˆ»ç”»",
      "authors": [
        "Edirlei Soares de Lima",
        "Marco A. Casanova",
        "Bruno FeijÃ³",
        "Antonio L. Furtado"
      ],
      "abstract": "Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¾¦æ¢å°è¯´åœ¨è®¡ç®—å™äº‹å­¦(Computational Narratology)ä¸­é¢ä¸´çš„å™äº‹ç»“æ„å¤æ‚åŠè§’è‰²åˆ†æå¯æ‰©å±•æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ç³»ç»ŸåŒ–åˆ»ç”»è™šæ„ä¾¦æ¢è°ƒæŸ¥æ–¹æ³•çš„æ–°é€”å¾„ã€‚ç ”ç©¶é‡‡ç”¨å¤šé˜¶æ®µå·¥ä½œæµï¼Œåˆ©ç”¨15ä¸ªä¸åŒçš„LLMså¯¹åŒ…æ‹¬Sherlock Holmeså’ŒHercule Poirotåœ¨å†…çš„ä¸ƒä½ç»å…¸ä¾¦æ¢çš„è°ƒæŸ¥é£æ ¼è¿›è¡Œæå–ã€åˆæˆä¸éªŒè¯ã€‚é€šè¿‡ä¸ç°æœ‰æ–‡å­¦åˆ†æå¯¹æ¯”åŠåå‘è¯†åˆ«æµ‹è¯•ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†91.43%çš„å‡†ç¡®ç‡ï¼Œæœ‰æ•ˆæ•æ‰äº†è§’è‰²çš„ç‹¬ç‰¹è°ƒæŸ¥ç‰¹è´¨ã€‚è¿™é¡¹å·¥ä½œä¸ºæ–‡å­¦è§’è‰²çš„ç³»ç»ŸåŒ–åˆ†ææä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯¹äºäººå·¥æ™ºèƒ½é©±åŠ¨çš„äº¤äº’å¼å™äº‹(Interactive Storytelling)å’Œè‡ªåŠ¨åŒ–å™äº‹ç”Ÿæˆ(Automated Narrative Generation)å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07601v1",
      "published_date": "2025-05-12 14:24:58 UTC",
      "updated_date": "2025-05-12 14:24:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:29.452304+00:00"
    },
    {
      "arxiv_id": "2505.07596v1",
      "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent",
      "title_zh": "é¢å‘é«˜æ•ˆè‡ªé€‚åº”æœç´¢æ™ºèƒ½ä½“çš„å¼ºåŒ–å†…å¤–éƒ¨çŸ¥è¯†ååŒæ¨ç†",
      "authors": [
        "Ziyang Huang",
        "Xiaowei Yuan",
        "Yiming Ju",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿‡ç¨‹ä¸­å› è¿‡åº¦ä¾èµ–å¤–éƒ¨æ£€ç´¢è€Œå¯¼è‡´çš„å†—ä½™æ£€ç´¢ã€çŸ¥è¯†å†²çªåŠæ¨ç†å»¶è¿Ÿç­‰é—®é¢˜ï¼Œæå‡ºäº† IKEAï¼ˆReinforced Internal-External Knowledge Synergistic Reasoning Agentï¼‰æ¡†æ¶ã€‚IKEA æ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè¯†åˆ«è‡ªèº«çŸ¥è¯†è¾¹ç•Œï¼ˆknowledge boundaryï¼‰çš„é«˜æ•ˆè‡ªé€‚åº”æœç´¢æ™ºèƒ½ä½“ï¼Œå®ç°å‚æ•°åŒ–å†…éƒ¨çŸ¥è¯†ä¸æ£€ç´¢åˆ°çš„å¤–éƒ¨çŸ¥è¯†çš„ååŒæ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç§æ–°é¢–çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥å¥–åŠ±å‡½æ•°ï¼ˆknowledge-boundary aware reward functionï¼‰å’Œä¸“é—¨è®¾è®¡çš„è®­ç»ƒæ•°æ®é›†ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±æ¨¡å‹ä»…åœ¨å†…éƒ¨çŸ¥è¯†ä¸è¶³æ—¶æ‰å¯åŠ¨å¤–éƒ¨æœç´¢ã€‚è¿™ç§æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆåˆ©ç”¨å…¶å†…éƒ¨çŸ¥è¯†ï¼Œåœ¨ä¿è¯å›ç­”å‡†ç¡®æ€§çš„åŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘ä¸å¿…è¦çš„æ£€ç´¢æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIKEA åœ¨å¤šé¡¹çŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨å¤§å¹…é™ä½æ£€ç´¢é¢‘ç‡çš„åŒæ—¶å±•ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07596v1",
      "published_date": "2025-05-12 14:21:57 UTC",
      "updated_date": "2025-05-12 14:21:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:04.087988+00:00"
    },
    {
      "arxiv_id": "2505.07591v1",
      "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models",
      "title_zh": "è¯„ä¼°ä¸æå‡å¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„å¤šç»´çº¦æŸæ¡†æ¶",
      "authors": [
        "Junjie Ye",
        "Caishuang Huang",
        "Zhuohan Chen",
        "Wenjie Fu",
        "Chenyuan Yang",
        "Leyi Yang",
        "Yilong Wu",
        "Peng Wang",
        "Meng Zhou",
        "Xiaolong Yang",
        "Tao Gui",
        "Qi Zhang",
        "Zhongchao Shi",
        "Jianping Fan",
        "Xuanjing Huang"
      ],
      "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦çº¦æŸæ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æŒ‡ä»¤éµå¾ª(Instruction Following)èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•ç”±äºæ¨¡æ¿åŒ–çº¦æŸæç¤ºå¯¼è‡´çš„å¤šæ ·æ€§ä¸è¶³å’Œè¯„ä¼°ç²’åº¦å—é™ç­‰é—®é¢˜ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†ä¸‰ç§çº¦æŸæ¨¡å¼ã€å››ä¸ªçº¦æŸç±»åˆ«ä»¥åŠå››ä¸ªéš¾åº¦ç­‰çº§ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºæ­¤æ¡†æ¶å¼€å‘äº†è‡ªåŠ¨åŒ–æŒ‡ä»¤ç”Ÿæˆæµæ°´çº¿ï¼Œé€šè¿‡çº¦æŸæ‰©å±•ã€å†²çªæ£€æµ‹å’ŒæŒ‡ä»¤é‡å†™ï¼Œæ„å»ºäº†1,200ä¸ªå¯ä»£ç éªŒè¯çš„æµ‹è¯•æ ·æœ¬ã€‚å¯¹19ä¸ªä¸»æµLLMsçš„è¯„ä¼°æ­ç¤ºäº†æ€§èƒ½éšçº¦æŸéš¾åº¦å¢åŠ è€Œå¤§å¹…ä¸‹æ»‘çš„è¶‹åŠ¿ï¼Œå¹³å‡å¾—åˆ†ä»Level Içš„77.67%é™è‡³Level IVçš„32.96%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡ç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ï¼Œåœ¨ä¸æŸå¤±é€šç”¨æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚åˆ†æè¡¨æ˜ï¼Œæ€§èƒ½æå‡çš„å…³é”®åœ¨äºæ³¨æ„åŠ›æ¨¡å—(Attention Modules)å‚æ•°çš„è°ƒæ•´ä¼˜åŒ–äº†å¯¹çº¦æŸçš„è¯†åˆ«ä¸æ‰§è¡Œã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07591v1",
      "published_date": "2025-05-12 14:16:55 UTC",
      "updated_date": "2025-05-12 14:16:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:04.556736+00:00"
    },
    {
      "arxiv_id": "2505.13484v1",
      "title": "Evaluating Large Language Models for Real-World Engineering Tasks",
      "title_zh": "é¢å‘ç°å®å·¥ç¨‹ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Rene Heesch",
        "Sebastian Eilermann",
        "Alexander Windmann",
        "Alexander Diedrich",
        "Philipp Rosenthal",
        "Oliver Niggemann"
      ],
      "abstract": "Large Language Models (LLMs) are transformative not only for daily activities but also for engineering tasks. However, current evaluations of LLMs in engineering exhibit two critical shortcomings: (i) the reliance on simplified use cases, often adapted from examination materials where correctness is easily verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture critical engineering competencies. Consequently, the assessment of LLMs on complex, real-world engineering problems remains largely unexplored. This paper addresses this gap by introducing a curated database comprising over 100 questions derived from authentic, production-oriented engineering scenarios, systematically designed to cover core competencies such as product design, prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art LLMs, including both cloud-based and locally hosted instances, to systematically investigate their performance on complex engineering tasks. Our results show that LLMs demonstrate strengths in basic temporal and structural reasoning but struggle significantly with abstract reasoning, formal modeling, and context-sensitive engineering logic.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡º Large Language Models (LLMs) åœ¨å·¥ç¨‹é¢†åŸŸçš„è¯„ä¼°æ™®éå­˜åœ¨ä¾èµ–ç®€åŒ–æ¡ˆä¾‹å’Œç¼ºä¹å…³é”®å·¥ç¨‹èƒ½åŠ›æ•æ‰çš„é—®é¢˜ï¼Œå¯¼è‡´å…¶åœ¨å¤„ç†çœŸå®å¤æ‚å·¥ç¨‹é—®é¢˜æ—¶çš„è¡¨ç°å°šä¸æ˜ç¡®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å« 100 å¤šä¸ªæºè‡ªçœŸå®ç”Ÿäº§å¯¼å‘å·¥ç¨‹åœºæ™¯é—®é¢˜çš„æ•°æ®åº“ï¼Œç³»ç»Ÿæ¶µç›–äº† product designã€prognosis å’Œ diagnosis ç­‰æ ¸å¿ƒå·¥ç¨‹èƒ½åŠ›ã€‚é€šè¿‡å¯¹å››ç§æœ€å…ˆè¿›çš„äº‘ç«¯åŠæœ¬åœ°éƒ¨ç½² LLMs è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å®é™…æ•ˆèƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ LLMs åœ¨åŸºç¡€çš„ temporal reasoning å’Œ structural reasoning æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œä½†åœ¨ abstract reasoningã€formal modeling ä»¥åŠ context-sensitive engineering logic æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£ LLMs åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„åº”ç”¨ç“¶é¢ˆæä¾›äº†é‡è¦ä¾æ®ï¼Œå¹¶ä¸ºæœªæ¥å·¥ç¨‹ä¸“ç”¨æ¨¡å‹çš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13484v1",
      "published_date": "2025-05-12 14:05:23 UTC",
      "updated_date": "2025-05-12 14:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:21.287790+00:00"
    },
    {
      "arxiv_id": "2505.07581v3",
      "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models",
      "title_zh": "YuLan-OneSimï¼šè¿ˆå‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä»£ç¤¾ä¼šæ¨¡æ‹Ÿå™¨",
      "authors": [
        "Lei Wang",
        "Heyang Gao",
        "Xiaohe Bo",
        "Xu Chen",
        "Ji-Rong Wen"
      ],
      "abstract": "Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†YuLan-OneSimï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºçš„æ–°å‹ç¤¾äº¤æ¨¡æ‹Ÿå™¨ã€‚è¯¥ç³»ç»Ÿå®ç°äº†æ— ä»£ç åœºæ™¯æ„å»º(Code-free scenario construction)ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è‡ªåŠ¨ç”Ÿæˆæ¨¡æ‹Ÿä»£ç ï¼Œæ˜¾è‘—é™ä½äº†ç¤¾äº¤æ¨¡æ‹Ÿçš„ç¼–ç¨‹é—¨æ§›ã€‚YuLan-OneSimé¢„ç½®äº†æ¶µç›–ç»æµå­¦ã€ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦ç­‰8ä¸ªé¢†åŸŸçš„50ä¸ªé»˜è®¤åœºæ™¯ï¼Œå¹¶æ”¯æŒè¿›åŒ–æ¨¡æ‹Ÿ(Evolvable simulation)ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨åé¦ˆå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æŒç»­ä¼˜åŒ–æ¨¡æ‹Ÿè´¨é‡ã€‚å‡­å€Ÿåˆ†å¸ƒå¼æ¶æ„(distributed simulation architecture)ï¼Œè¯¥å¹³å°å¯æ”¯æŒå¤šè¾¾10ä¸‡ä¸ªæ™ºèƒ½ä½“çš„å¤§è§„æ¨¡å¹¶è¡Œæ¨¡æ‹Ÿï¼Œç¡®ä¿äº†æ¨¡æ‹Ÿç»“æœçš„å¯é æ€§ä¸ç¨³å®šæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘çš„AIç¤¾ä¼šç ”ç©¶å‘˜(AI social researcher)èƒ½å¤Ÿè‡ªåŠ¨å®Œæˆä»è¯¾é¢˜åˆ†æã€ç¯å¢ƒæ­å»ºåˆ°æŠ¥å‘Šç”Ÿæˆçš„å…¨æµç¨‹ç ”ç©¶é—­ç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYuLan-OneSimåœ¨åœºæ™¯ç”Ÿæˆçš„å‡†ç¡®æ€§ã€æ¨¡æ‹Ÿæ•ˆç‡åŠå¯æ‰©å±•æ€§æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07581v3",
      "published_date": "2025-05-12 14:05:17 UTC",
      "updated_date": "2025-08-26 08:03:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:25.199190+00:00"
    },
    {
      "arxiv_id": "2505.07576v1",
      "title": "Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study",
      "title_zh": "åŠå¯¼ä½“åˆ¶é€ ä¸­ç°ä»£è§†è§‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„è¯„ä¼°ï¼šä¸€é¡¹å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Manuel Barusco",
        "Francesco Borsatti",
        "Youssef Ben Khalifa",
        "Davide Dalle Pezze",
        "Gian Antonio Susto"
      ],
      "abstract": "Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŠå¯¼ä½“åˆ¶é€ è¿™ä¸€å¤æ‚å¤šé˜¶æ®µè¿‡ç¨‹ä¸­çš„è‡ªåŠ¨è§†è§‰æ£€æµ‹é—®é¢˜ï¼ŒæŒ‡å‡ºåˆ©ç”¨æ‰«æç”µå­æ˜¾å¾®é•œ (Scanning Electron Microscope, SEM) å›¾åƒè¿›è¡Œæ£€æµ‹å¯¹é™ä½æˆæœ¬è‡³å…³é‡è¦ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶è¿‡åº¦ä¾èµ–ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸”éœ€å¤§é‡æ ‡æ³¨æ ·æœ¬çš„å±€é™æ€§ï¼Œæœ¬æ–‡èšç„¦äºæ–°å…´çš„è§†è§‰å¼‚å¸¸æ£€æµ‹ (Visual Anomaly Detection, VAD) é¢†åŸŸï¼Œå¼ºè°ƒå…¶æ— ç›‘ç£å­¦ä¹  (unsupervised learning) çš„ä¼˜åŠ¿ï¼Œæ—¢èƒ½é¿å…é«˜æ˜‚çš„ç¼ºé™·æ”¶é›†æˆæœ¬ï¼Œåˆèƒ½æä¾›é¢„æµ‹è§£é‡Šã€‚ç ”ç©¶äººå‘˜é€šè¿‡åˆ©ç”¨ MIIC æ•°æ®é›†ï¼Œé¦–æ¬¡å¼•å…¥äº†é’ˆå¯¹åŠå¯¼ä½“é¢†åŸŸçš„ VAD åŸºå‡† (benchmark) å¹¶è¿›è¡Œäº†å¯¹æ¯”ç ”ç©¶ã€‚å®éªŒç»“æœè¯æ˜äº†ç°ä»£ VAD æ–¹æ³•åœ¨å¤„ç†åŠå¯¼ä½“åˆ¶é€ ç¼ºé™·æ£€æµ‹ä»»åŠ¡ä¸­çš„æ˜¾è‘—æˆæ•ˆã€‚è¯¥é¡¹å·¥ä½œä¸ºåŠå¯¼ä½“å·¥ä¸šçš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„å’Œè¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07576v1",
      "published_date": "2025-05-12 13:56:59 UTC",
      "updated_date": "2025-05-12 13:56:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:25.044506+00:00"
    },
    {
      "arxiv_id": "2505.07573v1",
      "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework",
      "title_zh": "é²æ£’çš„è‚¾è„å¼‚å¸¸åˆ†å‰²ï¼šåŸºäºäººå·¥æ™ºèƒ½æ¡†æ¶çš„éªŒè¯ç ”ç©¶",
      "authors": [
        "Sarah de Boer",
        "Hartmut HÃ¤ntze",
        "Kiran Vaidhya Venkadesh",
        "Myrthe A. D. Buser",
        "Gabriel E. Humpire Mamani",
        "Lina Xu",
        "Lisa C. Adams",
        "Jawed Nawabi",
        "Keno K. Bressem",
        "Bram van Ginneken",
        "Mathias Prokop",
        "Alessa Hering"
      ],
      "abstract": "Kidney abnormality segmentation has important potential to enhance the clinical workflow, especially in settings requiring quantitative assessments. Kidney volume could serve as an important biomarker for renal diseases, with changes in volume correlating directly with kidney function. Currently, clinical practice often relies on subjective visual assessment for evaluating kidney size and abnormalities, including tumors and cysts, which are typically staged based on diameter, volume, and anatomical location. To support a more objective and reproducible approach, this research aims to develop a robust, thoroughly validated kidney abnormality segmentation algorithm, made publicly available for clinical and research use. We employ publicly available training datasets and leverage the state-of-the-art medical image segmentation framework nnU-Net. Validation is conducted using both proprietary and public test datasets, with segmentation performance quantified by Dice coefficient and the 95th percentile Hausdorff distance. Furthermore, we analyze robustness across subgroups based on patient sex, age, CT contrast phases, and tumor histologic subtypes. Our findings demonstrate that our segmentation algorithm, trained exclusively on publicly available data, generalizes effectively to external test sets and outperforms existing state-of-the-art models across all tested datasets. Subgroup analyses reveal consistent high performance, indicating strong robustness and reliability. The developed algorithm and associated code are publicly accessible at https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç¨³å¥ä¸”ç»è¿‡å……åˆ†éªŒè¯çš„è‚¾è„å¼‚å¸¸åˆ†å‰²(Kidney Abnormality Segmentation)ç®—æ³•ï¼Œä»¥è§£å†³ä¸´åºŠä¸­è¿‡åº¦ä¾èµ–ä¸»è§‚è¯„ä¼°ã€ç¼ºä¹å®šé‡ç”Ÿç‰©æ ‡å¿—ç‰©(biomarkers)çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å…¬å¼€è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶åŸºäºæœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶nnU-Netæ„å»ºæ¨¡å‹ã€‚éªŒè¯è¿‡ç¨‹ç»“åˆäº†ç§æœ‰å’Œå…¬å…±æµ‹è¯•é›†ï¼Œé€šè¿‡Diceç³»æ•°(Dice coefficient)å’Œç¬¬95ç™¾åˆ†ä½è±ªæ–¯å¤šå¤«è·ç¦»(Hausdorff distance)å¯¹æ€§èƒ½è¿›è¡Œé‡åŒ–ï¼Œå¹¶æ·±å…¥åˆ†æäº†ç®—æ³•åœ¨ä¸åŒæ€§åˆ«ã€å¹´é¾„ã€CTå¢å¼ºç›¸ä½åŠè‚¿ç˜¤ç»„ç»‡å­¦äºšå‹ä¸­çš„é²æ£’æ€§ã€‚å®éªŒå‘ç°ï¼Œä»…åœ¨å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒçš„ç®—æ³•èƒ½æœ‰æ•ˆæ³›åŒ–è‡³å¤–éƒ¨æµ‹è¯•é›†ï¼Œä¸”æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚äºšç»„åˆ†æè¯å®è¯¥æ¨¡å‹åœ¨å¤šç§æ¡ä»¶ä¸‹å‡èƒ½ä¿æŒä¸€è‡´çš„é«˜æ€§èƒ½ï¼Œå±•ç°å‡ºå“è¶Šçš„å¯é æ€§ï¼Œç›®å‰è¯¥ç®—æ³•åŠä»£ç å·²å…¬å¼€ä¾›ä¸´åºŠå’Œç ”ç©¶ä½¿ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07573v1",
      "published_date": "2025-05-12 13:53:19 UTC",
      "updated_date": "2025-05-12 13:53:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:23.649568+00:00"
    },
    {
      "arxiv_id": "2505.07911v1",
      "title": "Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review",
      "title_zh": "è´å¶æ–¯æ¨æ–­ä¸å¼ºåŒ–å­¦ä¹ åœ¨æ™ºèƒ½ä½“å†³ç­–ä¸­çš„ç»“åˆï¼šç»¼è¿°",
      "authors": [
        "Chengmin Zhou",
        "Ville Kyrki",
        "Pasi FrÃ¤nti",
        "Laura Ruotsalainen"
      ],
      "abstract": "Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†è´å¶æ–¯æ¨ç†(Bayesian Inference)ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨æ™ºèƒ½ä½“å†³ç­–ä¸­çš„ç»“åˆåº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé»‘ç›’ç¥ç»ç½‘ç»œåœ¨æ•°æ®æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›ã€å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„ä¸è¶³ã€‚æ–‡ç« é¦–å…ˆæ¢³ç†äº†åŒ…æ‹¬å˜åˆ†æ¨ç†(Variational Inference)ã€è´å¶æ–¯ä¼˜åŒ–(Bayesian Optimization)å’Œæ·±åº¦è´å¶æ–¯å­¦ä¹ (Bayesian Deep Learning)åœ¨å†…çš„å¤šç§æ ¸å¿ƒè´å¶æ–¯æ–¹æ³•ã€‚éšåï¼Œè®ºæ–‡è¯¦ç»†åˆ†æäº†è¿™äº›æ–¹æ³•ä¸åŸºäºæ¨¡å‹ã€æ— æ¨¡å‹åŠé€†å‘å¼ºåŒ–å­¦ä¹ (Inverse RL)çš„ç»å…¸ä¸å‰æ²¿ç»“åˆæ–¹å¼ï¼Œå¹¶åŸºäºå…³é”®æ€§èƒ½æŒ‡æ ‡è¿›è¡Œäº†å¯¹æ¯”ã€‚ç ”ç©¶è¿˜æ·±å…¥è®¨è®ºäº†åœ¨å¥–åŠ±æœªçŸ¥ã€éƒ¨åˆ†å¯è§‚æµ‹(Partial-observability)å’Œå¤šæ™ºèƒ½ä½“(Multi-agent)ç­‰å…­ç§å¤æ‚å¼ºåŒ–å­¦ä¹ å˜ä½“ä¸­è´å¶æ–¯æŠ€æœ¯çš„åº”ç”¨ã€‚é€šè¿‡æ€»ç»“è´å¶æ–¯æ¨ç†åœ¨æ•°æ®æ”¶é›†ã€å¤„ç†å’Œç­–ç•¥å­¦ä¹ é˜¶æ®µçš„å…·ä½“ä½œç”¨ï¼Œè¯¥æ–‡ä¸ºå¼€å‘æ›´å…·é²æ£’æ€§å’Œå¯è§£é‡Šæ€§çš„æ™ºèƒ½ä½“å†³ç­–ç­–ç•¥æä¾›äº†ç³»ç»Ÿæ€§çš„ç†è®ºæ¡†æ¶ä¸æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07911v1",
      "published_date": "2025-05-12 13:34:50 UTC",
      "updated_date": "2025-05-12 13:34:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:37.925552+00:00"
    },
    {
      "arxiv_id": "2505.07553v1",
      "title": "Towards Requirements Engineering for RAG Systems",
      "title_zh": "é¢å‘ RAG ç³»ç»Ÿçš„éœ€æ±‚å·¥ç¨‹",
      "authors": [
        "Tor Sporsem",
        "Rasmus Ulfsnes"
      ],
      "abstract": "This short paper explores how a maritime company develops and integrates large-language models (LLM). Specifically by looking at the requirements engineering for Retrieval Augmented Generation (RAG) systems in expert settings. Through a case study at a maritime service provider, we demonstrate how data scientists face a fundamental tension between user expectations of AI perfection and the correctness of the generated outputs. Our findings reveal that data scientists must identify context-specific \"retrieval requirements\" through iterative experimentation together with users because they are the ones who can determine correctness. We present an empirical process model describing how data scientists practically elicited these \"retrieval requirements\" and managed system limitations. This work advances software engineering knowledge by providing insights into the specialized requirements engineering processes for implementing RAG systems in complex domain-specific applications.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»¥ä¸€å®¶æµ·äº‹æœåŠ¡æä¾›å•†ä¸ºæ¡ˆä¾‹ï¼Œæ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLM)é›†æˆè¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹ä¸“å®¶ç¯å¢ƒä¸‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿè¿›è¡Œéœ€æ±‚å·¥ç¨‹(Requirements Engineering)çš„å®è·µæ–¹æ³•ã€‚ç ”ç©¶æ­ç¤ºäº†æ•°æ®ç§‘å­¦å®¶åœ¨ç”¨æˆ·å¯¹AIè¡¨ç°çš„å®Œç¾é¢„æœŸä¸ç³»ç»Ÿç”Ÿæˆè¾“å‡ºçš„å®é™…æ­£ç¡®æ€§ä¹‹é—´é¢ä¸´çš„æ ¹æœ¬å¼ åŠ›ã€‚è°ƒæŸ¥å‘ç°ï¼Œç”±äºåªæœ‰ç»ˆç«¯ç”¨æˆ·èƒ½åˆ¤å®šç»“æœçš„æ­£ç¡®æ€§ï¼Œæ•°æ®ç§‘å­¦å®¶å¿…é¡»é€šè¿‡ä¸ç”¨æˆ·åä½œçš„è¿­ä»£å®éªŒæ¥è¯†åˆ«ç‰¹å®šåœºæ™¯ä¸‹çš„â€œæ£€ç´¢éœ€æ±‚â€(Retrieval Requirements)ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ä¸ªå®è¯è¿‡ç¨‹æ¨¡å‹ï¼Œè¯¦ç»†æè¿°äº†æ•°æ®ç§‘å­¦å®¶å¦‚ä½•å®é™…è·å–è¿™äº›å…³é”®éœ€æ±‚å¹¶æœ‰æ•ˆç®¡ç†ç³»ç»Ÿçš„å±€é™æ€§ã€‚è¯¥å·¥ä½œä¸ºå¤æ‚é¢†åŸŸç‰¹å®šåº”ç”¨ä¸­RAGç³»ç»Ÿçš„éœ€æ±‚å·¥ç¨‹æµç¨‹æä¾›äº†æ·±åº¦è§è§£ï¼Œæ¨åŠ¨äº†è½¯ä»¶å·¥ç¨‹åœ¨è¿™ä¸€æ–°å…´é¢†åŸŸçš„çŸ¥è¯†å‘å±•ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to EASE 2025, 17-20 June, Istanbul, Turkey",
      "pdf_url": "https://arxiv.org/pdf/2505.07553v1",
      "published_date": "2025-05-12 13:30:44 UTC",
      "updated_date": "2025-05-12 13:30:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:54:42.786534+00:00"
    },
    {
      "arxiv_id": "2505.07552v2",
      "title": "Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies",
      "title_zh": "è¯¾å ‚è¡Œä¸ºç ”ç©¶ä¸­åŸºäºç§»åŠ¨å¼çœ¼åŠ¨è¿½è¸ªçš„è‡ªåŠ¨åŒ–è§†è§‰æ³¨æ„åŠ›æ£€æµ‹",
      "authors": [
        "Efe Bozkir",
        "Christian Kosel",
        "Tina Seidel",
        "Enkelejda Kasneci"
      ],
      "abstract": "Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯¾å ‚è¡Œä¸ºç ”ç©¶ä¸­æ•™å¸ˆè§†è§‰æ³¨æ„åŠ›åˆ†æä¾èµ–å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„å¤„ç†æµç¨‹ï¼Œåˆ©ç”¨ç§»åŠ¨çœ¼åŠ¨è¿½è¸ª(Mobile Eye Tracking)æŠ€æœ¯ç»“åˆå…ˆè¿›çš„äººè„¸æ£€æµ‹(Face Detection)å’Œäººè„¸è¯†åˆ«(Face Recognition)ç‰¹å¾åµŒå…¥ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿ç§»å­¦ä¹ (Transfer Learning)è®­ç»ƒç‰¹å®šè¯¾å ‚èƒŒæ™¯ä¸‹çš„äººè„¸è¯†åˆ«æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸æ•™å¸ˆçš„è§†çº¿(Gaze)æ•°æ®ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹æ•™å¸ˆå…³æ³¨å­¦ç”Ÿçš„è‡ªåŠ¨è¯†åˆ«ã€‚å®éªŒåœ¨å››ç§ä¸åŒç±»å‹çš„è¯¾å ‚ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿåœ¨å„ç§è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå…¶ä¸­åœ¨Uå‹å¸ƒå±€å’Œå°å‹è¯¾å ‚ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°0.7å’Œ0.9å·¦å³ã€‚ç”±äºè¯¥æ–¹æ³•æ— éœ€æµ·é‡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ä¸”å…·å¤‡éä¾µå…¥æ€§ï¼Œå®ƒä¸ºåˆ†ææ•™å¸ˆæ³¨æ„åŠ›åˆ†å¸ƒæä¾›äº†é«˜æ•ˆå·¥å…·ï¼Œæœ‰åŠ©äºä¼˜åŒ–æ•™å­¦ç­–ç•¥ã€æå‡è¯¾å ‚ç®¡ç†è´¨é‡å¹¶ä¸ºæ•™å¸ˆçš„ä¸“ä¸šåŒ–æˆé•¿æä¾›ç§‘å­¦åé¦ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Full paper at the Educational Data Mining (EDM) Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07552v2",
      "published_date": "2025-05-12 13:30:30 UTC",
      "updated_date": "2025-09-25 07:41:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:03.809087+00:00"
    },
    {
      "arxiv_id": "2505.07548v1",
      "title": "Noise Optimized Conditional Diffusion for Domain Adaptation",
      "title_zh": "ç”¨äºé¢†åŸŸè‡ªé€‚åº”çš„å™ªå£°ä¼˜åŒ–æ¡ä»¶æ‰©æ•£",
      "authors": [
        "Lingkun Luo",
        "Shiqiang Hu",
        "Liming Chen"
      ],
      "abstract": "Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples (\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical alignment, causing DA failures. To address this challenge, we propose \\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for \\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly integrates the generative capabilities of conditional diffusion models with the decision-making requirements of DA to achieve task-coupled optimization for efficient adaptation. For robust cross-domain consistency, we modify the DA classifier to align with the conditional diffusion classifier within a unified optimization framework, enabling forward training on noise-varying cross-domain samples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\) initialization in diffusion models often generates class-confused hcpl-tds, compromising discriminative DA. To resolve this, we introduce a class-aware noise optimization strategy that refines sampling regions for reverse class-specific hcpl-tds generation, effectively enhancing cross-domain alignment. Extensive experiments across 5 benchmark datasets and 29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over 31 state-of-the-art methods, validating its robustness and effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”(Unsupervised Domain Adaptation)ä¸­é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾ç›®æ ‡åŸŸæ ·æœ¬(hcpl-tds)ç¨€ç¼ºå¯¼è‡´çš„å¯¹é½ä¸å‡†é—®é¢˜ï¼Œæå‡ºäº†Noise Optimized Conditional Diffusion for Domain Adaptation (NOCDDA)æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†æ¡ä»¶æ‰©æ•£æ¨¡å‹(conditional diffusion models)çš„ç”Ÿæˆèƒ½åŠ›ä¸é¢†åŸŸè‡ªé€‚åº”(DA)çš„å†³ç­–éœ€æ±‚ç›¸ç»“åˆï¼Œé€šè¿‡ä»»åŠ¡è€¦åˆä¼˜åŒ–æå‡é€‚é…æ•ˆç‡ã€‚ä¸ºäº†ç»´æŒè·¨åŸŸä¸€è‡´æ€§ï¼Œç ”ç©¶ä¿®æ”¹äº†DAåˆ†ç±»å™¨ä»¥åœ¨ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶å†…ä¸æ‰©æ•£åˆ†ç±»å™¨å¯¹é½ï¼Œä»è€Œå®ç°å¯¹å™ªå£°å˜åŒ–æ ·æœ¬çš„å‰å‘è®­ç»ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„é«˜æ–¯å™ªå£°åˆå§‹åŒ–æ˜“å¯¼è‡´ç±»åˆ«æ··æ·†ï¼Œå¹¶æ®æ­¤å¼•å…¥äº†ç±»åˆ«æ„ŸçŸ¥å™ªå£°ä¼˜åŒ–(class-aware noise optimization)ç­–ç•¥ï¼Œä»¥ç²¾ç»†åŒ–é‡‡æ ·åŒºåŸŸå¹¶ç”Ÿæˆé«˜è´¨é‡çš„ç±»åˆ«ç‰¹å®šæ ·æœ¬ã€‚åœ¨5ä¸ªåŸºå‡†æ•°æ®é›†å’Œ29é¡¹ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒNOCDDAåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†31ç§ç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨å¢å¼ºè·¨åŸŸå¯¹é½æ–¹é¢çš„å¼ºå¤§é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures This work has been accepted by the International Joint Conference on Artificial Intelligence (IJCAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.07548v1",
      "published_date": "2025-05-12 13:28:31 UTC",
      "updated_date": "2025-05-12 13:28:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:00.856252+00:00"
    },
    {
      "arxiv_id": "2505.07546v3",
      "title": "GRADA: Graph-based Reranking against Adversarial Documents Attack",
      "title_zh": "GRADAï¼šé’ˆå¯¹å¯¹æŠ—æ€§æ–‡æ¡£æ”»å‡»çš„åŸºäºå›¾çš„é‡æ’åº",
      "authors": [
        "Jingjie Zheng",
        "Aryo Pradipta Gema",
        "Giwon Hong",
        "Xuanli He",
        "Pasquale Minervini",
        "Youcheng Sun",
        "Qiongkai Xu"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GRADA (Graph-based Reranking against Adversarial Document Attacks)ï¼Œä¸€ç§æ—¨åœ¨æŠµå¾¡å¯¹æŠ—æ€§æ–‡æ¡£æ”»å‡»çš„å›¾é‡æ’æ¡†æ¶ï¼Œä»¥å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚ç ”ç©¶å‘ç°å¯¹æŠ—æ€§æ–‡æ¡£è™½ç„¶ä¸æŸ¥è¯¢è¯­ä¹‰ç›¸ä¼¼ï¼Œä½†ä¸æ£€ç´¢é›†ä¸­çš„è‰¯æ€§æ–‡æ¡£ç›¸ä¼¼åº¦è¾ƒå¼±ï¼ŒGRADAåˆ©ç”¨è¿™ä¸€å›¾ç»“æ„ç‰¹å¾åœ¨ä¿æŒæ£€ç´¢è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡ã€‚å®éªŒåœ¨GPT-3.5-Turboã€GPT-4oã€Llama3.1å’ŒQwen2.5ç­‰äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä»¥åŠä¸‰ä¸ªæ•°æ®é›†ä¸Šå±•å¼€ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨Natural Questionsæ•°æ®é›†ä¸Šèƒ½å°†æ”»å‡»æˆåŠŸç‡é™ä½é«˜è¾¾80%ï¼Œä¸”å¯¹ç³»ç»ŸåŸæœ‰çš„å‡†ç¡®ç‡å½±å“æå°ã€‚è¯¥æ–¹æ¡ˆä¸ºåº”å¯¹LLMsåœ¨çŸ¥è¯†æ£€ç´¢è¿‡ç¨‹ä¸­çš„å®‰å…¨é£é™©æä¾›äº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„é˜²å¾¡æœºåˆ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07546v3",
      "published_date": "2025-05-12 13:27:35 UTC",
      "updated_date": "2025-09-18 01:20:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:23.176108+00:00"
    },
    {
      "arxiv_id": "2505.07910v2",
      "title": "Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization",
      "title_zh": "é¢å‘å¯ä¿¡åº¦çš„è°ƒä¼˜ï¼šç¥ç»ç½‘ç»œä¼˜åŒ–ä¸­æ€§èƒ½ä¸è§£é‡Šä¸€è‡´æ€§çš„å¹³è¡¡",
      "authors": [
        "Alexander Hinterleitner",
        "Thomas Bartz-Beielstein"
      ],
      "abstract": "Despite the growing interest in Explainable Artificial Intelligence (XAI), explainability is rarely considered during hyperparameter tuning or neural architecture optimization, where the focus remains primarily on minimizing predictive loss. In this work, we introduce the novel concept of XAI consistency, defined as the agreement among different feature attribution methods, and propose new metrics to quantify it. For the first time, we integrate XAI consistency directly into the hyperparameter tuning objective, creating a multi-objective optimization framework that balances predictive performance with explanation robustness. Implemented within the Sequential Parameter Optimization Toolbox (SPOT), our approach uses both weighted aggregation and desirability-based strategies to guide model selection. Through our proposed framework and supporting tools, we explore the impact of incorporating XAI consistency into the optimization process. This enables us to characterize distinct regions in the architecture configuration space: one region with poor performance and comparatively low interpretability, another with strong predictive performance but weak interpretability due to low \\gls{xai} consistency, and a trade-off region that balances both objectives by offering high interpretability alongside competitive performance. Beyond introducing this novel approach, our research provides a foundation for future investigations into whether models from the trade-off zone-balancing performance loss and XAI consistency-exhibit greater robustness by avoiding overfitting to training performance, thereby leading to more reliable predictions on out-of-distribution data.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†XAI consistencyè¿™ä¸€æ–°æ¦‚å¿µï¼Œå³ä¸åŒç‰¹å¾å½’å› æ–¹æ³•(feature attribution methods)ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæ—¨åœ¨è§£å†³è¶…å‚æ•°è°ƒä¼˜å’Œç¥ç»ç½‘ç»œä¼˜åŒ–è¿‡ç¨‹ä¸­é•¿æœŸå¿½è§†å¯è§£é‡Šæ€§(Explainable AI)çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†é‡åŒ–è¯¥ä¸€è‡´æ€§çš„æ–°æŒ‡æ ‡ï¼Œå¹¶é¦–æ¬¡å°†å…¶é›†æˆåˆ°è¶…å‚æ•°è°ƒä¼˜ç›®æ ‡ä¸­ï¼Œæ„å»ºäº†ä¸€ä¸ªå¹³è¡¡é¢„æµ‹æ€§èƒ½ä¸è§£é‡Šç¨³å¥æ€§çš„å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºSequential Parameter Optimization Toolbox (SPOT)å®ç°ï¼Œé‡‡ç”¨åŠ æƒèšåˆå’ŒåŸºäºåˆæ„æ€§(desirability-based)çš„ç­–ç•¥æ¥æŒ‡å¯¼æ¨¡å‹é€‰æ‹©ã€‚é€šè¿‡åˆ†ææ¶æ„é…ç½®ç©ºé—´ï¼Œç ”ç©¶è¯†åˆ«å‡ºäº†æ€§èƒ½ä¸å¯è§£é‡Šæ€§å‡è¾ƒå·®ã€é«˜æ€§èƒ½ä½†ä½ä¸€è‡´æ€§ï¼Œä»¥åŠå…¼é¡¾ä¸¤è€…çš„æŠ˜ä¸­åŒºåŸŸã€‚ç ”ç©¶ç»“æœä¸ºæœªæ¥æ¢ç´¢æ¨¡å‹é²æ£’æ€§å¥ å®šäº†åŸºç¡€ï¼Œè¡¨æ˜åœ¨æ€§èƒ½ä¸XAI consistencyä¹‹é—´å–å¾—å¹³è¡¡çš„æ¨¡å‹èƒ½æœ‰æ•ˆé¿å…å¯¹è®­ç»ƒè¡¨ç°çš„è¿‡æ‹Ÿåˆï¼Œä»è€Œåœ¨åˆ†å¸ƒå¤–æ•°æ®(out-of-distribution data)ä¸Šå®ç°æ›´å¯é çš„é¢„æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07910v2",
      "published_date": "2025-05-12 13:19:14 UTC",
      "updated_date": "2025-05-23 13:49:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:06.071936+00:00"
    },
    {
      "arxiv_id": "2505.07534v1",
      "title": "The Human-Data-Model Interaction Canvas for Visual Analytics",
      "title_zh": "é¢å‘å¯è§†åˆ†æçš„äºº-æ•°æ®-æ¨¡å‹äº¤äº’ç”»å¸ƒ",
      "authors": [
        "JÃ¼rgen Bernard"
      ],
      "abstract": "Visual Analytics (VA) integrates humans, data, and models as key actors in insight generation and data-driven decision-making. This position paper values and reflects on 16 VA process models and frameworks and makes nine high-level observations that motivate a fresh perspective on VA. The contribution is the HDMI Canvas, a perspective to VA that complements the strengths of existing VA process models and frameworks. It systematically characterizes diverse roles of humans, data, and models, and how these actors benefit from and contribute to VA processes. The descriptive power of the HDMI Canvas eases the differentiation between a series of VA building blocks, rather than describing general VA principles only. The canvas includes modern human-centered methodologies, including human knowledge externalization and forms of feedback loops, while interpretable and explainable AI highlight model contributions beyond their conventional outputs. The HDMI Canvas has generative power, guiding the design of new VA processes and is optimized for external stakeholders, improving VA outreach, interdisciplinary collaboration, and user-centered design. The utility of the HDMI Canvas is demonstrated through two preliminary case studies.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¯è§†åŒ–åˆ†æ(Visual Analytics, VA)ä¸­äººã€æ•°æ®ä¸æ¨¡å‹é›†æˆçš„å¤æ‚æ€§ï¼Œæå‡ºäº†HDMI Canvasæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸæä¾›å…¨æ–°çš„è§†è§’ã€‚è¯¥æ¡†æ¶åŸºäºå¯¹16ä¸ªç°æœ‰VAè¿‡ç¨‹æ¨¡å‹çš„ç ”ç©¶åŠ9é¡¹é«˜å±‚æ¬¡è§‚å¯Ÿï¼Œç³»ç»Ÿåœ°åˆ»ç”»äº†ä¸åŒå‚ä¸è€…çš„è§’è‰²åŠå…¶åœ¨VAæµç¨‹ä¸­çš„è´¡çŒ®ã€‚HDMI Canvasæ•´åˆäº†äººç±»çŸ¥è¯†å¤–éƒ¨åŒ–(human knowledge externalization)ã€åé¦ˆå¾ªç¯ä»¥åŠå¯è§£é‡Šäººå·¥æ™ºèƒ½(explainable AI)ç­‰ç°ä»£ä»¥äººä¸ºä¸­å¿ƒçš„æ–¹æ³•è®ºï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè¾“å‡ºçš„èŒƒç•´ã€‚ç›¸æ¯”äºä»…æè¿°é€šç”¨åŸåˆ™ï¼Œè¯¥ç”»å¸ƒå…·æœ‰æå¼ºçš„æè¿°èƒ½åŠ›ä¸å¼•å¯¼æ–°æµç¨‹è®¾è®¡çš„ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›è·¨å­¦ç§‘åä½œä¸ç”¨æˆ·ä¸­å¿ƒè®¾è®¡(user-centered design)ã€‚æœ€åï¼Œç ”ç©¶é€šè¿‡ä¸¤é¡¹åˆæ­¥æ¡ˆä¾‹å±•ç¤ºäº†HDMI Canvasåœ¨æå‡VAå¤–å»¶å’Œè¾…åŠ©è®¾è®¡æ–¹é¢çš„å®é™…æ•ˆç”¨ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 5 figures, LaTeX; to appear at the 16th International EuroVis Workshop on Visual Analytics (EuroVA'25) as a position paper",
      "pdf_url": "https://arxiv.org/pdf/2505.07534v1",
      "published_date": "2025-05-12 13:15:31 UTC",
      "updated_date": "2025-05-12 13:15:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:20.051339+00:00"
    },
    {
      "arxiv_id": "2505.07533v1",
      "title": "IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability",
      "title_zh": "IKrNetï¼šä¸€ç§ç”¨äºåœ¨ç”Ÿç†å˜å¼‚ç¯å¢ƒä¸‹æ£€æµ‹å¿ƒç”µå›¾ç‰¹å®šè¯ç‰©è¯±å¯¼æ¨¡å¼çš„ç¥ç»ç½‘ç»œ",
      "authors": [
        "Ahmad Fall",
        "Federica Granese",
        "Alex Lence",
        "Dominique Fourer",
        "Blaise Hanczar",
        "Joe-Elie Salem",
        "Jean-Daniel Zucker",
        "Edi Prifti"
      ],
      "abstract": "Monitoring and analyzing electrocardiogram (ECG) signals, even under varying physiological conditions, including those influenced by physical activity, drugs and stress, is crucial to accurately assess cardiac health. However, current AI-based methods often fail to account for how these factors interact and alter ECG patterns, ultimately limiting their applicability in real-world settings. This study introduces IKrNet, a novel neural network model, which identifies drug-specific patterns in ECGs amidst certain physiological conditions. IKrNet's architecture incorporates spatial and temporal dynamics by using a convolutional backbone with varying receptive field size to capture spatial features. A bi-directional Long Short-Term Memory module is also employed to model temporal dependencies. By treating heart rate variability as a surrogate for physiological fluctuations, we evaluated IKrNet's performance across diverse scenarios, including conditions with physical stress, drug intake alone, and a baseline without drug presence. Our assessment follows a clinical protocol in which 990 healthy volunteers were administered 80mg of Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art models' accuracy and stability in varying physiological conditions, underscoring its clinical viability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰AIæ–¹æ³•åœ¨å¤„ç†ç”Ÿç†å˜åŠ¨ï¼ˆå¦‚è¿åŠ¨ã€è¯ç‰©å’Œå‹åŠ›ï¼‰å¯¹å¿ƒç”µå›¾ (ECG) æ¨¡å¼å½±å“æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º IKrNet çš„æ–°å‹ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚IKrNet çš„æ¶æ„èåˆäº†ç©ºé—´ä¸æ—¶é—´åŠ¨æ€ç‰¹æ€§ï¼Œåˆ©ç”¨å…·æœ‰å¯å˜æ„Ÿå—é‡å¤§å°çš„å·ç§¯éª¨å¹²ç½‘ç»œ (convolutional backbone) æ•è·ç©ºé—´ç‰¹å¾ï¼Œå¹¶ç»“åˆåŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (Bi-directional LSTM) æ¨¡å—å»ºæ¨¡æ—¶é—´ä¾èµ–æ€§ã€‚ç ”ç©¶äººå‘˜å°†å¿ƒç‡å˜å¼‚æ€§ (heart rate variability) ä½œä¸ºç”Ÿç†æ³¢åŠ¨çš„ä»£ç†æŒ‡æ ‡ï¼Œå¹¶åœ¨åŒ…å«èº«ä½“å‹åŠ›ã€å•çº¯è¯ç‰©æ‘„å…¥åŠæ— è¯åŸºå‡†çš„å¤šç§åœºæ™¯ä¸‹è¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ã€‚è¯„ä¼°è¿‡ç¨‹éµå¾ªä¸´åºŠåè®®ï¼Œæ¶‰åŠ 990 åæœç”¨ 80mg ç´¢ä»–æ´›å°” (Sotalol) çš„å¥åº·å¿—æ„¿è€…ï¼Œè¯¥è¯ç‰©æ˜¯è¯±å‘è‡´å‘½æ€§å¿ƒå¾‹å¤±å¸¸ Torsades-de-Pointes çš„å…³é”®è¯±å› ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIKrNet åœ¨ä¸åŒç”Ÿç†æ¡ä»¶ä¸‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨å¤æ‚ç”Ÿç†æ³¢åŠ¨èƒŒæ™¯ä¸‹ç²¾ç¡®è¯†åˆ«è¯ç‰©å¼•èµ·çš„å¿ƒç”µå›¾å˜åŒ–æä¾›äº†å¯è¡Œçš„ä¸´åºŠæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07533v1",
      "published_date": "2025-05-12 13:14:47 UTC",
      "updated_date": "2025-05-12 13:14:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:19.403937+00:00"
    },
    {
      "arxiv_id": "2505.07531v2",
      "title": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads",
      "title_zh": "QuantXï¼šé¢å‘ç”Ÿæˆå¼ AI å·¥ä½œè´Ÿè½½çš„ç¡¬ä»¶æ„ŸçŸ¥é‡åŒ–æ¡†æ¶",
      "authors": [
        "Muhammad Ahmad",
        "Khurram Mazher",
        "Saqib Akram",
        "Ahmad Tameem",
        "Saad Bin Nasir"
      ],
      "abstract": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization. It is capable of quantizing down to 3-bit resolutions with minimal loss in performance. The quantization strategies in QuantX take into account hardware-specific constraints to achieve efficient dequantization during inference ensuring flexible trade-off between runtime speed, memory requirement and model accuracy. Our results demonstrate that QuantX achieves performance within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for multiple end user tasks and outperforms recently published state-of-the-art quantization techniques. We further integrate one particular technique from QuantX into the popular Llama.cpp framework and show its feasibility in terms of runtime compared to the mainstream quantization techniques from Llama.cpp. Lastly, this manuscript provides insights into the LLM quantization process that motivated the range of recipes and options that are incorporated in QuantX.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QuantXï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLM) å’Œè§†è§‰è¯­è¨€æ¨¡å‹ (VLM) è®¾è®¡çš„é‡åŒ–é…æ–¹å¥—ä»¶ï¼Œæ”¯æŒå°†æ¨¡å‹å‹ç¼©è‡³ 3-bit åˆ†è¾¨ç‡ä¸”æ€§èƒ½æŸå¤±æå°ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ç¡¬ä»¶æ„ŸçŸ¥ (hardware-aware) çš„é‡åŒ–ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸­çš„åé‡åŒ– (dequantization) æ•ˆç‡ï¼Œåœ¨è¿è¡Œé€Ÿåº¦ã€å†…å­˜å ç”¨ä¸æ¨¡å‹ç²¾åº¦ä¹‹é—´å®ç°äº†çµæ´»æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡åŒ–è‡³ 3-bit çš„ LlaVa-v1.6 æ¨¡å‹åœ¨å¤šé¡¹ç»ˆç«¯ç”¨æˆ·ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸æœªé‡åŒ–æ¨¡å‹å·®è·åœ¨ 6% ä»¥å†…ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„ SOTA é‡åŒ–æŠ€æœ¯ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥å°† QuantX çš„ç‰¹å®šæŠ€æœ¯é›†æˆåˆ° Llama.cpp æ¡†æ¶ä¸­ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸»æµæ¨ç†ç¯å¢ƒä¸‹çš„é«˜æ•ˆæ€§ä¸å¯è¡Œæ€§ã€‚æœ€åï¼Œæ–‡ä¸­æ·±å…¥æ¢è®¨äº† LLM é‡åŒ–æµç¨‹çš„å…³é”®è§è§£ï¼Œé˜è¿°äº†é©±åŠ¨ QuantX è®¾è®¡å¤šæ ·åŒ–é…æ–¹ä¸é€‰é¡¹çš„æ ¸å¿ƒåŠ¨å› ã€‚",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07531v2",
      "published_date": "2025-05-12 13:13:06 UTC",
      "updated_date": "2025-09-12 12:03:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:26.104830+00:00"
    },
    {
      "arxiv_id": "2505.07512v1",
      "title": "ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution",
      "title_zh": "ToolACE-DEVï¼šåŸºäºåˆ†è§£ä¸è¿›åŒ–çš„è‡ªæ”¹è¿›å·¥å…·å­¦ä¹ ",
      "authors": [
        "Xu Huang",
        "Weiwen Liu",
        "Xingshan Zeng",
        "Yuefeng Huang",
        "Xinlong Hao",
        "Yuxian Wang",
        "Yirong Zeng",
        "Chuhan Wu",
        "Yasheng Wang",
        "Ruiming Tang",
        "Defu Lian"
      ],
      "abstract": "The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å·¥å…·å­¦ä¹ ï¼ˆtool learningï¼‰ä¸­è¿‡åº¦ä¾èµ–é«˜çº§æ¨¡å‹è’¸é¦æ‰€å¯¼è‡´çš„æˆæœ¬é«˜æ˜‚ä»¥åŠçŸ¥è¯†èŒƒå›´å·®å¼‚å¼•å‘çš„æ•°æ®å…¼å®¹æ€§é—®é¢˜ï¼Œæå‡ºäº† ToolACE-DEV è¿™ä¸€è‡ªæˆ‘æ”¹è¿›ï¼ˆself-improvingï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆå°†å·¥å…·å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºå¢å¼ºåŸºç¡€å·¥å…·åˆ¶é€ ï¼ˆtool-makingï¼‰å’Œå·¥å…·ä½¿ç”¨ï¼ˆtool-usingï¼‰èƒ½åŠ›çš„å­ä»»åŠ¡ã€‚éšåï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘è¿›åŒ–ï¼ˆself-evolvingï¼‰èŒƒå¼ï¼Œä½¿è½»é‡çº§æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘è¿­ä»£å®ç°æ€§èƒ½æå‡ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†å¯¹é—­æºé«˜çº§æ¨¡å‹çš„ä¾èµ–ã€‚å¤§é‡å®éªŒç»“æœè¯æ˜ï¼ŒToolACE-DEV åœ¨ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„æ¨¡å‹ä¸Šå‡å±•ç°å‡ºå“è¶Šçš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ¡ˆä¸ºæå‡æ¨¡å‹è·å–å¤–éƒ¨å®æ—¶ä¿¡æ¯å’Œå¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›æä¾›äº†ä¸€ç§æ›´å…·å…¼å®¹æ€§ä¸”æˆæœ¬æ›´ä½çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07512v1",
      "published_date": "2025-05-12 12:48:30 UTC",
      "updated_date": "2025-05-12 12:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:44.313207+00:00"
    },
    {
      "arxiv_id": "2505.07511v1",
      "title": "MAIS: Memory-Attention for Interactive Segmentation",
      "title_zh": "MAISï¼šé¢å‘äº¤äº’å¼åˆ†å‰²çš„è®°å¿†æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Mauricio Orbes-Arteaga",
        "Oeslle Lucena",
        "Sabastien Ourselin",
        "M. Jorge Cardoso"
      ],
      "abstract": "Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision Transformer (ViT) åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ Segment Anything Model, SAMï¼‰åœ¨äº¤äº’å¼åŒ»å­¦å½±åƒåˆ†å‰²ä¸­å°†äº¤äº’è§†ä¸ºç‹¬ç«‹äº‹ä»¶ã€å¯¼è‡´å†—ä½™ä¿®æ­£çš„é—®é¢˜ï¼Œæå‡ºäº† MAIS (Memory-Attention for Interactive Segmentation)ã€‚MAIS å¼•å…¥äº†ä¸€ç§ Memory-Attention æœºåˆ¶ï¼Œé€šè¿‡å­˜å‚¨è¿‡å»çš„è¾“å…¥å’Œåˆ†å‰²çŠ¶æ€æ¥æ•´åˆæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ç°æœ‰æŠ€æœ¯åœ¨ç»†åŒ–å¢ç›Šæ–¹é¢çš„å±€é™æ€§ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å†å²äº¤äº’è¿›è¡Œæ›´æœ‰æ•ˆçš„é¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒMAIS åœ¨å¤šç§åŒ»å­¦å½±åƒæ¨¡æ€ä¸­å‡æ˜¾è‘—å¢å¼ºäº†åˆ†å‰²æ€§èƒ½ï¼Œå®ç°äº†æ›´é«˜æ•ˆç‡å’Œæ›´ç²¾ç¡®çš„ç»†åŒ–å¤„ç†ã€‚è¯¥æ¡†æ¶ä¸ºé™ä½æ ‡æ³¨æˆæœ¬å’Œæå‡äº¤äº’å¼åˆ†å‰²ç³»ç»Ÿçš„å‡†ç¡®æ€§æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07511v1",
      "published_date": "2025-05-12 12:48:27 UTC",
      "updated_date": "2025-05-12 12:48:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:44.161807+00:00"
    },
    {
      "arxiv_id": "2505.07509v1",
      "title": "HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs",
      "title_zh": "HALOï¼šæ—¶åºçŸ¥è¯†å›¾è°±ä¸­åŸºäºåŠè¡°æœŸçš„è¿‡æ—¶äº‹å®è¿‡æ»¤",
      "authors": [
        "Feng Ding",
        "Tingting Wang",
        "Yupeng Gao",
        "Shuo Yu",
        "Jing Ren",
        "Feng Xia"
      ],
      "abstract": "Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the expiration date of facts, which negatively impact reasoning performance on TKGs. However, existing reasoning methods primarily focus on positive importance of historical facts, neglecting adverse effects of outdated facts. Besides, training on these outdated facts yields extra computational cost. To address these challenges, we propose an outdated fact filtering framework named HALO, which quantifies the temporal validity of historical facts by exploring the half-life theory to filter outdated facts in TKGs. HALO consists of three modules: the temporal fact attention module, the dynamic relation-aware encoder module, and the outdated fact filtering module. Firstly, the temporal fact attention module captures the evolution of historical facts over time to identify relevant facts. Secondly, the dynamic relation-aware encoder module is designed for efficiently predicting the half life of each fact. Finally, we construct a time decay function based on the half-life theory to quantify the temporal validity of facts and filter outdated facts. Experimental results show that HALO outperforms the state-of-the-art TKG reasoning methods on three public datasets, demonstrating its effectiveness in detecting and filtering outdated facts (Codes are available at https://github.com/yushuowiki/K-Half/tree/main ).",
      "tldr_zh": "é’ˆå¯¹æ—¶åºçŸ¥è¯†å›¾è°± (Temporal Knowledge Graphs, TKGs) ä¸­è¿‡æ—¶äº‹å®å¯¼è‡´çš„æ¨ç†æ€§èƒ½ä¸‹é™å’Œé¢å¤–è®¡ç®—å¼€é”€é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åä¸º HALO çš„è¿‡æ—¶äº‹å®è¿‡æ»¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å¼•å…¥åŠè¡°æœŸç†è®º (half-life theory) æ¥é‡åŒ–å†å²äº‹å®çš„æ—¶æ•ˆæ€§ï¼Œä»¥è§£å†³ç°æœ‰æ¨ç†æ–¹æ³•å¿½è§†è¿‡æ—¶äº‹å®è´Ÿé¢å½±å“çš„é—®é¢˜ã€‚HALO åŒ…å«æ•æ‰äº‹å®æ¼”åŒ–çš„æ—¶é—´äº‹å®æ³¨æ„åŠ›æ¨¡å— (temporal fact attention module)ã€é¢„æµ‹åŠè¡°æœŸçš„åŠ¨æ€å…³ç³»æ„ŸçŸ¥ç¼–ç å™¨æ¨¡å— (dynamic relation-aware encoder module) ä»¥åŠåŸºäºæ—¶é—´è¡°å‡å‡½æ•°å®ç°é‡åŒ–è¿‡æ»¤çš„è¿‡æ—¶äº‹å®è¿‡æ»¤æ¨¡å— (outdated fact filtering module)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHALO åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„ SOTA æ¨ç†æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ£€æµ‹å’Œè¿‡æ»¤è¿‡æ—¶äº‹å®æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤§è§„æ¨¡ TKGs çš„æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07509v1",
      "published_date": "2025-05-12 12:47:20 UTC",
      "updated_date": "2025-05-12 12:47:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:55:41.862692+00:00"
    },
    {
      "arxiv_id": "2505.07508v1",
      "title": "EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection",
      "title_zh": "EAGLEï¼šåŸºäºå¯¹æ¯”å­¦ä¹ çš„é«˜æ•ˆå›¾å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Jing Ren",
        "Mingliang Hou",
        "Zhixuan Liu",
        "Xiaomei Bai"
      ],
      "abstract": "Graph anomaly detection is a popular and vital task in various real-world scenarios, which has been studied for several decades. Recently, many studies extending deep learning-based methods have shown preferable performance on graph anomaly detection. However, existing methods are lack of efficiency that is definitely necessary for embedded devices. Towards this end, we propose an Efficient Anomaly detection model on heterogeneous Graphs via contrastive LEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of their distances to the local context. The proposed method first samples instance pairs on meta path-level for contrastive learning. Then, a graph autoencoder-based model is applied to learn informative node embeddings in an unsupervised way, which will be further combined with the discriminator to predict the anomaly scores of nodes. Experimental results show that EAGLE outperforms the state-of-the-art methods on three heterogeneous network datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾å¼‚å¸¸æ£€æµ‹(Graph anomaly detection)æ–¹æ³•åœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šæ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†EAGLEæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„é«˜æ•ˆå¼‚æ„å›¾å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æ¯”å¼‚å¸¸èŠ‚ç‚¹ä¸æ­£å¸¸èŠ‚ç‚¹åˆ°å±€éƒ¨ä¸Šä¸‹æ–‡çš„è·ç¦»æ¥è¯†åˆ«å¼‚å¸¸ï¼Œå¹¶é¦–å…ˆåœ¨å…ƒè·¯å¾„(Meta path)çº§åˆ«é‡‡æ ·å®ä¾‹å¯¹ä»¥è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚éšåï¼Œç ”ç©¶åˆ©ç”¨åŸºäºå›¾è‡ªç¼–ç å™¨(Graph autoencoder)çš„æ¨¡å‹ä»¥æ— ç›‘ç£æ–¹å¼å­¦ä¹ èŠ‚ç‚¹åµŒå…¥(Node embeddings)ï¼Œå¹¶ç»“åˆåˆ¤åˆ«å™¨è®¡ç®—èŠ‚ç‚¹çš„å¼‚å¸¸åˆ†æ•°(Anomaly scores)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAGLEåœ¨ä¸‰ä¸ªå¼‚æ„ç½‘ç»œæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„(State-of-the-art)æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆä¸”ç²¾å‡†çš„å¼‚å¸¸æ£€æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07508v1",
      "published_date": "2025-05-12 12:45:07 UTC",
      "updated_date": "2025-05-12 12:45:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:08.765021+00:00"
    },
    {
      "arxiv_id": "2505.07908v1",
      "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny",
      "title_zh": "ä¸€é¡¹å¤ç°ç ”ç©¶ï¼šè‡ªæ³¨æ„åŠ›çš„æ ¸ä¸»æˆåˆ†åˆ†æï¼ˆKPCAï¼‰è§£é‡Šç»ä¸èµ·æ¨æ•²",
      "authors": [
        "Karahan SarÄ±taÅŸ",
        "Ã‡aÄŸatay YÄ±ldÄ±z"
      ],
      "abstract": "In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA (Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.",
      "tldr_zh": "è¿™é¡¹é‡ç°ç ”ç©¶ï¼ˆReproduction Studyï¼‰å®¡è§†äº†å…³äºè‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰æœºåˆ¶å®ç°äº†æ ¸ä¸»æˆåˆ†åˆ†æï¼ˆKernel PCA/KPCAï¼‰çš„è¿‘æœŸè§‚ç‚¹ï¼Œå³è®¤ä¸ºå€¼å‘é‡ $V$ æ•æ‰äº†é”® Gram çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼Œä¸”è‡ªæ³¨æ„åŠ›å°†æŸ¥è¯¢æŠ•å½±åˆ°äº†ç‰¹å¾ç©ºé—´çš„ä¸»æˆåˆ†è½´ä¸Šã€‚åˆ†ææ­ç¤ºäº†ä¸‰ä¸ªå…³é”®çš„ä¸ä¸€è‡´æ€§ï¼šé¦–å…ˆï¼Œå­¦ä¹ åˆ°çš„è‡ªæ³¨æ„åŠ›å€¼å‘é‡ä¸ KPCA è§†è§’æ‰€æè®®çš„å‘é‡ä¹‹é—´ä¸å­˜åœ¨å¯¹é½ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ä¸ CKA ç­‰æŒ‡æ ‡å‡æ˜¾ç¤ºä¸¤è€…å‡ ä¹æ²¡æœ‰å¯¹åº”å…³ç³»ã€‚å…¶æ¬¡ï¼ŒåŸç ”ç©¶ä¸­å…³äºé‡æ„æŸå¤± $J_\\text{proj}$ ä¸‹é™ä»¥è¯æ˜è‡ªæ³¨æ„åŠ›æœ€å°åŒ– KPCA æŠ•å½±è¯¯å·®çš„ç»“è®ºå­˜åœ¨è¯¯è¯»ï¼Œç›¸å…³æ•°å€¼é‡çº§ç›¸å·®è¾¾ä¸‰ä¸ªæ•°é‡çº§ã€‚æ­¤å¤–ï¼ŒåŸç ”ç©¶ç”¨äºæ”¯æ’‘è®ºç‚¹çš„ç‰¹å¾å€¼ç»Ÿè®¡æ•°æ®åœ¨æ ‡å‡†å®ç°ä¸‹æ— æ³•é‡ç°ã€‚é€šè¿‡å¯¹ 10 ç§ Transformer æ¶æ„çš„è¯„ä¼°ï¼Œè¯¥ç ”ç©¶æœ€ç»ˆå¾—å‡ºç»“è®ºï¼šè‡ªæ³¨æ„åŠ›çš„ KPCA è§£é‡Šç¼ºä¹å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07908v1",
      "published_date": "2025-05-12 12:38:46 UTC",
      "updated_date": "2025-05-12 12:38:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:12.357766+00:00"
    },
    {
      "arxiv_id": "2505.07473v1",
      "title": "Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks",
      "title_zh": "Web-Benchï¼šåŸºäº Web æ ‡å‡†ä¸æ¡†æ¶çš„å¤§è¯­è¨€æ¨¡å‹ä»£ç è¯„æµ‹åŸºå‡†",
      "authors": [
        "Kai Xu",
        "YiWei Mao",
        "XinYi Guan",
        "ZiLong Feng"
      ],
      "abstract": "The application of large language models (LLMs) in the field of coding is evolving rapidly: from code assistants, to autonomous coding agents, and then to generating complete projects through natural language. Early LLM code benchmarks primarily focused on code generation accuracy, but these benchmarks have gradually become saturated. Benchmark saturation weakens their guiding role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%. Among various attempts to address benchmark saturation, approaches based on software engineering have stood out, but the saturation of existing software engineering benchmarks is rapidly increasing. To address this, we propose a new benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks with sequential dependencies. The tasks implement project features in sequence, simulating real-world human development workflows. When designing Web-Bench, we aim to cover the foundational elements of Web development: Web Standards and Web Frameworks. Given the scale and complexity of these projects, which were designed by engineers with 5 to 10 years of experience, each presents a significant challenge. On average, a single project takes 4 to 8 hours for a senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better) than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss that in any development field, Standards and Frameworks represent foundational knowledge and efficiency tools, respectively, and LLMs require optimization tailored to them.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Web-Benchï¼Œä¸€ä¸ªåŸºäºWeb Standardså’ŒWeb Frameworksçš„æ–°å‹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»£ç åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•æ—¥ç›Šé¥±å’Œä¸”éš¾ä»¥æ¨¡æ‹ŸçœŸå®å¼€å‘æµç¨‹çš„é—®é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«50ä¸ªé¡¹ç›®ï¼Œæ¯ä¸ªé¡¹ç›®ç”±20ä¸ªå…·æœ‰é¡ºåºä¾èµ–å…³ç³»çš„ä»»åŠ¡ç»„æˆï¼Œèƒ½å¤ŸçœŸå®è¿˜åŸè½¯ä»¶å·¥ç¨‹å¸ˆçš„å¼€å‘å·¥ä½œæµã€‚Web-Benchæ¶µç›–äº†Webå¼€å‘çš„æ ¸å¿ƒè¦ç´ ï¼Œå…¶é¡¹ç›®å¤æ‚åº¦ç”±å…·å¤‡5è‡³10å¹´ç»éªŒçš„å·¥ç¨‹å¸ˆè®¾è®¡ï¼Œå•ä¸ªé¡¹ç›®å¹³å‡è€—æ—¶éœ€4è‡³8å°æ—¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSOTAæ¨¡å‹Claude 3.7 Sonnetåœ¨Web-Agentä¸Šçš„Pass@1ä»…ä¸º25.1%ï¼Œè¿œä½äºå…¶åœ¨SWE-Benchä¸Šçš„å¾—åˆ†ï¼Œè¯æ˜äº†è¯¥åŸºå‡†æµ‹è¯•çš„æé«˜æŒ‘æˆ˜æ€§ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†Web Standardså’ŒWeb Frameworksåœ¨å¼€å‘é¢†åŸŸä½œä¸ºåŸºç¡€çŸ¥è¯†ä¸æ•ˆç‡å·¥å…·çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰çš„LLMsåœ¨åº”å¯¹å¤æ‚å·¥ç¨‹ä»»åŠ¡æ—¶ä»éœ€é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07473v1",
      "published_date": "2025-05-12 12:06:23 UTC",
      "updated_date": "2025-05-12 12:06:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:18.212016+00:00"
    },
    {
      "arxiv_id": "2505.07460v1",
      "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸å°è¯­è¨€æ¨¡å‹ååŒæœºåˆ¶ç»¼è¿°",
      "authors": [
        "Yi Chen",
        "JiaHao Zhao",
        "HaoHao Han"
      ],
      "abstract": "Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸å°å‹è¯­è¨€æ¨¡å‹ (SLMs) ä¹‹é—´çš„åä½œæœºåˆ¶ï¼Œæ—¨åœ¨å¹³è¡¡é«˜æ€§èƒ½ä¸é«˜èµ„æºæˆæœ¬ã€é«˜å»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ã€‚æ–‡ç« è¯¦ç»†åˆ†ç±»å¹¶é˜è¿°äº†åŒ…æ‹¬æµæ°´çº¿ (pipeline)ã€è·¯ç”± (routing)ã€è¾…åŠ© (auxiliary)ã€çŸ¥è¯†è’¸é¦ (distillation) å’Œæ¨¡å‹èåˆ (fusion) åœ¨å†…çš„å¤šç§äº¤äº’æœºåˆ¶ï¼Œå¹¶æ·±å…¥åˆ†æäº†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨åœºæ™¯ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé€šè¿‡è¿™ç§ååŒèŒƒå¼å¯ä»¥å®ç°ä½å»¶è¿Ÿã€éšç§ä¿æŠ¤åŠä¸ªæ€§åŒ–ç­‰ç›®æ ‡ï¼Œæ˜¾è‘—æå‡äº† AI ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯è®¿é—®æ€§ã€‚åŒæ—¶ï¼Œç»¼è¿°ä¹Ÿè®¨è®ºäº†ç³»ç»Ÿå¼€é”€ã€æ¨¡å‹é—´ä¸€è‡´æ€§å’Œä»»åŠ¡åˆ†é…ç¨³å¥æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†æœªæ¥å‘è‡ªé€‚åº”æ¡†æ¶ã€å¤šæ¨¡æ€ (multimodal) åŠå…·èº«æ™ºèƒ½ (embodied AI) æ‰©å±•çš„è·¯å¾„ï¼Œè®¤ä¸º LLM-SLM åä½œå°†æ˜¯æ¨åŠ¨ä¸‹ä¸€ä»£æ™®åŠåŒ–äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒé©±åŠ¨åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07460v1",
      "published_date": "2025-05-12 11:48:42 UTC",
      "updated_date": "2025-05-12 11:48:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:28.980800+00:00"
    },
    {
      "arxiv_id": "2505.07457v1",
      "title": "Can Generative AI agents behave like humans? Evidence from laboratory market experiments",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“èƒ½å¦è¡¨ç°å‡ºç±»äººè¡Œä¸ºï¼Ÿæ¥è‡ªå®éªŒå®¤å¸‚åœºå®éªŒçš„è¯æ®",
      "authors": [
        "R. Maria del Rio-Chanona",
        "Marco Pangallo",
        "Cars Hommes"
      ],
      "abstract": "We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿå®éªŒå®¤ç»æµå¸‚åœºå®éªŒä¸­äººç±»è¡Œä¸ºçš„æ½œåŠ›ï¼Œé‡ç‚¹å…³æ³¨æ™ºèƒ½ä½“ä¹‹é—´çš„åŠ¨æ€åé¦ˆæœºåˆ¶ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œæœ¬ç ”ç©¶é€šè¿‡è®©LLMä»£ç†çš„å†³ç­–å½±å“å®æ—¶å¸‚åœºä»·æ ¼ï¼Œè¿›è€Œå½±å“åç»­å†³ç­–ï¼Œå¹¶å°†å…¶è¡Œä¸ºä¸å®éªŒå®¤è§‚å¯Ÿåˆ°çš„äººç±»åŠ¨æ€è¿›è¡Œå¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMså¹¶ä¸å®Œå…¨éµå¾ªç†æ€§é¢„æœŸ(rational expectations)ï¼Œè€Œæ˜¯è¡¨ç°å‡ºä¸äººç±»å‚ä¸è€…ç±»ä¼¼çš„æœ‰é™ç†æ€§(bounded rationality)ã€‚ç ”ç©¶å‘ç°ï¼Œç»“åˆåŒ…å«å‰ä¸‰ä¸ªæ—¶é—´æ­¥çš„è®°å¿†çª—å£(context window)å’Œæ•æ‰å“åº”å¼‚è´¨æ€§çš„é«˜å˜å¼‚æ€§è®¾ç½®ï¼ŒLLMsèƒ½å¤ŸæˆåŠŸå¤åˆ¶äººç±»å®éªŒä¸­çš„å®è§‚è¶‹åŠ¿ï¼Œå¦‚æ­£åé¦ˆå¸‚åœº(positive feedback markets)ä¸è´Ÿåé¦ˆå¸‚åœº(negative feedback markets)çš„åŒºåˆ«ã€‚ç„¶è€Œï¼Œåœ¨å¾®è§‚å±‚é¢ï¼ŒLLMsè¡¨ç°å‡ºçš„è¡Œä¸ºå¼‚è´¨æ€§(heterogeneity)ä»ä½äºäººç±»ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¡¨æ˜LLMsåœ¨æ¨¡æ‹Ÿç°å®ç»æµè¡Œä¸ºæ–¹é¢å…·æœ‰å·¨å¤§å‰æ™¯ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ä»¥æå‡å…¶è¡Œä¸ºå¤šæ ·æ€§ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07457v1",
      "published_date": "2025-05-12 11:44:46 UTC",
      "updated_date": "2025-05-12 11:44:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:18.993126+00:00"
    },
    {
      "arxiv_id": "2505.07453v3",
      "title": "How well do LLMs reason over tabular data, really?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®ä¸Šçš„æ¨ç†èƒ½åŠ›ç©¶ç«Ÿå¦‚ä½•ï¼Ÿ",
      "authors": [
        "Cornelius Wolff",
        "Madelon Hulsebos"
      ],
      "abstract": "Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¡¨æ ¼æ•°æ®æ¨ç†æ–¹é¢çš„çœŸå®èƒ½åŠ›ï¼Œé‡ç‚¹åˆ†æäº†é€šç”¨æ¨¡å‹åœ¨åº”å¯¹ç°å®ä¸–ç•Œè¡¨æ ¼ç‰¹å¾æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„å¤šé€‰é¢˜è¯„ä¼°ç­–ç•¥ä»¥åŠ SacreBleu å’Œ BERT-score ç­‰é€šç”¨æ–‡æœ¬æŒ‡æ ‡éš¾ä»¥å‡†ç¡®è¡¡é‡ LLMs å¤„ç†åˆ†ææ€§è¡¨æ ¼æŸ¥è¯¢çš„æ°´å¹³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº LLM-as-a-judge çš„è¯„ä¼°ç¨‹åºï¼Œæ­ç¤ºäº† LLMs åœ¨è¡¨æ ¼æ¨ç†æ€§èƒ½ä¸Šå­˜åœ¨çš„æ˜¾è‘—çŸ­æ¿ã€‚é€šè¿‡å¼•å…¥ç¼ºå¤±å€¼(missing values)ã€é‡å¤å®ä½“(duplicate entities)å’Œç»“æ„å˜åŒ–(structural variations)ç­‰ç°å®å˜é‡ï¼Œå®éªŒè¯æ˜ LLMs çš„æ¨ç†èƒ½åŠ›åœ¨è¿™äº›å› ç´ å½±å“ä¸‹ä¼šæ˜æ˜¾å—æŸã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†æå‡ LLMs åœ¨å¤„ç†çœŸå®è¡¨æ ¼è¾“å…¥æ—¶é²æ£’æ€§çš„ç´§è¿«æ€§ï¼Œå¹¶ä¸ºå»ºç«‹æ›´çœŸå®çš„è¯„ä¼°æ ‡å‡†æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.07453v3",
      "published_date": "2025-05-12 11:35:28 UTC",
      "updated_date": "2025-11-04 14:30:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:55.104985+00:00"
    },
    {
      "arxiv_id": "2505.07450v3",
      "title": "Prototype Augmented Hypernetworks for Continual Learning",
      "title_zh": "é¢å‘æŒç»­å­¦ä¹ çš„åŸå‹å¢å¼ºè¶…ç½‘ç»œ",
      "authors": [
        "Neil De La Fuente",
        "Maria Pilligua",
        "Daniel Vidal",
        "Albin Soutiff",
        "Cecilia Curreli",
        "Daniel Cremers",
        "Andrey Barsky"
      ],
      "abstract": "Continual learning (CL) aims to learn a sequence of tasks without forgetting prior knowledge, but gradient updates for a new task often overwrite the weights learned earlier, causing catastrophic forgetting (CF). We propose Prototype-Augmented Hypernetworks (PAH), a framework where a single hypernetwork, conditioned on learnable task prototypes, dynamically generates task-specific classifier heads on demand. To mitigate forgetting, PAH combines cross-entropy with dual distillation losses, one to align logits and another to align prototypes, ensuring stable feature representations across tasks. Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7 % and 4.4 % forgetting, respectively, surpassing prior methods without storing samples or heads.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Prototype-Augmented Hypernetworks (PAH)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æŒç»­å­¦ä¹ (Continual Learning)ä¸­å› æ¢¯åº¦æ›´æ–°è¦†ç›–æ—§æƒé‡å¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ã€‚PAHåˆ©ç”¨å—å¯å­¦ä¹ ä»»åŠ¡åŸå‹(task prototypes)çº¦æŸçš„å•ä¸€è¶…ç½‘ç»œ(hypernetwork)ï¼Œèƒ½å¤ŸæŒ‰éœ€åŠ¨æ€ç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„åˆ†ç±»å™¨å¤´ã€‚ä¸ºäº†å‡è½»é—å¿˜ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†äº¤å‰ç†µä¸åŒé‡è’¸é¦æŸå¤±(dual distillation losses)ï¼Œåˆ†åˆ«ç”¨äºå¯¹é½é€»è¾‘å€¼(logits)å’ŒåŸå‹ï¼Œä»è€Œç¡®ä¿è·¨ä»»åŠ¡çš„ç¨³å®šç‰¹å¾è¡¨ç¤ºã€‚åœ¨Split-CIFAR100å’ŒTinyImageNetä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒPAHåˆ†åˆ«å®ç°äº†74.5%å’Œ63.7%çš„å‡†ç¡®ç‡ï¼Œä¸”é—å¿˜ç‡ä»…ä¸º1.7%å’Œ4.4%ã€‚è¯¥æ–¹æ³•åœ¨ä¸å­˜å‚¨æ ·æœ¬æˆ–åˆ†ç±»å™¨å¤´çš„æƒ…å†µä¸‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†åºåˆ—ä»»åŠ¡æ—¶å“è¶Šçš„æ€§èƒ½ä¸ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "CVPR 2025 (LatinX in CV)",
      "pdf_url": "https://arxiv.org/pdf/2505.07450v3",
      "published_date": "2025-05-12 11:25:54 UTC",
      "updated_date": "2025-05-16 16:21:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:26.566720+00:00"
    },
    {
      "arxiv_id": "2505.07447v2",
      "title": "Unified Continuous Generative Models",
      "title_zh": "ç»Ÿä¸€è¿ç»­ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Peng Sun",
        "Yi Jiang",
        "Tao Lin"
      ],
      "abstract": "Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Unified Continuous Generative Models (UCGM)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒã€é‡‡æ ·å’Œåˆ†æè¿ç»­ç”Ÿæˆæ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç›®å‰å°†æ‰©æ•£æ¨¡å‹(diffusion)ã€æµåŒ¹é…(flow-matching)ç­‰ä¼—æ­¥é‡‡æ ·æ–¹æ³•ä¸ä¸€è‡´æ€§æ¨¡å‹(consistency models)ç­‰å°‘æ­¥é‡‡æ ·æ–¹æ³•è§†ä¸ºä¸åŒèŒƒå¼è€Œå¯¼è‡´çš„è®­ç»ƒä¸é‡‡æ ·æ–¹æ³•ä¸ç»Ÿä¸€çš„é—®é¢˜ã€‚é€šè¿‡ç ”å‘UCGM-Tè®­ç»ƒå™¨å’ŒUCGM-Sé‡‡æ ·å™¨ï¼Œè¯¥ç ”ç©¶åœ¨ImageNet 256x256æ•°æ®é›†ä¸Šåˆ©ç”¨Diffusion Transformerå®ç°äº†State-of-the-art (SOTA) çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUCGM-Tè®­ç»ƒçš„å¤šæ­¥æ¨¡å‹åœ¨20æ­¥å†…è¾¾åˆ°1.30 FIDï¼Œè€Œå°‘æ­¥æ¨¡å‹åœ¨ä»…2æ­¥æ—¶è¾¾åˆ°1.42 FIDï¼›æ­¤å¤–ï¼ŒUCGM-Sé‡‡æ ·å™¨è¿˜èƒ½å°†é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ä»250æ­¥ä¸‹çš„1.26 FIDæå‡è‡³40æ­¥ä¸‹çš„1.06 FIDã€‚è¯¥æ¡†æ¶æ˜¾è‘—ä¼˜åŒ–äº†ç”Ÿæˆè´¨é‡ä¸é‡‡æ ·æ•ˆç‡ï¼Œä¸ºè¿ç»­ç”Ÿæˆæ¨¡å‹çš„ç»Ÿä¸€åŒ–å‘å±•æä¾›äº†æœ‰æ•ˆçš„ç†è®ºä¸å®è·µæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "https://github.com/LINs-lab/UCGM",
      "pdf_url": "https://arxiv.org/pdf/2505.07447v2",
      "published_date": "2025-05-12 11:15:39 UTC",
      "updated_date": "2025-05-20 12:27:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:41.997132+00:00"
    },
    {
      "arxiv_id": "2505.07437v1",
      "title": "LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning",
      "title_zh": "LEADï¼šé¢å‘é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒçš„è¿­ä»£å¼æ•°æ®é€‰æ‹©",
      "authors": [
        "Xiaotian Lin",
        "Yanlin Qi",
        "Yizhang Zhu",
        "Themis Palpanas",
        "Chengliang Chai",
        "Nan Tang",
        "Yuyu Luo"
      ],
      "abstract": "Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LEADï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning) çš„é«˜æ•ˆè¿­ä»£æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å› åå¤æ‰§è¡Œå…¨æ•°æ®é›†æ¨ç†è€Œå¯¼è‡´çš„å·¨å¤§è®¡ç®—å¼€é”€ã€‚LEAD çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†å®ä¾‹çº§åŠ¨æ€ä¸ç¡®å®šæ€§ (Instance-Level Dynamic Uncertainty, IDU)ï¼Œè¯¥æ•ˆç”¨å‡½æ•°åœ¨æ ‡å‡†è®­ç»ƒå¾ªç¯å†…ç»“åˆç¬æ—¶è®­ç»ƒæŸå¤±ã€åŸºäºæ¢¯åº¦çš„æŸå¤±å˜åŒ–è¿‘ä¼¼ä»¥åŠå†å²æŸå¤±ä¿¡å·çš„æŒ‡æ•°å¹³æ»‘æ¥ç²¾ç¡®ä¼°è®¡æ ·æœ¬æ•ˆç”¨ï¼Œä»è€Œæ¶ˆé™¤äº†é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚ä¸ºäº†åº”å¯¹å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒLEAD é‡‡ç”¨äº†ä»ç²—åˆ°ç²¾çš„ä¸¤é˜¶æ®µé€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨å¤šè‡‚è€è™æœº (Multi-armed Bandit) æœºåˆ¶è‡ªé€‚åº”åœ°ä¼˜å…ˆå¤„ç†é«˜ä¿¡æ¯é‡èšç±»ï¼Œå†é€šè¿‡ IDU è¿›è¡Œé«˜ä»·å€¼æ ·æœ¬çš„ç²¾ç»†ç­›é€‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLEAD åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œä»…ä½¿ç”¨ 2.5% çš„è®­ç»ƒæ•°æ®å³å¯å°†æ¨¡å‹å¹³å‡æ€§èƒ½æé«˜ 6.1%-10.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°†æ•´ä½“è®­ç»ƒæ—¶é—´ç¼©çŸ­äº† 5-10 å€ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”ä½æˆæœ¬çš„æŒ‡ä»¤å¾®è°ƒæä¾›äº†æå…·å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07437v1",
      "published_date": "2025-05-12 10:57:51 UTC",
      "updated_date": "2025-05-12 10:57:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:43.927746+00:00"
    },
    {
      "arxiv_id": "2505.13483v1",
      "title": "EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors",
      "title_zh": "EmoMetaï¼šé¢å‘ä¸­æ–‡éšå–»ç»†ç²’åº¦æƒ…æ„Ÿåˆ†ç±»çš„å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Xingyuan Lu",
        "Yuxi Liu",
        "Dongyu Zhang",
        "Zhiyao Wu",
        "Jing Ren",
        "Feng Xia"
      ],
      "abstract": "Metaphors play a pivotal role in expressing emotions, making them crucial for emotional intelligence. The advent of multimodal data and widespread communication has led to a proliferation of multimodal metaphors, amplifying the complexity of emotion classification compared to single-mode scenarios. However, the scarcity of research on constructing multimodal metaphorical fine-grained emotion datasets hampers progress in this domain. Moreover, existing studies predominantly focus on English, overlooking potential variations in emotional nuances across languages. To address these gaps, we introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of metaphorical advertisements. Each entry is meticulously annotated for metaphor occurrence, domain relations and fine-grained emotion classification encompassing joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral. Our dataset is publicly accessible (https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in this burgeoning field.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç ”ç©¶åœ¨ä¸­æ–‡å¤šæ¨¡æ€éšå–»ç»†ç²’åº¦æƒ…æ„Ÿæ•°æ®é›†æ–¹é¢çš„ç©ºç™½ï¼Œæå‡ºäº†åä¸ºEmoMetaçš„æ–°å‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«5,000ç»„ä¸­æ–‡éšå–»å¹¿å‘Šçš„æ–‡æœ¬-å›¾åƒå¯¹ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ•°æ®èƒŒæ™¯ä¸‹ï¼Œéšå–»æƒ…æ„Ÿåˆ†ç±»æ¯”å•æ¨¡æ€åœºæ™¯æ›´ä¸ºå¤æ‚çš„é—®é¢˜ã€‚EmoMetaå¯¹éšå–»å‘ç”Ÿæƒ…å†µã€é¢†åŸŸå…³ç³»(domain relations)ä»¥åŠåŒ…æ‹¬å–œæ‚¦ã€çˆ±ã€ä¿¡ä»»ã€ææƒ§ã€æ‚²ä¼¤ã€åŒæ¶ã€æ„¤æ€’ã€æƒŠè®¶ã€é¢„æœŸå’Œä¸­æ€§åœ¨å†…çš„åç§ç»†ç²’åº¦æƒ…æ„Ÿè¿›è¡Œäº†ç²¾ç¡®æ ‡æ³¨ã€‚ç›¸è¾ƒäºä»¥å¾€å¤šå…³æ³¨è‹±æ–‡çš„ç ”ç©¶ï¼Œè¯¥å·¥ä½œæ·±å…¥æ¢è®¨äº†ä¸­æ–‡è¯­å¢ƒä¸‹æƒ…æ„Ÿè¡¨è¾¾çš„ç»†å¾®å·®åˆ«ã€‚ç›®å‰è¯¥æ•°æ®é›†å·²å‘å…¬ä¼—å¼€æ”¾ï¼Œä¸ºå¤šæ¨¡æ€éšå–»ç†è§£å’Œæƒ…æ„Ÿæ™ºèƒ½(emotional intelligence)é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºå’Œæ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13483v1",
      "published_date": "2025-05-12 10:23:39 UTC",
      "updated_date": "2025-05-12 10:23:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:56:49.240400+00:00"
    },
    {
      "arxiv_id": "2507.21058v1",
      "title": "Categorical Classification of Book Summaries Using Word Embedding Techniques",
      "title_zh": "åŸºäºè¯åµŒå…¥æŠ€æœ¯çš„ä¹¦ç±æ‘˜è¦ç±»åˆ«åˆ†ç±»",
      "authors": [
        "Kerem Keskin",
        "MÃ¼mine Kaya KeleÅŸ"
      ],
      "abstract": "In this study, book summaries and categories taken from book sites were classified using word embedding methods, natural language processing techniques and machine learning algorithms. In addition, one hot encoding, Word2Vec and Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are frequently used word embedding methods were used in this study and their success was compared. Additionally, the combination table of the pre-processing methods used is shown and added to the table. Looking at the results, it was observed that Support Vector Machine, Naive Bayes and Logistic Regression Models and TF-IDF and One-Hot Encoder word embedding techniques gave more successful results for Turkish texts.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ Word Embedding æŠ€æœ¯ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¯¹ä¹¦ç±ç½‘ç«™ä¸Šçš„æ‘˜è¦å’Œç±»åˆ«è¿›è¡Œäº†è‡ªåŠ¨åˆ†ç±»ç ”ç©¶ã€‚æ–‡ä¸­å¯¹æ¯”äº† Word2Vecã€Term Frequency - Inverse Document Frequency (TF-IDF) ä»¥åŠ One-Hot Encoding ç­‰ä¸»æµè¯åµŒå…¥æ–¹æ³•çš„åº”ç”¨æ•ˆæœã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åˆ†æäº†ä¸åŒé¢„å¤„ç†æµç¨‹çš„ç»„åˆå¯¹åˆ†ç±»å‡†ç¡®æ€§çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨é’ˆå¯¹åœŸè€³å…¶è¯­æ–‡æœ¬çš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ”¯æŒå‘é‡æœº (Support Vector Machine)ã€æœ´ç´ è´å¶æ–¯ (Naive Bayes) å’Œé€»è¾‘å›å½’ (Logistic Regression) æ¨¡å‹ç»“åˆ TF-IDF å’Œ One-Hot Encoding æŠ€æœ¯è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œè¿˜ä¸ºåœŸè€³å…¶è¯­è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæä¾›äº†å…·ä½“çš„ç‰¹å¾æå–å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "in Turkish language. This paper was published in the proceedings of the 6th International Conference on Data Science and Applications ICONDATA24, held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text book see https://www.icondata.org/en/proceedings-books",
      "pdf_url": "https://arxiv.org/pdf/2507.21058v1",
      "published_date": "2025-05-12 09:57:37 UTC",
      "updated_date": "2025-05-12 09:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:11.149058+00:00"
    },
    {
      "arxiv_id": "2505.07903v1",
      "title": "SEM: Reinforcement Learning for Search-Efficient Large Language Models",
      "title_zh": "SEMï¼šé¢å‘é«˜æ•ˆæœç´¢å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Zeyang Sha",
        "Shiwen Cui",
        "Weiqiang Wang"
      ],
      "abstract": "Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SEMï¼Œä¸€ç§å…¨æ–°çš„è®­ç»ƒåå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¯¹å¤–éƒ¨æœç´¢å·¥å…·çš„ä½¿ç”¨æ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­æ™®éå­˜åœ¨çš„å†—ä½™æœç´¢å’Œé«˜æ˜‚æˆæœ¬é—®é¢˜ï¼ŒSEMé€šè¿‡æ„å»ºç»“åˆMuSiQueå’ŒMMLUçš„å¹³è¡¡æ•°æ®é›†ï¼Œè®­ç»ƒæ¨¡å‹ç²¾å‡†è¾¨åˆ«ä½•æ—¶åº”ä¾èµ–å†…éƒ¨çŸ¥è¯†ä»¥åŠä½•æ—¶éœ€è¦è°ƒç”¨å¤–éƒ¨æ£€ç´¢ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ç»“æ„åŒ–çš„æ¨ç†æ¨¡æ¿ï¼Œå¹¶åˆ©ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)ç®—æ³•å¯¹æ¨¡å‹çš„æœç´¢è¡Œä¸ºè¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒã€‚å…¶æ ¸å¿ƒå¥–åŠ±å‡½æ•°æ—¨åœ¨é¼“åŠ±å‡†ç¡®å›ç­”çš„åŒæ—¶ï¼Œé€šè¿‡æŠ‘åˆ¶ä¸å¿…è¦çš„æœç´¢æ¥ä¼˜åŒ–èµ„æºæ¶ˆè€—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEMåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†å†—ä½™æœç´¢æ“ä½œï¼Œå¹¶åœ¨ä¿æŒæˆ–æå‡å›ç­”å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¤§å¹…å¢å¼ºäº†æ¨¡å‹å®¡æ…åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†çš„æ¨ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07903v1",
      "published_date": "2025-05-12 09:45:40 UTC",
      "updated_date": "2025-05-12 09:45:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:57:11.908763+00:00"
    },
    {
      "arxiv_id": "2505.07393v1",
      "title": "AI in Money Matters",
      "title_zh": "é‡‘èäº‹åŠ¡ä¸­çš„äººå·¥æ™ºèƒ½",
      "authors": [
        "Nadine Sandjo Tchatchoua",
        "Richard Harper"
      ],
      "abstract": "In November 2022, Europe and the world by and large were stunned by the birth of a new large language model : ChatGPT. Ever since then, both academic and populist discussions have taken place in various public spheres such as LinkedIn and X(formerly known as Twitter) with the view to both understand the tool and its benefits for the society. The views of real actors in professional spaces, especially in regulated industries such as finance and law have been largely missing. We aim to begin to close this gap by presenting results from an empirical investigation conducted through interviews with professional actors in the Fintech industry. The paper asks the question, how and to what extent are large language models in general and ChatGPT in particular being adopted and used in the Fintech industry? The results show that while the fintech experts we spoke with see a potential in using large language models in the future, a lot of questions marks remain concerning how they are policed and therefore might be adopted in a regulated industry such as Fintech. This paper aims to add to the existing academic discussing around large language models, with a contribution to our understanding of professional viewpoints.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)ï¼Œç‰¹åˆ«æ˜¯ ChatGPT åœ¨é‡‘èç§‘æŠ€(Fintech)è¡Œä¸šä¸­çš„å®é™…åº”ç”¨ç°çŠ¶ã€‚é’ˆå¯¹ç°æœ‰è®¨è®ºç¼ºä¹å—ç›‘ç®¡è¡Œä¸šä¸“ä¸šè§†è§’çš„é—®é¢˜ï¼Œè¯¥è®ºæ–‡é€šè¿‡å¯¹é‡‘èç§‘æŠ€ä»ä¸šè€…è¿›è¡Œå®è¯ç ”ç©¶(Empirical Investigation)å’Œæ·±åº¦è®¿è°ˆï¼Œåˆ†æäº†è¿™äº›å·¥å…·çš„é‡‡ç”¨ç¨‹åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ä¸“ä¸šäººå£«è®¤å¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœªæ¥çš„åº”ç”¨æ½œåŠ›ï¼Œä½†åœ¨å—ç›‘ç®¡çš„è¡Œä¸šç¯å¢ƒä¸­ï¼Œå…³äºå¦‚ä½•å¯¹å…¶è¿›è¡Œæœ‰æ•ˆç›‘ç®¡(Policing)ä»å­˜åœ¨è¯¸å¤šç–‘é—®ã€‚è¿™äº›ç›‘ç®¡å±‚é¢çš„ä¸ç¡®å®šæ€§ç›´æ¥å½±å“å¹¶é™åˆ¶äº†å½“å‰é‡‘èç§‘æŠ€é¢†åŸŸå¯¹è¯¥æŠ€æœ¯çš„å¹¿æ³›é‡‡ç”¨ã€‚è¯¥ç ”ç©¶é€šè¿‡æ­ç¤ºä¸“ä¸šäººå£«çš„çœŸå®è§‚ç‚¹ï¼Œä¸ºå­¦æœ¯ç•Œç†è§£äººå·¥æ™ºèƒ½åœ¨èŒä¸šç©ºé—´åŠå—ç›‘ç®¡è¡Œä¸šä¸­çš„è½åœ°æŒ‘æˆ˜è´¡çŒ®äº†é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07393v1",
      "published_date": "2025-05-12 09:43:51 UTC",
      "updated_date": "2025-05-12 09:43:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:57:25.421353+00:00"
    },
    {
      "arxiv_id": "2505.07381v1",
      "title": "Few-shot Semantic Encoding and Decoding for Video Surveillance",
      "title_zh": "è§†é¢‘ç›‘æ§ä¸­çš„å°æ ·æœ¬è¯­ä¹‰ç¼–è§£ç ",
      "authors": [
        "Baoping Cheng",
        "Yukun Zhang",
        "Liming Wang",
        "Xiaoyan Xie",
        "Tao Fu",
        "Dongkun Wang",
        "Xiaoming Tao"
      ],
      "abstract": "With the continuous increase in the number and resolution of video surveillance cameras, the burden of transmitting and storing surveillance video is growing. Traditional communication methods based on Shannon's theory are facing optimization bottlenecks. Semantic communication, as an emerging communication method, is expected to break through this bottleneck and reduce the storage and transmission consumption of video. Existing semantic decoding methods often require many samples to train the neural network for each scene, which is time-consuming and labor-intensive. In this study, a semantic encoding and decoding method for surveillance video is proposed. First, the sketch was extracted as semantic information, and a sketch compression method was proposed to reduce the bit rate of semantic information. Then, an image translation network was proposed to translate the sketch into a video frame with a reference frame. Finally, a few-shot sketch decoding network was proposed to reconstruct video from sketch. Experimental results showed that the proposed method achieved significantly better video reconstruction performance than baseline methods. The sketch compression method could effectively reduce the storage and transmission consumption of semantic information with little compromise on video quality. The proposed method provides a novel semantic encoding and decoding method that only needs a few training samples for each surveillance scene, thus improving the practicality of the semantic communication system.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç›‘æ§ä¸­ä¼ è¾“ä¸å­˜å‚¨å‹åŠ›æ—¥ç›Šå¢é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘ç›‘æ§è§†é¢‘çš„ Few-shot Semantic Encoding and Decoding æ–¹æ³•ï¼Œæ—¨åœ¨çªç ´ä¼ ç»Ÿé¦™å†œç†è®º (Shannon's theory) çš„ä¼˜åŒ–ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯­ä¹‰è§£ç æ–¹æ³•åœ¨æ¯ä¸ªåœºæ™¯ä¸­éƒ½éœ€è¦å¤§é‡è®­ç»ƒæ ·æœ¬çš„å±€é™æ€§ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†å°‘æ ·æœ¬å­¦ä¹ æœºåˆ¶ã€‚è¯¥æ–¹æ¡ˆé¦–å…ˆæå–è‰å›¾ (Sketch) ä½œä¸ºè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‰å›¾å‹ç¼© (Sketch compression) æŠ€æœ¯ä»¥æ˜¾è‘—é™ä½è¯­ä¹‰ä¿¡æ¯çš„æ¯”ç‰¹ç‡ã€‚éšåï¼Œåˆ©ç”¨å›¾åƒç¿»è¯‘ç½‘ç»œ (Image translation network) ç»“åˆå‚è€ƒå¸§ (Reference frame) å°†è‰å›¾è½¬æ¢ä¸ºè§†é¢‘å¸§ï¼Œå¹¶æœ€ç»ˆé€šè¿‡å°‘æ ·æœ¬è‰å›¾è§£ç ç½‘ç»œ (Few-shot sketch decoding network) å®Œæˆè§†é¢‘é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘é‡å»ºè´¨é‡ä¸Šä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œä¸”èƒ½åœ¨ä¿è¯ç”»è´¨çš„å‰æä¸‹æœ‰æ•ˆå‡å°‘ä¼ è¾“ä¸å­˜å‚¨æ¶ˆè€—ã€‚è¯¥ç ”ç©¶é€šè¿‡å¤§å¹…é™ä½æ¯ä¸ªç›‘æ§åœºæ™¯æ‰€éœ€çš„è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œæ˜¾è‘—æå‡äº†è¯­ä¹‰é€šä¿¡ç³»ç»Ÿ (Semantic communication system) çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07381v1",
      "published_date": "2025-05-12 09:27:28 UTC",
      "updated_date": "2025-05-12 09:27:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:25.593375+00:00"
    },
    {
      "arxiv_id": "2505.07902v1",
      "title": "Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach",
      "title_zh": "è¯¾å ‚è¯è¯­è´¨é‡çš„å¤šæ¨¡æ€è¯„ä¼°ï¼šä¸€ç§ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒã€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Ruikun Hou",
        "Babette BÃ¼hler",
        "Tim FÃ¼tterer",
        "Efe Bozkir",
        "Peter Gerjets",
        "Ulrich Trautwein",
        "Enkelejda Kasneci"
      ],
      "abstract": "Classroom discourse is an essential vehicle through which teaching and learning take place. Assessing different characteristics of discursive practices and linking them to student learning achievement enhances the understanding of teaching quality. Traditional assessments rely on manual coding of classroom observation protocols, which is time-consuming and costly. Despite many studies utilizing AI techniques to analyze classroom discourse at the utterance level, investigations into the evaluation of discursive practices throughout an entire lesson segment remain limited. To address this gap, our study proposes a novel text-centered multimodal fusion architecture to assess the quality of three discourse components grounded in the Global Teaching InSights (GTI) observation protocol: Nature of Discourse, Questioning, and Explanations. First, we employ attention mechanisms to capture inter- and intra-modal interactions from transcript, audio, and video streams. Second, a multi-task learning approach is adopted to jointly predict the quality scores of the three components. Third, we formulate the task as an ordinal classification problem to account for rating level order. The effectiveness of these designed elements is demonstrated through an ablation study on the GTI Germany dataset containing 92 videotaped math lessons. Our results highlight the dominant role of text modality in approaching this task. Integrating acoustic features enhances the model's consistency with human ratings, achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to human inter-rater reliability (0.326). Our study lays the groundwork for the future development of automated discourse quality assessment to support teacher professional development through timely feedback on multidimensional discourse practices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿäººå·¥è¯„ä¼°è¯¾å ‚è¯è¯­è´¨é‡è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€èåˆæ¶æ„ï¼Œæ—¨åœ¨è¯„ä¼°æ•´ä¸ªæ•™å­¦ç‰‡æ®µçš„è¯è¯­è´¨é‡ã€‚ç ”ç©¶åŸºäºGlobal Teaching InSights (GTI) è§‚å¯Ÿåè®®ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶(Attention mechanisms)æ•æ‰è½¬å½•æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æµä¹‹é—´çš„æ¨¡æ€å†…ä¸è·¨æ¨¡æ€äº¤äº’ã€‚ç ”ç©¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ (Multi-task learning)æ–¹æ³•è”åˆé¢„æµ‹è¯è¯­æ€§è´¨(Nature of Discourse)ã€æé—®(Questioning)å’Œè§£é‡Š(Explanations)çš„è´¨é‡å¾—åˆ†ï¼Œå¹¶å°†å…¶å»ºæ¨¡ä¸ºåºæ•°åˆ†ç±»(Ordinal classification)é—®é¢˜ã€‚åœ¨GTIå¾·å›½æ•°å­¦è¯¾æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–‡æœ¬æ¨¡æ€åœ¨ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œè€Œå£°å­¦ç‰¹å¾çš„åŠ å…¥è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹ä¸äººå·¥è¯„åˆ†çš„ä¸€è‡´æ€§ã€‚æœ€ç»ˆæ¨¡å‹å®ç°äº†0.384çš„äºŒæ¬¡åŠ æƒKappaç³»æ•°(Quadratic Weighted Kappa)ï¼Œå…¶è¡¨ç°ä¼˜äºäººå·¥è¯„åˆ†è€…é—´çš„ä¸€è‡´æ€§æ°´å¹³(0.326)ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘è‡ªåŠ¨åŒ–çš„è¯¾å ‚è¯è¯­è´¨é‡è¯„ä¼°å·¥å…·ã€æ”¯æŒæ•™å¸ˆä¸“ä¸šå‘å±•å¹¶æä¾›å¤šç»´åº¦åŠæ—¶åé¦ˆå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "The 18th International Conference on Educational Data Mining (EDM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.07902v1",
      "published_date": "2025-05-12 09:24:21 UTC",
      "updated_date": "2025-05-12 09:24:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:31.941744+00:00"
    },
    {
      "arxiv_id": "2505.07901v1",
      "title": "Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting",
      "title_zh": "äºŒå…ƒäº¤äº’åœºæ™¯ä¸‹ç”¨äºåºåˆ—ååº”ç”Ÿæˆçš„æ½œåœ¨è¡Œä¸ºæ‰©æ•£",
      "authors": [
        "Minh-Duc Nguyen",
        "Hyung-Jeong Yang",
        "Soo-Hyung Kim",
        "Ji-Eun Shin",
        "Seung-Won Kim"
      ],
      "abstract": "The dyadic reaction generation task involves synthesizing responsive facial reactions that align closely with the behaviors of a conversational partner, enhancing the naturalness and effectiveness of human-like interaction simulations. This paper introduces a novel approach, the Latent Behavior Diffusion Model, comprising a context-aware autoencoder and a diffusion-based conditional generator that addresses the challenge of generating diverse and contextually relevant facial reactions from input speaker behaviors. The autoencoder compresses high-dimensional input features, capturing dynamic patterns in listener reactions while condensing complex input data into a concise latent representation, facilitating more expressive and contextually appropriate reaction synthesis. The diffusion-based conditional generator operates on the latent space generated by the autoencoder to predict realistic facial reactions in a non-autoregressive manner. This approach allows for generating diverse facial reactions that reflect subtle variations in conversational cues and emotional states. Experimental results demonstrate the effectiveness of our approach in achieving superior performance in dyadic reaction synthesis tasks compared to existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒäººäº¤äº’(dyadic interaction)ä¸­çš„ååº”ç”Ÿæˆä»»åŠ¡ï¼Œæå‡ºäº† Latent Behavior Diffusion Modelï¼Œæ—¨åœ¨æ ¹æ®è¯´è¯è€…çš„è¡Œä¸ºåˆæˆè‡ªç„¶ä¸”å…·ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„é¢éƒ¨ååº”ã€‚è¯¥æ¨¡å‹ç”±ä¸Šä¸‹æ–‡æ„ŸçŸ¥è‡ªç¼–ç å™¨(context-aware autoencoder)å’ŒåŸºäºæ‰©æ•£çš„æ¡ä»¶ç”Ÿæˆå™¨(diffusion-based conditional generator)ç»„æˆã€‚è‡ªç¼–ç å™¨è´Ÿè´£å‹ç¼©é«˜ç»´ç‰¹å¾å¹¶æ•æ‰å¬è€…ååº”çš„åŠ¨æ€æ¨¡å¼ï¼Œå°†å¤æ‚è¾“å…¥è½¬åŒ–ä¸ºç®€æ´çš„æ½œè¡¨å¾(latent representation)ï¼Œä»è€Œä¿ƒè¿›æ›´å…·è¡¨ç°åŠ›çš„ååº”åˆæˆã€‚æ¡ä»¶ç”Ÿæˆå™¨åœ¨æ½œç©ºé—´å†…ä»¥éè‡ªå›å½’(non-autoregressive)çš„æ–¹å¼é¢„æµ‹é€¼çœŸçš„é¢éƒ¨ååº”ï¼Œèƒ½å¤Ÿæ•æ‰äº¤é™…çº¿ç´¢å’Œæƒ…ç»ªçŠ¶æ€ä¸­çš„ç»†å¾®å˜åŒ–å¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„ç»“æœã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒäººååº”åˆæˆä»»åŠ¡ä¸­å–å¾—äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ•ˆæœï¼Œæ˜¾è‘—æå‡äº†äººæœºäº¤äº’æ¨¡æ‹Ÿçš„è‡ªç„¶åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07901v1",
      "published_date": "2025-05-12 09:22:27 UTC",
      "updated_date": "2025-05-12 09:22:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:31.649803+00:00"
    },
    {
      "arxiv_id": "2505.07377v1",
      "title": "Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms",
      "title_zh": "è™šæ‹Ÿè¯¾å ‚ä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„äº¤äº’å¯¹æ³¨æ„åŠ›å’Œè®¤çŸ¥å‚ä¸çš„å½±å“ç ”ç©¶",
      "authors": [
        "Suleyman Ozdel",
        "Can Sarpkaya",
        "Efe Bozkir",
        "Hong Gao",
        "Enkelejda Kasneci"
      ],
      "abstract": "Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„è™šæ‹Ÿç°å®(VR)è¯¾å ‚ä¸­ï¼ŒLLMé©±åŠ¨çš„äº¤äº’å¯¹å­¦ç”Ÿæ³¨æ„åŠ›å’Œè®¤çŸ¥å‚ä¸çš„å½±å“ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå®Œå…¨ç”±LLMsé©±åŠ¨çš„è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒï¼Œå…¶ä¸­çš„åŒä¼´å’Œæ•™å¸ˆå‡ç”±æ¨¡å‹é©±åŠ¨ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†åŒä¼´æé—®è¡Œä¸º(Peer question-asking behaviors)å¯¹å­¦ç”Ÿå‚ä¸åº¦ã€æ³¨æ„åŠ›ã€è®¤çŸ¥è´Ÿè·(Cognitive load)åŠå­¦ä¹ ç»“æœçš„å½±å“ã€‚å®éªŒå‘ç°ï¼Œå½“LLMé©±åŠ¨çš„åŒä¼´æé—®æ—¶ï¼Œå­¦ç”Ÿçš„è§†è§‰æ‰«æè·¯å¾„(Visual scanpaths)æ›´å…·é’ˆå¯¹æ€§ï¼Œæ³¨æ„åŠ›æ›´é›†ä¸­äºå­¦ä¹ å†…å®¹ï¼Œä¸”åœ¨å¤æ‚å­¦ç§‘ä¸­è¡¨ç°å°¤ä¸ºæ˜¾è‘—ã€‚ç»“æœè¡¨æ˜ï¼ŒåŒä¼´æé—®å¹¶æœªç›´æ¥å¼•å…¥é¢å¤–çš„è®¤çŸ¥è´Ÿè·ï¼Œåè€Œä½¿è®¤çŸ¥è´Ÿè·ä¸å­¦ä¹ æ³¨æ„åŠ›çš„å¢åŠ å‘ˆå¼ºç›¸å…³ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè¯¥ç ”ç©¶ä¸ºä¼˜åŒ–VRå­¦ä¹ ç©ºé—´çš„äº¤äº’è®¾è®¡æä¾›äº†å»ºè®®ï¼Œæ­ç¤ºäº†LLMsåœ¨æå‡æ²‰æµ¸å¼æ•™è‚²è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to EDM 2025 (Eighteenth International Conference on Educational Data Mining)",
      "pdf_url": "https://arxiv.org/pdf/2505.07377v1",
      "published_date": "2025-05-12 09:21:19 UTC",
      "updated_date": "2025-05-12 09:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:57:39.579015+00:00"
    },
    {
      "arxiv_id": "2505.07374v1",
      "title": "AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review",
      "title_zh": "åŸºäº Transformer çš„ AIS æ•°æ®é©±åŠ¨æµ·äº‹ç›‘æµ‹ç»¼è¿°",
      "authors": [
        "Zhiye Xie",
        "Enmei Tu",
        "Xianping Fu",
        "Guoliang Yuan",
        "Yi Han"
      ],
      "abstract": "With the increasing demands for safety, efficiency, and sustainability in global shipping, Automatic Identification System (AIS) data plays an increasingly important role in maritime monitoring. AIS data contains spatial-temporal variation patterns of vessels that hold significant research value in the marine domain. However, due to its massive scale, the full potential of AIS data has long remained untapped. With its powerful sequence modeling capabilities, particularly its ability to capture long-range dependencies and complex temporal dynamics, the Transformer model has emerged as an effective tool for processing AIS data. Therefore, this paper reviews the research on Transformer-based AIS data-driven maritime monitoring, providing a comprehensive overview of the current applications of Transformer models in the marine field. The focus is on Transformer-based trajectory prediction methods, behavior detection, and prediction techniques. Additionally, this paper collects and organizes publicly available AIS datasets from the reviewed papers, performing data filtering, cleaning, and statistical analysis. The statistical results reveal the operational characteristics of different vessel types, providing data support for further research on maritime monitoring tasks. Finally, we offer valuable suggestions for future research, identifying two promising research directions. Datasets are available at https://github.com/eyesofworld/Maritime-Monitoring.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢å›é¡¾äº†åŸºäºTransformeræ¨¡å‹çš„AISæ•°æ®é©±åŠ¨å‹æµ·ä¸Šç›‘æµ‹ç ”ç©¶ï¼Œæ¢è®¨äº†å¦‚ä½•åˆ©ç”¨Transformerå¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›å¤„ç†å¤§è§„æ¨¡èˆ¹èˆ¶è‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿæ•°æ®ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†Transformeråœ¨è½¨è¿¹é¢„æµ‹(trajectory prediction)ã€è¡Œä¸ºæ£€æµ‹(behavior detection)å’Œé¢„æµ‹æŠ€æœ¯ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†å…¶æ•æ‰é•¿ç¨‹ä¾èµ–å’Œå¤æ‚æ—¶é—´åŠ¨æ€çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ”¶é›†å¹¶æ•´ç†äº†å…¬å¼€çš„AISæ•°æ®é›†ï¼Œé€šè¿‡æ•°æ®æ¸…æ´—å’Œç»Ÿè®¡åˆ†ææ­ç¤ºäº†ä¸åŒèˆ¹èˆ¶ç±»å‹çš„è¿è¡Œç‰¹å¾ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒã€‚æœ€åï¼Œè®ºæ–‡é’ˆå¯¹æœªæ¥ç ”ç©¶æå‡ºäº†å»ºè®¾æ€§å»ºè®®ï¼Œå¹¶ç¡®å®šäº†ä¸¤ä¸ªæå…·å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æå‡å…¨çƒèˆªè¿çš„å®‰å…¨æ€§ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07374v1",
      "published_date": "2025-05-12 09:17:43 UTC",
      "updated_date": "2025-05-12 09:17:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:07.740442+00:00"
    },
    {
      "arxiv_id": "2505.07372v1",
      "title": "Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data",
      "title_zh": "Synthetic Code Surgeryï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¸åˆæˆæ•°æ®ä¿®å¤ç¨‹åºç¼ºé™·ä¸æ¼æ´",
      "authors": [
        "David de-Fitero-Dominguez",
        "Antonio Garcia-Cabot",
        "Eva Garcia-Lopez"
      ],
      "abstract": "This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆåˆæˆæ•°æ®æ¥å¢å¼ºè‡ªåŠ¨ç¨‹åºä¿®å¤ (Automated Program Repair, APR) çš„æ–°æ–¹æ³•ã€‚ä¸ºè§£å†³é«˜è´¨é‡è®­ç»ƒæ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†åˆæˆæ ·æœ¬ç”Ÿæˆä¸ä¸¥æ ¼è´¨é‡è¯„ä¼°çš„åŒé˜¶æ®µæµç¨‹ï¼Œæ¶µç›–äº†12ç§ç¼–ç¨‹è¯­è¨€å’Œ13ä¸ªæ¼æ´ç±»åˆ«çš„çº¦30,000ä¸ªä»£ç ä¿®å¤å¯¹ã€‚é€šè¿‡å¯¹æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€æ€§èƒ½ç­‰äº”ä¸ªç»´åº¦è¿›è¡Œè·¨æ¨¡å‹è¯„ä¼°ï¼Œå®éªŒè¯æ˜åœ¨ VulRepair æµ‹è¯•é›†ä¸Šï¼Œç»è¿‡è¿‡æ»¤çš„åˆæˆæ•°æ®èƒ½æ˜¾è‘—æå‡å®Œç¾é¢„æµ‹ (Perfect Prediction) æˆåŠŸç‡ã€‚ç»Ÿè®¡å­¦æ£€éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ³•åœ¨è¾ƒä½çš„è®¡ç®—å¼€é”€ä¸‹ä»ä¼˜äºç°æœ‰ç³»ç»Ÿå’ŒåŸºäºçœŸå®ä¸–ç•Œæ•°æ®çš„é…ç½®ã€‚è¯¥ç ”ç©¶ç¡®ç«‹äº†ä¸€ç§è‡ªæˆ‘å¼•å¯¼ (self-bootstrapping) çš„èŒƒå¼ï¼Œä½¿ LLMs èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå¹¶è¯„ä¼°å…¶è®­ç»ƒæ•°æ®ï¼Œä¸ºè§£å†³è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„èµ„æºç¨€ç¼ºé—®é¢˜æä¾›äº†å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–ç»´æŠ¤å·¥å…·ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07372v1",
      "published_date": "2025-05-12 09:14:20 UTC",
      "updated_date": "2025-05-12 09:14:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:25.434151+00:00"
    },
    {
      "arxiv_id": "2505.07365v1",
      "title": "Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge",
      "title_zh": "DCASE 2025 æŒ‘æˆ˜èµ›ï¼šé¢å‘å£°å­¦å†…å®¹æ¨ç†çš„å¤šé¢†åŸŸéŸ³é¢‘é—®ç­”",
      "authors": [
        "Chao-Han Huck Yang",
        "Sreyan Ghosh",
        "Qing Wang",
        "Jaeyeon Kim",
        "Hengyi Hong",
        "Sonal Kumar",
        "Guirui Zhong",
        "Zhifeng Kong",
        "S Sakshi",
        "Vaibhavi Lokegaonkar",
        "Oriol Nieto",
        "Ramani Duraiswami",
        "Dinesh Manocha",
        "Gunhee Kim",
        "Jun Du",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "abstract": "We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering (AQA) benchmark spanning multiple domains of sound understanding. This task defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA) to test audio-language models on interactive question-answering over diverse acoustic scenes. We describe the dataset composition (from marine mammal calls to soundscapes and complex real-world clips), the evaluation protocol (top-1 accuracy with answer-shuffling robustness), and baseline systems (Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the development set are compared, showing strong variation across models and subsets. This challenge aims to advance the audio understanding and reasoning capabilities of audio-language models toward human-level acuity, which are crucial for enabling AI agents to perceive and interact about the world effectively.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†DCASE 2025 Challengeçš„Task 5ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šä¸ªå£°éŸ³ç†è§£é¢†åŸŸçš„éŸ³é¢‘é—®ç­”(Audio Question Answering, AQA)åŸºå‡†æµ‹è¯•ã€‚è¯¥ä»»åŠ¡å®šä¹‰äº†Bioacousticsã€Temporal Soundscapeså’ŒComplex QAä¸‰ä¸ªå­é›†ï¼Œæ—¨åœ¨æµ‹è¯•éŸ³é¢‘è¯­è¨€æ¨¡å‹(audio-language models)åœ¨å¤šæ ·åŒ–å£°å­¦åœºæ™¯ä¸‹çš„äº¤äº’å¼é—®ç­”ä¸æ¨ç†èƒ½åŠ›ã€‚æ•°æ®é›†æ¶µç›–äº†ä»æµ·æ´‹å“ºä¹³åŠ¨ç‰©é¸£å«åˆ°å¤æ‚ç°å®åœºæ™¯çš„å¤šç§éŸ³é¢‘ç´ æï¼Œå¹¶å»ºç«‹äº†åŸºäºTop-1å‡†ç¡®ç‡å’Œç­”æ¡ˆæ´—ç‰Œç¨³å¥æ€§(answer-shuffling robustness)çš„è¯„ä¼°åè®®ã€‚æ–‡ä¸­å¯¹æ¯”äº†Qwen2-Audio-7Bã€AudioFlamingo 2å’ŒGemini-2-Flashç­‰åŸºçº¿æ¨¡å‹åœ¨å¼€å‘é›†ä¸Šçš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºä¸åŒæ¨¡å‹åœ¨å„å­é›†ä»»åŠ¡é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚è¯¥æŒ‘æˆ˜èµ›æ—¨åœ¨æ¨åŠ¨éŸ³é¢‘ç†è§£ä¸æ¨ç†æŠ€æœ¯å‘äººç±»æ°´å¹³çš„æ•é”åº¦è¿ˆè¿›ï¼Œä¸ºAIæ™ºèƒ½ä½“æœ‰æ•ˆæ„ŸçŸ¥ä¸–ç•Œå¹¶è¿›è¡Œäº¤äº’å¥ å®šå…³é”®åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Preprint. DCASE 2025 Audio QA Challenge: https://dcase.community/challenge2025/task-audio-question-answering",
      "pdf_url": "https://arxiv.org/pdf/2505.07365v1",
      "published_date": "2025-05-12 09:04:16 UTC",
      "updated_date": "2025-05-12 09:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:27.089061+00:00"
    },
    {
      "arxiv_id": "2505.07364v1",
      "title": "GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models",
      "title_zh": "åŸºäºGANçš„T1è„‘éƒ¨MRIåˆæˆFDG PETå›¾åƒå¯æå‡æ·±åº¦æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½",
      "authors": [
        "Daria Zotova",
        "Nicolas Pinon",
        "Robin Trombetta",
        "Romain Bouet",
        "Julien Jung",
        "Carole Lartizien"
      ],
      "abstract": "Background and Objective. Research in the cross-modal medical image translation domain has been very productive over the past few years in tackling the scarce availability of large curated multimodality datasets with the promising performance of GAN-based architectures. However, only a few of these studies assessed task-based related performance of these synthetic data, especially for the training of deep models. Method. We design and compare different GAN-based frameworks for generating synthetic brain [18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first perform standard qualitative and quantitative visual quality evaluation. Then, we explore further impact of using these fake PET data in the training of a deep unsupervised anomaly detection (UAD) model designed to detect subtle epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic task-oriented quality metrics of the synthetic FDG PET data tailored to our unsupervised detection task, then use these fake data to train a use case UAD model combining a deep representation learning based on siamese autoencoders with a OC-SVM density support estimation model. This model is trained on normal subjects only and allows the detection of any variation from the pattern of the normal population. We compare the detection performance of models trained on 35 paired real MR T1 of normal subjects paired either on 35 true PET images or on 35 synthetic PET images generated from the best performing generative models. Performance analysis is conducted on 17 exams of epilepsy patients undergoing surgery. Results. The best performing GAN-based models allow generating realistic fake PET images of control subject with SSIM and PSNR values around 0.9 and 23.8, respectively and in distribution (ID) with regard to the true control dataset. The best UAD model trained on these synthetic normative PET data allows reaching 74% sensitivity. Conclusion. Our results confirm that GAN-based models are the best suited for MR T1 to FDG PET translation, outperforming transformer or diffusion models. We also demonstrate the diagnostic value of these synthetic data for the training of UAD models and evaluation on clinical exams of epilepsy patients. Our code and the normative image dataset are available.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ GAN æ¶æ„ä» T1 åŠ æƒ MRI æ•°æ®ç”Ÿæˆåˆæˆè„‘éƒ¨ [18F]fluorodeoxyglucose (FDG) PET å›¾åƒï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠä¸­å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜å¯¹æ¯”äº†å¤šç§ GAN æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹éç›‘ç£å¼‚å¸¸æ£€æµ‹ (UAD) ä»»åŠ¡å®šåˆ¶çš„æ–°å‹è¯Šæ–­ä»»åŠ¡è´¨é‡æŒ‡æ ‡ã€‚è¿™äº›åˆæˆæ•°æ®è¢«ç”¨äºè®­ç»ƒä¸€ä¸ªç»“åˆäº† siamese autoencoders æ·±åº¦è¡¨ç¤ºå­¦ä¹ å’Œ OC-SVM å¯†åº¦æ”¯æŒä¼°è®¡çš„ UAD æ¨¡å‹ï¼Œä»¥æ£€æµ‹ T1 MRI å’Œ FDG PET å½±åƒä¸­çš„å¾®å°ç™«ç—«ç—…ç¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä¼˜çš„ GAN æ¨¡å‹ç”Ÿæˆçš„å›¾åƒåœ¨è§†è§‰è´¨é‡ä¸Šä¸çœŸå®æ•°æ®é«˜åº¦ä¸€è‡´ï¼ŒSSIM å’Œ PSNR åˆ†åˆ«è¾¾åˆ°çº¦ 0.9 å’Œ 23.8ã€‚åœ¨ 17 ä¾‹ç™«ç—«æ‰‹æœ¯æ‚£è€…çš„ä¸´åºŠè¯„ä¼°ä¸­ï¼Œä»…åŸºäºåˆæˆè§„èŒƒåŒ–æ•°æ®è®­ç»ƒçš„ UAD æ¨¡å‹è¾¾åˆ°äº† 74% çš„æ•æ„Ÿåº¦ã€‚è¯¥ç ”ç©¶è¯å®äº† GAN åœ¨ MR T1 åˆ° FDG PET å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­ä¼˜äº transformer å’Œ diffusion æ¨¡å‹ï¼Œå¹¶å……åˆ†è¯æ˜äº†åˆæˆæ•°æ®åœ¨è®­ç»ƒä¸´åºŠè¯Šæ–­çº§ UAD æ¨¡å‹ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07364v1",
      "published_date": "2025-05-12 09:00:03 UTC",
      "updated_date": "2025-05-12 09:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:59:44.779145+00:00"
    },
    {
      "arxiv_id": "2505.07345v1",
      "title": "QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines",
      "title_zh": "QUPIDï¼šé¢å‘éŸ©å›½æœç´¢å¼•æ“æ€§èƒ½ã€æ´å¯Ÿä¸å†³ç­–å¢å¼ºçš„é‡åŒ–ç†è§£",
      "authors": [
        "Ohjoon Kwon",
        "Changsu Lee",
        "Jihye Back",
        "Lim Sun Suk",
        "Inho Kang",
        "Donghyeon Jeon"
      ],
      "abstract": "Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QUPIDï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡éŸ©å›½æœç´¢å¼•æ“æ€§èƒ½ã€æ´å¯ŸåŠ›ä¸å†³ç­–èƒ½åŠ›çš„é‡åŒ–ç†è§£æ¡†æ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡é›†æˆç”Ÿæˆå¼è½»é‡åŒ–æ¨¡å‹(SLM)ä¸åŸºäºåµŒå…¥(Embedding-based)çš„è½»é‡åŒ–æ¨¡å‹ï¼Œå¯ä»¥åœ¨ç›¸å…³æ€§è¯„ä¼°ä»»åŠ¡ä¸­è·å¾—è¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„è¡¨ç°ã€‚QUPIDåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”é¢†å…ˆLLMå¿«60å€çš„æ¨ç†é€Ÿåº¦ï¼Œæå¤§åœ°å¢å¼ºäº†åœ¨å¤„ç†æ¯æ—¥æ•°ç™¾ä¸‡æ¬¡æŸ¥è¯¢æ—¶çš„å¯æ‰©å±•æ€§ã€‚å®éªŒæ•°æ®è¯å®ï¼ŒQUPIDçš„Cohen's Kappaå€¼è¾¾åˆ°0.646ï¼Œæ˜¾è‘—é«˜äºä¸»æµLLMçš„0.387ï¼Œåœ¨å¤šç§æ–‡æ¡£ç±»å‹ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„ä¸€è‡´æ€§ã€‚åœ¨é›†æˆè‡³ç”Ÿäº§ç¯å¢ƒçš„æœç´¢æµæ°´çº¿åï¼Œè¯¥æ¨¡å‹æˆåŠŸå°†nDCG@5æŒ‡æ ‡æå‡äº†1.9%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ¨¡å‹ç»„åˆçš„æ¶æ„å¤šæ ·æ€§å¯¹äºæå‡ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„æœç´¢ç›¸å…³æ€§å’Œè¿è¥æ•ˆç‡å…·æœ‰æ˜¾è‘—æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07345v1",
      "published_date": "2025-05-12 08:35:09 UTC",
      "updated_date": "2025-05-12 08:35:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:53.541485+00:00"
    },
    {
      "arxiv_id": "2505.07344v5",
      "title": "Generative Pre-trained Autoregressive Diffusion Transformer",
      "title_zh": "ç”Ÿæˆå¼é¢„è®­ç»ƒè‡ªå›å½’æ‰©æ•£ Transformer",
      "authors": [
        "Yuan Zhang",
        "Jiacheng Jiang",
        "Guoqing Ma",
        "Zhiying Lu",
        "Haoyang Huang",
        "Jianlong Yuan",
        "Nan Duan",
        "Daxin Jiang"
      ],
      "abstract": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GPDiTï¼Œä¸€ç§ç”Ÿæˆå¼é¢„è®­ç»ƒè‡ªå›å½’æ‰©æ•£Transformer (Generative Pre-trained Autoregressive Diffusion Transformer)ï¼Œæ—¨åœ¨è¿ç»­æ½œç©ºé—´ (continuous latent space) å†…ç»Ÿä¸€æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ï¼Œä»¥å®ç°é•¿ç¨‹è§†é¢‘åˆæˆã€‚ä¸åŒäºé¢„æµ‹ç¦»æ•£è¯å…ƒ (discrete tokens)ï¼ŒGPDiT åˆ©ç”¨æ‰©æ•£æŸå¤± (diffusion loss) è‡ªå›å½’åœ°é¢„æµ‹æœªæ¥æ½œå¸§ï¼Œä»è€Œå®ç°äº†å¯¹è¿åŠ¨åŠ¨åŠ›å­¦å’Œå¸§é—´è¯­ä¹‰ä¸€è‡´æ€§çš„è‡ªç„¶å»ºæ¨¡ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†è½»é‡çº§å› æœæ³¨æ„åŠ› (causal attention) å˜ä½“å’Œæ— å‚æ•°çš„åŸºäºæ—‹è½¬çš„æ—¶é—´è°ƒèŠ‚æœºåˆ¶ (rotation-based time-conditioning mechanism)ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚è¿™ç§è¿ç»­è‡ªå›å½’æ¡†æ¶ä¸ä»…å¢å¼ºäº†ç”Ÿæˆè´¨é‡ï¼Œè¿˜èµ‹äºˆäº†æ¨¡å‹å¼ºå¤§çš„è¡¨å¾èƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒGPDiT åœ¨è§†é¢‘ç”Ÿæˆè´¨é‡ã€è§†é¢‘è¡¨å¾èƒ½åŠ›ä»¥åŠå°‘æ ·æœ¬å­¦ä¹  (few-shot learning) ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå‡¸æ˜¾äº†å…¶ä½œä¸ºè¿ç»­ç©ºé—´è§†é¢‘å»ºæ¨¡æœ‰æ•ˆæ¡†æ¶çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07344v5",
      "published_date": "2025-05-12 08:32:39 UTC",
      "updated_date": "2025-10-08 09:22:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:53.245202+00:00"
    },
    {
      "arxiv_id": "2505.08814v2",
      "title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test",
      "title_zh": "é€šè¿‡è¦†ç›–æµ‹è¯•æ¢ç©¶å›¾åƒè¯†åˆ«ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Wenkai Li",
        "Xiaoqi Li",
        "Yingjie Mao",
        "Yishun Wang"
      ],
      "abstract": "Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡è¦†ç›–æµ‹è¯•(Coverage Test)æ·±å…¥ç†è§£å›¾åƒè¯†åˆ«ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé‡ç‚¹æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)çš„å®‰å…¨æ€§æµ‹è¯•åŠå…¶è¦†ç›–åº¦é‡æ ‡å‡†ã€‚é’ˆå¯¹ç›®å‰ç¼ºä¹å…³äºæ¨¡å‹æ·±åº¦ã€é…ç½®ä¿¡æ¯ä¸ç¥ç»ç½‘ç»œè¦†ç›–ç‡ä¹‹é—´å…³ç³»çš„å®è¯ç ”ç©¶ç°çŠ¶ï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†ä¸»è¦åŠŸèƒ½(primary functionality)ã€è¾¹ç•Œ(boundary)ã€å±‚æ¬¡(hierarchy)å’Œç»“æ„è¦†ç›–(structural coverage)å››ç±»æŒ‡æ ‡ã€‚å®éªŒé€‰å–äº† LeNetã€VGG å’Œ ResNet ç­‰ä¸åŒæ¶æ„ï¼Œæ¶µç›–äº†ä» 5 å±‚åˆ° 54 å±‚çš„ 10 ç§æ¨¡å‹ï¼Œä»¥å¯¹æ¯”ç ”ç©¶ä¸åŒæ·±åº¦åŠé…ç½®å¯¹å¤šç§ç¥ç»å…ƒè¦†ç›–æŒ‡æ ‡çš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æäº†ä¿®æ­£åˆ¤å®š/æ¡ä»¶è¦†ç›–(modified decision/condition coverage)ä¸æ•°æ®é›†è§„æ¨¡ä¹‹é—´çš„å…³è”æ¨¡å¼ã€‚è¯¥è®ºæ–‡é€šè¿‡å®è¯æ•°æ®æ­ç¤ºäº†æ¨¡å‹ç‰¹æ€§ä¸è¦†ç›–ç‡ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œå¹¶ä¸ºæœªæ¥æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„å®‰å…¨æµ‹è¯•æå‡ºäº†ä¸‰ä¸ªå…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08814v2",
      "published_date": "2025-05-12 08:25:55 UTC",
      "updated_date": "2026-01-15 14:53:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:55.019405+00:00"
    },
    {
      "arxiv_id": "2505.07339v1",
      "title": "Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms",
      "title_zh": "å¤§ä¼—å¯¹å…¬å¹³ã€å¹³æƒåŠæ­§è§†æ€§å†³ç­–ç®—æ³•çš„æ€åº¦",
      "authors": [
        "Gabriel Lima",
        "Nina GrgiÄ‡-HlaÄa",
        "Markus Langer",
        "Yixin Zou"
      ],
      "abstract": "Affirmative algorithms have emerged as a potential answer to algorithmic discrimination, seeking to redress past harms and rectify the source of historical injustices. We present the results of two experiments ($N$$=$$1193$) capturing laypeople's perceptions of affirmative algorithms -- those which explicitly prioritize the historically marginalized -- in hiring and criminal justice. We contrast these opinions about affirmative algorithms with folk attitudes towards algorithms that prioritize the privileged (i.e., discriminatory) and systems that make decisions independently of demographic groups (i.e., fair). We find that people -- regardless of their political leaning and identity -- view fair algorithms favorably and denounce discriminatory systems. In contrast, we identify disagreements concerning affirmative algorithms: liberals and racial minorities rate affirmative systems as positively as their fair counterparts, whereas conservatives and those from the dominant racial group evaluate affirmative algorithms as negatively as discriminatory systems. We identify a source of these divisions: people have varying beliefs about who (if anyone) is marginalized, shaping their views of affirmative algorithms. We discuss the possibility of bridging these disagreements to bring people together towards affirmative algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä¸¤é¡¹å®éªŒï¼ˆN=1193ï¼‰æ¢è®¨äº†æ™®é€šå¤§ä¼—å¯¹å…¬å¹³ã€å¹³æƒ(Affirmative)å’Œæ­§è§†æ€§å†³ç­–ç®—æ³•çš„æ€åº¦ï¼Œæ—¨åœ¨è¯„ä¼°å¹³æƒç®—æ³•åœ¨è§£å†³ç®—æ³•æ­§è§†å’Œå†å²ä¸å…¬æ­£æ–¹é¢çš„ç¤¾ä¼šæ¥å—åº¦ã€‚ç ”ç©¶å¯¹æ¯”äº†äººä»¬åœ¨æ‹›è˜å’Œåˆ‘äº‹å¸æ³•é¢†åŸŸå¯¹ä¼˜å…ˆè€ƒè™‘è¾¹ç¼˜åŒ–ç¾¤ä½“ï¼ˆå¹³æƒï¼‰ã€ä¼˜å…ˆè€ƒè™‘ç‰¹æƒç¾¤ä½“ï¼ˆæ­§è§†æ€§ï¼‰ä»¥åŠä¸è€ƒè™‘äººå£ç‰¹å¾ï¼ˆå…¬å¹³ï¼‰ä¸‰ç§ç³»ç»Ÿçš„æ„ŸçŸ¥ã€‚ç»“æœæ˜¾ç¤ºï¼Œå—è¯•è€…æ— è®ºæ”¿æ²»å€¾å‘æˆ–èº«ä»½å¦‚ä½•ï¼Œæ™®éæ”¯æŒå…¬å¹³ç®—æ³•å¹¶è°´è´£æ­§è§†æ€§ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨å¹³æƒç®—æ³•ä¸Šå­˜åœ¨æ˜¾è‘—åˆ†æ­§ï¼šè‡ªç”±ä¸»ä¹‰è€…å’Œå°‘æ•°æ—è£”æŒç§¯ææ€åº¦ï¼Œè€Œä¿å®ˆä¸»ä¹‰è€…å’Œä¸»æµç§æ—ç¾¤ä½“åˆ™å°†å…¶è§†ä¸ºä¸æ­§è§†æ€§ç³»ç»ŸåŒæ ·è´Ÿé¢ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§åˆ†æ­§æ ¹æºäºäººä»¬å¯¹â€œè°æ˜¯è¢«è¾¹ç¼˜åŒ–è€…â€çš„è®¤çŸ¥å·®å¼‚ï¼Œå¹¶æ®æ­¤è®¨è®ºäº†å¼¥åˆç¤¾ä¼šåˆ†æ­§ä»¥æ¨åŠ¨å¹³æƒç®—æ³•åº”ç”¨çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07339v1",
      "published_date": "2025-05-12 08:25:15 UTC",
      "updated_date": "2025-05-12 08:25:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:58:57.942593+00:00"
    },
    {
      "arxiv_id": "2505.07336v1",
      "title": "SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction",
      "title_zh": "SAEN-BGSï¼šç”¨äºèƒŒæ™¯å‡é™¤çš„é«˜èƒ½æ•ˆè„‰å†²è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œ",
      "authors": [
        "Zhixuan Zhang",
        "Xiaopeng Li",
        "Qi Liu"
      ],
      "abstract": "Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SAEN-BGSï¼Œä¸€ç§åŸºäºè„‰å†²ç¥ç»ç½‘ç»œ (Spiking Neural Networks, SNNs) çš„é«˜èƒ½æ•ˆè„‰å†²è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³èƒŒæ™¯å»ºæ¨¡ (Background Subtraction, BGS) åœ¨é¢å¯¹å…‰ç…§å˜åŒ–å’ŒåŠ¨æ€å™ªå£°æ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚é€šè¿‡åˆ©ç”¨ SNN çš„å™ªå£°éŸ§æ€§å’Œæ—¶é—´åºåˆ—æ•æ„Ÿæ€§ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†å‰æ™¯ä¸èƒŒæ™¯çš„åˆ†ç¦»èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è®¾è®¡äº†è¿ç»­è„‰å†²å·ç§¯ä¸åå·ç§¯æ¨¡å— (continuous spiking conv-and-dconv block) ä½œä¸ºè§£ç å™¨çš„åŸºç¡€æ„å»ºå—ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†ä¸å¿…è¦çš„èƒŒæ™¯å™ªå£°å¹¶ä¿ç•™å…³é”®å‰æ™¯å…ƒç´ ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿½æ±‚æ›´é«˜çš„èƒ½é‡æ•ˆç‡ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäº ANN-to-SNN æ¡†æ¶çš„æ–°å‹è‡ªè’¸é¦è„‰å†²ç›‘ç£å­¦ä¹ æ–¹æ³• (self-distillation spiking supervised learning)ï¼Œå¤§å¹…é™ä½äº†ç³»ç»Ÿçš„åŠŸè€—ã€‚åœ¨ CDnet-2014 å’Œ DAVIS-2016 æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒSAEN-BGS åœ¨å¤æ‚åŠ¨æ€èƒŒæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼ŒæˆåŠŸå®ç°äº†é«˜ç²¾åº¦ä¸ä½èƒ½è€—çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Pattern Recognition",
      "pdf_url": "https://arxiv.org/pdf/2505.07336v1",
      "published_date": "2025-05-12 08:21:47 UTC",
      "updated_date": "2025-05-12 08:21:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:59:20.505105+00:00"
    },
    {
      "arxiv_id": "2505.07320v2",
      "title": "Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records",
      "title_zh": "é¢å‘æœ‰å™ªå£°ç”µå­å¥åº·æ¡£æ¡ˆçš„åŠ¨æ€æ ‡ç­¾å¢å¼ºä¸æ ¡å‡†",
      "authors": [
        "Yuhao Li",
        "Ling Luo",
        "Uwe Aickelin"
      ],
      "abstract": "Medical research, particularly in predicting patient outcomes, heavily relies on medical time series data extracted from Electronic Health Records (EHR), which provide extensive information on patient histories. Despite rigorous examination, labeling errors are inevitable and can significantly impede accurate predictions of patient outcome. To address this challenge, we propose an \\textbf{A}ttention-based Learning Framework with Dynamic \\textbf{C}alibration and Augmentation for \\textbf{T}ime series Noisy \\textbf{L}abel \\textbf{L}earning (ACTLL). This framework leverages a two-component Beta mixture model to identify the certain and uncertain sets of instances based on the fitness distribution of each class, and it captures global temporal dynamics while dynamically calibrating labels from the uncertain set or augmenting confident instances from the certain set. Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and several benchmark datasets from the UCR and UEA repositories, demonstrate that our model ACTLL has achieved state-of-the-art performance, especially under high noise levels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å¥åº·æ¡£æ¡ˆ (Electronic Health Records) åŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®ä¸­éš¾ä»¥é¿å…çš„æ ‡æ³¨é”™è¯¯é—®é¢˜ï¼Œæå‡ºäº†åä¸º ACTLL çš„æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å™ªå£°æ ‡ç­¾ä¸‹çš„é«˜æ•ˆå­¦ä¹ ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåˆ©ç”¨åŒåˆ†é‡ Beta mixture model è¯†åˆ«å®ä¾‹çš„ç¡®å®šæ€§ä¸ä¸ç¡®å®šæ€§é›†åˆï¼Œå¹¶æœ‰æ•ˆæ•æ‰å…¨å±€æ—¶é—´åŠ¨æ€ç‰¹æ€§ã€‚ACTLL é€šè¿‡åŠ¨æ€æ ¡å‡†ä¸ç¡®å®šå®ä¾‹çš„æ ‡ç­¾æˆ–å¢å¼ºé«˜ç½®ä¿¡åº¦å®ä¾‹ï¼Œæ˜¾è‘—ç¼“è§£äº†æ ‡æ³¨å™ªå£°å¯¹æ‚£è€…ç»“æœé¢„æµ‹çš„å½±å“ã€‚åœ¨ eICUã€MIMIC-IV-ED ä»¥åŠ UCR å’Œ UEA ç­‰å¤šä¸ªå¤§å‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§å™ªå£°æ°´å¹³ä¸‹å‡å–å¾—äº† state-of-the-art çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¸€æˆæœä¸ºå¤„ç†å¤æ‚åŒ»ç–—æ•°æ®ä¸­çš„æ ‡æ³¨åå·®æä¾›äº†ç¨³å¥çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæå‡äº†ä¸´åºŠé¢„æµ‹æ¨¡å‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07320v2",
      "published_date": "2025-05-12 08:06:16 UTC",
      "updated_date": "2025-06-03 05:38:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:59:13.583984+00:00"
    },
    {
      "arxiv_id": "2505.07317v2",
      "title": "How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations",
      "title_zh": "ä¼ä¸šå¦‚ä½•ç®¡ç†äººå·¥æ™ºèƒ½çš„ç¯å¢ƒå¯æŒç»­æ€§ï¼Ÿä¸€é¡¹å…³äºç»¿è‰²äººå·¥æ™ºèƒ½ä¸¾æªä¸ç›‘ç®¡çš„è®¿è°ˆç ”ç©¶",
      "authors": [
        "Ashmita Sampatsing",
        "Sophie Vos",
        "Emma Beauxis-Aussalet",
        "Justus Bogner"
      ],
      "abstract": "With the ever-growing adoption of artificial intelligence (AI), AI-based software and its negative impact on the environment are no longer negligible, and studying and mitigating this impact has become a critical area of research. However, it is currently unclear which role environmental sustainability plays during AI adoption in industry and how AI regulations influence Green AI practices and decision-making in industry. We therefore aim to investigate the Green AI perception and management of industry practitioners. To this end, we conducted a total of 11 interviews with participants from 10 different organizations that adopted AI-based software. The interviews explored three main themes: AI adoption, current efforts in mitigating the negative environmental impact of AI, and the influence of the EU AI Act and the Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that 9 of 11 participants prioritized business efficiency during AI adoption, with minimal consideration of environmental sustainability. Monitoring and mitigation of AI's environmental impact were very limited. Only one participant monitored negative environmental effects. Regarding applied mitigation practices, six participants reported no actions, with the others sporadically mentioning techniques like prompt engineering, relying on smaller models, or not overusing AI. Awareness and compliance with the EU AI Act are low, with only one participant reporting on its influence, while the CSRD drove sustainability reporting efforts primarily in larger companies. All in all, our findings reflect a lack of urgency and priority for sustainable AI among these companies. We suggest that current regulations are not very effective, which has implications for policymakers. Additionally, there is a need to raise industry awareness, but also to provide user-friendly techniques and tools for Green AI practices.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ä¼ä¸šåœ¨é‡‡ç”¨äººå·¥æ™ºèƒ½(AI)è¿‡ç¨‹ä¸­å¦‚ä½•ç®¡ç†ç¯å¢ƒå¯æŒç»­æ€§ï¼Œé€šè¿‡å¯¹æ¥è‡ª10å®¶ä¸åŒæœºæ„çš„11ä½ä»ä¸šè€…è¿›è¡Œè®¿è°ˆï¼Œæ¢è®¨äº†Green AIçš„å®è·µç°çŠ¶åŠå…¶å—ç›‘ç®¡æ”¿ç­–å½±å“çš„æƒ…å†µã€‚è®¿è°ˆé‡ç‚¹å›´ç»•AIé‡‡ç”¨ã€ç¼“è§£AIè´Ÿé¢ç¯å¢ƒå½±å“çš„ç°æœ‰å·¥ä½œï¼Œä»¥åŠæ¬§ç›ŸAIæ³•æ¡ˆ(EU AI Act)å’Œä¼ä¸šå¯æŒç»­å‘å±•æŠ¥å‘ŠæŒ‡ä»¤(CSRD)çš„å½±å“ä¸‰å¤§ä¸»é¢˜å±•å¼€ã€‚ç ”ç©¶å‘ç°ï¼Œç»å¤§å¤šæ•°å‚ä¸è€…åœ¨é‡‡ç”¨AIæ—¶ä¼˜å…ˆè€ƒè™‘ä¸šåŠ¡æ•ˆç‡ï¼Œå¯¹ç¯å¢ƒå¯æŒç»­æ€§çš„å…³æ³¨æå°‘ï¼Œä¸”å¯¹AIç¯å¢ƒå½±å“çš„ç›‘æµ‹å’Œç¼“è§£æªæ–½éå¸¸æœ‰é™ã€‚ç›®å‰ä»…æœ‰å°‘æ•°ä¼ä¸šé›¶æ˜Ÿé‡‡ç”¨äº†æç¤ºå·¥ç¨‹(Prompt Engineering)æˆ–ä½¿ç”¨è¾ƒå°æ¨¡å‹(Smaller Models)ç­‰æŠ€æœ¯ï¼Œå¤šæ•°ä¼ä¸šæœªé‡‡å–ä»»ä½•å®é™…è¡ŒåŠ¨ã€‚è°ƒæŸ¥è¿˜æ˜¾ç¤ºï¼Œå—è®¿è€…å¯¹EU AI Actçš„è®¤çŸ¥å’Œåˆè§„åº¦è¾ƒä½ï¼Œåæ˜ å‡ºä¼ä¸šå¯¹å¯æŒç»­AI(Sustainable AI)æ™®éç¼ºä¹ç´§è¿«æ„Ÿã€‚ç ”ç©¶æœ€åæŒ‡å‡ºå½“å‰æ³•è§„åœ¨æ¨åŠ¨ç»¿è‰²å®è·µæ–¹é¢æœ‰æ•ˆæ€§ä¸è¶³ï¼Œå»ºè®®æ”¿ç­–åˆ¶å®šè€…æå‡è¡Œä¸šæ„è¯†å¹¶æä¾›æ›´æ˜“ç”¨çš„Green AIå·¥å…·ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted for publication at the 11th International Conference on ICT for Sustainability (ICT4S'25), see https://conf.researchr.org/home/ict4s-2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07317v2",
      "published_date": "2025-05-12 08:03:55 UTC",
      "updated_date": "2025-11-26 14:27:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:59:23.678155+00:00"
    },
    {
      "arxiv_id": "2505.07315v2",
      "title": "FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes",
      "title_zh": "FedIFLï¼šé¢å‘æ•…éšœæ¨¡å¼ä¸ä¸€è‡´ç”µæœºé©±åŠ¨ç³»ç»Ÿçš„è”é‚¦è·¨åŸŸè¯Šæ–­æ¡†æ¶",
      "authors": [
        "Zexiao Wang",
        "Yankai Wang",
        "Xiaoqiang Liao",
        "Xinguo Ming",
        "Weiming Shen"
      ],
      "abstract": "Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focus on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features that from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedIFLï¼Œä¸€ç§é’ˆå¯¹å…·æœ‰ä¸ä¸€è‡´æ•…éšœæ¨¡å¼(inconsistent fault modes)çš„ç”µæœºé©±åŠ¨ç³»ç»Ÿ(MDSs)æ‰€è®¾è®¡çš„è”é‚¦è·¨åŸŸè¯Šæ–­æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è”é‚¦å­¦ä¹ ä¸­å› å®¢æˆ·ç«¯æ ‡ç­¾ç©ºé—´ä¸ä¸€è‡´å’ŒåŸŸåç§»(domain shifts)å¯¼è‡´çš„å…¨å±€æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—æŸé—®é¢˜ï¼ŒFedIFLåœ¨å®¢æˆ·ç«¯å†…éƒ¨è®­ç»ƒä¸­ç»“åˆäº†åŸå‹å¯¹æ¯”å­¦ä¹ (prototype contrastive learning)ä¸ç‰¹å¾ç”ŸæˆæŠ€æœ¯ï¼Œä»¥åœ¨ä¿æŠ¤éšç§çš„å‰æä¸‹ç¼“è§£åŸŸåç§»ã€‚åœ¨è·¨å®¢æˆ·ç«¯è®­ç»ƒä¸­ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ç‰¹å¾è§£è€¦(feature disentanglement)æœºåˆ¶ï¼Œé€šè¿‡è®¾è®¡è”é‚¦å®ä¾‹ä¸€è‡´æ€§æŸå¤±(federated instance consistency loss)ç¡®ä¿ä¸å˜ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨ä¸ªæ€§åŒ–æŸå¤±å’Œæ­£äº¤æŸå¤±(orthogonal loss)åŒºåˆ†ç‰¹å®šç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼ŒFedIFLä½¿èšåˆæ¨¡å‹åœ¨å…¨å±€æ ‡ç­¾ç©ºé—´å†…å®ç°äº†å‡ºè‰²çš„æ³›åŒ–æ€§èƒ½ï¼Œèƒ½å¤Ÿå¯¹æ ‡ç­¾ç©ºé—´ä¸ä¸€è‡´çš„ç›®æ ‡å®¢æˆ·ç«¯ç”µæœºç³»ç»Ÿè¿›è¡Œå‡†ç¡®çš„æ•…éšœè¯Šæ–­ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚å·¥ä¸šè¯Šæ–­åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Based on reviewer feedback, we realized that the proposed FedIFL framework does not strictly conform to federated learning principles, since sharing primary features, label spaces and generator parameters with a central server may violate FL privacy requirements",
      "pdf_url": "https://arxiv.org/pdf/2505.07315v2",
      "published_date": "2025-05-12 08:00:49 UTC",
      "updated_date": "2025-12-05 07:36:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:59:24.695393+00:00"
    },
    {
      "arxiv_id": "2505.07313v2",
      "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study",
      "title_zh": "è¿ˆå‘åä½œå¼ä¸“å®¶çŸ¥è¯†å§”æ´¾çš„å¤šæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿï¼šä¸€é¡¹æ¢ç´¢æ€§è®¾è®¡ç ”ç©¶",
      "authors": [
        "Baixuan Xu",
        "Chunyang Li",
        "Weiqi Wang",
        "Wei Fan",
        "Tianshi Zheng",
        "Haochen Shi",
        "Tao Fan",
        "Yangqiu Song",
        "Qiang Yang"
      ],
      "abstract": "Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.",
      "tldr_zh": "æœ¬ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†ç”¨äºå¢å¼ºé›†ä½“æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹(LLM)å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent systems)çš„åä½œç»“æ„è®¾è®¡ã€‚ç ”ç©¶é€šè¿‡è®¾è®¡å®éªŒæ·±å…¥åˆ†æäº†ä¸“ä¸šçŸ¥è¯†ä¸é¢†åŸŸçš„å¯¹é½(Expertise-Domain Alignment)ã€åä½œèŒƒå¼(Collaboration Paradigm)ä»¥åŠç³»ç»Ÿè§„æ¨¡(System Scale)ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œä¸“ä¸šçŸ¥è¯†å¯¹é½çš„æ”¶ç›Šé«˜åº¦ä¾èµ–äºç‰¹å®šé¢†åŸŸï¼Œä¸”åœ¨ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸­æœ€ä¸ºæœ‰æ•ˆã€‚åŒæ—¶ï¼Œç›¸æ¯”äºåƒµåŒ–çš„ä»»åŠ¡åˆ†è§£ï¼Œä¾§é‡äºæ•´åˆå¤šæ ·åŒ–çŸ¥è¯†çš„åä½œæ¨¡å¼åœ¨æå‡æ¨ç†æ€§èƒ½æ–¹é¢è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å®è¯æ¢è®¨äº†ç³»ç»Ÿæ‰©å±•å¸¦æ¥çš„è®¡ç®—æƒè¡¡(computational trade-off)ï¼Œå¼ºè°ƒäº†é«˜æ•ˆé€šä¿¡åè®®è®¾è®¡çš„å¿…è¦æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºé…ç½®ä¸“ä¸šåŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†å…·ä½“æŒ‡å—ï¼Œå¹¶è¯†åˆ«äº†å¯æ‰©å±•æ¨ç†æ¶æ„ä¸­çš„å…³é”®ç“¶é¢ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.07313v2",
      "published_date": "2025-05-12 07:59:13 UTC",
      "updated_date": "2025-05-16 09:41:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:59:25.475340+00:00"
    },
    {
      "arxiv_id": "2505.07299v1",
      "title": "Interpretable Event Diagnosis in Water Distribution Networks",
      "title_zh": "ä¾›æ°´ç®¡ç½‘ä¸­çš„å¯è§£é‡Šæ€§äº‹ä»¶è¯Šæ–­",
      "authors": [
        "AndrÃ© Artelt",
        "Stelios G. Vrachimis",
        "Demetrios G. Eliades",
        "Ulrike Kuhl",
        "Barbara Hammer",
        "Marios M. Polycarpou"
      ],
      "abstract": "The increasing penetration of information and communication technologies in the design, monitoring, and control of water systems enables the use of algorithms for detecting and identifying unanticipated events (such as leakages or water contamination) using sensor measurements. However, data-driven methodologies do not always give accurate results and are often not trusted by operators, who may prefer to use their engineering judgment and experience to deal with such events.\n  In this work, we propose a framework for interpretable event diagnosis -- an approach that assists the operators in associating the results of algorithmic event diagnosis methodologies with their own intuition and experience. This is achieved by providing contrasting (i.e., counterfactual) explanations of the results provided by fault diagnosis algorithms; their aim is to improve the understanding of the algorithm's inner workings by the operators, thus enabling them to take a more informed decision by combining the results with their personal experiences. Specifically, we propose counterfactual event fingerprints, a representation of the difference between the current event diagnosis and the closest alternative explanation, which can be presented in a graphical way. The proposed methodology is applied and evaluated on a realistic use case using the L-Town benchmark.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä¾›æ°´ç½‘ç»œï¼ˆWater Distribution Networksï¼‰ä¸­çš„äº‹ä»¶è¯Šæ–­ï¼Œæå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¯è§£é‡Šæ€§çš„è¯Šæ–­æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ“ä½œå‘˜å¯¹çº¯æ•°æ®é©±åŠ¨ç®—æ³•ä¿¡ä»»åº¦è¾ƒä½çš„ç°çŠ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æä¾›åäº‹å®è§£é‡Šï¼ˆCounterfactual Explanationsï¼‰ï¼Œå°†æ•…éšœè¯Šæ–­ç®—æ³•çš„è‡ªåŠ¨åŒ–ç»“æœä¸æ“ä½œå‘˜çš„å·¥ç¨‹ç›´è§‰å’Œå®è·µç»éªŒç›¸ç»“åˆã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®æ˜¯å¼•å…¥äº†åäº‹å®äº‹ä»¶æŒ‡çº¹ï¼ˆCounterfactual event fingerprintsï¼‰è¿™ä¸€æ¦‚å¿µï¼Œè¯¥æŒ‡çº¹èƒ½å¤Ÿä»¥å›¾å½¢åŒ–æ–¹å¼å±•ç¤ºå½“å‰äº‹ä»¶è¯Šæ–­ç»“æœä¸æœ€æ¥è¿‘çš„æ›¿ä»£è§£é‡Šä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ“ä½œå‘˜å¯ä»¥æ›´æ¸…æ™°åœ°ç†è§£ç®—æ³•çš„å†…éƒ¨è¿ä½œé€»è¾‘ï¼Œè¿›è€Œç»“åˆä¸ªäººç»éªŒåšå‡ºæ›´å…·ä¿¡æ¯æ”¯æ’‘çš„å†³ç­–ã€‚è¯¥æ–¹æ³•åœ¨åŸºäº L-Town åŸºå‡†æµ‹è¯•çš„ç°å®æ¡ˆä¾‹ä¸­å¾—åˆ°äº†æˆåŠŸåº”ç”¨ä¸è¯„ä¼°ï¼Œä¸ºæå‡æ°´åŠ¡ç³»ç»Ÿç›‘æ§çš„é€æ˜åº¦å’Œå¯é æ€§æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07299v1",
      "published_date": "2025-05-12 07:36:00 UTC",
      "updated_date": "2025-05-12 07:36:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:43.266823+00:00"
    },
    {
      "arxiv_id": "2505.07294v2",
      "title": "HuB: Learning Extreme Humanoid Balance",
      "title_zh": "HuBï¼šå­¦ä¹ ç±»äººæœºå™¨äººæé™å¹³è¡¡",
      "authors": [
        "Tong Zhang",
        "Boyuan Zheng",
        "Ruiqian Nai",
        "Yingdong Hu",
        "Yen-Jen Wang",
        "Geng Chen",
        "Fanqi Lin",
        "Jiongye Li",
        "Chuye Hong",
        "Koushil Sreenath",
        "Yang Gao"
      ],
      "abstract": "The human body demonstrates exceptional motor capabilities-such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters-both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose HuB (Humanoid Balance), a unified framework that integrates reference motion refinement, balance-aware policy learning, and sim-to-real robustness training, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy remains stable even under strong physical disturbances-such as a forceful soccer strike-while baseline methods consistently fail to complete these tasks. Project website: https://hub-robot.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HuBï¼ˆHumanoid Balanceï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººå½¢æœºå™¨äººåœ¨æ‰§è¡Œæé™å¹³è¡¡ä»»åŠ¡æ—¶é¢ä¸´çš„å‚è€ƒè¿åŠ¨è¯¯å·®ã€å½¢æ€ä¸åŒ¹é…ä»¥åŠSim-to-Realå·®è·ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå‚è€ƒè¿åŠ¨ç»†åŒ–ï¼ˆReference Motion Refinementï¼‰ã€å¹³è¡¡æ„ŸçŸ¥ç­–ç•¥å­¦ä¹ ï¼ˆBalance-Aware Policy Learningï¼‰å’ŒSim-to-Realé²æ£’æ€§è®­ç»ƒï¼Œä¸ºå¤æ‚çš„è¿åŠ¨æŠ€èƒ½æä¾›ç»Ÿä¸€çš„æ§åˆ¶æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜åœ¨Unitree G1äººå½¢æœºå™¨äººä¸ŠæˆåŠŸå®ç°äº†ç‡•å¼å¹³è¡¡ï¼ˆSwallow Balanceï¼‰å’Œæå°é¾™å¼è¸¢è…¿ï¼ˆBruce Lee's Kickï¼‰ç­‰æå…·æŒ‘æˆ˜æ€§çš„å•è…¿æ”¯æ’‘åŠ¨ä½œã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥åœ¨é¢å¯¹å¼ºåŠ›è¶³çƒæ’å‡»ç­‰å‰§çƒˆç‰©ç†å¹²æ‰°æ—¶ä»èƒ½ä¿æŒæé«˜çš„ç¨³å®šæ€§ï¼Œæ˜¾è‘—ä¼˜äºæ— æ³•å®Œæˆæ­¤ç±»ä»»åŠ¡çš„åŸºçº¿æ–¹æ³•ï¼Œä¸ºäººå½¢æœºå™¨äººå®ç°ç±»äººçš„æé™è¿åŠ¨èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2025 (Oral Presentation). Project website: https://hub-robot.github.io",
      "pdf_url": "https://arxiv.org/pdf/2505.07294v2",
      "published_date": "2025-05-12 07:31:42 UTC",
      "updated_date": "2025-08-17 08:03:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:03.170420+00:00"
    },
    {
      "arxiv_id": "2505.07289v1",
      "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?",
      "title_zh": "LLM çš„è¯­ä¹‰ä¿ç•™ä¸æç«¯å‹ç¼©ï¼šäºŒè€…èƒ½å¦å…¼å¾—ï¼Ÿ",
      "authors": [
        "Stanislas Laborde",
        "Martin Cousseau",
        "Antoun Yaacoub",
        "Lionel Prevost"
      ],
      "abstract": "The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.",
      "tldr_zh": "Large Language Model (LLM) éƒ¨ç½²éœ€æ±‚çš„æŒ‡æ•°çº§å¢é•¿ä½¿å¾—æ¨¡å‹å‹ç¼©æŠ€æœ¯åœ¨é™ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬æ–¹é¢å˜å¾—è‡³å…³é‡è¦ã€‚å°½ç®¡ Pruning (å‰ªæ) å’Œ Quantization (é‡åŒ–) å·²å±•ç°å‡ºåº”ç”¨å‰æ™¯ï¼Œä½†ä¸¤è€…çš„è”åˆå‹ç¼©æ½œåŠ›ä»æœ‰å¾…æŒ–æ˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å°† Pruning ä¸ Quantization ç­–ç•¥æ€§ç»“åˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ¯”å•ä¸€å‹ç¼©æ‰‹æ®µæ›´ä¼˜çš„æ€§èƒ½å‹ç¼©æ¯”ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ¡†æ¶çš„å±€é™æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸º Semantic Retention Compression Rate (SrCr) çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ¨¡å‹å‹ç¼©ä¸è¯­ä¹‰ä¿ç•™ (Semantic Retention) ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡è¯¥æŒ‡æ ‡ä¼˜åŒ–çš„é…ç½®æ–¹æ¡ˆåœ¨å®éªŒä¸­å±•ç°äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œåœ¨ç›¸åŒç†è®ºå‹ç¼©ç‡ä¸‹ï¼Œå…¶æ¨èçš„ç»„åˆæ–¹æ¡ˆæ¯”çº¯ Quantization æ¨¡å‹æ€§èƒ½å¹³å‡æå‡äº† 20%ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç° LLM çš„æè‡´å‹ç¼©ä¸è¯­ä¹‰ä¿ç•™å¹³è¡¡æä¾›äº†æ–°çš„è¯„ä¼°æ ‡å‡†ä¸ä¼˜åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in the Proceedings of the 2025 International Joint Conference on Neural Networks (IJCNN); this arXiv version includes an appendix with 6 result tables; 10 pages, 15 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.07289v1",
      "published_date": "2025-05-12 07:23:19 UTC",
      "updated_date": "2025-05-12 07:23:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:11.030079+00:00"
    },
    {
      "arxiv_id": "2505.07286v2",
      "title": "Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule",
      "title_zh": "é€šè¿‡æ¨¡æ€ç‰¹å®šæœ€ä¼˜è°ƒåº¦å¼•é¢†åŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡",
      "authors": [
        "Keyue Qiu",
        "Yuxuan Song",
        "Zhehuan Fan",
        "Peidong Liu",
        "Zhe Zhang",
        "Mingyue Zheng",
        "Hao Zhou",
        "Wei-Ying Ma"
      ],
      "abstract": "Structure-Based Drug Design (SBDD) is crucial for identifying bioactive molecules. Recent deep generative models are faced with challenges in geometric structure modeling. A major bottleneck lies in the twisted probability path of multi-modalities -- continuous 3D positions and discrete 2D topologies -- which jointly determine molecular geometries. By establishing the fact that noise schedules decide the Variational Lower Bound (VLB) for the twisted probability path, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored area, which optimizes VLB as a path integral for SBDD. Our model effectively enhances molecular geometries and interaction modeling, achieving state-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10% improvement upon strong baselines, while maintaining high affinities and robust intramolecular validity evaluated on held-out test set. Code is available at https://github.com/AlgoMole/MolCRAFT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡(Structure-Based Drug Design, SBDD)ä¸­æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å‡ ä½•ç»“æ„å»ºæ¨¡æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºè¿ç»­3Dä½ç½®ä¸ç¦»æ•£2Dæ‹“æ‰‘å…±åŒå†³å®šçš„æ‰­æ›²æ¦‚ç‡è·¯å¾„(twisted probability path)æ˜¯å…³é”®ç“¶é¢ˆã€‚é€šè¿‡æ­ç¤ºå™ªå£°è°ƒåº¦(noise schedules)å¯¹è¯¥è·¯å¾„å˜åˆ†ä¸‹ç•Œ(Variational Lower Bound, VLB)çš„å†³å®šæ€§ä½œç”¨ï¼Œç ”ç©¶æå‡ºäº†VLB-Optimal Scheduling (VOS)ç­–ç•¥ï¼Œå°†VLBä½œä¸ºè·¯å¾„ç§¯åˆ†è¿›è¡Œä¼˜åŒ–ä»¥å¢å¼ºåˆ†å­å‡ ä½•ä¸ç›¸äº’ä½œç”¨å»ºæ¨¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨CrossDockæ•°æ®é›†ä¸Šå–å¾—äº†95.9%çš„PoseBustersé€šè¿‡ç‡ï¼Œç›¸æ¯”å¼ºåŸºå‡†æ¨¡å‹æå‡è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜äº²å’ŒåŠ›çš„åŒæ—¶ï¼Œåœ¨ç•™å‡ºæµ‹è¯•é›†ä¸Šå±•ç°å‡ºç¨³å¥çš„åˆ†å­å†…æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07286v2",
      "published_date": "2025-05-12 07:18:09 UTC",
      "updated_date": "2025-06-05 12:37:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:12.929919+00:00"
    },
    {
      "arxiv_id": "2505.07899v2",
      "title": "On the Superimposed Noise Accumulation Problem in Sequential Knowledge Editing of Large Language Models",
      "title_zh": "è®ºå¤§è¯­è¨€æ¨¡å‹è¿ç»­çŸ¥è¯†ç¼–è¾‘ä¸­çš„å åŠ å™ªå£°ç´¯ç§¯é—®é¢˜",
      "authors": [
        "Ding Cao",
        "Yuchen Cai",
        "Yuqing Huang",
        "Xuesong He",
        "Rongxi Guo",
        "Guiquan Liu",
        "Guangzhong Sun"
      ],
      "abstract": "Sequential knowledge editing techniques aim to continuously update knowledge in large language models at low cost, preventing models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, our findings reveal that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the superimposed noise accumulation problem. Our further analysis demonstrates that the problem is related to the erroneous activation of irrelevant knowledge and conflicts between activated knowledge. Based on this analysis, a method named DeltaEdit is proposed that reduces conflicts between knowledge through dynamic orthogonal constraint strategies. Experiments show that DeltaEdit significantly reduces superimposed noise, achieving a 16.8% improvement in editing performance over the strongest baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨è¿ç»­çŸ¥è¯†ç¼–è¾‘(Sequential Knowledge Editing)ä¸­é¢ä¸´çš„æ€§èƒ½è¡°å‡é—®é¢˜ã€‚ä½œè€…é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒå‘ç°ï¼Œéšç€ç¼–è¾‘æ¬¡æ•°çš„å¢åŠ ï¼Œæ¨¡å‹è¾“å‡ºä¼šé€æ¸åç¦»ç›®æ ‡ï¼Œè¿™ç§ç°è±¡è¢«å®šä¹‰ä¸ºå åŠ å™ªå£°ç´¯ç§¯é—®é¢˜(Superimposed Noise Accumulation Problem)ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œè¯¥é—®é¢˜çš„æ ¹æºåœ¨äºæ— å…³çŸ¥è¯†çš„é”™è¯¯æ¿€æ´»ä»¥åŠä¸åŒæ¿€æ´»çŸ¥è¯†ä¹‹é—´çš„å†²çªã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†DeltaEditæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€æ­£äº¤çº¦æŸç­–ç•¥(Dynamic Orthogonal Constraint Strategies)æ¥æœ‰æ•ˆå‡å°‘çŸ¥è¯†é—´çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDeltaEditæ˜¾è‘—é™ä½äº†å åŠ å™ªå£°ï¼Œåœ¨ç¼–è¾‘æ€§èƒ½ä¸Šæ¯”æœ€å¼ºåŸºçº¿æ¨¡å‹æå‡äº†16.8%ï¼Œä¸ºå®ç°å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆã€é•¿ç¨‹çŸ¥è¯†æ›´æ–°æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07899v2",
      "published_date": "2025-05-12 07:11:26 UTC",
      "updated_date": "2025-11-27 09:22:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:43.422899+00:00"
    },
    {
      "arxiv_id": "2505.07280v1",
      "title": "Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform",
      "title_zh": "åŸºäº Spotify ç‰¹å¾åŠéŸ³é¢‘æ³¢å½¢é¢‘è°±å›¾çš„å·ç§¯ç¥ç»ç½‘ç»œéŸ³è½¨æµè¡Œåº¦é¢„æµ‹",
      "authors": [
        "Navid Falah",
        "Behnam Yousefimehr",
        "Mehdi Ghatee"
      ],
      "abstract": "In the digital streaming landscape, it's becoming increasingly challenging for artists and industry experts to predict the success of music tracks. This study introduces a pioneering methodology that uses Convolutional Neural Networks (CNNs) and Spotify data analysis to forecast the popularity of music tracks. Our approach takes advantage of Spotify's wide range of features, including acoustic attributes based on the spectrogram of audio waveform, metadata, and user engagement metrics, to capture the complex patterns and relationships that influence a track's popularity. Using a large dataset covering various genres and demographics, our CNN-based model shows impressive effectiveness in predicting the popularity of music tracks. Additionally, we've conducted extensive experiments to assess the strength and adaptability of our model across different musical styles and time periods, with promising results yielding a 97\\% F1 score. Our study not only offers valuable insights into the dynamic landscape of digital music consumption but also provides the music industry with advanced predictive tools for assessing and predicting the success of music tracks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°å­—æµåª’ä½“ç¯å¢ƒä¸‹éŸ³ä¹æˆåŠŸé¢„æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(Convolutional Neural Networks, CNNs)å’ŒSpotifyæ•°æ®åˆ†ææ¥é¢„æµ‹éŸ³ä¹æ›²ç›®å—æ¬¢è¿ç¨‹åº¦çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»¼åˆåˆ©ç”¨äº†Spotifyçš„å¤šç»´ç‰¹å¾ï¼ŒåŒ…æ‹¬åŸºäºéŸ³é¢‘æ³¢å½¢é¢‘è°±å›¾(spectrogram)çš„å£°å­¦å±æ€§ã€å…ƒæ•°æ®(metadata)ä»¥åŠç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡ï¼Œæ—¨åœ¨æ•æ‰å½±å“æ›²ç›®æµè¡Œçš„å¤æ‚æ¨¡å¼ä¸å…³è”ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶µç›–å¤šç§éŸ³ä¹æµæ´¾å’Œäººå£ç»Ÿè®¡ç‰¹å¾çš„å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†CNNæ¨¡å‹åœ¨é¢„æµ‹å—æ¬¢è¿ç¨‹åº¦æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹å…·æœ‰æå¼ºçš„ç¨³å¥æ€§ä¸è·¨é£æ ¼é€‚åº”æ€§ï¼Œå¹¶æœ€ç»ˆå–å¾—äº†97%çš„F1 scoreã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºç†è§£æ•°å­—éŸ³ä¹æ¶ˆè´¹è¶‹åŠ¿æä¾›äº†æ·±åˆ»è§è§£ï¼Œä¹Ÿä¸ºéŸ³ä¹äº§ä¸šæä¾›äº†ç”¨äºè¯„ä¼°å’Œé¢„æµ‹éŸ³ä¹ä½œå“æˆåŠŸçš„å…ˆè¿›å·¥å…·ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "12 pages, 6 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.07280v1",
      "published_date": "2025-05-12 07:03:17 UTC",
      "updated_date": "2025-05-12 07:03:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:33.596980+00:00"
    },
    {
      "arxiv_id": "2505.08810v1",
      "title": "Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication",
      "title_zh": "é¢å‘ç´§æ€¥è½¦è¾†é€šä¿¡çš„ VANETs ä¸­åŸºäºæœºå™¨å­¦ä¹ çš„ DDoS æ”»å‡»æ£€æµ‹",
      "authors": [
        "Bappa Muktar",
        "Vincent Fono",
        "Adama Nouboukpo"
      ],
      "abstract": "Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent Transportation Systems (ITS), particularly in enabling real-time communication for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks, which interfere with safety-critical communication channels, can severely impair their reliability. This study introduces a robust and scalable framework to detect DDoS attacks in highway-based VANET environments. A synthetic dataset was constructed using Network Simulator 3 (NS-3) in conjunction with the Simulation of Urban Mobility (SUMO) and further enriched with real-world mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM). Three traffic categories were simulated: DDoS, VoIP, and TCP-based video streaming (VideoTCP). The data preprocessing pipeline included normalization, signal-to-noise ratio (SNR) feature engineering, missing value imputation, and class balancing using the Synthetic Minority Over-sampling Technique (SMOTE). Feature importance was assessed using SHapley Additive exPlanations (SHAP). Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB), AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN). XGB and CB achieved the best performance, each attaining an F1-score of 96%. These results highlight the robustness of the proposed framework and its potential for real-time deployment in VANETs to secure critical emergency communications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¦è½½è‡ªç»„ç»‡ç½‘ç»œ (VANETs) åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿ (ITS) ä¸­é¢ä¸´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨ä¿æŠ¤åº”æ€¥è½¦è¾†é€šä¿¡å…å—åˆ†å¸ƒå¼æ‹’ç»æœåŠ¡ (DDoS) æ”»å‡»çš„é²æ£’æ£€æµ‹æ¡†æ¶ã€‚ç ”ç©¶äººå‘˜ç»“åˆ Network Simulator 3 (NS-3) å’Œ Simulation of Urban Mobility (SUMO)ï¼Œå¹¶æ•´åˆäº†ä» OpenStreetMap (OSM) æå–çš„å¾·å›½ A81 é«˜é€Ÿå…¬è·¯çœŸå®ç§»åŠ¨è½¨è¿¹ï¼Œæ„å»ºäº†ä¸€ä¸ªæ¶µç›– DDoSã€VoIP å’Œ VideoTCP ä¸‰ç±»æµé‡çš„åˆæˆæ•°æ®é›†ã€‚è¯¥æ¡†æ¶åŒ…å«äº†ä¸€å¥—ç”±å½’ä¸€åŒ–ã€ä¿¡å™ªæ¯” (SNR) ç‰¹å¾å·¥ç¨‹å’Œåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ (SMOTE) ç»„æˆçš„æ•°æ®é¢„å¤„ç†æµæ°´çº¿ï¼Œå¹¶åˆ©ç”¨ SHapley Additive exPlanations (SHAP) è¿›è¡Œäº†ç‰¹å¾é‡è¦æ€§è¯„ä¼°ã€‚é€šè¿‡å¯¹åŒ…æ‹¬ XGBoost (XGB)ã€CatBoost (CB) å’Œäººå·¥ç¥ç»ç½‘ç»œ (ANN) åœ¨å†…çš„ 11 ç§åˆ†ç±»å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå®éªŒç»“æœæ˜¾ç¤º XGBoost å’Œ CatBoost è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶ F1-score å‡è¾¾åˆ°äº† 96%ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…éªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¿˜å±•ç¤ºäº†å…¶åœ¨ VANETs ç¯å¢ƒä¸­å®æ—¶éƒ¨ç½²ä»¥ä¿éšœå…³é”®åº”æ€¥é€šä¿¡å®‰å…¨çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08810v1",
      "published_date": "2025-05-12 07:00:04 UTC",
      "updated_date": "2025-05-12 07:00:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:37.768070+00:00"
    },
    {
      "arxiv_id": "2505.07271v1",
      "title": "On the Robustness of Reward Models for Language Model Alignment",
      "title_zh": "è®ºè¯­è¨€æ¨¡å‹å¯¹é½ä¸­å¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§",
      "authors": [
        "Jiwoo Hong",
        "Noah Lee",
        "Eunki Kim",
        "Guijin Son",
        "Woojin Chung",
        "Aman Gupta",
        "Shao Tang",
        "James Thorne"
      ],
      "abstract": "The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)ä¸­å¹¿æ³›ä½¿ç”¨çš„Bradley-Terry(BT)æ¨¡å‹å®¹æ˜“å‡ºç°è¿‡ä¼˜åŒ–(over-optimization)å¹¶ä¸§å¤±æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚ä½œè€…åˆ†ææŒ‡å‡ºéšè—å±‚çŠ¶æ€èŒƒæ•°(hidden state norms)çš„è¿‡åº¦ç¦»æ•£æ˜¯è¿‡ä¼˜åŒ–çš„ä¸»è¦æ ¹æºï¼Œå¹¶æ®æ­¤æå‡ºäº†æ‰¹é‡å½’é›¶æ­£åˆ™åŒ–(batch-wise sum-to-zero regularization, BSR)æ–¹æ³•ã€‚BSRé€šè¿‡å¼ºåˆ¶æ¯æ‰¹æ¬¡çš„å¥–åŠ±æ€»å’Œä¸ºé›¶æ¥çº¦æŸæç«¯æ•°å€¼ï¼Œæ˜¾è‘—å¢å¼ºäº†å¥–åŠ±æ¨¡å‹(RM)åœ¨å››ç§è¿‡ä¼˜åŒ–åœºæ™¯ä¸‹çš„åˆ†å¸ƒé²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé²æ£’çš„å¥–åŠ±æ¨¡å‹èƒ½å¼•å¯¼ç­–ç•¥æ›´ç²¾å‡†åœ°å¯¹é½é»„é‡‘åå¥½æ¨¡å‹ï¼Œå…¶åœ¨8Bè§„æ¨¡æ¨¡å‹ä¸Šçš„åº”ç”¨åœ¨å¤æ‚åå¥½é¢„æµ‹ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰SOTAæ¨¡å‹5%ä»¥ä¸Šã€‚æœ€ç»ˆé€šè¿‡å¯¹8Bæ¨¡å‹è¿›è¡ŒRLOOè®­ç»ƒï¼Œåœ¨AlpacaEval 2.0è¯„æµ‹ä¸­å®ç°èƒœç‡æå‡7%çš„åŒæ—¶å‡å°‘äº†40%çš„ç”Ÿæˆé•¿åº¦ï¼Œæœ‰åŠ›è¯æ˜äº†å¢å¼ºå¥–åŠ±æ¨¡å‹é²æ£’æ€§å¯¹æå‡RLHFæ•´ä½“è¡¨ç°çš„å…³é”®ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07271v1",
      "published_date": "2025-05-12 06:48:26 UTC",
      "updated_date": "2025-05-12 06:48:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:39.281459+00:00"
    },
    {
      "arxiv_id": "2505.08809v2",
      "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of SchrÃ¶dinger Bridges",
      "title_zh": "MixBridgeï¼šåŸºäºè–›å®šè°”æ¡¥æ··åˆçš„å¼‚æ„å›¾åƒåˆ°å›¾åƒåé—¨æ”»å‡»",
      "authors": [
        "Shixi Qin",
        "Zhiyong Yang",
        "Shilong Bao",
        "Shi Wang",
        "Qianqian Xu",
        "Qingming Huang"
      ],
      "abstract": "This paper focuses on implanting multiple heterogeneous backdoor triggers in bridge-based diffusion models designed for complex and arbitrary input distributions. Existing backdoor formulations mainly address single-attack scenarios and are limited to Gaussian noise input models. To fill this gap, we propose MixBridge, a novel diffusion SchrÃ¶dinger bridge (DSB) framework to cater to arbitrary input distributions (taking I2I tasks as special cases). Beyond this trait, we demonstrate that backdoor triggers can be injected into MixBridge by directly training with poisoned image pairs. This eliminates the need for the cumbersome modifications to stochastic differential equations required in previous studies, providing a flexible tool to study backdoor behavior for bridge models. However, a key question arises: can a single DSB model train multiple backdoor triggers? Unfortunately, our theory shows that when attempting this, the model ends up following the geometric mean of benign and backdoored distributions, leading to performance conflict across backdoor tasks. To overcome this, we propose a Divide-and-Merge strategy to mix different bridges, where models are independently pre-trained for each specific objective (Divide) and then integrated into a unified model (Merge). In addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the stealthiness of MixBridge. Empirical studies across diverse generation tasks speak to the efficacy of MixBridge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MixBridgeï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£è–›å®šè°”æ¡¥ (Diffusion SchrÃ¶dinger Bridge, DSB) çš„æ–°å‹æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåœ¨å¤æ‚çš„å›¾åƒåˆ°å›¾åƒ (Image-to-Image, I2I) ä»»åŠ¡ä¸­æ¤å…¥å¤šä¸ªå¼‚æ„åé—¨è§¦å‘å™¨ã€‚ä¸ä»¥å¾€å±€é™äºé«˜æ–¯å™ªå£°è¾“å…¥å’Œå•ä¸€æ”»å‡»åœºæ™¯çš„ç ”ç©¶ä¸åŒï¼ŒMixBridge èƒ½å¤Ÿé€‚åº”ä»»æ„è¾“å…¥åˆ†å¸ƒï¼Œå¹¶æ”¯æŒé€šè¿‡ä¸­æ¯’å›¾åƒå¯¹ç›´æ¥è®­ç»ƒæ¥ç®€åŒ–åé—¨æ¤å…¥æµç¨‹ã€‚é’ˆå¯¹å•ä¸ªæ¨¡å‹åœ¨å¤„ç†å¤šè§¦å‘å™¨æ—¶å› åˆ†å¸ƒå†²çªå¯¼è‡´æ€§èƒ½ä¸‹é™çš„ç†è®ºå‘ç°ï¼Œè¯¥ç ”ç©¶æå‡ºäº†â€œå…ˆåˆ†ååˆâ€(Divide-and-Merge) ç­–ç•¥ï¼Œé€šè¿‡ç‹¬ç«‹é¢„è®­ç»ƒå„ç›®æ ‡æ¨¡å‹å†è¿›è¡Œç»Ÿä¸€é›†æˆã€‚åŒæ—¶ï¼Œæ–¹æ¡ˆä¸­å¼•å…¥äº†æƒé‡é‡æ–°åˆ†é…æ–¹æ¡ˆ (Weight Reallocation Scheme, WRS) ä»¥è¿›ä¸€æ­¥æå‡åé—¨æ”»å‡»çš„éšè”½æ€§ã€‚å®éªŒç»“æœåœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸ŠéªŒè¯äº† MixBridge åœ¨å®ç°é«˜æ•ˆä¸”éšè”½çš„å¼‚æ„åé—¨æ”»å‡»æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08809v2",
      "published_date": "2025-05-12 06:40:23 UTC",
      "updated_date": "2025-05-26 09:54:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:00:46.695769+00:00"
    },
    {
      "arxiv_id": "2505.07261v3",
      "title": "CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks",
      "title_zh": "CHDï¼šé¢å‘é•¿ç¨‹ä»»åŠ¡çš„è€¦åˆå±‚çº§æ‰©æ•£",
      "authors": [
        "Ce Hao",
        "Anxing Xiao",
        "Zhiwei Xue",
        "Harold Soh"
      ],
      "abstract": "Diffusion-based planners have shown strong performance in short-horizon tasks but often fail in complex, long-horizon settings. We trace the failure to loose coupling between high-level (HL) sub-goal selection and low-level (LL) trajectory generation, which leads to incoherent plans and degraded performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that models HL sub-goals and LL trajectories jointly within a unified diffusion process. A shared classifier passes LL feedback upstream so that sub-goals self-correct while sampling proceeds. This tight HL-LL coupling improves trajectory coherence and enables scalable long-horizon diffusion planning. Experiments across maze navigation, tabletop manipulation, and household environments show that CHD consistently outperforms both flat and hierarchical diffusion baselines. Our website is: https://sites.google.com/view/chd2025/home",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Coupled Hierarchical Diffusion (CHD) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºæ‰©æ•£æ¨¡å‹ (Diffusion-based) çš„è§„åˆ’å™¨åœ¨é•¿èˆªæ—¶ (long-horizon) ä»»åŠ¡ä¸­ï¼Œç”±äºé«˜å±‚ (high-level, HL) å­ç›®æ ‡é€‰æ‹©ä¸ä½å±‚ (low-level, LL) è½¨è¿¹ç”Ÿæˆè€¦åˆæ¾æ•£è€Œå¯¼è‡´çš„è§„åˆ’ä¸è¿è´¯é—®é¢˜ã€‚CHD å°† HL å­ç›®æ ‡å’Œ LL è½¨è¿¹å…±åŒå»ºæ¨¡åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¹¶åˆ©ç”¨å…±äº«åˆ†ç±»å™¨å°† LL åé¦ˆä¼ é€’è‡³ä¸Šæ¸¸ï¼Œä½¿å­ç›®æ ‡åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­èƒ½å¤Ÿå®ç°è‡ªæˆ‘ä¿®æ­£ã€‚è¿™ç§ç´§å¯†çš„ HL-LL è€¦åˆæœºåˆ¶æ˜¾è‘—å¢å¼ºäº†è½¨è¿¹çš„è¿è´¯æ€§ï¼Œå¹¶æœ‰æ•ˆæå‡äº†é•¿èˆªæ—¶æ‰©æ•£è§„åˆ’çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCHD åœ¨è¿·å®«å¯¼èˆª (maze navigation)ã€æ¡Œé¢æ“ä½œ (tabletop manipulation) å’Œå®¶åº­ç¯å¢ƒç­‰å¤šç§å¤æ‚ä»»åŠ¡ä¸­ï¼Œå…¶è¡¨ç°å‡ä¸€è‡´ä¼˜äºç°æœ‰çš„æ‰å¹³åŒ–å’Œåˆ†å±‚æ‰©æ•£åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07261v3",
      "published_date": "2025-05-12 06:21:48 UTC",
      "updated_date": "2025-10-12 04:52:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:01:03.430144+00:00"
    },
    {
      "arxiv_id": "2505.07260v2",
      "title": "UMoE: Unifying Attention and FFN with Shared Experts",
      "title_zh": "UMoEï¼šåˆ©ç”¨å…±äº«ä¸“å®¶ç»Ÿä¸€æ³¨æ„åŠ›æœºåˆ¶ä¸å‰é¦ˆç½‘ç»œ",
      "authors": [
        "Yuanhang Yang",
        "Chaozheng Wang",
        "Jing Li"
      ],
      "abstract": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, that reveals an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UMoEï¼Œä¸€ç§æ—¨åœ¨ç»Ÿä¸€Transformeræ¨¡å‹ä¸­Attentionå±‚å’ŒFFNå±‚Mixture of Experts (MoE)è®¾è®¡çš„æ¶æ„ã€‚é’ˆå¯¹ç°æœ‰Attention-based MoEå®ç°å¤æ‚ä¸”æ€§èƒ½æ¬ ä½³çš„ç°çŠ¶ï¼Œè¯¥ç ”ç©¶é€šè¿‡å¯¹Attentionæœºåˆ¶çš„é‡æ–°è¡¨è¿°ï¼Œæ­ç¤ºäº†å…¶å†…éƒ¨è•´å«çš„ç±»FFNç»“æ„ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼ŒUMoEå®ç°äº†Attentionå±‚çš„é«˜æ•ˆMoEåŒ–ï¼Œå¹¶æ”¯æŒFFNä¸Attentionç»„ä»¶ä¹‹é—´çš„å‚æ•°å…±äº«ã€‚å®éªŒè¡¨æ˜ï¼ŒUMoEä¸ä»…åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„Attention-based MoEæ¨¡å‹ï¼Œè¿˜æ˜¾è‘—æå‡äº†å‚æ•°åˆ©ç”¨æ•ˆç‡ã€‚è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ä¸ºå¤§è§„æ¨¡Transformeræ¨¡å‹çš„æ‰©å±•æä¾›äº†æ›´ç®€æ´ä¸”å¼ºåŠ›çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2505.07260v2",
      "published_date": "2025-05-12 06:21:44 UTC",
      "updated_date": "2025-10-23 09:59:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:02:29.574152+00:00"
    },
    {
      "arxiv_id": "2505.07258v2",
      "title": "No Query, No Access",
      "title_zh": "æ— éœ€æŸ¥è¯¢ï¼Œæ— éœ€è®¿é—®",
      "authors": [
        "Wenqiang Wang",
        "Siyuan Liang",
        "Yangshijie Zhang",
        "Xiaojun Jia",
        "Hao Lin",
        "Xiaochun Cao"
      ],
      "abstract": "Textual adversarial attacks mislead NLP models, including Large Language Models (LLMs), by subtly modifying text. While effective, existing attacks often require knowledge of the victim model, extensive queries, or access to training data, limiting real-world feasibility. To overcome these constraints, we introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which operates using only victim texts. To prevent access to the victim model, we create a shadow dataset with publicly available pre-trained models and clustering methods as a foundation for developing substitute models. To address the low attack success rate (ASR) due to insufficient information feedback, we propose the hierarchical substitution model design, generating substitute models to mitigate the failure of a single substitute model at the decision boundary.\n  Concurrently, we use diverse adversarial example generation, employing various attack methods to generate and select the adversarial example with better similarity and attack effectiveness. Experiments on the Emotion and SST5 datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR improvement of 52.08\\% while significantly reducing attack queries to 0. More importantly, we discover that VDBA poses a significant threat to LLMs such as Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without access to the API, confirming that advanced NLP models still face serious security risks. Our codes can be found at https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬å¯¹æŠ—æ€§æ”»å‡»(Textual adversarial attacks)åœ¨ç°å®ä¸­å› éœ€æ¨¡å‹çŸ¥è¯†ã€å¤§é‡æŸ¥è¯¢æˆ–è®­ç»ƒæ•°æ®è€Œå—é™çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå—å®³è€…æ•°æ®çš„å¯¹æŠ—æ”»å‡»(Victim Data-based Adversarial Attack, VDBA)ã€‚è¯¥æ–¹æ³•ä»…åˆ©ç”¨å—å®³è€…æ–‡æœ¬ï¼Œé€šè¿‡å…¬å¼€é¢„è®­ç»ƒæ¨¡å‹å’Œèšç±»æ–¹æ³•æ„å»ºå½±å­æ•°æ®é›†(shadow dataset)æ¥å¼€å‘æ›¿ä»£æ¨¡å‹ï¼Œå½»åº•é¿å…äº†å¯¹å—å®³è€…æ¨¡å‹çš„ç›´æ¥è®¿é—®ã€‚ä¸ºäº†è§£å†³åé¦ˆä¿¡æ¯ä¸è¶³å¯¼è‡´çš„æ”»å‡»æˆåŠŸç‡(ASR)è¾ƒä½çš„é—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨äº†å±‚æ¬¡åŒ–æ›¿ä»£æ¨¡å‹è®¾è®¡ï¼Œå¹¶ç»“åˆå¤šæ ·åŒ–å¯¹æŠ—æ ·æœ¬ç”ŸæˆæŠ€æœ¯æ¥ä¼˜åŒ–æ ·æœ¬çš„ç›¸ä¼¼åº¦ä¸æ”»å‡»æ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVDBAåœ¨Emotionå’ŒSST5æ•°æ®é›†ä¸Šå°†æŸ¥è¯¢æ¬¡æ•°é™è‡³0çš„åŒæ—¶ï¼Œä½¿æ”»å‡»æˆåŠŸç‡æå‡äº†52.08%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å‘ç°VDBAå¯¹Qwen2å’ŒGPTç³»åˆ—ç­‰å¤§è¯­è¨€æ¨¡å‹(LLMs)æ„æˆäº†ä¸¥é‡å¨èƒï¼Œå³ä½¿åœ¨æ— æ³•è®¿é—®APIçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°45.99%çš„æ”»å‡»æˆåŠŸç‡ï¼Œè¯å®äº†å…ˆè¿›è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ä»é¢ä¸´ä¸¥å³»çš„å®‰å…¨é£é™©ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07258v2",
      "published_date": "2025-05-12 06:19:59 UTC",
      "updated_date": "2025-08-07 18:03:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:02:19.928314+00:00"
    },
    {
      "arxiv_id": "2505.07251v1",
      "title": "Incomplete In-context Learning",
      "title_zh": "ä¸å®Œæ•´ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Wenqiang Wang",
        "Yangshijie Zhang"
      ],
      "abstract": "Large vision language models (LVLMs) achieve remarkable performance through Vision In-context Learning (VICL), a process that depends significantly on demonstrations retrieved from an extensive collection of annotated examples (retrieval database). Existing studies often assume that the retrieval database contains annotated examples for all labels. However, in real-world scenarios, delays in database updates or incomplete data annotation may result in the retrieval database containing labeled samples for only a subset of classes. We refer to this phenomenon as an \\textbf{incomplete retrieval database} and define the in-context learning under this condition as \\textbf{Incomplete In-context Learning (IICL)}. To address this challenge, we propose \\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage framework designed to mitigate the limitations of IICL. The Iterative Judgments Stage reformulates an \\(\\boldsymbol{m}\\)-class classification problem into a series of \\(\\boldsymbol{m}\\) binary classification tasks, effectively converting the IICL setting into a standard VICL scenario. The Integrated Prediction Stage further refines the classification process by leveraging both the input image and the predictions from the Iterative Judgments Stage to enhance overall classification accuracy. IJIP demonstrates considerable performance across two LVLMs and two datasets under three distinct conditions of label incompleteness, achieving the highest accuracy of 93.9\\%. Notably, even in scenarios where labels are fully available, IJIP still achieves the best performance of all six baselines. Furthermore, IJIP can be directly applied to \\textbf{Prompt Learning} and is adaptable to the \\textbf{text domain}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ (Vision In-context Learning, VICL)ä¸­é¢ä¸´çš„å®é™…æŒ‘æˆ˜ï¼Œå³ç”±äºæ ‡æ³¨ä¸å…¨æˆ–æ›´æ–°å»¶è¿Ÿå¯¼è‡´æ£€ç´¢æ•°æ®åº“ç¼ºå¤±éƒ¨åˆ†ç±»åˆ«çš„ç°è±¡ï¼Œå¹¶å°†å…¶å®šä¹‰ä¸ºä¸å®Œå…¨ä¸Šä¸‹æ–‡å­¦ä¹ (Incomplete In-context Learning, IICL)ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†è¿­ä»£åˆ¤æ–­ä¸é›†æˆé¢„æµ‹(Iterative Judgments and Integrated Prediction, IJIP)ä¸¤é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ç¬¬ä¸€é˜¶æ®µé€šè¿‡å°†å¤šåˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºä¸€ç³»åˆ—äºŒåˆ†ç±»ä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°å°†IICLè®¾å®šè½¬å˜ä¸ºæ ‡å‡†çš„VICLåœºæ™¯ï¼›ç¬¬äºŒé˜¶æ®µåˆ™ç»“åˆè¾“å…¥å›¾åƒä¸åˆå§‹é¢„æµ‹è¿›ä¸€æ­¥ä¼˜åŒ–ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIJIPåœ¨å¤šç§æ ‡ç­¾ä¸å®Œæ•´æ¡ä»¶ä¸‹å‡å–å¾—æ˜¾è‘—æˆæ•ˆï¼Œæœ€é«˜å‡†ç¡®ç‡è¾¾åˆ°93.9%ï¼Œä¸”åœ¨æ ‡ç­¾å®Œæ•´çš„æƒ…å†µä¸‹ä¾ç„¶è¶…è¶Šäº†å…­ç§åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒIJIPå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œå¯ç›´æ¥åº”ç”¨äºæç¤ºå­¦ä¹ (Prompt Learning)å¹¶æ‰©å±•è‡³çº¯æ–‡æœ¬é¢†åŸŸã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07251v1",
      "published_date": "2025-05-12 05:57:39 UTC",
      "updated_date": "2025-05-12 05:57:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:01:38.866728+00:00"
    },
    {
      "arxiv_id": "2505.07247v2",
      "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models",
      "title_zh": "SAS-Benchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çŸ­ç­”æ¡ˆè¯„åˆ†çš„ç»†ç²’åº¦åŸºå‡†",
      "authors": [
        "Peichao Lai",
        "Kexuan Zhang",
        "Yi Lin",
        "Linyihan Zhang",
        "Feiyang Ye",
        "Jinhao Yan",
        "Yanwei Xu",
        "Conghui He",
        "Yilei Wang",
        "Wentao Zhang",
        "Bin Cui"
      ],
      "abstract": "Subjective Answer Grading (SAG) plays a crucial role in education, standardized testing, and automated assessment systems, particularly for evaluating short-form responses in Short Answer Scoring (SAS). However, existing approaches often produce coarse-grained scores and lack detailed reasoning. Although large language models (LLMs) have demonstrated potential as zero-shot evaluators, they remain susceptible to bias, inconsistencies with human judgment, and limited transparency in scoring decisions. To overcome these limitations, we introduce SAS-Bench, a benchmark specifically designed for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring, expert-annotated error categories, and a diverse range of question types derived from real-world subject-specific exams. This benchmark facilitates detailed evaluation of model reasoning processes and explainability. We also release an open-source dataset containing 1,030 questions and 4,109 student responses, each annotated by domain experts. Furthermore, we conduct comprehensive experiments with various LLMs, identifying major challenges in scoring science-related questions and highlighting the effectiveness of few-shot prompting in improving scoring accuracy. Our work offers valuable insights into the development of more robust, fair, and educationally meaningful LLM-based evaluation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸»è§‚ç­”æ¡ˆè¯„åˆ†(Subjective Answer Grading)å’Œç®€ç­”é¢˜è¯„åˆ†(Short Answer Scoring)ä¸­è¯„åˆ†ç²’åº¦ç²—ç³™ä¸”ç¼ºä¹æ¨ç†ä¾æ®çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)è¡¨ç°çš„åŸºå‡†æµ‹è¯•SAS-Benchã€‚è¯¥åŸºå‡†é€šè¿‡æä¾›ç»†ç²’åº¦çš„åˆ†æ­¥è¯„åˆ†ã€ä¸“å®¶æ ‡æ³¨çš„é”™è¯¯ç±»åˆ«ä»¥åŠæºè‡ªçœŸå®å­¦ç§‘è€ƒè¯•çš„å¤šå…ƒåŒ–é¢˜å‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹è¯„åˆ†è¿‡ç¨‹çš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€æºäº†ä¸€ä¸ªåŒ…å«1,030ä¸ªé—®é¢˜åŠ4,109æ¡ç”±ä¸“å®¶æ ‡æ³¨çš„å­¦ç”Ÿå›ç­”çš„æ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†æ•°æ®æ”¯æ’‘ã€‚é€šè¿‡å¯¹å¤šç§LLMsçš„å®éªŒè¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨ç§‘å­¦ç±»é—®é¢˜è¯„åˆ†ä¸­çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶è¯å®äº†å°‘æ ·æœ¬æç¤º(few-shot prompting)å¯¹äºæå‡è¯„åˆ†å‡†ç¡®æ€§çš„æœ‰æ•ˆæ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ„å»ºæ›´ç¨³å¥ã€å…¬å¹³ä¸”å…·æœ‰æ•™è‚²åº”ç”¨ä»·å€¼çš„è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07247v2",
      "published_date": "2025-05-12 05:43:21 UTC",
      "updated_date": "2025-05-15 11:01:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:01:48.709476+00:00"
    },
    {
      "arxiv_id": "2505.07245v1",
      "title": "REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction",
      "title_zh": "REMEDIï¼šåŸºäºç›¸å¯¹ç‰¹å¾å¢å¼ºä¸è’¸é¦çš„å…ƒå­¦ä¹ ä¸å¹³è¡¡é¢„æµ‹æ¡†æ¶",
      "authors": [
        "Fei Liu",
        "Huanhuan Ren",
        "Yu Guan",
        "Xiuxu Wang",
        "Wang Lv",
        "Zhiqiang Hu",
        "Yaxi Chen"
      ],
      "abstract": "Predicting future vehicle purchases among existing owners presents a critical challenge due to extreme class imbalance (<0.5% positive rate) and complex behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning with Distillation for Imbalanced prediction), a novel multi-stage framework addressing these challenges. REMEDI first trains diverse base models to capture complementary aspects of user behavior. Second, inspired by comparative op-timization techniques, we introduce relative performance meta-features (deviation from ensemble mean, rank among peers) for effective model fusion through a hybrid-expert architecture. Third, we distill the ensemble's knowledge into a single efficient model via supervised fine-tuning with MSE loss, enabling practical deployment. Evaluated on approximately 800,000 vehicle owners, REMEDI significantly outperforms baseline approaches, achieving the business target of identifying ~50% of actual buyers within the top 60,000 recommendations at ~10% precision. The distilled model preserves the ensemble's predictive power while maintaining deployment efficiency, demonstrating REMEDI's effectiveness for imbalanced prediction in industry settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ±½è½¦å¢æ¢è´­é¢„æµ‹ä¸­å­˜åœ¨çš„æç«¯ç±»åˆ«ä¸å¹³è¡¡å’Œå¤æ‚è¡Œä¸ºæ¨¡å¼æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º REMEDI çš„å¤šé˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®­ç»ƒå¤šæ ·åŒ–çš„åŸºå‡†æ¨¡å‹ä»¥æ•æ‰ç”¨æˆ·è¡Œä¸ºçš„äº’è¡¥ç»´åº¦ï¼Œéšåå¼•å…¥äº†ç›¸å¯¹æ€§èƒ½å…ƒç‰¹å¾å¹¶é€šè¿‡æ··åˆä¸“å®¶(Hybrid-Expert)æ¶æ„è¿›è¡Œæ¨¡å‹èåˆï¼Œä»¥å¢å¼ºå¯¹å°‘æ•°ç±»æ ·æœ¬çš„è¯†åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ©ç”¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)æŠ€æœ¯å°†å¤æ‚çš„é›†æˆæ¨¡å‹è½¬åŒ–ä¸ºå•ä¸ªé«˜æ•ˆæ¨¡å‹ï¼Œç¡®ä¿äº†å·¥ä¸šåœºæ™¯ä¸‹çš„å®ç”¨éƒ¨ç½²ã€‚åœ¨çº¦80ä¸‡åè½¦ä¸»çš„å¤§è§„æ¨¡å®éªŒä¸­ï¼ŒREMEDI æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œåœ¨æ’åå‰6ä¸‡çš„æ¨èä¸­æˆåŠŸè¯†åˆ«å‡ºçº¦50%çš„å®é™…è´­ä¹°è€…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREMEDI åœ¨ä¿æŒé«˜æ€§èƒ½é¢„æµ‹çš„åŒæ—¶å…¼é¡¾äº†æ¨ç†æ•ˆç‡ï¼Œä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­çš„ä¸å¹³è¡¡é¢„æµ‹é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07245v1",
      "published_date": "2025-05-12 05:40:20 UTC",
      "updated_date": "2025-05-12 05:40:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:02:45.465982+00:00"
    },
    {
      "arxiv_id": "2505.07897v3",
      "title": "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows",
      "title_zh": "LongCodeBenchï¼šé¢å‘ 1M ä¸Šä¸‹æ–‡çª—å£çš„ç¼–ç¨‹å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Stefano Rando",
        "Luca Romani",
        "Alessio Sampieri",
        "Luca Franco",
        "John Yang",
        "Yuta Kyuragi",
        "Fabio Galasso",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5. The LCB dataset is available publicly at https://huggingface.co/datasets/Steefano/LCB and the codebase to replicate the work on this paper at https://github.com/Zteefano/long-code-bench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† LongCodeBench (LCB)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é•¿è¾¾ 1M ä¸Šä¸‹æ–‡çª—å£ä¸­ç¼–ç¨‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡åˆ©ç”¨çœŸå®çš„ GitHub issuesï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†æ¶µç›–ä»£ç ç†è§£ä¸ä¿®å¤çš„ LongCodeQA å’Œ LongSWE-Bench ä»»åŠ¡ï¼Œä¸ºç°ä»£é•¿ä¸Šä¸‹æ–‡æ¨¡å‹æä¾›äº†æå…·æŒ‘æˆ˜æ€§çš„çœŸå®æµ‹è¯•åœºæ™¯ã€‚è¯¥åŸºå‡†å¯¹ä»»åŠ¡å¤æ‚åº¦è¿›è¡Œäº†ç»†è‡´åˆ†å±‚ï¼Œè¯„ä¼°å¯¹è±¡æ¶µç›–äº†ä» Qwen2.5 14B Instruct åˆ° Google Gemini ç­‰ä¸åŒè§„æ¨¡çš„æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡å¤„ç†ç›®å‰ä»æ˜¯æ‰€æœ‰æ¨¡å‹çš„çŸ­æ¿ï¼Œé¢†å…ˆæ¨¡å‹å¦‚ Claude 3.5 Sonnet å’Œ Qwen2.5 åœ¨å¤„ç†æé•¿ä¸Šä¸‹æ–‡æ—¶å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹æ»‘ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¬å¼€ LCB æ•°æ®é›†å’Œä»£ç åº“ï¼Œä¸ºæå‡é•¿ä¸Šä¸‹æ–‡ç¼–ç¨‹æ™ºèƒ½æä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07897v3",
      "published_date": "2025-05-12 05:38:03 UTC",
      "updated_date": "2025-10-22 01:04:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:01:58.367034+00:00"
    },
    {
      "arxiv_id": "2505.07239v1",
      "title": "Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity",
      "title_zh": "Cometï¼šé€šè¿‡é¢„æµ‹æ¿€æ´»ç¨€ç–æ€§åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹éšç§æ¨ç†",
      "authors": [
        "Guang Yan",
        "Yuhui Zhang",
        "Zimu Guo",
        "Lutan Zhao",
        "Xiaojun Chen",
        "Chen Wang",
        "Wenhao Wang",
        "Dan Meng",
        "Rui Hou"
      ],
      "abstract": "With the growing use of large language models (LLMs) hosted on cloud platforms to offer inference services, privacy concerns about the potential leakage of sensitive information are escalating. Secure multi-party computation (MPC) is a promising solution to protect the privacy in LLM inference. However, MPC requires frequent inter-server communication, causing high performance overhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are not activated after non-linear activation functions, we propose an efficient private inference system, Comet. This system employs an accurate and fast predictor to predict the sparsity distribution of activation function output. Additionally, we introduce a new private inference protocol. It efficiently and securely avoids computations involving zero values by exploiting the spatial locality of the predicted sparse distribution. While this computation-avoidance approach impacts the spatiotemporal continuity of KV cache entries, we address this challenge with a low-communication overhead cache refilling strategy that merges miss requests and incorporates a prefetching mechanism. Finally, we evaluate Comet on four common LLMs and compare it with six state-of-the-art private inference systems. Comet achieves a 1.87x-2.63x speedup and a 1.94x-2.64x communication reduction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº‘å¹³å°å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†ä¸­çš„éšç§æ³„éœ²é—®é¢˜ï¼Œæ¢è®¨äº†åˆ©ç”¨å®‰å…¨å¤šæ–¹è®¡ç®—(MPC)å®ç°éšç§ä¿æŠ¤æ¨ç†æ—¶é¢ä¸´çš„é€šä¿¡å¼€é”€å·¨å¤§è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚å—åˆ°æ¨¡å‹ä¸­æ™®éå­˜åœ¨çš„æ¿€æ´»ç¨€ç–æ€§(activation sparsity)ç°è±¡å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºCometçš„é«˜æ•ˆéšç§æ¨ç†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¿«é€Ÿå‡†ç¡®çš„é¢„æµ‹å™¨æ¥é¢„åˆ¤æ¿€æ´»è¾“å‡ºçš„ç¨€ç–åˆ†å¸ƒï¼Œå¹¶ç»“åˆä¸€ç§æ–°å‹éšç§æ¨ç†åè®®ï¼Œåˆ©ç”¨ç©ºé—´å±€éƒ¨æ€§åœ¨å®‰å…¨å‰æä¸‹è§„é¿é›¶å€¼è®¡ç®—ã€‚é’ˆå¯¹è®¡ç®—è§„é¿å¯¼è‡´çš„KV cacheæ¡ç›®ä¸è¿ç»­é—®é¢˜ï¼ŒCometå¼•å…¥äº†åŒ…å«åˆå¹¶è¯·æ±‚ä¸é¢„å–æœºåˆ¶çš„ä½é€šä¿¡ç¼“å­˜å¡«å……ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å››ç§ä¸»æµLLMsä¸Šï¼ŒCometç›¸æ¯”å…­ç§ç°æœ‰æœ€å…ˆè¿›ç³»ç»Ÿå®ç°äº†1.87å€è‡³2.63å€çš„æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶å°†é€šä¿¡å¼€é”€é™ä½äº†1.94å€è‡³2.64å€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to SP 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07239v1",
      "published_date": "2025-05-12 05:29:30 UTC",
      "updated_date": "2025-05-12 05:29:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:02:11.229307+00:00"
    },
    {
      "arxiv_id": "2505.07236v1",
      "title": "UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning",
      "title_zh": "UAV-CodeAgentsï¼šåŸºäºå¤šæ™ºèƒ½ä½“ ReAct ä¸è§†è§‰-è¯­è¨€æ¨ç†çš„å¯æ‰©å±•æ— äººæœºä»»åŠ¡è§„åˆ’",
      "authors": [
        "Oleg Sautenkov",
        "Yasheerah Yaqoot",
        "Muhammad Ahsan Mustafa",
        "Faryal Batool",
        "Jeffrin Sam",
        "Artem Lykov",
        "Chih-Yung Wen",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "We present UAV-CodeAgents, a scalable multi-agent framework for autonomous UAV mission generation, built on large language and vision-language models (LLMs/VLMs). The system leverages the ReAct (Reason + Act) paradigm to interpret satellite imagery, ground high-level natural language instructions, and collaboratively generate UAV trajectories with minimal human supervision. A core component is a vision-grounded, pixel-pointing mechanism that enables precise localization of semantic targets on aerial maps. To support real-time adaptability, we introduce a reactive thinking loop, allowing agents to iteratively reflect on observations, revise mission goals, and coordinate dynamically in evolving environments.\n  UAV-CodeAgents is evaluated on large-scale mission scenarios involving industrial and environmental fire detection. Our results show that a lower decoding temperature (0.5) yields higher planning reliability and reduced execution time, with an average mission creation time of 96.96 seconds and a success rate of 93%. We further fine-tune Qwen2.5VL-7B on 9,000 annotated satellite images, achieving strong spatial grounding across diverse visual categories. To foster reproducibility and future research, we will release the full codebase and a novel benchmark dataset for vision-language-based UAV planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UAV-CodeAgentsï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å¯æ‰©å±•å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºæ— äººæœº(UAV)ä»»åŠ¡çš„è‡ªä¸»ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ReAct (Reason + Act) èŒƒå¼è§£è¯»å«æ˜Ÿå›¾åƒå¹¶è½å®é«˜å±‚è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œé€šè¿‡è§†è§‰è½åœ°çš„åƒç´ æŒ‡å‘æœºåˆ¶(pixel-pointing mechanism)å®ç°å¯¹ç©ºä¸­åœ°å›¾è¯­ä¹‰ç›®æ ‡çš„ç²¾ç¡®æœå¯»å’Œå®šä½ã€‚ä¸ºäº†æå‡å®æ—¶é€‚åº”æ€§ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ååº”å¼æ€ç»´å¾ªç¯ï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­é€šè¿‡è¿­ä»£åæ€å’Œè§‚æµ‹æ¥ä¿®è®¢ä»»åŠ¡ç›®æ ‡å¹¶è¿›è¡ŒååŒã€‚å®éªŒåœ¨ç«ç¾æ£€æµ‹ç­‰å¤§è§„æ¨¡ä»»åŠ¡åœºæ™¯ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿè¾¾åˆ°äº†93%çš„æˆåŠŸç‡ï¼Œå¹³å‡ä»»åŠ¡åˆ›å»ºæ—¶é—´ä»…ä¸º96.96ç§’ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡åœ¨9,000å¼ æ ‡æ³¨å«æ˜Ÿå›¾åƒä¸Šå¾®è°ƒQwen2.5VL-7Bï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨è§†è§‰ç±»åˆ«çš„ç©ºé—´å®šä½èƒ½åŠ›ï¼Œå¹¶æ‰¿è¯ºå‘å¸ƒä»£ç åº“å’Œå…¨æ–°çš„åŸºå‡†æ•°æ®é›†ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted",
      "pdf_url": "https://arxiv.org/pdf/2505.07236v1",
      "published_date": "2025-05-12 05:23:51 UTC",
      "updated_date": "2025-05-12 05:23:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:02:39.026592+00:00"
    },
    {
      "arxiv_id": "2505.07233v2",
      "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation",
      "title_zh": "DynamicRAGï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºä½œä¸ºåé¦ˆçš„æ£€ç´¢å¢å¼ºç”ŸæˆåŠ¨æ€é‡æ’åº",
      "authors": [
        "Jiashuo Sun",
        "Xianrui Zhong",
        "Sizhe Zhou",
        "Jiawei Han"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker. Since irrelevant documents in RAG systems can mislead the generator, the reranker plays a vital role in refining retrieved documents to enhance generation quality and explainability. However, it is challenging to determine the appropriate number of documents ($k$) that the reranker should select: too few may result in missing critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results among models of same parameter sizes. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-augmented generation, RAG)ç³»ç»Ÿä¸­é‡æ’åºå™¨(reranker)éš¾ä»¥ç¡®å®šæœ€ä¼˜æ–‡æ¡£é€‰æ‹©æ•°é‡($k$)çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºDynamicRAGçš„æ–°å‹æ¡†æ¶ã€‚DynamicRAGå°†é‡æ’åºå™¨å»ºæ¨¡ä¸ºä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ä¼˜åŒ–çš„æ™ºèƒ½ä½“ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„è¾“å‡ºè´¨é‡ä½œä¸ºå¥–åŠ±åé¦ˆï¼Œä»è€Œå®ç°æ ¹æ®æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æ£€ç´¢æ–‡æ¡£çš„é¡ºåºå’Œæ•°é‡ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æ–‡æ¡£è¿‡å°‘å¯¼è‡´å…³é”®ä¿¡æ¯ç¼ºå¤±æˆ–è¿‡å¤šå¼•å…¥å™ªå£°å¸¦æ¥çš„å¹²æ‰°ï¼Œå¢å¼ºäº†ç”Ÿæˆç»“æœçš„è´¨é‡ä¸ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚å®éªŒåœ¨ä¸ƒä¸ªçŸ¥è¯†å¯†é›†å‹æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜DynamicRAGåœ¨åŒç­‰å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art, SOTA)çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 7 figures, 15 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.07233v2",
      "published_date": "2025-05-12 05:19:01 UTC",
      "updated_date": "2025-05-16 02:47:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:03:07.567881+00:00"
    },
    {
      "arxiv_id": "2505.07215v1",
      "title": "Measuring General Intelligence with Generated Games",
      "title_zh": "é€šè¿‡ç”Ÿæˆå¼æ¸¸æˆè¯„ä¼°é€šç”¨æ™ºèƒ½",
      "authors": [
        "Vivek Verma",
        "David Huang",
        "William Chen",
        "Dan Klein",
        "Nicholas Tomlin"
      ],
      "abstract": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† gg-benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šç”¨æ¨ç†èƒ½åŠ›çš„åŠ¨æ€åšå¼ˆç¯å¢ƒåŸºå‡†é›†åˆã€‚ä¸ä¼ ç»Ÿé™æ€åŸºå‡†ä¸åŒï¼Œgg-bench é‡‡ç”¨ä¸€ç§åˆæˆç”Ÿæˆæµç¨‹ï¼Œåˆ©ç”¨ LLM ç”Ÿæˆæ–°é¢–æ¸¸æˆçš„è‡ªç„¶è¯­è¨€æè¿°å¹¶å°†å…¶å®ç°ä¸º Gym ç¯å¢ƒä»£ç ï¼Œéšåé€šè¿‡è‡ªæˆ‘åšå¼ˆï¼ˆself-playï¼‰è®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“ä½œä¸ºå¯¹æ‰‹ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œå¾…æµ‹æ¨¡å‹éœ€æ ¹æ®æ¸¸æˆæè¿°å’Œæ£‹ç›˜çŠ¶æ€è¾“å‡ºåŠ¨ä½œï¼Œå¹¶ä»¥å…¶é’ˆå¯¹ RL æ™ºèƒ½ä½“çš„èƒœç‡ä½œä¸ºè¡¡é‡æ ‡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥åŸºå‡†æå…·æŒ‘æˆ˜æ€§ï¼ŒGPT-4o å’Œ Claude 3.7 Sonnet ç­‰é¡¶å°–æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆin-context learningï¼‰ä¸‹çš„èƒœç‡ä»…ä¸º 7-9%ï¼Œè€Œ o1ã€o3-mini å’Œ DeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹çš„å¹³å‡èƒœç‡åˆ™èƒ½è¾¾åˆ° 31-36%ã€‚è¯¥é¡¹ç ”ç©¶é€šè¿‡å¼€æºæ¸¸æˆæ•°æ®ã€ç”Ÿæˆæµç¨‹åŠè¯„ä¼°ä»£ç ï¼Œä¸ºæœªæ¥å¤§æ¨¡å‹çš„é€šç”¨æ™ºèƒ½è¯„ä¼°å’ŒåŸºå‡†æ‰©å±•æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07215v1",
      "published_date": "2025-05-12 04:01:03 UTC",
      "updated_date": "2025-05-12 04:01:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:03:10.837171+00:00"
    },
    {
      "arxiv_id": "2505.07214v3",
      "title": "Towards user-centered interactive medical image segmentation in VR with an assistive AI agent",
      "title_zh": "å€ŸåŠ©è¾…åŠ© AI æ™ºèƒ½ä½“åœ¨ VR ä¸­å®ç°ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„äº¤äº’å¼åŒ»å­¦å›¾åƒåˆ†å‰²",
      "authors": [
        "Pascal Spiegler",
        "Arash Harirpoush",
        "Yiming Xiao"
      ],
      "abstract": "Crucial in disease analysis and surgical planning, manual segmentation of volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and challenging to master, while fully automatic algorithms can benefit from user feedback. Therefore, with the complementary power of the latest radiological AI foundation models and virtual reality (VR)'s intuitive data interaction, we propose SAMIRA, a novel conversational AI agent for medical VR that assists users with localizing, segmenting, and visualizing 3D medical concepts. Through speech-based interaction, the agent helps users understand radiological features, locate clinical targets, and generate segmentation masks that can be refined with just a few point prompts. The system also supports true-to-scale 3D visualization of segmented pathology to enhance patient-specific anatomical understanding. Furthermore, to determine the optimal interaction paradigm under near-far attention-switching for refining segmentation masks in an immersive, human-in-the-loop workflow, we compare VR controller pointing, head pointing, and eye tracking as input modes. With a user study, evaluations demonstrated a high usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as strong support for the proposed VR system's guidance, training potential, and integration of AI in radiological segmentation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å½±åƒåˆ†å‰²ä¸­æ‰‹åŠ¨æ“ä½œç¹çä¸”è‡ªåŠ¨ç®—æ³•ç¼ºä¹åé¦ˆçš„é—®é¢˜ï¼Œæå‡ºäº† SAMIRAï¼Œè¿™æ˜¯ä¸€ç§ä¸ºåŒ»ç–— Virtual Reality (VR) è®¾è®¡çš„æ–°å‹å¯¹è¯å¼ AI æ™ºèƒ½ä½“ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æ”¾å°„å­¦ AI åŸºç¡€æ¨¡å‹ä¸ VR çš„ç›´è§‚äº¤äº’ä¼˜åŠ¿ï¼Œé€šè¿‡è¯­éŸ³äº¤äº’è¾…åŠ©ç”¨æˆ·å®šä½ã€åˆ†å‰²å’Œå¯è§†åŒ– 3D åŒ»å­¦æ¦‚å¿µï¼Œå¹¶å…è®¸ä½¿ç”¨ç‚¹æç¤º (point prompts) å¿«é€Ÿç”Ÿæˆå¹¶ä¼˜åŒ–åˆ†å‰²æ©ç ã€‚SAMIRA æ”¯æŒç—…ç¶çš„ç­‰æ¯”ä¾‹ 3D å¯è§†åŒ–ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ç‰¹å®šè§£å‰–ç»“æ„çš„ç†è§£ã€‚ä¸ºäº†ä¼˜åŒ–äººæœºåä½œ (human-in-the-loop) å·¥ä½œæµï¼Œç ”ç©¶å¯¹æ¯”äº† VR controller pointingã€head pointing å’Œ eye tracking ä¸‰ç§è¾“å…¥æ¨¡å¼çš„äº¤äº’æ•ˆæœã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºç³»ç»Ÿå…·æœ‰æé«˜çš„å¯ç”¨æ€§è¯„åˆ† (SUS=90.0) å’Œè¾ƒä½çš„ä»»åŠ¡è´Ÿè·ã€‚å®éªŒè¯æ˜è¯¥ç³»ç»Ÿåœ¨æ”¾å°„å­¦åˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰æä½³çš„æŒ‡å¯¼ä½œç”¨ã€åŸ¹è®­æ½œåŠ›å’Œ AI é›†æˆä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07214v3",
      "published_date": "2025-05-12 03:47:05 UTC",
      "updated_date": "2025-05-25 01:26:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:03:29.100110+00:00"
    },
    {
      "arxiv_id": "2505.07896v1",
      "title": "Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability",
      "title_zh": "ç»“åˆå¤§è¯­è¨€æ¨¡å‹ä¸å•ç»†èƒè½¬å½•ç»„å­¦è§£æé€‰æ‹©æ€§è¿åŠ¨ç¥ç»å…ƒæ˜“æ„Ÿæ€§",
      "authors": [
        "Douglas Jiang",
        "Zilin Dai",
        "Luxuan Zhang",
        "Qiyi Yu",
        "Haoqi Sun",
        "Feng Tian"
      ],
      "abstract": "Understanding cell identity and function through single-cell level sequencing data remains a key challenge in computational biology. We present a novel framework that leverages gene-specific textual annotations from the NCBI Gene database to generate biologically contextualized cell embeddings. For each cell in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by expression level, retrieve their NCBI Gene descriptions, and transform these descriptions into vector embedding representations using large language models (LLMs). The models used include OpenAI text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as domain-specific models BioBERT and SciBERT. Embeddings are computed via an expression-weighted average across the top N most highly expressed genes in each cell, providing a compact, semantically rich representation. This multimodal strategy bridges structured biological data with state-of-the-art language modeling, enabling more interpretable downstream applications such as cell-type clustering, cell vulnerability dissection, and trajectory inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸å•ç»†èƒè½¬å½•ç»„å­¦(Single-Cell Transcriptomics)ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•ç»†èƒæµ‹åºæ•°æ®æ›´æ·±å…¥åœ°ç†è§£ç»†èƒèº«ä»½ä¸åŠŸèƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ NCBI Gene æ•°æ®åº“ä¸­çš„åŸºå› ç‰¹å¼‚æ€§æ–‡æœ¬æ³¨é‡Šï¼Œä¸ºå•ç»†èƒ RNA æµ‹åº(scRNA-seq)æ•°æ®é›†ç”Ÿæˆå…·æœ‰ç”Ÿç‰©å­¦èƒŒæ™¯çš„ç»†èƒåµŒå…¥(cell embeddings)ã€‚ç ”ç©¶é€šè¿‡æå–é«˜è¡¨è¾¾åŸºå› çš„ NCBI Gene æè¿°ï¼Œå¹¶åˆ©ç”¨ OpenAI text-embedding æ¨¡å‹ä»¥åŠé¢†åŸŸç‰¹å®šçš„ BioBERT å’Œ SciBERT æ¨¡å‹å°†è¿™äº›æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºå‘é‡åµŒå…¥è¡¨ç¤ºã€‚é€šè¿‡å¯¹æ¯ä¸ªç»†èƒä¸­è¡¨è¾¾é‡æœ€é«˜çš„å‰ N ä¸ªåŸºå› è¿›è¡Œè¡¨è¾¾åŠ æƒå¹³å‡è®¡ç®—ï¼Œè¯¥æ–¹æ³•æ„å»ºäº†ç´§å‡‘ä¸”è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚è¿™ç§ç­–ç•¥æœ‰æ•ˆåœ°æ¡¥æ¥äº†ç»“æ„åŒ–ç”Ÿç‰©æ•°æ®ä¸å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ä¸‹æ¸¸ç”Ÿç‰©å­¦åº”ç”¨çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶åœ¨ç»†èƒç±»å‹èšç±»(cell-type clustering)ã€ç»†èƒæ˜“æ„Ÿæ€§å‰–æ(cell vulnerability dissection)ä»¥åŠè½¨è¿¹æ¨æ–­(trajectory inference)ç­‰ä»»åŠ¡ä¸­å±•ç°äº†é‡è¦ä»·å€¼ï¼Œä¸ºå‰–æé€‰æ‹©æ€§è¿åŠ¨ç¥ç»å…ƒæ˜“æ„Ÿæ€§æä¾›äº†æ–°çš„è®¡ç®—ç”Ÿç‰©å­¦é€”å¾„ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07896v1",
      "published_date": "2025-05-12 03:39:33 UTC",
      "updated_date": "2025-05-12 03:39:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:03:43.924938+00:00"
    },
    {
      "arxiv_id": "2505.07895v3",
      "title": "Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks",
      "title_zh": "èåˆæ¨¡æ€é—´ç›¸äº’å½±å“çš„å¤šæ¨¡æ€å¼‚æ„ç½‘ç»œèŠ‚ç‚¹åˆ†ç±»è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Jiafan Li",
        "Jiaqi Zhu",
        "Liang Chang",
        "Yilin Li",
        "Miaomiao Li",
        "Yang Wang",
        "Hongan Wang"
      ],
      "abstract": "Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¼‚æ„ç½‘ç»œ(Multi-Modal Heterogeneous Networks, MMHNs)ä¸­çš„èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºHGNN-IMA (Heterogeneous Graph Neural Network with Inter-Modal Attention) çš„æ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰æ—©èåˆ(early fusion)å’Œæ™šèåˆ(late fusion)ç­–ç•¥åœ¨ä¿ç•™æ¨¡æ€ç‰¹å¾å’Œæ•æ‰è·¨æ¨¡æ€å¼•å¯¼æ–¹é¢çš„å±€é™æ€§ï¼Œè¯¥æ¨¡å‹åœ¨Heterogeneous Graph Transformeræ¡†æ¶å†…ï¼Œé€šè¿‡æ•æ‰ä¿¡æ¯ä¼ æ’­è¿‡ç¨‹ä¸­å„æ¨¡æ€é—´çš„ç›¸äº’å½±å“æ¥ä¼˜åŒ–èŠ‚ç‚¹è¡¨ç¤ºå­¦ä¹ ã€‚å…·ä½“æ–¹æ³•æ˜¯å°†åµŒå¥—çš„æ¨¡æ€é—´æ³¨æ„åŠ›æœºåˆ¶(nested inter-modal attention mechanism)é›†æˆåˆ°èŠ‚ç‚¹é—´æ³¨æ„åŠ›ä¸­ï¼Œå®ç°äº†è‡ªé€‚åº”çš„å¤šæ¨¡æ€èåˆï¼Œå¹¶å¼•å…¥æ¨¡æ€å¯¹é½(modality alignment)ä»¥ç¡®ä¿èŠ‚ç‚¹é—´ä¼ æ’­çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¢åŠ æ³¨æ„åŠ›æŸå¤±(attention loss)æ¥å‡è½»æ¨¡æ€ç¼ºå¤±å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœéªŒè¯äº†HGNN-IMAåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œä¸ºå¤„ç†å¸¦æœ‰ç½‘ç»œç»“æ„çš„å¤šæ¨¡æ€æ•°æ®æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07895v3",
      "published_date": "2025-05-12 02:59:46 UTC",
      "updated_date": "2025-06-19 09:49:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:06.651635+00:00"
    },
    {
      "arxiv_id": "2505.08808v1",
      "title": "SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction",
      "title_zh": "SparseMeXTï¼šé‡Šæ”¾é«˜ç²¾åœ°å›¾æ„å»ºä¸­ç¨€ç–è¡¨ç¤ºçš„æ½œåŠ›",
      "authors": [
        "Anqing Jiang",
        "Jinhao Chai",
        "Yu Gao",
        "Yiru Wang",
        "Yuwen Heng",
        "Zhigang Sun",
        "Hao Sun",
        "Zezhong Zhao",
        "Li Sun",
        "Jian Zhou",
        "Lijuan Zhu",
        "Shugong Xu",
        "Hao Zhao"
      ],
      "abstract": "Recent advancements in high-definition \\emph{HD} map construction have demonstrated the effectiveness of dense representations, which heavily rely on computationally intensive bird's-eye view \\emph{BEV} features. While sparse representations offer a more efficient alternative by avoiding dense BEV processing, existing methods often lag behind due to the lack of tailored designs. These limitations have hindered the competitiveness of sparse representations in online HD map construction. In this work, we systematically revisit and enhance sparse representation techniques, identifying key architectural and algorithmic improvements that bridge the gap with--and ultimately surpass--dense approaches. We introduce a dedicated network architecture optimized for sparse map feature extraction, a sparse-dense segmentation auxiliary task to better leverage geometric and semantic cues, and a denoising module guided by physical priors to refine predictions. Through these enhancements, our method achieves state-of-the-art performance on the nuScenes dataset, significantly advancing HD map construction and centerline detection. Specifically, SparseMeXt-Tiny reaches a mean average precision \\emph{mAP} of 55.5% at 32 frames per second \\emph{fps}, while SparseMeXt-Base attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for sparse representations in HD map construction. These results underscore the untapped potential of sparse methods, challenging the conventional reliance on dense representations and redefining efficiency-performance trade-offs in the field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SparseMeXTï¼Œæ—¨åœ¨æŒ–æ˜ç¨€ç–è¡¨ç¤º(sparse representations)åœ¨åœ¨çº¿é«˜ç²¾åœ°å›¾(HD map)æ„å»ºä¸­çš„æ½œåŠ›ï¼Œå¹¶è§£å†³å…¶é•¿æœŸä»¥æ¥åœ¨æ€§èƒ½ä¸Šè½åäºç¨ å¯†bird's-eye view (BEV)è¡¨ç¤ºçš„é—®é¢˜ã€‚SparseMeXTå¼•å…¥äº†ä¸“ä¸ºç¨€ç–åœ°å›¾ç‰¹å¾æå–ä¼˜åŒ–çš„ç½‘ç»œæ¶æ„ï¼Œå¹¶ç»“åˆäº†ç¨€ç–-ç¨ å¯†åˆ†å‰²è¾…åŠ©ä»»åŠ¡(sparse-dense segmentation auxiliary task)ä»¥æ›´å¥½åœ°åˆ©ç”¨å‡ ä½•å’Œè¯­ä¹‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åŒ…å«ä¸€ä¸ªå—ç‰©ç†å…ˆéªŒå¼•å¯¼çš„å»å™ªæ¨¡å—(denoising module)ï¼Œç”¨äºè¿›ä¸€æ­¥ç²¾ç»†åŒ–é¢„æµ‹ç»“æœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSparseMeXTåœ¨nuScenesæ•°æ®é›†ä¸Šè¾¾åˆ°äº†state-of-the-artæ€§èƒ½ï¼Œå…¶ä¸­SparseMeXt-Tinyåœ¨32 fpsçš„é€Ÿåº¦ä¸‹è¾¾åˆ°äº†55.5%çš„mAPï¼Œè€ŒSparseMeXt-Largeåˆ™å®ç°äº†68.9%çš„mAPã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†ç¨€ç–è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„ç¨ å¯†æ–¹æ¡ˆï¼Œé‡æ–°å®šä¹‰äº†è¯¥é¢†åŸŸçš„æ•ˆç‡ä¸æ€§èƒ½æƒè¡¡ï¼Œå¹¶ä¸ºé«˜ç²¾åœ°å›¾æ„å»ºå’Œä¸­å¿ƒçº¿æ£€æµ‹(centerline detection)å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.08808v1",
      "published_date": "2025-05-12 02:26:58 UTC",
      "updated_date": "2025-05-12 02:26:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:36.489781+00:00"
    },
    {
      "arxiv_id": "2505.07178v1",
      "title": "Accountability of Generative AI: Exploring a Precautionary Approach for \"Artificially Created Nature\"",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é—®è´£ï¼šé’ˆå¯¹â€œäººé€ è‡ªç„¶â€çš„é¢„é˜²æ€§è·¯å¾„æ¢ç´¢",
      "authors": [
        "Yuri Nakao"
      ],
      "abstract": "The rapid development of generative artificial intelligence (AI) technologies raises concerns about the accountability of sociotechnical systems. Current generative AI systems rely on complex mechanisms that make it difficult for even experts to fully trace the reasons behind the outputs. This paper first examines existing research on AI transparency and accountability and argues that transparency is not a sufficient condition for accountability but can contribute to its improvement. We then discuss that if it is not possible to make generative AI transparent, generative AI technology becomes ``artificially created nature'' in a metaphorical sense, and suggest using the precautionary principle approach to consider AI risks. Finally, we propose that a platform for citizen participation is needed to address the risks of generative AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) çš„é—®è´£åˆ¶ (Accountability) æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç³»ç»Ÿå¤æ‚æ€§å¯¼è‡´å³ä½¿æ˜¯ä¸“å®¶ä¹Ÿéš¾ä»¥å®Œå…¨è¿½è¸ªå…¶è¾“å‡ºé€»è¾‘ã€‚æ–‡ç« è®ºè¯äº†è™½ç„¶é€æ˜åº¦ (Transparency) å¹¶éé—®è´£åˆ¶çš„å……åˆ†æ¡ä»¶ï¼Œä½†å®ƒæ˜¯æå‡é—®è´£æ°´å¹³çš„å…³é”®å› ç´ ã€‚é’ˆå¯¹æ— æ³•å®ç°å®Œå…¨é€æ˜çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œç ”ç©¶æå‡ºå°†å…¶è§†ä¸ºä¸€ç§éšå–»å±‚é¢çš„â€œäººå·¥åˆ›é€ çš„è‡ªç„¶â€ (Artificially Created Nature)ï¼Œå¹¶å»ºè®®é‡‡ç”¨é¢„é˜²åŸåˆ™ (Precautionary Principle) æ¥è¯„ä¼°å’Œåº”å¯¹ç›¸å…³é£é™©ã€‚æœ€åï¼Œä½œè€…ä¸»å¼ å»ºç«‹å…¬æ°‘å‚ä¸ (Citizen Participation) å¹³å°ï¼Œé€šè¿‡ç¤¾ä¼šåŒ–ååŒæœºåˆ¶è§£å†³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¼•å‘çš„å„ç§ç¤¾ä¼šæŠ€æœ¯é£é™©ï¼Œä»è€Œä¸ºæ„å»ºæ›´è´Ÿè´£ä»»çš„ AI ç³»ç»Ÿæä¾›ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.07178v1",
      "published_date": "2025-05-12 02:10:55 UTC",
      "updated_date": "2025-05-12 02:10:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:53.845664+00:00"
    },
    {
      "arxiv_id": "2505.08807v1",
      "title": "Security of Internet of Agents: Attacks and Countermeasures",
      "title_zh": "æ™ºèƒ½ä½“äº’è”ç½‘å®‰å…¨ï¼šæ”»å‡»ä¸å¯¹ç­–",
      "authors": [
        "Yuntao Wang",
        "Yanghe Pan",
        "Shaolong Guo",
        "Zhou Su"
      ],
      "abstract": "With the rise of large language and vision-language models, AI agents have evolved into autonomous, interactive systems capable of perception, reasoning, and decision-making. As they proliferate across virtual and physical domains, the Internet of Agents (IoA) has emerged as a key infrastructure for enabling scalable and secure coordination among heterogeneous agents. This survey offers a comprehensive examination of the security and privacy landscape in IoA systems. We begin by outlining the IoA architecture and its distinct vulnerabilities compared to traditional networks, focusing on four critical aspects: identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. We then review existing and emerging defense mechanisms and highlight persistent challenges. Finally, we identify open research directions to advance the development of resilient and privacy-preserving IoA ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ™ºèƒ½ä½“äº’è”ç½‘(Internet of Agents, IoA)ç³»ç»Ÿçš„å®‰å…¨ä¸éšç§æ™¯è§‚è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å…´èµ·ï¼ŒAIæ™ºèƒ½ä½“å·²æ¼”å˜ä¸ºå…·å¤‡æ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›çš„è‡ªä¸»äº¤äº’ç³»ç»Ÿã€‚æ–‡ç« é¦–å…ˆæ¦‚è¿°äº†IoAæ¶æ„åŠå…¶ç›¸å¯¹äºä¼ ç»Ÿç½‘ç»œçš„ç‹¬ç‰¹æ¼æ´ï¼Œé‡ç‚¹åˆ†æäº†èº«ä»½è®¤è¯å¨èƒ(identity authentication threats)ã€è·¨æ™ºèƒ½ä½“ä¿¡ä»»é—®é¢˜(cross-agent trust issues)ã€å…·èº«å®‰å…¨(embodied security)å’Œéšç§é£é™©(privacy risks)å››ä¸ªå…³é”®ç»´åº¦ã€‚éšåï¼Œç ”ç©¶å›é¡¾äº†ç°æœ‰åŠæ–°å…´çš„é˜²å¾¡æœºåˆ¶ï¼Œå¹¶æ€»ç»“äº†è¯¥é¢†åŸŸé¢ä¸´çš„æŒç»­æŒ‘æˆ˜ã€‚æœ€åï¼Œæœ¬æ–‡ç¡®å®šäº†å¼€æ”¾æ€§ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨æ¨åŠ¨æ„å»ºå…·æœ‰éŸ§æ€§å’Œéšç§ä¿æŠ¤èƒ½åŠ›çš„IoAç”Ÿæ€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 5 figures, 3 tables, submitted to IEEE OJCS",
      "pdf_url": "https://arxiv.org/pdf/2505.08807v1",
      "published_date": "2025-05-12 02:04:57 UTC",
      "updated_date": "2025-05-12 02:04:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:06.496882+00:00"
    },
    {
      "arxiv_id": "2505.07176v2",
      "title": "Internet of Agents: Fundamentals, Applications, and Challenges",
      "title_zh": "æ™ºèƒ½ä½“äº’è”ç½‘ï¼šåŸºç¡€ã€åº”ç”¨ä¸æŒ‘æˆ˜",
      "authors": [
        "Yuntao Wang",
        "Shaolong Guo",
        "Yanghe Pan",
        "Zhou Su",
        "Fahao Chen",
        "Tom H. Luan",
        "Peng Li",
        "Jiawen Kang",
        "Dusit Niyato"
      ],
      "abstract": "With the rapid proliferation of large language models and vision-language models, AI agents have evolved from isolated, task-specific systems into autonomous, interactive entities capable of perceiving, reasoning, and acting without human intervention. As these agents proliferate across virtual and physical environments, from virtual assistants to embodied robots, the need for a unified, agent-centric infrastructure becomes paramount. In this survey, we introduce the Internet of Agents (IoA) as a foundational framework that enables seamless interconnection, dynamic discovery, and collaborative orchestration among heterogeneous agents at scale. We begin by presenting a general IoA architecture, highlighting its hierarchical organization, distinguishing features relative to the traditional Internet, and emerging applications. Next, we analyze the key operational enablers of IoA, including capability notification and discovery, adaptive communication protocols, dynamic task matching, consensus and conflict-resolution mechanisms, and incentive models. Finally, we identify open research directions toward building resilient and trustworthy IoA ecosystems.",
      "tldr_zh": "è¯¥ç»¼è¿°å¼•å…¥äº†æ™ºèƒ½ä½“äº’è”ç½‘ (Internet of Agents, IoA) è¿™ä¸€åŸºç¡€æ€§æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤§è§„æ¨¡å¼‚æ„æ™ºèƒ½ä½“ (heterogeneous agents) ä¹‹é—´çš„æ— ç¼äº’è¿ã€åŠ¨æ€å‘ç°å’Œåä½œç¼–æ’ã€‚æ–‡ç« é¦–å…ˆå±•ç¤ºäº† IoA çš„å±‚çº§æ¶æ„åŠå…¶ä¸ä¼ ç»Ÿäº’è”ç½‘çš„åŒºåˆ«ï¼Œå¹¶æ¢è®¨äº†å…¶ä»è™šæ‹ŸåŠ©æ‰‹åˆ°å…·èº«æœºå™¨äººç­‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº† IoA çš„å…³é”®è¿è¡Œä½¿èƒ½å› ç´ ï¼ŒåŒ…æ‹¬èƒ½åŠ›é€šçŸ¥ä¸å‘ç° (capability notification and discovery)ã€è‡ªé€‚åº”é€šä¿¡åè®® (adaptive communication protocols)ã€åŠ¨æ€ä»»åŠ¡åŒ¹é…ä»¥åŠå…±è¯†ä¸å†²çªè§£å†³æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜æ¢è®¨äº† IoA çš„æ¿€åŠ±æ¨¡å‹åœ¨åä½œç¼–æ’ä¸­çš„é‡è¦æ€§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶ç¡®å®šäº†æ„å»ºå…·æœ‰éŸ§æ€§å’Œå¯ä¿¡åº¦çš„ IoA ç”Ÿæ€ç³»ç»Ÿæ‰€é¢ä¸´çš„å¼€æ”¾æ€§ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "25 pages,10 figures, 10 tables. Accepted by IEEE TCCN in Oct. 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07176v2",
      "published_date": "2025-05-12 02:04:37 UTC",
      "updated_date": "2025-10-16 09:32:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:07.623554+00:00"
    },
    {
      "arxiv_id": "2505.07171v1",
      "title": "ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion",
      "title_zh": "ReCDAPï¼šåŸºäºå…³ç³»æ¡ä»¶æ‰©æ•£ä¸æ³¨æ„åŠ›æ± åŒ–çš„å°‘æ ·æœ¬çŸ¥è¯†å›¾è°±è¡¥å…¨",
      "authors": [
        "Jeongho Kim",
        "Chanyeong Heo",
        "Jaehee Jung"
      ],
      "abstract": "Knowledge Graphs (KGs), composed of triples in the form of (head, relation, tail) and consisting of entities and relations, play a key role in information retrieval systems such as question answering, entity search, and recommendation. In real-world KGs, although many entities exist, the relations exhibit a long-tail distribution, which can hinder information retrieval performance. Previous few-shot knowledge graph completion studies focused exclusively on the positive triple information that exists in the graph or, when negative triples were incorporated, used them merely as a signal to indicate incorrect triples. To overcome this limitation, we propose Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First, negative triples are generated by randomly replacing the tail entity in the support set. By conditionally incorporating positive information in the KG and non-existent negative information into the diffusion process, the model separately estimates the latent distributions for positive and negative relations. Moreover, including an attention pooler enables the model to leverage the differences between positive and negative cases explicitly. Experiments on two widely used datasets demonstrate that our method outperforms existing approaches, achieving state-of-the-art performance. The code is available at https://github.com/hou27/ReCDAP-FKGC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•ŒçŸ¥è¯†å›¾è°±(Knowledge Graphs)ä¸­å…³ç³»åˆ†å¸ƒå‘ˆç°é•¿å°¾åˆ†å¸ƒä¸”å°‘æ ·æœ¬å­¦ä¹ æ•ˆæœå—é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºReCDAPçš„åŸºäºå…³ç³»çš„å¸¦æ³¨æ„åŠ›æ± åŒ–çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚ä»¥å¾€çš„å°‘æ ·æœ¬çŸ¥è¯†å›¾è°±è¡¥å…¨(Few-Shot Knowledge Graph Completion)ç ”ç©¶é€šå¸¸åªå…³æ³¨æ­£å‘ä¸‰å…ƒç»„ï¼Œæˆ–ä»…å°†è´Ÿå‘ä¸‰å…ƒç»„ä½œä¸ºé”™è¯¯ä¿¡å·ï¼Œè€ŒReCDAPé€šè¿‡éšæœºæ›¿æ¢å°¾å®ä½“ç”Ÿæˆè´Ÿå‘ä¸‰å…ƒç»„ï¼Œå¹¶å°†æ­£è´Ÿä¿¡æ¯æ¡ä»¶åŒ–åœ°å¼•å…¥æ‰©æ•£è¿‡ç¨‹(Diffusion Process)ï¼Œä»è€Œåˆ†åˆ«ä¼°è®¡æ­£è´Ÿå…³ç³»çš„æ½œåœ¨åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†æ³¨æ„åŠ›æ± åŒ–å™¨(Attention Pooler)ä»¥æ˜¾å¼åˆ©ç”¨æ­£è´Ÿæ¡ˆä¾‹ä¹‹é—´çš„å·®å¼‚ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªé€šç”¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReCDAPè¶…è¶Šäº†ç°æœ‰æ–¹æ³•å¹¶å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†ç»“åˆè´Ÿå‘ä¿¡æ¯ä¸æ‰©æ•£æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by SIGIR 2025, 5 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.07171v1",
      "published_date": "2025-05-12 01:49:52 UTC",
      "updated_date": "2025-05-12 01:49:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:23.902342+00:00"
    },
    {
      "arxiv_id": "2505.07161v1",
      "title": "Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue",
      "title_zh": "è¿ˆå‘å¯æ“ä½œçš„æ•™å­¦åé¦ˆï¼šæ•°å­¦æ•™å­¦ä¸è¾…å¯¼å¯¹è¯çš„å¤šè§†è§’åˆ†æ",
      "authors": [
        "Jannatun Naim",
        "Jie Cao",
        "Fareen Tasneem",
        "Jennifer Jacobs",
        "Brent Milne",
        "James Martin",
        "Tamara Sumner"
      ],
      "abstract": "Effective feedback is essential for refining instructional practices in mathematics education, and researchers often turn to advanced natural language processing (NLP) models to analyze classroom dialogues from multiple perspectives. However, utterance-level discourse analysis encounters two primary challenges: (1) multifunctionality, where a single utterance may serve multiple purposes that a single tag cannot capture, and (2) the exclusion of many utterances from domain-specific discourse move classifications, leading to their omission in feedback. To address these challenges, we proposed a multi-perspective discourse analysis that integrates domain-specific talk moves with dialogue act (using the flattened multi-functional SWBD-MASL schema with 43 tags) and discourse relation (applying Segmented Discourse Representation Theory with 16 relations). Our top-down analysis framework enables a comprehensive understanding of utterances that contain talk moves, as well as utterances that do not contain talk moves. This is applied to two mathematics education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through distributional unigram analysis, sequential talk move analysis, and multi-view deep dive, we discovered meaningful discourse patterns, and revealed the vital role of utterances without talk moves, demonstrating that these utterances, far from being mere fillers, serve crucial functions in guiding, acknowledging, and structuring classroom discourse. These insights underscore the importance of incorporating discourse relations and dialogue acts into AI-assisted education systems to enhance feedback and create more responsive learning environments. Our framework may prove helpful for providing human educator feedback, but also aiding in the development of AI agents that can effectively emulate the roles of both educators and students.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°å­¦æ•™å­¦å¯¹è¯åˆ†æä¸­è¯è¯­å¤šåŠŸèƒ½æ€§åŠéé¢†åŸŸç‰¹å®šè¯è¯­è¢«å¿½è§†çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šè§†è§’è¯è¯­åˆ†ææ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°é›†æˆäº†é¢†åŸŸç‰¹å®šçš„ talk movesã€åŸºäº SWBD-MASL æ¨¡å¼çš„ dialogue act ä»¥åŠåŸºäº Segmented Discourse Representation Theory (SDRT) çš„è¯è¯­å…³ç³»åˆ†æã€‚é€šè¿‡å¯¹ TalkMoves å’Œ SAGA22 ä¸¤ä¸ªæ•°æ®é›†çš„ distributional unigram analysis å’Œåºåˆ—åˆ†æï¼Œç ”ç©¶å‘ç°é‚£äº›ä»¥å¾€è¢«è§†ä¸ºå¡«å……è¯­ã€ä¸å« talk moves çš„è¯è¯­åœ¨å¼•å¯¼ã€ç¡®è®¤å’Œæ„å»ºè¯¾å ‚è¯è¯­ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¿™äº›æ´å¯Ÿå¼ºè°ƒäº†å°†è¯è¯­å…³ç³»å’Œ dialogue act çº³å…¥ AI è¾…åŠ©æ•™è‚²ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œæ—¨åœ¨å¢å¼ºæ•™å­¦åé¦ˆè´¨é‡å¹¶åˆ›å»ºæ›´å…·å“åº”æ€§çš„å­¦ä¹ ç¯å¢ƒã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½ä¸ºäººç±»æ•™è‚²è€…æä¾›å¯æ“ä½œçš„æ”¹è¿›å»ºè®®ï¼Œè¿˜èƒ½æœ‰æ•ˆè¾…åŠ©å¼€å‘èƒ½å¤Ÿæ¨¡æ‹Ÿæ•™å¸ˆä¸å­¦ç”Ÿè§’è‰²çš„ AI agentsã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EDM'2025",
      "pdf_url": "https://arxiv.org/pdf/2505.07161v1",
      "published_date": "2025-05-12 00:48:17 UTC",
      "updated_date": "2025-05-12 00:48:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T08:04:28.141467+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 139,
  "processed_papers_count": 139,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T08:05:51.523141+00:00"
}
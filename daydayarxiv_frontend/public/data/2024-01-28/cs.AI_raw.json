[
  {
    "arxiv_id": "2401.15803v2",
    "title": "GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes and Minimalist Workflow",
    "authors": [
      "Liguo Zhou",
      "Yinglei Song",
      "Yichao Gao",
      "Zhou Yu",
      "Michael Sodamin",
      "Hongshen Liu",
      "Liang Ma",
      "Lian Liu",
      "Hao Liu",
      "Yang Liu",
      "Haichuan Li",
      "Guang Chen",
      "Alois Knoll"
    ],
    "abstract": "Conducting real road testing for autonomous driving algorithms can be\nexpensive and sometimes impractical, particularly for small startups and\nresearch institutes. Thus, simulation becomes an important method for\nevaluating these algorithms. However, the availability of free and open-source\nsimulators is limited, and the installation and configuration process can be\ndaunting for beginners and interdisciplinary researchers. We introduce an\nautonomous driving simulator with photorealistic scenes, meanwhile keeping a\nuser-friendly workflow. The simulator is able to communicate with external\nalgorithms through ROS2 or Socket.IO, making it compatible with existing\nsoftware stacks. Furthermore, we implement a highly accurate vehicle dynamics\nmodel within the simulator to enhance the realism of the vehicle's physical\neffects. The simulator is able to serve various functions, including generating\nsynthetic data and driving with machine learning-based algorithms. Moreover, we\nprioritize simplicity in the deployment process, ensuring that beginners find\nit approachable and user-friendly.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15803v2",
    "published_date": "2024-01-28 23:26:15 UTC",
    "updated_date": "2024-01-30 15:57:22 UTC"
  },
  {
    "arxiv_id": "2401.15801v1",
    "title": "On the Statistical Properties of Generative Adversarial Models for Low Intrinsic Data Dimension",
    "authors": [
      "Saptarshi Chakraborty",
      "Peter L. Bartlett"
    ],
    "abstract": "Despite the remarkable empirical successes of Generative Adversarial Networks\n(GANs), the theoretical guarantees for their statistical accuracy remain rather\npessimistic. In particular, the data distributions on which GANs are applied,\nsuch as natural images, are often hypothesized to have an intrinsic\nlow-dimensional structure in a typically high-dimensional feature space, but\nthis is often not reflected in the derived rates in the state-of-the-art\nanalyses. In this paper, we attempt to bridge the gap between the theory and\npractice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),\nby deriving statistical guarantees on the estimated densities in terms of the\nintrinsic dimension of the data and the latent space. We analytically show that\nif one has access to $n$ samples from the unknown target distribution and the\nnetwork architectures are properly chosen, the expected Wasserstein-1 distance\nof the estimates from the target scales as $O\\left( n^{-1/d_\\mu } \\right)$ for\nGANs and $O\\left( n^{-1/(d_\\mu+\\ell)} \\right)$ for BiGANs, where $d_\\mu$ and\n$\\ell$ are the upper Wasserstein-1 dimension of the data-distribution and\nlatent-space dimension, respectively. The theoretical analyses not only suggest\nthat these methods successfully avoid the curse of dimensionality, in the sense\nthat the exponent of $n$ in the error rates does not depend on the data\ndimension but also serve to bridge the gap between the theoretical analyses of\nGANs and the known sharp rates from optimal transport literature. Additionally,\nwe demonstrate that GANs can effectively achieve the minimax optimal rate even\nfor non-smooth underlying distributions, with the use of larger generator\nnetworks.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15801v1",
    "published_date": "2024-01-28 23:18:10 UTC",
    "updated_date": "2024-01-28 23:18:10 UTC"
  },
  {
    "arxiv_id": "2401.16450v2",
    "title": "ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections",
    "authors": [
      "Calista Huang",
      "Alyssa Ma",
      "Suchir Vyasamudri",
      "Eugenie Puype",
      "Sayem Kamal",
      "Juan Belza Garcia",
      "Salar Cheema",
      "Michael Lutz"
    ],
    "abstract": "With the increasing need for inclusive and user-friendly technology, web\naccessibility is crucial to ensuring equal access to online content for\nindividuals with disabilities, including visual, auditory, cognitive, or motor\nimpairments. Despite the existence of accessibility guidelines and standards\nsuch as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility\nInitiative (W3C), over 90% of websites still fail to meet the necessary\naccessibility requirements. For web users with disabilities, there exists a\nneed for a tool to automatically fix web page accessibility errors. While\nresearch has demonstrated methods to find and target accessibility errors, no\nresearch has focused on effectively correcting such violations. This paper\npresents a novel approach to correcting accessibility violations on the web by\nmodifying the document object model (DOM) in real time with foundation models.\nLeveraging accessibility error information, large language models (LLMs), and\nprompt engineering techniques, we achieved greater than a 51% reduction in\naccessibility violation errors after corrections on our novel benchmark:\nACCESS. Our work demonstrates a valuable approach toward the direction of\ninclusive web content, and provides directions for future research to explore\nadvanced methods to automate web accessibility.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.16450v2",
    "published_date": "2024-01-28 22:49:33 UTC",
    "updated_date": "2024-02-10 20:17:11 UTC"
  },
  {
    "arxiv_id": "2401.15773v1",
    "title": "Evaluation of k-means time series clustering based on z-normalization and NP-Free",
    "authors": [
      "Ming-Chang Lee",
      "Jia-Chun Lin",
      "Volker Stolz"
    ],
    "abstract": "Despite the widespread use of k-means time series clustering in various\ndomains, there exists a gap in the literature regarding its comprehensive\nevaluation with different time series normalization approaches. This paper\nseeks to fill this gap by conducting a thorough performance evaluation of\nk-means time series clustering on real-world open-source time series datasets.\nThe evaluation focuses on two distinct normalization techniques:\nz-normalization and NP-Free. The former is one of the most commonly used\nnormalization approach for time series. The latter is a real-time time series\nrepresentation approach, which can serve as a time series normalization\napproach. The primary objective of this paper is to assess the impact of these\ntwo normalization techniques on k-means time series clustering in terms of its\nclustering quality. The experiments employ the silhouette score, a\nwell-established metric for evaluating the quality of clusters in a dataset. By\nsystematically investigating the performance of k-means time series clustering\nwith these two normalization techniques, this paper addresses the current gap\nin k-means time series clustering evaluation and contributes valuable insights\nto the development of time series clustering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 6 figures, 8 tables, 13th International Conference on\n  Pattern Recognition Applications and Methods (ICPRAM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.15773v1",
    "published_date": "2024-01-28 21:23:13 UTC",
    "updated_date": "2024-01-28 21:23:13 UTC"
  },
  {
    "arxiv_id": "2401.15766v1",
    "title": "EEG for fatigue monitoring",
    "authors": [
      "Ildar Rakhmatulin"
    ],
    "abstract": "Physiological fatigue, a state of reduced cognitive and physical performance\nresulting from prolonged mental or physical exertion, poses significant\nchallenges in various domains, including healthcare, aviation, transportation,\nand industrial sectors. As the understanding of fatigue's impact on human\nperformance grows, there is a growing interest in developing effective fatigue\nmonitoring techniques. Among these techniques, electroencephalography (EEG) has\nemerged as a promising tool for objectively assessing physiological fatigue due\nto its non-invasiveness, high temporal resolution, and sensitivity to neural\nactivity. This paper aims to provide a comprehensive analysis of the current\nstate of the use of EEG for monitoring physiological fatigue.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15766v1",
    "published_date": "2024-01-28 21:01:45 UTC",
    "updated_date": "2024-01-28 21:01:45 UTC"
  },
  {
    "arxiv_id": "2401.15753v2",
    "title": "An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion",
    "authors": [
      "Sharib Ali",
      "Yamid Espinel",
      "Yueming Jin",
      "Peng Liu",
      "Bianca GÃ¼ttner",
      "Xukun Zhang",
      "Lihua Zhang",
      "Tom Dowrick",
      "Matthew J. Clarkson",
      "Shiting Xiao",
      "Yifan Wu",
      "Yijun Yang",
      "Lei Zhu",
      "Dai Sun",
      "Lan Li",
      "Micha Pfeiffer",
      "Shahid Farid",
      "Lena Maier-Hein",
      "Emmanuel Buc",
      "Adrien Bartoli"
    ],
    "abstract": "Augmented reality for laparoscopic liver resection is a visualisation mode\nthat allows a surgeon to localise tumours and vessels embedded within the liver\nby projecting them on top of a laparoscopic image. Preoperative 3D models\nextracted from CT or MRI data are registered to the intraoperative laparoscopic\nimages during this process. In terms of 3D-2D fusion, most of the algorithms\nmake use of anatomical landmarks to guide registration. These landmarks include\nthe liver's inferior ridge, the falciform ligament, and the occluding contours.\nThey are usually marked by hand in both the laparoscopic image and the 3D\nmodel, which is time-consuming and may contain errors if done by a\nnon-experienced user. Therefore, there is a need to automate this process so\nthat augmented reality can be used effectively in the operating room. We\npresent the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge\n(P2ILF), held during the Medical Imaging and Computer Assisted Interventions\n(MICCAI 2022) conference, which investigates the possibilities of detecting\nthese landmarks automatically and using them in registration. The challenge was\ndivided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2D\nregistration task. The teams were provided with training data consisting of 167\nlaparoscopic images and 9 preoperative 3D models from 9 patients, with the\ncorresponding 2D and 3D landmark annotations. A total of 6 teams from 4\ncountries participated, whose proposed methods were evaluated on 16 images and\ntwo preoperative 3D models from two patients. All the teams proposed deep\nlearning-based methods for the 2D and 3D landmark segmentation tasks and\ndifferentiable rendering-based methods for the registration task. Based on the\nexperimental outcomes, we propose three key hypotheses that determine current\nlimitations and future directions for research in this domain.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.15753v2",
    "published_date": "2024-01-28 20:30:14 UTC",
    "updated_date": "2024-02-07 11:47:38 UTC"
  },
  {
    "arxiv_id": "2401.15741v7",
    "title": "SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks",
    "authors": [
      "Serdar Erisen"
    ],
    "abstract": "Improving the efficiency of state-of-the-art methods in semantic segmentation\nrequires overcoming the increasing computational cost as well as issues such as\nfusing semantic information from global and local contexts. Based on the recent\nsuccess and problems that convolutional neural networks (CNNs) encounter in\nsemantic segmentation, this research proposes an encoder-decoder architecture\nwith a unique efficient residual network, Efficient-ResNet. Attention-boosting\ngates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to\nfuse the equivariant and feature-based semantic information with the equivalent\nsizes of the output of global context of the efficient residual network in the\nencoder. Respectively, the decoder network is developed with the additional\nattention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve\nthe efficiency in the one-to-one conversion of the semantic information by\ndeploying additional convolution layers in the decoder part. Our network is\ntested on the challenging CamVid and Cityscapes datasets, and the proposed\nmethods reveal significant improvements on the residual networks. To the best\nof our knowledge, the developed network, SERNet-Former, achieves\nstate-of-the-art results (84.62 % mean IoU) on CamVid dataset and challenging\nresults (87.35 % mean IoU) on Cityscapes validation dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15741v7",
    "published_date": "2024-01-28 19:58:19 UTC",
    "updated_date": "2024-07-02 15:48:30 UTC"
  },
  {
    "arxiv_id": "2401.16448v1",
    "title": "LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging",
    "authors": [
      "Weimin Fu",
      "Kaichen Yang",
      "Raj Gautam Dutta",
      "Xiaolong Guo",
      "Gang Qu"
    ],
    "abstract": "This paper presents LLM4SecHW, a novel framework for hardware debugging that\nleverages domain specific Large Language Model (LLM). Despite the success of\nLLMs in automating various software development tasks, their application in the\nhardware security domain has been limited due to the constraints of commercial\nLLMs and the scarcity of domain specific data. To address these challenges, we\npropose a unique approach to compile a dataset of open source hardware design\ndefects and their remediation steps, utilizing version control data. This\ndataset provides a substantial foundation for training machine learning models\nfor hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this\ndataset, enabling the identification and rectification of bugs in hardware\ndesigns. This pioneering approach offers a reference workflow for the\napplication of fine tuning domain specific LLMs in other research areas. We\nevaluate the performance of our proposed system on various open source hardware\ndesigns, demonstrating its efficacy in accurately identifying and correcting\ndefects. Our work brings a new perspective on automating the quality control\nprocess in hardware design.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "6 pages. 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2401.16448v1",
    "published_date": "2024-01-28 19:45:25 UTC",
    "updated_date": "2024-01-28 19:45:25 UTC"
  },
  {
    "arxiv_id": "2401.15721v2",
    "title": "A Study of Acquisition Functions for Medical Imaging Deep Active Learning",
    "authors": [
      "Bonaventure F. P. Dossou"
    ],
    "abstract": "The Deep Learning revolution has enabled groundbreaking achievements in\nrecent years. From breast cancer detection to protein folding, deep learning\nalgorithms have been at the core of very important advancements. However, these\nmodern advancements are becoming more and more data-hungry, especially on\nlabeled data whose availability is scarce: this is even more prevalent in the\nmedical context. In this work, we show how active learning could be very\neffective in data scarcity situations, where obtaining labeled data (or\nannotation budget is very limited). We compare several selection criteria\n(BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the\neffect of acquired pool size on the model's performance. Our results suggest\nthat uncertainty is useful to the Melanoma detection task, and confirms the\nhypotheses of the author of the paper of interest, that \\textit{bald} performs\non average better than other acquisition functions. Our extended analyses\nhowever revealed that all acquisition functions perform badly on the positive\n(cancerous) samples, suggesting exploitation of class unbalance, which could be\ncrucial in real-world settings. We finish by suggesting future work directions\nthat would be useful to improve this current work. The code of our\nimplementation is open-sourced at\n\\url{https://github.com/bonaventuredossou/ece526_course_project}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Best Poster Award at Deep Learning Indaba 2023 Conference",
    "pdf_url": "http://arxiv.org/pdf/2401.15721v2",
    "published_date": "2024-01-28 18:09:02 UTC",
    "updated_date": "2024-02-29 11:02:02 UTC"
  },
  {
    "arxiv_id": "2401.15713v3",
    "title": "Contrastive Learning and Mixture of Experts Enables Precise Vector Embeddings",
    "authors": [
      "Logan Hallee",
      "Rohan Kapur",
      "Arjun Patel",
      "Jason P. Gleghorn",
      "Bohdan Khomtchouk"
    ],
    "abstract": "The advancement of transformer neural networks has significantly elevated the\ncapabilities of sentence similarity models, but they still struggle with highly\ndiscriminative tasks and may produce sub-optimal representations of important\ndocuments like scientific literature. With the increased reliance on retrieval\naugmentation and search, representing diverse documents as concise and\ndescriptive vectors is crucial. This paper improves upon the vectors embeddings\nof scientific text by assembling niche datasets using co-citations as a\nsimilarity metric, focusing on biomedical domains. We apply a novel Mixture of\nExperts (MoE) extension pipeline to pretrained BERT models, where every\nmulti-layer perceptron section is enlarged and copied into multiple distinct\nexperts. Our MoE variants perform well over $N$ scientific domains with $N$\ndedicated experts, whereas standard BERT models excel in only one domain at a\ntime. Notably, extending just a single transformer block to MoE captures 85% of\nthe benefit seen from full MoE extension at every layer. This holds promise for\nversatile and efficient One-Size-Fits-All transformer networks for numerically\nrepresenting diverse inputs. Our methodology marks advancements in\nrepresentation learning and holds promise for enhancing vector database search\nand compilation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15713v3",
    "published_date": "2024-01-28 17:34:42 UTC",
    "updated_date": "2024-12-17 20:58:26 UTC"
  },
  {
    "arxiv_id": "2402.01732v2",
    "title": "Identifying and Improving Disability Bias in GPT-Based Resume Screening",
    "authors": [
      "Kate Glazko",
      "Yusuf Mohammed",
      "Ben Kosa",
      "Venkatesh Potluri",
      "Jennifer Mankoff"
    ],
    "abstract": "As Generative AI rises in adoption, its use has expanded to include domains\nsuch as hiring and recruiting. However, without examining the potential of\nbias, this may negatively impact marginalized populations, including people\nwith disabilities. To address this important concern, we present a resume audit\nstudy, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against\nthe same resume enhanced with an additional leadership award, scholarship,\npanel presentation, and membership that are disability related. We find that\nGPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this\nprejudice can be quantifiably reduced by training a custom GPTs on principles\nof DEI and disability justice. Our study also includes a unique qualitative\nanalysis of the types of direct and indirect ableism GPT-4 uses to justify its\nbiased decisions and suggest directions for additional bias mitigation work.\nAdditionally, since these justifications are presumably drawn from training\ndata containing real-world biased statements made by humans, our analysis\nsuggests additional avenues for understanding and addressing human bias.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01732v2",
    "published_date": "2024-01-28 17:04:59 UTC",
    "updated_date": "2024-05-22 19:15:18 UTC"
  },
  {
    "arxiv_id": "2402.00060v2",
    "title": "Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory",
    "authors": [
      "Luis Sanchez",
      "Massimiliano Vasile",
      "Silvia Sanvido",
      "Klaus Mertz",
      "Christophe Taillan"
    ],
    "abstract": "The paper presents an approach to the modelling of epistemic uncertainty in\nConjunction Data Messages (CDM) and the classification of conjunction events\naccording to the confidence in the probability of collision. The approach\nproposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence\nand starts from the assumption that the observed CDMs are drawn from a family\nof unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is\nused to construct robust bounds on such a family of unknown distributions\nstarting from a time series of CDMs. A DSt structure is then derived from the\nprobability boxes constructed with DKW inequality. The DSt structure\nencapsulates the uncertainty in the CDMs at every point along the time series\nand allows the computation of the belief and plausibility in the realisation of\na given probability of collision. The methodology proposed in this paper is\ntested on a number of real events and compared against existing practices in\nthe European and French Space Agencies. We will show that the classification\nsystem proposed in this paper is more conservative than the approach taken by\nthe European Space Agency but provides an added quantification of uncertainty\nin the probability of collision.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 23 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.00060v2",
    "published_date": "2024-01-28 15:39:29 UTC",
    "updated_date": "2024-02-13 18:06:21 UTC"
  },
  {
    "arxiv_id": "2401.15675v1",
    "title": "Detection of a facemask in real-time using deep learning methods: Prevention of Covid 19",
    "authors": [
      "Gautam Siddharth Kashyap",
      "Jatin Sohlot",
      "Ayesha Siddiqui",
      "Ramsha Siddiqui",
      "Karan Malik",
      "Samar Wazir",
      "Alexander E. I. Brownlee"
    ],
    "abstract": "A health crisis is raging all over the world with the rapid transmission of\nthe novel-coronavirus disease (Covid-19). Out of the guidelines issued by the\nWorld Health Organisation (WHO) to protect us against Covid-19, wearing a\nfacemask is the most effective. Many countries have necessitated the wearing of\nface masks, but monitoring a large number of people to ensure that they are\nwearing masks in a crowded place is a challenging task in itself. The\nnovel-coronavirus disease (Covid-19) has already affected our day-to-day life\nas well as world trade movements. By the end of April 2021, the world has\nrecorded 144,358,956 confirmed cases of novel-coronavirus disease (Covid-19)\nincluding 3,066,113 deaths according to the world health organization (WHO).\nThese increasing numbers motivate automated techniques for the detection of a\nfacemask in real-time scenarios for the prevention of Covid-19. We propose a\ntechnique using deep learning that works for single and multiple people in a\nframe recorded via webcam in still or in motion. We have also experimented with\nour approach in night light. The accuracy of our model is good compared to the\nother approaches in the literature; ranging from 74% for multiple people in a\nnightlight to 99% for a single person in daylight.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Research Advances in Network Technologies (Volume 2) (CRC Press\n  Taylor and Francis), 2023 (Accepted)",
    "pdf_url": "http://arxiv.org/pdf/2401.15675v1",
    "published_date": "2024-01-28 14:45:52 UTC",
    "updated_date": "2024-01-28 14:45:52 UTC"
  },
  {
    "arxiv_id": "2401.15670v1",
    "title": "YODA: Teacher-Student Progressive Learning for Language Models",
    "authors": [
      "Jianqiao Lu",
      "Wanjun Zhong",
      "Yufei Wang",
      "Zhijiang Guo",
      "Qi Zhu",
      "Wenyong Huang",
      "Yanlin Wang",
      "Fei Mi",
      "Baojun Wang",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ],
    "abstract": "Although large language models (LLMs) have demonstrated adeptness in a range\nof tasks, they still lag behind human learning efficiency. This disparity is\noften linked to the inherent human capacity to learn from basic examples,\ngradually generalize and handle more complex problems, and refine their skills\nwith continuous feedback. Inspired by this, this paper introduces YODA, a novel\nteacher-student progressive learning framework that emulates the\nteacher-student education process to improve the efficacy of model fine-tuning.\nThe framework operates on an interactive \\textit{basic-generalized-harder}\nloop. The teacher agent provides tailored feedback on the student's answers,\nand systematically organizes the education process. This process unfolds by\nteaching the student basic examples, reinforcing understanding through\ngeneralized questions, and then enhancing learning by posing questions with\nprogressively enhanced complexity. With the teacher's guidance, the student\nlearns to iteratively refine its answer with feedback, and forms a robust and\ncomprehensive understanding of the posed questions. The systematic procedural\ndata, which reflects the progressive learning process of humans, is then\nutilized for model training. Taking math reasoning as a testbed, experiments\nshow that training LLaMA2 with data from YODA improves SFT with significant\nperformance gain (+17.01\\% on GSM8K and +9.98\\% on MATH). In addition, we find\nthat training with curriculum learning further improves learning robustness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.15670v1",
    "published_date": "2024-01-28 14:32:15 UTC",
    "updated_date": "2024-01-28 14:32:15 UTC"
  },
  {
    "arxiv_id": "2402.10222v1",
    "title": "Autonomous Vehicle Patrolling Through Deep Reinforcement Learning: Learning to Communicate and Cooperate",
    "authors": [
      "Chenhao Tong",
      "Maria A. Rodriguez",
      "Richard O. Sinnott"
    ],
    "abstract": "Autonomous vehicles are suited for continuous area patrolling problems.\nFinding an optimal patrolling strategy can be challenging due to unknown\nenvironmental factors, such as wind or landscape; or autonomous vehicles'\nconstraints, such as limited battery life or hardware failures. Importantly,\npatrolling large areas often requires multiple agents to collectively\ncoordinate their actions. However, an optimal coordination strategy is often\nnon-trivial to be manually defined due to the complex nature of patrolling\nenvironments. In this paper, we consider a patrolling problem with\nenvironmental factors, agent limitations, and three typical cooperation\nproblems -- collision avoidance, congestion avoidance, and patrolling target\nnegotiation. We propose a multi-agent reinforcement learning solution based on\na reinforced inter-agent learning (RIAL) method. With this approach, agents are\ntrained to develop their own communication protocol to cooperate during\npatrolling where faults can and do occur. The solution is validated through\nsimulation experiments and is compared with several state-of-the-art patrolling\nsolutions from different perspectives, including the overall patrol\nperformance, the collision avoidance performance, the efficiency of battery\nrecharging strategies, and the overall fault tolerance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10222v1",
    "published_date": "2024-01-28 14:29:30 UTC",
    "updated_date": "2024-01-28 14:29:30 UTC"
  },
  {
    "arxiv_id": "2403.08782v1",
    "title": "Procedural terrain generation with style transfer",
    "authors": [
      "Fabio Merizzi"
    ],
    "abstract": "In this study we introduce a new technique for the generation of terrain\nmaps, exploiting a combination of procedural generation and Neural Style\nTransfer. We consider our approach to be a viable alternative to competing\ngenerative models, with our technique achieving greater versatility, lower\nhardware requirements and greater integration in the creative process of\ndesigners and developers. Our method involves generating procedural noise maps\nusing either multi-layered smoothed Gaussian noise or the Perlin algorithm. We\nthen employ an enhanced Neural Style transfer technique, drawing style from\nreal-world height maps. This fusion of algorithmic generation and neural\nprocessing holds the potential to produce terrains that are not only diverse\nbut also closely aligned with the morphological characteristics of real-world\nlandscapes, with our process yielding consistent terrain structures with low\ncomputational cost and offering the capability to create customized maps.\nNumerical evaluations further validate our model's enhanced ability to\naccurately replicate terrain morphology, surpassing traditional procedural\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.08782v1",
    "published_date": "2024-01-28 14:22:27 UTC",
    "updated_date": "2024-01-28 14:22:27 UTC"
  },
  {
    "arxiv_id": "2402.00059v1",
    "title": "FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting",
    "authors": [
      "Tao Han",
      "Song Guo",
      "Fenghua Ling",
      "Kang Chen",
      "Junchao Gong",
      "Jingjia Luo",
      "Junxia Gu",
      "Kan Dai",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "abstract": "Kilometer-scale modeling of global atmosphere dynamics enables fine-grained\nweather forecasting and decreases the risk of disastrous weather and climate\nactivity. Therefore, building a kilometer-scale global forecast model is a\npersistent pursuit in the meteorology domain. Active international efforts have\nbeen made in past decades to improve the spatial resolution of numerical\nweather models. Nonetheless, developing the higher resolution numerical model\nremains a long-standing challenge due to the substantial consumption of\ncomputational resources. Recent advances in data-driven global weather\nforecasting models utilize reanalysis data for model training and have\ndemonstrated comparable or even higher forecasting skills than numerical\nmodels. However, they are all limited by the resolution of reanalysis data and\nincapable of generating higher-resolution forecasts. This work presents\nFengWu-GHR, the first data-driven global weather forecasting model running at\nthe 0.09$^{\\circ}$ horizontal resolution. FengWu-GHR introduces a novel\napproach that opens the door for operating ML-based high-resolution forecasts\nby inheriting prior knowledge from a pretrained low-resolution model. The\nhindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to\nthe IFS-HRES. Furthermore, evaluations on station observations and case studies\nof extreme events support the competitive operational forecasting skill of\nFengWu-GHR at the high resolution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.00059v1",
    "published_date": "2024-01-28 13:23:25 UTC",
    "updated_date": "2024-01-28 13:23:25 UTC"
  },
  {
    "arxiv_id": "2401.15647v2",
    "title": "UP-CrackNet: Unsupervised Pixel-Wise Road Crack Detection via Adversarial Image Restoration",
    "authors": [
      "Nachuan Ma",
      "Rui Fan",
      "Lihua Xie"
    ],
    "abstract": "Over the past decade, automated methods have been developed to detect cracks\nmore efficiently, accurately, and objectively, with the ultimate goal of\nreplacing conventional manual visual inspection techniques. Among these\nmethods, semantic segmentation algorithms have demonstrated promising results\nin pixel-wise crack detection tasks. However, training such networks requires a\nlarge amount of human-annotated datasets with pixel-level annotations, which is\na highly labor-intensive and time-consuming process. Moreover, supervised\nlearning-based methods often struggle with poor generalizability in unseen\ndatasets. Therefore, we propose an unsupervised pixel-wise road crack detection\nnetwork, known as UP-CrackNet. Our approach first generates multi-scale square\nmasks and randomly selects them to corrupt undamaged road images by removing\ncertain regions. Subsequently, a generative adversarial network is trained to\nrestore the corrupted regions by leveraging the semantic context learned from\nsurrounding uncorrupted regions. During the testing phase, an error map is\ngenerated by calculating the difference between the input and restored images,\nwhich allows for pixel-wise crack detection. Our comprehensive experimental\nresults demonstrate that UP-CrackNet outperforms other general-purpose\nunsupervised anomaly detection algorithms, and exhibits satisfactory\nperformance and superior generalizability when compared with state-of-the-art\nsupervised crack segmentation algorithms. Our source code is publicly available\nat mias.group/UP-CrackNet.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15647v2",
    "published_date": "2024-01-28 12:51:01 UTC",
    "updated_date": "2024-05-06 07:45:53 UTC"
  },
  {
    "arxiv_id": "2401.15626v1",
    "title": "TA&AT: Enhancing Task-Oriented Dialog with Turn-Level Auxiliary Tasks and Action-Tree Based Scheduled Sampling",
    "authors": [
      "Longxiang Liu",
      "Xiuxing Li",
      "Yang Feng"
    ],
    "abstract": "Task-oriented dialog systems have witnessed substantial progress due to\nconversational pre-training techniques. Yet, two significant challenges\npersist. First, most systems primarily utilize the latest turn's state label\nfor the generator. This practice overlooks the comprehensive value of state\nlabels in boosting the model's understanding for future generations. Second, an\noverreliance on generated policy often leads to error accumulation, resulting\nin suboptimal responses when adhering to incorrect actions. To combat these\nchallenges, we propose turn-level multi-task objectives for the encoder. With\nthe guidance of essential information from labeled intermediate states, we\nestablish a more robust representation for both understanding and generation.\nFor the decoder, we introduce an action tree-based scheduled sampling\ntechnique. Specifically, we model the hierarchical policy as trees and utilize\nthe similarity between trees to sample negative policy based on scheduled\nsampling, hoping the model to generate invariant responses under perturbations.\nThis method simulates potential pitfalls by sampling similar negative policy,\nbridging the gap between task-oriented dialog training and inference. Among\nmethods without continual pre-training, our approach achieved state-of-the-art\n(SOTA) performance on the MultiWOZ dataset series and was also competitive with\npre-trained SOTA methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.15626v1",
    "published_date": "2024-01-28 11:02:23 UTC",
    "updated_date": "2024-01-28 11:02:23 UTC"
  },
  {
    "arxiv_id": "2401.15625v1",
    "title": "Generative AI-enabled Blockchain Networks: Fundamentals, Applications, and Case Study",
    "authors": [
      "Cong T. Nguyen",
      "Yinqiu Liu",
      "Hongyang Du",
      "Dinh Thai Hoang",
      "Dusit Niyato",
      "Diep N. Nguyen",
      "Shiwen Mao"
    ],
    "abstract": "Generative Artificial Intelligence (GAI) has recently emerged as a promising\nsolution to address critical challenges of blockchain technology, including\nscalability, security, privacy, and interoperability. In this paper, we first\nintroduce GAI techniques, outline their applications, and discuss existing\nsolutions for integrating GAI into blockchains. Then, we discuss emerging\nsolutions that demonstrate the effectiveness of GAI in addressing various\nchallenges of blockchain, such as detecting unknown blockchain attacks and\nsmart contract vulnerabilities, designing key secret sharing schemes, and\nenhancing privacy. Moreover, we present a case study to demonstrate that GAI,\nspecifically the generative diffusion model, can be employed to optimize\nblockchain network performance metrics. Experimental results clearly show that,\ncompared to a baseline traditional AI approach, the proposed generative\ndiffusion model approach can converge faster, achieve higher rewards, and\nsignificantly improve the throughput and latency of the blockchain network.\nAdditionally, we highlight future research directions for GAI in blockchain\napplications, including personalized GAI-enabled blockchains, GAI-blockchain\nsynergy, and privacy and security considerations within blockchain ecosystems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15625v1",
    "published_date": "2024-01-28 10:46:17 UTC",
    "updated_date": "2024-01-28 10:46:17 UTC"
  },
  {
    "arxiv_id": "2401.15621v2",
    "title": "SNAP: Semantic Stories for Next Activity Prediction",
    "authors": [
      "Alon Oved",
      "Segev Shlomov",
      "Sergey Zeltyn",
      "Nir Mashkif",
      "Avi Yaeli"
    ],
    "abstract": "Predicting the next activity in an ongoing process is one of the most common\nclassification tasks in the business process management (BPM) domain. It allows\nbusinesses to optimize resource allocation, enhance operational efficiency, and\naids in risk mitigation and strategic decision-making. This provides a\ncompetitive edge in the rapidly evolving confluence of BPM and AI. Existing\nstate-of-the-art AI models for business process prediction do not fully\ncapitalize on available semantic information within process event logs. As\ncurrent advanced AI-BPM systems provide semantically-richer textual data, the\nneed for novel adequate models grows. To address this gap, we propose the novel\nSNAP method that leverages language foundation models by constructing semantic\ncontextual stories from the process historical event logs and using them for\nthe next activity prediction. We compared the SNAP algorithm with nine\nstate-of-the-art models on six benchmark datasets and show that SNAP\nsignificantly outperforms them, especially for datasets with high levels of\nsemantic content.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15621v2",
    "published_date": "2024-01-28 10:20:15 UTC",
    "updated_date": "2024-03-14 17:22:37 UTC"
  },
  {
    "arxiv_id": "2401.15620v1",
    "title": "Data-Driven Strategies for Coping with Incomplete DVL Measurements",
    "authors": [
      "Nadav Cohen",
      "Itzik Klein"
    ],
    "abstract": "Autonomous underwater vehicles are specialized platforms engineered for deep\nunderwater operations. Critical to their functionality is autonomous\nnavigation, typically relying on an inertial navigation system and a Doppler\nvelocity log. In real-world scenarios, incomplete Doppler velocity log\nmeasurements occur, resulting in positioning errors and mission aborts. To cope\nwith such situations, a model and learning approaches were derived. This paper\npresents a comparative analysis of two cutting-edge deep learning\nmethodologies, namely LiBeamsNet and MissBeamNet, alongside a model-based\naverage estimator. These approaches are evaluated for their efficacy in\nregressing missing Doppler velocity log beams when two beams are unavailable.\nIn our study, we used data recorded by a DVL mounted on an autonomous\nunderwater vehicle operated in the Mediterranean Sea. We found that both deep\nlearning architectures outperformed model-based approaches by over 16% in\nvelocity prediction accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15620v1",
    "published_date": "2024-01-28 10:17:36 UTC",
    "updated_date": "2024-01-28 10:17:36 UTC"
  },
  {
    "arxiv_id": "2401.15617v2",
    "title": "Diffusion-based Graph Generative Methods",
    "authors": [
      "Hongyang Chen",
      "Can Xu",
      "Lingyu Zheng",
      "Qiang Zhang",
      "Xuemin Lin"
    ],
    "abstract": "Being the most cutting-edge generative methods, diffusion methods have shown\ngreat advances in wide generation tasks. Among them, graph generation attracts\nsignificant research attention for its broad application in real life. In our\nsurvey, we systematically and comprehensively review on diffusion-based graph\ngenerative methods. We first make a review on three mainstream paradigms of\ndiffusion methods, which are denoising diffusion probabilistic models,\nscore-based genrative models, and stochastic differential equations. Then we\nfurther categorize and introduce the latest applications of diffusion models on\ngraphs. In the end, we point out some limitations of current studies and future\ndirections of future explorations. The summary of existing methods metioned in\nthis survey is in\nhttps://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15617v2",
    "published_date": "2024-01-28 10:09:05 UTC",
    "updated_date": "2024-07-16 12:21:30 UTC"
  },
  {
    "arxiv_id": "2402.01730v1",
    "title": "Evaluating LLM -- Generated Multimodal Diagnosis from Medical Images and Symptom Analysis",
    "authors": [
      "Dimitrios P. Panagoulias",
      "Maria Virvou",
      "George A. Tsihrintzis"
    ],
    "abstract": "Large language models (LLMs) constitute a breakthrough state-of-the-art\nArtificial Intelligence technology which is rapidly evolving and promises to\naid in medical diagnosis. However, the correctness and the accuracy of their\nreturns has not yet been properly evaluated. In this work, we propose an LLM\nevaluation paradigm that incorporates two independent steps of a novel\nmethodology, namely (1) multimodal LLM evaluation via structured interactions\nand (2) follow-up, domain-specific analysis based on data extracted via the\nprevious interactions. Using this paradigm, (1) we evaluate the correctness and\naccuracy of LLM-generated medical diagnosis with publicly available multimodal\nmultiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a\nsystemic and comprehensive analysis of extracted results. We used\nGPT-4-Vision-Preview as the LLM to respond to complex, medical questions\nconsisting of both images and text, and we explored a wide range of diseases,\nconditions, chemical compounds, and related entity types that are included in\nthe vast knowledge domain of Pathology. GPT-4-Vision-Preview performed quite\nwell, scoring approximately 84\\% of correct diagnoses. Next, we further\nanalyzed the findings of our work, following an analytical approach which\nincluded Image Metadata Analysis, Named Entity Recognition and Knowledge\nGraphs. Weaknesses of GPT-4-Vision-Preview were revealed on specific knowledge\npaths, leading to a further understanding of its shortcomings in specific\nareas. Our methodology and findings are not limited to the use of\nGPT-4-Vision-Preview, but a similar approach can be followed to evaluate the\nusefulness and accuracy of other LLMs and, thus, improve their use with further\noptimization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Department of Informatics, University of Piraeus, Greece",
    "pdf_url": "http://arxiv.org/pdf/2402.01730v1",
    "published_date": "2024-01-28 09:25:12 UTC",
    "updated_date": "2024-01-28 09:25:12 UTC"
  },
  {
    "arxiv_id": "2402.01729v3",
    "title": "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
    "authors": [
      "Dawei Li",
      "Zhen Tan",
      "Tianlong Chen",
      "Huan Liu"
    ],
    "abstract": "While textual information significantly enhances the performance of\npre-trained language models (PLMs) in knowledge graph completion (KGC), the\nstatic and noisy nature of existing corpora collected from Wikipedia articles\nor synsets definitions often limits the potential of PLM-based KGC models. To\nsurmount these challenges, we introduce the Contextualization Distillation\nstrategy, a versatile plug-in-and-play approach compatible with both\ndiscriminative and generative KGC frameworks. Our method begins by instructing\nlarge language models (LLMs) to transform compact, structural triplets into\ncontext-rich segments. Subsequently, we introduce two tailored auxiliary tasks,\nreconstruction and contextualization, allowing smaller KGC models to assimilate\ninsights from these enriched triplets. Comprehensive evaluations across diverse\ndatasets and KGC techniques highlight the efficacy and adaptability of our\napproach, revealing consistent performance enhancements irrespective of\nunderlying pipelines or architectures. Moreover, our analysis makes our method\nmore explainable and provides insight into generating path selection, as well\nas the choosing of suitable distillation tasks. All the code and data in this\nwork will be released at\nhttps://github.com/David-Li0406/Contextulization-Distillation",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EACL 2024 findings v3: add missing citations",
    "pdf_url": "http://arxiv.org/pdf/2402.01729v3",
    "published_date": "2024-01-28 08:56:49 UTC",
    "updated_date": "2024-02-24 07:01:22 UTC"
  },
  {
    "arxiv_id": "2402.03357v1",
    "title": "Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning",
    "authors": [
      "Xiaofei Xu",
      "Ke Deng",
      "Michael Dann",
      "Xiuzhen Zhang"
    ],
    "abstract": "This study aims to minimize the influence of fake news on social networks by\ndeploying debunkers to propagate true news. This is framed as a reinforcement\nlearning problem, where, at each stage, one user is selected to propagate true\nnews. A challenging issue is episodic reward where the \"net\" effect of\nselecting individual debunkers cannot be discerned from the interleaving\ninformation propagation on social networks, and only the collective effect from\nmitigation efforts can be observed. Existing Self-Imitation Learning (SIL)\nmethods have shown promise in learning from episodic rewards, but are\nill-suited to the real-world application of fake news mitigation because of\ntheir poor sample efficiency. To learn a more effective debunker selection\npolicy for fake news mitigation, this study proposes NAGASIL - Negative\nsampling and state Augmented Generative Adversarial Self-Imitation Learning,\nwhich consists of two improvements geared towards fake news mitigation:\nlearning from negative samples, and an augmented state representation to\ncapture the \"real\" environment state by integrating the current observed state\nwith the previous state-action pairs from the same campaign. Experiments on two\nsocial networks show that NAGASIL yields superior performance to standard GASIL\nand state-of-the-art fake news mitigation models.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "10 pages, full version of this paper is accepted by AAAI'24",
    "pdf_url": "http://arxiv.org/pdf/2402.03357v1",
    "published_date": "2024-01-28 06:05:01 UTC",
    "updated_date": "2024-01-28 06:05:01 UTC"
  },
  {
    "arxiv_id": "2401.16444v1",
    "title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain",
    "authors": [
      "Yiming Gao",
      "Feiyu Liu",
      "Liang Wang",
      "Zhenjie Lian",
      "Dehua Zheng",
      "Weixuan Wang",
      "Wenjin Yang",
      "Siqin Li",
      "Xianliang Wang",
      "Wenhui Chen",
      "Jing Dai",
      "Qiang Fu",
      "Wei Yang",
      "Lanxiao Huang",
      "Wei Liu"
    ],
    "abstract": "Existing game AI research mainly focuses on enhancing agents' abilities to\nwin games, but this does not inherently make humans have a better experience\nwhen collaborating with these agents. For example, agents may dominate the\ncollaboration and exhibit unintended or detrimental behaviors, leading to poor\nexperiences for their human partners. In other words, most game AI agents are\nmodeled in a \"self-centered\" manner. In this paper, we propose a\n\"human-centered\" modeling scheme for collaborative agents that aims to enhance\nthe experience of humans. Specifically, we model the experience of humans as\nthe goals they expect to achieve during the task. We expect that agents should\nlearn to enhance the extent to which humans achieve these goals while\nmaintaining agents' original abilities (e.g., winning games). To achieve this,\nwe propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG\napproach introduces a \"baseline\", which corresponds to the extent to which\nhumans primitively achieve their goals, and encourages agents to learn\nbehaviors that can effectively enhance humans in achieving their goals better.\nWe evaluate the RLHG agent in the popular Multi-player Online Battle Arena\n(MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both\nobjective performance and subjective preference results show that the RLHG\nagent provides participants better gaming experience.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at ICLR 2024. arXiv admin note: text overlap with\n  arXiv:2304.11632",
    "pdf_url": "http://arxiv.org/pdf/2401.16444v1",
    "published_date": "2024-01-28 05:05:57 UTC",
    "updated_date": "2024-01-28 05:05:57 UTC"
  },
  {
    "arxiv_id": "2401.15568v1",
    "title": "Intriguing Equivalence Structures of the Embedding Space of Vision Transformers",
    "authors": [
      "Shaeke Salman",
      "Md Montasir Bin Shams",
      "Xiuwen Liu"
    ],
    "abstract": "Pre-trained large foundation models play a central role in the recent surge\nof artificial intelligence, resulting in fine-tuned models with remarkable\nabilities when measured on benchmark datasets, standard exams, and\napplications. Due to their inherent complexity, these models are not well\nunderstood. While small adversarial inputs to such models are well known, the\nstructures of the representation space are not well characterized despite their\nfundamental importance. In this paper, using the vision transformers as an\nexample due to the continuous nature of their input space, we show via analyses\nand systematic experiments that the representation space consists of large\npiecewise linear subspaces where there exist very different inputs sharing the\nsame representations, and at the same time, local normal spaces where there are\nvisually indistinguishable inputs having very different representations. The\nempirical results are further verified using the local directional estimations\nof the Lipschitz constants of the underlying models. Consequently, the\nresulting representations change the results of downstream models, and such\nmodels are subject to overgeneralization and with limited semantically\nmeaningful generalization capability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.15568v1",
    "published_date": "2024-01-28 04:59:51 UTC",
    "updated_date": "2024-01-28 04:59:51 UTC"
  },
  {
    "arxiv_id": "2401.15564v1",
    "title": "Design of UAV flight state recognition and trajectory prediction system based on trajectory feature construction",
    "authors": [
      "Xingyu Zhou",
      "Zhuoyong Shi"
    ],
    "abstract": "With the impact of artificial intelligence on the traditional UAV industry,\nautonomous UAV flight has become a current hot research field. Based on the\ndemand for research on critical technologies for autonomous flying UAVs, this\npaper addresses the field of flight state recognition and trajectory prediction\nof UAVs. This paper proposes a method to improve the accuracy of UAV trajectory\nprediction based on UAV flight state recognition and verifies it using two\nprediction models. Firstly, UAV flight data acquisition and data preprocessing\nare carried out; secondly, UAV flight trajectory features are extracted based\non data fusion and a UAV flight state recognition model based on PCA-DAGSVM\nmodel is established; finally, two UAV flight trajectory prediction models are\nestablished and the trajectory prediction errors of the two prediction models\nare compared and analyzed after flight state recognition. The results show\nthat: 1) the UAV flight state recognition model based on PCA-DAGSVM has good\nrecognition effect. 2) compared with the traditional UAV trajectory prediction\nmodel, the prediction model based on flight state recognition can effectively\nreduce the prediction error.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15564v1",
    "published_date": "2024-01-28 04:14:35 UTC",
    "updated_date": "2024-01-28 04:14:35 UTC"
  },
  {
    "arxiv_id": "2401.15545v1",
    "title": "PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models",
    "authors": [
      "Simin Chen",
      "Xiaoning Feng",
      "Xiaohong Han",
      "Cong Liu",
      "Wei Yang"
    ],
    "abstract": "In recent times, a plethora of Large Code Generation Models (LCGMs) have been\nproposed, showcasing significant potential in assisting developers with complex\nprogramming tasks. Benchmarking LCGMs necessitates the creation of a set of\ndiverse programming problems, and each problem comprises the prompt (including\nthe task description), canonical solution, and test inputs. The existing\nmethods for constructing such a problem set can be categorized into two main\ntypes: manual methods and perturbation-based methods. However, manual methods\ndemand high effort and lack scalability, while also risking data integrity due\nto LCGMs' potentially contaminated data collection, and perturbation-based\napproaches mainly generate semantically homogeneous problems with the same\ncanonical solutions and introduce typos that can be easily auto-corrected by\nIDE, making them ineffective and unrealistic. In this work, we propose the idea\nof programming problem merging (PPM) and provide two implementation of this\nidea, we utilize our tool on two widely-used datasets and compare it against\nnine baseline methods using eight code generation models. The results\ndemonstrate the effectiveness of our tool in generating more challenging,\ndiverse, and natural programming problems, comparing to the baselines.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "This paper has been accepted to The ACM International Conference on\n  the Foundations of Software Engineering FSE 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.15545v1",
    "published_date": "2024-01-28 02:27:38 UTC",
    "updated_date": "2024-01-28 02:27:38 UTC"
  }
]
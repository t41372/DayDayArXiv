[
  {
    "arxiv_id": "2409.00294v1",
    "title": "Quantum Machine Learning for Anomaly Detection in Consumer Electronics",
    "authors": [
      "Sounak Bhowmik",
      "Himanshu Thapliyal"
    ],
    "abstract": "Anomaly detection is a crucial task in cyber security. Technological\nadvancement brings new cyber-physical threats like network intrusion, financial\nfraud, identity theft, and property invasion. In the rapidly changing world,\nwith frequently emerging new types of anomalies, classical machine learning\nmodels are insufficient to prevent all the threats. Quantum Machine Learning\n(QML) is emerging as a powerful computational tool that can detect anomalies\nmore efficiently. In this work, we have introduced QML and its applications for\nanomaly detection in consumer electronics. We have shown a generic framework\nfor applying QML algorithms in anomaly detection tasks. We have also briefly\ndiscussed popular supervised, unsupervised, and reinforcement learning-based\nQML algorithms and included five case studies of recent works to show their\napplications in anomaly detection in the consumer electronics field.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "7 pages, 2 figures, 1 table, under ISVLSI 2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2409.00294v1",
    "published_date": "2024-08-30 23:28:00 UTC",
    "updated_date": "2024-08-30 23:28:00 UTC"
  },
  {
    "arxiv_id": "2409.00286v1",
    "title": "OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance under Billion Parameters",
    "authors": [
      "Zexin Chen",
      "Chengxi Li",
      "Xiangyu Xie",
      "Parijat Dube"
    ],
    "abstract": "This paper explores the potential of a small, domain-specific language model\ntrained exclusively on sports-related data. We investigate whether extensive\ntraining data with specially designed small model structures can overcome model\nsize constraints. The study introduces the OnlySports collection, comprising\nOnlySportsLM, OnlySports Dataset, and OnlySports Benchmark. Our approach\ninvolves: 1) creating a massive 600 billion tokens OnlySports Dataset from\nFineWeb, 2) optimizing the RWKV architecture for sports-related tasks,\nresulting in a 196M parameters model with 20-layer, 640-dimension structure, 3)\ntraining the OnlySportsLM on part of OnlySports Dataset, and 4) testing the\nresultant model on OnlySports Benchmark. OnlySportsLM achieves a 37.62%/34.08%\naccuracy improvement over previous 135M/360M state-of-the-art models and\nmatches the performance of larger models such as SomlLM 1.7B and Qwen 1.5B in\nthe sports domain. Additionally, the OnlySports collection presents a\ncomprehensive workflow for building high-quality, domain-specific language\nmodels, providing a replicable blueprint for efficient AI development across\nvarious specialized fields.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 4 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.00286v1",
    "published_date": "2024-08-30 22:39:35 UTC",
    "updated_date": "2024-08-30 22:39:35 UTC"
  },
  {
    "arxiv_id": "2409.00284v2",
    "title": "Reframing Data Value for Large Language Models Through the Lens of Plausibility",
    "authors": [
      "Mohamad Rida Rammal",
      "Ruida Zhou",
      "Suhas Diggavi"
    ],
    "abstract": "Data valuation seeks to answer the important question, \"How much is this data\nworth?\" Existing data valuation methods have largely focused on discriminative\nmodels, primarily examining data value through the lens of its utility in\ntraining. However, with the push for ever-larger language models, relying on\nvaluation methods that require training becomes increasingly expensive and\ndependent on specific techniques. We propose an alternative perspective on the\ndata value problem for language models, centering around the plausibility of\nthe data. We posit that data holds lesser value if it can be plausibly\ngenerated by the model itself. Starting from some intuitive criteria that align\nwith our notions of valuable data, we develop a novel value function that is\ncomputationally tractable and derived from first principles with provable\nproperties. We conduct a theoretical analysis of our value function and\nevaluate it across multiple scenarios and datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00284v2",
    "published_date": "2024-08-30 22:32:24 UTC",
    "updated_date": "2024-10-15 20:04:22 UTC"
  },
  {
    "arxiv_id": "2409.00265v2",
    "title": "Explainable Artificial Intelligence: A Survey of Needs, Techniques, Applications, and Future Direction",
    "authors": [
      "Melkamu Mersha",
      "Khang Lam",
      "Joseph Wood",
      "Ali AlShami",
      "Jugal Kalita"
    ],
    "abstract": "Artificial intelligence models encounter significant challenges due to their\nblack-box nature, particularly in safety-critical domains such as healthcare,\nfinance, and autonomous vehicles. Explainable Artificial Intelligence (XAI)\naddresses these challenges by providing explanations for how these models make\ndecisions and predictions, ensuring transparency, accountability, and fairness.\nExisting studies have examined the fundamental concepts of XAI, its general\nprinciples, and the scope of XAI techniques. However, there remains a gap in\nthe literature as there are no comprehensive reviews that delve into the\ndetailed mathematical representations, design methodologies of XAI models, and\nother associated aspects. This paper provides a comprehensive literature review\nencompassing common terminologies and definitions, the need for XAI,\nbeneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI\nmethods in different application areas. The survey is aimed at XAI researchers,\nXAI practitioners, AI model developers, and XAI beneficiaries who are\ninterested in enhancing the trustworthiness, transparency, accountability, and\nfairness of their AI models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00265v2",
    "published_date": "2024-08-30 21:42:17 UTC",
    "updated_date": "2025-01-13 00:29:56 UTC"
  },
  {
    "arxiv_id": "2409.00264v1",
    "title": "The Artificial Intelligence Act: critical overview",
    "authors": [
      "Nuno Sousa e Silva"
    ],
    "abstract": "This article provides a critical overview of the recently approved Artificial\nIntelligence Act. It starts by presenting the main structure, objectives, and\napproach of Regulation (EU) 2024/1689. A definition of key concepts follows,\nand then the material and territorial scope, as well as the timing of\napplication, are analyzed. Although the Regulation does not explicitly set out\nprinciples, the main ideas of fairness, accountability, transparency, and\nequity in AI underly a set of rules of the regulation. This is discussed before\nlooking at the ill-defined set of forbidden AI practices (manipulation and e\nexploitation of vulnerabilities, social scoring, biometric identification and\nclassification, and predictive policing). It is highlighted that those rules\ndeal with behaviors rather than AI systems. The qualification and regulation of\nhigh-risk AI systems are tackled, alongside the obligation of transparency for\ncertain systems, the regulation of general-purpose models, and the rules on\ncertification, supervision, and sanctions. The text concludes that even if the\noverall framework can be deemed adequate and balanced, the approach is so\ncomplex that it risks defeating its own purpose of promoting responsible\ninnovation within the European Union and beyond its borders.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00264v1",
    "published_date": "2024-08-30 21:38:02 UTC",
    "updated_date": "2024-08-30 21:38:02 UTC"
  },
  {
    "arxiv_id": "2409.00255v1",
    "title": "MAPWise: Evaluating Vision-Language Models for Advanced Map Queries",
    "authors": [
      "Srija Mukhopadhyay",
      "Abhishek Rajgaria",
      "Prerana Khatiwada",
      "Vivek Gupta",
      "Dan Roth"
    ],
    "abstract": "Vision-language models (VLMs) excel at tasks requiring joint understanding of\nvisual and linguistic information. A particularly promising yet under-explored\napplication for these models lies in answering questions based on various kinds\nof maps. This study investigates the efficacy of VLMs in answering questions\nbased on choropleth maps, which are widely used for data analysis and\nrepresentation. To facilitate and encourage research in this area, we introduce\na novel map-based question-answering benchmark, consisting of maps from three\ngeographical regions (United States, India, China), each containing 1000\nquestions. Our benchmark incorporates 43 diverse question templates, requiring\nnuanced understanding of relative spatial relationships, intricate map\nfeatures, and complex reasoning. It also includes maps with discrete and\ncontinuous values, encompassing variations in color-mapping, category ordering,\nand stylistic patterns, enabling comprehensive analysis. We evaluate the\nperformance of multiple VLMs on this benchmark, highlighting gaps in their\nabilities and providing insights for improving such models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "30 Pages, 46 Tables, 6 Figure",
    "pdf_url": "http://arxiv.org/pdf/2409.00255v1",
    "published_date": "2024-08-30 20:57:34 UTC",
    "updated_date": "2024-08-30 20:57:34 UTC"
  },
  {
    "arxiv_id": "2409.00240v1",
    "title": "One-Frame Calibration with Siamese Network in Facial Action Unit Recognition",
    "authors": [
      "Shuangquan Feng",
      "Virginia R. de Sa"
    ],
    "abstract": "Automatic facial action unit (AU) recognition is used widely in facial\nexpression analysis. Most existing AU recognition systems aim for\ncross-participant non-calibrated generalization (NCG) to unseen faces without\nfurther calibration. However, due to the diversity of facial attributes across\ndifferent identities, accurately inferring AU activation from single images of\nan unseen face is sometimes infeasible, even for human experts -- it is crucial\nto first understand how the face appears in its neutral expression, or\nsignificant bias may be incurred. Therefore, we propose to perform one-frame\ncalibration (OFC) in AU recognition: for each face, a single image of its\nneutral expression is used as the reference image for calibration. With this\nstrategy, we develop a Calibrating Siamese Network (CSN) for AU recognition and\ndemonstrate its remarkable effectiveness with a simple iResNet-50 (IR50)\nbackbone. On the DISFA, DISFA+, and UNBC-McMaster datasets, we show that our\nOFC CSN-IR50 model (a) substantially improves the performance of IR50 by\nmitigating facial attribute biases (including biases due to wrinkles, eyebrow\npositions, facial hair, etc.), (b) substantially outperforms the naive OFC\nmethod of baseline subtraction as well as (c) a fine-tuned version of this\nnaive OFC method, and (d) also outperforms state-of-the-art NCG models for both\nAU intensity estimation and AU detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00240v1",
    "published_date": "2024-08-30 20:20:12 UTC",
    "updated_date": "2024-08-30 20:20:12 UTC"
  },
  {
    "arxiv_id": "2409.00237v1",
    "title": "Deep learning surrogate models of JULES-INFERNO for wildfire prediction on a global scale",
    "authors": [
      "Sibo Cheng",
      "Hector Chassagnon",
      "Matthew Kasoar",
      "Yike Guo",
      "Rossella Arcucci"
    ],
    "abstract": "Global wildfire models play a crucial role in anticipating and responding to\nchanging wildfire regimes. JULES-INFERNO is a global vegetation and fire model\nsimulating wildfire emissions and area burnt on a global scale. However,\nbecause of the high data dimensionality and system complexity, JULES-INFERNO's\ncomputational costs make it challenging to apply to fire risk forecasting with\nunseen initial conditions. Typically, running JULES-INFERNO for 30 years of\nprediction will take several hours on High Performance Computing (HPC)\nclusters. To tackle this bottleneck, two data-driven models are built in this\nwork based on Deep Learning techniques to surrogate the JULES-INFERNO model and\nspeed up global wildfire forecasting. More precisely, these machine learning\nmodels take global temperature, vegetation density, soil moisture and previous\nforecasts as inputs to predict the subsequent global area burnt on an iterative\nbasis. Average Error per Pixel (AEP) and Structural Similarity Index Measure\n(SSIM) are used as metrics to evaluate the performance of the proposed\nsurrogate models. A fine tuning strategy is also proposed in this work to\nimprove the algorithm performance for unseen scenarios. Numerical results show\na strong performance of the proposed models, in terms of both computational\nefficiency (less than 20 seconds for 30 years of prediction on a laptop CPU)\nand prediction accuracy (with AEP under 0.3\\% and SSIM over 98\\% compared to\nthe outputs of JULES-INFERNO).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00237v1",
    "published_date": "2024-08-30 20:05:00 UTC",
    "updated_date": "2024-08-30 20:05:00 UTC"
  },
  {
    "arxiv_id": "2409.00230v2",
    "title": "Spatially-Aware Diffusion Models with Cross-Attention for Global Field Reconstruction with Sparse Observations",
    "authors": [
      "Yilin Zhuang",
      "Sibo Cheng",
      "Karthik Duraisamy"
    ],
    "abstract": "Diffusion models have gained attention for their ability to represent complex\ndistributions and incorporate uncertainty, making them ideal for robust\npredictions in the presence of noisy or incomplete data. In this study, we\ndevelop and enhance score-based diffusion models in field reconstruction tasks,\nwhere the goal is to estimate complete spatial fields from partial\nobservations. We introduce a condition encoding approach to construct a\ntractable mapping mapping between observed and unobserved regions using a\nlearnable integration of sparse observations and interpolated fields as an\ninductive bias. With refined sensing representations and an unraveled temporal\ndimension, our method can handle arbitrary moving sensors and effectively\nreconstruct fields. Furthermore, we conduct a comprehensive benchmark of our\napproach against a deterministic interpolation-based method across various\nstatic and time-dependent PDEs. Our study attempts to addresses the gap in\nstrong baselines for evaluating performance across varying sampling\nhyperparameters, noise levels, and conditioning methods. Our results show that\ndiffusion models with cross-attention and the proposed conditional encoding\ngenerally outperform other methods under noisy conditions, although the\ndeterministic method excels with noiseless data. Additionally, both the\ndiffusion models and the deterministic method surpass the numerical approach in\naccuracy and computational cost for the steady problem. We also demonstrate the\nability of the model to capture possible reconstructions and improve the\naccuracy of fused results in covariance-based correction tasks using ensemble\nsampling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00230v2",
    "published_date": "2024-08-30 19:46:23 UTC",
    "updated_date": "2024-11-01 19:58:32 UTC"
  },
  {
    "arxiv_id": "2409.00196v1",
    "title": "A Generative Adversarial Network-based Method for LiDAR-Assisted Radar Image Enhancement",
    "authors": [
      "Thakshila Thilakanayake",
      "Oscar De Silva",
      "Thumeera R. Wanasinghe",
      "George K. Mann",
      "Awantha Jayasiri"
    ],
    "abstract": "This paper presents a generative adversarial network (GAN) based approach for\nradar image enhancement. Although radar sensors remain robust for operations\nunder adverse weather conditions, their application in autonomous vehicles\n(AVs) is commonly limited by the low-resolution data they produce. The primary\ngoal of this study is to enhance the radar images to better depict the details\nand features of the environment, thereby facilitating more accurate object\nidentification in AVs. The proposed method utilizes high-resolution,\ntwo-dimensional (2D) projected light detection and ranging (LiDAR) point clouds\nas ground truth images and low-resolution radar images as inputs to train the\nGAN. The ground truth images were obtained through two main steps. First, a\nLiDAR point cloud map was generated by accumulating raw LiDAR scans. Then, a\ncustomized LiDAR point cloud cropping and projection method was employed to\nobtain 2D projected LiDAR point clouds. The inference process of the proposed\nmethod relies solely on radar images to generate an enhanced version of them.\nThe effectiveness of the proposed method is demonstrated through both\nqualitative and quantitative results. These results show that the proposed\nmethod can generate enhanced images with clearer object representation compared\nto the input radar images, even under adverse weather conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00196v1",
    "published_date": "2024-08-30 18:22:39 UTC",
    "updated_date": "2024-08-30 18:22:39 UTC"
  },
  {
    "arxiv_id": "2408.17443v3",
    "title": "HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics",
    "authors": [
      "Gueter Josmy Faure",
      "Jia-Fong Yeh",
      "Min-Hung Chen",
      "Hung-Ting Su",
      "Shang-Hong Lai",
      "Winston H. Hsu"
    ],
    "abstract": "Existing research often treats long-form videos as extended short videos,\nleading to several limitations: inadequate capture of long-range dependencies,\ninefficient processing of redundant information, and failure to extract\nhigh-level semantic concepts. To address these issues, we propose a novel\napproach that more accurately reflects human cognition. This paper introduces\nHERMES: temporal-coHERent long-forM understanding with Episodes and Semantics,\na model that simulates episodic memory accumulation to capture action sequences\nand reinforces them with semantic knowledge dispersed throughout the video. Our\nwork makes two key contributions: First, we develop an Episodic COmpressor\n(ECO) that efficiently aggregates crucial representations from micro to\nsemi-macro levels, overcoming the challenge of long-range dependencies. Second,\nwe propose a Semantics ReTRiever (SeTR) that enhances these aggregated\nrepresentations with semantic information by focusing on the broader context,\ndramatically reducing feature dimensionality while preserving relevant\nmacro-level information. This addresses the issues of redundancy and lack of\nhigh-level concept extraction. Extensive experiments demonstrate that HERMES\nachieves state-of-the-art performance across multiple long-video understanding\nbenchmarks in both zero-shot and fully-supervised settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "This is an improved and expanded version of our EVAL-FoMo Workshop at\n  ECCV'24 (v1 of this paper). Project page:\n  https://joslefaure.github.io/assets/html/hermes.html",
    "pdf_url": "http://arxiv.org/pdf/2408.17443v3",
    "published_date": "2024-08-30 17:52:55 UTC",
    "updated_date": "2024-11-09 06:46:41 UTC"
  },
  {
    "arxiv_id": "2408.17431v1",
    "title": "Advancing Multi-talker ASR Performance with Large Language Models",
    "authors": [
      "Mohan Shi",
      "Zengrui Jin",
      "Yaoxun Xu",
      "Yong Xu",
      "Shi-Xiong Zhang",
      "Kun Wei",
      "Yiwen Shao",
      "Chunlei Zhang",
      "Dong Yu"
    ],
    "abstract": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "8 pages, accepted by IEEE SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.17431v1",
    "published_date": "2024-08-30 17:29:25 UTC",
    "updated_date": "2024-08-30 17:29:25 UTC"
  },
  {
    "arxiv_id": "2409.10528v1",
    "title": "From Latent to Engine Manifolds: Analyzing ImageBind's Multimodal Embedding Space",
    "authors": [
      "Andrew Hamara",
      "Pablo Rivas"
    ],
    "abstract": "This study investigates ImageBind's ability to generate meaningful fused\nmultimodal embeddings for online auto parts listings. We propose a simplistic\nembedding fusion workflow that aims to capture the overlapping information of\nimage/text pairs, ultimately combining the semantics of a post into a joint\nembedding. After storing such fused embeddings in a vector database, we\nexperiment with dimensionality reduction and provide empirical evidence to\nconvey the semantic quality of the joint embeddings by clustering and examining\nthe posts nearest to each cluster centroid. Additionally, our initial findings\nwith ImageBind's emergent zero-shot cross-modal retrieval suggest that pure\naudio embeddings can correlate with semantically similar marketplace listings,\nindicating potential avenues for future research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "The 26th International Conference on Artificial Intelligence\n  (ICAI'24)",
    "pdf_url": "http://arxiv.org/pdf/2409.10528v1",
    "published_date": "2024-08-30 17:16:33 UTC",
    "updated_date": "2024-08-30 17:16:33 UTC"
  },
  {
    "arxiv_id": "2408.17422v5",
    "title": "Open-Vocabulary Action Localization with Iterative Visual Prompting",
    "authors": [
      "Naoki Wake",
      "Atsushi Kanehira",
      "Kazuhiro Sasabuchi",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ],
    "abstract": "Video action localization aims to find the timings of specific actions from a\nlong video. Although existing learning-based approaches have been successful,\nthey require annotating videos, which comes with a considerable labor cost.\nThis paper proposes a training-free, open-vocabulary approach based on emerging\noff-the-shelf vision-language models (VLMs). The challenge stems from the fact\nthat VLMs are neither designed to process long videos nor tailored for finding\nactions. We overcome these problems by extending an iterative visual prompting\ntechnique. Specifically, we sample video frames and create a concatenated image\nwith frame index labels, allowing a VLM to identify the frames that most likely\ncorrespond to the start and end of the action. By iteratively narrowing the\nsampling window around the selected frames, the estimation gradually converges\nto more precise temporal boundaries. We demonstrate that this technique yields\nreasonable performance, achieving results comparable to state-of-the-art\nzero-shot action localization. These results support the use of VLMs as a\npractical tool for understanding videos. Sample code is available at\nhttps://microsoft.github.io/VLM-Video-Action-Localization/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 5 figures, 6 tables. Published in IEEE Access. Last updated\n  on April 7th, 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.17422v5",
    "published_date": "2024-08-30 17:12:14 UTC",
    "updated_date": "2025-04-07 10:55:13 UTC"
  },
  {
    "arxiv_id": "2408.17404v1",
    "title": "Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach",
    "authors": [
      "Jialiang Wei",
      "Anne-Lise Courbis",
      "Thomas Lambolais",
      "Binbin Xu",
      "Pierre Louis Bernard",
      "Gérard Dray",
      "Walid Maalej"
    ],
    "abstract": "Over the past decade, app store (AppStore)-inspired requirements elicitation\nhas proven to be highly beneficial. Developers often explore competitors' apps\nto gather inspiration for new features. With the advance of Generative AI,\nrecent studies have demonstrated the potential of large language model\n(LLM)-inspired requirements elicitation. LLMs can assist in this process by\nproviding inspiration for new feature ideas. While both approaches are gaining\npopularity in practice, there is a lack of insight into their differences. We\nreport on a comparative study between AppStore- and LLM-based approaches for\nrefining features into sub-features. By manually analyzing 1,200 sub-features\nrecommended from both approaches, we identified their benefits, challenges, and\nkey differences. While both approaches recommend highly relevant sub-features\nwith clear descriptions, LLMs seem more powerful particularly concerning novel\nunseen app scopes. Moreover, some recommended features are imaginary with\nunclear feasibility, which suggests the importance of a human-analyst in the\nelicitation loop.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "To Appear In Proceedings of 39th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.17404v1",
    "published_date": "2024-08-30 16:42:26 UTC",
    "updated_date": "2024-08-30 16:42:26 UTC"
  },
  {
    "arxiv_id": "2408.17401v2",
    "title": "Exploring the Effect of Explanation Content and Format on User Comprehension and Trust in Healthcare",
    "authors": [
      "Antonio Rago",
      "Bence Palfi",
      "Purin Sukpanichnant",
      "Hannibal Nabli",
      "Kavyesh Vivek",
      "Olga Kostopoulou",
      "James Kinross",
      "Francesca Toni"
    ],
    "abstract": "AI-driven tools for healthcare are widely acknowledged as potentially\nbeneficial to health practitioners and patients, e.g. the QCancer regression\ntool for cancer risk prediction. However, for these tools to be trusted, they\nneed to be supplemented with explanations. We examine how explanations' content\nand format affect user comprehension and trust when explaining QCancer's\npredictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding\nformat, we present SHAP explanations, conventionally, as charts (SC) and\nOcclusion-1 explanations as charts (OC) as well as text (OT), to which their\nsimpler nature lends itself. We conduct experiments with two sets of\nstakeholders: the general public (representing patients) and medical students\n(representing healthcare practitioners). Our experiments showed higher\nsubjective comprehension and trust for Occlusion-1 over SHAP explanations based\non content. However, when controlling for format, only OT outperformed SC,\nsuggesting this trend is driven by preferences for text. Other findings\ncorroborated that explanation format, rather than content, is often the\ncritical factor.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.17401v2",
    "published_date": "2024-08-30 16:36:53 UTC",
    "updated_date": "2025-02-17 17:49:36 UTC"
  },
  {
    "arxiv_id": "2408.17383v2",
    "title": "MoRe Fine-Tuning with 10x Fewer Parameters",
    "authors": [
      "Wenxuan Tan",
      "Nicholas Roberts",
      "Tzu-Heng Huang",
      "Jitian Zhao",
      "John Cooper",
      "Samuel Guo",
      "Chengyu Duan",
      "Frederic Sala"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential\nto cheaply and easily specialize large pretrained models. However, the most\nprominent approaches, like low-rank adapters (LoRA), depend on heuristics or\nrules-of-thumb for their architectural choices -- potentially limiting their\nperformance for new models and architectures. This limitation suggests that\ntechniques from neural architecture search could be used to obtain optimal\nadapter architectures, but these are often expensive and difficult to\nimplement. We address this challenge with Monarch Rectangular Fine-tuning\n(MoRe), a simple framework to search over adapter architectures that relies on\nthe Monarch matrix class. Theoretically, we show that MoRe is more expressive\nthan LoRA. Empirically, our approach is more parameter-efficient and performant\nthan state-of-the-art PEFTs on a range of tasks and models, with as few as 5\\%\nof LoRA's parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17383v2",
    "published_date": "2024-08-30 16:24:27 UTC",
    "updated_date": "2025-04-05 19:41:18 UTC"
  },
  {
    "arxiv_id": "2409.00163v1",
    "title": "Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery",
    "authors": [
      "Yuhan Zheng",
      "Jessie A Elliott",
      "John V Reynolds",
      "Sheraz R Markar",
      "Bartłomiej W. Papież",
      "ENSURE study group"
    ],
    "abstract": "Esophageal cancer is a major cause of cancer-related mortality\ninternationally, with high recurrence rates and poor survival even among\npatients treated with curative-intent surgery. Investigating relevant\nprognostic factors and predicting prognosis can enhance post-operative clinical\ndecision-making and potentially improve patients' outcomes. In this work, we\nassessed prognostic factor identification and discriminative performances of\nthree models for Disease-Free Survival (DFS) and Overall Survival (OS) using a\nlarge multicenter international dataset from ENSURE study. We first employed\nCox Proportional Hazards (CoxPH) model to assess the impact of each feature on\noutcomes. Subsequently, we utilised CoxPH and two deep neural network\n(DNN)-based models, DeepSurv and DeepHit, to predict DFS and OS. The\nsignificant prognostic factors identified by our models were consistent with\nclinical literature, with post-operative pathologic features showing higher\nsignificance than clinical stage features. DeepSurv and DeepHit demonstrated\ncomparable discriminative accuracy to CoxPH, with DeepSurv slightly\noutperforming in both DFS and OS prediction tasks, achieving C-index of 0.735\nand 0.74, respectively. While these results suggested the potential of DNNs as\nprognostic tools for improving predictive accuracy and providing personalised\nguidance with respect to risk stratification, CoxPH still remains an adequately\ngood prediction model, with the data used in this study.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 3 figures, 4 tables. To appear in CaPTion: MICCAI Workshop\n  on Cancer Prevention, detection, and intervenTion, Sharib Ali et al., MICCAI\n  2024, Lecture Notes in Computer Science, Springer",
    "pdf_url": "http://arxiv.org/pdf/2409.00163v1",
    "published_date": "2024-08-30 16:20:47 UTC",
    "updated_date": "2024-08-30 16:20:47 UTC"
  },
  {
    "arxiv_id": "2408.17380v2",
    "title": "Traffic expertise meets residual RL: Knowledge-informed model-based residual reinforcement learning for CAV trajectory control",
    "authors": [
      "Zihao Sheng",
      "Zilin Huang",
      "Sikai Chen"
    ],
    "abstract": "Model-based reinforcement learning (RL) is anticipated to exhibit higher\nsample efficiency compared to model-free RL by utilizing a virtual environment\nmodel. However, it is challenging to obtain sufficiently accurate\nrepresentations of the environmental dynamics due to uncertainties in complex\nsystems and environments. An inaccurate environment model may degrade the\nsample efficiency and performance of model-based RL. Furthermore, while\nmodel-based RL can improve sample efficiency, it often still requires\nsubstantial training time to learn from scratch, potentially limiting its\nadvantages over model-free approaches. To address these challenges, this paper\nintroduces a knowledge-informed model-based residual reinforcement learning\nframework aimed at enhancing learning efficiency by infusing established expert\nknowledge into the learning process and avoiding the issue of beginning from\nzero. Our approach integrates traffic expert knowledge into a virtual\nenvironment model, employing the Intelligent Driver Model (IDM) for basic\ndynamics and neural networks for residual dynamics, thus ensuring adaptability\nto complex scenarios. We propose a novel strategy that combines traditional\ncontrol methods with residual RL, facilitating efficient learning and policy\noptimization without the need to learn from scratch. The proposed approach is\napplied to CAV trajectory control tasks for the dissipation of stop-and-go\nwaves in mixed traffic flow. Experimental results demonstrate that our proposed\napproach enables the CAV agent to achieve superior performance in trajectory\ncontrol compared to the baseline agents in terms of sample efficiency, traffic\nflow smoothness and traffic mobility. The source code and supplementary\nmaterials are available at: https://zihaosheng.github.io/traffic-expertise-RL/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Communications in Transportation Research",
    "pdf_url": "http://arxiv.org/pdf/2408.17380v2",
    "published_date": "2024-08-30 16:16:57 UTC",
    "updated_date": "2025-02-03 03:48:01 UTC"
  },
  {
    "arxiv_id": "2408.17379v2",
    "title": "EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution",
    "authors": [
      "Francesco Argenziano",
      "Michele Brienza",
      "Vincenzo Suriani",
      "Daniele Nardi",
      "Domenico D. Bloisi"
    ],
    "abstract": "Task planning for robots in real-life settings presents significant\nchallenges. These challenges stem from three primary issues: the difficulty in\nidentifying grounded sequences of steps to achieve a goal; the lack of a\nstandardized mapping between high-level actions and low-level commands; and the\nchallenge of maintaining low computational overhead given the limited resources\nof robotic hardware. We introduce EMPOWER, a framework designed for\nopen-vocabulary online grounding and planning for embodied agents aimed at\naddressing these issues. By leveraging efficient pre-trained foundation models\nand a multi-role mechanism, EMPOWER demonstrates notable improvements in\ngrounded planning and execution. Quantitative results highlight the\neffectiveness of our approach, achieving an average success rate of 0.73 across\nsix different real-life scenarios using a TIAGo robot.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at IROS 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.17379v2",
    "published_date": "2024-08-30 16:15:28 UTC",
    "updated_date": "2024-10-22 16:58:31 UTC"
  },
  {
    "arxiv_id": "2409.00162v1",
    "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback",
    "authors": [
      "Jiayi Zhou",
      "Jiaming Ji",
      "Juntao Dai",
      "Yaodong Yang"
    ],
    "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions\nand values remains a critical challenge. Reinforcement learning from human\nfeedback (RLHF) aligns LLMs by training a reward model (RM) on human\npreferences and fine-tuning the LLMs to maximize RM feedback. Despite its\neffectiveness and popularity, RLHF is prone to biased local optimization. It\nmeans RM fails to provide feedback that accurately aligns with human\npreference, causing LLMs to explore unexpected generalizations, and failing to\nachieve alignment objectives. To mitigate this issue, we propose a novel\n\\textit{sequence-to-sequence (seq2seq) reward modeling} method. Its key insight\nis that learning from language feedback rather than scalar feedback improves\nRLHF without additional annotations. We replaced the reward modeling target\nfrom binary maximum likelihood estimation (MLE) with sequence MLE. This method\nenables richer and fine-grained language feedback without additional\nannotations, models, or training stages. Our experiments demonstrated its\neffectiveness, specifically, reducing the refusal-to-response paradigm in\nsingle-turn safety dialogues and the long-response bias in text summarization\ntasks. We provide further analysis that seq2seq RM improves RLHF performance\nacross 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\\%.\nWe further show that seq2seq RM can still improve the performance of RLHF under\nout-of-distribution prompts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.00162v1",
    "published_date": "2024-08-30 16:14:35 UTC",
    "updated_date": "2024-08-30 16:14:35 UTC"
  },
  {
    "arxiv_id": "2408.17377v1",
    "title": "NDP: Next Distribution Prediction as a More Broad Target",
    "authors": [
      "Junhao Ruan",
      "Abudukeyumu Abudula",
      "Xinyu Liu",
      "Bei Li",
      "Yinqiao Li",
      "Chenglong Wang",
      "Yuchun Fan",
      "Yuan Ge",
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "abstract": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages,5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17377v1",
    "published_date": "2024-08-30 16:13:49 UTC",
    "updated_date": "2024-08-30 16:13:49 UTC"
  },
  {
    "arxiv_id": "2409.00160v1",
    "title": "Learning-Based Finite Element Methods Modeling for Complex Mechanical Systems",
    "authors": [
      "Jiasheng Shi",
      "Fu Lin",
      "Weixiong Rao"
    ],
    "abstract": "Complex mechanic systems simulation is important in many real-world\napplications. The de-facto numeric solver using Finite Element Method (FEM)\nsuffers from computationally intensive overhead. Though with many progress on\nthe reduction of computational time and acceptable accuracy, the recent CNN or\nGNN-based simulation models still struggle to effectively represent complex\nmechanic simulation caused by the long-range spatial dependency of distance\nmesh nodes and independently learning local and global representation. In this\npaper, we propose a novel two-level mesh graph network. The key of the network\nis to interweave the developed Graph Block and Attention Block to better learn\nmechanic interactions even for long-rang spatial dependency. Evaluation on\nthree synthetic and one real datasets demonstrates the superiority of our work.\nFor example, on the Beam dataset, our work leads to 54.3\\% lower prediction\nerrors and 9.87\\% fewer learnable network parameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00160v1",
    "published_date": "2024-08-30 15:56:50 UTC",
    "updated_date": "2024-08-30 15:56:50 UTC"
  },
  {
    "arxiv_id": "2408.17366v1",
    "title": "Leveraging Graph Neural Networks to Forecast Electricity Consumption",
    "authors": [
      "Eloi Campagne",
      "Yvenn Amara-Ouali",
      "Yannig Goude",
      "Argyris Kalogeratos"
    ],
    "abstract": "Accurate electricity demand forecasting is essential for several reasons,\nespecially as the integration of renewable energy sources and the transition to\na decentralized network paradigm introduce greater complexity and uncertainty.\nThe proposed methodology leverages graph-based representations to effectively\ncapture the spatial distribution and relational intricacies inherent in this\ndecentralized network structure. This research work offers a novel approach\nthat extends beyond the conventional Generalized Additive Model framework by\nconsidering models like Graph Convolutional Networks or Graph SAGE. These\ngraph-based models enable the incorporation of various levels of\ninterconnectedness and information sharing among nodes, where each node\ncorresponds to the combined load (i.e. consumption) of a subset of consumers\n(e.g. the regions of a country). More specifically, we introduce a range of\nmethods for inferring graphs tailored to consumption forecasting, along with a\nframework for evaluating the developed models in terms of both performance and\nexplainability. We conduct experiments on electricity forecasting, in both a\nsynthetic and a real framework considering the French mainland regions, and the\nperformance and merits of our approach are discussed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, ECML PKDD 2024 Workshop paper",
    "pdf_url": "http://arxiv.org/pdf/2408.17366v1",
    "published_date": "2024-08-30 15:54:50 UTC",
    "updated_date": "2024-08-30 15:54:50 UTC"
  },
  {
    "arxiv_id": "2409.10527v1",
    "title": "Towards Empathetic Conversational Recommender Systems",
    "authors": [
      "Xiaoyu Zhang",
      "Ruobing Xie",
      "Yougang Lyu",
      "Xin Xin",
      "Pengjie Ren",
      "Mingfei Liang",
      "Bo Zhang",
      "Zhanhui Kang",
      "Maarten de Rijke",
      "Zhaochun Ren"
    ],
    "abstract": "Conversational recommender systems (CRSs) are able to elicit user preferences\nthrough multi-turn dialogues. They typically incorporate external knowledge and\npre-trained language models to capture the dialogue context. Most CRS\napproaches, trained on benchmark datasets, assume that the standard items and\nresponses in these benchmarks are optimal. However, they overlook that users\nmay express negative emotions with the standard items and may not feel\nemotionally engaged by the standard responses. This issue leads to a tendency\nto replicate the logic of recommenders in the dataset instead of aligning with\nuser needs. To remedy this misalignment, we introduce empathy within a CRS.\nWith empathy we refer to a system's ability to capture and express emotions. We\npropose an empathetic conversational recommender (ECR) framework.\n  ECR contains two main modules: emotion-aware item recommendation and\nemotion-aligned response generation. Specifically, we employ user emotions to\nrefine user preference modeling for accurate recommendations. To generate\nhuman-like emotional responses, ECR applies retrieval-augmented prompts to\nfine-tune a pre-trained language model aligning with emotions and mitigating\nhallucination. To address the challenge of insufficient supervision labels, we\nenlarge our empathetic data using emotion labels annotated by large language\nmodels and emotional reviews collected from external resources. We propose\nnovel evaluation metrics to capture user satisfaction in real-world CRS\nscenarios. Our experiments on the ReDial dataset validate the efficacy of our\nframework in enhancing recommendation accuracy and improving user satisfaction.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10527v1",
    "published_date": "2024-08-30 15:43:07 UTC",
    "updated_date": "2024-08-30 15:43:07 UTC"
  },
  {
    "arxiv_id": "2408.17356v1",
    "title": "C-RADAR: A Centralized Deep Learning System for Intrusion Detection in Software Defined Networks",
    "authors": [
      "Osama Mustafa",
      "Khizer Ali",
      "Talha Naqash"
    ],
    "abstract": "The popularity of Software Defined Networks (SDNs) has grown in recent years,\nmainly because of their ability to simplify network management and improve\nnetwork flexibility. However, this also makes them vulnerable to various types\nof cyber attacks. SDNs work on a centralized control plane which makes them\nmore prone to network attacks. Research has demonstrated that deep learning\n(DL) methods can be successful in identifying intrusions in conventional\nnetworks, but their application in SDNs is still an open research area. In this\nresearch, we propose the use of DL techniques for intrusion detection in SDNs.\nWe measure the effectiveness of our method by experimentation on a dataset of\nnetwork traffic and comparing it to existing techniques. Our results show that\nthe DL-based approach outperforms traditional methods in terms of detection\naccuracy and computational efficiency. The deep learning architecture that has\nbeen used in this research is a Long Short Term Memory Network and\nSelf-Attention based architecture i.e. LSTM-Attn which achieves an Fl-score of\n0.9721. Furthermore, this technique can be trained to detect new attack\npatterns and improve the overall security of SDNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17356v1",
    "published_date": "2024-08-30 15:39:37 UTC",
    "updated_date": "2024-08-30 15:39:37 UTC"
  },
  {
    "arxiv_id": "2408.17355v4",
    "title": "Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling",
    "authors": [
      "Yuejiang Liu",
      "Jubayer Ibn Hamid",
      "Annie Xie",
      "Yoonho Lee",
      "Maximilian Du",
      "Chelsea Finn"
    ],
    "abstract": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its effects on the learned policy remain\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity to\nunexpected states. To address this tradeoff, we propose Bidirectional Decoding\n(BID), a test-time inference algorithm that bridges action chunking with\nclosed-loop adaptation. At each timestep, BID samples multiple candidate\npredictions and searches for the optimal one based on two criteria: (i)\nbackward coherence, which favors samples that align with previous decisions;\n(ii) forward contrast, which seeks samples of high likelihood for future plans.\nBy coupling decisions within and across action chunks, BID promotes both\nlong-term consistency and short-term reactivity. Experimental results show that\nour method boosts the performance of two state-of-the-art generative policies\nacross seven simulation benchmarks and two real-world tasks. Code and videos\nare available at https://bid-robot.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://bid-robot.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2408.17355v4",
    "published_date": "2024-08-30 15:39:34 UTC",
    "updated_date": "2025-04-25 17:27:10 UTC"
  },
  {
    "arxiv_id": "2408.17354v1",
    "title": "Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage",
    "authors": [
      "Md Rafi Ur Rashid",
      "Jing Liu",
      "Toshiaki Koike-Akino",
      "Shagufta Mehnaz",
      "Ye Wang"
    ],
    "abstract": "Fine-tuning large language models on private data for downstream applications\nposes significant privacy risks in potentially exposing sensitive information.\nSeveral popular community platforms now offer convenient distribution of a\nlarge variety of pre-trained models, allowing anyone to publish without\nrigorous verification. This scenario creates a privacy threat, as pre-trained\nmodels can be intentionally crafted to compromise the privacy of fine-tuning\ndatasets. In this study, we introduce a novel poisoning technique that uses\nmodel-unlearning as an attack tool. This approach manipulates a pre-trained\nlanguage model to increase the leakage of private data during the fine-tuning\nprocess. Our method enhances both membership inference and data extraction\nattacks while preserving model utility. Experimental results across different\nmodels, datasets, and fine-tuning setups demonstrate that our attacks\nsignificantly surpass baseline performance. This work serves as a cautionary\nnote for users who download pre-trained models from unverified sources,\nhighlighting the potential risks involved.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17354v1",
    "published_date": "2024-08-30 15:35:09 UTC",
    "updated_date": "2024-08-30 15:35:09 UTC"
  },
  {
    "arxiv_id": "2408.17352v1",
    "title": "AASIST3: KAN-Enhanced AASIST Speech Deepfake Detection using SSL Features and Additional Regularization for the ASVspoof 2024 Challenge",
    "authors": [
      "Kirill Borodin",
      "Vasiliy Kudryavtsev",
      "Dmitrii Korzh",
      "Alexey Efimenko",
      "Grach Mkrtchian",
      "Mikhail Gorodnichev",
      "Oleg Y. Rogov"
    ],
    "abstract": "Automatic Speaker Verification (ASV) systems, which identify speakers based\non their voice characteristics, have numerous applications, such as user\nauthentication in financial transactions, exclusive access control in smart\ndevices, and forensic fraud detection. However, the advancement of deep\nlearning algorithms has enabled the generation of synthetic audio through\nText-to-Speech (TTS) and Voice Conversion (VC) systems, exposing ASV systems to\npotential vulnerabilities. To counteract this, we propose a novel architecture\nnamed AASIST3. By enhancing the existing AASIST framework with\nKolmogorov-Arnold networks, additional layers, encoders, and pre-emphasis\ntechniques, AASIST3 achieves a more than twofold improvement in performance. It\ndemonstrates minDCF results of 0.5357 in the closed condition and 0.1414 in the\nopen condition, significantly enhancing the detection of synthetic voices and\nimproving ASV security.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages, 2 figures, 2 tables. Accepted paper at the ASVspoof 2024\n  (the 25th Interspeech Conference)",
    "pdf_url": "http://arxiv.org/pdf/2408.17352v1",
    "published_date": "2024-08-30 15:30:01 UTC",
    "updated_date": "2024-08-30 15:30:01 UTC"
  },
  {
    "arxiv_id": "2408.17344v2",
    "title": "rerankers: A Lightweight Python Library to Unify Ranking Methods",
    "authors": [
      "Benjamin Clavié"
    ],
    "abstract": "This paper presents rerankers, a Python library which provides an easy-to-use\ninterface to the most commonly used re-ranking approaches. Re-ranking is an\nintegral component of many retrieval pipelines; however, there exist numerous\napproaches to it, relying on different implementation methods. rerankers\nunifies these methods into a single user-friendly interface, allowing\npractitioners and researchers alike to explore different methods while only\nchanging a single line of Python code. Moreover ,rerankers ensures that its\nimplementations are done with the fewest dependencies possible, and re-uses the\noriginal implementation whenever possible, guaranteeing that our simplified\ninterface results in no performance degradation compared to more complex ones.\nThe full source code and list of supported models are updated regularly and\navailable at https://github.com/answerdotai/rerankers.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17344v2",
    "published_date": "2024-08-30 15:16:52 UTC",
    "updated_date": "2024-09-03 10:50:17 UTC"
  },
  {
    "arxiv_id": "2409.10526v1",
    "title": "Effective Monitoring of Online Decision-Making Algorithms in Digital Intervention Implementation",
    "authors": [
      "Anna L. Trella",
      "Susobhan Ghosh",
      "Erin E. Bonar",
      "Lara Coughlin",
      "Finale Doshi-Velez",
      "Yongyi Guo",
      "Pei-Yao Hung",
      "Inbal Nahum-Shani",
      "Vivek Shetty",
      "Maureen Walton",
      "Iris Yan",
      "Kelly W. Zhang",
      "Susan A. Murphy"
    ],
    "abstract": "Online AI decision-making algorithms are increasingly used by digital\ninterventions to dynamically personalize treatment to individuals. These\nalgorithms determine, in real-time, the delivery of treatment based on accruing\ndata. The objective of this paper is to provide guidelines for enabling\neffective monitoring of online decision-making algorithms with the goal of (1)\nsafeguarding individuals and (2) ensuring data quality. We elucidate guidelines\nand discuss our experience in monitoring online decision-making algorithms in\ntwo digital intervention clinical trials (Oralytics and MiWaves). Our\nguidelines include (1) developing fallback methods, pre-specified procedures\nexecuted when an issue occurs, and (2) identifying potential issues\ncategorizing them by severity (red, yellow, and green). Across both trials, the\nmonitoring systems detected real-time issues such as out-of-memory issues,\ndatabase timeout, and failed communication with an external source. Fallback\nmethods prevented participants from not receiving any treatment during the\ntrial and also prevented the use of incorrect data in statistical analyses.\nThese trials provide case studies for how health scientists can build\nmonitoring systems for their digital intervention. Without these algorithm\nmonitoring systems, critical issues would have gone undetected and unresolved.\nInstead, these monitoring systems safeguarded participants and ensured the\nquality of the resulting data for updating the intervention and facilitating\nscientific discovery. These monitoring guidelines and findings give digital\nintervention teams the confidence to include online decision-making algorithms\nin digital interventions.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10526v1",
    "published_date": "2024-08-30 15:13:58 UTC",
    "updated_date": "2024-08-30 15:13:58 UTC"
  },
  {
    "arxiv_id": "2409.00159v3",
    "title": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities",
    "authors": [
      "Gurvan Richardeau",
      "Samy Chali",
      "Erwan Le Merrer",
      "Camilla Penzo",
      "Gilles Tredan"
    ],
    "abstract": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "A preliminary version of this work appeared in the Complex Networks\n  2024 conference, under the title \"LLMs hallucinate graphs too: a structural\n  perspective\"",
    "pdf_url": "http://arxiv.org/pdf/2409.00159v3",
    "published_date": "2024-08-30 15:04:11 UTC",
    "updated_date": "2025-04-04 10:58:40 UTC"
  },
  {
    "arxiv_id": "2409.00158v1",
    "title": "Developing an End-to-End Framework for Predicting the Social Communication Severity Scores of Children with Autism Spectrum Disorder",
    "authors": [
      "Jihyun Mun",
      "Sunhee Kim",
      "Minhwa Chung"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) is a lifelong condition that significantly\ninfluencing an individual's communication abilities and their social\ninteractions. Early diagnosis and intervention are critical due to the profound\nimpact of ASD's characteristic behaviors on foundational developmental stages.\nHowever, limitations of standardized diagnostic tools necessitate the\ndevelopment of objective and precise diagnostic methodologies. This paper\nproposes an end-to-end framework for automatically predicting the social\ncommunication severity of children with ASD from raw speech data. This\nframework incorporates an automatic speech recognition model, fine-tuned with\nspeech data from children with ASD, followed by the application of fine-tuned\npre-trained language models to generate a final prediction score. Achieving a\nPearson Correlation Coefficient of 0.6566 with human-rated scores, the proposed\nmethod showcases its potential as an accessible and objective tool for the\nassessment of ASD.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00158v1",
    "published_date": "2024-08-30 14:43:58 UTC",
    "updated_date": "2024-08-30 14:43:58 UTC"
  },
  {
    "arxiv_id": "2408.17324v1",
    "title": "Modularity in Transformers: Investigating Neuron Separability & Specialization",
    "authors": [
      "Nicholas Pochinkov",
      "Thomas Jones",
      "Mohammed Rashidur Rahman"
    ],
    "abstract": "Transformer models are increasingly prevalent in various applications, yet\nour understanding of their internal workings remains limited. This paper\ninvestigates the modularity and task specialization of neurons within\ntransformer architectures, focusing on both vision (ViT) and language (Mistral\n7B) models. Using a combination of selective pruning and MoEfication clustering\ntechniques, we analyze the overlap and specialization of neurons across\ndifferent tasks and data subsets. Our findings reveal evidence of task-specific\nneuron clusters, with varying degrees of overlap between related tasks. We\nobserve that neuron importance patterns persist to some extent even in randomly\ninitialized models, suggesting an inherent structure that training refines.\nAdditionally, we find that neuron clusters identified through MoEfication\ncorrespond more strongly to task-specific neurons in earlier and later layers\nof the models. This work contributes to a more nuanced understanding of\ntransformer internals and offers insights into potential avenues for improving\nmodel interpretability and efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T07 (Primary) 68Q32, 68T05 (Secondary)",
      "I.2.4; I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17324v1",
    "published_date": "2024-08-30 14:35:01 UTC",
    "updated_date": "2024-08-30 14:35:01 UTC"
  },
  {
    "arxiv_id": "2408.17322v1",
    "title": "Investigating Neuron Ablation in Attention Heads: The Case for Peak Activation Centering",
    "authors": [
      "Nicholas Pochinkov",
      "Ben Pasero",
      "Skylar Shibayama"
    ],
    "abstract": "The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "68T07 (Primary) 68T30, 68T50 (Secondary)",
      "I.2.4; I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 2 figures, XAI World Conference 2024 Late-Breaking Work",
    "pdf_url": "http://arxiv.org/pdf/2408.17322v1",
    "published_date": "2024-08-30 14:32:25 UTC",
    "updated_date": "2024-08-30 14:32:25 UTC"
  },
  {
    "arxiv_id": "2408.17316v1",
    "title": "Bridging Domain Knowledge and Process Discovery Using Large Language Models",
    "authors": [
      "Ali Norouzifar",
      "Humam Kourani",
      "Marcus Dees",
      "Wil van der Aalst"
    ],
    "abstract": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper is accepted at the AI4BPM 2024 workshop and to be\n  published in their proceedings",
    "pdf_url": "http://arxiv.org/pdf/2408.17316v1",
    "published_date": "2024-08-30 14:23:40 UTC",
    "updated_date": "2024-08-30 14:23:40 UTC"
  },
  {
    "arxiv_id": "2408.17313v1",
    "title": "Fair Best Arm Identification with Fixed Confidence",
    "authors": [
      "Alessio Russo",
      "Filippo Vannella"
    ],
    "abstract": "In this work, we present a novel framework for Best Arm Identification (BAI)\nunder fairness constraints, a setting that we refer to as \\textit{F-BAI} (fair\nBAI). Unlike traditional BAI, which solely focuses on identifying the optimal\narm with minimal sample complexity, F-BAI also includes a set of fairness\nconstraints. These constraints impose a lower limit on the selection rate of\neach arm and can be either model-agnostic or model-dependent. For this setting,\nwe establish an instance-specific sample complexity lower bound and analyze the\n\\textit{price of fairness}, quantifying how fairness impacts sample complexity.\nBased on the sample complexity lower bound, we propose F-TaS, an algorithm\nprovably matching the sample complexity lower bound, while ensuring that the\nfairness constraints are satisfied. Numerical results, conducted using both a\nsynthetic model and a practical wireless scheduling application, show the\nefficiency of F-TaS in minimizing the sample complexity while achieving low\nfairness violations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17313v1",
    "published_date": "2024-08-30 14:18:34 UTC",
    "updated_date": "2024-08-30 14:18:34 UTC"
  },
  {
    "arxiv_id": "2408.17307v1",
    "title": "Hybridizing Base-Line 2D-CNN Model with Cat Swarm Optimization for Enhanced Advanced Persistent Threat Detection",
    "authors": [
      "Ali M. Bakhiet",
      "Salah A. Aly"
    ],
    "abstract": "In the realm of cyber-security, detecting Advanced Persistent Threats (APTs)\nremains a formidable challenge due to their stealthy and sophisticated nature.\nThis research paper presents an innovative approach that leverages\nConvolutional Neural Networks (CNNs) with a 2D baseline model, enhanced by the\ncutting-edge Cat Swarm Optimization (CSO) algorithm, to significantly improve\nAPT detection accuracy. By seamlessly integrating the 2D-CNN baseline model\nwith CSO, we unlock the potential for unprecedented accuracy and efficiency in\nAPT detection. The results unveil an impressive accuracy score of $98.4\\%$,\nmarking a significant enhancement in APT detection across various attack\nstages, illuminating a path forward in combating these relentless and\nsophisticated threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17307v1",
    "published_date": "2024-08-30 14:11:12 UTC",
    "updated_date": "2024-08-30 14:11:12 UTC"
  },
  {
    "arxiv_id": "2409.09053v1",
    "title": "Deep learning-based classification of breast cancer molecular subtypes from H&E whole-slide images",
    "authors": [
      "Masoud Tafavvoghi",
      "Anders Sildnes",
      "Mehrdad Rakaee",
      "Nikita Shvetsov",
      "Lars Ailo Bongo",
      "Lill-Tove Rasmussen Busund",
      "Kajsa Møllersen"
    ],
    "abstract": "Classifying breast cancer molecular subtypes is crucial for tailoring\ntreatment strategies. While immunohistochemistry (IHC) and gene expression\nprofiling are standard methods for molecular subtyping, IHC can be subjective,\nand gene profiling is costly and not widely accessible in many regions.\nPrevious approaches have highlighted the potential application of deep learning\nmodels on H&E-stained whole slide images (WSI) for molecular subtyping, but\nthese efforts vary in their methods, datasets, and reported performance. In\nthis work, we investigated whether H&E-stained WSIs could be solely leveraged\nto predict breast cancer molecular subtypes (luminal A, B, HER2-enriched, and\nBasal). We used 1,433 WSIs of breast cancer in a two-step pipeline: first,\nclassifying tumor and non-tumor tiles to use only the tumor regions for\nmolecular subtyping; and second, employing a One-vs-Rest (OvR) strategy to\ntrain four binary OvR classifiers and aggregating their results using an\neXtreme Gradient Boosting (XGBoost) model. The pipeline was tested on 221\nhold-out WSIs, achieving an overall macro F1 score of 0.95 for tumor detection\nand 0.73 for molecular subtyping. Our findings suggest that, with further\nvalidation, supervised deep learning models could serve as supportive tools for\nmolecular subtyping in breast cancer. Our codes are made available to\nfacilitate ongoing research and development.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "16 pages, 5 figures (+4 supplementary figures), 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.09053v1",
    "published_date": "2024-08-30 13:57:33 UTC",
    "updated_date": "2024-08-30 13:57:33 UTC"
  },
  {
    "arxiv_id": "2408.17298v1",
    "title": "Accelerating the discovery of steady-states of planetary interior dynamics with machine learning",
    "authors": [
      "Siddhant Agarwal",
      "Nicola Tosi",
      "Christian Hüttig",
      "David S. Greenberg",
      "Ali Can Bekar"
    ],
    "abstract": "Simulating mantle convection often requires reaching a computationally\nexpensive steady-state, crucial for deriving scaling laws for thermal and\ndynamical flow properties and benchmarking numerical solutions. The strong\ntemperature dependence of the rheology of mantle rocks causes viscosity\nvariations of several orders of magnitude, leading to a slow-evolving stagnant\nlid where heat conduction dominates, overlying a rapidly-evolving and strongly\nconvecting region. Time-stepping methods, while effective for fluids with\nconstant viscosity, are hindered by the Courant criterion, which restricts the\ntime step based on the system's maximum velocity and grid size. Consequently,\nachieving steady-state requires a large number of time steps due to the\ndisparate time scales governing the stagnant and convecting regions.\n  We present a concept for accelerating mantle convection simulations using\nmachine learning. We generate a dataset of 128 two-dimensional simulations with\nmixed basal and internal heating, and pressure- and temperature-dependent\nviscosity. We train a feedforward neural network on 97 simulations to predict\nsteady-state temperature profiles. These can then be used to initialize\nnumerical time stepping methods for different simulation parameters. Compared\nto typical initializations, the number of time steps required to reach\nsteady-state is reduced by a median factor of 3.75. The benefit of this method\nlies in requiring very few simulations to train on, providing a solution with\nno prediction error as we initialize a numerical method, and posing minimal\ncomputational overhead at inference time. We demonstrate the effectiveness of\nour approach and discuss the potential implications for accelerated simulations\nfor advancing mantle convection research.",
    "categories": [
      "physics.flu-dyn",
      "astro-ph.EP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17298v1",
    "published_date": "2024-08-30 13:55:19 UTC",
    "updated_date": "2024-08-30 13:55:19 UTC"
  },
  {
    "arxiv_id": "2408.17286v2",
    "title": "Risk-averse Total-reward MDPs with ERM and EVaR",
    "authors": [
      "Xihong Su",
      "Julien Grand-Clément",
      "Marek Petrik"
    ],
    "abstract": "Optimizing risk-averse objectives in discounted MDPs is challenging because\nmost models do not admit direct dynamic programming equations and require\ncomplex history-dependent policies. In this paper, we show that the risk-averse\n{\\em total reward criterion}, under the Entropic Risk Measure (ERM) and\nEntropic Value at Risk (EVaR) risk measures, can be optimized by a stationary\npolicy, making it simple to analyze, interpret, and deploy. We propose\nexponential value iteration, policy iteration, and linear programming to\ncompute optimal policies. Compared with prior work, our results only require\nthe relatively mild condition of transient MDPs and allow for {\\em both}\npositive and negative rewards. Our results indicate that the total reward\ncriterion may be preferable to the discounted criterion in a broad range of\nrisk-averse reinforcement learning domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17286v2",
    "published_date": "2024-08-30 13:33:18 UTC",
    "updated_date": "2024-12-18 16:10:18 UTC"
  },
  {
    "arxiv_id": "2409.09052v1",
    "title": "OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography",
    "authors": [
      "Youzhu Jin",
      "Yichen Zhang"
    ],
    "abstract": "Multimodal large language models (MLLMs) have achieved significant success in\nthe general field of image processing. Their emerging task generalization and\nfreeform conversational capabilities can greatly facilitate medical diagnostic\nassistance, helping patients better understand their conditions and enhancing\ndoctor-patient trust. Computed Tomography (CT) is a non-invasive imaging\ntechnique used to capture the internal mechanisms of a patient's condition and\nis widely utilized. However, in past research, the complex textural features of\nthis imaging data have made accurate interpretation by algorithms challenging,\nimpeding the performance of general LLMs in diagnostic assistance. To address\nthis, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is\ntrained on 120,000 CT images and diagnostic reports and includes a\nRetrieval-Augmented Generation (RAG) module capable of effectively mitigating\nmodel hallucinations. This module is informed by extensive medical literature,\ntextbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT\nimages but also stores, understands, and reasons over medical knowledge and\nlanguage. In extensive experiments, OrthoDoc outperforms commercial models led\nby GPT-4, demonstrating superior diagnostic capabilities and accuracy.\nSpecifically, OrthoDoc significantly surpasses existing models in the diagnosis\nof common orthopedic conditions such as fractures, arthritis, and tumors.\nAdditionally, OrthoDoc exhibits robust generalization and stability when\nhandling rare and complex cases.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.09052v1",
    "published_date": "2024-08-30 13:31:32 UTC",
    "updated_date": "2024-08-30 13:31:32 UTC"
  },
  {
    "arxiv_id": "2408.17280v2",
    "title": "Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts",
    "authors": [
      "Rhui Dih Lee",
      "Laura Wynter",
      "Raghu Kiran Ganti"
    ],
    "abstract": "We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE)\nfrom trained models. The toolkit can be used for creating a mixture from models\nor from adapters. We perform extensive tests and offer guidance on defining the\narchitecture of the resulting MOE using the toolkit. A public repository is\navailable.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17280v2",
    "published_date": "2024-08-30 13:28:45 UTC",
    "updated_date": "2024-09-11 02:52:19 UTC"
  },
  {
    "arxiv_id": "2408.17267v3",
    "title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios",
    "authors": [
      "Baichuan Zhou",
      "Haote Yang",
      "Dairong Chen",
      "Junyan Ye",
      "Tianyi Bai",
      "Jinhua Yu",
      "Songyang Zhang",
      "Dahua Lin",
      "Conghui He",
      "Weijia Li"
    ],
    "abstract": "Recent evaluations of Large Multimodal Models (LMMs) have explored their\ncapabilities in various domains, with only few benchmarks specifically focusing\non urban environments. Moreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under singular views,\nleading to incomplete evaluations of LMMs' abilities in urban environments. To\naddress these issues, we present UrBench, a comprehensive benchmark designed\nfor evaluating LMMs in complex multi-view urban scenarios. UrBench contains\n11.6K meticulously curated questions at both region-level and role-level that\ncover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene\nUnderstanding, and Object Understanding, totaling 14 task types. In\nconstructing UrBench, we utilize data from existing datasets and additionally\ncollect data from 11 cities, creating new annotations using a cross-view\ndetection-matching method. With these images and annotations, we then integrate\nLMM-based, rule-based, and human-based methods to construct large-scale\nhigh-quality questions. Our evaluations on 21 LMMs show that current LMMs\nstruggle in the urban environments in several aspects. Even the best performing\nGPT-4o lags behind humans in most tasks, ranging from simple tasks such as\ncounting to complex tasks such as orientation, localization and object\nattribute recognition, with an average performance gap of 17.4%. Our benchmark\nalso reveals that LMMs exhibit inconsistent behaviors with different urban\nviews, especially with respect to understanding cross-view relations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17267v3",
    "published_date": "2024-08-30 13:13:35 UTC",
    "updated_date": "2025-03-09 09:48:31 UTC"
  },
  {
    "arxiv_id": "2408.17253v3",
    "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters",
    "authors": [
      "Mouxiang Chen",
      "Lefei Shen",
      "Zhuo Li",
      "Xiaoyun Joy Wang",
      "Jianling Sun",
      "Chenghao Liu"
    ],
    "abstract": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either repurpose large language models\n(LLMs) or build large-scale time series datasets to develop TSF foundation\nmodels for universal forecasting. However, these methods face challenges due to\nthe severe cross-domain gap or in-domain heterogeneity. This paper explores a\nnew road to building a TSF foundation model from rich, high-quality natural\nimages. Our key insight is that a visual masked autoencoder, pre-trained on the\nImageNet dataset, can naturally be a numeric series forecaster. By\nreformulating TSF as an image reconstruction task, we bridge the gap between\nimage pre-training and TSF downstream tasks. Surprisingly, without further\nadaptation in the time series domain, the proposed VisionTS could achieve\nbetter zero-shot forecast performance than existing TSF foundation models. With\nfine-tuning for one epoch, VisionTS could further improve the forecasting and\nachieve state-of-the-art performance in most cases. Extensive experiments\nreveal intrinsic similarities between images and real-world time series,\nsuggesting that visual models may offer a \"free lunch\" for TSF and highlight\nthe potential for future cross-modality research. Our code is publicly\navailable at https://github.com/Keytoyze/VisionTS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "v3: add GIFT-EVAL results",
    "pdf_url": "http://arxiv.org/pdf/2408.17253v3",
    "published_date": "2024-08-30 12:51:55 UTC",
    "updated_date": "2025-02-04 04:37:48 UTC"
  },
  {
    "arxiv_id": "2408.17251v1",
    "title": "Abstracted Gaussian Prototypes for One-Shot Concept Learning",
    "authors": [
      "Chelsea Zou",
      "Kenneth J. Kurtz"
    ],
    "abstract": "We introduce a cluster-based generative image segmentation framework to\nencode higher-level representations of visual concepts based on one-shot\nlearning inspired by the Omniglot Challenge. The inferred parameters of each\ncomponent of a Gaussian Mixture Model (GMM) represent a distinct topological\nsubpart of a visual concept. Sampling new data from these parameters generates\naugmented subparts to build a more robust prototype for each concept, i.e., the\nAbstracted Gaussian Prototype (AGP). This framework addresses one-shot\nclassification tasks using a cognitively-inspired similarity metric and\naddresses one-shot generative tasks through a novel AGP-VAE pipeline employing\nvariational autoencoders (VAEs) to generate new class variants. Results from\nhuman judges reveal that the generative pipeline produces novel examples and\nclasses of visual concepts that are broadly indistinguishable from those made\nby humans. The proposed framework leads to impressive but not state-of-the-art\nclassification accuracy; thus, the contribution is two-fold: 1) the system is\nuniquely low in theoretical and computational complexity and operates in a\ncompletely standalone manner compared while existing approaches draw heavily on\npre-training or knowledge engineering; and 2) in contrast with competing neural\nnetwork models, the AGP approach addresses the importance of breadth of task\ncapability emphasized in the Omniglot challenge (i.e., successful performance\non generative tasks). These two points are critical as we advance toward an\nunderstanding of how learning/reasoning systems can produce viable, robust, and\nflexible concepts based on literally nothing more than a single example.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17251v1",
    "published_date": "2024-08-30 12:50:15 UTC",
    "updated_date": "2024-08-30 12:50:15 UTC"
  },
  {
    "arxiv_id": "2409.10525v1",
    "title": "\"Is This It?\": Towards Ecologically Valid Benchmarks for Situated Collaboration",
    "authors": [
      "Dan Bohus",
      "Sean Andrist",
      "Yuwei Bao",
      "Eric Horvitz",
      "Ann Paradiso"
    ],
    "abstract": "We report initial work towards constructing ecologically valid benchmarks to\nassess the capabilities of large multimodal models for engaging in situated\ncollaboration. In contrast to existing benchmarks, in which question-answer\npairs are generated post hoc over preexisting or synthetic datasets via\ntemplates, human annotators, or large language models (LLMs), we propose and\ninvestigate an interactive system-driven approach, where the questions are\ngenerated by users in context, during their interactions with an end-to-end\nsituated AI system. We illustrate how the questions that arise are different in\nform and content from questions typically found in existing embodied question\nanswering (EQA) benchmarks and discuss new real-world challenge problems\nbrought to the fore.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10525v1",
    "published_date": "2024-08-30 12:41:23 UTC",
    "updated_date": "2024-08-30 12:41:23 UTC"
  },
  {
    "arxiv_id": "2409.10524v1",
    "title": "3CSim: CARLA Corner Case Simulation for Control Assessment in Autonomous Driving",
    "authors": [
      "Matúš Čávojský",
      "Eugen Šlapak",
      "Matúš Dopiriak",
      "Gabriel Bugár",
      "Juraj Gazda"
    ],
    "abstract": "We present the CARLA corner case simulation (3CSim) for evaluating autonomous\ndriving (AD) systems within the CARLA simulator. This framework is designed to\naddress the limitations of traditional AD model training by focusing on\nnon-standard, rare, and cognitively challenging scenarios. These corner cases\nare crucial for ensuring vehicle safety and reliability, as they test advanced\ncontrol capabilities under unusual conditions. Our approach introduces a\ntaxonomy of corner cases categorized into state anomalies, behavior anomalies,\nand evidence-based anomalies. We implement 32 unique corner cases with\nadjustable parameters, including 9 predefined weather conditions, timing, and\ntraffic density. The framework enables repeatable and modifiable scenario\nevaluations, facilitating the creation of a comprehensive dataset for further\nanalysis.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10524v1",
    "published_date": "2024-08-30 12:38:22 UTC",
    "updated_date": "2024-08-30 12:38:22 UTC"
  },
  {
    "arxiv_id": "2408.17235v2",
    "title": "AI-Driven Intrusion Detection Systems (IDS) on the ROAD Dataset: A Comparative Analysis for Automotive Controller Area Network (CAN)",
    "authors": [
      "Lorenzo Guerra",
      "Linhan Xu",
      "Paolo Bellavista",
      "Thomas Chapuis",
      "Guillaume Duc",
      "Pavlo Mozharovskyi",
      "Van-Tam Nguyen"
    ],
    "abstract": "The integration of digital devices in modern vehicles has revolutionized\nautomotive technology, enhancing safety and the overall driving experience. The\nController Area Network (CAN) bus is a central system for managing in-vehicle\ncommunication between the electronic control units (ECUs). However, the CAN\nprotocol poses security challenges due to inherent vulnerabilities, lacking\nencryption and authentication, which, combined with an expanding attack\nsurface, necessitates robust security measures. In response to this challenge,\nnumerous Intrusion Detection Systems (IDS) have been developed and deployed.\nNonetheless, an open, comprehensive, and realistic dataset to test the\neffectiveness of such IDSs remains absent in the existing literature. This\npaper addresses this gap by considering the latest ROAD dataset, containing\nstealthy and sophisticated injections. The methodology involves dataset\nlabelling and the implementation of both state-of-the-art deep learning models\nand traditional machine learning models to show the discrepancy in performance\nbetween the datasets most commonly used in the literature and the ROAD dataset,\na more realistic alternative.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17235v2",
    "published_date": "2024-08-30 12:26:23 UTC",
    "updated_date": "2024-09-05 13:59:21 UTC"
  },
  {
    "arxiv_id": "2408.17233v1",
    "title": "A methodological framework for Resilience as a Service (RaaS) in multimodal urban transportation networks",
    "authors": [
      "Sara Jaber",
      "Mostafa Ameli",
      "S. M. Hassan Mahdavi",
      "Neila Bhouri"
    ],
    "abstract": "Public transportation systems are experiencing an increase in commuter\ntraffic. This increase underscores the need for resilience strategies to manage\nunexpected service disruptions, ensuring rapid and effective responses that\nminimize adverse effects on stakeholders and enhance the system's ability to\nmaintain essential functions and recover quickly. This study aims to explore\nthe management of public transport disruptions through resilience as a service\n(RaaS) strategies, developing an optimization model to effectively allocate\nresources and minimize the cost for operators and passengers. The proposed\nmodel includes multiple transportation options, such as buses, taxis, and\nautomated vans, and evaluates them as bridging alternatives to rail-disrupted\nservices based on factors such as their availability, capacity, speed, and\nproximity to the disrupted station. This ensures that the most suitable\nvehicles are deployed to maintain service continuity. Applied to a case study\nin the Ile de France region, Paris and suburbs, complemented by a microscopic\nsimulation, the model is compared to existing solutions such as bus bridging\nand reserve fleets. The results highlight the model's performance in minimizing\ncosts and enhancing stakeholder satisfaction, optimizing transport management\nduring disruptions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17233v1",
    "published_date": "2024-08-30 12:22:34 UTC",
    "updated_date": "2024-08-30 12:22:34 UTC"
  },
  {
    "arxiv_id": "2409.00151v1",
    "title": "Speaker Tagging Correction With Non-Autoregressive Language Models",
    "authors": [
      "Grigor Kirakosyan",
      "Davit Karamyan"
    ],
    "abstract": "Speech applications dealing with conversations require not only recognizing\nthe spoken words but also determining who spoke when. The task of assigning\nwords to speakers is typically addressed by merging the outputs of two separate\nsystems, namely, an automatic speech recognition (ASR) system and a speaker\ndiarization (SD) system. In practical settings, speaker diarization systems can\nexperience significant degradation in performance due to a variety of factors,\nincluding uniform segmentation with a high temporal resolution, inaccurate word\ntimestamps, incorrect clustering and estimation of speaker numbers, as well as\nbackground noise.\n  Therefore, it is important to automatically detect errors and make\ncorrections if possible. We used a second-pass speaker tagging correction\nsystem based on a non-autoregressive language model to correct mistakes in\nwords placed at the borders of sentences spoken by different speakers. We first\nshow that the employed error correction approach leads to reductions in word\ndiarization error rate (WDER) on two datasets: TAL and test set of Fisher.\nAdditionally, we evaluated our system in the Post-ASR Speaker Tagging\nCorrection challenge and observed significant improvements in cpWER compared to\nbaseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "68T01 General topics in artificial intelligence"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.00151v1",
    "published_date": "2024-08-30 11:02:17 UTC",
    "updated_date": "2024-08-30 11:02:17 UTC"
  },
  {
    "arxiv_id": "2408.17198v2",
    "title": "Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features",
    "authors": [
      "Thomas Schnake",
      "Farnoush Rezaei Jafari",
      "Jonas Lederer",
      "Ping Xiong",
      "Shinichi Nakajima",
      "Stefan Gugler",
      "Grégoire Montavon",
      "Klaus-Robert Müller"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) plays a crucial role in fostering\ntransparency and trust in AI systems, where traditional XAI approaches\ntypically offer one level of abstraction for explanations, often in the form of\nheatmaps highlighting single or multiple input features. However, we ask\nwhether abstract reasoning or problem-solving strategies of a model may also be\nrelevant, as these align more closely with how humans approach solutions to\nproblems. We propose a framework, called Symbolic XAI, that attributes\nrelevance to symbolic queries expressing logical relationships between input\nfeatures, thereby capturing the abstract reasoning behind a model's\npredictions. The methodology is built upon a simple yet general multi-order\ndecomposition of model predictions. This decomposition can be specified using\nhigher-order propagation-based relevance methods, such as GNN-LRP, or\nperturbation-based explanation methods commonly used in XAI. The effectiveness\nof our framework is demonstrated in the domains of natural language processing\n(NLP), vision, and quantum chemistry (QC), where abstract symbolic domain\nknowledge is abundant and of significant interest to users. The Symbolic XAI\nframework provides an understanding of the model's decision-making process that\nis both flexible for customization by the user and human-readable through\nlogical formulas.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17198v2",
    "published_date": "2024-08-30 10:52:18 UTC",
    "updated_date": "2024-10-01 11:35:49 UTC"
  },
  {
    "arxiv_id": "2408.17190v1",
    "title": "Reasoning with maximal consistent signatures",
    "authors": [
      "Matthias Thimm",
      "Jandson Santos Ribeiro Santos"
    ],
    "abstract": "We analyse a specific instance of the general approach of reasoning based on\nforgetting by Lang and Marquis. More precisely, we discuss an approach for\nreasoning with inconsistent information using maximal consistent subsignatures,\nwhere a maximal consistent subsignature is a maximal set of propositions such\nthat forgetting the remaining propositions restores consistency. We analyse\nmaximal consistent subsignatures and the corresponding minimal inconsistent\nsubsignatures in-depth and show, among others, that the hitting set duality\napplies for them as well. We further analyse inference relations based on\nmaximal consistent subsignatures wrt. rationality postulates from non-monotonic\nreasoning and computational complexity. We also consider the relationship of\nour approach with inconsistency measurement and paraconsistent reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17190v1",
    "published_date": "2024-08-30 10:43:14 UTC",
    "updated_date": "2024-08-30 10:43:14 UTC"
  },
  {
    "arxiv_id": "2408.17186v1",
    "title": "\"Benefit Game: Alien Seaweed Swarms\" -- Real-time Gamification of Digital Seaweed Ecology",
    "authors": [
      "Dan-Lu Fei",
      "Zi-Wei Wu",
      "Kang Zhang"
    ],
    "abstract": "\"Benefit Game: Alien Seaweed Swarms\" combines artificial life art and\ninteractive game with installation to explore the impact of human activity on\nfragile seaweed ecosystems. The project aims to promote ecological\nconsciousness by creating a balance in digital seaweed ecologies. Inspired by\nthe real species \"Laminaria saccharina\", the author employs Procedural Content\nGeneration via Machine Learning technology to generate variations of virtual\nseaweeds and symbiotic fungi. The audience can explore the consequences of\nhuman activities through gameplay and observe the ecosystem's feedback on the\nbenefits and risks of seaweed aquaculture. This Benefit Game offers dynamic and\nreal-time responsive artificial seaweed ecosystems for an interactive\nexperience that enhances ecological consciousness.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.HC",
    "comment": "Paper accepted at ISEA 24, The 29th International Symposium on\n  Electronic Art, Brisbane, Australia, 21-29 June 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.17186v1",
    "published_date": "2024-08-30 10:36:11 UTC",
    "updated_date": "2024-08-30 10:36:11 UTC"
  },
  {
    "arxiv_id": "2408.17183v2",
    "title": "Causal Reasoning in Software Quality Assurance: A Systematic Review",
    "authors": [
      "Luca Giamattei",
      "Antonio Guerriero",
      "Roberto Pietrantuono",
      "Stefano Russo"
    ],
    "abstract": "Context: Software Quality Assurance (SQA) is a fundamental part of software\nengineering to ensure stakeholders that software products work as expected\nafter release in operation. Machine Learning (ML) has proven to be able to\nboost SQA activities and contribute to the development of quality software\nsystems. In this context, Causal Reasoning is gaining increasing interest as a\nmethodology to go beyond a purely data-driven approach by exploiting the use of\ncausality for more effective SQA strategies. Objective: Provide a broad and\ndetailed overview of the use of causal reasoning for SQA activities, in order\nto support researchers to access this research field, identifying room for\napplication, main challenges and research opportunities. Methods: A systematic\nreview of the scientific literature on causal reasoning for SQA. The study has\nfound, classified, and analyzed 86 articles, according to established\nguidelines for software engineering secondary studies. Results: Results\nhighlight the primary areas within SQA where causal reasoning has been applied,\nthe predominant methodologies used, and the level of maturity of the proposed\nsolutions. Fault localization is the activity where causal reasoning is more\nexploited, especially in the web services/microservices domain, but other tasks\nlike testing are rapidly gaining popularity. Both causal inference and causal\ndiscovery are exploited, with the Pearl's graphical formulation of causality\nbeing preferred, likely due to its intuitiveness. Tools to favour their\napplication are appearing at a fast pace - most of them after 2021.\nConclusions: The findings show that causal reasoning is a valuable means for\nSQA tasks with respect to multiple quality attributes, especially during V&V,\nevolution and maintenance to ensure reliability, while it is not yet fully\nexploited for phases like ...",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to Information and Software Technology journal",
    "pdf_url": "http://arxiv.org/pdf/2408.17183v2",
    "published_date": "2024-08-30 10:34:11 UTC",
    "updated_date": "2024-10-10 11:00:02 UTC"
  },
  {
    "arxiv_id": "2409.00149v1",
    "title": "From Semantics to Hierarchy: A Hybrid Euclidean-Tangent-Hyperbolic Space Model for Temporal Knowledge Graph Reasoning",
    "authors": [
      "Siling Feng",
      "Zhisheng Qi",
      "Cong Lin"
    ],
    "abstract": "Temporal knowledge graph (TKG) reasoning predicts future events based on\nhistorical data, but it's challenging due to the complex semantic and\nhierarchical information involved. Existing Euclidean models excel at capturing\nsemantics but struggle with hierarchy. Conversely, hyperbolic models manage\nhierarchical features well but fail to represent complex semantics due to\nlimitations in shallow models' parameters and the absence of proper\nnormalization in deep models relying on the L2 norm. Current solutions, as\ncurvature transformations, are insufficient to address these issues. In this\nwork, a novel hybrid geometric space approach that leverages the strengths of\nboth Euclidean and hyperbolic models is proposed. Our approach transitions from\nsingle-space to multi-space parameter modeling, effectively capturing both\nsemantic and hierarchical information. Initially, complex semantics are\ncaptured through a fact co-occurrence and autoregressive method with\nnormalizations in Euclidean space. The embeddings are then transformed into\nTangent space using a scaling mechanism, preserving semantic information while\nrelearning hierarchical structures through a query-candidate separated modeling\napproach, which are subsequently transformed into Hyperbolic space. Finally, a\nhybrid inductive bias for hierarchical and semantic learning is achieved by\ncombining hyperbolic and Euclidean scoring functions through a learnable\nquery-specific mixing coefficient, utilizing embeddings from hyperbolic and\nEuclidean spaces. Experimental results on four TKG benchmarks demonstrate that\nour method reduces error relatively by up to 15.0% in mean reciprocal rank on\nYAGO compared to previous single-space models. Additionally, enriched\nvisualization analysis validates the effectiveness of our approach, showing\nadaptive capabilities for datasets with varying levels of semantic and\nhierarchical complexity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00149v1",
    "published_date": "2024-08-30 10:33:08 UTC",
    "updated_date": "2024-08-30 10:33:08 UTC"
  },
  {
    "arxiv_id": "2408.17180v1",
    "title": "Identifying and Clustering Counter Relationships of Team Compositions in PvP Games for Efficient Balance Analysis",
    "authors": [
      "Chiu-Chou Lin",
      "Yu-Wei Shih",
      "Kuei-Ting Kuo",
      "Yu-Cheng Chen",
      "Chien-Hua Chen",
      "Wei-Chen Chiu",
      "I-Chen Wu"
    ],
    "abstract": "How can balance be quantified in game settings? This question is crucial for\ngame designers, especially in player-versus-player (PvP) games, where analyzing\nthe strength relations among predefined team compositions-such as hero\ncombinations in multiplayer online battle arena (MOBA) games or decks in card\ngames-is essential for enhancing gameplay and achieving balance. We have\ndeveloped two advanced measures that extend beyond the simplistic win rate to\nquantify balance in zero-sum competitive scenarios. These measures are derived\nfrom win value estimations, which employ strength rating approximations via the\nBradley-Terry model and counter relationship approximations via vector\nquantization, significantly reducing the computational complexity associated\nwith traditional win value estimations. Throughout the learning process of\nthese models, we identify useful categories of compositions and pinpoint their\ncounter relationships, aligning with the experiences of human players without\nrequiring specific game knowledge. Our methodology hinges on a simple technique\nto enhance codebook utilization in discrete representation with a deterministic\nvector quantization process for an extremely small state space. Our framework\nhas been validated in popular online games, including Age of Empires II,\nHearthstone, Brawl Stars, and League of Legends. The accuracy of the observed\nstrength relations in these games is comparable to traditional pairwise win\nvalue predictions, while also offering a more manageable complexity for\nanalysis. Ultimately, our findings contribute to a deeper understanding of PvP\ngame dynamics and present a methodology that significantly improves game\nbalance evaluation and design.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.IR",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "TMLR 09/2024 https://openreview.net/forum?id=2D36otXvBE",
    "pdf_url": "http://arxiv.org/pdf/2408.17180v1",
    "published_date": "2024-08-30 10:28:36 UTC",
    "updated_date": "2024-08-30 10:28:36 UTC"
  },
  {
    "arxiv_id": "2408.17175v3",
    "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
    "authors": [
      "Zhen Ye",
      "Peiwen Sun",
      "Jiahe Lei",
      "Hongzhan Lin",
      "Xu Tan",
      "Zheqi Dai",
      "Qiuqiang Kong",
      "Jianyi Chen",
      "Jiahao Pan",
      "Qifeng Liu",
      "Yike Guo",
      "Wei Xue"
    ],
    "abstract": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17175v3",
    "published_date": "2024-08-30 10:24:07 UTC",
    "updated_date": "2024-11-27 11:47:45 UTC"
  },
  {
    "arxiv_id": "2408.17162v1",
    "title": "Deep Feature Embedding for Tabular Data",
    "authors": [
      "Yuqian Wu",
      "Hengyi Luo",
      "Raymond S. T. Lee"
    ],
    "abstract": "Tabular data learning has extensive applications in deep learning but its\nexisting embedding techniques are limited in numerical and categorical features\nsuch as the inability to capture complex relationships and engineering. This\npaper proposes a novel deep embedding framework with leverages lightweight deep\nneural networks to generate effective feature embeddings for tabular data in\nmachine learning research. For numerical features, a two-step feature expansion\nand deep transformation technique is used to capture copious semantic\ninformation. For categorical features, a unique identification vector for each\nentity is referred by a compact lookup table with a parameterized deep\nembedding function to uniform the embedding size dimensions, and transformed\ninto a embedding vector using deep neural network. Experiments are conducted on\nreal-world datasets for performance evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 2figures, accepted to ICONIP 2024, Paper ID: 1399",
    "pdf_url": "http://arxiv.org/pdf/2408.17162v1",
    "published_date": "2024-08-30 10:05:24 UTC",
    "updated_date": "2024-08-30 10:05:24 UTC"
  },
  {
    "arxiv_id": "2408.17150v1",
    "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning",
    "authors": [
      "Xiaoye Qu",
      "Jiashuo Sun",
      "Wei Wei",
      "Yu Cheng"
    ],
    "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal context comprehension. However, they still suffer\nfrom hallucination problems referring to generating inconsistent outputs with\nthe image content. To mitigate hallucinations, previous studies mainly focus on\nretraining LVLMs with custom datasets. Although effective, they inherently come\nwith additional computational costs. In this paper, we propose a training-free\nframework, \\textbf{MVP}, that aims to reduce hallucinations by making the most\nof the innate capabilities of the LVLMs via \\textbf{M}ulti-\\textbf{V}iew\nMulti-\\textbf{P}ath Reasoning. Specifically, we first devise a multi-view\ninformation-seeking strategy to thoroughly perceive the comprehensive\ninformation in the image, which enriches the general global information\ncaptured by the original vision encoder in LVLMs. Furthermore, during the\nanswer decoding, we observe that the occurrence of hallucinations has a strong\ncorrelation with the certainty of the answer tokens. Thus, we propose\nmulti-path reasoning for each information view to quantify and aggregate the\ncertainty scores for each potential answer among multiple decoding paths and\nfinally decide the output answer. By fully grasping the information in the\nimage and carefully considering the certainty of the potential answers when\ndecoding, our MVP can effectively reduce hallucinations in LVLMs.The extensive\nexperiments verify that our proposed MVP significantly mitigates the\nhallucination problem across four well-known LVLMs. The source code is\navailable at: \\url{https://github.com/GasolSun36/MVP}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 7 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17150v1",
    "published_date": "2024-08-30 09:40:10 UTC",
    "updated_date": "2024-08-30 09:40:10 UTC"
  },
  {
    "arxiv_id": "2408.17145v1",
    "title": "Towards Hyper-parameter-free Federated Learning",
    "authors": [
      "Geetika",
      "Drishya Uniyal",
      "Bapi Chatterjee"
    ],
    "abstract": "The adaptive synchronization techniques in federated learning (FL) for scaled\nglobal model updates show superior performance over the vanilla federated\naveraging (FedAvg) scheme. However, existing methods employ additional tunable\nhyperparameters on the server to determine the scaling factor. A contrasting\napproach is automated scaling analogous to tuning-free step-size schemes in\nstochastic gradient descent (SGD) methods, which offer competitive convergence\nrates and exhibit good empirical performance. In this work, we introduce two\nalgorithms for automated scaling of global model updates. In our first\nalgorithm, we establish that a descent-ensuring step-size regime at the clients\nensures descent for the server objective. We show that such a scheme enables\nlinear convergence for strongly convex federated objectives. Our second\nalgorithm shows that the average of objective values of sampled clients is a\npractical and effective substitute for the objective function value at the\nserver required for computing the scaling factor, whose computation is\notherwise not permitted. Our extensive empirical results show that the proposed\nmethods perform at par or better than the popular federated learning algorithms\nfor both convex and non-convex problems. Our work takes a step towards\ndesigning hyper-parameter-free federated learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17145v1",
    "published_date": "2024-08-30 09:35:36 UTC",
    "updated_date": "2024-08-30 09:35:36 UTC"
  },
  {
    "arxiv_id": "2408.17136v2",
    "title": "Leveraging Digital Twin Technologies for Public Space Protection and Vulnerability Assessment",
    "authors": [
      "Artemis Stefanidou",
      "Jorgen Cani",
      "Thomas Papadopoulos",
      "Panagiotis Radoglou-Grammatikis",
      "Panagiotis Sarigiannidis",
      "Iraklis Varlamis",
      "Georgios Th. Papadopoulos"
    ],
    "abstract": "Over the recent years, the protection of the so-called `soft-targets', i.e.\nlocations easily accessible by the general public with relatively low, though,\nsecurity measures, has emerged as a rather challenging and increasingly\nimportant issue. The complexity and seriousness of this security threat growths\nnowadays exponentially, due to the emergence of new advanced technologies (e.g.\nArtificial Intelligence (AI), Autonomous Vehicles (AVs), 3D printing, etc.);\nespecially when it comes to large-scale, popular and diverse public spaces. In\nthis paper, a novel Digital Twin-as-a-Security-Service (DTaaSS) architecture is\nintroduced for holistically and significantly enhancing the protection of\npublic spaces (e.g. metro stations, leisure sites, urban squares, etc.). The\nproposed framework combines a Digital Twin (DT) conceptualization with\nadditional cutting-edge technologies, including Internet of Things (IoT), cloud\ncomputing, Big Data analytics and AI. In particular, DTaaSS comprises a\nholistic, real-time, large-scale, comprehensive and data-driven security\nsolution for the efficient/robust protection of public spaces, supporting: a)\ndata collection and analytics, b) area monitoring/control and proactive threat\ndetection, c) incident/attack prediction, and d) quantitative and data-driven\nvulnerability assessment. Overall, the designed architecture exhibits increased\npotential in handling complex, hybrid and combined threats over large, critical\nand popular soft-targets. The applicability and robustness of DTaaSS is\ndiscussed in detail against representative and diverse real-world application\nscenarios, including complex attacks to: a) a metro station, b) a leisure site,\nand c) a cathedral square.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17136v2",
    "published_date": "2024-08-30 09:27:12 UTC",
    "updated_date": "2024-12-15 11:48:23 UTC"
  },
  {
    "arxiv_id": "2408.17131v1",
    "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
    "authors": [
      "Juncan Deng",
      "Shuaiting Li",
      "Zeyu Wang",
      "Hong Gu",
      "Kedong Xu",
      "Kejie Huang"
    ],
    "abstract": "The Diffusion Transformers Models (DiTs) have transitioned the network\narchitecture from traditional UNets to transformers, demonstrating exceptional\ncapabilities in image generation. Although DiTs have been widely applied to\nhigh-definition video generation tasks, their large parameter size hinders\ninference on edge devices. Vector quantization (VQ) can decompose model weight\ninto a codebook and assignments, allowing extreme weight quantization and\nsignificantly reducing memory usage. In this paper, we propose VQ4DiT, a fast\npost-training vector quantization method for DiTs. We found that traditional VQ\nmethods calibrate only the codebook without calibrating the assignments. This\nleads to weight sub-vectors being incorrectly assigned to the same assignment,\nproviding inconsistent gradients to the codebook and resulting in a suboptimal\nresult. To address this challenge, VQ4DiT calculates the candidate assignment\nset for each weight sub-vector based on Euclidean distance and reconstructs the\nsub-vector based on the weighted average. Then, using the zero-data and\nblock-wise calibration method, the optimal assignment from the set is\nefficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT\nXL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending\non the different quantization settings. Experiments show that VQ4DiT\nestablishes a new state-of-the-art in model size and performance trade-offs,\nquantizing weights to 2-bit precision while retaining acceptable image\ngeneration quality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2; I.4"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17131v1",
    "published_date": "2024-08-30 09:15:54 UTC",
    "updated_date": "2024-08-30 09:15:54 UTC"
  },
  {
    "arxiv_id": "2408.17129v2",
    "title": "Controllable Edge-Type-Specific Interpretation in Multi-Relational Graph Neural Networks for Drug Response Prediction",
    "authors": [
      "Xiaodi Li",
      "Jianfeng Gui",
      "Qian Gao",
      "Haoyuan Shi",
      "Zhenyu Yue"
    ],
    "abstract": "Graph Neural Networks have been widely applied in critical decision-making\nareas that demand interpretable predictions, leading to the flourishing\ndevelopment of interpretability algorithms. However, current graph\ninterpretability algorithms tend to emphasize generality and often overlook\nbiological significance, thereby limiting their applicability in predicting\ncancer drug responses. In this paper, we propose a novel post-hoc\ninterpretability algorithm for cancer drug response prediction, CETExplainer,\nwhich incorporates a controllable edge-type-specific weighting mechanism. It\nconsiders the mutual information between subgraphs and predictions, proposing a\nstructural scoring approach to provide fine-grained, biologically meaningful\nexplanations for predictive models. We also introduce a method for constructing\nground truth based on real-world datasets to quantitatively evaluate the\nproposed interpretability algorithm. Empirical analysis on the real-world\ndataset demonstrates that CETExplainer achieves superior stability and improves\nexplanation quality compared to leading algorithms, thereby offering a robust\nand insightful tool for cancer drug prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17129v2",
    "published_date": "2024-08-30 09:14:38 UTC",
    "updated_date": "2024-09-03 08:45:37 UTC"
  },
  {
    "arxiv_id": "2409.10523v1",
    "title": "Harnessing Artificial Intelligence for Wildlife Conservation",
    "authors": [
      "Paul Fergus",
      "Carl Chalmers",
      "Steve Longmore",
      "Serge Wich"
    ],
    "abstract": "The rapid decline in global biodiversity demands innovative conservation\nstrategies. This paper examines the use of artificial intelligence (AI) in\nwildlife conservation, focusing on the Conservation AI platform. Leveraging\nmachine learning and computer vision, Conservation AI detects and classifies\nanimals, humans, and poaching-related objects using visual spectrum and thermal\ninfrared cameras. The platform processes this data with convolutional neural\nnetworks (CNNs) and Transformer architectures to monitor species, including\nthose which are critically endangered. Real-time detection provides the\nimmediate responses required for time-critical situations (e.g. poaching),\nwhile non-real-time analysis supports long-term wildlife monitoring and habitat\nhealth assessment. Case studies from Europe, North America, Africa, and\nSoutheast Asia highlight the platform's success in species identification,\nbiodiversity monitoring, and poaching prevention. The paper also discusses\nchallenges related to data quality, model accuracy, and logistical constraints,\nwhile outlining future directions involving technological advancements,\nexpansion into new geographical regions, and deeper collaboration with local\ncommunities and policymakers. Conservation AI represents a significant step\nforward in addressing the urgent challenges of wildlife conservation, offering\na scalable and adaptable solution that can be implemented globally.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.10523v1",
    "published_date": "2024-08-30 09:13:31 UTC",
    "updated_date": "2024-08-30 09:13:31 UTC"
  },
  {
    "arxiv_id": "2409.10522v1",
    "title": "Bridging User Dynamics: Transforming Sequential Recommendations with Schrödinger Bridge and Diffusion Models",
    "authors": [
      "Wenjia Xie",
      "Rui Zhou",
      "Hao Wang",
      "Tingjia Shen",
      "Enhong Chen"
    ],
    "abstract": "Sequential recommendation has attracted increasing attention due to its\nability to accurately capture the dynamic changes in user interests. We have\nnoticed that generative models, especially diffusion models, which have\nachieved significant results in fields like image and audio, hold considerable\npromise in the field of sequential recommendation. However, existing sequential\nrecommendation methods based on diffusion models are constrained by a prior\ndistribution limited to Gaussian distribution, hindering the possibility of\nintroducing user-specific information for each recommendation and leading to\ninformation loss. To address these issues, we introduce the Schr\\\"odinger\nBridge into diffusion-based sequential recommendation models, creating the\nSdifRec model. This allows us to replace the Gaussian prior of the diffusion\nmodel with the user's current state, directly modeling the process from a\nuser's current state to the target recommendation. Additionally, to better\nutilize collaborative information in recommendations, we propose an extended\nversion of SdifRec called con-SdifRec, which utilizes user clustering\ninformation as a guiding condition to further enhance the posterior\ndistribution. Finally, extensive experiments on multiple public benchmark\ndatasets have demonstrated the effectiveness of SdifRec and con-SdifRec through\ncomparison with several state-of-the-art methods. Further in-depth analysis has\nvalidated their efficiency and robustness.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "CIKM '24",
    "pdf_url": "http://arxiv.org/pdf/2409.10522v1",
    "published_date": "2024-08-30 09:10:38 UTC",
    "updated_date": "2024-08-30 09:10:38 UTC"
  },
  {
    "arxiv_id": "2408.17119v1",
    "title": "Exploring User Acceptance Of Portable Intelligent Personal Assistants: A Hybrid Approach Using PLS-SEM And fsQCA",
    "authors": [
      "Gustave Florentin Nkoulou Mvondo",
      "Ben Niu"
    ],
    "abstract": "This research explores the factors driving user acceptance of Rabbit R1, a\nnewly developed portable intelligent personal assistant (PIPA) that aims to\nredefine user interaction and control. The study extends the technology\nacceptance model (TAM) by incorporating artificial intelligence-specific\nfactors (conversational intelligence, task intelligence, and perceived\nnaturalness), user interface design factors (simplicity in information design\nand visual aesthetics), and user acceptance and loyalty. Using a purposive\nsampling method, we gathered data from 824 users in the US and analyzed the\nsample through partial least squares structural equation modeling (PLS-SEM) and\nfuzzy set qualitative comparative analysis (fsQCA). The findings reveal that\nall hypothesized relationships, including both direct and indirect effects, are\nsupported. Additionally, fsQCA supports the PLS-SEM findings and identifies\nthree configurations leading to high and low user acceptance. This research\nenriches the literature and provides valuable insights for system designers and\nmarketers of PIPAs, guiding strategic decisions to foster widespread adoption\nand long-term engagement.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "HCC"
    ],
    "primary_category": "cs.HC",
    "comment": "36,",
    "pdf_url": "http://arxiv.org/pdf/2408.17119v1",
    "published_date": "2024-08-30 09:01:34 UTC",
    "updated_date": "2024-08-30 09:01:34 UTC"
  },
  {
    "arxiv_id": "2408.17103v1",
    "title": "Understanding the User: An Intent-Based Ranking Dataset",
    "authors": [
      "Abhijit Anand",
      "Jurek Leonhardt",
      "V Venktesh",
      "Avishek Anand"
    ],
    "abstract": "As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17103v1",
    "published_date": "2024-08-30 08:40:59 UTC",
    "updated_date": "2024-08-30 08:40:59 UTC"
  },
  {
    "arxiv_id": "2408.17101v1",
    "title": "Strategic Arms with Side Communication Prevail Over Low-Regret MAB Algorithms",
    "authors": [
      "Ahmed Ben Yahmed",
      "Clément Calauzènes",
      "Vianney Perchet"
    ],
    "abstract": "In the strategic multi-armed bandit setting, when arms possess perfect\ninformation about the player's behavior, they can establish an equilibrium\nwhere: 1. they retain almost all of their value, 2. they leave the player with\na substantial (linear) regret. This study illustrates that, even if complete\ninformation is not publicly available to all arms but is shared among them, it\nis possible to achieve a similar equilibrium. The primary challenge lies in\ndesigning a communication protocol that incentivizes the arms to communicate\ntruthfully.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17101v1",
    "published_date": "2024-08-30 08:36:45 UTC",
    "updated_date": "2024-08-30 08:36:45 UTC"
  },
  {
    "arxiv_id": "2409.10521v1",
    "title": "LSTM Recurrent Neural Networks for Cybersecurity Named Entity Recognition",
    "authors": [
      "Houssem Gasmi",
      "Jannik Laval",
      "Abdelaziz Bouras"
    ],
    "abstract": "The automated and timely conversion of cybersecurity information from\nunstructured online sources, such as blogs and articles to more formal\nrepresentations has become a necessity for many applications in the domain\nnowadays. Named Entity Recognition (NER) is one of the early phases towards\nthis goal. It involves the detection of the relevant domain entities, such as\nproduct, version, attack name, etc. in technical documents. Although generally\nconsidered a simple task in the information extraction field, it is quite\nchallenging in some domains like cybersecurity because of the complex structure\nof its entities. The state of the art methods require time-consuming and labor\nintensive feature engineering that describes the properties of the entities,\ntheir context, domain knowledge, and linguistic characteristics. The model\ndemonstrated in this paper is domain independent and does not rely on any\nfeatures specific to the entities in the cybersecurity domain, hence does not\nrequire expert knowledge to perform feature engineering. The method used relies\non a type of recurrent neural networks called Long Short-Term Memory (LSTM) and\nthe Conditional Random Fields (CRFs) method. The results we obtained showed\nthat this method outperforms the state of the art methods given an annotated\ncorpus of a decent size.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10521v1",
    "published_date": "2024-08-30 08:35:48 UTC",
    "updated_date": "2024-08-30 08:35:48 UTC"
  },
  {
    "arxiv_id": "2408.17090v2",
    "title": "FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition",
    "authors": [
      "Chen Hu",
      "Hanchi Ren",
      "Jingjing Deng",
      "Xianghua Xie",
      "Xiaoke Ma"
    ],
    "abstract": "Federated learning is a machine learning paradigm that enables decentralized\nclients to collaboratively learn a shared model while keeping all the training\ndata local. While considerable research has focused on federated image\ngeneration, particularly Generative Adversarial Networks, Variational\nAutoencoders have received less attention. In this paper, we address the\nchallenges of non-IID (independently and identically distributed) data\nenvironments featuring multiple groups of images of different types. Non-IID\ndata distributions can lead to difficulties in maintaining a consistent latent\nspace and can also result in local generators with disparate texture features\nbeing blended during aggregation. We thereby introduce FissionVAE that\ndecouples the latent space and constructs decoder branches tailored to\nindividual client groups. This method allows for customized learning that\naligns with the unique data distributions of each group. Additionally, we\nincorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder\narchitectures within FissionVAE. We also explore strategies for setting the\nlatent prior distributions to enhance the decoupling process. To evaluate our\napproach, we assemble two composite datasets: the first combines MNIST and\nFashionMNIST; the second comprises RGB datasets of cartoon and human faces,\nwild animals, marine vessels, and remote sensing images. Our experiments\ndemonstrate that FissionVAE greatly improves generation quality on these\ndatasets compared to baseline federated VAE models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.17090v2",
    "published_date": "2024-08-30 08:22:30 UTC",
    "updated_date": "2025-05-05 13:51:42 UTC"
  },
  {
    "arxiv_id": "2408.17064v3",
    "title": "Instant Adversarial Purification with Adversarial Consistency Distillation",
    "authors": [
      "Chun Tong Lei",
      "Hon Ming Yam",
      "Zhongliang Guo",
      "Yifei Qian",
      "Chun Pong Lau"
    ],
    "abstract": "Neural networks have revolutionized numerous fields with their exceptional\nperformance, yet they remain susceptible to adversarial attacks through subtle\nperturbations. While diffusion-based purification methods like DiffPure offer\npromising defense mechanisms, their computational overhead presents a\nsignificant practical limitation. In this paper, we introduce One Step Control\nPurification (OSCP), a novel defense framework that achieves robust adversarial\npurification in a single Neural Function Evaluation (NFE) within diffusion\nmodels. We propose Gaussian Adversarial Noise Distillation (GAND) as the\ndistillation objective and Controlled Adversarial Purification (CAP) as the\ninference pipeline, which makes OSCP demonstrate remarkable efficiency while\nmaintaining defense efficacy. Our proposed GAND addresses a fundamental tension\nbetween consistency distillation and adversarial perturbation, bridging the gap\nbetween natural and adversarial manifolds in the latent space, while remaining\ncomputationally efficient through Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA, eliminating the high computational budget request from\nfull parameter fine-tuning. The CAP guides the purification process through the\nunlearnable edge detection operator calculated by the input image as an extra\nprompt, effectively preventing the purified images from deviating from their\noriginal appearance when large purification steps are used. Our experimental\nresults on ImageNet showcase OSCP's superior performance, achieving a 74.19%\ndefense success rate with merely 0.1s per purification -- a 100-fold speedup\ncompared to conventional approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2408.17064v3",
    "published_date": "2024-08-30 07:49:35 UTC",
    "updated_date": "2025-03-21 13:58:47 UTC"
  },
  {
    "arxiv_id": "2408.17059v4",
    "title": "A Survey of the Self Supervised Learning Mechanisms for Vision Transformers",
    "authors": [
      "Asifullah Khan",
      "Anabia Sohail",
      "Mustansar Fiaz",
      "Mehdi Hassan",
      "Tariq Habib Afridi",
      "Sibghat Ullah Marwat",
      "Farzeen Munir",
      "Safdar Ali",
      "Hannan Naseem",
      "Muhammad Zaigham Zaheer",
      "Kamran Ali",
      "Tangina Sultana",
      "Ziaurrehman Tanoli",
      "Naeem Akhter"
    ],
    "abstract": "Deep supervised learning models require high volume of labeled data to attain\nsufficiently good results. Although, the practice of gathering and annotating\nsuch big data is costly and laborious. Recently, the application of self\nsupervised learning (SSL) in vision tasks has gained significant attention. The\nintuition behind SSL is to exploit the synchronous relationships within the\ndata as a form of self-supervision, which can be versatile. In the current big\ndata era, most of the data is unlabeled, and the success of SSL thus relies in\nfinding ways to utilize this vast amount of unlabeled data available. Thus it\nis better for deep learning algorithms to reduce reliance on human supervision\nand instead focus on self-supervision based on the inherent relationships\nwithin the data. With the advent of ViTs, which have achieved remarkable\nresults in computer vision, it is crucial to explore and understand the various\nSSL mechanisms employed for training these models specifically in scenarios\nwhere there is limited labelled data available. In this survey, we develop a\ncomprehensive taxonomy of systematically classifying the SSL techniques based\nupon their representations and pre-training tasks being applied. Additionally,\nwe discuss the motivations behind SSL, review popular pre-training tasks, and\nhighlight the challenges and advancements in this field. Furthermore, we\npresent a comparative analysis of different SSL methods, evaluate their\nstrengths and limitations, and identify potential avenues for future research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "34 Pages, 5 Figures, 7 Tables",
    "pdf_url": "http://arxiv.org/pdf/2408.17059v4",
    "published_date": "2024-08-30 07:38:28 UTC",
    "updated_date": "2025-03-20 04:10:51 UTC"
  },
  {
    "arxiv_id": "2409.00147v1",
    "title": "MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models",
    "authors": [
      "Shuai Peng",
      "Di Fu",
      "Liangcai Gao",
      "Xiuqin Zhong",
      "Hongguang Fu",
      "Zhi Tang"
    ],
    "abstract": "The rapid development of large language models (LLMs) has spurred extensive\nresearch into their domain-specific capabilities, particularly mathematical\nreasoning. However, most open-source LLMs focus solely on mathematical\nreasoning, neglecting the integration with visual injection, despite the fact\nthat many mathematical tasks rely on visual inputs such as geometric diagrams,\ncharts, and function plots. To fill this gap, we introduce\n\\textbf{MultiMath-7B}, a multimodal large language model that bridges the gap\nbetween math and vision. \\textbf{MultiMath-7B} is trained through a four-stage\nprocess, focusing on vision-language alignment, visual and math\ninstruction-tuning, and process-supervised reinforcement learning. We also\nconstruct a novel, diverse and comprehensive multimodal mathematical dataset,\n\\textbf{MultiMath-300K}, which spans K-12 levels with image captions and\nstep-wise solutions. MultiMath-7B achieves state-of-the-art (SOTA) performance\namong open-source models on existing multimodal mathematical benchmarks and\nalso excels on text-only mathematical benchmarks. Our model and dataset are\navailable at\n{\\textcolor{blue}{\\url{https://github.com/pengshuai-rin/MultiMath}}}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00147v1",
    "published_date": "2024-08-30 07:37:38 UTC",
    "updated_date": "2024-08-30 07:37:38 UTC"
  },
  {
    "arxiv_id": "2409.10520v1",
    "title": "Achieving Responsible AI through ESG: Insights and Recommendations from Industry Engagement",
    "authors": [
      "Harsha Perera",
      "Sung Une Lee",
      "Yue Liu",
      "Boming Xia",
      "Qinghua Lu",
      "Liming Zhu",
      "Jessica Cairns",
      "Moana Nottage"
    ],
    "abstract": "As Artificial Intelligence (AI) becomes integral to business operations,\nintegrating Responsible AI (RAI) within Environmental, Social, and Governance\n(ESG) frameworks is essential for ethical and sustainable AI deployment. This\nstudy examines how leading companies align RAI with their ESG goals. Through\ninterviews with 28 industry leaders, we identified a strong link between RAI\nand ESG practices. However, a significant gap exists between internal RAI\npolicies and public disclosures, highlighting the need for greater board-level\nexpertise, robust governance, and employee engagement. We provide key\nrecommendations to strengthen RAI strategies, focusing on transparency,\ncross-functional collaboration, and seamless integration into existing ESG\nframeworks.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 1 table, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.10520v1",
    "published_date": "2024-08-30 05:48:03 UTC",
    "updated_date": "2024-08-30 05:48:03 UTC"
  },
  {
    "arxiv_id": "2408.17017v3",
    "title": "Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling",
    "authors": [
      "Guangya Wan",
      "Yuqi Wu",
      "Jie Chen",
      "Sheng Li"
    ],
    "abstract": "Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by\nsampling multiple reasoning paths,but it lacks a systematic approach to\ndetermine the optimal number of samples or select the most faithful rationale.\nTo address this limitation, we introduce Reasoning-Aware Self-Consistency\n(RASC), a novel framework that enhances sampling efficiency and reasoning\nfaithfulness by dynamically evaluating both outputs and rationales. RASC\nassesses the quality of reasoning and the consistency of answers for each\ngenerated sample, using these assessments to guide early stopping decisions and\nrationale selection. The framework employs criteria-based stopping and weighted\nmajority voting, enabling more informed choices on when to halt sampling and\nwhich rationale to select. Our comprehensive experiments across diverse\nquestion-answering datasets demonstrate that RASC outperforms existing methods,\nreducing sample usage by approximately 70% while maintaining accuracy.\nMoreover, RASC facilitates the selection of high-fidelity rationales, thereby\nimproving the faithfulness of LLM outputs. Our approach effectively addresses\nthe efficiency-accuracy trade-off in LLM reasoning tasks, offering a new\nperspective for more nuanced, faithful, and effective utilization of LLMs in\nresource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.17017v3",
    "published_date": "2024-08-30 05:14:59 UTC",
    "updated_date": "2025-02-04 03:59:49 UTC"
  },
  {
    "arxiv_id": "2408.17011v2",
    "title": "Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities",
    "authors": [
      "Jutika Borah",
      "Kumaresh Sarmah",
      "Hidam Kumarjit Singh"
    ],
    "abstract": "Imaging techniques such as Chest X-rays, whole slide images, and optical\ncoherence tomography serve as the initial screening and detection for a wide\nvariety of medical pulmonary and ophthalmic conditions respectively. This paper\ninvestigates the intricacies of using pretrained deep convolutional neural\nnetworks with transfer learning across diverse medical imaging datasets with\nvarying modalities for binary and multiclass classification. We conducted a\ncomprehensive performance analysis with ten network architectures and model\nfamilies each with pretraining and random initialization. Our finding showed\nthat the use of pretrained models as fixed feature extractors yields poor\nperformance irrespective of the datasets. Contrary, histopathology microscopy\nwhole slide images have better performance. It is also found that deeper and\nmore complex architectures did not necessarily result in the best performance.\nThis observation implies that the improvements in ImageNet are not parallel to\nthe medical imaging tasks. Within a medical domain, the performance of the\nnetwork architectures varies within model families with shifts in datasets.\nThis indicates that the performance of models within a specific modality may\nnot be conclusive for another modality within the same domain. This study\nprovides a deeper understanding of the applications of deep learning techniques\nin medical imaging and highlights the impact of pretrained networks across\ndifferent medical imaging datasets under five different experimental settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pages, 3 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.17011v2",
    "published_date": "2024-08-30 04:51:19 UTC",
    "updated_date": "2024-09-02 06:31:48 UTC"
  },
  {
    "arxiv_id": "2408.17010v1",
    "title": "Improving Time Series Classification with Representation Soft Label Smoothing",
    "authors": [
      "Hengyi Ma",
      "Weitong Chen"
    ],
    "abstract": "Previous research has indicated that deep neural network based models for\ntime series classification (TSC) tasks are prone to overfitting. This issue can\nbe mitigated by employing strategies that prevent the model from becoming\noverly confident in its predictions, such as label smoothing and confidence\npenalty. Building upon the concept of label smoothing, we propose a novel\napproach to generate more reliable soft labels, which we refer to as\nrepresentation soft label smoothing. We apply label smoothing, confidence\npenalty, and our method representation soft label smoothing to several TSC\nmodels and compare their performance with baseline method which only uses hard\nlabels for training. Our results demonstrate that the use of these enhancement\ntechniques yields competitive results compared to the baseline method.\nImportantly, our method demonstrates strong performance across models with\nvarying structures and complexities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages,6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.17010v1",
    "published_date": "2024-08-30 04:50:27 UTC",
    "updated_date": "2024-08-30 04:50:27 UTC"
  },
  {
    "arxiv_id": "2408.17003v5",
    "title": "Safety Layers in Aligned Large Language Models: The Key to LLM Security",
    "authors": [
      "Shen Li",
      "Liuyi Yao",
      "Lan Zhang",
      "Yaliang Li"
    ],
    "abstract": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ICLR 2025. The code is available at\n  https://github.com/listen0425/Safety-Layers",
    "pdf_url": "http://arxiv.org/pdf/2408.17003v5",
    "published_date": "2024-08-30 04:35:59 UTC",
    "updated_date": "2025-04-07 07:23:33 UTC"
  },
  {
    "arxiv_id": "2409.00143v2",
    "title": "Semantic-Guided Multimodal Sentiment Decoding with Adversarial Temporal-Invariant Learning",
    "authors": [
      "Guoyang Xu",
      "Junqi Xue",
      "Yuxin Liu",
      "Zirui Wang",
      "Min Zhang",
      "Zhenxi Song",
      "Zhiguo Zhang"
    ],
    "abstract": "Multimodal sentiment analysis aims to learn representations from different\nmodalities to identify human emotions. However, existing works often neglect\nthe frame-level redundancy inherent in continuous time series, resulting in\nincomplete modality representations with noise. To address this issue, we\npropose temporal-invariant learning for the first time, which constrains the\ndistributional variations over time steps to effectively capture long-term\ntemporal dynamics, thus enhancing the quality of the representations and the\nrobustness of the model. To fully exploit the rich semantic information in\ntextual knowledge, we propose a semantic-guided fusion module. By evaluating\nthe correlations between different modalities, this module facilitates\ncross-modal interactions gated by modality-invariant representations.\nFurthermore, we introduce a modality discriminator to disentangle\nmodality-invariant and modality-specific subspaces. Experimental results on two\npublic datasets demonstrate the superiority of our model. Our code is available\nat https://github.com/X-G-Y/SATI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "change Title, Authors, Abstract",
    "pdf_url": "http://arxiv.org/pdf/2409.00143v2",
    "published_date": "2024-08-30 03:28:40 UTC",
    "updated_date": "2024-09-11 04:44:06 UTC"
  },
  {
    "arxiv_id": "2409.00142v1",
    "title": "Dynamic Depth Decoding: Faster Speculative Decoding for LLMs",
    "authors": [
      "Oscar Brown",
      "Zhengjie Wang",
      "Andrea Do",
      "Nikhil Mathew",
      "Cheng Yu"
    ],
    "abstract": "The acceleration of Large Language Models (LLMs) with speculative decoding\nprovides a significant runtime improvement without any loss of accuracy.\nCurrently, EAGLE-2 is the state-of-the-art speculative decoding method,\nimproving on EAGLE with a dynamic draft tree. We introduce Dynamic Depth\nDecoding (DDD), which optimises EAGLE-2's tree drafting method using a dynamic\ndepth. This extends the average speedup that EAGLE-2 achieves over EAGLE by\n$44\\%$, giving DDD an average speedup of $3.16$x.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00142v1",
    "published_date": "2024-08-30 03:27:48 UTC",
    "updated_date": "2024-08-30 03:27:48 UTC"
  },
  {
    "arxiv_id": "2408.16984v2",
    "title": "Beyond Preferences in AI Alignment",
    "authors": [
      "Tan Zhi-Xuan",
      "Micah Carroll",
      "Matija Franklin",
      "Hal Ashton"
    ],
    "abstract": "The dominant practice of AI alignment assumes (1) that preferences are an\nadequate representation of human values, (2) that human rationality can be\nunderstood in terms of maximizing the satisfaction of preferences, and (3) that\nAI systems should be aligned with the preferences of one or more humans to\nensure that they behave safely and in accordance with our values. Whether\nimplicitly followed or explicitly endorsed, these commitments constitute what\nwe term a preferentist approach to AI alignment. In this paper, we characterize\nand challenge the preferentist approach, describing conceptual and technical\nalternatives that are ripe for further research. We first survey the limits of\nrational choice theory as a descriptive model, explaining how preferences fail\nto capture the thick semantic content of human values, and how utility\nrepresentations neglect the possible incommensurability of those values. We\nthen critique the normativity of expected utility theory (EUT) for humans and\nAI, drawing upon arguments showing how rational agents need not comply with\nEUT, while highlighting how EUT is silent on which preferences are normatively\nacceptable. Finally, we argue that these limitations motivate a reframing of\nthe targets of AI alignment: Instead of alignment with the preferences of a\nhuman user, developer, or humanity-writ-large, AI systems should be aligned\nwith normative standards appropriate to their social roles, such as the role of\na general-purpose assistant. Furthermore, these standards should be negotiated\nand agreed upon by all relevant stakeholders. On this alternative conception of\nalignment, a multiplicity of AI systems will be able to serve diverse ends,\naligned with normative standards that promote mutual benefit and limit harm\ndespite our plural and divergent values.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages (excl. references), 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16984v2",
    "published_date": "2024-08-30 03:14:20 UTC",
    "updated_date": "2024-11-06 20:34:14 UTC"
  },
  {
    "arxiv_id": "2408.16978v2",
    "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
    "authors": [
      "Jinghan Yao",
      "Sam Ade Jacobs",
      "Masahiro Tanaka",
      "Olatunji Ruwase",
      "Hari Subramoni",
      "Dhabaleswar K. Panda"
    ],
    "abstract": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "The Eighth Annual Conference on Machine Learning and Systems\n  (MLSys'25)",
    "pdf_url": "http://arxiv.org/pdf/2408.16978v2",
    "published_date": "2024-08-30 02:44:26 UTC",
    "updated_date": "2025-05-13 15:07:26 UTC"
  },
  {
    "arxiv_id": "2408.16975v3",
    "title": "Technical Report of HelixFold3 for Biomolecular Structure Prediction",
    "authors": [
      "Lihang Liu",
      "Shanzhuo Zhang",
      "Yang Xue",
      "Xianbin Ye",
      "Kunrui Zhu",
      "Yuxin Li",
      "Yang Liu",
      "Jie Gao",
      "Wenlai Zhao",
      "Hongkun Yu",
      "Zhihua Wu",
      "Xiaonan Zhang",
      "Xiaomin Fang"
    ],
    "abstract": "The AlphaFold series has transformed protein structure prediction with\nremarkable accuracy, often matching experimental methods. AlphaFold2,\nAlphaFold-Multimer, and the latest AlphaFold3 represent significant strides in\npredicting single protein chains, protein complexes, and biomolecular\nstructures. While AlphaFold2 and AlphaFold-Multimer are open-sourced,\nfacilitating rapid and reliable predictions, AlphaFold3 remains partially\naccessible through a limited online server and has not been open-sourced,\nrestricting further development. To address these challenges, the PaddleHelix\nteam is developing HelixFold3, aiming to replicate AlphaFold3's capabilities.\nLeveraging insights from previous models and extensive datasets, HelixFold3\nachieves accuracy comparable to AlphaFold3 in predicting the structures of the\nconventional ligands, nucleic acids, and proteins. The initial release of\nHelixFold3 is available as open source on GitHub for academic research,\npromising to advance biomolecular research and accelerate discoveries. The\nlatest version will be continuously updated on the HelixFold3 web server,\nproviding both interactive visualization and API access.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16975v3",
    "published_date": "2024-08-30 02:36:36 UTC",
    "updated_date": "2024-12-23 04:57:47 UTC"
  },
  {
    "arxiv_id": "2408.16967v1",
    "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
    "authors": [
      "Weijie Liu",
      "Zecheng Tang",
      "Juntao Li",
      "Kehai Chen",
      "Min Zhang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16967v1",
    "published_date": "2024-08-30 02:01:56 UTC",
    "updated_date": "2024-08-30 02:01:56 UTC"
  },
  {
    "arxiv_id": "2408.16966v2",
    "title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization Approaches",
    "authors": [
      "Chao Wang",
      "Neo Wu",
      "Lin Ning",
      "Jiaxing Wu",
      "Luyang Liu",
      "Jun Xie",
      "Shawn O'Banion",
      "Bradley Green"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16966v2",
    "published_date": "2024-08-30 01:56:57 UTC",
    "updated_date": "2024-09-05 23:18:00 UTC"
  },
  {
    "arxiv_id": "2408.16958v1",
    "title": "Discovery of False Data Injection Schemes on Frequency Controllers with Reinforcement Learning",
    "authors": [
      "Romesh Prasad",
      "Malik Hassanaly",
      "Xiangyu Zhang",
      "Abhijeet Sahu"
    ],
    "abstract": "While inverter-based distributed energy resources (DERs) play a crucial role\nin integrating renewable energy into the power system, they concurrently\ndiminish the grid's system inertia, elevating the risk of frequency\ninstabilities. Furthermore, smart inverters, interfaced via communication\nnetworks, pose a potential vulnerability to cyber threats if not diligently\nmanaged. To proactively fortify the power grid against sophisticated cyber\nattacks, we propose to employ reinforcement learning (RL) to identify potential\nthreats and system vulnerabilities. This study concentrates on analyzing\nadversarial strategies for false data injection, specifically targeting smart\ninverters involved in primary frequency control. Our findings demonstrate that\nan RL agent can adeptly discern optimal false data injection methods to\nmanipulate inverter settings, potentially causing catastrophic consequences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16958v1",
    "published_date": "2024-08-30 01:09:32 UTC",
    "updated_date": "2024-08-30 01:09:32 UTC"
  },
  {
    "arxiv_id": "2408.16952v1",
    "title": "Transient Fault Tolerant Semantic Segmentation for Autonomous Driving",
    "authors": [
      "Leonardo Iurada",
      "Niccolò Cavagnero",
      "Fernando Fernandes Dos Santos",
      "Giuseppe Averta",
      "Paolo Rech",
      "Tatiana Tommasi"
    ],
    "abstract": "Deep learning models are crucial for autonomous vehicle perception, but their\nreliability is challenged by algorithmic limitations and hardware faults. We\naddress the latter by examining fault-tolerance in semantic segmentation\nmodels. Using established hardware fault models, we evaluate existing hardening\ntechniques both in terms of accuracy and uncertainty and introduce ReLUMax, a\nnovel simple activation function designed to enhance resilience against\ntransient faults. ReLUMax integrates seamlessly into existing architectures\nwithout time overhead. Our experiments demonstrate that ReLUMax effectively\nimproves robustness, preserving performance and boosting prediction confidence,\nthus contributing to the development of reliable autonomous driving systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted ECCV 2024 UnCV Workshop -\n  https://github.com/iurada/neutron-segmentation",
    "pdf_url": "http://arxiv.org/pdf/2408.16952v1",
    "published_date": "2024-08-30 00:27:46 UTC",
    "updated_date": "2024-08-30 00:27:46 UTC"
  }
]